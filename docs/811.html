<html>
<head>
<title>Object Detection with Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积神经网络的目标检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-with-convolutional-neural-networks-c9d729eedc18#2022-01-30">https://towardsdatascience.com/object-detection-with-convolutional-neural-networks-c9d729eedc18#2022-01-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="fe17" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">基于卷积神经网络的目标检测</h1></div><div class=""><h2 id="1499" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多阶段(RCNN，快速RCNN，更快的RCNN)和单阶段(SSD，YOLO)结构的对象检测和他们的使用训练自己的对象检测模型进行解释</h2></div><p id="316b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我之前的帖子中，我们学习了什么是图像分类以及如何创建图像分类模型。</p><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/image-classification-with-convolutional-neural-networks-12a7b4fb4c91"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">基于卷积神经网络的图像分类</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">图像分类卷积和卷积神经网络综合指南，从实现…</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="lr l ls lt lu lq lv lw lh"/></div></div></a></div><p id="bea5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候进一步了解对象检测了。</p><p id="1bcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">物体检测(物体识别)</strong></p><ul class=""><li id="b6f4" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">虽然图像分类问题侧重于对图像进行分类，但在一幅图像中，我们可能要搜索不止一个类别，在对象识别中，我们的任务是找到所有这些类别，并将其放入最合适的<strong class="kk iu">框</strong>。</li></ul><p id="4d56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">包围盒— ROI(感兴趣区域)</em>:物体识别需要满足的新名词。当我们试图识别物体时，我们使用包围盒，在其中物体可能被检测到。我们将学习如何获得尽可能接近后来探测到的物体的盒子。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/6b1ec9d460a14de277d661d0470cd445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YGMLD9vflGQJpuKRwTcXQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><ul class=""><li id="2eaf" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">正如你可能注意到的，物体识别是一个比图像分类更复杂的任务，在图像分类中，我们试图<strong class="kk iu">定位</strong>和<strong class="kk iu">识别</strong>图像中的物体。</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3b9332c4af8a2e29293a284e1091b2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*uVefPzeamMpHo7xkTbHNDw.jpeg"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><p id="1fe6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次继续这3个不同的任务:</p><p id="8bd7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">图像分类</strong>:预测图像中物体的类别</p><p id="12a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">目标定位</strong>:定位图像中目标的存在，并用边界框指示其位置。</p><p id="041b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">物体检测</strong>:用一个包围盒定位物体的存在，并检测这些包围盒中被定位物体的类别</p><p id="503e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">迄今为止创建的对象识别神经网络架构分为两个主要组:<strong class="kk iu">多级</strong>与<strong class="kk iu">单级检测器。</strong></p><p id="355e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">多级探测器</em></p><ol class=""><li id="0076" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mx md me mf bi translated">RCNN <em class="mg"> 2014 </em></li><li id="8d5b" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">快速RCNN <em class="mg"> 2015 </em></li><li id="3399" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">更快的RCNN <em class="mg"> 2015 </em></li></ol><p id="b041" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">单级探测器</em></p><ol class=""><li id="6a7a" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mx md me mf bi translated">固态硬盘<em class="mg"> 2016 </em></li><li id="02e8" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">YOLO <em class="mg"> 2016 </em></li><li id="ef10" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">约洛夫2 <em class="mg"> 2016 </em>，约洛夫3 <em class="mg"> 2018 </em>，约洛夫4 <em class="mg"> 2020 </em>，约洛夫5 <em class="mg"> 2020 </em></li></ol><p id="4490" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从检查多级检测器开始🔮</p><h2 id="1b21" class="nd ne it bd nf ng nh dn ni nj nk dp nl kr nm nn no kv np nq nr kz ns nt nu nv bi translated"><strong class="ak">多级探测器</strong></h2><p id="6699" class="pw-post-body-paragraph ki kj it kk b kl nw ju kn ko nx jx kq kr ny kt ku kv nz kx ky kz oa lb lc ld im bi translated">正如我们从名称中所理解的，这些是具有两个独立阶段的对象检测器。通常，他们首先提取一些感兴趣的区域(边界框),然后对这些框进行分类以获得最终结果。这就是它们被称为<strong class="kk iu">基于区域的卷积神经网络</strong>的原因</p><ul class=""><li id="eeb3" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">RCNN</li></ul><p id="b0a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为这个家族的第一个成员，我们将在未来的版本中看到基础方法论及其缺点的改进。该模型的方法如下:</p><ol class=""><li id="5ff9" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mx md me mf bi translated">使用选择性搜索从输入图像中提取区域建议</li></ol><p id="4398" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择性搜索算法的工作方式是，它应用一种<strong class="kk iu">分割算法</strong>来寻找图像<strong class="kk iu">中的斑点，从而计算出什么可能是物体</strong>。选择性搜索递归地将这些区域组合成更大的区域，以创建<strong class="kk iu">2000个</strong>待调查区域。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d0104cc2bcb2e26df3809dcd9f9dbf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/0*Jmjsu17T-OU3kGrF.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><p id="2f80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">分段的选择性搜索</em></p><p id="3ca3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的帖子中，我提到了不同的算法来应用图像分割，不使用基于人工智能的方法，而是使用经典的基于计算机视觉的方法。</p><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/image-segmentation-with-classical-computer-vision-based-approaches-80c75d6d995f"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">基于经典计算机视觉方法的图像分割</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">经典的基于计算机视觉的图像分割方法，如阈值法、基于区域的方法、基于边缘检测的方法和</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="oc l ls lt lu lq lv lw lh"/></div></div></a></div><p id="de30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在同一类别中进行选择性搜索，它应用以下步骤来分割图像:</p><p id="2de5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先计算所有相邻区域之间的相似性。</p><p id="1ec1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将两个最相似的区域组合在一起，并计算结果区域与其相邻区域之间的新相似性。</p><p id="0ddf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后重复该过程，直到整个对象被覆盖在单个区域中。</p><p id="74e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.在每个ROI上使用CNN的特征提取来自上一步</p><p id="0766" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在根据分割提取了可能具有对象的几乎2000个可能的框之后，CNN被逐一应用于所有这些框，以提取用于下一步分类的特征</p><p id="8049" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.利用SVM和包围盒预测进行分类</p><p id="333f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，使用SVM(支持向量机)进行分类，并使用包围盒回归器，该模型给出最终的包围盒以及检测到的类，其中包围盒回归器的任务只是改进提议的包围盒以更好地包围对象。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi od"><img src="../Images/daa7410b3b1181b59d6b624de998c008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59Tjab1Gp6ugzfPhK4GYrg.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><p id="f09c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是RCNN的主要方法论，现在让我们看看它的弱点</p><ul class=""><li id="e62c" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">选择性搜索已经是一种复杂的算法，仅在第一步使用它会大大增加模型的计算成本。-&gt; <strong class="kk iu">慢</strong></li><li id="4a79" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">获得2000个区域以逐个应用特征提取又是太多的计算！-&gt; <strong class="kk iu">慢</strong></li></ul><p id="d205" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这给了我们<strong class="kk iu"> 47秒</strong>用于1个图像检测，因此不可能使用该模型进行实时物体检测任务！</p><ul class=""><li id="71bf" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">它不是一个端到端的可训练模型，因为选择性搜索算法不是一种可训练的方法，这使得不可能通过训练RCNN来开发区域提议。-&gt; <strong class="kk iu">某些部位不可训练</strong></li><li id="e277" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">使用SVM是没有端到端架构的另一个原因，我们需要分别培训SVM和CNN，这带来了更困难的任务。</li></ul><p id="ea9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，尽管它是一种先进的架构，比以前的模型具有更好的精度，但很明显，该模型需要改进，尤其是在速度性能方面。出于这个原因，我们发现自己正在检查<strong class="kk iu">快速RCNN </strong>模型，它是<strong class="kk iu">RCNN</strong>的改进版本。</p><ul class=""><li id="bdb2" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">快速RCNN</li></ul><ol class=""><li id="c2b7" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mx md me mf bi translated">它改变了区域提议步骤和特征提取的顺序，因此我们<strong class="kk iu">首先将CNN应用于输入图像，然后提取ROI。</strong>这样，我们不用对2000个不同的区域应用CNN，而是只应用一次，这提高了模型的速度性能。- &gt; <strong class="kk iu">不再那么慢了</strong></li><li id="2975" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">快速RCNN带来的另一个变化是使用带有softmax输出激活功能的<strong class="kk iu">全连接层，而不是SVM </strong>，这使得模型更加集成为一体式模型。- &gt; <strong class="kk iu">单步训练</strong></li><li id="a4e3" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">为了使来自区域提议的区域大小适应完全连接的层，应用ROI最大池。</li><li id="6522" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mx md me mf bi translated">边界框回归器和分类任务都是通过完全连接的层实现的，因此在这两个不同的输出成本组合起来更新模型的情况下应用“多任务损失”。-&gt; <strong class="kk iu">单阶段训练</strong></li></ol><p id="cc10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们获得了比RCNN快9倍的<strong class="kk iu">用于训练，比RCNN快0.32秒的<strong class="kk iu">用于检测1幅图像。</strong></strong></p><p id="6a38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这是个好消息，但是快速RCNN <strong class="kk iu">仍然有RCNN </strong>的一个缺点！即使我们不为快速RCNN中的每个建议区域应用CNN，使用选择性搜索提取2000个区域建议仍然是一个问题，这使得模型不必要地复杂。</p><ul class=""><li id="bcb6" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">更快的RCNN</li></ul><p id="f432" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以想象，更快的RCNN的主要变化是使用另一种方法而不是选择性搜索算法来提出区域。这种方法是使用<strong class="kk iu">一个单独的神经网络来学习区域提议</strong>。</p><p id="e418" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，该模型首先使用1个CNN来获得区域建议，然后遵循与快速RCNN完全相同的逻辑来检测对象，因此它使用这些建议来用CNN提取特征，然后用全连接层进行分类，并使用包围盒回归器来改进ROI。</p><p id="8964" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所知，<strong class="kk iu">Fast-RCNN</strong>在某些资源中被称为<strong class="kk iu"> RPN + Fast-RCNN </strong>，因为从Fast到Fast RCNN的唯一更新是区域提议方法。</p><p id="7637" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg"> RPN </em>:地区提案网。在这个使用inf快速RCNN的网络中，<strong class="kk iu">主要目标是预测锚盒的偏移以获得最终的包围盒。</strong>其中锚框是一些默认大小和形状的框。</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="e7e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着这些逐步的改进，我们看到模型的速度有了很大的提高，尤其是在快速RCNN中</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ol"><img src="../Images/8aa00f85bb95abd198e1ccddce8dd919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_xl1y7ZtGxPNIVHRpcWsA.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><p id="9345" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg"> mAP </em>:平均精度百分比是对象检测模型最常见的精度度量，其中计算每个类别的精度，然后取平均值。关于地图计算的更多信息，可以看看这篇很棒的帖子。</p><div class="le lf gp gr lg lh"><a href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener follow" target="_blank"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">用于对象检测的mAP(平均精度)</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">AP(平均精度)是一个测量物体检测器精度的常用指标，如更快的R-CNN、SSD等…</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">jonathan-hui.medium.com</p></div></div><div class="lq l"><div class="om l ls lt lu lq lv lw lh"/></div></div></a></div><h2 id="724f" class="nd ne it bd nf ng nh dn ni nj nk dp nl kr nm nn no kv np nq nr kz ns nt nu nv bi translated">单级检测器</h2><p id="8442" class="pw-post-body-paragraph ki kj it kk b kl nw ju kn ko nx jx kq kr ny kt ku kv nz kx ky kz oa lb lc ld im bi translated">现在该检查单级检测器了，与多级检测器相反，在单级检测器中，盒子预测和分类是同时进行的。</p><p id="1b98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> SSD:单次多盒探测器</strong></p><ul class=""><li id="be6b" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">标准的预训练神经网络被用作<strong class="kk iu">特征提取器</strong>，如<strong class="kk iu"> VGG16 </strong>、VGG19或ResNet。</li><li id="33ab" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">在这个CNN之后，应用一些附加的卷积层来获得不同大小的特征图。在获得边界框之后，这些特征图中的一些<strong class="kk iu">和来自特征提取网络的输出图被发送到由全连接层<strong class="kk iu">组成的图像分类部分</strong>。</strong></li><li id="67db" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">对于这些特征图中的每个单元，我们提取4或6个边界框，将它们直接发送到完全连接的层。与多级检测器相反，我们<strong class="kk iu">不对边界框</strong>做任何预测，我们只获取来自不同卷积层的特征图，并将它们转换成网格单元，其中每个像素为1个单元。以这些单元格为中心点，<strong class="kk iu">我们产生4或6个不同的边界框。</strong></li><li id="044b" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">为了产生边界框，我们有固定大小和形状的<strong class="kk iu">锚框</strong>，预定义的框。</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi on"><img src="../Images/567b3de430d7a7916b0754f4bb1bb834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ce_lUgG8RN6QEm0TQcmXBg.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><ul class=""><li id="b090" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">非最大抑制机制被应用作为最后一步，该步骤决定是否有多于一个的盒子指向相同的对象，我们选择最佳拟合。</li></ul><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oo"><img src="../Images/2d874d106cfd61168c5a515f0f634c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYx8t2-0u0zAMGfpWnE90Q.jpeg"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><ul class=""><li id="9d99" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">要了解更多关于非最大值抑制机制以及如何选择最佳拟合边界框的信息，您可以阅读这篇不错的帖子:</li></ul><div class="le lf gp gr lg lh"><a rel="noopener follow" target="_blank" href="/non-maximum-suppression-nms-93ce178e177c"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">非最大抑制(NMS)</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">一种消除目标检测中重复和误报的技术</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">towardsdatascience.com</p></div></div><div class="lq l"><div class="op l ls lt lu lq lv lw lh"/></div></div></a></div><p id="136f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们想象一下固态硬盘架构:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oq"><img src="../Images/ac968efdc0856bf66db62b5d88824e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MsR_6msIX4fduCRoi8HGvQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">作者图片</p></figure><ul class=""><li id="1391" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">我们看到整个架构有8732个包围盒，我们需要决定每个包围盒是否有类对象。我们有8732边界框的原因是:</li></ul><p id="33fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">con v4 _ 3:38×38×4 = 5776盒(每个电池4盒)<br/>con V7:19×19×6 = 2166盒(每个电池6盒)<br/> Conv8_2: 10×10×6 = 600盒(每个电池6盒)<br/> Conv9_2: 5×5×6 = 150盒(每个电池6盒)<br/>con v10 _ 2:3×4 = 36盒</p><p id="41a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5776+2166+60ch 0+150+36+4 = 8732</p><ul class=""><li id="d20c" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated"><strong class="kk iu"> SSD在VOC2007上以59 FPS实现了74.3%的mAP1，而更快的RCNN只有10 FPS。对于速度来说，这是非常令人印象深刻的进步，对吗？！</strong></li></ul><p id="7b3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">每幅图像的FPS与秒</em>:我们比较了RCNN家族使用“秒”单位的速度，因为他们的论文大多使用这个单位。每幅图像的<strong class="kk iu">秒</strong>是解释预测1幅图像需要多少秒。<strong class="kk iu">所以值越小，表示性能越好</strong>。另一方面，<strong class="kk iu"> FPS </strong>表示每秒<strong class="kk iu">帧</strong>，这个单位说明了我们在1秒钟内可以预测多少帧图像。<strong class="kk iu">因此，数值越大，性能越好</strong>。</p><p id="b1d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLO:你只看一次</p><p id="541e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其基本思想与固态硬盘非常相似，以下是YOLO的一些不同之处:</p><ul class=""><li id="1fc7" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">它使用基于Googlenet架构名称Darknet的定制网络。这个暗网架构由<strong class="kk iu"> 24个卷积层和2个全连接层</strong>组成。</li><li id="fcce" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">卷积部分使用Imagenet-1000数据集进行预训练，来自CNN的特征图用于转换为<strong class="kk iu">网格单元，就像SSD </strong>中一样。<strong class="kk iu">全连接层</strong>用于从这些特征地图网格预测<strong class="kk iu">边界框坐标</strong>。</li><li id="66bf" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">最后，我们有<strong class="kk iu"> 98 </strong>个职业的预测，而我们在SSD中有<strong class="kk iu"> 8732 </strong>个职业的预测，但是<strong class="kk iu"> YOLO有45 FPS和63.4mAP，</strong>所以都比SSD低。</li></ul><p id="4f16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLO在SSD之前发布，所以我们可以说它是第一个单级探测器，虽然SSD没有任何更新的版本，但YOLO到目前为止有5个不同的版本。让我们看看这个模型是如何随着时间的推移得到改进的。</p><p id="ff93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLO有一些弱点，比如:</p><ul class=""><li id="87d8" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">本地化(所以边界框有问题…)</li><li id="ab43" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">规模问题(来自训练数据集的不同大小的对象是一个问题)</li></ul><p id="f705" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> YOLOv2 </strong></p><p id="4cfc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">获得67 FPS和76.8 mAP </strong>！不错的改进，但如何改进？</p><ul class=""><li id="bdd7" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated"><strong class="kk iu">卷积层增加批量归一化</strong>。</li><li id="346d" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated"><strong class="kk iu">高分辨率分类器</strong>:原YOLO以224 × 224训练分类器网络，提高分辨率到448进行检测。这意味着网络必须同时切换到学习对象检测并适应新的输入分辨率。对于YOLOv2，输入图像分辨率提高至448 × 448。</li><li id="c546" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated"><strong class="kk iu">锚框</strong>:锚框用于计算偏移，而不是使用完全连接的层来预测边界框坐标。但与其他锚框方法不完全一样，他们不是手动选择先验，而是在训练集边界框上运行<strong class="kk iu"> k均值聚类</strong>，以自动为锚框找到良好的先验。</li><li id="5bf9" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">基本模型架构略有变化，名称为<strong class="kk iu"> Darknet-19 </strong>，有19个卷积层和5个最大池层，完全连接的层被删除，如我上面提到的🐟</li></ul><p id="3909" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> YOLOv3 </strong></p><ul class=""><li id="90f0" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">它使用的是<strong class="kk iu">逻辑量词(像sigmoid！)</strong>不像以前的版本那样使用softmax，使1个对象属于1个以上的类。例如，检测到的1个对象可以同时是狗和动物，对吗？如果预测高于给定的阈值，这些预测将传递到输出，因此您可以看到1个边界框同时表示2个类！</li><li id="1d6f" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">与YOLOv2只将1个框(最好的一个)分配给地面真实对象不同，在这一个中，如果边界框超过给定的阈值(使用0.5)，则进入<strong class="kk iu">损失计算，用于</strong>边界框预测、对象和类别预测。如果它不能通过，它只进行盒预测。(看看论文中的包围盒预测副标题吧！)</li><li id="5cdd" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">由于使用了特征提取器<strong class="kk iu"> Darknet-53 </strong>而不是稍微深一点的Darknet-19，因此<strong class="kk iu">更准确但速度更慢</strong>。</li></ul><p id="02b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在论文中，我找不到VOC数据集的准确性和速度指标，我以前一直将它与以前的模型进行比较，直到现在。所以我只能分享<strong class="kk iu">COCO数据集上的一些结果</strong></p><p id="e817" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">yolov 3–320给出51.5图，推理时间22ms，<br/>yolov 3–416给出55.3图，推理时间29 ms，<br/>yolov 3–608给出57.9图，推理时间51 ms，</p><p id="87ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而</p><p id="2952" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更快的RCNN给出了59.1的mAP，推理时间为172毫秒<br/>，并且</p><p id="3fc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">yolov 2–608给出了48个图，推理时间为40毫秒</p><p id="7f44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意-320，-416，-608是输入图像分辨率。</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="26b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⏰ 👽在继续之前稍微停顿一下！正如我检查论文一样，我意识到一些“准确性”指标可能会令人困惑，因为一些论文给出了VOC结果，一些论文给出了不同架构的COCO结果(小版本、大版本、不同主干的不同实验等)。<strong class="kk iu">我可以说，更快的RCNN通常比YOLO模型有更好的准确性，而YOLO模型几乎总是比它快，直到现在</strong>！</p><p id="5936" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">主干</em>:物体检测模型的特征提取器部分。通常，我们在之前的<a class="ae or" rel="noopener" target="_blank" href="/image-classification-with-convolutional-neural-networks-12a7b4fb4c91"> <strong class="kk iu">帖</strong> </a>中看到的图像分类架构像VGG、瑞思网等</p><p id="4a48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> YOLOv4 </strong></p><ul class=""><li id="d656" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">YOLOv4通过使用<strong class="kk iu"> BoF </strong> ( <strong class="kk iu">赠品袋</strong>)和几个<strong class="kk iu"> BoS(特价袋)</strong>来改进YOLOv3的模式。BoF提高了检测器的准确性，而不增加推断时间。它们只会增加培训成本。另一方面，BoS在显著提高目标检测精度的同时，增加了少量的推理成本。</li><li id="bf83" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">该模型使用Darknet中的跨级局部网络(CSPNet)，创建了一个新的特征提取器主干，称为<strong class="kk iu"> CSPDarknet53 </strong>。卷积架构基于改进的DenseNet。</li><li id="fd61" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">因此，就FPS而言，<strong class="kk iu"> YOLOv4比YOLOv3 </strong>精度高%10，速度快%12。</li></ul><p id="79f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mg">一袋赠品:</em>我们把这些只改变训练策略或者只增加训练成本的方法称为“一袋赠品”。" "[1]关于免费赠品袋对物体检测模型训练的影响的更详细研究请参考本文:<a class="ae or" href="https://arxiv.org/pdf/1902.04103.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1902.04103.pdf</a></p><p id="9c2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">约洛夫5 </strong></p><p id="8475" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能是讨论最多和最引人注目的YOLO模型，因为没有官方文件，但Roboflow初创公司在比较YOLOv5和YOLOv4时获得了非常令人印象深刻的结果。</p><p id="2d78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它使用<strong class="kk iu"> Pytorch </strong>代替c中实现的Darknet。</p><p id="7dd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据他们的结果:</p><ul class=""><li id="8aa6" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">YOLOv5几乎比YOLOv4快3倍！</li><li id="2444" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated">YOLOv5比YOLOv4小了将近%90！</li></ul><p id="73a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLOv5的一些关键特性如下:</p><ul class=""><li id="7a69" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">使用<strong class="kk iu"> CSPDarknet53作为YOLOv4 </strong></li><li id="920a" class="lx ly it kk b kl my ko mz kr na kv nb kz nc ld mc md me mf bi translated"><strong class="kk iu">镶嵌数据增加</strong>被添加到模型中</li></ul></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="0ee2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在结束理论部分之前，我想补充一下，YOLO有一些“<strong class="kk iu">微型</strong>”版本，特别是当你想拥有超快的模型时使用。几乎每一个YOLO版本(v2，v3，v4，v5)都有它的小YOLO版本，精确度稍低，但速度快了将近4倍。值得尝试的实时对象检测项目！</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="8de2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理论部分做完了！是时候学习如何真正使用这些模型来为您自己的数据集进行对象检测了。</p><p id="760f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这一部分，我将分享两个非常好的更快的RCNN和YOLOV4的仓库。</p><p id="4200" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用更快的RCNN进行对象检测</strong></p><p id="af17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我使用了一个定制的实现库，因为我找不到任何官方的更快的RCNN实现。这是一个非常简单的repo，可以下载并用于Keras实现的训练和测试。</p><p id="62fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的GitHub链接中，您会发现我的repo是从基础源代码派生出来的，我在其中添加了一些更多的属性，并详细解释了如何构建和使用这个实现。</p><div class="le lf gp gr lg lh"><a href="https://github.com/YCAyca/Faster_RCNN_for_Open_Images_Dataset_Keras" rel="noopener  ugc nofollow" target="_blank"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">GitHub-YCAyca/Faster _ RCNN _ for _ Open _ Images _ Dataset _ Keras:Faster R-CNN for Open Images Dataset by…</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">快速RCNN实现的分叉存储库。在本文档中，构建说明和回购的使用将…</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">github.com</p></div></div><div class="lq l"><div class="os l ls lt lu lq lv lw lh"/></div></div></a></div><p id="cc62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">用YOLOV4进行物体检测</strong></p><p id="35eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于YOLOV4，我使用了基于C的官方实现，并分叉为我的GitHub repo，其中我添加了关于<strong class="kk iu">如何构建</strong>和<strong class="kk iu">如何使用</strong> YOLOV4的指令。这个实现很难理解和构建，所以我强烈建议查看一下repo，并按照步骤一步一步来训练和测试您自己的数据集。</p><div class="le lf gp gr lg lh"><a href="https://github.com/YCAyca/darknet" rel="noopener  ugc nofollow" target="_blank"><div class="li ab fo"><div class="lj ab lk cl cj ll"><h2 class="bd iu gy z fp lm fr fs ln fu fw is bi translated">GitHub-YCAyca/darknet:yolov 4/Scaled-yolov 4/YOLO-用于对象检测的神经网络…</h2><div class="lo l"><h3 class="bd b gy z fp lm fr fs ln fu fw dk translated">YOLOv4 / Scaled-YOLOv4 / YOLO -用于对象检测的神经网络(Windows和Linux版本的Darknet ) - GitHub …</h3></div><div class="lp l"><p class="bd b dl z fp lm fr fs ln fu fw dk translated">github.com</p></div></div><div class="lq l"><div class="ot l ls lt lu lq lv lw lh"/></div></div></a></div><p id="f3c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢您的关注，我希望这对您自己开始使用对象检测模型有所帮助！</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="7d38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择性搜索算法官方论文:<a class="ae or" href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" rel="noopener ugc nofollow" target="_blank">https://IVI . fnwi . UVA . nl/ISIS/publications/2013/uijlingsijcv 2013/uijlingsijcv 2013 . pdf</a></p><p id="561b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">RCNN官方论文:<a class="ae or" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2524.pdf</a></p><p id="62f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">快速RCNN官方论文:<a class="ae or" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1504.08083.pdf</a></p><p id="08b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更快RCNN官方论文:<a class="ae or" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01497.pdf</a></p><p id="f32c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SSD官方论文:<a class="ae or" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.02325</a></p><p id="83f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">https://arxiv.org/pdf/1506.02640.pdf官方文件:<a class="ae or" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="8737" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLOv2官方论文:<a class="ae or" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></p><p id="8eae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">YOLOv3官方论文:<a class="ae or" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767.pdf</a></p><p id="0db3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] Yolov4官方论文:<a class="ae or" href="https://arxiv.org/pdf/2004.10934.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2004.10934.pdf</a></p><p id="dc4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Yolov5还没有正式论文！</p></div></div>    
</body>
</html>