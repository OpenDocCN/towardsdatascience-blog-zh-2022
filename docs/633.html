<html>
<head>
<title>A Quick Introduction to Bag of Words and TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词袋和TF-IDF的快速介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-introduction-to-bag-of-words-and-tf-idf-fbd3ab84ecbf#2022-01-21">https://towardsdatascience.com/a-quick-introduction-to-bag-of-words-and-tf-idf-fbd3ab84ecbf#2022-01-21</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""><h1 id="d880" class="pw-post-title is it iu bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">单词袋和TF-IDF的快速介绍</h1></div><div class=""><h2 id="b76e" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">机器学习和自然语言处理模型如何处理文本</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/85a534c358493213966672643b91bb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xq7Fh__FzseNhI-9"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">克里斯蒂安·卢在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="febc" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是一袋单词</h1><p id="f8b2" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">你有没有想过，当机器学习(ML)是基于数学和统计的时候，ML是如何处理文本的？我的意思是，文本毕竟不是一串数字…对不对？</p><p id="6dd4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我给你介绍一下<strong class="lu iv">字袋(BoW) </strong>模型。除了它听起来有趣的名字，BoW是自然语言处理(NLP)的重要组成部分，也是对文本进行机器学习的基础之一。</p><p id="2549" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">BoW仅仅是一个无序的单词及其频率(计数)的集合。例如，让我们看看下面这段文字:</p><pre class="kk kl km kn gu mu mv mw bn mx my bi"><span id="43e7" class="mz lb iu mv b be na nb l nc nd">"I sat on a plane and sat on a chair."<br/><br/>and  chair  on  plane  sat<br/>  1      1   2      1    2</span></pre><p id="f4c2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">注意</strong>:令牌(单词)的长度必须为<code class="fe ne nf ng mv b">2</code>或更多字符。</p><p id="275b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">就这么简单。让我们看看这是如何计算的，如果再多加几个句子，或者我们通常所说的<strong class="lu iv">文档</strong>会是什么样子。首先，我们将导入必要的库。</p><pre class="kk kl km kn gu mu mv mw bn mx my bi"><span id="406e" class="mz lb iu mv b be na nb l nc nd">import pandas as pd<br/>import numpy as np<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span></pre><p id="a55b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们将使用来自<a class="ae kz" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>的文本矢量器。我们已经导入了其中的两个，一个是创建弓的<code class="fe ne nf ng mv b">CountVectorizer</code>，另一个是<code class="fe ne nf ng mv b">TfidfVectorizer</code>，我们稍后会谈到。让我们以字符串列表<em class="mt">的形式处理几个文档。</em></p><pre class="kk kl km kn gu mu mv mw bn mx my bi"><span id="f7bf" class="mz lb iu mv b be na nb l nc nd">corpus = [<br/>    "Tune a hyperparameter.",<br/>    "You can tune a piano but you can't tune a fish.",<br/>    "Fish who eat fish, catch fish.",<br/>    "People can tune a fish or a hyperparameter.",<br/>    "It is hard to catch fish and tune it.",<br/>]<br/><br/>vectorizer = CountVectorizer(stop_words='english') <br/>X = vectorizer.fit_transform(corpus) <br/>pd.DataFrame(X.A, columns=vectorizer.get_feature_names_out())</span></pre><pre class="nh mu mv mw bn mx my bi"><span id="cdcb" class="mz lb iu mv b be na nb l nc nd">catch  eat  fish  hard  hyperparameter  like  people  piano  tune<br/>0      0    0     0     0               1     0       0      0     1<br/>1      0    0     1     0               0     1       0      1     1<br/>2      1    1     3     0               0     0       0      0     0<br/>3      0    0     1     0               1     0       1      0     1<br/>4      1    0     1     1               0     0       0      0     1</span></pre><p id="74ce" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以更清楚地看到矩阵是什么样子的。<strong class="lu iv">行是文档</strong>,<strong class="lu iv">列是专有字</strong>。<code class="fe ne nf ng mv b">CountVectorizer</code>带有各种内置的文本预处理，比如我们在这里做的删除停用词。如果一个句子包含一个单词，它会计算出现的次数，如果没有，它会使用一个<code class="fe ne nf ng mv b">0</code>。BoW方法会将更多的权重放在出现频率更高的单词上，因此您必须删除停用的单词。</p><h1 id="4375" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是TF-IDF？</h1><p id="ea21" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们看到，BoW模型会计算出现的次数，并对大多数单词赋予更多的权重。另一种叫做<strong class="lu iv"> TF-IDF </strong>的方法正好相反。TF-IDF代表<strong class="lu iv">Term Frequency-Inverse Document Frequency</strong>，很好地说明了它使用的方法。它不是给更频繁出现的单词更多的权重，而是给不太频繁出现<strong class="lu iv">的单词更高的权重(在整个语料库中<em class="mt"/>)。在你的文本中有更多的领域特定语言的用例中，这个模型通过给这些不经常出现的单词加权来表现得更好。让我们像以前一样在相同的文件上运行它。</strong></p><p id="3d8f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在我们做逆文档频率之前，让我们用<strong class="lu iv">术语频率</strong>，它将像一把弓一样工作，但是给我们每个术语的值，其中矢量(文档)<code class="fe ne nf ng mv b">= 1</code>的<em class="mt">平方和</em>。这与BoW模型相同，但是是归一化的。</p><pre class="kk kl km kn gu mu mv mw bn mx my bi"><span id="f233" class="mz lb iu mv b be na nb l nc nd">vectorizer = TfidfVectorizer(stop_words='english', use_idf=False) <br/>X = vectorizer.fit_transform(corpus) <br/>df = pd.DataFrame(np.round(X.A,3), columns=vectorizer.get_feature_names_out())<br/>df</span></pre><pre class="nh mu mv mw bn mx my bi"><span id="1071" class="mz lb iu mv b be na nb l nc nd">catch    eat   fish  hard  hyperparameter  people  piano   tune<br/>0  0.000  0.000  0.000   0.0           0.707     0.0  0.000  0.707<br/>1  0.000  0.000  0.408   0.0           0.000     0.0  0.408  0.816<br/>2  0.302  0.302  0.905   0.0           0.000     0.0  0.000  0.000<br/>3  0.000  0.000  0.500   0.0           0.500     0.5  0.000  0.500<br/>4  0.500  0.000  0.500   0.5           0.000     0.0  0.000  0.500</span></pre><p id="0b84" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在第一个文档(<code class="fe ne nf ng mv b">0</code>)中，我们看到单词<code class="fe ne nf ng mv b">hyperparameter</code>，我们可以认为它是一个非常特定于领域的单词，与在整个语料库中更频繁出现的<code class="fe ne nf ng mv b">tune,</code>具有相同的权重。</p><p id="d6eb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于文档<code class="fe ne nf ng mv b">2</code>，我们可以看到单词<code class="fe ne nf ng mv b">fish</code>的值很大，因为它经常出现。现在我们有了自己的值，让我们看看应用<strong class="lu iv">逆文档频率</strong>时会发生什么。</p><pre class="kk kl km kn gu mu mv mw bn mx my bi"><span id="3df2" class="mz lb iu mv b be na nb l nc nd">vectorizer = TfidfVectorizer(stop_words='english') <br/>X = vectorizer.fit_transform(corpus) <br/>df = pd.DataFrame(np.round(X.A,3), columns=vectorizer.get_feature_names_out())<br/>df</span></pre><pre class="nh mu mv mw bn mx my bi"><span id="ca36" class="mz lb iu mv b be na nb l nc nd">catch    eat   fish   hard  hyperparameter  people  piano   tune<br/>0  0.000  0.000  0.000  0.000           0.820   0.000  0.000  0.573<br/>1  0.000  0.000  0.350  0.000           0.000   0.000  0.622  0.701<br/>2  0.380  0.471  0.796  0.000           0.000   0.000  0.000  0.000<br/>3  0.000  0.000  0.373  0.000           0.534   0.661  0.000  0.373<br/>4  0.534  0.000  0.373  0.661           0.000   0.000  0.000  0.373</span></pre><p id="d9a0" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们来对比一下这两款。在第一个文档中，<code class="fe ne nf ng mv b">hyperparameter</code>比tune具有更高的权重，因为它比单词tune少出现<code class="fe ne nf ng mv b">50%</code>。但是，请注意，权重仍然依赖于文档；<code class="fe ne nf ng mv b">tune</code>根据不同的上下文，在不同的文档中具有不同的权重。</p><p id="8beb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于文档<code class="fe ne nf ng mv b">2</code>，我们可以看到，由于术语<code class="fe ne nf ng mv b">fish</code>出现的频率，它的权重稍低。</p><h1 id="a73d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="8823" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">希望这个快速概述有助于您理解BOW和TF-IDF。虽然使用像<strong class="lu iv"> Scikit-Learn </strong>这样的库来构建它们确实很容易，但是理解概念以及一个库何时可能比另一个库执行得更好还是很重要的。如果你想在实践中看到TF-IDF，请查看关于数据科学的<a class="ae kz" rel="noopener" target="_blank" href="/clustering-text-with-k-means-c2953c8a9772">聚类文本和<strong class="lu iv">的k-Means </strong></a>的帖子。</p><p id="de7a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在<a class="ae kz" href="https://github.com/broepke/BoW_TF-IDF" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上查看这篇文章的完整代码</p><p id="c522" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，可以考虑报名成为一名媒体成员。一个月5美元，让你可以无限制地访问成千上万篇文章。如果你使用<a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener">我的链接</a>注册，我会赚一小笔佣金，不需要你额外付费。</p><h1 id="cb60" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">参考</h1><ol class=""><li id="8d52" class="ni nj iu lu b lv lw ly lz mb nk mf nl mj nm mn nn no np nq bi translated"><a class="ae kz" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="noopener ugc nofollow" target="_blank">对单词袋模型的温和介绍</a></li><li id="f469" class="ni nj iu lu b lv nr ly ns mb nt mf nu mj nv mn nn no np nq bi translated"><a class="ae kz" href="https://jonathansoma.com/lede/foundations/classes/text%20processing/tf-idf/" rel="noopener ugc nofollow" target="_blank">乔纳森·索玛的TF-IDF</a></li></ol></div></div>    
</body>
</html>