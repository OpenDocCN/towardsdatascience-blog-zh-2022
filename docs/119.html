<html>
<head>
<title>Semantic Segmentation of Aerial Imagery captured by a Drone using Different U-Net Approaches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用不同U-Net方法对无人机拍摄的航空影像进行语义分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-captured-by-a-drone-using-different-u-net-approaches-91e32c92803c#2022-01-05">https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-captured-by-a-drone-using-different-u-net-approaches-91e32c92803c#2022-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="2ffe" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用不同U-Net方法对无人机拍摄的航空影像进行语义分割</h1></div><div class=""><h2 id="df8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用python从头开始实施配置的U-Net架构，并使用不同方法对无人机拍摄的航空影像进行语义分割</h2></div><p id="6de4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在机器学习中，模型通过各种应用进行训练，特别是在深度学习和图像数据集上。基于卷积运算的方法在许多领域得到了广泛的研究，尤其是增强现实中的手臂检测、自动驾驶汽车、无人机航拍图像、战争技术。人眼有能力很容易地分类和区分它所看到的东西。然而，这种能力在人工智能技术中的等价物，即理解图像的问题，在计算机视觉的标题下讨论。顾名思义(计算机视觉)，就是用计算机可以理解的方式引入(分类)图像，下一步就是通过使用不同的方法，使对这些图像的操作成为可能。本文解释了一种分割方法，即U-Net架构，它是为生物医学图像分割而开发的，并包括一个使用U-Net分割无人机拍摄的航空影像的真实项目。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/9495b35f32d13b89590f4f59a8395882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C4kw6hYCcrrUy7Qa"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">Jaromír Kavan 在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><pre class="lf lg lh li gt lv lw lx ly aw lz bi"><span id="7388" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu"><em class="mg">Table of Contents<br/></em>1. Semantic Segmentation<br/>2. U-Net Architecture from <br/>3. Tutorial<br/>3.1. Data Preprocessing<br/>3.2. Semantic Segmentation using U-Net from scratch<br/>3.3. Semantic Segmentation using U-Net with Transfer Learning<br/>4. Conclusion<br/>5. References</strong></span></pre><h1 id="52e5" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">语义分割</h1><p id="3259" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">图像是由数学数字构成的像素矩阵。在图像处理技术中，对这些数学数字进行一些调整，然后以不同的方式表达图像，并使其适合于相关的研究或解释。卷积过程是一种基本的数学像素运算，它提供了从不同角度评估图像的机会。例如，图像的边缘检测可以通过应用的过滤器来完成，或者图像可以通过将其从RGB格式转换为灰度从不同的角度来解释和使用。基于深度学习模型和卷积层，对图像内容进行了更全面的研究，如特征提取和分类。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nd"><img src="../Images/327b7182a9e24ce2a41c5f73fbc35262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDyq6AKU28HY4-A1IH_wng.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图一。对象检测，语义分割，实例分割，<a class="ae lu" href="http://A. Arnab et al., “Conditional Random Fields Meet Deep Neural Networks for Semantic Segmentation,” IEEE Signal Process. Mag., vol. XX, 2018." rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="5099" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上图所示，用包围盒检测图像内容中的物体称为<strong class="kk iu"> <em class="mg">物体检测</em> </strong>。<strong class="kk iu"> <em class="mg">语义分割</em> </strong>这是一种逐像素的标注操作，就是用一个标签，也就是颜色，来显示图片中同一类型的物体(天空、猫、狗、人、道路、汽车、山、海等)。<strong class="kk iu"> <em class="mg">即时分割</em> </strong>每个实例都被单独标记，通过用不同的颜色显示来分隔每个对象。如上所述，在这些操作的背景下，为了不同的用途，已经开发了各种不同的CNN模型和复杂的模型。PSPNet、DeepLab、LinkNet、U-Net、Mask R-CNN只是其中的一些模式。我们可以说，在自动驾驶汽车等基于机器学习的应用中，分割过程是项目的眼睛。下面的视频包含实时语义分割过程，该过程比较了人类视角和PSPNet视角。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ne nf l"/></div></figure><blockquote class="ng nh ni"><p id="a854" class="ki kj mg kk b kl km ju kn ko kp jx kq nj ks kt ku nk kw kx ky nl la lb lc ld im bi translated">简单来说，计算机视觉中的语义分割是一种像素级的标注方法。如果同一类型的对象用单一颜色表示，则称为语义分割，如果每个对象用唯一的颜色(标签)表示，则称为实例分割。</p></blockquote><h1 id="297e" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">U-Net架构</h1><p id="1e8a" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">U-Net是一种特定类型的卷积神经网络架构，是2015年德国弗赖堡大学计算机科学系和生物信号研究BIOSS中心为生物医学图像(计算机断层扫描、显微图像、MRI扫描等)开发的。文章—<em class="mg">“U-Net:用于生物医学图像分割的卷积网络”</em> —可在<strong class="kk iu"> </strong> <a class="ae lu" href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">此处</strong> </a>链接访问。当我们考虑技术思想时，该模型由<strong class="kk iu">编码器(收缩)</strong>和<strong class="kk iu">解码器(提取)</strong>组成，其中<strong class="kk iu">编码器是<em class="mg">下采样</em>(大部分是迁移学习中预训练的权重)，而</strong>解码器是<em class="mg">上采样</em>部分，由于其方案是如图2所示的U形，所以命名为U-Net。该模型可根据不同的研究进行配置。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nm"><img src="../Images/50920de2edb7a1bcfc4233c92eb11bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5kH0lsf9awWZU3dLh9vKA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图二。U-Net架构，<a class="ae lu" href="http://O. Ronneberger, P. Fischer, and T. Brox, “LNCS 9351 - U-Net: Convolutional Networks for Biomedical Image Segmentation,” 2015, doi: 10.1007/978-3-319-24574-4_28." rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="176f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以下教程中，U-Net模型被配置用于航空影像的语义分割，如下所示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nn nf l"/></div></figure><p id="cc7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们一步一步地看上面的代码块和图2(从左上到右上，跟随“U”字母的流程):</p><p id="7766" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.输入定义为<strong class="kk iu"> 256x256x3 </strong>尺寸。</p><p id="b2bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.作为使用16个过滤器的<code class="fe no np nq lw b">conv_1</code>的结果，获得了<strong class="kk iu"> 256x256x16 </strong>尺寸。用<code class="fe no np nq lw b">pool_1</code>中的Maxpooling减少到<strong class="kk iu"> 128x128x16 </strong>。</p><p id="5772" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.使用过滤器号为32的<code class="fe no np nq lw b">conv_2</code>得到<strong class="kk iu"> 128x128x32 </strong>的尺寸，同样，使用<code class="fe no np nq lw b">pool_2</code>得到<strong class="kk iu"> 64x64x32 </strong>的尺寸。</p><p id="ea26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.<strong class="kk iu"> 64x64x64 </strong>的尺寸由<code class="fe no np nq lw b">conv_3</code>用64号滤镜获得，<strong class="kk iu"> 32x32x64 </strong>用<code class="fe no np nq lw b">pool_3</code>获得。</p><p id="2819" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.<strong class="kk iu">32×32×128</strong>的尺寸是用过滤数为128的<code class="fe no np nq lw b">conv_4</code>得到的，作为<code class="fe no np nq lw b">pool_4</code>的结果，得到了<strong class="kk iu">16×16×128</strong>。</p><p id="eb40" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.对于滤波器数为256的<code class="fe no np nq lw b">conv_5</code>，得到<strong class="kk iu">16×16×256</strong>的大小，上采样从这一点开始。在滤镜数为128和(2x2)的<code class="fe no np nq lw b">u6</code>中，用<code class="fe no np nq lw b">Conv2DTranspose</code>将<code class="fe no np nq lw b">conv_5</code>转换为<strong class="kk iu"> 32x32x128 </strong>，用<code class="fe no np nq lw b">u6</code>、<code class="fe no np nq lw b">conv_4</code>进行层叠。因此，<code class="fe no np nq lw b">u6</code>更新为<strong class="kk iu"> 32x32x256 </strong>。有了128个滤镜的<code class="fe no np nq lw b">conv_6</code>，就变成了<strong class="kk iu"> 32x32x128 </strong>。</p><p id="f555" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.通过应用于<code class="fe no np nq lw b">conv_6</code>并将<code class="fe no np nq lw b">u7 </code>与<code class="fe no np nq lw b">conv_3</code>串联，滤波器编号为64和(2x2)的<code class="fe no np nq lw b">u7</code>变为<strong class="kk iu"> 64x64x64 </strong>。此操作的结果是，<code class="fe no np nq lw b">u7</code>被定义为<strong class="kk iu"> 64x64x128 </strong>并且变成带有<code class="fe no np nq lw b">conv_7</code>的<strong class="kk iu"> 64x64x64 </strong>。</p><p id="8584" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8.通过应用于<code class="fe no np nq lw b">conv_7</code>并将<code class="fe no np nq lw b">u7 </code>与<code class="fe no np nq lw b">conv_2</code>连接起来，具有32和(2x2)过滤器的<code class="fe no np nq lw b">u8</code>变成<strong class="kk iu"> 128x128x32 </strong>。该操作的结果是，<code class="fe no np nq lw b">u8</code>被定义为<strong class="kk iu"> 128x128x64 </strong>并变成带有<code class="fe no np nq lw b">conv_8</code>的<strong class="kk iu"> 128x128x32 </strong>。</p><p id="fa6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">9.通过应用于<code class="fe no np nq lw b">conv_8</code>并将<code class="fe no np nq lw b">u9</code>与<code class="fe no np nq lw b">conv_1</code>连接起来，滤波器号为16且(2x2)的<code class="fe no np nq lw b">u9</code>变成了<strong class="kk iu"> 256x256x16 </strong>。该操作的结果是，<code class="fe no np nq lw b">u9</code>被定义为<strong class="kk iu"> 256x256x32 </strong>并且变成带有<code class="fe no np nq lw b">conv_9</code>的<strong class="kk iu"> 256x256x16 </strong>。</p><p id="1935" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">10.输出使用<em class="mg"> softmax激活</em>完成分类过程，最终输出采用<strong class="kk iu"> 256x256x1 </strong>的形式。</p><blockquote class="ng nh ni"><p id="cced" class="ki kj mg kk b kl km ju kn ko kp jx kq nj ks kt ku nk kw kx ky nl la lb lc ld im bi translated">各种比率的下降用于防止过度拟合。</p></blockquote><h1 id="ee36" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">辅导的</h1><p id="6076" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在编码部分，可以用不同的方法训练数据集。在本研究中，当<em class="mg"> RGB(原始图像)</em>数据集被定义为x时，模型通过使用<em class="mg">地面实况</em>(分段标记图像)作为y来训练。在以后的文章中，还将讨论使用掩膜数据集的方法。RGB图像和地面实况如图3所示。该研究旨在用这种方法训练数据集，并使外部呈现的图像能够像在训练数据中一样执行分割。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nr"><img src="../Images/9df9a0578b23684b8c06e511ca5f83af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnHHbYW0AYOICpgeaLdiVA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图3。原始RGB图像(左)和地面真相(右)，作者提供的图像</p></figure><p id="4127" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它专注于编码架构部分，而不是实现高性能。这是由于处理图像数据集时涉及的计算复杂性。例如，虽然原始图像是6000x4000像素，但它已被转换为256x256像素，以避免计算复杂性。通过这样的操作，旨在通过放弃准确性来使编码架构正确工作。</p><p id="78c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mg">数据集链接:</em></strong><a class="ae lu" href="https://www.kaggle.com/awsaf49/semantic-drone-dataset" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/awsaf49/semantic-drone-dataset</a></p><p id="58ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mg">许可:</em> </strong> <em class="mg"> </em> CC0:公共域</p><h2 id="5617" class="ma mb it bd mi ns nt dn mm nu nv dp mq kr nw nx ms kv ny nz mu kz oa ob mw oc bi translated">数据预处理</h2><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nn nf l"/></div></figure><p id="feec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1-导入库。<code class="fe no np nq lw b">from architecture import multiclass_unet_architecture, jacard, jacard_loss</code>是从上一节定义并导入的。</p><p id="b27a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有6000x4000像素和相应标签的2- RGB原始图像被调整为256x256像素。</p><p id="1a4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3- <code class="fe no np nq lw b">MinMaxScaler</code>用于缩放RGB图像。</p><p id="5e21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4-导入地面事实的标签。在地面实况数据集中检测到23个标签，并且基于像素值将标签分配给图像的内容。</p><p id="cf88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5标签数据集是用于分类的<code class="fe no np nq lw b">one-hot-encoded</code>，数据被分为训练集和测试集。</p><h2 id="c0f2" class="ma mb it bd mi ns nt dn mm nu nv dp mq kr nw nx ms kv ny nz mu kz oa ob mw oc bi translated">使用U-Net的语义分割(从头开始)</h2><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nn nf l"/></div></figure><p id="d246" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6-准确性和<strong class="kk iu"> Jaccard </strong>索引用于训练过程。优化器设置为<code class="fe no np nq lw b">‘adam’</code>，损失设置为<code class="fe no np nq lw b">‘categorical_crossentropy’</code>，因为这只是一个复杂的分类问题。模型符合这些设置。</p><p id="883f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7- <code class="fe no np nq lw b">validation_jaccard</code>培训过程和损失被可视化。图4展示了val_jaccard。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f24ff3604ecb937731d866c1c0ff2b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*bIEkFx5W6KasD7cfxCU5vg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图4。Jaccard值按时代，图像按作者</p></figure><p id="c243" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8-测试数据集的Jaccard索引值计算为<strong class="kk iu"> 0.5532 </strong>。</p><p id="62a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从测试数据集中选择9- 5个随机图像，用训练好的算法进行预测，结果如图5所示。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/de4c5c3b81fcb3aa17f19ac4c898cc4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*Z96kWXJjU3tmteK93lPT3A.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图5。5张随机测试图片的预测结果，图片由作者提供</p></figure><h2 id="fe2c" class="ma mb it bd mi ns nt dn mm nu nv dp mq kr nw nx ms kv ny nz mu kz oa ob mw oc bi translated">基于迁移学习的U-Net语义切分</h2><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nn nf l"/></div></figure><p id="e1f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">10-使用<strong class="kk iu"> resnet34 </strong>重新准备数据集。<code class="fe no np nq lw b">“Adam”</code>设为优化器，<code class="fe no np nq lw b">“categorical_crossentropy”</code>设为损失函数，训练模型。</p><p id="65f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">11- <code class="fe no np nq lw b">validation_jaccard</code>和培训过程的损失被可视化。图6展示了val_jaccard。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4b15c353b899c09573088ec6f77df6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*xS7aFl2OTFZ8tJHszFSBpg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图6。Jaccard值按时代，图像按作者</p></figure><p id="543e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">12-测试数据集的Jaccard索引值计算为<strong class="kk iu"> 0.6545 </strong>。</p><p id="455b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从测试数据集中选择13- 5个随机图像，用训练好的算法进行预测，结果如图7所示。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi of"><img src="../Images/ade6d07a8803b89aebbfb3a5861d9aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Jdq6EPchN6wsWa2EYhEag.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图7。5张随机测试图片的预测结果，图片由作者提供</p></figure><h1 id="0d36" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">结论</h1><p id="bb1c" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">提出了一种基于生物医学图像分割的U-Net的卫星图像语义分割方法。研究中考虑了两种主要方法。第一种方法涉及用从头开始的实现来训练已配置的u-net模型。第二种方法涉及用迁移学习技术训练模型，即预先训练的权重。在实现部分中，对应的地面真实图像被一热编码，并且该模型像分类过程一样被训练。Jaccard指数用于衡量指标。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nn nf l"/></div></figure><p id="b0ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">调整大小过程不是推荐的方法，因为在分割操作中大小变化会有不希望的偏移，但是由于计算复杂性，数据集从6000x4000调整到256x256。因此，模型的成功率非常低。防止这种情况的一些主要方法是使用高分辨率数据集和/或使用<code class="fe no np nq lw b">patchfying</code>(裁剪图像和相应的地面实况图像)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi og"><img src="../Images/8457800e9ef8079eb9683d462dec1083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UIbLJ3Wl5BCxzwyrTg67w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图8。两种方法的比较，图片由作者提供</p></figure><p id="599a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用调整后的数据集，评估了两种不同的方法，结果如图8所示。查看Jaccard指数值，使用迁移学习方法获得0.6545，而使用临时构建的模型获得0.5532。可以看出，用预训练模型获得的分割过程更成功。</p><blockquote class="oh"><p id="5e3c" class="oi oj it bd ok ol om on oo op oq ld dk translated">不同的方法将在后续文章中用不同的编码方法来介绍。</p></blockquote><div class="or os ot ou ov ow"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd iu gy z fp pb fr fs pc fu fw is bi translated">机器学习指南</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">所有与机器学习相关的文章</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk lo ow"/></div></div></a></div><h1 id="b453" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">参考</h1><p id="abc3" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">O.Ronneberger，P. Fischer和T. Brox，“LNCS 9351-U-Net:生物医学图像分割的卷积网络”，2015年，doi:10.1007/978–3–319–24574–4 _ 28。</p><p id="5d6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">A.Arnab <em class="mg">等</em>，“条件随机场遇上深度神经网络进行语义切分”，<em class="mg"> IEEE信号处理。玛格。</em>，第二十卷，2018。</p><p id="59fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">J.陈永春，郭福荣，顾国辉，2020。2020.</p><p id="fc84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">J.Maurya，R. Hebbalaguppe和P. Gupta，“用于手势界面的头戴式设备上的实时手部分割”，Proc .— Int。糖膏剂图像处理。ICIP，第4023–4027页，2018年，doi:10.1109/icip . 1455125367</p></div></div>    
</body>
</html>