<html>
<head>
<title>Random Projection in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的随机投影</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-projection-in-python-705883a19e48#2022-01-11">https://towardsdatascience.com/random-projection-in-python-705883a19e48#2022-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="20aa" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">降维</h2><div class=""><h1 id="d434" class="pw-post-title jb jc it bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated">Python中的随机投影</h1></div><div class=""><h2 id="3363" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在这篇文章中，我简要介绍了降维工具——随机投影的概念，以及它在Python中的实现。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0fb634137d1d5165647cfc7dc30561b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CgNYl50X6LqszwKV"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Gabriella Clare Marino 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f3c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> D </span>维度缩减通常是处理大数据时必须做的预处理。最广泛使用的方法之一是<strong class="lk jd">主成分分析</strong> ( <strong class="lk jd"> <em class="mn"> PCA </em> </strong>)，但是PCA的主要缺点是对于高维数据来说计算量非常大。一种替代方法是<strong class="lk jd">随机投影</strong>，这是一种计算成本较低的降维工具。</p><p id="d7a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我将简要描述一下<em class="mn">随机投影</em>的思想及其在Python中的实现。</p><h2 id="2068" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">TLDR</h2><blockquote class="ng nh ni"><p id="107e" class="li lj mn lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">1.随机投影适用于高维数据处理。</p><p id="4e60" class="li lj mn lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">2.随机投影的核心思想在Johnson-Lindenstrauss引理中给出。它基本上说明了高维空间中的数据可以被投影到低得多的维空间中，而几乎没有距离失真。</p><p id="525a" class="li lj mn lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">3.随机投影中的投影矩阵可以使用高斯分布生成，这被称为高斯随机投影；或者稀疏矩阵，称为稀疏随机投影。</p><p id="b9fa" class="li lj mn lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">4.随机预测可用作管道中的早期步骤之一，以更好地理解数据。</p><p id="4ba7" class="li lj mn lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">5.X_new = <code class="fe nm nn no np b">sklearn.random_projection</code>。GaussianRandomProjection(n _ components = ' auto '，eps = 0.05)。拟合_转换(X)</p></blockquote><h2 id="62ed" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">随机投影的概念</h2><p id="a7e7" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">随机投影是一种降维工具。“<em class="mn">投影</em>表示该技术将数据从高维空间投影到低维空间，“<em class="mn">随机</em>表示投影矩阵是随机生成的。直截了当，对吧？</p><p id="862d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，投影是怎么做的？通过投影矩阵的线性变换。</p><p id="b27b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具体来说，假设我们有一个带有<em class="mn"> d </em>行(特征)和<em class="mn"> N </em>列(样本)的原始数据集<em class="mn"> X </em>，我们希望将特征维数从<em class="mn"> d </em>减少到<em class="mn">k</em>(<em class="mn">d&gt;&gt;k</em>)。<em class="mn">随机投影</em>使用随机生成的具有<em class="mn"> k </em>行和<em class="mn"> d </em>列的投影矩阵<em class="mn"> R </em>来获得变换后的新数据集<em class="mn"> X_new </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/3bd3aa4d8d7a63fe05d1a0fe23a7d3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*yrNyeR4v7N3_8WRP8bhfzg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从k维到d维的投影(图片由作者提供)</p></figure><p id="a1e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机投影不会改变样本之间的成对相似性(或距离)，这得到了<strong class="lk jd">约翰逊-林登斯特劳斯引理</strong>的支持。它基本上说明了高维空间中的数据可以被投影到低得多的维空间中，而几乎没有距离失真。</p><p id="50d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们用数学语言来描述这个引理，它是这样的，</p><p id="af80" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给定<em class="mn"> 0 &lt; ε &lt; 1 </em>，一个特征中<em class="mn"> d </em>维的数据集<em class="mn"> X </em>和<em class="mn"> N </em>个数据点<em class="mn">k</em>&gt;<em class="mn">8ln</em>(<em class="mn">N</em>)/<em class="mn">ε</em>，从<em class="mn"> d </em>有一个线性映射(投影)<em class="mn"> f </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/1389b28becd69c3aa71842ff5ad39f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntmnwtuuYt14srMxNiEu7w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">约翰逊-林登施特劳斯引理方程(图片由作者提供)</p></figure><p id="cdee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="mn"> u </em>和<em class="mn"> v </em>均来自原始特征空间，<em class="mn"> f(u) </em>和<em class="mn"> f(v) </em>来自变换后的特征空间。</p><p id="486c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于上面的描述，我们可以看到两件重要的事情。首先，投影并没有完全保持从尺寸<em class="mn"> d </em>到尺寸<em class="mn"> k </em>的成对距离，而是带有一个误差参数<em class="mn"> ε </em>。第二，对于任何固定的<em class="mn"> ε </em>和样本大小<em class="mn"> N </em>，对于成对距离失真的“可接受”水平，存在最小的最终变换尺寸<em class="mn"> k </em>。如果我们仍然想更努力地减少尺寸<em class="mn"> k </em>，我们可能需要通过接受更大的<em class="mn"> ε </em>来失去失真的容差。</p><p id="8ad1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，在样本大小固定的情况下，在成对距离的失真<em class="mn"> ε </em>和最终特征空间的最小维度<em class="mn"> k </em>之间存在权衡。</p><h2 id="bd30" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">生成投影矩阵的两种方法</h2><p id="75be" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">生成投影矩阵<em class="mn"> R </em>的一种方法是让<em class="mn"> {r_ij} </em>遵循正态分布。而另一种方式是使用稀疏随机矩阵作为<em class="mn"> R </em>。“稀疏”是指<em class="mn"> {r_ij} </em>的大部分为零。典型使用的分布是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8d44d86e46e559f279b7ce53aea08170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*CEbEqt0QTSEqAkMVZGr6kQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">稀疏投影矩阵的分布。</p></figure><p id="90c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">稀疏随机投影比高斯随机投影的计算量小，主要是因为两个原因。第一，上面的公式只涉及整数运算；第二，稀疏投影矩阵几乎没有非零值(更稀疏)。</p><h2 id="1ede" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">实现随机投影的Python代码</h2><p id="f5fa" class="pw-post-body-paragraph li lj it lk b ll nq kd ln lo nr kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">随机投影作为一种降维工具，可以作为数据分析的前期步骤之一。基于<em class="mn">约翰逊-林登斯特劳斯引理</em>，我们可以研究数据集的结构，并在一个低得多的维度上可视化数据。</p><p id="c6bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是<strong class="lk jd"> Python </strong>中实现<em class="mn">高斯</em>和<em class="mn">稀疏随机投影</em>的代码，</p><pre class="ks kt ku kv gt ny np nz oa aw ob bi"><span id="ee4e" class="mo mp it np b gy oc od l oe of"># Gaussian Random Projection<br/><strong class="np jd">from</strong> <strong class="np jd">sklearn.random_projection</strong> <strong class="np jd">import</strong> GaussianRandomProjection<br/><br/>projector = GaussianRandomProjection(n_components='auto',eps=0.05)<br/>X_new = projector.fit_transform(X)<br/></span></pre><p id="b7ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="mn"> X </em>是我的原始数据，<em class="mn"> n_components </em>是目标空间的维数(<em class="mn"> auto </em>表示根据参数自动调整，<em class="mn"> ε </em>)，<em class="mn"> eps </em>是<em class="mn"> Johnson-Lindenstrauss引理</em>中的<em class="mn"> ε </em>，<em class="mn">X _ 1</em></p><p id="149f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">稀疏随机投影也是如此，</p><pre class="ks kt ku kv gt ny np nz oa aw ob bi"><span id="dcf8" class="mo mp it np b gy oc od l oe of"># Sparse Random Projection<br/><strong class="np jd">from</strong> <strong class="np jd">sklearn.random_projection</strong> <strong class="np jd">import</strong> SparseRandomProjection<br/><br/>projector = SparseRandomProjection(n_components='auto',density = 'auto', eps=0.05)<br/>X_new = projector.fit_transform(X)</span></pre><p id="637f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="mn">密度</em>是随机投影矩阵中的非零分量密度。</p><p id="bbde" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！希望文章有帮助。</p><p id="ad3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢读这篇文章，这里有一些其他的片段，</p><p id="1663" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/gaussian-mixture-models-with-python-36dabed6212a">https://towards data science . com/Gaussian-mixture-models-with-python-36 dabed 6212 a</a></p><p id="583b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/fuzzy-c-means-clustering-with-python-f4908c714081">https://towards data science . com/fuzzy-c-means-clustering-with-python-f 4908 c 714081</a></p><p id="617d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/regression-splines-in-r-and-python-cfba3e628bcd">https://towards data science . com/regression-splines-in-r-and-python-cfba 3 e 628 BCD</a></p><h2 id="ee1f" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">参考资料:</h2><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Random_projection" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">随机投影-维基百科</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在数学和统计学中，随机投影是一种用于减少一组点的维数的技术…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div><div class="os l"><div class="ot l ou ov ow os ox lb oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">约翰逊-林登施特劳斯引理-维基百科</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在数学中，约翰逊-林登施特劳斯引理是以威廉·b·约翰逊和乔拉姆·林登施特劳斯命名的一个结果…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.GaussianRandomProjection.html" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">sklearn.random_projection。高斯随机投影</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">编辑描述</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">scikit-learn.org</p></div></div><div class="os l"><div class="oy l ou ov ow os ox lb oj"/></div></div></a></div><div class="og oh gp gr oi oj"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jd gy z fp oo fr fs op fu fw jc bi translated">sklearn.random_projection。稀疏投影</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">通过稀疏随机投影降低维数。稀疏随机矩阵是密集随机矩阵的一种替代…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">scikit-learn.org</p></div></div><div class="os l"><div class="oz l ou ov ow os ox lb oj"/></div></div></a></div></div></div>    
</body>
</html>