<html>
<head>
<title>Dropout is Drop-Dead Easy to Implement</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">辍学是非常容易实现的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dropout-is-drop-dead-easy-to-implement-67f08a87ccff#2022-01-11">https://towardsdatascience.com/dropout-is-drop-dead-easy-to-implement-67f08a87ccff#2022-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="a7be" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">辍学是非常容易实现的</h1></div><div class=""><h2 id="cbe3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">大概是1行python…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d47c008c979577d9984faa44d2be283d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PsJrHsx-CESZLmDDxZOIzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自描述辍学的原始<a class="ae ky" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><h1 id="3441" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">简介</strong></h1><p id="86bb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们都听说过辍学。从历史上看，这是正则化神经网络的最著名的方法之一，尽管现在它已经有些失宠，并被批量归一化所取代。不管怎样，这是一项很酷的技术，在PyTorch中实现起来非常简单。在这篇简短的博文中，我们将从头实现dropout，并展示我们在PyTorch中获得了与标准dropout相似的性能。运行这个快速教程的所有实验的完整笔记本可以在<a class="ae ky" href="https://gist.github.com/nbertagnolli/35eb960d08c566523b4da599f6099b41" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="448a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是辍学？</h1><p id="3531" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">退出是指在每个训练步骤中有效地随机删除神经网络的一些节点。这个想法是，通过不过度依赖任何一个节点，这将有助于网络变得更加健壮。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d47c008c979577d9984faa44d2be283d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PsJrHsx-CESZLmDDxZOIzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自最初描述辍学的<a class="ae ky" href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></figure><p id="0176" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">实际上，我们忽略了每个训练周期中的一些随机节点集。很简单，对吧？</p><h1 id="ae18" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数学</h1><p id="43ae" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因此，如果我们将网络中的每一层都视为一个矩阵，那么在每个训练步骤中，我们只需要将一组随机节点清零。这闻起来很像哈达玛产品。考虑一个单层神经网络，其中层由矩阵<strong class="lt iu"> A </strong>表示。该网络将接受二维输入并输出三维结果，因此<strong class="lt iu"> A </strong>是2×3。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/5651c2904e62ef60953c057accd26719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JYPvc1HS3x4JrlXn9UHJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们单层网络的可视化。</p></figure><p id="4404" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们想将一些随机的节点集清零，我们可以创建一个二进制矩阵<strong class="lt iu"> D </strong>，其中每一列都是零，概率为<em class="mt"> p </em>，然后取元素乘积。我们可以通过创建一个具有与<strong class="lt iu"> A </strong>相同列数的阈值化统一向量，然后在乘法运算中广播来轻松实现这一点。</p><p id="b9c7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们在PyTorch中实现它:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="b3f7" class="mz la it mv b gy na nb l nc nd">p = .5<br/>A = torch.tensor([[1, 2, 3], [4, 5, 6]])<br/>D = (torch.empty(A.size()[1]).uniform_(0, 1) &gt;= p)<br/>x = A.mul(D)</span><span id="66a4" class="mz la it mv b gy ne nb l nc nd">====Output===<br/>tensor([[0, 2, 3],<br/>        [0, 5, 6]])</span></pre><p id="4354" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如您所见，上述计算将会遗漏一些随机的节点集。</p><p id="fb66" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一种方法是使用DropConnect，我们随机删除权重，而不是节点。这里唯一的区别是创建一个与<strong class="lt iu">和</strong>大小相同的随机均匀矩阵，而不是进行广播。这看起来像这样:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="63e0" class="mz la it mv b gy na nb l nc nd">p = .5<br/>A = torch.tensor([[1, 2, 3], [4, 5, 6]])<br/>D = (torch.empty(A.size()).uniform_(0, 1) &gt;= p)<br/>x = A.mul(D)</span><span id="42b9" class="mz la it mv b gy ne nb l nc nd">====Output===<br/>tensor([[1, 2, 0],<br/>        [0, 5, 6]])</span></pre><h1 id="07f3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">在PyTorch中实现一个层</strong></h1><p id="1b7a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有了最初的数学知识，让我们在PyTorch中实现一个dropout层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="1c58" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第6–7行检查以确保传递给该层的概率实际上是一个概率。</p><p id="f699" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第10行确定该层是处于训练模式还是测试模式。这很重要，因为我们不希望仅在训练期间在推断过程中遗漏节点。这个简单的if语句为我们解决了这个问题。</p><p id="5c36" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第11行完成了我们之前讨论的所有魔术，它创建了一个与<strong class="lt iu"> x </strong>大小相同的二进制矩阵，其中有一个概率为<em class="mt"> p </em>，任何节点都被遗漏了。</p><p id="b113" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们现在可以在一个简单的模型中使用这种下降。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="4588" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们用标准PyTorch dropout替换我们的自定义dropout，我们会得到相同的结果。相当整洁！</p><h1 id="a64b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后一个音符</h1><p id="8805" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">敏锐的读者会注意到，这并不是辍学者在实践中应该采取的方式。我们不是通过一个节点被训练的次数来标准化的。想一想这个。当我们丢弃一堆随机节点时，一些节点将比其他节点得到更多的训练，并且在最终预测中应该具有不同的权重。考虑到这一点，我们需要在推理期间通过保持概率<code class="fe nh ni nj mv b">1/(1-p)</code>的倒数来调整每个节点的权重。但是在推理的时候这样做是很痛苦的。大多数实现在实践中所做的是在训练期间按这个量来缩放权重。所以真正的辍学层应该是这样的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="d4c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">就是这样！感谢阅读！</p></div></div>    
</body>
</html>