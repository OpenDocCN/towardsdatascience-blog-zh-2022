<html>
<head>
<title>Scientific Documents Similarity Search With Deep Learning Using Transformers (SciBERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用变形金刚进行深度学习的科学文档相似性搜索(SciBERT)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scientific-documents-similarity-search-with-deep-learning-using-transformers-scibert-d47c4e501590#2022-01-17">https://towardsdatascience.com/scientific-documents-similarity-search-with-deep-learning-using-transformers-scibert-d47c4e501590#2022-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="d30d" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用变形金刚进行深度学习的科学文档相似性搜索(SciBERT)</h1></div><div class=""><h2 id="3001" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这篇文章是一个全面的概述，为具有k-NN和余弦相似性的文档构建一个语义相似性搜索工具</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6c28d195827f9ea099ae7f733e4e886b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GeIPsXx9Q4VWj8vr"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Maksym Kaharlytskyi 在<a class="ae ky" href="https://unsplash.com/photos/Q9y3LRuuxmg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="a4ee" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="ac11" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当阅读一篇有趣的文章时，您可能希望从大量数据中找到相似的文章。手动处理显然不是我们应该采取的策略。那么，为什么不利用人工智能的力量来解决这类问题呢？从这篇文章中，你将能够使用SciBERT和两种不同的相似性方法(余弦和k-NN)来找到与你的特定查询在意义上最相似的科学文章。</p><p id="4af8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="ms">注意:嵌入</em> </strong>和<strong class="lt iu"> <em class="ms">向量</em> </strong>将在整篇文章中互换使用，表示同一件事。</p><p id="52cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的工作流程给出了构建我们的工具的所有步骤，从数据提取到推荐类似的文章。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8a40b2c7022f721ec0cee06c65690368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RnJeA0Xho4xztwVNP-DJ8Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正在解决的解决方案的一般工作流程(图片由作者提供)</p></figure><h1 id="65f9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于数据</h1><ul class=""><li id="be20" class="mu mv it lt b lu lv lx ly ma mw me mx mi my mm mz na nb nc bi translated">这是CORD-19数据集，包含超过59，000篇学术文章，包括超过48，000篇关于新冠肺炎、新型冠状病毒和相关冠状病毒的全文。</li><li id="aa87" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">这些数据由白宫和一个领先的研究团体联盟免费提供，以帮助全球研究产生见解，支持正在进行的抗击这种传染病的斗争。</li><li id="82d3" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">它可以从Kaggle 的<a class="ae ky" href="https://www.kaggle.com/keitazoumana/scientific-document-similarity-search-with-scibert/data" rel="noopener ugc nofollow" target="_blank">页面下载。</a></li><li id="c505" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">关于数据集的更多细节可在此<a class="ae ky" href="https://www.kaggle.com/danielwolffram/discovid-ai-a-search-and-recommendation-engine/data" rel="noopener ugc nofollow" target="_blank">页面</a>中找到。</li></ul><p id="551c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在继续下一步之前，重要的是考虑导入以下对项目成功至关重要的库。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有用_libs.py</p></figure><h2 id="50ed" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">收集数据</h2><p id="c77a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">数据从Kaggle下载并保存在一个名为<strong class="lt iu"> <em class="ms"> input的文件夹中。</em> </strong>使用shape函数我们得到(47110，16)，意思是它包含47110篇文章，每篇有16列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">load_data_scibert.py</p></figure><p id="1542" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">16个专栏——但哪一个最重要？</p><p id="1f96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是关于数据中缺失信息百分比的一些详细信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/3d9da488c75ad164f304f84fcf4e953d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJecVINxYX4XI6IaYjRx-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">关注抽象列的数据中缺失值的百分比(图片由作者提供)</p></figure><p id="872e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了简单起见，我们将把我们的分析集中在<strong class="lt iu">摘要</strong>列，它也是0%缺失数据的列。但是您可以使用其他文本列，比如<strong class="lt iu">body _ text</strong>；由你来决定。此外，为了加快处理速度，我们将使用2000个观察值的子集。</p><p id="c8fa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">数据中的文章摘要</strong></p><p id="60cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们看看一些随机的文章。这里我们将打印限制在前100个单词，因为有些单词很长。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">show_random_articles.py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/d7973b97ef159c7e7c777f99e9768229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iK6wlhhXcOLUkIVdYlOddg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">三篇随机文章的数据摘要(作者图片)</p></figure><h1 id="04fb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据处理和矢量化</h1><p id="693a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这一步旨在对文章的摘要文本进行矢量化，以便我们可以执行相似性分析。因为我们处理的是科学文档，所以我们将使用SciBERT，这是一种用于科学文本数据的预训练语言模型。你可以在<a class="ae ky" href="https://www.semanticscholar.org/paper/SciBERT%3A-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo/5e98fe2163640da8ab9695b9ee9c433bb30f5353" rel="noopener ugc nofollow" target="_blank">语义学者</a>上找到更多关于它的信息。</p><p id="d68c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这一部分涉及的主要步骤是:</p><h2 id="75e4" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">加载模型工件</h2><p id="76fb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">加载预训练的模型和标记器。在加载模型时，我们需要将<strong class="lt iu"> <em class="ms">输出_隐藏_状态</em> </strong>设置为<strong class="lt iu">真</strong>，这样我们就可以提取嵌入了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">load _模型_工件. py</p></figure><h2 id="9246" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">将文本数据转换为嵌入内容</h2><p id="6dda" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个函数<strong class="lt iu"><em class="ms">convert _ single _ abstract _ to _ embedding</em></strong>大多是受Chris McCormick的BERT Word<a class="ae ky" href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#3-extracting-embeddings" rel="noopener ugc nofollow" target="_blank">embedding教程</a>的启发。它旨在使用预先训练的模型为给定的文本数据创建嵌入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">嵌入_from_text.py</p></figure><p id="8295" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">对单个文本数据进行测试</strong></p><p id="3a72" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里我们在第30篇文章上测试函数。你可以选择任何你想要的数字，只要它存在于数据中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测试_嵌入. py</p></figure><p id="c3d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="ms">第6行</em> </strong>显示<strong class="lt iu">嵌入形状:(768，)。</strong>这意味着一个向量由768个值组成。最后，我们可以使用<strong class="lt iu"><em class="ms">convert _ overall _ text _ to _ embedding</em></strong>函数将转换过程应用于数据中的所有文章。但是，在此之前，我们将使用<strong class="lt iu"><em class="ms">get _ min _ valid _ data</em></strong>函数从数据中删除一些列，以便在最终的查询搜索结果中有更少的列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="81b7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用前面的帮助器函数，我们可以创建一个新的列，为每篇文章包含其对应的抽象嵌入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">创建最终版本</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/215381858f069c30a2a80821cc733f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ibC6lrZVecgEHS5tL50isQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有嵌入列的前3行数据(作者提供的图片)</p></figure><p id="ff29" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">到目前为止，一切顺利！一切终于为相似性搜索设置好了。</p><h1 id="012f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">相似性搜索</h1><p id="559d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们可以执行给定的<strong class="lt iu"> <em class="ms">查询</em> </strong>向量和所有嵌入向量之间的相似性分析。<strong class="lt iu"> <em class="ms">查询</em> </strong>处理将类似于余弦和k-NN。下面是负责这个的函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">流程查询. py</p></figure><h2 id="7de6" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">余弦相似性搜索</h2><p id="8062" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">两个文档嵌入之间的余弦相似性度量这些文档的相似程度，而不考虑这些嵌入的大小。它测量在多维空间中投影的两个向量之间的角度的余弦。</p><ul class=""><li id="5554" class="mu mv it lt b lu mn lx mo ma nz me oa mi ob mm mz na nb nc bi translated"><strong class="lt iu"> <em class="ms">余弦相似度为1 </em> </strong>表示两个文档100%相似</li><li id="4604" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><strong class="lt iu"> <em class="ms">余弦相似度为0 </em> </strong>意味着两个文档有0%的相似度</li></ul><p id="7923" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下函数返回与查询文本相似的前N篇文章(N是要返回的相似文章的数量)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">余弦_建议. py</p></figure><p id="fbc6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们可以调用函数来获取前5篇最相似的文章。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">余弦_结果. py</p></figure><p id="b6d9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是前一个查询的结果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/55ea7b91afadf11b56b191999c4d3990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*D7NR7ttVAZNP1tYMjAeltQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与查询最相似的前5篇文章(作者图片)</p></figure><p id="d897" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了更好地理解，我们可以只看前两篇最相似的文章。</p><p id="3241" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">下面的<em class="ms">查询正文</em>下面的</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/faa61418afdb5e4a28975c169ce9f469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OZGDOJa05K6xLXGcOkdIKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从原始数据帧中查询文本数据</p></figure><p id="5670" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="ms">最相似的两篇文章来自前面的top_articles数据帧</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/1d4452c7baa6b0f633999fa3d67d79cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e-KbK6XsYD6S8B27cAONSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从推荐的数据框架中摘录最相似的两篇文章(图片由作者提供)</p></figure><h2 id="fc35" class="nk la it bd lb nl nm dn lf nn no dp lj ma np nq ll me nr ns ln mi nt nu lp nv bi translated">基于KNN和Faiss的相似性搜索</h2><p id="d014" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">FAISS是由脸书人工智能研究所开发的库。根据他们的<a class="ae ky" href="https://github.com/facebookresearch/faiss/wiki" rel="noopener ugc nofollow" target="_blank">维基页面</a>:</p><blockquote class="of og oh"><p id="fddc" class="lr ls ms lt b lu mn ju lw lx mo jx lz oi mp mc md oj mq mg mh ok mr mk ml mm im bi translated">Faiss是一个用于高效相似性搜索和密集向量聚类的库。它包含在任意大小的向量集中搜索的算法，直到那些可能不适合RAM的向量</p></blockquote><p id="f5c9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是使用之前构建的嵌入来构建搜索算法的步骤</p><ul class=""><li id="9c80" class="mu mv it lt b lu mn lx mo ma nz me oa mi ob mm mz na nb nc bi translated">创建平面索引。该索引使用L2(欧几里德)距离度量来测量查询向量和所有向量(嵌入)之间的相似性。</li><li id="a4d4" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">将所有向量添加到索引中</li><li id="eb35" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">定义我们想要的相似文件的数量</li><li id="b359" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated">运行相似性搜索以获得结果</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">faiss_setup.py</p></figure><p id="d3f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用相同的查询文本数据对k-NN部分执行搜索。您可以随意将其更改为另一个值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">faiss_run_search.py</p></figure><ul class=""><li id="5fb5" class="mu mv it lt b lu mn lx mo ma nz me oa mi ob mm mz na nb nc bi translated"><strong class="lt iu"> <em class="ms"> I </em> </strong>包含所有同类文章的索引</li><li id="488a" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><strong class="lt iu"> <em class="ms"> D </em> </strong>包含所有相似文章的L2距离值。距离越小，文章与查询越相似。</li></ul><p id="53f3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">注意:我决定故意分解所有的步骤，以确保你能正确理解它们。但是你可以把所有的东西放在一个函数里。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">faiss_show_recommendations.py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/742e9ccab0980f78698ac65f03105eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*cqLmYr4iQcnNLslln0EBWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">与查询最相似的5篇文章(图片由作者提供)</p></figure><p id="26ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">观察</strong></p><ul class=""><li id="b5e9" class="mu mv it lt b lu mn lx mo ma nz me oa mi ob mm mz na nb nc bi translated">第一个文档的L2 = 0，这意味着100%相似。这是显而易见的，因为查询是与其自身进行比较的，但是我们可以简单地将其从分析中删除。</li></ul><h1 id="755b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="50f2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们研究了使用SciBERT嵌入、余弦和k-NN相似性方法实现语义相似性搜索工具的整个过程。同时，我也希望它对你有所帮助！</p><p id="75ad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可以在下面找到更多资源来进一步学习。在YouTube 上关注我，了解更多互动会话！</p><h1 id="d0c0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外资源</h1><p id="0489" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://github.com/keitazoumana/SciBERT-Document-Similarity-Search" rel="noopener ugc nofollow" target="_blank">GitHub上的文章源代码</a></p><p id="e3de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://www.semanticscholar.org/paper/SciBERT%3A-A-Pretrained-Language-Model-for-Scientific-Beltagy-Lo/5e98fe2163640da8ab9695b9ee9c433bb30f5353" rel="noopener ugc nofollow" target="_blank"> SciBERT:科技文本的预训练语言模型</a></p><p id="224a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#3-extracting-embeddings" rel="noopener ugc nofollow" target="_blank">伯特单词嵌入教程</a></p><p id="645a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://research.facebook.com/research-areas/facebook-ai-research-fair/" rel="noopener ugc nofollow" target="_blank">脸书人工智能研究</a></p><p id="fb1e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">再见🏃🏾</p></div></div>    
</body>
</html>