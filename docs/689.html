<html>
<head>
<title>Analyze Big Sequence Alignments with PySpark in AWS EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PySpark分析AWS EMR中的大序列比对</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyze-big-sequence-alignments-with-pyspark-in-aws-emr-a044acaa60af#2022-01-24">https://towardsdatascience.com/analyze-big-sequence-alignments-with-pyspark-in-aws-emr-a044acaa60af#2022-01-24</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""><h1 id="33a8" class="pw-post-title ip iq ir bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">用PySpark分析AWS EMR中的大序列比对</h1></div><div class=""><h2 id="ef50" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">如何处理生物信息学中的大数据</h2></div><p id="a47c" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">如今，生物领域的程序员需要从第一天起就为大数据设计他们的应用。这都是因为最新的DNA测序仪可以产生大量的数据。比如<a class="ae lc" href="https://www.illumina.com/systems/sequencing-platforms/nextseq-1000-2000/specifications.html" rel="noopener ugc nofollow" target="_blank"> Illumina NextSeq 2000 </a>两天产生360 Gb，而来自MGI的<a class="ae lc" href="https://en.mgi-tech.com/products/instruments_info/5/" rel="noopener ugc nofollow" target="_blank"> DNBSEQ-T7 </a>每天可以输出1到6 Tb。在一台计算机上处理它们可能需要几天时间。为了加快速度，我们需要分布式计算。</p><figure class="le lf lg lh gu li gi gj paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gi gj ld"><img src="../Images/9cb578d30e980fa47847ef6fd93a6775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-jEQEVYGO-czQXpw035OA.jpeg"/></div></div><p class="lp lq gk gi gj lr ls bd b be z dk translated"><a class="ae lc" href="https://unsplash.com/@3dparadise?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Braňo </a>在<a class="ae lc" href="https://unsplash.com/s/photos/dna?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="32f7" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">在分布式计算中，我们将数据存储和数据处理分布在许多计算机上。但是我们开发人员不想自己处理这些细节。我们需要的是一个中间人，负责分发并为我们提供一个简单的编程接口。Apache Spark就是这样一个大数据分析的中间人。</p><p id="7e92" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">与其他并行计算如AWS Batch(阅读我的介绍文章<a class="ae lc" href="https://aws.plainenglish.io/parallel-blast-against-cazy-with-aws-batch-8239a45d8116" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae lc" href="https://aws.plainenglish.io/metagenomic-binning-with-nextflow-866f6c0b0d0c" rel="noopener ugc nofollow" target="_blank">这里</a>)、Slurm或单个节点上的多线程/进程编程相比，用Spark编写的脚本是简洁的。主脚本几乎不包含任何关于工作分配的提示，因为Spark会自己处理输入拆分和并行性。不再有队列、信号量或池。相同的脚本可以在本地和集群中运行。即使是在本地运行，Spark也会将计算分散到所有CPU内核中。但是当Spark在多节点环境中运行时，我们确实需要在主脚本之外配置基础设施。</p><p id="42ea" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">有大量关于Spark的文章(<a class="ae lc" rel="noopener" target="_blank" href="/production-data-processing-with-apache-spark-96a58dfd3fe7">勒米厄的这个</a>是一个很好的切入点)，但很少有关于它在生物信息学中的部署。雪上加霜的是，许多文章都是“快速入门”，没有为实际的大规模项目提供足够的细节。由于生物信息学中对大数据分析的需求如此之大，而文献又如此匮乏，我们迫切需要用新鲜的教程来填补这一空白。在本文中，我将向您展示如何使用Spark来处理AWS EMR上的SAM校准文件。因为在实践中，很少有人拥有昂贵的内部基础设施来扩展Spark，他们很可能会转向云。最后，我将向你展示如何用一个Sankey图表来可视化结果。您可以在这里找到我的Github资源库中的所有代码。测试数据<code class="fe lt lu lv lw b">test.sam</code>是从<em class="lx"> E </em>中随机生成的SAM文件。<em class="lx">大肠杆菌</em>基因组。</p><div class="ly lz gq gs ma mb"><a href="https://github.com/dgg32/pyspark_sam/tree/master" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fp"><div class="md ab me cl cj mf"><h2 class="bd is gz z fq mg fs ft mh fv fx iq bi translated">GitHub - dgg32/pyspark_sam</h2><div class="mi l"><h3 class="bd b gz z fq mg fs ft mh fv fx dk translated">这个repo托管了我的文章“在AWS EMR中用PySpark分析大序列比对”的代码。火花跟随…</h3></div><div class="mj l"><p class="bd b dl z fq mg fs ft mh fv fx dk translated">github.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ln mb"/></div></div></a></div><h1 id="f605" class="mq mr ir bd ms mt mu mv mw mx my mz na jx nb jy nc ka nd kb ne kd nf ke ng nh bi translated">1.SAM文件和四核苷酸频率</h1><p id="b301" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">生物信息学测序中最常见的任务之一是将短阅读序列映射到参考基因组。结果是<a class="ae lc" href="https://www.zymoresearch.com/blogs/blog/what-are-sam-and-bam-files" rel="noopener ugc nofollow" target="_blank"> SAM或BAM文件</a>。两种格式包含相同的信息:新测序的读数、参考序列和一些其他数据。区别在于SAM是纯文本，而BAM是二进制的。所以巴姆比萨姆小多了。然而，在分布式环境中，纯文本文件是首选，因为它们可以很容易地分割和分散在许多计算机上。相比之下，对二进制文件进行切片并不是一件小事，而且通常情况下，它们必须作为一个单独的数据流来读取。为此，我将在本教程中处理一个SAM文件。</p><p id="8e94" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">大多数基因组是由DNA组成的。它们由四种类型的核苷酸组成:A、T、C和g。因此，基因组就像是由四个字母组成的书(阅读我的介绍<a class="ae lc" href="https://medium.com/geekculture/analyzing-genomes-in-a-graph-database-27a45faa0ae8" rel="noopener">这里</a>)。除了同卵双胞胎，大多数个体都有独特的基因组。但是当你一个字母一个字母地比较时，亲缘关系近的生物确实有很大一部分基因组是相同的。此外，<a class="ae lc" href="https://sfamjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.1462-2920.2004.00624.x" rel="noopener ugc nofollow" target="_blank">密切相关的生物也共享相似的四核苷酸分布</a>。也就是说，如果你将它们的基因组分成三个字母组，即三胞胎，并计算它们的相对丰度，如果这两种生物在进化上接近，你会看到类似的结果。因此，计算基因组序列的四核苷酸频率是生物信息学中的一项常见任务。在我们的案例中，我们希望看到四核苷酸频率如何从参考基因组变化到新测序的基因组。</p><figure class="le lf lg lh gu li gi gj paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gi gj nn"><img src="../Images/12f47a48cc066d6ab2c3f56e23be5e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HizgY9ovC2DE5h07eC0wyQ.png"/></div></div><p class="lp lq gk gi gj lr ls bd b be z dk translated">图一。项目工作流程。图片作者。</p></figure><p id="fe01" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">我将使用AWS CLI命令向AWS EMR提交PySpark脚本进行计算。虽然<a class="ae lc" href="https://aws.amazon.com/emr/serverless/" rel="noopener ugc nofollow" target="_blank">亚马逊EMR无服务器</a>仍处于预览阶段，但常规EMR中的<code class="fe lt lu lv lw b">auto-terminate</code>选项使其成为目前你所能获得的尽可能多的无服务器。最后，我选择了Plotly的Sankey图表进行可视化，因为它可以优雅地显示详细的变化。</p><h1 id="28f0" class="mq mr ir bd ms mt mu mv mw mx my mz na jx nb jy nc ka nd kb ne kd nf ke ng nh bi translated">2.PySpark</h1><p id="3b4e" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">Python代码由一个<code class="fe lt lu lv lw b">helper_function.py</code>和一个主脚本<code class="fe lt lu lv lw b">spark_3mer.py</code>组成。<code class="fe lt lu lv lw b">helper_function.py</code>包含任务的一些辅助功能。计算的主脚本非常短。</p><figure class="le lf lg lh gu li"><div class="bz fq l di"><div class="no np l"/></div></figure><p id="8f84" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">该脚本遵循MapReduce模型。SAM文件中的每个比对都将在<code class="fe lt lu lv lw b">map</code>功能中具体化。在<code class="fe lt lu lv lw b">flatMap</code>函数中，从这些对齐对象中提取变异的三元组。这两个函数的区别在于，<code class="fe lt lu lv lw b">map</code>函数为每个输入行生成一个输出，而<code class="fe lt lu lv lw b">flatMap</code>压缩数据，换句话说，它使输出变平。最后，参考-新序列三联体对在<code class="fe lt lu lv lw b">reduceByKey</code>函数中求和。<code class="fe lt lu lv lw b">collect</code>功能为后续的<code class="fe lt lu lv lw b">format</code>操作向驱动器节点检索数据。</p><p id="d92e" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">一旦数据被格式化成字典，就该把它写到S3了。我发现了两种方法。第一个选项将字典“并行化”为RDD。然后，它通过<code class="fe lt lu lv lw b">RDD.coalesce().saveAsTextFile()</code>链，将词典作为一个名为“part-00000”的文本文件保存在S3文件夹中。缺点是我们不能命名输出文件，并且文件不是JSON格式的。第二个选项使用boto3包。这个选项解决了以上两个问题，更加方便。值得注意的是，我们不需要在脚本中提供AWS凭证。</p><h1 id="2f1d" class="mq mr ir bd ms mt mu mv mw mx my mz na jx nb jy nc ka nd kb ne kd nf ke ng nh bi translated">3.在AWS EMR中运行PySpark脚本</h1><p id="ddf6" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">通常，我们需要一些Python库来完成我们的任务。我们需要在EMR中安装这些库。幸运的是，这很容易。我们只需要在一个<code class="fe lt lu lv lw b">emr_bootstrap.sh</code>文件中写下指令，并在稍后的AWS CLI命令中引用它:</p><figure class="le lf lg lh gu li"><div class="bz fq l di"><div class="no np l"/></div></figure><p id="c4d7" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">上传<code class="fe lt lu lv lw b">emr_bootstrap.sh</code>、<code class="fe lt lu lv lw b">helper_function.py</code>、<code class="fe lt lu lv lw b">spark_3mer.py</code>、<code class="fe lt lu lv lw b">test.sam</code>(可以在我的仓库里找到)到你的S3桶里。以下命令可以一次性创建基础设施并运行PySpark脚本。</p><figure class="le lf lg lh gu li"><div class="bz fq l di"><div class="no np l"/></div></figure><p id="5ac7" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">该命令将emr-6.5.0用于该作业。它调用引导脚本来安装必要的依赖项。<code class="fe lt lu lv lw b">steps</code>部分调用PySpark脚本，并为它们提供输入和输出路径。一旦工作完成，它还将终止集群，以避免额外的成本。您可以在AWS EMR控制台中监控作业状态。</p><figure class="le lf lg lh gu li gi gj paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gi gj nq"><img src="../Images/072798281228aa419e505149cdadeb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDiTQHcs1Rf_QMEicoMXjQ.png"/></div></div><p class="lp lq gk gi gj lr ls bd b be z dk translated">图二。AWS EMR控制台中的作业状态。图片作者。</p></figure><p id="2174" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">如果一切运行成功，您可以在您的S3存储桶中找到一个文件<code class="fe lt lu lv lw b">sankey.json</code>。</p><h1 id="1e41" class="mq mr ir bd ms mt mu mv mw mx my mz na jx nb jy nc ka nd kb ne kd nf ke ng nh bi translated">4.桑基图表</h1><p id="7eb8" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">一旦JSON结果文件在我们的本地机器上，我们可以用Plotly Sankey可视化它。</p><figure class="le lf lg lh gu li"><div class="bz fq l di"><div class="no np l"/></div></figure><p id="8dfc" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">就像这样运行上面的脚本:</p><figure class="le lf lg lh gu li"><div class="bz fq l di"><div class="no np l"/></div></figure><p id="960a" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">它会自动打开一个带有视觉效果的浏览器标签。这是一个交互式桑基图表:</p><figure class="le lf lg lh gu li gi gj paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gi gj nr"><img src="../Images/24e4c1fc6477217a562e651930dc0e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1514memw8Zak44v_Koxjw.png"/></div></div><p class="lp lq gk gi gj lr ls bd b be z dk translated">图3。与Plotly的互动桑基图表。图片作者。</p></figure><h1 id="0489" class="mq mr ir bd ms mt mu mv mw mx my mz na jx nb jy nc ka nd kb ne kd nf ke ng nh bi translated">5.成本</h1><p id="e6a5" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">AWS EMR的成本适中。我的设置包含一个主节点和两个核心节点。所有的都是m5.xlarge，这个实例类型每小时花费0.048美元。对于本文，我的多个实验花费了大约5.6小时的计算时间，总共花费了0.27美元(0.048 * 5.6美元)。据估计，如果我一个月每天工作10小时，我将花费43.80美元。</p></div><div class="ab cl ns nt hv nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ik il im in io"><h1 id="ff8d" class="mq mr ir bd ms mt nz mv mw mx oa mz na jx ob jy nc ka oc kb ne kd od ke ng nh bi translated">结论</h1><p id="477a" class="pw-post-body-paragraph kg kh ir ki b kj ni js kl km nj jv ko kp nk kr ks kt nl kv kw kx nm kz la lb ik bi translated">成功！在这个项目中，我们使用Spark来计算四核苷酸频率的变化。Spark的初始化花了相当长的时间。似乎简单的本地Python运行可以更快地完成工作。所以你可能想知道我们为什么要使用Spark。原因是这个项目中的输入文件很小，Spark的开销似乎过大。但是当输入文件达到Gb级别或以上时，单节点程序就开始挣扎，是时候让Spark大放异彩了。Spark可以将输入分成多个分区，并并行处理它们。这也是开销非常值得的地方。</p><p id="c419" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">目前，我们运行并行程序有几种选择:多线程/进程、Slurm/SGE、AWS ParallelCluster、AWS Batch、mrjob和Spark。都有利弊。Spark很容易学习和设置。它的脚本简洁，运行速度快。然而，与其他选项相比，Spark在AWS EMR上的调试变得非常困难，因为它会生成许多日志文件，而没有明确的错误消息。在迁移到EMR之前，您最好在本地机器上彻底测试您的脚本。</p><p id="6c0d" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">您可以为将来的项目修改这里的脚本。您可以通过修改<code class="fe lt lu lv lw b">spark_3mer.py</code>脚本来计算更多的SAM统计数据。或者在一些大型SAM文件上尝试一下。如果您需要更强大的实例，请在AWS CLI命令中更改<code class="fe lt lu lv lw b">instance-type</code>。或者可以用Spark的MLlib做一些机器学习。</p><p id="4380" class="pw-post-body-paragraph kg kh ir ki b kj kk js kl km kn jv ko kp kq kr ks kt ku kv kw kx ky kz la lb ik bi translated">所以请在你的项目中多使用一些火花，说说你的经历吧！</p><div class="ly lz gq gs ma mb"><a href="https://dgg32.medium.com/membership" rel="noopener follow" target="_blank"><div class="mc ab fp"><div class="md ab me cl cj mf"><h2 class="bd is gz z fq mg fs ft mh fv fx iq bi translated">加入媒介与我的介绍链接-黄思兴</h2><div class="mi l"><h3 class="bd b gz z fq mg fs ft mh fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mj l"><p class="bd b dl z fq mg fs ft mh fv fx dk translated">dgg32.medium.com</p></div></div><div class="mk l"><div class="oe l mm mn mo mk mp ln mb"/></div></div></a></div></div></div>    
</body>
</html>