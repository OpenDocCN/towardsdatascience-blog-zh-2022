# 人工智能的最后一周——2021 年(全年)回顾

> 原文：<https://towardsdatascience.com/lets-talk-ai-2021-the-full-year-in-reveiew-c68af9ee18b2>

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

# 人工智能的最后一周——2021 年(全年)回顾

## 在人工智能播客的最后一周，与我们的朋友一起回顾 2021 年下半年人工智能领域的最大事件

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:TDS 播客由杰雷米·哈里斯主持，他是人工智能安全初创公司墨丘利的联合创始人。每周，Jeremie 都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。*

2021 在许多方面都是一场狂野之旅，但它最狂野的特征可能实际上与人工智能有关。我们已经看到了从语言建模到多模态学习、开放式学习甚至人工智能对齐等各个方面的重大进步。

所以，我们想，还有什么比上周在人工智能播客中与我们的朋友在[的一段交叉插曲更好的方式来盘点我们在 2021 年达到的与人工智能相关的重大里程碑呢(psst:你也可以在这里](https://www.lastweekinai.com/)查看他们的精彩简讯[)。](http://lastweekin.ai)

以下是我在对话中最喜欢的一些观点:

*   基础模型继续增长，但一个有趣的趋势是关注效率以及(而不是？)尺度。例如，虽然 DeepMind 的 Gopher 模型的参数不到 GPT-3 的两倍，但据报道，它的效率是 25 倍，这意味着从相同的训练数据和计算中挤出了更多的价值。AI21Labs 的 Jurrassic 模型在参数计数方面也与 GPT-3 相当，但反映了我们对架构优化而非原始扩展的关注，我们预计这种关注将持续到 2022 年。(这并不是说大规模的扩张不会发生，或者说它还没有发生；几个月前发布的微软图灵-NLG，有超过 5000 亿个参数。但可以肯定地说，如果没有同步的效率优化，就无法实现扩展，而效率优化在 2020 年末还不是重点。)
*   过程环境的生成一直是强化学习的一个重要主题。在[开放式学习导致普遍有能力的代理](https://arxiv.org/abs/2107.12808)中，DeepMind 的团队展示了如何在广泛的环境中训练 RL 代理可以导致与泛化相关的紧急行为，如试错和与友好代理的合作。
*   开放式学习(OEL)似乎是一个有趣的通配符，一些研究人员认为这可能是最终 AGI 食谱的重要成分。在本期 TDS 播客中，我们采访了 OpenAI 的开放式学习负责人 Ken Stanley，探讨了 OEL 在未来人工智能中可能扮演的角色。
*   NeurIPS spotlight 的一篇题为[最优政策倾向于寻求权力](https://arxiv.org/abs/1912.01683)的论文，以及同一作者随后的工作表明，我们应该预计高度能干的人工智能系统会从事与人类价值观不符的危险行为，默认情况下*。具体来说，高度称职的代理人将倾向于寻找强大的国家，因为它们提供了许多下游选项。这一发现令人信服地表明，人工智能的一致性应该得到优先考虑，特别是考虑到我们在更广泛的人工智能能力方面看到的进展速度。如果真的是这样，有能力的人工智能系统在默认情况下将是危险的，那么必须在安全研究方面投入积极的努力。*

*![](img/3b2980d404719c92071b7aaa1d9f5b8a.png)*

## *章节:*

*   *0:00 介绍*
*   *2:15 多模式模式的兴起*
*   *7:40 的硬件和计算增长*
*   *13:20 强化学习*
*   *20:45 开放式学习*
*   *26:15 求权纸*
*   *32:30 安全和假设*
*   *35:20 内在动机与外在动机*
*   *42:00 映射自然语言*
*   *46:20 Timnit Gebru 的研究所*
*   *49:20 总结*