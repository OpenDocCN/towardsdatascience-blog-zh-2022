# 用 Python 对文本进行总结

> 原文：<https://towardsdatascience.com/summarize-a-text-with-python-b3b260c60e72>

## 如何用 Python 和 NLTK 高效地概括文本

![](img/f3989a52f2cb8b6844d83c3b363e6540.png)

梅尔·普尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

有时你需要一个给定文本的摘要。我在收集新闻帖子时遇到了这个挑战。使用完整的文本来解释一篇文章的意思需要很多时间(我收集了大约 250.000)，所以我开始寻找一种将一篇文本总结为三句话的方法。这篇文章描述了创建摘要的相对有效的方法。

## 该算法

该算法的目标是将几段文本的内容总结成几个句子。这些句子将取自原文。将不使用文本生成。

这个想法是，文本的目标可以通过识别文本中最常用的单词来找到。常见的停用字词将被排除。找到 mot 使用的单词后，使用加权计数算法(文本中使用的单词越多，权重越高)找到包含这些单词最多的句子。具有最高权重的句子被选择并形成摘要:

```
1\. Count occurrences per word in the text (stop words excluded)
2\. Calculate weight per used word
3\. Calculate sentence weight by summarizing weight per word
4\. Find sentences with the highest weight
5\. Place these sentences in their original order
```

该算法使用自然语言工具包( [NLTK](https://www.nltk.org/index.html) )将文本拆分成句子，将句子拆分成单词。NLTK 是使用 pip 安装的:

```
pip install nltk numpy
```

但是我们首先导入所需的模块，并构建要从算法中排除的停用词列表:

在互联网上可以很容易地找到每种语言的停用词列表。

该算法的第一步是用文本中的词频建立列表:

代码构建了一个字典，将单词作为键，将单词出现的次数作为值。使用 NLTK 的单词标记器将文本拆分成小写单词(第 4–5 行)。所有的单词都被转换成小写，所以我们在一个句子中和一个句子的开头计算单词。否则，“cat”将被识别为与“Cat”不同的单词。

如果单词的长度至少为两个字符，则删除诸如“、”和“.”之类的标记等等，并且它不在停用字词列表中(第 6 行)，如果该字词已经在字典中，或者添加到字典中，计数为 1(第 7-10 行)，则出现次数增加 1。

当完整的文本被解析时,‘word _ weights’字典包含所有单词及其各自的计数。实际计数用作单词权重。通过将出现次数最多的出现次数相除，可以在 0 和 1 之间调整这些值，但是尽管这样做可能很直观，但它不会改变算法的工作。不执行这种划分可以节省宝贵的时间。

现在我们可以确定文本中每个句子的权重，并找到最高的权重:

首先，用`sent_tokenize()`将文本拆分成句子。然后，将每个句子拆分成单词，并对每个句子的单个单词权重进行汇总，以确定句子权重。在这些循环之后，字典 sentence_weights 包含每个句子的权重以及它们的重要性。

最重要的句子权重可以通过从字典中获取值、对这些值进行排序并获取最后的`n`值来找到(第 10 行),其中`n`是我们想要用于摘要的句子数量。变量`highest_weights`包含那些需要在摘要中结束的句子的权重。

最后一步是把这些句子组合成一个摘要。有两个选择。首先，我们可以将它们按重要性排序，其次，我们可以使用它们在提供的文本中出现的原始顺序。经过一些实验后，后一种选择更好:

摘要是通过遍历所有句子及其权重并使用来自`highest_weights`的具有权重的句子来创建的。字典保持加法的顺序，所以句子根据它们在文本中的出现被解析。最后，进行了一些清理，我们最终得到了一个惊人精确的摘要。

最后一步是将所有步骤组合成一个函数:

这个功能在我的新闻存档项目中有着重要的地位。把一篇文章从几十句话减少到三句话真的很有帮助。

代码并非完美无瑕。句子分词器不是最好的，但是相当快。有时我会以 4 句话的总结结束，因为它忽略了两句话的分离。但是速度值得这些误差。

这同样适用于创建摘要的最后部分。如果在`heighest_weights`中有多个权重最低的句子，则将它们全部添加到摘要中。

编写完美的代码并不总是必要的。一句话太多的总结，不会倾家荡产，也不会让系统崩溃。在这种情况下，速度高于精度。

如果一篇较大的文章需要总结，把它分成 20 到 50 个句子、段落或章节，并为每个部分生成一个摘要。合并这些摘要将导致整个文本的摘要。一个简单的方法是:

注意，这段代码在调用`summarize`方法之前从文本中提取句子，将它们分组并连接成一个字符串。这种方法将再次提取句子。如果需要对大量文本进行总结，可以使用`summarize`函数来接受句子列表。

该函数适用于多种语言。我用英语、荷兰语和德语测试过，当你使用正确的停用词列表时，它对每种语言都有效。

## 例子

为了帮助理解算法的工作原理，给出了一个完整的例子。在本例中，以下两句话被总结为一句话:

```
It is dark and the dark is scary. I do not like scary moments.
```

只有单词“黑暗”、“可怕”和“瞬间”不在停用词列表中，因此单词权重(词频)列表被如下占据:

```
{
  'dark'    : 2, 
  'scary'   : 2, 
  'moments' : 1
}
```

“黑暗”和“可怕”在测试中出现两次，“瞬间”出现一次。那么句子的权重是

```
{
    'It is dark and the dark is scary.' : 6, 
    'I do not like scary moments.'      : 3
}
```

第一句话的权重是 6，因为它包含两次单词“黑暗”(权重=2)和“可怕”(权重=2)。第二句包含‘吓人’(2)和瞬间(1)。“黑暗”和“可怕”在文本中出现了两次，但对句子权重的影响很大，因为第一句话包含了这些词的三次出现。

我们只寻找一个句子的摘要，所以' highest_weights '数组包含一个值:[6]。

遍历句子以找到具有最高值的句子，或者在本例中为 value，结果得到一个句子的摘要:

```
It is dark and the dark is scary.
```

# 最后的话

我希望你喜欢这篇文章。要获得更多灵感，请查看我的其他文章:

*   [F1 分析和 Python 入门](https://medium.com/p/5112279d743a)
*   [太阳能电池板发电分析](/solar-panel-power-generation-analysis-7011cc078900)
*   [对 CSV 文件中的列执行功能](https://towardsdev.com/perform-a-function-on-columns-in-a-csv-file-a889ef02ca03)
*   从你的活动跟踪器的日志中创建一个热图
*   [使用 Python 实现并行 web 请求](/parallel-web-requests-in-python-4d30cc7b8989)

如果你喜欢这个故事，请点击关注按钮！

*免责声明:本文包含的观点和看法仅归作者所有。*