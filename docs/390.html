<html>
<head>
<title>Accelerating ResNet-50 Training on the IPU: Behind our MLPerf Benchmark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在IPU加速ResNet-50训练:在我们的MLPerf基准之后</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-resnet-50-training-on-the-ipu-behind-our-mlperf-benchmark-2cefe43ab2b2#2022-01-13">https://towardsdatascience.com/accelerating-resnet-50-training-on-the-ipu-behind-our-mlperf-benchmark-2cefe43ab2b2#2022-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="0e5c" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated"><strong class="ak">在IPU上加速ResNet-50训练:落后于我们的MLPerf基准</strong></h1></div><div class=""><h2 id="fc8b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于高效硬件扩展、内存优化策略和性能工具的技术指南</h2></div><p id="fa15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者:<a class="lb lc ep" href="https://medium.com/u/24372940c2b3?source=post_page-----2cefe43ab2b2--------------------------------" rel="noopener" target="_blank">Mario Michael Krell</a>博士、刘振英、Emmanuel Menage和Bartosz Bogdanski</p><p id="0af9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi ld translated"><span class="l le lf lg bm lh li lj lk ll di"> G </span> raphcore工程师在2021年12月发布的最新MLPerf v1.1培训结果中表现出色[9]，我们的IPU-POD16在ResNet-50上的表现超过了英伟达的旗舰产品DGX A100。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/bb938da2d5e0a9762fe1a0ffbdc2bf1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tl0tLtrpB5TruENrcEGckg.jpeg"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片作者。</p></figure><p id="d41c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们将解释我们的团队如何为这个流行的计算机视觉模型实现大规模加速。在本技术指南中，我们将揭示Graphcore工程师使用的各种技术和策略，涵盖高效硬件扩展、内存优化、实验跟踪、性能优化等。虽然本指南主要关注IPU [9]的MLPerf中ResNet-50的性能指标评测优化，但它提供了性能指标评测优化的一般思路，该思路也适用于其他应用和硬件。</p><h2 id="c6b4" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">加速ResNet-50</h2><p id="6da0" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">对于我们在2021年12月发布的MLPerf v1.1结果[9]，我们在ImageNet (RN50)上实现了28.3分钟的ResNet-50训练时间，每秒30k图像吞吐量，在IPU-POD16上以75.9%的验证精度收敛之前有38个时期。在我们于2020年开始我们的第一个MLPerf项目之前，ResNet-50训练的吞吐量约为每秒16k幅图像，大约需要65个历元才能达到收敛。将这些数字加在一起，我们获得了3.2倍的总体改善。然后，我们将应用从IPU-POD16扩展到IPU-POD256，这使我们的培训时间又提高了7.5倍，吞吐量提高了12倍。那么，我们是如何实现这种加速的呢？</p><p id="d91c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在此过程中开发的工具在我们的<a class="ae na" href="https://github.com/graphcore/examples/tree/master/vision/cnns/tensorflow1/training" rel="noopener ugc nofollow" target="_blank">公共示例</a>中提供，并不特定于RN50或MLPerf基准。我们认为其他IPU开发者可能会发现它们对他们的模型有用。关键成分是:</p><ul class=""><li id="5ed3" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">优化的工作分配和沟通</li><li id="c360" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">优化的主机IO和数据布局，分布在多个主机上</li><li id="2e47" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">对权重和偏差进行实验跟踪</li><li id="ae82" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">小有效批量</li><li id="c3d1" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">从SGD with Momentum切换到LARS optimizer</li><li id="8cbd" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">带统计缓存的分布式批量定额</li><li id="7dd3" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">重新计算检查点，而不是流水线处理</li><li id="11bc" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">随机舍入</li><li id="a1c8" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">优化的编译和内存配置文件</li></ul><h1 id="69f9" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">IPU-POD:专为人工智能加速至超级计算规模而打造</h1><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/54bfe7864697be9764680ed382ea8042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGJrPVCRxgPwwiHME12iNw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">将应用从IPU-波德16扩展到我们的IPU-波德256。图片作者。</p></figure><h2 id="8712" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">IPU硬件如何扩展机器学习应用</h2><p id="f190" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">为了扩展我们的应用程序，必须部署许多功能。缩放时的一个主要挑战是增加的批量大小和通信，这需要时间进行数据传输和存储代码。对于本地副本，我们希望保持微批量大小尽可能大，以获得批量范数的良好统计估计，这是ResNet-50模型的关键部分。需要聚集每个副本的梯度，并且优化器执行其更新。</p><p id="0ac2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这带来了计算和通信开销。减少这种情况的一个技巧是在单个复制品上进行梯度累积，并具有更大的有效批量。通过添加更多的副本，有效的批量大小将线性扩展。尽管这种方法将保证几乎线性的缩放，但是更高的有效批量增加了获得收敛所需的历元数。因此，对于较大的系统，需要减少梯度累积，并且梯度的有效通信是必不可少的。</p><p id="9286" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的实验中，IPU-POD128和IPU-POD256的最佳折衷方案是梯度累积计数为2，通信量略有减少，但保持有效批量尽可能小。考虑到较大的批量，“LARS优化器和超参数优化”中提到的更改是必要的，因为SGD会比LARS增加更多的所需时期。此外，我们过度使用了负载分布工具PopDist，如“扩展主机-IPU通信”中所述。通过增加具有本地数据存储的主机数量，我们提高了数据加载带宽，并确保无论使用多少加速器，数据传输都不会变慢。</p><h2 id="8d2b" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">硬件扩展:连接IPU-POD64平台</h2><p id="edcc" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">Graphcore的IPU-PODs从一开始就考虑到了可扩展性，正如我们最新提交的MLPerf中展示的难以置信的可扩展性所示。在一个机架内，IPU通过高带宽IPU链路连接直接相互连接，形成两个环。使用这些直接连接，每个IPU-POD64提供10.24TB/s的巨大IPU链路带宽，具有超低延迟(250ns)。在这些垂直机架内连接的顶部，IPU跨机架连接(直接或通过交换机)以形成水平环。这意味着机架数量增加一倍会使可用带宽增加一倍以上。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oa"><img src="../Images/36f48ab98fbb6906c475e8e6e27a2e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH414SuT1GppDPshXOmAmw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片作者。</p></figure><p id="4787" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Graphcore通信库(GCL)旨在充分利用这些硬件特性。</p><p id="12bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">IPU-POD128和IPU-POD256是由多架IPU-pod 64建造的。单个IPU-POD64由16个IPU-M2000垂直堆叠而成，用IPU链连接形成一个2D环。我们横向扩展解决方案的灵活性允许我们将IPU-pod 64横向连接在一起，形成更大的系统。连接此类系统有两种可能的方法:直接连接(一个IPU-M上的GW-link直接连接到其他IPU-POD64系统上的IPU-M 2000)或通过交换结构连接，其中所有IPU-M 2000上的GW-link连接到交换机，然后交换机将流量转发到其他IPU-pod 64上的正确目的地IPU-M 2000。在这两种情况下，多个IPU-pod 64之间的同步通过GW链路发送，而不需要物理同步线路来实现更简单的横向扩展。对于我们用于MLPerf工作的IPU-POD128和IPU-POD256，我们使用交换解决方案，因为它允许我们更加灵活，并提供更好的故障转移。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ob"><img src="../Images/0c164a714c4ccdb6879c9a6d7856965d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQLJ_BaluGTs-yE9cZrANw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">Graphcore针对IPU-POD128至IPU-POD512的横向扩展解决方案。图片作者。</p></figure><p id="b767" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看IPU-POD横向扩展拓扑的各个方面，现在我们可以了解一下实际情况。这表明，即使在现有交换结构上部署带来了所有的灵活性和强大的选项，它实际上并不像最初想象的那样需要“额外的套件”。该系统有8个IPU-POD64机架，因此我们可以称之为IPU-POD512。仅使用两台128端口100GbE交换机就可以实现IPU-POD512中每个IPU-M2000之间所有GW链路的互连。</p><h2 id="be1f" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">使用Graphcore通信库(GCL)扩展通信</h2><p id="41b6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">为了加快人工智能模型的训练，一个想法是在许多IPU上复制模型，每个IPU处理不同的批次。处理完一个批处理后，复本使用all reduce共享它们从该批处理中获得的信息(渐变)。在理想情况下，如果All-Reduce是瞬时的，我们将在吞吐量方面实现完美的扩展:副本数量翻倍将允许我们处理两倍的训练样本。</p><p id="0eee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GCL已经实施了多项优化，支持横向扩展到单个IPU-POD64之外。所有模型副本内部数据交换在编译时都是已知的，并且由Poplar详细描述，并且假设所有集体通信、ML模型细节和网络拓扑在编译时也是已知的，则GCL执行是确定性的，并且可以相应地被调度和优化。GCL还公开了一个公共API，用于将通信组指定为总副本集的子集，从而为我们在大型系统上执行操作提供了灵活性。对于IPU-POD256的更大规模，我们使用分阶段的全部缩减，其中在多POD系统上的张量缩减作为三个不同的操作来执行。</p><p id="4c0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，在作为IPU-POD256的构造块的每个IPU-pod 64内的IPU链路上进行减少散射。Reduce分散的输出成为在GW链路上运行的All-Reduce的输入。这在多个IPU pod之间交换数据元素，确保同一级别的每个副本包含相同的数据。最后，对All-Reduce的输出执行All-Gather集合操作，它再次在每个单独的IPU-POD64中运行，以便最终每个复制副本都包含相同的数据拷贝。</p><p id="b70c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的实现的一个重要特征是，在All-Reduce中花费的时间是要共享的数据总量除以可用总带宽的函数。因为IPU数量加倍意味着带宽量加倍，所以全缩减的时间保持不变。Graphcore产品的主要特点是恒定时间完全减少:它使创新者能够通过使用更多硬件来更快地进行实验，有效地减少反馈回路，最终导致令人兴奋的发现。</p><p id="ec40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴于我们对大型系统的梯度累积计数为2，梯度通信的任何优化都会对我们的扩展性能产生重大影响。例如，我们将不同类型和形状的张量组合在一起，一次性完成通信。此外，为了充分利用带宽，我们优化了主机和IPU之间的网关，以立即转发数据，从而显著降低各种应用的延迟。每个IPU-POD64都有自己的网关。因此，这种变化主要受益于从IPU-POD16到IPU-POD64的扩展。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/83cadaad8de5b739ae709f3d7ae35928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcRSOo6s6FDMXmJwein8Iw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">三相全还原的直观表示。图片作者。</p></figure><h2 id="fe58" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">针对扩展的内存优化</h2><p id="feb7" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">尽管数据所需的内存在缩放时保持不变，但交换代码需要额外的内存。因此，例如在“其他内存和速度优化”中提到的内存优化对我们的扩展成功至关重要，特别是因为LARS优化器甚至比SGD需要更多的内存。扩展到IPU-POD128和IPU-POD256的具体优化是减少IPU-POD之间交换的代码大小。</p><h2 id="ed1e" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">将验证扩展到IPU-波德256</h2><p id="ac9c" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">对于验证，不需要向后传递，并且在批范数层中，应用同步移动平均值和方差。因此，计算布局是完全不同的。为了不浪费图形交换的时间，我们改为应用离线评估方案，即，我们在训练期间存储检查点，然后，我们使用验证数据来评估性能。我们还保持IPUs的相同配置，以避免重新配置的时间。这意味着对于POD256的情况，我们有128个主机进程，其中每个进程负责一个验证文件，然后汇总结果以计算整体性能。因为我们使用的是静态图，所以即使批量大小为1，50000个验证图像也不能完全分割，这将是低效的。但是，对于MLPerf设置，需要处理所有图像，并且不能遗漏样本。因此，我们将数据填充到下一个更大的批次，然后在后处理中移除填充结果。</p><h1 id="6986" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">在IPU硬件上大规模加速ResNet-50</h1><h2 id="a787" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">实验跟踪</h2><p id="ea7b" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">在整个项目过程中，跟踪实验、查找详细的参数配置和深入研究日志非常重要。因此，我们充分利用了通过权重和偏差进行实验跟踪的优势，并定期创建报告来与同事交流结果，并比较运行情况以找出任何潜在的差异。使实验具有挑战性的是，在低历元计数的情况下，收敛对任何变化都过于敏感。例如，将累积计数减半可以很容易地将非收敛设置转换为收敛设置。此外，实验之间有很大的差异，需要这种方法来跟踪实验。我们跟踪了超过2600个实验。最近，我们在我们的CNN知识库中扩展了我们的追踪。使用<code class="fe od oe of og b"><em class="oh">--wandb </em></code>和<code class="fe od oe of og b"><em class="oh">--wandb-project</em></code>选项，结果会立即上传，而不是等待最终结果上传。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oi"><img src="../Images/366effe36f42a39c8abd168df6c4506b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rTZRFPTdS_p3WlYd36ueQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">权重和偏差报告。图片作者。</p></figure><h2 id="0b32" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">减少纪元的数量</h2><h2 id="d48a" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">批量常模与群体常模</h2><p id="64cf" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">实际上，人们通常在大约90个纪元甚至更多的时候训练他们的网络。由于MLPerf基准只关心训练的时间，所以有很多调整来减少这个数字。我们的目标是将数量减少到44个时代，甚至更少。我们最初的实现使用了组范数而不是批范数，以便更快地处理和避免样本之间的依赖性。有关讨论，请参见[1]。</p><p id="7c40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于65个或更多的时期，这种标准化完美地工作。然而，在微调超参数时，我们了解到需要更积极和成本更高的归一化来在期望的小历元计数中获得收敛。这意味着我们开发的支持大批量范数的工具是特定于MLPerf基准的，因为必须减少历元的数量。如果没有这个要求，我们可能会坚持团体规范。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oj"><img src="../Images/913db76e77026319a4c0310817cf0c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*231pRYgea5hjzVYkxaRIiQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">每个子图对应于一种特定的归一化技术，其特征在于其各组分量共享相同的归一化统计量。在每个子图中，中间激活的张量由批轴B、通道轴C和空间轴(H，W)构成。图片由作者提供，改编自[7]。</p></figure><h2 id="cbb7" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">增加批量</h2><p id="7c34" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们最初的设置是一个超过4个IPUs(智能处理单元)的流水线，复制因子为4，用于数据并行处理和分组调度。我们使用的批量为16。将该设置更改为batch-norm是一个简单的配置更改，但收敛性仍略有改善。当我们通过使用内存高效通信、限制每个IPU的代码内存大小以及调整管道阶段的长度来微调内存和管道阶段以获得24的批处理大小时，情况变得更好。尽管如此，我们还是无法达到预期的44个纪元。因此，我们进行了一些模拟，发现32的批量应该足够了。通过模拟，我们可以在44个时期内达到收敛，但吞吐量低于可接受的水平，我们需要进行一些调整，如“增加吞吐量”一节中所述。</p><h2 id="d4bb" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">小有效批量</h2><p id="1f9a" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">为了获得良好的硬件加速，通常要处理大量的数据。在单个加速器上使用更多数据通常会增加ALU(算术逻辑单元)的使用率，并确保加速器获得足够的工作负载。请注意，IPU有一种独特的分配工作负载的方式，也可以加速小批量的工作负载。此外，梯度和优化器更新的通信可能是昂贵的，因此可以通过降低这些更新的频率来增加吞吐量，例如通过使用梯度累积。请注意，梯度累积的回报随着每次增加而递减。在我们的实验中，我们意识到较大的批量需要更多的历元用于训练，这也可以在MLPerf参考收敛点中看到[8]。因此，更大批量的好处可能超过速度的提高。另一方面，每个“复制品”需要32的最小批量大小，以使批量标准工作。</p><p id="982c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于最佳IPU-POD16配置，我们最终得到3200的有效批量大小(16个副本，每个副本的微批量大小为20，梯度累积计数为10)，其在38个时期内收敛。<strong class="kh ir">为了扩展到更大的系统</strong>，我们分别减少了梯度累积计数。由于较大机器上的通信效率和优化，保持梯度累积计数为2就足够了。对于IPU-POD64系统，我们使用类似的有效批量3840 (64个复制品，梯度累积计数为3)也在38个时期内收敛。对于IPU-波德128和IPU-波德256，我们使用的梯度累积计数为2，有效批量为5120和10240，相应的时期为41和45，直到收敛。使用更大的尺寸会增加更多的历元计数，并且是不可行的。</p><h2 id="fff9" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">LARS优化器和超参数优化</h2><p id="cef7" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">尽管正常的ML设置在超参数的选择中表现出一定的鲁棒性，但是MLPerf中的低历元计数使得收敛对任何变化都敏感。这可以通过优化器的选择得到最好的体现。众所周知，LARS是大批量的良好优化器。在我们的项目中，我们了解到它也是一个比动量随机梯度下降(SGD)更好的优化器，在收敛之前需要更少的时间。这种好处随着批量的增加而增加。请注意，LARS需要更多的内存和计算，但次要成本超过了主要节省。我们的第一次和第二次MLPerf提交之间的一个主要变化是优化内存使用和启用LARS，这导致了很大的加速。例如，对于我们的IPU-POD16设置，LARS将所需的历元数从44个减少到38个。一个关键因素是热身时期的数量。SGD需要5个历元，而LARS只需要2个历元。另一个敏感的超参数是LARS中的权重衰减参数，将其减少一倍并调整学习速率，可将收敛速度加快多达2个时期。</p><h2 id="b062" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">增加产量</h2><h2 id="c02d" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">管道平衡</h2><p id="d918" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">与自然语言处理模型BERT相比，针对RN50优化流水线可能具有挑战性。有许多不同的组件需要不同的代码，并且各层之间的数据大小变化很大。因此，稍微修改一下分割策略就可能对内存配置文件产生重大影响，从剩余大量内存跳到需要过多内存。此外，正如我们在PopVision图形分析器中发现的那样，处理负载可能非常不平衡。第四个IPU的处理时间是第二个的两倍，并且由于数据的内存需求，没有平衡的空间。另一方面，IPU0不能做太多的处理，因为它有最大的激活存储。类似地，IPU1和IPU2也有一些空闲时间，甚至比绿条所示的同步时间更长。由于不平衡的计算和内存需求，很难进一步改善，我们选择了不同的途径，使用重新计算方法，可以在单个IPU上工作，并使用分布式批处理规范来实现足够大的批处理大小。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ok"><img src="../Images/8cd107dbb59e65271513c5d04c31a7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B6jlwRyA0fdFY71O2DRjdQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">批量为20的4个IPU以上的分组调度管道的PopVision图形分析器摘录。第三行(IPU 2)对应于最后的第4级。与IPU 2相比，IPU 1只需要不到60%的处理周期。图片作者。</p></figure><h2 id="6981" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">分布式批量定额</h2><p id="c320" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们的批处理规范实现是在杨树级别上融合和优化的。通过之前用于流水线设置的所有内存微调，我们意识到现在可以在单个IPU上将新设置的批处理大小从6增加到12，而无需任何流水线操作。现在，为了达到批量定额的批量大小32，我们只需要一个分布式批量定额。对于其他一切，正常的数据并行处理就足够了。我们将这个特性添加到我们的框架中，只需要几行代码，我们在标准化实现中聚合了多个副本的统计数据。这种方法非常有效，并且不需要太多的内存，特别是因为统计数据的向量非常小。此外，向后传递只是IPU之间的梯度的集合，这有助于分布式批处理规范，因此只产生单行代码。</p><p id="6119" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于TensorFlow 1.15中的应用程序代码，界面保持了简约。只能设置一个配置号:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="6d4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于12的批量大小，我们需要在至少3个IPU之间合计数据，以达到至少32的批量标准批量大小。为了避免IPU-M2000机器之间的通信，它必须改为4个IPU。</p><h2 id="106f" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">重新计算</h2><p id="fec8" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">为了从分布式批处理规范中获得更好的吞吐量和更少的通信开销，我们希望增加批处理大小并将分布式批处理规范限制为两个IPU。因此，我们利用了重新计算。由于TensorFlow 1.15中没有对此的原生支持，我们使用了一个编程技巧。流水线的IPU实现附带了一个顺序调度，可用于在多个IPU上共享计算图。相反，我们没有将处理分布在多个IPU上，而是将它们都放在一个IPU上。请注意，片内存储器中的代码大小几乎没有增加，但是许多中间激活没有被存储，而是被重新计算。因此，我们释放了大量内存，可用于增加批处理大小。通过将我们的splits设置为<code class="fe od oe of og b">b1/0/relu, b1/2/relu, b2/0/relu, b2/2/relu, b3/0/relu, and b3/3/relu</code>，我们将批量增加到20。我们可以通过将一些优化器状态卸载到主机来使用更少的重新计算点。我们避免了这种可能性，并将所有内容都保存在内存中，因为我们通过将更多数据传送到IPU，更好地利用了通信带宽。因此，重新计算只需要三个要素:</p><ul class=""><li id="2667" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">在<a class="ae na" href="https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/api.html#tensorflow.python.ipu.pipelining_ops.pipeline" rel="noopener ugc nofollow" target="_blank">https://docs . graph core . ai/projects/tensor flow 1-user-guide/en/latest/API . html # tensor flow . python . IPU . pipeline _ ops . pipeline</a>使用IPU管道API</li><li id="8690" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将<code class="fe od oe of og b">“PipelineSchedule.Sequential”</code>设置为<code class="fe od oe of og b">“pipeline_schedule”</code></li><li id="8be3" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">通过设置:<br/> <code class="fe od oe of og b">device_mapping = [0] * len(computational_stages)</code>将所有计算阶段放在同一个设备上(IPU 0)——在我们的例子中，有7个计算阶段</li></ul><p id="5480" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平衡重新计算点需要了解计算图中不同阶段的内存占用情况。为了进行微调，我们使用了PopVision图形分析器中的内存配置文件，以确保我们最佳地利用了内存。因此，我们在初始层有一组更密集的重新计算检查点，并保留更长的最终阶段以减少计算开销，因为对于最后阶段，前向传递永远不会重新计算。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi on"><img src="../Images/e7216e720bef3008fd4357577fc632e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sLsKgas6Yqcx94YxyMm1GQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">单个IPU的1472个图块的内存利用率。它达到了每个图块的内存限制。作者使用PopVision图形分析器制作的图片。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi on"><img src="../Images/7bfb121124191a2822dd1b5a09a698d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJsQOCwEx0mAR62SFEY_bw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">用PopVision图形分析器创建的活性报告。我们可以看到除了最后一段之外的前向传递，然后是前向传递(内存增加)和后向传递(内存减少)的每个重新计算段的7次颠簸。图片作者。</p></figure><h2 id="9144" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">扩展主机-IPU通信</h2><p id="1358" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">对于ResNet-50基准数据加载来说，预处理是工作负载中至关重要的一部分。在我们的一个实验中，我们尝试了一个基于英特尔的主机，与使用AMD主机相比，它的CPU内核更少，大大降低了我们的处理速度。因此，我们的分解主机架构对于能够为手头的应用选择正确的主机尤为重要。</p><p id="eaa6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">获得更多传输数据的一个简单方法是减小其大小。增强的图像数据以Int8格式传输，然后在IPU上而不是在主机上进行归一化。归一化是作为一个融合操作实现的，它将数据转换为Float16，并添加第四个带零的通道，以减少后续卷积层的内存占用。</p><p id="cfce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，为了传输更多的数据，数据加载需要使用Graphcore的PopRun框架进行并行化[6]。我们的分析表明，对于每组两个副本，一个主机进程是最好的。每台主机都以8个numa感知节点最佳运行，每个CPU 4个节点。因此，对于IPU-POD 16、64、128和256，我们分别使用了1、4、8和16台主机。该命令类似于:</p><p id="99b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe od oe of og b">poprun --host $HOSTS $MPI_SETTINGS --num-instances “$INSTANCES” --num-replicas “$REPLICAS” python train.py args</code></p><p id="9897" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用$MPI_SETTINGS中的<code class="fe od oe of og b">“<em class="oh">--numa-aware 1”</em></code>激活Numa感知。尽管主机进程是独立的MPI进程，但是跨多个机架创建了所有副本的计算图的组合表示。每个主机进程读取一组大小相等的独立输入文件。因此，避免了文件读取中的任何冲突。即使对性能的影响可以忽略不计，分布式读取数据也会保存在缓存中。</p><h2 id="eec9" class="mc md iq bd me mf mg dn mh mi mj dp mk ko ml mm mn ks mo mp mq kw mr ms mt mu bi translated">其他内存和速度优化</h2><p id="4a03" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">除了重新计算，我们还使用了一些优化策略:</p><ul class=""><li id="6937" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">我们存储批量定额统计数据以避免重新计算。</li><li id="ba0a" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">我们在FP16中做了权重、激活和渐变的所有计算。我们还使用随机舍入来提高收敛速度，使用半偏卷积和矩阵乘法来提高速度。我们的硬件支持随机舍入。请注意，权重更新需要对每个副本进行相同的更新，以确保权重保持一致。这是通过使用horovod广播<br/> <code class="fe od oe of og b">with tf.Graph().as_default(), tf.Session(): <br/> identical_seed = hvd.broadcast( <br/> identical_seed, root_rank=0, name=”broadcast_seed”).eval()</code> <br/>同步起始种子并确保当应用权重更新时IPU被均等地播种来实现的。这是通过<br/>和<code class="fe od oe of og b">export POPLAR_ENGINE_OPTIONS=’{“target.deterministicWorkers”:”portable”}’</code>实现的</li><li id="88ed" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">我们通过优化IPUConfig [5]的<code class="fe od oe of og b">“availableMemoryProportion”</code>,微调了IPU上用于代码的内存和用于存储激活的内存之间的平衡，使其值约为0.15。<br/> <code class="fe od oe of og b">config.convolutions.poplar_options[‘availableMemoryProportion’] = …</code></li></ul><p id="d05f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有关内存和处理优化的更多想法，请查看我们的新指南[4]。</p><h1 id="eddd" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">限制</h1><p id="e9e6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">这个博客中介绍的所有特性在最新版本中都可以使用。然而，我们想指出的是，尽管所有这些技术技巧可能对您的应用程序有所帮助，但它们并不是必须的。MLPerf基准测试带有特殊的规则和限制，不适用于实际应用。</p><ul class=""><li id="3b1c" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">基准要求的目标精度低于最大可达到的精度。</li><li id="b1a5" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">对于基准测试，5次运行中只有4次需要收敛。如果有一个失败，它就会被忽略。</li><li id="e3b1" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">只有超参数的子集可以被改变，并且唯一可用的优化器是具有动量的SGD和LARS。</li><li id="27e2" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">目标度量是训练/收敛时间，而不是吞吐量。这可能是因为随着批次大小和累积计数的增加，处理量可以增加，但收敛时间将需要更多的时间。</li><li id="3a34" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">我们仅每4个时期评估一次。更高的评估频率是违反规则的。</li><li id="bb1d" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">培训仅在ImageNet上进行。</li><li id="8f58" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">学习率计划是二次多项式计划，而不是步进或余弦学习率计划。</li></ul><p id="89fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，没有人会在ImageNet上将ResNet-50训练到75.9%的验证准确率。更容易下载精度更高的预训练网络。</p><p id="f545" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实践中，人们可以在不同的数据集上进行更高精度的训练。但是，当在不同的数据集上训练时，为该基准选择的参数可能不会转移。此外，可能需要更大的历元计数和不同的学习速率计划。随着时代数量的增加，如果训练是从零开始，而不是用预训练的模型，那么使用群体常模(或代理常模)可能会更好。</p><p id="09de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">75.9%的最高准确率对于搜索等应用来说是不够的。想象一下，你有一个像YFCC100M数据集这样的100.000.000张图像的搜索引擎，你试图使用ResNet-50搜索某一类图像，例如折纸。丢失24%的相关图片对你来说可能没问题。挑战在于，因为其他类别也会被错误分类，你会得到更多不是折纸而是来自其他图像的错误分类的图像。事实上，这些错误的分类将是大多数[2]。为了实现更高的精度，值得考虑更多像EfficientNet这样的先进模型，它们受益于本博客中提到的一些特性。你可能还想读一下，<a class="ae na" href="https://www.graphcore.ai/posts/how-we-made-efficientnet-more-efficient" rel="noopener ugc nofollow" target="_blank">“我们如何让EfficientNet更有效率”</a> [3]。</p><h1 id="e2ce" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">结论</h1><p id="c126" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">将强大的TensorFlow 1.15框架与Weights and Biases、Poplar和PopVision工具以及强大的MK2 IPU结合起来，我们成功地充分利用了处理速度和内存，并实现了令人惊叹的性能。</p><h1 id="61de" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">谢谢</h1><p id="fee4" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">非常感谢Brian Nguyen、Godfrey da Costa(和他的团队)以及Mrinal Iyer为完成这项工作所付出的努力。</p><p id="2c18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还要感谢亚历克斯·库尼亚、菲尔·布朗、斯图尔特·科内尔、<a class="lb lc ep" href="https://medium.com/u/b23ad1d05ffe?source=post_page-----2cefe43ab2b2--------------------------------" rel="noopener" target="_blank">多米尼克·马斯特斯、</a>、亚当·桑德斯、霍肯·桑德斯马克、乔治·帕韦尔扎克、西蒙·朗、<a class="lb lc ep" href="https://medium.com/u/83b5c2605cec?source=post_page-----2cefe43ab2b2--------------------------------" rel="noopener" target="_blank">卢克·哈德拉斯-格里</a>、豪尔赫·贝隆·卡斯特罗、乔治·马修以及其他许多人的宝贵贡献。</p><h1 id="1cc1" class="np md iq bd me nq nr ns mh nt nu nv mk jw nw jx mn jz nx ka mq kc ny kd mt nz bi translated">参考</h1><p id="58d6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">[1] A. Labatie，<a class="ae na" rel="noopener" target="_blank" href="/removing-batch-dependence-in-cnns-by-proxy-normalising-activations-bf4824eb0ba4">通过代理标准化激活消除CNN中的批次依赖性</a>，走向数据科学2021</p><p id="1c90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] D. Ma，G. Friedland，M. M. Krell，<a class="ae na" href="https://arxiv.org/abs/2101.05470" rel="noopener ugc nofollow" target="_blank"> OrigamiSet1.0:用于折纸分类和难度估计的两个新数据集</a>，arxiv 2021</p><p id="9091" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] D. Masters，<a class="ae na" rel="noopener" target="_blank" href="/how-we-made-efficientnet-more-efficient-61e1bf3f84b3">我们如何让EfficientNet更加高效</a>，迈向2021年的数据科学</p><p id="f53b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] <a class="ae na" href="https://docs.graphcore.ai/projects/memory-performance-optimisation/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">内存和性能优化指南</a>，Graphcore</p><p id="417c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] <a class="ae na" href="https://docs.graphcore.ai/projects/available-memory/en/latest/available-memory.html" rel="noopener ugc nofollow" target="_blank">在IPU </a>、Graphcore上优化卷积和Matmuls的临时内存使用</p><p id="49a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6] <a class="ae na" href="https://docs.graphcore.ai/projects/poprun-user-guide/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> PopDist和PopRun:用户指南</a>，Graphcore</p><p id="3eae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[7] N. Dimitriou和O. Arandjelovic，<a class="ae na" href="https://arxiv.org/pdf/2007.08554.pdf" rel="noopener ugc nofollow" target="_blank">重影标准化的新观点</a>，arXiv 2020</p><p id="6333" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8] <a class="ae na" href="https://github.com/mlcommons/logging/blob/master/mlperf_logging/rcp_checker/training_1.1.0/rcps_resnet.json" rel="noopener ugc nofollow" target="_blank"> MLPerf参考收敛点</a>，MLCommons</p><p id="efee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[9] MLPerf v1.1培训结果。ml perf ID:1.1–2040，1.1–2042，1.1–2044，1.1–2045，1.1–2065。MLPerf名称和徽标是商标。更多信息请见www.mlperf.org。</p></div></div>    
</body>
</html>