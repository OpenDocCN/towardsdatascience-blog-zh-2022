<html>
<head>
<title>Using Principal Component Analysis (PCA) for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析(PCA)进行机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-principal-component-analysis-pca-for-machine-learning-b6e803f5bf1e#2022-01-31">https://towardsdatascience.com/using-principal-component-analysis-pca-for-machine-learning-b6e803f5bf1e#2022-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="6e4f" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用主成分分析(PCA)进行机器学习</h1></div><div class=""><h2 id="aa1f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用PCA来降低数据集的维数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/191c95ac1614cd243ae4cef0e8c05f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K3ySpLZ4hOYqvvC7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">萨姆·张在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="618f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习中，经常会有包含大量特征的<em class="lv">高维度</em>数据集。高维数据集带来了许多问题，最常见的是过度拟合，这降低了超出训练集中内容的概括能力。因此，您应该采用<em class="lv">降维</em>技术来减少数据集中的要素数量。<strong class="lb iu">主成分分析</strong> ( <strong class="lb iu"> PCA </strong>)就是这样一种技术。</p><p id="5c9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将讨论PCA以及如何将它用于机器学习。特别是，我将向您展示如何在一个样本数据集上应用PCA。</p><h1 id="16d9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">什么是主成分分析？</h1><p id="32a4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">简而言之，PCA是一种<strong class="lb iu">降维</strong>技术<strong class="lb iu"> </strong>，它将数据集中的一组特征转换为数量更少的特征，称为<em class="lv">主成分</em>，同时试图在原始数据集中保留尽可能多的信息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/c35004458aa914558f04f37ceb16f15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*yueVLtJQfw_59fQlrMmAgg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><blockquote class="mu mv mw"><p id="009b" class="kz la lv lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated">PCA的主要目的是<em class="it">减少数据集的变量数量，同时尽可能多地保留信息。</em></p></blockquote><p id="629c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我不解释PCA的工作原理，而是将解释留到下面的视频中，该视频提供了一个很好的PCA工作原理的演示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【https://www.youtube.com/watch?v=FgakZw6K1QQ T4】</p></figure><p id="c5bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解PCA的一个很好的类比是想象自己拍了一张照片。想象你在一场音乐会上，你想通过拍照来捕捉气氛。一张照片只能在二维空间捕捉，而不是在三维空间捕捉大气。虽然维度的减少会导致您丢失一些细节，但是您仍然能够捕获大部分信息。例如，照片中一个人的相对大小告诉我们谁站在前面，谁站在后面。因此，2D图像仍将使我们能够对大部分信息进行编码，否则这些信息只能在3D中获得:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/9eedee9492abc510a70f626fc51089e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X9z8RX8FiSLC1FLa"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼古拉斯·格林在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5187" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，在对数据集应用PCA后，减少的特征(称为<em class="lv">主成分</em>)仍然能够充分表示原始数据集中的信息。</p><p id="9a51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么在数据集上使用PCA有什么优势呢？以下是您希望使用PCA的几个原因:</p><ul class=""><li id="ac77" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated"><strong class="lb iu">删除相关特征</strong>。PCA将帮助您移除所有相关的要素，这种现象称为<em class="lv">多重共线性</em>。查找相关的要素非常耗时，尤其是在要素数量很大的情况下。</li><li id="961d" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><strong class="lb iu">提高机器学习算法性能</strong>。随着PCA减少了特征的数量，训练模型所需的时间也大大减少了。</li><li id="9450" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><strong class="lb iu">减少过拟合</strong>。通过移除数据集中不必要的要素，PCA有助于克服过度拟合。</li></ul><p id="4241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，PCA也有其缺点:</p><ul class=""><li id="63c8" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated"><strong class="lb iu">自变量现在更难解释了</strong>。PCA把你的特征简化成更少的组件。每个组件现在都是原始特征的线性组合，这使得它的可读性和可解释性更差。</li><li id="57e1" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><strong class="lb iu">信息丢失</strong>。如果不小心选择正确的组件数量，可能会发生数据丢失。</li><li id="b1a5" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"><strong class="lb iu">特征缩放</strong>。因为PCA是一个<em class="lv">方差最大化</em>练习，所以PCA要求在处理之前对特征进行缩放。</li></ul><p id="7410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在数据集中有大量要素的情况下，PCA非常有用。</p><blockquote class="mu mv mw"><p id="a722" class="kz la lv lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated">在机器学习中，PCA是一种无监督的机器学习算法。</p></blockquote><h1 id="6972" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使用样本数据集</h1><p id="cc36" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我将使用来自sklearn的经典乳腺癌数据集来演示PCA:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9569" class="nw lx it ns b gy nx ny l nz oa">from sklearn.datasets import load_breast_cancer<br/>breast_cancer = load_breast_cancer()</span></pre><p id="cbaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乳腺癌数据集是说明PCA的一个很好的例子，因为它具有大量的特征，并且所有特征的数据类型都是浮点数。让我们打印该数据集中的要素名称和数量:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="ca46" class="nw lx it ns b gy nx ny l nz oa">print(breast_cancer.feature_names)<br/>print(len(breast_cancer.feature_names))</span></pre><p id="43ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该看到以下内容:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="a062" class="nw lx it ns b gy nx ny l nz oa">['mean radius' 'mean texture' 'mean perimeter' 'mean area'<br/> 'mean smoothness' 'mean compactness' 'mean concavity'<br/> 'mean concave points' 'mean symmetry' 'mean fractal dimension'<br/> 'radius error' 'texture error' 'perimeter error' 'area error'<br/> 'smoothness error' 'compactness error' 'concavity error'<br/> 'concave points error' 'symmetry error' 'fractal dimension error'<br/> 'worst radius' 'worst texture' 'worst perimeter' 'worst area'<br/> 'worst smoothness' 'worst compactness' 'worst concavity'<br/> 'worst concave points' 'worst symmetry' 'worst fractal dimension']<br/>30</span></pre><p id="23cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们也打印出目标，并检查目标的含义和目标的分布:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="560d" class="nw lx it ns b gy nx ny l nz oa">import numpy as np</span><span id="2e29" class="nw lx it ns b gy ob ny l nz oa">print(breast_cancer.target) <br/>print(breast_cancer.target_names) <br/>print(np.array(np.unique(breast_cancer.target, return_counts=True)))</span></pre><p id="32aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该会看到类似这样的内容:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="6ab6" class="nw lx it ns b gy nx ny l nz oa">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 <br/> ...<br/> 1 1 1 1 1 1 1 0 0 0 0 0 0 1]<br/>['malignant' 'benign']<br/>[[  0   1]<br/> [212 357]]</span></pre><p id="dd0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标值0表示肿瘤是恶性的，而1表示肿瘤是良性的。目标不平衡，但不严重。</p><h1 id="574b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">检查特征和目标之间的关系</h1><p id="888b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">此时，能够可视化每个特征如何影响诊断是有用的——肿瘤是恶性的还是良性的。让我们为每个特征绘制一个直方图，然后用颜色区分恶性和良性肿瘤:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="3ae7" class="nw lx it ns b gy nx ny l nz oa">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="6aab" class="nw lx it ns b gy ob ny l nz oa">_, axes = plt.subplots(6,5, figsize=(15, 15))</span><span id="f766" class="nw lx it ns b gy ob ny l nz oa">malignant = breast_cancer.data[breast_cancer.target==0]<br/>benign = breast_cancer.data[breast_cancer.target==1]</span><span id="5f3e" class="nw lx it ns b gy ob ny l nz oa">ax = axes.ravel()                     # flatten the 2D array</span><span id="d979" class="nw lx it ns b gy ob ny l nz oa">for i in range(30):                   # for each of the 30 features<br/>    bins = 40</span><span id="91f1" class="nw lx it ns b gy ob ny l nz oa">    #---plot histogram for each feature---<br/>    ax[i].hist(malignant[:,i], bins=bins, color='r', alpha=.5)<br/>    ax[i].hist(benign[:,i], bins=bins, color='b', alpha=0.3)</span><span id="d4a5" class="nw lx it ns b gy ob ny l nz oa">    #---set the title---<br/>    ax[i].set_title(breast_cancer.feature_names[i], fontsize=12)    </span><span id="8ad1" class="nw lx it ns b gy ob ny l nz oa">    #---display the legend---<br/>    ax[i].legend(['malignant','benign'], loc='best', fontsize=8)<br/>    <br/>plt.tight_layout()<br/>plt.show()</span></pre><p id="4967" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该会看到以下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/2cdd39613dfded6ec64b6b062d5406fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9a0FhUroZi81RxDrbviinQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个特征，如果两个直方图是分开的，这意味着该特征是重要的，并且它直接影响目标(诊断)。例如，如果您查看<strong class="lb iu">平均半径</strong>特征的直方图，您会发现肿瘤越大，肿瘤越有可能是恶性的(红色):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dd373cede8f9fe0af4ab02d73d98eb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*FYg_QMwQlGq83_ppBVtkKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，<strong class="lb iu">平滑度误差</strong>功能并不能真正告诉您肿瘤是恶性还是良性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3917157e090bc2563a8bbc9a70cc23a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*G6QnP-2FcXOU_Adm4t2Jhw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="a75e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">将数据加载到数据帧中</h1><p id="a06f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下一步是将乳腺癌数据加载到熊猫数据框架中:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="42aa" class="nw lx it ns b gy nx ny l nz oa">import pandas as pd</span><span id="aaf6" class="nw lx it ns b gy ob ny l nz oa">df = pd.DataFrame(breast_cancer.data, <br/>                  columns = breast_cancer.feature_names)<br/>df['diagnosis'] = breast_cancer.target<br/>df</span></pre><p id="eb03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该会看到以下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/5b10d5b3c3c72a222d0d7723924cd392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pke12IVg6zk_41OeH6rGXA.png"/></div></div></figure><h1 id="947e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">方法1-使用所有特征训练模型</h1><p id="98a8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在我们对数据集执行PCA之前，让我们使用逻辑回归来训练一个使用数据集中所有30个特征的模型，并看看它的表现如何:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="8f99" class="nw lx it ns b gy nx ny l nz oa">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split</span><span id="3e83" class="nw lx it ns b gy ob ny l nz oa">X = df.iloc[:,:-1]      <br/>y = df.iloc[:,-1]</span><span id="355f" class="nw lx it ns b gy ob ny l nz oa">#---perform a split---<br/>random_state = 12<br/>X_train, X_test, y_train, y_test = \<br/>    train_test_split(X, y,<br/>                     test_size = 0.3,<br/>                     shuffle = True,<br/>                     random_state=random_state)</span><span id="bc65" class="nw lx it ns b gy ob ny l nz oa">#---train the model using Logistic Regression---<br/>log_reg = LogisticRegression(max_iter = 5000)<br/>log_reg.fit(X_train, y_train)</span><span id="d0d5" class="nw lx it ns b gy ob ny l nz oa">#---evaluate the model---<br/>log_reg.score(X_test,y_test)</span></pre><p id="ac59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型的精度为:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="3f13" class="nw lx it ns b gy nx ny l nz oa">0.9239766081871345</span></pre><h1 id="f755" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">方法2-使用简化的特征训练模型</h1><p id="519f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对于下一个方法，让我们检查各种特性，并尝试消除那些与目标最不相关的特性。同时，我们还希望移除那些显示多重共线性的特征。目的是减少特征的数量，看看是否可以提高模型的精度。</p><h2 id="0fa9" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">获取相关因子</h2><p id="f29f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们首先获得每个特征相对于目标(诊断)的相关性:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="aecd" class="nw lx it ns b gy nx ny l nz oa">df_corr = df.corr()['diagnosis'].abs().sort_values(ascending=False)<br/>df_corr</span></pre><p id="9b41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该看到以下内容:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="bfe1" class="nw lx it ns b gy nx ny l nz oa">diagnosis                  1.000000<br/>worst concave points       0.793566<br/>worst perimeter            0.782914<br/>mean concave points        0.776614<br/>worst radius               0.776454<br/>mean perimeter             0.742636<br/>worst area                 0.733825<br/>mean radius                0.730029<br/>mean area                  0.708984<br/>mean concavity             0.696360<br/>worst concavity            0.659610<br/>mean compactness           0.596534<br/>worst compactness          0.590998<br/>radius error               0.567134<br/>perimeter error            0.556141<br/>area error                 0.548236<br/>worst texture              0.456903<br/>worst smoothness           0.421465<br/>worst symmetry             0.416294<br/>mean texture               0.415185<br/>concave points error       0.408042<br/>mean smoothness            0.358560<br/>mean symmetry              0.330499<br/>worst fractal dimension    0.323872<br/>compactness error          0.292999<br/>concavity error            0.253730<br/>fractal dimension error    0.077972<br/>smoothness error           0.067016<br/>mean fractal dimension     0.012838<br/>texture error              0.008303<br/>symmetry error             0.006522<br/>Name: diagnosis, dtype: float64</span></pre><p id="a3de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们提取与目标具有相对高相关性的所有那些特征(我们任意地将阈值设置为0.6):</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="07fc" class="nw lx it ns b gy nx ny l nz oa"># get all the features that has at least 0.6 in correlation to the <br/># target<br/>features = df_corr[df_corr &gt; 0.6].index.to_list()[1:]<br/>features                          # without the 'diagnosis' column</span></pre><p id="c6a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在你看到了以下特征:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="a5e1" class="nw lx it ns b gy nx ny l nz oa">['worst concave points',<br/> 'worst perimeter',<br/> 'mean concave points',<br/> 'worst radius',<br/> 'mean perimeter',<br/> 'worst area',<br/> 'mean radius',<br/> 'mean area',<br/> 'mean concavity',<br/> 'worst concavity']</span></pre><p id="993d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是很明显，有几个特征是相关的-例如，半径、周长和面积都是相关的。其中一些功能必须删除。</p><h2 id="8611" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">检查多重共线性</h2><p id="5742" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们移除那些显示多重共线性的要素:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="6d04" class="nw lx it ns b gy nx ny l nz oa">import pandas as pd<br/>from sklearn.linear_model import LinearRegression</span><span id="fb30" class="nw lx it ns b gy ob ny l nz oa">def calculate_vif(df, features):    <br/>    vif, tolerance = {}, {}</span><span id="b8fa" class="nw lx it ns b gy ob ny l nz oa">    # all the features that you want to examine<br/>    for feature in features:<br/>        # extract all the other features you will regress against<br/>        X = [f for f in features if f != feature]        <br/>        X, y = df[X], df[feature]</span><span id="917e" class="nw lx it ns b gy ob ny l nz oa">        # extract r-squared from the fit<br/>        r2 = LinearRegression().fit(X, y).score(X, y)                <br/>        <br/>        # calculate tolerance<br/>        tolerance[feature] = 1 - r2</span><span id="521f" class="nw lx it ns b gy ob ny l nz oa">        # calculate VIF<br/>        vif[feature] = 1/(tolerance[feature])<br/>    # return VIF DataFrame<br/>    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})</span><span id="8b1f" class="nw lx it ns b gy ob ny l nz oa">calculate_vif(df,features)</span></pre><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">Python中的统计数据-共线性和多重共线性</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">了解如何发现数据集中的多重共线性</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div><p id="7265" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该会看到以下输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/95ea19510a8d89e185da8fc68a70ad3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*ekxiHUmJCDSV0aDmBMrCsA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="50c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您的目标是移除VIF大于5的那些要素。您可以用不同的特性反复调用<code class="fe pk pl pm ns b">calculate_vif()</code>函数，直到您得到一个所有VIF值都小于5的特性集。</p><p id="3911" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过一些尝试，我将范围缩小到3个特征:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="0312" class="nw lx it ns b gy nx ny l nz oa"># try to reduce those feature that has high VIF until each feature <br/># has VIF less than 5<br/>features = [<br/><strong class="ns iu">    'worst concave points',<br/>    'mean radius',<br/>    'mean concavity',<br/></strong>]<br/>calculate_vif(df,features)</span></pre><p id="50bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们的VIF价值观看起来很棒:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/f781ce21739f4d8b75564b399be8f46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*ukd5ZOdy8hPW095yLiF4TA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="8f72" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">训练模型</h2><p id="4eb2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">随着30个特征减少到3个，现在让我们使用逻辑回归来训练模型:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="b90c" class="nw lx it ns b gy nx ny l nz oa">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split</span><span id="ad82" class="nw lx it ns b gy ob ny l nz oa">X = df.loc[:,features]            # get the reduced features in the <br/>                                  # dataframe<br/>y = df.loc[:,'diagnosis']</span><span id="44b9" class="nw lx it ns b gy ob ny l nz oa"># perform a split<br/>X_train, X_test, y_train, y_test = \<br/>    train_test_split(X, y, <br/>                     test_size = 0.3,<br/>                     shuffle = True,                                                    <br/>                     random_state=random_state)</span><span id="f6dd" class="nw lx it ns b gy ob ny l nz oa">log_reg = LogisticRegression()<br/>log_reg.fit(X_train, y_train)<br/>log_reg.score(X_test,y_test)</span></pre><p id="d04b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一次，精确度下降到:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="84af" class="nw lx it ns b gy nx ny l nz oa">0.847953216374269</span></pre><h1 id="d652" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">方法3-使用简化特征(PCA)训练模型</h1><p id="ff43" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最后，让我们将PCA应用于数据集，看看是否可以训练出更好的模型。</p><h2 id="e458" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">执行标准缩放</h2><p id="b5e3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">还记得PCA对缩放很敏感吗？因此，第一步是对30个特征执行标准缩放:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d488" class="nw lx it ns b gy nx ny l nz oa">from sklearn.preprocessing import StandardScaler</span><span id="c475" class="nw lx it ns b gy ob ny l nz oa"># get the features and label from the original dataframe<br/>X = df.iloc[:,:-1]<br/>y = df.iloc[:,-1]</span><span id="c0ff" class="nw lx it ns b gy ob ny l nz oa"># performing standardization<br/>sc = StandardScaler()<br/>X_scaled = sc.fit_transform(X)</span></pre><h2 id="8b00" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated"><strong class="ak">应用主成分分析</strong></h2><p id="9d19" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">您现在可以使用<strong class="lb iu"> sklearn.decomposition </strong>模块中的<code class="fe pk pl pm ns b">PCA</code>类将PCA应用于特性:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="7f57" class="nw lx it ns b gy nx ny l nz oa">from sklearn.decomposition import PCA</span><span id="54d1" class="nw lx it ns b gy ob ny l nz oa">components = None<br/>pca = PCA(n_components = components)</span><span id="e94d" class="nw lx it ns b gy ob ny l nz oa"># perform PCA on the scaled data<br/>pca.fit(X_scaled)</span></pre><p id="423a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA类的初始化器有一个名为<code class="fe pk pl pm ns b">n_components</code>的参数。您可以为其提供下列值之一:</p><ul class=""><li id="b3db" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">一个整数，用于指示要将要素减少到多少个主分量。</li><li id="8291" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">0 <n and="" it="" will="" return="" the="" number="" of="" components="" needed="" to="" capture="" specified="" percentage="" variability="" in="" data.="" for="" example="" if="" you="" want="" find="" data="" pass="" parameter.=""/></li><li id="2fe8" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated"> 【T5】 . In this case, the number of components returned will be the same as the number of original features in the dataset</li></ul><p id="459c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Once the components are determined using the  【T6】  method, you can print out the <em class="lv">之间的浮点数解释差异</em>:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="b569" class="nw lx it ns b gy nx ny l nz oa"># print the explained variances<br/>print("Variances (Percentage):")<br/>print(pca.explained_variance_ratio_ * 100)<br/>print()</span></pre><p id="9b78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该会看到以下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="03e1" class="nw lx it ns b gy nx ny l nz oa">Variances (Percentage):<br/>[4.42720256e+01 1.89711820e+01 9.39316326e+00 6.60213492e+00<br/> 5.49576849e+00 4.02452204e+00 2.25073371e+00 1.58872380e+00<br/> 1.38964937e+00 1.16897819e+00 9.79718988e-01 8.70537901e-01<br/> 8.04524987e-01 5.23365745e-01 3.13783217e-01 2.66209337e-01<br/> 1.97996793e-01 1.75395945e-01 1.64925306e-01 1.03864675e-01<br/> 9.99096464e-02 9.14646751e-02 8.11361259e-02 6.01833567e-02<br/> 5.16042379e-02 2.72587995e-02 2.30015463e-02 5.29779290e-03<br/> 2.49601032e-03 4.43482743e-04]</span></pre><p id="0244" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么如何解读上面的输出呢？你可以这样理解:</p><ul class=""><li id="f549" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">仅第一个分量就捕获了数据中约44%的可变性</li><li id="d9fa" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">第二个捕获了数据中大约19%的可变性，等等。</li><li id="b715" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">这30个组件总共捕获了100%的数据可变性。</li></ul><p id="0924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解上述结果的一个更简单的方法是打印累积差异:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="f8c9" class="nw lx it ns b gy nx ny l nz oa">print("Cumulative Variances (Percentage):")<br/>print(pca.explained_variance_ratio_.cumsum() * 100)<br/>print()</span></pre><p id="28a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您现在应该会看到以下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d98b" class="nw lx it ns b gy nx ny l nz oa">Cumulative Variances (Percentage):<br/>[ 44.27202561  63.24320765  72.63637091  79.23850582  84.73427432<br/>  88.75879636  91.00953007  92.59825387  93.98790324  95.15688143<br/>  96.13660042  97.00713832  97.81166331  98.33502905  98.64881227<br/>  98.91502161  99.1130184   99.28841435  99.45333965  99.55720433<br/>  99.65711397  99.74857865  99.82971477  99.88989813  99.94150237<br/>  99.96876117  99.99176271  99.99706051  99.99955652 100.        ]</span></pre><p id="de3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以将累计差异解释如下:</p><ul class=""><li id="0472" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">仅第一个分量就捕获了数据中约44%的可变性</li><li id="9bd4" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">前两个组件捕获数据中大约63%的可变性，依此类推。</li><li id="c08d" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">前8个分量一起捕获了数据中大约92.6%的可变性。</li></ul><p id="958e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看累积方差的直观方法是绘制<strong class="lb iu"> scree图</strong>。</p><blockquote class="mu mv mw"><p id="96fb" class="kz la lv lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated"><strong class="lb iu">碎石图</strong>是主成分的线形图。</p></blockquote><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="dcb6" class="nw lx it ns b gy nx ny l nz oa"># plot a scree plot<br/>components = len(pca.explained_variance_ratio_) \<br/>    if components is None else components</span><span id="22c1" class="nw lx it ns b gy ob ny l nz oa">plt.plot(range(1,components+1), <br/>         np.cumsum(pca.explained_variance_ratio_ * 100))<br/>plt.xlabel("Number of components")<br/>plt.ylabel("Explained variance (%)")</span></pre><p id="118f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">scree图使您可以轻松直观地看到捕捉数据中各种可变性所需的组件数量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/8483843e8f76a95ff4cb7ff837054770.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*MbUKlDAp7hw7Ha530it0lg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="f581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们应用主成分分析，根据期望的解释方差(比如85%)找到期望的组件数量:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="316b" class="nw lx it ns b gy nx ny l nz oa">from sklearn.decomposition import PCA</span><span id="a17c" class="nw lx it ns b gy ob ny l nz oa">pca = <strong class="ns iu">PCA(n_components = 0.85)</strong><br/>pca.fit(X_scaled)</span><span id="1e96" class="nw lx it ns b gy ob ny l nz oa">print("Cumulative Variances (Percentage):")<br/>print(np.cumsum(pca.explained_variance_ratio_ * 100))</span><span id="476c" class="nw lx it ns b gy ob ny l nz oa">components = len(pca.explained_variance_ratio_)<br/>print(f'Number of components: {components}')</span><span id="1b9d" class="nw lx it ns b gy ob ny l nz oa"># Make the scree plot<br/>plt.plot(range(1, components + 1), np.cumsum(pca.explained_variance_ratio_ * 100))<br/>plt.xlabel("Number of components")<br/>plt.ylabel("Explained variance (%)")</span></pre><p id="0720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将获得以下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="0aeb" class="nw lx it ns b gy nx ny l nz oa">Cumulative Variances (Percentage):<br/>[44.27202561 63.24320765 72.63637091 79.23850582 84.73427432 88.75879636]<br/>Number of components: 6</span></pre><p id="5e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您从图表中看到的，需要6个组件来涵盖数据中85%的可变性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/993e4fe0df06580463b9cc5bce356b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*3m0bQoGmx-IIdCld1MyXtQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dcac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还可以使用<code class="fe pk pl pm ns b">pca</code>对象的<code class="fe pk pl pm ns b">components_</code>属性找出对每个组件有贡献的每个特性的重要性:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="4a49" class="nw lx it ns b gy nx ny l nz oa">pca_components = abs(<strong class="ns iu">pca.components_</strong>)<br/>print(pca_components)</span></pre><p id="9c05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将看到如下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="c4f0" class="nw lx it ns b gy nx ny l nz oa">[[2.18902444e-01 1.03724578e-01 2.27537293e-01 2.20994985e-01<br/>  1.42589694e-01 2.39285354e-01 2.58400481e-01 2.60853758e-01<br/>  1.38166959e-01 6.43633464e-02 2.05978776e-01 1.74280281e-02<br/>  2.11325916e-01 2.02869635e-01 1.45314521e-02 1.70393451e-01<br/>  1.53589790e-01 1.83417397e-01 4.24984216e-02 1.02568322e-01<br/>  2.27996634e-01 1.04469325e-01 2.36639681e-01 2.24870533e-01<br/>  1.27952561e-01 2.10095880e-01 2.28767533e-01 2.50885971e-01<br/>  1.22904556e-01 1.31783943e-01]<br/> [2.33857132e-01 5.97060883e-02 2.15181361e-01 2.31076711e-01<br/>  1.86113023e-01 1.51891610e-01 6.01653628e-02 3.47675005e-02<br/>  1.90348770e-01 3.66575471e-01 1.05552152e-01 8.99796818e-02<br/>  8.94572342e-02 1.52292628e-01 2.04430453e-01 2.32715896e-01<br/>  1.97207283e-01 1.30321560e-01 1.83848000e-01 2.80092027e-01<br/>  2.19866379e-01 4.54672983e-02 1.99878428e-01 2.19351858e-01<br/>  1.72304352e-01 1.43593173e-01 9.79641143e-02 8.25723507e-03<br/>  1.41883349e-01 2.75339469e-01]<br/> [8.53124284e-03 6.45499033e-02 9.31421972e-03 2.86995259e-02<br/>  1.04291904e-01 7.40915709e-02 2.73383798e-03 2.55635406e-02<br/>  4.02399363e-02 2.25740897e-02 2.68481387e-01 3.74633665e-01<br/>  2.66645367e-01 2.16006528e-01 3.08838979e-01 1.54779718e-01<br/>  1.76463743e-01 2.24657567e-01 2.88584292e-01 2.11503764e-01<br/>  4.75069900e-02 4.22978228e-02 4.85465083e-02 1.19023182e-02<br/>  2.59797613e-01 2.36075625e-01 1.73057335e-01 1.70344076e-01<br/>  2.71312642e-01 2.32791313e-01]<br/> [4.14089623e-02 6.03050001e-01 4.19830991e-02 5.34337955e-02<br/>  1.59382765e-01 3.17945811e-02 1.91227535e-02 6.53359443e-02<br/>  6.71249840e-02 4.85867649e-02 9.79412418e-02 3.59855528e-01<br/>  8.89924146e-02 1.08205039e-01 4.46641797e-02 2.74693632e-02<br/>  1.31687997e-03 7.40673350e-02 4.40733510e-02 1.53047496e-02<br/>  1.54172396e-02 6.32807885e-01 1.38027944e-02 2.58947492e-02<br/>  1.76522161e-02 9.13284153e-02 7.39511797e-02 6.00699571e-03<br/>  3.62506947e-02 7.70534703e-02]<br/> [3.77863538e-02 4.94688505e-02 3.73746632e-02 1.03312514e-02<br/>  3.65088528e-01 1.17039713e-02 8.63754118e-02 4.38610252e-02<br/>  3.05941428e-01 4.44243602e-02 1.54456496e-01 1.91650506e-01<br/>  1.20990220e-01 1.27574432e-01 2.32065676e-01 2.79968156e-01<br/>  3.53982091e-01 1.95548089e-01 2.52868765e-01 2.63297438e-01<br/>  4.40659209e-03 9.28834001e-02 7.45415100e-03 2.73909030e-02<br/>  3.24435445e-01 1.21804107e-01 1.88518727e-01 4.33320687e-02<br/>  2.44558663e-01 9.44233510e-02]<br/> [1.87407904e-02 3.21788366e-02 1.73084449e-02 1.88774796e-03<br/>  2.86374497e-01 1.41309489e-02 9.34418089e-03 5.20499505e-02<br/>  3.56458461e-01 1.19430668e-01 2.56032561e-02 2.87473145e-02<br/>  1.81071500e-03 4.28639079e-02 3.42917393e-01 6.91975186e-02<br/>  5.63432386e-02 3.12244482e-02 4.90245643e-01 5.31952674e-02<br/>  2.90684919e-04 5.00080613e-02 8.50098715e-03 2.51643821e-02<br/>  3.69255370e-01 4.77057929e-02 2.83792555e-02 3.08734498e-02<br/>  4.98926784e-01 8.02235245e-02]]</span></pre><p id="602d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个要素的重要性通过输出中相应值的大小来反映，大小越大，重要性越高。下图显示了如何解释上述结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/5ae69b323d09dc9c07349bec4b2bd996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iDWUR7wJ-iHyMiexa1aGbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ea3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于好奇，让我们打印出对6个组成部分贡献最大的前4个特征:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="d1b8" class="nw lx it ns b gy nx ny l nz oa">print('Top 4 most important features in each component')<br/>print('===============================================')<br/>for row in range(pca_components.shape[0]):<br/>    # get the indices of the top 4 values in each row<br/>    temp = np.argpartition(-(pca_components[row]), 4)<br/>    <br/>    # sort the indices in descending order<br/>    indices = temp[np.argsort((-pca_components[row])[temp])][:4]<br/>    <br/>    # print the top 4 feature names<br/>    print(f'Component {row}: {df.columns[indices].to_list()}')</span></pre><p id="cf2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将看到以下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="88a6" class="nw lx it ns b gy nx ny l nz oa">Top 4 most important features in each component<br/>===============================================<br/>Component 0: ['mean concave points', 'mean concavity', <br/>              'worst concave points', 'mean compactness']<br/>Component 1: ['mean fractal dimension', 'fractal dimension error', <br/>              'worst fractal dimension', 'mean radius']<br/>Component 2: ['texture error', 'smoothness error', <br/>              'symmetry error', 'worst symmetry']<br/>Component 3: ['worst texture', 'mean texture', <br/>              'texture error', 'mean smoothness']<br/>Component 4: ['mean smoothness', 'concavity error', <br/>              'worst smoothness', 'mean symmetry']<br/>Component 5: ['worst symmetry', 'symmetry error', <br/>              'worst smoothness', 'mean symmetry']</span></pre><h2 id="5f55" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">将所有30列转换为6个主成分</h2><p id="aa65" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，您可以将数据集中30列的标准化数据转换为6个主要成分:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="3e4b" class="nw lx it ns b gy nx ny l nz oa">X_pca = pca.transform(X_scaled)<br/>print(X_pca.shape)<br/>print(X_pca)</span></pre><p id="b9cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您应该得到以下输出:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="b41e" class="nw lx it ns b gy nx ny l nz oa">(569, 6)<br/>[[ 9.19283683  1.94858307 -1.12316616  <br/>   3.6337309  -1.19511012  1.41142445]<br/> [ 2.3878018  -3.76817174 -0.52929269  <br/>   1.11826386  0.62177498  0.02865635]<br/> [ 5.73389628 -1.0751738  -0.55174759  <br/>   0.91208267 -0.1770859   0.54145215]<br/> ...<br/> [ 1.25617928 -1.90229671  0.56273053 <br/>   -2.08922702  1.80999133 -0.53444719]<br/> [10.37479406  1.67201011 -1.87702933 <br/>   -2.35603113 -0.03374193  0.56793647]<br/> [-5.4752433  -0.67063679  1.49044308<br/>  -2.29915714 -0.18470331  1.61783736]]</span></pre><h2 id="e954" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">创建机器学习管道</h2><p id="7ed2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，让我们创建一个机器学习管道，以便我们可以正式化整个过程:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="ca42" class="nw lx it ns b gy nx ny l nz oa">from sklearn.pipeline import Pipeline<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA<br/>from sklearn.linear_model import LogisticRegression</span><span id="7b99" class="nw lx it ns b gy ob ny l nz oa">_sc = StandardScaler()<br/>_pca = PCA(n_components = components)<br/>_model = LogisticRegression()</span><span id="b87e" class="nw lx it ns b gy ob ny l nz oa">log_regress_model = Pipeline([<br/>    ('std_scaler', _sc),<br/>    ('pca', _pca),<br/>    ('regressor', _model)<br/>])</span></pre><p id="a8f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将数据集分为训练集和测试集，并使用训练集训练模型:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="1257" class="nw lx it ns b gy nx ny l nz oa"># perform a split<br/>X_train, X_test, y_train, y_test = \<br/>    train_test_split(X, y, <br/>                     test_size=0.3,<br/>                     shuffle=True, <br/>                     random_state=random_state)</span><span id="d5ef" class="nw lx it ns b gy ob ny l nz oa"># train the model using the PCA components<br/>log_regress_model.fit(X_train,y_train)</span></pre><p id="602b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对模型进行评分，看看它的表现如何:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="177d" class="nw lx it ns b gy nx ny l nz oa">log_regress_model.score(X_test,y_test)</span></pre><p id="73cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在有以下准确度:</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="b1f0" class="nw lx it ns b gy nx ny l nz oa">0.9824561403508771</span></pre><h1 id="a67f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="9e4a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我们讨论了PCA背后的思想以及使用它的利与弊。特别是，我们训练了三个模型:</p><ul class=""><li id="3719" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">使用乳腺癌数据集中的所有30个特征</li><li id="e9eb" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">仅使用数据集中的3个要素</li><li id="5a23" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">将PCA应用于数据集，然后使用6个组件进行训练</li></ul><p id="7fe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于您自己的数据集，尝试一下我在本文中演示的各种方法，看看哪种方法性能更好，将会很有用。</p><h2 id="a414" class="nw lx it bd ly og oh dn mc oi oj dp mg li ok ol mi lm om on mk lq oo op mm oq bi translated">如果你喜欢阅读我的文章，并且认为它对你的职业/学习有所帮助，请考虑注册成为一名灵媒会员。每月5美元，你可以无限制地访问Medium上的所有文章(包括我的)。如果你使用下面的链接注册，我会赚一小笔佣金(不需要你额外付费)。你的支持意味着我将能够投入更多的时间来写这样的文章。</h2><div class="or os gp gr ot ou"><a href="https://weimenglee.medium.com/membership" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">加入媒介与我的介绍链接-李伟孟</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">weimenglee.medium.com</p></div></div><div class="pd l"><div class="pz l pf pg ph pd pi ks ou"/></div></div></a></div></div></div>    
</body>
</html>