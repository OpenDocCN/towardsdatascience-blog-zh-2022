# 聚类—对数据科学家来说是一项艰巨的任务

> 原文：<https://towardsdatascience.com/clustering-a-daunting-task-for-a-data-scientist-5c7f362ccfb8>

## 设计群集模型的准则

![](img/108a93530d210f1390cae8a43481960e.png)

照片由 [Unsplash](https://unsplash.com/s/photos/clouds-sky?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的[尼克·费因斯](https://unsplash.com/@jannerboy62?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄

考虑这样一种情况，一个政党邀请您(一名数据科学家)分析人口数据，以帮助他们赢得即将到来的选举。你的任务是找出全国人口中支持该党的群体。有了这些信息，党就可以在不同的地区策划他们的运动。这是在大规模数据集上聚类(形成组)的经典例子之一。还有许多其他情况下数据集很小，主要是因为数据隐私和收集数据本身的方式。

无论您拥有的是小数据还是大数据，即使对于经验丰富的数据科学家来说，聚类也始终是一项挑战。这主要是因为“集群”本身的概念没有精确的定义。一个数据科学家可能在一个数据集中找到三个聚类，而另一个数据科学家可能将同一个数据集聚类成四个区域。通常，数据科学家之间可能不会就此达成共识；然而，最终的决定通常是由数据集的利益相关者做出的。正是他们对数据的知识和理解最终满足了他们的需求。

那么，数据科学家如何满足客户的要求呢？

# 数据科学家的方法

对于数据科学家来说，解决聚类问题的第一个也是最重要的方面是检查数据集的大小。根据大小，数据科学家选择合适的聚类模型和算法。一段时间以来，研究人员开发了许多满足各种数据大小的特定需求的聚类算法。对于小到中等大小的数据集，我们使用简单的基于连通性的聚类算法；而对于具有多元分布的非常大的数据集，必须使用基于非常先进的统计技术的算法。选择聚类算法是数据科学家的知识和技能。在本文中，我将描述数据科学家解决这个重要问题的方法。为了理解这种方法，让我们简要地考虑一下聚类类别和每一类别中的各种算法。

# 聚类类别

聚类算法大致分类如下:

*   基于质心的
*   基于连通性
*   基于分布的
*   基于密度
*   聚类巨大的数据集
*   八卦式聚类
*   基于网格的

我现在将向您介绍这些类别，并简要描述每个类别中的算法。

# 基于质心的

*k-means* 、 *k-medoids* 算法就属于这一类。这些可能是最容易理解的，并且通常是学习集群的起点。在这里，您需要在运行算法之前指定想要形成的聚类数。有一些可用的技术，如*肘*、*平均轮廓*和*间隙统计*，用于选择集群形成的最佳数量。这种情况下的优化问题是 NP 难的，因此它通常只提供近似解。该模型非常适合中小型数据集。

# 基于连通性

*凝聚*和*分裂*聚类算法就属于这一类。这些算法基于这样的概念，即根据对象之间的距离来连接对象。他们为整个数据集创建一个*树状图*或层次结构。这种可视化表示有助于您识别集群。聚类主要取决于距离函数的类型和连锁准则的选择。我们在专门的应用中使用这种类型的聚类，如确定动物进化的系统进化树、通过绘制系统进化树来跟踪病毒、通过分析文本相似性来分类文档等等。如果您希望在确定聚类时获得数据集的分层表示，可以使用这些算法。

# 基于分布的

*高斯混合模型* (GMM)就是属于这一类的一种算法。这里，我们假设利益相关者对其数据集的统计分布有所了解。由于在数据分布中存在强假设，如果假设出错，算法将产生不利的聚类。因此，仅当您确定数据集的统计分布时，才使用此类算法，在这种情况下，该算法将产生出色的聚类结果。

# 基于密度

*DBSCAN* 算法就属于这一类。某个区域中数据点的高密度定义了群集。密度的下降标志着星团的边界。对于像高斯混合这样的分布，这种算法非常有效。这种类型的聚类的最重要的方面是它可以创建任意形状的聚类，而前面讨论的算法大多产生圆形聚类。第二，算法复杂度相当低，并且输出是确定的——这意味着它们在每次运行中发现相同的结果。

*光学*是属于同一类别的另一种算法。它推广了 DBSCAN。该算法还产生链接可视化，这有助于数据科学家创建他选择的集群。

*Mean-Shift* 算法是这一类别中的另一种算法，其中我们基于核密度估计(KDE)来形成聚类。该算法可以检测任意形状的簇，但比 DBSCAN 慢。此外，它不能在多维数据集上产生良好的聚类。

当数据集很大时，数据科学家使用基于密度的聚类。

# 庞大的数据集

你如何聚集巨大规模的数据集？你使用了一个众所周知的策略——*分而治之*。T4 桦树 T5 算法就属于这一类。我们将数据集分成更小的单元，在每次分割中尽可能多地保留原始数据集的信息。然后，我们使用其他已知的聚类技术对每个分割单元进行聚类。这种技术的优势在于，它消除了将整个数据集加载到内存中的需要。

*CLARANS* 是这一类别中的另一种算法，它在对数据集进行聚类时使用随机搜索。然后，它对每个单元应用 PAM(围绕 Medoids 划分)算法来确定其聚类质量。如果不好，它重复整个聚类过程。因此，它保持了计算成本和数据随机采样之间的平衡。它允许多边形物体和可以使用任何任意距离测量功能。它提供了高质量的聚类。

# 八卦式聚类

也被称为*相似性传播*聚类，这里我们通过对等体之间的闲聊来形成聚类。这就像社交网络。我们根据成员和他们的领导之间的某种亲密关系来组建团体。同伴消息传递和一个人成为小组领导的意愿决定了亲和力度量。你不需要预先估计集群的数量。该算法肯定会产生非常好的结果，尤其是当您不知道数据集可能具有的聚类数时。该算法计算量很大。

# 基于网格的

到目前为止，我们考虑的所有上述算法都是基于查询的。如果您决定尝试另一个查询，您将需要再次扫描整个数据集。因此，所有这些聚类技术的计算量都很大。

*STING* 算法就属于这一类。最初，我们将整个数据集分割成矩形单元，层次结构中的每个较高级别的单元包含较低级别的单元集合的摘要。由于该结构是独立于查询的，因此我们可以并行化整个集群过程，从而节省集群时间。不仅如此，由于其性质，它可以处理增量更新，这是动态数据集中的一个要求。另一个优点是该算法的低复杂度。

*CLIQUE* 是这一类的另一个算法。它结合了基于密度和网格的聚类技术。该算法从一维子空间开始，不断融合计算高维子空间。它使用一种先验技术来寻找可聚类的子空间，非常适合聚类高维数据集。

# 结束语

到目前为止，我已经讨论了许多聚类类别，这些类别可以帮助您根据数据集大小和客户对聚类的期望来决定使用哪种算法。我在这篇小文章中介绍的概述可以帮助您选择一种算法来对小型、中型、大型甚至空间数据集进行聚类。

聚类通常是非平凡的，因为它的无监督的性质和事实上可能没有对最终结果的共识。作为一名数据科学家，您应该对这些不同的聚类算法、构建它们的目的以及它们所解决的聚类问题有一个很好的概念性概述。几个库提供了这些算法的有效实现，作为一名数据科学家，你不必担心它们背后的数学。只要了解每一类算法的用途，你就能解决任何聚类问题。

> 要了解更多信息，你可能想参考我即将出版的书，[思考数据科学](https://link.springer.com/book/9783031023620)(应用机器学习中的斯普林格系列)。它有一大部分是关于聚类的，有关于所有算法的实际例子。

[](https://medium.com/@profsarang/membership) 