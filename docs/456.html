<html>
<head>
<title>Implementing Linear Regression with Gradient Descent From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始用梯度下降实现线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-linear-regression-with-gradient-descent-from-scratch-f6d088ec1219#2022-01-16">https://towardsdatascience.com/implementing-linear-regression-with-gradient-descent-from-scratch-f6d088ec1219#2022-01-16</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="cd2a" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">从头做起</h2><div class=""><h1 id="9f5d" class="pw-post-title jc jd iu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">从零开始用梯度下降实现线性回归</h1></div><div class=""><h2 id="0299" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">通过充分理解和推导梯度下降，建立强大的机器学习基础</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/8bf531cbe89c37edd4b7949a11fce4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HXXyDFWGhV7pvo8E"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Pawel Czerwinski 在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="656e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi mf translated">在我个人在机器学习和数据科学领域的旅程中，我基本上跳过了所有的基础知识，带着一种荒谬的紧迫感奔向被认为是更奇特的东西<em class="mo"/>。</p><p id="cc2c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">不用说，我也忽略了线性回归。我认为这是浪费时间，因为这是一个简单的初学者算法，没有什么需要学习的。</p><blockquote class="mp"><p id="cb4b" class="mq mr iu bd ms mt mu mv mw mx my me dk translated">回顾过去，我大错特错了。</p></blockquote><p id="1e3a" class="pw-post-body-paragraph lj lk iu ll b lm mz ke lo lp na kh lr ls nb lu lv lw nc ly lz ma nd mc md me in bi translated">在接下来的章节中，我们将使用Python和NumPy一步步实现<em class="mo">线性回归</em>。我们还将学习<em class="mo">梯度下降</em>，这是机器学习领域最常见的优化算法之一，通过从头开始推导它。</p><p id="09a4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是在直接进入实现细节之前，让我们建立一些关于线性回归和梯度下降的基本直觉。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="22b1" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">为什么是线性回归？</h1><p id="5322" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">线性回归是一种简单的监督学习算法，用于预测定量反应。它已经存在了相当一段时间，与更现代的机器学习方法相比，它被认为是非常基础的。</p><blockquote class="mp"><p id="ff09" class="mq mr iu bd ms mt mu mv mw mx my me dk translated">然而，线性回归可以作为一个很好的起点，因为它涵盖了机器学习中许多重要和基本的概念。</p></blockquote><div class="oi oj ok ol om"><div role="button" tabindex="0" class="ab bv gw cb fq on oo bn op lc ex"><div class="oq l"><div class="ab q"><div class="l di"><img alt="Marvin Lanhenke" class="l de bw or os fe" src="../Images/5b1b337a332bf18381aa650edc2190bd.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*iEJ9qn9i9dTw3uyXG2jsTA.jpeg"/><div class="fb bw l or os fc n aw fd"/></div><div class="hi l fp"><p class="bd b dl z fq fr fs ft fu fv fw fx dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@marvinlanhenke?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">马文·兰亨克</a></p></div></div><div class="ov ow gx l"><h2 class="bd je wl qt fq wm fs ft rt fv fx jd bi translated">机器学习基础</h2></div><div class="ab q"><div class="l fp"><a class="bd b be z bi wn au wo wp wq tc wr an eh ei ws wt wu el em eo de bk ep" href="https://medium.com/@marvinlanhenke/list/machine-learning-fundamentals-3f7110a3669b?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fp"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="pi dh pj fq ab pk fp di"><div class="di pa bv pb pc"><div class="dh l"><img alt="" class="dh" src="../Images/fa288a91f2ab1aecbea1f0cbde524f22.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*S-fQ7kmjq6OY0GUb"/></div></div><div class="di pa bv pd pe pf"><div class="dh l"><img alt="" class="dh" src="../Images/04d334592a1527722ced57fe140b531e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*nRQOfdHqS0D4q3l4"/></div></div><div class="di bv pg ph pf"><div class="dh l"><img alt="" class="dh" src="../Images/fa565992c62ad24e2f468acda820c749.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*WZw13eggaGBaQ6CU"/></div></div></div></div></div><p id="7d33" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">假设线性，线性回归的目标是通过优化权重和偏差以最小化残差平方和来找到最佳估计量。</p><p id="3cb4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">或者简单地说，我们试图找到最佳拟合线、平面或超平面，使预测值和真实值之间的平方距离最小化。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pp"><img src="../Images/07bbe707f8ad439972b7aedbd500521e.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*1Y77kIQXPfM3wy9c8Kr7ig.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">简单线性回归最小化MSE的例子[图片由作者提供]</p></figure><p id="fc3e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">更正式地说，我们试图最小化的损失函数可以表述为下面的等式</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pq"><img src="../Images/3fd21d6f2e76488cb4a3c9086b1a956b.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/0*PvjYsW-ivVuxt66h"/></div></figure><p id="6551" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">其中<em class="mo"> n </em>描述样本数量，<em class="mo"> Y </em>等于真实值，<em class="mo"> Y-hat </em>定义我们的预测值。</p><p id="fc03" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">根据特征的数量，我们可以将线性回归视为简单的线性回归或多重线性回归。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="2fbb" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">导出梯度下降</h1><p id="9dcd" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">尽管在1847年首次提出，梯度下降仍然是机器学习中最常见的优化算法之一。</p><blockquote class="mp"><p id="2508" class="mq mr iu bd ms mt mu mv mw mx my me dk translated">主要思想是通过在梯度的相反方向上迭代地采取小步骤来寻找可微函数的局部最小值。</p></blockquote><p id="357f" class="pw-post-body-paragraph lj lk iu ll b lm mz ke lo lp na kh lr ls nb lu lv lw nc ly lz ma nd mc md me in bi translated">我们可以把梯度想成包含函数在某一点的一阶偏导数的向量。不严格地说，梯度描述了一个函数的斜率，告诉我们最快增长的方向和速率。</p><p id="c437" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">因为我们想向下移动并找到函数的最小值，所以我们需要向相反的方向移动。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pr"><img src="../Images/51c15a8b9b03a5ee9764ca823b102afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*4Wob-p3zS7V9hBPlJ6kJjA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">梯度下降的例子[图片由作者提供]</p></figure><blockquote class="ps pt pu"><p id="c11b" class="lj lk mo ll b lm ln ke lo lp lq kh lr pv lt lu lv pw lx ly lz px mb mc md me in bi translated"><strong class="ll je">注意</strong>:步长由一个叫做学习率的超参数控制。</p></blockquote><p id="5de7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在线性回归的情况下，这意味着我们必须通过找到其局部最小值来最小化损失函数<em class="mo"> (MSE) </em>。因此，我们需要计算梯度，并在相反的方向上迭代地采取小的步骤。</p><p id="3aa0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了能够计算梯度，我们有一些工作要做——我们需要<em class="mo">导出损失函数相对于每个参数的一阶偏导数</em>。</p><p id="1324" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但首先，让我们重申并稍微修改损失函数:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj py"><img src="../Images/887d6874ac484e087a3abf6fb4df0fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*Co4CiFtojSRjFbj3"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">n =样本；w =重量；b =偏差；x =特征</p></figure><blockquote class="ps pt pu"><p id="6801" class="lj lk mo ll b lm ln ke lo lp lq kh lr pv lt lu lv pw lx ly lz px mb mc md me in bi translated"><strong class="ll je">注</strong>:我们添加了一个常数“2”只是为了在求导时方便。</p></blockquote><h2 id="a051" class="pz nm iu bd nn qa qb dn nr qc qd dp nv ls qe qf nx lw qg qh nz ma qi qj ob ja bi translated">重量偏导数:</h2><p id="192f" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">现在，我们需要对<em class="mo">外部函数</em>和<em class="mo">内部函数</em>求相对于权重的偏导数，并应用链式法则。</p><p id="6fdb" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果我们认为括号内的表达式为<code class="fe qk ql qm qn b">z = ((Xw + b)-Y)</code>，那么外部函数可以描述为<code class="fe qk ql qm qn b">z²</code>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qo"><img src="../Images/6d372cb51c62a80452ab2e4310ddc902.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/0*U_q_T4zVMYmp8Kqf"/></div></figure><p id="0f64" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">接下来，我们需要考虑内部函数，并对权重求导。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qp"><img src="../Images/20e38ad46ec13bea6362163732a0986e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/0*PrR3YvQlin4nbKqG"/></div></figure><p id="5b88" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">应用链式法则，我们可以计算损失函数相对于权重的偏导数。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qq"><img src="../Images/da90a294625118cebeee943ed884013d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*p3ag4tX1lyHJCE9s"/></div></figure><h2 id="72d2" class="pz nm iu bd nn qa qb dn nr qc qd dp nv ls qe qf nx lw qg qh nz ma qi qj ob ja bi translated">偏导数w.r.t .偏差:</h2><p id="7b7f" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">现在，我们简单地重复之前的步骤，但是这一次对内函数求导。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qr"><img src="../Images/044f51fb03f8719774e531a188d80bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/0*XzK2YvsyqQHwshq_"/></div></figure><p id="7f65" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">再次应用链式法则，我们得到如下结果:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qs"><img src="../Images/35242ede227a0cd9a8c233d41db94305.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/0*v4Wzqi_BwLTi8Qf2"/></div></figure><p id="0970" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">就是这样——我们已经计算完了权重和偏差的梯度。当我们在下一节实现梯度下降时，这两个方程都会派上用场。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="08a6" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">介绍算法</h1><p id="671f" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">在我们从头开始编码和实现线性回归之前，我们还有一件事要做——我们需要知道算法一般是如何工作的。</p><p id="5b65" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">基本上，我们需要执行3个步骤:</p><ol class=""><li id="6856" class="qt qu iu ll b lm ln lp lq ls qv lw qw ma qx me qy qz ra rb bi translated">初始化权重和偏差</li><li id="5fff" class="qt qu iu ll b lm rc lp rd ls re lw rf ma rg me qy qz ra rb bi translated">执行<em class="mo"> n </em>次迭代的梯度下降，这包括进行预测、计算梯度以及更新权重和偏差</li><li id="277b" class="qt qu iu ll b lm rc lp rd ls re lw rf ma rg me qy qz ra rb bi translated">做最后的预测</li></ol><p id="f54a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于第二步涉及多个动作，我们将把它分解成几个辅助函数。</p><p id="c239" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们将用Python和Numpy在一个类中实现该算法。下面的<em class="mo">骨架级</em>可以被视为某种蓝图，指导我们通过下一节。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="af8f" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">从头开始实施</h1><h2 id="911a" class="pz nm iu bd nn qa qb dn nr qc qd dp nv ls qe qf nx lw qg qh nz ma qi qj ob ja bi translated">基本设置</h2><p id="1d03" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">首先，我们需要做一些内务处理，<strong class="ll je">设置一些参数</strong>(学习率，迭代次数)以及权重和偏差。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><h2 id="3578" class="pz nm iu bd nn qa qb dn nr qc qd dp nv ls qe qf nx lw qg qh nz ma qi qj ob ja bi translated">主算法</h2><p id="d4c1" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">现在，我们能够实现算法的核心计算步骤。</p><p id="dde2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们要做的第一件事是计算样本的数量和训练数据的特征。接下来，我们调用助手函数<strong class="ll je">来初始化权重和偏差。</strong></p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><p id="0169" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">继续进行<strong class="ll je">梯度下降</strong> —在一次迭代中，我们必须做出预测，计算权重和偏差的梯度，并相应地更新参数。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><p id="4ed3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">做出预测</strong>非常简单，因为它只需要我们计算训练数据和权重的点积。可以在顶部添加偏置。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><p id="77c1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">一旦我们获得了初始预测，我们需要<strong class="ll je">计算梯度</strong>。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><p id="af79" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">上面代码中的等式对我们来说应该很熟悉，因为我们已经做了一些工作，并在前面的部分中推导了它们。</p><p id="fc2d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们要做的最后一件事是<strong class="ll je">用计算出的梯度更新权重和偏差</strong>。</p><p id="8da8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这也很简单——我们只需将学习率乘以相关的梯度，然后从当前值中减去乘积。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><h2 id="4d82" class="pz nm iu bd nn qa qb dn nr qc qd dp nv ls qe qf nx lw qg qh nz ma qi qj ob ja bi translated">预测并把它们放在一起</h2><p id="b2f8" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">到目前为止，我们基本上完成了所有的艰苦工作——我们只需要实现最后一个方法来进行最终预测，并将所有代码放在一起<strong class="ll je">来完成我们的最终类</strong>。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="ccda" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">测试回归量</h1><p id="4c02" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">已经完成了我们自己的梯度下降线性回归的实现，我们仍然需要测试它。</p><p id="40e0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">出于测试目的，我们将简单地使用<code class="fe qk ql qm qn b">sklearn.datasets.make_regression()</code>并创建一个只有一个特征的基本数据集(这使我们更容易可视化)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj rj"><img src="../Images/633f5aa5337df0809f48e3aaab54336f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*BRkP6RFi51k_P425cXyQZQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">数据集的散点图[图片由作者提供]</p></figure><p id="75a7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们可以分割数据集，实例化并拟合我们的回归变量来进行预测。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="rh ri l"/></div></figure><p id="4b7c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">运行上面的代码，我们得到大约20.5的均方根误差。通过绘制我们的结果，我们可以看到最佳拟合线，看起来我们的算法是可行的。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj rj"><img src="../Images/960d1571904d08020eb4d26900546f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*MJOd78UmkaOQWITf0jYl3w.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">估计最佳拟合线[图片由作者提供]</p></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="8fcd" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">结论</h1><p id="7b49" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">在本文中，我们从零开始一步一步地实现了线性回归。我们还学习了梯度下降是如何工作的，以及如何从头开始推导它。</p><p id="d1da" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">虽然与更现代、更花哨的机器学习模型相比，线性回归似乎非常基础和枯燥，但它仍然对我们有巨大的价值。</p><p id="388b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">例如，更深入地了解梯度下降的实际工作方式，可以让我们建立一个强大的基础——为我们在机器学习领域的旅程中更复杂的算法做准备。</p><p id="c96d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你可以在我的GitHub上的这里找到完整的代码。</p><div class="rk rl gq gs rm"><div role="button" tabindex="0" class="ab bv gw cb fq on oo bn op lc ex"><div class="oq l"><div class="ab q"><div class="l di"><img alt="Marvin Lanhenke" class="l de bw or os fe" src="../Images/5b1b337a332bf18381aa650edc2190bd.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*iEJ9qn9i9dTw3uyXG2jsTA.jpeg"/><div class="fb bw l or os fc n aw fd"/></div><div class="hi l fp"><p class="bd b dl z fq fr fs ft fu fv fw fx dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@marvinlanhenke?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">马文·兰亨克</a></p></div></div><div class="ov ow gx l"><h2 class="bd je wl qt fq wm fs ft rt fv fx jd bi translated">从零开始的ML算法</h2></div><div class="ab q"><div class="l fp"><a class="bd b be z bi wn au wo wp wq tc wr an eh ei ws wt wu el em eo de bk ep" href="https://medium.com/@marvinlanhenke/list/ml-algorithms-from-scratch-7621d01922ad?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fp"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="pi dh pj fq ab pk fp di"><div class="di pa bv pb pc"><div class="dh l"><img alt="" class="dh" src="../Images/230471643f4baf6e91fca0422ddfb56b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qT41zaPnyJHQeGKz0FvkQA.png"/></div></div><div class="di pa bv pd pe pf"><div class="dh l"><img alt="" class="dh" src="../Images/ca1fa6207e26d895ef8ba30bb66ef3a2.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*67IA2OsmsNXtckdF"/></div></div><div class="di bv pg ph pf"><div class="dh l"><img alt="" class="dh" src="../Images/33c295e3ac2a12346fba6df057ae448b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HXXyDFWGhV7pvo8E"/></div></div></div></div></div><div class="rk rl gq gs rm"><div role="button" tabindex="0" class="ab bv gw cb fq on oo bn op lc ex"><div class="oq l"><div class="ab q"><div class="l di"><img alt="Marvin Lanhenke" class="l de bw or os fe" src="../Images/5b1b337a332bf18381aa650edc2190bd.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*iEJ9qn9i9dTw3uyXG2jsTA.jpeg"/><div class="fb bw l or os fc n aw fd"/></div><div class="hi l fp"><p class="bd b dl z fq fr fs ft fu fv fw fx dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@marvinlanhenke?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">马文·兰亨克</a></p></div></div><div class="ov ow gx l"><h2 class="bd je wl qt fq wm fs ft rt fv fx jd bi translated"># 30日</h2></div><div class="ab q"><div class="l fp"><a class="bd b be z bi wn au wo wp wq tc wr an eh ei ws wt wu el em eo de bk ep" href="https://medium.com/@marvinlanhenke/list/30daysofnlp-3974a0c731d6?source=post_page-----f6d088ec1219--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fp"><span class="bd b dl z dk">30 stories</span></div></div></div><div class="pi dh pj fq ab pk fp di"><div class="di pa bv pb pc"><div class="dh l"><img alt="" class="dh" src="../Images/97f1fa41b4f518ab6047fde7d260d65f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMx8KrP6WWr--LtJXG5x0A.png"/></div></div><div class="di pa bv pd pe pf"><div class="dh l"><img alt="" class="dh" src="../Images/d605569da3c842a21a16b568b04cf244.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*xuW2bol3CmI5YIyrAKXb9A.png"/></div></div><div class="di bv pg ph pf"><div class="dh l"><img alt="" class="dh" src="../Images/382a4c66fbeaa2d71de204436d7b4f68.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*h_3pnMi0WF2rd9Ci4GNfGw.png"/></div></div></div></div></div></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><p id="0bcc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mo">喜欢这篇文章吗？成为</em> <a class="ae li" href="https://medium.com/@marvinlanhenke/membership" rel="noopener"> <em class="mo">中等会员</em> </a> <em class="mo">继续无限学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="rk rl gq gs rm rn"><a href="https://medium.com/@marvinlanhenke/membership" rel="noopener follow" target="_blank"><div class="ro ab fp"><div class="rp ab rq cl cj rr"><h2 class="bd je gz z fq rs fs ft rt fv fx jd bi translated">通过我的推荐链接加入Medium-Marvin Lanhenke</h2><div class="ru l"><h3 class="bd b gz z fq rs fs ft rt fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="rv l"><p class="bd b dl z fq rs fs ft rt fv fx dk translated">medium.comm</p></div></div><div class="rw l"><div class="rx l ry rz sa rw sb lc rn"/></div></div></a></div></div></div>    
</body>
</html>