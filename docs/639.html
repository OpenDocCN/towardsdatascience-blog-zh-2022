<html>
<head>
<title>End-to-End Attention-Based Machine Translation Model with Minimum Tensorflow Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于端到端注意力的最小张量流代码机器翻译模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-attention-based-machine-translation-model-with-minimum-tensorflow-code-ae2f08cc8218#2022-01-22">https://towardsdatascience.com/end-to-end-attention-based-machine-translation-model-with-minimum-tensorflow-code-ae2f08cc8218#2022-01-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="28d5" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">基于端到端注意力的最小张量流代码机器翻译模型</h1></div><p id="b3b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇博文介绍了仅使用高级Tensorflow API的基于注意力的机器翻译的训练和推理。这是一个简化的端到端教程，旨在减少代码行。</p><p id="d114" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于这个话题有很多在线教程。但大多数都是包含大量样板代码的<a class="ae ko" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention" rel="noopener ugc nofollow" target="_blank"> Tensorflow官方教程</a>的变种。虽然理解它是如何使用低级API实现的是有帮助的，但是容易获得和简化的实现通常是受欢迎的——初学者可以使用它来快速形成对主题的直觉；从业者可以很容易地借用自包含的代码片段。这篇博文提供了一个简化的基于注意力的机器翻译代码的端到端实现，仅使用高级Tensorflow Keras层实现。</p><blockquote class="kp kq kr"><p id="62fc" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">翻译任务</strong></p></blockquote><p id="3104" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将执行的翻译任务与Tensorflow官方教程中的任务相同。我们要把西班牙语翻译成英语。我们将重用与Tensorflow官方教程中相同的数据集，它是西班牙语和英语对的列表，存储在一个文本文档中，每对一行。我们下载数据集并解压。注意，我们还需要用于文本处理的<code class="fe kw kx ky kz b">tensorflow_text</code>包。</p><pre class="la lb lc ld gt le kz lf lg aw lh bi"><span id="5015" class="li lj it kz b gy lk ll l lm ln">$ curl <a class="ae ko" href="http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip" rel="noopener ugc nofollow" target="_blank">http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip</a> –output spa-eng.zip<br/>$ unzip spa-eng.zip<br/>$ pip install –user tensorflow-text</span></pre><blockquote class="kp kq kr"><p id="5c7e" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">预处理</strong></p></blockquote><p id="9ac4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">文本预处理是任何自然语言文章中不可或缺的一步。如果您有兴趣了解各种文本预处理技术的影响，请随意查看这篇<a class="ae ko" href="https://levelup.gitconnected.com/does-text-preprocessing-affect-natural-language-processing-performance-ccadaaaab39b" rel="noopener ugc nofollow" target="_blank">博客文章</a>了解更多细节。我们利用<code class="fe kw kx ky kz b">TextLineDataset</code> API直接构建数据集，避免了Tensorflow官方教程中的大量样板代码。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">数据加载</p></figure><p id="0646" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在数据集已经可以使用了，我们使用<code class="fe kw kx ky kz b">TextVectorization</code>层来索引文本中的记号。这部分和Tensorflow官方教程几乎一模一样。目标是为西班牙语和英语构建各自的词汇集，这样我们就可以将标记映射到索引，稍后将使用索引进行嵌入。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">文本预处理</p></figure><blockquote class="kp kq kr"><p id="4ba5" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">模型架构</strong></p></blockquote><p id="02a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇博文的核心是:模型。在进入模型代码之前，让我们看看模型架构的高级视图。该模型是典型的序列对序列模型。西班牙语输入被送入双向递归神经网络，我们称之为双向编码器。双向编码器的最终状态<code class="fe kw kx ky kz b">s</code>成为解码器的初始状态。双向编码器<code class="fe kw kx ky kz b">h</code>的输出由<a class="ae ko" href="https://en.wikipedia.org/wiki/Attention_(machine_learning)" rel="noopener ugc nofollow" target="_blank">注意</a>层<code class="fe kw kx ky kz b">aij</code>加权，并与英文输入组合。我们使用<a class="ae ko" href="https://en.wikipedia.org/wiki/Teacher_forcing" rel="noopener ugc nofollow" target="_blank">教师强制</a>机制来训练解码器，即解码器的英语输入来自上一步的预期(而不是实际)解码器输出。众所周知，这种机制可以加快训练速度，因为各个步骤可以通过正确的输入独立学习，而不是依赖于来自其先前步骤的潜在虚假输入。这也使得实现更加容易。解码器输出最终的英语。请参见下图进行概述。</p><figure class="la lb lc ld gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lv"><img src="../Images/39b60ec443fe8d739442a2ab238c8406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fdMPXLK9Azb1PlhSYH5iGQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型架构</p></figure><p id="6959" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，解码器的英语输入具有<code class="fe kw kx ky kz b">[START]</code>标记，解码器的英语输出具有<code class="fe kw kx ky kz b">[END]</code>标记。一个用于启动文本生成，另一个用于终止文本生成。英语输入/输出内容也移动一步。双向编码器的西班牙语输入具有<code class="fe kw kx ky kz b">[START]</code>和<code class="fe kw kx ky kz b">[END]</code>标记。这仅仅是因为我们为两种语言重用了相同的<code class="fe kw kx ky kz b">standardize</code>函数。理论上，西班牙语输入不需要这两个标记。</p><p id="b95e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在推理时间，我们不再有预期的英语作为输入。因此，我们必须使用上一步中解码器的输出，一步一步地生成输出序列。</p><blockquote class="kp kq kr"><p id="da12" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">型号代码</strong></p></blockquote><p id="c676" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型代码实现为Tensorflow Keras模型的子类。请参见接口的以下代码片段。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型界面</p></figure><p id="c2f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在初始化过程中，我们为模型构建了必要的层，包括西班牙语和英语的<code class="fe kw kx ky kz b">TextVectorization</code>层、西班牙语和英语的<code class="fe kw kx ky kz b">Embedding</code>层、西班牙语编码器的<code class="fe kw kx ky kz b">Bidirectional</code>层、英语解码器的<code class="fe kw kx ky kz b">LSTM</code>层、编码器和解码器之间的<code class="fe kw kx ky kz b">Attention</code>层，以及将解码器的输出映射到英语词汇索引的最终<code class="fe kw kx ky kz b">Dense</code>层。参见下面初始化函数的代码片段。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型初始化</p></figure><p id="1e9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">几个值得注意的点:<code class="fe kw kx ky kz b">Bidirectional</code>层的内部<code class="fe kw kx ky kz b">LSTM</code>有其他层的一半单元，因为<code class="fe kw kx ky kz b">Bidirectional</code>层将连接向前和向后输出。<code class="fe kw kx ky kz b">Bidirectional</code>编码器和<code class="fe kw kx ky kz b">LSTM</code>解码器都设置为返回序列和状态，因为它们将在训练/推断过程中使用。两个<code class="fe kw kx ky kz b">Embedding</code>层都将<code class="fe kw kx ky kz b">mask_zero</code>设置为<code class="fe kw kx ky kz b">True</code>，因为由于句子长度可变，我们将依靠蒙版来忽略0填充。</p><p id="d758" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型调用调用这些层并返回输出。请参见以下调用函数的代码片段。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型调用</p></figure><p id="21b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，有几点值得注意:<code class="fe kw kx ky kz b">Bidirectional</code>层单独返回状态。所以我们需要把它们连接起来。解码器的英语输入是没有终止符<code class="fe kw kx ky kz b">[END]</code>的英语句子。来自解码器的预期英语输出是没有起始标记<code class="fe kw kx ky kz b">[START]</code>的英语句子。解码器的输出形状为<code class="fe kw kx ky kz b">(batch, Te-1, rnn_output_dim)</code>。<code class="fe kw kx ky kz b">eng_vocab_size</code>单元的最终输出<code class="fe kw kx ky kz b">Dense</code>层足够智能以产生形状<code class="fe kw kx ky kz b">(batch, Te-1, eng_vocab_size)</code>的输出。最初由嵌入层创建的掩码通过所有层传递，有时是隐式的(在递归神经网络中)，有时是显式的(在注意层和最终函数返回中)。</p><blockquote class="kp kq kr"><p id="34d6" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">训练代码</strong></p></blockquote><p id="2aff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练代码遍历成批的句子对，收集模型的输出，计算损失，并应用梯度下降。有关培训部分，请参见下面的代码片段。最重要的一点是，在损失计算中，我们需要屏蔽掉不相关的元素——因为它们只是可变长度句子的填充。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模特培训</p></figure><p id="37fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过50个时期后，我们可以看到模型损耗接近于零。这意味着我们的模型有能力过度拟合数据，这是模型能力的一个很好的指标。这里可以应用许多正则化技术来保证模型的泛化能力。但是这超出了这篇博文的范围。</p><figure class="la lb lc ld gt lo gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/17493e8fbf48f5dde8e7b1ad968cfe9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*405HWRK5mmoZX_ykd5lQ8Q.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">培训损失</p></figure><blockquote class="kp kq kr"><p id="d9cb" class="jq jr ks js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated"><strong class="js iu">推理代码</strong></p></blockquote><p id="2cb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">推理代码与模型调用代码非常相似。关键的区别在于，我们需要在推理过程中一步一步地生成英语输出，而不是一次生成全部，因为我们需要上一步的解码器输出。有关推理代码，请参见以下代码片段。注意，推理代码还返回注意力矩阵，我们可以用它来可视化西班牙语输入和英语输出之间的模型注意力。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">模型推理</p></figure><p id="9f8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，让我们为随机选取的西班牙语输入<code class="fe kw kx ky kz b">“Hoy tengo la tarde libre, así que pienso ir al parque, sentarme bajo un árbol y leer un libro.”</code>调用<code class="fe kw kx ky kz b">translate</code>推理函数。输出是<code class="fe kw kx ky kz b">“i have the afternoon off today , so i plan to go to the park , sit under a tree and read a book .”</code>，它与数据集中的英文输出完全相同，只是由于我们的文本预处理，去掉了字母大小写和标点符号空间。不奇怪。正如我们上面讨论的，模型是过度拟合的。让我们也使用一个类似于Tensorflow官方教程中的helper函数的<code class="fe kw kx ky kz b">plot_attention</code>函数来可视化注意力。</p><figure class="la lb lc ld gt lo"><div class="bz fp l di"><div class="lp lq l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">注意力绘图</p></figure><figure class="la lb lc ld gt lo gh gi paragraph-image"><div class="gh gi md"><img src="../Images/db57bd3b021c6fd02e7e088935cc2729.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*Zd5VeDGHmNBQo-bwa7bOIg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">注意力图</p></figure></div></div>    
</body>
</html>