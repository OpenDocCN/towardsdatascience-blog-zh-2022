# 机器学习中的数据泄漏:如何检测和最小化风险

> 原文：<https://towardsdatascience.com/data-leakage-in-machine-learning-how-it-can-be-detected-and-minimize-the-risk-8ef4e3a97562>

![](img/6b3f9356904353292367335f648fbf0e.png)

作者图片

# **简介**

我们可以将数据泄漏定义为:

“当数据集包含相关数据，但在使用模型进行预测时无法获得类似数据时，就会发生数据泄漏(或泄露)。这导致了在训练数据集上的巨大成功(甚至可能是验证准确性)，但是在生产中缺乏性能。”

数据泄露，或仅仅是泄露，是机器学习过程中使用的一个术语，用来描述用于教授机器学习算法的数据包含关于你正在估计的主题的意外额外信息的情况。当在学习过程中引入关于目标标签或号码的信息，而在实际使用过程中无法合法访问这些信息时，就会发生泄漏。数据泄漏最基本的例子是数据集的真实标签作为一个特征包含在模型中。如果这个物体被归类为苹果，算法将学习预测它是苹果。

# **为什么会发生数据泄露？**

数据泄漏可能有多种原因，通常以微妙和难以检测的方式发生。当数据泄漏发生时，它通常会在模型构建阶段导致过于乐观的结果，随后是在预测模型实施并对新数据进行测试后令人不快的糟糕结果。换句话说，泄漏可能会导致您的系统训练一个次优模型，该模型在实践中的表现比在无泄漏环境中生成的模型差得多。泄漏会产生各种各样的实际影响，从造成可怕的财务和技术支出的财务费用到损害消费者对系统可靠性的看法或影响雇主产品的系统故障。因此，数据泄露是机器学习和统计学中最重要和最普遍的挑战之一，也是深度学习实践者必须注意的一个问题。

**“现在，我们来了解一下什么是数据泄露，为什么它很重要，如何识别它，以及如何在您的实际应用中防止它？”**

在数据保护领域，术语“数据泄露”也与未经授权将信息移出受保护设施(如数据中心)有关。然而，考虑到将有关预测的细节与再培训和模型开发阶段完全分开的重要性，这种保护方法实际上相对适合我们的机器教育环境。

# **更复杂版本中的数据泄漏**

让我们来看看一些更微妙的数据泄漏问题。

**第一个例子:**

一个典型的例子是当将来的信息被合并到训练数据中时，这些信息在实际使用中是不合法的。假设您在一个零售网站上工作，需要创建一个分类器来预测用户是会留下来阅读另一个页面还是会离开。如果分类器预测他们将离开，网站可能会显示一些鼓励他们留下来购物的内容。用户的总会话长度或他们访问网站期间看到的页面总数是包含泄漏信息的特征的例子。例如，在访问日志数据的后处理阶段，这个总数经常作为新列添加。该功能包含用户未来访问的信息，例如用户还将进行多少次旅行。在现实部署中，这是不可能确定的。“总会话长度”功能可能会被“会话中页面访问”功能所取代，该功能只知道会话中到目前为止已经访问了多少页面，而不知道还剩下多少页面。

**第二个例子:**

泄露的第二个例子可能是试图预测访问银行网站的客户是否会开户。如果用户的记录包括帐号字段，对于仍在浏览网站的用户，它可能是空白的，但在用户创建帐户后，它将被填充。显然，在这种情况下，用户帐户字段不是一个可行的功能，因为当用户仍在浏览站点时，它可能不可用。

**第三个例子:**

如果您正在构建一个诊断测试来预测特定的医疗状况，另一个在过去泄漏未来信息的例子是，如果您正在生成一个诊断测试来预测特定的医疗状况。现有患者数据集中的二进制变量可能指示患者是否为该疾病做过手术。显然，这样的特征将是医学状况的极好预测。预测数据可以通过多种不同的方式进入该功能集。有可能缺失诊断代码的特定组合是医学状况的高度症状。然而，由于在检查病人的情况时，这些信息是不可用的，因此利用这些信息是非法的。最后，同一个病人的另一个例子可能是病人 ID 表单。可以基于所采用的诊断路径来分配 ID。换句话说，如果 ID 是去看专家的结果，而最初的医生判断可能是医学问题，那么 ID 可能是不同的。

最后一个例子说明了这样一个事实，即数据泄漏可以以多种方式在训练集中表现出来，并且许多泄漏问题经常同时出现。例如，修复一个漏洞有时可能会揭示另一个漏洞的存在。

# **其他数据泄露的例子**

以下是更多数据泄露的例子，以供参考。**泄漏可分为两类**。 ***训练数据中的泄漏*** ，当测试或未来数据与训练数据混合时发生，以及 ***特征中的泄漏，*** 当关于真实标签的非常有信息的东西作为特征被包括时发生。

*   **训练数据中的数据泄露:**

对整个数据集执行某种预处理是数据泄漏的最常见原因之一，其结果会影响训练期间看到的内容。这可能包括计算归一化和重缩放的参数、查找最小和最大特征值以检测和移除异常值、估计训练集中的缺失值，或者使用变量在整个数据集中的分布来执行要素选择。在处理时间序列数据时，当未来事件的记录被错误地用于计算特定预测的特征时，另一个重要的谨慎要求就出现了。我们看到的会话长度示例只是一个示例，但是如果存在数据收集错误或缺少值指示符，可能会产生更微妙的后果。如果某个要素要求在特定时间段内至少收集一条记录，则错误的存在可能会提供有关未来的信息。换句话说，不再预测更多的观测值。

*   **功能泄漏:**

特征泄漏是指当我们删除一个变量，如诊断 ID 或患者记录，但我们忘记删除包含相同或可比信息的其他变量，称为代理变量。一个很好的例子是患者 ID，它包含了由于入院程序而导致的患者病情严重程度的提示。在某些情况下，预测变量记录被故意随机化，或者共享用户个人数据(包括他们的姓名、位置等)的特定部分被抽象出来。根据预测任务，解除这种隐私保护可能会暴露在实际使用中无法合法访问的用户或其他敏感数据。

最后，上述任何场景都可以在第三方数据集中表示，这些数据集也作为新特征的来源添加到训练集中。因此，要时刻注意这些外部数据的特征，以及它们的解释和来源。

# **如何检测数据泄露？**

在这一节中，我定义了三个步骤来检测和避免应用程序中的数据泄漏。

1.  **在开始构建模型之前，做好以下准备工作:**

在开始构建模型之前，探索性数据分析可能会在开发模型之前揭示数据中的意外情况。例如，寻找与期望的标签或价值高度相关的品质。在医疗诊断示例中，指示患者经历了针对疾病的特定外科治疗的二元特征可以是这样的示例。这可能与某种特定的疾病密切相关。

**2。** **在你建立了你的模型之后:**

建立模型后，在拟合的模型中搜索不寻常的特征行为，例如与变量相关的异常高的特征权重或极大的信息博弈。接下来，搜索一款车型出乎意料的整体表现。如果您的评估模型结果明显高于相同或可比的情况和数据集，请注意与模型最相关的事件或特征。

**3。** **训练好的模型在现实世界中的有限应用:**

训练样本的有限真实世界安装，以确定由模型的学习和成长结果给出的动作电位和真实结果之间是否存在显著差异，这是另一种可靠但可能昂贵的泄漏检查。

这种对模型有效地推广到新数据的检查是有用的，但它可能无法提供任何关于是否发生泄漏的快速洞察，或者任何性能降低是否是由于其他因素(如经典的过拟合)造成的。

# **如何最大限度减少数据泄露？**

您可以使用一些最佳实践来帮助限制任何应用程序中的数据泄漏风险。

一个重要的指导方针是确保每个交叉验证扫描的任何数据预处理都是单独进行的。换句话说，您为标准化或均衡化要素计算的任何统计数据或变量都应基于交叉验证分割中提供的数据，而不是整个数据集。确保在匹配的保留测试折叠上应用相同的参数。

如果您正在处理时间序列数据，请注意与分析特定数据事件(如用户点击网页)相关的时间戳，并确保用于计算此实例特征的任何数据不包含时间戳晚于截止值的记录。这将确保未来的数据不会包含在现有的要素计算或训练数据中。

如果您已经有了足够的数据，请考虑在使用新数据集之前创建一个单独的测试集，最后才评估您的最终概念和这个测试数据集。其目就像现实世界中的部署，因为您希望确保您的训练模型能够有效地推广到新数据。如果性能没有大幅下降，那就太棒了。如果有，泄漏，以及典型的嫌疑，如经典的过度拟合，可能是一个促成的原因。

最后，我们可以得出结论，以下几点有助于打击数据泄漏。

通过在感兴趣的事件发生之前删除所有数据，并专注于你正在了解的观察或事实

在输入数据中加入随机噪声，以消除可能泄漏变量的影响。

通过移除泄漏变量和评估简单的基于规则的模型，像检查这些变量是否泄漏，如果是，移除它们。如果你对一个有漏洞的变量有任何疑问，我们可以删除它。

通过使用 pipelines 架构，很容易为数据准备执行不同的步骤序列，这些步骤通常在交叉验证折叠中执行。为此，不同的编程语言提供了不同的包或库，比如 R 中的 caret 包和 Python 中的 scikit-learn。

通过使用维持数据集，我们可以在使用验证数据集之前对其进行最终的稳定性检查。

# 摘要

我们在这篇文章中发现了以下内容:

什么是机器学习中的数据泄露？

数据泄露的不同真实例子

如何检测数据泄露？

如何最大限度减少数据泄露？