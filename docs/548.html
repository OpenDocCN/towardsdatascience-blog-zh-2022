<html>
<head>
<title>How to Choose the Right Activation Function for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为神经网络选择合适的激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#2022-01-19">https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#2022-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="6688" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">如何为神经网络选择合适的激活函数</h1></div><div class=""><h2 id="ad4c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用视觉表征分析不同类型的激活函数——神经网络和深度学习课程:第五部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/17d30b5b489d2637d44fa1fd3cf4c128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vwhp6Ia8BDT_fdGN7iJlUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供，使用draw.io和matplotlib制作</p></figure><h1 id="4a16" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="1e8f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在我们的<a class="ae mm" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener"> <strong class="ls iu">神经网络和深度学习课程</strong> </a> <strong class="ls iu"> </strong>的<a class="ae mm" rel="noopener" target="_blank" href="/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc">第1部分</a>中，正如这里介绍的<a class="ae mm" href="https://rukshanpramoditha.medium.com/introducing-my-neural-networks-and-deep-learning-course-everyone-can-understand-6335fd541c9e" rel="noopener"/>，我们已经讨论了在神经网络模型中使用激活函数的主要目的。</p><p id="01af" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在隐藏层和输出层的每个节点，激活函数被应用于称为<strong class="ls iu"> z </strong>的输入的加权和(这里输入可以是原始数据或前一层的输出)。</p><p id="4764" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">今天，我们将讨论神经网络中使用的以下不同类型的激活函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e7165eec635aa4889ab78603e798e0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*QU2y327exe_euRCofyETwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">不同类型的激活功能</strong>(图片由作者提供，用draw.io制作)</p></figure><p id="7df8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">可视化表示将帮助您理解激活功能的功能定义和不同的使用场景。</p><p id="7937" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">文末:</p><ul class=""><li id="0f25" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">你会对什么时候使用哪个激活功能有一个清晰的认识。</li><li id="d019" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">你会明白不同激活功能的定义。</li></ul><h1 id="05d6" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">神经网络中不同层的激活函数</h1><p id="74de" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">神经网络通常由三种类型的层组成:输入层、隐藏层和输出层。</p><p id="d83c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">输入层仅保存输入数据，不执行任何计算。因此，这里没有使用激活函数。</p><p id="7ec8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们必须在神经网络的隐藏层中使用非线性激活函数。这是因为我们需要在网络中引入非线性来学习复杂的模式。如果没有非线性激活函数，具有许多隐藏层的神经网络将成为一个巨大的线性回归模型，对于从现实世界的数据中学习复杂的模式是无用的。神经网络模型的性能将根据我们在隐藏层内部使用的激活函数的类型而显著变化。</p><p id="243d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们还必须在神经网络的输出层内部使用激活函数。激活函数的选择取决于我们想要解决的问题的类型。</p><h1 id="a46a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">线性与非线性函数</h1><p id="b866" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">大多数激活函数是非线性的。然而，我们也在神经网络中使用线性激活函数。例如，我们在解决回归问题的神经网络模型的输出层中使用线性激活函数。一些激活函数由两个或三个线性分量组成。这些函数也被归类为非线性函数。</p><p id="1f04" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">区分线性和非线性函数将是有用的。一个线性函数(称为<strong class="ls iu"> f </strong>)接受输入<strong class="ls iu"> z </strong>并返回输出<strong class="ls iu"> cz </strong>，它是输入与常数<strong class="ls iu"> c </strong>的乘积。数学上，这可以表示为<strong class="ls iu"> f(z) = cz </strong>。当c=1时，函数按原样返回输入，不对输入进行任何更改。线性函数的图形是一条<em class="ni">单一直线</em>。</p><p id="597f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">任何非线性函数都可以归类为非线性函数。非线性函数的图形不是一条直线。它可以是一个复杂的模式，也可以是两个或多个线性组件的组合。</p><h1 id="ecaf" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">不同类型的激活功能</h1><p id="0110" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们将讨论神经网络中常用的激活函数。</p><h2 id="b43c" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">1.Sigmoid激活函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/01f82bd0549c5ea10162cf478adb4639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SuwKNT1Y9K9dsD5Dzm5I_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> Sigmoid激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="9574" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特性:</strong></p><ul class=""><li id="2a98" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">这也称为逻辑回归模型中使用的<em class="ni">逻辑函数</em>。</li><li id="3ff0" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">sigmoid函数有一个<strong class="ls iu"> s形的</strong>图形。</li><li id="00a6" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">显然，这是一个非线性函数。</li><li id="0f24" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">sigmoid函数将其输入转换为0到1之间的概率值。</li><li id="727c" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">它将较大的负值转换为0，将较大的正值转换为1。</li><li id="8d0e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">对于输入0，它返回0.5。值0.5被称为阈值，其可以决定给定的输入属于哪两类。</li></ul><p id="70ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="ecf5" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">在早期，sigmoid函数被用作MLPs、CNN和RNNs中隐藏层的激活函数。</li><li id="9349" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">然而，在RNNs中仍然使用sigmoid函数。</li><li id="189b" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">目前，我们通常不将sigmoid函数用于MLPs和CNN中的隐藏层。相反，我们在那里使用ReLU或Leaky ReLU。</li><li id="2f32" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">当我们构建二元分类器时，必须在输出层使用sigmoid函数，在该分类器中，根据函数返回的输入的概率值，将输出解释为类标签。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a66ee402a71569889174a3aca1304dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*FtnJJ51HSSvKTx7h3hwfww.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">使用sigmoid函数的二元分类</strong>(图片由作者提供，用draw.io制作)</p></figure><ul class=""><li id="7e8d" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">当我们构建多标签分类模型时，使用sigmoid函数，在该模型中，每个相互包含的类有两个结果。不要将这与多类分类模型相混淆。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/ac8bf7ffa70aad9f0818fded8c14e7bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRpj8ta5fL8V_eUYQLGzCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">【sigmoid函数多标签分类(图片由作者提供，用draw.io制作)</p></figure><p id="1747" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">弊端:</strong></p><p id="e99f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于以下缺点，我们通常不在隐藏层中使用sigmoid函数。</p><ul class=""><li id="ebcf" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">sigmoid函数存在梯度消失的问题。这也被称为梯度的饱和。</li><li id="b72d" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">sigmoid函数收敛较慢。</li><li id="4c15" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">它的输出不是以零为中心的。因此，这使得优化过程更加困难。</li><li id="a5c2" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">由于包含了<strong class="ls iu"> e^z </strong>项，该函数的计算量很大。</li></ul><h2 id="81bd" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">2.Tanh激活函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dbc83d72ff7808e20fbffc5328d156c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*qWEluqh-LVWC0Mt8vVNn_Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> Tanh激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="6aca" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">关键特性:</strong></p><ul class=""><li id="af19" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">双曲正切函数的输出范围总是在-1和+1之间。</li><li id="36cb" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">像sigmoid函数一样，它有一个s形图形。这也是一个非线性函数。</li><li id="5894" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">使用双曲正切函数优于sigmoid函数的一个优点是双曲正切函数以零为中心。这使得优化过程更加容易。</li><li id="1a71" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">双曲正切函数比sigmoid函数具有更陡的梯度。</li></ul><p id="9905" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="c341" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">直到最近，tanh函数被用作MLPs、CNN和RNNs中隐藏层的激活函数。</li><li id="e9fe" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">然而，在RNNs中仍然使用tanh函数。</li><li id="7d7b" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">目前，我们通常不为MLPs和CNN中的隐藏层使用tanh函数。相反，我们在那里使用ReLU或Leaky ReLU。</li><li id="4a3e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们从不在输出层使用双曲正切函数。</li></ul><p id="803f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">弊端:</strong></p><p id="a87f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于以下缺点，我们通常不在隐藏层中使用双曲正切函数。</p><ul class=""><li id="6228" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">双曲正切函数有消失梯度问题。</li><li id="0bff" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">由于包含了<strong class="ls iu"> e^z </strong>项，该函数的计算量很大。</li></ul><h2 id="b34f" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">3.ReLU激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/cd15de5f7005c53db569d81fe77ea7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*rsyrrFMTwULMw32bjW7JgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> ReLU激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="872f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特性:</strong></p><ul class=""><li id="83c3" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">ReLU(<strong class="ls iu">Re</strong>ctived<strong class="ls iu">L</strong>linear<strong class="ls iu">U</strong>nit)激活函数是sigmoid和tanh激活函数的一个很好的替代品。</li><li id="09a8" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">发明ReLU是深度学习取得的最重要突破之一。</li><li id="4e8f" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这个函数没有消失梯度的问题。</li><li id="231e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这个函数的计算开销很小。认为ReLU的收敛速度比sigmoid和tanh函数快6倍。</li><li id="08a5" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">如果输入值为0或大于0，ReLU函数按原样输出输入。如果输入小于0，ReLU函数输出值0。</li><li id="e982" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">ReLU函数由两个线性分量组成。因此，ReLU函数是分段线性函数。实际上，ReLU函数是一个非线性函数。</li><li id="62ad" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">ReLU函数的输出范围可以从0到正无穷大。</li><li id="66e2" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">收敛速度比sigmoid和tanh函数快。这是因为ReLU函数对于一个线性分量具有固定导数(斜率),而对于另一个线性分量具有零导数。因此，使用ReLU功能，学习过程要快得多。</li><li id="5820" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">使用ReLU可以更快地执行计算，因为函数中不包含指数项。</li></ul><p id="bcea" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="30ea" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">ReLU函数是现代MLP和CNN神经网络模型中隐藏层的默认激活函数。</li><li id="d6b4" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们通常不在RNN模型的隐藏层中使用ReLU函数。相反，我们使用sigmoid或tanh函数。</li><li id="37e6" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们从不在输出层使用ReLU函数。</li></ul><p id="bb72" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">缺点:</strong></p><ul class=""><li id="726c" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">使用ReLU函数的主要缺点是它有一个垂死的ReLU问题。</li><li id="b71e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">积极面的价值可以非常高。这可能导致训练期间的计算问题。</li></ul><h2 id="a1c1" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">4.泄漏ReLU激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/40af29bef6d35d2ba1325e7329066e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*R15CMX7M8Fx_dlAuhZUlWA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> Leaky ReLU激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="a21f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特点:</strong></p><ul class=""><li id="c556" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">泄漏ReLU激活函数是默认ReLU函数的修改版本。</li><li id="cb2e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">和ReLU激活函数一样，这个函数没有消失渐变的问题。</li><li id="3e63" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">如果输入值是大于0的0，leaky ReLU函数就像默认ReLU函数一样输出输入。但如果输入小于0，leaky ReLU函数输出一个由<strong class="ls iu"> αz </strong>定义的小负值(其中<strong class="ls iu"> α </strong>为小常数值，通常为0.01，<strong class="ls iu"> z </strong>为输入值)。</li><li id="6dd8" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">它没有任何导数为零的线性分量(斜率)。因此可以避免将死的ReLU问题。</li><li id="d3b0" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">使用泄漏ReLU的学习过程比默认ReLU更快。</li></ul><p id="57d0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="4d7e" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">ReLU函数的相同用法也适用于泄漏ReLU函数。</li></ul><h2 id="50a8" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">5.参数ReLU (PReLU)激活功能</h2><p id="ea2d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">主要特性:</strong></p><ul class=""><li id="5dea" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">这是ReLU函数的另一个变体。</li><li id="1b12" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这几乎类似于泄漏的ReLU函数。唯一的区别是值<strong class="ls iu"> α </strong>变成了一个可学习的参数(因此得名)。我们将<strong class="ls iu"> α </strong>设为网络中每个神经元的参数。所以<strong class="ls iu"> α </strong>的最优值是从网络中学习的。</li></ul><h2 id="6a5c" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">6.ReLU6激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5ebb8da797f5c7eaf3996c1ecf8fe47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*P5EB-6rNeucah1F5_8a3KQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> ReLU6激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="135e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特点:</strong></p><ul class=""><li id="ad9e" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">ReLU和ReLU6之间的主要区别在于，ReLU允许在正侧有非常高的值，而ReLU6在正侧限制为值6。任何大于或等于6的输入值都将被限制为值6(因此得名)。</li><li id="958e" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">ReLU6函数由三个线性分量组成。这是一个非线性函数。</li></ul><h2 id="bd87" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">7.Softmax激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dba192d698b2040173c4138acf8c0b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*WWTFPwIgU_iV_Q1l_Co-Uw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> Softmax激活功能</strong>(图片由作者提供，用latex编辑器制作)</p></figure><p id="922a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特点:</strong></p><ul class=""><li id="95c4" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">这也是一个非线性激活函数。</li><li id="a8cd" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">softmax函数计算一个事件(类)在<strong class="ls iu"> K </strong>个不同事件(类)上的概率值。它计算每个类的概率值。所有概率的总和是1，意味着所有事件(类)是互斥的。</li></ul><p id="f970" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="c524" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">我们必须在多类分类问题的输出层使用softmax函数。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/8fe765f4334e8c48797ec4093b4e80cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*He7Di4m3YWr1AqFxV9PiwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">使用softmax函数进行多类分类</strong>(图片由作者提供，使用draw.io制作)</p></figure><ul class=""><li id="3eb4" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">我们从不在隐藏层中使用softmax函数。</li></ul><h2 id="29f7" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">8.二元阶跃激活函数</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/28891143c88415ed8091ea3ef1dd9071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*kLXCnlfmGCRrLbZ_aSV6rg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">二进制步进激活函数</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="ea1a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">关键特性:</strong></p><ul class=""><li id="e99a" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">该功能也被称为<em class="ni">阈值激活功能</em>。我们可以为阈值设置任何值，这里我们指定值为0。</li><li id="ba33" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">如果输入大于阈值，该函数输出值1。如果输入等于或小于阈值，此函数输出值0。</li><li id="fac0" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这会输出一个二进制值，0或1。</li><li id="077a" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">二元阶跃函数由两个线性分量组成。正因为如此，这个函数是一个分段线性函数。实际上，二元阶跃函数是一个非线性函数。</li><li id="88d2" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这个函数不是光滑函数。</li></ul><p id="1cb2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="2d94" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">实际上，我们通常不在现代神经网络模型中使用这个函数。</li><li id="decd" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">然而，我们可以用这个函数来解释理论概念，如“激发一个神经元”，“感知器的内部工作原理”。因此，阶跃函数在理论上是重要的。</li></ul><h2 id="a7dd" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">9.身份激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/21a8321be2c72e3ecaea6560afb86e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*h_-X4Ggt_YEWD58ha6x23w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">身份激活功能</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="95b9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特性:</strong></p><ul class=""><li id="60b0" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">这也被称为<em class="ni">线性激活函数</em>。</li><li id="70d6" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">当我们谈论激活函数时，这是唯一被认为是线性函数的函数。</li><li id="03b1" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">该函数按原样输出输入值。不会对输入进行任何更改。</li></ul><p id="b45d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="da39" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">此函数仅用于解决回归问题的神经网络模型的输出层。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7bdf8870f368caa88367a23159b8e071.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*EQiiLZlCnB4x5e8rJwenjw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">带恒等函数的回归</strong>(图片由作者提供，用draw.io制作)</p></figure><ul class=""><li id="3dbf" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">我们从不在隐藏层中使用标识函数。</li></ul><h2 id="5c56" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">10.Swish激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c7565aed2c7023d0b92a0bb5b4f99116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*9Fwkav2gO5kvx_OazeSx-g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt"> Swish激活功能</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="e686" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特性:</strong></p><ul class=""><li id="c0ff" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">该函数由sigmoid函数乘以输入<strong class="ls iu"> z </strong>构成。</li><li id="2db6" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这是一个非线性函数。</li><li id="0399" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">该图非常类似于ReLU激活函数的图。</li><li id="0e4b" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">曲线比ReLU激活函数更平滑。这种平滑度在训练模型时非常重要。函数在训练时容易收敛。</li></ul><p id="b508" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="ea1c" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">该功能仅用于隐藏层。</li><li id="7c41" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们从不在神经网络模型的输出层使用这个函数。</li></ul><p id="7c0b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">缺点:</strong></p><ul class=""><li id="089e" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">Swish函数的主要缺点是计算量大，因为函数中包含了<strong class="ls iu"> e^z </strong>项。这可以通过使用下面定义的称为“硬刷”的特殊功能来避免。</li></ul><h2 id="d866" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">11.硬嗖嗖(H-Swish)激活功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c47a07cde311fa6c1bb8397809b389a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*vLyRo4eQ_evf2VslEwGttg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mt">硬Swish (H-Swish)激活功能</strong>(图片由作者提供，用latex编辑器和matplotlib制作)</p></figure><p id="641e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">主要特点:</strong></p><ul class=""><li id="6d2f" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">该图与Swish函数的图相同。</li><li id="c615" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">这在计算上是廉价的，因为sigmoid函数被线性模拟代替。</li></ul><p id="26c5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu">用法:</strong></p><ul class=""><li id="526e" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">硬Swish的用法类似于Swish激活功能的用法。</li></ul><h1 id="a5e9" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><p id="73b3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">激活函数只是数学函数。激活函数应该具有的主要特征是该函数应该是可微分的，因为这是模型训练中反向传播的要求。</p><p id="0778" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">选择正确的激活函数是主要的挑战，它可以被认为是一种超参数调整，其中程序员通过理解问题定义并考虑模型的性能和损失函数的收敛性来手动选择激活函数。</p><h2 id="e2bb" class="nj kz it bd la nk nl dn le nm nn dp li lz no np lk md nq nr lm mh ns nt lo nu bi translated">选择正确激活功能的一般准则</h2><p id="db55" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这里总结了上面讨论的不同激活功能的使用场景。当您训练自己的神经网络模型时，您可能会发现这很有用。</p><ul class=""><li id="2053" class="mu mv it ls b lt mn lw mo lz mw md mx mh my ml mz na nb nc bi translated">在神经网络的输入层节点中不需要激活函数。因此，在定义输入层时，您不需要担心激活函数。</li><li id="d5b3" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">输出层激活函数取决于我们想要解决的问题的类型。在回归问题中，我们使用一个节点的线性(恒等)激活函数。在二元分类器中，我们使用具有一个节点的sigmoid激活函数。在多类分类问题中，我们使用softmax激活函数，每个类一个节点。在多标签分类问题中，我们使用sigmoid激活函数，每个类一个节点。</li><li id="a6f7" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们应该在隐藏层中使用非线性激活函数。通过考虑模型的性能或损失函数的收敛性来做出选择。从ReLU激活功能开始，如果你有一个垂死的ReLU问题，试试leaky ReLU。</li><li id="ace2" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">在MLP和CNN神经网络模型中，ReLU是隐藏层的默认激活函数。</li><li id="64cf" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">在RNN神经网络模型中，我们对隐藏层使用sigmoid或tanh函数。双曲正切函数具有更好的性能。</li><li id="b3a7" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">只有身份激活函数被认为是线性的。所有其他激活函数都是非线性的。</li><li id="9a1b" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们从不在隐藏层中使用softmax和identity函数。</li><li id="968b" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">我们只在隐藏层中使用tanh、ReLU、ReLU的变体、swish和hard swish函数。</li><li id="085d" class="mu mv it ls b lt nd lw ne lz nf md ng mh nh ml mz na nb nc bi translated">最近的研究发现了swish和hard swish函数。</li></ul></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="6bed" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">今天的帖子到此结束。感谢阅读！</p><p id="cc89" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">通过<a class="ae mm" href="https://rukshanpramoditha.medium.com/subscribe" rel="noopener"> <strong class="ls iu">订阅我的</strong> <strong class="ls iu">邮箱列表</strong> </a>，再也不会错过一个精彩的故事。我一点击发布按钮，你就会在收件箱里收到每一篇文章。</p><p id="83f1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果你愿意，你可以<a class="ae mm" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="ls iu">注册成为会员</strong> </a>来获得我写的每一个故事的全部信息，我将会收到你的一部分会员费。</p><p id="8458" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">一如既往，祝大家学习愉快！</p><p id="5df7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><a class="ol om ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----3941ff0e6f9c--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="ls iu">2022–01–19</strong></p></div></div>    
</body>
</html>