<html>
<head>
<title>Boruta SHAP: A Tool for Feature Selection Every Data Scientist Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">博鲁塔·SHAP:每个数据科学家都应该知道的特征选择工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boruta-shap-an-amazing-tool-for-feature-selection-every-data-scientist-should-know-33a5f01285c0#2022-01-25">https://towardsdatascience.com/boruta-shap-an-amazing-tool-for-feature-selection-every-data-scientist-should-know-33a5f01285c0#2022-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="c805" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated"><em class="jn">博鲁塔·SHAP:每个数据科学家都应该知道的特征选择工具</em></h1></div><div class=""><h2 id="db36" class="pw-subtitle-paragraph jo ip iq bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">我们如何使用博鲁塔和SHAP构建一个惊人的特征选择过程——以python为例</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/cacc85b02eda49e67c5569d56047caea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uC8LjkEpbQfJyVfjHYDmTg.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">原文由<a class="ae kw" href="https://unsplash.com/@noahdavis" rel="noopener ugc nofollow" target="_blank">诺亚纳夫</a>于<a class="ae kw" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="c09d" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当建立机器学习模型时，我们知道拥有太多的功能会带来一些问题，如<a class="ae kw" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-50dc6e49aa1e">维数灾难</a>，此外还需要更多的内存、处理时间和功率。</p><p id="f963" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我们的特征工程管道中，我们采用特征选择技术来尝试从数据集中移除不太有用的特征。这就提出了一个问题:我们如何确定哪些特性是有用的？</p><p id="a95b" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于这个任务，我们可以使用Boruta，一个基于统计方法的特征选择算法。它依赖于两个原则:阴影特征和二项式分布。</p><h1 id="0358" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">1.阴影特征</h1><p id="21dc" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Boruta算法的第一步是评估特征的重要性。这通常是在基于树的算法中完成的，但在Boruta上，这些特征之间并不竞争，它们与被称为“阴影特征”的随机版本竞争。</p><p id="81c2" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设我们有一个包含3个要素和100个观测值的数据集。在这种情况下，我们制作了数据集的副本，并混洗每个要素列。置换的特征然后被称为“阴影特征”(顺便说一下，很酷的名字)，并创建一个新的数据集，Boruta数据集，连接所有3个原始和3个新的阴影特征。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mq"><img src="../Images/1d5d8f8777153d1683a14627f5915e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3qtCi-VAU4KT4QGRqCfPw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><p id="12da" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，我们使用任何偏好方法，如随机森林或任何其他方法，来评估所有6个特征的特征重要性。Boruta算法的思想是选择比纯粹的随机性表现更好的特征，这里由阴影特征表示，因此我们将原始特征的重要性与阴影特征的<em class="mr">最高</em>特征重要性进行比较。每当一个特征的重要性高于这个阈值时，我们就称之为“命中”。</p><p id="0894" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们现在可以只保留命中的，而丢弃其他的，对吗？但是如果他们中的一些因为运气不好而被丢弃了呢？答案在于迭代，这是下一步。</p><h1 id="0885" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">2.二项式分布</h1><p id="28a4" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">所有特征只有两种结果:“命中”或“未命中”，因此我们可以多次执行前面的步骤，并根据特征构建二项式分布。</p><p id="aab7" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">考虑一个具有三个特征的电影数据集:“流派”、“观众_评分”和“评论家_评分”。经过20次迭代，我们可以得到以下结果:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8160fad38f61393571bda9348b6e50d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Er2g2Pj_HPdbM-hSMyGkcw.png"/></div></figure><p id="dc6e" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以将这些结果放在二项式分布图上:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mt"><img src="../Images/c2ffdb6c4fc52d79b289a9e7d7afca19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LwJWlPGxpUvzqVs-mAjgg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><p id="ba3f" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">分布的尾部是最重要的部分。在这个例子中，各占0.5%的概率。</p><p id="9aa8" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe mu mv mw mx b">genre</code>变量落在红色区域，即“拒绝”区域。在这里，我们确信这些特性对目标变量没有影响。</p><p id="5d5d" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">绿色区域是“验收”区域。我们也确信这些特征是可预测的，并且应该保留在模型中。在这个例子中，<code class="fe mu mv mw mx b">critic_score</code>是一个应该保留的好特性。</p><p id="d4be" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在蓝色区域，Boruta不确定该特征是否是预测性的。在这种情况下，我们可以保留这些特征，甚至可以使用其他方法来查看它们是否会对模型预测产生任何影响。上例中，<code class="fe mu mv mw mx b">audience__score</code>在这个区域上，尽管它靠近绿尾。</p><blockquote class="my mz na"><p id="1914" class="kx ky mr kz b la lb js lc ld le jv lf nb lh li lj nc ll lm ln nd lp lq lr ls ij bi translated">我们保留在绿色和蓝色区域分类的特征，并丢弃红色区域的特征。</p></blockquote><p id="abda" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你可以点击查看博鲁塔算法<a class="ae kw" rel="noopener" target="_blank" href="/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a">的精彩解释。</a></p><h1 id="1031" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">3.Python中的Boruta</h1><p id="de17" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">示例代码也可以在我的github上找到，所以可以跳过这一节。</p><div class="ne nf gp gr ng nh"><a href="https://github.com/vinyluis/Articles/tree/main/Boruta%20SHAP" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">文章/博鲁塔SHAP在主要乙烯/文章</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">第1篇:用SHAP价值观解释你的机器学习模型是如何工作的【恩】什么是SHAP价值观，它们是如何…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv kq nh"/></div></div></a></div><p id="9896" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">要使用Boruta，我们可以使用BorutaPy库[1]:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="0a9f" class="oa lu iq mx b gy ob oc l od oe">pip install boruta</span></pre><p id="287c" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后我们可以导入糖尿病数据集(可从<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html" rel="noopener ugc nofollow" target="_blank">sci kit-Learn</a>【2】获得):</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="bcc2" class="oa lu iq mx b gy ob oc l od oe">from sklearn.datasets import load_diabetes<br/>from sklearn.model_selection import train_test_split</span><span id="b19c" class="oa lu iq mx b gy of oc l od oe"># Fetches the data<br/>dataset = load_diabetes(as_frame = True)<br/># Gets the independent variables<br/>X = dataset['data']<br/># Gets the dependent variable (the target)<br/>y = dataset['target']<br/># Splits the dataset<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)</span></pre><p id="e1c4" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了使用Boruta，我们需要定义一个估计量，它将用于估计特征的重要性。在这种情况下，我选择了RandomForestRegressor:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="8901" class="oa lu iq mx b gy ob oc l od oe">from sklearn.ensemble import RandomForestRegressor</span><span id="90c3" class="oa lu iq mx b gy of oc l od oe"># Defines the estimator used by the Boruta algorithm<br/>estimator = RandomForestRegressor()</span></pre><p id="d878" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，我们可以创建BorutaPy对象，并使用估计器使其适合数据:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="a368" class="oa lu iq mx b gy ob oc l od oe">from boruta import BorutaPy</span><span id="705b" class="oa lu iq mx b gy of oc l od oe"># Creates the BorutaPy object<br/>boruta = BorutaPy(estimator = estimator, n_estimators = 'auto', max_iter = 100)<br/># Fits Boruta<br/>boruta.fit(np.array(X_train), np.array(y_train))</span></pre><p id="2ab9" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们可以发现哪些特性是重要的，哪些是不重要的，哪些是不确定的:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="dc25" class="oa lu iq mx b gy ob oc l od oe"># Important features<br/>important = list(X.columns[boruta.support_])<br/>print(f"Features confirmed as important: {important}")</span><span id="9b66" class="oa lu iq mx b gy of oc l od oe"># Tentative features<br/>tentative = list(X.columns[boruta.support_weak_])<br/>print(f"Unconfirmed features (tentative): {tentative}")</span><span id="47c9" class="oa lu iq mx b gy of oc l od oe"># Unimportant features<br/>unimportant = list(X.columns[~(boruta.support_ | boruta.support_weak_)])<br/>print(f"Features confirmed as unimportant: {unimportant}")</span></pre><p id="b341" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">输出是:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="c566" class="oa lu iq mx b gy ob oc l od oe">Features confirmed as important: ['bmi', 'bp', 's5', 's6']<br/>Unconfirmed features (tentative): []<br/>Features confirmed as unimportant: ['age', 'sex', 's1', 's2', 's3', 's4']</span></pre><h1 id="2429" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">4.博鲁塔SHAP特征选择</h1><p id="c65a" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Boruta是一种健壮的特征选择方法，但它强烈依赖于特征重要性的计算，这可能会有偏差或对数据不够好。</p><p id="fc01" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这就是SHAP加入团队的地方。通过使用SHAP值作为特征选择方法，我们得到了博鲁塔SHAP特征选择算法。通过这种方法，我们可以得到SHAP方法中存在的强可加性特征解释，同时具有博鲁塔算法的鲁棒性，以确保只有重要变量保留在集合中。</p><p id="7d1a" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果你不知道SHAP是什么，看看我解释它的文章:</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">用SHAP值来解释你的机器学习模型是如何工作的</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">学习使用工具来显示每个特征如何影响模型的每个预测</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="og l ns nt nu nq nv kq nh"/></div></div></a></div><h1 id="3f7d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">5.蟒蛇皮博鲁塔·SHAP</h1><p id="f7bb" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">要使用Boruta，我们可以使用BorutaShap库[4]:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="06a4" class="oa lu iq mx b gy ob oc l od oe">pip install BorutaShap</span></pre><p id="a7b3" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">首先我们需要创建一个BorutaShap对象。因为我们想使用SHAP作为特征重要性鉴别器，所以<code class="fe mu mv mw mx b">importance_measure</code>的默认值是“shap”。当问题是分类问题时，我们可以将<code class="fe mu mv mw mx b">classification</code>参数更改为True。</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="dfa0" class="oa lu iq mx b gy ob oc l od oe">from BorutaShap import BorutaShap</span><span id="aeeb" class="oa lu iq mx b gy of oc l od oe"># Creates a BorutaShap selector for regression<br/>selector = BorutaShap(importance_measure = 'shap', classification = False)</span></pre><p id="8dd6" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然后，我们在数据或数据样本中拟合BorutaShap选择器。<code class="fe mu mv mw mx b">n_trials</code>参数定义了Boruta算法的迭代次数，而<code class="fe mu mv mw mx b">sample</code>布尔值决定了该方法是否会在内部对数据进行采样以加速该过程。</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="ce24" class="oa lu iq mx b gy ob oc l od oe"># Fits the selector<br/>selector.fit(X = X_train, y = y_train, n_trials = 100, sample = False, verbose = True)<br/># n_trials -&gt; number of iterations for Boruta algorithm<br/># sample -&gt; samples the data so it goes faster</span></pre><p id="25c5" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">拟合后，将显示以下结果:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="0983" class="oa lu iq mx b gy ob oc l od oe">4 attributes confirmed important: ['s5', 'bp', 'bmi', 's6']<br/>5 attributes confirmed unimportant: ['s2', 's4', 's3', 'age', 'sex']<br/>1 tentative attributes remains: ['s1']</span></pre><p id="7293" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们可以看到哪些功能将被删除，并将其从我们的数据中删除:</p><pre class="kh ki kj kk gt nw mx nx ny aw nz bi"><span id="ecef" class="oa lu iq mx b gy ob oc l od oe"># Display features to be removed<br/>features_to_remove = selector.features_to_remove<br/>print(features_to_remove)</span><span id="4ce7" class="oa lu iq mx b gy of oc l od oe"># Removes them<br/>X_train_boruta_shap = X_train.drop(columns = features_to_remove)<br/>X_test_boruta_shap = X_test.drop(columns = features_to_remove)</span></pre><h1 id="9bb2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">6.结论</h1><p id="7672" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">与特征选择对我们的ML管道同样重要的是，我们需要使用最好的算法来确保最好的结果。</p><p id="66a5" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这种方法的缺点是评估时间，对于许多Boruta迭代，或者当SHAP适合许多观测值时，评估时间可能太长。当心时间！</p><p id="90be" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">考虑到这一点，博鲁塔that是我们可以用来选择机器学习管道上最重要特征的最佳方法之一。</p><blockquote class="my mz na"><p id="3740" class="kx ky mr kz b la lb js lc ld le jv lf nb lh li lj nc ll lm ln nd lp lq lr ls ij bi translated">始终使用它，但要记得将结果与其他方法进行比较，以确保更高的可靠性。</p></blockquote><h1 id="9a43" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated">如果你喜欢这个帖子…</h1><p id="823e" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">支持我一杯咖啡！</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><a href="https://www.buymeacoffee.com/vinitrevisan"><div class="gh gi oh"><img src="../Images/acf4154cfebdc13859934db49fd502cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*h_y4o6IwDDFFWIyKQE7Rww.png"/></div></a><p class="ks kt gj gh gi ku kv bd b be z dk translated">给我买杯咖啡！</p></figure><p id="cf7c" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">看看这个很棒的帖子</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/using-shap-values-to-explain-how-your-machine-learning-model-works-732b3f40e137"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ir gy z fp nm fr fs nn fu fw ip bi translated">用SHAP值来解释你的机器学习模型是如何工作的</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">学习使用工具来显示每个特征如何影响模型的每个预测</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="og l ns nt nu nq nv kq nh"/></div></div></a></div><h1 id="71ac" class="lt lu iq bd lv lw lx ly lz ma mb mc md jx me jy mf ka mg kb mh kd mi ke mj mk bi translated"><strong class="ak">参考文献</strong></h1><p id="c5a7" class="pw-post-body-paragraph kx ky iq kz b la ml js lc ld mm jv lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">[1 ]特别套装:<a class="ae kw" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">https://github.com/scikit-learn-contrib/boruta_py</a></p><p id="ee9e" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[2] Scikit-Learn糖尿病数据集:<a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/generated/sk Learn . datasets . load _ Diabetes . html</a></p><p id="47ac" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">【3】https://shap.readthedocs.io/en/latest/index.html<a class="ae kw" href="https://shap.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">SHAP包</a></p><p id="907d" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[4]博鲁塔沙普包:<a class="ae kw" href="https://github.com/Ekeany/Boruta-Shap" rel="noopener ugc nofollow" target="_blank">https://github.com/Ekeany/Boruta-Shap</a></p><p id="9437" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[5]<a class="ae kw" href="https://medium.com/analytics-vidhya/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677" rel="noopener">https://medium . com/analytics-vid hya/is-this-the-best-feature-selection-algorithm-borutashap-8bc 238 aa 1677</a></p><p id="19b3" class="pw-post-body-paragraph kx ky iq kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[6]<a class="ae kw" rel="noopener" target="_blank" href="/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a">https://towardsdatascience . com/boruta-explained-the-way-I-wish-someone-explained-it-to-me-4489d 70 e 154 a</a></p></div></div>    
</body>
</html>