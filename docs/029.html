<html>
<head>
<title>JSON in Databricks and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Databricks和PySpark中的JSON</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9#2022-01-03">https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9#2022-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="de11" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Databricks和PySpark中的JSON</h1></div><div class=""><h2 id="67b9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用PySpark在数据块中处理JSON数据的技巧和诀窍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9545cebea9129e47ea3ad08ae1003fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CBDdyqVS6ZMuCyxl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@fatosi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法托斯Bytyqi </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e43d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在简单的情况下，JSON很容易在Databricks中处理。您可以将JSON对象文件直接读入DataFrame或表中，Databricks知道如何将JSON解析成单独的字段。但是，正如大多数与软件相关的事情一样，也存在一些问题和变化。本文展示了如何处理最常见的情况，并包括详细的编码示例。</p><p id="1e28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的用例是已经被转换成JSON的HL7医疗保健数据，但是这里的方法适用于任何JSON数据。考虑的三种格式是:</p><ul class=""><li id="57c8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">包含完整JSON对象的文本文件，每行一个。当您将JSON文件加载到Databricks表时，这是很典型的。</li><li id="088a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">包含各种数据字段(列)的文本文件，其中一个是JSON对象。这在计算机日志中很常见，其中有一些纯文本元数据，后面是JSON字符串中的更多细节。</li><li id="391d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">上述内容的变体，其中JSON字段是一个对象数组。</li></ul><p id="24f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将每种类型的输入放入数据块需要不同的技术。</p><p id="e973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设…</p><ul class=""><li id="6972" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">你能接触到数据块并知道基本操作。如果你是新手，试试免费的<a class="ae kv" href="https://community.cloud.databricks.com/login.html" rel="noopener ugc nofollow" target="_blank">社区版</a>。</li><li id="0a68" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我从json.org的<a class="ae kv" href="http://json.org/" rel="noopener ugc nofollow" target="_blank">那里拿到JSON规范，用jsonlint.com</a>的<a class="ae kv" href="https://jsonlint.com/" rel="noopener ugc nofollow" target="_blank">测试JSON对象的有效性。</a></li><li id="a6f4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我忽略[1，2，3]和“hello”之类的“裸JSON”。按照规范，这些是完整的、有效的JSON对象，但是我认为它们的形式不好，因为这些字段没有名称，所以很难在下游使用。(Databricks可以将这些视为普通的输入字段，而不关心它们是否是有效的JSON。)</li></ul><p id="9ca1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">包含所有这些代码的完整Databricks笔记本位于<a class="ae kv" href="https://github.com/ChuckConnell/articles/blob/master/json_tricks.dbc" rel="noopener ugc nofollow" target="_blank">https://github . com/chuck connell/articles/blob/master/JSON _ tricks . DBC</a>。复制URL，然后在Databricks中做Workspace / Import / URL。</p><p id="81d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这段代码是在Databricks中开发的，但它应该也可以在安装了PySpark的本机Apache Spark中工作，尽管我没有在那里测试过。</p><h1 id="4ebe" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">标准JSON对象</h1><p id="d8fa" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">标准的JSON文本文件如下所示:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0ff3" class="ni mh iq ne b gy nj nk l nl nm">{ "Text1":"hello", "Text2":"goodbye", "Num1":5, "Array1":[7,8,9] }<br/>{ "Text1":"this", "Text2":"that", "Num1":6.6, "Array1":[77,88,99] }<br/>{ "Text1":"yes", "Text2":"no", "Num1":-0.03, "Array1":[555,444,222] }</span></pre><p id="db7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要将这个文件读入DataFrame，可以使用标准的JSON导入，它从提供的字段名和数据项中推断出模式。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4401" class="ni mh iq ne b gy nj nk l nl nm">test1DF = spark.read.json("/tmp/test1.json")</span></pre><p id="f17f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">得到的DataFrame具有与JSON标记匹配的列，并且数据类型被合理地推断出来。(类型推断并不完美，尤其是对于ints vs floats和boolean。)您现在可以只使用它们的普通名称来读取DataFrame列；所有的JSON语法都不见了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/b7334e29c94c918bae4464f84cdaf7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OiQXFJMKcC87j_1J"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="a47d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">带有JSON字段的文本文件</h1><p id="2c5a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">包含一些常规字段和一个JSON字段的文本文件如下所示:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8038" class="ni mh iq ne b gy nj nk l nl nm">Text1|Text2|Num1|JSON1<br/>hello | goodbye | 5 | {"Sub1":"john", "Sub2":3}<br/>this | that | 6.6 | {"Sub1":"betty", "Sub2":4}<br/>yes | no | -0.03 | {"Sub1":"bobby", "Sub2":5}</span></pre><p id="f46e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一行包含字段名称，这是数据文本文件的标准。我使用竖线来分隔字段，以避免与JSON语法中的逗号混淆。(您可能也想这样做，因为Databricks文本解析器很难处理嵌入的逗号和引号的转义语法。)</p><p id="8c3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要像这样导入文件，使用一个两阶段的过程，首先将JSON字段作为文本读取。注意，我使用“inferSchema ”,因为文件很小；对于大文件，您应该使用。schema(my_schema)哪个更快。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="ad20" class="ni mh iq ne b gy nj nk l nl nm">test2DF = spark.read\<br/>.option("inferSchema", True)\<br/>.option("header", True)\<br/>.option("delimiter", "|")\<br/>.csv("/tmp/test2.txt")</span></pre><p id="1deb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">常规字段现在是正确的，JSON字段是单个文本字符串。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/e03c78b09bf67bfc627346b08a7baeb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dIt2BC4k-WAWUVih"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fdf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要将JSON字符串转换成适当的结构，这样我们就可以访问它的各个部分。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="24b7" class="ni mh iq ne b gy nj nk l nl nm">from pyspark.sql.functions import from_json, col<br/>from pyspark.sql.types import StructType, StructField, StringType, IntegerType</span><span id="9737" class="ni mh iq ne b gy np nk l nl nm"># Define the schema of the JSON string.<br/>schema = StructType([<br/>  StructField("Sub1", StringType()), <br/>  StructField("Sub2", IntegerType()<br/>  )<br/>])</span><span id="614d" class="ni mh iq ne b gy np nk l nl nm"># Use the schema to change the JSON string into a struct, overwriting the JSON string.</span><span id="9e3d" class="ni mh iq ne b gy np nk l nl nm">test2DF = test2DF.withColumn("JSON1", from_json(col("JSON1"), schema))</span><span id="0600" class="ni mh iq ne b gy np nk l nl nm"># Make a separate column from one of the struct fields.</span><span id="5281" class="ni mh iq ne b gy np nk l nl nm">test2DF = test2DF.withColumn("JSON1_Sub2", col("JSON1.Sub2"))</span></pre><p id="4e49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们得到了我们想要的——原来的非JSON字段，作为真正结构的JSON字段，以及提取一个JSON项的示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/25b1a6e0cd2fef1ccba71f752752e319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CVx8aMqp_a9WZ5WT"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="0260" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">带有JSON数组字段的文本文件</h1><p id="f6f8" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">包含JSON对象数组字段的文本文件如下所示:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6a9d" class="ni mh iq ne b gy nj nk l nl nm">Text1|Text2|Num1|JSON1<br/>hello | goodbye | 5 | [{"Sub1":"stop", "Sub2":3}, {"Sub1":"go", "Sub2":6}]<br/>this | that | 6.6 | [{"Sub1":"eggs", "Sub2":4}, {"Sub1":"bacon", "Sub2":8}]<br/>yes | no | -0.03 | [{"Sub1":"apple", "Sub2":5}, {"Sub1":"pear", "Sub2":10}]</span></pre><p id="275c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我假设数组中的每个JSON对象都有相同的结构。当然，这并不是严格要求的，但是对于逻辑上相关并且在单个数组中的对象来说，这是可能的。</p><p id="0ddd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这种类型的JSON输入，以同样的方式开始，将常规字段读入它们的列，并将JSON作为纯文本字段。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="d118" class="ni mh iq ne b gy nj nk l nl nm">test3DF = spark.read\<br/>.option("inferSchema", True)\<br/>.option("header", True)\<br/>.option("delimiter", "|")\<br/>.csv("/tmp/test3.txt")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/bdeea03ef6ef9c92c4305e536469423e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oaAvEUMItQFW0sKC"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9c54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，使用用户定义的函数(UDF)将JSON字符串更改为实际的结构数组。这种编码技巧归功于<a class="ae kv" href="https://kontext.tech/column/spark/284/pyspark-convert-json-string-column-to-array-of-object-structtype-in-data-frame" rel="noopener ugc nofollow" target="_blank">https://kontext . tech/column/spark/284/py spark-convert-JSON-string-column-to-array-of-object-struct type-in-data-frame</a>。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="cd6f" class="ni mh iq ne b gy nj nk l nl nm">from pyspark.sql.functions import col, udf<br/>from pyspark.sql.types import *<br/>import json</span><span id="322a" class="ni mh iq ne b gy np nk l nl nm"># Schema for the array of JSON objects.</span><span id="59fb" class="ni mh iq ne b gy np nk l nl nm">json_array_schema = ArrayType(<br/>  StructType([<br/>    StructField('Sub1', StringType(), nullable=False), <br/>    StructField('Sub2', IntegerType(), nullable=False)<br/>  ])<br/>)</span><span id="8dc6" class="ni mh iq ne b gy np nk l nl nm"># Create function to parse JSON using standard Python json library.</span><span id="acbd" class="ni mh iq ne b gy np nk l nl nm">def parse_json(array_str):<br/>  json_obj = json.loads(array_str)<br/>  for item in json_obj:<br/>    yield (item['Sub1'], item['Sub2'])</span><span id="cd4b" class="ni mh iq ne b gy np nk l nl nm"># Create a UDF, whose return type is the JSON schema defined above.    </span><span id="c0eb" class="ni mh iq ne b gy np nk l nl nm">parse_json_udf = udf(lambda str: parse_json(str), json_array_schema)<br/>  <br/># Use the UDF to change the JSON string into a true array of structs.</span><span id="aac6" class="ni mh iq ne b gy np nk l nl nm">test3DF = test3DF.withColumn("JSON1arr", parse_json_udf((col("JSON1"))))</span><span id="5b99" class="ni mh iq ne b gy np nk l nl nm"># We don't need to JSON text anymore.<br/>test3DF = test3DF.drop("JSON1")</span></pre><p id="657c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结构数组很有用，但是“反规范化”并将每个JSON对象放在自己的行中通常更有帮助。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="ff79" class="ni mh iq ne b gy nj nk l nl nm">from pyspark.sql.functions import col, explode</span><span id="a55d" class="ni mh iq ne b gy np nk l nl nm">test3DF = test3DF.withColumn("JSON1obj", explode(col("JSON1arr")))</span><span id="a2e8" class="ni mh iq ne b gy np nk l nl nm"># The column with the array is now redundant.<br/>test3DF = test3DF.drop("JSON1arr")</span></pre><p id="879f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，非JSON字段现在被复制到多行中，每行一个JSON对象。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/7991f8ef3e839d67eaff55b83f49840a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l1bKHs9zYfL_VVD1"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7467" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以跨所有阵列从一个特定的JSON字段收集数据，这在展开的阵列上要容易得多。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="3a32" class="ni mh iq ne b gy nj nk l nl nm">display (test3DF.select("JSON1obj.Sub1"))</span></pre><h1 id="2737" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="5b0b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">JSON是一种标记文本格式。它是一个可读的文件，包含名称、值、冒号、花括号和各种其他语法元素。另一方面，PySpark数据帧是一种二进制结构，数据可见，元数据(类型、数组、子结构)内置于数据帧中。上面的说明帮助你把第一个翻译成第二个。</p><p id="37c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关更多信息，请参见:</p><ul class=""><li id="938c" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://kb.databricks.com/scala/create-df-from-json-string-python-dictionary.html" rel="noopener ugc nofollow" target="_blank">https://kb . databricks . com/Scala/create-df-from-JSON-string-python-dictionary . html</a></li><li id="c9fd" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://docs.databricks.com/data/data-sources/read-json.html" rel="noopener ugc nofollow" target="_blank">https://docs . databricks . com/data/data-sources/read-JSON . html</a></li></ul></div></div>    
</body>
</html>