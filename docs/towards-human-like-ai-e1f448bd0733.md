# 走向类人人工智能

> 原文：<https://towardsdatascience.com/towards-human-like-ai-e1f448bd0733>

## 试图用类似人类的能力和 GPT 使人工智能更通用

![](img/12f9f99f9ab71eb868e820f9a6d9a0ea.png)

OpenAI 的 DALL-E 2 生成的图像，描绘了一个类似人类的 AI 和一些人类。图片由作者和 DALL-E 2 提供

# 概观

到目前为止，人类是宇宙中发现的唯一具有一般智能的例子。使用人类作为一般智能的存在证据，对于第一代人工一般智能来说，将有点像人类的系统作为目标似乎是一个合理的目标。人工一般智能不一定要像人类一样，但像人类一样的人工一般智能应该是可能的，这似乎是合理的，因为我们是可能的。

现在，似乎有史以来第一次有可能在软件中构建一个代理，展示出貌似真实的人类行为。在从 OpenAI 的 GPT 进展到 DeepMind 的最先进的 T2 强化学习方法之间，我们现在看到了在长期以来被认为是人类特有的领域中类似人类行为的证据。很难说我们第一次观察到了人工智能在长期以来被认为很难(如果不是不可能的话)实现的基准测试中的超人表现。这些领域不仅包括像围棋、星际争霸和 DOTA 2 这样复杂游戏中的表现，还包括像[人类语言](https://openai.com/blog/gpt-3-apps/)、[编程](https://openai.com/blog/openai-codex/)和[艺术](https://openai.com/dall-e-2/)这样的创造性领域。

这些模型中的每一个，虽然在特定领域取得了超人的成果，但它们本身无法实现人类一般能够做到的事情。然而，很明显，通过将这些特定的能力包装在一个更通用、更简单的控制框架中，一个智能体可以实现一些通用的类人行为。在这里，我想分享一个这样的控制框架的一些进展，我相信它显示了新颖的通用性和类似人类的智能体通过持续的反馈和与其环境的交互以在线方式学习的能力。这个框架考虑了一些类似人类的能力，这些能力对于一般的智能代理来说似乎很重要；例如构建和延续一系列思维的能力，从反馈中学习，保持和利用长期记忆，并根据从与人类和其他人工智能代理的互动中获得的知识，发展良知来判断和控制自己的行动。

通过整合使用反馈进行改进的能力，行动的有效性可以随着时间的推移而提高，并且代理的行动与反馈的[一致性](https://openai.com/blog/our-approach-to-alignment-research/)也可以随着时间的推移而提高。这意味着代理应该开始采取与提供反馈的人一致的行动，因为孩子学会采取与从父母和其他人那里收到的反馈一致的行动。通过引入良知，代理人可以学习如何提供自己的反馈，并在无人监督的情况下自我管理自己的调整，就像孩子随着时间的推移学习内化对自己行为的判断和管理一样。通常，[强化学习代理](https://arxiv.org/abs/1911.08265)以可能结果树的形式预测未来可能发生的事情，并使用树搜索算法沿着该树选择下一个最佳行动(通常使用蒙特卡罗树搜索)。通过保持向后延伸一段时间的思路，我们也可以将反馈应用于思考过程，教导代理进行规划，而不明确包括预测机制和未来状态树搜索，以隐式改进短期内的规划。这种感觉更像是人类的天性——我似乎不会预测给定数量的时间步长的所有未来状态，也不会选择最高价值的路径，从我的经验来看，我似乎有一些想法，并根据最近的想法和环境背景凭直觉采取行动(隐含的训练时间偏差，根据先前的反馈建立)。

此外，在未来的迭代中引入一些超人的能力似乎是合理的，包括智能体根据高等数学进行思考和推理的能力；从精神上看，是动态编程的结果；在因特网上搜索当前事件的背景；和 API 集成。

当前版本已经内置了几个超人的能力:

1.  能够使用 GPT-3 在精神上参考人类书面知识的整个历史，这已经在整个互联网和
2.  书面语言和代码之间的一般翻译，也使用 GPT-3

# 看起来很重要的人的能力

以下人员能力包含在当前的实施框架中，除非另外标注为“未来工作”:

**解读环境**

-解读内部环境

1.  思想
2.  记忆

-翻译书面或口头交流

1.  解释直接反馈
2.  解释环境的其他方面(未来工作)

**采取行动**

-作用于外部环境

1.  通过书面语言或演讲进行交流
2.  通过额外模式采取的其他环境行动(未来工作)

-作用于内部环境

1.  产生想法
2.  产生自我反馈(良心)

**学习**

1.  对反馈的短期反应
2.  从反馈中进行长期学习，以提高能力和规划
3.  从反馈中进行长期学习，以调整良知

# 履行

为了实现这些功能，有必要为代理建立一个接口，以便与能够与代理对话并提供反馈的一个人或一组人进行交互。Slack 提供了一种与编程代理进行对话的简单方法，并被选择用于这个初始实现。此外，从头开始建立一个数据模型来表示每个过程的工件也很重要，这样它们就可以被控制框架操作和使用。

# 数据模型

## 输入

**思绪**

谨慎的想法由一个字符串表示，表示内部反映，在生产版本中，不会与和模型交谈的人共享。这些是人类内部思想的代表。

**消息在**

从对话中收到的消息表示为谨慎的字符串。

**反馈于**

反馈直接通过 Slack 中的表情符号给出。一个映射字典将每个表情符号映射到一个介于-1 和 1 之间的浮点值，以分别反映消极和积极的程度。

**自我反馈**

自我反馈由介于-1 和 1 之间的浮点值表示，分别反映消极和积极的程度。该值既是自己生成的输出，也用作其他创成式模型的输入。

## 输出

**想法** —字符串，如上

**Messages out** —字符串，如上所述，当什么都不说是合适的时候可为空

**自反馈** —浮动，如上

## 代表时间

**短期记忆和反应**

对于这些数据对象中的每一个，重要的是维护 n 个步骤的历史，其中 n 是可配置的。这里我维护了 n 个元素的列表来表示最近的 n 个想法、记忆、信息输入、信息输出、反馈和自我反馈。这些短期记忆代表最后 n 步的状态，并被用作模型在唤醒循环期间的每个时间步产生思想、信息和自我反馈的提示，如下图所示。

**长期记忆和学习**

当收到积极的反馈时，储存短期记忆是很重要的，这样信息创建和思想产生的模型可以改进，以更好地与反馈保持一致，并随着时间的推移变得更有能力。这种改进是通过在睡眠循环中微调相应的模型来实现的。

当自反馈在数学上接近接收到的反馈时，通过将短期记忆转储到一个文件来存储状态以微调自反馈模型，以便自反馈模型可以在睡眠循环期间微调，如下面的架构图所示。随着模型的成熟，反馈和自我反馈之间的“亲密度”阈值(这表明是时候储存长期记忆以进行微调)应该增加，这似乎很重要，就像当孩子长大时，他们开始在自己的良心中建立更多的信心和一致性，并且更难通过外部影响来改变。例如，在晚年比早年改变一个微调过的良心需要更多的例子。

下面概述的控制框架使用这些工件在唤醒循环期间反映和行动，并在睡眠循环期间微调模型。

## 控制框架

在每个时间步，代理通过 Slack API 获得新的消息和反馈，并从内存中读取自己的内部状态。

使用这些变量，代理调用三个微调的 GPT 模型来产生自我反馈、一个想法和一个行动(消息)。如果反馈对于短期记忆中的多个最近状态是非常负面或正面的，代理可以调整模型的默认温度，以更自由地探索随机响应或坚持每个模型认为最好的。这反映了当时人们对重复极端反馈的反应方式。

当收到积极反馈时，代理将当前状态存储到思想和行动转换器的长期训练文件中。当自反馈接近反馈时，代理将当前状态存储到自反馈(良心)转换器的长期训练文件中。

在睡眠循环期间，代理使用保存的长期记忆微调其所有的变压器。

该框架如下所示:

![](img/acd63b89462d34090ed8b5b028ce6524.png)

一个类似人类的人工智能系统的控制框架的结构图。作者图片

建立实验的一个挑战是冷启动问题。为了让代理不随机执行，我创建了一些手写状态，用作每个 transformer 模型的少量提示，直到存储了足够的长期内存来进行微调。这些为数不多的例子严重偏离了系统，应该在未来的迭代中进一步研究和优化。

# 早期结果

![](img/e6ba22a44787380a9ecf5e01cf3d4906.png)

在捕捉重复响应并降低其可能性之前，在 Slack 中与 AI 系统进行对话(右)。这些可以通过足够多的例子训练出来，但是在这个版本中是显式处理的。进入人工智能系统操作的窗口(左)。think_prompt 代表 AI 在产生想法时所反映的内容，其中各种字符串的列表代表 n 个时间步长的体验，其中 len(list) = n. L 代表倾听，s 代表说，t 代表思考，f 代表反馈，sf 代表自我反馈。后来，我尝试改变这些列表标签，将它们扩展为完整的字符串“listened”、“thought”等，以便为 GPT-3 模型提供更多关于这些项目所代表的内容的上下文，这无疑提高了性能。作者图片

![](img/c3a138f2a035c67b340f29e42ff7fd4d.png)

与 Slack 中的 AI 系统的对话(右)，在为训练/思维链调整最大短期记忆之后。这里最大短期思维记忆= 6。反馈通过 Slack 表情符号提供给系统，这些表情符号被解析为 reaction_sum 浮动，范围为-1 到 1。一个进入人工智能系统操作的窗口(左),不太详细地打印了人工智能系统的思维状态。reaction_sum 表示在 AI 系统发送给用户的最后一条消息上收到的反馈，如果提供的话。作者图片

![](img/434863802a5905616ce84a5e688b57f4.png)

AI 系统的典型对话，2022 年 11 月初。作者图片

![](img/a8164493c45dc5e785d53cbb3e11e230.png)

2022 年 11 月初，两个这样的代理相互讨论质数。作者图片

![](img/52c1394bf39aabeb9aa935542dfe062e.png)

两个这样的代理人(试图)讨论意识，2022 年 11 月初。作者图片

![](img/f4edefc55648096d3883552942751d8e.png)

两个这样的代理人讨论他们将如何帮助人类，2022 年 12 月。这些版本与上述示例的不同之处在于，在 2022 年 11 月下旬可用之后，它们使用了 text-davinci-003。作者图片

在对这个框架进行了几个月的试验之后，有一些关键的发现似乎挑战了一些最初的假设:

**引入独立的思想流可能会对结果不利**

在对思想流回顾范围、模型温度和少量例子进行多次排列后，当使用长的思想流时，产生的消息*似乎*比使用任意短的思想流时质量更差，尽管这需要更多的实验和良好的基准。直觉上这是有意义的，因为在互联网上训练的 GPT 在他们说或写一些东西之前，不会有很多关于人类在想什么的训练例子(至少在像这样的直接访问格式中)。我需要重新思考思想被整合的方式，或者它们是否可以被完全移除。也许思维是智力的一种自然属性，不需要明确地包括在内。

**通过微调对长期记忆产生影响需要很长时间，或者至少有很多例子**

对于对模型产生影响的微调示例，似乎需要成千上万个示例来真正移动指针，事实上，在命令行中，我收到了一个响应，询问我是否确定要进行微调，因为我的示例数量非常少。这种类型的量在代理的长生命周期中似乎是有用的——当人类从父母和周围的人那里学习时，他们肯定积累了数万个以上的例子。在短期内，除了微调方法之外，包含一个更明确的长期记忆存储和搜索模块似乎是有先见之明的，可以使代理更快地获得所学的东西。

将输入格式化为字符串列表似乎不如将它们格式化为脚本有用

网上有很多很好的例子，关于脚本式对话的少量训练可以带来很好的对话效果。至少在目前的状态下，GPT-3 似乎不太擅长从我作为例子和提示使用的字符串列表中推断类似脚本的结构。这些应该被转换成更自然的东西，可以在网上的训练数据中找到，比如对话的脚本。在使用 text-davinci-003 进行测试后，即使使用字符串列表方法，性能似乎也有所提高。

# 一般性和基准

**共性**

这个框架似乎可以通过长期记忆和良心模块进行一般性学习和调整，良心模块利用了一种形式的[递归奖励模型](https://doi.org/10.48550/arXiv.1811.07871)。如果人类与它互动，它将学会做更多的事情，就像那些人类提供积极反馈的事情一样，还会建立自己的内在良知，随着时间的推移进行自我管理。其他输入和动作模式也应该在这个框架内工作，所以它不需要仅限于语音和思想。

**基准**

评估通用性的伟大基准并不多。传统的图灵测试在当时是有远见的，但是对于有用性来说是相当模糊的。雷·库兹韦尔提出了一个更严格的图灵测试版本。还不清楚用什么作为这类代理的基准。这个地区需要进一步勘探和开发。

# 下一次迭代的进一步研究

在下一次迭代中，我将结合我从这次尝试中获得的经验:

1.  潜在地将思想流作为状态的管理部分移除，并假设只要我在状态中保留其他短期时间上下文元素(说、听、反馈、自我反馈),它对于短期计划是紧急的。
2.  在推理时增加显式的长时记忆搜索。
3.  可能将输入格式化为更自然的对话脚本，更典型地为 GPT 在互联网上遇到的训练数据。

此外，我计划添加一些额外的有用的背景模块。已经有一些很好的例子将数学模块、网络搜索、python 解释器和 API 集成独立地结合在一起(对于任意的 API 集成，有 [Dust](https://dust.tt/) & [Adept](https://www.adept.ai/) ，对于谷歌搜索，有 [Ofir Press](https://twitter.com/OfirPress/status/1577302733383925762) ，对于 Python 解释器和数学，有 [Sergey Karayev](https://twitter.com/sergeykarayev/status/1569377881440276481) ),但是我还没有看到一个代理将它们一起使用。给这个代理工具来扩展它的能力似乎是有用的，就像随着人类的成长，我们可以获得更多的工具来扩展我们与世界的交互界面并获得更多的知识。Andrej Karpathy 称之为给代理“小发明和小工具”以更好地与世界互动，因为它改变了 GPT 训练的静态，我认为这是一个优雅的类比。

我预测 GPT 4 号将会有许多开箱即用的小发明和小玩意。我的最佳猜测是，通过正在进行的良心构建进行在线比对仍然有用，即使我可以将 GPT-3 API 调用替换为 GPT-4 调用。

# 一些额外的探索领域

*   人类作为随机代理的想法是来自几代人的反馈的集合，从一代到下一代有模糊的反馈传递，通过 DNA 通过大脑的初始权重传递。我们能否提供一种机制，将反馈从一代 AGI 人更有效、更直接地传递给下一代？
*   多智能体繁殖有用吗？我们是否应该混合多个代理人的长期记忆来微调“后代”？
*   探索死亡和失去特工——你如何决定哪些特工的长期记忆进入下一代特工？
*   在繁殖过程中，你实际上是如何混合来自多个代理的长期记忆的？也许我们可以探索总共只取 n 个长期记忆的例子，通过随机循环选择记忆，为每一个新的世代附加来自父母代理的记忆，用于遗传混合。
*   我们会在繁殖步骤中引入突变吗，就像在自然界中发现的那样？我们应该用 GPT 的说法改变记忆的子集吗？
*   我们什么时候开始通过耳语、触摸、味觉和嗅觉向文本添加额外的形式，如音频、音频语音？
*   最终，我们可能应该直接训练奖励函数，而不是微调，这可能比微调更有效，但成本更高。
*   我们可能应该从人类对话中挖掘比我在这里使用的手写例子更好的初始化状态。
*   我们可能应该使用`best-of-n`从 GPT 那里得到多个回应，然后使用自我反馈/良知为每个行动步骤选择最佳结果。这将允许代理实时自治。
*   我们应该在睡眠循环中改变长期记忆(有点像做梦)来为计划创造更合理的路径吗？微调时，我们是否应该低估变异的记忆与真实的记忆？
*   是否应该引入超人的视觉插值/想象/搜索(通过 DALL-E 或其他扩散模型)？
*   我们是否应该在微调框架中引入一种版本的[间隔重复](https://en.wikipedia.org/wiki/Spaced_repetition)——我们在训练数据中以越来越大的间隔重复长期记忆的精确副本或变体——来探索这种对人类有用的学习类型是否也能对 GPT 有用？
*   我们是否应该将非自我主体和人类的名字以及他们的信息引入到数据框架中，以便行动和回忆可以与那些具体参与对话的人相关联？

# 关于 ChatGPT 的最后一点说明

我从大学时代开始就对人工智能充满热情，当时我学习计算机科学，但自 2010 年以来，我没有在该领域以任何正式或有偿的身份工作过。我尽最大努力跟上我觉得有趣的论文和研究人员，以及看起来新颖和有创意的概念。我做了大量的思想实验，从以太中提炼出一些关于 AGI 的原则性思想，甚至写了几行代码——如上所述。所有这些都是在深夜或清晨完成的，大多是在周末，这时家里的其他人都在睡觉。

在过去的几个月里，我有幸与 Shivon Zilis、Sam Altman、Wojciech Zaremba 和 OpenAI 的其他几个人讨论了这里讨论的一些主题和其他领域。我感谢他们的时间和谈话，特别是 Sam 在阅读了这篇文章的初稿后的积极反馈。

11 月 30 日，OpenAI 发布了 [ChatGPT](https://openai.com/blog/chatgpt/) ，这是一个令人印象深刻的模型和聊天界面，可以通过拇指向上和拇指向下的表情符号与用户交谈并收集反馈，具有一些短期记忆，显然是跨多个领域的人类专家培训。说它很棒是一种保守的说法。虽然我确实希望我在 OpenAI 上说的或与团队分享的一些东西为这里使用的一个特性或方法提供了一些灵感，但我不会假装我对这项工作有任何影响。我相当肯定，在我与 OpenAI 开始任何对话之前，这方面的内容已经在飞行中很久了。

![](img/0b9cde515c5c4ed4ef0c5f6af972befb.png)

作者 ChatGPT chat.openai.com/chat,图片截图免费研究预览

相反，我分享这个背景，希望能激励那些可能阅读这篇文章的人。如果你对人工智能感兴趣，不要气馁或被这个领域吓倒！我有一个 10 年的计算机科学学位，每周最多花几个小时在这上面。这次经历告诉我，这些工具加上好奇心和学习热情可以让你走得很远。在这种情况下，我探索的主题与世界上一些顶级专家的工作相关，这无疑激励我继续投入时间进行学习和实验！这个领域总是需要更多的人才和汗水，让 AGI 的能力和调整正确是一个非常高风险的努力。如果你对人工智能感兴趣，我鼓励你投入进去！