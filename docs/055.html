<html>
<head>
<title>Explainability Using Bayesian Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用贝叶斯网络的可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainability-using-bayesian-networks-4dc706680294#2022-01-03">https://towardsdatascience.com/explainability-using-bayesian-networks-4dc706680294#2022-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="d4e5" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用贝叶斯网络的可解释性</h1></div><div class=""><h2 id="6f20" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们可以使用DAG来满足DL需求吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9eac2e6b2e5dc26bf8ffc2a9ab74b146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jx4DCqGReYMAjpDbaWNtdQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><h2 id="f624" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h2><p id="9a99" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">这篇文章是作为上一篇<a class="ae mk" rel="noopener" target="_blank" href="/a-dl-solution-for-tab-text-data-f92e2b68eb16">文章</a>的相邻研究步骤而写的。在那篇文章中，我们介绍了一种DL方法，它接收文本和表格数据的组合，目标是超越经典的ML工具，比如XGBoost。回想一下，这些机制之间的比较提供了这个图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/44acbfeaa55a9f438f818af5d0be03f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pm-etVps6eIfHFmmWtJusg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者-红色曲线是DL引擎，蓝色曲线是由大量数据训练的现有XGBoost，这是一个precision\TP图表</p></figure><p id="b18c" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">数据科学家可能会认为这是成功的:处理表格数据的DL引擎比处理这种数据的常见ML领先算法取得了更好的结果。然而，当我们考虑现实世界的ML解决方案时，其要求不仅限于模型性能，还包括其他问题，如可解释性风险和因果关系，这种“数据科学家期望的结果”可能会给业务部门和客户留下一些未解决的问题和挑战。众所周知，虽然经典的ML算法几乎总是提供特征重要性分数。DL不仅避免提供这种索引，而且经常掩盖这种信息。</p><h1 id="244f" class="mq kw iq bd kx mr ms mt la mu mv mw ld jw mx jx lh jz my ka ll kc mz kd lp na bi translated">什么是贝叶斯网络？</h1><p id="c493" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">使用贝叶斯网络(<strong class="lt ir"> BN </strong>)的动机是学习一组随机变量之间的依赖关系。网络本身是<strong class="lt ir">有向无环图</strong> ( <a class="ae mk" href="https://en.wikipedia.org/wiki/Directed_acyclic_graph" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ir"> DAG </strong> </a>)，模拟随机变量的联合分布。图结构遵循联合分布的概率依赖性分解:节点V仅依赖于其父节点(即，独立于其他节点的r.v X将被表示为父自由节点)。关于这个主题的教程可以在<a class="ae mk" href="https://www.youtube.com/watch?v=SkC8S3wuIfg" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><h1 id="d189" class="mq kw iq bd kx mr ms mt la mu mv mw ld jw mx jx lh jz my ka ll kc mz kd lp na bi translated">BN学习的步骤</h1><ul class=""><li id="dc6a" class="nb nc iq lt b lu lv lx ly le nd li ne lm nf mj ng nh ni nj bi translated"><strong class="lt ir">构建DAG</strong>——正如我们所描述的，BN训练的结果是一个非常近似的DAG。此图旨在说明变量之间的相关性和联合分布的最简单的因子分解。显然，有了专家知识，这项任务可以更好地完成。即当它的一些弧已知时。作为一个例子，如果我们有一个问题，很明显有一个目标变量，那么我们可以启动包含它的边，并让训练机制使用它作为先验知识。为了获得DAG，有两类主要的训练BN的方法:</li></ul><p id="85b8" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><a class="ae mk" href="https://arxiv.org/pdf/1406.7648.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ir">基于约束的结构学习</strong></a>——为了理解这种方法，我们使用以下定义:</p><p id="1a37" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><strong class="lt ir">Markov Blanket-</strong>r . v Y的Markov Blanket是r.v {X1，…Xn}的集合A，其中对于A的任何子集S，<strong class="lt ir"> Y独立于A\S |S，</strong>即如果我们对S上的随机变量进行条件化，我们得到一个独立于Y的余集</p><p id="e33a" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">因此，寻找DAG的算法由三个阶段组成:学习Markov Blanket、寻找邻居和确定弧方向。在第一阶段，我们使用chi评分(通常用于t检验)来确定变量之间的条件独立性</p><p id="6cb3" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><a class="ae mk" href="https://www.jmlr.org/papers/volume9/perrier08a/perrier08a.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ir">基于分数的结构学习</strong></a>——这个类实际上更有用，因为“<strong class="lt ir">由于统计评分方法”</strong>它被认为更有前途。</p><p id="56e9" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">我们预先确定一个搜索和评分方法。定义初始DAG(也可以是空DAG)。在每一步，我们增加，撤回或恢复一个优势，并计算一个统计分数。我们重复这个过程，直到收敛。我们很少有少量的变量，因此强力方法(<strong class="lt ir">穷举搜索</strong>)很少实用。但是我们可以使用启发式算法。这种算法的一个常见示例如下:</p><p id="b8dd" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><strong class="lt ir">爬山</strong> - <strong class="lt ir"> </strong>简单定义一个DAG，只要分数提高就跑。</p><p id="2fb2" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">另一种常用的算法是Chow Liu树。</p><p id="6586" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><strong class="lt ir"> Chow Liu- </strong>该算法通过分解KL散度项(类似于变分推断的概念)将分布Q近似为未知的联合分布P。显然，评分依赖于对数似然性。</p><p id="9074" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">构建DAG后，可能希望优化其参数:</p><ul class=""><li id="e318" class="nb nc iq lt b lu ml lx mm le nk li nl lm nm mj ng nh ni nj bi translated"><strong class="lt ir">参数学习</strong> -在此阶段，我们假设DAG的结构，并在此基础上优化贯穿其条件组件的联合分布。实际上，我们可以使用MLE方法，该方法与分布上的贝叶斯先验信念相结合。</li></ul><p id="89b8" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">当我们完成这两个步骤(构造DAG并学习其参数)时。我们可以通过网络进行推理。即在数据输入时确定概率(类似于某些特征未被观察到的情况下的DL引擎)</p><h1 id="ed28" class="mq kw iq bd kx mr ms mt la mu mv mw ld jw mx jx lh jz my ka ll kc mz kd lp na bi translated">真实世界的例子</h1><p id="c526" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在下一节课中，我将介绍一个简短的研究，这个研究使用了我之前介绍的DL引擎的表格数据。为了这个研究，我安装了python包<a class="ae mk" href="https://github.com/erdogant/bnlearn" rel="noopener ugc nofollow" target="_blank"> <strong class="lt ir"> bnlearn </strong> </a>。在这里可以找到这个图书馆的杰出指南<a class="ae mk" rel="noopener" target="_blank" href="/a-step-by-step-guide-in-detecting-causal-relationships-using-bayesian-structure-learning-in-python-c20c6b31cee5"/>。</p><p id="4f39" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">在我们开始之前，我要说明的是，由于这个项目是商业性的，我已经屏蔽了变量名，因此它们将有无意义的名字。</p><h2 id="f14e" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">构建我们的DAG </strong></h2><p id="40bf" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我们从寻找最佳DAG开始。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="6ecc" class="kv kw iq no b gy ns nt l nu nv"><strong class="no ir">import </strong>bnlearn <strong class="no ir">as </strong>bn<br/>DAG = bn.structure_learning.fit(dataframe)</span></pre><p id="5298" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">我们现在有一条匕首。它有一组节点和一个邻接矩阵，如下所示:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="a9a2" class="kv kw iq no b gy ns nt l nu nv">print(DAG[<strong class="no ir">'adjmat'</strong>])</span></pre><p id="8ee9" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">结果是这样的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/80f7d2901a48fac303902ceda0ddc213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLR3AKLjzHYKoKCIcqDqfA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><p id="f872" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">其中行是源(即弧线的方向是从左列到行中的元素)，列是目标(即列的标题接收弧线)。</p><p id="4d19" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">当我们开始绘制获得的DAG时，我们获得了一组变量，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/af37e04928db1a01388ade33babc589d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-02MnCvBLNHccgpIIYZhQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><p id="e61c" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">我们可以看到矩形中的目标节点是许多节点的源。我们可以看到它仍然将箭头指向两个节点。我们将在讨论部分讨论这个问题</p><p id="7c9a" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">显然我们有更多的变量，因此我增加了节点的数量。</p><p id="f6a7" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">添加为目标提供的now源的信息(即，其整行为“假”)</p><p id="8c77" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">获得的图形如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/b7354ea83ce8ee384d4d5543c85c6252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0jnBVZIQCnVb0-3mfpkWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><p id="6591" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">所以我们知道如何构建DAG。现在我们需要训练它的参数。</p><p id="d0af" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">在代码方面，我们执行如下:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="9da7" class="kv kw iq no b gy ns nt l nu nv">model_mle = bn.parameter_learning.fit(DAG, dataframe, methodtype=<strong class="no ir">'maximumlikelihood'</strong>)</span></pre><p id="a709" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">我们可以用'<strong class="lt ir"> bayes </strong>'来改变'<strong class="lt ir">maximum likelihood</strong>'，如下所述。</p><p id="858a" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">该训练的结果是反映DAG结构的一组分解的条件分布。</p><p id="cdf0" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">对于给定的变量，它的形式是这样的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/da3af82bb7efa945a6628e9c62e7476c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0FiW2_EB4L0F1mL-29Dd7Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><p id="5c01" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">字段<strong class="lt ir">变量</strong>是作为目标的节点的名称</p><p id="7784" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">字段<strong class="lt ir">变量</strong>是其来源</p><p id="c59d" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">字段“<strong class="lt ir">值</strong>”是确定每个“变量”值的条件概率的n维立方体。</p><p id="9aec" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">现在，我们可以对这个矩阵进行分析，以确定信息(条件分布)存在于何处。</p><p id="416e" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">对于推论，我们决定关于<strong class="lt ir">证据</strong>(即我们正在调节目标的哪个源)并且我们希望得到低条件熵</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="7db3" class="kv kw iq no b gy ns nt l nu nv">bn.inference.fit(model_mle, variables=[<strong class="no ir">'Target'</strong>], evidence={<strong class="no ir">'cov_0'</strong>:0,<strong class="no ir">'cov_1'</strong>:1  })</span></pre><p id="28bb" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">这一行的结果如下表所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e1f124fd17085cf0440a5210969d6a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*JR-zXqsEmCqF3Za7W6EItg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者</p></figure><p id="e604" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">也就是说，如果我们以这些值作为目标的条件，我们可以很好地预测目标结果。</p><h1 id="2da4" class="mq kw iq bd kx mr ms mt la mu mv mw ld jw mx jx lh jz my ka ll kc mz kd lp na bi translated">讨论</h1><p id="3619" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我们已经介绍了贝叶斯网络的一些理论概念，以及它们在为一组变量构建近似DAG时提供的用法。此外，我们提出了一个端到端DAG学习的真实世界的例子:使用BN构建它，使用MLE方法训练它的参数，以及执行和推理。我们现在可以讨论这个主题与DL问题的关系。为什么贝叶斯网络对这类问题有益？</p><p id="1606" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated"><strong class="lt ir">可解释性</strong> -从BN训练中获得DAG，提供关于数据库中独立变量的一致信息。在一般的DL问题中，特征是这些变量的函数。因此，我们可以得出哪些变量在我们的系统中占主导地位。当客户或业务单位对神经网络结果的原因感兴趣时，该DAG结构可以是提供重要性以及阐明模型的来源。</p><ul class=""><li id="a2ae" class="nb nc iq lt b lu ml lx mm le nk li nl lm nm mj ng nh ni nj bi translated"><strong class="lt ir">降维</strong> — BN提供了变量的联合分布及其关联。后者可能会在减少我们归纳的DL引擎的特征方面发挥作用:如果我们知道对于随机变量X，Y，X在Y中的<a class="ae mk" href="https://en.wikipedia.org/wiki/Conditional_entropy" rel="noopener ugc nofollow" target="_blank">条件熵</a>很低，我们可以省略X，因为Y提供了它的几乎全部信息。因此，我们有一个工具，可以从统计上排除多余的变量</li><li id="f9c4" class="nb nc iq lt b lu ob lx oc le od li oe lm of mj ng nh ni nj bi translated"><strong class="lt ir">标记行为</strong> -这一部分对于那些在视觉或语音等领域工作的人来说可能不太明显。在一些框架中，标记可能是一项模糊的任务(举例说明考虑一个具有大量可能重叠的类别的情感问题)。当一个人标记数据时，他可以依赖于数据集中的一些特征并生成条件概率。当我们初始化空DAG时，训练BN可以提供其中目标target是其他节点的父节点的结果。观察几个被测试的例子，这些结果只是反映了这种“标记者的方式”。因此，我们可以不使用Dag来满足ML需求，而是学习taggers策略，并在需要时进行改进。</li><li id="47e9" class="nb nc iq lt b lu ob lx oc le od li oe lm of mj ng nh ni nj bi translated"><strong class="lt ir">逻辑推理与因果推理的结合</strong> — <a class="ae mk" href="https://www.youtube.com/watch?v=CfzO4IEMVUk&amp;list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0" rel="noopener ugc nofollow" target="_blank">因果推理</a>是数据分析中一个非常成熟的领域。它提供了解决问题的工具，这些问题一方面是DL模型通常没有的，另一方面是现实世界提出的。需要找到这些工具协同工作框架。事实上，这种框架已经存在(例如GNN)。但是合并典型DL问题因果关系的机制不太常见。我相信这篇文章中描述的心流是从这种结合中获益的一个很好的步骤</li></ul><h2 id="2b99" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">感谢</h2><p id="b4b9" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我要感谢齐夫·巴尔-约瑟夫教授向我介绍了这个主题，并感谢尤里·伊泰进行了富有成效的讨论</p></div></div>    
</body>
</html>