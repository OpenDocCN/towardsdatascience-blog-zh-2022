<html>
<head>
<title>ML Algorithm that Natively Supports Missing Values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">本机支持缺失值的ML算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-algorithm-that-natively-supports-missing-values-40b42559c1ec#2022-01-18">https://towardsdatascience.com/ml-algorithm-that-natively-supports-missing-values-40b42559c1ec#2022-01-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="b2af" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">本机支持缺失值的ML算法</h1></div><div class=""><h2 id="c80e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不需要显式处理缺少的值</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0f682e6f3f5319693cf64ecdaac7687e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lY0pi4onq9xAHSEzFlTmfQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=693873" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae ky" href="https://pixabay.com/users/wilhei-883152/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=693873" rel="noopener ugc nofollow" target="_blank">威利·海德尔巴赫</a></p></figure><p id="b92a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现实世界的数据集通常包含大量缺失值，这可能是由于数据损坏或未能记录数据而导致的。数据中缺失值的存在可能会影响训练模型的稳健性。数据科学家需要在预处理管道中显式地处理丢失的值。</p><p id="56b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有多种技术可以显式处理数据中的缺失值。在我以前的一篇文章中，我们讨论了在预处理管道中处理缺失值的7种技术:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">机器学习中处理缺失值的7种方法</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">处理数据集中缺失值的常用策略</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="4e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数机器学习算法无法处理数据集中的缺失值，因此必须在建模管道之前处理缺失值。在本文中，我们将讨论不需要显式处理缺失值的机器学习算法:</p><ul class=""><li id="1a88" class="mn mo it lb b lc ld lf lg li mp lm mq lq mr lu ms mt mu mv bi translated">基于直方图的梯度增强分类器/回归器</li></ul><p id="8fa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述估计器可以支持数据集中的缺失值，并且不需要在建模之前显式处理nan。</p><h1 id="36ca" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">基于直方图的梯度增强:</h1><p id="f934" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">梯度提升是一种集成机器学习技术，它将AdaBoost等提升算法推广到一个统计框架，该框架将训练过程视为一个重复使用先前网络的任意损耗的加性模型，以提高估计器的能力。</p><p id="f295" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升将树模型顺序添加到集成中，其中每个基于树的模型试图校正前一个模型的误差。</p><p id="7d87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn库提供了支持直方图技术的梯度增强的实验实现。它提供了<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nt">histgradientsboostingclassifier</em></strong></a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nt">histgradientsboostingregressor</em></strong></a>类，分别实现分类和回归任务。</p><blockquote class="nu nv nw"><p id="c0a6" class="kz la nt lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated">根据<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn文档</a>:</p></blockquote><p id="5e97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该估计器对缺失值(nan)有本机支持。在训练模型时，树基于潜在的增益，在每次分裂时学习缺失样本应该跟随左孩子还是右孩子。在推断过程中，缺失的样本记录被相应地分配给左边或右边的孩子。如果对于训练样本的特征没有遇到缺失记录，则这些样本被映射到具有最多样本的子代。</p><p id="5093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度推进算法在计算上是昂贵的，因为每个树是顺序训练和连接的，因此不能并行训练。对于较大的数据集，基于直方图的梯度增强的实现相对来说比<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">GradientBoostingClassifier</strong></a></code>快得多。</p><h2 id="b6a7" class="oe mx it bd my of og dn nc oh oi dp ng li oj ok ni lm ol om nk lq on oo nm op bi translated">实施:</h2><blockquote class="nu nv nw"><p id="2a3d" class="kz la nt lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated"><strong class="lb iu">数据集来源:</strong>我将使用从<a class="ae ky" href="https://www.openml.org/d/40945" rel="noopener ugc nofollow" target="_blank">openml.org</a>下载的开源<a class="ae ky" href="https://www.openml.org/d/40945" rel="noopener ugc nofollow" target="_blank"> titanic数据集</a>来演示实现。根据知识共享协议的法律条款，数据集<a class="ae ky" href="https://creativecommons.org/publicdomain/mark/1.0/" rel="noopener ugc nofollow" target="_blank">是公开和免费的。</a></p></blockquote><p id="8bd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn包提供了<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nt">HistGradientBoostingClassifier</em></strong></a></code> <strong class="lb iu"> <em class="nt"> </em> </strong>类，实现了基于直方图的梯度增强分类器。titanic数据集包含大量不需要显式估算或处理的缺失值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/64a19d411045cb939dbc9829375376a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*6Ht2ls9GfY8je1V57h7r6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，Titanic数据集的缺失值计数</p></figure><p id="5201" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">泰坦尼克号数据集有891个实例，特征:“年龄”、“上船”有缺失值。</p><p id="fbed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对分类特征进行编码并将数据样本分成训练样本和测试样本之后。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，Titanic数据集预处理</p></figure><p id="87e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们没有在预处理阶段处理丢失的值，训练和测试数据已经有了丢失的值。</p><p id="b749" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们实现<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nt">HistGradientBoostingClassifier</em></strong></a></code> <strong class="lb iu"> <em class="nt"> </em> </strong>算法</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，基于直方图的梯度推进分类实现</p></figure><p id="d6e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn还使用类<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nt">HistGradientBoostingRegressor</em></strong></a><strong class="lb iu"><em class="nt">.</em></strong></code>提供了基于直方图的梯度增强的回归实现</p><h2 id="9a40" class="oe mx it bd my of og dn nc oh oi dp ng li oj ok ni lm ol om nk lq on oo nm op bi translated">本身支持缺失值的其他算法:</h2><p id="3c9b" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">k-NN和随机森林算法也可以支持缺失值。k-NN算法通过取K个最近值中的大多数来考虑缺失值。不幸的是，k-NN和RandomForest的scikit-learn库实现不支持缺失值的存在。</p><h1 id="b574" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">结论:</h1><p id="951f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在本文中，我们讨论了基于直方图的梯度增强算法，该算法受<a class="ae ky" href="https://github.com/Microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>的启发，对于较大的数据集来说比<code class="fe oa ob oc od b"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">GradientBoostingClassifier</strong></a></code>快得多。</p><p id="fcd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的上一篇文章中，我们讨论了显式处理缺失值的7种技术:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">机器学习中处理缺失值的7种方法</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">处理数据集中缺失值的常用策略</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="a483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果样本数据有很多缺失值，我们可以使用剩余的数据来预测缺失的记录。我在以前的一篇文章中简要讨论过这个问题:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">机器学习中处理缺失值的7种方法</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">处理数据集中缺失值的常用策略</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><h1 id="7390" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">参考资料:</h1><p id="6a5b" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">[1] Scikit-learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . histgradientsboostingclassifier . html</a></p><p id="46f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]泰坦尼克号数据集(<a class="ae ky" href="https://creativecommons.org/publicdomain/mark/1.0/" rel="noopener ugc nofollow" target="_blank">公开发布</a>):<a class="ae ky" href="https://www.openml.org/d/40945" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/d/40945</a></p><blockquote class="ot"><p id="dec4" class="ou ov it bd ow ox oy oz pa pb pc lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>