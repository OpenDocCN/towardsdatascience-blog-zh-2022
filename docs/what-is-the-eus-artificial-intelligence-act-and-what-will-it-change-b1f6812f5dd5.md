# 欧盟的人工智能法案是什么，会改变什么？

> 原文：<https://towardsdatascience.com/what-is-the-eus-artificial-intelligence-act-and-what-will-it-change-b1f6812f5dd5>

## 解释欧盟的人工智能法规提案

![](img/6eaa8d45face89d94df7b74a9c43f515.png)

照片由 [**安德烈·德·森蒂斯峰**](https://unsplash.com/@santesson89) **上**[**unsplash**](https://unsplash.com)**。**

**欧盟 AI 新规提案，简称“AI 法案”，于 2021 年 4 月公布**，一直是数据界广泛讨论的话题。这并不奇怪，因为它将对人工智能的一些领域产生巨大影响，而且在实践中如何应用这一规定还有很多不确定性。作为一名高级数据科学家，我已经参与了许多数据科学项目，我在可解释的人工智能方法和人工智能伦理方面有特殊的专业知识，所以我想阅读这份提案，以评估建议的法规的可行性。在多次阅读了欧盟 AI 法案的拟议版本后，我写了这篇文章来分享我对这一法规的解读和想法。请注意，该提案包含了如此多的小细节，以至于一篇帖子无法涵盖所有细节，并且围绕该主题有许多误解，我无法在一篇简短的帖子中澄清。该提案有所有欧盟语言版本，您可以在此处找到[的所有版本。](https://eur-lex.europa.eu/legal-content/EN/TXT/?qid=1623335154975&uri=CELEX%3A52021PC0206)

这篇文章的大纲如下:首先，我要说明为什么需要这项条例，然后，我将解释这项条例的主要部分和如何履行这些要求，违反这项法令的惩罚是什么，以及预计何时会发生的时间表是什么。最后，我列出了自己对这一行为的一些想法和担忧。在这篇文章的结尾，你也可以找到这个规则的摘要。

***来自《走向数据科学》编辑的提示:*** *虽然我们允许独立作者根据我们的* [*规则和指导方针*](/questions-96667b06af5) *发表文章，但我们并不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的* [*读者术语*](/readers-terms-b5d780a700a4) *。*

# 为什么欧盟提议对人工智能进行监管？

为了理解这一规定的紧迫性，有必要了解 AI 法案背后的动机是什么。AI 行为的两个主要原因是:

1.  **确保人工智能系统安全，尊重基本权利和欧盟价值观**:欧盟的法律基于欧盟的基本价值观，包括尊重人类尊严、自由和民主。最重要的是，欧盟受到基本权利的约束，包括不受歧视的权利、男女平等、数据保护和隐私权以及儿童权利。尽管人工智能技术有多种好处，但人工智能系统可能会强烈抵触工会的价值观和权利，并为有害的做法提供强大的工具。确保人工智能的发展尊重人们的权利并赢得他们的信任，让欧洲适应数字时代。
2.  **防止欧盟内部市场分裂**:随着人工智能系统的风险变得明显，一些成员国已经在考虑人工智能产品和服务的国家规则。由于产品和服务可能会跨境流通，不同的国家规则将分割欧盟内部的市场，并危及不同成员国对基本权利和欧盟价值观的保护。

![](img/b776ad4d5d9a72f4f3329fe90eafc5b8.png)

照片由[克里斯蒂安卢](https://unsplash.com/@christianlue)在 [**unsplash**](https://unsplash.com) **拍摄。**

# 《大赦国际法》提出了什么样的规定？

**本提案中的监管力度取决于人工智能系统产生的风险水平。**为了减少不必要的成本，并防止新的人工智能应用程序的采用速度放缓，人工智能法案希望对那些对人类风险低或最小的人工智能系统保持温和，并只针对那些带来最高风险的人工智能系统。该法规将禁止某些人工智能实践，为高风险人工智能系统规定义务，并要求某些人工智能应用程序透明。法规还建议建立欧洲人工智能委员会，就具体问题提供建议和帮助，以确保法规的顺利有效实施。

## 该法案中“AI”是如何定义的？

**术语“人工智能”的定义非常宽泛，令人惊讶的是，许多应用都属于“人工智能”**。在本条例中，“人工智能系统”是指采用一种或多种技术开发的软件(清单见附件一):

1.  **机器学习方法**，包括监督、非监督和强化学习，使用包括深度学习在内的多种方法；
2.  **基于逻辑和知识的方法**，包括知识表示、归纳(逻辑)编程、知识库、推理和演绎引擎、(符号)推理和专家系统；
3.  **统计方法**，贝叶斯估计，搜索和优化方法。

## 风险等级是如何定义的？

本提案中的风险级别分为三组:不可接受的风险、高风险和低或最低风险。

![](img/4868c6909015bd13bf24ec56ea36a7e5.png)

摄影由[飞:D](https://unsplash.com/@flyd2069) 上[下](https://unsplash.com) **。**

## **A)不可接受的风险**

在这一类别中，人工智能系统的风险如此之高，以至于它们是被禁止的(有三个例外)。被禁止的做法是:

1.  **操纵系统**:超出人的意识范围的技术，或利用特定群体的任何弱点(年龄、身体或精神残疾)以扭曲某人的行为，从而对该人或另一人造成伤害的技术。
2.  **社会评分算法**:公共机构使用的人工智能系统，用于评估自然人的可信度，从而对公民进行“社会评分”，
3.  **实时生物识别系统**:使用实时系统，在公共场所远距离识别人员，用于执法目的，除非在以下三种例外情况下绝对必要:
    **i)** 有针对性地搜索潜在的犯罪受害者或失踪儿童；
    **ii)** 预防迫在眉睫的安全威胁或恐怖袭击；
    **iii)** 对严重刑事犯罪的作案人或嫌疑人的侦查。
    这三种例外情况要求评估使用该系统与不使用该系统时危害的严重性和规模，并且它们的使用需要始终得到相关机构的批准。

## **B)高风险**

被确定为高风险的人工智能系统可能会对一个人的生活和保障其生计的能力产生重大影响，或者会使一个人参与社会变得复杂。设计不当的系统可能会以有偏见的方式运行，并显示出历史歧视的模式，因此，为了降低这些系统的风险，只有当它们符合下一节讨论的某些强制性要求时，才能投入使用。在以下情况下，人工智能系统被视为高风险:

1.  它包含在附件二所列的欧盟协调立法中，在投放市场之前，必须经过第三方一致性评估。属于这一类别的产品有机械、医疗器械和玩具等。
2.  该清单列于附件三。此类别中的系统分为八大类，分别是
    **i)** 使用生物特征识别；
    **ii)** 运营关键基础设施(道路交通、水、热、气、电)；
    **iii)** 决定受教育机会或评价学生；
    **iv)** 用于招聘，或决定晋升、终止合同或任务分配；
    **【v】**确定获得服务和福利的途径(如社会援助、补助金、信用评分)；
    **vi)** 用于执法；
    **vii)** 用于移民、避难或边境管制管理；
    **viii)** 协助司法系统(如协助研究事实和法律)。

## c)低风险或最小风险

对于不被认为是高风险的 AI 系统，只需要某些透明度规则。这些义务是:

1.  如果一个人工智能系统与人交互，它必须通知用户，用户正在与人工智能系统交互，除非从使用的上下文来看这是显而易见的。
2.  如果人们接触到情绪识别系统或根据性别、年龄、发色、纹身等将人们划分到特定类别的系统，必须告知他们。
3.  经过处理的图像、音频或视频内容，如果与可能看起来真实可信的现有人物、地点或事件相似(如“深度伪造”)，则必须明确声明该内容是人工生成的。

然而，鼓励低风险和最小风险人工智能系统的提供商自愿创建和实施行为准则。这些行为准则可以遵循为高风险系统设置的要求，或者可以包括对环境可持续性、残疾人无障碍、开发团队多样性以及利益相关方参与人工智能系统的设计和开发过程的承诺。

为军事目的开发或使用的人工智能系统不在本条例的范围之内。法规适用于 a)投放市场、b)投入服务、c)在联盟内使用或 d)影响联盟内人员的所有系统(例如，如果 AI 执行的活动在联盟外运行，但其结果在联盟内使用)。同样，人工智能系统的工作是有偿还是无偿也没有区别。

# 高风险 AI 应用需要什么？

![](img/336b1cee5dbbeb531265148e6491056b.png)

照片由 [Desola Lanre-Ologun](https://unsplash.com/@heylagostechie) 在[**unsplash**](https://unsplash.com)**上拍摄。**

考虑到监管干预的早期阶段以及人工智能行业正在快速发展的事实，以及审计专业知识现在才开始积累，**监管严重依赖内部评估和报告**。高风险人工智能应用的需求清单很长，而且经常令人困惑，很难理解这些需求必须满足的精确程度。高风险系统的提供商必须至少:

1.  **建立风险管理系统**:需要在高风险 AI 系统的整个生命周期中定期更新。它需要识别和分析所有已知和可预见的风险，这些风险可能在高风险人工智能系统用于预期目的或合理滥用时出现，特别是如果它对儿童产生影响。
2.  **编写技术文件:**必须随时更新，文件必须遵循附录 IV 中规定的要素，因此必须至少包含:
    **a)** 人工智能系统的一般描述，例如其预期目的、系统版本和硬件描述，
    **b)** 人工智能系统的详细描述，包括人工智能系统的一般逻辑、关键设计选择、 培训数据的主要特征、系统的预期用户群以及系统旨在优化的内容，
    **【c)**人工智能系统的能力和性能限制的详细信息，包括总体预期准确度水平和特定人群的准确度水平，以及对健康、安全、基本权利和歧视风险的评估。
    如果高风险人工智能系统是受附件二中所列法令管制的产品的一部分(如机械、医疗设备和玩具)，技术文件也必须包含这些法令所要求的信息。
3.  **满足对使用的训练、测试和验证数据集的要求**(如果系统是用数据训练的):数据集需要相关、有代表性、没有错误和完整。它们必须具有适当的统计特性，尤其是针对打算使用高风险人工智能系统的人群。必须考虑相关的设计选择、收集的数据、数据准备过程(如注释、标记、清理、汇总)、数据假设(数据应该测量和代表的内容)、可能偏差的检查以及任何可能的数据差距和缺点的识别。
4.  **实现适当级别的准确性、健壮性和网络安全:**高风险的人工智能系统必须实现适当级别的准确性，并且必须在其整个生命周期中保持一致。它需要对人工智能系统使用中可能出现的错误、故障和不一致具有弹性。用户必须能够中断系统或者决定不使用系统的输出。此外，人工智能系统必须对通过利用系统漏洞来改变其用途或性能的企图具有弹性。
5.  **对系统进行符合性评估**:在某些情况下，全面的内部评估(遵循附件六中的步骤)就足够了，但在某些情况下，需要第三方评估(参见附件七)。请注意，对于那些属于附件二所列法令范围内的高风险系统(如机械、医疗器械和玩具)，必须由这些法令中指定的机构进行符合性评估。
6.  **将详细说明交给用户**:用户必须能够解释系统的输出，监控其性能(例如识别异常、功能障碍和意外性能的迹象)，并且必须了解如何正确使用系统。说明应包含提供商及其授权代表的联系信息，具体说明性能的特征、能力和限制(包括可能影响预期性能水平的情况)，并且说明必须明确说明输入数据的规格。
7.  **将系统注册到公众可访问的欧盟数据库**:所有高风险系统及其汇总表必须注册到欧盟数据库，并且这些信息必须随时保持最新。汇总表必须包含附录 VIII 中列出的所有信息，包括供应商的详细联系信息、人工智能系统的商品名称(以及任何其他相关识别信息)、系统预期用途的描述、人工智能系统的状态(已上市/不再上市/已召回)、某些证书的副本、系统可用的成员国列表以及电子使用说明。
8.  **在系统使用时保持记录**:当系统在合同安排或法律允许的范围内运行时，人工智能系统必须自动记录事件(“日志”)。这些日志可用于监控实际操作，它们有助于评估人工智能系统是否正常运行，特别注意危险情况的发生。
9.  **保持上市后监控并报告严重事件和故障**:提供商有义务记录和分析从用户(或通过其他来源)收集的关于高风险人工智能系统在其整个生命周期内性能的数据。提供商还必须立即报告已经发生的任何严重事故或故障。

**当局必须被授予对训练、验证和测试数据集的完全访问权，并且如果必要的话，还可以访问源代码。**请注意，当局必须尊重在此过程中获得的信息和数据的保密性(包括商业信息和商业秘密)。由于有关当局拥有很大的权力来决定哪些人工智能系统可以在欧盟市场上设置，该法规为可以进行符合性评估的各方制定了严格的规则。例如，不允许出现利益冲突，当局不能提供任何可能与人工智能系统竞争的活动或咨询服务。

**该法规还列出了人工智能系统的进口商和经销商的义务**，他们需要确保人工智能系统满足该法案中列出的要求。**此外，高风险人工智能系统的用户也有义务**:例如，他们需要确保输入数据与高风险人工智能系统的预期用途相关，如果用户遇到任何严重事故或系统故障，用户必须中断人工智能系统的使用，并将事件通知提供商或经销商。

# 违反该法案的处罚是什么？

![](img/a9a932f5ac92a719e346829c061bc381.png)

照片由[马蒂厄斯特恩](https://unsplash.com/@mathieustern)在[上 **unsplash**](https://unsplash.com) **。**

罚款分为三类:

1.  **使用禁止的人工智能做法或违反数据要求**:3000 万欧元或上一财年全球年营业额的 6 %，以较高者为准。
2.  **不符合本法规的任何其他要求**:2000 万欧元或上一财年全球年营业额的 4 %，以较高者为准。
3.  **就本条例**的要求提供不正确、不完整或误导性信息:1000 万欧元或上一财政年度全球年营业额的 2 %，以较高者为准。

在决定罚款金额时，应考虑具体情况，例如不遵守行为的性质和持续时间及其后果，以及实施侵权行为的经营者的规模和市场。

# 什么时候举行？

2018 年 3 月，欧洲委员会成立了人工智能专家组，起草人工智能道德准则的提案。2019 年 4 月，“可信人工智能的道德准则”发表。欧盟人工智能法规的第一份草案于 2020 年 2 月发布，被称为“人工智能白皮书”。该文件邀请所有感兴趣的各方对拟议的条例提出反馈意见，共收到了来自公司、商业组织、个人、学术机构和公共当局的 1000 多份意见。根据这些反馈，2021 年 4 月公布了人工智能法案的当前版本。尚不清楚该法规将于何时生效，但据估计，该法规最早将于 2023 年被接受，假设有两年的过渡期，该法规将于 2025 年开始适用。

# 我觉得这个表演怎么样？

![](img/e9e244cc0e8a5241d962d9e85a6b65ee.png)

照片由[粘土堤](https://unsplash.com/@claybanks)上的[unsplash](https://unsplash.com)**拍摄。**

我已经关注人工智能伦理和透明度讨论好几年了，似乎更多的人已经开始理解盲目信任人工智能系统的风险。我们已经知道人工智能系统歧视女性的情况，在招聘工具中给她们较低的分数([链接](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G))和较低的信用额度([链接](https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html))。此外，一个旨在为美国医院工作的人工智能系统决定，白人病人比同样生病的黑人更迫切需要帮助，即使黑人比普通白人病得更严重(训练数据有偏差，因为白人有更好的健康保险，所以他们的疾病得到更好的诊断，[链接](https://www.nature.com/articles/d41586-019-03228-6))。所以 AI 的风险不是想象出来的。

这就是为什么我赞扬监管人工智能系统的尝试，以确保人们将得到平等对待。太多的提供商和开发者不知道或不关心检查他们的系统是否会歧视某些人，以及这对这些人可能意味着什么。但我不认为这项规定中建议的过度报道是实现预期目标的正确方式。我认为有些要求听起来不错，但很难或不可能实现。

在多次阅读该提案后，有几个句子特别突出。例如，如果用于训练人工智能系统的数据不满足本法规中设定的要求，则会受到最高级别的处罚。例如(从提案中直接引用):“训练、验证和测试数据集应该是相关的、有代表性的、没有错误的和完整的。”此外，数据必须对作为系统预期用户组的人群具有适当的统计属性。这些要求在真实的数据科学案例中非常沉重，令人失望的是，该法规没有更好地说明如何确保数据集符合限制。例如:一个数据集可能包含大量不同的年龄组、种族和同等数量的男性和女性，但在详细的分析中，我们注意到年轻的黑人女性非常少。所以如果我们看一个特征的统计，数据集看起来不错。但是如果我们观察三个特征的组合，例如年龄、种族和性别，我们就开始发现问题了。如果我们同时关注多个特征，那么不可避免的是，群体规模将变得如此之小，以至于不再具有统计学意义。那么，我们如何确保我们的数据集中不存在偏见，并且我们不必支付 3000 万欧元的罚款呢？

该条例指出，它严重依赖内部报告，因为审计方面的专门知识现在才开始积累。然而，罚款似乎非常高，即使我们正处于这种监管的早期阶段。对我来说，这就像是一场赌博:你是要承担意外忘记提及某件事的风险，如果你被抓到，你的公司就要完蛋了，还是为了安全起见，你会放弃所有人工智能系统的开发？我认为后果似乎不必要的严厉，因为立法者似乎并不完全知道满足一些要求有多困难。出于这个原因，我认为对基本人工智能系统的投资下降的风险非常高，因为投资者担心高额和不可预测的罚款。我希望立法者能够修改这一规定，只要求现实世界的人工智能系统能够完成的步骤。让我们不要用不公平的立法来破坏良好的意愿！

# 《大赦国际法》概述

欧盟监管人工智能系统的最新提案包含以下几点:

*   人工智能系统根据其产生的风险分为三类。
*   不可接受的风险系统通常是被禁止的。这类系统包括操纵系统、社会评分算法和实时监控系统。
*   高风险系统对一个人的生活和保障其生计的能力有重大影响，或者会使一个人参与社会变得复杂。这些系统可以确定服务和福利的获取，用于招聘或评估学生。只有符合某些强制性要求(包括内部评估和报告),才能投入使用。
*   低风险和最小风险系统必须遵守特定的透明度规则。例如，如果一个人工智能系统与人交互，它必须通知用户，用户正在与人工智能系统交互。还有“深度假货”视频等。必须明确说明内容是人为生成的。
*   该法规预计最早将于 2025 年开始实施。

作者是一名高级数据科学家，也是可解释人工智能方法的专家，她花了多年时间关注人工智能伦理的讨论。她有学术背景，拥有多年的计算物理模拟经验和理论粒子物理博士学位。她对监管人工智能的尝试很感兴趣，因为人工智能模型不是以直截了当的方式工作，所以编写这样的监管并不容易，但她完全支持监管人工智能，因为人工智能开发者太专注于快速构建模型，而没有考虑人工智能伦理。