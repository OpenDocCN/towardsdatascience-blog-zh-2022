# 为数据科学选择数据存储时的八个考虑因素

> 原文：<https://towardsdatascience.com/eight-considerations-when-choosing-a-data-store-for-data-science-c71ce72b05de>

## *选择合适的技术，使数据科学家能够专注于数据科学而不是 IT 基础设施，这将有助于企业获得其在数据科学和机器学习方面投资的收益*

![](img/ed3fd52dd4249d2f6376d0b4e133f357.png)

[法比奥](https://unsplash.com/@fabioha)在 [Unsplash](https://unsplash.com/photos/oyXis2kALVg) 上的照片

这里有一个悖论，它绊倒了许多扩展数据科学运营的企业:当他们意识到他们需要积极主动地组装合适的基础架构，以将数据科学置于尽可能多的业务运营的核心时，在不招致一点痛苦的情况下进行新的基础架构投资已经太晚了。

但是，无论您的组织刚刚开始扩展其数据科学运营，还是您正在努力将数据科学置于尽可能多的业务流程的核心，您都需要考虑支持强大的数据科学实践的要求。对于所有企业来说，这将是你的组织未来生存的基础。

在本次讨论中，我们将重点关注数据存储和支持整个企业大规模数据科学的功能。如果你想让你的公司走上成功之路，你需要考虑收集和组织数据的最佳方式。此外，您需要利用所有对您的 MLOps 计划有用的数据；您不希望使用因数据存储无法捕获或管理某些数据馈送而受到损害的数据集。

记住，在机器学习和人工智能中，垃圾入/垃圾出( [GIGO](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out) )效应极其强烈。如果缺少存储平台，您就无法获得具有可靠访问和可用性能的高质量数据。

因此，需要考虑八个因素:

**1。可扩展性:更多的数据胜过更好的算法，我们已经看到了大量令人信服的证据。在 Halevy、Norvig 和 Pereira 的《数据的不合理有效性》一书中，作者认为“……简单的模型和大量的数据总是胜过基于更少数据的更复杂的模型。”因此，当您的数据存储无法扩展以容纳更多数据，并且在项目进行过程中，您发现由于这种限制，您需要进行数据平台迁移时，您的项目就注定要失败。**

今天的神经网络能够做出惊人的事情！这并不是因为这些架构自 20 世纪 60 年代开发以来已经有所改进。相反，这是因为我们现在有能力存储和处理海量数据。非常聪明的算法无法弥补数据的缺乏。这根本不是一个选项。

每个行业都变得由数据驱动。但是，想想因无法扩展数据存储而失去的机会，就令人难以置信。以医疗保健行业为例:Statista [报告](https://www.statista.com/statistics/1038042/global-healthcare-data-storage-limitations/#:~:text=Projected%20health%20data%20storage%20limitations%20in%202020&text=Despite%20the%20growing%20amount%20of,exabytes%20of%20healthcare%20data%20generated.)称，到 2020 年，医疗保健数据生成量超过数据存储能力的比例超过 2 比 1。当超过一半的生成数据因为组织没有存储这些数据的方法而被丢弃时，这意味着大量情报的丢失，而这些情报本来可以很好地用于改善患者的结果。请这样想:这个问题的答案，*“我们到底该不该储存它？”*应该基于数据是否有用，而不是基于容量限制。

此外，可伸缩性不仅仅意味着容纳大量数据的能力。当您处理这些数据时，系统还必须能够保持良好的性能和响应能力。大量的机器学习算法被迭代训练，这意味着要对数据进行成百上千次的检查。如果您正在操作一个大型数据集(不能固定到内存中)，数据存储的访问时间和速度将成为一个重要的瓶颈。

数据科学家通常喜欢开源工具，喜欢 PostgreSQL、MongoDB 等。例如，它们远没有雪花的大规模并行处理(MPP)能力。当考虑将开源工具用于小规模数据操作时，它们很有吸引力。但是，当您转向企业范围的 MLOps 计划时，您会很快发现依赖这些开源解决方案并在内部维护它们变得完全不可行。

**2。灵活性**:关系数据库为王的日子已经一去不复返了。关系数据库非常适合处理结构化数据。然而，在许多机器学习领域，如计算机视觉或自然语言处理(NLP)，对半结构化和非结构化数据的支持是必不可少的。

企业数据库对非结构化数据的支持有限。这不是它们的设计初衷，因此它们在 MLOps 中的作用并不明显。在为您的数据操作选择数据存储时，您不仅需要考虑您当前的需求，还需要尽可能考虑您将在两年、三年或五年内运行的模型。支持机器学习工作负载的数据存储系统在格式方面必须灵活。它应该很容易处理表格数据和其他格式，如 JSON、Avro、ORC、Parquet、XML 等等。

**3。混合工作负载**:一些数据存储擅长处理事务性工作负载，这些工作负载只需要在一两个数据行上执行计算，并以亚秒级响应时间执行。其他人擅长管理分析工作负载，这些工作负载需要处理数百万行数据，并且响应时间相对宽松。出于数据操作的目的，数据科学家需要在这两方面都出色的数据存储。

雪花的建筑真的很有趣。他们采用压缩的列方法来存储数据，这允许大规模地高性能处理分析和事务性工作负载。雪花的“虚拟仓库”，本质上是 MPP 计算集群，具有内在的灵活性，因为它们可以随着资源需求的变化而动态地创建、调整和删除。这些微分区支持高效扫描各个列，以支持所需的分析和事务查询的执行。

**4。可靠性**:当停电导致公司数据不可用时，所有数据操作会突然停止；对于您的数据科学家来说，除了等待 it 团队纠正这种情况之外，别无他法。从财务角度来看，让您的数据操作闲置一段时间已经够糟了，但在停机期间无法接收新数据很可能会加剧这一问题。

数据是组织可以拥有的最大资产。不要误解我——人力资本也非常重要。数据科学人才需求量很大。有大量的报道称，在一个[继续快速增长的领域](https://fortune.com/education/business/articles/2022/03/08/glassdoors-no-3-best-job-in-the-u-s-has-seen-job-growth-surge-480/)，数据科学家持续短缺。如果你失去了整个数据科学/机器学习团队，你将会经历一段痛苦的旅程。但是可以雇佣新员工，对现有员工进行再培训。是的，这将是昂贵的，对企业来说将是一个挫折，但公司仍然可以恢复。另一方面，如果你丢失了所有的数据，很可能几个月后你的公司将不复存在。

数据存储的可靠性是绝对必要的。在内部运行数据库的单个实例很容易，但是如果您需要在集群中运行多台服务器，并提供负载平衡、灾难恢复和其他服务，那么管理起来就要复杂得多，成本也高得多。在大多数情况下，最好使用云提供商的服务来进行数据存储操作。像雪花，GCP，AWS 和其他供应商有专门的团队来照顾基础设施的可靠性，从可用性和安全性的角度来看。这是他们每周 7 天、每天 24 小时都在做的事情，他们积累的经验让他们比内部团队更有优势。

**5。云原生存储:**这在某种程度上是上面讨论的可靠性考虑的扩展。利用云原生存储资源可以获得多项重要优势:

*   灵活的容量:利用云资源允许您的数据科学团队根据需求调整分配的资源。如果您的系统完全是内部部署的，您将不得不投资昂贵的硬件来进行扩展。然后，一旦需求下降，缩小规模就更成问题了。
*   维护和安全性:如上所述，将这些服务卸载给供应商几乎总是有利的。他们通常会比内部团队做得更好，因为这是他们的主要关注点；他们以此为生，每天应对威胁会让服务提供商比您的内部团队更有经验。
*   地理恢复能力—这不仅涉及通过冗余实现的群集可靠性，还涉及在特定区域隔离数据以符合区域法规的能力。

处理敏感数据的公司通常会有多个禁止相互连接的网络。他们可能有专门的内部环境来存放敏感数据，以符合当地或区域的数据治理要求。其他不受限制的数据可以存储在任何地方，因此经常被发送到云。数据科学家面临的问题是，当我们处理两个(或更多)截然不同的孤立环境时，我们如何进行协作？

当数据不能跨越国界时，你需要对数据进行计算，而不是反过来。但是，如果您有云数据存储支持的分布式数据操作，您可以在您最好的数据科学家居住的任何地方开发和试验模型(只要您只使用样本/匿名数据)。借助云原生存储，您可以在世界任何地方构建和训练这些模型。一旦远程数据科学家调整了模型，他们可以轻松地将它们导出到敏感数据所在的区域，本地数据科学家可以在那里部署模型并针对实际的敏感数据执行操作。

**6。易用性:**这里的关键思想是让数据科学家做数据科学。数据存储必须易于从任何语言和工具(Python、R、MATLAB 等)访问。)你的数据科学家更喜欢。同样重要的是，要有一个开放的数据科学平台，可以轻松地与各种数据存储集成。没有必要经历大量的数据存储选择过程，而只是意识到当前的 DS 堆栈不支持它。

此外，使用数据存储的学习曲线不应该太陡。如果您的数据科学团队有自我改进的时间预算，他们应该花时间学习新的算法和技术，获取更多的领域知识，或者更加精通他们日常使用的语言和框架。他们不应该花费分配的时间学习晦涩的专有 API 或 SQL 语法，仅仅是为了获得他们需要的数据。如果给定的解决方案要求用户开发大量的专有知识，例如，开发人员认证途径包括五门培训课程(每门 3-5 天)和五场认证考试，那么这可能不是您的数据科学团队可以依赖的理想系统。

您所使用的技术工具应该尽可能对您的数据科学家不可见。您的数据科学家已被聘用，并根据他们所带来的深厚专业知识获得报酬。因此，他们的时间是非常宝贵的；他们应该把钱花在开发、培训、部署和监控模型上，而不是浪费在 it 基础设施上。

考虑一个医院环境:我们不希望看到外科医生花费时间来确保初级保健医生或急诊室人员在电子健康记录(EHR)中捕获的笔记是可访问的和完整的。毫无疑问，外科医生获得这些信息对患者的结果至关重要，但是外科医生需要将他们的时间和精力花在他们唯一胜任的活动上。在数据科学运营中，我们不希望数据科学家必须管理他们用于开发、培训和部署模型的基础架构。或者，更糟糕的是，我们不希望看到他们调整他们的流程来适应或解决他们的技术工具带来的限制。

7 .**。数据版本化:**数据科学中的再现性极其重要。如果您不想手动处理数据版本控制，支持这种现成功能的数据库可以让事情变得非常简单。[雪花时间旅行](https://docs.snowflake.com/en/user-guide/data-time-travel.html#time-travel-sql-extensions)和[零拷贝克隆](https://docs.snowflake.com/en/user-guide/tables-storage-considerations.html#label-cloning-tables)是这种重要能力的极好例子。

假设以下情况:为一家金融机构工作的数据科学家使用机器学习开发和训练一个评分模型，以批准或拒绝贷款申请人。该模型已部署，并在前两年在整个企业中运行良好。但随后，该银行受到了该模型存在偏见的指控。数据科学团队在审核期间必须回答的第一个问题是他们如何训练模型。他们需要访问两年前使用的培训数据。

为了重现性，公司需要他们的原始数据。一些系统很好地自动化了这一点；Oracle 拥有“闪回”技术，可以提供特定时间段的数据。它自动跟踪一段时间内数据的所有更改，并返回查询中请求的日期之后的数据。并非所有数据存储都以这种方式工作，而是需要数据科学家手动跟踪更改或手动在不同的时间点拍摄快照。

**8。未来焦点:**这是一个长期的数据运营策略。您的数据存储的潜在供应商是否具有前瞻性？他们的路线图是什么？他们懂机器学习和数据科学吗？他们是否提供(或考虑)对 Python/R 的数据库内执行的支持？他们能利用 GPU 吗？他们可以部署模型进行数据库内评分吗？他们会考虑 Kubernetes 的支持吗？诸如此类的问题将有助于您了解数据存储提供商在未来几年的发展前景。

了解您的数据操作中使用的工具和产品在多大程度上符合供应商的产品战略，以及他们的路线图的哪一部分是以机器学习和数据科学为导向的，这一点非常重要。简而言之，寻找一家了解数据科学和机器学习的数据存储供应商，其对该学科在五年内的发展前景的愿景与您的数据科学团队的愿景相吻合。

**结论**

几乎在所有情况下，数据存储的购买决策都取决于企业内部的购买委员会。组织数据运营的成熟度将决定委员会的组成人员。那些没有深入到将数据模型置于其业务运营核心的组织可能会将决定权交给 IT 领导层。那些在成为模型驱动型企业的道路上走得更远的公司可能会有一位首席数据分析官(CDAO)或同等职位的人来做出最终决定。但是，最终，IT 人员和数据科学领导者和实践者应该根据上述八个考虑因素，共同决定一个能够提供最佳功能组合的解决方案。

无论您的组织是否比您所希望的晚一点认识到数据存储的功能对数据操作的成功至关重要，或者您是否能够选择理想的数据存储来发展您的数据操作，所有人员都需要了解选择满足您现在和未来需求的最佳解决方案有多重要。