<html>
<head>
<title>PCA 102: Should you use PCA? How many components to use? How to interpret them?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA 102:该不该用PCA？使用多少组件？如何解读它们？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-102-should-you-use-pca-how-many-components-to-use-how-to-interpret-them-da0c8e3b11f0#2022-01-04">https://towardsdatascience.com/pca-102-should-you-use-pca-how-many-components-to-use-how-to-interpret-them-da0c8e3b11f0#2022-01-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="8e0d" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">PCA 102:该不该用PCA？使用多少组件？如何解读它们？</h1></div><div class=""><h2 id="bf7a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对PCA分析中一些中间概念的探讨</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b032f1ecce0e9e9d9934ba5b3b9bf6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XtDfWEqfW4NZv3XG"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">约翰·施诺布里奇在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a521" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主成分分析(PCA)是数据科学领域中最常用的降维技术之一。由于它的重要性，提高我们对它的理解是更好地使用这项技术的必要条件。</p><p id="a495" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在介绍性课程中经常可以看到PCA是如何制作的以及它代表了什么，然而，有些方面通常不会在这些课程中进行评论。一些改进PCA的想法只能在论文中找到，因此许多数据科学家没有接触过它。</p><p id="f60e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章旨在提高对PCA的理解，提供工具来回答，至少在一定程度上，以下问题:</p><ul class=""><li id="d707" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我如何知道是否应该对我的数据使用PCA？</li><li id="7772" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我应该使用多少个主成分？我怎么知道有多少是相关的？</li><li id="a4ce" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">我能解释主要成分并对它们有某种可解释性吗？</li></ul><p id="7191" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，首先，我们将(真正)简要描述一下PCA是如何构建的，之后，我们将直接回答这些问题。这款笔记本的代码(甚至更多)可以在<a class="ae kv" href="https://www.kaggle.com/tiagotoledojr/a-primer-on-pca" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和<a class="ae kv" href="https://github.com/TNanukem/paper_implementations/blob/main/PCA.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。</p><p id="5096" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的灵感来自于论文[1]。</p><h1 id="5030" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">主成分分析的简要回顾</h1><p id="f574" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">PCA算法相当简单，在互联网上有广泛的解释。因此，这一节将快速概述算法。它的实现可以在上面链接的笔记本中找到。</p><p id="ca42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，算法如下:</p><ul class=""><li id="4c55" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">计算数据集的协方差矩阵</li><li id="57d4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">提取矩阵的特征向量和特征值</li><li id="8e86" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">选择所需维度的数量，过滤特征向量以匹配它，并按照相关的特征值对它们进行排序</li><li id="b1f9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将原始空间乘以上一步生成的特征向量。</li></ul><p id="fcd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提醒一下，在应用PCA之前对数据进行标准化是一个很好的做法。这就避免了所有方差只在一个分量上的情况，因为方差只在一个或两个更大范围的变量上。</p><h1 id="f809" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">评估我们是否应该对空间应用PCA</h1><p id="9ce8" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">判断我们是否应该在机器学习问题上使用PCA的通常程序包括应用有和没有PCA的建模过程，并查看哪一个产生最好的结果。由于机器学习是一个非常经验性的领域，这对于几种方法来说是常见的。</p><p id="0216" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，有一种更好的方法来评估主成分分析的有效性，而不必多次拟合昂贵的模型。</p><p id="2bc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，重要的是要知道什么时候主成分分析产生的主成分是无用的:什么时候你的特征彼此不相关。</p><p id="3a0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，验证PCA有用性的第一个方法是验证数据集的不相关程度。如果它是非常不相关的，那么你有一个不应用主成分分析的好理由。</p><p id="1541" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有几个指标可以用来评估这一点。这里，我将关注两个有界的度量。无界度量需要一个比较度量，因此更难处理。</p><h2 id="c6ea" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">分散系数</h2><p id="1b51" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">该度量的几何解释是，数据集在空间上创建的超大体积随着数据集变量之间的相关性增加而减小。因此，我们期望该度量的非常低的值将指示使用PCA是一个好的选择。</p><p id="917f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从代数的角度来看，为了计算这个度量，我们只需要计算数据集的相关矩阵的行列式。此指标的最大值是空间的维数，最小值为零。</p><p id="61fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段包含使用和不使用标准化来计算该指标的代码:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="4176" class="nd mh iq nq b gy nu nv l nw nx">def scatter_coefficient(X, normalize=True):<br/>    corr = np.corrcoef(X, rowvar=False)<br/>    if normalize:<br/>        return np.linalg.det(corr) / np.trace(corr)<br/>    else:<br/>        return np.linalg.det(corr)</span></pre><p id="4017" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种度量与另一种称为广义方差的度量非常相似，它使用协方差矩阵而不是相关性。</p><h2 id="703c" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">Psi指数</h2><p id="d39f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">该度量寻找从相关矩阵获得的特征值的幅度。如果变量不相关，每个PC倾向于解释与单个变量一样多的方差，并且它们的特征值倾向于1。因此，越靠近y = 1行，面积越小，数据集的不相关程度越高。</p><p id="bacb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于此指标，值越大越好。其最大值为p(p-1)，最小值为零。它由以下等式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8806b3be1e6a6080c75fd7ecf928acd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*ZkMzBsldX_WDcHrEcGbdBQ.png"/></div></figure><p id="1be6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用下面的代码很容易地计算出来:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="d753" class="nd mh iq nq b gy nu nv l nw nx">def psi_index(X, normalize=False):<br/>    corr = np.corrcoef(X, rowvar=False)<br/>    <br/>    # Eigenvalues and eigenvectors from the correlation matrix<br/>    eig_val, eig_vec = np.linalg.eig(corr)<br/>    idx = eig_val.argsort()[::-1]<br/>    eig_val = eig_val[idx]</span><span id="54aa" class="nd mh iq nq b gy nz nv l nw nx">if normalize:<br/>        p = X.shape[0]<br/>        return np.sum((eig_val - 1)**2) / (p*(p-1))<br/>    else:<br/>        return np.sum((eig_val - 1)**2)</span></pre><h1 id="f2df" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">选择主成分</h1><p id="8cc4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">选择要使用的主成分的通常方法是设置解释方差的阈值，例如80%，然后选择产生尽可能接近该阈值的解释方差的累积和的成分的数量。</p><p id="9c9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法有两个主要问题:</p><ul class=""><li id="421d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">它需要一个阈值的主观选择。在大多数情况下，选择80%或90%的门槛没有公平的动机，它们是任意的。</li><li id="2d4f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方差某些部分可能是纯噪声而不是信号。你无法事先知道，你选择的这个阈值是否只去除了噪声，如果是，去除了多少，或者你实际上是否去除了信号。</li></ul><p id="7963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择组件数量的一种方法是使用排列测试。</p><p id="5e6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">置换测试是统计学中一种非常常见的测试，它包括置换您正在评估的实例，以查看您从假设测试中获得的见解是否仍然成立。为了更好地解释排列测试，我强烈推荐<a class="ae kv" href="https://www.jwilber.me/permutationtest/" rel="noopener ugc nofollow" target="_blank">这个网站</a>。</p><p id="92e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在处理PCA时，策略如下:</p><ul class=""><li id="af72" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">对您的数据运行PCA，并保存每个主成分解释的方差</li><li id="ee5d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">定义多个测试，例如1000个，并且对于每个测试，不替换地抽取彼此独立的数据集列</li><li id="a944" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对这些测试中的每一个再次运行PCA，并保存每个主成分解释的方差</li><li id="1c11" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将原始数据集的解释差异与置换版本进行比较</li></ul><p id="a910" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里的想法是，通过对数据集的列进行采样，我们将去相关特征，因此，在这个新的采样数据集上，PCA不应该生成良好的变换。因此，我们期望，如果主成分是相关的，在PC上置换解释的方差不应该大于最初解释的方差。</p><p id="34f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把它编码起来，看看我们能得到什么。</p><h2 id="60da" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">编码排列测试</h2><p id="3103" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，让我们创建一个函数来消除数据列的相关性:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a16b" class="nd mh iq nq b gy nu nv l nw nx">def de_correlate_df(df):<br/>    X_aux = df.copy()<br/>    for col in df.columns:<br/>        X_aux[col] = df[col].sample(len(df)).values<br/>        <br/>    return X_aux</span></pre><p id="45fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们保存我们最初解释的方差比:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="f55b" class="nd mh iq nq b gy nu nv l nw nx">pca = PCA()<br/>pca.fit(df)<br/>original_variance = pca.explained_variance_ratio_</span></pre><p id="b127" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将定义测试的数量，并创建一个矩阵来保存我们的实验结果:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c4cc" class="nd mh iq nq b gy nu nv l nw nx">N_permutations = 1000<br/>variance = np.zeros((N_permutations, len(df.columns)))</span></pre><p id="192c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们生成新的数据集并保存结果:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a06c" class="nd mh iq nq b gy nu nv l nw nx">for i in range(N_permutations):<br/>    X_aux = de_correlate_df(df)<br/>    <br/>    pca.fit(X_aux)<br/>    variance[i, :] = pca.explained_variance_ratio_</span></pre><p id="98c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了这些，我们就可以计算p值，看看哪些成分是相关的:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="078f" class="nd mh iq nq b gy nu nv l nw nx">p_val = np.sum(variance &gt; original_variance, axis=0) / N_permutations</span><span id="4d2d" class="nd mh iq nq b gy nz nv l nw nx">fig = go.Figure()<br/>fig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=p_val, name='p-value on significance'))</span><span id="9eb4" class="nd mh iq nq b gy nz nv l nw nx">fig.update_layout(title="PCA Permutation Test p-values")</span></pre><p id="025d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果将此应用于在BSD许可下在<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>上免费获得的乳腺癌数据集[2],结果将是前5个主成分是相关的，如图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/9b35695e1283c52cf134a50a59928cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGfWbcU8rtpDu6k2vZkkjg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在乳腺癌数据集上获得置换检验的p值。由作者创作。</p></figure><p id="2e1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">链接的笔记本有一些使用这种方法的实验，我强烈建议读者看看这在实践中的工作。事实上，这种方法似乎更适用于具有更好有用性度量的数据集。</p><h1 id="519b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">解释主要成分</h1><p id="b321" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">当手头的问题需要解释的时候，我在使用PCA时看到了很多阻力。这是一个合理的阻力，因为，例如，如果你要用SHAP来解释你的模型，它将输出主成分对模型的影响，而不是直接输出单个特征的影响，在大多数情况下，这是我们感兴趣的。</p><p id="716b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在我们的PCA中增加一点可解释性可以在日常生活中给我们很大帮助。首先，我们将看一个可以用来估计每个主成分与我们的每个特征的相关程度的度量。</p><h2 id="f677" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">装货</h2><p id="5cd6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">首先，我们需要理解载荷的概念。假设每个分量是变量的线性组合，我们有:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4eaa89dd8843de87828bb17261eddeb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*ZZrAPNkzm7I3lZfBxS2L8g.png"/></div></figure><p id="94a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个组合的重量就是我们所说的主分量的载荷。如你所见，它们表明了每个变量在每个主成分上的权重。</p><p id="66ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">寻找“归一化”载荷的一种常用方法是将特征向量乘以特征值的平方根。</p><h2 id="eaef" class="nd mh iq bd mi ne nf dn mm ng nh dp mq lf ni nj ms lj nk nl mu ln nm nn mw no bi translated">负载指数(IL)</h2><p id="b88d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这是我们将用来估计每个变量和每个主成分之间的相关性的指标。它将使用载荷的平方值和特征值的平方值，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e05f101362c76c3723cc2844faad5417.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*xn3AjUBhZ-mK00e0J0JUAw.png"/></div></figure><p id="1a2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="od"> sj </em>分量是变量<em class="od"> j的标准差，uij </em>是第j个变量对第I个PC的加载。</p><p id="f423" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定协方差矩阵，此指标计算起来很简单，下面的代码片段有一个非优化的实现:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="49c9" class="nd mh iq nq b gy nu nv l nw nx">def index_of_loadings_matrix(cov_matrix):<br/>    eig_val, eig_vec = np.linalg.eig(cov_matrix)<br/>    r = np.zeros((len(eig_vec), cov_matrix.shape[0]))<br/>    for i in range(len(eig_vec)):<br/>        for j in range(cov_matrix.shape[0]):<br/>            r[i][j] = (eig_vec[i][j]**2 * eig_val[i])**2 / np.sqrt(cov_matrix[j][j])</span><span id="e172" class="nd mh iq nq b gy nz nv l nw nx">return r</span></pre><p id="abf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将返回一个矩阵，其中估计了变量和PCs之间的“相关性”。</p><h1 id="2443" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="2097" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这里介绍的技术可以帮助我们在分析和日常建模实践中更好地利用PCA。</p><p id="d48f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有更多的评估有用性的方法，更多类型的排列测试，以及更多解释结果的方法。链接笔记本提供了一些其他的度量和方法，以及一些数据集的初步分析。</p><p id="9722" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望通过这篇文章和笔记本，你可以开始提高你对这个工具的知识，而不仅仅是入门课程中通常教授的知识。</p><p id="3e72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] Vieira，Vasco，估计主成分分析显著性的排列检验(2012)。计算生态学和软件。2.103–123.</p><p id="909f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] O. L. Mangasarian和W. H. Wolberg:“通过线性规划进行癌症诊断”，《暹罗新闻》，第23卷，第5期，1990年9月，第1和18页。</p></div></div>    
</body>
</html>