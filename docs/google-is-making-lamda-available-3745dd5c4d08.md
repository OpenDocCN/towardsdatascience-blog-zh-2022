# 谷歌正在提供 LaMDA

> 原文：<https://towardsdatascience.com/google-is-making-lamda-available-3745dd5c4d08>

## 你将能够自己判断它是否有知觉

![](img/cf6f20ee9e0bd1749d8ee3a990020918.png)

鸣谢:Lumartist#7282 via Midjourney(没错，这是 AI 生成的)

两个月前，全世界都在关注一件事:谷歌的人工智能系统 LaMDA。一场关于感知和人工智能明显技能的对话吸引了人们长达两周的注意力。现在，谷歌决定是时候向世界开放这个系统了。如果你参加了关于人工智能的辩论，这条新闻会让你感兴趣。

## 简要回顾:LaMDA 和 AI sentience

除了 GPT-3， [LaMDA](https://blog.google/technology/ai/lamda/) (对话应用的语言模型)可以说是最著名的大型语言模型(LLM)。谷歌在 [2021 年 I/O 主题演讲](https://www.youtube.com/watch?v=nP-nMZpLM1A&t=2381s&ab_channel=Google)上宣布了这一消息，但它只是在去年 6 月引起了公众的注意，当时《华盛顿邮报》记者妮塔莎·蒂库发表了[一篇关于布雷克·莱莫因(现谷歌前工程师)和他对 LaMDA 感知的说法的报道](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)。

莱莫因的信念——加上 LLM 明显的语言掌握——引发了一场关于意识、拟人化和人工智能炒作的激动人心的对话。当时我给了[我的两分钱](https://thealgorithmicbridge.substack.com/p/why-is-lamda-sentient-is-an-empty)。我没有参与支持或反对莱莫因立场的辩论，而是采取了不同的视角。我解释说，问 LaMDA 是否有知觉，这本身就是一个空洞的问题。我们没有合适的术语或合适的工具来认为这是一个科学问题——它是不可证伪的，因此“[甚至没有错。](https://en.wikipedia.org/wiki/Not_even_wrong)”

正如妮塔莎·蒂库所写的，莱莫因澄清说，他认为 LaMDA 是一个“以牧师的身份，而不是科学家”的人。尽管如此，他还是花了很多时间试图拿出科学证据来说服他在谷歌的同事。但是他失败了。谷歌的一位发言人总结道:“没有证据表明 LaMDA 是有感情的(有很多证据反对这一点)。”

莱莫因写了一篇文章公开分享了一段可能未经编辑的与 LaMDA 的对话。我读了它，我看到了和人工智能伦理学家玛格丽特·米歇尔看到的一样的东西，“一个计算机程序，而不是一个人。”然而，尽管事实上大多数人不同意莱莫因的观点，LaMDA 的知名度越来越高，许多人想得到它，亲眼看看到底是怎么回事。现在，谷歌正在让这成为可能。

## 谷歌正在提供 LaMDA

该公司计划通过 [AI Test Kitchen](https://aitestkitchen.withgoogle.com/) 发布 LaMDA，这是一个旨在让人们“了解、体验和反馈”该模型的网站(可能还有其他人，如 PaLM)。他们承认 LaMDA 还没有准备好在世界上部署，并希望通过真人的反馈来改进它。人工智能测试厨房不像 GPT 3 的游乐场那样开放——谷歌已经建立了三个主题演示，但一旦他们积极地重新评估 LaMDA 为开放对话做好准备，将有望解除限制。

该模型是 LaMDA 的第二个版本，比桑德尔·皮帅去年在主题演讲中展示的有所改进。LaMDA 2 可以进行创造性和引人入胜的对话，并像第一代一样“即时”做出回应，但在安全性、实用性和对话质量(分为敏感性、特异性和趣味性)方面有所改善，变量[由谷歌精心定义](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html)。

为了有资格使用它，你可以注册[一个等候名单](https://aitestkitchen.withgoogle.com/signup) (OpenAI 的风格)。谷歌将让“小群体的人逐渐”进入。在使用 LaMDA 时要小心，因为谷歌已经声明他们可以使用你的对话来改进产品和服务，人类评论者将能够“阅读、注释和处理”你的聊天(如果你不希望谷歌存储它们，你可以在退出演示之前删除你的会议)。不用说，不要在你的交流中包含任何个人信息。除此之外，你不应该明确地写性(例如，你不能描述性场景)，充满仇恨(例如，你不能使用诽谤)，或者是非法的(例如，你不能宣传恐怖主义)。

就目前而言，AI 测试厨房提供了三个演示:想象、列举、谈论(狗版)。在第一个故事中，你从一个地方开始，LaMDA 将“提供探索你想象力的途径。”在第二篇中，LAMDA 将尝试将一个主题或目标分解成子任务。第三个是一个开放式的环境，你可以谈论任何事情，LaMDA 会尝试将话题转回到狗身上。

谷歌已经实施了一套过滤器来降低 LaMDA 产生不适当或不准确响应的概率，但他们并没有完全消除这些风险(用当前的方法是不可能的)。对于 GPT-3、J1-Jumbo 和任何其他公开可用的 LLM 也是如此。

虽然谷歌已经建立的演示不是成品，但它们标志着该公司最终将 LaMDA 嵌入搜索和谷歌助手等服务的意图。正如我在之前的一篇文章中所说的，科技公司将 LLM 整合到现有服务中只是时间问题——这就是为什么我们应该仔细评估这项技术是否已经准备好了。

让我来说明这是一个怎样的问题。一方面，LaMDA(以及 GPT-3 和其他 LLM)会产生有害的输出和事实上不准确的信息。另一方面，它掌握了创造人格幻觉的能力——正如莱莫因的主张所暗示的那样。想象一下，如果 LaMDA 为 Google Assistant 提供动力，会产生什么样的下游影响。人们可能是模型的毒性和偏见的受害者，他们会盲目地相信它说的话，尽管模型有幻觉的倾向，而且他们可能——就像莱莫因发生的那样——对它产生依恋。

我们已经一次又一次地看到大型科技公司如何容易地失去对这些强大的人工智能模型的控制(谷歌和 Meta 不断地为他们模型的不当行为道歉——这是一种后验行为)。如果连他们都不能准确预测 LLM 的不当行为，外行人又有多没准备好与他们健康互动呢？

## 最后的想法

谷歌正在大步将其新兴的人工智能技术部署到现实世界的日常产品中。AI 测试厨房就是这么回事。但是他们也声称关心安全和负责任的发展。他们知道 LaMDA 可能非常有用，但也可能无法满足用户的期望，甚至更糟，以这样或那样的方式造成伤害。这就是为什么他们希望反馈有所改善。

我相信，正如[在](http://blenderbot.ai)之前发生的那样，反馈不足以让 LaMDA 准备好投入生产。透明的研究非常重要(谷歌还没有部署 LaMDA，只是在收集人们的反馈)，因此在这一层面进行监管会适得其反。然而，从研发到生产的过渡并不简单，应该小心谨慎。随着这些模型走向世界，公司将失去对它们的治理。但是，只要利润能弥补安全的不足——甚至是可能的负面新闻——他们就会这么做。这就是为什么对这些强大人工智能的[监管](https://techcrunch.com/2022/09/06/the-eus-ai-act-could-have-a-chilling-effect-on-open-source-efforts-experts-warn/)应该到位，至少在应用层面。

即使谷歌不打算在短期内将 LAMDA 集成到其服务中，与它交谈也可能是一种有趣和令人大开眼界的体验，即使对那些厌倦了玩 GPT 3 的人来说也是如此。无论 LaMDa 是否超过了谷歌为其设定的生产资格门槛，你都可以自己评估 LaMDA 确实没有感知能力。

*订阅* [**算法桥**](https://thealgorithmicbridge.substack.com/) *。弥合算法和人之间的鸿沟。关于与你生活相关的人工智能的时事通讯。*

*您也可以直接支持我在 Medium 上的工作，并通过使用我的推荐链接* [**这里**](https://albertoromgar.medium.com/membership) 成为会员来获得无限制的访问权限！ *:)*