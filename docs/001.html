<html>
<head>
<title>3 ways to deal with large datasets in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python处理大型数据集的3种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-ways-to-deal-with-large-datasets-in-python-9a80786c4182#2022-01-01">https://towardsdatascience.com/5-ways-to-deal-with-large-datasets-in-python-9a80786c4182#2022-01-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="e548" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">用Python处理大型数据集的3种方法</h1></div><p id="4094" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">作为一名数据科学家，我发现自己越来越需要处理“<em class="ko">大数据”</em>。我戏称为<em class="ko">的大数据</em>对应的数据集，虽然并不真的那么大，但却大到足以让我的计算机努力处理它们，并真的减慢一切。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/188e54a69d2943052f459671005b40e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTwZq7ceSHClNuAconBNig.jpeg"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">米卡·鲍梅斯特在<a class="ae lf" href="https://unsplash.com/collections/9683625/big-data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1d7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个问题并不新鲜，就像所有事情一样，不存在放之四海而皆准的神奇公式。最佳方法将取决于您的数据和应用程序的目的。然而，最<a class="ae lf" rel="noopener" target="_blank" href="/loading-large-datasets-in-pandas-11bdddd36f7b">流行的</a> <a class="ae lf" href="https://www.codementor.io/@guidotournois/4-strategies-to-deal-with-large-datasets-using-pandas-qdw3an95k" rel="noopener ugc nofollow" target="_blank">解决方案</a>通常属于下述类别之一。</p><h1 id="2694" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">1.通过优化数据类型减少内存使用</h1><p id="5aac" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">当使用Pandas从文件中加载数据时，它会自动推断数据类型，除非另有说明。大多数情况下，这样做很好，但是推断的类型不一定是优化的。此外，如果数字列包含缺失值，那么推断的类型将自动为float。</p><p id="a63b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最近使用这个方法<a class="ae lf" rel="noopener" target="_blank" href="/an-analysis-of-daily-mortality-in-france-during-the-covid19-pandemic-95286928e80c"/>进行了一项分析，我主要处理表示年、月或日的整数类型:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mj"><img src="../Images/2c6765863ee4c1c0e53ffa67e3e087e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y42pld3_CykZDRZD174CDQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">使用Pandas加载文件并指定dtypes(图片由作者提供)</p></figure><p id="4436" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于这种特殊的情况，指定数据类型可以大大减少所使用的内存。请注意，在上面的例子中，我使用了熊猫类型<a class="ae lf" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Int16Dtype.html" rel="noopener ugc nofollow" target="_blank">熊猫。Int16Dtype </a>强制包含缺失值的列为Int类型。这是通过使用熊猫在内部实现的。NA，而不是numpy.nan</p><p id="a0cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/optimize-pandas-memory-usage-while-reading-large-datasets-1b047c762c9b">处理大型数据集时，优化数据类型</a>需要事先了解您正在处理的数据。在未知数据集的纯探索阶段，它可能没有用。</p><h1 id="504e" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">2.将数据分割成块</h1><p id="2a58" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">当数据太大而不适合内存时，您可以使用Pandas的<em class="ko"> chunksize </em>选项将数据分割成块，而不是处理一个大块。使用此选项创建一个<a class="ae lf" href="https://wiki.python.org/moin/Iterator" rel="noopener ugc nofollow" target="_blank"> <em class="ko">迭代器</em> </a> <em class="ko"> </em>对象，该对象可用于遍历不同的块并执行过滤或分析，就像加载完整数据集时所做的一样。</p><p id="48f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是一个使用该选项遍历<a class="ae lf" href="https://www.yelp.com/dataset" rel="noopener ugc nofollow" target="_blank"> Yelp评论数据集</a>的示例，提取每个块的最小和最大评论日期，然后重建评论的完整时间跨度:</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="4e52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从最初的探索性分析到<a class="ae lf" rel="noopener" target="_blank" href="/strategies-to-train-out-of-memory-data-with-scikit-learn-7b2ed15b9a80">模型训练</a>，都可以使用分块，并且只需要很少的额外设置。</p><h1 id="027d" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">3.利用懒惰评估</h1><p id="b966" class="pw-post-body-paragraph jq jr it js b jt me jv jw jx mf jz ka kb mg kd ke kf mh kh ki kj mi kl km kn im bi translated">惰性求值指的是将表达式的求值延迟到实际需要该值时进行的策略。<a class="ae lf" href="https://data-flair.training/blogs/apache-spark-lazy-evaluation/" rel="noopener ugc nofollow" target="_blank">惰性求值</a>是一个重要的概念(尤其在函数式编程中使用)，如果你想了解更多关于它在Python中的不同用法，你可以从这里开始<a class="ae lf" rel="noopener" target="_blank" href="/what-is-lazy-evaluation-in-python-9efb1d3bfed0"/>。</p><p id="1e60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">惰性评估是构建分布式计算框架的基础，如<a class="ae lf" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Spark </a>或<a class="ae lf" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask </a>。尽管它们是为集群设计的，但是您仍然可以利用它们在您的个人计算机上处理大型数据集。</p><p id="adb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与Pandas的主要区别在于，它们不直接将数据加载到内存中。相反，在read命令期间发生的是，它们扫描数据，推断数据类型，并将其分成分区(到目前为止没有什么新内容)。计算图是为这些分区独立构建的，只有在真正需要的时候才执行(这就是懒惰的原因)。</p><p id="e184" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在另一篇文章中提供了一个使用Pyspark对大于内存的数据集进行探索性分析的例子，并在另一篇文章中写了关于<a class="ae lf" href="https://medium.com/@georgiadeaconu/understanding-core-concepts-in-apache-spark-73dce67a5f73" rel="noopener"> Spark核心原则</a>的内容，所以我在这里不再重复。<a class="ae lf" href="https://pythonspeed.com/articles/faster-pandas-dask/" rel="noopener ugc nofollow" target="_blank"> Dask </a>也很受欢迎<a class="ae lf" href="https://examples.dask.org/machine-learning/incremental.html" rel="noopener ugc nofollow" target="_blank">例子</a>不难找到(要比较两者，你可以从<a class="ae lf" href="https://docs.dask.org/en/latest/spark.html" rel="noopener ugc nofollow" target="_blank">这里开始</a>)。Dask的语法模仿了Pandas的语法，所以看起来很熟悉，但是，它仅限于Python的使用，而Spark也可以在Java或Scala中工作。</p><p id="dad1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他库，如<a class="ae lf" rel="noopener" target="_blank" href="/4-python-libraries-that-ease-working-with-large-dataset-8e91632b8791"> Vaex或Modin </a>提供了类似的功能，但我个人没有使用过它们。</p></div></div>    
</body>
</html>