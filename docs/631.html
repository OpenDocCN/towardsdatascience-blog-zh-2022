<html>
<head>
<title>Getting more bang for your buck out of pre-trained language models with GroupBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GroupBERT从预先训练的语言模型中获得更多回报</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-more-bang-for-your-buck-out-of-pre-trained-language-models-with-groupbert-cf16953d7c0d#2022-01-21">https://towardsdatascience.com/getting-more-bang-for-your-buck-out-of-pre-trained-language-models-with-groupbert-cf16953d7c0d#2022-01-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="a413" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用GroupBERT从预先训练的语言模型中获得更多回报</h1></div><div class=""><h2 id="582d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Graphcore IPU上实现2倍的训练速度</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/431cb73bce19c5ecf274891a1174e5e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8EdITLfTtDsX6_TPrGkAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者Graphcore</p></figure><p id="bc3c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated">ERT已经成为当今最流行、最通用的人工智能模型之一。然而，BERT对密集运算的依赖意味着它的准确性和灵活性需要很高的计算成本。</p><p id="e825" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了应对这一挑战，Graphcore Research开发了GroupBERT，这是一种基于BERT的新模型，它使用分组转换，非常适合我们的情报处理单元(IPU)。GroupBERT将增强的转换器结构与高效的分组卷积和矩阵乘法相结合，使IPU用户能够有效地将模型中的参数数量减半，并将训练时间减少50%，同时保持相同的精度水平。</p><h1 id="162a" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">由IPU解锁的增强型BERT</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/2f98c92f9c25ceef4006530595760079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hq0fUg0_tgo-bzQIxLatag.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图GroupBERT模型结构示意图。图片作者。</p></figure><p id="e166" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的论文“<a class="ae mt" href="https://arxiv.org/abs/2106.05822" rel="noopener ugc nofollow" target="_blank"> GroupBERT:具有高效分组结构的增强型Transformer Architecture</a>”[1]展示了IPU如何允许Graphcore Research探索非常高效和轻量级的构建块，以推导出在非常大的文本语料库上更有效地进行掩蔽预训练的Transformer编码器结构。</p><p id="2cd8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">GroupBERT利用分组转换。我们用分组矩阵乘法来扩充全连接模块，并在变换器的结构中引入一个新的卷积模块。因此，每个GroupBERT层都扩展到四个模块，而不是像最初的BERT中那样扩展到两个模块。</p><p id="1286" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用GroupBERT，我们可以看到通过验证损失衡量的FLOPs和任务性能之间的权衡得到了显著改善。为了达到相同的损失值，GroupBERT只需要不到常规BERT模型一半的FLOPs，后者只使用密集运算，没有利用其深度潜力。</p><p id="fd08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增加的深度和减少的<a class="ae mt" href="https://www.graphcore.ai/posts/delving-deep-into-modern-computer-vision-models" rel="noopener ugc nofollow" target="_blank">运算强度</a>【2】的组伯特的组成部分强调了内存访问。与密集操作相比，对于给定的输入激活张量，分组操作执行较少的FLOPs。为了利用计算，这些低算术强度运算需要比密集计算更快的数据访问。由于IPU允许所有权重和激活存储在具有极高47.5 TB/s带宽的片内SRAM中，因此它可以将GroupBERT的理论效率优势转化为各种型号的实际训练时间减少。</p><h1 id="6dff" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">分组的完全连接的层</h1><p id="4240" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">原变压器编码器层由两个模块组成:多头关注(MHA)和全连接网络(FFN)。许多工作都致力于提高其效率。<a class="ae mt" href="https://arxiv.org/abs/2009.06732" rel="noopener ugc nofollow" target="_blank"> Tay等人(2020)</a>【3】概述了各种不同的方法，其中大多数修改集中在减少MHA模块对序列长度的二次计算依赖性。然而，在BERT中，大部分计算是以相对适中的序列长度128来执行的，并且FFN模块消耗了迄今为止最多的资源，在模型执行期间贡献了近三分之二的FLOPs。</p><p id="04ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">FFN模块的结构非常简单:它由两个矩阵和一个非线性组成。第一个矩阵将表示投影到更高维度，通常比模型隐藏表示大四倍。这个维度扩展操作之后是非饱和激活函数，其执行表示的非线性变换并将其稀疏化。最后，通过下投影矩阵将稀疏表示缩减到模型维数。</p><p id="7db7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">GroupBERT介绍了一种新颖的方案，使FFN计算更便宜、更快。一个关键的观察结果是，由分组引起的稀疏性最好应用于接收稀疏输入的矩阵。因此，我们只在第二个下投影矩阵中引入分组矩阵乘法，使其分块对角化。然而，分组操作在传入的隐藏表示上引入了局部性约束，限制了转换期间的通道信息传播。为了解决这个问题，我们将组与输出投影混合在一起，类似于MHA块，它也为多个头部划分隐藏表示。</p><p id="11f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总的来说，这种分组FFN (GFFN)方案允许GroupBERT将FFN层的参数数量减少25%,而任务性能的降低最小。这与所有以前尝试使用分组变换的方法形成对比，但是将它应用于两个矩阵。这导致不相交的隐藏表示，并导致显著的性能下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/6bcfeacc259950d624eef83d0b3c3c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiCK4CTviY0Z_ZpHpQjlkA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图2: GFFN使用一种新颖的方案来应用分组矩阵乘法，既保持了性能又节省了计算。图片作者。</em></p></figure><h1 id="1960" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">作为注意力补充的分组卷积</h1><p id="4f08" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">计算BERT中使用的序列长度的全部到全部的注意力不会导致显著的计算开销。然而，最近的一项研究(<a class="ae mt" href="https://arxiv.org/abs/1911.03584" rel="noopener ugc nofollow" target="_blank"> Cordonnier et al. 2020 </a> ) [4]表明，首先，对于语言模型来说，只使用多头注意力可能是多余的。变换器层中注意力头的子集折叠成卷积模式，以单独模拟本地令牌交互。</p><p id="2502" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了减少计算密集注意力图来模拟序列中的局部相互作用的冗余，GroupBERT引入了专用的卷积模块。分组卷积充当滑动窗口，混合彼此接近的单词标记之间的信息。然后，我们用一个附加的GFFN来扩展编码器，以跟随卷积模块，确保每个令牌处理模块都与一个特征处理GFFN模块相耦合。</p><p id="a64f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有了这些附加模块，本地令牌交互就有了一个专用的轻量级模型元素。这反过来使得MHA注意力仅在模拟远程交互时表现得更好，因为花费在本地令牌交互上的注意力容量更少。图3显示了训练前验证集上BERT和GroupBERT的注意力图。这是一个清晰的可视化，说明了利用回旋如何让注意力在长距离互动中更有效，因为这种模式更平滑、更分散。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/5e755b0ee906a2154d462445a46819a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cbdOuqVMiL4HzKGRlB-uA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图3:</em><em class="na">BERT(左)和GroupBERT(右)从验证集数据中生成的注意力地图。GroupBERT注意力较少依赖于相对位置，这使它能够更好地专注于长期互动。图片作者。</em></p></figure><h1 id="c338" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">让模型参数发挥作用</h1><h2 id="a1be" class="nc mb iq bd mc nd ne dn mg nf ng dp mk le nh ni mm li nj nk mo lm nl nm mq nn bi translated"><strong class="ak">标准化</strong></h2><p id="c774" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">许多工作已经考虑了在变压器中应用标准化的最佳方法。虽然层规格化总是规格化的首选方法，但它有两种应用方式:前规格化和后规格化。</p><p id="cfb6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PostNorm对跳过连接和残差之和的输出进行归一化，而PreNorm在应用任何投影之前对残差分支的表示进行归一化，如图4所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/e7cf82cc4aa104b1e7a98dc10258180b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0keBWNWiJR3umj2lJ2syw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图4:后置规格化(左)和前置规格化(右)将层规格化(LN)函数定位在相对于剩余和跳过连接的不同位置。图片作者。</em></p></figure><p id="b2bf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">BERT的标准实现使用PostNorm，在使用默认学习率的设置下，它的好处是提供比PreNorm更强的任务性能。然而，我们发现前范式配置明显更稳定，可以使用更高的学习率，这是后范式模型所不具备的。</p><p id="623d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">GroupBERT使用PreNorm并实现了优异的任务性能，因为该模型现在可以稳定地适应4倍于PostNorm基线的学习速率增长，而post norm基线在如此高的学习速率下会出现偏差。更高的学习速率对于增加模型泛化和实现更好的收敛速率是重要的。</p><p id="0162" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更高的学习率不会直接带来计算上的节省。然而，实现更高的任务性能将需要更大的模型，从而导致更高的计算成本。因此，通过增加模型稳定性来实现更高的学习率增加了利用模型参数的效率。</p><h2 id="8118" class="nc mb iq bd mc nd ne dn mg nf ng dp mk le nh ni mm li nj nk mo lm nl nm mq nn bi translated"><strong class="ak">辍学</strong></h2><p id="b8d4" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">许多基于Transformer架构的语言模型受益于使用dropout，因为它减少了对训练数据集的过度拟合，并有助于泛化。然而，BERT是在非常大的数据集上预先训练的，在这种情况下，过度拟合通常不是问题。</p><p id="78b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">出于这个原因，GroupBERT在预培训期间取消了辍学。丢弃掩码乘法中涉及的触发器可以忽略不计，因此也可以将其视为触发器中立优化。然而，生成丢弃掩码会有很大的吞吐量开销，因此在BERT中删除这种正则化方法有利于更快的模型执行和更好的任务性能。</p><p id="b0a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然在没有退出的情况下对维基百科数据集执行预训练是有利的，但退出仍然是微调GroupBERT期间的重要工具，因为微调数据集比预训练语料库小几个数量级。</p><h1 id="027b" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">IPU变压器</h1><p id="7929" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">在将所有概述的修改组合到一个模型中之前，我们必须验证每个组件的有效性。然而，成组的ffn减少了模型flop的数量，而添加卷积模块增加了flop。因此，直接与BERT进行比较是不公平的，因为模型会消耗不同数量的资源。为了确定不同大小的模型增强的质量，我们必须将它们与不同大小的BERT模型之间的对数插值进行比较:中等、基本和大。</p><p id="2fdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图5显示了引入GroupBERT的每个增强的局部消融研究。与基线BERT相比，模型的每一次添加都会提高帕累托效率。当组合时，GroupBERT基本模型实现了与BERT大模型相同的MLM验证损失，即使它只有不到50%的参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f62ea5752cef99672524904791197378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CF8GrHYzfc0IdDiH_uWDFQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图5: GroupBERT消融研究。GroupBERT中引入的所有增强提高了BERT的帕累托效率，因为它们实现了更低的验证MLM损失。图片作者。</em></p></figure><p id="83b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了明确地得出GroupBERT优于BERT的结论，我们证明了GroupBERT基相对于BERT Large的优势在大范围的模型尺度上持续存在。我们用两个额外的模型尺寸来补充GroupBERT基本模型，以创建一个连续的Pareto前沿。这种方法，如图6所示，使我们能够展示GroupBERT相对于BERT以及其他试图在Transformer中使用分组转换的模型的持续优势。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/63332f12435f8b1ec1d0ffad9deae21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*QQwFIWxkjvHG2rd7I3WIQA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图6:针对FLOPs的训练前验证准确性</em>。图片作者。</p></figure><p id="1d8e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于GroupBERT是IPU本地模型，理论上在FLOPs方面实现的节省也转化为达到给定验证准确度水平所需的更好的端到端预训练时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/a2e20cfae4c125755db50fb915b22bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZLyxFz4-rxno16G8QKvqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="na">图7:在IPU-POD16上，不同优化器的GroupBERT Base与BERT Large的训练时间。所有模型都达到了同等的训练前验证精度。图片作者。</em></p></figure><h1 id="10c6" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">关键要点</h1><p id="bfde" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">我们的结果表明:</p><ul class=""><li id="f6e0" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq nw nx ny nz bi translated">分组转换可以创建更有效的模型</li><li id="f382" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq nw nx ny nz bi translated">将卷积与密集的全部对全部的注意力结合起来，即使对于需要长距离交互的任务也有有益的影响</li><li id="8f83" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq nw nx ny nz bi translated">IPU将理论上的浮点运算节省转化为计算时间的实际节省</li><li id="8810" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq nw nx ny nz bi translated">通过架构变化，而不是简单的模型缩放，可以实现显著的性能提升</li></ul><p id="effc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总之，在预训练性能方面，GroupBERT可以实现比原始BERT高达2倍的改进，从而实现更快、更高效的计算。</p><p id="9bf8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mt" href="https://arxiv.org/abs/2106.05822" rel="noopener ugc nofollow" target="_blank">阅读论文</a> <br/> <a class="ae mt" href="https://github.com/graphcore/examples/tree/master/nlp/bert/tensorflow1" rel="noopener ugc nofollow" target="_blank">访问GitHub上的代码</a></p><h1 id="256c" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">谢谢你</h1><p id="53dc" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">感谢Daniel Justus、Douglas Orr、Anastasia Dietrich、Frithjof Gressmann、Alexandros Koliousis、Carlo Luschi和David Svantesson，他们也为这项研究做出了贡献，感谢Graphcore的其他同事给予的支持和见解。</p><h1 id="3acb" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">参考</h1><p id="aebd" class="pw-post-body-paragraph kv kw iq kx b ky mu jr la lb mv ju ld le mw lg lh li mx lk ll lm my lo lp lq ij bi translated">[1] I. Chelombiev，D. Justus，D. Orr，A. Dietrich，F. Gressmann，A. Koliousis，C. Luschi，<a class="ae mt" href="https://arxiv.org/abs/2106.05822" rel="noopener ugc nofollow" target="_blank"> GroupBERT:具有高效分组结构的增强型变压器架构</a> (2021)，arXiv</p><p id="d4bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] D. Masters，<a class="ae mt" href="https://www.graphcore.ai/posts/delving-deep-into-modern-computer-vision-models" rel="noopener ugc nofollow" target="_blank">深入研究现代计算机视觉模型</a> (2020)，graphcore.ai</p><p id="e7df" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] Y. Tay，M. Dehghani，D. Bahri，D. Metzler，<a class="ae mt" href="https://arxiv.org/abs/2009.06732" rel="noopener ugc nofollow" target="_blank">高效变压器:调查</a> (2020年)，arXiv</p><p id="b085" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4] J.B. Cordonnier，A. Loukas，M. Jaggi，<a class="ae mt" href="https://arxiv.org/abs/1911.03584" rel="noopener ugc nofollow" target="_blank">论自我注意与卷积层的关系</a> (2020)，arXiv</p></div></div>    
</body>
</html>