# 机器学习系统中的反馈回路

> 原文：<https://towardsdatascience.com/feedback-loops-in-machine-learning-systems-701296c91787>

## 在机器学习系统设计中设计反馈回路

![](img/6fe168abeb43c895d2c367e182c1bea5.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由[Tine ivani](https://unsplash.com/@tine999?utm_source=medium&utm_medium=referral)拍摄的照片

# 介绍

在机器学习系统中，我们经常从模型的环境中接收某种反馈，然后反馈到系统中。这可以采取多种形式，例如使用模型的输出来训练模型的新版本，或者使用用户对模型决策的反馈来改进模型。

**虽然许多反馈循环是有用的，并且随着时间的推移会提高模型的性能，但随着时间的推移，一些反馈循环会严重降低机器学习系统的性能。**设计有用的反馈回路是机器学习系统设计的重要组成部分，需要深思熟虑地完成，以确保您的系统是可持续的。

在这篇文章中，我将回顾有益的和有害的反馈循环的例子，并解释为什么每种类型都会有这样的结果。

# 有用的反馈循环

一个有益的反馈循环通常涉及**将不带偏见的外部信息**引入你的机器学习系统。这通常以通过与机器学习模型的输出不强烈相关的来源获得标签的形式出现。

这些反馈机制允许系统获得关于系统当前行为的非自我强化的信息。这确保了随着时间的推移，模型不会由于在不反映真实数据分布的有偏数据上重新训练而退化。

在许多情况下，这些外部数据提供了**关于模型何时正确、何时错误的反馈**。这通常通过隐式或显式地允许用户以结构化的方式纠正系统的错误来实现。

## 用户报告

在用户生成内容的平台上，通常有用户报告内容为无关内容、垃圾邮件或攻击性内容的功能。这允许最终用户向机器学习排名和**内容审核系统**提供关于算法发送给他们的内容片段的反馈。

![](img/77a3feec182b05e82c0b2c6b6a36e509.png)

贾斯汀·摩根在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

这方面的一个例子是，Gmail 允许你将收件箱中的邮件报告为垃圾邮件。如果邮件进入了你的收件箱，而不是垃圾邮件文件夹，那么这意味着谷歌的垃圾邮件分类系统不认为这是垃圾邮件。通过自己举报该邮件为垃圾邮件，您向 Google 提供了有价值的反馈，表明该邮件是误报。下一次他们的模型被重新训练时，它现在可以考虑这个反馈，并希望在未来对这种类型的垃圾邮件表现得更好。

## 用户交互日志

在任何涉及**内容排名或购物**的平台上，通常都有某种形式的内容排名系统来决定哪些项目最符合用户的搜索查询或历史参与度。然而，在许多情况下，算法的建议并不完美，用户可能会选择使用未被推荐的项目，或者使用排名较低的项目而不是排名较高的项目。

![](img/ce09ad27917a14ced5a9532faed20199.png)

照片由[马克斯·托马斯](https://unsplash.com/@querysprout?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

来自这些用户交互的日志为排名系统提供了用户真正感兴趣的基本事实。当用户与算法排名不高的东西进行交互时，它会为系统的下一次重新训练提供有价值的反馈。此外，平台拥有的用户交互数据越多，它就越能够针对用户的特定兴趣和特定需求建立档案。

# 有害的反馈循环

机器学习系统中有害的反馈回路通常**通过消除无偏数据的存在来阻止系统随着时间的推移而改进**。

当用于训练模型的数据仅由先前模型的输出确定时，系统无法随着时间的推移而改进。事实上，随着时间的推移，这种情况最有可能导致系统性能下降，因为它将继续根据越来越偏向其自身输出的数据进行训练。

在这种情况下，**系统根本无法纠正其错误**。随着时间的推移，这些误差累积起来，对系统性能的影响越来越大，随着模型被重新训练，其性能逐渐降低。

## 干预

在许多机器学习系统中，模型会触发某种类型的干预，从而改变用户体验。当干预被触发时，一些用户如果无法或不愿意绕过干预，自然会掉线。随着时间的推移，这些用户的减少可能会显著改变标记数据的数据分布。

例如，在**欺诈检测**系统中，具有高风险分值的用户可能需要完成额外的验证。如果验证是有效的，并导致欺诈用户在没有解决它的情况下放弃，那么这些用户将永远不会有机会实际实施欺诈并被贴上欺诈标签。

这个来自 [Stripe](https://stripe.com/) 的视频解释了他们是如何处理这种类型的反馈回路的。他们采用了抵制小组方法，并能够解除他们的机器学习训练和评估数据的偏见。

如果不明确处理这一点，模型的未来训练数据将会有偏差，并且将只包括关于欺诈用户的积极标记的数据点，这些欺诈用户要么 1)解决了验证挑战，要么 2)低于模型的阈值并且从未被挑战。随着时间的推移，这将逐渐使模型的训练数据偏向更“困难”的欺诈，因为“容易”的欺诈会被干预阻止。它还减少了训练数据中(已经很少的)正面例子的数量，因为现有的系统可能会阻止大多数欺诈的发生。

**解决方案:**我们可以通过使用不受干预的小型(例如 1%的流量)[维持组](https://www.analytics-toolkit.com/glossary/holdout-group/)来解决这个问题。在抵制小组中，我们不进行干预，让所有用户在没有挑战的情况下通过，即使他们有风险。然后我们可以看到当我们不需要任何验证时会发生什么；一些用户会真的实施欺诈，而另一些则不会。这个小群体为我们提供了用于训练和测量的无偏数据，而不会因为其规模小而对业务造成重大损害。

## 位置偏差

**搜索排名**系统通常使用用户交互日志来创建带标签的训练数据。在二元分类框架下，如果用户决定与某样东西互动，那么这个标签就是正面的，如果不是，这个标签就是负面的。

这种标记方法会导致偏差，因为与其他内容相比，先前模型排名较高的项目更有可能被交互，即使排名较高的项目实际上并不相关。如果不进行偏差调整，这将导致模型在很大程度上只是记住它过去的决策逻辑(以及它过去的错误)，而不是了解用户的真实口味。

![](img/c586bc7fec680d48b66288ee662bbe7c.png)

Christian Wiediger 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

**解决方案:**这个问题可以通过修改模型来解决。有几种方法可以模拟位置偏差的影响，然后在推理时消除这种影响。例如，[推荐接下来看什么视频:多任务排名系统](https://dl.acm.org/doi/10.1145/3298689.3346997)的文章解释了谷歌是如何为 YouTube 实现这一点的。通过学习对位置的影响进行建模，这些方法可以在推断时传入空值或静态值，以向模型发出信号，表明所有内容都具有相同的位置因素，而是应该基于内容本身进行排序。

谷歌论文作者的这段视频对论文进行了更详细的描述，描述了 YouTube 的整体内容推荐系统。

# 结论

反馈回路是现实世界机器学习系统的关键元素。它们使模型能够改进我们的时间，并帮助收集关于模型哪里出错的反馈。在大多数情况下，这种改进需要您的系统的反馈循环包括获得不纯粹由模型本身产生的无偏数据。

希望本文能让您更好地了解什么样的反馈循环会帮助或损害系统的性能。**在任何行业中，设计反馈回路都是机器学习系统设计的重要组成部分**，因此这篇文章中的知识应该适用于各种各样的环境。

如果你对更广泛地学习机器学习系统设计感兴趣，[斯坦福有一门很棒的课程](https://stanford-cs329s.github.io/syllabus.html)，它涵盖了构建和部署机器学习系统的许多方面。