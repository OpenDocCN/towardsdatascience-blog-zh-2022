<html>
<head>
<title>Graph Neural Networks: A learning journey since 2008 — Diffusion Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络:2008年以来的学习之旅——扩散卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-diffusion-convolutional-neural-networks-329d45471fd9#2022-01-20">https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-diffusion-convolutional-neural-networks-329d45471fd9#2022-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="d799" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">图形神经网络:2008年以来的学习之旅——扩散卷积神经网络</h1></div><p id="4b16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图中邻接矩阵的真正威力是什么？什么是扩散卷积？请跟随我探索图形和机器学习的新冒险，探索DCNN理论</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e8e1f0c8fad9a692f33f63b7f099b0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BB8SuG-tXpnOjpW75x7Jfg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://unsplash.com/@jeremybezanger" rel="noopener ugc nofollow" target="_blank">杰里米·贝赞格</a>在<a class="ae le" href="https://unsplash.com/photos/XyoEsSW6UG8" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="eacf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我以前关于图形和ML的帖子:</p><ul class=""><li id="0a5f" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——第一部分</a></li><li id="01c7" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-2-22dbf7a3b0d?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——第二部分</a></li><li id="c455" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——深度行走</a></li><li id="7a26" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-python-deep-walk-29c3e31432f?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——Python&amp;深度行走</a></li><li id="b8c0" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-graph-convolution-network-aadd77e91606">图神经网络:2008年以来的学习之旅——图卷积网络</a></li><li id="4d0b" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-python-graph-convolutional-network-5edfd99f8190">图神经网络:2008年以来的学习之旅——Python&amp;图卷积网络</a></li></ul><p id="a649" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lt">通过我的推荐链接支持我的写作加盟媒介:</em></p><div class="lu lv gp gr lw lx"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">medium.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ky lx"/></div></div></a></div><p id="fecc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个系列中，我们正在跟踪和研究机器学习算法处理图的演变[1，2]。图表可以以非常简洁的方式存储大量信息[3–6]，它们可以用于根据人们的社会关系评估人群[7–9]或利用简单线性分析无法检测的潜在特征[10]。另一方面，图形是复杂的数学结构[11，12]，它们的输入信号不能在网格上描述，并且很难定义主要的局部统计，除非我们对节点的邻居进行平均。由于这些原因，卷积神经网络[13–15]等强大的最大似然算法的应用有时很复杂，需要了解图形的数学基础。</p><p id="396b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-graph-convolution-network-aadd77e91606">在上一篇帖子中，我们学习了Kipf和布鲁纳的卷积神经网络</a>【11，12】，突出了一个图的谱分析和卷积运算之间的关系。今天我们将继续这项研究，分析Atwood和Towsley关于扩散卷积神经网络的论文[16，17]。在这种方法中，作者利用邻接矩阵和频谱分析之间的关系，利用邻接矩阵的内在属性，得出马尔可夫链公式[18–20]。</p><h1 id="4257" class="mm mn it bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">扩散卷积神经网络</h1><p id="d132" class="pw-post-body-paragraph jq jr it js b jt nk jv jw jx nl jz ka kb nm kd ke kf nn kh ki kj no kl km kn im bi translated">扩散卷积神经网络(DCNN)通过学习“过滤器”来解决图形分类问题，这些过滤器通过扩散过程总结局部信息。这些信息是通过利用邻接矩阵属性获得的图的潜在表示来检索的。最后的算法大量使用张量运算，它需要一种稀疏表示，以允许DCNN处理数百万或数十亿节点的图形。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/227410627ab7e109ee3914696384dea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eX6kJdm0OxCC68JfOQiG4A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图DCNN的总结。根据图的邻接矩阵，我们可以计算从一个起始节点到给定H跳的最终节点的概率转移张量。这个张量反映了邻接矩阵的相同特征以及节点之间的“扩散”行为。张量P、输入特征X和神经网络权重之间的乘积给出了扩散卷积层z。该层可被完全连接的层接收，以返回输入图上的预测。</p></figure><p id="3083" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从GCN及其理论[21–25]中，我们了解到图的拉普拉斯描述与傅立叶变换密切相关。从这里，我们可以通过将自连接邻接矩阵<em class="lt"> A </em>与节点的特征相乘来直接获得卷积运算，为图定义一个卷积神经网络层:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6927dc72e9ad68131d93fe8d57e2adab.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*6h0zvKfEOGyQBT7UChjAKw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式1: l+1卷积层的l+1激活矩阵，其被用作图形卷积神经网络(GCN)算法的传播规则</p></figure><p id="3103" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="lt"> H </em>是第<em class="lt">l</em>或<em class="lt">l+1</em>层的激活矩阵，σ是类似于<em class="lt"> ReLu </em>的激活函数，<em class="lt"> D </em>是图度矩阵，<em class="lt"> A </em>是自连接邻接矩阵，<em class="lt"> W </em>是层特定的可训练权重矩阵。</p><p id="77f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从这个基本计算出发，阿特伍德和陶斯利在卷积运算中加入了扩散。这种扩散允许节点“浏览”它们的邻居，并考虑来自它们的社会影响——这是GCN和DeepWalk的一种混合。然而，我们如何在一个图上添加扩散呢[26]？让我们考虑一个简单的3个顶点的图和它们的邻接矩阵(图2)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/0990a61beada03db4a47e9ad257ea916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84Uw_Nc2EXqxmya2F902IQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:一个简单的3顶点图及其邻接矩阵A</p></figure><p id="1a5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个顶点都可以与这些坐标相关联:<em class="lt"> (1，0，0) </em>表示<em class="lt"> v1 </em>，<em class="lt"> v2 </em>，<em class="lt"> (0，0，1) </em>表示<em class="lt"> v3 </em>。如果我们把顶点乘以邻接矩阵，我们就能得到它的邻居的视图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c68b1796942b6e67dec56556e722e29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*pb7iu_0Hgy1_bGwaAie66g.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:顶点邻接矩阵乘法返回给定顶点的邻居</p></figure><p id="ef02" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图3显示，如果我们从<em class="lt">顶点1 </em>开始，我们可以跳到<em class="lt">顶点2，</em>对于<em class="lt">顶点2 </em>我们可以跳到<em class="lt"> v1 </em>或<em class="lt"> v3 </em>，从<em class="lt">顶点3 </em>我们可以跳到<em class="lt">顶点2 </em>。现在，如果我们多次应用邻接矩阵会怎么样？我们以<em class="lt"> v2 </em>为例。<em class="lt"> v2 </em>和<em class="lt"> A </em>之间的乘积返回两次<em class="lt"> v2 </em>，即如果我们沿着图中从<em class="lt"> v2 </em>开始的路径走两次，我们将返回到<em class="lt"> v2 </em>。因此，我们可以陈述这个定理:<em class="lt">给定邻接矩阵a的幂h，Aʰ(i，j)等于长度正好为h的图g中从I到j的路径数。</em>这意味着邻接矩阵<em class="lt"> A </em>的幂<em class="lt"> h </em>给了我们一个图，该图给出了每个起始顶点在<em class="lt"> h </em>跳中的去向——因此我们在图上是“扩散”的。</p><p id="64c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们能给这个公式增加一点概率吗？我们当然可以！如果我们对邻接矩阵进行归一化，我们将获得一个转移矩阵，即给定一些跳数，从节点<em class="lt"> i </em>到节点<em class="lt"> j </em>的概率<em class="lt"> h. </em>这里应该有一个铃声在你脑海中响起，让你思考马尔可夫链。这里的归一化邻接矩阵可以充当转移概率矩阵，就像在马尔可夫链过程中一样。这个矩阵的力量可以给我们一个关于如何在图上扩散每个顶点的想法。</p><p id="f267" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在可以将卷积运算与扩散概念结合起来。如图1所示，扩散卷积算子可以认为是<em class="lt">转移概率张量P，</em>与输入节点特征矩阵<em class="lt"> X </em>和层随机初始化权重矩阵<em class="lt"> W的乘积，P </em>是一个<em class="lt">N×H×N</em>张量， <em class="lt"> X </em>是一个<em class="lt"> N x F </em>矩阵，而<em class="lt"> W </em>是一个<em class="lt"> F x N </em>矩阵，其中<em class="lt"> N </em>是节点数，<em class="lt"> H </em>是跳数(或邻接矩阵的幂)，而<em class="lt"> F </em>是输入特征数。 最终的扩散卷积算子<em class="lt"> Z </em>是一个<em class="lt"> N x H x H </em>张量。这个张量可以被一个完全连接的神经网络层摄取，其输入节点与存储在张量中的矩阵一样多。然后可以通过<em class="lt"> softmax </em>激活处理完全连接层的结果，以返回概率标签<em class="lt"> Y. </em></p><p id="0ac8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总之，等式2报告了在DCNNs中执行的操作。首先，根据转移概率矩阵P和输入特征计算扩散卷积层Z。然后，通过对来自完全连接的神经网络的输出的softmax激活来产生概率，该神经网络已经摄取了z。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/e401aa26907ac49d8e1d2e1c2aacf0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*iw5I5SGrlyXtleWHqHwpAg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式DCNNs操作总结。最初，根据输入概率转移张量P和节点特征x计算扩散卷积算子Z。最后，在全连接层输出上，通过softmax激活返回概率预测。</p></figure><p id="d57e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">今天就到这里吧！请继续关注我们的下一篇文章，在那里我们将编写Python代码来实现DCNNs！</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="59f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有任何问题或意见，请随时给我发电子邮件，地址是:stefanobosisio1@gmail.com，或者直接在Medium这里。</p><h1 id="3a09" class="mm mn it bd mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj bi translated">文献学</h1><ol class=""><li id="9255" class="lf lg it js b jt nk jx nl kb ob kf oc kj od kn oe ll lm ln bi translated">克劳德·贝尔热。<em class="lt">图论</em>。快递公司，2001年。</li><li id="34c2" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">弗朗西斯·安斯科姆，《统计分析中的图表》美国统计学家27.1(1973):17–21。</li><li id="a9b8" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">沃索伊，索罗什，德布罗伊和西南阿拉尔。"真假新闻在网上的传播."<em class="lt">理科</em>359.6380(2018):1146–1151。</li><li id="8976" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">维拉、何塞和约兰达·戈麦斯。"从图表中提取商业信息:眼球追踪实验."<em class="lt">商业研究杂志</em>69.5(2016):1741–1746。</li><li id="043a" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">特鲁科，欧内斯特。"关于图的信息量的一个注记."数学生物物理学通报18.2(1956):129–135。</li><li id="c97d" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">González-Díaz，Humberto，et al .〈基于分子图和社会网络信息指数的美国县级抗HIV药物活性与艾滋病流行率的人工神经网络多尺度模型〉。<em class="lt">化学信息与建模杂志</em>54.3(2014):744–755。</li><li id="8863" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">尹，郝，等，“局部高阶图聚类”第23届ACM SIGKDD知识发现和数据挖掘国际会议论文集。2017.</li><li id="7f36" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">拉蒂根，马修j，马克梅尔和大卫·延森。"用网络结构指数进行图聚类."第24届机器学习国际会议论文集。2007.</li><li id="00f5" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">张，邓杰，陈金印，陆。"区块链钓鱼诈骗检测通过多通道图分类."<em class="lt">区块链和可信系统国际会议</em>。新加坡斯普林格，2021。</li><li id="40c4" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">佩罗齐、布莱恩、拉米·艾尔弗和史蒂文·斯基纳。"深度行走:社交表征的在线学习."<em class="lt">第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。2014.</li><li id="3f94" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">基普夫，托马斯n，和马克斯韦林。"图卷积网络的半监督分类."<em class="lt"> arXiv预印本arXiv:1609.02907 </em> (2016)。</li><li id="99d7" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">琼·布鲁纳等着《图上的谱网络和深局部连通网络》<em class="lt">第二届国际学习代表大会，ICLR </em>。第2014卷。2014.</li><li id="a883" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">阿尔巴维、萨阿德、塔里克·阿贝德·穆罕默德和萨阿德·扎维。"了解卷积神经网络."<em class="lt"> 2017国际工程与技术大会(ICET) </em>。Ieee，2017。</li><li id="8fd1" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">卡尔·布伦纳、纳尔、爱德华·格雷芬斯特特和菲尔·布伦松。"一个用于建模句子的卷积神经网络."<em class="lt"> arXiv预印本arXiv:1404.2188 </em> (2014)。</li><li id="0148" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">克里日夫斯基、亚历克斯、伊利亚·苏茨基弗和杰弗里·e·辛顿。"使用深度卷积神经网络的图像网络分类."<em class="lt">神经信息处理系统进展</em>25(2012):1097–1105。</li><li id="1860" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">阿特伍德，詹姆斯和唐·陶斯利。"扩散卷积神经网络."<em class="lt">神经信息处理系统的进展</em>。2016.</li><li id="52dd" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">稀疏扩散-卷积神经网络。<em class="lt"> arXiv预印本arXiv:1710.09813 </em> (2017)。</li><li id="2441" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">叶，侬。"用于异常检测的时间行为的马尔可夫链模型."2000年IEEE系统、人和控制论信息保证和安全研讨会会议录。第166卷。纽约西点军校，2000年。</li><li id="0472" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">海因斯布莱恩。"马尔可夫链中的第一个环节."<em class="lt">美国科学家</em> 101.2 (2013): 252。</li><li id="b739" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">奥雷，史蒂文。<em class="lt">马尔可夫链转移概率的极限定理</em>。伦敦:范·诺斯特朗，1971年。</li><li id="a8ef" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">学习分子指纹的图形卷积网络。<em class="lt"> arXiv预印本arXiv:1509.09292 </em> (2015)。</li><li id="0af6" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">哈蒙德、大卫·k、皮埃尔·范德盖恩斯特和雷米·格里邦瓦尔。"通过谱图论研究图上的小波."<em class="lt">应用和计算谐波分析</em>30.2(2011):129–150。</li><li id="77fb" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">学习仿射变换。<em class="lt">模式识别</em>32.10(1999):1783–1799。</li><li id="fa21" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">李，文静，和唐利。"仿射不变匹配的Hopfield神经网络."《IEEE神经网络汇刊》12.6(2001):1400–1410。</li><li id="9fa1" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">离散傅立叶变换和卷积的算法。斯普林格，1989年。</li><li id="c849" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn oe ll lm ln bi translated">邻接矩阵的好资源在这里:<a class="ae le" href="https://www.bowaggoner.com/courses/2019/csci5454/docs/spectral.pdf" rel="noopener ugc nofollow" target="_blank">https://www . bowaggoner . com/courses/2019/csci 5454/docs/spectral . pdf</a></li></ol></div></div>    
</body>
</html>