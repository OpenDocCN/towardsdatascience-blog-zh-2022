<html>
<head>
<title>8 Alternatives to Pandas for Processing Large Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于处理大型数据集的熊猫的8种替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/8-alternatives-to-pandas-for-processing-large-datasets-928fc927b08c#2022-01-17">https://towardsdatascience.com/8-alternatives-to-pandas-for-processing-large-datasets-928fc927b08c#2022-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="30e7" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">用于处理大型数据集的熊猫的8种替代方案</h1></div><div class=""><h2 id="b079" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">停止使用熊猫</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a89ec1c4f6092425f1636d1eb3eb989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_lb9qYhBy5sMxac_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·麦克林在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas库已经成为python中数据操作的事实库，并被数据科学家和分析师广泛使用。然而，有时数据集太大，熊猫可能会遇到内存错误。以下是熊猫处理大型数据集的8个替代方案。对于每个备选库，我们将研究如何从CSV加载数据并执行一个简单的<code class="fe lv lw lx ly b">groupby</code>操作。幸运的是，这些库中有许多与Pandas相似的语法，因此学习曲线不会太陡。</p><h1 id="d842" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">达斯克</h1><blockquote class="mr ms mt"><p id="fd88" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://docs.dask.org/" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Dask </em> </a> <em class="it">在大于内存的数据集上提供多核分布式并行执行。</em></p></blockquote><p id="562b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Dask数据帧是一个大型并行数据帧，由许多较小的Pandas数据帧组成，沿索引拆分。这些Pandas数据帧可能存在于磁盘上，用于单台机器或集群中许多不同机器上的大内存计算。一个Dask数据帧操作触发对组成Pandas数据帧的许多操作。</p><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="2cb5" class="nc ma it ly b gy nd ne l nf ng">pip install dask</span><span id="a25a" class="nc ma it ly b gy nh ne l nf ng">import dask.dataframe as dd<br/>df = dd.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv', dtype={'Tolls_Amt': 'float64'})<br/>df2 = df.groupby('vendor_name').agg({'Passenger_Count':'mean'})</span></pre><p id="0551" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，还没有执行任何计算。计算<code class="fe lv lw lx ly b">df2</code>的结果</p><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="4872" class="nc ma it ly b gy nd ne l nf ng">df2.compute()</span></pre><p id="3ecd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Dask不仅仅是一个数据操作库。<a class="ae ky" href="https://ml.dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask-ML </a>提供可扩展的机器学习，可以与Scikit-learn、Xgboost和LightBGM等流行的机器学习库一起使用。</p><h1 id="8e05" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">摩丁</h1><blockquote class="mr ms mt"><p id="033f" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://modin.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Modin </em> </a> <em class="it">使用Ray或Dask提供一种毫不费力的方式来加速你的熊猫笔记本、脚本和库。</em></p></blockquote><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="0460" class="nc ma it ly b gy nd ne l nf ng">pip install modin</span><span id="9279" class="nc ma it ly b gy nh ne l nf ng">import modin.pandas as pd<br/>df = pd.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg({'Passenger_Count':'mean'})</span></pre><h1 id="f86d" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">数据表</h1><blockquote class="mr ms mt"><p id="51a5" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://datatable.readthedocs.io/en/latest/start/quick-start.html" rel="noopener ugc nofollow" target="_blank"><em class="it">Datatable</em></a><em class="it">是一个用于操作表格数据的python库。它支持内存不足的数据集、多线程数据处理和灵活的API。</em></p></blockquote><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="2633" class="nc ma it ly b gy nd ne l nf ng">pip install datatable</span><span id="f69e" class="nc ma it ly b gy nh ne l nf ng">from datatable import dt, f, by<br/>df = dt.fread('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv', skip_blank_lines = True)<br/>df2 = df[:, dt.mean(f.Passenger_Count), by('vendor_name')]</span></pre><h1 id="99f4" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">极地</h1><blockquote class="mr ms mt"><p id="3052" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://pola-rs.github.io/polars-book/user-guide/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Polars </em> </a> <em class="it">是Rust中实现的一个速度惊人的数据帧库，使用Apache Arrow Columnar格式作为内存模型。</em></p></blockquote><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="9276" class="nc ma it ly b gy nd ne l nf ng">pip install polars</span><span id="c7d0" class="nc ma it ly b gy nh ne l nf ng">import polars as pl</span><span id="911d" class="nc ma it ly b gy nh ne l nf ng">df = pl.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg([pl.mean('Passenger_Count')])</span></pre><h1 id="a211" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">Vaex</h1><blockquote class="mr ms mt"><p id="2c52" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://vaex.io/docs/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Vaex </em> </a> <em class="it">是一个用于懒惰的核外数据框架(类似于熊猫)的python库，用于可视化和探索大型表格数据集</em></p></blockquote><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="b7d2" class="nc ma it ly b gy nd ne l nf ng">pip install vaex</span><span id="6d66" class="nc ma it ly b gy nh ne l nf ng">df = vaex.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg({'Passenger_Count':'mean'})</span></pre><h1 id="436b" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">Pyspark</h1><blockquote class="mr ms mt"><p id="6616" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/index.html#" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Pyspark </em> </a> <em class="it">是Apache Spark的Python API，用于通过分布式计算处理大型数据集。</em></p></blockquote><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="fe19" class="nc ma it ly b gy nd ne l nf ng">pip install pyspark</span><span id="3d8f" class="nc ma it ly b gy nh ne l nf ng">from pyspark.sql import SparkSession, functions as f<br/>spark = SparkSession.builder.appName("SimpleApp").getOrCreate()</span><span id="26a3" class="nc ma it ly b gy nh ne l nf ng">df = spark.read.option('header', True).csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg(f.mean('Passenger_Count'))</span></pre><p id="b8d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，还没有执行任何计算。计算<code class="fe lv lw lx ly b">df2</code>的结果</p><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="c819" class="nc ma it ly b gy nd ne l nf ng">df2.show()</span></pre><h1 id="f521" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">树袋熊</h1><blockquote class="mr ms mt"><p id="3c98" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><em class="it"/><a class="ae ky" href="https://koalas.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"><em class="it">考拉</em> </a> <em class="it">项目通过在Apache Spark之上实现pandas DataFrame API，使数据科学家在与大数据交互时更有效率。</em></p></blockquote><p id="58a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于考拉运行在Apache Spark之上，Spark也必须安装。</p><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="15e2" class="nc ma it ly b gy nd ne l nf ng">pip install pyspark<br/>pip install koalas</span><span id="6267" class="nc ma it ly b gy nh ne l nf ng">import databricks.koalas as ks<br/>from pyspark.sql import SparkSession<br/>df = ks.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg({'Passenger_Count':'mean'})</span></pre><h1 id="0022" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">cuDF</h1><blockquote class="mr ms mt"><p id="db2e" class="kz la mu lb b lc ld ju le lf lg jx lh mv lj lk ll mw ln lo lp mx lr ls lt lu im bi translated"><a class="ae ky" href="https://docs.rapids.ai/api/cudf/stable/user_guide/10min.html" rel="noopener ugc nofollow" target="_blank"> <em class="it"> cuDF </em> </a> <em class="it">是一个Python GPU DataFrame库，构建在Apache Arrow columnar内存格式上，用于数据操作。cuDF还提供了一个类似熊猫的API，数据工程师&amp;数据科学家会很熟悉，所以他们可以使用它来轻松地加速他们的工作流程，而无需进入CUDA编程的细节。</em></p></blockquote><p id="9c8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">cuDF可以通过conda安装，看看<a class="ae ky" href="https://github.com/rapidsai/cudf" rel="noopener ugc nofollow" target="_blank">这个指南</a>。</p><pre class="kj kk kl km gt my ly mz na aw nb bi"><span id="d60a" class="nc ma it ly b gy nd ne l nf ng">import cudf<br/>df = cudf.read_csv('../input/yellow-new-york-taxi/yellow_tripdata_2009-01.csv')<br/>df2 = df.groupby('vendor_name').agg({'Passenger_Count':'mean'})</span></pre><h1 id="99b4" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">结论</h1><p id="2f15" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在本文中，我们研究了8个用于操作大型数据集的python库。它们中的许多都有与Pandas相似的语法，这使得学习曲线不那么陡峭。如果你想知道不同包的执行速度，<a class="ae ky" href="http://H2O.ai" rel="noopener ugc nofollow" target="_blank"> H2O.ai </a>已经在其中一些包上创建了一个有用的<a class="ae ky" href="https://h2oai.github.io/db-benchmark/" rel="noopener ugc nofollow" target="_blank"> ops基准</a>。它比较了不同大小的数据集上各种包的执行速度，以及不同常见操作(如连接和分组)的执行速度。</p><p id="29e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这是一个有用的开始，请在您处理python大型数据集的常用库下面留下评论。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="7e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@edwin.tan/membership" rel="noopener">加入Medium </a>阅读更多这样的故事。</p></div></div>    
</body>
</html>