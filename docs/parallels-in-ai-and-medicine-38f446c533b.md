# 人工智能和医学的相似之处

> 原文：<https://towardsdatascience.com/parallels-in-ai-and-medicine-38f446c533b>

## 意见

## 对人工智能未来的医学主题思考

对于那些已经认识我一段时间的人来说，我喜欢阅读书籍、文章和 Youtube 系列中的医学主题。这与我的职业有些距离，但我认为有这种兴趣是健康的(明白吗？).下面是我最近喜欢的一些书。

![](img/bacc72aee4f46ec39f6297de6202a536.png)

作者编辑的书籍封面。

最终，我在我的学科和医学科学之间找到了一些相似之处。就像免疫系统表现得像一个微服务架构，其中没有集中控制，只有一个具有本地输入和输出的应急系统。或者基因通向不同人类特征和条件的路径如此复杂，类似于具有 1750 亿个参数的 GPT3。

我一直在思考的一件事是，医学如何告知人工智能的未来。前者是一个影响深远的实践，其轨迹是停滞、可怕的实践、再创造和持续发展。后者深入人心，成就斐然。但是它没有几个世纪的学习带来的好处。事实上，它的影响不仅越来越大，而且从我们的身体、自然生态系统和地缘政治中在意想不到的地方被发现。

在接下来的几篇文章中，我想把我的写作组织成**输出、输入、**和**过程。**非常像软件工程，我知道。此外，我将转而使用术语“机器学习”，因为人工智能可能是一个太大的术语。机器学习(ML)作为人工智能的一个子集，是与从大型数据集学习有用程序相关的工作主体。这个子领域享受着人工智能聊天机器人、推荐器、视觉应用等的持续宣传，但它实际上归结为从数据中学习。

*免责声明:我是一名机器学习从业者，因此请不要将我的任何陈述视为完全等同于解剖学事实和流行的医学观点。还有，这个帖子不包含医学建议，也不代表医学上准确*。

# 输出

显而易见，ML 将变得更加普遍。它的输出将更加广泛，它对我们日常生活的影响甚至会比 Instagram 滤镜更深入。除了自动驾驶汽车和全自动客户服务的宣传之外，人们还必须考虑这些新技术将影响的系统。所以我将这些趋势分为“微观”和“宏观”两个角度。

微观上，房屋、仓库等小系统将被改造。[亚马逊一直在做后一种](https://www.vox.com/recode/2019/12/11/20982652/robots-amazon-warehouse-jobs-automation)，工业领域的其他公司也会跟进。从宏观上来说，像整个生态系统和[整个城市](https://www.scmp.com/news/china/science/article/3136661/across-china-ai-city-brains-are-changing-how-government-runs)这样的大系统将被建模和管理。在这方面，我为我的菲律宾同胞感到骄傲，他们在[利用卫星图像制作流行病和流动性模型的工作获得了美国国家航空航天局的奖励。](https://www.spacetechasia.com/filipino-startup-cirrolytix-wins-earth-observation-award-for-tools-such-as-dengue-hotspot-prediction/)

![](img/0f008f6bc4fc0c0b9f585d369ee3b2f5.png)

个性化医疗中一个潜在有用的数据点——可穿戴设备。卡罗琳娜·格拉博斯卡在[Pexels.com](https://www.pexels.com/photo/anonymous-sportswoman-checking-smart-watch-and-sitting-on-mat-4498483/)拍摄的照片

在医学领域，产出也很有希望。[个性化医疗](https://www.genome.gov/genetics-glossary/Personalized-Medicine)，医疗保健和遗传学(以及商业利益——苹果手表，看着你)的交汇点，在当前的媒体中很突出。但从我的阅读来看，机器学习和统计也可以在检测保险欺诈、医疗保健过度治疗和治疗不足、医生倦怠以及总体医疗保健系统的压力方面发挥重要作用。这会很痛，这一点尤其明显。

# 输入

大型硬件。大模特。大数据。这是深度学习飞速发展的趋势。与云时代的全球数字化和无服务器应用相一致，从业者可以依靠不断增长的数据量，同时降低成本。但近年来，几位机器学习重量级人物呼吁重新评估方法——最著名的是吴恩达。人们并不总是每天都能拿出大数据。正如他所引用的，

> *当你拥有相对较小的数据集时，领域知识就变得很重要。(人工智能系统应该)有两个知识来源——来自数据和人类经验。当我们拥有大量数据时，人工智能将更多地依赖数据，而不是人类知识。*
> 
> [*吴恩达接受 Venturebeat 采访*](https://venturebeat.com/2022/03/21/andrew-ng-predicts-the-next-10-years-in-ai/)

在医学中，人类的经验被概括在大型医学本体论中。统一医学语言系统就是这样一个数据库，它汇集了医学中不同的相关领域。这是许多专家系统的基础，自然语言处理的进步也推动了这些专家系统的发展。

我相信，我们应该有积累和处理这种“小数据”的诀窍。象征性的人工智能可以提供一条前进的道路。这是机器学习的又一次重新想象，它有可能成为解释我们可解释性问题的一个框架。

# 处理

说到可解释性，我相信这是我们 ML 从业者必须面对的最重要的问题。归根结底是信任的问题。在漫长的医学世纪中，增进信任一直是医患关系的焦点。信任也应该是构建 ML 系统的指导原则。ML 社区应该关注一些职业道路来建立信任。我在这里提出了几个反映医疗保健角色的角色。

## 侧重于 ML 输出的操作

这与 MLOps 的上升趋势有关联但又有点不同。这些分析师或代表了解机器学习，并拥有在系统出现严重故障时提供解决方案的工具。他们就像机器学习的急救医学专家。有时，最快的可能解决方案应该完成。分析师可以发现令人震惊的推荐(例如普通观众网站中的软核色情)，由于种族原因的热修复计算机视觉错误(例如移动应用程序中的身份证照片证明)，并报告聊天机器人发生的有趣事情([还记得 Tay 吗？](https://www.zdnet.com/article/microsoft-and-the-learnings-from-its-failed-tay-artificial-intelligence-bot/))。在更深层的后端，工程师应该注意数据漂移和模型漂移，以及与 ML 系统相关的事件。

当系统出现问题时，这个角色体现了护理人员快速到达需要他们的地方。在我们建立信任的范例中，他们是服务补救型的工作。然而，我们不应该如此被动。他们还应该汇编他们的学习，最终应达到…

## 机器学习设计师-审计师

他们有点像医疗保健管理员，医院依靠他们来提供高效的护理。在 ML 术语中，他们建立了对*产品*的信任。他们从用户那里学习，并从运营中收集知识，以形成一个能够提供最佳体验的产品。这并不容易，因为早期的数据输入可能有很大的偏差，只有通过仔细的管理和注释才能解决。如何使用输出也可能被重新评估。例如，设计师可以创建候选过滤器(消除可疑内容)，甚至“弹出”过滤器气泡(例如，使新闻主题多样化)，而不是直接提供推荐。

这份工作需要超越准确性的衡量标准——公平性、可用性和影响力。不容易量化但非常关键的指标。在我们早期的“紧急事件”中，软核色情可能是由于 clickbait 图像，这是由热衷于利用点击而不是更有意义的交互的模型产生的。我们的身份证明捕获应用程序的输入数据可能缺少有色人种的图像。由于地区方言，聊天机器人无法充分理解它声称理解的外语。

我想起了悉达多·慕克吉的医学第三定律。“对于每一个完美的医学实验，都有一个完美的人类偏见”。在追求最大精确度的过程中，我们工程师可能已经忘记了我们的偏见。

遗憾的是，超准确性指标并不总是底线的一部分。设计师如何向董事会证明这些指标的合理性？风气的改变是必需的，这就是为什么我们需要…

## 机器学习伦理学家

设计师关注产品，伦理学家关注 T2 人口。一个很好的比喻是公共卫生官员，在那里疾病在时间和空间上受到监测。设计师建立对产品的信任，而伦理学家建立对整个 T4 领域的信任。这是一个非常艰巨的任务，但是必须完成。我们需要更多像 Timnit Gebru 这样的伦理学家，她和她的同事引发了与谷歌关于偏见、环境问题和模型透明度的辩论。

有一些突出的应用程序的例子，如果某些东西失败了，会产生深远的影响——[监控数据](https://www.technologyreview.com/2022/03/03/1046676/police-surveillance-minnesota-george-floyd)、[公司招聘](https://hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias)和[信用评分](https://www.technologyreview.com/2021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness-machine-learning/)。如果太成功了呢？《剑桥分析》的报道公开揭示了 ML 的潜力，它不仅影响了社会的微观部分，而且也分化了整个人群。像癌细胞一样，ML 生成的假货是我们自己认知偏见的破坏性镜子，并且继续繁殖和入侵。定向营销、假新闻和心理战的领域正在合二为一。

显然，在这方面，特别是在通信方面，需要做大量的工作。我们应该有一个类似 CDC 的组织吗？是的，我想是的。类似于 NIH 在 80 年代是如何通过志愿者组织的活动从基层建立支持的。这就是为什么伦理学家应该是好的教育者，能够分享他们的发现和建议，这样我们才能逐步前进，建立对人工智能的信任。假以时日，它将创造出新的传销从业者……

# 每个从业者也需要进化

人工智能专业人员也必须做出贡献，以促进该领域的信任。我可以想到两种方法:为 ML 和公共教育创建一个道德准则。

![](img/b0fb106d4bbb55aa9975f6f21c028543.png)

很难想象程序员是道德的典范，但是底线应该是，是的，每个人都应该有道德准则。奥利娅·丹尼利维奇在[Pexels.com](https://www.pexels.com/photo/two-men-looking-at-a-laptop-4974920/)拍摄的照片。

道德准则将作为所有道德 ML 活动的指导框架。道德准则是难以捉摸的，因为哲学和实施。我们可以从医学中寻找一些哲学上的灵感。为了纠正第二次世界大战中的大规模错误，日内瓦宣言建立在希波克拉底誓言的基础上，并将其置于现代世界的背景下。继临床研究中的严重不公正之后，贝尔蒙特报告概述了人类受试者的原则。回到曼梯·里，我不确定剑桥分析公司的故事是否已经成为一个分水岭，或者是否会错过一个改变的机会。

至于实现，在医学中，有医生委员会管理医疗事故的问题，但它不是 ML 和 AI 的直接端口，更不用说它的相邻领域，软件工程。请注意，ACM 有一套道德规范，但不幸的是，没有太多的机构遵守这一规范，甚至没有通知他们的员工。在医学中，有四大支柱:仁慈、无害、自主和公正。有时，这些相互冲突，但有一个成熟的申诉法律制度。在 ML 里还不是那么清楚，至少，现在还不是。这就是为什么我们需要更多的人参与对话，他们的声音不仅仅来自股东的立场。多元文化、经济和生态视角至关重要。

公众教育将有助于围绕 ML 话题展开并继续对话。希望我们中小学的计算机素养科目涵盖 ML 不断扩大的影响力。它可以向我们的年轻人介绍互联网微妙的运作方式。对于更成熟的人，我推荐像[速成班人工智能](https://www.youtube.com/watch?v=GvYYFloV0aA&list=PL8dPuuaLjXtO65LeD2p4_Sb5XQ51par_b)这样的努力，以扩大外行人的理解。

我们应该尽可能扩大对话的范围，保持对话，开始行动，继续回顾。就像过去的医学一样，人工智能还有很长的路要走。尽管如此，我还是希望我们能尽早结束“数字黑暗时代”,继续创新和持续发展。

感谢阅读！

*原载于 2022 年 5 月 3 日*[*【http://itstherealdyl.com】*](https://itstherealdyl.com/2022/05/03/a-medically-themed-ai-reflection/)*。*