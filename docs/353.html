<html>
<head>
<title>Predicting the Number of Dislikes on YouTube Videos. Part 2 — Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测YouTube视频上不喜欢的数量。第2部分—模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-the-number-of-dislikes-on-youtube-videos-part-2-model-aa981a69a8b2#2022-01-12">https://towardsdatascience.com/predicting-the-number-of-dislikes-on-youtube-videos-part-2-model-aa981a69a8b2#2022-01-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="6e5d" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">预测YouTube视频上不喜欢的数量。第2部分—模型</h1></div><div class=""><h2 id="6e18" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何(尝试)使用Tensorflow/Keras建立深度神经网络来预测不喜欢的数量</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd078eb40bf80fbc19e483d5acf381dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qpK04P89HvWNYSjbdOzew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预览。作者图片</p></figure><p id="dd85" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，您将学习如何:</p><ul class=""><li id="4eb4" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">使用NLTK预处理文本数据；</li><li id="1200" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">在TensorFlow/Keras嵌入层中使用预训练的单词嵌入；</li><li id="f501" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">使用TensorFlow Functional API构建非时序神经网络架构；</li><li id="7a4d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">使用<code class="fe mi mj mk ml b">tensorflow.keras.utils.plot_model()</code>和<a class="ae mm" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> Netron </a>可视化神经网络结构。</li></ul><p id="fd2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是下一篇文章的逻辑延续— <a class="ae mm" rel="noopener" target="_blank" href="/predicting-the-number-of-dislikes-on-youtube-videos-part-1-dataset-9ec431585dc3">预测YouTube视频上不喜欢的数量。第1部分—数据集</a>，我使用YouTube数据API v3收集数据集。</p><p id="30ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在本文中使用的所有代码都可以在同一个<a class="ae mm" href="https://gitlab.com/Winston-90/youtube_dislikes" rel="noopener ugc nofollow" target="_blank"> GitLab资源库</a>中获得；但是，请记住，本文和存储库中的代码可能略有不同，因为为了简化，这里我没有使用函数。</p><h1 id="3db8" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">免责声明</strong></h1><p id="5760" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">老实对你和我自己说，我必须说我没能成功地解决这个问题。但是我认为不好的经历也是一种经历，这就是为什么我要和你分享它。最后，我会告诉你为什么会这样。</p><h1 id="7590" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">资料组</h1><p id="084c" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我在这个项目的第一部分详细描述了收集数据集的过程。现在我想记下我将使用其中的哪些字段进行预测:</p><ul class=""><li id="39fb" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">video_id</code> -唯一视频ID -不作为特征使用，因为它不包含信息</li><li id="fcd3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">title</code> -视频标题-用作特写</li><li id="036b" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">channel_id</code> -频道ID -不作为特征使用，因为它不包含信息</li><li id="cc21" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">channel_title</code> -频道标题-用作特写</li><li id="0fd3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">published_at</code> -视频发布日期-不作为特征使用，因为所有视频属于同一时间段(2021)</li><li id="bdf9" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">view_count</code> -视图数量-用作特征</li><li id="c8aa" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">likes</code> -点赞数-用作特征</li><li id="8dd5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">dislikes</code> -不喜欢的数量-用作目标标签</li><li id="451a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">comment_count</code> -评论数量-用作特性</li><li id="98f3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">tags</code> -作为一个字符串的视频标签-用作特性</li><li id="b62b" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">description</code> -视频描述-用作功能</li><li id="b50f" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">comments</code> - 20个视频评论作为一个字符串-用作特征</li></ul><p id="007d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们面临一个回归任务，有3个数字输入特性(<code class="fe mi mj mk ml b">view_count</code>、<code class="fe mi mj mk ml b">likes</code>和<code class="fe mi mj mk ml b">comment_count</code>)和5个文本输入特性(<code class="fe mi mj mk ml b">title</code>、<code class="fe mi mj mk ml b">channel_title</code>、<code class="fe mi mj mk ml b">tags</code>、<code class="fe mi mj mk ml b">description</code>和<code class="fe mi mj mk ml b">comments</code>)。</p><h1 id="f2b5" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据准备</h1><p id="7d0f" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">在来自数据集(37k)的所有视频中，我决定从那些在美国流行的视频开始——这大约是16k行。</p><p id="6ca3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为不需要准备数字特性，所以我们需要做的就是分割所需的列，并将它们转换成numpy数组。代码非常简单:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="94e9" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">清除文本数据</h2><p id="bcff" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">对于文本数据，一切都更复杂。首先，我创建了一个新的<code class="fe mi mj mk ml b">author_text</code>特性，包含作者提供的所有文本信息。它由以下特征的串联组成:<code class="fe mi mj mk ml b">title</code>、<code class="fe mi mj mk ml b">channel_title</code>、<code class="fe mi mj mk ml b">tags</code>和<code class="fe mi mj mk ml b">description</code>。这样我们将把文本特征的数量从五个减少到两个，这将大大减少后面的模型参数的数量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="591e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但在此之前，我们至少需要做一点数据清理。我决定去掉<code class="fe mi mj mk ml b">title</code>、<code class="fe mi mj mk ml b">channel_title</code>、<code class="fe mi mj mk ml b">tags</code>的标点符号，去掉<code class="fe mi mj mk ml b">description</code>、<code class="fe mi mj mk ml b">comments</code>栏的无意义词。</p><p id="b3ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用了<a class="ae mm" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK库</a>来做这件事。为了第一次运行下面的代码，您必须下载一些NLTK数据——通过运行<code class="fe mi mj mk ml b">ntlk.download('something')</code>命令记住这一点。</p><p id="5b6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用<a class="ae mm" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html" rel="noopener ugc nofollow" target="_blank"> pandas应用函数</a>来清理数据，这允许我们为数据集的每一行应用<em class="ny">一些函数。剩下的只是编写一个函数，它将返回一个干净的输入版本。</em></p><p id="6805" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免废话，我们只保留那些包含在<code class="fe mi mj mk ml b">nltk.corpus.words.words()</code>中的单词。这套包含235，000个英语单词——请随意复制和探索下面的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/924006ef6ec08c8d7dec0af0d4d2d5b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFv4-RxqeDEbHiDv-9fLog.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">执行上述代码的结果。作者图片</p></figure><p id="259e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">函数将这个集合中出现的所有单词连接起来。它接收一个输入句子(当前行的一个元素)并返回它的干净版本。它将应用于某一列的所有行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="01fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，<code class="fe mi mj mk ml b">nltk.tokenize.word_tokenize(sentence)</code>将一个<code class="fe mi mj mk ml b">sentence</code>字符串转换成一个单词数组，其中每个元素都是一个单词或一个标点符号。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/1db050b02729b2fea62b110605ce4100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ugxYORM70ynh6pBY7U0MKg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">执行<code class="fe mi mj mk ml b">nltk.tokenize.word_tokenize()</code>功能的结果。作者图片</p></figure><p id="a56a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们删除标点符号时，我们应该只保留那些只包含数字和字母的单词。这也将删除在单词中间包含特殊字符的单词(例如“c@t”)，但是这些单词将在嵌入阶段被忽略，所以这不是一个大问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="696e" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">使用预训练的单词嵌入将文本转换为矢量</h1><p id="2636" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">现在我们需要将文本转换成计算机和我们的模型可以理解的数字。这里最重要的两个参数是:</p><ul class=""><li id="50cb" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">max_tokens</code>——词汇量的大小。<code class="fe mi mj mk ml b">TextVectorization</code>层只会考虑<code class="fe mi mj mk ml b">max_tokens</code>热门词汇；</li><li id="b7c6" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe mi mj mk ml b">sentence_length</code> -一句话的最大字数。较长的句子将被截断，较短的句子将用特殊的<code class="fe mi mj mk ml b">&lt;pad&gt;</code>符号填充。</li></ul><p id="ad39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果<code class="fe mi mj mk ml b">max_tokens</code>参数对性能影响不大，通常等于一个很大的数(10k，20k等等)，<code class="fe mi mj mk ml b">sentence_length</code>对它影响很大。稍后我们将看到，该值是矩阵的维度之一，如果它太大，矩阵将会很大，但很稀疏，如果它非常小，我们就有丢失大量信息的风险。</p><p id="0fe9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">选择此参数的一个好方法是建立一个直方图，即句子中单词数量的直方图，并计算一些分位数。之后，您可以选择一个覆盖数据集80%或95%的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/b16b19d2982fdf9d3e6faae5ad07a026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQVQggprE5t5MRBgWjDX-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评论和作者_文本的句子字数直方图。作者图片</p></figure><p id="c12b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在下面看到创建这些直方图的代码(这是<code class="fe mi mj mk ml b">data_transformations.ipynb</code>笔记本的一部分):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="ac36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让句子成为文本数据集的一个单元。例如，您将通过执行<code class="fe mi mj mk ml b">df['comments'][0]</code>命令获得一个句子。</p><p id="76ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最初，我们正在处理大小为<code class="fe mi mj mk ml b">(n, ?)</code>(其中<code class="fe mi mj mk ml b">n</code>是数据集的大小)的张量，因为所有的句子都有不同的长度。我们必须将这些句子转换成固定长度的单词数组。在标记化之后(我们在上面看到的<code class="fe mi mj mk ml b">nltk.tokenize.word_tokenize(sentence)</code>)，短于<code class="fe mi mj mk ml b">sentence_length</code>的数组用一个特殊的填充符号填充，长于<code class="fe mi mj mk ml b">sentence_length</code>的数组被截断。现在我们正在处理一个大小为<code class="fe mi mj mk ml b">(n, sentence_length)</code>的张量。</p><p id="379c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后，我们需要给句子中的每个单词分配一个向量——这就是嵌入。如果不使用预训练的单词嵌入，这些向量将在学习过程中由网络学习，以及其他参数。但是获得代表词嵌入是一项相当困难的任务，需要大量的数据和计算能力。给每个单词分配一个长度为<code class="fe mi mj mk ml b">emb_dim</code>(嵌入维数)的向量，我们将得到一个大小为<code class="fe mi mj mk ml b">(n, sentence_length, emb_dim)</code>的张量。</p><p id="0196" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在这里描述的过程正式记录在<code class="fe mi mj mk ml b"><a class="ae mm" href="https://gitlab.com/Winston-90/youtube_dislikes/-/blob/main/pretrained_word_embeddings.ipynb" rel="noopener ugc nofollow" target="_blank">pretrained_word_embeddings.ipynb</a></code> <a class="ae mm" href="https://gitlab.com/Winston-90/youtube_dislikes/-/blob/main/pretrained_word_embeddings.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中。基于这段代码，我创作了下面这张图片，与文本相比，它会更加清晰。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/8462d385aede3862cf403b31d78db388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZL_WEDcHGlZzHbqsanVrZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用单词嵌入预处理文本。作者图片</p></figure><p id="210f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个玩具数据集包含8个独特的词:“我”，“喜欢”，“猫”，“和”，“狗”，“apdnv”，“是”，“什么”。所有这些词都存在于从8+2 ≤ <code class="fe mi mj mk ml b">max_tokens</code>(设置为10)以来的数据集中学习到的词汇中。同时，词汇表的第一个元素(<code class="fe mi mj mk ml b">’’</code>)是<code class="fe mi mj mk ml b">&lt;pad&gt;</code>符号，第二个元素(<code class="fe mi mj mk ml b">’[UNK]’</code> )—未知单词的特殊符号(在这种情况下，如果我们将<code class="fe mi mj mk ml b">max_tokens</code> &lt;设为10，就会用到它)。这两个符号总是出现在词汇表中。</p><p id="368a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，单词“I”、“like”和“cats”的单词嵌入与第一句和第二句相似，也与<code class="fe mi mj mk ml b">embeddings\glove.6B\glove.6B.50d.txt</code>文件中的相应行匹配。此外，单词“apdnv”被嵌入到零向量中，因为嵌入字典不包含这个单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/e817da1386e0ed35a94c15773905f77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssjPlxQxZzqkMfhSxuYJVQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确保嵌入匹配。作者图片</p></figure><p id="7165" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以从下面的链接下载40多种语言的免费预训练单词嵌入。</p><div class="oe of gp gr og oh"><a href="http://vectors.nlpl.eu/repository/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">NLPL单词嵌入知识库</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">204 300 2俄罗斯国家语料库俄语维基百科转储2018年12月俄语新闻来自对话评测2020…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">vectors.nlpl.eu</p></div></div></div></a></div><h1 id="1de0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">建筑模型</h1><p id="b5a5" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">最后，我们可以去网络。为什么是神经网络？</p><p id="e35a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这项任务中的主要信息由文本特征携带——它们具有最强的预测能力。时间表明，经典的机器学习算法处理文本处理的能力比神经网络差得多。我们说的是NLP — <em class="ny">自然语言处理</em>。这个领域包括各种各样的任务——从<em class="ny">命名实体识别</em>和<em class="ny">词性标注</em>到<em class="ny">文本摘要</em>和<em class="ny">机器翻译</em>。当前的任务最类似于<em class="ny">情感分析</em>——预测文本的“情绪”。</p><h2 id="2ddb" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">神经网络体系结构</h2><p id="2f91" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">让我们转到最有趣的部分——构建模型的架构。</p><p id="75f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我描述一下我在构建模型架构时使用的主要启发方法:</p><ol class=""><li id="ffe1" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt oq ma mb mc bi translated">为了将文本转换成向量，我们将使用<strong class="la iu">预训练的单词嵌入</strong>；</li><li id="c371" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">由于任务不是严格顺序的，而且整个文本对模型是可见的，我建议用<strong class="la iu">双向LSTM </strong>代替单向；</li><li id="afcf" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">为了从文本中提取更多的信息，我们将使用<strong class="la iu">多层LSTM </strong>。同时我们会用<strong class="la iu">跳过连接</strong>来简化学习过程；</li><li id="14d5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">对于评论和作者的数据，我们将进行类似的分层，但权重不同，因为作者/观众的观点可能会有很大差异；</li><li id="5216" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">模型的数值参数不会通过深层网络传递，不会使结构复杂化；</li><li id="c993" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">在从文本中提取特征后，我们将组合所有可用的信息，并将其发送到一个小小的前馈神经网络的输入端。</li></ol><p id="354b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面可以看到网络结构和生成它的详细代码(是<code class="fe mi mj mk ml b">model.ipynb</code>笔记本的一部分)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/5dcfa7488a5a8aa4e26f53d7c127add9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yWJknft5YKitKVQj_tX2PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络架构。作者图片</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="12a0" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">可视化神经网络架构</h2><p id="6fd8" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">下面你可以看到绘制网络结构的<code class="fe mi mj mk ml b">tensorflow.keras.utils.plot_model()</code>函数的输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/af5fb3fd59ddd991e4475ccba4dd3daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcB0XrWT3uxVLsyDO16JCw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络架构——<code class="fe mi mj mk ml b">plot_model()</code>函数的输出。作者图片</p></figure><p id="e67f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要在项目中使用此功能，请执行以下操作:</p><ol class=""><li id="67c0" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt oq ma mb mc bi translated">确保您使用TensorFlow版或更高版本(您可以通过打印<code class="fe mi mj mk ml b">tf.__version__</code>来实现)，因为该功能仅在软件包的更高版本中可用。如果您使用的是TensorFlow的早期版本，请更新软件包(<code class="fe mi mj mk ml b">pip install --upgrade tensorflow</code>)。</li><li id="5c3f" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">你需要安装pydot和Graphviz来执行这个功能。对于pydot运行<code class="fe mi mj mk ml b">pip install pydot</code>，对于Graphviz遵循<a class="ae mm" href="https://graphviz.org/download/" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</li></ol><p id="6ec4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也可以用<a class="ae mm" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> Netron </a>。这种方法在处理大型网络时效果较差，并且在处理网络架构时不太方便，因为您需要保存网络并使用应用程序打开它，但它可以创建非常漂亮的可视化效果，并允许您详细浏览网络，甚至下载特定图层的权重。一定要试试！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/0a2d41f2c28b28c1a0849f1140d06b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPl8Ae-duVtbHJ_6EEk51A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络架构——由<a class="ae mm" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> Netron </a>进行可视化。作者图片</p></figure><h1 id="ab6a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">结论，以及可以做得更好的地方</strong></h1><p id="a4d5" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">你已经看到了最上面的免责声明，你已经知道我失败了。好吧，虽然我希望并且努力到最后，但是是时候接受它了。我得到的最好的质量是关于一个<strong class="la iu"> 6k验证错误(MAE) </strong>。没有想象中的那么糟糕，但是一点也不好。</p><p id="3873" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">什么可以做得更好？是的，事实上，几乎一切都是因为任务仍然没有解决。但是让我分享一些关于如何提高模型质量的想法:</p><ol class=""><li id="f3f7" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt oq ma mb mc bi translated">使用更强大的单词嵌入——例如，用BERT代替GloVe</li><li id="02a7" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">使用注意机制代替双向LSTM；</li><li id="f362" class="lu lv it la b lb md le me lh mf ll mg lp mh lt oq ma mb mc bi translated">使用更强大的模型——更多的<code class="fe mi mj mk ml b">LSTM_and_skip_connection</code>模块、类似变压器的架构等。</li></ol><p id="a5d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来有点奇怪，不是吗？在这里，我列出了可以解决问题的特定猜测，那么我为什么不这样做呢？为什么我没有尝试所有这些并告诉你结果呢？</p><p id="31e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上我试过了。最初，我使用50-dim手套嵌入，但也尝试了fastText和ELMo。我尝试了更多的区块，更多的层次，更多的训练时期。我删除了功能(尝试了没有<code class="fe mi mj mk ml b">author_text</code>或没有<code class="fe mi mj mk ml b">comments</code>)，改变了数据预处理和准备，没有任何帮助。但同时，我确信架构和嵌入工作得很好(例如，它能够使用<a class="ae mm" href="https://www.kaggle.com/kritanjalijain/amazon-reviews" rel="noopener ugc nofollow" target="_blank">这个情感分析数据集</a>成功地解决分类问题)。</p><p id="77ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">机器学习中有一个流行的原则(另一个变体<strong class="la iu">垃圾输入——垃圾输出</strong>):</p><blockquote class="ot"><p id="f2c5" class="ou ov it bd ow ox oy oz pa pb pc lt dk translated">如果领域专家不能从数据中得出结论，机器学习模型很可能也不能做到这一点。</p></blockquote><p id="d737" class="pw-post-body-paragraph ky kz it la b lb pd ju ld le pe jx lg lh pf lj lk ll pg ln lo lp ph lr ls lt im bi translated">当然，我不是专家，但评论数据是如此嘈杂和不确定，以至于我无法近似预测不喜欢的数量，甚至无法说用户是否喜欢某个特定的视频！</p><p id="2c74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据不具有代表性——这是主要问题。这并不是因为我收集不正确，但这可能也是我的错。也许，使用一两条最受欢迎的评论是比20条更好的选择，这样可以提高质量。但无论如何，要训练一个好的网络，你需要花费大量的资源主要在数据收集上。而且很遗憾，我没有这样的资源。</p><p id="0fd5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">负面体验也是一种体验。此外，这是一个比积极的经验更有用的经验。所以我才和你分享。这个项目教会了我很多。希望对你也有用，至少有一点。</p><h1 id="74c1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><div class="oe of gp gr og oh"><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" rel="noopener  ugc nofollow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">Keras文档:使用预先训练的单词嵌入</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">作者:fchollet创建日期:2020/05/05最近修改时间:2020/05/05描述:新闻组上的文本分类20…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">keras.io</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ks oh"/></div></div></a></div></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h1 id="4b3a" class="mn mo it bd mp mq pv ms mt mu pw mw mx jz px ka mz kc py kd nb kf pz kg nd ne bi translated">感谢您的阅读！</h1><ul class=""><li id="302d" class="lu lv it la b lb nf le ng lh qa ll qb lp qc lt lz ma mb mc bi translated">我希望这些材料对你有用。<a class="ae mm" href="https://medium.com/@andimid" rel="noopener">在Medium </a>上关注我，获取更多类似的文章。</li><li id="a947" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">如果您有任何问题或意见，我将很高兴得到任何反馈。在评论中问我，或者通过<a class="ae mm" href="https://www.linkedin.com/in/andimid/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae mm" href="https://twitter.com/dimid_ml" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。</li><li id="411c" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">为了支持我作为一名作家，并获得数以千计的其他媒体文章，使用<a class="ae mm" href="https://medium.com/@andimid/membership" rel="noopener">我的推荐链接</a>获得媒体会员资格(不收取额外费用)。</li></ul></div></div>    
</body>
</html>