<html>
<head>
<title>Distillation of BERT-like models: the code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">类伯特模型的提炼:代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a#2022-01-24">https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a#2022-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="5a1e" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">类伯特模型的提炼:代码</h1></div><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d9a2c408ecb484c7463e74f114251a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7faqyVHrGa9JcZoJCfBXaQ.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">就像化学蒸馏一样，我们将从模型中提取重要的东西:知识。照片由<a class="ae kc" href="https://unsplash.com/@elevatebeer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">的</a>上提升</a></p></figure></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="60c2" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">概述</h1><p id="e19f" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">如果你没有注意到，机器学习模型已经变得越来越大，以至于训练一个模型可能会对那些没有多余集群的人造成伤害。此外，即使有一个训练有素的模型，当你的硬件不符合模型对它应该运行的预期时，推理的时间和内存成本也会飙升。因此，为了缓解这个问题，与其说<em class="mg">放弃</em>类似伯特的模型和它们的厚层，不如说被称为<strong class="lk ir">蒸馏</strong>的技术已经被开发出来，用来将网络缩小到一个合理的大小，同时将性能损失降到最低。</p><p id="ae13" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">如果你已经阅读了本系列的第一篇文章，那就不是新闻了，这里是<a class="ae kc" rel="noopener" target="_blank" href="/distillation-of-bert-like-models-the-theory-32e19a02641f">这里是</a>。在这篇文章中，我们讨论了DistilBERT [1]如何引入一种简单而有效的蒸馏技术，这种技术可以很容易地应用于任何类似BERT的模型，但是我们避开了任何具体的实现。现在，我们将进入细节，看看我们如何从想法进入。py文件。</p><p id="8b19" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">本文是关于以DistilBERT的方式提取类似BERT的模型的两部分系列文章的第2部分。对于第一部分，你可以跟随这个<a class="ae kc" rel="noopener" target="_blank" href="/distillation-of-bert-like-models-the-theory-32e19a02641f">链接</a>。然而，如果你认为你已经很好地掌握了蒸馏法，可以跳过阅读。</p><h1 id="dac1" class="kk kl iq bd km kn mm kp kq kr mn kt ku kv mo kx ky kz mp lb lc ld mq lf lg lh bi translated">摘要</h1><p id="8e62" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">一、学生模型的初始化<br/>二。自定义损失函数<br/> III。一个更漂亮的实现。五、结论</p></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="e28d" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">一.学生模型的初始化</h1><p id="1465" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">因为我们想从一个现有的模型初始化一个新的模型，我们需要访问旧模型的权重，也就是老师。我们会认为预先存在的模型是在PyTorch 上实现的<strong class="lk ir">拥抱脸模型，因为它当然是。因此，要获得权重，我们首先必须知道如何访问它们。我们将使用罗伯塔[2]大号作为我们的教师模型。</strong></p><h2 id="9d0c" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">拥抱脸的模型结构</h2><p id="55b5" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我们可以尝试的第一件事是打印出模型，这应该能让我们深入了解它是如何制造的。当然，我们总是可以挖掘拥抱脸文档[3]，但这并不有趣。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="2740" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">运行这段代码后，我们会得到:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nj"><img src="../Images/cef4faecae4feecd7124e307605c1fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9kZBoX9RGKGVPi9v9Gb9g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">简单的罗伯塔印花给人第一印象(图片由作者提供)</p></figure><p id="c1a6" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">模型的结构开始出现，但我们可以让它更漂亮。在拥抱脸模型中，我们可以通过使用。children()生成器。因此，如果我们想要搅动整个模型，我们需要调用。children()，在每个child上产生，等等…这描述了一个<strong class="lk ir">递归</strong>函数，我们在这里编码:</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="1658" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">这给出了:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cc1f1abbdc19a9ea2bb21a9be7868dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*pY8Il4g9e3vcXhwitRJnPA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">罗伯塔的递归窥视(作者图片)</p></figure><p id="e903" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">通过展开这一树，RoBERTa模型的结构似乎与其他类似BERT的模型一样，如下所示:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/0b89a6bcd5ecbe7b4a05930a09554ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MyYe-lT0ksj30jRdpxqnHw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">类伯特模型的架构(图片由作者提供)</p></figure><h2 id="0c1b" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">复制老师的体重</h2><p id="6888" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我们知道，要以DistilBERT [1]的方式初始化一个类似BERT的模型，我们只需要复制除了Roberta层最深层次之外的所有内容，我们忽略了其中的一半。首先，我们需要创建学生模型，其架构与教师相同，但隐藏层数是教师的一半。<br/>要做到这一点，我们只需要使用教师模型的<strong class="lk ir">配置</strong>，它是一个类似字典的对象，描述了拥抱脸模型的架构。当查看roberta.config属性时，我们可以看到以下内容:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/de5443baf0ea0ff3042910dbdae4b01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*ngawu68BS3XPZxXPVhh0Xw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">罗伯塔配置(图片由作者提供)</p></figure><p id="6544" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">我们感兴趣的是num-hidden-layers属性。让我们编写一个函数来复制这个配置，通过将其除以2来更改该属性，并使用新配置创建一个新模型:</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="e780" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">当然，这个函数引入了一个缺失的部分，distill _ roberta _ weights。这个函数会把老师一半的权重放在学生的图层里，但是我们还是需要编码。既然<strong class="lk ir">递归</strong>在探索教师模型方面工作得很好，我们可以用同样的想法来探索和复制它的一部分。我们将同时浏览教师模型和学生模型，在此过程中从一个模型向另一个模型复制零件。唯一的诀窍是小心隐藏层部分，只复制一半。<br/>该函数实现了以下功能:</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="bcea" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">该函数通过递归和类型检查，确保学生模型与教师模型相同，对于Roberta层是安全的。人们可以注意到，如果我们想在初始化教师时改变复制哪些层，只有编码器部分的for循环需要改变。</p><p id="5869" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">现在我们有了学生模型，我们需要训练它。这部分相对简单，除了我们将要使用的<strong class="lk ir">损失函数</strong>。</p></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="77cf" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">二。自定义损失函数</h1><p id="9d4e" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">作为对DistilBERT培训过程的回顾，我们可以看看下图:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/12b8e9da55046bcde6e7d06f5eecc125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZlfE2c6wgDtCT_MmKfgC4A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">蒸馏过程(图片由作者提供)</p></figure><p id="eae2" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">我们将注意力转向写着“损失”的红色大盒子。但是在揭示里面是什么之前，我们需要知道<em class="mg">我们如何</em>收集我们要喂它的东西。从这个图表中，我们可以看到我们需要3样东西:标签、学生和教师的嵌入。标签，我们已经有了，否则，我们可能会有更大的问题。现在让我们得到另外两个。</p><h2 id="4d63" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">检索教师和学生的输入</h2><p id="8911" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">在这里，我们将坚持我们的例子，并使用一个带有<strong class="lk ir">分类</strong>头的RoBERTa来说明这一部分。我们需要的是一个函数，给定一个类似BERT的模型的输入，那么两个张量，input_ids和attention_mask，以及模型本身，将返回该模型的logits。由于我们使用了拥抱脸，这非常简单，我们唯一需要的知识就是往哪里看。</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d796" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">我们为学生和老师这样做，第一个有渐变，第二个没有。</p><h2 id="9f9a" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">编码损失</h2><p id="4928" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">如果损失函数有点模糊，我们可以推荐回到第一篇文章来阅读损失函数。但是，如果您没有时间做这个，这个图表应该有所帮助:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/5f94660f98d78e8d8e425671b2b65a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMSr6nNUoayAKZvMuVqw_A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">蒸馏者的损失(图片由作者提供)</p></figure><p id="735f" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">我们称之为“收敛余弦损失”的是用于对齐两个输入向量的常规余弦损失。要了解更多信息，请参考本系列的第一部分。这是代码:</p><figure class="nd ne nf ng gt jr"><div class="bz fp l di"><div class="nh ni l"/></div></figure></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="4d31" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">三。更漂亮的实现</h1><p id="babd" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">我希望python是一种面向对象的编程语言不会让您感到震惊。因此，由于所有这些函数使用几乎相同的对象，不把它们作为<strong class="lk ir">类</strong>的一部分似乎有些奇怪。如果你想实现这一点，我建议使用一个蒸馏器类来整理代码，就像这个<a class="ae kc" href="https://gist.github.com/remi-or/4814577c59f4f38fcc89729ce4ba21e6" rel="noopener ugc nofollow" target="_blank">要点</a>一样。我们不打算嵌入这个，因为它很长。</p><p id="86a4" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">当然，还缺少一些东西，比如GPU支持，整个训练程序等等。但是DistilBERT所有的关键思想都可以在那里找到。</p></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="64cd" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">四。结果</h1><p id="5e50" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">那么，以这种方式提炼出来的模型最终表现如何呢？对于DistilBERT，可以阅读原文[1]。对于罗伯塔来说，拥抱脸已经有了一个蒸馏过的版本，就在这里<a class="ae kc" href="https://huggingface.co/distilroberta-base" rel="noopener ugc nofollow" target="_blank"/>。在GLUE benchmark [4]上，我们可以比较这两个模型:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi np"><img src="../Images/d3d37fb70d130c54fa7f4720619f7ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*328-2quPW9e2BY-ClGJyBA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">罗伯塔对迪夫罗伯塔(图片由作者提供)</p></figure><p id="904e" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">至于时间和内存成本，这个模型的大小大约是roberta-base的三分之二，速度是Roberta-base的两倍。</p></div><div class="ab cl kd ke hu kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="ij ik il im in"><h1 id="719b" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">动词 （verb的缩写）结论</h1><p id="d5de" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">通过这一系列的两篇文章，您应该有足够的知识来提炼出您遇到的任何类似BERT的模型。但是为什么就此打住呢？自然界充满了蒸馏方法，像TinyBERT [5]或MobileBERT [6]。如果你认为其中一个更符合你的需要，你应该读一读这些文章。谁知道呢，你可能想尝试一种全新的蒸馏方法，因为这是一个日益发展的领域。</p><h2 id="6087" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">谢谢</h2><p id="8bd0" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">感谢我以前在<a class="ae kc" href="http://ProfessorBob.ai" rel="noopener ugc nofollow" target="_blank">professor ob . ai</a>的同事Ha Quang Lee和Meng，他们首先教会了我关于DistilBERT的知识，并纠正了我的代码。</p><h2 id="c24c" class="mr kl iq bd km ms mt dn kq mu mv dp ku lt mw mx ky lx my mz lc mb na nb lg nc bi translated">参考</h2><p id="50ac" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">[1]维克多·桑，弗拉达利出道，朱利安·肖蒙德，托马斯·沃尔夫，<a class="ae kc" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank">蒸馏伯特，伯特的蒸馏版:更小，更快，更便宜，更轻</a> (2019)，拥抱脸</p><p id="d8fe" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">[2]刘、米莱奥特、纳曼戈亚尔、杜、曼达尔乔希、陈、奥梅尔列维、、卢克塞特勒莫耶、韦塞林斯托扬诺夫、<a class="ae kc" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">罗伯塔:稳健优化的伯特预训练方法</a> (2019)、arXiv</p><p id="b488" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">[3]拥抱脸团队归功于朱利安·肖蒙德，<a class="ae kc" href="https://huggingface.co/docs/transformers/model_doc/roberta" rel="noopener ugc nofollow" target="_blank">拥抱脸的罗伯塔文档</a>，拥抱脸</p><p id="9cf3" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">[4] Alex WANG，Amanpreet SINGH，Julian MICHAEL，Felix HILL，Omer LEVY，Samuel R. BOWMAN，<a class="ae kc" href="https://arxiv.org/pdf/1804.07461v3.pdf" rel="noopener ugc nofollow" target="_blank"> GLUE:一个用于自然语言理解的多任务基准和分析平台</a> (2019)，arXiv</p><p id="7936" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">[5]焦，尹宜春，，尚，辛江，，，，李，，<a class="ae kc" href="https://arxiv.org/pdf/1909.10351.pdf" rel="noopener ugc nofollow" target="_blank"> TinyBERT:蒸馏BERT用于自然语言理解</a> (2019)，arXiv</p><p id="092f" class="pw-post-body-paragraph li lj iq lk b ll mh ln lo lp mi lr ls lt mj lv lw lx mk lz ma mb ml md me mf ij bi translated">[6]孙志清，于鸿坤，，宋，，刘，，丹尼·周，<a class="ae kc" href="https://arxiv.org/pdf/2004.02984.pdf" rel="noopener ugc nofollow" target="_blank"> MobileBERT:一种面向资源受限设备的紧凑任务不可知BERT</a>(2020)，arXiv</p></div></div>    
</body>
</html>