<html>
<head>
<title>Hands On Image Compression using Principal Component Analysis, with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python通过主成分分析进行图像压缩</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-image-compression-using-principal-component-analysis-with-python-1cd6cad72366#2022-01-31">https://towardsdatascience.com/hands-on-image-compression-using-principal-component-analysis-with-python-1cd6cad72366#2022-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/45b3f855e8180f91f390c35653a5daa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D7Fj76A9aAfMy8F9p4wYsw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/s/photos/small?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的记者拍摄</p></figure><div class=""><h1 id="afd8" class="pw-post-title je jf jg bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">使用Python通过主成分分析进行图像压缩</h1></div><div class=""><h2 id="e637" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">从理论到实践，这就是如何使用PCA压缩你的图像而不破坏质量</h2></div><p id="c5ba" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我和埃森哲<a class="ae jd" href="https://www.accenture.com/us-en?c=acn_glb_brandexpressiongoogle_12722872&amp;n=psgs_1221&amp;&amp;c=ad_usadfy17_10000001&amp;n=psgs_Brand-%7c-US-%7c-Exact_accenture&amp;gclid=Cj0KCQiAi9mPBhCJARIsAHchl1zGVurARH3g4nQKqa8WBZgAE0gxEfbulbQYZ0hrLC72izQCidNYa1caAuXpEALw_wcB&amp;gclsrc=aw.ds" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"/></a>一起写硕士学位论文时，我们曾经使用一个确定的固定大小的图像。特别是，我们正在研究一个非常著名的机器学习挑战，它被称为<a class="ae jd" href="https://fastmri.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">快速MRI </strong> </a> <strong class="kx jh"> </strong>，是由<a class="ae jd" href="https://ai.facebook.com/" rel="noopener ugc nofollow" target="_blank">脸书人工智能研究</a>(现在的<strong class="kx jh">元人工智能</strong>)提出的。</p><p id="0aa9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我不会说得太详细，但我们有一个具体的目标，那就是提高这些图像的质量。一旦我们做到了这一点，我们开始问自己是否有办法以压缩的形式存储这些大图像，以节省一些存储空间。</p><p id="6659" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们决定应用<strong class="kx jh">主成分分析(PCA)。</strong></p><p id="db01" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望这篇(相当长的)介绍能让你了解PCA是如何在<strong class="kx jh">行业</strong>和<strong class="kx jh">研究</strong>中使用的，因为它是一种非常强大而又相当简单的算法，可以<strong class="kx jh">降低你的图像的维度</strong>并节省一些空间<strong class="kx jh">而不会真正破坏你的图像质量</strong>。</p><p id="2258" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们开始吧。</p><h2 id="33a4" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">1.直观的解释</h2><p id="7339" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">好吧，假设你有这样的数据:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="ce64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你看看这些数据，你会发现一个围绕对角线的特定分布。在某种程度上，如果你只考虑对角线，你也许可以改变数据的维度，从二维转换到一维。</p><p id="63bc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">主成分分析方法正是这样做的。它会找到您拥有最多数据集信息的方向，并将您的数据投影到这些方向上。</p><p id="e948" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看结果:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="8f74" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如你所看到的,<strong class="kx jh">组件0 </strong>非常有用，你可以使用这个组件来绘制数据。</p><p id="f209" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设你的数据集中有M个维度。当您使用PCA时，您最终会得到:</p><ul class=""><li id="8497" class="mv mw jg kx b ky kz lb lc le mx li my lm mz lq na nb nc nd bi translated">P<m/></li><li id="41cf" class="mv mw jg kx b ky ne lb nf le ng li nh lm ni lq na nb nc nd bi translated">允许您切换回原始尺寸的模型</li></ul><p id="b77d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当P &lt;<m because="" obviously="" storing="" a="" model="" and="" small="" matrix="" is="" computationally="" cheaper="" than="" huge="" matrix.=""/></p><p id="b233" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Imagine that your data is an image. By all means, this is just a matrix of numbers, let’s say N rows and M columns. What you can do is reduce the columns of your matrix and have a N x P matrix with P &lt;<m. in="" this="" way="" you="" will="" have="" a="" smaller="" matrix="" and="" model="" to="" convert="" your="" image="" back="" the="" original="" matrix.=""/></p><blockquote class="nj nk nl"><p id="b632" class="kv kw nm kx b ky kz kh la lb lc kk ld nn lf lg lh no lj lk ll np ln lo lp lq ij bi translated">The next chapter is pretty technical and explains exactly how PCA does what we said it does. Nonetheless, above there should be all you really need to know to go on and produce a practical implementation of the PCA method. So you should be able to implement the PCA and understand it even if you skip chapter 2.</p></blockquote><h2 id="2ce4" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">2. Technical Explanation</h2><p id="7412" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">So that was the idea. Let’s try to make it more technical. Let’s say that the matrix (image) is <strong class="kx jh"> A </strong>时，这允许具有重要的计算优势。假设A是m x p <br/>那么，我们来定义矩阵<strong class="kx jh"> B: </strong></p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e72bd55fcedd7059be66bf65a03df65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:158/1*7y-P9dz5F2mWZj5IZvJ7lA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我用<a class="ae jd" href="https://latex.codecogs.com/eqneditor/editor.php" rel="noopener ugc nofollow" target="_blank">这个工具</a>制作的图像。</p></figure><p id="c3c0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">特征值分解如下:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1bab91fab958136fe993f45a6b390a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/1*qzsn3oA3zv5uy8BYXL2iCQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我用<a class="ae jd" href="https://latex.codecogs.com/eqneditor/editor.php" rel="noopener ugc nofollow" target="_blank">这个工具</a>制作的图像。</p></figure><p id="23c9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些分解矩阵是:</p><ul class=""><li id="8f6c" class="mv mw jg kx b ky kz lb lc le mx li my lm mz lq na nb nc nd bi translated"><strong class="kx jh"> V </strong>是矩阵的特征向量的p×p正交矩阵。</li><li id="d3bf" class="mv mw jg kx b ky ne lb nf le ng li nh lm ni lq na nb nc nd bi translated"><strong class="kx jh">λ</strong>是一个p×p的正方形对角矩阵。对角线上的条目是非负实数的特征值。</li></ul><p id="abc8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PCA定义的矩阵如下:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/992d77fe04ee8965a489ac120aa8c628.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/1*4RAzb49XNRE_D4ufO2p69A.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我用<a class="ae jd" href="https://latex.codecogs.com/eqneditor/editor.php" rel="noopener ugc nofollow" target="_blank">这个工具</a>制作的图像</p></figure><p id="ea49" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它是在这p个标准正交特征向量上的投影。它的形状和原来的m x p一样。</p><p id="522c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果使用所有的特征向量，可以使用下面的公式重建<strong class="kx jh"> A </strong>。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/dfd2663483b032ac665589818b8b1e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/1*pEDO3qKG2ZVwSdwTgW5LQA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我用<a class="ae jd" href="https://latex.codecogs.com/eqneditor/editor.php" rel="noopener ugc nofollow" target="_blank">这个工具</a>制作的图像</p></figure><p id="dd19" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是因为特征向量是正交的。通常是前几个(按特征值降序排列)特征向量包含原始矩阵<strong class="kx jh"> A </strong>的大部分整体信息。在这种情况下，我们可以只用少量的特征向量来重构矩阵<strong class="kx jh"> A </strong>。例如我们可以用k个&lt; &lt; p个特征向量。k个向量按照<strong class="kx jh">方差</strong>值排序。使用它们，你可以有一个更小的矩阵<strong class="kx jh">、一个</strong> _PCA和一个模型(我们讨论过的矩阵乘法)来将其转换成原始大小的矩阵。</p><h2 id="3e7f" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">3.实际实施</h2><p id="812d" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">即使执行上述操作并不十分困难，也有一些非常实用的工具可以帮你完成并加速这个过程。伟大的sklearn可能是数据科学领域最著名的一个。这里是<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">的<strong class="kx jh">文档</strong>的</a>。</p><p id="033b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用一张免费下载的<a class="ae jd" href="https://unsplash.com/photos/NE1ePoe3gSI" rel="noopener ugc nofollow" target="_blank">我的团队成员(❤):)的图片</a></p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/f4c2582f6504c1105debc51a95cf25ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13kqo1Xfx9qZ_RtqfzFVvw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@tamentali?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Danilo Obradovic </a>在<a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="24bd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们要做这些事情:</p><ol class=""><li id="645c" class="mv mw jg kx b ky kz lb lc le mx li my lm mz lq nv nb nc nd bi translated"><strong class="kx jh">对不同数量的组件应用</strong>PCA方法</li></ol><p id="5a5f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.<strong class="kx jh">重建</strong>组件数量较少的图像</p><p id="367e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3.<strong class="kx jh">评估</strong>重建图像的质量，将其与原始图像进行比较</p><p id="462e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们分析了1。第二。工作细节，所以我们将花一些时间进行评估。</p><p id="3b64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有许多方法来评估具有地面真实性的重建图像的质量。尽管如此，为了保持简单，让我们使用一个相当通用但有用的方法，即<a class="ae jd" href="https://en.wikipedia.org/wiki/Matrix_norm" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">归一化的Frobenius范数</strong> </a> <strong class="kx jh"> </strong>的差。</p><p id="54dc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们用4种不同数量的成分:<strong class="kx jh"> 0% </strong>、<strong class="kx jh"> 33% </strong>、<strong class="kx jh"> 66% </strong>和<strong class="kx jh"> 100% </strong>。</p><p id="b2e5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们绘制重建的图像:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="655f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我觉得这个<strong class="kx jh">很漂亮</strong>。第一次重建绝对是扯淡。当然，当我们增加组件的数量时，重建变得与原始图像相似。尽管如此，这意味着<strong class="kx jh">我们也在减少我们的计算优势</strong>，因为我们存储了更多的组件(减少的图像的维度增加)。</p><p id="16e0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们绘制原始图像和重建图像之间的距离(Frobenius范数),作为组件数量的变化:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="2828" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我在专业或研究环境中使用它，我可能会使用更多的点来更好地拟合曲线，并了解最佳的组件数量。然而，我觉得这对于我们的目的来说已经足够了，因为<strong class="kx jh">表明当我们增加组件的数量时，重建变得更好，</strong>这是完全合理的。</p><h2 id="0a8d" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">4.结论</h2><p id="3a5f" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在某些情况下，降维是<strong class="kx jh">必须</strong>做的事情。当你必须对高维图像进行预处理时，看看你是否可以使用PCA来存储低维图像是一个很好的实践，并且让你这样做的代码非常简单。</p><p id="3738" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，你想知道更多关于机器学习的知识，或者你只是想问我一些你可以问的问题:</p><p id="ce33" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">A.在<a class="ae jd" href="https://www.linkedin.com/in/pieropaialunga/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh"> Linkedin </strong> </a>上关注我，在那里我发布我所有的故事<br/> B .订阅我的<a class="ae jd" href="https://piero-paialunga.medium.com/subscribe" rel="noopener"> <strong class="kx jh">简讯</strong> </a>。这会让你了解新的故事，并给你机会发短信给我，让我收到你所有的更正或疑问。<br/> C .成为<a class="ae jd" href="https://piero-paialunga.medium.com/membership" rel="noopener"> <strong class="kx jh">推荐会员</strong> </a>，这样你就不会有任何“本月最大数量的故事”，你可以阅读我(以及成千上万其他机器学习和数据科学顶级作家)写的任何关于最新可用技术的文章。</p><p id="5e18" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">再见:)</p></div></div>    
</body>
</html>