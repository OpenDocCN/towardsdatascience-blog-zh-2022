# 马尔可夫决策过程的五个组成部分

> 原文：<https://towardsdatascience.com/the-five-building-blocks-of-markov-decision-processes-997dc1ab48a7>

## 通过掌握马尔可夫决策过程的基本原则来定义和交流您的强化学习模型

![](img/fa890c5eb488c5dc9018f3919412f10e.png)

照片由 [Mourizal Zativa](https://unsplash.com/@mourimoto?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

正确制定**马尔可夫决策过程** (MDP)的能力对于成功的强化学习(RL)实践者是必不可少的。需要一个清晰的数学模型来(I)正确地编码它并训练解决算法，(ii)用统一的语言与利益相关者交流，以及(iii)明确抽象和简化。

MDP 作为一个框架来形式化顺序决策问题，其中随着时间的推移决策被分解成一系列子问题。在规范形式中，MDP 由一个**元组** `(S,A,R,P,γ)`描述。本文将详细阐述每个元素。

# 状态空间

状态`s∈S`包含计算决策、奖励和转换所需的所有信息；*状态空间*T2 是包含所有状态的集合。MDPs 的一个核心概念是决策完全基于系统的当前状态(无记忆属性)；过去的状态不应该成为决策的因素。一个常见的误解是，一个国家不能纳入*过去的信息*；这实际上是允许的(高阶马尔可夫模型，见 Salnikov 等人)。

在某些情况下，状态是一个简单的标量，例如代理在一维线上的位置。通常，状态将是一个**向量**。考虑一个机械臂，其状态由三个关节的角度`s=[x,y,z]`描述。股票投资组合状态可以用当前价格水平和每只股票的持有量`s=[p_1,p_2,…,p_n,s_1,s_2,…,s_n]`来描述。一个大型仓库可能有 10，000 个条目状态，描述每个产品的数量。

没有必要停留在物理世界的边界。鲍威尔区分了三种状态成分:(一)物质，(二)信息，(三)知识。大纲如下。

> **一、身体**
> 
> “身体的”这个词在这里可能用得有点不严格；想想卡车的位置，你银行账户上的钱，或者无人机的电池电量。它捕获系统或可用资源的物理属性。不幸的是，许多建模者将状态等同于物理属性，因此忽略了相关信息。
> 
> **二。信息**
> 
> 除了直接从系统中获得的属性，我们还经常在决策中使用额外的信息。如果我们有一个股票投资组合，股票价格(市场数据)就是信息。对于一个风电场，它可能是风速和风向。
> 
> 信息也可以与过去或未来有关。过去的股票价格可能揭示相关的趋势，这些趋势不能仅仅从现在的价格中获得。如果我们有一个下周确认的大生产订单，这是本周起草生产计划时的相关信息。
> 
> 这些类型的信息似乎与无记忆属性相冲突。然而:如果你知道世界末日是在下周四，你可能不会在周五完成那份报告，对吗？我们只是简单地包括我们知道的州的相关信息。
> 
> 重点是‘知道’。从蒙特卡洛模拟或建模者的知识中，我们可以得到许多代理人不知道的信息。应该从决策代理人的角度添加信息，决策代理人不具备透视技能。
> 
> **三。信念**
> 
> 我们可能不仅仅基于已知信息做出决策，还会基于不确定的信息或信念。我们现在可以测量出准确的风速，但在本周的其余时间里，我们要依靠天气预报。仓库可以利用销售预测模型。过去的股票价格可以用来预测未来的股票价格。对于信念，我们通常指的是捕捉某些知识的概率分布。

作为指导方针，州政府应该包含**所有用于做出明智决策的相关数据**——不多也不少。如果某些数据不影响决策、奖励或转换，则不应将其纳入状态。

[](https://wvheeswijk.medium.com/a-deep-dive-into-problem-states-498ad0746c98)  

# 行动空间(A)

对于 MDPs 来说，不可或缺的是对系统进行某种程度的控制的能力。动作`a∈A` —在某些域中也称为*决策*或*控制*—描述了代理的这种影响；动作空间`A`包含所有(可行的)动作。至于状态，动作可以是简单的标量(*‘行权选项*`*a∈{0,1}*`*’*)，也可以是高维向量(*‘订单* `*a_i*` *产品类型*`*i*`*’*)。

对于**状态无关的**动作，相同的动作空间`A`适用于每个状态。对于很多问题来说，动作是**状态依赖的** (动作空间`A(s)`):棋盘上的走法取决于当前的棋子及其位置，蓄水池不能排到负数。有时，假设动作空间是状态无关的(例如，在[深度 Q 学习](/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e)中)并简单地屏蔽不可行的动作是方便的。

对于允许各种排列的基于向量的决策，动作空间的枚举可能会成为问题。此外，通常很难**评估行动是否可行**。考虑原材料、优先关系、存储能力、机器可用性等有限的生产线的决策。仅仅验证一个时间表是否有效就构成了一个复杂的子问题。在这种情况下，我们需要使动作空间服从一组约束，例如，通过将动作问题公式化为数学程序**。一个额外的好处是，它大大增加了可以处理的行动空间的大小。**

**[](/using-linear-programming-to-boost-your-reinforcement-learning-algorithms-994977665902)  

# 奖励函数(R)

MDP 模型需要优化一些*目标*函数，通常是(贴现的)累积报酬。*奖励*功能`R(s,a)`捕捉**直接奖励**，与个体决策时刻相关的成本或贡献，依赖于状态`s`和/或行动`a`。有时回报是显而易见的，例如股票交易带来的金钱收益和损失。在其他情况下，需要更多的人工构造，例如赢得游戏或达到目标的奖励。然后，有必要将奖励与解决问题的目标结合起来。

非零奖励不一定在每个时期发生，例如，它们可能仅在一集的结尾被接收(赢得/输掉一场游戏)。在 RL 中，我们经常求助于**奖励塑造**，引入中间奖励来识别‘好’的行动，并指导算法。奖励塑造应该小心使用，因为它改变了原来的问题设置，并可能引入意想不到的行为。

# 转移函数(P)

在 MDP 中，我们采取离散的时间步骤，从一个状态(`s`)移动到下一个状态(`s’`)。这个*转换函数*取决于(I)当前状态`s`、( ii)动作`a`和(iii)随机信息`ω`。转换可以分解为确定性部分和随机部分，其中`a`描述确定性部分，而`ω`描述随机部分(尽管通常是混合，转换可能是完全确定性的或完全随机的)。

在 MDP 公式中，转移函数通常隐藏在看似简单的概率中`P(s’|s,a)`。确实具有欺骗性，因为它通常是最难建模的组件。事实上，转换函数的范围可能从一个平凡的确定性函数(例如 GridWorld 中的一个步骤)到一个高度复杂的[现实的数字孪生](/need-help-making-decisions-ask-your-digital-twin-6e4cf328cb0)表示，有许多不确定性需要建模。

虽然转移函数是 MDP 的必要组成部分，但在 RL 中我们不一定要明确定义它。相反，我们可以**观察来自环境**的转变 **。如果我们学习玩超级马里奥游戏，我们可能不知道实际的转换函数；我们只是试着移动，然后观察会发生什么。我们甚至可以利用真实世界的观察(例如，当控制取决于天气情况的风电场时)。在这种情况下，世界本身就是我们的过渡函数！**

[](/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40) [## 什么是后决策状态？他们想从我们这里得到什么？

towardsdatascience.com](/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40) [](/about-post-decision-states-again-5725e5c15d90)  

# 折扣系数(γ)

*贴现因子* `γ ∈ [0,1]`并不总是包含在 MDP 元组中，因为它对于有限范围是可选的。简而言之，它表明**未来奖励在多大程度上被纳入当前决策**，其中`γ=0`完全不考虑未来奖励，而`γ=1`平等地权衡所有未来奖励。使用贴现率有各种各样的理由，主要的一个理由是确保无限期问题的价值函数收敛。

任何设置`γ<1`都足以**保证(理论上的)收敛**，但是精确的设置对于优化目的很重要。一个普遍的经验法则是，系统的不确定性越大，贴现率就越高，这反映了今天的行动对未来表现的影响有限。

[](/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942)  

# 摘要

马尔可夫决策过程由以下构件组成:

*   **状态空间**状态可以分为**物理** -、**信息** -和**信念**属性，并且应该精确地包含前述目的所需的属性。
*   **动作空间**T2——包含所有(可行)动作的集合。对于**依赖于状态的决策** `a(s)`，可能需要使动作空间服从一组约束，例如，使用数学编程。
*   **奖励功能** `R` —表示`s`状态下`a`动作时**直接奖励**。在许多问题中，奖励不是自明的，需要**奖励塑造**来完成解决问题的目标。不良的奖励功能可能会导致不良行为。
*   **转移函数** `P` —控制系统随时间的**动态**的函数，引导代理从状态`s`到`s’`。该转换通常包括确定性成分(动作`a`)和随机成分(外部信息`ω`)。**可以从外部环境中观察到**转换，而无需对其进行显式建模。
*   **贴现因子** `γ` —定义未来奖励对当前决策的影响程度。当问题是无限时域的并且依赖于累积报酬目标函数时，贴现率`γ<1`对于**确保收敛**是必要的。贴现率通常根据环境的不确定性来设定。

每个构件都可能隐藏着巨大的挑战。MDP 的建模方式可能会显著影响求解算法的有效性。此外，在商业和学术环境中，定义清晰的 MDP 的能力对于统一交流问题抽象非常重要。

虽然 MDP 公式可能有点麻烦，但从长远来看，正确地使用它是值得的。

## **参考文献**

鲍威尔，W. B. (2019)。随机优化的统一框架。*欧洲运筹学杂志*， *275* (3)，795–821。

使用高阶马尔可夫模型揭示网络中基于流的社区。 *Sci Rep* **6、** 23194 (2016)。[https://doi.org/10.1038/srep23194](https://doi.org/10.1038/srep23194)**