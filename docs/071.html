<html>
<head>
<title>Assumptions in OLS Regression — Why do they matter?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OLS回归中的假设——它们为什么重要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/assumptions-in-ols-regression-why-do-they-matter-9501c800787d#2022-01-04">https://towardsdatascience.com/assumptions-in-ols-regression-why-do-they-matter-9501c800787d#2022-01-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="00fc" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">OLS回归中的假设——它们为什么重要？</h1></div><div class=""><h2 id="9e88" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么你应该关心他们，他们是如何建立或破坏你的回归模型的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/49f8bf650bba2f0f31509823496e4775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3oV4tm4gc-N0UFrUGEUdg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="184d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当然，你知道线性回归！</p><p id="0fbe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">很有可能，你已经用R和Python运行了数百个线性回归模型。你只需要从R/Python库中调用一个函数，瞧！</p><p id="31d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这还不是全部。对于如何运行回归诊断，您也有一个合理的想法(或者可能是您的数据管道为您做的！).</p><p id="bcba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，为什么回归最基本的东西并理解“假设”有那么重要呢？</p><h1 id="4065" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">为什么？</h1><p id="8e4b" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">事实证明，对这些假设的合理理解似乎过于“理论化”，最重要的是，可以接受的是，对于大多数数据科学初学者(包括我自己)来说，<em class="mo">有点无聊</em>，他们希望投入建模并测试他们的模型的预测能力。</p><p id="5c96" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，模型得出的结果可能从“无效”到“无用”，这取决于这些假设是否得到满足(模型仍然会给出那些结果！).大量的ML和统计建模尝试失败的原因只有一个——缺乏正确的问题定义，没有关注模型的基本理论。</p><p id="5bfe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在数据科学中，更常见的是在计量经济学中，通常最重要的不是简单的预测，而是建立可靠的因果联系，使人们能够操纵自变量以实现因变量的预期结果。如果一个计算模型的参数由于理论上的不一致而被直接拒绝，我们就不能这样做！</p><p id="b2c4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然许多讨论这一主题的教科书需要耐心地浏览所有的方程和符号，但许多关于这一主题的在线文章完全跳过了数学，解释也变得过于肤浅。目前的文章介于两个极端之间，主要目的是容易地提供正确的直觉；同时为感兴趣的读者提供了一个对数学的初步了解。</p><p id="0ccb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总结一下——这篇文章用简单的英语<strong class="kx ir"> </strong>(以及一些对大脑友好的例子)解释了以下内容</p><ol class=""><li id="760a" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">OLS回归的假设到底是什么？有些教科书说有5个，有些说有6个，有些说有10个…让我们永远解决这个困惑吧！</li><li id="8ce0" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">最重要的是，每个假设在数学上有什么贡献？</li><li id="53ea" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">什么会真的(<strong class="kx ir"> <em class="mo">真的！</em> </strong>)发生，如果其中任何一条被违反了呢？</li></ol></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="c1e0" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">在我们开始之前你必须知道的。</h1><p id="7789" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在我们开始之前你需要一些大脑纹身。</p><ol class=""><li id="4464" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">‘线性回归’是一个<strong class="kx ir">模型</strong>。“普通最小二乘法”，缩写为OLS，是模型参数的<strong class="kx ir">估计器</strong>(在许多其他可用的估计器中，例如<em class="mo">最大似然</em>)。了解模型和它的估算者之间的区别是至关重要的。</li><li id="2ba8" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">当我们讨论假设时，需要注意的是，所有这些假设都是对<strong class="kx ir"><em class="mo">【OLS估计量】</em> </strong>的<em class="mo">假设(而<strong class="kx ir">不是对线性回归模型本身的</strong>)。</em></li><li id="9cb7" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">正如我们将看到的，这些假设中的一些是模型的基本有效性所需要的，另一些是任何估计量的一些特定的“期望的”统计特性所需要的。当最有意义的时候，我们会介绍它们(例如高斯马尔可夫定理的大脑友好版本)。</li><li id="1eb9" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">虽然不同教材和文章中假设的数量和顺序有所不同；从概念上讲，为了更好地理解，我们可以把几个假设放在一个标题下。这给了我们总共<strong class="kx ir">七个</strong> <strong class="kx ir">假设</strong>，这将在下面讨论。</li><li id="4399" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">为了符号的方便，我们坚持线性回归的经典矩阵符号</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6224f035ad9b6640b2d4ac54f9afae84.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*-PIFa3ClPXFn-CcWhOMYcA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="baee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<strong class="kx ir"> <em class="mo"> y </em> </strong>为响应变量向量，<strong class="kx ir"> ϵ </strong>为随机扰动向量，<strong class="kx ir"> X </strong>为自变量数值矩阵(其中<strong class="kx ir"> n </strong>行数据点，<strong class="kx ir"> k </strong>列回归变量<strong class="kx ir"> xᵢ </strong> <em class="mo">包括截距)。</em>第一列<strong class="kx ir"> x₁ </strong>仅包含1，以说明截距参数β₁的系数)</p><p id="9b2c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在来看假设，让我们直接开始吧！</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="8d70" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">1.线性</h1><p id="a3fa" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">虽然这看起来是最基本的，但我向你保证它不是。</p><p id="a38d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是唯一可能同时适用于OLS <strong class="kx ir">估计器</strong>和LR <strong class="kx ir">模型</strong>本身<strong class="kx ir">、</strong>的假设，原因略有不同。</p><h2 id="f0a8" class="nq ls iq bd lt nr ns dn lx nt nu dp mb le nv nw md li nx ny mf lm nz oa mh ob bi translated">首先，线性对于“模型”意味着什么？</h2><p id="f4ba" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">嗯，线性回归是很容易理解的，“线性”。</p><p id="93b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当您选择线性回归作为备选模型时，您打算拟合一条<strong class="kx ir">线</strong>，或者更一般地说，拟合一个<strong class="kx ir">超平面</strong>来解释回归量y和回归量X(x₁、x₂,…,xₖ).之间的关系</p><p id="6965" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您选择使用LR一定是基于对数据的某种预先探索。例如，你可能在<strong class="kx ir"> Y </strong>和每个<strong class="kx ir"> xᵢ </strong>之间画了一个老式的散点图，并注意到一个没有任何明显曲率的相当线性的图案。</p><p id="64d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是如果你注意到y和某个xᵢ的曲线有明显的弯曲呢？还能用LR吗？</p><p id="036d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不。用一条线(超平面)来拟合一个明显的非线性关系是没有意义的。这不是一个很好的契合，也违背了建模的目的。那你是做什么的？</p><p id="7945" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是<strong class="kx ir"> <em class="mo">变量转换</em> </strong>的用武之地。LR不需要y总是与x线性相关。它需要的是y的函数，比如说<em class="mo"> f(y) </em>与x的函数线性相关，比如说<em class="mo"> g(x) </em>这样我们就可以将关系建模为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/8a44edad53b8268eabc041913556fcd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/1*SInxBJAT5nlHeLpiVoZjPA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6c99" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">尽管y和x之间的关系不完全是线性的，但以上所有都是有效的线性回归模型。</p><p id="fbb2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">警告:虽然变量转换可以模拟回归变量和回归变量之间的非线性关系，但它可能会使系数的解释有点棘手。</p><h2 id="0ad6" class="nq ls iq bd lt nr ns dn lx nt nu dp mb le nv nw md li nx ny mf lm nz oa mh ob bi translated">第二，线性对于“OLS估计量”意味着什么？</h2><p id="e4a4" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated"><em class="mo">真实</em>形式OLS假设不是关于“变量的线性”，而是“参数的线性”。</p><p id="2812" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事情是这样的，一般来说，这不是你需要确保的事情。这是一个理论上的假设，OLS推导真的</p><p id="a17b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">OLS估计量分两步导出:</p><ol class=""><li id="35ad" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">得到“误差平方和”表达式相对于每个βᵢ.的偏导数这会给你k个表情。使这些都等于零(对于最小的表达式，一阶导数应该为零)。所以现在你有k个未知量的k个方程。</li><li id="ca49" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">当你求解β系数估计向量的这些方程时，你得到如下的封闭形式的解(用矩阵符号)</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a86cf4d60e74caa4bbe2b8dd6e4befd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/1*EpXmHjV6aPl99VukZ8Bo9w.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e5c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这样找到的上述解依赖于我们在上述步骤1中获得的方程。<strong class="kx ir">如果βᵢ的模型不是线性的</strong>，这些方程看起来会完全不同，上面第2点的解决方案也是如此。</p><p id="e177" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，参数的线性是OLS回归的基本假设。然而，每当我们选择进行OLS回归时，我们只需要确保<strong class="kx ir">‘y’</strong>和<strong class="kx ir">‘x’</strong>(或变换后的’<strong class="kx ir">y’</strong>和变换后的’<strong class="kx ir">x’</strong>)是线性相关的。在OLS估计过程本身中假设β的线性。虽然我们不需要太担心，但理解它是很重要的。</p><h1 id="c963" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">2.满秩</h1><p id="2cf4" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">这个假设本身包含三个假设。</p><ol class=""><li id="5a46" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">任何回归变量之间都不应该有任何完美的多重共线性。</li><li id="491f" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">观察值的数量(矩阵X的<strong class="kx ir"> n </strong>行)应该大于回归量的数量(矩阵X的<strong class="kx ir"> k </strong>列)。</li><li id="99e1" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">对于简单线性回归的情况，回归量x的所有值不应该相同。</li></ol><p id="0f17" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然许多教科书分别提到了这些假设，但这些假设可以被准确地理解为一个假设，即回归矩阵X应该是“满秩”的。</p><p id="8ca2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">矩阵“满秩”是什么意思？简单地说，你不能将矩阵中任何一列的值表示为其他列的线性组合。换句话说，所有列应该彼此线性独立。如果一个列可以用其他列来线性表示，那么这个列就是多余的，矩阵就变得“秩亏”了。</p><p id="62db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当一个矩阵秩亏(非满秩)时，它的行列式为零，因此它的逆矩阵不存在。因此，确定β系数的OLS“公式”不起作用</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a64b31091a7d1e3c7d1b665b950523ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*-D40BBce05e1IdP6pO8aXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d2d6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，为什么上述3个假设都归结为一个单一的假设“满秩”？</p><p id="3828" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们再仔细看看这三个假设。</p><ol class=""><li id="620e" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">如果任意两个或多个回归量之间存在完美的多重共线性，矩阵X没有“k”个线性无关列。因此，<strong class="kx ir">不是满秩</strong>。</li><li id="6e4f" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">如果n <k then="" the="" rank="" of="" matrix="" is="" less="" than="" k="" class="kx ir">它没有全列排名。因此，当我们有<strong class="kx ir"> k </strong>个回归变量时，我们至少应该有<strong class="kx ir"> k </strong>个观测值，以便能够通过OLS估计β。</k></li><li id="eaca" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">对于<strong class="kx ir">简单线性回归，</strong>如果自变量X的所有值都相同，则矩阵X中有两列，一列全为1(截距项的系数)，另一列全为<strong class="kx ir"> <em class="mo"> c </em> </strong>(回归变量的常数值)<strong class="kx ir"> <em class="mo">、</em> </strong>，如下所示。显然，在这种情况下，矩阵也是<strong class="kx ir">非满秩的，</strong>逆不存在，<strong class="kx ir"> β无法确定。</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b2c0add1d7cc074f07ba010d0f265532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*-qY19Vgw1DD-eEsiBrCfDQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="2261" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">3.扰动的零条件均值</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9c8ed38f9c84e70ea047af5eb97db42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*mSqbjE4zLbcQpQ2JhnWwTg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c503" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">扰动项ϵᵢ应该独立于回归量xᵢ.它的期望值应该为零。</p><p id="5a9a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们用两个问题来解开上述问题:</p><p id="d928" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> <em class="mo">首先，为什么不同观测值之间扰动项的均值应该为零？</em>T25】</strong></p><p id="a871" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">嗯，<em class="mo">误差或干扰</em>很小(希望如此！)与模型预测的估计值的随机偏差。如果模型预测精确到小数点后一位，那就奇怪了！因此，偏差是很正常的。</p><p id="e033" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，虽然干扰是可预期的，但我们不期望这些干扰总是<strong class="kx ir">正的(这意味着模型<strong class="kx ir">总是</strong>低估实际值)或<strong class="kx ir">总是</strong>负的(这意味着我们的模型<strong class="kx ir">总是</strong>高估实际值)。</strong></p><p id="2685" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">相反，扰动不应该有任何固定的模式。换句话说，我们希望我们的错误是随机的——有时是正的，有时是负的。大致来说，所有这些单个误差的<strong class="kx ir">和</strong>将相互抵消，变得等于零，从而使误差项的平均值(或期望值)也等于零。</p><p id="3af5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> <em class="mo">其次，为什么误差项的均值要独立于回归变量？</em> </strong></p><p id="3f86" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设你想预测你办公室里的员工的年收入，仅仅使用他们的工作经验。你有三组人——第一组有不到3年经验的员工，第二组有3到7年的经验，第三组有7到10年的经验。</p><p id="efe2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您创建了一个LR模型，并预测了每个组中每个成员的估计收入。接下来，为了查看您的模型的真实表现，您计算了所有三组成员的平均误差(称之为E(ϵᵢ)。此外，您想知道您的模型在预测特定群体的收入方面是否特别好，因此您还分别计算了3个群体(E(ϵᵢ|group1、E(ϵᵢ|group2和E(ϵᵢ|group3)).)的平均预测误差</p><p id="1c22" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在的问题是，你是否希望你对一组<strong class="kx ir"> <em class="mo">的预测比对另外两组</em> </strong>的预测更错误？</p><p id="14f0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当然不是！您希望公正地评估每一组，并且对每一组都同样正确(或不正确)!因此，理想情况下，单个组的平均误差应该与总平均误差相同。</p><p id="1980" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">e(ϵᵢ)= e(ϵᵢ|group1)= e(ϵᵢ|group2)= e(ϵᵢ|group3)</p><p id="a409" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此,“经验年数”这个变量的值不应该在决定你的估计有多错误上起任何作用。</p><p id="ead2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">概括地说，在一个好的模型中，回归量xᵢ本身应该不包含关于误差ϵᵢ的有用信息——它们应该与误差项“无关”(回归量应该是“外生的”)。</p><p id="ecd8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在只要结合以上两个问题的答案，你就得到我们的第三个假设零条件均值，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/a62c80df0318c79df09191e96ff0a449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEQ4tBkaoO91yp28dnAqTQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="639e" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">实现了无偏性！</h1><p id="fc3d" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在让我们喘口气。三个假设已经完成，那么我们目前取得了什么成果？</p><p id="f6d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好了，我们已经实现了线性回归模型参数的线性<strong class="kx ir">无偏</strong>估计量。</p><blockquote class="oi oj ok"><p id="4117" class="kv kw mo kx b ky kz jr la lb lc ju ld ol lf lg lh om lj lk ll on ln lo lp lq ij bi translated">无偏性是估计量的一个理想的统计特性。它说，平均而言，估计量不会系统地低估或高估实际总体参数。</p></blockquote><p id="cd1d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于一个无偏的估计量，我们只需要证明它的期望值等于真实的总体参数。虽然我们很难知道人口参数的真实值来验证这一点，但可以通过分析来证明这一点(但前提是满足上述3个假设)。</p><p id="5ea4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有兴趣的，下面给出证明。对于那些不是的人，至少注意一下，这三个假设是如何被纳入证明的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/f67b0aef5e23624b32290baaf975f98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7jeyNRsWaYePjr8WSAFyA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d493" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面所示的第一个等式与假设1所示的β系数估计值的闭合解相同。在接下来的步骤中，我们只需展开y并分配乘积，最后，对两边都取期望值。我们需要注意的是这三个假设是如何实现的-</p><p id="2276" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a.如前所述，在没有线性和满秩假设的情况下，证明的第一行中所示的β-hat的表达式是无效的。</p><p id="611f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b.请注意用红色圈出的部分。由于零误差期望值(或零条件均值)的第三个假设，被圈起来的部分变为零，从而使得β-hat的期望值等于β。</p><p id="3902" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最终结果——前三个假设足以说明OLS估计量是一个<strong class="kx ir"> <em class="mo">无偏</em> </strong>线性估计量。</p><p id="71ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，这就是我们所需要的吗？</p><blockquote class="oi oj ok"><p id="16b7" class="kv kw mo kx b ky kz jr la lb lc ju ld ol lf lg lh om lj lk ll on ln lo lp lq ij bi translated">估计量的另一个理想特性是“低方差”。估计量在样本间变化越大，它对总体参数的估计就越不可靠。在所有的线性无偏估计量中，<strong class="kx ir">方差</strong>最小的是<strong class="kx ir"> </strong>称为“最佳线性无偏估计量”(蓝色)。</p></blockquote><p id="6b94" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么我们能证明OLS估计量是蓝色的吗？</p><p id="95cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">是的，但前提是另外两个假设成立。让我们看看它们是什么。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="4217" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">4.同方差性</h1><p id="819c" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">我们刚刚讨论了误差项的<strong class="kx ir">条件<em class="mo">意味着</em>独立</strong>的假设。现在我们来谈谈误差项的<strong class="kx ir">条件<em class="mo">方差</em>独立性</strong>假设(也叫‘同方差’)。</p><p id="f453" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望我们已经清楚了一件事——我们不希望回归者携带任何关于错误的有用信息。</p><p id="64c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这意味着，它是否是误差的均值(期望值);或者误差的方差，它不应该基于不同的回归xᵢ.值而不同</p><p id="106e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回想一下假设3中提到的基于多年经验的收入预测的同一个例子。您希望第一组(少于3年的经验)的误差比两组的误差“更分散”(或更少分散)吗？</p><p id="7aba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不会。如果一个或多个组的误差比其他组更分散(误差方差更大)，这基本上意味着你对这些组的预测不太可靠。</p><p id="19f2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同方差性表示，无论回归函数值是多少，误差项ϵᵢ的方差都应该等于一个常数。数学表述如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/488cda18e3510071ea9e5310c46f6f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/1*SE_q3d0l7dJ8BYVWrzhNPw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="2b50" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">5.非自相关</h1><p id="e626" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">让我们继续我们的探索，使误差尽可能随机和独立于其他因素。</p><p id="d1ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经讨论了两个假设——一个是关于误差的平均值独立于<strong class="kx ir">回归变量</strong>，另一个是关于误差的方差独立于<strong class="kx ir">回归变量</strong>。</p><p id="5550" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">均值独立性和同方差性确保<strong class="kx ir">误差</strong> <em class="mo">与</em> <strong class="kx ir"> <em class="mo">回归量</em> </strong>无关。</p><p id="87a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">非自相关确保<strong class="kx ir">错误</strong> <em class="mo">与</em> <strong class="kx ir"> <em class="mo">其他错误</em> </strong>无关。</p><p id="3eaa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从数学上来说，观测误差的协方差应该为零。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5b92d0579b14d178105a00f197324754.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/1*LYAQdb7ZelFAlHaPvkZIGw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c6f4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这听起来有点不直观，但有时，对于以某种方式排序的数据观察(例如，时间序列数据)，在连续观察中观察到具有“惯性”的误差值是很常见的，即正误差后跟着正误差或负误差后跟着负误差(称为正自相关)。误差也可能在观测值之间具有交替模式——正误差跟随负误差，反之亦然(称为负自相关)。</p><p id="3806" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有几种技术可以处理和转换LR模型，以便自相关不会造成问题，尽管它自然存在于数据中。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="b04e" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">达到的最小方差(蓝色)！</h1><p id="54c0" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在前三个假设的基础上增加假设4和假设5，借助于<strong class="kx ir"> <em class="mo">高斯-马尔可夫定理</em> </strong> <em class="mo">，可以证明OLS估计量是蓝色的。</em></p><p id="d9b9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">高斯马尔可夫定理的证明表明，所有线性无偏估计量的方差都将大于OLS估计量的方差。</p><p id="5449" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了使事情变得简单，让我们如下象征性地理解高斯马尔可夫定理的最终输出。根据证据，可以证明</p><p id="9a59" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> Var(任意线性无偏估计量)= X +一个正量</strong></p><p id="dc42" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的等式清楚地表明X小于LHS。</p><p id="f6ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">重要的是，利用同方差(Var(ϵᵢ)=σ)和非自相关(对所有i ≠ j的Cov(ϵᵢ,ϵⱼ)=0)条件，我们可以证明<strong class="kx ir">x = ols估计量的方差</strong>。</p><p id="7b2a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了简单起见，我们直观地理解了高斯马尔可夫定理。感兴趣的读者可以在网上<a class="ae or" href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem" rel="noopener ugc nofollow" target="_blank">这里</a>或者任何标准教科书中找到更多关于证明的信息。</p><p id="7d77" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然上面讨论的五个假设足以使OLS估计量变得令人沮丧(如果没有这五个假设，一个估计量就不会真正达到它应有的水平！)，还有两个假设值得一提。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="cfa1" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">6.随机或非随机回归变量</h1><p id="ff5d" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">这个假设可能是你唯一可以忘记的(一旦你明白了！).</p><p id="e014" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我把这个包括在内，这样当你在每本教科书中看到它时，你就知道它的意思了。</p><p id="7ba8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">非随机回归器是指你固定了某些<strong class="kx ir">【x】</strong>值(独立变量值)，只收集了这些x值对应的<strong class="kx ir">【y】</strong>上的数据，但是收集了多个样本。</p><p id="194e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回到我们之前的例子，假设你想根据多年的经验预测年收入。你的经理给了你三份员工名单和他们的年收入。每个列表都对应一个特定的经验水平——比如分别是3年、6年和10年的经验。每个列表包含50名员工的数据。如您所见，x值是固定的(3，6，10)，但有重复的样本(每个样本50个数据点)。这就是所谓的<strong class="kx ir">非随机回归变量</strong>(理论上，遵循这一点的模型也被称为<strong class="kx ir">经典线性回归模型</strong>或<strong class="kx ir"> CLRM </strong>)。</p><p id="95d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你的经理可能会给你一份150名员工的年收入清单，而不是给你三份单独的清单。该列表可能包含具有随机变化的工作年限的员工(例如，1年、2年、3年、7年、10年)。这里的x值不是固定的，而是可以根据多年的经验取任何(现实的)值。在这种情况下，我们使用<strong class="kx ir">随机(random)回归变量</strong>(理论上，遵循这一点的模型被称为<strong class="kx ir">新古典线性回归模型</strong>或<strong class="kx ir"> NLRM </strong>)。</p><p id="e00e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在创建回归模型之前，您使用哪种数据生成过程并不重要，只要前面陈述的5个假设成立，所有的统计特性和OLS估计器的优点仍然成立(这就是为什么您可以忘记这一点！).</p><p id="432b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数学上，唯一的区别是——如果你使用非随机回归，你会得到一个数学上的便利——删除所有假设表达式中“给定的X”部分，使它们成为无条件的(因为我们将X视为已知常数，而不是随机变量本身)。</p><p id="a827" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">E(ϵᵢ|X) = 0就会变成，E(ϵᵢ) = 0</p><p id="76b9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Var(ϵᵢ|X) = σ会变成Var(ϵᵢ)= σ</p><p id="f6a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在大多数数据科学研究中，对回归变量的数据收集通常是随机的，所以我们通常坚持使用条件表达式。在计量经济分析的一些实验设置中，有时以非随机方式设置回归变量，然后收集这些回归变量值的响应数据。</p><h1 id="3f7b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">7.常态</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d98d60cac17997163b50ed91c86110f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/1*uDjkB0lYKyHCYLLa8LbQFA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="214c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个假设非常简单。它只是说误差<strong class="kx ir">应该是正态分布的。</strong>这种分布应该具有均值零(假设3)和恒定方差σ(假设4 ),这已经在前面展示过了。</p><p id="4e0d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，请注意，这一假设并不是获得良好的统计特性(如OLS估计量的“无偏性”和“最小方差”)的必要条件。只要满足前5个假设，即使误差不是正态分布，OLS估计量也将是蓝色的(最佳线性无偏估计量)。</p><p id="47c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那我们到底为什么需要这个假设呢？</p><p id="09ea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，如果我们需要的只是一个蓝色估计量，我们就不需要它。事实上，对于足够大的样本，通常满足正态性条件，而不需要我们为它担心(渐近正态性)。对于随机收集的最具代表性的数据样本，预期正态误差是合理的。</p><p id="a554" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，把这作为一个明确的假设，会使两件有趣的事情成为可能-</p><ol class=""><li id="dc86" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">当所有其他假设都符合正态假设时，OLS估计与<strong class="kx ir">最大似然</strong>估计相一致(这在估计者中是一个很大的问题！)，为我们提供了一些有用的属性。在以后的文章中会有更多的介绍！</li><li id="7a6b" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">βᵢ的系数估计值可以显示为误差ϵᵢ.的线性函数要记住一个非常酷的特性——正态分布随机变量的线性函数也是正态分布的。因此，假设<strong class="kx ir"> ϵᵢ </strong>为正态分布，我们得到<strong class="kx ir"> βᵢ估计值也为正态分布。</strong>这使我们更容易计算置信区间和估算β系数的p值(常见于R和Python模型总结中)。如果不满足误差正态条件，那么β系数的各个t检验的所有置信区间和p值都是不可靠的。</li></ol></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="d903" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">总结一下！</h1><p id="815a" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">如果你只对线性<strong class="kx ir">无偏</strong>估计量感兴趣，你只需要—</p><ol class=""><li id="14ef" class="mp mq iq kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated"><em class="mo">线性度</em></li><li id="5c31" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><em class="mo">满级</em></li><li id="23fb" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><em class="mo">扰动的零条件均值(外生回归量)</em></li></ol><p id="4a95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果除了无偏的线性估计量之外，您还想要最佳(或最小方差)估计量，那么您还需要</p><p id="c407" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mo"> 4。同质性</em></p><p id="67e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mo"> 5。非自相关</em></p><p id="7c99" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了蓝色的估计量之外，如果您还想要单个β系数的可靠置信区间和p值，并且估计量与MLE(最大似然)估计量一致，那么除了上述五个假设之外，您还需要确保—</p><p id="50d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">7。常态</p><p id="e3ed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，如果你想写下一篇关于这个主题的研究论文，并且想要数学上正确的方程，不要忘记假设6(非随机和随机回归)。</p><h1 id="6c5c" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结论</h1><p id="c736" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">既然你已经看到了每一个OLS假设的实际应用，那么就该由你来决定应该验证哪些假设，以确保你的模型能够正确工作。</p><p id="b400" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这些假设中的一些是证明理论的数学便利，但大多数是创建具体和稳健模型的必要基础。</p><p id="c2df" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对这些假设的良好理解使我作为一名数据从业者能够掌握统计建模和机器学习中更难的概念。希望对你也有帮助！</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="5e20" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">加入我的旅程</h1><p id="877c" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated"><em class="mo">如果您喜欢这篇文章，我邀请您关注我的</em> <a class="ae or" href="https://medium.com/@ghoshayan6666" rel="noopener"> <em class="mo"> Medium </em> </a> <em class="mo">页面，了解更多关于数据科学有趣话题的更新，并通过</em><a class="ae or" href="http://www.linkedin.com/in/ayanghosh-001" rel="noopener ugc nofollow" target="_blank"><em class="mo">LinkedIn</em></a><em class="mo">与我联系。</em></p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="d450" class="lr ls iq bd lt lu nk lw lx ly nl ma mb jw nm jx md jz nn ka mf kc no kd mh mi bi translated">教科书参考</h1><ol class=""><li id="8390" class="mp mq iq kx b ky mj lb mk le ot li ou lm ov lq mu mv mw mx bi translated">William H. Greene — <em class="mo">计量经济学分析</em>第七版，第2章，第51–65页。</li><li id="4a86" class="mp mq iq kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">Jeffrey M. Woolridge — <em class="mo">计量经济学导论</em>第七版，第2章，第40–57页。</li></ol></div></div>    
</body>
</html>