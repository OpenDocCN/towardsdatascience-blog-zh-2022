# Pandas vs Dask vs Datatable:处理 CSV 文件的性能比较

> 原文：<https://towardsdatascience.com/pandas-vs-dask-vs-datatable-a-performance-comparison-for-processing-csv-files-3b0e0e98215e>

## 熊猫可能不再是最好的选择了

![](img/03ece7ab837167e4225b0de212688e1b.png)

马丁·雷施在 [Unsplash](https://unsplash.com/) 上的照片

W 说到处理 CSV 文件，每个人脑海中出现的第一个工具就是熊猫。毫无疑问，pandas 是一个伟大的框架，dataframe 提供了一种极其精简的数据表示形式，帮助我们更好地分析和理解数据。

最近做了一个任务，要求我合并 30K+ CSV 文件。我的神感是用熊猫，但是因为某些文件操作的表现不太顺利。在这篇文章中，我想和你分享另外两种选择，并比较它们和熊猫的行为和表现。最后，您将理解每个库的权衡，并能够做出正确的选择。

> 我对实验得出的一些结果感到非常惊讶，因为除非你实际尝试，否则无法预测。希望这篇文章可以节省你试错的时间，帮助你在选择库的时候做出更好的决定。

## [Dask](https://www.dask.org/)

大多数数据分析 Python 库(如 Numpy、pandas 和 scikit-learn)的一个问题是，它们不能扩展到单台机器之外。Dask 是一个开源库，当您处理大型数据时，它为分析提供了高级并行化。它可以在需要时将这些分析包扩展到多核机器和分布式集群。它提供了一个与 pandas 相似的 API 接口，以确保一致性并最小化摩擦。

## [数据表](https://datatable.readthedocs.io/en/latest/)

Datatable 是另一个考虑到性能的 Python 库。与 dask 不同，datatable 的目标是在一台**单节点机器上以尽可能快的速度执行大型数据处理**。同时，它与 pandas 的互操作性提供了轻松转换到另一个数据处理框架的能力。

这两个库都旨在提高 pandas 的性能，并保持与 pandas 相似的界面以方便使用。在接下来的几节中，我在我的 Macbook Pro(2.6 GHz 6 核英特尔酷睿 i7，16GB 内存)上进行了实验，在不同的环境中运行时，您可能会得到不同的结果。

## 读取单个 CSV 文件

让我们从最简单的操作开始——读取单个 CSV 文件。让我惊讶的是，在最基本的操作上，我们已经可以看到巨大的差异了。数据表比熊猫快 70%，而 dask 快 500%！结果是各种各样的数据帧对象，它们有非常相同的接口。

## 读取多个 CSV 文件

当我试图用熊猫来完成这个任务时，这就是我被卡住的地方。pandas 没有提供可以在一行中读取多个 CSV 文件的接口。唯一的方法是做一个 for 循环，将每个数据帧追加到一个列表中，最后，使用`pd.concat`来组合所有这些数据帧。但这是相当低效的。让我们检查以下内容:

这个结果也很有趣，因为事实证明，在读取多个 CSV 文件方面，datatable 的性能比 pandas 差，这与我的预期相反。尽管如此，达斯克仍然赢得了比赛，表现比熊猫好 4 倍。

**警告— CSV 文件有不同的格式**

组合多个 CSV 文件时，由于版本不同或数据文件损坏，CSV 文件可能会有不同的格式。在这种情况下，我们应该小心不要无意中混淆了不同的 CSV 文件。

**新列**

在这个例子中，我准备了两个文件，其中一个有一个额外的列。我试图模拟一个真实的场景，在这个场景中，源文件中有一个微小的模式变化。

这是你所期望的吗？

Pandas 将使用新模式作为目标模式，**新列将被回填为旧数据**中的 `**NaN**` **。列表`files`中的顺序无关紧要，因为 pandas 将选择具有更多列的模式，而不管读取顺序如何。**

另一方面，Dask 让我很惊讶。它使用列表中第一个文件的**模式作为目标模式，并忽略不匹配的文件。**当我反转`files`时，我得到了完全不同的结果，因为先读取了一个不同的文件。这有点冒险，因为它可能会在不通知您的情况下忽略许多行。一种解决方法是读取每个文件的文件头，并在合并之前进行比较。尽管这产生了一点点开销，但总的处理时间仍然比 pandas 快，因为在“合并”阶段获得了巨大的性能增益。

与其他的相比，数据表是相当安全的。如果在模式中发现差异，它将引发异常。你可以在`rbind`函数中添加`force=True`，这样它就会有和熊猫一样的行为。

**重命名现有列**

另一个常见的模式更改是重命名现有的列，这是一个突破性的更改。在这个例子中，`data.csv`只包含`gross_amount`，而`data_rename_col.csv`只包含`net_amount`。看到前面的例子后，在检查结果之前，让我们猜一猜。

所以，熊猫的结果既包含了`gross_amount`又包含了`net_amount`，它用`NaN`填充了缺失的值。Dask 给出与之前相同的结果，这取决于它首先读取的文件。在这种情况下，Datatable 抛出一个不同的异常，其名称为中断列。这在调试过程中很有帮助。一般来说，它们都继承了上一个例子的相同行为。

## 计算聚合

一项重要的分析是计算聚合，如最小值、最大值、总和、平均值和中值，其中单个数字可以洞察整个(部分)数据集。那么这些计算的性能如何呢？

> 根据 [dask 文档](https://examples.dask.org/dataframes/02-groupby.html):一般来说，dask . data frame group-by-aggregations 的性能与 pandas group-by-aggregations 大致相同，只是可伸缩性更强。

计算聚合的性能也是一样的。但与熊猫相比，dask 能够在集群中扩展解决方案。

## 写入 CSV 文件

最后一部分是卸载。所有这 3 个库都有相同的接口`.to_csv()`将数据帧保存到 CSV 中。除此之外，dask 还支持 [Apache Parquet](https://parquet.apache.org/) 格式，这是一种流行的柱状二进制格式，旨在实现高效的数据存储和检索。它提供了高效的数据压缩和编码方案，增强了批量处理复杂数据的性能。

如你所见，获胜者是镶木地板格式的 dask。Pandas 和 datatable 的表现一样。常规 CSV 格式的 Dask 性能最差，这与读取 CSV 文件的性能完全相反。parquet 的高性能是因为数据被分成了几个分区。默认情况下，dask 会将每个 parquet 文件作为数据帧中的一个分区单独加载，这样更便于并行加载。

此外，dask 在默认情况下用 [snappy compression](https://en.wikipedia.org/wiki/Snappy_(compression)) 写出 parquet 文件。快速压缩通常是分布式计算中文件的最佳选择。虽然它压缩文件的力度不如 gzip 等其他压缩算法，但在解压文件时速度更快。您也可以用其他压缩算法覆盖它。

## 结论

我希望这篇文章能够让您对 pandas、dask 和 datatable 的不同方面有一个整体的了解。事实证明，没有一个图书馆是完美的。Dask 擅长读写文件，尤其是使用它的 parquet 格式。它能够将您的解决方案分发到一个集群。Datatable 试图以稍好的性能模仿熊猫的行为。Pandas 是另外两个库的核心，提供了最完整的计算方法。

比较不同的工具并了解它们的优缺点总是很有趣的。我鼓励你在你感兴趣的领域也做这样的练习，并与社区分享。它将使许多人受益。

让我知道你对这三个图书馆的看法！干杯！

## 参考

[](https://coiled.io/blog/speed-up-pandas-query-10x-with-dask/#:~:text=Dask%20runs%20faster%20than%20pandas,cores%20to%20run%20the%20computation) 