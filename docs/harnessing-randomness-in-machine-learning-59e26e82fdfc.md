# 利用机器学习中的随机性

> 原文：<https://towardsdatascience.com/harnessing-randomness-in-machine-learning-59e26e82fdfc>

## 随机应该有多“随机”？

![](img/724d80e76629eed20856c6bfdce84cc8.png)

照片由来自 [Pexels](https://www.pexels.com/photo/close-up-view-of-two-white-dices-on-black-surface-4668244/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) 的[亚当·费杰斯](https://www.pexels.com/@adam-fejes-1510707?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)拍摄

您是否曾经在每次运行相同的代码时，您的模型都显示出不同的结果？您是否经历过训练数据不断变化，或者您的机器学习模型在每次调整时都有不同的参数？

如果你有，很可能你忽略了大多数机器学习任务中存在的一个核心成分:随机性。

随机性是机器学习中的一个重要元素。它有助于消除固有的偏见，并有助于建立一个通用的机器学习模型。

然而，就像其他任何事情一样，不小心加入随机性只会妨碍你项目的结果。

在这里，我们深入研究随机性的重要性，以及如何在机器学习模型的构建中利用它。

## 机器学习中的随机性

随机性可以在无数的场景中被识别。

当您使用 Sklearn 模块将数据拆分为训练/测试集或拆分训练数据以进行交叉验证时，子集是随机选择的。

此外，包含采样的算法通常包含随机性。例如，随机森林分类器通过替换创建训练数据的随机子样本(称为引导)。

随机性也在深度学习模型中发挥作用。

当训练神经网络时，神经元的初始权重和偏差是随机选择的。此外，神经网络中的脱落层具有从层中随机移除神经元的功能。

## 随机数生成

在进一步讨论随机性之前，我们需要了解 Python 模块是如何生成随机值的。

机器学习中的随机值是由随机数生成器导出的。

为了创建随机值，发生器首先用一个**种子**初始化，这个数字代表随机数生成的起点。然后，生成器使用特定算法从该起始点创建随机值。

在这一点上，你可能想知道:如果这些数字是用算法生成的，那么它们就不是真正随机的，不是吗？

那是正确的。

由于这些值并不体现真正的随机性，描述这个概念更合适的术语是**伪随机性**。

用于创建伪随机数序列的生成器被称为**伪随机数生成器** (PRNG)。

像 Numpy 这样的许多模块使用 PRNGs 来生成一个数字序列，这个序列“看起来”是随机的，但实际上具有确定性的性质。

## 随机性的问题

尽管随机性是机器学习模型中至关重要的组成部分，但它有一个严重的缺陷。

随机性是随机的。

当您执行任何种类的实验时，您都希望控制实验的所有组件。

不幸的是，随机性是控制的对立面。

如果您的数据处理或模型构建步骤包含了随机性，您怎么能期望在每次迭代中生成相同的结果呢？

很容易看出这是如何对您的机器学习任务构成障碍的。

您将无法与您的同行适当地共享您的工作，因为您将从相同的代码中获得不同的结果。

此外，由于每次迭代都生成不同的值，您将很难评估和改进您的模型。

真正的随机性没有提供的是**再现性**的元素，即在任意次数的迭代中获得一致结果的能力。

例如，让我们用 numpy 模块生成 10 个随机值。

![](img/d2268c12a51f5f51cc7acdcff3c59a74.png)

代码输出(由作者创建)

输出与预期的一样，但是当您再次运行相同的代码时会发生什么呢？

![](img/c18d7c45f2f4e3c46cbdb1a5ad7907d6.png)

代码输出(由作者创建)

我们得到一组不同的数字。如果模型在训练中表现出相似的行为，那么很容易理解为什么它们会在不同的迭代中产生不同的结果。

为了进一步展示随机性的问题，我们可以对来自 Sklearn 模块的内置数据集执行两次训练测试分割，以创建两个训练和测试集并对它们进行比较。

![](img/a9a40f809d3197b9a1e2ac0ae2008b9d.png)

代码输出(由作者创建)

理想情况下，训练集和测试集应该是相同的，因为它们是用同一行代码创建的。不幸的是，随机性的影响意味着在每次迭代中，不同的记录被分配给训练集和测试集。

## 解决方案

将随机性成功纳入机器学习的唯一方法是“控制”生成的随机值，而不失去它们的随机性质。

换句话说，随机生成的值需要满足两个条件:

1.  生成的值需要随机出现

2.生成的值需要是可再现的

虽然这些条件可能看起来相互矛盾，但它们都可以通过 PRNGs 来满足。

PRNGs 的算法生成看似随机的值，因此它们完全满足第一个条件。

下一步是确保这些值也是可重复的。

换句话说，需要一种方法来保证无论代码运行多少次，从 PRNG 获得的值都保持不变。

实现这一点的方法很简单:*定义*PRNG 的后裔。

在前面的示例中，用于初始化 PRNG 的种子从未明确定义。因此，每次运行代码时，生成的数字都会有不同的起点。这自然意味着导出的数字在每次迭代中都是不同的。

那么，如果在生成随机值之前定义种子，会发生什么情况呢？

让我们先通过设置种子来修改前面的随机数生成。

![](img/952d6315f5956e9bfdc7d418f7ba6524.png)

代码输出(由作者创建)

现在，无论你运行代码多少次，你都将得到相同的随机数集合，因为 PRNG 将在每次迭代的同一点开始。

在 Sklearn 模块中，许多包含随机性的函数和分类器允许您使用“random_state”参数初始化 PRNG。

以下是拆分数据时如何执行此操作。

![](img/dea3d63a8caff47c4f14b023f4428cfd.png)

代码输出(由作者创建)

这一次，两个训练集都匹配。

也可以为深度学习应用设置种子。

以下是如何为 Tensorflow 做到这一点:

以下是你可以为 Pytorch 做的:

注意:分配给种子的实际值实际上并不重要。只要种子保持不变，生成的数字就保持不变。

## 结论

![](img/21ce14c8acd5e0d7fbe32f559d9ac7e8.png)

照片由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的 [Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral) 拍摄

简而言之，对于一个成功的机器学习项目，我们需要接受伪随机性，而不是真随机性。

我们可以通过直接设置 PRNGs 的种子来消除随机性，以确保生成的数字尽管看起来是随机的，但却是可再现的。

我祝你在机器学习的努力中好运！