<html>
<head>
<title>Debugging Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">调试神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/debugging-neural-networks-abdc6273a3f1#2022-01-02">https://towardsdatascience.com/debugging-neural-networks-abdc6273a3f1#2022-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="4553" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">调试神经网络</h1></div><p id="0b5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">传统的软件开发已经建立了测试软件的四个阶段:单元测试、集成测试、系统测试和验收测试。第一种猜测是，当建立机器学习模型时，没有什么变化，因为它仍然是一些软件代码。但这是短视的，因为机器学习模型在没有冻结时具有不确定性行为，其参数不再适应数据。</p><p id="fa99" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在训练或连续交付机器学习模型的情况下，可能会出现额外的误差源。这些问题可能在长时间的训练过程后被发现，也可能在最早检查结果时被发现——可能在一小时、一天、一周或一个月后，这取决于你预计训练何时结束。你能想象的最简单的错误是你的神经网络的组件(例如神经元)没有连接。这个错误的关键是，你只能在一个无效的验证错误中看到它。</p><p id="fb87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将解释一些调试神经网络的技术。当然，这些技术也可以用于在训练前和训练时测试模型。因此，很难区分设计模型中的错误和改进模型以收敛。</p><p id="d02f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">单元测试</strong></p><p id="89f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在您自己编写层的级别，首先要做的是编写单元测试。这样做的目的是使实现不容易受到代码更改的影响。这里，我们将实际输出与预期输出进行比较。举个简单的例子，考虑Tensorflow中的密集层:</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="9537" class="ku kv iq kq b gy kw kx l ky kz">import tensorflow as tf <br/>def dense(inputs, weights):<br/>       return tf.linalg.matvec(weights,inputs)</span></pre><p id="047e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是这个函数的一个简单单元测试。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="82c2" class="ku kv iq kq b gy kw kx l ky kz">import numpy as np <br/>import pytest  <br/>def test_dense():<br/>      inputs = np.random.rand(5)<br/>      weights = np.identity(5)<br/>      assert np.linalg.norm(dense(inputs=inputs, weights=weights) - inputs) == 0</span></pre><p id="051a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本例中，选择了一个恒定权重矩阵，因为我们知道会发生什么。这个例子当然是从单元测试开始的一个小练习。我相信Tensorflow团队会测试他们的库。</p><p id="6ad6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">检查一个小数据集</strong></p><p id="7c44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">合理的测试方法是在小数据集上训练模型。这也可以写成一个简单的测试，例如可以在任何管道管理器上实现。在这里，少量的数据点就足够了。在训练的第一个时期之后，我们可以分别看到训练如何改进模型或者代码或数据集中是否有bug。这些第一纪元对于节省大量时间很重要。</p><p id="5f20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果误差没有减少，学习率可能太高。损耗或梯度的错误符号也可能导致误差增大。如果误差振荡，少数样本中也可能存在错误的标记。另一方面，当模型不是过拟合时，学习率可能太小或者梯度消失。</p><p id="c5f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">继续训练有助于观察模型是否能够完美地拟合数据。如果损失不会收敛到零，模型将总是对数据进行欠拟合。请注意，对于这一步，重要的是只考虑描述模型准确性的损失—没有正则化项！当考虑正规化时，损失会增加。不要忘记:仅仅因为你的模型不符合要求，它仍然可能失败。</p><p id="a0af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">监控所有内部状态</strong></p><p id="d372" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练神经网络时，很多事情都可能出错。这就是为什么建议跟踪所有内部状态。特别地，应该考虑任何权重、任何激活以及可能还有任何仿射变换的分布。Tensorboard是一个很好的工具，可以在训练过程中直观地监控这些量级。</p><p id="0412" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一个检查是考虑网络是否在学习。一个好的指标是考虑权重更新的硬阈值——这可以是10^{-3}.比如，香草渐变体面的时候</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div class="gh gi la"><img src="../Images/26d72ab0404d0b451a0eddf53e03cfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*3BQvtN7KNGeJ7AvOGTRiog.png"/></div></figure><p id="1822" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">被认为是，比例</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div class="gh gi le"><img src="../Images/1cbb5e8bb91cb2c2184e851978f9dce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*CJ9-ZjJFvPSUxzIFNBTtLg.png"/></div></figure><p id="9a5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应该在10^{-3}.附近该阈值仅允许影响很小但足以避免缓慢收敛的更新。调整学习速率是很重要的，因为在固定的学习速率下，这个比率偏离了10^{-3}，权重将不会学习到有用的特征。由于梯度下降法的理论与数据集的大小无关[07]，因此可以在少量但有代表性的数据子集上探索学习率的良好策略。</p><p id="a828" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，梯度也可能为零。这有几个原因。最明显的是激活饱和或死亡——这意味着仿射变换导致激活函数的一个区域(几乎)恒定。对于重新激活，有人说，当输出为零时，“激活死亡”。虽然几乎恒定的激活可能有希望回到梯度不几乎为零的区域，但是ReLUs没有希望，因为梯度为零。这就是所谓的<em class="lf">消失渐变</em>。</p><p id="3329" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑任何仿射变换和输出的直方图有助于解决卡住的问题。仿射变换的数据应该类似于正态分布的数据，并且具有大约0.5到2的标准偏差。这里，两个方面起着重要的作用:数据不应该接近激活函数的几乎恒定的区域，并且变换数据的幅度应该与权重相同。因此，命名顺序取决于数据、学习过程和数据。当意识到内部激活远离0时，<em class="lf">批量归一化</em>对于卷积层是一个好的解决方案，而<em class="lf">层归一化</em>对于递归层是一个好的解决方案。请注意，即使使用分段线性激活，也建议对数据进行标准化。<br/>还要注意，Tensorflow中有两个函数进行批量规格化:TF . nn . Batch _ normalization只是应用规格化，而TF . keras . layers . Batch normalization是可训练层。在最后一个中，也有两种模式:用于训练和推理。而训练移动统计量的均值和方差时，训练但不使用，因为它将提供推理时间。</p><p id="1a2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于权重的分配也很重要。当后面的权重比网络第一层中的权重学习得慢时，我们得到巨大的梯度——所谓的<em class="lf">爆炸梯度</em>。在某些点上，梯度在数值上变得不规则，或者我们看到不规则的振荡，这是由于在局部最小值附近振荡。这些可以通过梯度限幅来解决，其中阈值用于限制最大绝对值。On还可以通过向优化器添加l2惩罚项来惩罚权重(所谓的岭正则化)。大多数情况下，权重看起来呈正态分布，但没有证据表明这是良好性能的充分或必要条件。</p><p id="aa65" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可视化权重矩阵可以通过<em class="lf">辛顿图</em>【10】来完成。这是一个正方形的方桌，每个重量的大小用正方形的大小来表示。当然，也可以通过热图来可视化。</p><p id="6fd0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">权重更新的不良行为的另一个原因可能是不良的初始化。最糟糕的做法是将所有权重初始化为零，因为一个层中的所有权重行为相同。众所周知，好的初始化器有各种各样的激活函数。然而，训练可能仍然需要很长时间，这就是为什么在[04]中建议使用受限的波尔兹曼机器或自动编码器进行预训练。[16]中的作者还强调，这种预训练给出了更好的泛化误差。</p><p id="4850" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于梯度近似的样本也可能导致局部最小值。在这种情况下，增加批量是一个合理的选择。</p><p id="610f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在考虑卷积层时，可视化第一个隐藏卷积层的滤波器也是有帮助的。它可以给网络正在做的事情一些启发。</p><p id="e39c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">检查你的梯度计算</strong></p><p id="c49e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当在梯度的实施中有疑问时，可以通过使用数值方法近似梯度并考虑相对误差来实施简单的检查。为了选择计算梯度的方法，我们考虑f在x点的泰勒级数:</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lg"><img src="../Images/b03cf2db85398119d7abc5f7c3d4e73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rryzWV6tbjBsVviEyk0E0w.png"/></div></div></figure><p id="0d74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用大O符号。所以正向差分公式有一个O(h)阶的误差项。还要考虑后向差异</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ll"><img src="../Images/06e13a20e7dba28dfe316829feea1233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*goFEc_DQPJwYy18hOgKsPA.png"/></div></div></figure><p id="1a3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不会带来任何更好的近似。然而，将两者结合起来会产生O(h)的误差近似值:</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lm"><img src="../Images/e54f85072ddd7eff0c172450f2d04209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vdQ6WfIv-RopbMGQny8Uw.png"/></div></div></figure><p id="42e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">例如，一个缺陷是ReLU作为函数f，因为它有不连续的梯度。假设|x| &lt; h and x &lt;0 then</p><figure class="kl km kn ko gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ln"><img src="../Images/667ef96b97b38d95d0cc9bed3b0cf393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtHmJFLe7axUaX6ux5w-Ww.png"/></div></div></figure><p id="e365" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Hence, around an discontinue gradient, one should monitor the gradient check carefully. Moreover, for checking whether gradient is working correctly, it is enough to use only a few datapoints. Step size h should be also considered wisely since numerical issues may appear. In [03] a comparision of accuracy and step size is given.</p><p id="728f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">比较初始损失和你计算的预期损失</strong></p><p id="3cbf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过计算您的预期损失，可以轻松检查数据集的不平衡或缺失的归一化。分类网络必须输出随机权重的<br/> -log(1/NumberOfClasses)。否则数据集是不平衡的。当考虑一个不平衡的数据集时，最终层中的权重应该被初始化，使得网络为每个类输出<br/> -log(1/NumberOfClasses)。</p><p id="5921" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用正则化会导致问题，因为例如l2惩罚项可能会造成大量数据丢失。这意味着你告诉网络惩罚权重，而不是逼近数据。因此，特定损失项的比例是不正确的。</p><p id="2fc3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">观看学习过程</strong></p><p id="e306" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于所有步骤，当代码是可再现的时是有帮助的。因此，随机种子需要固定。这里需要知道内置的Python随机数生成器和Numpy有不同的种子，Tensorflow的种子也需要设置。当使用GPU进行训练时，还需要小心，因为训练也可能是随机的，因为GPU中的一些快速方法是随机的，但可以设置为较慢的确定性函数。</p><pre class="kl km kn ko gt kp kq kr ks aw kt bi"><span id="ff98" class="ku kv iq kq b gy kw kx l ky kz">seed = 1  </span><span id="fbf8" class="ku kv iq kq b gy lo kx l ky kz"># Built-in Python <br/>random.seed(seed)  </span><span id="8dcd" class="ku kv iq kq b gy lo kx l ky kz"># Derandomize hashes of strings, bytes and datetime objects os.environ['PYTHONHASHSEED']=str(seed)  </span><span id="5f6e" class="ku kv iq kq b gy lo kx l ky kz"># Numpy <br/>np.random.seed(seed)  </span><span id="c336" class="ku kv iq kq b gy lo kx l ky kz"># Tensorflow <br/>tf.random.set_seed(seed) <br/>os.environ['TF_DETERMINISTIC_OPS'] = '1' # determininistic convolution and max-pool <br/>tf.config.threading.set_inter_op_parallelism_threads(1) </span></pre><p id="9e6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑分布给出了一个很好的全局概观，但是考虑网络内部沿着流动的几个样本也可以给出一个很好的直觉。几个样本的层的所有输出的可视化显示了一组样本的行为。在检查数据流的同时，还可以检查张量维数的正确性。</p><p id="3ab9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大多数培训都围绕优化器的超参数进行，当培训效果不佳时，分析这些超参数非常重要。这里列出了关键的超参数及其影响。</p><p id="c0cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">优化器</strong>:存在许多用于训练神经网络的选项。自动调整学习率的梯度下降解算器主要设计用于小梯度。由于RMSProp、Adam等中的指数平均，大梯度对另一侧的影响消失。<br/>此外，已经观察到[13]用Adam进行训练，例如，在初始阶段非常好，但是由于梯度的非均匀缩放，在某些点上停滞不前。[13]中的作者建议转换到SDG。这里必须采用学习率。</p><p id="cfcf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">批量</strong>:该参数给出近似梯度的样本数量。当不使用整个数据集时，我们称之为<em class="lf">批量梯度下降</em>。当存在巨大的泛化差距时，这可能是批量过大的原因。在[01]中，作者观察到大批量导致急剧的最小值，即在小邻域中的点快速增加的网络的损失函数图上的最小值。另一方面，小批量似乎倾向于平坦的最小值。然而，当数据具有高方差时，小批量会降低学习速度。优化器对噪声也非常敏感。在[09]中，作者讨论了在训练期间增加批量大小。[08]中还强调，开始时更多的重量更新(即小批量)有助于探索损失情况。</p><p id="2224" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，对于小批量梯度下降，所有的例子都需要随机洗牌。该过程模拟了数据分布中的随机抽样，并改善了概括差距，尤其是当一组后续样本中存在偏差时。</p><p id="9acd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">学习率</strong>:该参数给出了<em class="lf">梯度步长有多大</em>。小的学习率会导致收敛缓慢和陷入局部极小值的风险。大的学习率有在最小值附近振荡的风险。然而，改变学习率可能有助于加快收敛。已经有一些技术来实现良好的学习率，该学习率在训练过程时改变，例如阶跃衰减、指数衰减和基于时间的衰减。</p><p id="bfed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一般来说，一个持续的或不断增加的训练损失可以由高学习率引起。低学习率导致训练损失减少，但是这里第一个时期中训练损失的斜率是重要的:当训练开始时，网络是一个随机函数，它学习以适应数据:因此大部分学习在第一个时期中完成。如果训练损失的斜率较低，仍然可以提高学习率。</p><p id="19ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练和验证准确性</strong>:训练时，建议对当前网络进行验证。两种精度之间的巨大差距表明存在严重的过拟合。</p><p id="ab3c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">更新权重的特征值谱</strong>:特征值的广泛分布导致了玉米薄壳形的最小值【08】。</p><p id="fe64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">使用掉线错误</strong></p><p id="6bef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">辍学是一种强有力的正规化方法，但会导致许多错误:</p><ul class=""><li id="c575" class="lp lq iq jp b jq jr ju jv jy lr kc ls kg lt kk lu lv lw lx bi translated">辍学是培训中最常用的一种方法。评估网络时，必须关闭漏失层，否则它会像[05]中那样考虑模型的不确定性。</li><li id="737d" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">使用Dropout时，应该非常小心地使用早期停止。由于训练中的随机行为，早期停止可能会过早停止，而不知道退出会导致损失的小幅增加。</li><li id="0b88" class="lp lq iq jp b jq ly ju lz jy ma kc mb kg mc kk lu lv lw lx bi translated">必须在批量标准化后使用退出。在[02]中，作者描述了当我们从训练转移到测试时，退出改变了特定单元的方差。批量标准化将保持这种统计方差，因为它是从整个训练过程中积累的。</li></ul><p id="535a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">检查您的数据</strong></p><p id="a73c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于数据的第一件事也是最重要的一点是，您需要对输入数据进行标准化。在回归问题中，对输出进行归一化也可能是一个不错的选择。</p><p id="aed5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除了查看网络的内部状态，还应该考虑数据本身。不平衡的数据可能会导致分类任务中大多数相关类的过度拟合。当一些地区的数据过多时，回归分析也是如此。下采样数据是一个很好的选择。回归问题也需要解决同样的问题。</p><p id="766d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">手工检查数据也是非常重要的。不正确的标签和相同的样本导致训练过程出现问题。通常，误差最大的样本标记不佳。</p><p id="0393" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不确定性的一个原因也是数据的质量。测量数据时总会有一些噪声，如果在某个邻域中只有一个数据点，网络会在这个点上过度拟合。用不同的噪声多次使用这个数据点将有助于获得更稳定的结果。另一方面，当一个邻域中有许多数据点时，对它们进行批量平均是一种减少过度拟合并提高训练性能的方法。</p><p id="de38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，可以通过不使用数据来检查数据:当训练在文献中多次使用的不同数据集上工作并且已知它是可解的时，您可以确定，您的网络能够工作并且您的数据集有问题。</p><p id="7c26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">总结</strong></p><p id="9dd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章给出了很多如何调试神经网络的想法。当所有的提示和技巧都不起作用时，你可能做了一个错误的超参数选择。选择不同数量的层和神经元，然后重试。如果你有进一步的想法或问题，请告诉我。</p><p id="148d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">文献:<br/>【01】凯斯卡尔等人。艾尔。、论深度学习的大批量训练:泛化差距与尖锐极小值(2017)、<a class="ae md" href="https://arxiv.org/pdf/1609.04836.pdf" rel="noopener ugc nofollow" target="_blank"/><br/>【02】李等。艾尔。、通过方差移位理解脱落与批量归一化的不协调性(2018)、h<a class="ae md" href="https://arxiv.org/pdf/1801.05134.pdf" rel="noopener ugc nofollow" target="_blank">ttps://arxiv . org/pdf/1801.05134 . pdf</a><br/>【03】<a class="ae md" href="https://en.wikipedia.org/wiki/Numerical_differentiation" rel="noopener ugc nofollow" target="_blank">维基、</a><br/>【04】辛顿等人。艾尔。，用神经网络降低数据的维数，(Science 2006)，<a class="ae md" href="http://www.cs.toronto.edu/~hinton/science.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.toronto.edu/~hinton/science.pdf</a><br/>【05】Gal等。艾尔。、作为贝叶斯近似的辍学:深度学习中的表征模型不确定性(2016)、<a class="ae md" href="https://arxiv.org/abs/1506.02142" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02142</a><br/>【06】卡帕西，《训练神经网络的诀窍》(2019)、<a class="ae md" href="http://karpathy.github.io/2019/04/25/recipe/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2019/04/25/recipe/</a><br/>【07】村田，《在线学习的统计研究》(1998)、在线学习与神经网络，剑桥大学出版社<br/>【08】神经网络:交易的诀窍，2012、斯普林格<br/>【09】莫勒 人工智能<br/>【11】邵，神经网络调试检查表(2019)，<a class="ae md" rel="noopener" target="_blank" href="/checklist-for-debugging-neural-networks-d8b2a9434f21#--responses">https://towards data science . com/check-for-debugging-neural-networks-d 8 B2 a 9434 f 21</a><br/>【12】Nikishaev，神经网络如何调试。 (2018)，<a class="ae md" href="https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2" rel="noopener">https://medium . com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a 200 F10 f 2</a><br/>【13】Keskar等人，通过从Adam切换到SGD来提高泛化性能(2017)<br/>【14】Stanford Course cs 231，<a class="ae md" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank"/><br/>【15】Hui，可视化深度网络模型与度量(第四部分)(2018)，al，为什么无监督预训练有助于深度学习？(2010年)，《机器学习杂志》</p></div></div>    
</body>
</html>