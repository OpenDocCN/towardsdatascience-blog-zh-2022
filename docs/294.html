<html>
<head>
<title>Less is More: Labeled data just isn’t as important anymore</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">少即是多:带标签的数据不再重要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/less-is-more-labeled-data-just-isnt-as-important-anymore-7cf3b403969#2022-01-10">https://towardsdatascience.com/less-is-more-labeled-data-just-isnt-as-important-anymore-7cf3b403969#2022-01-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="d44a" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">少即是多:带标签的数据不再重要</h1></div><div class=""><h2 id="d7c5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">对半监督学习的新研究表明，较少标记的数据实际上使机器学习算法更强大</em></h2></div><p id="f293" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我过去总是认为<em class="lf">数据</em>本质上是平静有序的——一组准备处理的整齐包装的信息。我想大多数没尝过现实世界混乱滋味的人也会这么认为。专业人士(或者，实际上，任何与数据打交道的人)都知道，数据的流动性和无定形性要大得多。很少有数据被有序地组织起来，并带有一个漂亮的标签。事实上，</p><blockquote class="lg"><p id="2309" class="lh li it bd lj lk ll lm ln lo lp le dk translated">大多数时候，数据是未标记的、非结构化的和混乱的。</p></blockquote><p id="916c" class="pw-post-body-paragraph kj kk it kl b km lq ju ko kp lr jx kr ks ls ku kv kw lt ky kz la lu lc ld le im bi translated">不幸的是，今天广泛使用的大多数机器学习算法都严重依赖于标记数据和完全监督的算法。数据科学家和数据工程师花费了大量的时间和精力来对抗熵并产生这些干净的数据集，我们已经习惯于在像<a class="ae lv" href="https://kaggle.com" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>这样的网站上看到这些数据集。</p><p id="92ad" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">然而，事实证明，带有故意噪声注入的半监督方法可能比任何监督学习方法都好，即使在使用<strong class="kl iu">明显较少标记的数据</strong>时也是如此。</p><h1 id="5842" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">半监督学习</h1><p id="40af" class="pw-post-body-paragraph kj kk it kl b km mo ju ko kp mp jx kr ks mq ku kv kw mr ky kz la ms lc ld le im bi translated">监督学习需要大量的标记数据，而我们大多数人都没有这些数据。半监督学习(SSL)是一种不同的方法，它结合了无监督和监督学习，以利用各自的优势。下面是一个可能的过程(称为SSL和“域相关数据过滤”):</p><blockquote class="mt mu mv"><p id="a86e" class="kj kk lf kl b km kn ju ko kp kq jx kr mw kt ku kv mx kx ky kz my lb lc ld le im bi translated">1.训练一个模型(<strong class="kl iu"> <em class="it"> M </em> </strong>)上标注的数据(<strong class="kl iu"><em class="it">【X】</em></strong><em class="it">)</em>和真实标签(<strong class="kl iu"><em class="it">【Y】</em></strong><em class="it">)。<br/>2</em>2。计算误差。<br/> 3。对未标记的数据应用<strong class="kl iu"><em class="it">M</em></strong>(<strong class="kl iu"><em class="it">【X’</em></strong><em class="it">)</em>“预测”标签(<strong class="kl iu"><em class="it">【Y’</em></strong><em class="it">)</em>。<br/> 4。从(2)中取出任何高可信度的猜测，并将它们从<strong class="kl iu"><em class="it">X’</em></strong>移动到<strong class="kl iu"> <em class="it"> X </em> </strong> <em class="it">。<br/> </em> 5。重复一遍。</p></blockquote><p id="e7bd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">随着时间的推移，标注数据集的大小会增加，并为模型提供相当多的数据。这已经令人印象深刻，因为<strong class="kl iu">未标记的数据远比标记的数据</strong>便宜和丰富，但SSL <a class="ae lv" href="https://www.cs.cmu.edu/~aarti/pubs/NIPS08_ASingh.pdf" rel="noopener ugc nofollow" target="_blank">并不能保证是一个好的解决方案</a> (Singh，Nowak，&amp; Zhu)。然而，一项新的研究表明，我们可以改变一些事情来提高SSL的性能。</p><p id="6182" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果我告诉你，这个SSL的新版本几乎优于监督学习，只有一小部分必要的标记数据，会怎么样？</p><h1 id="323c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">无监督数据增强(UDA)</h1><p id="36d4" class="pw-post-body-paragraph kj kk it kl b km mo ju ko kp mp jx kr ks mq ku kv kw mr ky kz la ms lc ld le im bi translated">如果你的孩子只从你告诉他们的东西中学到东西，很有可能在很多情况下他们会完全迷失。机器学习算法也是如此。事实证明，让算法处理棘手的数据会让它们在以后变得更加健壮。这些“棘手的数据”也可以被称为“噪音”加强机器学习算法的一个真正有用的技术是故意向数据添加噪声(在本文中称为“噪声注入”或“数据扩充”)。在CMU和谷歌大脑团队在<a class="ae lv" href="https://arxiv.org/pdf/1904.12848.pdf" rel="noopener ugc nofollow" target="_blank">的一篇新论文中，一个关心“噪音质量”的新SSL框架也被注入。以下是总体模型的流程图:</a></p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi mz"><img src="../Images/7f042516eb200a8d8bbe082e5dfdfcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sg59HorCt1Uv4Z7qywqQfw.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">转载自“用于一致性训练的无监督数据增强”(谢等。艾尔。2020).</p></figure><p id="9d7d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">以下是同一流程图的分步格式:</p><blockquote class="mt mu mv"><p id="0628" class="kj kk lf kl b km kn ju ko kp kq jx kr mw kt ku kv mx kx ky kz my lb lc ld le im bi translated">1.在标签数据(<strong class="kl iu"> <em class="it"> X </em> </strong>)和真实标签(<strong class="kl iu"> <em class="it"> Y </em> </strong>)上训练一个模型(<strong class="kl iu"> <em class="it"> M </em> </strong>)。<br/> 2。计算监督误差。<br/> 3。对未标记的数据应用<strong class="kl iu"><em class="it">【M】</em></strong><strong class="kl iu"/>(<strong class="kl iu"><em class="it">X’</em></strong><em class="it">)</em><strong class="kl iu"/>“预测”标签(<strong class="kl iu"><em class="it">Y’</em></strong><em class="it">)</em>。<br/> 4。巧妙的将噪声引入到<strong class="kl iu"><em class="it"/></strong>产生<strong class="kl iu"><em class="it">【X】</em></strong>。<br/> 5。将<strong class="kl iu"> <em class="it"> M </em> </strong>应用于未标记的、被扰乱的数据(<strong class="kl iu"><em class="it">【X】</em></strong><em class="it">)</em>“预测”标签(<strong class="kl iu"><em class="it">【Y】</em></strong><em class="it">)</em>。<br/> 6。通过比较<strong class="kl iu"><em class="it">Y’</em></strong>和<strong class="kl iu"><em class="it">Y’</em></strong>计算无监督误差。<br/> 7。使用(2)和(6)计算总误差。<br/> 8。从(3)中取出任何高置信度的猜测，并将它们从<strong class="kl iu"> <em class="it"> X </em> ' </strong>移动到<strong class="kl iu"> <em class="it"> X </em> </strong> <em class="it">。<br/> 9。</em>重复。</p></blockquote><p id="c0f3" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在上面的步骤过程中，有很多方法可以做到(4)。一种有趣的方法叫做反向翻译。想想这句话:</p><blockquote class="mt mu mv"><p id="8f1f" class="kj kk lf kl b km kn ju ko kp kq jx kr mw kt ku kv mx kx ky kz my lb lc ld le im bi translated">“<strong class="kl iu">标注的</strong>数据不如<strong class="kl iu">未标注的</strong>数据便宜，因此<strong class="kl iu">是过去的<strong class="kl iu">遗迹</strong>。”</strong></p></blockquote><p id="6225" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我可以把这个句子转换成另一种语言，然后再转换成英语。这是我用挪威语作为中间媒介的结果。</p><blockquote class="mt mu mv"><p id="b2df" class="kj kk lf kl b km kn ju ko kp kq jx kr mw kt ku kv mx kx ky kz my lb lc ld le im bi translated">“<strong class="kl iu">标明的</strong>数据不像<strong class="kl iu">未标明的</strong>数据那么便宜，因此<strong class="kl iu">已经是过去的<strong class="kl iu">了。”</strong></strong></p></blockquote><p id="d932" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在我们添加了噪声。细微的变化，但对一个算法来说是天壤之别。</p><h1 id="51c8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果&amp;标签数据的困境</h1><p id="dbb7" class="pw-post-body-paragraph kj kk it kl b km mo ju ko kp mp jx kr ks mq ku kv kw mr ky kz la ms lc ld le im bi translated">前面提到的论文对上一节中概述的方法进行了压力测试。在文本分类数据集上使用<a class="ae lv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> Google的BERT (Large)算法</a>，他们生成了以下结果，其中<em class="lf"> n </em>是使用的标记样本的数量):</p><pre class="na nb nc nd gt np nq nr ns aw nt bi"><span id="387e" class="nu lx it nq b gy nv nw l nx ny">       + — — — — + — — — — — — — —  + — — — — — — — — — — — +<br/>       | <strong class="nq iu">Dataset</strong> | <strong class="nq iu">Supervised Error </strong>| <strong class="nq iu">Semi-Supervised Error </strong>|<br/>       +<strong class="nq iu"> — — — — </strong>+ <strong class="nq iu">— — — — — — — —  </strong>+<strong class="nq iu"> — — — — — — — — — — —</strong> +<br/>       | <em class="lf">IMDb</em>    | 4.51  (n = 20k)  | 4.78 (n = 20)         |<br/>       | <em class="lf">Yelp-2</em>  | 1.89  (n = 560k) | 2.50 (n = 20)         |<br/>       | <em class="lf">Yelp-5</em>  | 29.32 (n = 650k) | 33.54(n = 2.5k)       |<br/>       | <em class="lf">Amazon-2</em>| 2.63  (n = 3.6m) | 3.93 (n = 20)         |<br/>       | <em class="lf">Amazon-5</em>| 34.17 (n = 3.0m) | 37.80(n = 2.5k)       |<br/>       | <em class="lf">DBpedia</em> | 0.64  (n = 560k) | 1.09 (n = 140)        |<br/>       + — — — — + — — — — — — — —  + — — — — — — — — — — — +</span></pre><p id="e9ac" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这些结果表明，这种特殊的<strong class="kl iu"> SSL方法在处理一小部分标记数据</strong>时几乎与监督方法处理所有标记数据时一样好！</p><p id="c70f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">至于带标签的数据的有用性——自然，如果带标签的数据是可用的，你就使用它。但是有了这样的结果，任何创建有序、结构化、有标签的数据的基本原理正在迅速消失，特别是因为无标签数据可以广泛获得并且非常便宜(计算、时间和精力方面)。未来是不可预测的(也是未标记的)，但我们正在慢慢获得捕捉它的必要工具！</p><p id="a508" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="lf">本帖原文可在</em> <a class="ae lv" href="https://abhiraghunathan.com/posts/unsupervised_data_augmentation.html" rel="noopener ugc nofollow" target="_blank"> <em class="lf">这里</em> </a> <em class="lf">找到。</em></p></div></div>    
</body>
</html>