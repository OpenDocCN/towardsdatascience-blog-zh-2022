# 模型部署系统的设计考虑

> 原文：<https://towardsdatascience.com/design-considerations-of-model-deployment-system-c16a4472e2be>

## **理解部署机器学习模型的跨职能需求的工程师指南**

![](img/3fb6a0818cbadd790e425ffed967bc38.png)

由[凯利·西克玛](https://unsplash.com/@kellysikkema)在 [Unsplash](https://unsplash.com) 上拍摄的照片

机器学习的影响日益广泛，推动了产品推荐、欺诈检测和对话式人工智能等应用。数据科学团队不再只是通过仪表板或演示文稿与决策者分享业务见解。更常见的是，只有将模型应用到最终应用中，才能实现 ML 的全部潜力。

然而，部署机器学习模型仍然是最耗时的工程挑战之一。作为负责构建模型部署管道的工程师，理解这是一项跨职能的工作并需要与多个组织的利益相关者合作是至关重要的。

在本文中，我们将深入模型部署系统的关键需求，这些需求来自三个利益相关者:数据科学团队、DevOps 团队和产品团队。然后，我们将分解主要的设计考虑事项(标有💡)以满足这些要求，同时提出切实可行的建议。

# 数据科学家

数据科学团队可能是需要考虑最多(也可能是最具挑战性)需求的团队，毕竟，您要实现的是他们的模型。他们的主要目标是**建立准确而有影响力的模型**。

## #1:从培训环境无缝过渡

数据科学家使用专门的开发工具开发他们的模型，例如 Jupyter notebook、不同类型的 ML 培训框架和实验管理平台，这些环境对于许多软件工程师来说可能是陌生的。这使得模型工件和特性提取代码的产品化成为部署管道中最容易出错的过程之一。

> **💡** *一个成熟的模型部署系统必须能够轻松地与多种 ML 框架和模型开发环境集成，让数据科学团队能够灵活地选择任何适合其模型培训需求的工具。*

## #2:不断重新部署新模型的能力

数据科学团队知道，模型在部署到生产环境后不会保持不变，概念漂移和数据漂移可能会随着时间的推移而发生，需要定期重新培训和更新新模型。您的数据科学团队可能还想加入新的特性，更新模型架构，或者尝试新的 ML 框架。部署系统应该允许数据科学团队在生产中轻松更新新模型，并且充满信心。

> ***💡*** *你需要一种方法来管理代码，以及模型和库的所有依赖关系。此外，为了可再现性和平滑回滚(如果必要)，所有这些部分都必须进行版本控制并一起部署。*

这个需求还引入了一个版本化的挑战:发布的模型越多，就越难跟踪重要的细节，比如每个模型是如何被训练的，谁有权限，以及谁负责监控以确保质量。

> ***💡*** *记录和调用特定细节的模型注册中心将简化与管理模型的多个版本相关的操作开销。*

## #3:模型性能的可见性

了解模型在运行后的表现是至关重要的。考虑这个例子:当一个推荐产品的新模型导致*比以前少*的销售时，这可能是一个新模型预测不太准确的信号。您的系统应该帮助检测到这一点，并尽快回滚到以前的版本。

随着时间的推移，随着习惯和趋势的改变，概念和数据也会发生漂移。当这种情况发生时，数据科学团队需要进行调查，以确定模型是否需要重新训练。

> **💡** *您的 ML 服务解决方案应该让数据科学家能够轻松地实时检索和分析预测。CloudWatch、DataDog 或 Grafana Loki 等简单解决方案可能足以满足某些情况，但更复杂的情况可能需要 WhyLabs、Fiddler.ai 或 Arize 等公司的专业 ML 监控解决方案。*

## #4:轻松访问功能

ML 模型的可行性不仅依赖于其原始输入数据，还依赖于创建附加派生特征的变换。在在线预测场景中，很多时候这些转换也需要在数据被摄取时实时执行。关键的挑战是定义一个检索特征数据的通用接口，并以同样的方式处理训练和在线服务的特征转换。

> ***💡*** *对于大多数特征转换，最好直接将代码嵌入服务管道，因为这样可以确保模型和特征处理代码始终处于同一版本。然而，当处理需要对时间序列数据进行在线聚合的特征时，以低延迟计算这些特征是具有挑战性的。对于这种类型的用例，您可以考虑使用一个专用的特性库，它为在线服务和培训管道提供一个公共接口来访问特性。一个好的特征库还可以帮助协调新特征或特征的新版本的推出，以便它与您的模型同步。*

# DevOps 工程师

您可能会与 DevOps 团队紧密合作来部署 ML 模型。这个团队主要**负责为公司软件产品提供动力的底层技术基础设施**，监督关键领域，如整个运营的稳定性和安全性。

## #1:在首选平台上部署模型

模型服务工作负载通常依赖于 DevOps 团队管理的其他现有服务来配置资源、检索功能、流式日志和监控。在已经建立了特定的云或本地环境、部署模式和工具的情况下，以类似的方式部署新的 ML 服务以减轻维护负担是很重要的。

> **💡** *您的预测服务应该能够与不同的日志记录、监控和供应工具一起工作。此外，它应该根据您的使用情况进行优化，以便以多种不同的形式运行——无论是作为 Kubernetes 上运行的实时服务，使用 Spark 的批处理推理作业，还是通过 Terraform 或 CloudFormation 提供的云中的无服务器部署。*

## #2:确保服务的可靠性和维护

一旦部署了模型，服务的可靠性对 DevOps 团队来说是极其重要的。

> **💡** *开发运维团队应该能够监控、重现相同的部署以创建测试环境，并整合 CI 测试。*

**用于监控、跟踪和警报:**服务于工作负载的模型应该易于与 Prometheus、OpenTelemetry、CloudWatch 或 Datadog 等系统集成。这使得 DevOps 团队可以像监控其他现有服务一样监控这些新的 ML 服务。

**为了可再现性:**为了最小化将来失败部署的机会，系统应该能够再现相同的部署来创建测试环境。在生产中断的情况下，能够轻松地将所有系统恢复到以前的状态让 DevOps 团队高枕无忧。GitOps 流是一种标准的、声明性的方式，用于描述所需的部署状态，并在需要时自动重新生成整个堆栈。

例如，下面的代码示例显示了 Kubernetes 与 Yatai 和 BentoML 的模型部署规范:

**对于 CI 和测试:**由于服务逻辑或模型预测的变化而导致的意外行为可能具有极大的破坏性，这就是为什么 CI 测试是在问题进入生产之前必须进行的。

> ***💡*** *您的模型部署解决方案应该考虑到与 CI 管道的集成，这样可以测试整个服务管道。内联自动化测试不仅应该评估代码中的业务逻辑，还应该验证关键模型预测指标没有超出可接受的范围。*

## #3:保护对模型的访问

保护对 ML 模型的访问与保护对业务的其他关键部分(如客户数据或您公司的知识产权)的访问同等重要。

> ***💡*** *ML 预测服务，就像任何其他在线服务一样，应该在认证和访问方面应用通用安全* [*最佳实践*](https://owasp.org/www-project-top-ten/) *。我们看到许多用户在 Istio 背后部署预测服务。这种类型的服务网格可以针对更复杂的身份验证和路由场景进行配置。同样值得注意的是，特别是 ML 服务可能会出现新类型的安全挑战，如* [*对抗性示例*](https://openai.com/blog/adversarial-example-research/) *，旨在导致模型出错。*

# 产品经理

你的产品团队是一个重要的利益相关者——毕竟，他们不仅要对 ML 的商业影响负责，还要对 ML 的所有经济、社会和法律问题负责。他们对 ML 部署系统的考虑主要集中在**证明模型的价值**和**交付具有出色用户体验的最终产品**这两个阵营。

*注意:这里的产品经理通常是指产品负责人——在你的组织中负责 ML 项目的成功并不断倡导最终用户和商业价值的任何人。*

## #1:快速验证模型的价值

产品团队希望尽快将一个功能性的原型推向市场，以验证其价值。由于这种上市时间的要求，许多 ML 专家认为 ML 项目应该从较小的规模开始，然后随着时间的推移进行调整和完善。

> ***💡*** *将模型部署到生产中时，最初跳过更高级的功能。当模型看起来可行时，一切都可以微调以获得最佳结果。*

## #2:满足延迟要求

预测延迟是产品团队的一个关键考虑因素，因为它直接影响最终用户的体验(因此也影响收入)。考虑这个例子:一家视频游戏公司使用 ML 模型来匹配具有相似技能的竞争玩家，这必须是接近即时的——如果它运行太慢，可能会导致沮丧的用户退出。

> ***💡*** *根据不同的用例，模型服务系统可能需要考虑不同的延迟 SLA(服务级别协议)。对于一些时间敏感和任务关键的用例，如广告定位，ML 服务系统需要以 50 毫秒的延迟实时提供预测。对于其他人来说，比如用于发行信用卡的欺诈检测模型，几分钟就足够了。*

## #3:降低运营成本

大规模机器学习是极其计算密集型的工作负载，并且可能涉及数千台同时运行的机器。这为您的组织带来了能源、维护和云支出方面的成本。一个 ML 模型只对一个企业有意义，如果它提供的价值超过运行它的成本。这就是为什么产品团队希望确保模型交付的价值，同时优化成本。

> ***💡*** *开箱即用的水平和垂直扩展优化解决方案可确保从一开始就将成本降至最低。不同的用例可能需要其中之一。*

**垂直扩展和资源利用:**如果预测流量需要专门的硬件加速，或者通过垂直扩展可以更加经济高效，您需要确保**软件针对您选择的硬件进行了优化**。具体来说，您需要确保它支持常见的性能最佳实践，例如[预测请求的自适应批处理](https://rise.cs.berkeley.edu/wp-content/uploads/2017/02/clipper_final.pdf)，将负载适当地分配给所有可用的 CPU 内核，并在需要的地方利用 GPU 或自定义加速器。这不仅有助于最大限度地减少单个请求的延迟，而且通过高效利用资源减少了所需的计算资源量。

**水平自动扩展:**如果预测流量在指定时间段内发生变化(例如，午餐时间或黑色星期五和圣诞节前后与事件相关的高峰时段的食品配送服务)，解决方案应该能够自动水平扩展，以便平衡延迟要求和资源利用率。

**无服务器和零扩展:**如果预测流量仅限于每月几次预测，您可能会考虑像 [Knative](https://cloud.google.com/knative) 或 [AWS Lambda](https://aws.amazon.com/lambda/) 这样的无服务器架构。当没有请求正在进行时，这些解决方案可以**缩减到 0** ，从而节省成本。

# 结论

数据科学、开发运维以及产品团队都与 ML 项目的成功息息相关。这些团队需求的多样性带来了与典型的软件部署过程非常不同的挑战。虽然可能没有必要在 ML 模型部署系统的第一次迭代中解决所有这些考虑事项，但是知道未来可能会发生什么是有帮助的，以便可以进行适当的规划和研究来确保最大的成功可能性。

*一点背景:我叫杨，是开源模型服务框架*[*BentoML*](https://github.com/bentoml/BentoML)*的创建者。此前，我曾在 Databricks 的统一数据科学平台早期帮助创建该平台。如果你有兴趣了解更多关于这个话题的信息或参与讨论，请加入我们的* [*社区 slack*](https://link.bentoml.com/44cb) *，在这里数百名传销从业者聚集一堂，讨论传销的所有事情。*