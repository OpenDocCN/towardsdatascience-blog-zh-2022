<html>
<head>
<title>Meet BERTopic— BERT’s Cousin For Advanced Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">认识一下BERTopic——BERT的堂兄，学习高级主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/meet-bertopic-berts-cousin-for-advanced-topic-modeling-ea5bf0b7faa3#2022-01-26">https://towardsdatascience.com/meet-bertopic-berts-cousin-for-advanced-topic-modeling-ea5bf0b7faa3#2022-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="1620" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">认识一下BERTopic——BERT的堂兄，学习高级主题建模</h1></div><div class=""><h2 id="ceff" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用转换器进行自动主题发现的综合概述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3408243f148245444f396300b06b008e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9N_KNyRK1w9dclod"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">善良好奇的在<a class="ae ky" href="https://unsplash.com/photos/ZDUXvlyU_iI" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae ky" href="https://unsplash.com/@kindandcurious" rel="noopener ugc nofollow" target="_blank"/></p></figure><h1 id="4475" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="344e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们生活在一个被大量文本信息包围的时代，如调查回复、社交媒体评论、推文等。找到满足个人需求的合适信息是一项挑战，尤其是在处理大量不同的数据时。</p><p id="719b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">多亏了<a class="ae ky" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank">主题建模</a>，一个自然语言处理的时代用来通过将大<strong class="lt iu"> <em class="ms">未标记文本</em> </strong>数据分组/聚类成主题来高效地分析它们。最近，我遇到了BERTopic，这是一个用于主题建模的简单而强大的库。所以，我决定试一试，结果很厉害！</p><h1 id="3409" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">伯托皮—凯扎科？</h1><p id="c693" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">该工具由<a class="ae ky" href="https://www.maartengrootendorst.com" rel="noopener ugc nofollow" target="_blank"> Maarten Grootendorst </a>于2020年开发，根据文档:</p><blockquote class="mt mu mv"><p id="ddb8" class="lr ls ms lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">BERTopic是一种主题建模技术，它利用了🤗transformers和c-TF-IDF创建密集的集群，允许轻松解释主题，同时在主题描述中保留重要的单词。</p></blockquote><p id="55c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从模型创建到各种可视化功能，它都非常简单且易于操作。</p><h2 id="3122" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">BERTopic的主要组件</h2><p id="cd16" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下图显示了该算法的三个主要组件:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/161874378d63173624e3522866783741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DxAqCqx6dzg0KQlg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:算法<a class="ae ky" href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html" rel="noopener ugc nofollow" target="_blank">文档页面</a>上由<a class="ae ky" href="https://www.maartengrootendorst.com" rel="noopener ugc nofollow" target="_blank">马腾·格罗腾德斯特</a>制作的BERTopic主要部件</p></figure><ol class=""><li id="c389" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu">文件嵌入</strong></li></ol><p id="314e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第一步是通过利用<a class="ae ky" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">句子转换器</a>来执行的，这是一个多语言框架，提供了一种简单的方法来为我们的数据语料库中的每个文档生成密集的向量表示。</p><p id="f087" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2。文档聚类</strong></p><p id="53b1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">根据之前的嵌入，我们可以使用UMAP方法对维度执行聚类，结果被传输到HDBSCAN以聚类语义相似的聚类(文档集)。</p><p id="4122" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 3。主题表示</strong></p><p id="e5dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这最后一步是通过使用基于类别的TF-IDF方法为每个聚类提取最相关的词来执行的，以便获得主题的表示。公式如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/2b08b2247f66dae3c330f6782ef24907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GCIX-inco8jAt15X.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:算法<a class="ae ky" href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html" rel="noopener ugc nofollow" target="_blank">文档页面</a>上<a class="ae ky" href="https://www.maartengrootendorst.com" rel="noopener ugc nofollow" target="_blank"> Maarten Grootendorst </a>的c-TF-IDF公式</p></figure><p id="bf43" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">既然我们已经了解了BERTopic是什么以及它是如何工作的，我们现在就可以准备用Python实现一个端到端的实现了。</p><p id="9cb4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您喜欢视频，可以观看本文的视频演示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="068c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于数据</h1><ul class=""><li id="9a8d" class="nm nn it lt b lu lv lx ly ma ny me nz mi oa mm ob ns nt nu bi translated">这是澳大利亚广播公司历时八年发布的新闻，<strong class="lt iu"> <em class="ms">可免费获得</em> </strong>上的<a class="ae ky" href="https://www.kaggle.com/therohk/million-headlines" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和<strong class="lt iu"> <em class="ms">不得用于商业用途</em> </strong>。它有两个主要栏目:</li><li id="659d" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><strong class="lt iu"> <em class="ms">发布日期</em> </strong>:文章以yyyyMMdd格式发布的日期。</li><li id="1b8b" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><strong class="lt iu"><em class="ms">headline _ text</em></strong>:英文标题的文字。<strong class="lt iu"> <em class="ms">这是主题模型将使用</em> </strong>的信息。</li></ul><p id="587e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了能够使BERTopic训练更快，我使用了数据集的一个子集，你可以在我的GitHub页面上访问它。</p><p id="f8f4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">头条分布</strong></p><p id="8e08" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的目标是检查标题的长度分布，以便使用更适合数据集的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据信息. py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/a014a714accd2d717437d93ec635b527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PsyMO95-oXkShuYj_VM6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:标题长度分布:最大长度为15(图片由作者提供)</p></figure><p id="e5a3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到，最长的标题有13个标记，下面是3个随机标题:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">random _ headlines.py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/00733ac0fa3e479ac81552c83428e8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fKMWcSYBulcjcG313d-GYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:数据集中的3个随机标题</p></figure><h1 id="63b1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">BERTopic实现</h1><p id="6f55" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与本文相关的所有代码都可以通过我的<a class="ae ky" href="https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Advanced_Topic_Modeling_BERTopic.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>访问。</p><h2 id="6339" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">模型结构</h2><p id="60cd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这一步与BERTopic模型的实现相关，对默认参数进行如下微调:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERTopic_model.py</p></figure><ul class=""><li id="2b84" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm ob ns nt nu bi translated"><code class="fe ok ol om on b">verbose</code>到<code class="fe ok ol om on b">True</code>:使模型初始化过程不显示消息。</li><li id="6573" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><code class="fe ok ol om on b">paraphrase-MiniLM-L3-v2</code>是性能和速度权衡最好的句子变形金刚模型。</li><li id="4014" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><code class="fe ok ol om on b">min_topic_size</code>设置为50，默认值为10。该值越高，分类/主题的数量越少。</li><li id="6b46" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><code class="fe ok ol om on b">.fit_transform()</code>在头条数据集上训练BERTopic模型。</li></ul><h2 id="bd53" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">主题抽取和可视化</h2><p id="174d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦模型经过训练，就可以执行许多操作，例如主题提取和可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">top_topics.py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/f1cbb8ba24fdc5173ef57210aa92af2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdSI8UKMIa7LVsdyv6Zq9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:前5个“热门”话题信息(作者图片)</p></figure><p id="35da" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上表有3个主要列，按照主题大小/数量的降序提供了所有54个主题的信息。</p><ul class=""><li id="44e0" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm ob ns nt nu bi translated"><strong class="lt iu"> Topic </strong>是主题号，一种标识符，离群值标记为-1，那些对应的主题应该被忽略，因为它们没有带来任何附加值。</li><li id="691f" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><strong class="lt iu">计数</strong>是题目中的字数/术语数。</li><li id="0419" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><strong class="lt iu">名称</strong>是赋予主题的名称。</li></ul><p id="e1f7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于每个主题，我们可以检索出最热门的词及其对应的c-TF-IDF得分。分数越高，这个词就越能代表主题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">topic_info.py</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/34c4588b2e474cc7c9a6ca6666eed7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*D5Fxst5IUbgjoBim5t_ktA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:第一个热门话题的术语(top #1)及其c-TF-IDF得分(图片由作者提供)</p></figure><p id="e3e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从这个话题中，我们观察到所有的单词对于潜在的话题都是连贯的，这个话题似乎是关于⚽️.足球的</p><h1 id="ecc4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">主题可视化</h1><p id="f8d5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">主题可视化有助于更深入地了解每个主题。BERTopic提供了几种可视化的可能性，例如<em class="ms">术语可视化、主题间距离图、主题层次聚类</em>等等，我们的重点将放在那些已经被引用的方面。</p><h2 id="f3f7" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">主题术语</h2><p id="5e6f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">每个话题最相关的词可以用c-TF-IDF评分出来的柱状图的形式可视化出来，直观对比话题很有意思。下面是主题6主题的相应可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">主题词得分(按作者)</p></figure><p id="e9b5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">话题1的热门词汇是男人，被控谋杀入狱，这显然是一个与犯罪有关的话题。同样的分析可以很容易地从每个剩余的主题中得出。横条越长，与主题的相关性越大。</p><h2 id="9e43" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">主题间距离图</h2><p id="1c87" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于熟悉潜伏狄利克雷分配<a class="ae ky" href="https://github.com/cpsievert/LDAvis" rel="noopener ugc nofollow" target="_blank"> LDAvis </a>库的人来说，如果不是<a class="ae ky" rel="noopener" target="_blank" href="/do-you-want-to-cluster-unlabeled-text-data-try-out-topic-modeling-235795ae7cb7"> <strong class="lt iu">下面是我关于它的文章</strong> </a>。这个库为用户提供了一个交互式仪表板，显示每个主题对应的单词及其分数。BERTopic用visualize_topics()函数做了同样的事情，甚至更进一步，给出了主题之间的距离(越低越相似)，所有这些都用一个函数<code class="fe ok ol om on b">visualize_topics().</code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">主题间距离图(作者)</p></figure><h2 id="4341" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">可视化主题层次结构</h2><p id="0b88" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如您在Interdistance主题仪表板中看到的，有些主题非常接近。我能想到的一件事是，我如何减少话题的数量？好消息是，这些主题可以分等级，以便选择适当数量的主题。可视化有助于理解它们之间的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/3a9cc2566b6f6a62e7f7330167f65b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2V-pCIvqqNAwRHpn6EXzPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:前30个主题的层次聚类(图片由作者提供)</p></figure><p id="ca87" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过查看树状图的第一层(第0层),我们可以看到具有相同颜色的主题被分组在一起。例如:</p><ul class=""><li id="b746" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm ob ns nt nu bi translated">主题7(健康、医院、精神)和主题14(死亡、车祸、死亡)因为相近而被归为一组。</li><li id="15a8" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated">主题6(农民，农场，农民)和16(牛，羊，牛肉)也必须以同样的方式分组。</li><li id="750a" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated">等等。</li></ul><p id="8467" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有这些信息可以帮助用户更好地理解为什么主题被认为是相似的。</p><h1 id="47ec" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">搜索主题</h1><p id="0362" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦训练了主题模型，我们就可以使用<code class="fe ok ol om on b">find_topics</code>函数搜索与输入查询词/术语语义相似的主题。在我们的例子中，我们可以搜索与单词“<strong class="lt iu"><em class="ms"/></strong>”相关的前3个主题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">topic _ search.py</p></figure><ul class=""><li id="611d" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm ob ns nt nu bi translated"><code class="fe ok ol om on b">similar_topics </code>相似主题包含从最相似到最不相似的主题索引。</li><li id="52a4" class="nm nn it lt b lu oc lx od ma oe me of mi og mm ob ns nt nu bi translated"><code class="fe ok ol om on b">similarity</code>包含降序排列的相似性得分。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">show_top_similar_topic.py</p></figure><p id="02f2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以从下面的输出中看到，最相似的主题是显示术语“选举”、“特朗普”、“奥巴马”等。这显然与政治有关。</p><pre class="kj kk kl km gt ot on ou ov aw ow bi"><span id="fb86" class="mz la it on b gy ox oy l oz pa">Most Similar Topic Info:  <br/>[('election', 0.08389822503224101), ('trump', 0.0512571921683764), ('party', 0.034052442556456154), ('donald', 0.03268734381432749), ('obama', 0.030983388040422003), ('liberal', 0.02869493503505037), ('bush', 0.022854654022153992), ('liberals', 0.022814234525823825), ('vote', 0.02273902178387999), ('presidential', 0.02256653331627359)] </span><span id="9167" class="mz la it on b gy pb oy l oz pa">Similarity Score: 0.7048206256962174</span></pre><h1 id="a06e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">模型序列化和加载</h1><p id="0013" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当您对模型的结果感到满意时，可以使用以下说明保存它以供进一步分析:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh nx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型_序列化. py</p></figure><h1 id="e395" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="533e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我真的很喜欢试验BERTopic，结果非常令人鼓舞。请不要犹豫，在您的业务案例中尝试一下。然而，重要的是要注意，BERTopic的结果在运行之间并不一致，这是由于用于降维的UMAP的随机性质。所以，在UMAP使用random_state参数任何随机行为。</p><p id="041e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在YouTube<a class="ae ky" href="https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ" rel="noopener ugc nofollow" target="_blank">上关注我</a>获取更多互动会话！</p><h1 id="04d9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外资源</h1><p id="cd51" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://maartengr.github.io/BERTopic/algorithm/algorithm.html" rel="noopener ugc nofollow" target="_blank"> BERTopic算法</a><br/><a class="ae ky" href="https://maartengr.github.io/BERTopic/faq.html#:~:text=Why%20are%20the%20results%20not%20consistent%20between%20runs%3F,-%C2%B6&amp;text=Due%20to%20the%20stochastic%20nature,topics%20that%20suit%20you%20best." rel="noopener ugc nofollow" target="_blank">BERTopic FAQ</a><br/><a class="ae ky" href="https://github.com/MaartenGr/BERTopic" rel="noopener ugc nofollow" target="_blank">BERTopic Github</a><br/>ka ggle上的数据集</p><p id="1da0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">再见🏃🏾</p></div></div>    
</body>
</html>