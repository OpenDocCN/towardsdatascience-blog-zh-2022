# 数据科学行业的不平等

> 原文：<https://towardsdatascience.com/the-inequality-in-the-data-science-industry-937992032851>

## 意见

在过去十年中，数据科学作为一门学科在公众意识中受到了越来越多的关注。无论是正在考虑有利可图的职业道路的本科生，还是正在寻找高效工具来通知或自动化重要决策的公司和组织的领导者。虽然专注于数据科学可能会建立一个人的技术职业生涯或优化公司的服务，但作为一门学科，我们在很大程度上未能认识到数据科学在实践中发挥的作用:当有人实践数据科学时，他们要么是在挑战要么是在强化现有的权力结构。

![](img/5f360a79d35e1244538c8bfa81edf467.png)

fauxels 拍摄的照片:[https://www . PEX els . com/photo/group-of-person-sitting-in-户内-3184306/](https://www.pexels.com/photo/group-of-person-sitting-indoors-3184306/)

这在构成我们社会的个人和集体层面上都发生了变化:在集体层面上，组织有义务让代表各种背景和生活经历的全面发展的数据科学家加入他们的数据科学团队(目前这不是一项容易的任务)。在个人层面上，每个数据科学家都有责任了解他们在人为权力系统中的位置，这样他们就不会在实践中无意中扮演压迫者的角色。

像任何工具一样，数据科学本身没有好坏之分。它可以用来改善数百万人的健康，也可以强化种族主义，扩大不平等。数据科学的许多有害应用发生在工业环境中:*工业数据科学，*与*学术数据科学相对。*工业数据科学最终是为利润服务的数据科学，而学术数据科学是为知识服务的数据科学。目前的情况是，工业数据科学在建设和维护我们的社会方面的影响力远远超过其知识驱动的同行，因为实践数据科学对组织来说成本高昂，并且只有在有足够资源的情况下才能实现。

今天，工业数据科学通过处理大量数据来创建自动化决策模式，称为*模型。*这些模型最终提供了自动化决策能力，为企业节省了时间和金钱。从表面上看，这本身并不是一件坏事，它被证明是一种构建有用的定制产品体验或优化组织内部工作的强大方法。我们有工业数据科学来感谢高效的食品配送应用程序、搜索引擎和各种自动推荐，等等。

**数据内的表示**

数据科学的从业者经常说，生成模型是预测未来的一种方式。这是对模型工作的一种常见而危险的误解。一个模型*将过去*投射到未来，而不是*预测未来*。随着每一个自动化的决策，过去的世界规则在数据处理的瞬间重现，进而塑造未来。结果产生的信息将通知明天的新模式。通过这种方式，我们构建了一个工业驱动的数据处理循环，这种循环反复地、盲目地作用于自身。这意味着我们用来构建模型的数据在这个重复的循环中被后续模型的构建所塑造。

明智地选择数据集是数据科学家在训练新模型时可以做出的最重要的决定之一。通常目标是生成具有高预测准确性的模型。为了创建准确的模型，数据科学家需要找到与他们打算训练的模型的需求最相关的数据。

这里有一个例子:一家金融机构的数据科学家的任务是训练一个模型来预测客户的信用评分。他们的下一步是梳理现有客户的可用数据集，并在所有这些数据中确定哪些信息可能有助于训练一个模型，以最准确的方式预测信用评分。

如果用于训练新模型的数据包括性别或与性别相关的特征作为其变量之一，则新的信用评分模型将可能检测到一种模式，即女性平均比男性挣得少，并且信用评分较低，因为这反映在现实世界的客户数据中。作为一个意想不到的有害结果，如果所有其他因素都相同，新模型将预测女性的收入低于男性。

鉴于性别薪酬差距的现实，这可能会提高模型信用评分预测的准确性，但模型本身无法认识到这是不公平的。这就是数据科学家在选择未来不会产生不公正偏见的数据和模型时的过失所在。查看他们的信用评分模型的输出就是查看过去的现状，因此，如果金融机构随后依赖该模型来生成客户的信用评分，就会存在危险:这将导致性别歧视的预测，从而对人们的现实生活产生影响。这将产生新的真实世界数据，这些数据是由信用评分模型中的偏见形成的。所有来自过去的数据都是基于我们现在拥有的不完美的现实图景，当它被疏忽地用来塑造未来时，这个循环还在继续。

凯茜·奥尼尔的《数学毁灭的武器》一书中有一些真实的例子。她指出了导致带有编码种族主义或其他偏见的模型的短视决策的例子。这些例子包括预测教师质量风险、信誉和累犯的模型。

例如，奥尼尔描述了如何训练累犯风险模型来预测被定罪的罪犯在未来再次犯罪的可能性。如果一个人失业，以前与警察有过冲突，或者来自一个警察过度的社区，这个模型会得到更高的风险。虽然这些趋势在现实世界的数据中显而易见，但该模型没有内置复杂的背景-在这种情况下，警察遭遇往往是由于不成比例地针对黑人和拉丁裔美国人的种族主义警务实践而发生的。由于数据中反映的系统性种族主义，该模型建议有色人种在监狱中获得不应有的更长刑期。

从事该项目的数据科学家对刑事司法系统中的种族偏见如何反映在他们用来训练累犯模型的数据中没有足够的认识。

这说明了只掌握工作机制而不了解工作背景的数据科学家如何没有足够的信息来有效地完成工作。

在工业数据科学领域，数据科学家通常只不过是一个知道如何使用数据开发模型的可互换的人，而不是在给定数据的主题方面具有特定背景的人。这就是数据科学作为一门学科对其自身影响如此盲目的原因。其结果是，数据科学家及其雇主往往无法考虑他们在权力体系中的背景，这最终导致系统性不平等永久化。

当研究人员 Joy Buolamwini 和 Timnit Gebru 发现微软、IBBM 和 Face++等大型科技公司使用的商业性别分类模型存在准确性差异，并就他们的发现发表了一篇论文时，这一问题首次被提出。他们的工作表明，美国科技公司为其模型使用的数据集中的图像超过 77%是男性，超过 83%是白人。结果，当身为黑人女性的 Buolamwini 试图上传自己的照片时，那些商业面部识别程序错误地分类了她的性别，或者根本没有将她的照片识别为人脸。数据缺乏多样性导致了一个无效的模型。

**工作场所内的代表权**

如果数据科学家使用反映系统性偏差或缺乏足够多样性水平的数据集，那么产生的模型也会如此。房间里有一个了解数据本身的人可以大大降低数据被无效或不公正使用的可能性。当团队由来自一个同质群体的人组成时，视角受到他们的生活经验和接触其他观点的范围的限制。如果生活经验没有足够的多样性，同质群体很容易不经意地做出排除其他身份和观点的决定。

在工业数据科学的世界中，越来越多的声音呼吁解决这个问题。例如，前述性别分类研究的另一位作者 Gebru 博士与 Joy Buolamwini 合作，后来成为谷歌道德人工智能团队的联合领导者。在那里，她公开批评了在谷歌做出最终人工智能相关决策的大多数人是男性。她后来被该公司解雇，因为她批评了该公司雇用少数族裔的方式以及当今人工智能系统中固有的偏见。“他们不仅没有优先雇用更多来自少数族裔社区的人，还压制了他们的声音，”她说。

对于一个组织来说，实践数据科学是一种特权，因为它需要大量昂贵的物理和计算资源。无论是夫妻店，还是在车库里创办新企业的创意企业家，或是一些苦苦挣扎的非营利组织，都无法利用数据科学。获取、维护、存储和访问用于训练模型的足够数据会产生巨大的成本，并且在实践中应用该模型还需要更多的计算资源成本。唯一能够实践工业数据科学的组织是富有的——要么是足够大的公司企业，要么是强大的政府，要么是特别富有的学术机构。

结果是权力失衡，有利于拥有数据并决定如何使用数据的少数人。工业数据科学的实践着眼于优化而不是理解。很多时候，数据科学被用作系统内自动化决策的手段，而不是更深入地理解这些决策及其背景的手段。这是危险的，因为它会导致短视的决策，无法检查其在世界上的影响，除了是否实现了即时(通常是)利润驱动的目标。它最终强调了那些构建数据科学驱动系统的人的特殊需求和世界观。

造成这种不公平数据集的偏见也塑造了世界上能够首先实践数据科学的个人的价值观。由于各种可能的原因，从种族、性别到阶级等等，这些系统性偏见没有充分地过滤掉人们。根据美国平等就业机会委员会(EEOC)的数据，科技行业约 80%的高管、高级官员和经理是男性，约 83%是白人。至于行业中的数据科学家，根据 Zippia 的数据科学人口分析，所有工人中有 65.2%是男性，当涉及到种族统计时，据报道，所有数据科学家中有 66.1%是白人。

我们能做什么？

D'Ignazio 和 Lauren F. Klein 合著的《数据去中心主义》( Data Deminism)一书为试图了解女权主义如何帮助他们实现正义的数据科学家，以及试图了解他们的工作如何应用于不断发展的数据科学领域的女权主义者提供了具体的行动步骤。数据女权主义是一种思考数据的方式，包括数据的用途和限制，它是由直接经验、对行动的承诺和交叉女权主义思想提供信息的。《数据女权主义》一书的作者*，*讨论了数据科学工作通常是一项孤独事业的问题本质。提出让一个人代表所有人做决定而不代表交叉声音的概念可能会在工作中引入很多偏见，并可能伤害没有代表的声音。数据女权主义的原则之一是接受多元化，这意味着在数据科学过程的所有阶段重视和包括与数据集有联系的声音。

承认两个重要问题很重要。当涉及到与人类相关的应用时，数据科学不仅仅是一个定量领域。从上述数据科学应用的例子来看，很明显，负责设计这些系统的数据科学家缺乏在系统发布后检测其危害和偏见的能力。

工作场所中的表示导致数据中的表示。这两个概念是相互关联的。相反，数据中的表示最终有助于建立一个更加平等的招聘群体。

**参考文献**

1 Buolamwini、Joy 和 Timnit Gebru。"性别差异:商业性别分类的交叉准确性差异."[https://dam-prod . media . MIT . edu/x/2018/02/06/Gender % 20 shades % 20 intersect % 20 accuracy % 20 disparities . pdf .](https://dam-prod.media.mit.edu/x/2018/02/06/Gender%20Shades%20Intersectional%20Accuracy%20Disparities.pdf.)

2“美国的数据科学家人口统计学和统计学”*Zippia——寻找工作、薪水、公司、简历帮助、职业道路等*，【https://www.zippia.com/data-scientist-jobs/demographics/.】T22022 年 5 月 28 日访问。

3d ignazio，Catherine 和 Lauren F. Klein。*数据女权*。麻省理工学院出版社，2020 年。2022 年 5 月 28 日访问。

4《纽约时报》。“谷歌研究员表示，她被解雇是因为论文突出了人工智能的偏见。”https://www . nytimes . com/2020/12/03/technology/Google-research-Tim nit-gebru . html

5 奥尼尔，凯茜。*数学毁灭武器:大数据如何增加不平等并威胁民主*。皇冠，2016。2022 年 5 月 28 日访问。