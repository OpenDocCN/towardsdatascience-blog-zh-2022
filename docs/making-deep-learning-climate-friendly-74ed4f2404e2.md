# 让深度学习变得气候友好

> 原文：<https://towardsdatascience.com/making-deep-learning-climate-friendly-74ed4f2404e2>

## 如何解决人工智能的碳足迹

![](img/d760952bb82125514fe531be9af1b482.png)

佩吉·安克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

深度学习正在给世界带来许多好处:解决 50 年的蛋白质折叠问题，检测癌症，改善电网。虽然深度学习推动了很多事情，但我们也需要考虑成本。

在寻求更精确和更通用的模型的过程中，网络规模和功耗出现了爆炸式增长。似乎每年模特的尺寸都在飙升。2020 年，GPT-3 发布，拥有**1750 亿**参数和 460 万美元的理论培训成本，拥有市场上价格最低的 GPU 云。

不到一年后，谷歌发布了参数超过**万亿**的 Switch Transformer。OpenAI 发现，同类最佳大型模型所需的计算能力每 3.4 个月翻一番[(六年内翻了 300，000 倍](https://openai.com/blog/ai-and-compute/))。随着参数的指数增长，能量也有了指数增长。

人工智能研究人员和工程师如何解决这个问题？

![](img/ba6bff91a5e5de80e6a6d08ff2992fab.png)

迪伦·吉利斯在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# **神经架构**

常识告诉我们，模型越大，结果越好。然而，情况并非总是如此。谷歌研究刚刚发布了一份[元分析](https://arxiv.org/abs/2110.02095?utm_campaign=The%20Batch&utm_medium=email&_hsmi=203440838&_hsenc=p2ANqtz-8qegFif9haSvA8KwypHHya-kyDgcMp7_qkYf-6icESppyp55Ih7vmMg4uWFhFU-hSj3bAnxZIrhUE8Q14hvqi0VPHg6g&utm_content=203440838&utm_source=hs_email)，展示了**更高的预训练准确度如何达到饱和点**。随着模型精度的提高，其被微调以解决特定任务的能力*(即迁移学习)*并没有同样快速地提高。在某些时候，**精度甚至会降低**。

当您开始质疑更大的模型是否总是答案时，探索改进当前架构的机会就来了。更多的努力被放在架构选择上，像 [**进化算法**这样的工具被用来为一项任务选择最佳的神经架构](https://tinyml.mit.edu/projects/spvnas/papers/spvnas_eccv.pdf)，从而大大减少了所需的计算量。

另一个引起兴趣的领域是**彩票假说**，其中神经网络的密集子网络(“彩票”)可以被训练以实现与整个网络类似的性能。以前，这只能在训练完整个网络后才能完成，但是我在这里详细讨论了这些彩票是如何更早地被发现的。

这些方法挑战了越大越好的观念。与此同时，他们跳过了训练大型模型的成本，只修剪模型的大部分。相反，他们直接跳到最佳架构，使训练模型更便宜、更快、更节能。

# **数据移动**

虽然训练大型模型消耗大量的能量，但是大部分能量都用于推理([80–90%](https://www.forbes.com/sites/moorinsights/2019/05/09/google-cloud-doubles-down-on-nvidia-gpus-for-inference/#244ad92a6792))。理想情况下，像 ResNet50 这样的模型被训练一次，然后在应用程序中使用多次。结果，大量的努力进入了推理阶段。这里最大的领域是数据传输。缩小我们的网络规模是有帮助的，但之后，无论如何数据都必须通过。这有软件和硬件方法。

![](img/b4868f80ba4500559ba4668d6302c1c1.png)

照片由[艾哈迈德·奥德赫](https://unsplash.com/@aoddeh?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

*软件*

在软件方面，**数据类型选择**是关键。虽然某些应用程序可能需要更高精度的数据类型，但在其他应用程序中，它们可能不会提高模型性能。精度较低的数据类型需要传输的数据较少，比如 16 位浮点数与 32 位浮点数。不同的公司正在创建不同的数据类型，进一步减少数据大小和处理需求，如微软的 MSFP。

经常运行的模型也可以利用**批处理。**批量发送请求通常比发送单个请求或数据包更有效。

*硬件*

在硬件方面，确保数据紧密相连非常重要。深度学习服务器和客户端之间的共享内存比来回发送更有效。在你的 iPhone 上做边缘机器学习更高效(也更安全！)比把这些数据发送到苹果的数据中心要容易得多。数据越接近，需要传输的数据就越少。

也有针对不同深度学习应用的专门芯片，如 [Eyeriss](https://ieeexplore.ieee.org/document/8686088) ，它通过可重构的片上网络最大化数据重用。

压缩也常用于最小化传输的数据，因为压缩数据比直接发送数据需要更少的资源。这对于稀疏网络来说尤其如此，在稀疏网络中，许多技巧被用来利用稀疏性。稀疏也是剪枝的一个常见动机，因为在存储和计算中 0 值的条目通常可以被忽略。

![](img/9f39ce09f637072c361ca93ad11dbfb5.png)

克林特·王茂林在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

# **数据存储**

除了数据移动，数据存储是另一个丰富的硬件节能领域。当深度学习从大脑中获取灵感时，研究人员会查看大脑的能效。大脑的效率高达深度学习的 1000 倍。不像我们数字计算机世界的 1 和 0，大脑的突触看起来更像模拟系统。随着大脑学习和遗忘，信号变得越来越弱。利用这种更加连续的模拟光谱，研究人员已经创造出更接近大脑能量效率的系统(T21)。

以类似的方式，其他形式的计算正在被探索。比如存储光子而不是电子，一次以多种状态表示数据(*量子计算*)。这些可能是更有效的存储数据的方式。

# 结束语

深度学习已经为世界做了很多好事，它有潜力做得更多。随着人工智能使用的增长，提高其效率变得至关重要。研究人员正在将神经结构和大小视为一个关键领域。与此同时，他们正在探索降低数据存储和数据传输成本的方法，**影响了大多数技术**，包括*你正在*上阅读这篇文章的设备。