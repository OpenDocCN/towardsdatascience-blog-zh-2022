<html>
<head>
<title>Machine Learning 102: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习102:逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-102-logistic-regression-9e6dc2807772#2022-01-14">https://towardsdatascience.com/machine-learning-102-logistic-regression-9e6dc2807772#2022-01-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="5e37" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">机器学习102:逻辑回归</h1></div><div class=""><h2 id="8a8a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">回到分类的基础</h2></div><p id="a3d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在之前的一篇文章中，我探索了线性回归——机器学习和数据科学中使用的所有其他高级模型的基础。线性回归模型连续因变量，如股票价格。</p><p id="aceb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在数据科学和机器学习中，我们通常也必须处理布尔或分类相关变量。典型的布尔因变量包括像债务人的贷款违约状态——完全偿还/违约，或分类猫的图片——是猫/不是猫🐈！</p><p id="b421" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于模拟布尔因变量的最基本模型之一是逻辑回归模型。在本文中，我将深入探讨以下细节:</p><ul class=""><li id="7320" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">逻辑回归模型背后的基本数学。</li><li id="c9b4" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">布尔因变量的损失函数。</li><li id="209c" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">随机梯度下降法。</li><li id="653f" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">如何使用Python创建逻辑回归模型🐍。</li></ul><p id="da06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您刚刚开始学习数据科学，或者尽管您已经有了很高的职业生涯，但只是想修改一些基本概念，请继续阅读，直到本文结束！</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/8a6a446282308865a19217cd887b7f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPKibMI4VXNrdlWk4dkoFQ.jpeg"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated"><a class="ae lb" href="https://unsplash.com/s/photos/skye%2C-united-kingdom" rel="noopener ugc nofollow" target="_blank">英国斯凯岛</a>。由<a class="ae lb" href="https://unsplash.com/@robertlukeman?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">罗伯特·卢克曼</a>在<a class="ae lb" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h1 id="ff61" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">布尔因变量、概率和赔率</h1><p id="1c84" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在本节中，我们将探索逻辑回归背后的数学，从机器学习中最基本的模型开始— <a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-101-linear-regression-72ba6c04fbf1">线性回归</a>。</p><p id="9339" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在线性回归中，因变量<em class="nd"> d </em>连续无界，与解释变量<em class="nd">m</em>g<em class="nd">₁、<em class="nd"> g </em> ₂、… <em class="nd"> gₘ </em>成线性关系:</em></p><p id="ce52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">d</em>=<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ，</em></p><p id="e959" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd"> c </em> ₁、<em class="nd"> c </em> ₂、… <em class="nd"> cₘ </em>为与解释变量相关联的<em class="nd"> m </em>模型参数。</p><p id="e271" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在二进制分类问题中，因变量<em class="nd"> d </em>是布尔型的:<em class="nd"> d </em>只有0或1的值。由于上述线性回归方程为<em class="nd"> d </em>返回连续且无界的值，因此不能直接用于模拟布尔因变量。</p><p id="fa6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以尝试将<em class="nd"> d </em>的连续输出视为一种概率。然而，我们不能有负概率或大于1的概率！因此，<em class="nd"> d </em>不能作为概率来对待。</p><p id="fe46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许我们可以试着用<a class="ae lb" href="https://en.wikipedia.org/wiki/Odds" rel="noopener ugc nofollow" target="_blank">几率</a>来处理它，而不是把<em class="nd"> d </em>当作一种概率。赔率通过以下关系与概率密切相关:</p><p id="676c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd"> O </em> = <em class="nd"> P </em> / (1 - <em class="nd"> P </em>)，</p><p id="59ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd"> O </em>是赔率，<em class="nd"> P </em>是概率。</p><p id="88b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当概率<em class="nd"> P </em>的范围为[0，1]时，赔率<em class="nd"> O </em>的范围为[0，∞)使得其对于正值是无界的。我们仍然需要考虑负值。为此，我们取几率的对数O 得到几率的对数l :</p><p id="2374" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd"> l </em> = log( <em class="nd"> O </em>)，</p><p id="22db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd"> l </em>是连续的，有值域(-∞，∞)，并且没有任何一般性损失，log是以<em class="nd"> e </em>为底的对数。</p><p id="839e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这正是我们需要的解决方案，以便使用上面的线性回归方程，使用赔率和概率对布尔因变量进行建模！我们需要做的就是将<em class="nd"> d </em>视为对数比<em class="nd"> l </em>:</p><p id="e686" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">l</em>= log(<em class="nd">o</em>)= log(<em class="nd">p</em>/(1-<em class="nd">p</em>)=<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ.</em></p><p id="2940" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以重新排列这个等式，用P代替:</p><p id="b5d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">p</em>/(1-<em class="nd">p</em>)= exp(<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ</em>)<br/><em class="nd">p</em>= 1/(1+exp(-(<em class="nd">c</em>₁</p><p id="ebd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以更简洁地写成:</p><p id="5860" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">p</em>= 1/(1+exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>)，</p><p id="4373" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> m </strong>是长度为<em class="nd"> m </em>的向量，包含模型参数:<strong class="kh ir">m</strong>=<strong class="kh ir"/>[<em class="nd">c</em>₁，<em class="nd"> c </em> ₂，… <em class="nd"> cₘ </em> ]ᵀ，<strong class="kh ir"> g </strong>也是长度为<em class="nd"> m </em>的向量，包含解释变量:<strong class="kh ir"> g该方程也被称为<a class="ae lb" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">逻辑函数</a>，因此被称为“逻辑回归”！</strong></p><p id="94b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归模型<em class="nd">d</em>=<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong><em class="nd"/>已经转化为逻辑回归模型<em class="nd">p</em>= 1/(1+exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>))，将概率<em class="nd"> P </em>建模为<strong class="kh ir"> m </strong>和<strong class="kh ir"> g </strong>的非线性函数！请注意，原始布尔因变量<em class="nd"> d </em>没有出现在逻辑回归模型中——我们只处理概率！<em class="nd">d</em>’稍后将再次出现在损失函数中。</p><p id="ceb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等式<em class="nd">p</em>= 1/(1+exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>))可能很难理解，所以让我们用图表来看看最简单的可能情况。最简单的情况，<em class="nd">P</em>= 1/(1+exp(-<em class="nd">g</em>))只有1个解释变量<em class="nd"> g </em>如下图所示。当<em class="nd"> g </em>增大时，<em class="nd"> P </em>趋于1，当<em class="nd"> g </em>减小时，<em class="nd"> P </em>趋于0。图表中还包括一些合成布尔数据点，用于<em class="nd"> d </em> ' = 0和<em class="nd"> d </em> ' = 1。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ne"><img src="../Images/9d17cfb62cb8bdddc652cf43c3d48d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L802xF3dcDldoN_vqw6vRA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">蓝色表示<em class="nf"> P </em> = 1 / (1 + exp(- <em class="nf"> g </em>))，橙色点表示d' = 0，绿色点表示d' = 1。作者创造的形象。</p></figure><p id="9172" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当<em class="nd"> d </em> ' = 1时，<em class="nd"> P </em>理想情况下应具有接近1的对应值，而当<em class="nd"> d </em> ' = 0时，<em class="nd"> P </em>应具有接近0的对应值。由逻辑回归模型定义的分类问题被简化为寻找一组模型参数<strong class="kh ir">m</strong>=<strong class="kh ir"/>[<em class="nd">c</em>₁、<em class="nd"> c </em> ₂、… <em class="nd"> cₘ </em> ]ᵀ <em class="nd"> </em>，这导致了这里描述的行为。</p><h1 id="4a47" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">布尔因变量的向量</h1><p id="f9a9" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在上一节中，我们算出了单布尔因变量<em class="nd"> d </em>的逻辑回归方程。在现实生活中，我们更可能需要处理一个布尔因变量的向量，它对应于一组不同的测量值:<strong class="kh ir"> d </strong> ' = [ <em class="nd"> d' </em> ₁，<em class="nd"> d' </em> ₂，… <em class="nd"> d'ₙ </em> ]ᵀ.</p><p id="ca85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，对应于<strong class="kh ir"> d </strong>中第<em class="nd"> u </em>个元素的概率由逻辑回归模型用一组对应的<em class="nd"> m </em>解释变量<em class="nd"> Gᵤ </em> ₁ <em class="nd">、Gᵤ </em> ₂、… <em class="nd"> Gᵤₘ </em>来建模:</p><p id="e2e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">pᵤ</em>= 1/(1+exp(-(<em class="nd">gᵤ</em>₁<em class="nd">c</em>₁<em class="nd">+gᵤ</em>₂<em class="nd">c</em>₂+…+<em class="nd">gᵤₘcₘ</em>))。</p><p id="2aac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，<em class="nd"> Pᵤ </em>只是向量中的第<em class="nd"> u </em>个元素:<strong class="kh ir"> P </strong> = [ <em class="nd"> P </em> ₁，<em class="nd"> P </em> ₂，… <em class="nd"> Pₙ </em> ]ᵀ.对于<strong class="kh ir"> P </strong>,该等式也可以向量形式写成:</p><p id="c5c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">P</strong>= 1/(1+exp(-<strong class="kh ir">Gm</strong>))，</p><p id="30d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> G </strong>是一个大小为<em class="nd">n</em>×m的矩阵，结构如下:</p><p id="0883" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">g</strong>=【<em class="nd">g</em>₁₁、<em class="nd"> G </em> ₁₂、… <em class="nd"> G </em> ₁ <em class="nd"> ₘ </em>、<br/>。…….[ <em class="nd"> G </em> ₂₁、<em class="nd"> G </em> ₂₂、… <em class="nd"> G </em> ₂ <em class="nd"> ₘ </em>、<br/>。…….… <br/>。…….[ <em class="nd"> Gₙ </em> ₁，<em class="nd"> Gₙ </em> ₂，… <em class="nd"> Gₙₘ </em> ]]，</p><p id="68da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd">u</em>g<strong class="kh ir">g</strong>第一行包含<em class="nd"> m </em>解释变量的向量<em class="nd"> Gᵤ </em> ₁ <em class="nd">、Gᵤ </em> ₂、… <em class="nd"> Gᵤₘ </em>、<strong class="kh ir"> </strong>和<strong class="kh ir">m</strong>=<strong class="kh ir"/>[<em class="nd">c</em>₁、<em class="nd"/></p><h1 id="ba1c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">最小化二元交叉熵损失函数</h1><p id="be7f" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">为了量化我们的逻辑回归模型的表现，我们需要最小化模型的概率预测<em class="nd"> P </em>和关于模型参数<strong class="kh ir"> m </strong>的布尔基础真值<em class="nd"> d </em>之间的误差。</p><p id="6170" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于量化分类问题中的预测误差的最常用损失函数之一是<a class="ae lb" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">二元交叉熵</a>。对于第<em class="nd"> u </em>个数据点，二元交叉熵为:</p><p id="d620" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)=-<em class="nd">d</em>'<em class="nd">ᵤ</em>原木(<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)-(1-<em class="nd">d</em>'<em class="nd">ᵤ</em>)原木(1 - <em class="nd"> Pᵤ </em> ( <strong class="kh ir"> m </strong>)。</p><p id="15d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd"> Lᵤ </em> ( <strong class="kh ir"> m </strong>)中的第一项仅在<em class="nd"> d'ᵤ </em> = 1时有效，第二项仅在<em class="nd"> d </em> ' <em class="nd"> ᵤ </em> = 0时有效。要获得所有<em class="nd"> n </em>个数据点的损失，我们只需求和:</p><p id="5446" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">l</em>(<strong class="kh ir">m</strong>)=σ<em class="nd">ᵤlᵤ</em>(<strong class="kh ir">m</strong>)。</p><p id="ebfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们希望最小化关于模型参数<strong class="kh ir"> m </strong>的损失函数<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>)。为此，我们将<em class="nd"> Lᵤ </em> ( <strong class="kh ir"> m </strong>)相对于<strong class="kh ir"> m </strong>进行区分:</p><p id="5254" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>=∂/∂<strong class="kh ir">m</strong>(-<em class="nd">d</em>'<em class="nd">ᵤ</em>原木(<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)-(1-<em class="nd">d</em>'<em class="nd">ᵤ</em>)原木(1 - <em class="nd"> Pᵤ </em> ( <strong class="kh ir"> m </strong>))</p><p id="b35c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个导数看起来令人生畏，但是我们可以使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Product_rule" rel="noopener ugc nofollow" target="_blank">乘积法则</a>将∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>分解成更简单的成分:</p><p id="bbc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>=∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>∂<em class="nd">pǐ</em>(<strong class="kh ir">m</strong>)/∩<strong class="kh ir">m</strong>。</p><p id="0d10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">术语∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)可以评估为:</p><p id="964f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)<em class="nd">=-</em><em class="nd">d</em>'<em class="nd">ᵤ</em>/<em class="nd">pᵤ</em>+(1-<em class="nd">d</em>'<em class="nd">ᵤ</em>)/(1</p><p id="d615" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而∂<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>可以由<em class="nd">p</em>= 1/(1+exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>))计算为:</p><p id="3313" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∂<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>=-1/(1+exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>ᵤ)(-<strong class="kh ir">g</strong>t118】ᵤ<strong class="kh ir">t121】exp(-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong></strong></p><p id="237a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir">g</strong>t140】ᵤ=[<em class="nd">gₙ</em>₁、<em class="nd"> Gₙ </em> ₂、… <em class="nd"> Gₙₘ </em> ]ᵀ是包含矩阵<strong class="kh ir"> G </strong>的第<em class="nd"> u </em>行的向量。</p><p id="33f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用这些结果，导数∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>现在可以被评估为:</p><p id="5242" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>=(<em class="nd">pᵤ-</em><em class="nd">d</em>'<em class="nd">ᵤ</em>)/(<em class="nd">pᵤ</em>(1-<em class="nd">pᵤ</em>)<strong class="kh ir">g</strong><em class="nd">ᵤ</em><em class="nd">p</em>(1-<em class="nd">p</em>)</p><p id="ad5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理想情况下，我们将设置∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>= 0，并求解最佳模型参数<strong class="kh ir"> m </strong>。不幸的是，我们无法求解出<strong class="kh ir"> m </strong>，因为它们没有出现在∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>的表达式中！因此，看起来我们可能需要求助于数值方法来最小化相对于<strong class="kh ir"> m </strong>的<em class="nd"> Lᵤ </em> ( <strong class="kh ir"> m </strong>)。</p><h1 id="6299" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">随机梯度下降</h1><p id="1e59" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">可以使用各种数值方法，例如<a class="ae lb" href="https://en.wikipedia.org/wiki/Newton's_method" rel="noopener ugc nofollow" target="_blank">牛顿法</a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降法</a>来最小化逻辑回归模型的损失函数。对于本文，我们将探索随机梯度下降法，其变体仍在更高级的模型中积极使用，如神经网络或<a class="ae lb" href="https://arxiv.org/pdf/1602.05629.pdf" rel="noopener ugc nofollow" target="_blank">联邦学习框架</a>。</p><p id="49bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随机梯度下降是一种一阶迭代优化算法，用于通过沿最陡负梯度方向迭代移动来搜索可微函数的局部最小值。如下图所示，从绿点开始，如果我们沿着蓝色曲线朝着最陡负梯度的方向继续在<em class="nd"> x </em>中小步前进，最终应该会到达红点——最小点。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/f0b29c5e5040d144ab65150ae92199ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqIUYGjsxMYlVKP-ja8R4g.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">从绿点到红点向最陡的负梯度方向移动。作者创造的形象。</p></figure><p id="2264" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住，我们希望相对于<strong class="kh ir"> m </strong>最小化<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>)。利用随机梯度下降算法，我们沿着最陡负梯度的方向在<strong class="kh ir"> m </strong>中采取多个小步骤:-∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>向<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>)的最小值点前进。</p><p id="5faa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们使用∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m</strong>对<strong class="kh ir"> d </strong> ' = [ <em class="nd"> d' </em> ₁、<em class="nd"> d' </em> ₂、… <em class="nd"> d'ₙ </em> ]ᵀ:中的所有<em class="nd"> n </em>元素迭代更新模型参数<strong class="kh ir"> m </strong></p><p id="40bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">m</strong>:=<strong class="kh ir">m</strong>-<em class="nd">r</em>∂<em class="nd">lᵤ</em>(<strong class="kh ir">m</strong>)/∂<strong class="kh ir">m，</strong></p><p id="4ade" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd"> R </em>是某个学习率，它控制我们所走的步数的大小。</p><p id="feec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">学习率的大小非常重要:如果<em class="nd"> R </em>太小，步长几乎不会改变<strong class="kh ir"> m </strong>的值，另一方面，如果<em class="nd"> R </em>太大，那么我们可能会超过最小点！为<em class="nd"> R </em>选择一个好的值取决于所涉及的数据。</p><p id="7e87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随机梯度下降算法可以用伪代码编写:</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="03ac" class="nm mh iq ni b gy nn no l np nq"># Initialize the model parameters with some initial value.<br/>m = initialize_m()</span><span id="039f" class="nm mh iq ni b gy nr no l np nq"># Iterate the algorithm until some termination condition is reached.<br/>while termination_condition == False:<br/>    # Shuffle the rows the training data.<br/>    shuffle_training_data()</span><span id="29d7" class="nm mh iq ni b gy nr no l np nq">    # Iterate through all the n elements in the data.<br/>    for i in range(n):<br/>        # Update the model parameters m using the derivative of the <br/>        # loss function for that element.<br/>        m = m - R * dLdm[i]</span><span id="4983" class="nm mh iq ni b gy nr no l np nq">    # Check if the termination condition is reached.<br/>    check_termination_condition()</span></pre><h1 id="ed3b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">概述</h1><p id="8e70" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我们已经讨论了很多细节，所以在继续用Python编码之前，让我们回顾一下我们已经讨论过的内容！</p><ol class=""><li id="ccfe" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la ns li lj lk bi translated">我们希望对布尔因变量<em class="nd"> d </em> ' ∈ {0，1}进行建模。</li><li id="aa9e" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ns li lj lk bi translated">我们没有直接对布尔值建模，而是使用:<em class="nd">p</em>= 1/(1+exp(-(<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ</em>))对概率建模。</li><li id="fb1f" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ns li lj lk bi translated">我们用二元交叉熵来衡量模型的预测误差:<br/><em class="nd"/>(<strong class="kh ir">m</strong>)=-<em class="nd">d</em>'<em class="nd">ᵤ</em>log(<em class="nd">pᵤ</em>(<strong class="kh ir">m</strong>)-(1-<em class="nd">d</em>'<em class="nd">ᵤ</em>)log(1-【t74</li><li id="5cc3" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ns li lj lk bi translated">为了找到最佳的模型参数<strong class="kh ir"> m </strong>，我们相对于<strong class="kh ir"> m </strong>最小化<em class="nd"> Lᵤ </em> ( <strong class="kh ir"> m </strong>)。不幸的是，这个最小化问题必须使用数值方法来解决，例如随机梯度下降。</li></ol><h1 id="5a63" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使用Python进行逻辑回归</h1><p id="876d" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我们现在已经做了足够的数学！在本节中，我们将使用Python创建一个逻辑回归模型求解器！</p><p id="3f5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们定义实现逻辑回归模型的<code class="fe nt nu nv ni b">logistic_regression</code>:</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="2961" class="nm mh iq ni b gy nn no l np nq">import numpy as np</span><span id="cd3e" class="nm mh iq ni b gy nr no l np nq">def logistic_regression(G, m):<br/>    """<br/>    Logistic regression model.</span><span id="5aca" class="nm mh iq ni b gy nr no l np nq">    Inputs<br/>    G: np.array of shape nxm containing the explanatory variables.<br/>    m: np.array of length m containing the model parameters.</span><span id="eb0e" class="nm mh iq ni b gy nr no l np nq">    Returns<br/>    P: np.array of length n containing the modeled probabilities.<br/>    """<br/>    return 1 / (1 + np.exp(-np.dot(G, m)))</span></pre><p id="1a5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来我们定义<code class="fe nt nu nv ni b">binary_cross_entropy</code>和<code class="fe nt nu nv ni b">binary_cross_entropy_grad</code>，它们计算二元交叉熵及其梯度。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="a051" class="nm mh iq ni b gy nn no l np nq">def binary_cross_entropy(d, P):<br/>    """<br/>    Calculates the mean binary cross entropy for all n data points.</span><span id="d725" class="nm mh iq ni b gy nr no l np nq">    Inputs<br/>    d: np.array of length n containing the boolean dependent <br/>       variables.<br/>    P: np.array of length n containing the model's probability <br/>       predictions.</span><span id="e2fe" class="nm mh iq ni b gy nr no l np nq">    Returns<br/>    bce: float containing the mean binary cross entropy.<br/>    """<br/>    <br/>    # For d = 1:<br/>    d_1 = d[d == 1] * np.log(P[d == 1])<br/>    # For d = 0:<br/>    d_0 = (1 - d[d == 0]) * np.log(1 - P[d == 0])<br/>    return -np.mean(d_1) - np.mean(d_0)</span><span id="5ab7" class="nm mh iq ni b gy nr no l np nq">def binary_cross_entropy_grad(g, d, p):<br/>    """<br/>    Calculates the gradient of the binary cross entropy loss for a <br/>    single data point.</span><span id="43ab" class="nm mh iq ni b gy nr no l np nq">    Inputs:<br/>    g: np.array of length m containing the vector of explanatory <br/>       variables for 1 data point.<br/>    d: integer containing the boolean dependent variable for 1 data <br/>       point.<br/>    p: float containing the model's probability predictions for 1 <br/>       data point.</span><span id="76e3" class="nm mh iq ni b gy nr no l np nq">    Returns:<br/>    bce_grad: np.array of length m containing the gradients of the <br/>              binary cross entropy.<br/>    """<br/>    return g * (p - d)</span></pre><p id="013d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们定义<code class="fe nt nu nv ni b">SGD</code>，它执行随机梯度下降算法，使用所有数据点逐一迭代更新模型参数。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="180b" class="nm mh iq ni b gy nn no l np nq">def SGD(d, G, m, R = 0.01):<br/>    """<br/>    Stochastic gradient descent. Updates the model parameters m <br/>    iteratively using all individual data points in d.<br/>    <br/>    Inputs<br/>    d: np.array of length n containing the boolean dependent <br/>       variables.<br/>    G: np.array of shape nxm containing the explanatory variables.<br/>    m: np.array of length m containing the initial model parameters.<br/>    R: float containing the SGD learning rate.<br/>    <br/>    Returns<br/>    m: np.array of length m containing the updated model parameters.<br/>    """<br/>    # Shuffle the data points.<br/>    indices = np.arange(len(d))<br/>    np.random.shuffle(indices)<br/>    d = d[indices]<br/>    G = G[indices]</span><span id="4ef1" class="nm mh iq ni b gy nr no l np nq">    # Iteratively update the model parameters using every single<br/>    # data point one by one.<br/>    for i in range(len(d)):<br/>        P = logistic_regression(G[i], m)<br/>        m = m - R * binary_cross_entropy_grad(G[i], d[i], P)</span><span id="cd7a" class="nm mh iq ni b gy nr no l np nq">    return m</span></pre><p id="ee34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们只需要一些数据来测试我们的逻辑回归求解器！我们使用从<code class="fe nt nu nv ni b">sklearn</code>获得的鸢尾花分类数据集。原始数据集提供了3个不同的类:0、1和2。我们将数据集仅限于0和1的类。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="3ea2" class="nm mh iq ni b gy nn no l np nq">from sklearn.datasets import load_iris</span><span id="6ca6" class="nm mh iq ni b gy nr no l np nq"># Load the iris flower dataset from sklearn.<br/>data = load_iris()<br/>G = data["data"]   # Explanatory variable matrix.<br/>d = data["target"] # Dependent variable array.</span><span id="2668" class="nm mh iq ni b gy nr no l np nq"># The dataset has 3 classes. For the time being, restrict the data<br/># to only classes 0 and 1 in order to create boolean dependent<br/># variables.                  <br/>want = (d == 0) | (d == 1)</span><span id="c0e8" class="nm mh iq ni b gy nr no l np nq">G = G[want]<br/>d = d[want]</span></pre><p id="5d1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们的逻辑回归模型</p><p id="6d85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">p</em>= 1/(1+exp(-(<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ</em>))</p><p id="2bfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有明确包括偏差项，我们需要在矩阵<strong class="kh ir"> G </strong>的第一列添加一列1。这列1将扮演偏置项的角色。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="a018" class="nm mh iq ni b gy nn no l np nq"># Because we do not explicitly account for a bias term in our<br/># logistic regression model, we need to add a column of 1s to the<br/># matrix G. This will play the term of the bias term.<br/>G = np.hstack([np.ones([len(G), 1]), G])</span><span id="3e34" class="nm mh iq ni b gy nr no l np nq"># Take a look at the first 3 data points in the dataset.<br/>for i in range(3):<br/>    print(G[i], d[i])<br/></span><span id="b59e" class="nm mh iq ni b gy nr no l np nq">[1.  5.1 3.5 1.4 0.2] 0<br/>[1.  4.9 3.  1.4 0.2] 0<br/>[1.  4.7 3.2 1.3 0.2] 0</span></pre><p id="b8db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经准备好了数据结构<strong class="kh ir"> G </strong>和<strong class="kh ir"> d </strong>，是时候对模型参数<strong class="kh ir"> m </strong>进行初步猜测了！由于每个数据点有5个解释变量，因此在<strong class="kh ir"> m </strong>中将有5个模型参数。我们猜测模型参数的值为1。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="4230" class="nm mh iq ni b gy nn no l np nq">m = np.ones(5)</span></pre><p id="b122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们运行随机梯度下降解算器20次迭代，使用学习率<em class="nd"> R </em> = 0.01来防止算法超过最小点。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="fb45" class="nm mh iq ni b gy nn no l np nq"># Run the SGD algorithm 20 times.<br/>for i in range(20):<br/>    # Use a learning rate of 0.01.<br/>    m = SGD(d, G, m, 0.01)</span><span id="55da" class="nm mh iq ni b gy nr no l np nq">    # Calculate the mean loss for all data in the dataset.<br/>    P = logistic_regression(G, m)<br/>    bce = binary_cross_entropy(d, P)</span><span id="29da" class="nm mh iq ni b gy nr no l np nq">    print("Step {:3d} : m = [ ".format(i+1), end = "")<br/>    for j in range(len(m)):<br/>        print("{:6.3f}".format(m[j]), end = " ")<br/>    print("], loss = {:.3f}.".format(bce))<br/></span><span id="bcd1" class="nm mh iq ni b gy nr no l np nq">Step   1 : m = [  0.692 -0.506 -0.080  0.640  0.961 ], loss = 0.548. Step   2 : m = [  0.674 -0.517 -0.207  0.854  1.050 ], loss = 0.417. Step   3 : m = [  0.643 -0.609 -0.355  0.985  1.110 ], loss = 0.322. Step   4 : m = [  0.629 -0.620 -0.440  1.124  1.168 ], loss = 0.258. Step   5 : m = [  0.616 -0.638 -0.514  1.234  1.214 ], loss = 0.218. Step   6 : m = [  0.608 -0.635 -0.568  1.337  1.257 ], loss = 0.187. Step   7 : m = [  0.598 -0.652 -0.626  1.417  1.291 ], loss = 0.165. Step   8 : m = [  0.592 -0.652 -0.668  1.495  1.324 ], loss = 0.148. Step   9 : m = [  0.579 -0.691 -0.726  1.545  1.348 ], loss = 0.135. Step  10 : m = [  0.573 -0.699 -0.765  1.605  1.373 ], loss = 0.123. Step  11 : m = [  0.568 -0.697 -0.796  1.665  1.398 ], loss = 0.113. Step  12 : m = [  0.563 -0.702 -0.828  1.716  1.420 ], loss = 0.104. Step  13 : m = [  0.556 -0.716 -0.862  1.759  1.439 ], loss = 0.097. Step  14 : m = [  0.551 -0.723 -0.891  1.802  1.457 ], loss = 0.091. Step  15 : m = [  0.547 -0.729 -0.918  1.843  1.475 ], loss = 0.086. Step  16 : m = [  0.542 -0.736 -0.944  1.880  1.491 ], loss = 0.081. Step  17 : m = [  0.538 -0.741 -0.968  1.916  1.506 ], loss = 0.077. Step  18 : m = [  0.535 -0.741 -0.987  1.953  1.522 ], loss = 0.073. Step  19 : m = [  0.531 -0.748 -1.009  1.985  1.536 ], loss = 0.069. Step  20 : m = [  0.528 -0.748 -1.027  2.018  1.549 ], loss = 0.066.</span></pre><p id="0c25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的结果来看，平均二进制交叉熵似乎一直在下降，这意味着该模型应该朝着一组令人满意的参数收敛，所以接下来让我们检查该模型的准确性。由于这是一个小数据集，我们将简单地直接打印所有<em class="nd"> n </em>个数据点的结果，并直观地检查它们。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="0f06" class="nm mh iq ni b gy nn no l np nq"># Print the ground truth.<br/>print(d)</span><span id="dddc" class="nm mh iq ni b gy nr no l np nq"># Print the modeled probabilities of the logistic regression model, <br/># rounded to the nearest integer value.<br/>print(np.round(logistic_regression(G, m)).astype(int))<br/></span><span id="b761" class="nm mh iq ni b gy nr no l np nq">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</span><span id="f6b1" class="nm mh iq ni b gy nr no l np nq">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</span></pre><p id="d25d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">似乎我们的模型已经完美地分类了数据集中的每一个数据点！接下来让我们仔细看看模型的实际非舍入输出，包括<em class="nd"> d </em> ' = 1和<em class="nd"> d </em> ' = 0两种情况。</p><pre class="lr ls lt lu gt nh ni nj nk aw nl bi"><span id="de08" class="nm mh iq ni b gy nn no l np nq"># Get the probability predictions from the model.<br/>P = logistic_regression(G, m)</span><span id="aad1" class="nm mh iq ni b gy nr no l np nq"># d' = 0<br/>print("d' = {}, P = {:.3f}.".format(d[0], P[0]))<br/># d' = 1<br/>print("d' = {}, P = {:.3f}.".format(d[-1], P[-1]))<br/></span><span id="1867" class="nm mh iq ni b gy nr no l np nq">d' = 0, P = 0.023. <br/>d' = 1, P = 0.975.</span></pre><p id="0698" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到，当<em class="nd"> d </em> ' = 1时，<em class="nd"> P </em>非常接近1，当<em class="nd"> d </em> ' = 0时，<em class="nd"> P </em>非常接近0——这正是我们预期的行为！</p><h1 id="6032" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">提醒一句</h1><p id="10bd" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在上面的例子中，使用的数据是非常容易分离的。也就是说，<em class="nd"> d </em> ' = 1和<em class="nd"> d </em> ' = 0之间的解释变量存在明显差异。这就是我们如何在预测中获得完美结果的。</p><p id="aeee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实际的数据科学项目中，数据通常没有这么干净——我们需要<a class="ae lb" href="https://en.wikipedia.org/wiki/Feature_engineering" rel="noopener ugc nofollow" target="_blank">设计数据</a>中的特征，考虑<strong class="kh ir"> d </strong>中0和1的数量之间的比例，以及<a class="ae lb" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">微调模型训练过程</a>。根据数据，其他损失函数，如<a class="ae lb" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">聚焦损失</a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化损失函数</a>可能比二元交叉熵更好。</p><p id="b3b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们使用随机梯度下降来最小化损失函数。随机梯度下降并不总是适用于所有数据集，在其他数据集中，其他更高级的优化算法，如<a class="ae lb" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank">有限内存BFGS </a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" rel="noopener ugc nofollow" target="_blank"> Adam </a>可能适用。</p><p id="4367" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">指标也是另一个需要考虑的重要因素。在上面的例子中，我们简单地比较了模型的输出。在现实生活中，对于包含数千个数据点的数据集，这种方法显然是行不通的！使用哪种指标将取决于所涉及的数据。分类中使用的常用指标包括<a class="ae lb" href="https://en.wikipedia.org/wiki/Accuracy_and_precision" rel="noopener ugc nofollow" target="_blank">准确度</a>、<a class="ae lb" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank">接收器工作特性</a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精确度和召回率</a>。</p><p id="d00f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们在用于训练模型的数据上测试了我们的模型——实际上，整个数据集应该分成训练和验证数据集！该模型应在训练数据集上进行训练，然后在验证数据集上进行测试-这是为了确保该模型不只是记住了训练数据集，而且能够将其预测推广到以前没有见过的数据！</p><p id="27a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据科学和机器学习是极其深入和广阔的领域！一旦你完全理解了基本原理是如何工作的，那么就开始花些时间探索更高级的话题吧！</p><h1 id="b468" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">摘要</h1><p id="d000" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在本文中，我们探索了逻辑回归模型背后的数学，并使用Python创建了一个基于随机梯度下降的求解器！今天探索的概念，如使用数值方法最小化损失函数可能是基本的，但适用于所有其他高级模型，如深度学习图像分类或分割模型！我希望您能够更好地理解分类模型今天是如何工作的。一如既往的感谢您的阅读，下篇文章再见！</p><h1 id="2f95" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="bc2f" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">[1] W. M. Menke (2012)，<em class="nd">地球物理数据分析:离散逆理论MATLAB版</em>，Elsevier。<br/>【2】艾伦·b·唐尼(2014)，<em class="nd">Think Stats Python中的探索性数据分析</em>，绿茶出版社。</p></div></div>    
</body>
</html>