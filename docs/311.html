<html>
<head>
<title>Recursive Feature Selection: Addition or Elimination?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归特征选择:增加还是消除？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recursive-feature-selection-addition-or-elimination-755e5d86a791#2022-01-11">https://towardsdatascience.com/recursive-feature-selection-addition-or-elimination-755e5d86a791#2022-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="6a53" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">递归特征选择:增加还是消除？</h1></div><div class=""><h2 id="b24e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种执行详尽特征选择的聪明方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7596e8ea9a417f04aecaf885da5d4041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yY6JpOD83Hezc123"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@frostroomhead?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rodion Kutsaev </a>拍照</p></figure><p id="58b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当建立一个机器学习模型时，拥有更多的特征比拥有更少的特征更重要。但是只使用你需要的比全部使用更有用。因此，在第一次建模试验之后，我们需要一个半自动的过程，可以只过滤对我们的监督任务有用的特征。</p><p id="d43d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只是指特征选择过程。它包括从原始数据集中仅选择变量子集的所有算法，以用作我们预测模型的输入。有各种技术来执行特征选择，并且每种方法都有各种利弊要考虑。</p><p id="4afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在生产简单和可维护的管线时，少量特征是有效的；提高泛化能力；降低可能的存储空间或成本；减少推断时间(无论最终预测值是多少)或提供更好的可解释结果。</strong></p><p id="6777" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时，仅产生有用的预测特征的子集可能是昂贵的、困难的和不明确的。一个好的特征选择算法应该尝试所有可能的特征组合，并根据我们的验证策略标注哪一个导致性能提高。由于时间的限制，数据科学家不鼓励采用大型数据集的特征选择方法。此外，所选特征取决于所用的模型和参数。因此，简单地改变其中一个可能会以一种不好的方式改变最终结果。对于在我们的机器学习工作流程中在哪里放置特征选择以获得最佳效果，也可能会产生疑问。</p><p id="dbfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了应对这些挑战，我们发布了<a class="ae ky" href="https://github.com/cerlymarco/shap-hypetune" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">shap-hype tune</strong></a>:<em class="lv">一个用于同步超参数调整和特性选择的python包。</em>其目的是在单个流水线中优化最佳数量的特征，同时搜索梯度增强模型的最佳参数配置。它提供了各种参数搜索方法(网格、随机或贝叶斯搜索)和特征选择策略(递归特征消除/添加和Boruta)以及使用SHAP值提高泛化能力的<strong class="lb iu">。</strong></p><p id="1631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们关注递归特征选择算法的优点。<strong class="lb iu">递归特征选择能够搜索可靠的特征子集，同时提高性能并保持可接受的计算成本</strong>。因此，它具备成为现实应用中最有效的特征过滤方法之一的所有先决条件。我们旨在探索不太为人所知的加法方法(递归特征添加)，在标准分类问题中将其与最著名的减法方法(递归特征消除)进行比较。</p><h1 id="c5f6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">递归特征添加</h1><p id="8a44" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">递归要素添加按照递归添加过程选择要素。工作流程如下图所示:</p><ol class=""><li id="d0d3" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">使用所有可用的特征来拟合估计量(在我们的例子中是梯度增强)。</li><li id="9b84" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">提取要素重要性排名(标准的基于树的重要性或SHAP重要性有效)。</li><li id="0472" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">根据功能的贡献对其进行排序。</li><li id="bcc3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">仅使用最相关的特征来拟合估计量，并根据验证数据计算性能。</li><li id="7480" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">包括下一个最重要的特征，并适合新的估计器。</li><li id="f760" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">在步骤5和6中计算模型之间的性能差异。</li><li id="0b93" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">如果性能提高，该特征被认为是有价值的预测器。</li><li id="78e6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">重复第5步到第7步，直到所有特性都被考虑在内。</li></ol><p id="9fe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用<strong class="lb iu"> shap-hypetune </strong>执行递归特征添加是简单明了的。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c3e8" class="nm lx it ni b gy nn no l np nq">rfa = <strong class="ni iu">BoostRFA</strong>(<br/>    LGBMClassifier(), <br/>    step=3, min_features_to_select=1<br/>)<br/>rfa.fit(X_train, y_train, eval_set=[(X_val, y_val)])</span></pre><p id="f6c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我们简单地使用RFA和一个<em class="lv"> LGBMClassifier </em>。定制有很多。例如，我们可以使用具有SHAP特性重要性的<em class="lv"> BoostRFA </em>实例(而不是传统的基于树的实例),或者在搜索最佳参数配置时使用。</p><p id="9795" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的分类任务，我们使用<em class="lv"> BoostRFA </em>，连同<em class="lv"> LGBMClassifier </em>，计算一个贝叶斯参数搜索。结果报告如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/4b254ee9627175a4a95680aa41133662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Hlwp7YcYtu2S5Z-Cm8VTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">折叠外样本的交叉验证结果(图片由作者提供)</p></figure><h1 id="349d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">递归特征消除</h1><p id="a737" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">递归要素消除按照递归消除过程选择要素。工作流程如下图所示:</p><ol class=""><li id="9a6d" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">使用所有可用的特征来拟合估计量(在我们的例子中是梯度增强)。</li><li id="663b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">提取要素重要性排名(标准的基于树的重要性或SHAP重要性有效)。</li><li id="92bb" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">根据功能的贡献对其进行排序。</li><li id="8f6b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">排除最不重要的特征并拟合新的估计量。</li><li id="0ac7" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">在连续迭代中，计算步骤4中模型之间的性能差异。</li><li id="024b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">如果性能提高，该特征被释放。</li><li id="5c66" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">重复第4步到第7步，直到所有特性都被考虑在内。</li></ol><p id="38df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至于RFA的情况，用<strong class="lb iu"> shap-hypetune </strong>执行递归特征添加是简单明了的。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="3418" class="nm lx it ni b gy nn no l np nq">rfe = <strong class="ni iu">BoostRFE</strong>(<br/>    LGBMClassifier(), <br/>    step=3, min_features_to_select=1<br/>)<br/>rfe.fit(X_train, y_train, eval_set=[(X_val, y_val)])</span></pre><p id="3bdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我们简单地使用了带有<em class="lv"> LGBMClassifier </em>的RFE。定制有很多。例如，我们可以使用具有SHAP特性重要性的<em class="lv"> BoostRFE </em>实例(而不是传统的基于树的实例),或者在搜索最佳参数配置时使用。</p><p id="95f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的分类任务，我们使用<em class="lv"> BoostRFE </em>，连同<em class="lv"> LGBMClassifier </em>，计算贝叶斯参数搜索。结果报告如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/5acb3cdf624f7a357bbfe86a23b8f670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhoPa2E5f7jN3wRslmLyxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">折叠外样本的交叉验证结果(图片由作者提供)</p></figure><h1 id="fff0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="5a8d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">RFA和RFE在我们的模拟分类任务中都显示出惊人的结果。它们还表现出良好的过滤能力。从交叉验证的结果来看，选择最多的是信息特征，其次是线性组合特征(冗余)和噪声特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/a0a0c9754214bc9d15e8c9506e6f6b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oc6ftl4f6oknFzHBWQoc9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用交叉验证比较RFA和RFE的选择能力(图片由作者提供)</p></figure><p id="6dc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，我们证明了递归算法对于特征选择的优越性。我们介绍了不太为人所知的递归特征添加，并将其与最流行的递归特征消除方法进行了比较。两种方法都显示出令人满意的结果。然而，完美的选择方法并不存在。我们必须根据我们的任务找到并优化正确的过滤策略。</p><p id="4093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你对题目感兴趣，我建议:</strong></p><ul class=""><li id="307a" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu nu mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/shap-for-feature-selection-and-hyperparameter-tuning-a330ec0ea104"> <strong class="lb iu"> SHAP用于特征选择和超参数调谐</strong> </a></li><li id="270f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nu mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/boruta-and-shap-for-better-feature-selection-20ea97595f4a"> <strong class="lb iu">博鲁塔和SHAP进行更好的特征选择</strong> </a></li><li id="c3ee" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nu mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/boruta-shap-for-temporal-feature-selection-96a7840c7713"> <strong class="lb iu">博鲁塔SHAP进行时态特征选择</strong> </a></li><li id="c280" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu nu mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/shap-for-drift-detection-effective-data-shift-monitoring-c7fb9590adb0"> <strong class="lb iu">漂移检测SHAP:有效数据漂移监控</strong> </a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="ea43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="1e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>