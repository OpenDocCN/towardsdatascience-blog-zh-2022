<html>
<head>
<title>Implementing KNN From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始实施KNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-knn-from-scratch-70c800f6f64b#2022-01-12">https://towardsdatascience.com/implementing-knn-from-scratch-70c800f6f64b#2022-01-12</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="1261" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">从头做起</h2><div class=""><h1 id="3a34" class="pw-post-title jc jd iu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">从零开始实施KNN</h1></div><div class=""><h2 id="25f8" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">用Python和NumPy分解一个基本的机器学习算法</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/6c926c51a2f01e5f5463da1b4a32aa32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AvG16nlDPcL1LGXn"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">乔恩·泰森在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6da8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi mf translated">深入理解和欣赏机器学习算法的内部工作原理的最有效方法之一是从头开始实现它。</p><p id="ca78" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">从头开始实现它迫使我们不仅要将算法分解成主要的计算步骤，还要理解数学描述。我们还获得了如何使用参数的宝贵见解。</p><p id="9a28" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在接下来的部分，我们将从头开始实现一个经典的机器学习算法:<em class="mo"> K近邻。</em>我们将使用Python和NumPy逐步实现它。</p><p id="b508" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是在讨论一些实现细节之前，我们需要打下基础，对算法有一个大致的了解。</p></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="7118" class="mw mx iu bd my mz na nb nc nd ne nf ng kj nh kk ni km nj kn nk kp nl kq nm nn bi translated">概述</h1><p id="c893" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">由<a class="ae li" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank">于1951年</a>首次开发的K近邻(KNN)是一种非参数学习算法。KNN通常被认为是简单的，因为底层模型基本上不存在，仅仅由存储的训练数据集定义。</p><p id="25c2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">KNN很大程度上依赖于距离的概念，通过计算新观察值与训练数据集的每一行之间的欧几里德距离，将新观察值与存储的训练数据进行比较。</p><p id="53f5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在检索和排序距离之后，可以选择k个最近邻居<em class="mo">(索引)</em>。超参数<em class="mo"> k </em>指定相关邻域的大小，必须手动调整。</p><blockquote class="nt"><p id="69d3" class="nu nv iu bd nw nx ny nz oa ob oc me dk translated">k-最近邻可用于分类和回归任务。</p></blockquote><p id="640e" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">分别返回类别成员的多个投票结果或k个最近值的平均值。</p><p id="8f42" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以将算法的<strong class="ll je">主要步骤</strong>描述如下:</p><ol class=""><li id="c278" class="oi oj iu ll b lm ln lp lq ls ok lw ol ma om me on oo op oq bi translated">计算新观测值与训练数据集的每一行之间的距离。</li><li id="651c" class="oi oj iu ll b lm or lp os ls ot lw ou ma ov me on oo op oq bi translated">对距离进行升序排序<em class="mo">(从最小到最大)</em>并检索k个最近邻的索引。</li><li id="f1ab" class="oi oj iu ll b lm or lp os ls ot lw ou ma ov me on oo op oq bi translated">获取相应的类标签并返回最常用的标签。</li><li id="ba56" class="oi oj iu ll b lm or lp os ls ot lw ou ma ov me on oo op oq bi translated">对每个新观察重复步骤1-3。</li></ol><p id="77ce" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在下一节中，我们将在一个类中实现主要步骤。整体结构可以通过下面的<em class="mo">骨架类</em>来描述，它将指导我们完成接下来的部分。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="8703" class="mw mx iu bd my mz na nb nc nd ne nf ng kj nh kk ni km nj kn nk kp nl kq nm nn bi translated">从头开始实施</h1><p id="0f7c" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">在我们直接进入实际算法之前，我们需要打下一些基础——定义超参数<em class="mo"> k </em>并拟合训练数据集，或者换句话说，存储完整的训练数据。这将为我们即将到来的计算做好准备。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure><p id="8999" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们已经准备好继续学习那些更有趣的东西了。</p><h2 id="46bb" class="oy mx iu bd my oz pa dn nc pb pc dp ng ls pd pe ni lw pf pg nk ma ph pi nm ja bi translated">计算距离</h2><p id="ff46" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">我们已经存储了完整的训练数据集，允许我们计算新观察和训练数据的每一行之间的距离。</p><p id="8a00" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了计算距离，我们定义了一个辅助函数，返回每对行的欧几里德距离。</p><p id="9880" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mo">欧几里德距离</em>就是两行之间的平方差的平方根<em class="mo">(向量)</em> —或者更正式地表述为以下等式:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/70fe384206c8eac87060faaa16ae73aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/0*B0sf6ejUh8GVlwcx"/></div></figure><p id="a1f5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">helper函数的实现相对简单，因为我们可以依靠NumPy来处理向量操作。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure><p id="0054" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">一旦定义好了，我们就可以调用helper函数，并在一个简单的列表中存储新观察和完整训练数据之间的所有距离。</p><h2 id="8936" class="oy mx iu bd my oz pa dn nc pb pc dp ng ls pd pe ni lw pf pg nk ma ph pi nm ja bi translated">获取k-最近索引</h2><p id="d007" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">接下来的步骤也很简单。我们需要按照升序对距离列表进行排序<em class="mo">(从最小到最大)</em>，允许我们只选择k个最近的邻居。</p><p id="2288" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">借助下图，我们可以想到由<em class="mo"> k </em>定义的选择半径:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pk"><img src="../Images/a9b34c1e22ca98ea5a4a858f72d159ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*dHuhavD4ckuLb6cWY8hBSA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">k选择半径的示例说明[图片由作者提供]</p></figure><blockquote class="pl pm pn"><p id="46e4" class="lj lk mo ll b lm ln ke lo lp lq kh lr po lt lu lv pp lx ly lz pq mb mc md me in bi translated"><strong class="ll je">注意</strong>:k值是一个重要的参数，因为大的k值扩大了半径，允许更好的泛化。另一方面，较小的k值提供了更大的灵活性，但更容易受到噪声的影响。我们可以使用可视化技术，如<a class="ae li" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">“肘法”</a>来调整超参数。</p></blockquote><p id="c74b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以使用内置的NumPy函数<code class="fe pr ps pt pu b">numpy.argsort()</code>和Python切片符号对索引进行排序和选择。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure><h2 id="9f63" class="oy mx iu bd my oz pa dn nc pb pc dp ng ls pd pe ni lw pf pg nk ma ph pi nm ja bi translated">返回最常用的标签</h2><p id="6b55" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">现在，我们只剩下一件事要做了—我们需要检索相应的类标签，并返回最常见的标签作为我们的预测。</p><p id="ab4b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">基于k-最近邻的索引，我们可以从我们的训练数据中选择相关联的标签，并将它们存储在列表中。</p><p id="ef81" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">利用两个不同的NumPy函数——<code class="fe pr ps pt pu b">numpy.bincount()</code>和<code class="fe pr ps pt pu b">numpy.argmax()</code>——我们可以选择最常见的标签作为返回值。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure><h2 id="95ee" class="oy mx iu bd my oz pa dn nc pb pc dp ng ls pd pe ni lw pf pg nk ma ph pi nm ja bi translated">预测并把它们放在一起</h2><p id="8c35" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">到现在为止，我们已经完成了大部分艰苦的工作。</p><p id="839b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们需要做的最后一件事是对测试数据集中的每一行重复这些步骤，这就完成了我们的实现，并把它们放在一起。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="e664" class="mw mx iu bd my mz na nb nc nd ne nf ng kj nh kk ni km nj kn nk kp nl kq nm nn bi translated">测试分类器</h1><p id="8ab7" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">完成我们的实现后，我们需要用真实的数据集来测试它。</p><p id="3b75" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为此，我们将使用虹膜数据集，该数据集由150个样本组成，具有4个不同的特征<em class="mo">(萼片长度、萼片宽度、花瓣长度、花瓣宽度)</em>。我们可以通过绘制前两个特征来可视化数据。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pv"><img src="../Images/1d879236a5637980bb348abd20a0edf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*ucWGlFyRlgWMR3IcAxf8mA.png"/></div></figure><p id="c152" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">用<code class="fe pr ps pt pu b">k=3</code>实例化我们的分类器，用<code class="fe pr ps pt pu b">n_splits=5</code>进行交叉验证，我们得到了大约96%的准确率——对于这样一个简单的算法来说，这绝对是不错的。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="ow ox l"/></div></figure><blockquote class="pl pm pn"><p id="4427" class="lj lk mo ll b lm ln ke lo lp lq kh lr po lt lu lv pp lx ly lz pq mb mc md me in bi translated"><strong class="ll je">注意</strong>:由于算法依赖于距离，如果数据包含不同的物理单位或位于非常不同的尺度上，归一化作为预处理步骤可能是必要的。</p></blockquote></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="fc88" class="mw mx iu bd my mz na nb nc nd ne nf ng kj nh kk ni km nj kn nk kp nl kq nm nn bi translated">结论</h1><p id="88e7" class="pw-post-body-paragraph lj lk iu ll b lm no ke lo lp np kh lr ls nq lu lv lw nr ly lz ma ns mc md me in bi translated">在本文中，我们从头开始一步一步地实现了K近邻。我们了解了根据距离对新观测值进行分类所需的主要计算步骤。</p><p id="e67b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">虽然KNN是一个简单明了的算法，但从头开始实现它可以让我们获得更深入的理解。当算法应用于不同的环境时，例如缺失值的插补，这可能证明是特别有用的。</p><p id="97c6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你可以在我的GitHub上找到<a class="ae li" href="https://github.com/marvinlanhenke/DataScience/tree/main/MachineLearningFromScratch" rel="noopener ugc nofollow" target="_blank">的完整代码。</a></p><div class="pw px gq gs py"><div role="button" tabindex="0" class="ab bv gw cb fq pz qa bn qb lc ex"><div class="qc l"><div class="ab q"><div class="l di"><img alt="Marvin Lanhenke" class="l de bw qd qe fe" src="../Images/5b1b337a332bf18381aa650edc2190bd.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*iEJ9qn9i9dTw3uyXG2jsTA.jpeg"/><div class="fb bw l qd qe fc n aw fd"/></div><div class="hi l fp"><p class="bd b dl z fq fr fs ft fu fv fw fx dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@marvinlanhenke?source=post_page-----70c800f6f64b--------------------------------" rel="noopener follow" target="_top">马文·兰亨克</a></p></div></div><div class="qh qi gx l"><h2 class="bd je vy oi fq vz fs ft rh fv fx jd bi translated">从零开始的ML算法</h2></div><div class="ab q"><div class="l fp"><a class="bd b be z bi wa au wb wc wd sq we an eh ei wf wg wh el em eo de bk ep" href="https://medium.com/@marvinlanhenke/list/ml-algorithms-from-scratch-7621d01922ad?source=post_page-----70c800f6f64b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wi l fp"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="qu dh qv fq ab qw fp di"><div class="di qm bv qn qo"><div class="dh l"><img alt="" class="dh" src="../Images/230471643f4baf6e91fca0422ddfb56b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qT41zaPnyJHQeGKz0FvkQA.png"/></div></div><div class="di qm bv qp qq qr"><div class="dh l"><img alt="" class="dh" src="../Images/ca1fa6207e26d895ef8ba30bb66ef3a2.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*67IA2OsmsNXtckdF"/></div></div><div class="di bv qs qt qr"><div class="dh l"><img alt="" class="dh" src="../Images/33c295e3ac2a12346fba6df057ae448b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HXXyDFWGhV7pvo8E"/></div></div></div></div></div><blockquote class="pl pm pn"><p id="19d4" class="lj lk mo ll b lm ln ke lo lp lq kh lr po lt lu lv pp lx ly lz pq mb mc md me in bi translated">感谢您的阅读！确保保持联系&amp;在<a class="ae li" href="https://medium.com/@marvinlanhenke" rel="noopener"> Medium </a>、<a class="ae li" href="https://www.kaggle.com/mlanhenke" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上关注我，或者在<a class="ae li" href="https://www.linkedin.com/in/marvin-lanhenke-11b902211/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上说声“嗨”</p></blockquote></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><p id="9995" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="mo">喜欢这篇文章吗？成为</em> <a class="ae li" href="https://medium.com/@marvinlanhenke/membership" rel="noopener"> <em class="mo">中等会员</em> </a> <em class="mo">继续无限学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="pw px gq gs py rb"><a href="https://medium.com/@marvinlanhenke/membership" rel="noopener follow" target="_blank"><div class="rc ab fp"><div class="rd ab re cl cj rf"><h2 class="bd je gz z fq rg fs ft rh fv fx jd bi translated">通过我的推荐链接加入Medium-Marvin Lanhenke</h2><div class="ri l"><h3 class="bd b gz z fq rg fs ft rh fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="rj l"><p class="bd b dl z fq rg fs ft rh fv fx dk translated">medium.com</p></div></div><div class="rk l"><div class="rl l rm rn ro rk rp lc rb"/></div></div></a></div></div></div>    
</body>
</html>