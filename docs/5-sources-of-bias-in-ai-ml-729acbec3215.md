# 偏见会影响你的机器学习模型的 5 种偷偷摸摸的方式

> 原文：<https://towardsdatascience.com/5-sources-of-bias-in-ai-ml-729acbec3215>

## 发现你的模型中隐藏的偏见并修正它们

![](img/9c1d1dca2c6a221fec227231098de6a6.png)

由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的[真空背景](https://unsplash.com/@vackground?utm_source=medium&utm_medium=referral)拍摄

我们现在应该使用更多的人工智能解决方案。但是有这个偏见问题要考虑！

我们已经看到人工智能模型对代表性不足的群体表现不同。近年来，这些问题引起了激烈的辩论。在寻找产生偏见的原因时，我们发现，除了人类训练员的意图之外，还有更多方式可以导致偏见。

然而，当涉及到其他人的生活和工作时，造物主的清白是不可原谅的。顾客的反对、公众舆论和诽谤可能会损害你的声誉，而且可能很难恢复。

因此，理解人工智能偏见至关重要。你不能管理你不懂的东西。

这里有五种情况，偏见可能会潜入你的模型。

# 改变旧型号的用途是有用的，但也是危险的。

可重用性是开发人员和组织的首要任务。机器学习模型的好处甚至更高。

培训时间和资源消耗是公司采用人工智能实践的主要关注点。因此，重新利用旧模型或重用为不同目的构建的模型更有意义。

当然，云计算和[平台即服务](https://www.techtarget.com/searchcloudcomputing/definition/Platform-as-a-Service-PaaS) (PAAS)解决方案已经彻底改变了数据科学。然而，我们训练的模型近年来也变大了。

语言模型 [GPT3](https://openai.com/blog/gpt-3-apps/) 有 1750 亿个参数。训练这个模型大约需要 460 万美元。嗯，那不是每个人都能负担得起的。

但是你可以花 0.0004 美元使用开放的 AI API。那更实惠。更好的是，您可以针对您的特定用例对这些模型进行微调。

然而，你不知道这些模型被训练的原始数据集。盲目重用会在应用程序中引入偏见。

## 现有模型的直接重用可能会引入偏差

像 Open AI 的 GPT-3 一样，你可以在很多其他地方找到预训练的模型。 [Keras 文档](https://keras.io/api/applications/)列出了一系列此类车型。您可以抓住一个并在类似的用例上使用它。

例如，您可以抓起 VGG16，开始对图像进行分类。但它可能会将一个人标记为动物，因为模型没有看到足够多具有某些特征(例如，肤色)的人的例子。

在不同的环境中使用它可能会产生偏见，即使这是你自己的模型。例如，你用美国输入训练的聊天机器人可能在澳大利亚用户身上表现更差。

因此，最好不要在不同的上下文中重用模型，除非你 100%确定它的所有后果。必要的时候，如果你能让**人参与其中，**就去做。在预测开始产生影响之前，你可以让所有的预测或者可信度较低的预测得到人类的验证。

当然，并不是所有的应用程序都有人参与。如果是这样，使用迁移学习技术或集成方法更新模型是明智的。

## 使用迁移学习来更新模型，并为其提供上下文信息。

迁移学习是 ML 工程师重用现有模型的一种普遍做法。你可以在深度神经网络(DNN)上使用这种技术。

本质上，如果你有一个识别图像中的狗的模型，转移学习就会训练它识别猫。毕竟，猫和狗有很多相似之处——四条腿、耳朵、尾巴等等。

您可以向 DNN 添加新图层，也可以解冻最后一个图层。然后用您的特定领域示例训练模型。

迁移学习是一种节省成本和时间的技术，可以产生更好的结果。在你将它们应用到新的环境之前，先在你的模型上使用它将会减少偏见的机会。

你可以在我之前的文章中读到更多。

[](/transfer-learning-in-deep-learning-641089950f5d) [## 迁移学习:你能学到的最有效的深度学习技能。

### 迁移学习是一种加速深度学习训练的惊人方法。它有助于解决复杂的问题…

towardsdatascience.com](/transfer-learning-in-deep-learning-641089950f5d) 

## 使用集合来移除模型中不必要的偏差。

集合意味着一组模型。如果已经有了预测猫的模型，可以附加一个额外的模型，增加一个新的职责——找狗。

如果您使用的是为类似目的而训练的模型，则可以使用另一个模型并用新数据准备它，以最大限度地减少偏差。

# 当你的模型从相互作用中学习时，偏见就产生了

当你的模型向你的用户学习时，你应该更加小心。

[微软的 Twitter 聊天机器人 Tay](https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation) 就是一个很好的案例研究。Tay 一直在从 Twitter 与其他用户的对话中学习。但是这个机器人仅仅在几个小时后就被关闭了，因为 Tay 从其他用户那里选择了攻击性的语言，并开始像他们一样说话。

如果你的模型是从用户那里学习的，它就暴露在偏见的风险中。在主动学习中避免偏见仍处于研究阶段，我们还没有可靠的解决方法。因此，当你选择这个的时候，你应该更加小心。

如果学习是分批进行的，你就可以控制它。在将新数据输入模型之前，您可以检查它们是否存在任何已知的偏差。此外，您可以在发布新版本的模型之前进行更多的检查。

拥有一个[模型注册中心](https://neptune.ai/blog/ml-model-registry)是一个很好的实践。模型注册中心可以帮助您试验几种模型来解决同一个问题。当您在生产环境中发现一个模型有问题时，您可以很容易地切换到一个旧的模型，并将影响最小化。

但是，在主动学习模型中，比如进化的[强化算法](https://ai.googleblog.com/2021/04/evolving-reinforcement-learning.html)，你把控制权留给了机器。

# 合成数据的使用可能会导致偏差。

有时，合成或人工创建的数据被用于训练机器学习。虽然听起来可能违反直觉，但合成数据有很多用处。

当收集新数据很困难或成本很高时，工程师使用合成数据来训练 ML 模型。此外，当匿名很重要时，合成数据是有益的。

通常，合成数据生成会对变量的潜在概率分布进行建模，并从中提取新样本。

随着合成数据对分布进行概化，它**失去了数据集的原始上下文**。因此，在偏见产生后果之前发现任何偏见的机会很小。

这也是近年来大多数图像生成算法备受争议的原因之一。使用图像增强来训练神经网络是一种普遍的做法。避免过度合身是必要的。

除了隐藏细节，合成数据增强还可以放大差异。有了更多的人工数据，你现在有更多的数据来代表你的主导类。

# 偏见可能隐藏在抽象的维度中。

像合成数据一样，PCA 等降维技术也掩埋了上下文，产生了抽象变量。这并不意味着我们应该避免这样的做法。但是要注意风险。

它们使得在早期阶段很难理解输入变量和检测偏差。追溯抽象变量偏差的来源将是一个挑战。

想象一个场景，你建立一个模型来预测信用评分。您的输入数据集包含收入变量。PCA 之后，您有 PC1 和 PC2，…而不是标签。

也许你会抽样调查低收入人群最有可能违约的人群。但是你永远不会知道这是一个抽样问题，因为变量是抽象的。

# 无法解释的模型导致无法解释的偏见。

偏见并不总是有偏见的数据集的产物。它还取决于模型从数据集中选择要素的方式。

深度神经网络(DNN)最大的前景是自动特征选择。它非常有益，但也有缺点。你对你的 DNN 型号的特征选择没有多少控制力。

从 A 市开车到 B 市，只要安全准时到达 B 市，可能不太在乎路径。但在机器学习中，这很重要！

可解释的人工智能(XAI)近年来受到了很多关注，因为[对人工智能模型对代表性不足群体的预测提出了](https://levity.ai/blog/ai-bias-how-to-avoid)的担忧。

话虽如此，我们不能忽视和回避 DNN 的好处。合理的基准是人的水平的表现。确保人类也能以同样的方式表演。当预测置信度较低时，过滤它们并尝试手动进行。

# 最后的想法

将人工智能偏见的影响降至最低是数据科学界面临的一项挑战。

至此，世界已经清楚地明白了 AI 的好处，想要前进。然而，我们已经看到了机器预测的几个问题，我们知道还有更多要学习的。

算法天生没有偏见。但是他们从中学习的例子可以改变他们的行为。在这篇文章中，我们讨论了偏见进入你的模型的一些间接方式。

> 感谢阅读，朋友！在[**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)[**Twitter**](https://twitter.com/Thuwarakesh)[**Medium**](https://thuwarakesh.medium.com/)上跟我打招呼。
> 
> 还不是中等会员？请使用此链接 [**成为会员**](https://thuwarakesh.medium.com/membership) 因为，在没有额外费用的情况下，我赚取了一点佣金。