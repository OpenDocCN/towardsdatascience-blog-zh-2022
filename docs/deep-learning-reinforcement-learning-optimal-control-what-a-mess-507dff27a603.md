# 深度学习，强化学习，最优控制:知道区别

> 原文：<https://towardsdatascience.com/deep-learning-reinforcement-learning-optimal-control-what-a-mess-507dff27a603>

在计算智能领域有很多热门关键词。你可能分别知道深度学习、强化学习或最优控制，它们是如何工作的，或者你甚至可能知道如何使用这些工具。但有时很难全面了解它们之间的关系。

![](img/3b23ac7444b5bee58fb8ba9f5d932726.png)

马丁·鲍登在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

你可能也不熟悉最优控制。如果是这样的话，我建议你在这里看一些题目[的介绍。无论如何，最优控制通常被认为属于控制理论领域，而不是所谓的人工智能领域。](/optimal-control-a-preamble-6c0bc2b12ed6?source=your_stories_page----------------------------------------&gi=ec93a360d147)

控制理论和人工智能有很多相似之处。强化学习的概念实际上是在人工智能和控制理论的旗帜下出现的，只是符号不同。

让我们深入研究一下这件事，看看共同点和不同点。在整篇文章中，DL 将作为深度学习的简称，RL 作为强化学习的简称。

# 预测工具

我们可以很快提醒在哪些应用中可以使用这三种工具:

*   深度学习用于根据**现有数据**进行预测。深度学习的主要应用之一是图像处理。例如特征识别，给定一组猫和狗的图片，算法应该预测先前未见过的图片是否属于狗或猫类别。

![](img/9cc4bdc4bef073edafecb7e42bf512d9.png)

来源:[https://arxiv.org/pdf/1604.07043.pdf](https://arxiv.org/pdf/1604.07043.pdf)

![](img/bbe1252d8f8d0cf292ecfa51857f3969.png)

作者图片

*   虽然深度学习可以回答一个特定的问题，是猫还是狗，但强化学习和最优控制有助于以行动的形式做出**连续的选择**。如果我们使用深度学习来预测 2 个连续的图片，这些预测是独立的。在 RL/最优控制中，一个选择会影响下一个选择。例如，如果我们试图确定股票市场的最佳月度投资，当前的投资可能会扰乱市场，并改变我们进行下一个选择的方式。从这个意义上说，我们做出的选择是在对国家采取行动，因此我们正在采取行动。

![](img/1f8a67c2d5050ea78e36cf8e4151845c.png)

作者图片

总的来说，我们可以区分**状态**(或声明)和**控制**(或动作)的概念。
——深度学习让我们可以对最有可能的说法做出预测，是猫还是狗。
- RL 和最优控制试图预测最佳行动、要进行的投资、已知的状态，这里是股票市场的当前状态。

# 这都是最小化问题

![](img/b701ece943c015ab6f2d3ea90d63a168.png)

照片由[乌萨马·阿扎姆](https://unsplash.com/@ussamaazam?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

所有这些工具都有另一个重要的共同点:总体目标是最小化某个值，(或最大化它)，有时被称为**成本函数**，下面用 J 表示。

*   对于深度学习，最小化的成本是预测函数(神经网络)预测的结果与样本状态的实际值之间的差异。对于数据集的每个样本，该误差被累加。深度学习方法的训练过程在于找到预测函数，以使其最小化总预测误差。

![](img/f52115131477223019ac1e9df7eef7eb.png)

深度学习问题的近似表述。

*   对于强化学习和最优控制，成本可以是用户设计的任何东西。火箭使用的燃料，机器人到达期望点所需的时间……与深度学习的区别在于，这里的成本也取决于所采取的行动。因为这是一个连续的行动过程，我们需要把它们加起来计算最终成本。

在这两种情况下，我们将通过采用公式的离散和无限时间版本来简化。

*   对于强化学习，我们试图最小化任何潜在状态(someState)的总成本，形成一个**策略**。这意味着，无论火箭目前的状态如何，我们总是知道该采取的最佳行动(相对于成本)。

![](img/82df524e4e3f47f024e021fb5b454cde.png)

强化学习问题的近似表述。

*   在最优控制框架中，我们只想在给定一些初始条件的情况下最小化成本，这将为我们提供一系列要采取的最佳行动。然而，只要初始条件改变，我们就可以重复这个过程。这是模型预测控制(MPC)的基础。

![](img/bd23be5092ac74b51b17adf6b6f03360.png)

最优控制问题的近似公式，这里 f 代表系统的动态方程。

# 不同的方法来解决最低成本

我们看到，每个问题都必须找到总成本函数 j 的最优最小值。然而，对于每个问题，用于降低成本的技术各不相同:

*   对于深度学习，使用神经网络。神经网络对用户友好的主要原因是它们的高度非线性特性，但是能够非常容易地知道这个复杂函数的梯度(通过反向传递)。因为我们有梯度，**梯度下降**可以用于向最优成本 J*移动。
*   至于强化学习，也是一个迭代的过程。然而，任务更加复杂。我们必须为系统的每个状态计算 J*。为此，选择策略的初始猜测。通过遵循当前近似策略运行连续实验来改进策略。
*   在控制理论中，由于没有使用数据，最重要的部分在于选择正确的工具以最小的成本求解。最常见的是，将使用**优化工具**，但是如果能够提供例如动态梯度，则**梯度下降**也是一个选项。

# 无模型与基于模型

目前，强化学习看起来更像最优控制，而不是深度学习！令人惊讶的是，RL 和 DL 应该都属于人工智能领域。

RL 和最优控制都处理**动态系统**，即状态随时间变化的系统。系统的状态根据一些确定的(但可能是未知的)规律而变化。比如说经济学，是由一系列复杂的规律(通货膨胀、通货紧缩等)驱动的。)是我们试图近似的。一个机械系统在万有引力定律下运转。

为了理解这种无模型与基于模型的方法，请考虑从手中放下一个物体。如果你知道万有引力定律，你就能够用牛顿力学来预测物体在空中下落的运动。如果你对重力一无所知，但重复同样的经历足够多的次数，你就能从统计学上预测物体从不同位置落下的运动。

最优控制将试图利用我们所拥有的关于动力系统如何进化的知识，并将它以约束的形式包含在我们试图解决的问题中。

另一方面，强化学习不对模型做任何假设。从收集的数据中反复计算出最佳策略。这里的关键方面是 RL 不是试图提出系统的模型，而是直接针对成本函数的特定选择的策略。

现在，每种方法的优点/缺点是什么？

*   动态系统的模型可以是任意复杂的。通常，在最优控制框架中使用的模型需要被线性化，以便找到足够快的解决方案。如果我们还包括不确定性或扰动，用于计算的模型很可能离现实太远。对于难以建模的系统，绕过模型直接使用数据可能是有利的。但当模型足够可靠时，它总是会击败数据，因为数据可能很难收集。
*   有了动态系统的模型，我们可以很容易地在不同的成本之间切换。假设你正在使用基于模型的方法进行投资，你可以在不同的目标之间切换，在更快的投资回报或更长期的利益之间交替。对于无模型方法，如果您对成本函数进行任何修改，您将需要再次重新计算整个策略。
*   关于不确定性问题，无模型方法可以与随机结果分离，并且比固定模型方法更好地处理随机性。
*   然而，使用模型，即使是简化的，也总能提供最佳解决方案的基线，并具有一定程度的一致性。否则求解者会抱怨问题不可行，可以使用一些更安全的方法(停车，停止投资)。使用无模型方法，如果收集的数据没有探索该区域，则该政策对于一组州来说可能是糟糕的。
*   最后，在处理真实系统时，我们必须考虑约束。它们可以转化为行动约束(例如火箭推力有限)或状态约束(例如火箭不能离太阳太近)。虽然无模型方法可以处理动作约束，但是对于状态约束却很难。

# 神经网络融合

![](img/187f6d6e420b36bd4972a2ca1bb82a3b.png)

艾莉娜·格鲁布尼亚克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

我们现在能够更好地理解为什么强化学习被归为人工智能类别。与深度学习一样，它依赖于数据(而不是模型)。

然而，它不止于此。总的来说，强化学习试图制定一项政策。但是政策只不过是将状态映射到要采取的最佳行动的函数。由于这个函数可能是高度非线性的，我们可以用神经网络来逼近它。并且由于 RL 使用数据，因此可以在 RL 框架中策略迭代的同时训练该神经网络。这叫做**深度强化学习**。关于这个话题的更多实用信息，请看这篇[文章](/how-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b?source=your_stories_page----------------------------------------)。
现在我们更好地理解了为什么强化学习属于人工智能社区。实际上，无模型方法与神经网络的结合将强化学习推向了另一个层次。

然而，人们可以在最优控制框架内很好地使用神经网络。由于最优控制是基于模型的，模型本身也可以用神经网络近似，如[1]中所做的。在这种情况下，在使用常规的最优控制技术之前，还需要收集数据和训练网络。

最后，还可以将基于模型的方法的可靠性与系统的扰动和随机性的数据驱动估计相结合。例如，文献[2]提供了一种基于 MPC 框架的机器人手臂控制器，并结合了噪声的高斯近似。

# 结论

*   神经网络:一个高度非线性的函数，具有容易计算的梯度。
*   深度学习:使用神经网络来发表声明。
*   强化学习:构建给定状态(策略)下的最佳行动图。地图通过收集数据按顺序更新，不依赖于任何模型。
*   深度强化学习:用神经网络近似策略。
*   给定一些初始条件和系统如何随时间演变的模型，最优控制提供了采取行动的最佳顺序。

虽然深度学习显然是在人工智能方面，而最优控制显然是在控制理论方面，但强化学习是这两个领域的交叉点。它试图使用人工智能工具解决控制问题。
我们还看到，最终，所有这些人都试图预测最佳结果。虽然无模型和基于模型的方法都有自己的优势，但我们不能否认，利用数据可以达到手工无法达到的复杂程度。同时，系统的模型提供了一个提供可靠性的基础事实。

利用数据和模型将是一条出路，但这需要打破控制和人工智能领域之间的区别。

参考资料:

[1]:神经网络连续时间系统辨识:模型结构和拟合准则， [Marco Forgione，Dario Piga](https://www.sciencedirect.com/science/article/pii/S0947358021000169#!)

[2]:机械臂轨迹跟踪的数据驱动模型预测控制。