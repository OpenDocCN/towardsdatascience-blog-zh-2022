<html>
<head>
<title>Optimizing Style Transfer Models for Videos on Mobile Devices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化移动设备上视频的风格转移模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-style-transfer-models-for-videos-on-mobile-devices-a924e6f633d6#2022-01-18">https://towardsdatascience.com/optimizing-style-transfer-models-for-videos-on-mobile-devices-a924e6f633d6#2022-01-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="0f92" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">优化移动设备上视频的风格转移模型</h1></div><div class=""><h2 id="f2b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">尝试PyTorch照明和闪光灯的灵活性，以创建一个面部卡通模型</h2></div><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated"><a class="ae kq" href="https://apps.apple.com/us/app/toonclip/id1536285338" rel="noopener ugc nofollow" target="_blank"> iOS ToonClip App </a>采用本帖描述的人脸转卡通模型——模型、App、图片均由作者提供。</p></figure><p id="3bb7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">拥抱面部空间演示在:<a class="ae kq" href="https://huggingface.co/spaces/Jacopo/ToonClip" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/Jacopo/ToonClip</a></p><p id="21e7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">免费的iOS移动应用位于:【https://apps.apple.com/us/app/toonclip/id1536285338 T2】</p><p id="1279" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">源代码可在(WIP)获得:【https://github.com/JacopoMangiavacchi/ToonClip-ComicsHero T4】</p><h1 id="8e4d" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">介绍</h1><p id="355d" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">gan是构建图像风格传输模型的超级流行的架构，但有时需要相当长的时间来训练它们，更重要的是优化生成器模型，以便能够在移动设备上实时运行。</p><p id="0945" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这次实验中，我主要关注这三个特定领域:</p><ol class=""><li id="296d" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm mp mq mr ms bi translated">利用PyTorch照明和闪光灯的灵活性，轻松试验不同的模型架构和主干</li><li id="c3bb" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">使用定制Pytorch感知特征损失在没有GAN鉴别器的情况下快速训练</li><li id="99fb" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm mp mq mr ms bi translated">不断转换到ONNX / CoreML，并在设备上对模型进行基准测试，以找到更好的移动设备优化</li></ol><h1 id="8759" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">信用</h1><p id="192a" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">在开始之前，我真的想感谢多伦阿德勒提供了一个伟大的项目，真正激发了我的实验和这篇文章，U2网络英雄风格转移:</p><ul class=""><li id="7619" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm my mq mr ms bi translated">GitHub代码和数据:<a class="ae kq" href="https://github.com/Norod/U-2-Net-StyleTransfer" rel="noopener ugc nofollow" target="_blank">https://github.com/Norod/U-2-Net-StyleTransfer</a></li><li id="9794" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm my mq mr ms bi translated">拥抱脸空间演示:<a class="ae kq" href="https://huggingface.co/spaces/Norod78/ComicsHeroHD" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/Norod78/ComicsHeroHD</a></li></ul><p id="db39" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该U2Net模型特别实现了muti _ bce _ loss _ fusion函数的核心思想，以建立感知特征损失来比较U2Net编码器-解码器的所有层输出。</p><p id="8aae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我的实验中，我采用了一种类似但同时更通用且有望更灵活的技术，该技术使用感知特征损失，在分离的图像模型上进行计算，并且能够独立于用于编码器-解码器的特定模型架构捕获不同的层特征。</p><p id="f2f4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">具体来说，我试图利用PyTorch之上的现代高级框架的所有优势来:</p><ul class=""><li id="3534" class="mk ml iq kt b ku kv kx ky la mm le mn li mo lm my mq mr ms bi translated">使用PyTorch Lighting Flash提供的预训练权重，支持编码器-解码器架构的任何已知图像模型和编码器上的任何图像特征化器后端</li><li id="f3a9" class="mk ml iq kt b ku mt kx mu la mv le mw li mx lm my mq mr ms bi translated">使用支持GPU/TPU等的PyTorch照明实现自定义丢失和快速训练。</li></ul><h1 id="3189" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">资料组</h1><p id="8b09" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">在我的实验中，我重复使用了上面提到的Doron Adler在他的U2Net Heros风格转换项目中提供的相同数据。这是从FFHQ数据集中选择的100个高分辨率(1024x1024)人脸图像的图像数据集，以及相关的卡通图像。数据集在上面提到的相同GitHub repo上提供，并在Apache License 2.0下获得许可</p><p id="c3cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下面的代码中，您可以看到PyTorch数据集类加载并转换这些数据:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="d461" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如你可能注意到的，在这个小数据集之上，我使用了Albumentations库来添加一些图像增强转换，并且能够在从移动相机捕获的真实图像和视频上推断模型时更好地进行概括。</p><p id="561a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">特别是，由于数据包含作为输入(面部)和输出(风格化面部)的图像，我重用了Albumentation api的ReplayCompose功能，以确保对每一对面部和风格化面部图像应用相同的随机变换。</p><p id="31d0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下面的代码片段中，您将看到一个示例，说明如何使用不同的增强过滤器将此图像配置到图像数据集:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="846a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你会在这篇文章的后面看到，我使用迁移学习技术从之前在ImageNet数据集上训练的模型中导入权重，所以我还在上面的白蛋白管道中添加了一个用ImageNet stats配置的标准化步骤。</p><p id="d6f0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，为了能够在同一个分布上多次重复执行训练循环，在随机分割训练和评估数据中的数据集之前，我指定了一个特定的随机手动种子，然后将该种子重置为初始PyTorch随机种子。通过这种方式，我能够每次都使用不同的增强管道随机应用程序进行训练，但仍然在相同的训练和验证分布上工作。</p><h1 id="ed6e" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">感知特征损失</h1><p id="9e37" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">在GAN中，并且通常在风格转移领域中，基于特征激活和风格损失的损失函数的思想是一种众所周知的技术，例如在诸如超分辨率的场景中经常采用。</p><p id="12de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这个实验中，我特别遵循众所周知的技术，即使用外部的、预先训练的VGG网络从不同的卷积块获得特征，并使用这些特征来弥补不同特征的损失，而不是简单地计算生成的图像和原始标签/样式图像之间的MSE或L1距离。</p><p id="f2b9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">来自论文“实时风格传输和超分辨率的感知损失”(<a class="ae kq" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1603.08155</a>)的下图说明了这一思想，并且存在许多文章来全面描述这一过程并提供代码的参考实现(包括Fast。AI MOOC培训由杰瑞米·霍华德)。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a6412243d5c57c1a1498c3d5b9b0837f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Jsx9DTrFN8CnypIWjjTlNQ.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">模型结构提供<a class="ae kq" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="e21a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我的例子中，我重用了Alper ahmetoluon在GitHub Gist上提供的代码片段(<a class="ae kq" href="https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49" rel="noopener ugc nofollow" target="_blank">https://Gist . GitHub . com/Alper 111/8233 CDB 0414 B4 CB 5853 F2 f 730 ab 95 a 49</a>)，但是我已经更新了它，添加了对每个块特性进行加权的选项，以便遵循Fast的论文和杰瑞米·霍华德建议。人工智能课程在这些层的输出上加电格拉米矩阵产品。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="7ae6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我简单地注释了将输入图像规范化为ImageNet stats的调用，因为我已经在数据集转换管道中规范化了所有输入的面部和样式图像。</p><h1 id="fa2d" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">图像到图像建模和分割</h1><p id="ee3c" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">图像到图像的风格转换问题最终非常类似于更一般的语义分割问题，并且任何用于该一般图像域的模型架构，如完全卷积编码器-解码器，在风格转换场景中也确实工作良好。</p><p id="c5c5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在我的探索中，我希望尽可能灵活地试验不同的特定编码器-解码器模型架构，并能够在以下架构上测试真实的性能结果:'<em class="nd"> deeplabv3 </em>'、<em class="nd"> deeplabv3plus </em>'、<em class="nd"> fpn </em>'、<em class="nd"> linknet </em>'、<em class="nd"> manet </em>'、<em class="nd"> pan </em>'、<em class="nd"> pspnet </em>'、<em class="nd"/></p><p id="f0e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我发现<em class="nd"> PyTorch Lighting Flash </em>库在这个特定的上下文中特别有用，因为它专门提供了一个高级的<strong class="kt ir"><em class="nd">semantic segmentation</em></strong>类，支持上面所有的架构。</p><p id="fb01" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，它为图像特征骨干的不同组合提供权重，以用于这些全卷积网络(FCN)的编码器部分。例如，在UNet++上，它专门支持主干，例如:</p><pre class="kf kg kh ki gt ne nf ng nh aw ni bi"><span id="c76b" class="nj lo iq nf b gy nk nl l nm nn">'densenet121', 'densenet161', 'densenet169', 'densenet201', 'dpn107', 'dpn131', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'efficientnet-b0', 'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b6', 'efficientnet-b7', 'efficientnet-b7', 'efficientnet-b8', 'efficientnet-l2', 'gernet_l', 'gernet_m', 'gernet_s', 'inceptionresnetv2', 'inceptionv4', 'mobilenet_v2', 'mobilenetv3_large_075', 'mobilenetv3_large_100', 'mobilenetv3_large_minimal_100', 'mobilenetv3_small_075', 'mobilenetv3_small_100', 'mobilenetv3_small_minimal_100', 'regnetx_002', 'regnetx_004', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_064', 'regnety_080', 'regnety_120', 'regnety_160', 'regnety_320', 'res2net101_26w_4s', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2next50', 'resnest101e', 'resnest14d', 'resnest200e', 'resnest269e', 'resnest26d', 'resnest50d', 'resnest50d_1s4x24d', 'resnest50d_4s2x40d', 'resnet101', 'resnet152', 'resnet18', 'resnet34', 'resnet50', 'resnext101_32x16d', 'resnext101_32x32d', 'resnext101_32x48d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext50_32x4d', 'se_resnet101', 'se_resnet152', 'se_resnet50', 'se_resnext101_32x4d', 'se_resnext50_32x4d', 'senet154', 'skresnet18', 'skresnet34', 'skresnext50_32x4d', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'xception'</span></pre><p id="3e88" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">语义分割实际上是在像素级执行分类的任务，这意味着每个像素将与给定的类别相关联。通常，当这些像素分类器FCN模型被用于语义分割时，有必要为最终卷积层指定数量等于预期类别数量的通道，并在最后调用ArgMax激活来为每个像素选择得分最高的类别。</p><p id="5ded" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在风格转换(图像到图像)场景中，采用FCN甚至更容易，因为只需使用具有3个通道的卷积层作为最终输出，就可以让模型预测彩色图像。</p><p id="414c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的代码片段显示了如何轻松灵活地获得您想要使用的特定FCN模型，提供预期模型架构(unetplusplus)、头部编码器的主干(mobilenetv3_large_100)和迁移学习(imagenet)中重用的特定权重作为参数。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="2503" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如您在这个具体测试中所看到的，我使用了mobilenetv3_large_100主干来实现移动设备上实时视频推理的特定性能，但当然也可以使用其他主干来获得更高质量的图像以进行最终预测。</p><h1 id="5f46" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">培养</h1><p id="64a1" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">不幸的是<em class="nd"> PyTorch Lighting Flash </em>还没有灵活到可以直接用Flash Trainer训练上面图像到图像场景的语义分割模型。</p><p id="507a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">无论如何，非常幸运的是，我仍然能够使用<em class="nd"> PyTorch Lighting </em>并编写我自己的ImageToImage Lighting模块类，最终训练并验证具有先前描述的VGG感知特征损失的模型。</p><p id="74ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的代码片段显示了ImageToImageModel类，以及如何实例化该类并将其传递给PyTorch Lighting Trainer，以便对您拥有的特定ML加速启动一个训练和评估循环。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><h1 id="6d00" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">ONNX导出、结果和HuggingFace空间演示</h1><p id="8214" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">从PyTorch Lighting甚至PyTorch导出模型到ONNX格式现在是一个非常简单的过程。</p><p id="9f42" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">具体到PyTorch照明，我必须调用以下api将模型权重首先移回CPU(显然ONNX exporter需要这样做):</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="33c9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还值得一提的是，对于我在该模型实例中使用的MobileNetV3主干，我必须指定一个最新的opset版本，以便在ONNX运行时支持该模型使用的所有新ops，例如hardwish之类的激活函数(有关更多详细信息，请参见下面的CoreML段落)。</p><p id="071d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了快速测试ONNX模型，我发现拥抱面部空间环境非常有用，特别是支持易于使用的Gradio api。</p><p id="b809" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是使用Gradio在用户图像上快速测试ONNX模型并从Hugging Face Hub缓存模型图像的代码:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="fd0b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这款车型的现场Gradio演示可以在拥抱脸空间上找到，网址:【https://huggingface.co/spaces/Jacopo/ToonClip<a class="ae kq" href="https://huggingface.co/spaces/Jacopo/ToonClip" rel="noopener ugc nofollow" target="_blank"/></p><p id="1e14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了方便重复使用，我还在HuggingFace模型中心——<a class="ae kq" href="https://huggingface.co/Jacopo/ToonClip" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/Jacopo/ToonClip</a>上直接导出了ONNX模型</p><p id="555c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">无论如何，以下是由该模型从未经训练的FFHQ验证集中生成的一些示例图像:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c0cf5742d3f26c59e0e41fec72ba85e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMCC6pM61PV2mWpv79go8g.jpeg"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">FFHQ数据集提供的样本图像以及作者提供的模型生成的相应预测</p></figure><p id="6144" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请在拥抱脸上直接尝试这个演示，并最终使用演示上提供的UI功能(<a class="ae kq" href="https://huggingface.co/spaces/Jacopo/ToonClip" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/Jacopo/ToonClip</a>)标记任何问题或异常</p><h1 id="17c9" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">CoreML导出和图像处理</h1><p id="34f6" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">苹果提供了一个非常有用的Python包，名为<em class="nd">CoreMLTools</em>(【https://coremltools.readme.io/docs】T2)，可以快速地将ML模型从PyTorch和TensforFlow等不同框架导出到CoreML格式，允许模型在苹果生态系统上可用的特定硬件(GPU、神经引擎、CPU加速器)上得到加速。</p><p id="e4cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不幸的是，导出图像模型需要在导出过程中进行更多的编码，特别是如果您想要在视频上实现实时图像处理的最高性能。</p><p id="0e60" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">开发人员特别需要对推理平台中获得的图像进行预处理和后处理，通常使用Swift编程语言和本地图像捕获和处理库，并对该图像进行适当的归一化和整形。</p><p id="1209" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有不同的技术可以在模型本身上实现这一点，用python编码，或者在客户端使用Swift、iOS/macOS库和CoreML运行时API。</p><p id="e2c6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我的建议通常是将模型封装在包装Python模块中，并将该包装直接提供给CoreML exporter api，最后更新CoreML生成的规范(实际上是proto-buffer格式),以转换图像中的输出张量，从而轻松地将模型集成到客户端应用程序流中。</p><p id="77b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下面的代码片段中，您可以找到前面讨论的模型的PyTorch包装类，以及使用输入和输出参数的图像类型导出和格式化模型所需的代码。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><p id="b4db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最后，本示例中描述的特定UNet++主干模型使用了CoreMLTools转换库尚不支持的PyTorch运算符。幸运的是，CoreMLTools允许ML开发人员使用他们的MIL框架api来定义定制的Op，以便为缺失的PyTorch Op实现快速重新定义和注册正确的转换。</p><p id="ccfb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">具体来说，以下代码重新实现了UNet++模型使用的hardwish和hardsigmoid PyTorch激活器:</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="mz kl l"/></div></figure><h1 id="dfc0" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">源代码</h1><p id="618e" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">这个实验的大部分代码已经在本文上面的代码片段中提供了。</p><p id="82dc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">完整的源代码和完整的端到端笔记本将很快在https://github.com/JacopoMangiavacchi/ToonClip-ComicsHero<a class="ae kq" href="https://github.com/JacopoMangiavacchi/ToonClip-ComicsHero" rel="noopener ugc nofollow" target="_blank"/>提供</p><p id="fc88" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢您阅读本文，测试iPhone应用程序或拥抱脸演示，并再次感谢本文中提到的人给了我们巨大的灵感。在玩卡通的时候学习新的东西很有趣！</p></div></div>    
</body>
</html>