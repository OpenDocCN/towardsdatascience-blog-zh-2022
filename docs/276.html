<html>
<head>
<title>Paper explained: Momentum Contrast for Unsupervised Visual Representation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释:无监督视觉表征学习的动量对比</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb#2022-01-10">https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb#2022-01-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="9c42" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">论文解释:无监督视觉表征学习的动量对比</h1></div><div class=""><h2 id="095b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">查看MoCo v1和v2文件中的原则</h2></div><p id="8522" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，我们将回顾MoCo论文，这是一种使用动量对比(MoCo)对计算机视觉模型进行预训练的无监督方法，已经由作者从版本1到版本3进行了迭代改进。</p><p id="1c77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">于2020年由何等人在论文<a class="ae le" href="https://arxiv.org/pdf/1911.05722.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">【动量对比无监督视觉表征学习】</em> </a> <em class="lf"> </em>中首次提出，并在<a class="ae le" href="https://arxiv.org/pdf/2003.04297.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">【改进的基线与动量对比学习】</em> </a> <em class="lf">中进一步改进为v2。</em>在陈等人的<a class="ae le" href="https://arxiv.org/pdf/2104.02057.pdf" rel="noopener ugc nofollow" target="_blank">“训练自监督视觉变形器的实证研究”</a>中有另一个称为v3的迭代，它介绍了训练过程和模型架构的基本适应，我们将在未来的故事中介绍。</p><p id="403e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于这种方法一直在不断改进，我们将首先回顾本文背后的最初想法，然后概述新版本引入的改进。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/5f6ec6d7a04d4f0e8951896923ed5131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AURmxepRI4G6WT0DirxZDw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">对最初的MoCo论文背后的关键思想的简单说明。来源:<a class="ae le" href="https://arxiv.org/pdf/1911.05722.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="69f6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:计算机视觉的自我监督和非监督预训练</h1><p id="35be" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们深入研究MoCo的论文之前，有必要快速回顾一下自我监督的职前培训到底是怎么回事。<strong class="kk iu">如果你一直在阅读我的其他自我监督学习故事，或者你熟悉自我监督预培训，请随意跳过这一部分。</strong></p><p id="091e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统上，计算机视觉模型总是使用<strong class="kk iu">监督学习</strong>来训练。这意味着人类看着这些图像并为它们创建各种标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。</p><p id="bf50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，<strong class="kk iu">自我监督学习不需要任何人为创造的标签</strong>。顾名思义，<strong class="kk iu">模型学会自我监督</strong>。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，<strong class="kk iu">我们让模型知道这些图像仍然包含相同的视觉信息</strong>，即相同的对象。<strong class="kk iu">这导致模型学习相同对象的相似潜在表示(输出向量)。</strong></p><p id="0f31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。</p><h1 id="a5bf" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使用动量对比进行无监督学习</h1><p id="90ec" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在MoCo的论文中，无监督学习过程被框定为字典查找:<strong class="kk iu">每个视图或图像被分配一个关键字</strong>，就像在字典中一样。<strong class="kk iu">该密钥通过使用卷积神经网络对每个图像</strong>进行编码而生成，输出是图像的矢量表示。现在，如果一个查询以另一个图像的形式呈现给这个字典，这个查询图像也被编码成矢量表示，并且将属于字典中的一个关键字，具有最小距离的关键字。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/26c72a5d58d9ff56990497a7b6d9a6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WerzRQgq8gZgo3w-L5oWkg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">左边是查询图像和编码器，右边是小批量密钥队列和momentum编码器。来源:<a class="ae le" href="https://arxiv.org/pdf/1911.05722.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="d311" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练过程设计如下:编码器网络选择并处理查询图像，以计算编码的查询图像<em class="lf"> q </em>。由于该模型的目标是学习区分大量不同的图像，因此该查询图像编码不仅与一个小批量的编码关键图像相比较，而且与它们中的多个相比较。为了实现这一目标，MoCo形成了一个由动量编码器网络编码的小批量队列。当选择新的小批量时，其编码被排队，并且数据结构中最旧的编码被出列。<strong class="kk iu">这将由队列表示的字典大小与批处理大小分离，并允许查询更大的字典。</strong></p><p id="de86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果查询图像的编码与字典中的关键字匹配，则这两个视图被认为来自同一图像(例如，多个不同的裁剪)。</p><p id="e8f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练目标由信息损失函数表示:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/942b535f666b8b55def5248a58a52242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gw975iVCkURNV48emKs1g.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">信息对比损失函数。来源:<a class="ae le" href="https://arxiv.org/pdf/1911.05722.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="f7e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该损失函数允许模型学习来自同一图像的视图的较小距离以及不同图像之间的较大距离。为了实现这一点，损失类似于基于softmax的分类器损失，其目的是将<em class="lf"> q </em>分类为<em class="lf"> k+ </em>。对查询<em class="lf"> q </em>的一个正键和<em class="lf"> K </em>个负键计算总和。</p><p id="e9ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，<strong class="kk iu">如第一幅图所示，通过编码器网络执行反向传播，但不通过动量编码器。</strong></p><p id="0646" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者试图用动量编码器对每幅图像实现相对一致的编码。由于保持权重冻结不会反映模型的学习进度，<strong class="kk iu">他们建议使用动量更新</strong>而不是复制编码器的权重来获得一致的动量编码器结果。为了实现这一点，将动量编码器网络设置为由查询编码器的<strong class="kk iu">基于动量的移动平均值来更新。每次训练迭代都执行这种更新。</strong></p><h1 id="3a0d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">MoCo v2的进一步改进</h1><p id="6e27" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在模型的第二个版本中，<strong class="kk iu"> MoCo v2 </strong>，在他们的论文<a class="ae le" href="https://arxiv.org/pdf/2003.04297.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">“改进的基线与动量对比学习”</em> </a> <em class="lf">，</em>中，作者将<a class="ae le" rel="noopener" target="_blank" href="/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703"> SimCLR论文</a>中的一些贡献转移到他们的模型中。最初，为了在预训练MoCo之后训练线性分类器，他们向模型添加了单个线性层。在MoCo v2中，该层已被MLP投影头取代，从而提高了分类性能。另一个显著的改进是引入了更强的数据扩充，类似于SimCLR中的数据扩充。他们通过添加模糊和更强的颜色失真来扩展v1增强。</p><p id="ff25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最新版本的<strong class="kk iu"> MoCo v3 </strong>，在<a class="ae le" href="https://arxiv.org/pdf/2104.02057.pdf" rel="noopener ugc nofollow" target="_blank">“训练自我监督视觉变形者的实证研究”</a>论文中介绍，将该模型改编为不同的训练过程。由于这些变化不仅仅是简单的扩展，我们将在以后的文章中讨论这篇文章。</p><h1 id="c15d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="193d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您已经了解了MoCo，这是一篇最受欢迎的自我监督框架论文，为该领域更有前途的论文铺平了道路。</p><p id="d3c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常在我的论文故事中，我会在结尾介绍这种方法的结果和表现。这一次，我们将跳过这一部分，因为这两种方法都不是最先进的了，但MoCo v3是。尽管如此，我发现了解MoCo背后的原理作为最先进的MoCo v3论文的基础是非常重要的。</p><p id="aea5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我希望这个故事能让你对MoCo有一个很好的初步了解，但仍有很多东西有待发现。因此，即使你是这个领域的新手，我也会鼓励你自己阅读这些论文。你必须从某个地方开始；)</p><p id="97df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在Twitter上给我留言，我的账户链接在我的媒体简介上。</p><p id="ca8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="aefb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="9613" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="14c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]何，，等.“用于无监督视觉表征学习的动量对比”<em class="lf">IEEE/CVF计算机视觉和模式识别会议论文集</em>。2020.<a class="ae le" href="https://arxiv.org/pdf/1911.05722.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1911.05722.pdf</a></p></div></div>    
</body>
</html>