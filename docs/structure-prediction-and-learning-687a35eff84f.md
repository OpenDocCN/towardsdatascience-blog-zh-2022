# 结构预测和学习

> 原文：<https://towardsdatascience.com/structure-prediction-and-learning-687a35eff84f>

## 预测建模与结构推理相结合

![](img/64c0b0bf0d0e7e69728cebfc35ea73f5.png)

[Evgeniy Surzhan](https://unsplash.com/@followtherabbit?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

监督机器学习包括从一些输入中预测一个*结果*变量的值。通常，结果是实值或分类的。

*结构化预测*将其推广到预测具有显式*结构*或由多个*交互*标量结果组成的结果。让我们看看涉及显式结构的例子。

**结构化结果**

这些是典型的序列、树或图。我们将看到每一个的例子。大部分摘自[2]。

**光学字符识别**:识别手写图像中的文字。单词是由原子和单个字符组成的结构化结果。(结构化的)结果空间是所有单词的字典。它允许将单词级信息与单个字符的 OCR 相结合，从而获得更高的准确性。例如，如果第一个字母看起来像 *c* ，第三个像 *r* ，第二个像一个 *o* 或一个 *a* ，那么这个单词很可能就是 *car* 。

**NLP 解析**:预测句子的解析树，即单词序列。结果空间是所有解析树的集合。

**物体识别**:识别图像或视频中的物体。不是单独地，而是集体地，作为一个场景图。也就是说，作为一个结构化的结果。

**蛋白质结构预测**:从蛋白质的序列预测蛋白质的二级或三级结构。

**短语翻译**:将英语短语翻译成法语短语。

**词性标注**:输入一种语言(比如英语)的单词序列，输出单词(名词、动词、形容词等)的词性标签序列。结果是结构化的，因为我们想要输出 POS 标签的*序列*。为单个单词预测的标签会相互影响，因此预测每个单词的最佳标签并简单地组合它们是不够的。

**走向结构化感知器**

**关节特征功能**

其中一个关键思想是联合特征函数F( **x** ， **y** )，其中 **x** 是输入， **y** 是输出。考虑词性标注。现在，我们仅限于标记一个单词，表示为 *x* 。如果我们认为 *x* 的某些方面与 *y* 的某些方面相关，我们可以将它们捕获到特征中。下面是一些例子，来自[5]。

*   *x* 的第一个字母大写， *y* =名词
*   *x* 以 *ed* ， *y* =动词

**用联合特征函数表示的 HMMs**

在[6]之后，这在一个例子中最容易看到。认为

```
**x**   The   dog    ate    the   homework
**y** Det → Noun → Verb → Det → Noun
```

这里→表示 HMM 中的状态转换。在 HMM 下分解的联合概率是

```
*P*(**x**,**y**) = *T*(Det|Begin)**E*(The|Det)**T*(Noun|Det)**E*(dog|Noun)*…
```

这里 *T* (B|A)表示 HMM 中从状态 A 转移到状态 B 的概率， *E* (B|A)表示从状态 A 发射 B 的概率，Begin 表示开始计算 *P* ( **x** ， **y** )时 HMM 开始的状态。

*P* ( **x** ， **y** )就是从状态*开始*跃迁到状态 *Det* 的概率，然后从状态 *Det* 发射字*到*，再从状态 *Det* 跃迁到状态*名词*，然后发射字 *dog【的概率诸如此类。*

拿走木头

```
log *P*(**x**,**y**) = log *T*(Det|Begin) + log *E*(The|Det) + log *T*(Noun|Det) + log *E*(dog|Noun) + …                                              (1)
```

等式(1)可以重新表示为

```
S(**x**, **y**) = *w*_{*T*, Begin, Det}*F_{*T*, Begin, Det}(Begin, y1) +
*w*_{*E*, Det, The}**F*_{*E*, Det, The}(x1) + …                          (2)
```

在哪里

*S* ( **x** ， **y** )是 log *P* ( **x** ， **y** )， *w* _{ *T* ，Begin，Det}是 log *T* (Det|Begin)，F _ {【Begin 是 log *E* (The|Det)，F_{ *E* ，Det，如果*x*1， *y* 1 是，则为 1，如果 *y* 1 是 Det，则为 0

诸如此类。

乍一看，方程(2)似乎只是方程(1)的一种更不透明的写法。

然而还有更多。事实证明，很容易将等式(2)变形为比等式(1)更丰富的得分函数。要看到这一点，考虑一下我们句子中的单词*x*3 =*eat*。方程(1)中其值受其影响的项是 log *E* (ate|Verb)，在方程(2)的符号中是

```
*w*_{*E*, Verb, ate}*F_{*E*, Verb, ate}(*x*3, *y*3)
```

现在想象用一个不同的词来代替 *x* 3，比如说*咀嚼*。此外，假设*咀嚼过的*在我们的数据集中没有出现过。所以 *E* (咀嚼|动词)未知。

考虑添加以下特征函数 F_{ *E* ，动词，*ed}(x，y)。它有一个可学习的参数 *w* _{ *E* ，动词，*ed}与之配对。F_{ *E* ，动词，*ed}( *x* 3， *y* 3)如果 *x* 3 以 ed 结尾， *y* 3 为动词，则为 1，否则为 0。

请注意，这个函数正在被*添加到我们的得分函数中。如果在训练集中看到了*嚼过的*，那么在对 *x* =嚼过的， *y* =动词的评分中，两个特征函数会相加地发挥作用*

```
*w*_{*E*, Verb, chewed}*F_{*E*, Verb, chewed}(*x*, *y*)+
*w*_{*E*, Verb, *ed}*F_{*E*,Verb, *ed}(*x*, *y*)
```

从这个意义上来说，有些冗余。另一方面，F_{ *E* ，动词，*ed}潜在地概括得更广泛，因为它适用于在训练期间没有看到但当从状态*动词*发出时以 *ed* 结尾的单词。

**高效推理**

正如我们所见，score 函数在支持哪些特性方面的能力非常重要。同样重要的是，我们可以使用 score 函数高效地进行推理。在我们的设置中，最重要的推断是寻找一个对于给定的 **x** 最大化 *S* ( **x** ， **y** )的 **y** 。在我们运行的示例中，这将为我们提供给定单词序列的最佳 POS 标签序列。

HMM 中的有效推理

为了寻找对于给定的 **x** 使方程(1)最大化的 **y** ，我们可以利用 HMM 的特殊结构，使其遵守*最优化原则*:从小问题的最优解，我们可以建立更大问题的最优解。

为了更准确地理解这一点，让我们在我们的词性标注示例中举例说明。我们为*寻找一个最优标签序列的分数，狗吃了作业*。对于*作业*的每一个可能的标签 *t* ，让我们在*作业*必须从 *t* 发出的附加约束下，求*最优解的分数狗吃了*。现在我们有了几个子问题的最优分数，每个值对应一个 *t* 。我们将这些分数乘以从 *t* 发出*作业*的概率。然后，在从 *t* 发出*作业*的附加约束下，每个结果分数对于整个句子都是最优的。取这些分数中的最大值，我们就得到一个去掉了这个约束的最佳分数。

我们在前面的段落中直观地描述了维特比算法。

**高效的推论更一般一点**

因此，等式(2)给出了比等式(1)更丰富的得分函数。推理的效率呢？我们将通过一个案例研究间接回答这个问题，这个案例研究使用了比普通 HMM 更丰富的评分函数，同时确保推理仍然有效。我们采用这种教学方法是因为我们觉得“确保推理保持有效”最好用一个真实的例子来理解。

**功能更加丰富的词性标注**

首先，我们将从 HMM 的所有特征函数开始。特别地，在等式(2)的符号中重新表达的等式(1)的那些。对此，我们将在每个新添加的功能适用于单个(*单词*、*状态*)对的约束下添加更多的功能。这些新增加的可以被视为 HMM 的发射函数的增强版本。

我们已经看到一个了，F_{ *E* ，动词，*ed}( *x* ， *y* )。这可以看作是一种发射:从状态动词*发出以 *ed* 结尾的单词的概率。*

读者可能会猜测这是新增加的特征函数对一对(*单词*，*状态*)的约束，这将允许我们保持推理的效率。

再来看一个这样的函数:F_{ *E* ，名词，first _ letter _ is _ capitalized }(*x*， *y* )。F_{ *E* ，名词，first _ letter _ is _ capitalized }(*x*， *y* )如果单词 *x* 以大写字母开头， *y* 为名词，则为 1，否则为 0。增加这个功能有什么帮助？

Say *x* 是在推理过程中看到的，但在我们的训练集中没有遇到的词。说 *x* 的第一个字母是大写的。在没有这个特征函数的情况下，我们最多只能说, *x* 和任何其他未知单词一样有可能是名词。有了这个特性，并且假设一个单词的首字母大写和它的 POS 标签是名词*之间确实存在关联，我们将能够预测 *x* 是一个更好的名词*。**

*我们可以认为这些特性的增加允许我们将领域知识整合到我们的推理引擎中。*

***推理效率***

*由于我们每个新添加的特征函数仅适用于(*单词*，*状态*)对，所以我们的推理中的最优性原则继续有效。*

*为了看到这一点，让我们首先回顾一下我们对 HMM 中推理效率的解释，这是通过词性标注的例子给出的。我们在下面逐字重复。*

*我们为*寻找一个最优标签序列的分数，狗吃了作业*。对于*作业*的每一个可能的标签 *t* ，让我们在*作业*必须从 *t* 发出的附加约束下，求*狗吃了*的最优解的分数。现在我们有了几个子问题的最优分数，每个值对应一个 *t* 。我们将这些分数乘以从 *t* 发出*作业*的概率。然后，在从 *t* 发出*作业*的附加约束下，每个结果分数对于整个句子都是最优的。取这些分数中的最大值，我们就得到一个去掉了这个约束的最佳分数。*

*即使在添加了我们的新特性函数之后，这个解释仍然有效。具体是因为新增的特征函数适用于(*字*、*态*)对的约束。*

*有可能稍微放松我们对新特征函数的限制，并且仍然保持最优性原则。*

***自动发现新功能***

*将我们自己限制在适用于( *word* ， *state* )对的特征函数在另一方面是有用的。我们可以使用监督学习来自动发现这种类型的有效特征函数。这种发现所需的训练集是一组(*单词*，POS-tag)对。*字*被输入；贴上标签。标签可能会有噪音，这意味着一个单词的标签不一定是唯一的。(如果标签是唯一的，词性标注问题本身就不需要结构化预测。)*

*从输入的单词中，我们可以选择提取任何我们认为可以预测其标签的特征。比如第一个字母是否大写，单词的最后 *k* 个字母是什么，对于 *k* = 1，2，3，…单词的第一个 *k* 个字母是什么，对于 *k* = 1，2，3，…*

*在这个问题的标签数据集上运行监督学习算法，我们可以找到高度相关的对( *f* ( *x* )， *y* )。这里的 *f* ( *x* )是单词 *x* 的一个特定特征。(即使是一个简单的监督学习算法，一个明确学习*P*(*y*|*f*(*x*))的算法也可能是有效的。每个这样的对( *f* ( *x* )， *y* )定义了我们发现的关节特征函数。*

***学习得分函数的权重***

*到目前为止，我们已经关注了推理的效率。这假设我们已经以某种方式了解了得分函数中的权重，该权重决定了各种特征函数对总得分的贡献。我们现在解决这个问题。*

*我们将在我们正在运行的词性标注示例中说明这一点。假设我们有一组可用的(*单词*、*标签*)对。这里的*单词*表示一系列单词。如在一个句子、一个短语甚至整个文档中。标签表示相应的位置标签序列。*

*下面是这种训练集中的一对示例。*

```
***words** The dog  ate  the homework
**tags** Det Noun Verb Det Noun*
```

*学习权重的一种方法是遵循 HMM 方法。也就是说，用概率表示它们，并直接从训练集中估计这些概率。*

*考虑一下 *w* _{ *E* ，Det，The}我们表示为 log *E* (The|Det)。量 *E* (The|Det)是从状态 Det 发出的概率。这就是训练集中来自状态 *Det* 的单词*的发射次数除以来自状态 Det 的发射总数。类似地，转换权重 *w* _{ *T* ，Det，Noun}是 log *T* (Noun|Det)，其中 *T* 是从状态 *Det* 转换到状态 *Noun* 的概率。这就是从状态 *Det* 到状态*名词*的转换次数除以训练集中状态 *Det* 的出现次数。**

*我们可以通过添加伪计数来稍微平滑这些估计。这个想法是要涵盖在训练集中从未出现过的概率略大于 0 的事件。*

***结构化感知器***

*一种更精细的学习方法包括错误驱动学习。我们从一个初始模型开始，如下迭代改进它。*

*— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -*

1.  *呈现*字**
2.  *从当前模型预测*预测标签**
3.  *调整模型以使预测的标签更好地与(目标)*标签*对齐。*

*— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -*

*错误驱动学习，也称为判别学习，可能产生比上述概率学习更准确的模型。*

*步骤 2 可以使用维特比算法来完成。*

*为了解释步骤 3，即进行辨别学习的步骤，形式符号将是方便的。当输入*字*时，让 **t_pred** 表示应用于当前模型的维特比算法产生的标签序列，让 **t_target** 表示*标签*，标签的目标序列。*

*到目前为止，在我们运行的例子中，我们的特征函数的形式是 F_{ *T* ，curr_tag，next_tag}( *t* 1，t2)或 F_{ *E* ，word，tag}( *w* ， *t* )。这里 curr_tag、next_tag、tag 表示特定的标签， *word* 表示字典中的特定单词， *w* 表示单词变量， *t* 1、 *t* 2、 *t* 表示标签变量。*

*在呈现鉴别学习时，我们将如下表达特征函数。设 F_ *i* ( **w** ， **t** )表示第 *i* 个特征函数。这里的 **w** 是一个单词序列，而 **t** 是一个相应的标签序列。我们这样做是为了揭示这种辨别学习的更一般的性质(见下文)。事实证明，这种符号也更简洁。*

*对于每个 *i* ，我们将在中更新重量*，如下所示:**

```
**wi* += a*(Fi(**w**, **t_target**)-Fi(**w**, **t_pred**))                         (SP1*
```

*这里的 *a* 是一个小的正常数，称为学习率。*

*等式(SP1)用更新权重*，试图使 **t_pred** 向 **t_target** 移动。让我们在运行的示例中的一个特定场景中说明这一点。说一下，对于某个 *j* ， *wj* 是*啃过的*， *tj* _target 是*动词*。假设在 **w** 上运行维特比算法，当前模型上的完整单词序列(其中仅显示了 *wj* )产生 *tj* _pred 为*名词*。**

*考虑特征函数 2*F_{ *E* ，*ed，Verb}( *w* ， *t* )-1。它的值对于(*嚼过的*，动词)为 1，对于(*嚼过的*，*名词*)为-1。所以我们会增加 w_{ *E* ，*ed，Verb}的值。如果 Viterbi 算法推断出除了动词*之外的任何标签*是发生*咀嚼*的标签，这将更加不利于 Viterbi 算法。*

***总结***

*在这篇文章中，我们介绍了结构预测的问题，并讨论了常见的用例。接下来，我们讨论了联合特征函数的重要概念，以及它如何概括 HMM 中的关键计算。接下来，我们讨论了 HMM 中有效推理的原因和方式。由此，我们开始讨论在联合特征函数的更一般的设置中的有效推理。最后，我们引入判别学习来训练联合特征函数的权值。这在结构化感知器中达到了顶峰:在更一般的联合特征函数设置中，混合了有效的推理和鉴别学习。简而言之，相对于 HMM，我们获得了更丰富的表示(联合特征函数)和更准确的学习，同时保持了推理的效率。*

***延伸阅读***

1.  *[使用递归神经网络进行序列间标记| Arun ja gota |走向数据科学](/sequence-to-sequence-labeling-with-recurrent-neural-networks-5405716a2ffa)*
2.  *[结构化预测](http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Hoffman_1_2.pdf)*
3.  *[自然语言处理中的命名实体识别。真实世界的用例、模型、方法…](/named-entity-recognition-in-nlp-be09139fa7b8)*
4.  *[结构化预测—维基百科](https://en.wikipedia.org/wiki/Structured_prediction)*
5.  *[http://www . phontron . com/slides/NLP-programming-en-12-struct . pdf](http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf)*
6.  *[https://svivek . com/teaching/讲义/幻灯片/结构化感知器/struct-perceptron.pdf](https://svivek.com/teaching/lectures/slides/structured-perceptron/struct-perceptron.pdf)*