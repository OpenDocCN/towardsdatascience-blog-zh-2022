<html>
<head>
<title>Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-ac90b73f68f5#2022-01-04">https://towardsdatascience.com/principal-component-analysis-ac90b73f68f5#2022-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="fbff" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">主成分分析</h1></div><p id="fbc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">机器学习(ML)建模包括在考虑的数据中寻找模式。在监督学习中，模型通过标记数据学习模式；也就是说，提供的数据有自变量和因变量。基于领域，独立变量可能有其他名称，如<em class="ko">解释变量、预测变量、回归变量、协变量、特征(在机器学习和模式识别中)、输入或控制</em>。另一方面，因变量也被称为<em class="ko">目标、标签、响应、预测、输出、回归、结果、解释或测量。</em></p><p id="ca91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通常，有几个特性，只有一个目标。根据数据的可用性和手头任务的性质，独立变量和数据点的数量因数据集而异。一个有很多自变量的数据集被称为<strong class="js iu">高维</strong>。虽然数据可用性是一件好事，但高维数据会直接影响ML模型的性能，并带来数据存储需求的问题。</p><p id="2ae4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">减少高维数据中的特征数量称为<strong class="js iu">降维</strong>。降维的目的是:</p><ul class=""><li id="79aa" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">移除数据集中的多重共线性。</li><li id="4923" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">减少数据所需的存储空间。</li><li id="f5d6" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">轻松可视化。</li><li id="a635" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">降低拟合和测试模型所需的计算能力，因为很少使用低维数据执行操作。</li><li id="f081" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">防止过度拟合。</li><li id="39ca" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">避免维数灾难——与高维数据相关的一系列问题。</li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="472d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">降维有两种方式:<strong class="js iu">特征消除或特征提取</strong>。</p><p id="6007" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">(一)。特征消除</strong>包括识别不重要的独立变量并将其移除。比如两个高度相关的变量，去掉一个就行了。</p><p id="f357" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">不足</strong>:特征消除是指完全移除变量，使得被移除变量中的少量信息完全丢失。</p><p id="4884" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> (b)。特征提取</strong> —这项技术需要从原始特征中构建一个新变量。新功能旨在提供信息(最大限度地减少信息损失)和非冗余(没有不重要的变量)。<strong class="js iu">主成分分析</strong>是最流行的特征提取方法。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="3174" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了理解PCA，让我们看一些线性代数和统计概念，即，</p><ul class=""><li id="88de" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">方差和协方差</li><li id="fe85" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">特征值和特征向量</li></ul><h1 id="9563" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">方差和协方差</h1><p id="c7aa" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">方差衡量给定变量中数据围绕其均值的分布。变量<code class="fe mn mo mp mq b">X</code>的方差由下式给出</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4c04a622097cc0ca37f8e44fd7934ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*eStsAdmAdQKJApCCdSKOyw.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">变量x的方差。</p></figure><p id="64d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与测量一维分布的方差不同，协方差测量两个变量相对于彼此的平均值的偏差。换句话说，协方差衡量两个维度之间的关系。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/962f16d2c44ef3caae4d8a93a09de421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*6h1Z-ekt3uFTc0hhd75_3w.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">X和Y的协方差</p></figure><p id="01ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注:</strong>一个变量与其自身之间的协方差等于该变量的方差，即<code class="fe mn mo mp mq b">cov(x, x)=var(x)</code>。</p><h2 id="b7ca" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">协方差值的解释</h2><p id="12b9" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">协方差值的符号是解释的组成部分。正值表示两个变量之间的直接关系；也就是说，这两个变量同时增加或减少。另一方面，负协方差值表示变量之间的负关系，即一个变量增加，另一个变量减少，反之亦然。协方差为零表示不存在任何关系；也就是说，这两个变量是独立的。<br/> <br/>协方差值的大小(多大或多小)并不重要。它没有解释所考虑的两个变量之间关系的强度。这是因为量值受测量单位的影响。要衡量关系的强度，请使用相关系数。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bdbfb8a3b591c7af8da22118b4f86a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*ZmgaRzRZacmWWc1octlraQ.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">X和Y的相关性</p></figure><p id="caf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意:</strong>相关性只是协方差的一种缩放形式。这意味着缩放数据的协方差等于原始数据的相关性。</p><p id="745e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于低维数据(2和3个特征)，可视化更容易，并且可以绘制数据(例如，在散点图上)，并解释关系。然而，在高维数据上，可视化变得复杂，这就是协方差计算变得重要的地方。</p><h1 id="f4cd" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">协方差矩阵</h1><p id="f760" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">对于m维数据，协方差是针对变量和<strong class="js iu">协方差矩阵中呈现的值的m×m组合计算的。</strong> <br/>对于m维数据，可以对协方差矩阵中呈现的变量和值的m×m个组合计算协方差。例如，对于X、Y和Z变量的m=3，协方差矩阵将表示如下。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/077d2d0345c5e62339c351c58eca962e.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*83itHxHQShwl4Tq5BiyQ-w.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><code class="fe mn mo mp mq b">3×3 covariance matrix for X, Y and Z variables.</code></p></figure><h2 id="1b96" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">此矩阵的属性</h2><ul class=""><li id="fe6d" class="kp kq it js b jt mi jx mj kb ns kf nt kj nu kn ku kv kw kx bi translated">协方差矩阵沿主对角线对称。这是be- <br/>原因，对于任意两个变量<code class="fe mn mo mp mq b">X</code>和<code class="fe mn mo mp mq b">Y</code>、<code class="fe mn mo mp mq b">cov(x, y) = cov(y, x)</code>。</li><li id="fb89" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">变量的方差在主对角线上。记得<br/>我们说<code class="fe mn mo mp mq b">cov(y, y) = var(y)</code>为变量<code class="fe mn mo mp mq b">Y</code>。</li><li id="0efa" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">对于<code class="fe mn mo mp mq b">m</code>维数据，构建<code class="fe mn mo mp mq b">m × m</code>协方差矩阵。</li></ul><h1 id="6362" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">特征值和特征向量</h1><p id="1ba2" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">设<code class="fe mn mo mp mq b">A</code>是一个<code class="fe mn mo mp mq b">m × m</code>矩阵，使得</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b1246caee8364112d8a81b3a10168ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*PZ5vd2pF__7YQE-wiBmyww.png"/></div></figure><p id="e5c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么<code class="fe mn mo mp mq b">λ</code>是<code class="fe mn mo mp mq b"><strong class="js iu">A</strong></code>的特征值，<code class="fe mn mo mp mq b"><strong class="js iu">v</strong></code>是与<code class="fe mn mo mp mq b">λ</code>相关联的<code class="fe mn mo mp mq b"><strong class="js iu">A</strong></code>的特征向量。<code class="fe mn mo mp mq b">v</code>是非零向量。</p><p id="92fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注1 </strong>:矩阵<code class="fe mn mo mp mq b"><strong class="js iu">A</strong></code>必须是正方形。<br/> <strong class="js iu">注2 </strong>:特征向量根据定义是非零的。</p><p id="4cce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关系<code class="fe mn mo mp mq b"><strong class="js iu">Av</strong> = λ<strong class="js iu">v</strong></code>意味着<code class="fe mn mo mp mq b"><strong class="js iu">Av</strong></code>和<code class="fe mn mo mp mq b">λ<strong class="js iu">v</strong></code>与原点共线。那<br/>就是，原点的一个非零向量<code class="fe mn mo mp mq b"><strong class="js iu">v</strong></code>经过<code class="fe mn mo mp mq b"><strong class="js iu">Av</strong></code>，如下图所示。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f25a0ce7e64f4ec00deacd571bc95d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*UYhvl2h_ujkkkDkTP5U9Tg.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated"><strong class="bd nx"> v </strong>对于特征值λ是<strong class="bd nx"> A </strong>的一个特征向量但是<strong class="bd nx"> z </strong>不是因为<br/>找不到对应的特征值。</p></figure><p id="c981" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定一个<code class="fe mn mo mp mq b">m × m</code>矩阵<code class="fe mn mo mp mq b"><strong class="js iu">A</strong></code>，那么我们可以通过公式找到特征值<code class="fe mn mo mp mq b">λ</code>和特征向量<code class="fe mn mo mp mq b">v</code></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/cffc14c6d435ac66242215650e88cf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*JdkQo6OAjk4mKk6X5J9z1g.png"/></div></figure><p id="7380" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mn mo mp mq b"><strong class="js iu">I</strong></code>是<code class="fe mn mo mp mq b">m × m</code>单位矩阵，<code class="fe mn mo mp mq b">det(<strong class="js iu">B</strong>)</code>是矩阵<code class="fe mn mo mp mq b">B</code>的行列式。</p><h2 id="0637" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated"><strong class="ak"> <em class="nz">举例(求一个</em> </strong> <code class="fe mn mo mp mq b"><strong class="ak">2 × 2</strong></code> <strong class="ak">矩阵的特征值和特征向量)</strong></h2><p id="9019" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">在这个例子中，我们感兴趣的是寻找<code class="fe mn mo mp mq b">2 × 2</code>矩阵<code class="fe mn mo mp mq b"><strong class="js iu">A</strong></code>的特征值和特征向量。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/07c2109a2fe0e2a5b5634358bbb26610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWX3w5jyBQu7qlnrjBN9hw.png"/></div></div></figure><p id="0464" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们得到了两个特征值。接下来的事情就是找到这些特征值对应的特征向量<br/>。我们将使用关系<code class="fe mn mo mp mq b"><strong class="js iu">Av</strong> = λ<strong class="js iu">v</strong></code>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/50d3cd7a93b94789ac8468b3f07d052f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*4YX-duHP2O-jkOGLuBhjRQ.png"/></div></figure><h2 id="5a5d" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">特征值和特征向量的性质</h2><ul class=""><li id="2746" class="kp kq it js b jt mi jx mj kb ns kf nt kj nu kn ku kv kw kx bi translated"><strong class="js iu">只能对方阵</strong>计算特征向量。我的意思是，我们在它的计算中使用行列式，行列式只能对方阵进行计算。</li><li id="ae2b" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated"><strong class="js iu">不是所有的方阵都有特征向量。</strong></li><li id="7641" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated"><strong class="js iu">关键性质:</strong> <strong class="js iu">一个对称矩阵的所有特征向量都是正交的</strong>，即垂直/独立。无论维数多少都会发生这种情况(惊讶？我也是)。这个性质是关键的，因为协方差矩阵是对称的，这意味着，通过这个性质，通过PCA产生的新变量将是独立的(这将很快清楚)。</li></ul><p id="5bdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以继续讨论PCA了。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi og"><img src="../Images/93b7a0c7b42b88f205cc0a5f1d89939d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjJwqCtnoUL75FyfmeQ79g.jpeg"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">Stephen Dawson 在<a class="ae oh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="b3c1" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">主成分分析</h1><p id="3ebc" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">对于有<code class="fe mn mo mp mq b">m</code>个独立变量的数据集，PCA将在下面的<br/>步骤中实现:</p><h2 id="7458" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">步骤1:数据标准化</h2><p id="93fc" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">这包括将数据转换为所有独立<br/>变量的统一尺度，以便在计算方差等统计数据时进行公平比较。让我解释一下。如果不进行标准化，范围大的变量将支配范围小的变量。例如，如果我们有两个<br/>变量，一个范围为200–1000，另一个范围为0–10，<br/>，那么前者将支配后者，导致有偏分析。<br/>数学上，变量<code class="fe mn mo mp mq b">X</code>中原始值<code class="fe mn mo mp mq b">x</code>的标准分数<br/>计算如下。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/1ff4b9a4c9634b5762f059d2084f2e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*sFaCN8ewRIbIrAGspXB63A.png"/></div></figure><p id="ba0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mn mo mp mq b">x̄</code> (x_bar)是平均值，<code class="fe mn mo mp mq b">S</code>是<code class="fe mn mo mp mq b">X</code>的标准差。标准差就是前面解释的方差的平方根。标准<br/>分数<code class="fe mn mo mp mq b">z</code>正态分布，平均值为<code class="fe mn mo mp mq b">0</code>，标准差为<code class="fe mn mo mp mq b">1</code>。</p><h2 id="5c98" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">步骤2:为标准化数据构建协方差矩阵</h2><p id="af68" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">如前所述，协方差矩阵将解释数据中任意两个要素之间的关系。此过程用于共线变量。正协方差值表示直接关系(两个变量都增加或减少)。负协方差意味着属性之间的反向关系(一个变量增加，而另一个变量减少，反之亦然)。</p><p id="3a77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如前所述，协方差矩阵是对称的，意味着生成的特征向量是正交的，因此，最终导出的特征是独立的。</p><p id="67d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一步相当于计算原始未标准化数据的相关矩阵。</p><h2 id="bf01" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">步骤3:计算协方差矩阵的特征值λ和特征向量。</h2><p id="907f" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated"><code class="fe mn mo mp mq b"><strong class="js iu">λ</strong> = [λ1 , λ2 , · · · , λm ]</code>是特征值列表，<code class="fe mn mo mp mq b"><strong class="js iu">V</strong></code>是矩阵，第一列是对应于<code class="fe mn mo mp mq b">λ1</code>的特征向量，第二列是对应于<code class="fe mn mo mp mq b">λ2</code>的特征向量，依此类推。</p><h2 id="e16e" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">步骤4:决定要保留的特征数量。</h2><p id="3a89" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">在这一步中，您将决定我们需要保留的要素数量并生成新要素。可以通过以下方式决定要保留的功能:</p><ul class=""><li id="8719" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated"><em class="ko">任意选择</em>。例如，如果您对<br/>可视化的易用性感兴趣，那么您可以使用2或3个特性来运行它。</li><li id="a054" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated"><em class="ko">使用变化百分比解释</em>。</li></ul><p id="bccf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于这些方法中的任何一种，首先按降序对<code class="fe mn mo mp mq b">λ</code>中的特征值进行排序。<br/>第一个<code class="fe mn mo mp mq b">r</code>主成分解释的方差比例定义如下</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oj"><img src="../Images/c057f0f4a1561298292627a38e3317ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mMAphggsn7osvTwmLJJZ9A.png"/></div></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="0134" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果原始数据上的特征高度相关，我们需要很少的<br/>主成分，即<code class="fe mn mo mp mq b">r</code>比<code class="fe mn mo mp mq b">m</code>小得多。另一方面，如果特征不相关，<code class="fe mn mo mp mq b">r</code>接近<code class="fe mn mo mp mq b">m</code>并且PCA在特征提取中失去其<br/>重要性。</p><h2 id="9c91" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">第五步:衍生新特征。</h2><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ac091bac1f94c6df69d8f59ff6b725d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*8n5l4is_N3GMuT_mUvpx-w.png"/></div></figure><p id="ec5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mn mo mp mq b">X</code>是原始缩放特征数据，而<code class="fe mn mo mp mq b">V^T</code>是基于在步骤<code class="fe mn mo mp mq b">(iv)</code>中做出的选择的特征向量的<br/>矩阵的转置。这一步是线性代数 中<strong class="js iu"> <em class="ko">变基的一个概念。如果你想更深入地了解为什么会这样，也许你可以查一下。</em></strong></p><h1 id="37a2" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">PCA实施示例</h1><p id="933b" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">考虑具有3个特征<code class="fe mn mo mp mq b"> x1</code>、<code class="fe mn mo mp mq b">x2</code>和<code class="fe mn mo mp mq b">x3</code>以及10条记录的数据集。</p><p id="5b7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注:</strong>在<code class="fe mn mo mp mq b"><a class="ae oh" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">sklearn.decomposition.PCA</a></code>中有一个函数可以用来以更直接的方式实现PCA。我们将在本文后面使用它，但现在我们将使用其他Python包(如numpy)一步一步地实现PCA。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段1:这个代码片段为原始数据生成一个pandas数据帧。我们只是用熊猫来以表格的形式显示数据。我的意思是，我们更经常看到表格数据。</p></figure><p id="8f17" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">实现PCA的第一步是缩放数据。这是通过使用代码<code class="fe mn mo mp mq b">snippet 2 below</code>和显示在<code class="fe mn mo mp mq b">Figure 2 below</code>中的结果完成的</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段2:使用前面讨论的公式和zscore缩放数据集。</p></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi on"><img src="../Images/add927b2121b8aa96edb0cb920869ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6c3pZPqt56p-XN7uczlrA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图2:原始数据和缩放数据。</p></figure><p id="73bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">第二步:计算协方差矩阵</em></p><p id="4025" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一步，我们计算协方差矩阵。这个矩阵用于建立变量之间的关系。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段3:协方差矩阵的计算</p></figure><p id="d2b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的输出显示<code class="fe mn mo mp mq b">x1s</code>和<code class="fe mn mo mp mq b">x2s</code>为强正相关<code class="fe mn mo mp mq b">(0.96)</code>和<code class="fe mn mo mp mq b">cov(x1s, x3s)=0.82</code>和<code class="fe mn mo mp mq b">cov(x2s, x3s)=0.89</code> ( <em class="ko">不要被这里使用的相关性混淆。缩放数据上的协方差只是相关性</em>)。</p><pre class="ms mt mu mv gt oo mq op oq aw or bi"><span id="0c4b" class="ne ll it mq b gy os ot l ou ov">array([[1.        , 0.96240631, 0.82362337],<br/>       [0.96240631, 1.        , 0.89326123],<br/>       [0.82362337, 0.89326123, 1.        ]])</span></pre><p id="ae21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">步骤3:协方差矩阵的特征值和特征向量</em></p><p id="9c39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这一步中，我们要生成主成分(特征值)，并建立每个解释的变化比例。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段4:特征值和特征向量</p></figure><p id="1d97" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出(分类的特征值、分类的特征向量和分别由每个主成分解释的方差比例)</p><pre class="ms mt mu mv gt oo mq op oq aw or bi"><span id="7323" class="ne ll it mq b gy os ot l ou ov">sorted_eigenvalues: <br/>[2.7873923775066234, 0.1864183595384753, 0.026189262954900528]</span><span id="31f8" class="ne ll it mq b gy ow ot l ou ov">sorted_eigenvectors:<br/>[array([-0.57770188, -0.59193503, -0.56202621]),<br/> array([-0.57125709, -0.19861728,  0.79637712]),<br/> array([-0.58303163,  0.78113002, -0.22340551])]</span><span id="4995" class="ne ll it mq b gy ow ot l ou ov">variance_explained:<br/>[0.9291307925022081, 0.06213945317949179, 0.008729754318300179]</span></pre><p id="c601" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一个主成分解释了原始数据变化的<code class="fe mn mo mp mq b">92.9%</code>，第二个主成分解释了<code class="fe mn mo mp mq b">6.2%</code>，第三个主成分解释了<code class="fe mn mo mp mq b">0.9%</code>。在这一点上，你现在可以决定主成分的数量。如果你选择前两个，那么它们将解释总共<code class="fe mn mo mp mq b">99.1%</code>个变量。以下两个图显示了由单个组件和累积解释的变化。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ox"><img src="../Images/04962b7397a68ca7c5763070855121c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9z4SbEmOsz_w7h-ldcFHQA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图3:左—各主成分解释的方差比例，右—解释的累积比例。</p></figure><p id="e38c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">第四步:生成新数据</em></p><p id="41ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后一步是通过等式<code class="fe mn mo mp mq b">new _ data = np.dot(X,V.T)</code>生成新数据，其中<code class="fe mn mo mp mq b">X</code>是原始缩放数据，而<code class="fe mn mo mp mq b">V.T</code>是根据特征值排序后的特征向量矩阵的转置。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段5:用一个特性生成新数据。当然，您可以选择生成2个甚至3个特征的新数据。</p></figure><p id="9adb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出(z1)</p><pre class="ms mt mu mv gt oo mq op oq aw or bi"><span id="c283" class="ne ll it mq b gy os ot l ou ov">[-2.1859,<br/> -2.967,<br/> 1.3283,<br/> 1.3625,<br/> -0.2836,<br/> -0.4782,<br/> 1.8748,<br/> 1.7916,<br/> 0.3921,<br/> -0.8345]</span></pre><h2 id="3068" class="ne ll it bd lm nf ng dn lq nh ni dp lu kb nj nk ly kf nl nm mc kj nn no mg np bi translated">使用<code class="fe mn mo mp mq b">sklearn.decomposition</code>实现PCA。主成分分析</h2><p id="8747" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">使用<code class="fe mn mo mp mq b"><a class="ae oh" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">sklearn.decomposition.PCA</a></code>功能可将上述所有步骤浓缩如下。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="ol om l"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">代码片段6:使用sklearn实现PCA。</p></figure><p id="9d5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出(2列PCA生成数据的前5行):</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2ec6870d0606e23430fb2ca783766638.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*jcwgJD10aQpex_c3Ht_bIw.png"/></div></figure></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="deb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PCA是一个很好的特征提取工具，但是也有一些缺点。以下是一些例子:</p><ul class=""><li id="101b" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">PCA生成的特征可解释性较差。这些特征是原始特征的组合，因此我们无法清楚地解释原始特征如何影响因变量。</li><li id="7933" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">在降维过程中，PCA会导致一定程度的数据丢失。然而，这种损失比使用特征消除方法要好。</li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="798b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是结尾，我希望这篇文章能让你明白这个概念。</p><p id="3000" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读:-)</p></div></div>    
</body>
</html>