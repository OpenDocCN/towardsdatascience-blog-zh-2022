# 强化学习的精彩世界

> 原文：<https://towardsdatascience.com/exciting-world-of-reinforcement-learning-6dcb9a54cb21>

## 消费者企业的案例

![](img/26c74294982c13221030bbc98eed8bfd.png)

Preethi Viswanathan 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

自从我对强化学习领域及其在行业中的众多应用感到好奇和着迷以来，我对该领域的兴奋与日俱增。在这里，我想分享一些我对强化学习(RL)在消费者业务中的潜在应用的了解。但是，在我深入细节之前，先为初涉这一主题的 ML 从业者快速介绍一下 RL。

RL 是机器学习的一个分支，涉及智能代理的培训，它可以通过在环境中反复试验来学习执行目标，在培训结束时，我们有一个代理可以在现实生活中独立执行目标。现在，如果你熟悉其他类型的 ML——监督和非监督学习技术——这听起来可能非常类似于监督学习方法。但是两者之间的最大区别(以及其他区别)是 RL 不需要提供任何明确的标签，这与监督学习技术不同。要了解更多细节和背景，你可以阅读一些关于 RL 的博客/文章。(TDS/medium 上也有几个。)你还可以查阅一些由 [Deepmind](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch) 、 [OpenAI](https://openai.com/blog/) 所做的开创性工作，以了解这些年来的成就，也可以阅读理查德·萨顿&安德鲁·g·巴尔托所著的《强化学习——简介》一书，以了解强化学习领域是如何产生的。

在众多超级令人兴奋的 RL 应用中，我的搜索集中在消费者业务的个性化用例上。虽然我的用例集中在媒体和出版行业，但这可以很容易地扩展到其他行业，如电子零售商、旅游/酒店等。最后，我们将看看可以帮助完成这些用例的 RL 解决方案的大致轮廓。

a)新闻快递个性化——任何媒体和出版公司的主要流量来源之一是通过新闻快递。我们经常会遇到这样的情况:不管我们想什么时候阅读，我们最喜欢的日报/周报和杂志的简讯都会在同一时间到达我们的手中。换句话说，简讯在一周的同一时间/同一天发给所有用户并不罕见。现在，在当前的数字化时代，情况未必如此。理想的解决方案是在用户很可能打开它的时候发送它。RL 可用于在每个用户的最佳时间发送电子邮件，为读者带来个性化体验。

![](img/f011adbcbb28476e0a87aa4be66f646f.png)

马库斯·温克勒在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

b) NL 容量识别——其次，营销人员经常遇到的另一个问题是识别发送给订户的 NL 的最佳数量。“每个用户发多少封邮件算多？”众所周知，NL 偏好因用户而异，并且不总是相同的。然而，我们习惯于始终向所有订户发送相同数量的电子邮件。我承认，没有简单的方法可以为每个用户动态地确定这个神奇的数字。但是，通过应用 RL 技术，这是一个可以解决的问题。

c)个性化盒子订阅——盒子订阅是这样设计的订阅产品，即一期订阅由某类产品组成。美容盒的月订阅将包含美容产品的随机分类，例如面部、皮肤、头发等产品。下个月的问题可能是一个完全不同的产品分类。请注意，在这种模式下，订阅者没有选择他/她想要的产品的权利，从用户那里得到的唯一反馈是更新订阅的用户。这个问题的主要挑战是确定正确的产品组合，以最大限度地保留我们的用户。

将这个问题公式化为 RL 问题，我们可以为每个订阅问题确定最个性化的最佳分类，同时最大化订户的保留。

![](img/cc39ae2690195ac81b2d666656cc8fc0.png)

图片由威斯康星大麻科学于 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 由[批出](https://unsplash.com/@batch_by_whs?utm_source=medium&utm_medium=referral)

d)动态付费墙计量——在数字媒体和出版行业，出版商需要做出的一个关键决策是，在通过允许用户免费阅读文章提供广告来获取收入和通过阻止数字付费墙免费访问(在一定数量的免费文章后)诱导用户订阅来获取收入之间进行权衡。付费墙的号召可以是订阅，也可以是让读者注册以便继续阅读。通常情况下，付费墙的收费标准是所有用户每月 2/4/6 篇免费文章。

![](img/9c4d984bd635e29d6926eb4188de4820.png)

安妮·斯普拉特在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

但是这种实现并不是最佳的解决方案，因为一个品牌的忠实读者会继续阅读更多的文章，通过更多的广告收入做出贡献，并且将该用户的读者群减少到每月只有 4 篇文章，这是过早地从该用户中减少了潜在的更多广告收入。理想情况下，我们可以为这样的用户带来 5 月后，每月 6-7 篇文章的付费墙。另一方面，不太可能回来阅读第二篇文章的参与度较低的用户不需要有 4 篇文章的限制，因为该用户不太可能通过广告产生任何收入，所以我们甚至可以在该用户第二次访问时为其设置付费墙，并推动该用户订阅。

RL 可以学习每个用户的阅读模式，并推荐一个最佳的付费墙限制，以最大化每个用户的收入潜力，而不是为每个用户设置这样的手动规则。不仅如此，它的学习会随着时间的推移适应每个用户不断变化的阅读行为，并自动调整付费墙的限制，以最大限度地提高企业的收入潜力。

现在我们已经看了用例，让我先睹为快，看看其中一个用例的 RL 解决方案设计。我们将使用一种叫做 DQN(深度 Q 网络)的 RL 算法来解决这个问题，这是深度学习和 Q 学习原理的结合。我认为大多数 ML 从业者都熟悉深度学习。Q-learning 是一类称为表格解决方案的 RL 解决方案的算法，旨在学习每个状态的 q 值。(一个州的 Q 值是代理人未来可能去的所有州的累积(贴现)奖励)。对于具有有限状态空间的问题，例如[冰封湖](https://gym.openai.com/envs/FrozenLake-v0/)问题，这是一个很好的解决方案。然而，对于更大的状态空间，这种解决方案变得笨拙，我们需要采用一种近似的方法来估计状态值，这类解决方案被称为“近似方法”。DQN 算法是近似方法中最流行的算法。

在 DQN，深度学习网络作为一个函数近似值，估计给定状态/(动作)的值。所有用例的解决方案设计、算法和设置都是相同的，但是 MDP(马尔可夫决策过程)的配置(状态空间、奖励和要采取的行动)会因每个用例而异。

用例 a)的 MDP 配置如下。

> 状态—过去半个月的 NL 打开/点击模式。
> 
> 行动—一天中的 1-24 小时。这可以进一步减少到 12 个动作值，每个动作代表可以发送电子邮件的 2 小时时间段。
> 
> 奖励——本地点击+2，本地打开+1，否则为 0

我还分享了一篇[媒体博客文章](/deep-reinforcement-learning-in-production-part-2-personalizing-user-notifications-812a68ce2355)的链接(作者是 Zynga ML 工程团队的[迈赫迪·本·阿耶德](https://www.linkedin.com/in/mehdi-ben-ayed/)和[帕特里克·哈利娜](https://www.linkedin.com/in/patrick-halina-88aa463a/)),解释了他们如何解决定制应用程序通知的问题。这是验证这些用例的 DQN 解决方案/方法的非常有用的参考来源和动机。

我希望您发现上面的应用程序 RL 的业务用例是有见地和有用的。在这篇文章之后，我想介绍一些广告/商业业务的使用案例和一些学习资源，我发现这些资源对于在我的团队中建立 RL 能力很有用。