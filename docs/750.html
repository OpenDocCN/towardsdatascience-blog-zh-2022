<html>
<head>
<title>An Introduction Lasso and Ridge Regression using scitkit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用scitkit-learn介绍套索和岭回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-lasso-and-ridge-regression-using-scitkit-learn-d3427700679c#2022-01-26">https://towardsdatascience.com/an-introduction-lasso-and-ridge-regression-using-scitkit-learn-d3427700679c#2022-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="fa43" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用scitkit-learn介绍套索和岭回归</h1></div><div class=""><h2 id="3ecf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">UCL数据科学学会12a研讨会:偏差-方差权衡、套索实施、山脊实施及其差异</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/163334455b2c89bf80df00485536728e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p6OYEhieBFZpr0yf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迈克·考克斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5f1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今年，作为UCL数据科学学会的科学负责人，该学会将在整个学年举办一系列20场研讨会，涵盖的主题包括数据科学家工具包Python和机器学习方法简介。每个人的目标是创建一系列的小博客文章，这些文章将概述主要观点，并为任何希望跟进的人提供完整研讨会的链接。所有这些都可以在我们的<a class="ae ky" href="https://github.com/UCL-DSS" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中找到，并将在全年更新新的研讨会和挑战。</p><p id="bc7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第十二次研讨会是对高级回归方法的介绍，包括Lasso和Ridge回归的理解和实现。虽然亮点将在这篇博文中呈现，但完整的研讨会可以在我们的GitHub账户<a class="ae ky" href="https://github.com/UCL-DSS/advanced-regression" rel="noopener ugc nofollow" target="_blank">这里找到。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="36af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了之前的任何研讨会，可以在这里找到:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/introduction-to-logistic-regression-predicting-diabetes-bc3a88f1e60e"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">逻辑回归简介:预测糖尿病</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会研讨会11:什么是逻辑回归、数据探索、实施和评估</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/linear-regression-in-python-for-data-scientists-16caef003012"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的Python线性回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会研讨会10:什么是线性回归，数据探索，Scikit学习实施和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-sql-for-data-scientists-e3bb539decdf"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">数据科学家的SQL介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会工作坊9:什么是SQL，选择数据，查询数据，汇总统计，分组数据和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="e914" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated"><strong class="ak">偏差与方差的权衡</strong></h2><p id="aa70" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">在建模阶段，有必要在您创建的模型中尽可能多地决定<a class="ae ky" rel="noopener" target="_blank" href="/bias-and-variance-for-machine-learning-in-3-minutes-4e5770e4bf1b">偏差和方差</a>的水平。当构建监督机器学习模型时，目标是实现最准确预测的低偏差和方差。这意味着在处理训练数据时，我们必须处理欠拟合和过拟合的问题。</p><p id="b3c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以认为偏差是模型的精确度(即与目标的接近程度)，而方差是预测与目标的相关程度。我们的目标是同时具有低偏差(即准确)和低方差(即始终准确)，但在训练模型时，这两者之间往往存在权衡。例如，在过度拟合的情况下，我们可能对训练数据非常准确，因此具有较低的偏差，但我们可能对看不见的数据具有较高的方差，因为它已经过度拟合。因此，两者之间存在一定程度的权衡，我们可以这样设想:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/4ccbe0b1e6c127b039523f224d68f10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gZpn9zQgNLviH09aI_VAnA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="16b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在建模中，我们试图在新数据集上实现尽可能低的验证误差的中间点，而不会过度接近与过度拟合相关联的低训练误差。就回归而言，可以对此进行控制的两个模型是Lasso和Ridge回归，如下所示。</p><h2 id="64cc" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">套索回归</h2><p id="57e6" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">Lasso regression是<strong class="lb iu">最小绝对收缩和选择运算符</strong>的缩写，是一种执行方差选择和正则化的回归方法，能够提高回归模型的预测准确性和可解释性。</p><p id="dd6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">早期的线性<a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-in-python-for-data-scientists-16caef003012">回归研讨会</a>展示了什么是方差选择，在这种情况下，我们根据变量之间的相关性或它们在回归中缺乏显著性来移除变量。这样做的目的是通过仅使用对目标变量重要的变量来提高模型的有效性，并从模型中去除噪声。因此，这可以提高模型的有效性和效率。</p><p id="b350" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，正则化试图通过引入惩罚或约束来解决过度拟合的问题，以避免模型过度拟合训练数据。拉索回归能够做到这一点使用“L1正则化”。这里，将罚项添加到普通最小二乘回归目标(减少平方和)，使得回归目标变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c74a5b154e15d284af0e04614866ac82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DLyVTZzXW69In7Z9or_mA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="2bae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里引入的惩罚，由等式的第二部分给出，相当于系数绝对值的<strong class="lb iu">和</strong>。这意味着系数的子集被强制为零，充当自动特征选择。因此，只有最重要特征的子集在模型中具有非零权重，从而更容易解释。</p><p id="19fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型中这种正则化的程度由𝛼参数控制，这意味着我们可以控制欠拟合和过拟合之间的权衡，以找到一个舒适的中间地带。这是因为随着𝛼增加，它会降低系数值并减少方差(过拟合)，但是在某个点之后，模型开始失去对数据的重要理解，并导致偏差增加(导致欠拟合)。因此，应使用超参数调整或交叉验证技术仔细选择𝛼的值。</p><p id="6357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以以类似于基本回归的方式实现一个简单的套索回归。为此，我们使用了来自<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/energy+efficiency" rel="noopener ugc nofollow" target="_blank"> UCI机器学习数据库</a>的关于建筑能效的数据集。这在训练数据集上实现如下:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="100b" class="mw mx it nx b gy ob oc l od oe">#import the linear model class<br/>from sklearn import linear_model</span><span id="a29c" class="mw mx it nx b gy of oc l od oe">#initiate the model with an alpha = 0.1<br/>lasso_model = linear_model.Lasso(alpha = 0.1)</span><span id="07d5" class="mw mx it nx b gy of oc l od oe">#fit the model to the data<br/>lasso_model.fit(X=X_train, y=y_train)</span></pre><p id="e635" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里可以看到，实现lasso回归模型遵循了与实现线性回归模型几乎相同的语法。主要区别在于，我们必须为实现指定alpha值(默认值为0.1)，这会影响应用于系数的惩罚。</p><p id="abea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，如前所述，我们可以通过从模型实现中提取系数来检查模型中的系数，如下所示:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="5fbe" class="mw mx it nx b gy ob oc l od oe">#extract the coefficients<br/>df_coef_lasso = pd.DataFrame({"var": X_train.columns.values, <br/>                              "coef":lasso_model.coef_})</span><span id="95ad" class="mw mx it nx b gy of oc l od oe">#show the dataframe<br/>df_coef_lasso</span><span id="e106" class="mw mx it nx b gy of oc l od oe">#out:<br/>   var   coef<br/>0  X1    -0.000000<br/>1  X2    0.001935<br/>2  X3    0.050211<br/>3  X4    -0.000000<br/>4  X5    4.823518<br/>5  X6    -0.000000 <br/>6  X7    14.491866<br/>7  X8    0.291431</span></pre><p id="65f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此我们可以看到，这些系数似乎与原始回归系数略有不同，其中一些系数实际上已经减少到0。这意味着引入的惩罚意味着X1、X4和X6在回归方程中不再重要。</p><p id="4880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一点，即模型旨在降低模型的复杂性以避免过度拟合，我们可以检查模型在训练数据集上的得分:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="fd75" class="mw mx it nx b gy ob oc l od oe">lasso_model.score(X_train, y_train)</span><span id="3b92" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.9068673512124443</span></pre><p id="5a67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个分数低于原始模型，但是它已经设法用模型中更少的显式变量实现了这个分数。在某些情况下，由于降低了达到相同预测能力水平所需的模型复杂性，这可能导致改进的调整后R。</p><p id="83f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像以前一样，我们可以使用模型来预测测试数据集，并查看它在那里的表现:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9cc0" class="mw mx it nx b gy ob oc l od oe">#extract model predictions<br/>model_predictions = lasso_model.predict(X_test)</span><span id="e110" class="mw mx it nx b gy of oc l od oe">#R2 score on test data<br/>lasso_model.score(X=X_test, y=y_test)</span><span id="dbed" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.9036176174398927</span></pre><p id="657c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然在这种情况下，与线性回归模型相比，该模型没有提高测试数据的R，但我们可以开始看到，当我们在未来处理更复杂的数据集时，该模型可能会做得更好。当我们有彼此相关的独立方差或者我们有大量的独立变量要处理时，尤其如此。</p><p id="00b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一件重要的事情是，我们最初选择的<code class="fe og oh oi nx b">alpha = 0.1</code>不一定是最佳值。我们可以使用交叉验证或超参数调整来检查这一点，但就我们的目的而言，我们可以简单地看到惩罚如何改变变量的重要性，例如随着alpha增加，越来越多的系数减少到0，如下所示:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="71ef" class="mw mx it nx b gy ob oc l od oe">#import the necessary modules<br/>from itertools import cycle<br/>from sklearn.linear_model import lasso_path<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="d210" class="mw mx it nx b gy of oc l od oe">#run the lasso path<br/>alphas_lasso, coefs_lasso, _ = lasso_path(X_train, y_train.values.reshape(-1),<br/>                                          alphas = [.0001, .001, .01,.1, .5, 1, 10, 100, 1000, 10000], <br/>                                          fit_intercept=False)</span><span id="508b" class="mw mx it nx b gy of oc l od oe">#plot the coefficients over the path<br/>log_alphas_lasso = np.log10(alphas_lasso)<br/>for index, coef_l in enumerate(coefs_lasso):<br/>    l1 = plt.plot(log_alphas_lasso, coef_l,<br/>                 label = X_train.columns.values[index])</span><span id="35fd" class="mw mx it nx b gy of oc l od oe">#add labels<br/>plt.xlabel('Log(alpha)')<br/>plt.ylabel('coefficients')<br/>plt.title('Lasso Path')<br/>plt.axis('tight')</span><span id="fe90" class="mw mx it nx b gy of oc l od oe">plt.legend(bbox_to_anchor = (0.7, 0.3, 0.5, 0.5))<br/>#sho the model<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/4a25de3cb7c1fbe8540ffdfa2011c7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rr0UogRpeVAfP8IB3R8O2A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1ca7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这显示了模型的系数如何在可能的alpha值范围内变化，由此小的alpha值意味着几个变量仍然对模型有影响，而当它增加时，较少的变量对模型有影响。</p><h2 id="6e76" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">里脊回归</h2><p id="9291" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">岭回归还能够对数据集执行正则化，以便能够控制模型中的过拟合和欠拟合。这通过使用以下等式来实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/eebef28ee66198681cf52508a688fe0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xDwsJE8Le_YKFxQm69eynA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="59a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭回归的强大之处在于，它通过强制降低系数来最小化残差平方和(RSS )(实际上引入了一种惩罚),但它不会强制系数为零(如上面的lasso回归)。这种惩罚的效果实际上是系数幅度的<strong class="lb iu">平方。这利用了被称为L2正规化的另一种正规化形式。</strong></p><p id="a75b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这又留下了𝛼的选择，它可以用几种方法来完成。更传统的统计方法是选择𝛼，这样可以减少一些信息标准，如AIC或BIC。备选方案是对𝛼值执行交叉验证和选择，这将最小化交叉验证的残差平方和(或一些其他度量)。虽然前一种方法强调模型对数据的拟合，但后一种方法更关注其预测性能。</p><p id="942b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用和以前几乎一样的语法实现岭回归:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="2794" class="mw mx it nx b gy ob oc l od oe">from sklearn.linear_model import Ridge</span><span id="f2ef" class="mw mx it nx b gy of oc l od oe">ridge_model = Ridge(alpha = 1.0)<br/>ridge_model.fit(X_train, y_train)</span></pre><p id="a90d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以用与之前类似的方式检查系数，如下所示:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="f721" class="mw mx it nx b gy ob oc l od oe">df_coef_ridge = pd.DataFrame({"var": X_train.columns,<br/>                              "coef":ridge_model.coef_[0]})<br/>df_coef_ridge</span><span id="f9b2" class="mw mx it nx b gy of oc l od oe">#out:<br/>   var   coef<br/>0  X1    -3.648339<br/>1  X2     0.012609<br/>2  X3     0.035442<br/>3  X4     -0.011416<br/>4  X5     5.307185 <br/>5  X6     -0.008634<br/>6  X7     18.218816<br/>7  X8     0.269722</span></pre><p id="12ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此我们可以看出岭回归和原始套索回归之间的主要区别是，尽管一些系数接近于0，但它们实际上并没有达到0。这是因为惩罚不会完全减少它们，事实上也永远不会。因此，尽管引入了惩罚，但是在该模型中没有执行完整的特征选择。</p><p id="d5c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以看到模型在测试数据集上的表现:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9519" class="mw mx it nx b gy ob oc l od oe">ridge_model.score(X_train, y_train)</span><span id="973b" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.9128636836305432</span></pre><p id="7789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，该模型比Lasso回归表现得更好，但仍然比基本线性回归表现得更差(至少在这个度量方面)。</p><p id="e647" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，重要的是它能够在看不见的数据上执行得如何，我们可以检查:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6142" class="mw mx it nx b gy ob oc l od oe">ridge_predictions = ridge_model.predict(X_test)</span><span id="c3da" class="mw mx it nx b gy of oc l od oe">print(ridge_model.score(X_test, y_test))</span><span id="a997" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.9085287497226727</span></pre><p id="22d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，像以前一样，知道模型正在处理看不见的数据，模型的性能下降，尽管这是可以预料的。</p><p id="d4d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们知道我们只为原始模型选择了一个单一的alpha值，而我们可以使用<code class="fe og oh oi nx b">RidgeCV</code>找到最佳的alpha值。这是通过执行留一交叉验证来实现的，以确保我们通过一系列值获得模型的最佳alpha值。这要求我们输入一系列的值来训练它，我们可以这样实现它:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="3d93" class="mw mx it nx b gy ob oc l od oe">#import the model<br/>from sklearn.linear_model import RidgeCV</span><span id="b54b" class="mw mx it nx b gy of oc l od oe">#create the model<br/>ridgeCV_model = RidgeCV(alphas = np.logspace(-6, 6, 13))<br/>#implement it on training data<br/>ridgeCV_model.fit(X_train, y_train)</span></pre><p id="fa02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过以下方法提取最佳alpha值:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="92bc" class="mw mx it nx b gy ob oc l od oe">optimal_alpha = ridgeCV_model.alpha_<br/>print(optimal_alpha)</span><span id="b403" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.001</span></pre><p id="1649" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们可以重做模型，看看性能如何变化:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ef37" class="mw mx it nx b gy ob oc l od oe">#implement the model with the new alpha<br/>ridge_model = Ridge(alpha = 0.001)<br/>#fit the model<br/>ridge_model.fit(X_train, y_train)</span><span id="4362" class="mw mx it nx b gy of oc l od oe">#check the score on the original data<br/>print(ridge_model.score(X_train, y_train))</span><span id="a3e6" class="mw mx it nx b gy of oc l od oe">#check the score with the new data<br/>print(ridge_model.score(X_test, y_test))</span><span id="ff55" class="mw mx it nx b gy of oc l od oe">#out:<br/>0.9173454869664193<br/>0.9128137902284486</span></pre><p id="4b0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以看到，使用R度量，具有最佳alpha的训练岭模型现在实际上优于基本回归模型(至少只是略微)。因此，某种程度的惩罚提高了模型的通用性，以确保模型中过拟合的机会或程度更小。</p><h2 id="fc6b" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">拉索v型山脊</h2><p id="104a" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">重要的问题是，在什么情况下应该使用套索或岭回归。这将从根本上取决于所讨论的数据，因为:</p><ol class=""><li id="68f7" class="ol om it lb b lc ld lf lg li on lm oo lq op lu oq or os ot bi translated">Lasso通过将一些系数设置为零来执行有效的变量选择，而在岭回归中并非如此。</li><li id="147f" class="ol om it lb b lc ou lf ov li ow lm ox lq oy lu oq or os ot bi translated">如果有少量独立特征或重要参数(仅当少数预测因素影响响应时)，Lasso的性能会更好。</li><li id="5362" class="ol om it lb b lc ou lf ov li ow lm ox lq oy lu oq or os ot bi translated">如果有大量独立特征或重要参数(当大多数预测器影响反应时)，岭很好地工作。</li><li id="ccb0" class="ol om it lb b lc ou lf ov li ow lm ox lq oy lu oq or os ot bi translated">通常最好通过交叉验证来使用这两种模型，以选择最适合该案例的模型。</li></ol><p id="5fc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是脊和套索回归的介绍，以及如何在您的数据上实现它们！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4f44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想了解我们协会的更多信息，请随时关注我们的社交网站:</p><p id="a976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.facebook.com/ucldata</p><p id="a7be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">insta gram:【https://www.instagram.com/ucl.datasci/ T2】</p><p id="6dea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">领英:【https://www.linkedin.com/company/ucldata/ T4】</p><p id="1f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解UCL数据科学协会和其他优秀作者的最新信息，请使用我下面的推荐代码注册medium。</p><div class="mc md gp gr me mf"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mo l"><div class="oz l mq mr ms mo mt ks mf"/></div></div></a></div><p id="8a00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者看看我写的其他故事:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/git-and-github-basics-for-data-scientists-b9fd96f8a02a"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的Git和GitHub基础知识</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学研讨会8:什么是Git，创建本地存储库，提交第一批文件，链接到远程…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pa l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/multi-variate-outlier-detection-in-python-e900a338da10"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的多元异常检测</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">能够检测数据集中异常值/异常值的六种方法</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pb l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/london-convenience-store-classification-using-k-means-clustering-70c82899c61f"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">使用K-均值聚类的伦敦便利店分类</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">伦敦的便利店怎么分类？</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pc l mq mr ms mo mt ks mf"/></div></div></a></div></div></div>    
</body>
</html>