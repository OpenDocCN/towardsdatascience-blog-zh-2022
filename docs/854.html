<html>
<head>
<title>How to Automatically Design an Efficient Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何自动设计高效的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-automatically-design-an-efficient-neural-network-ee842a631885#2022-01-31">https://towardsdatascience.com/how-to-automatically-design-an-efficient-neural-network-ee842a631885#2022-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="cb2c" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">如何自动设计高效的神经网络</h1></div><div class=""><h2 id="414c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经结构搜索的简明介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e2e774a3c88bd94f2d25c0068f048645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WulxauMTnuGRv63I"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Ethan Sykes 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="95ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你曾经使用过深度学习方法，你可能已经知道，每当你考虑一个新的数据集，你的模型的性能就非常依赖于网络的架构。</p><p id="8217" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么<strong class="ky ir"> <em class="ls">虽然</em> </strong>微调预训练网络可以在你的旅程中帮助你，但你可能必须尝试多种模式才能找到你的最佳选择。在某些情况下，您甚至可能需要调整所选网络的某些层，以获得更好的预测能力。</p><p id="a1f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个过程可能会很快变得乏味和重复，尤其是当您需要在多个数据集上执行相同的任务时。这就是为什么知道不总是手动设计或选择您的网络架构，而是在一定条件下自动完成是可能的。这被称为<strong class="ky ir"> <em class="ls">神经架构搜索(NAS) </em> </strong>，是<strong class="ky ir"> <em class="ls"> AutoML </em> </strong>的一个子领域，旨在自动设计神经网络。</p><h2 id="b310" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">为什么以及何时应该使用NAS</h2><p id="cdc1" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">NAS非常方便，特别是当你想通过自动化过去手动完成的工作(设计神经网络)将基于深度学习的解决方案扩展到数十甚至数百个数据集时。</p><p id="2007" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最重要的是，生成的架构不仅在预测能力方面，而且在内存大小和推理时间方面都是高效的。例如，谷歌的模型<strong class="ky ir"><em class="ls">NAS-FPN</em></strong>【1】实现了比SOTA物体检测模型更好的准确性/延迟权衡。</p><h2 id="ef7b" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">执行NAS需要什么</h2><p id="ee60" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">寻找一个好的网络架构可能就像大海捞针。只要想想可能的神经操作类型(全连接、卷积、LSTMs等)的数量。)，以及它们参数的可能组合(例如卷积层的内核大小)和网络的可能深度。所有这些变量导致了无限多的候选架构。</p><p id="3fd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，为了明智地找到最佳候选，您需要为NAS算法提供如下所示的3个元素:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d1635526855868e16a53cd18f39ec702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*P1NmpurkGh0BC49N4_FjUA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NAS的一般框架[8]</p></figure><ul class=""><li id="4dc2" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated"><strong class="ky ir"> <em class="ls">搜索空间</em> </strong> 𝒜 <strong class="ky ir"> <em class="ls"> : </em> </strong>包含可以采样的候选架构集合。为了定义一个搜索空间，你需要定义可能的神经操作和网络的转换动力学(即网络的节点是如何连接的)。后者要求事先知道最终网络应该是什么样子，但这并不总是强制性的。例如，一种常见的做法是将最终的网络视为层的堆叠，其中每一层都表示为一个有向无环图，就像DARTS [2]中那样。</li><li id="f570" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir"> <em class="ls">评估策略:</em> </strong>用于评估每个采样网络的性能。标准做法是使用模型的预测能力(例如，交叉熵用于分类任务)。一些工作引入了正则化项，以便通过限制网络的大小来实现准确性/等待时间的折衷。例如，在AdaBERT[3]中，重模型通过在损失函数中整合推理触发器(浮点运算)的数量来进行惩罚。</li><li id="1565" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated"><strong class="ky ir"> <em class="ls">搜索策略</em> </strong>:一旦从搜索空间中对架构进行了采样，并且使用评估策略对其性能进行了评估，搜索策略就会告诉NAS算法接下来要对什么架构进行采样和评估。搜索策略通常定义了大多数NAS方法之间的根本区别。</li></ul><p id="01a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的部分中，我将介绍三种常用的NAS方法以及它们的一些优缺点。</p><h2 id="73b8" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">具有强化学习的NAS</h2><p id="cd7d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">使用强化学习来寻找方便的体系结构是NAS中的标准做法。例如在[4]中，作者观察到网络的节点和连接可以通过字符串来表示。</p><p id="1b19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，他们设计了一个名为“<strong class="ky ir"> <em class="ls">控制器</em> </strong>”的递归单元，生成网络架构字符串。相应的网络在任务数据集上被训练，并且验证准确性被转换成用于通过策略梯度方法更新控制器权重的回报信号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ab75f71b6a994a10743dd2e480c763c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*E6hVE6b4ZIRRVekrtEulew.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有强化学习的NAS的一般框架[4]</p></figure><p id="70a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与许多其他方法相比，基于RL的架构搜索更彻底地探索搜索空间，并且对每个采样的架构具有更准确的性能估计。因此，RL-NAS [4]在流行的数据集上取得了最先进的结果也就不足为奇了。然而，它主要受到时间和硬件资源效率问题的困扰。例如，为CIFAR-10分类任务找到一个最佳架构需要3 000个GPU小时，而不是所有的公司和机构都能够负担得起这样的费用。</p><p id="89d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多策略可以用来显著提高普通RL-NAS的效率，例如跨层共享参数权重(也称为参数共享)，以减少可学习权重的数量和搜索空间的大小，这基本上是在[5]中完成的。</p><h2 id="b5f1" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">具有进化算法的NAS</h2><p id="003b" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">进化算法是基于启发式搜索的算法，主要受达尔文进化和自然选择过程的启发。在深度学习中，这些算法最初用于优化神经网络的权重，最近它们被推广到架构搜索。</p><p id="9e1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">进化方法的独创性在于在优化过程中使用随机抽样而不是梯度下降。这种随机抽样使用三次:</p><ul class=""><li id="1cc2" class="ms mt iq ky b kz la lc ld lf mu lj mv ln mw lr mx my mz na bi translated">在<strong class="ky ir"> <em class="ls">初始化步骤</em> </strong>中，生成候选架构的随机初始群体。该群体是全局搜索空间𝒜的子集，并且可以是任意大小。每一个人(建筑)都被评估，并选出最合适的。</li><li id="48bb" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">在<strong class="ky ir"> <em class="ls">交叉步骤</em> </strong>中，利用初始种群中最适合的个体(即最高性能的架构)的特征，随机产生新的个体。</li><li id="c0ca" class="ms mt iq ky b kz nb lc nc lf nd lj ne ln nf lr mx my mz na bi translated">在<strong class="ky ir"> <em class="ls">变异步骤</em> </strong>中，对新种群的个体进行随机的细微修改，生成新的个体。</li></ul><p id="cbca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后两步与<strong class="ky ir"> <em class="ls">适应性评估和选择步骤</em> </strong>一起进行，直到满足收敛标准。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/cdda81b1b799d69cdb1d8bc98725ba90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*2XroRjLuWHIZm6EtlB7tgw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">进化NAS的一般框架(图表改编自[6])</p></figure><p id="97fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NAS中的进化算法主要区别在于如何进行交叉和变异。为了探索NAS中流行的进化方法，我推荐你阅读这篇出色的<a class="ae kv" href="https://arxiv.org/abs/2008.10937" rel="noopener ugc nofollow" target="_blank">调查</a>【6】。</p><h2 id="e7a6" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">可区分NAS </strong></h2><p id="2579" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">RL和进化算法的时间和资源效率问题导致了基于微分梯度的方法的出现，例如DARTS [2]和GDAS [7]。</p><p id="0fe0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这些算法中，所寻求的架构是相同单元(或层)的堆叠。每个单元被表示为由<strong class="ky ir"> <em class="ls"> N个节点</em> </strong>(表示中间数据状态)和<strong class="ky ir"> <em class="ls"> K条边</em> </strong>组成的有向无环图(DAG ),其中每一个都表示从<strong class="ky ir"> <em class="ls"> E个可能的候选神经操作</em> </strong>(密集层、卷积层等)中采样的转换操作。).</p><p id="9de8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">下图中:N = 4个节点，K = 6条边，E = 3个候选操作。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4c7bd8a326e668a74fb2b0ba6ca77931.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*mBufZuZMvNjwdjF9plagVA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">网络单元的DAG表示。在每条边上有三种不同采样概率的候选操作。(<a class="ae kv" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank">来源</a> [2])</p></figure><p id="b59d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算法的目标是为DAG中的每条边选择最佳的神经操作。为此，为每个边缘内的每个候选操作分配随机采样概率，这给出了一个<strong class="ky ir"> <em class="ls"> E * K </em>概率矩阵</strong>。在每个学习步骤中，使用这些概率对架构进行采样。在评估了网络的性能之后，我们使用反向传播来更新参数权重和采样概率。</p><p id="df21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设采样过程是不可微的，我们使用一个重新参数化的技巧将分类样本放松为连续可微的样本。</p><p id="6bf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GDAS和DARTS的主要优势在于，我们同时在寻找最佳架构和学习网络的权重，这使得训练比基于RL的方法快得多。这个叫做<strong class="ky ir"> <em class="ls">一次性NAS </em> </strong>。</p><p id="1870" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，如果你没有应用一个合适的训练开始时间和学习率时间表，这种并行优化可能会变得不稳定。事实上，为权重和架构优化选择一个方便的学习速率对于获得满意的结果绝对至关重要。</p><h1 id="51c4" class="nj lu iq bd lv nk nl nm ly nn no np mb jw nq jx me jz nr ka mh kc ns kd mk nt bi translated"><strong class="ak"> <em class="nu">外卖信息</em> </strong></h1><p id="2dfe" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">神经结构搜索是一种强大的自动化方法，允许自动设计神经网络。如果部署正确，所选架构在预测能力、内存需求和推理时间方面产生令人满意的结果。</p><p id="d1ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，当在单个数据集上工作或用于有限的用途时，使用现成的预先训练的网络最初可能工作得很好。但请记住，这些网络通常有数千万个参数(ResNet-50有24M个参数，BERT-base有110M个参数)。因此，每当您考虑扩展您的解决方案时，您迟早会意识到您必须考虑轻型模型，NAS是一个值得认真考虑的真正选择。</p><p id="23ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，即使NAS旨在通过自动化架构设计过程来使深度学习更容易为科学界所接受，我相信大多数当前的方法都需要专家干预来确保它们的效率和稳定性。</p><h1 id="4177" class="nj lu iq bd lv nk nl nm ly nn no np mb jw nq jx me jz nr ka mh kc ns kd mk nt bi translated"><strong class="ak">参考文献</strong></h1><p id="6331" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">[1]吉亚西，高尔纳茨&amp;林，宗毅&amp;乐，郭。<a class="ae kv" href="https://arxiv.org/abs/1904.07392" rel="noopener ugc nofollow" target="_blank"> NAS-FPN:学习用于对象检测的可扩展特征金字塔架构</a> (2019)。</p><p id="a979" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]刘，&amp;西蒙扬，卡伦&amp;杨，。<a class="ae kv" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank">飞镖:差异化架构搜索</a> (2018)。</p><p id="82e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]陈、李道元、、邱、王明辉、李、、丁、林、黄、林、周、。<a class="ae kv" href="https://arxiv.org/abs/2001.04246" rel="noopener ugc nofollow" target="_blank"> AdaBERT:具有可区分神经架构搜索的任务自适应BERT压缩</a> (2020)。</p><p id="2746" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] Zoph，Barret &amp; Le，Quoc。<a class="ae kv" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">具有强化学习的神经架构搜索</a> (2016)。</p><p id="b134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] Pham，Hieu &amp; Guan，Melody &amp; Zoph，Barret &amp; Le，Quoc &amp; Dean，Jeff。<a class="ae kv" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">通过参数共享进行高效的神经架构搜索</a> (2018)。</p><p id="d8cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]刘、余巧和孙、延安和薛、张和冰、和晏、盖瑞和谭、凯。<a class="ae kv" href="https://arxiv.org/abs/2008.10937" rel="noopener ugc nofollow" target="_blank">进化神经结构搜索综述</a> (2021)。</p><p id="7740" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]董，宣仪&amp;杨，易。<a class="ae kv" href="https://arxiv.org/abs/1910.04465" rel="noopener ugc nofollow" target="_blank">在四个GPU小时内搜索健壮的神经架构</a> (2019)。</p><p id="585a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]埃尔斯肯，托马斯&amp;梅岑，简&amp;哈特，弗兰克。<a class="ae kv" href="https://arxiv.org/pdf/1808.05377.pdf" rel="noopener ugc nofollow" target="_blank">神经架构搜索:一项调查</a> (2019)。</p></div></div>    
</body>
</html>