<html>
<head>
<title>Implementation of SimpleRNN, GRU, and LSTM Models in Keras and Tensorflow For an NLP Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Keras和Tensorflow中为NLP项目实现SimpleRNN、GRU和LSTM模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-recurrent-neural-network-and-implementation-of-simplernn-gru-and-lstm-models-in-keras-f7247e97c405#2022-01-13">https://towardsdatascience.com/what-is-a-recurrent-neural-network-and-implementation-of-simplernn-gru-and-lstm-models-in-keras-f7247e97c405#2022-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cfafb6a1398f2433a25732dd98fa5d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kBop4AcNBaQKonwV"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阿特·哈廷顿在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""><h1 id="51b6" class="pw-post-title jh ji jj bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">在Keras和Tensorflow中为NLP项目实现SimpleRNN、GRU和LSTM模型</h1></div><div class=""><h2 id="31cf" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">递归神经网络介绍及情感分析中三种递归神经网络在Tensorflow和Keras中的实现</h2></div><p id="d088" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">递归神经网络(RNNs)是深度学习中最先进的算法之一，尤其适用于序列数据。它被用于许多备受瞩目的应用，包括谷歌的语音搜索和苹果的Siri。它变得如此受欢迎的原因是它的内存。RNNs是第一个可以记住输入的深度学习算法。在LSTM和GRU发明之后，rnn变得非常强大。在本文中，我们将使用所有这三种类型的rnn进行一个自然语言处理项目。</p><p id="3efa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将讨论RNNs如何工作，并在深度学习领域带来如此多的效率。对于情感分析任务，将有一个简单的RNN、GRU和LSTM的实际实现。</p><blockquote class="lu"><p id="2db9" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">我将非常简要地讨论一个简单的递归神经网络如何为复习者工作，然后深入到实现中。如果您愿意，可以跳过这一部分，直接进入Tensorflow实现部分。</p></blockquote><h2 id="b742" class="me mf jj bd mg mh mi dn mj mk ml dp mm lh mn mo mp ll mq mr ms lp mt mu mv mw bi translated">什么是递归神经网络？</h2><p id="25e3" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">因为递归神经网络有内部记忆，所以可以从输入和前几层记住重要的东西。这就是它比常规神经网络更有效的原因，rnn是文本数据、时间序列、金融数据、天气预报等序列数据的首选算法。</p><h2 id="7f08" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">递归神经网络如何工作？</h2><p id="1b41" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">为了真正理解递归神经网络如何工作以及它为什么特殊，我们需要将它与常规的前馈神经网络进行比较。</p><p id="ffb0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是常规前馈神经网络的图片演示:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dc2ad11a5cf7fbf3c0b14e7b84db634d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*v8djn2P4zcaswpnxCIVnqg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="62d9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在前馈神经网络中，信息从一层移动到另一层。我们使用来自输入层的信息来计算隐藏层。如果有几个隐藏层，则仅使用前一个隐藏层的信息来计算下一个隐藏层。计算输出图层时，仅使用前一图层的信息。因此，当我们计算输出层时，我们忘记了输入层或其他层。</p><p id="82af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，当我们处理文本数据、时间序列数据或任何其他顺序数据时，记住前面的层中有什么也很重要。</p><p id="c8df" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在RNN，信息在内部循环。因此，当RNN计算输出图层时，它会考虑前一个图层以及前一个图层。因为它有短期记忆。这里有一个图片演示:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0de5183d905911fc5c9d851c78070b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Nvprfz1FIqpr9Z-CBKoQeQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="29b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图所示，信息在这一层循环。当循环发生时，它具有来自最近过去的信息。下面是RNN结构的展开版本，可以更好地理解它的工作原理:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/33d576748f61a770d86e54fc2869b327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*j4BjO3uNwuO2jeUII8eD_Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="1447" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，x0、x1和x2表示输入。H0、H1和H2是隐藏层中的神经元，y0、y1和y2是输出。</p><p id="8649" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上图所示，每个时间戳都从前一个神经元和输入中获取信息。信息在一层中循环。</p><p id="8ece" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这非常重要，因为前一个神经元可能包含关于下一个神经元的重要信息。例如，考虑这个句子，</p><blockquote class="lu"><p id="9771" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">天空是蓝色的</p></blockquote><p id="e7d6" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">如果我只知道“是”这个词，我无法想象接下来会发生什么。但是如果我知道两个连续的单词“sky is ”,那么我们可能会想到单词“blue”。</p><p id="7e91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但这也是递归神经网络的一个局限。RNNs只有短期记忆。但是短期记忆始终不足以发现接下来会发生什么。举个例子，</p><blockquote class="lu"><p id="8adb" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">她是中国人，她的语言是…</p></blockquote><p id="ca6e" class="pw-post-body-paragraph ky kz jj la b lb no kk ld le np kn lg lh nq lj lk ll nr ln lo lp ns lr ls lt im bi translated">在这里，仅仅记住前面的两三个单词并不能让我们知道语言是什么。我们要一路走来记住“中国人”这个词。只有到那时，我们才能预测这种语言的名称。</p><p id="25ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是长短期记忆(LSTM)或门控循环单元(GRU)有所帮助的时候。它们都是简单RNN的更高级版本。解释它们的机制超出了本文的范围。本文的重点是展示如何在TensorFlow中实现它们。</p><h2 id="85e8" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">资料组</h2><p id="421e" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">我将使用Tensorflow附带的IMDB数据集。这是一个大型电影评论数据集。数据是文本数据，标签是二进制的。它已经为我们分离了25000个训练数据和25000个测试数据。点击了解关于<a class="ae jg" href="https://www.tensorflow.org/datasets/catalog/imdb_reviews" rel="noopener ugc nofollow" target="_blank">该数据集的更多信息。对于练习一些自然语言处理任务来说，这是一个非常好的数据集。该数据集的每一行都包含预期的文本数据，标签为0或1。所以，它代表好的情绪或坏的情绪。</a></p><p id="562a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们深入研究这个项目。</p><p id="b0e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是进口货:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="3702" class="me mf jj nu b gy ny nz l oa ob">import numpy as np<br/>import tensorflow as tf<br/>import tensorflow_datasets as tfds</span></pre><p id="2ce1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我正在导入IMDB数据集及其信息:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="b764" class="me mf jj nu b gy ny nz l oa ob">imdb, info = tfds.load("imdb_reviews",<br/>                      with_info=True, as_supervised=True)</span></pre><p id="dcdd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在单独的变量中设置训练和测试数据:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="7f2c" class="me mf jj nu b gy ny nz l oa ob">train_data, test_data = imdb['train'], imdb['test']</span></pre><h2 id="eb84" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">数据预处理</h2><p id="6751" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">将所有文本作为一个列表，将标签作为一个单独的列表将会很有帮助。因此，训练句子和标签以及测试句子和标签作为列表被检索如下:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="c77b" class="me mf jj nu b gy ny nz l oa ob">training_sentences = []<br/>training_labels = []</span><span id="b60a" class="me mf jj nu b gy oc nz l oa ob">testing_sentences = []<br/>testing_labels = []</span><span id="100c" class="me mf jj nu b gy oc nz l oa ob">for s,l in train_data:<br/>    training_sentences.append(str(s.numpy()))<br/>    training_labels.append(l.numpy())</span><span id="1a58" class="me mf jj nu b gy oc nz l oa ob">for s,l in test_data:<br/>    testing_sentences.append(str(s.numpy()))<br/>    testing_labels.append(l.numpy())</span></pre><p id="9b9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将标签转换为NumPy数组:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="37f0" class="me mf jj nu b gy ny nz l oa ob">training_labels_final = np.array(training_labels)<br/>testing_labels_final = np.array(testing_labels)</span></pre><p id="4052" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我设置一些重要的参数是必要的模型。我将在此之后解释它们是什么:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="ab48" class="me mf jj nu b gy ny nz l oa ob">vocab_size = 10000<br/>embedding_dim=16<br/>max_length = 120<br/>trunc_type= 'post'<br/>oov_tok="&lt;OOV&gt;"</span></pre><p id="31c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，vocal_size 10000。这意味着这个模型将使用10000个独特的单词。如果IMDB数据集超过10000个单词，多余的单词将不会用于训练模型。所以，通常情况下，我们会小心翼翼地取这个数。请随意尝试不同的vocab_size。</p><p id="c12d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一个参数是‘embedding _ dim’。它表示将用于表示每个单词的向量的大小。这里embedding_dim是16意味着，大小为16的向量将表示每个单词。您也可以在此尝试不同的号码。</p><p id="092f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每段文本或预测标签的最大长度为120个单词。这就是max_length参数所表示的内容。如果文本比原来大，它将被截断。</p><p id="6745" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一个参数trunc_type设置为‘post’。这意味着文本将在末尾被截断。</p><p id="c127" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果有一个未知单词将由oov_tok表示。</p><p id="0215" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据预处理从标记NLP项目中的文本开始。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="75d8" class="me mf jj nu b gy ny nz l oa ob">from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="d0f7" class="me mf jj nu b gy oc nz l oa ob">tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)<br/>tokenizer.fit_on_texts(training_sentences)<br/>word_index = tokenizer.word_index</span><span id="23d9" class="me mf jj nu b gy oc nz l oa ob">word_index</span></pre><p id="8aa3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是word_index的部分输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="ff26" class="me mf jj nu b gy ny nz l oa ob">{'&lt;OOV&gt;': 1,<br/> 'the': 2,<br/> 'and': 3,<br/> 'a': 4,<br/> 'of': 5,<br/> 'to': 6,<br/> 'is': 7,<br/> 'br': 8,<br/> 'in': 9,<br/> 'it': 10,<br/> 'i': 11,<br/> 'this': 12,<br/> 'that': 13,<br/> 'was': 14,<br/> 'as': 15,<br/> 'for': 16,<br/> 'with': 17,<br/> 'movie': 18,</span></pre><p id="29f6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，每个单词都有一个唯一的整数值。这里我们用这些整数值代替单词来安排句子。此外，如果句子少于我们设置的max_length 120个单词，请使用填充。这样，我们将为每个文本提供相同大小的向量。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="ab68" class="me mf jj nu b gy ny nz l oa ob">sequences = tokenizer.texts_to_sequences(training_sentences)<br/>padded = pad_sequences(sequences, maxlen=max_length, <br/>                       truncating = trunc_type)</span><span id="85c1" class="me mf jj nu b gy oc nz l oa ob">testing_sequences = tokenizer.texts_to_sequences(testing_sentences)<br/>testing_padded = pad_sequences(testing_sequences, maxlen=max_length)</span></pre><p id="0438" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据预处理完成。</p><h2 id="8657" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">模型开发</h2><p id="ebf4" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">这是有趣的部分。</p><blockquote class="od oe of"><p id="933b" class="ky kz og la b lb lc kk ld le lf kn lg oh li lj lk oi lm ln lo oj lq lr ls lt im bi translated"><strong class="la jk">简单的RNN </strong></p></blockquote><p id="06a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一个模型将是简单的递归神经网络模型。</p><p id="044e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个模型中，第一层将是嵌入层，其中句子将通过嵌入向量表示为max_length。下一层是一个简单的RNN层。然后是致密层。这是模型</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="0480" class="me mf jj nu b gy ny nz l oa ob">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim,<br/>                             input_length=max_length),<br/>    tf.keras.layers.SimpleRNN(32),<br/>    tf.keras.layers.Dense(10, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/>model.summary()</span></pre><p id="2086" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="b28b" class="me mf jj nu b gy ny nz l oa ob">Model: "sequential_8"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_8 (Embedding)      (None, 120, 16)           160000    <br/>_________________________________________________________________<br/>simple_rnn (SimpleRNN)       (None, 32)                1568      <br/>_________________________________________________________________<br/>dense_16 (Dense)             (None, 10)                330       <br/>_________________________________________________________________<br/>dense_17 (Dense)             (None, 1)                 11        <br/>=================================================================<br/>Total params: 161,909<br/>Trainable params: 161,909<br/>Non-trainable params: 0<br/>_________________________</span></pre><p id="4b42" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看每一层的输出形状。第一层输出形状为(120，16)。记住我们每个句子的max_length是120，嵌入维数是16。请随意更改这些数字并检查结果。</p><p id="4b0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第二层中，我们将32作为SimpleRNN层中的参数，输出形状也是32。</p><p id="744e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们将使用binary_crossentropy的损失函数、“adam”优化器以及作为准确性的评估度量来编译模型。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="c420" class="me mf jj nu b gy ny nz l oa ob">model.compile(loss='binary_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])</span></pre><p id="921b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将训练30个纪元的模型。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="8aaa" class="me mf jj nu b gy ny nz l oa ob">num_epochs=30<br/>history=model.fit(padded, training_labels_final, epochs=num_epochs, validation_data = (testing_padded, testing_labels_final))</span></pre><p id="3f52" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="41fd" class="me mf jj nu b gy ny nz l oa ob">Epoch 1/30<br/>782/782 [==============================] - 17s 20ms/step - loss: 0.6881 - accuracy: 0.5256 - val_loss: 0.6479 - val_accuracy: 0.6355<br/>Epoch 2/30<br/>782/782 [==============================] - 15s 19ms/step - loss: 0.5035 - accuracy: 0.7632 - val_loss: 0.4880 - val_accuracy: 0.7865<br/>Epoch 3/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 0.2917 - accuracy: 0.8804 - val_loss: 0.5812 - val_accuracy: 0.7457<br/>Epoch 4/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 0.1393 - accuracy: 0.9489 - val_loss: 0.6386 - val_accuracy: 0.7952<br/>Epoch 5/30<br/>782/782 [==============================] - 15s 19ms/step - loss: 0.0655 - accuracy: 0.9768 - val_loss: 0.9400 - val_accuracy: 0.7277<br/>Epoch 6/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 0.9493 - val_accuracy: 0.7912<br/>Epoch 7/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 0.0273 - accuracy: 0.9900 - val_loss: 1.1033 - val_accuracy: 0.7491<br/>Epoch 8/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 0.0993 - accuracy: 0.9639 - val_loss: 1.1237 - val_accuracy: 0.5752<br/>Epoch 9/30<br/>782/782 [==============================] - 15s 19ms/step - loss: 0.2071 - accuracy: 0.9136 - val_loss: 1.0613 - val_accuracy: 0.6309<br/>Epoch 10/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 0.0267 - accuracy: 0.9928 - val_loss: 1.4416 - val_accuracy: 0.6720<br/>Epoch 11/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 1.6674 - val_accuracy: 0.6721<br/>Epoch 12/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 5.8072e-04 - accuracy: 1.0000 - val_loss: 1.8338 - val_accuracy: 0.6714<br/>Epoch 13/30<br/>782/782 [==============================] - 15s 19ms/step - loss: 2.5399e-04 - accuracy: 1.0000 - val_loss: 1.8619 - val_accuracy: 0.6824<br/>Epoch 14/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 1.4048e-04 - accuracy: 1.0000 - val_loss: 1.8995 - val_accuracy: 0.6927<br/>Epoch 15/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 8.4974e-05 - accuracy: 1.0000 - val_loss: 1.9867 - val_accuracy: 0.6934<br/>Epoch 16/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 5.2411e-05 - accuracy: 1.0000 - val_loss: 2.0710 - val_accuracy: 0.6940<br/>Epoch 17/30<br/>782/782 [==============================] - 17s 22ms/step - loss: 3.2760e-05 - accuracy: 1.0000 - val_loss: 2.1278 - val_accuracy: 0.6980<br/>Epoch 18/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 2.0648e-05 - accuracy: 1.0000 - val_loss: 2.2035 - val_accuracy: 0.6988<br/>Epoch 19/30<br/>782/782 [==============================] - 15s 19ms/step - loss: 1.3099e-05 - accuracy: 1.0000 - val_loss: 2.2611 - val_accuracy: 0.7031<br/>Epoch 20/30<br/>782/782 [==============================] - 15s 20ms/step - loss: 8.3039e-06 - accuracy: 1.0000 - val_loss: 2.3340 - val_accuracy: 0.7038<br/>Epoch 21/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 5.2835e-06 - accuracy: 1.0000 - val_loss: 2.4453 - val_accuracy: 0.7003<br/>Epoch 22/30<br/>782/782 [==============================] - 16s 20ms/step - loss: 3.3794e-06 - accuracy: 1.0000 - val_loss: 2.4580 - val_accuracy: 0.7083<br/>Epoch 23/30<br/>782/782 [==============================] - 20s 26ms/step - loss: 2.1589e-06 - accuracy: 1.0000 - val_loss: 2.5184 - val_accuracy: 0.7112<br/>Epoch 24/30<br/>782/782 [==============================] - 18s 23ms/step - loss: 1.3891e-06 - accuracy: 1.0000 - val_loss: 2.6400 - val_accuracy: 0.7055<br/>Epoch 25/30<br/>782/782 [==============================] - 20s 25ms/step - loss: 8.9716e-07 - accuracy: 1.0000 - val_loss: 2.6727 - val_accuracy: 0.7107<br/>Epoch 26/30<br/>782/782 [==============================] - 20s 25ms/step - loss: 5.7747e-07 - accuracy: 1.0000 - val_loss: 2.7517 - val_accuracy: 0.7105<br/>Epoch 27/30<br/>782/782 [==============================] - 17s 22ms/step - loss: 3.7458e-07 - accuracy: 1.0000 - val_loss: 2.7854 - val_accuracy: 0.7159<br/>Epoch 28/30<br/>782/782 [==============================] - 18s 23ms/step - loss: 2.4265e-07 - accuracy: 1.0000 - val_loss: 2.8592 - val_accuracy: 0.7158<br/>Epoch 29/30<br/>782/782 [==============================] - 17s 22ms/step - loss: 1.5808e-07 - accuracy: 1.0000 - val_loss: 2.9216 - val_accuracy: 0.7172<br/>Epoch 30/30<br/>782/782 [==============================] - 17s 22ms/step - loss: 1.0345e-07 - accuracy: 1.0000 - val_loss: 2.9910 - val_accuracy: 0.7174</span></pre><p id="64c0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">30个周期后，训练精度变为1.00或100%。完美，对吧？但验证准确率为71.74%。没那么糟。但是巨大的过度拟合问题。</p><p id="d7ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将是很好的，看看如何精度和损失随着每个时代的变化。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="dc23" class="me mf jj nu b gy ny nz l oa ob">import matplotlib.pyplot as plt</span><span id="de36" class="me mf jj nu b gy oc nz l oa ob">def plot_graphs(history, string):<br/>    plt.plot(history.history[string])<br/>    plt.plot(history.history['val_'+string])<br/>    plt.xlabel("Epochs")<br/>    plt.ylabel(string)<br/>    plt.legend([string, 'val_'+string])<br/>    plt.show()</span><span id="5ab6" class="me mf jj nu b gy oc nz l oa ob">plot_graphs(history, 'accuracy')<br/>plot_graphs(history, 'loss')</span></pre><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/3279c162f71b086bedb846b86c149548.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*LyU5juP_mP4wT3oZo0jiBQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9a25" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">验证精度在开始时波动很大，然后稳定在71.74%。另一方面，训练精度稳步上升到100%,除了一个下降。</p><p id="253e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是验证的损失曲线看起来很糟糕。它一直在上升。我们希望损失减少。</p><h2 id="5f70" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">门控循环单元(GRU)</h2><p id="5e09" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">这是RNN模型的改进版本。它比简单的神经网络模型更有效。它有两个门:复位和更新。在本次演示中，我将用具有相同数量单位的双向GRU层替换SimpleRNN层。</p><p id="0206" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是模型</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="f6d5" class="me mf jj nu b gy ny nz l oa ob">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim,<br/>                             input_length=max_length),<br/>    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),<br/>    tf.keras.layers.Dense(10, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/>model.summary()</span></pre><p id="2e7c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="adf1" class="me mf jj nu b gy ny nz l oa ob">Model: "sequential_6"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_6 (Embedding)      (None, 120, 16)           160000    <br/>_________________________________________________________________<br/>bidirectional_6 (Bidirection (None, 64)                9600      <br/>_________________________________________________________________<br/>dense_12 (Dense)             (None, 10)                650       <br/>_________________________________________________________________<br/>dense_13 (Dense)             (None, 1)                 11        <br/>=================================================================<br/>Total params: 170,261<br/>Trainable params: 170,261<br/>Non-trainable params: 0</span></pre><p id="f792" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，除了GRU层，所有层的输出形状与前一层完全相同。因为是双向的，所以这里32变成了64。</p><p id="69d0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将使用与之前完全相同的参数来编译模型，并为相同数量的时期训练模型。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="bc2e" class="me mf jj nu b gy ny nz l oa ob">model.compile(loss="binary_crossentropy",<br/>             optimizer='adam',<br/>             metrics=['accuracy'])</span><span id="56cc" class="me mf jj nu b gy oc nz l oa ob">history=model.fit(padded, training_labels_final, epochs=num_epochs,<br/>            validation_data = (testing_padded,testing_labels_final))</span></pre><p id="cc6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="f880" class="me mf jj nu b gy ny nz l oa ob">Epoch 1/30<br/>782/782 [==============================] - 50s 60ms/step - loss: 0.5664 - accuracy: 0.6990 - val_loss: 0.4313 - val_accuracy: 0.8025<br/>Epoch 2/30<br/>782/782 [==============================] - 43s 56ms/step - loss: 0.3638 - accuracy: 0.8440 - val_loss: 0.3667 - val_accuracy: 0.8394<br/>Epoch 3/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.2852 - accuracy: 0.8882 - val_loss: 0.3695 - val_accuracy: 0.8420<br/>Epoch 4/30<br/>782/782 [==============================] - 43s 55ms/step - loss: 0.2330 - accuracy: 0.9120 - val_loss: 0.3979 - val_accuracy: 0.8381<br/>Epoch 5/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.1942 - accuracy: 0.9323 - val_loss: 0.4386 - val_accuracy: 0.8334<br/>Epoch 6/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.1573 - accuracy: 0.9472 - val_loss: 0.4546 - val_accuracy: 0.8290<br/>Epoch 7/30<br/>782/782 [==============================] - 44s 57ms/step - loss: 0.1223 - accuracy: 0.9612 - val_loss: 0.5259 - val_accuracy: 0.8244<br/>Epoch 8/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.0897 - accuracy: 0.9729 - val_loss: 0.6248 - val_accuracy: 0.8234<br/>Epoch 9/30<br/>782/782 [==============================] - 44s 57ms/step - loss: 0.0690 - accuracy: 0.9788 - val_loss: 0.6511 - val_accuracy: 0.8169<br/>Epoch 10/30<br/>782/782 [==============================] - 50s 64ms/step - loss: 0.0514 - accuracy: 0.9847 - val_loss: 0.7230 - val_accuracy: 0.8223<br/>Epoch 11/30<br/>782/782 [==============================] - 45s 57ms/step - loss: 0.0402 - accuracy: 0.9882 - val_loss: 0.8357 - val_accuracy: 0.8167<br/>Epoch 12/30<br/>782/782 [==============================] - 52s 67ms/step - loss: 0.0323 - accuracy: 0.9902 - val_loss: 0.9256 - val_accuracy: 0.8140<br/>Epoch 13/30<br/>782/782 [==============================] - 49s 63ms/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 0.9685 - val_accuracy: 0.8184<br/>Epoch 14/30<br/>782/782 [==============================] - 45s 58ms/step - loss: 0.0232 - accuracy: 0.9930 - val_loss: 0.8898 - val_accuracy: 0.8146<br/>Epoch 15/30<br/>782/782 [==============================] - 44s 57ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 1.0375 - val_accuracy: 0.8144<br/>Epoch 16/30<br/>782/782 [==============================] - 43s 55ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 1.0408 - val_accuracy: 0.8131<br/>Epoch 17/30<br/>782/782 [==============================] - 45s 57ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 1.0855 - val_accuracy: 0.8143<br/>Epoch 18/30<br/>782/782 [==============================] - 43s 55ms/step - loss: 0.0122 - accuracy: 0.9964 - val_loss: 1.1825 - val_accuracy: 0.8105<br/>Epoch 19/30<br/>782/782 [==============================] - 45s 57ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 1.3037 - val_accuracy: 0.8097<br/>Epoch 20/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 1.2140 - val_accuracy: 0.8125<br/>Epoch 21/30<br/>782/782 [==============================] - 45s 58ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 1.2321 - val_accuracy: 0.8136<br/>Epoch 22/30<br/>782/782 [==============================] - 45s 58ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 1.1726 - val_accuracy: 0.8072<br/>Epoch 23/30<br/>782/782 [==============================] - 46s 59ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 1.2273 - val_accuracy: 0.8029<br/>Epoch 24/30<br/>782/782 [==============================] - 45s 57ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 1.3390 - val_accuracy: 0.8118<br/>Epoch 25/30<br/>782/782 [==============================] - 50s 64ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 1.2323 - val_accuracy: 0.8088<br/>Epoch 26/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 1.2998 - val_accuracy: 0.8123<br/>Epoch 27/30<br/>782/782 [==============================] - 44s 57ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 1.3976 - val_accuracy: 0.8098<br/>Epoch 28/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 1.6791 - val_accuracy: 0.8043<br/>Epoch 29/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 1.4269 - val_accuracy: 0.8101<br/>Epoch 30/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 1.4012 - val_accuracy: 0.8150</span></pre><p id="5ee6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如您所见，30个时期后，训练集的最终准确度为0.9982或99.82%，验证准确度的准确度为81.50%。训练精度略低于简单的RNN模型，但验证精度显著提高。</p><p id="c7fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">仍然存在过度拟合的问题。但是在NLP项目中，一些过度拟合是正常的。因为不管你的训练集里有什么词库，验证数据可能还是会有一些新词。</p><p id="edb5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个历元的精度和损耗图将更好地显示每个历元的精度和损耗是如何变化的。我将使用上一节中的plot_graphs函数来绘制图形。</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="3bdf" class="me mf jj nu b gy ny nz l oa ob">plot_graphs(history, 'accuracy')<br/>plot_graphs(history, 'loss')</span></pre><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/014bfafbc82ca1c8c6b51ae429426958.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ajfDKpg8zi7FaV8IvH_Hmw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="b8c0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练的准确率逐渐提高，随着epochs的增加达到99%以上。但是验证准确率上升到了85%,并保持相当稳定。</p><p id="5730" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如您所见，验证损失仍在上升。但是如果你注意y轴上的数字，对于简单的RNN，验证损失从0.5变成了3.0。但是对于这个GRU模型，它从0.4上升到1.7。因此，总体验证损失要小得多。</p><blockquote class="od oe of"><p id="3e38" class="ky kz og la b lb lc kk ld le lf kn lg oh li lj lk oi lm ln lo oj lq lr ls lt im bi translated"><strong class="la jk">长期短期模型(LSTM) </strong></p></blockquote><p id="f44c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还想展示一个LSTM模型的实现。LSTM模型和GRU模型的主要区别是，LSTM模型有三个门(输入、输出和遗忘门)，而GRU模型有两个门，如前所述。</p><p id="1907" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我将只取代GRU层从以前的模型，并使用LSTM层。模型如下:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="9403" class="me mf jj nu b gy ny nz l oa ob">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),<br/>    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),<br/>    tf.keras.layers.Dense(10, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])<br/>model.summary()</span></pre><p id="eaf5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="6564" class="me mf jj nu b gy ny nz l oa ob">Model: "sequential_5"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_5 (Embedding)      (None, 120, 16)           160000    <br/>_________________________________________________________________<br/>bidirectional_5 (Bidirection (None, 64)                12544     <br/>_________________________________________________________________<br/>dense_10 (Dense)             (None, 10)                650       <br/>_________________________________________________________________<br/>dense_11 (Dense)             (None, 1)                 11        <br/>=================================================================<br/>Total params: 173,205<br/>Trainable params: 173,205<br/>Non-trainable params: 0</span></pre><p id="efc7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型摘要看起来几乎接近于上一节中的GRU模型。输出形状完全相同。但是params #在LSTM层是不同的。因为如前所述，LSTM模型有三个门，而不是两个门。我会再训练30个纪元:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="c534" class="me mf jj nu b gy ny nz l oa ob">num_epochs=30<br/>history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data = (testing_padded, testing_labels_final))</span></pre><p id="5ff9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="3b0c" class="me mf jj nu b gy ny nz l oa ob">Epoch 1/30<br/>782/782 [==============================] - 46s 54ms/step - loss: 0.4740 - accuracy: 0.7665 - val_loss: 0.3732 - val_accuracy: 0.8413<br/>Epoch 2/30<br/>782/782 [==============================] - 38s 48ms/step - loss: 0.3010 - accuracy: 0.8789 - val_loss: 0.3687 - val_accuracy: 0.8397<br/>Epoch 3/30<br/>782/782 [==============================] - 37s 48ms/step - loss: 0.2380 - accuracy: 0.9079 - val_loss: 0.4850 - val_accuracy: 0.8311<br/>Epoch 4/30<br/>782/782 [==============================] - 38s 48ms/step - loss: 0.1890 - accuracy: 0.9306 - val_loss: 0.4697 - val_accuracy: 0.8355<br/>Epoch 5/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.1453 - accuracy: 0.9494 - val_loss: 0.4581 - val_accuracy: 0.8305<br/>Epoch 6/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.1118 - accuracy: 0.9622 - val_loss: 0.7176 - val_accuracy: 0.8212<br/>Epoch 7/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.0952 - accuracy: 0.9676 - val_loss: 0.7107 - val_accuracy: 0.8230<br/>Epoch 8/30<br/>782/782 [==============================] - 41s 52ms/step - loss: 0.0769 - accuracy: 0.9753 - val_loss: 0.6845 - val_accuracy: 0.8205<br/>Epoch 9/30<br/>782/782 [==============================] - 39s 49ms/step - loss: 0.0567 - accuracy: 0.9824 - val_loss: 0.9333 - val_accuracy: 0.8163<br/>Epoch 10/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.0564 - accuracy: 0.9826 - val_loss: 0.8228 - val_accuracy: 0.8171<br/>Epoch 11/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.0466 - accuracy: 0.9862 - val_loss: 0.9426 - val_accuracy: 0.8177<br/>Epoch 12/30<br/>782/782 [==============================] - 44s 56ms/step - loss: 0.0445 - accuracy: 0.9861 - val_loss: 0.9138 - val_accuracy: 0.8144<br/>Epoch 13/30<br/>782/782 [==============================] - 44s 57ms/step - loss: 0.0343 - accuracy: 0.9897 - val_loss: 0.9876 - val_accuracy: 0.8149<br/>Epoch 14/30<br/>782/782 [==============================] - 42s 53ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 1.0017 - val_accuracy: 0.8152<br/>Epoch 15/30<br/>782/782 [==============================] - 41s 52ms/step - loss: 0.0251 - accuracy: 0.9932 - val_loss: 1.0724 - val_accuracy: 0.8158<br/>Epoch 16/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 1.0648 - val_accuracy: 0.8117<br/>Epoch 17/30<br/>782/782 [==============================] - 43s 55ms/step - loss: 0.0258 - accuracy: 0.9922 - val_loss: 0.9267 - val_accuracy: 0.8109<br/>Epoch 18/30<br/>782/782 [==============================] - 40s 51ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 1.0909 - val_accuracy: 0.8104<br/>Epoch 19/30<br/>782/782 [==============================] - 38s 49ms/step - loss: 0.0114 - accuracy: 0.9967 - val_loss: 1.1444 - val_accuracy: 0.8134<br/>Epoch 20/30<br/>782/782 [==============================] - 38s 48ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 1.1040 - val_accuracy: 0.8124<br/>Epoch 21/30<br/>782/782 [==============================] - 38s 49ms/step - loss: 0.0196 - accuracy: 0.9941 - val_loss: 1.2061 - val_accuracy: 0.8128<br/>Epoch 22/30<br/>782/782 [==============================] - 41s 53ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 1.3365 - val_accuracy: 0.8032<br/>Epoch 23/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.0155 - accuracy: 0.9953 - val_loss: 1.2835 - val_accuracy: 0.8123<br/>Epoch 24/30<br/>782/782 [==============================] - 43s 55ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 1.2476 - val_accuracy: 0.8160<br/>Epoch 25/30<br/>782/782 [==============================] - 38s 48ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 1.0810 - val_accuracy: 0.8091<br/>Epoch 26/30<br/>782/782 [==============================] - 39s 50ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 1.3154 - val_accuracy: 0.8118<br/>Epoch 27/30<br/>782/782 [==============================] - 42s 54ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 1.4012 - val_accuracy: 0.8106<br/>Epoch 28/30<br/>782/782 [==============================] - 40s 51ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 1.4809 - val_accuracy: 0.8134<br/>Epoch 29/30<br/>782/782 [==============================] - 40s 51ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 1.4389 - val_accuracy: 0.8104<br/>Epoch 30/30<br/>782/782 [==============================] - 41s 52ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 1.2202 - val_accuracy: 0.8124</span></pre><p id="a92d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，训练精度和验证精度非常接近GRU模型。剧情是这样的:</p><pre class="ni nj nk nl gt nt nu nv nw aw nx bi"><span id="d481" class="me mf jj nu b gy ny nz l oa ob">plot_graphs(history, 'accuracy')<br/>plot_graphs(history, 'loss')</span></pre><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/32e8d2106207b7084c1faec0c8fc06f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*zE0YTGAKbTcuUypFK5eMgw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="3373" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">验证损失这次更好。</p><h2 id="d91c" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">结论</h2><p id="e1c1" class="pw-post-body-paragraph ky kz jj la b lb mx kk ld le my kn lg lh mz lj lk ll na ln lo lp nb lr ls lt im bi translated">在这篇文章中，我想解释什么是递归神经网络，以及为什么对于序列数据它比常规神经网络更好。然后演示了一个简单的RNN、GRU和LSTM模型的实现，该模型使用相同的数据集执行自然语言处理任务。这是一个特别的情感分析项目。</p><p id="cc31" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章展示了GRU和LSTM模型的准确性结果看起来相似。但这并不是一个普遍的结论。对于不同的项目，您可能会看到不同的结果。事实上，如果你改变一些参数，如神经元的数量，GRU或LSTM单位，层数，你可能会看到这个项目的结果有很大的不同。</p><p id="935c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请随时在<a class="ae jg" href="https://twitter.com/rashida048" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae jg" href="https://www.facebook.com/Regenerative-149425692134498" rel="noopener ugc nofollow" target="_blank">脸书页面</a>上关注我，并查看我的新<a class="ae jg" href="https://www.youtube.com/channel/UCzJgOvsJJPCXWytXWuVSeXw" rel="noopener ugc nofollow" target="_blank"> YouTube。频道</a>。</p><h2 id="a04a" class="me mf jj bd mg mh nc dn mj mk nd dp mm lh ne mo mp ll nf mr ms lp ng mu mv mw bi translated">更多阅读</h2><div class="is it gp gr iu on"><a rel="noopener follow" target="_blank" href="/convolutional-neural-network-good-understanding-of-the-layers-and-an-image-classification-example-a280bc02c13e"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jk gy z fp os fr fs ot fu fw ji bi translated">卷积神经网络:对层的良好理解和图像分类示例</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">包含了大量的信息</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ja on"/></div></div></a></div><div class="is it gp gr iu on"><a rel="noopener follow" target="_blank" href="/developing-a-convolutional-neural-network-model-using-the-unlabeled-image-files-directly-from-the-124180b8f21f"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jk gy z fp os fr fs ot fu fw ji bi translated">开发一个卷积神经网络模型使用未标记的图像文件直接从…</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">使用图像生成器将图像自动标记为子目录</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb ja on"/></div></div></a></div><div class="is it gp gr iu on"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-with-some-cool-visualizations-in-pythons-matplotlib-and-seaborn-library-99dde20d98bf"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jk gy z fp os fr fs ot fu fw ji bi translated">利用Python的Matplotlib和Seaborn库中一些很酷的可视化工具进行探索性的数据分析</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">探索国际足联数据集</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pd l oy oz pa ow pb ja on"/></div></div></a></div><div class="is it gp gr iu on"><a rel="noopener follow" target="_blank" href="/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jk gy z fp os fr fs ot fu fw ji bi translated">30个非常有用的熊猫函数，用于日常数据分析任务</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">熊猫小型张</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pe l oy oz pa ow pb ja on"/></div></div></a></div><div class="is it gp gr iu on"><a rel="noopener follow" target="_blank" href="/a-beginners-guide-to-match-any-pattern-using-regular-expressions-in-r-fd477ce4714c"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jk gy z fp os fr fs ot fu fw ji bi translated">使用R中的正则表达式匹配任何模式的初学者指南</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">这比你想象的要容易</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pf l oy oz pa ow pb ja on"/></div></div></a></div></div></div>    
</body>
</html>