<html>
<head>
<title>SHAP for XGBoost: From NP-completeness to polynomial time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost的SHAP:从NP完全到多项式时间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shap-for-xgboost-from-np-completness-to-polynomial-time-20c9e78907a0#2022-01-03">https://towardsdatascience.com/shap-for-xgboost-from-np-completness-to-polynomial-time-20c9e78907a0#2022-01-03</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""><h1 id="bf86" class="pw-post-title it iu iv bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">XGBoost的SHAP:从NP完全到多项式时间</h1></div><figure class="gl gn jt ju jv jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi js"><img src="../Images/991f276abe8207ee1c6e67090e6d9848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nJyC-l80_gGakSou"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">丹-克里斯蒂安·pădureț在<a class="ae kh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="ki kj kk"><p id="33d3" class="kl km kn ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj io bi translated">更新:发现我关于渐变提升的新书，<a class="ae kh" href="https://www.amazon.com/dp/B0BJ82S916" rel="noopener ugc nofollow" target="_blank">实用渐变提升</a>。这是用python中的许多例子对渐变增强的深入探究。</p></blockquote><div class="lk ll gp gr lm ln"><a href="https://www.amazon.com/dp/B0BJ82S916" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iw gy z fp ls fr fs lt fu fw iu bi translated">实用的渐变增强:深入探究Python中的渐变增强</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">这本书的梯度推进方法是为学生，学者，工程师和数据科学家谁希望…</h3></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kb ln"/></div></div></a></div><p id="2820" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">网络上有一些很好的文章，解释了如何使用和解释机器学习的Shapley值。例如参见<a class="me mf ep" href="https://medium.com/u/319122a619c6?source=post_page-----20c9e78907a0--------------------------------" rel="noopener" target="_blank">博士Dataman </a>的文章:</p><div class="lk ll gp gr lm ln"><a rel="noopener follow" target="_blank" href="/explain-your-model-with-the-shap-values-bc36aac4de3d"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iw gy z fp ls fr fs lt fu fw iu bi translated">用SHAP价值观解释你的模型</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">使用SHAP值来解释任何复杂的ML模型</h3></div><div class="mg l"><p class="bd b dl z fp ls fr fs lt fu fw dk translated">towardsdatascience.com</p></div></div><div class="lv l"><div class="mh l lx ly lz lv ma kb ln"/></div></div></a></div><p id="a036" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">然而，没有多少论文详细说明这些值是如何计算的。然而，这是一个非常有趣的主题，因为计算Shapley值是一个np完全问题，但一些库，如<a class="ae kh" href="https://shap.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> shap </a>可以在一个小故障中计算它们，即使是对于具有数百个特征的非常大的基于树的XGBoost模型。这怎么可能呢？</p><p id="dd93" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这就是我们将在本文中通过给出该方法的python实现来发现的。</p></div><div class="ab cl mi mj hz mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="io ip iq ir is"><h1 id="5040" class="mp mq iv bd mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm bi translated">ML方法用于ML方法</h1><p id="bb44" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">激发使用Shapley值的潜在想法是，理解一个现象的最好方法是为它建立一个模型。一旦你有了模型，你就可以玩它，数学分析它，模拟它，理解输入变量，内部参数和输出之间的关系。一句话，解释一下。</p><p id="c289" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">然而，当处理决策树的森林时，正如XGBoost、CatBoost和LightGBM所构建的，底层模型很难理解，因为它混合了数百个决策树。</p><p id="5172" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">因此，<a class="ae kh" href="https://arxiv.org/pdf/1705.07874.pdf" rel="noopener ugc nofollow" target="_blank"> SHAP的论文</a>提出在任何ML模型之上建立一个解释模型，这将带来对底层模型的一些洞察。为了便于理解这个解释模型，SHAP论文的作者建议使用一个简单的线性加法模型，该模型考虑以下三个特性:</p><ul class=""><li id="761e" class="ns nt iv ko b kp kq kt ku mb nu mc nv md nw lj nx ny nz oa bi translated">局部精度:特征重要性的总和必须等于预测值。</li><li id="49d1" class="ns nt iv ko b kp ob kt oc mb od mc oe md of lj nx ny nz oa bi translated">缺失:如果一个特性没有参与到模型中，那么相关的重要性必须为空。</li><li id="b9d3" class="ns nt iv ko b kp ob kt oc mb od mc oe md of lj nx ny nz oa bi translated">一致性:如果比较两个模型，其中一个模型对某个特性的贡献高于另一个模型，那么这个特性的重要性也必然高于另一个模型。</li></ul><p id="d18d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">信不信由你，但只有一种价值观符合这些要求:由诺贝尔经济学奖获得者沙普利创造的价值观，这些价值观以他的名字命名。即使有不止一个公式，也只有一种方法来计算它们。最简单的一个是:</p><figure class="oh oi oj ok gt jw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b6edda31668a16eb885b611a20a46b0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*y51EoXwiG-J3U6i0mtXRug.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">作者的公式。</p></figure><p id="63ab" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">其中n指定模型中存在的特征的数量，R是这些特征的可能排列的集合，PiR是索引低于所考虑的排列的I的特征的列表，f是必须计算其Shapley值的模型。</p><p id="eb82" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">请注意，维数为n的集合的排列数是n的阶乘，因此n！以总数为因子。</p><p id="5820" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这种方法的工作原理简单而通用。它适用于任何类型的模型:它包括为每个可能的子模型建立没有特征I的模型。为此，扫描所有可能的排列。然后计算每个模型和具有所考虑特征的相同模型所获得的预测之间的差异。根据Shapley，该差异的平均值给出了特征的重要性。</p><p id="ceae" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">虽然非常简单，但在一般情况下，该公式的计算时间非常昂贵，因为要训练的模型数量随着特征数量的增加而成阶乘增加。因此是np完全的。有了x₂x₁的两个特征，可以为特征1建造两个模型:一个没有任何特征，一个只有x₂.</p><p id="dbbe" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">有了三个特征，它已经更复杂了。可以构建6个模型:2个不带特征，1个带x₂，1个带x₃，1个带x₂和x₃，1个带x₃和x₂.此外，必须为每个预测迭代操作。</p><h1 id="4fe4" class="mp mq iv bd mr ms ol mu mv mw om my mz na on nc nd ne oo ng nh ni op nk nl nm bi translated">线性模型的Shapley值</h1><p id="c9b7" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">为了理解这个概念，下面给出了SHAP方法的一个实现，首先是线性模型:</p><figure class="oh oi oj ok gt jw"><div class="bz fp l di"><div class="oq or l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">计算线性模型的Shapley值。作者代码。</p></figure><p id="28c1" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">第一个函数列出了n个特征的所有可能的排列。因此，它建立了前一个公式的集合R。</p><p id="77a0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">该函数<em class="kn"> compute_theta_i </em>构成了该方法的核心，因为它将计算给定特征I的theta值。为此，它将遍历所有可能的排列，构建包含和不包含该特征的集合，最后使用该模型进行两次预测，并计算其差值。</p><p id="14d9" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">然后计算这些差值的总和，用特征数量的阶乘的倒数进行加权。</p><p id="e1ae" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">然后使用函数<em class="kn"> train_linear_model对回归数据训练的两个模型进行测试。</em></p><p id="93b6" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">第一个模型只使用了两个特性。获得的θ值与理论非常一致，因为它们等于特征与相应回归系数的乘积。具有3个特征的模型也是如此。<br/> <br/>这证实了实现是正确的，并提供了理论预测的结果。</p><h1 id="55af" class="mp mq iv bd mr ms ol mu mv mw om my mz na on nc nd ne oo ng nh ni op nk nl nm bi translated">通用实现</h1><p id="a9c5" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">为了支持任何类型的模型，对先前的代码进行改进以对每个特征子集执行重新训练就足够了。</p><p id="3307" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">注意，在线性模型的情况下，重新训练是没有用的。事实上，线性模型本质上是相加的，删除一个特征意味着不考虑它，给它赋一个空值。</p><p id="e6b3" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">修改后的代码如下:</p><figure class="oh oi oj ok gt jw"><div class="bz fp l di"><div class="oq or l"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">计算任何类型模型的Shapley值。作者代码。</p></figure><p id="a6e3" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">不同排列的计算保持不变。</p><p id="ab49" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">引入了一个ZeroModel类来训练没有任何特征的模型。按照惯例，这种类型的模型返回零。</p><p id="b0b0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">执行训练的函数已更改为获取有用的数据。这一次，它不是为线性模型定型，而是为回归定型XGBoost模型。为了避免过度学习，估计器的数量和深度已经减少。</p><p id="5575" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">事实上，在过度拟合的情况下，计算的Shapley值是无效的，因为模型有足够的自由度来拟合数据，即使只有单个特征。然后，它会在每种情况下做出几乎精确的预测，所有特征最终都具有相同的Shapley值。<br/>最后，计算Shapley值的方法本身已得到改进，以执行重新训练。最有趣的部分是关于有和没有要加权的特征的特征集的生成。</p><p id="de18" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这个新的实现可以像以前一样在相同的数据集上进行测试。shap库也用于确保计算出的值是一致的。</p><p id="947e" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">由于Shapley值的总和给出了预测值，因此局部精度特性受到了很好的重视。<br/>此外，该代码获得的值与shap库提供的值符号相同。数量级相当。<br/>数据越复杂，差距越小。</p><p id="bdcd" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这种差异是由于shap库使用的方法，该方法利用决策树的结构，而不是像这里一样重新计算所有模型。</p><h1 id="f6e0" class="mp mq iv bd mr ms ol mu mv mw om my mz na on nc nd ne oo ng nh ni op nk nl nm bi translated">计算基于树的模型的Shapley值</h1><p id="35c1" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">上一小节中的方法仅用于教学目的。</p><p id="35af" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">实际上，构建n因子模型的需求是禁止的。即使是5个特性，我们也需要训练不少于5个！=120个模型，这与要分析的预测一样多。<br/>幸运的是，有一个解决方案，由SHAP方法的作者提出，利用决策树的结构，大大减少计算时间。然后只需要训练一个模型。</p><p id="be48" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">请注意，计算Shapley值的一般方法是一个NP完全问题。也就是说，没有办法在多项式时间内计算它们。</p><h2 id="3204" class="os mq iv bd mr ot ou dn mv ov ow dp mz mb ox oy nd mc oz pa nh md pb pc nl pd bi translated">决策树的存储信息</h2><p id="b6e4" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">首先提醒一下，在决策树的构建过程中，每个节点的增益、权重和覆盖都是存储的。这些值用于计算特征重要性，但也可用于以较低的成本计算Shapley值的良好估计。</p><h2 id="929a" class="os mq iv bd mr ot ou dn mv ov ow dp mz mb ox oy nd mc oz pa nh md pb pc nl pd bi translated">SHAP方法的开发</h2><p id="baad" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">Lundberg和Lee在他们关于该主题的第一篇出版物的补充论文中，提出了在决策树的情况下计算Shapley值的多项式时间实现。</p><p id="e367" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这个想法是依靠单一的模型，从而避免必须训练快速指数数量的模型。为了做到这一点，他们使用与树叶和盖子相关的重量。目标是从这个单一模型中获得所有可能的特征组合的预测。</p><p id="1606" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">方法如下:对于给定的观察值，以及对于要计算Shapley值的特征，我们简单地遍历模型的决策树。</p><p id="f81d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">在每一个节点上，如果决策涉及子集的一个特征，那么所有的事情都像一个标准的行走一样发生。另一方面，如果节点处的决策是基于没有被子集选择的特征，则不可能选择跟随树的哪个分支。在这种情况下，对两个分支都进行了探索，并且通过覆盖对结果权重进行加权，即通过测试所涉及的观察数量进行加权。</p><p id="1a43" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">剩下的就是计算没有特征的子模型和具有特征的子模型之间的差异，并对其进行平均。</p><p id="6921" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这个策略被用在SHAP库中，该库在上面被用来验证所提出的通用实现。</p><h1 id="1ea6" class="mp mq iv bd mr ms ol mu mv mw om my mz na on nc nd ne oo ng nh ni op nk nl nm bi translated">结论</h1><p id="a80e" class="pw-post-body-paragraph kl km iv ko b kp nn kr ks kt no kv kw mb np kz la mc nq ld le md nr lh li lj io bi translated">本文给出了计算任何模型的Shapley值的最小代码。</p><p id="370c" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">然而，如介绍中所述，该方法是NP完全的，并且不能在多项式时间内计算。</p><p id="51a4" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">SHAP正在使用一个技巧来快速计算Shapley值，重用先前计算的决策树值。</p></div></div>    
</body>
</html>