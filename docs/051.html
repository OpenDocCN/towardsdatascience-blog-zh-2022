<html>
<head>
<title>Sorry Ladies… This Is Still a MAN’s World!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对不起女士们…这仍然是一个男人的世界！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sorry-ladies-this-is-still-a-mans-world-1e5195c5fcbd#2022-01-03">https://towardsdatascience.com/sorry-ladies-this-is-still-a-mans-world-1e5195c5fcbd#2022-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="12fd" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fairness-and-bias" rel="noopener" target="_blank">公平和偏见</a></h2><div class=""><h1 id="7bba" class="pw-post-title iy iz iq bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated"><strong class="ak">对不起，女士们……这仍然是一个男人的世界！</strong></h1></div><div class=""><h2 id="7599" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">算法如何助长性别偏见</strong></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/115f11db9ed34c8482fbad71e3322581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4WUiYRs-qfXaKlR3"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">克里斯蒂安·布纳在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c88c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">到现在为止，你一定听说过，也许已经忘记了亚马逊的自动招聘工具基于性别的歧视(Dastin，2018)。但是性别偏见兔子洞<em class="mk">在人工智能中到底有多深？除了女性的简历被财富500强公司的可怕算法忽略之外，还有什么其他后果？本文将快速探究一系列涉及性别问题的不幸事件，旨在探索数据集内的不一致如何影响算法偏差，以及这些暗示对特定群体可能意味着什么。</em></p><h1 id="2534" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated"><em class="nd">重复亚马逊的故事</em></h1><p id="1e7a" class="pw-post-body-paragraph lf lg iq lh b li ne ka lk ll nf kd ln lo ng lq lr ls nh lu lv lw ni ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"/>经过适当训练的人工智能模型是已经学会基于具有代表性和包容性的高质量数据集进行预测的模型(Buolamwini等人，2018)。然而，显然并非在所有情况下都是如此，因为训练算法的数据可能会过度代表一个群体。亚马逊的失败证明了这一点，他们的招聘模式可以识别模式，并根据10年来主要来自男性的申请做出预测，从而提出对他们有利的建议。</p><p id="9849" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这一事件凸显了数据收集对算法训练过程的重要性:如果在深度学习发生时，训练数据是基于偏见的，那么结果将对参与倾向产生负面影响。换句话说，如果算法一次又一次地被输入某种类型的数据，它不会奇迹般地自动包含它没有学到的东西，从而导致人工智能中的排他性和偏见放大的支持，正如上面的场景所证明的那样——多少有才华的女性因为亚马逊的性别歧视招聘工具而被剥夺了机会？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/579ff440313d33ba4009e2fb6f8e5514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otMAaFIDAJLps6KL6QTFRg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://unsplash.com/s/photos/error?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae le" href="https://unsplash.com/@elisa_ventur?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Elisa Ventur </a>拍摄的照片</p></figure><p id="6a5d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更进一步说，自适应算法有能力调整其学习以促进新的发展，从而做出最佳决策；因此，这可能解释了为什么亚马逊的招聘模式仍然处于有偏见的循环中(拉万基2018)。但是这对普通人来说意味着什么呢？我是说，吉尔？</p><p id="5866" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">算法是人类工作的结果，从数据收集一直到模型训练，这是问题的关键:组成该领域的专业人员中只有22%是女性(Young等人，2021年)，这强化了为什么代表性不足会严重影响结果。如果那些管理机器学习过程的人在确定需要什么和多少训练数据来提高准确性时，不考虑与性别动态相关的公平和道德特征，最终结果将是不具代表性和不可取的结果。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><h1 id="07fb" class="ml mm iq bd mn mo nr mq mr ms ns mu mv kf nt kg mx ki nu kj mz kl nv km nb nc bi translated"><em class="nd">禁果——女人Vs苹果</em></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/d48ca59c599c793a0b96a0a8b14a9327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-skizTOY9x4mn1faj_t20Q.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@seankkkkkkkkkkkkkk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">肖恩·孔</a>在<a class="ae le" href="https://unsplash.com/s/photos/woman-apple?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="2a64" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">现在，想象一下，亚马逊的招聘工具甚至不考虑你的申请，因为你是女性，却被苹果公司的性别歧视信用额度打了一巴掌。2019年，有人对Apple card似乎厌恶女性的偏好进行了几次投诉；虽然调查清除了这家科技巨头的任何算法“违规行为”——对近40万名申请人的<em class="mk">核保</em> <em class="mk">数据</em>进行了彻底检查(沙欣等人，2021)——但一些非常重要的问题仍有待回答。</p><p id="8965" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">谁探测了算法？是否对训练数据集进行了可能未被发现的偏差检查？教授模型的数据是如何收集和丰富的？这些肯定是最基本的步骤，以确保用于确定苹果公司信用价值的算法实际上是性别中立的，而不仅仅是筛选一些记录来迅速安抚大众:这与正在讨论的机器驱动程序无关，该程序对一种性别抱有偏见。</p><p id="998c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果算法开发的决策过程在很大程度上是由男性主导的(D'Ignazio等人，2020)，那么机器智能的设计师在多大程度上系统地和自愿地消除他们创建的模型中存在的遗传偏见，以消除性别偏见？反对使用基于肤浅的男女二分法的主观数据标签的动力是什么？充满传统性别观念的训练数据将永远产生增加有害的意识形态不对称的统计模型，其影响表明性别不平等的永久化(Leavy，2018)。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><h1 id="5755" class="ml mm iq bd mn mo nr mq mr ms ns mu mv kf nt kg mx ki nu kj mz kl nv km nb nc bi translated"><strong class="ak"> Bowdlerizing </strong>偏见——性别动态中的公平</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/6af47c0599d16b8ad26060a44d58d63a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qeeGTCChaZ0xFTVyta1d6A.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@thatsherbusiness?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拍摄那是她的事</a>在<a class="ae le" href="https://unsplash.com/s/photos/equality?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="29b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一个人不需要非常理智就能感知“服务器机房中的大象”(D'Ignazio等人，2020):有偏见的人工智能模型来自有偏见的历史训练数据，而这些数据又来自有偏见的人类特征。为此，如果要在算法程序中实现性别平等，那么具有代表性和标准化的历史数据应该用于模型训练目的。</p><p id="4ab1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这需要重新评估算法的开发方式，包括对数据收集过程的仔细审查，对数据的仔细管理，以及在深度学习过程之前、期间和之后对人类交互、干预和干扰训练数据的彻底检查。Gebru等人(2018年)进一步提出，数据集应附有详细说明其应用、设计、传播、基本原理、框架和延续的规范，以期增强问责制和透明度。</p><p id="5c65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">偏见极大地影响规范行为和社会原则；因此，必须消除这种现象，以阻止不断强化旨在征服的意识形态——否则就永远不会享有社会正义。因此，应该不断扩大关于机器学习中性别偏见的对话，以便开发最佳实践，消除支持霸权力量的人工智能特征，努力过渡到更加民主、包容和公平的数字经济。</p><p id="8d78" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">文献学</p><p id="a6d7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">BBC (2019)苹果公司的“性别歧视”信用卡被美国监管机构调查。可在线查阅:https://www.bbc.co.uk/news/business-50365609[2021年12月28日查阅]。</p><p id="96fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Buolamwini，j .和Gebru，T. (2018年)。机器学习研究会议录81:1–15。<em class="mk"> 2018年公平、问责和透明性别阴影会议:商业性别分类的交叉准确性差异。</em>纽约，2月23日至24日，1日至15日。</p><p id="63c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">杰弗里·达斯廷(2018)。亚马逊废弃了显示对女性有偏见的秘密人工智能招聘工具。网上有:https://www . Reuters . com/article/us-Amazon-com-jobs-automation-insight/Amazon-scraps-secret-ai-recruiting-tool-that-show-bias-against-women-iduskcn 1 MK 08g[2021年12月28日访问]。</p><p id="6038" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">D'Ignazio，c .和Klein，L. (2020年)。数据女权主义。剑桥:麻省理工学院出版社。</p><p id="5a77" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Gebru，j . Morgenstern，b . Vecchione，b . wort man Vaughan，j . Wallach，h . daum III，h .和Crawford，K. (2018年)。数据集的数据表。可在线查阅:https://www . Microsoft . com/en-us/research/uploads/prod/2019/01/1803.09010 . pdf[2022年2月1日查阅]。</p><p id="61f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Lambrecht，A和Tucker，C E (2019年)。<em class="mk">算法偏差？STEM招聘广告中明显性别歧视的实证研究。</em>管理科学，65卷7期。第2966-2981页。</p><p id="d408" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">拉万基，男(2018)。亚马逊性别歧视的招聘算法仍然可能比人类更好。期望算法完美执行可能对我们自己要求过高。可在线查阅:https://www . IMD . org/research-knowledge/articles/amazons-sex ist-hiring-algorithm-can-been-be-better-than-a-human/[2021年12月28日查阅]。</p><p id="83a6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Leavy，S. (2018)人工智能中的性别偏见:机器学习中对多样性和性别理论的需求Susan Leavy。在线提供:https://dl.acm.org/doi/pdf/10.1145/3195570.3195580?casa _ token = 01 yph _ loxi 4 aaaaa:G1 bwwkoywh 4 ovozgbvhtggm _ w5 cxdrot 5 ujgpzjpmbo 0 liv dnxdnjpoojlzzghve-fahiehymkdeg。[访问日期:2022年1月1日]。</p><p id="8dee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Nasiripour，s .和Farrel，G. (2021)高盛在纽约审查苹果卡时澄清了偏见。网上有:https://www . Bloomberg . com/news/articles/2021-03-23/Goldman-not-discrimina-with-apple-card-n-y-regulator-says。访问日期:2022年1月1日。</p><p id="7a8e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">史密斯和鲁斯塔吉(2021年)。当好的算法变得性别歧视:为什么以及如何推进人工智能性别平等。<em class="mk">斯坦福社会创新评论</em>。https://doi.org/10.48558/A179-B138</p><p id="2c0a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Young，e .，Wajcman，j .和Sprejer，L. (2021年)。女人在哪里？绘制人工智能中的性别工作差距。政策简报:完整报告。<em class="mk">艾伦图灵研究所</em>。</p></div></div>    
</body>
</html>