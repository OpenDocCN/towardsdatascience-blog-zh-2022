# 易受骗的算法

> 原文：<https://towardsdatascience.com/gullible-algorithms-67da88d423ad>

## 如何对机器学习模型进行社会工程

![](img/205fb1466e4a08c2c9df24b57ca29737.png)

图片由[皮克斯拜](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5276925)的 Gerd Altmann 提供

人类容易上当，我也不例外。我最尴尬的一次是在卖钢琴的时候被骗了。网上买家看起来很真诚。这架钢琴是送给她侄子的生日礼物。因为它的大小和重量，她雇佣了一家搬家公司来搬运。我收到了她的邮寄支票，并把它存入了我的支票账户。她在支票上加了一些钱，这样我就可以支付搬运工的费用——搬运工后来坚持说他们需要钱汇给他们。当然有警告信号。对我来说付钱给搬运工很奇怪。电子邮件地址有点不对。事后看来，这张支票看起来也有点奇怪。不用说，搬运工根本没来，银行也撤销了这张伪造的支票。我上当了，赔了钱。

社会工程有多种形式，骗局只是其中一种。熟练的社会工程师挖掘我们的偏见、恐惧、紧迫感和社会习俗。例如，社交工程师发送钓鱼邮件，引诱易受影响的用户点击链接或打开附件。由于我们收到的大多数电子邮件都是合法的，我们默认该电子邮件是无害的。对手利用这种倾向来捕食人类。

社会工程师利用了我们对不良后果的恐惧。社会工程师可能会弹出一个窗口，说你的电脑被感染了。在那一刻，恐惧可能会影响你的判断，你可能会让攻击者控制你的电脑来“修复”问题。

除了这种恐惧，社会工程师也激发了我们的紧迫感。例如，针对组织的炸弹威胁勒索邮件现在很常见。这些电子邮件试图在截止日期前强制支付加密货币。尽管这些炸弹威胁勒索的企图大多是不真实的，恐惧和最后期限的结合可能会迫使你采取行动。

社会工程师也利用了我们乐于助人的愿望。当一个员工进来的时候，一个社会工程师可能会出现在你的大门前，手里拿着满满的东西。人们乐于助人是一种习惯，所以我们自然倾向于为他们开门。这使得社会工程师可以绕过安全系统，未经授权进入大楼。

人们没有那么难被欺骗。既然人类如此容易受到社会工程的影响，那么模仿人类智能的机器学习算法呢？

神经网络模仿人脑中的神经元。该算法在训练网络时给人工神经元之间的链接分配不同的权重。当输入数据出现时，神经网络通过多层神经元传递数据，以在另一端创建决策或标签。

决策树是一种类似于人类逻辑的算法。当一个人计划出一个决定或一系列行动时，通常使用规则来帮助确定预定义的结果。决策树被设计用来学习这些类型的决策规则，而不用硬编码逻辑。

贝叶斯算法使用概率和信念。这类似于人们如何利用先前的经验做决定。通过确定哪种情况最有可能是真的，贝叶斯算法可以模式化人类如何确定不同场景的可能性，以帮助做出决策。

支持向量机(SVM)着眼于项目之间的相似性。通过使用比较，他们决定某样东西是否像用于训练 SVM 的特征集。这也非常类似于我们的大脑如何在物体之间建立联系。

最后，遗传算法模仿进化概念，比如适者生存。他们使用随机选择和适应度函数来选择最佳选项。这些算法是在繁殖方法之后形成的，并且可能在人类如何发展方面有一些基础。

因此，如果我们的机器学习算法如此频繁地基于人类思维过程和生物学，它们可能会像我们一样容易受到社会工程的影响。如果他们易受影响，会有什么影响？我们如何着手社会工程和算法？我们到底有没有考虑算法的易受骗性？

针对机器学习算法的社会工程攻击的含义非常重要，因为它在如此多的不同应用中被越来越多地采用。因此，机器学习为网络对手创造了一种新的攻击服务。社会工程师的目标可能是逃避检测。这可能是为了利用算法的偏见。社会工程师可能会攻击机器学习算法，对信息进行错误分类。对手可能会勒索赎金，以阻止算法充斥不良信息。此外还有安全问题。如果自动驾驶汽车或自动化工厂机器人可能被误导到危及生命的危险场景中，会怎么样？

为了帮助回答这个问题，如果机器学习算法容易受到社会工程的攻击，我们只需要一个例子。漏洞就像你在家里发现的一只老鼠。很少只有一个。因此我们可以推断，如果这些算法容易受到一种形式的社会工程的攻击，它们可能容易受到许多形式的攻击。

在这个例子中，研究人员避开了一个名为 PDFrate 的在线恶意软件检测系统[1]。该系统用于检测嵌入用户上传的 PDF 文档中的恶意软件。PDFrate 使用随机森林算法，并在公开可用的数据集上进行训练。

研究人员从侦察开始。他们提出，如果对手知道使用了什么算法，知道使用了什么训练数据，或者使用了什么特定特征来训练算法，他们就可以实施成功的攻击。在 PDFrate 一案中，几乎不需要侦察，因为所有信息都是公开的。研究人员断言，并非所有这些信息都是策划攻击所必需的，但如果这三个组成部分都可用，攻击就会容易得多。

一旦他们知道了训练数据、特征和算法，他们就创建了一个代理系统，他们可以自由地攻击而不被检测到。有了这种自由，他们可以尝试数千种攻击组合，直到找到成功的特征组合。在他们创建了代理算法后，研究人员确定了最重要的特征，并绘制了如何将逃避性恶意软件嵌入 PDF 文件中。

对于最重要的特征，他们分析了训练数据中的值的分布。通过确保他们的恶意软件注入 PDF 的特征落在平均值附近或平均值的一个标准偏差内，该算法得出结论，PDF 文件是正常的。这只对最重要的特征是必要的，因为其他特征中的值没有足够的权重来触发警报。

使用这种方法，他们建立了基于算法、特征和训练数据的完整和部分知识的代理分类器。如果没有规避，PDFrate 通常可以检测到接近 100%的恶意软件样本，但使用这种技术，研究人员可以用 75%的恶意软件样本规避检测。

除了用代理算法进行实验，研究人员还发现代理数据集可以用来替代实际的训练数据。他们发现并不是所有的特征都需要被识别，因为即使是特征也可以在不知道所有信息的情况下被近似。最后，他们甚至发现，分类器不必完全相同，而是相似类型的算法适合于策划一次成功的攻击。

从这个例子中，很明显，对机器学习算法进行社会工程化是可能的。这只是一种方法。研究人员利用了算法的偏差，就像一个人可能会表现出确认偏差一样。如果他们习惯于看到正常的信息，他们更有可能将异常归类为正常。

知道我们的算法可能容易受骗，我们能做些什么呢？以下是在将您的算法用于在线生产系统之前需要考虑的一些准则。

首先考虑使用易受影响的算法是否会产生太大的风险。如果一个错误决策的潜在结果是危及生命的，导致操作问题，或者错过检测关键信息，那么最好是重新考虑使用机器学习算法。

对算法、训练数据和特征的信息保密至关重要。数据科学家和机器学习倡导者通常会发表他们的研究，公开谈论他们的算法，并对训练数据保持透明。当您的算法将在线使用时，对这些信息的谨慎是至关重要的。

此示例还展示了可能让对手隐藏在数据正态分布中的偏差。有些算法比其他算法更容易受到影响。考虑使用集成方法，在这种方法中，不同的算法使对手更难成功。

这项研究还表明，根据静态数据训练的算法需要定期重新训练。在使用你的算法之前，考虑一下你如何能够包含周期性的再训练。同时让一个人在循环中检查数据，并确保算法没有被滥用。

这只是社会工程的一个例子，可能还有更多。针对机器学习算法保持最新的威胁模型。

网络安全是一场升级的游戏。随着组织弥补其系统中的漏洞，网络对手开始寻找新的攻击方式。如果你认为你已经解决了算法中的所有漏洞，并且保护了数据，请记住，对手会探索你没有考虑到的可能性。

最后，你的算法容易受骗吗？随着机器学习方法和应用的发展，我们应该考虑算法易受骗性的含义。机器学习算法可能会成为下一个攻击面，如果我们给它们太多的自主权，我们可能会后悔。

保护您的算法、保护您的训练数据，以及保护有关您选择的功能的信息。对你的信息保密。考虑集成技术来分散和迷惑对手，并确保您使用定期重新训练来制作更健壮和更不易受影响的算法。这些方法只是一个起点。让安全专家为威胁建模，领先于算法社会工程师。

[1] N. Srndic 和 P. Laskov，[规避基于学习的分类器:案例研究](https://ieeexplore.ieee.org/abstract/document/6956565) (2014)，IEEE 安全和隐私研讨会