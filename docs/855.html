<html>
<head>
<title>Exploring Large Collections of Documents with Unsupervised Topic Modelling — Part 2/4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用无监督主题建模探索大型文档集—第2/4部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-2-4-8897248053d3#2022-01-31">https://towardsdatascience.com/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-2-4-8897248053d3#2022-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="c72f" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">利用无监督主题建模探索大型文档集—第2/4部分</h1></div><div class=""><h2 id="4ce4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过聚类了解文档分布</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9adf31568b4f19b17bbba85b4252a4a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Un0oaKJyummD-WoywtEFw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="e65a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一系列的文章中，我们将重点探索基于主题建模的大量未标记文档。我们假设除了语料库的上下文之外，我们对语料库的内容一无所知。我们的目标是用一些新的、量化的知识来完成对语料库中所讨论内容的探索。</p><ul class=""><li id="94b0" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">转到本系列的<a class="ae ma" rel="noopener" target="_blank" href="/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-1-4-404f4931dab7">第1部分。</a></li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="6c99" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">关于前一部分…</h2><p id="d39e" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在这个系列的<a class="ae ma" rel="noopener" target="_blank" href="/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-1-4-404f4931dab7">第一部分</a>中，我们看到了如何执行主题建模，并且回顾了几种尝试和解释提取的主题的方法(这不是一个简单的任务！).我们收集的文件由1904个短篇恐怖故事组成，摘自<a class="ae ma" href="https://www.reddit.com/r/shortscarystories/" rel="noopener ugc nofollow" target="_blank">r/short cary stories subreddit</a>。如果你想学习如何自己提取这些和其他的，请阅读我的帖子<a class="ae ma" rel="noopener" target="_blank" href="/how-to-collect-a-reddit-dataset-c369de539114">，详细介绍整个过程</a>。根据Reddit的<a class="ae ma" href="https://www.reddit.com/wiki/api-terms" rel="noopener ugc nofollow" target="_blank">使用条款</a>，这些数据是使用Reddit的API ( <a class="ae ma" href="https://praw.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> PRAW </a>和<a class="ae ma" href="https://pypi.org/project/psaw/" rel="noopener ugc nofollow" target="_blank"> PSAW </a>收集的。和前一部分一样，我们假设文本已经过充分的预处理，文档中的单词由一个空格分隔。</p><p id="574b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，从一个完全未知的短篇恐怖故事集开始，我们学到了以下内容:</p><ol class=""><li id="60e2" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq ng lx ly lz bi translated">一些恐怖故事发生在圣诞节期间，或者以某种方式与一年中的那个时候有关。</li><li id="596b" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">一些恐怖故事涉及家庭成员。</li><li id="31c2" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">一些恐怖故事涉及有趣的物体或地点。</li><li id="3597" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">我们知道哪些恐怖故事具有这些特征，并能准确量化它们具有多少这些特征:<br/> →所有故事中0.5%的故事在topic 0 — Christmas中的权重超过0.5；<br/> → 2.3%的所有故事在主题7 —感兴趣的物体/位置中的权重超过0.5；<br/> → 4.7%的故事在话题8——家庭成员中的权重超过0.5。</li><li id="e7eb" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">通过同样的量化，我们可以确定一个给定的故事是否比任何其他故事与这些特征更相关。</li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="a54c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是4部分系列的第二部分。让我们开始吧。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="35cf" class="nm mj iq bd mk nn no np mn nq nr ns mq jw nt jx mt jz nu ka mw kc nv kd mz nw bi translated">介绍</h1><h2 id="66e1" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">问题是</h2><p id="e25d" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">从本系列第一部分的结论中，我们看到我们可以做出类似“4.7%的文集以某种方式提到家庭成员”的陈述。但这是我们有目的地去寻找的东西，依赖于特定的查询。我们现在感兴趣的是学习如何概括这种说法，让我们回答以下问题:</p><ol class=""><li id="2c3a" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq ng lx ly lz bi translated">就提到的话题而言，所有的恐怖故事都是相似的吗？</li><li id="1735" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">如果不是，它们是如何关联或分组的？</li><li id="2180" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated">如果有多个小组，每个小组讨论什么？</li></ol><h2 id="1fb0" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">拟议解决方案</h2><p id="eaca" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在本帖中，我们将看到如何依靠聚类技术来回答上述问题。换句话说，我们尝试根据主题分布对文档进行分组，并根据这些结果回答这些问题。</p><h1 id="9f16" class="nm mj iq bd mk nn nx np mn nq ny ns mq jw nz jx mt jz oa ka mw kc ob kd mz nw bi translated">履行</h1><h2 id="8614" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">聚类101</h2><p id="935f" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">聚类是试图将给定人群的样本分组到离散的组中的任务。这些样本属于给定的D维空间，并且，一般来说，它们越接近，它们越有可能属于同一组或聚类。然而，我们没有一个标签或任何指示，表明具体的样本应该聚集在一起。</p><p id="a97e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，我们如何知道样本的一个聚类比另一个更好呢？我们永远无法确定，因为，再说一次，我们没有黄金标准，但我们可以评估D维空间中的聚类和样本分布。例如，我们可以说，一个聚类越紧密(低聚类样本方差)，它的定义就越好。我们也可以说，集群之间的距离越大，它们的定义就越清晰。下图举例说明了这些概念(请记住，我们也不知道可能有多少个集群，所以我们需要进行一些实验，或者使用其他指标，我们将在后面的部分中讨论)。我们也在2D思考和概念化，因为它更容易形象化，但这很少发生。例如，在我们的<em class="oc"> k </em>主题的例子中，每个文档都被投影在一个<em class="oc"> k </em>维空间中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/6726ee3f79023e2fcf12da1add4d130f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDMmY4CVY-QxSSLMT8otdQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">以各种方式在二维空间中对样本进行聚类。图片作者。</p></figure><h2 id="257a" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">文档聚类—第一种方法</h2><p id="bde5" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">关注我们的主题建模环境，让我们假设我们的语料库是根据主题分布来描述的。也就是说，对于任何给定的文档，我们都有一个权重向量对应于每一个<em class="oc"> k </em>主题。因此，每个文档向量是<em class="oc"> k- </em>维空间中的一个样本。</p><p id="166e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们假设我们知道在主题空间中有多少个文档集群。具体说就是<em class="oc"> C=10。</em>这就是我们如何用<a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> K-Means </a>算法对它们进行聚类:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="2270" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的代码中，我们用指定数量的集群<em class="oc"> C </em>实例化了集群类，然后对主题空间中投影的文档进行集群，返回给每个文档的集群标签。预计文档(变量<em class="oc"> doc_topic </em>)在本系列第一部分<a class="ae ma" rel="noopener" target="_blank" href="/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-1-4-404f4931dab7">之后获得。</a></p><h2 id="17fc" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">文档聚类—找到理想的聚类数</h2><p id="1be4" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">然而，正如我们之前所说的，我们不知道理想的集群数量，我们也永远无法确切知道。在一个<em class="oc"> D </em> -dim空间(其中<em class="oc"> D </em>远高于2)中，应对这一挑战的标准方法是观察样本集群分布指标，并找到集群数量的最佳点。一些常见的指标是:</p><ol class=""><li id="9726" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq ng lx ly lz bi translated"><a class="ae ma" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">惯性</strong> </a> <strong class="kx ir"> <br/> </strong>对于给定的聚类模型，该度量作为整体给出。它报告每个样本与其最近的聚类中心之间的平方距离之和。惯性越低，聚类的定义越好——但在极限情况下，每个样本都是自己的单样本聚类，具有完美的惯性，但不是很有趣。该度量促进高度凸起的簇，具有尽可能最小的直径。</li><li id="9403" class="lr ls iq kx b ky nh lb ni le nj li nk lm nl lq ng lx ly lz bi translated"><a class="ae ma" href="https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">剪影系数</strong> </a> <strong class="kx ir"> <br/> </strong>这个分数范围从-1到1，适用于每一个样本。简而言之，一个样本与同一聚类中的所有其他样本越接近，该样本与下一个最近聚类中的所有样本越远，该样本的得分越高。因此，零左右的分数表示重叠的聚类。这个度量促进了高度凸的、分离良好的集群，就像上图中显示的那样。</li></ol><p id="2007" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过在一系列值上采取这些措施并找到“最佳点”，即平衡上述权衡，来找到最合适的集群数量。对于惯性，人们通常在图中寻找“肘”，也就是说，惯性只是从这个点稍微减小。这可以被认为是在更多的群中分离样本没有相关的增益。对于轮廓系数，通常寻找使所有样本的平均轮廓系数最大化的聚类数。</p><p id="3350" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以用以下代码绘制这些指标:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="166c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我们的恐怖故事文档的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/6d0f744cc150da9756dbee5d6597354e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4sZ6ChLv3nsZMf8WNq5F9Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">主题空间中投影文档的聚类度量。图片作者。</p></figure><p id="c0be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上面的图中，我们可以看到最大平均轮廓值是用10个聚类获得的。我们还在惯性图中看到了一个清晰的“肘”，在相同的集群数下。具体来说，在2到10个集群之间，惯性下降超过300点，而在10到20个集群之间，惯性仅下降大约75点。这是“在更多的群中分离样本没有相关的增益”的经验版本。从这两个图中，我们可以有把握地说<em class="oc"> C=10 </em>是一个合适的集群数。</p><h2 id="572d" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">文档聚类—样本轮廓</h2><p id="594d" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">既然我们已经决定了合适的集群数量，我们想知道样本是如何分布在这些集群中的。它们中的大部分是在一个巨大的星团中，而其他几个形成了微小的星团吗？还是分布好，分好？绘制每一个样本的轮廓系数，每一个聚类的颜色，可以让我们回答这些问题。</p><p id="fb07" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">绘制样本轮廓系数的代码如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="f3cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是我们10组文档的轮廓:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/0d5b3981e66af78d61436ed43a9774bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAV_lcx_Ujrhhs0CpgO2cQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">10个聚类的主题空间中每个文档的轮廓系数。图片作者。</p></figure><p id="e970" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的图中，每条彩色水平线是给定样品的轮廓。如前所述，该值介于-1和1之间。垂直虚线是平均轮廓分数，其与轮廓-惯性度量图中的最大值一致。从该图中，我们得出结论，样本分布似乎几乎是均匀的(除了少数例外)，但是每个聚类都具有密集的质心和分散的样本，因此每个聚类中的轮廓值快速下降。一个理想的、高密度的星团应该有一个类似于水平条的轮廓，而不是我们所拥有的尖三角形。这也反映在平均剪影分数(~0.28)并不太大。</p><h2 id="45b9" class="mi mj iq bd mk ml mm dn mn mo mp dp mq le mr ms mt li mu mv mw lm mx my mz na bi translated">文档聚类—他们在讨论什么？</h2><p id="a67e" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">假设我们知道每个主题是关于什么的，那么发现每个集群通常在讨论什么是相当简单的。一种简单的方法是在给定的群集中抽取每个样本，并观察哪些主题是“重要的”，即赋予它们很高的权重。简而言之，我们可以查看每个集群的平均主题分布，以推断该集群中通常讨论的内容。下面是代码和结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/74318da15562fefc9417deeacb3c19d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9rEymvIR-2Hbccu19AO0w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个聚类的平均主题重要性，共10个聚类(从0到9)。总共有10个主题(从0到9)。图片作者。</p></figure><p id="54fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上面的图表中，我们可以很容易地得出结论，一般来说，每个集群正在讨论什么。例如，我们可以陈述如下:“群集0包含对主题0非常重要的简短恐怖故事”。</p><h1 id="3560" class="nm mj iq bd mk nn nx np mn nq ny ns mq jw nz jx mt jz oa ka mw kc ob kd mz nw bi translated">结论</h1><p id="6619" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在第二部分(<a class="ae ma" rel="noopener" target="_blank" href="/exploring-large-collections-of-documents-with-unsupervised-topic-modelling-part-1-4-404f4931dab7">在这里阅读第一部分</a>，我们继续进一步探索我们收集的文件，这些文件我们在开始时一无所知。这一次，我们将重点放在集群上。</p><p id="4674" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们发现这些文档可以根据它们的主题分布分成10个组。我们了解到，这种分组虽然不是完美的，但也是定义良好的。我们还了解到，除了少数例外，这些组的规模大体相同。</p><p id="ba38" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们继续进一步探索每个集群在谈论什么，一般来说，这是由该集群的平均主题分布/重要性给出的。我们了解到，聚类主要基于一个单一的高权重主题，这解释了为什么<em class="oc"> C=10 </em>是最合适的聚类数(记住<em class="oc"> k=10 </em>是主题数)。</p><p id="a604" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们可以将这一分析与第一部分的分析结合起来，并为“话题X”的陈述添加语义，例如“圣诞节的话题”。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="d4d4" class="nm mj iq bd mk nn no np mn nq nr ns mq jw nt jx mt jz nu ka mw kc nv kd mz nw bi translated">下一部分</h1><p id="7364" class="pw-post-body-paragraph kv kw iq kx b ky nb jr la lb nc ju ld le nd lg lh li ne lk ll lm nf lo lp lq ij bi translated">在本系列的下一部分，我们将再次深入主题本身。我们将试图准确理解每个话题被提及的次数，或少，以及它与总体人口的关系。</p></div></div>    
</body>
</html>