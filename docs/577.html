<html>
<head>
<title>All You Need to Know about Gradient Boosting Algorithm − Part 1. Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于梯度提升算法，您只需知道第1部分。回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502#2022-01-20">https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502#2022-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="116e" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">关于梯度提升算法，您只需知道第1部分。回归</h1></div><div class=""><h2 id="fdb7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用例子、数学和代码解释算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e3e4da158343f2b00d0b7b74ab629737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cm1GxkkP4DCQe3Lg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卢卡·布拉沃在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d9f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度推进是表格数据集最流行的机器学习算法之一。它功能强大，足以找到模型目标和要素之间的任何非线性关系，并且具有强大的可用性，可以处理要素上的缺失值、异常值和高基数分类值，而无需任何特殊处理。虽然您可以使用一些流行的库(如<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或<a class="ae kv" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>)来构建准系统梯度增强树，而无需了解算法的任何细节，但是当您开始调整超参数、定制损失函数等时，您仍然想知道它是如何工作的。，以获得更好的模型质量。</p><p id="a3a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文旨在为您提供关于该算法的所有细节，特别是它的回归算法，包括从头开始的数学和Python代码。如果你对分类算法更感兴趣，请看<a class="ae kv" href="https://medium.com/p/d3ed8f56541e" rel="noopener"> Part 2 </a>。</p><h1 id="a96f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">算法与实例</h1><p id="9b4d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">梯度增强是集成方法的一种变体，在这种方法中，您可以创建多个弱模型并将它们组合在一起，以获得更好的整体性能。</p><p id="591f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本节中，我们使用下面的样本一步一步地构建梯度推进回归树，该样本在<code class="fe mp mq mr ms b"><em class="mt">x</em></code> <em class="mt"> </em>和<code class="fe mp mq mr ms b"><em class="mt">y</em></code>之间具有非线性关系，以直观地理解它是如何工作的(下面的所有图片都是作者创建的)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/85a8f53054536db83617f3a0815d63e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SZdrLzQaXBxSoTgRMtSxig.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">回归问题的示例</p></figure><p id="6ca9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是对目标<code class="fe mp mq mr ms b"><em class="mt">y</em></code>做出非常天真的预测。我们将初始预测<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>作为<code class="fe mp mq mr ms b"><em class="mt">y</em></code> <em class="mt"> : </em>的整体平均值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d3193d1fbc9c534f5540a60280f5e1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Cff0iDWUiC_-TnWzwUm_Vg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">初始预测:F0 =平均值(y)</p></figure><p id="96dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能会觉得用平均值来预测很傻，但是不要担心。当我们添加更多的弱模型时，我们将改进我们的预测。</p><p id="2f4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了改进我们的预测，我们将重点关注第一步的残差(即预测误差)，因为这是我们希望最小化以获得更好预测的内容。残差<code class="fe mp mq mr ms b"><em class="mt">r₁</em></code>在下图中显示为蓝色垂直线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/52335b3c052e8212a2edd7fd8a43dca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*32xSZb7FIIxwFQeq9MKCgQ.png"/></div></figure><p id="8d31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了最小化这些残差，我们正在建立一个回归树模型，以<code class="fe mp mq mr ms b"><em class="mt">x</em></code>为特征，残差<code class="fe mp mq mr ms b"><em class="mt">r₁ = y − mean(y)</em></code> <em class="mt"> </em>为目标。背后的原因是，如果我们可以通过建立附加的弱模型找到<code class="fe mp mq mr ms b"><em class="mt">x</em></code>和<code class="fe mp mq mr ms b"><em class="mt">r₁</em></code>之间的一些模式，我们就可以利用它来减少残差。</p><p id="7cb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了简化演示，我们正在构建非常简单的树，每个树只有一个分支和两个终端节点，称为“stump”。请注意，梯度推进树通常有一个稍微深一点的树，比如有8到32个终端节点的树。</p><p id="ed3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们创建第一棵树，用两个不同的值<code class="fe mp mq mr ms b"><em class="mt">γ₁ = {6.0, −5.9}</em></code>预测残差(我们用<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> (gamma)来表示预测)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/bf4a393a119efcd8a9667d5e2d07a30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3FYNWBEUO1bVZo4BulWIQ.png"/></div></div></figure><p id="7606" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该预测<code class="fe mp mq mr ms b"><em class="mt">γ₁</em></code>被添加到我们的初始预测<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>中，以减少残差。事实上，梯度推进算法并不是简单地将<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>加到<code class="fe mp mq mr ms b"><em class="mt">F</em></code>上，因为它使模型过度适应训练数据。相反，<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>按比例缩小<strong class="ky ir">学习率</strong> <code class="fe mp mq mr ms b"><em class="mt">ν</em></code>，范围在0和1之间，然后加到<code class="fe mp mq mr ms b"><em class="mt">F</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/06172a693276b88d2fe4248cffff3207.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*mFDwsLH5kAisZKYM3oxUoA.png"/></div></figure><p id="d3e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个例子中，我们使用了一个相对较大的学习率<code class="fe mp mq mr ms b"><em class="mt">ν = 0.9</em></code>来使优化过程更容易理解，但是它通常应该是一个小得多的值，例如0.1。</p><p id="6fc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更新后，我们的组合预测<code class="fe mp mq mr ms b"><em class="mt">F₁</em></code>变为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d9743dbf405de9e5e4174eafa395ee24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*B-wU7blCqWUWd_PWI4opnQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/d9e0b9654f2373ea44b14021b15b1485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*xDDhfFTC57fYHkhvlK4BTg.png"/></div></div></figure><p id="32d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，更新的残差<code class="fe mp mq mr ms b"><em class="mt">r₂</em></code>看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/bd53360f268e9c637314e070bffa8b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*L9GpEucS0n4VvveK0ssI-A.png"/></div></figure><p id="e854" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步，我们将使用相同的<code class="fe mp mq mr ms b"><em class="mt">x</em></code>作为特征，更新的残差<code class="fe mp mq mr ms b"><em class="mt">r₂</em></code>作为目标，再次创建回归树。下面是创建的树:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/b310c8d9b89ae20ee84ba7ed9ee8ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iVz3E-vN4EVNk1zUtd0pIg.png"/></div></div></figure><p id="2231" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们用新的树预测<code class="fe mp mq mr ms b"><em class="mt">γ₂</em></code>更新我们先前的组合预测<code class="fe mp mq mr ms b"><em class="mt">F₁</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d1dbeb7d981dd37a9489fd6c6bd627c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*7dtSxqYLS8teaSV7Zv7uUA.png"/></div></figure><p id="b947" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们重复这些步骤，直到模型预测停止改进。下图显示了从0到6次迭代的优化过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/79533aaf1b10988f106da9f9c36b4c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmFFQVKyDPAD5Qu66hHbxg.png"/></div></div></figure><p id="eac0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以看到，随着我们向组合模型<em class="mt">中添加更多的树，组合预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code> <em class="mt">越来越接近我们的目标</em> <code class="fe mp mq mr ms b">y</code>。这就是梯度增强如何通过组合多个弱模型来预测复杂目标。</em></p><h1 id="2de0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数学</h1><p id="1dd5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这一节中，我们将深入算法的数学细节。这是数学公式中的整个算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/ea536371fd005d302021c49253f023f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIHrPFBT2fmXuTXMb-3_Xw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:改编自<a class="ae kv" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">维基百科</a>和<a class="ae kv" href="https://jerryfriedman.su.domains/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">弗里德曼的论文</a></p></figure><p id="6549" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们一行一行地解开这个谜团。</p><h2 id="a713" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">第一步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/879827e243fed0cb240ed4cc8fac4510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1eTixNM_JzslkPWU4FkX2Q.png"/></div></div></figure><p id="a6b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是创建一个初始常数值预测<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>。<code class="fe mp mq mr ms b"><em class="mt">L</em></code>是损失函数，在我们的回归案例中是平方损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1dff453bb11b7e3ae1685722cd75e205.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*KXf-q-8lmgXP940-io2SVw.png"/></div></figure><p id="1ea7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">argmin</em></code> <em class="mt"> </em>的意思是我们在寻找最小化<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">ΣL(y</em>ᵢ<em class="mt">,γ)</em></code>的值<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>。让我们通过使用实际损失函数来计算值<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>。为了找到最小化<code class="fe mp mq mr ms b"><em class="mt">ΣL</em></code>的<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>，我们对<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>求<code class="fe mp mq mr ms b"><em class="mt">ΣL</em></code>的导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e9c618ffeadeab161520ba59ef4a15d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5Q49z-ig1Jwz5jZn9iBf_g.png"/></div></figure><p id="5b40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>使得<code class="fe mp mq mr ms b"><em class="mt">∂ΣL/∂γ</em></code>等于0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f50e60da840248d490b65474e90141ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oLI_LaZ6wfMz5N6BYuReFw.png"/></div></figure><p id="1d28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原来最小化<code class="fe mp mq mr ms b"><em class="mt">ΣL</em></code>的值<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>就是<code class="fe mp mq mr ms b"><em class="mt">y</em></code>的平均值。这就是为什么我们在上一节中使用<code class="fe mp mq mr ms b"><em class="mt">y</em></code>平均值进行初始预测<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/93b8d4c07be0ef81b955903ea334ecc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*sf2bYf2JIJvZj6iovFusqg.png"/></div></figure><h2 id="c356" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">第二步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/62239d36b6227c38a8846f6fbfa5bd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DQq-_GpOmE0tIiuU240kvA.png"/></div></div></figure><p id="bff6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从2–1到2–4的整个步骤2过程重复<code class="fe mp mq mr ms b"><em class="mt">M</em></code>次。<code class="fe mp mq mr ms b"><em class="mt">M</em></code>表示我们正在创建的树的数量，小的<code class="fe mp mq mr ms b"><em class="mt">m</em></code>表示每棵树的索引。</p><h2 id="e9b1" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">第2步–1</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/a67954a83e30e241e99a07fb9b1ef8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwcvjKDxy_GuzKfOQdKabg.png"/></div></div></figure><p id="f62a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们计算残差<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>的方法是对损失函数相对于之前的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>求导，然后乘以1。正如您在下标索引中看到的，计算每个单个样本<code class="fe mp mq mr ms b"><em class="mt">i</em></code>的<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>。你们中的一些人可能想知道为什么我们称之为<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>残差。该值实际上是<strong class="ky ir">负梯度</strong>，为我们提供方向(+/)和幅度方面的指导，使损失函数最小化。你很快就会明白为什么我们称之为残差。顺便说一下，这种使用梯度来最小化模型损失的技术非常类似于通常用于优化神经网络的g <a class="ae kv" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>技术。(其实两者略有不同。如果你有兴趣，请看看<a class="ae kv" href="https://explained.ai/gradient-boosting/descent.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>详述的那个话题。)</p><p id="ea4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在这里计算残差。方程中的<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>表示上一步的预测。在第一次迭代中，它是<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>。我们正在求解残差方程<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/3118e881f4a07ec0f041437626f205de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Y_1bsDKl-zoKtmZU4GnA7g.png"/></div></figure><p id="7209" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以去掉2，因为它只是一个常数。剩下我们<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚 = yᵢ − F𝑚-₁</em></code>。你现在可能明白为什么我们称之为残差了。这也给了我们有趣的见解，即负梯度为我们提供了方向和大小，使损失最小化，实际上只是残差。</p><h2 id="a342" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">步骤2–2</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/ae7167e91224d26b3de25f692aa0bcaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WRZuGsljLPAWeKZ9c3r2Q.png"/></div></div></figure><p id="a95a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">j</em></code>表示树中的末端节点(即叶子)<code class="fe mp mq mr ms b"><em class="mt">m</em></code>表示树索引，大写<code class="fe mp mq mr ms b"><em class="mt">J</em></code>表示叶子总数。</p><h2 id="e1d3" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">第2-3步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/c9cd6dab906a4627020222d2b9bc2fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FM9KvlIC70j-8UsIvPFtoA.png"/></div></div></figure><p id="684a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们正在寻找使每个终端节点<code class="fe mp mq mr ms b"><em class="mt">j</em></code>上的损失函数最小化的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>。<code class="fe mp mq mr ms b"><em class="mt">Σxᵢ∈Rⱼ𝑚 L</em></code>表示我们正在合计属于终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ𝑚</em></code>的所有样本<code class="fe mp mq mr ms b"><em class="mt">xᵢ</em></code>的损失。让我们把损失函数代入方程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/632f954e6a3041314070ec4f9cf142c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*6OkG6PP31DEjj391-TIjMw.png"/></div></figure><p id="f79f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们找到使σ(*)的导数等于零的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f4d12d684b0bd2106d8cc2a681f6e463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Wln5S6Jmu9HOtvFGgBEArQ.png"/></div></figure><p id="3aed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，<code class="fe mp mq mr ms b"><em class="mt">nⱼ</em></code>表示终端节点<code class="fe mp mq mr ms b"><em class="mt">j</em></code>的样本数。这意味着最小化损失函数的最优<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>是终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ𝑚</em></code>中残差<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>的平均值。换句话说，<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>是回归树的常规预测值，其是每个终端节点中目标值(在我们的情况下，残差)的平均值。</p><h2 id="92cd" class="nb lt iq bd lu nc nd dn ly ne nf dp mc lf ng nh me lj ni nj mg ln nk nl mi nm bi translated">第2-4步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/d5bf745406685806542968ffa26b5199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvLO78g0uMntR3T6vVqOpA.png"/></div></div></figure><p id="86b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最后一步，我们正在更新组合模型<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>的预测。<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚1(x ∈ Rⱼ𝑚)</em></code>意味着如果给定的<code class="fe mp mq mr ms b"><em class="mt">x</em></code>落在终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ𝑚</em></code>中，我们选择值<code class="fe mp mq mr ms b"><em class="mt">γⱼm</em></code>。由于所有的终端节点都是排他的，任何给定的单个<code class="fe mp mq mr ms b"><em class="mt">x</em></code>只落入一个终端节点，相应的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>被添加到先前的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>中，并进行更新预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>。</p><p id="929a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如前所述，<code class="fe mp mq mr ms b"><em class="mt">ν</em></code>是范围在0和1之间的学习率，其控制附加树预测<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>对组合预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>的贡献程度。较小的学习率降低了额外的树预测的效果，但是它基本上也降低了模型过度适应训练数据的机会。</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="4125" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经完成了所有步骤。为了获得最佳的模型性能，我们希望迭代第2步<code class="fe mp mq mr ms b"><em class="mt">M</em></code>次，这意味着向组合模型添加<code class="fe mp mq mr ms b"><em class="mt">M</em></code>树。实际上，您可能经常想要添加超过100棵树来获得最佳的模型性能。</p><p id="2ce6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你们中的一些人可能会觉得所有这些数学都是不必要的复杂，因为前面的部分以一种简单得多的方式展示了基本的思想，而没有那些复杂的东西。其背后的原因是梯度推进被设计成能够处理任何损失函数，只要它是可微的，并且我们所回顾的数学是具有这种灵活性的梯度推进算法的一般化形式。这使得公式有点复杂，但这是算法的美妙之处，因为它在处理各种类型的问题时具有巨大的灵活性和便利性。例如，如果您的问题需要绝对损失而不是平方损失，您可以只替换损失函数，整个算法就像上面定义的那样工作。事实上，流行的梯度增强实现如<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或<a class="ae kv" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>有各种各样的损失函数，因此您可以选择适合您问题的任何损失函数(参见<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或<a class="ae kv" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>中提供的各种损失函数)。</p><h1 id="ee35" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">密码</h1><p id="3438" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这一节中，我们将把刚刚复习过的数学知识转化为可行的python代码，以帮助我们进一步理解算法。代码主要来自Matt Bowers的实现，所以所有的荣誉都归于他的工作。我们使用scikit-learn的<code class="fe mp mq mr ms b">DecisionTreeRegressor</code>来构建树，这有助于我们只关注梯度推进算法本身，而不是树算法。我们正在模仿scikit-learn风格的实现，其中您使用<code class="fe mp mq mr ms b">fit</code>方法训练模型，并使用<code class="fe mp mq mr ms b">predict</code>方法进行预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="965f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，所有经过训练的树都存储在<code class="fe mp mq mr ms b">self.trees</code>列表对象中，当我们使用<code class="fe mp mq mr ms b">predict</code>方法进行预测时，会检索到这些树。</p><p id="abb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将通过查看他们在我们数据上的RMSE来检查我们的<code class="fe mp mq mr ms b">CustomGradientBoostingRegressor</code>是否与scikit-learn的<code class="fe mp mq mr ms b">GradientBoostingRegressor</code>表现相同。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dd35e88734b89ef4185178e5c233da42.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*kymwIZbu0JXMPhvaNU3OcQ.png"/></div></figure><p id="5f50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您在上面的输出中所看到的，两种型号都有完全相同的RMSE。</p><h1 id="6ed6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="ee34" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们在这篇文章中回顾的算法只是梯度推进算法的一个选项，它专用于平方损失的回归问题。如果您也对分类算法感兴趣，请查看第2部分。</p><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">关于梯度提升算法，您只需知道第2部分。分类</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">用例子、数学和代码解释算法</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="on l oo op oq om or kp od"/></div></div></a></div><p id="f98c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您想了解该算法的更多细节，还有一些其他的好资源:</p><ul class=""><li id="33e8" class="os ot iq ky b kz la lc ld lf ou lj ov ln ow lr ox oy oz pa bi translated"><strong class="ky ir"> StatQuest，Gradient Boost</strong><a class="ae kv" href="https://www.youtube.com/watch?v=3CC4N4z3GJc&amp;t=1s" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">Part 1</strong></a><strong class="ky ir">和</strong> <a class="ae kv" href="https://www.youtube.com/watch?v=2xudPOBz-vs" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Part 2 </strong> </a> <br/>这是一个YouTube视频，以初学者友好的方式解释了GB回归算法，具有很好的视觉效果。</li><li id="59da" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><strong class="ky ir">特伦斯·帕尔和杰瑞米·霍华德，</strong> <a class="ae kv" href="https://explained.ai/gradient-boosting/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">如何解释渐变助推</strong> </a> <strong class="ky ir"> <br/> </strong>本文还重点介绍了GB回归。它解释了平方损失和绝对损失之间的算法差异。</li><li id="0dcf" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated"><strong class="ky ir">杰罗姆弗里德曼，</strong> <a class="ae kv" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">贪婪函数逼近:一个梯度推进机</strong></a><strong class="ky ir"><em class="mt"><br/></em></strong>这是弗里德曼的论文原文。虽然有点难以理解，但它确实展示了算法的灵活性，他展示了一个通用算法，可以处理任何类型的具有可微损失函数的问题。</li></ul><p id="f05a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你也可以在Google Colab链接或者下面的Github链接中查看完整的Python代码。</p><div class="oa ob gp gr oc od"><a href="https://colab.research.google.com/drive/1mWKDoEtUo82bcxLlwA7YNuEoFzlIY-GG" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">谷歌联合实验室</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">梯度推进算法—回归</h3></div></div><div class="om l"><div class="pg l oo op oq om or kp od"/></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://github.com/tomonori-masui/gradient-boosting/blob/main/gradient_boosting_regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">梯度推进算法—tomo nori-masui处的回归/梯度推进</h2><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">github.com</p></div></div><div class="om l"><div class="ph l oo op oq om or kp od"/></div></div></a></div><h1 id="023b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><ul class=""><li id="7970" class="os ot iq ky b kz mk lc ml lf pi lj pj ln pk lr ox oy oz pa bi translated">杰罗姆·弗里德曼，<a class="ae kv" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">贪婪函数逼近:一个梯度推进机</a></li><li id="f4e7" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated">特伦斯·帕尔和杰瑞米·霍华德，<a class="ae kv" href="https://explained.ai/gradient-boosting/index.html" rel="noopener ugc nofollow" target="_blank">如何解释梯度推进</a></li><li id="9580" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated">Matt Bowers，<a class="ae kv" href="https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch" rel="noopener ugc nofollow" target="_blank">如何从零开始建立一个梯度推进机</a></li><li id="8b6c" class="os ot iq ky b kz pb lc pc lf pd lj pe ln pf lr ox oy oz pa bi translated">维基百科，<a class="ae kv" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">渐变提升</a></li></ul></div></div>    
</body>
</html>