<html>
<head>
<title>Using Deep Learning for Text-Generation of Wikipedia Articles</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习进行维基百科文章的文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-deep-learning-for-text-generation-of-wikipedia-articles-c9f1897f54a0#2022-01-27">https://towardsdatascience.com/using-deep-learning-for-text-generation-of-wikipedia-articles-c9f1897f54a0#2022-01-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="5fb5" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用深度学习进行维基百科文章的文本生成</h1></div><div class=""><h2 id="29ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在fastai和Pytorch中对自然语言模型使用AWD-lstm</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/574b00eaa84464820fc36fd257cd5bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*08uX8ioWmjtwIRGc"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卢克·切瑟在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="11e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预测序列中的下一个单词是<em class="ls">语言模型</em>的任务。语言模型是一种自我监督学习的形式。与大多数深度学习任务不同，语言模型没有通过手动标记提供给它们的问题目标。相反，数据本身包含所需的标签。在我们的例子中，这些标签只是句子中的下一个单词。除了显而易见的原因之外，你可能有很多原因想要承担这样一个自我监督的学习问题。例如，拟合一个语言模型，然后使用迁移学习极大地提高了文本分类器的性能:它们可以理解它们预测的语料库的细节。这种方法被称为通用语言模型微调(ULMFit)和<a class="ae kv" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">是由杰瑞米·霍华德和西尔万·格鲁格首创的。</a></p><p id="2945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然变形金刚目前在深度学习社区风靡一时，但在Pytorch和fastai中很难找到关于语言模型的资源。从导入文本数据和符号化到编写复杂的模型，理解整个语言管道更加困难。因此，我们的任务将是预测Wikitext微小数据集中的未来单词，由Stephen Merity，Xiong，James Bradbury和Richard Socher提供(2016，<a class="ae kv" href="http://arxiv.org/abs/1609.07843" rel="noopener ugc nofollow" target="_blank">Pointer Sentinel Mixture Models</a>)。这个数据集在<a class="ae kv" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="noopener ugc nofollow" target="_blank">知识共享署名-共享许可</a>下可用。</p><p id="507a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">。这是从维基百科文章中收集的超过1亿个单词的语料库的一个小样本。这篇文章结合了自然语言处理的基础，包括标记化，以及让RNNs真正良好工作的最新技术。这同时解决了杰瑞米·霍华德快速书<a class="ae kv" href="https://github.com/fastai/fastbook" rel="noopener ugc nofollow" target="_blank">第12章和第14章的主题，并详细阐述了需要进一步概括的概念和代码。</a></p><h1 id="39ba" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据预处理</h1><p id="65b1" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">你可能想知道为什么我们使用一个较小的模型。如果我们有完整的Wikitext数据可用(通过fastai)，那么为什么不使用它呢？有两个主要原因:无论何时开始建模任务，总是从数据的子集开始。它允许你建立你的管道，更快地迭代，更容易地实验。一旦你让整个事情运转起来，接下来就是在更大的数据集中进行替换，然后等待模型训练完成。</p><p id="438b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将首先回顾将文本转换成可以输入模型的东西的两个主要步骤:标记化和数值化。</p><h2 id="cbff" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">符号化</h2><p id="74be" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">标记化是将单词列表转换成单个标记的过程。但是，我们不能只在空格上打散字。如果你想一想，英语有很多奇怪的地方，有很多奇怪的符号，我们需要解释。例如，fastai使用一套称为<code class="fe nc nd ne nf b">spaCy</code>的令牌化规则。我们可以通过使用<code class="fe nc nd ne nf b">WordTokenizer</code>来访问这个默认的标记器。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c123" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用函数<code class="fe nc nd ne nf b">coll_repr(collection, n)</code>来显示这个<code class="fe nc nd ne nf b">collection</code>的前<code class="fe nc nd ne nf b">n</code>项。我们还必须将<code class="fe nc nd ne nf b">txt</code>包装在一个列表中，因为标记器获取一组文档，我们可以将它们表示为一个列表。然而，我们不能就此罢休。熟悉语言模型的人会知道，我们需要添加特殊字符来表示新文本的开始:<code class="fe nc nd ne nf b">xxbos</code>，其中BOS代表“流的开始”。还有许多其他规则也使语言模型更容易学习，例如通过对重复字符使用特殊标记而不是简单地重复几次标记来减小嵌入矩阵的大小。这是由fastai的<code class="fe nc nd ne nf b">Tokenizer()</code>处理的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="835f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其他特殊令牌包括:</p><ul class=""><li id="a742" class="ni nj iq ky b kz la lc ld lf nk lj nl ln nm lr nn no np nq bi translated">下一个单词以大写字母开头</li><li id="795b" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nn no np nq bi translated"><code class="fe nc nd ne nf b">xxunk</code>:未知令牌</li></ul><h2 id="feb5" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">数值化</h2><p id="7253" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">一旦我们对输入流进行了标记，我们仍然需要一种方法来对这些标记进行分类，就像我们可以在表格数据的嵌入矩阵中嵌入分类变量一样。事实上，这个过程几乎是相同的:列出所有可能的标记，代表分类变量(这只是我们的词汇表)，然后用它在这个列表中的索引或位置替换每个标记。</p><p id="d30c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们实际上首先需要我们的标记化输入流，所以我们从上面调用我们的标记器:<code class="fe nc nd ne nf b">toks=tkn(txt)</code>。然后我们需要设置<code class="fe nc nd ne nf b">Numericalize</code>:这是使用一个类方法完成的，将我们的令牌作为参数传入。这样，我们构建了语料库的词汇。设置好之后，我们可以向它传递任何输入流，将令牌转换成整数。我们指定<code class="fe nc nd ne nf b">min_freq=0</code>，因为缺省值是3；由于我们的文本甚至不是一个句子，这意味着任何出现少于三次的单词都将被替换为<code class="fe nc nd ne nf b">xxunk</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c2cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们将文本从上面传递到<code class="fe nc nd ne nf b">num</code>，我们会期望什么？这个过程的要点是将令牌转换成整数，表示它们在vocab中的索引。让我们仔细检查我们得到了这个。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="1e47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能想知道为什么我们有这么多0。这是因为我们的输入流很小，所以大部分令牌都是未知的。我们需要大量的批处理来构建足够大的词汇表。我推荐看看fastai的书的第10章<a class="ae kv" href="https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb" rel="noopener ugc nofollow" target="_blank">中的例子。</a></p><p id="56fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么我们首先想要<code class="fe nc nd ne nf b">unk</code>代币？向我们的模型提供尽可能多的信息当然是好的。然而，避免过大的嵌入矩阵是有用的，因为它会使用太多的内存并降低训练速度。此外，语料库中没有很好表示的单词可能没有足够的数据来形成表示。</p><h2 id="4f25" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">创建数据加载器</h2><p id="2c30" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">最初，我们将基本上从零开始为我们的语言模型创建数据加载器，然后我们将把所有的文本放入Pandas数据帧，并让fastai为我们处理它。这第一步对于理解如何向语言模型提供数据非常重要。</p><p id="0071" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们以通常的方式提取数据，然后使用内置函数<code class="fe nc nd ne nf b">L()</code>读取文本数据。这本质上只是一个Python列表的花哨版本，带有一些受NumPy启发的高级索引技术。不要太担心这个的语义。您可以在将来为不同的文本文件或CSV文件回收这些代码。这只是将训练集和测试集连接成一个连续的流。这将使我们能够在以后将它分成重要的和合理的批量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="5455" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以看看我们的文本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="f308" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将通过在每个空格处分割文本来标记。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="5f13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建一个唯一标记的列表为我们提供了语料库的词汇。同样，我们使用内置函数<code class="fe nc nd ne nf b">L()</code>。然后，我们可以通过枚举<code class="fe nc nd ne nf b">vocab</code>将每个标记映射到它在词汇表中的索引。正如我们所预料的，一次输入一个令牌会给出一个整数列表。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8423" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要使用我们的语言模型，我们需要确保批处理是以特定的方式组织的。将来，我们可以使用内置的fastai功能来完成这项工作，但现在我们将手动完成这项工作。</p><p id="54d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集被分成<code class="fe nc nd ne nf b">bs</code>个大小相等的组，代表<code class="fe nc nd ne nf b">bl</code>个小序列流。每个批次都有<code class="fe nc nd ne nf b">bs</code>行(批次大小)和<code class="fe nc nd ne nf b">bl</code>列(序列长度)。第一批的第一行包含第一个迷你流的开始。第一批的第二行包含第二个迷你流的开始。第二批的第一行包含第一迷你流的第二块，从最后一批结束的地方继续。该模型将“看到”从一批流到另一批的文本。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/cc3baef8b82d683d4438749caf829fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1eyVIAru0TGWCqK4ZutJA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的文本流如何跨批排列的示例(图片由作者提供)。</p></figure><p id="3c9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们这里的<code class="fe nc nd ne nf b">group_chunks</code>函数直接来自fastai，它所做的就是这个重新索引。我们将文本流分成大小相等的<code class="fe nc nd ne nf b">m = len(dset) // bs</code>批，其中<code class="fe nc nd ne nf b">m</code>是每个批的长度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能想知道如果文本流的长度不能被批处理大小整除会发生什么。在这种情况下，当使用<code class="fe nc nd ne nf b">drop_last=True</code>创建<code class="fe nc nd ne nf b">DataLoaders</code>时，我们可以删除最后一个形状不正确的批次(除以批次大小后的余数)。您可能还会问，如果数据加载器是基于向模型随机提供批处理的概念构建的，那么文本将如何成为连续的流。正因为如此，我们也通过<code class="fe nc nd ne nf b">shuffle=False</code>。</p><p id="2cdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们需要将文本流分解成相应的输入和目标。为此，我们获取输入的序列长度，比如说16个单词，并从当前单词到未来的16个单词对我们的<code class="fe nc nd ne nf b">words</code>标记进行索引。然后，目标变成从当前单词开始的16个单词(偏移1)。确保您理解下面<code class="fe nc nd ne nf b">seqs</code>中的循环范围。然后，我们为训练/测试分割指定一个截止值，即我们数据的80%。最后，我们像往常一样使用<code class="fe nc nd ne nf b">DataLoaders</code>，除了这次直接使用方法<code class="fe nc nd ne nf b">from_dsets</code>传入我们的训练和有效数据集，并记住传入我们的批处理大小。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c12b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过查看<code class="fe nc nd ne nf b">seqs</code>列表中的第一项，我们可以确认我们的目标仅仅是我们的输入，偏移了一个元素。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="6159" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这正是我们想要的:我们输入特定的单词流，自我监督语言模型的任务是预测该流中的下一个单词。</p><h2 id="2c24" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">基线RNN</h2><p id="b91b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们最初的模型仍将是一个递归神经网络，但它不会有LSTM的内部细胞结构。相反，我们将使用一个香草RNN，只有一层。我们的任务是在每个时间步输入最后一个<code class="fe nc nd ne nf b">n</code>字，输出一个预测，并在输入下一个字时使用隐藏单元状态进行预测。我们对序列中的所有单词循环这个过程，减去一个。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/55f9fb4b06853b99f4500a5702ab76ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-i9RPfEB5X11BLkkrcp52g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们单层RNN的象征性表现(图片由作者提供)。</p></figure><p id="b8c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在RNN中使用了三层:一个嵌入层和两个线性层，最后一个线性层输出预测的单词。这里唯一稍微不熟悉的是嵌入层。它所做的就是获取一个索引列表，代表我们的令牌，然后<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">输出单词embeddeds</a>。我们也注意初始化我们的隐藏状态，由<code class="fe nc nd ne nf b">h</code>表示，在初始化中为0。如果我们在forward方法中这样做，我们将在每个序列被传入后丢弃模型的隐藏状态。这将违背整个目的——模型将不会知道它在序列中的位置。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ee4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的正向方法中，我们初始化一个输出列表。然后，我们在序列长度上循环，在每次迭代中连接隐藏状态和应用于当前输入的嵌入层(直到沿着第二个轴的当前时间步长)。然后，我们应用中间的线性层和ReLU非线性，最后将最后一个线性层的输出附加到outputs。当返回这些输出时，我们沿着第一维堆叠它们。</p><p id="55d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">唯一不太说得通的线是<code class="fe nc nd ne nf b">self.h=self.h.detach()</code>线。这是必要的，因为RNNs是如何工作的。通过仅在一个序列的开始将隐藏状态初始化为0(在我们的例子中，一个序列是一篇文章)，那么对于文章中的数千个单词，该模型可能不得不在自身上循环数千次。(记住，这是因为我们一次前进一个单词。)如果你考虑RNN的展开表示，这基本上等同于几千层的神经网络。如果你还没有发现这个问题，这里就是:如果我们在第8000个单词上，也就是RNN的第8000层，反向传播要求我们计算一直到第一层的导数。可以想象，这需要很长的时间和很大的内存。</p><p id="22e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们简单地从我们的 <code class="fe nc nd ne nf b"><em class="ls">sl</em></code> <em class="ls">长度输入中移除除梯度之外的所有梯度<em class="ls">。</em>如果我们的序列长度是16左右，这使得它更容易管理。我们用<code class="fe nc nd ne nf b">detach</code>方法来实现这一点，在<code class="fe nc nd ne nf b">sl</code>迭代的每一次循环之后都会调用这个方法。通过递归层向后计算导数被称为<strong class="ky ir">通过时间的反向传播</strong> (BPTT)。分离方法通常被称为<strong class="ky ir">截断BPTT </strong>。</em></p><p id="bea9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还要注意，我们添加了一个<code class="fe nc nd ne nf b">reset</code>方法，可以在创建学习者时通过回调来访问它。所有这一切都是自动初始化<code class="fe nc nd ne nf b">h</code>到零在每个时期的开始，并在验证之前。这样做的理由是，我们不希望模型在阅读并预测一篇关于烹饪的文章的第一个单词时，具有来自高尔夫文章的某种隐藏状态。</p><p id="5914" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以创建我们的学习者并调用<code class="fe nc nd ne nf b">fit_one_cycle</code>。请记住，语言模型需要一段时间来训练，所以即使我们的简单模型的一个纪元也需要大约20分钟。注意我们如何将<code class="fe nc nd ne nf b">ModelResetter</code>传递到<code class="fe nc nd ne nf b">cbs</code>，代表回调。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/1529e9072aa2c8fdae66dab82def1fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEpTHu7XiqTGNLsYn4Cqqw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单层RNN的结果。</p></figure><p id="fb92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">15%的准确度。还不错，但是让我们看看我们是否能在这方面有所改进。</p><h1 id="5078" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">创造LSTM模式</h1><p id="c0e7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">虽然与RNNs相比，LSTM可以部分解决爆炸或消失梯度的问题，但是我们仍然可以做三件关键的事情来确保我们的LSTM被正则化并且不会过度拟合。这些都是由Merity，Keskar和Socher在一篇开创性的论文中介绍的。按重要性排序，它们是:</p><ol class=""><li id="522c" class="ni nj iq ky b kz la lc ld lf nk lj nl ln nm lr nz no np nq bi translated">拒绝传统社会的人</li><li id="00cc" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nz no np nq bi translated">激活和时间正则化</li><li id="74a5" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nz no np nq bi translated">重量捆绑</li></ol><p id="5e37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将会看到上述技术与普通RNN的性能相比有什么贡献。</p><h2 id="5a60" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">拒绝传统社会的人</h2><p id="974c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们使用dropout来调整神经网络，防止过度拟合。它的工作原理是在神经网络的特定层随机归零激活，概率为<code class="fe nc nd ne nf b">p</code>。这防止了网络对某些激活模式的过度依赖，允许神经元更好地合作。添加的噪声提高了模型的鲁棒性。这个想法实际上是深度学习的创始人之一杰弗里·辛顿(Geoffrey Hinton)在一次在线讲座中提出的。</p><blockquote class="oa"><p id="c234" class="ob oc iq bd od oe of og oh oi oj lr dk translated">辛顿辍学的灵感来自一次去银行的旅行。“我去了我的银行。出纳员一直在变，我问其中一个为什么。他说他不知道，但是他们经常搬家。我想这一定是因为它需要员工之间的合作才能成功地欺骗银行。这让我意识到，随机移除每个例子中不同的神经元子集将防止共谋，从而减少过度拟合。”</p></blockquote><p id="21fa" class="pw-post-body-paragraph kw kx iq ky b kz ok jr lb lc ol ju le lf om lh li lj on ll lm ln oo lp lq lr ij bi translated">然而，我们不能只是随机地删除一些激活，就此打住。考虑下一层的后果。假设我们最初有10个神经元，在整个退出过程中，我们只剩下5个。然后，将这些激活传递到下一层，我们可以想象下一层是相当混乱的。以前，它得到10个正激活的总和(因为我们在层之间应用ReLU ),现在它只得到该信号的一半。就是尺度不一样。</p><p id="e940" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想规模保持不变，我们需要重新调整。例如，如果<code class="fe nc nd ne nf b">p=0.5</code>，那么我们期望一半的神经元被清零，这将使传递给下一层的信息减半。我们还剩下大约<code class="fe nc nd ne nf b">1-p</code>个神经元。因此，我们除以<code class="fe nc nd ne nf b">1-p=0.5</code>，将这些激活的大小增加到与之前相同的水平(因为我们除以一个小于1的数)。</p><h2 id="f443" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">激活和时间正则化</h2><p id="7de2" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">权重衰减在深度学习中被用作一种正则化形式，以防止过度拟合。权重衰减通过向损失添加惩罚来减小模型权重的大小，这意味着权重与所有其他权重相比不会变得过于重要。类似地，我们可以将两种其他形式的正则化应用于递归神经网络:激活正则化和时间激活正则化。</p><p id="ed7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">激活正则化(AR)旨在使模型的最终激活尽可能小。要做到这一点，我们只需将激活平方，并将它们添加到损失中，乘以一个超参数<code class="fe nc nd ne nf b">alpha</code>。减少激活的大小实现了<em class="ls">稀疏性</em>，当我们有大的隐藏层倾向于过度拟合数据时，这是可取的。引入这样的稀疏惩罚允许模型更好地一般化。为了使AR工作，我们将它应用于退出激活，这样我们就不会惩罚在常规退出中被清零的激活。</p><p id="ddd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">时间激活正则化(TAR)通过确保序列中连续令牌之间的激活是靠近的来工作。这是因为在句子中看到一个新单词后，激活不应该发生剧烈的变化。因此，我们通过增加连续激活之间的平方差来惩罚损失。与AR相反，TAR应用于非退出激活，因为我们想要时间步长之间的激活差异，这可以通过训练中的退出而显著减少。</p><h2 id="185c" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">重量捆绑</h2><p id="3da0" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">AWD·LSTM的论文还介绍了一种被称为<em class="ls">重量捆绑</em>的技术。这与模型中映射的对称性有关。在我们的语言模型中，嵌入输入是一个将单词转化为激活的映射过程。相反，最终输出层激活英语单词。根据直觉，这些映射可能是对称的。我们可以强制Pytorch为嵌入的输入和输出层分配相同的权重矩阵，如下所示:</p><pre class="kg kh ki kj gt op nf oq or aw os bi"><span id="dca9" class="mq lu iq nf b gy ot ou l ov ow">self.h_o.weight = self.i_h.weight</span></pre><h2 id="2175" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">AWD-LSTM</h2><p id="11ca" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们把所有这些技巧都包括进来，看看我们的准确度提高了多少。回想一下，我们仍然使用相同的LSTM模型架构；我们只是在每一步之间做一些小的调整，让它成为AWD-LSTM。请注意我们通常在Pytorch中构造LSTM的方式与这里的模型之间的相似性。关键步骤是创建层<code class="fe nc nd ne nf b">nn.LSTM</code>，在这里我们接受<code class="fe nc nd ne nf b">n_hidden</code>输入并输出<code class="fe nc nd ne nf b">n_hidden</code>输出，一定数量的层由<code class="fe nc nd ne nf b">n_layers</code>表示。Pytorch为我们处理其余的。我们还在LSTM层之后实现了dropout，请注意实例化权重绑定的行。</p><p id="829c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们为通过网络的每个前向传递返回三个内容:常规输出、丢失前来自RNN层的激活以及丢失后的激活。这允许我们使用回调<code class="fe nc nd ne nf b">RNNRegularizer</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="3c7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使用激活正则化和时间正则化，我们实际上并没有把它传递到模型中。这在很大程度上是因为我们自己实施它的复杂本质，也因为它可以直接附加到损失上。相反，在创建学习器时，我们将<code class="fe nc nd ne nf b">RNNRegularizer</code>传递给回调列表，超参数alpha和beta分别用于激活和时间正则化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ba2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能想知道为什么我们创建了两个学习者。第一个是以通常的方式创建的:传入数据加载器、模型、损失函数、指标和回调，我们就完成了。第二种方法更简单。我们可以让<code class="fe nc nd ne nf b">TextLearner</code>处理类似于<code class="fe nc nd ne nf b">ModelResetter</code>和<code class="fe nc nd ne nf b">RNNRegularizer</code>的回调，这是自动完成的。</p><p id="f325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过一个时期的训练，我们的模型已经大大优于多层RNN。如果你有时间进行更长时间的训练，我强烈建议进行多次训练，因为这样做后模型之间的差异会更加明显。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/1e648ba202cf5e43200f3057c6af1d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sJwGZN_1VRUDTk6EfHqiVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们手工制作的AWD LSTM的结果(图片由作者提供)。</p></figure><h1 id="112a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">使用fastai进行培训</h1><h2 id="667f" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">一批</h2><p id="e5b8" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">虽然了解语言模型中的符号化和数字化是如何工作的对于调试很重要，但我们实际上可以使用fastai的内置模块来完成这项工作。出于我们的目的，我们甚至不需要向<code class="fe nc nd ne nf b">DataLoaders</code>传递一个<code class="fe nc nd ne nf b">DataLoader</code>对象。我们可以使用一个<code class="fe nc nd ne nf b">Datasets</code>对象创建一个数据加载器，我发现这在处理Pandas <code class="fe nc nd ne nf b">DataFrames.</code>时更容易。首先，我们将把我们的训练和测试文本连接成一个大的连续流，就像我们上面做的那样。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="df6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将创建<code class="fe nc nd ne nf b">Datasets</code>对象。我们将使用它来创建我们的数据加载器。让我们一行一行地分解这段代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显然，splits用于创建我们的训练集和验证集的索引，我们使用<code class="fe nc nd ne nf b">range_of</code>函数来完成这项工作(提示:请仔细查看括号的位置)。然后，我们需要将特定的转换传递给<code class="fe nc nd ne nf b">Datasets</code>类初始化器，其中包括记号化和数值化。如果您阅读<code class="fe nc nd ne nf b">Tokenizer</code>的<a class="ae kv" href="https://docs.fast.ai/text.data.html#TextBlock.from_df" rel="noopener ugc nofollow" target="_blank">文档</a>，您会发现我们在这里传递的<code class="fe nc nd ne nf b">0</code>只是文本列的名称。(打印出<code class="fe nc nd ne nf b">df_all</code>，您将看到这确实是列名。)</p><p id="644c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，还有一个来自Python标准库的奇怪操作符叫做<code class="fe nc nd ne nf b">attrgetter("text")</code>。这实质上是构造了一个从类中获取某个属性的函数。例如，如果<code class="fe nc nd ne nf b">boy</code>是某个类的实例，我们定义了<code class="fe nc nd ne nf b">func = attrgetter("age")</code>，那么调用<code class="fe nc nd ne nf b">func(boy)</code>将返回<code class="fe nc nd ne nf b">boy.age.</code>，因此，在我们的调用中，<code class="fe nc nd ne nf b">attrgetter("text")</code>将获取在我们的<code class="fe nc nd ne nf b">txt_cols</code>的标记化中创建的<code class="fe nc nd ne nf b">"text"</code>属性，并将其数值化。</p><p id="edbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们将所有这些参数传递给我们的<code class="fe nc nd ne nf b">Datasets</code>构造函数。请注意，我们将数据加载器类型指定为语言模型，这意味着当我们在调用类方法<code class="fe nc nd ne nf b">Datasets.dataloaders()</code>期间传入附加参数时，我们的数据加载器将执行我们上面概述的操作，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ecf0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在这里做的是将所有的文本样本(dataframe的行)连接成一个流，将它分割成大小为<code class="fe nc nd ne nf b">bs</code>(表示批处理大小)的连续序列，然后一次遍历这些<code class="fe nc nd ne nf b">seq_len</code>标记。然后我们就完事了！在用<code class="fe nc nd ne nf b">dls.show_batch(max_n=3)</code>创建了数据加载器之后，我们可以用通常的方式显示一些批处理。请注意，右边的因变量只是输入流偏移了一个标记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/9eeb4fc05758954ff6a4d819db1cfac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Og73adBBYmZD_sfCjlcwiA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><h2 id="beaf" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">语言模型学习者</h2><p id="a02d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">通过使用fastai中内置的语言模型学习器，我们可以省去很多麻烦，该学习器还使用了优化的AWD-LSTM架构。这也将是一个比我们上面使用的模型大得多的模型。我们以几乎相同的方式构造学习者，但是将<code class="fe nc nd ne nf b">AWD-LSTM</code>作为模型传入。我们还将困惑作为一种度量，它只是损失的指数，通常用于语言模型中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="025a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在学习者上调用<code class="fe nc nd ne nf b">fit_one_cycle</code>,一旦一个时期完成，我们将把我们的模型保存到我们的本地机器上，这样我们可以在以后加载它，而不用担心再次经历所有上述步骤。警告:如果没有GPU，即使只是这一个纪元也需要一段时间。准备等几个小时。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/e1af1cac418f9202525706f0ff03ebb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E51aC9BvqN5ZaL13ZrrFrA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="799c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太神奇了！我们的精度比我们自己的AWD-LSTM高一倍，比普通的RNN高近三倍。</p><h2 id="fe1d" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">保存和加载模型</h2><p id="b004" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">调用<code class="fe nc nd ne nf b">learn.save("final_model")</code>。这将在<code class="fe nc nd ne nf b">learn.path/models/</code>中创建一个名为“final_model”的文件。我们可以用<code class="fe nc nd ne nf b">learn.load("final_model")</code>把这个模型的权重重新加载给学习者。</p><h2 id="80c2" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">文本生成</h2><p id="3fbc" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">使用上面的<code class="fe nc nd ne nf b">language_model_learner</code>而不是普通的<code class="fe nc nd ne nf b">Learner</code>的原因之一是<code class="fe nc nd ne nf b">language_model_learner</code>有生成新文本的<code class="fe nc nd ne nf b">.predict</code>方法。要做到这一点，我们只需要指定开始的文本，字数，我们将循环这个期望的句子数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2ec6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在要做的就是把它们打印出来。</p><pre class="kg kh ki kj gt op nf oq or aw os bi"><span id="ad13" class="mq lu iq nf b gy ot ou l ov ow">print(“\n”.join(preds))</span><span id="a948" class="mq lu iq nf b gy pa ou l ov ow">Michael Jordan was a professional golfer , who won a Golden Globe Award and the WWE Emmy Awards . Jordan was named to the NBA All - Star Game Team for that year</span><span id="9469" class="mq lu iq nf b gy pa ou l ov ow">Michael Jordan was a professional basketball player . Jordan said that " i have a lot of support for my career " , and that he had changed his mind after being recalled to Utah . Jordan said he would not be</span></pre><p id="fbfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">至少对我来说，这是惊人的。我们的语言模型显然不符合事实，但如果你不是很了解迈克尔·乔丹的传记，这种自动生成的文本几乎可以作为维基百科的文章。为<code class="fe nc nd ne nf b">Temperature</code>使用相对较高的值增加了模型的随机性，我们也可以传递像<code class="fe nc nd ne nf b">min_p</code>这样的参数，不考虑低于某个阈值概率的单词。</p><p id="3322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些预测还拥有近乎完美的语法(正如杰瑞米·霍华德所指出的，这些“I”没有大写，仅仅是因为标记化规则不允许将单个字符大写)。我们的语言模型，仅仅在CPU上经过几个小时的训练，就已经掌握了构成英语基础的核心概念。</p><h1 id="4bd2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="ac8a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在本文中，我们介绍了如何使用Pytorch和fastai构建语言模型，这是一个可以预测句子中下一个单词的模型。我们在几千篇维基百科文章上训练我们的模型，将准确度提高了近三倍，并使用LSTM生成了一篇关于迈克尔·乔丹的文章。</p><p id="e336" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您想了解更多关于如何处理Wikitext数据集(以及一般的NLP数据集)的信息，请查看这里的简短fastai演练<a class="ae kv" href="https://docs.fast.ai/tutorial.wikitext.html" rel="noopener ugc nofollow" target="_blank"/>。未来的探索方向将是采用本文中创建的语言模型，并使用它来训练语言分类器模型，就像ULMFit中一样。</p><h2 id="96de" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">参考</h2><p id="98e1" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">[1] S. Merity，N. S. Keskar和R. Socher，<a class="ae kv" href="https://arxiv.org/abs/1708.02182" rel="noopener ugc nofollow" target="_blank">规范和优化LSTM语言模型</a> (2017)，CoRR</p><p id="8281" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] J. Howard和S. Ruder，<a class="ae kv" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">用于文本分类的通用语言模型微调</a> (2018)，CoRR</p><p id="150e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] J. Howard和S. Glugger，<a class="ae kv" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank">fastai和Pytorch的程序员深度学习</a> (2020)，O'Reilly出版社</p><p id="ed2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]斯蒂芬·梅里蒂，·熊，詹姆斯·布拉德伯里，理查德·索彻，<a class="ae kv" href="https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/" rel="noopener ugc nofollow" target="_blank">维基文本</a> (2016，<a class="ae kv" href="http://arxiv.org/abs/1609.07843" rel="noopener ugc nofollow" target="_blank">指针哨兵混合模型</a>)。这个数据集在<a class="ae kv" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="noopener ugc nofollow" target="_blank">知识共享署名-相似分享许可</a>下可用。</p></div></div>    
</body>
</html>