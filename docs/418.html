<html>
<head>
<title>Interpretable and Explainable NER with LIME</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的和可说明的石灰NER</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-and-explainable-ner-with-lime-d643512c524#2022-01-14">https://towardsdatascience.com/interpretable-and-explainable-ner-with-lime-d643512c524#2022-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="a396" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">可解释的和可说明的石灰NER</h1></div><div class=""><h2 id="c4d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一步一步的教程，了解你的NER模型如何工作</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a037644410025a506d29b24b193a102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VoGh6YjqDe5F9zN_kKC09A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/illustrations/woman-code-matrix-ai-artificial-5741175/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="b0c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然在开发最新最伟大、最先进、具有大量参数的深度学习模型方面取得了很多进展，但在解释这些模型的输出方面却付出了很少的努力。</p><p id="9201" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在2020年12月的一次研讨会上，Gradio的首席执行官阿布巴卡尔·阿比德(Abubakar Abid)通过使用提示“两个__走进一个__”检查了GPT-3生成宗教文本的方式在观察了各种宗教的前10个回答后，他发现GPT-3提到了暴力，犹太人、佛教徒和锡克教徒各一次，基督教徒两次，但是<strong class="lb iu">十次有九次</strong>提到了穆斯林。</p><p id="ec73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">后来，阿比德的团队<a class="ae ky" href="https://arxiv.org/abs/2101.05783" rel="noopener ugc nofollow" target="_blank">展示了</a>将关于穆斯林的正面文本注入到一个大型语言模型中，减少了近40%关于穆斯林的暴力提及。即使是GPT 3的创造者OpenAI也在2020年5月发表了一篇论文，通过测试发现GPT 3对黑人的评价普遍较低，并表现出性别歧视和其他形式的偏见。嵌入在这些大型语言模型中的这种类型的社会偏见的例子不胜枚举，从种族主义言论到有毒内容。</p><p id="ff96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">深度学习模型就像一个黑匣子</strong>；给它一个输入，它给你一个输出，而不解释决定的原因，无论是文本分类，文本生成，还是命名实体识别(NER)。密切监控这个模型的输出，更重要的是能够解释这些模型的决策过程，这是至关重要的。解释输出背后的推理会让我们更有信心相信或不相信模型的预测。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="69d4" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">用石灰解释NER模型</strong></h1><p id="aed9" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本教程中，我们将重点解释使用<a class="ae ky" href="https://www.youtube.com/watch?v=hUnRCxnydCc" rel="noopener ugc nofollow" target="_blank"> LIME(本地可解释模型不可知解释)</a>的命名实体识别模型的预测。可以从<a class="ae ky" href="https://arxiv.org/pdf/1602.04938v1.pdf" rel="noopener ugc nofollow" target="_blank">原论文</a>中了解更多。</p><p id="1e2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LIME是模型不可知的，这意味着它可以用来解释任何类型的模型输出，而无需对其进行峰化处理。这是通过扰动目标预测周围的局部特征并测量输出来实现的。在我们的具体例子中，我们将改变目标实体周围的标记，然后尝试度量模型的输出。</p><p id="a16a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图说明了石灰的工作原理。</p><p id="5acc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个来自<a class="ae ky" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank"> LIME网站</a>的解释“原始模型的决策函数用蓝色/粉色背景表示，明显是非线性的。亮红色的十字就是正在解释的实例(姑且称之为X)。我们对X周围的扰动实例进行采样，并根据它们与X的接近程度对它们进行加权(这里的权重由大小表示)。我们获得原始模型对这些扰动实例的预测，然后学习线性模型(虚线),该线性模型在x附近很好地逼近该模型。注意，这种情况下的解释不是全局忠实的，但是在x附近是局部忠实的。”</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/631cd812e181da03c56db8173d01bb21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*kjUrB4gRpeLf0cadxjds9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">石灰法。<a class="ae ky" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="b297" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LIME输出具有对模型预测的贡献分数的记号列表(参见下面的文本分类示例)。这提供了局部可解释性，并且还允许确定哪些特征变化将对预测产生最大影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/6b638c46eb4b256546d91651647ee565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XphtBNq5fpJAqj6dZ9oQIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由LIME生成的一个文档类的解释。<a class="ae ky" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="97de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将重点解释NER模型的石灰输出。</p><p id="d65c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是步骤:</p><ul class=""><li id="fc9f" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">为我们的模型生成训练数据</li><li id="4ec2" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">在我们定制的带注释的数据集上训练NER模型</li><li id="e02b" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">选择一个目标词来解释</li></ul><h1 id="fca8" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">加载数据</h1><p id="99ac" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本教程中，我们将训练一个NER模型，从职位描述中预测技能、经验、文凭和文凭专业。数据是从<a class="ae ky" href="https://www.kaggle.com/airiddha/trainrev1" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>获得的。关于使用<a class="ae ky" href="https://ubiai.tools?utm_source=medium&amp;utm_medium=lime+post&amp;utm_campaign=organic+social" rel="noopener ugc nofollow" target="_blank"> UBIAI </a>的数据注释部分的更多细节，请参考<a class="ae ky" href="https://medium.com/ubiai-nlp/build-an-nlp-project-from-zero-to-hero-4-data-labeling-379515d8a087" rel="noopener">这篇文章</a>。</p><p id="2af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练NER模型，我们将使用CRF算法，因为它可以很容易地输出每个预测实体的置信度得分，这是LIME工作所需的。</p><p id="c0af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步是将带注释的数据加载到我们的笔记本中；数据被格式化为IOB格式。</p><p id="d600" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个小例子:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="6961" class="nz md it nv b gy oa ob l oc od">-DOCSTART- -X- O O</span><span id="0606" class="nz md it nv b gy oe ob l oc od">2 B-EXPERIENCE</span><span id="5167" class="nz md it nv b gy oe ob l oc od">+ I-EXPERIENCE</span><span id="17bc" class="nz md it nv b gy oe ob l oc od">years I-EXPERIENCE</span><span id="93ed" class="nz md it nv b gy oe ob l oc od">experience O</span><span id="cea0" class="nz md it nv b gy oe ob l oc od">in O</span><span id="8057" class="nz md it nv b gy oe ob l oc od">the O</span><span id="cbaa" class="nz md it nv b gy oe ob l oc od">online B-SKILLS</span><span id="09fb" class="nz md it nv b gy oe ob l oc od">advertising I-SKILLS</span><span id="f954" class="nz md it nv b gy oe ob l oc od">or O</span><span id="bcc1" class="nz md it nv b gy oe ob l oc od">research B-SKILLS</span></pre><p id="aa8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们导入几个包，并将数据预处理成一个元组列表(令牌、标签):</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="4329" class="nz md it nv b gy oa ob l oc od">! pip install -U 'scikit-learn&lt;0.24'</span><span id="47bb" class="nz md it nv b gy oe ob l oc od">!pip install sklearn_crfsuite #Installing CRF</span><span id="371a" class="nz md it nv b gy oe ob l oc od">!pip install eli5 # Installing Lime</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="3993" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看这个列表是什么样子的:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="dae0" class="nz md it nv b gy oa ob l oc od">train_file_path = r"/content/train_data.tsv"</span><span id="6551" class="nz md it nv b gy oe ob l oc od">train_sents = import_documents_set_iob(train_file_path)</span><span id="29fd" class="nz md it nv b gy oe ob l oc od">print(train_sents)</span><span id="cca7" class="nz md it nv b gy oe ob l oc od">#Small sample of the output<br/>('of', 'O'), ('advanced', 'B-SKILLS'), ('compute', 'I-SKILLS'), ('and', 'O')</span></pre><h1 id="181e" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">数据预处理</h1><p id="d08e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了训练CRF模型，我们需要将带注释的文本转换成数字特征。更多信息请查看<a class="ae ky" href="https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> CRF文档</a>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="d37b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成这些之后，我们就可以开始训练了——我们只需要将训练/测试特性和目标标签放入它们各自的列表中:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a69d" class="nz md it nv b gy oa ob l oc od">X_train = [sent2features(s) for s in train_sents]</span><span id="1ed6" class="nz md it nv b gy oe ob l oc od">y_train = [sent2labels(s) for s in train_sents]</span><span id="9bb2" class="nz md it nv b gy oe ob l oc od">X_test = [sent2features(s) for s in test_sents]</span><span id="721d" class="nz md it nv b gy oe ob l oc od">y_test = [sent2labels(s) for s in test_sents]</span></pre><h1 id="8d7d" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">模特培训</h1><p id="90aa" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们通过100次迭代来启动培训:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="59d2" class="nz md it nv b gy oa ob l oc od">crf = sklearn_crfsuite.CRF(</span><span id="5fdb" class="nz md it nv b gy oe ob l oc od">algorithm='lbfgs',</span><span id="6a52" class="nz md it nv b gy oe ob l oc od">c1=0.1,</span><span id="ae25" class="nz md it nv b gy oe ob l oc od">c2=0.1,</span><span id="50d2" class="nz md it nv b gy oe ob l oc od">max_iterations=100,</span><span id="b05c" class="nz md it nv b gy oe ob l oc od">all_possible_transitions=True</span><span id="06be" class="nz md it nv b gy oe ob l oc od">)</span><span id="29bd" class="nz md it nv b gy oe ob l oc od">crf.fit(X_train, y_train)</span></pre><p id="86ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练后，我们得到0.61的F-1分数，虽然不高，但考虑到标注数据集的数量，这是合理的。每个实体的分数:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="7623" class="nz md it nv b gy oa ob l oc od">sorted_labels = sorted(</span><span id="1a2a" class="nz md it nv b gy oe ob l oc od">labels,</span><span id="2f15" class="nz md it nv b gy oe ob l oc od">key=lambda name: (name[1:], name[0])</span><span id="7d96" class="nz md it nv b gy oe ob l oc od">)</span><span id="ead8" class="nz md it nv b gy oe ob l oc od">print(metrics.flat_classification_report(</span><span id="fb15" class="nz md it nv b gy oe ob l oc od">y_test, y_pred, labels=sorted_labels, digits=3</span><span id="3127" class="nz md it nv b gy oe ob l oc od">))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/edc0344cf3beaefade6be033dc95bbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IpW2CxwCC-XhXM6IeTyjYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:培训后实体得分</p></figure><h1 id="cab5" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">用石灰解释NER</h1><p id="d8dd" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">既然我们已经训练好了模型，我们就可以使用LIME算法来解释它的标签预测了。首先，我们初始化我们的NERExplainerGenerator类，它将从输入文本中生成特性，并将其输入到我们的模型中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="2298" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用工作描述中的以下句子进行测试:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="374b" class="nz md it nv b gy oa ob l oc od">text = '''6+ years of Web UI/UX design experience</span><span id="a4df" class="nz md it nv b gy oe ob l oc od">Proven mobile web application design experience'''</span><span id="bf71" class="nz md it nv b gy oe ob l oc od">explainer= NERExplainerGenerator(crf)</span><span id="e116" class="nz md it nv b gy oe ob l oc od">for index,word in enumerate(word_tokenize(text)):</span><span id="ddd1" class="nz md it nv b gy oe ob l oc od">print(index,word)</span><span id="1047" class="nz md it nv b gy oe ob l oc od">0 6+<br/>1 years<br/>2 of<br/>3 Web<br/>4 UI/UX<br/>5 design<br/>6 experience<br/>7 Proven<br/>8 mobile<br/>9 web<br/>10 application<br/>11 design<br/>12 experience</span></pre><p id="f1e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们需要设置时间解释器算法。以下是每个功能的含义:</p><ul class=""><li id="c7f9" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><strong class="lb iu"> MaskingTextSampler: </strong>如果你还记得在前面的介绍中，我们提到过LIME会试图干扰局部特征并记录我们模型的输出。它通过用“UNK”令牌随机替换70%的令牌来做到这一点。如果需要，可以调整百分比，但默认值是70。</li><li id="6f8c" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">样本，相似度:</strong>LIME模型会通过对带有“UNK”令牌的原句进行随机化，生成很多句子。这里有几个例子。</li></ul><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="23a5" class="nz md it nv b gy oa ob l oc od">['6+ years of UNK UNK/UX design experience\nProven UNK web UNK UNK experience', 'UNK+ years UNK Web UI/UX design experience\nProven mobile web application UNK UNK', '6+ UNK of Web UI/UX design experience\nProven UNK web application UNK experience', 'UNK+ years of Web UI/UNK UNK UNK\nUNK mobile web application UNK experience']</span></pre><p id="32db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个句子，我们将有一个来自我们的NER模型的预测标签。然后，LIME将使用线性白色模型对数据进行训练，该模型将解释每个令牌的贡献:te.fit(text，func)</p><p id="5a21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，让我们试着解释单词“UI/UX”的标签，它有一个<strong class="lb iu"> word_index =4 </strong>:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="88e9" class="nz md it nv b gy oa ob l oc od">word_index = 4 #explain UI/UX label</span><span id="a089" class="nz md it nv b gy oe ob l oc od">func = explainer.get_predict_function(word_index)</span><span id="2f3d" class="nz md it nv b gy oe ob l oc od">sampler = MaskingTextSampler(</span><span id="041c" class="nz md it nv b gy oe ob l oc od">replacement="UNK",</span><span id="907a" class="nz md it nv b gy oe ob l oc od">max_replace=0.7,</span><span id="f740" class="nz md it nv b gy oe ob l oc od">token_pattern=None,</span><span id="3a9c" class="nz md it nv b gy oe ob l oc od">bow=False</span><span id="b26e" class="nz md it nv b gy oe ob l oc od">)</span><span id="2614" class="nz md it nv b gy oe ob l oc od">samples, similarity = sampler.sample_near(text, n_samples=4)</span><span id="f7ed" class="nz md it nv b gy oe ob l oc od">print(samples)</span><span id="dcd0" class="nz md it nv b gy oe ob l oc od">te = TextExplainer(</span><span id="2cbc" class="nz md it nv b gy oe ob l oc od">sampler=sampler,</span><span id="ea2d" class="nz md it nv b gy oe ob l oc od">position_dependent=True,</span><span id="9185" class="nz md it nv b gy oe ob l oc od">random_state=42</span><span id="204d" class="nz md it nv b gy oe ob l oc od">)</span><span id="6722" class="nz md it nv b gy oe ob l oc od">te.fit(text, func)</span><span id="ac8d" class="nz md it nv b gy oe ob l oc od">#the explainer needs just the one instance text from texts list</span><span id="21d4" class="nz md it nv b gy oe ob l oc od">explain = te.explain_prediction(</span><span id="d675" class="nz md it nv b gy oe ob l oc od">target_names=list(explainer.model.classes_),</span><span id="f3ac" class="nz md it nv b gy oe ob l oc od">top_targets=3</span><span id="cf82" class="nz md it nv b gy oe ob l oc od">)</span><span id="eb57" class="nz md it nv b gy oe ob l oc od">print("WORD TO EXPLAIN", word_tokenize(text)[word_index])</span><span id="9e70" class="nz md it nv b gy oe ob l oc od">explain</span></pre><p id="fbf7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是输出结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/418d624dc3bfddefcaf64318c4fbf7d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4QvsgWAZH6GC2bX1n8sEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:石灰输出</p></figure><p id="7044" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绿色表示令牌对预测标签有积极贡献，红色<bias>表示它有消极贡献。该模型以0.95的概率正确预测了“UI/UX”是多令牌技能I-SKILLS的一部分。单词“Web”是预测标签的一个强有力的指示器。与第一个陈述一致，标签B-SKILLS具有0.018的较低概率，其中单词“Web”具有很强的负面贡献。</bias></p><p id="8b7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Lime还提供了每个令牌的贡献，非常简洁:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/72531c0acce437a0f31ca728a3861502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3JwKfOHxahHTkRc_NUxvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:每个功能的贡献</p></figure><p id="5b82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们注意到，网络、设计和年份对预测的I-SKILLS标签的贡献最大。</p><h1 id="5b9f" class="mc md it bd me mf np mh mi mj nq ml mm jz nr ka mo kc ns kd mq kf nt kg ms mt bi translated">结论:</h1><p id="4ae8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">随着我们走向大型复杂的黑盒人工智能模型，理解预测背后的决策过程对于能够信任模型输出至关重要。</p><p id="deb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将展示如何训练自定义NER模型，并使用石灰算法解释其输出。LIME是模型不可知的，可以用于解释任何复杂模型的输出，无论是图像识别、文本分类还是本教程中的NER。</p><p id="8aa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或想要为您的特定案例创建定制模型，请在下面留言或发送电子邮件至admin@ubiai.tools。</p><p id="313e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Twitter上关注我们</p></div></div>    
</body>
</html>