<html>
<head>
<title>Speed up your TensorFlow Training with Mixed Precision on GPUs and TPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用GPU和TPU上的混合精度加速TensorFlow训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-your-tensorflow-training-with-mixed-precision-on-gpu-tpu-acf4c8c0931c#2022-01-09">https://towardsdatascience.com/speed-up-your-tensorflow-training-with-mixed-precision-on-gpu-tpu-acf4c8c0931c#2022-01-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="a249" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">利用GPU和TPU上的混合精度加速TensorFlow训练</h1></div><div class=""><h2 id="9e54" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">简单的分步指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a25845b59d25851d3b5b55fcb549cb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0LvDVYe_HoOLtalgpmbUA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">按作者</p></figure><p id="6ea8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di">在</span>这篇文章中，我将向你展示如何使用混合精度位表示在合适的GPU或TPU上加速训练。首先，我将简要介绍不同的浮点格式。其次，我将一步一步向您展示如何使用TensorFlow实现显著的速度提升。详细的文档可以在<a class="ae ma" href="#a741" rel="noopener ugc nofollow">【1】</a>中找到。</p><h2 id="933e" class="mb mc iq bd md me mf dn mg mh mi dp mj le mk ml mm li mn mo mp lm mq mr ms mt bi translated">概述</h2><ol class=""><li id="f6a5" class="mu mv iq kx b ky mw lb mx le my li mz lm na lq nb nc nd ne bi translated"><a class="ae ma" href="#a33a" rel="noopener ugc nofollow">简介</a></li><li id="6217" class="mu mv iq kx b ky nf lb ng le nh li ni lm nj lq nb nc nd ne bi translated"><a class="ae ma" href="#33e2" rel="noopener ugc nofollow">在TensorFlow中激活混合精度</a></li><li id="0627" class="mu mv iq kx b ky nf lb ng le nh li ni lm nj lq nb nc nd ne bi translated"><a class="ae ma" href="#8621" rel="noopener ugc nofollow">损失比例</a></li><li id="afde" class="mu mv iq kx b ky nf lb ng le nh li ni lm nj lq nb nc nd ne bi translated"><a class="ae ma" href="#4761" rel="noopener ugc nofollow">结论</a></li></ol><h2 id="a33a" class="mb mc iq bd md me mf dn mg mh mi dp mj le mk ml mm li mn mo mp lm mq mr ms mt bi translated">1.介绍</h2><p id="0cba" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">代表一个值的位数越多，占用的内存就越多。因此，对这些值执行的计算需要更多的时间，因为必须从存储器中读取和写入更多的位，并在寄存器中来回移动。</p><p id="9a3d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">混合精度指的是一种技术，其中使用16位和32位浮点值来表示变量，以减少所需的内存并加快训练速度。它依赖于这样一个事实，即现代硬件加速器，如GPU和TPU，可以在16位上更快地运行计算。</p><p id="4d14" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在TensorFlow中，有两种16位浮点类型:float16和bfloat16。Float16遵循半精度浮点数的IEEE标准，与float32相比，指数用5位而不是8位表示，尾数用10位而不是23位表示。这大大缩小了float16值可以表示的可能值的范围。Bfloat16通过保持指数的8位并且仅将尾数的位从23位减少到7位来解决这个问题。这样，当每个值之间的步长增加时，范围保持不变。见图1</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/972e6a14cbaeccabc2100bf7b3fdc166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iU2-8l0W6S9GA7jbWSvddQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 —作者受<a class="ae ma" href="#1b34" rel="noopener ugc nofollow">【2】</a>启发，对比不同的浮点表示法</p></figure><h2 id="33e2" class="mb mc iq bd md me mf dn mg mh mi dp mj le mk ml mm li mn mo mp lm mq mr ms mt bi translated">2.在TensorFlow中激活混合精度</h2><p id="b172" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">要激活TensorFlow中的混合精度，可以实施全局策略。为TPU激活时，策略应为“mixed_bfloat16”，而为GPU激活时，配置应为“mixed_float16”。使用全局策略设置，所有后续层将使用float32中的变量执行float16中的计算。为了数值的稳定性，变量保持在float32中。在代码1中，程序员检查TPU或GPU是否可用，并设置相应的全局策略。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码1 —使用全局策略激活混合精度</p></figure><p id="fcea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了数值稳定性，建议模型的输出层使用float32。这可以通过在最后一层设置<code class="fe nq nr ns nt b">dtype=tf.float32</code>或者激活，或者在模型输出处添加一个线性激活层<code class="fe nq nr ns nt b">tf.keras.layers.Activation("linear", dtype=tf.float32)</code>来实现。抛开数值稳定性不谈，当使用matplotlib绘制模型预测时，数据必须在float32中，因为不支持绘制float16数据。</p><p id="2e6f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你用<a class="ae ma" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit" rel="noopener ugc nofollow" target="_blank"> tf.keras.model.fit </a>训练你的模型，就大功告成了！如果使用mixed_float16实现自定义训练循环，则需要进一步的步骤；损失比例。</p><h2 id="8621" class="mb mc iq bd md me mf dn mg mh mi dp mj le mk ml mm li mn mo mp lm mq mr ms mt bi translated">3.损耗缩放</h2><p id="20cd" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">如上图1所示，float16的范围较小。尤其是在反向传播期间，梯度可能变得如此之小，以至于触发下溢事件。当应用损失缩放时，损失被乘以一个大的数字，例如512，这导致梯度也以相同的程度缩放。这样，发生下溢的可能性大大降低。在计算梯度之后，结果除以相同的缩放因子，以获得实际的未缩放梯度。</p><p id="2dd8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了防止在训练期间发生溢出和/或下溢，您的优化器必须包装在<a class="ae ma" href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer" rel="noopener ugc nofollow" target="_blank">TF . keras . mixed _ precision中。losscale optimizer</a>。这个包装器实现了方法<code class="fe nq nr ns nt b">get_scaled_loss()</code>和<code class="fe nq nr ns nt b">get_unscaled_gradients()</code>，它们实现了上一段中描述的缩放。您的自定义训练循环可能看起来像Code2。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码2 —应用损失和梯度缩放</p></figure><h2 id="4761" class="mb mc iq bd md me mf dn mg mh mi dp mj le mk ml mm li mn mo mp lm mq mr ms mt bi translated">4.结论</h2><p id="3f0a" class="pw-post-body-paragraph kv kw iq kx b ky mw jr la lb mx ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">混合精度可以加速某些GPU和TPU上的训练。当使用<a class="ae ma" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit" rel="noopener ugc nofollow" target="_blank"> tf.keras.model.fit </a>来训练您的模型时，唯一需要的步骤是通过使用例如全局策略来构建具有混合精度的模型。如果实现了定制的训练循环，优化器包装器<a class="ae ma" href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer" rel="noopener ugc nofollow" target="_blank">TF . keras . mixed _ precision。应实现losscale optimizer</a>以防止溢出和下溢。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><p id="a741" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1]混合精度，张量流，2021年1月9日访问，<a class="ae ma" href="https://www.tensorflow.org/guide/mixed_precision" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/mixed_precision</a></p><p id="1b34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] BFloat16:云tpus高性能的秘密，谷歌云博客，2021年1月9日访问，<a class="ae ma" href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus" rel="noopener ugc nofollow" target="_blank">https://Cloud . Google . com/Blog/products/ai-machine-learning/BF loat 16-云TPUs高性能的秘密</a></p></div></div>    
</body>
</html>