<html>
<head>
<title>Beginner’s Guide to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-reinforcement-learning-f296e8dd8260#2022-01-26">https://towardsdatascience.com/beginners-guide-to-reinforcement-learning-f296e8dd8260#2022-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="d592" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">强化学习初学者指南</h1></div><div class=""><h2 id="663c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习模型的高级概述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dddcdad28f32392a1a93447d79411272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5Enk3dtKymRGBvI8vzSqQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com/s/photos/reinforcement-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="16d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习是<a class="ae ky" href="https://databasecamp.de/en/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>中的第四种主要学习方法，与<a class="ae ky" href="https://databasecamp.de/en/ml/supervised-learning-models" rel="noopener ugc nofollow" target="_blank">监督</a>，无监督，半监督学习并列。主要区别在于模型不需要任何<a class="ae ky" href="https://databasecamp.de/en/data" rel="noopener ugc nofollow" target="_blank">数据</a>来训练。它通过奖励想要的行为和惩罚坏的行为来学习结构。</p><h1 id="6273" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习的例子</h1><p id="ebc0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们可以详细了解这种模型的训练过程之前，我们应该了解这些算法在哪些情况下可以有所帮助:</p><ul class=""><li id="7b45" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">当教计算机玩游戏时，使用强化学习。目的是学习哪些策略会导致胜利，哪些不会。</li><li id="52cf" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">在自动驾驶中，也使用这些学习算法，以便车辆可以自行决定哪种行动是最好的。</li><li id="32dc" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">对于服务器机房的空调，强化学习模型决定何时以及在多大程度上冷却房间以有效地使用能量。</li></ul><p id="8c66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习的应用通常以必须做出大量连续决策的事实为特征。程序员也可以向计算机具体说明这些(例如室温:“如果温度上升到24°C以上，则冷却到20°C”)。</p><p id="c4ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在强化学习的帮助下，人们希望避免形成一连串的如果-那么条件。一方面，这在许多用例中可能根本不可能，比如自动驾驶，因为程序员无法预见所有的可能性。另一方面，人们希望这些模型也能为复杂问题开发新的策略，而这可能是人类根本做不到的。</p><h1 id="31d9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习如何工作</h1><p id="f258" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">强化学习模型应该被训练成能够独立做出一系列决策。假设我们要训练这样一个算法，代理人，尽可能成功地玩游戏吃豆人。代理从游戏领域中的任意位置开始，并且它可以执行有限数量的可能动作。在我们的例子中，这将是四个方向(上、下、右或左),它可以在运动场上前进。</p><p id="c042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法在这个游戏中找到自己的环境是游戏场和幽灵的移动，这是一定不能遇到的。在每一个动作之后，比如上升，代理会收到一个直接的反馈，即奖励。在《吃豆人》中，这些要么是获得积分，要么是遭遇幽灵。也有可能在一次行动后没有直接的回报，但它会在未来发生，例如在一两次进一步的行动中。对代理人来说，未来的奖励不如眼前的奖励有价值。</p><p id="2fd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着时间的推移，代理人制定了所谓的政策，即承诺最高长期回报的行动策略。在第一轮中，算法选择完全随机的动作，因为它还不能获得任何经验。然而，随着时间的推移，一个有希望的策略出现了。</p><h1 id="7307" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">机器学习方法之间的差异</h1><p id="b61f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在<a class="ae ky" href="https://databasecamp.de/en/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>领域，总共有四种不同的学习方法:</p><ol class=""><li id="73b5" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu ng my mz na bi translated"><a class="ae ky" href="https://databasecamp.de/en/ml/supervised-learning-models" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">监督学习</strong> </a>算法使用已经包含模型应该预测的标签的数据集来学习关系。但是，它们只能识别和学习包含在训练数据中的结构。例如，监督模型用于图像的分类。使用已经分配到一个类的图像，他们学习识别关系，然后他们可以应用到新的图像。</li><li id="eec6" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated"><strong class="lb iu">无监督学习</strong>算法从一个数据集学习，但是这个数据集还没有这些标签。它们试图识别自己的规则和结构，以便能够将数据分类到尽可能具有相同属性的组中。例如，当您想要根据共同特征将客户分组时，可以使用无监督学习。例如，订单频率或订单金额可用于此目的。然而，由模型本身来决定它使用哪些特征。</li><li id="02b5" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated"><strong class="lb iu">半监督学习</strong>是<a class="ae ky" href="https://databasecamp.de/en/ml/supervised-learning-models" rel="noopener ugc nofollow" target="_blank">监督学习</a>和非监督学习的混合。该模型具有相对较小的带有标签的数据集和较大的带有未标签数据的数据集。目标是从少量已标记的信息中学习关系，并在未标记的数据集中测试这些关系以从中学习。</li><li id="3b95" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu ng my mz na bi translated">强化学习不同于以前的方法，因为它不需要训练数据，而是通过所描述的奖励系统简单地工作和学习。</li></ol><h1 id="1fe7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习需要训练数据吗？</h1><p id="4d81" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">是的，强化学习模型也需要数据来进行训练。但与其他机器学习方法相比，这些信息不必在外部数据集中给出，而是可以在训练过程中创建。</p><p id="6c56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" href="https://databasecamp.de/en/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的世界里，数据对于训练好的、健壮的模型是必不可少的。<a class="ae ky" href="https://databasecamp.de/en/ml/supervised-learning-models" rel="noopener ugc nofollow" target="_blank">监督学习</a>为此使用人类标记的数据，这种数据最好在大量情况下可用。这通常很昂贵，而且数据集很难获得或创建。<a class="ae ky" href="https://databasecamp.de/en/ml/unsupervised-learnings" rel="noopener ugc nofollow" target="_blank">无监督学习</a>另一方面，也需要大量数据，但不需要有标签。这使得获取信息更加便宜和容易。</p><p id="6030" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们已经看到的，强化学习与监督学习和非监督学习完全相反。然而，大量的数据通常会导致更好的训练结果的原则在这里也适用。然而，与其他类型的机器学习模型的不同之处在于，这些数据不一定是外部提供的，而是由模型本身生成的。</p><p id="648b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们以学习一个游戏为例:一个机器学习模型被训练来赢得游戏，即导致胜利的移动被认为是积极的，导致失败的移动被认为是消极的。在这种情况下，模型可以使用许多游戏运行作为训练数据，因为目标“赢得游戏”是明确定义的。随着每一个新的游戏，模型学习，新的训练数据产生，模型变得更好。</p><h1 id="7e22" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习是深度学习的未来吗？</h1><p id="aa3c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">强化学习在未来无法取代<a class="ae ky" href="https://databasecamp.de/en/ml/deep-learning-en" rel="noopener ugc nofollow" target="_blank">深度学习</a>。这两个子领域是紧密相连的，但它们并不相同。<a class="ae ky" href="https://databasecamp.de/en/ml/deep-learning-en" rel="noopener ugc nofollow" target="_blank">深度学习</a>算法非常擅长识别大型数据集中的结构，并将其应用于新的未知数据。另一方面，强化学习模型即使没有训练数据集也能做出决策。</p><p id="b40e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在许多领域，<a class="ae ky" href="https://databasecamp.de/en/machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>和<a class="ae ky" href="https://databasecamp.de/en/ml/deep-learning-en" rel="noopener ugc nofollow" target="_blank">深度学习</a>模型将继续足以取得良好的效果。另一方面，强化学习的成功意味着现在可以开辟以前不可想象的人工智能新领域。然而，也有一些应用，如股票交易，强化学习将取代<a class="ae ky" href="https://databasecamp.de/en/ml/deep-learning-en" rel="noopener ugc nofollow" target="_blank">深度学习</a>模型，因为它提供了更好的结果。</p><p id="6f49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个领域中，已经尝试学习如何从过去的市场数据中识别和交易新股票。然而，对于股票业务来说，训练一种强化学习算法来制定具体的策略，而不依赖于过去的数据，可能更有前途。</p><h1 id="a249" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">这是你应该带走的东西</h1><ul class=""><li id="44ce" class="ms mt it lb b lc mn lf mo li nh lm ni lq nj lu mx my mz na bi translated">强化学习是机器学习领域的一种学习方法。</li><li id="7ddb" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">它指的是被训练来预测一系列决策的模型，这些决策承诺最高的可能成功率。</li><li id="f3d2" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">例如，强化学习用于教计算机玩游戏或在自动驾驶中做出正确的决定。</li></ul></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="6497" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nr">如果你喜欢我的作品，请在这里订阅</em><a class="ae ky" href="https://medium.com/subscribe/@niklas_lang" rel="noopener"><em class="nr"/></a><em class="nr">或者查看我的网站</em> <a class="ae ky" href="http://www.databasecamp.de/en/homepage" rel="noopener ugc nofollow" target="_blank"> <em class="nr">数据大本营</em> </a> <em class="nr">！还有，媒体允许你每月免费阅读</em> <strong class="lb iu"> <em class="nr"> 3篇</em> </strong> <em class="nr">。如果你想让</em><strong class="lb iu"><em class="nr"/></strong><em class="nr">无限制地访问我的文章和数以千计的精彩文章，不要犹豫，通过点击我的推荐链接:</em><a class="ae ky" href="https://medium.com/@niklas_lang/membership" rel="noopener">https://medium.com/@niklas_lang/membership</a>获得会员资格，每个月只需支付 <strong class="lb iu"> <em class="nr"> 5 </em> </strong> <em class="nr"/></p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ns"><a href="https://medium.com/@niklas_lang/basics-of-ai-deep-learning-vs-machine-learning-93a8499d5679" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">人工智能基础:深度学习与机器学习</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">深度学习是一种来自信息处理的方法，使用神经网络分析大量数据。这个…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div><div class="oh oi gp gr oj ns"><a href="https://medium.com/@niklas_lang/basics-of-ai-supervised-learning-8505219f07cf" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">人工智能基础:监督学习</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">监督学习是人工智能和机器学习的一个子类。它的特点是…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="ok l od oe of ob og ks ns"/></div></div></a></div><div class="oh oi gp gr oj ns"><a href="https://medium.com/@niklas_lang/what-are-recurrent-neural-networks-5c48f4908e34" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">理解递归神经网络</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">递归神经网络(RNNs)是第三种主要类型的神经网络，前向网络和神经网络</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="ol l od oe of ob og ks ns"/></div></div></a></div></div></div>    
</body>
</html>