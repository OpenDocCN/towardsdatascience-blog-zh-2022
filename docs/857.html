<html>
<head>
<title>Accelerated Distributed Training with TensorFlow on Google’s TPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在谷歌TPU上使用TensorFlow加速分布式训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerated-distributed-training-with-tensorflow-on-googles-tpu-52f1fe21da33#2022-01-31">https://towardsdatascience.com/accelerated-distributed-training-with-tensorflow-on-googles-tpu-52f1fe21da33#2022-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="13db" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">在谷歌TPU上使用TensorFlow加速分布式训练</h1></div><div class=""><h2 id="2b6d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解您的硬件以优化您的软件</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e6e3500ccde4150853dfc7fe7775d1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoMoriYF6mWQjQeZt-sq8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Cloud TPU v3 POD BY<a class="ae kv" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank">Google Cloud</a>下<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> (CC BY 4.0) </a></p></figure><p id="c6c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这篇文章中，我将从硬件的角度向您展示张量处理单元(TPU)的基本原理，并一步一步地向您展示如何使用TensorFlow在TPU上执行加速分布式训练来训练您自己的模型。</p><h1 id="c3ac" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">概述</h1><ol class=""><li id="e7be" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated"><a class="ae kv" href="#258a" rel="noopener ugc nofollow">简介</a> <br/> 1.1。<a class="ae kv" href="#cda1" rel="noopener ugc nofollow">张量处理单元</a>1.2<br/>。<a class="ae kv" href="#Distribution Strategies" rel="noopener ugc nofollow">分销策略</a></li><li id="69e1" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#dade" rel="noopener ugc nofollow">用TensorFlow </a> <br/> 2.1在TPU上实现分布式训练。<a class="ae kv" href="#8056" rel="noopener ugc nofollow">硬件检测</a> <br/> 2.2。<a class="ae kv" href="http://89ae" rel="noopener ugc nofollow" target="_blank">分布式数据集</a> <br/> 2.3。<a class="ae kv" href="#a35e" rel="noopener ugc nofollow">手动降损</a> <br/> 2.4。<a class="ae kv" href="#9fce" rel="noopener ugc nofollow">自定义训练循环</a></li><li id="1ad2" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#40af" rel="noopener ugc nofollow">结论</a></li><li id="2e1d" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#11c7" rel="noopener ugc nofollow">更多资源</a></li></ol><h1 id="258a" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">1.介绍</h1><p id="01aa" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">无论是谷歌的<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>，OpenAI的<a class="ae kv" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>，英伟达的<a class="ae kv" href="https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" rel="noopener ugc nofollow" target="_blank">威震天图灵NLG </a>还是<a class="ae kv" href="https://arxiv.org/abs/2101.03961" rel="noopener ugc nofollow" target="_blank">谷歌的开关变形金刚</a>，这些模型都表明，通过大规模缩放你的模型和数据集，可以实现令人印象深刻的结果。虽然在这样的模型上运行推理已经是一个挑战，但训练模型更具挑战性。训练一个模型通常需要比推理多几个数量级的计算资源。需要大型硬件加速器集群在合理的时间内训练这些模型。除了GPU之外，张量处理单元(TPU)在训练模型中也起着重要的作用。为了利用TPU的高计算性能，理解底层硬件架构和理解必须如何准备代码是至关重要的。</p><p id="4abe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从一些解释开始，这些解释将帮助您了解TPU硬件的基本原理，并在实现代码时采取正确的措施。</p><p id="94bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章的底部，你会找到一个笔记本的链接，在那里我用本文描述的技术改编了TensorFLow的DCGAN模型，并在TPU上训练该模型。</p><h2 id="cda1" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">1.1.张量处理单元</h2><p id="aefa" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">张量处理单元是一种硬件加速器，用于加速矩阵的MAC(乘法和累加)运算，这是机器学习中的主要运算之一。</p><p id="581a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">底层硬件的性能依赖于许多方面，但有一个限制因素:内存接口。为了进行计算，处理器必须从内存中提取数据。有两个主要问题。第一个是内存访问延迟。访问存储器比对提取的数据执行计算花费更多的时钟周期，因为需要等待直到数据可用于计算。第二个是能耗。从DRAM读取数据比从芯片内部存储器读取数据需要更多的能量[3]。这种影响与时钟速度成线性比例关系，这意味着如果试图通过提高时钟速度来减少延迟，就必须考虑冷却策略。因此，性能瓶颈是内存接口。</p><p id="a93e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果可以以某种方式减少访问存储器的次数，就可以充分利用可用的计算资源，同时节省大量的电力。几十年前就已经提出了这个问题的解决方案:脉动阵列。想法很简单:对从内存中读取的单个数据项执行多个操作。它的灵感来自于通过我们身体泵送的血细胞。它们在心脏中开始循环，被泵送通过我们身体中的几个细胞，并循环回到心脏。心脏类似于记忆，每个细胞都是对血液进行操作的处理单元。图1中描绘了脉动阵列的抽象表示，其中PE是处理元件的一般形式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/38269d8ed8c17fa965e49bbadcfd7be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*H_V_Ea8qQPoVIAtOkmhfGQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1心脏收缩系统的基本原理[4]</p></figure><p id="af56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，PE可以是任何东西，从单个逻辑门到整个处理器(链接回我们的TPU硬件)。过程如下:从内存中读取数据，由PE执行任意操作，将数据传递给下一个PE，最后，在最后一个PE之后，将计算结果写回内存。</p><p id="58f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们更具体地看一个简单的例子。考虑一维卷积问题，这里我们有一个权重序列<em class="nz"> W </em>，一个输入序列<em class="nz"> X </em>。我们将计算输出序列<em class="nz"> Y </em>及其元素<em class="nz"> y_i </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f01dbeb192894929cf2a8c0f40c00d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*iZJFGKFLmdUijr2InWBDUA.png"/></div></figure><p id="1650" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个问题可以用图2所示的收缩期阵列来解决</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/92d31bac01554611e3a4a6555b4d6524.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*csScn46BmQNBJ7yzIYSmYA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2收缩期卷积阵列(a)和单元(b ),其中w_i保持不变，x_i和y_j在收缩期向相反方向移动[4]</p></figure><p id="3c78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，PE有两个输入和两个输出。权重被预加载，同时数据<em class="nz"> x </em>流经数组，计算各自的输出<em class="nz"> y </em>。</p><p id="3747" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的脉动阵列是一个简化的例子，有助于我们理解其背后的思想。在TPU中，脉动阵列以更复杂的方式使用，因为矩阵乘法单元伴随有向量和标量单元、高带宽互连和高带宽存储器接口。图3描绘了谷歌的TPUv2和TPUv3。</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk od oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/42c5d4f11ec76f787ff4a45d6c678b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*Ge7WHyPdVAty0-4DW3vCGg.png"/></div></figure><figure class="oc kk oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2536573c8e85ff85b8694df7ae56e42f.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*bQlAb3-TtoqongNZNXzm6Q.png"/></div></figure><figure class="oc kk oj oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/cb320e5630bfa41c4a1e6bb328ce95ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/1*MOBWTbjiwDRWm1SP9Lkj5A.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk ok di ol om translated">图3: Google的张量处理单元，(左)Google Colab中可用的TPUv2，(中)Kaggle中可用的TPUv3，(右)由<a class="ae kv" href="https://cloud.google.com/tpu/docs/intro-to-tpu" rel="noopener ugc nofollow" target="_blank"> Google Cloud </a>在<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> (CC BY 4.0) </a>下制作的TPU的矩阵乘法单元动画</p></figure></div><p id="3f2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别是Google的TPUv2实现了8核180万亿次浮点运算，每个都有一个128x128 PEs的二维脉动阵列。谷歌的TPUv3实现了高达420万亿次浮点运算。在其他改进中，TPUv3将每个内核的脉动阵列数量增加了一倍，将每个内核的内存容量增加了一倍，引入了高带宽内存并改善了互连。上面提到的FLOPS是理论上限。在实践中，很难实现这些价值，因为不是所有的操作都受益于这种非常专业化的架构。此外，应该优化数据的输入管道，以保持TPU的计算资源繁忙。</p><p id="ad71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">脉动阵列和TPU一样，对于某些任务来说是高度专业化的。然而，训练机器学习模型需要更多的步骤，例如预处理数据或更新优化器状态。对于这样的任务，通用处理器(即CPU)仍然是有益的。如果建立了到Google的一个云TPU的连接，那么实际上就建立了到一个主机VM的连接。该主机虚拟机管理与本地环境的通信，并通过高带宽PCIe接口控制TPU(参见图4)。</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk on oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/407c713f31bd5b6381bd3521e5002a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/0*NmrQQSa7NByNxjef.jpg"/></div></figure><figure class="oc kk oo oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/67eef1d7dd2a001a16d938076116396d.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*WOuYOJbtWQtqGIKWSBUCZQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk op di oq om translated">图4<a class="ae kv" href="https://cloud.google.com/tpu/docs/intro-to-tpu" rel="noopener ugc nofollow" target="_blank">Google Cloud</a>在<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> (CC BY 4.0) </a>、【右】WARP架构下(左)云TPU界面对比[5]</p></figure></div><p id="fea4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">几十年前(也是在80年代)，通过具有特殊编译器的通用主机来控制快速专用脉动阵列的想法再次被提出，即WARP架构。[5]这样的系统是可扩展的，最重要的是，对程序员隐藏了它们的复杂性。</p><p id="6f41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你对谷歌TPUs的内部架构感兴趣，我推荐你观看<a class="ae kv" href="https://www.youtube.com/watch?v=4bGoGjTRT9U" rel="noopener ugc nofollow" target="_blank">谷歌来自Hot Chip 2020 </a>的演示。</p><h2 id="97a4" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">1.2.分销策略</h2><p id="5b92" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">在研究TensorFlow中的实现之前，理解分布式培训的概念很重要。我们之前已经看到TPU有几个内核。多核的出现实现了真正的时间并行，这意味着代码可以并行执行，而不仅仅是以时间复用的方式执行。至少有两种范例可以实现并行以加快训练过程:模型并行和数据并行。模型并行性和数据并行性的图形表示如图3所示。</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk or oe of og oh paragraph-image"><img src="../Images/3dfd63edc7a38feb72af5756a50c12a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*96FlUW8hnau2WVFFm6R59Q.png"/></figure><figure class="oc kk os oe of og oh paragraph-image"><img src="../Images/b6ec95602e310c53a5ac41c3e816ff70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*XQJXCpPJOD1pYctpAwZJww.png"/><p class="kr ks gj gh gi kt ku bd b be z dk ot di ou om translated">图3:并行训练编程习惯用法，(左)模型并行训练，(右)数据并行训练[2]</p></figure></div><p id="49d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在模型并行中，模型的不同部分在不同的设备上训练，但是在相同的训练批次上。</p><p id="5b0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在数据并行训练中，单个模型被镜像并分布在由主机设备管理的多个设备(也称为工作者)中。主机设备充当模型的参数服务器。每个工人在训练批次的子集上训练其模型。对每个员工单独计算损失，并将其发送到外派国。各个损失值通过例如将它们相加来组合。最后，主机更新模型的权重，并重新分配更新后的模型。在这种情况下，客户端的单个线程可以控制训练循环，而大部分模型计算分布在几个设备上。由于所有模型总是具有相同的参数，并且同步更新，这种方法被称为同步数据并行。</p><h1 id="dade" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">2.用TensorFlow在TPU上实现分布式训练</h1><p id="533a" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">在TPU上分配你的训练并不像听起来那么琐碎，但绝对值得努力。在Github 上的<a class="ae kv" href="https://github.com/sascha-kirch/ML_Notebooks/blob/7591d0c595620a28ccccc028618619815fd0d607/GAN_distributed_training.ipynb" rel="noopener ugc nofollow" target="_blank">这个笔记本里可以找到在TPU训练DCGAN的完整例子。</a></p><h2 id="8056" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">2.1.硬件检测</h2><p id="f761" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">第一步是连接到TPU。与CPU和GPU环境相比，在TPU环境中，您必须显式检测并连接到TPU。由于Google Colab和Kaggle中的TPU资源有限，您可能希望在CPU/GPU上开发和调试，并在TPU上执行培训。因此，脚本自动检测可用的硬件并自动设置所需的策略是很有用的(参见代码1)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ov ow l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码1:自动检测硬件并设置相应的策略</p></figure><blockquote class="ox oy oz"><p id="5eb7" class="kw kx nz ky b kz la jr lb lc ld ju le pa lg lh li pb lk ll lm pc lo lp lq lr ij bi translated">提示:自动硬件检测的实现可以在我在<a class="ae kv" href="https://github.com/sascha-kirch/DeepSaki" rel="noopener ugc nofollow" target="_blank"> Github </a>或<a class="ae kv" href="https://pypi.org/project/DeepSaki/" rel="noopener ugc nofollow" target="_blank"> PyPi </a>上的深度学习库DeepSaki中找到。</p></blockquote><p id="727b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您正在运行一个TPU环境，例如GoogleColab或Kaggle，您现在应该连接到您的TPU。</p><h2 id="89ae" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">2.2.分布式数据集</h2><p id="c421" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">如前所述，当遵循同步数据并行范例时，您的训练数据将分布在所有工作人员中。TensorFlow提供了一个从<code class="fe pd pe pf pg b"><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank">tf.data.Dataset</a></code>转换到<code class="fe pd pe pf pg b"><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset" rel="noopener ugc nofollow" target="_blank">tf.distribute.DistributedDataset</a></code>的简单API。这里举个例子:</p><pre class="kg kh ki kj gt ph pg pi pj aw pk bi"><span id="a1e1" class="nm mc iq pg b gy pl pm l pn po">train_distributed = strategy.experimental_distribute_dataset(train)<br/>test_distributed = strategy.experimental_distribute_dataset(test)</span></pre><p id="6c3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的例子中，<code class="fe pd pe pf pg b">train</code>和<code class="fe pd pe pf pg b">test</code>属于<code class="fe pd pe pf pg b">tf.data.Dataset</code>类型。通常你会首先使用<code class="fe pd pe pf pg b">batch()</code>、<code class="fe pd pe pf pg b">cache()</code>或<code class="fe pd pe pf pg b">map()</code>来定义你的输入管道，然后将你的数据集转换成分布式数据集。使用全局批处理大小而不是每个副本的批处理大小进行批处理非常重要。全局批次大小由所有副本中训练的批次数量决定。如果数据集除以可用于培训的员工人数，则为批量大小。</p><h2 id="a35e" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">2.3.人工减少损失</h2><p id="bec3" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">您可能想知道为什么总是得到标量损失值，即使在计算多维损失时，比如两幅图像之间的像素损失。原因是:TensorFlow执行自动损耗减少，在大多数情况下，损耗是通过对一个批次内的所有值求和并除以批次大小来减少的。实现自定义训练循环时，TensorFlow要求您手动减少损失，因为不允许使用<code class="fe pd pe pf pg b">tf.keras.losses.Reduction.AUTO</code>。</p><p id="5b68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们要做的基本上是重新实现上述行为。以下代码显示了使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">铰链损耗</a>的程序:</p><pre class="kg kh ki kj gt ph pg pi pj aw pk bi"><span id="439b" class="nm mc iq pg b gy pl pm l pn po">import tf.keras.losses as losses<br/>loss_fn = losses.Hinge(reduction=losses.Reduction.NONE)<br/>loss = loss_fn(ground_truth, prediction)<br/>loss = tf.reduce_sum(loss) * (1./GLOBAL_BATCH_SIZE)</span></pre><p id="3c07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，定义损失函数，并应用无减少策略。因此不执行任何缩减。接下来，计算<code class="fe pd pe pf pg b">ground_truth</code>和模型的<code class="fe pd pe pf pg b">prediction</code>之间的损失。然后，<code class="fe pd pe pf pg b">loss</code>的所有值相加得到一个标量。最后，标量损失由全局批处理大小来缩放，而不是当前副本的批处理大小。使用全局批处理大小而不是每个副本的批处理大小进行缩放的原因是，在引言中描述的数据并行性范例中，所有工作线程的损失被加在一起。关于实现的细节将在定制训练循环一节中介绍。</p><h2 id="9fce" class="nm mc iq bd md nn no dn mh np nq dp ml lf nr ns mn lj nt nu mp ln nv nw mr nx bi translated">2.4.自定义训练循环</h2><p id="d8ab" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">到目前为止，我们已经连接到TPU，准备了分布式数据集并执行了手动损失缩放。最后缺失的部分是分布式训练循环。</p><p id="c89c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该示例侧重于培训的实现，但也可以以类似的方式应用于验证。</p><p id="d562" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们已经通过子类化<code class="fe pd pe pf pg b">tf.keras.Model</code>实现了自己的定制模型(参见代码2)。为了实现我们的定制训练循环，我们首先必须实现两个方法:<code class="fe pd pe pf pg b">train_step()</code>和<code class="fe pd pe pf pg b">distributed_train_step()</code>。</p><p id="08de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">占位符<code class="fe pd pe pf pg b">(…)</code>表示缺少代码，这是特定于给定示例的，但对于解释定制训练循环并不重要。</p><p id="19df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方法<code class="fe pd pe pf pg b">train_step()</code>计算损失函数相对于模型权重的梯度。梯度计算在<code class="fe pd pe pf pg b"><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank">tf.GradientTape</a></code>的上下文中完成。最后，优化器应用更新的权重。</p><p id="04d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方法<code class="fe pd pe pf pg b">distributed_train_step()</code>将<code class="fe pd pe pf pg b">train_step()</code>的执行分配给所有工人，收集每个工人的最终损失，并对所有损失项求和。</p><p id="5740" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">装饰器<code class="fe pd pe pf pg b">@<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">tf.function</a></code>将装饰过的函数编译成图形，这加速了它的执行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ov ow l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码2:分布式培训步骤的实现</p></figure><p id="d16f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">定义好培训步骤后，我们现在可以继续最后一步:实现实际的培训循环。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ov ow l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码3:训练循环的实现</p></figure><p id="d334" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个函数中，我们使用for循环一次执行一个时期，直到所有时期都被执行。然后，我们在分布式数据集上逐批迭代，并在每一批上调用模型的<code class="fe pd pe pf pg b">distributed_train_step()</code>方法。该方法返回所有工作线程的损失。我们对每一批的损失进行求和，并在迭代所有批后，根据训练的批数对结果进行缩放。</p><p id="af32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！祝贺你，你成功了！👏🎉</p><h1 id="40af" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">3.结论</h1><p id="5bbe" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">我们已经从理解传统计算硬件的内存瓶颈走了很长一段路，引入了脉动阵列作为谷歌TPU的关键元素，讨论了分布式训练，最后研究了TensorFlow中的实现。TPU具有惊人的性能指标，但因此是高度专业化的硬件，这意味着您可能无法充分利用它。</p><p id="1a0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，你可以免费访问TPUs是惊人的，因此，如果你知道如何使用它，这是一个很好的机会。</p><p id="c2db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，TPU并不是加速你训练的唯一方法。在其他技术中，你可以在GPU/TPU 上使用混合精度进一步<a class="ae kv" href="https://medium.com/@SaschaKirch/speed-up-your-tensorflow-training-with-mixed-precision-on-gpu-tpu-acf4c8c0931c" rel="noopener">加速你的TensorFlow训练，通过它你可以利用专门的float16加速。</a></p></div><div class="ab cl pp pq hu pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="ij ik il im in"><h1 id="11c7" class="mb mc iq bd md me pw mg mh mi px mk ml jw py jx mn jz pz ka mp kc qa kd mr ms bi translated">4.更多资源</h1><ol class=""><li id="2ad2" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated">在TPU上训练DCGAN的笔记本:<a class="ae kv" href="https://github.com/sascha-kirch/ML_Notebooks/blob/7591d0c595620a28ccccc028618619815fd0d607/GAN_distributed_training.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="3f0c" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">深度学习python库DeepSaki: <a class="ae kv" href="https://github.com/sascha-kirch/DeepSaki" rel="noopener ugc nofollow" target="_blank"> Github </a>或者<a class="ae kv" href="https://pypi.org/project/DeepSaki/" rel="noopener ugc nofollow" target="_blank"> PyPi </a></li><li id="9b87" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">关于<a class="ae kv" href="https://medium.com/@SaschaKirch/speed-up-your-tensorflow-training-with-mixed-precision-on-gpu-tpu-acf4c8c0931c" rel="noopener">的指南在GPU/TPU上用混合精度加速你的TensorFlow训练</a></li></ol></div><div class="ab cl pp pq hu pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="ij ik il im in"><p id="3fa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]谷歌。2021."云TPU . "谷歌云。检索到2021年5月2日(<a class="ae kv" href="https://cloud.google.com/tpu/docs/system-architecture?hl=de" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/tpu/docs/system-architecture?hl=de</a>)。</p><p id="6c15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]阿巴迪等人2016。"张量流:异构分布式系统上的大规模机器学习."<em class="nz">ArXiv:1603.04467【Cs】</em>。</p><p id="9518" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] M. Horowitz，“计算的能源问题(以及我们可以做些什么)”，2014年IEEE国际固态电路会议技术论文摘要(ISSCC)，美国加利福尼亚州三藩市，2014年2月，第10-14页。doi: 10.1109/ISSCC.2014</p><p id="0002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]孔汉东(1982年)。为什么选择脉动架构？IEEE计算机，15(1)，37–46。<a class="ae kv" href="https://doi.org/10.1109/MC.1982.1653825" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/MC.1982.1653825</a></p><p id="0b24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] Annaratone，m .，Arnould，e .，Gross，t .，Kung，H. T .，Lam，m .，Menzilcioglu，o .，和Webb，J. A. (1987年)。翘曲计算机:结构、实现和性能。IEEE计算机汇刊，36(12)。<a class="ae kv" href="https://doi.org/10.1109/TC.1987.5009502" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/TC.1987.5009502</a></p></div></div>    
</body>
</html>