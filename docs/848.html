<html>
<head>
<title>The Science behind AlphaGo and AlphaGo Zero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlphaGo和AlphaGo Zero背后的科学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-science-behind-alphago-and-alphago-zero-44aeef000448#2022-01-31">https://towardsdatascience.com/the-science-behind-alphago-and-alphago-zero-44aeef000448#2022-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="774b" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">AlphaGo和AlphaGo Zero背后的科学</h1></div><div class=""><h2 id="27ea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概念性的友好解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5ebd7d0758d026e0a28bab2fdcbe96cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8Z6WG427K8u1UORY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@raysontjr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">陈泰铭</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="edac" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">背景</h1><p id="3c7a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我观看由谷歌研究实验室<a class="ae ky" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>开发的围棋计算机程序AlphaGo上的<a class="ae ky" href="https://www.youtube.com/watch?v=WXuK6gekU1Y" rel="noopener ugc nofollow" target="_blank">纪录片</a>之前，我不知道在AlphaGo之前，计算机程序还无法在专业水平上与人类棋手竞争。我天真地认为，计算机程序能够从游戏的特定状态简单地模拟每一个可能的移动序列(“穷举搜索”)，并选择具有最佳结果的移动。事实证明，由于可能的场景数量惊人，这甚至对于计算机也是不可行的。</p><p id="efb7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这部纪录片只是简单地触及了AlphaGo背后的科学，我被这部纪录片迷住了，于是我转向DeepMind发表的研究论文，以了解用于开发AlphaGo(以及AlphaGo Zero，一个性能更好的AlphaGo版本)的机器学习技术。研究论文写得很好，但对于没有深度强化学习和蒙特卡罗树搜索知识的人来说，可能有点太专业了。尽管如此，这不应该阻止任何人抓住这些论文介绍的有趣概念和观察结果，因为人们普遍认为AlphaGo和AlphaGo Zero代表了开创性的机器学习应用。</p><p id="d65a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">写这篇文章的目的是用一种语言来解释AlphaGo和AlphaGo Zero背后的科学，这种语言让那些有兴趣了解他们如何在超人的水平上学会玩围棋的人理解，但可能会被强化学习或蒙特卡洛树搜索等术语所阻止。</p><h1 id="c786" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简单的游戏规则，复杂的游戏方式</h1><p id="997e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">围棋的规则相对简单。游戏在棋盘上19×19的格子上进行，两名围棋手轮流在格子的交叉点上放置石头。如果被对手的石头包围，玩家的石头可以被捕获并从棋盘上移走。玩家的最终目标是用他们的石头在游戏板上包围尽可能多的区域。</p><p id="0dc8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，给定简单的规则，人类多年来已经发展出复杂的策略，以有效地走在周围的领土上。人类做出判断，例如评估现在形成坚固的领土和形成有助于在游戏后期建立影响力以获得领土的石头图案之间的权衡，识别连接不同石头组并防止它们被捕获的方法，等等。</p><p id="bcd5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从数学上来说，这个可能的移动序列(“搜索空间”)的数量大约是250 ⁵⁰.这是因为为了决定一场围棋比赛的获胜者，棋手之间要进行大约150轮比赛，在给定的一轮中，每一步平均有大约250个合法位置。根据下面的Python代码，该数字有360位。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="59f0" class="mx la it mt b gy my mz l na nb">print(len(str(250**150)))</span></pre><p id="8e96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">问题陈述实际上可以归结为寻找缩小搜索空间的方法，DeepMind通过以下方式实现了这一点:</p><ul class=""><li id="d94f" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm nh ni nj nk bi translated">训练一个策略来指导从游戏的当前状态中值得探索的移动顺序。这减少了(250)个合法移动的搜索空间。</li><li id="09ce" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">通过给定的棋盘位置训练游戏获胜者的预测器。这减少了(150)轮次的搜索空间。</li></ul><h1 id="853f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">政策</strong></h1><p id="2614" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如前所述，需要一个策略来指导计算机程序搜索值得探索的移动序列，以便它最大程度地模拟这些序列中的游戏。对于AlphaGo来说，这一策略是通过观察职业选手如何基于来自<a class="ae ky" href="https://www.gokgs.com/" rel="noopener ugc nofollow" target="_blank"> KGS Go服务器</a>的3000万场比赛来训练的。本质上，这个策略(“策略A”)允许AlphaGo模仿人类职业比赛。</p><p id="c5b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">策略A的输出是合理移动的概率分布。为了用一个简单的例子来解释这一点，假设我们正在用下面游戏棋盘中的白石头进行下一步棋，策略A可能建议两种可能的棋，棋1和棋2，概率分别为60%和40%。60%和40%将基于人类职业游戏的历史，由此给定游戏的当前状态(即，具有一个黑石的棋盘)，人类职业玩家在100次中已经移动1 60次(并且在100次中移动2 40次)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/462ae7c5b6e7e867a43f43c2ba4c6c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*vOQ4zcAwhZ3pGMw7ZQEJVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:策略网络演示。作者图片</p></figure><p id="6e64" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，凭直觉，即使是职业棋手的模仿，也不一定能让计算机程序在围棋比赛中胜出。然后，策略A通过<strong class="lt iu">的自我游戏</strong>进行精炼，由策略A指导的两个计算机程序(最初)相互对抗，直到确定赢家。策略A将被更新以反映这个博弈的结果，成为策略A的更好版本。</p><p id="a445" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有人可能会问，这如何提高政策的绩效。使用与上面相同的例子，计算机程序A可以选择进行移动1，而计算机程序B可以选择相同的移动，或者选择<strong class="lt iu">探索</strong>移动2，因为策略A最初应该引导计算机程序进行100次移动1 60次和100次移动2 40次。随着这种情况在游戏的每个回合都发生，可以证明移动2(或者更一般地，在策略A下不一定达到最高概率的移动)将最终导致计算机程序B获胜，在这种情况下，两个移动的概率可以被更新。在这种情况下，在给定游戏结果的情况下，进行移动1的60%概率减少，同时进行移动2的40%概率增加。同样的过程在数百万次自我游戏中反复重复，直到形成接近最优的策略B。理论上，策略B分配给一步棋的“学习”概率越高，这步棋导致的博弈结果就越好。</p><p id="c576" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">退一步说，我观察到，自我对弈隐含地奖励了人类职业棋手的发现，这在一定程度上促成了AlphaGo的成功。</p><h1 id="5a58" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">预测器</h1><p id="273c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">预测器输出单个值，指示在给定游戏状态(即当前棋盘位置)的情况下玩家获胜的可能性。</p><p id="d89b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用于训练预测器的数据通过自玩再次收集，这一次是在两个计算机程序之间，这两个程序都由策略b指导。在自玩的特定游戏中，收集许多棋盘位置和游戏结果(即，赢或输)。通过足够多的自玩游戏，预测器可以被训练为以良好的准确性告知在特定棋盘位置的玩家赢得游戏的概率。也就是说，它告知游戏的当前状态(和后续状态)对玩家进行下一步棋有多有利。</p><h1 id="6403" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">走一步棋——alpha go</h1><p id="2571" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">概括地说，到目前为止，我们已经培训了以下内容:</p><ul class=""><li id="5f65" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm nh ni nj nk bi translated">基于人类职业游戏的最佳策略</li><li id="4f68" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">策略B，它根据策略A发起的自我游戏来通知最佳移动</li><li id="759c" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">预测器，根据游戏的当前状态通知玩家获胜的可能性</li></ul><p id="ee1f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了选择下一步棋，AlphaGo从游戏的当前状态开始执行<em class="nr"> N </em>次模拟。简单介绍一下模拟，在我的工作中，模拟不仅有助于理解特定事件的平均结果(例如，平均索赔成本)，而且有助于理解可能结果的范围(例如，每200年发生一次的不利索赔事件的成本，可以从200次模拟中返回最高索赔金额的模拟中获得)。在围棋中，模拟的走法序列假设你和你的对手会选择最佳走法，直到游戏结束。此外，模拟增加了选择一系列移动的随机性，这一点很重要，因为我们确实想探索可能没有被策略看到或引导的移动。</p><p id="bda4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于AlphaGo来说，在一个特定的模拟中<em class="nr"> n，</em>要模拟的走法顺序主要由以下几个因素控制:</p><ol class=""><li id="be00" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm ns ni nj nk bi translated">特定序列被访问的次数</li><li id="04d0" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">之前在特定动作序列中获胜的经验</li></ol><p id="610c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我将用一个简单的例子来解释这两个要点。让我们假设在之前的模拟<em class="nr"> n -1 </em>中，在<strong class="lt iu">策略</strong>的指导下，选择了第一步之后是第四步的顺序(由下图中的蓝色实心箭头表示)，并且基于第四步的游戏状态，预测器通知获胜。也就是说，这个特殊的模拟序列导致了胜利。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/237ec557fbcc150ba17a680b02ec93ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*HeI8KT50yL-tQMIEg-4jnQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:移动选择演示。作者图片</p></figure><p id="3f7f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于当前的模拟，AlphaGo识别出已经访问了移动1之后是移动4的序列，然后它将驱动模拟更可能采用以前没有访问过的其他序列，例如移动2，或者移动1之后是移动3。这样做是为了鼓励探索。</p><p id="bcb0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另外，由于移动1后跟随移动4的顺序导致了上一次模拟中的胜利，它也将驱动模拟在当前模拟中采用这一特定顺序(反之亦然，即失败)。这有时被称为<strong class="lt iu">剥削</strong>。</p><p id="c100" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我想强调一下上面提到的一些有趣的结果:</p><ul class=""><li id="a6ed" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm nh ni nj nk bi translated">在游戏开始时，当a玩家有更多的选择来放置下一颗石头时，AlphaGo鼓励搜索人员<strong class="lt iu">探索</strong>，因为先前的获胜经验是不确定的，因为即使在政策的指导下，下一步最好的棋也有很大的搜索空间。</li><li id="8dab" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">相比之下，当棋盘上合理地填满了石头时，来自策略或预测者的先前获胜经验会变得更有影响力，因为AlphaGo会推动一系列移动，从而导致最有可能获胜的状态或棋盘位置。这个<strong class="lt iu">利用了</strong>以前从人类专业人员以及自我游戏中获得的经验。</li><li id="d4c3" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">我觉得有趣的是，模拟中的探索通常是由策略A而不是策略B指导的，因为人类的智能被认为是更加多样化的，机器无法完全学习。AlphaGo策略B的主要用途是训练预测器。</li></ul><p id="9102" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在进行了<em class="nr"> N </em>次模拟后，AlpohaGo选择了访问次数最多的移动顺序(尽管只有第一个顺序是相关的)。默认情况下，这应该是在勘探和开发之间取得正确平衡的举措。</p><h1 id="a3b8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">开始行动——alpha go Zero</h1><p id="539e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">简而言之，AlphaGo Zero是AlphaGo的更好版本，“Zero”强调了与人类专业游戏的解耦。</p><p id="db99" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然AlphaGo Zero的整体(训练)结构在很大程度上与AlphaGo一致，但它们的主要区别在于:</p><ol class=""><li id="abac" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm ns ni nj nk bi translated">AlphaGo Zero的策略和预测器完全是靠自弹自演训练出来的。也就是说，在训练AlphaGo Zero时没有使用人类输入，如人类专业游戏。</li><li id="26e1" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">AlphaGo架构使用了许多“手工制作”的功能。例子是空的相邻点的数量和捕获的石头的数量。这些在AlphaGo Zero中被删除，它只使用原始棋盘位置作为输入。</li><li id="37a8" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">Alpha Go Zero结合了大部分策略和预测模型。也就是说，他们是一起受训的。</li><li id="ba6f" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm ns ni nj nk bi translated">在AlphaGo Zero下训练的预测器进一步减少了模拟中的回合数。这在一定程度上加快了AlphaGo Zero的训练速度。</li></ol><p id="5ca3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">总的来说，AlphaGo Zero是机器学习更通用的应用，相比AlphaGo结构更简单，人机交互更少。DeepMind认识到，人类在围棋游戏中的知识可以通过单独的自我游戏<strong class="lt iu">和随机策略发起的</strong>来学习(并超越)。事实上，AlphaGo Zero仅用了40个小时的训练就超过了AlphaGo的表现。</p><p id="30da" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我发现这很有趣，但同时也很直观。让我们假设总有一个最优策略来引导玩家稳赢。给定我们的当前策略(很可能是一个随机的表现不佳的策略)，通过自我游戏的迭代，随着时间的推移，当前策略应该逐渐收敛到最优策略。</p><p id="b5fa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用另一个简单的例子来说明，在下图所示的博弈状态中，我们假设最优策略的概率分布为(100%，0%)，也就是说，采取行动1肯定会赢。另外，为了简单起见，我们假设游戏从当前状态一步结束。为了了解最优策略，如果我们通过自玩游戏的互动，分别从移动1和移动2的随机策略(50%，50%)开始，随机策略将总是收敛到最优策略，因为在每次自玩游戏中，策略将随着移动1的进行而更新以增加移动1的概率，反之亦然。</p><p id="cfbe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在AlphaGo下，我们以上面的(50%，50%)开始自玩的随机策略可能更接近策略A下的最优策略(例如(90%，10%))，因为它是基于人类职业游戏形成的。然而，AlphaGo Zero证明了无论我们如何启动策略，机器总是可以通过与先前版本的策略进行博弈来学习迭代地改进策略，并最终找到接近最优策略的代理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/462ae7c5b6e7e867a43f43c2ba4c6c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*vOQ4zcAwhZ3pGMw7ZQEJVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:自学演示。作者图片</p></figure><h1 id="edb9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">人类直觉的学习</h1><p id="6ec9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如DeepMind的研究论文所示，在AlphaGo Zero接受训练时，它学会了玩一些“普通角球序列”，这些策略是由人类开发并经常玩的，而不需要像AlphaGo一样“观察”人类专业人士如何玩。令人着迷的是，在这些策略被发现后的几个小时内，一些策略被忽略了，因为它发现了新的和据称更好的变化。</p><p id="45aa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，在训练的早期，AlphaGo Zero像人类初学者一样，专注于尝试立即捕捉对手的石头。这与展示更平衡的策略和随着自我游戏的继续掌握游戏的基本原理相比。</p><h1 id="55e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="de72" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我用一些简单的例子解释了AlphaGo和AlphaGo Zero是如何被训练来选择最佳走法的。在高水平上，AlphaGo和AlphaGo Zero通过有效地减少下一个最佳步骤的搜索空间的呼吸和深度，成功地实现了超人的性能。</p><p id="e0a1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">关于AlphaGo和AlphaGo Zero的运行方式，有许多发人深省的观察，我想再次强调一下:</p><ul class=""><li id="6d53" class="nc nd it lt b lu mn lx mo ma ne me nf mi ng mm nh ni nj nk bi translated">尽管AlphaGo Zero并不依赖人类知识(即策略A，尽管策略B)，但人类知识在AlphaGo搜索下一步最佳棋时具有更大的影响力，因为人类往往会选择“一系列不同的有前途的棋步”，尤其是在游戏的早期阶段，玩家往往会探索更多。</li><li id="eb9c" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">Alpha Go和AlphaGo Zero背后的走法选择是由一个政策引导的，但同时鼓励违背政策建议的探索。在哲学层面上，它告诉我们，有时我们不应该害怕选择让我们走出舒适区的道路，这可能会更有回报。</li><li id="4115" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated">模拟或采样一系列动作有助于形成最佳策略。这与随机森林算法优于单个决策树的优势没有什么不同。你永远不知道在未知的道路上会发现什么！</li></ul><p id="94da" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是我从上述观察中得到的一些自我反思。</p><h2 id="6ead" class="mx la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated"><strong class="ak">完美与实用</strong></h2><p id="3c94" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">想象一下，我们确实有一个计算机程序，它可以进行彻底的搜索，寻找最佳的走法，并在每个回合都打出完美的走法。这有必要吗？</p><p id="d625" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在许多商业应用中，某些任务达到100%准确率的边际收益(相比之下，比如说95%的准确率)可能不会超过收益递减的成本。例如，训练一个0.05或5%精度改进的模型可能需要10倍的时间。另一个例子是，在职业锦标赛中，以4比1或5比0击败Lee Sedol是否有助于决定获胜者。</p><h2 id="8ece" class="mx la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">指导与说明</h2><p id="f5e8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">执行某项任务的政策或指令可能是一把双刃剑，在某种意义上，它为用户提供了指导，也带来了限制。AlphaGo Zero通过删除先前被认为对训练AlphaGo有价值的人类知识，实现了更好的性能，这证明了这一点。</p><p id="b705" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为人事经理，我有时会根据自己的经验对完成任务的最佳方式有一个主观的看法，并倾向于根据这一看法向我的直接下属提供和执行这样的指示。指令可能更受过程驱动，而AlphaGo和AlphaGo Zero的成功告诉我，通过关注最终目标(或机器学习语言中的奖励)来领导更有效，允许人们“搜索”并学习完成任务的最佳方式。</p><h2 id="0d1f" class="mx la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated"><strong class="ak">勘探与开采</strong></h2><p id="65d0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">AlphaGo和AlphaGo Zero背后的科学让我想起了生活中发生的事情。难道我们不都倾向于<strong class="lt iu">探索</strong>以获得经验，然后<strong class="lt iu">利用这些经验</strong>来做决定吗？</p><p id="6e8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，对于有兴趣深入了解AlphaGo和AlphaGo Zero技术方面的读者，我推荐阅读这篇<a class="ae ky" href="https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319" rel="noopener">文章</a>。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="5ba7" class="kz la it bd lb lc om le lf lg on li lj jz oo ka ll kc op kd ln kf oq kg lp lq bi translated">参考</h1><p id="3e4f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] Silver，d .，Huang，a .，C. <em class="nr">等</em>用深度神经网络和树搜索掌握围棋博弈。<em class="nr">自然</em> <strong class="lt iu"> 529，</strong>484–489(2016)，<a class="ae ky" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">https://storage . Google APIs . com/deepmindmaedia/alpha go/alpha gonaturepaper . pdf</a>，2022年1月25日获取</p><p id="b2c2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] Silver，d .，Schrittwieser，j .，Simonyan，K. <em class="nr">等</em>掌握无人类知识的围棋博弈。<em class="nr">自然</em> <strong class="lt iu"> 550，</strong>354–359(2017)。https://doi.org/10.1038/nature24270，2022年1月26日访问</p></div></div>    
</body>
</html>