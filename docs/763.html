<html>
<head>
<title>Reading .h5 Files Faster with PyTorch Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch数据集更快地读取. h5文件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc#2022-01-27">https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc#2022-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="02c8" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用PyTorch数据集更快地读取. h5文件</h1></div><div class=""><h2 id="fd3b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用弱洗牌的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ac9630edf203e9b990748b5ff7fa9fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iVHT640dX8bQgm-2ZCez2w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@amoltyagi2" rel="noopener ugc nofollow" target="_blank"> Amol </a>提供，取自<a class="ae ky" href="https://unsplash.com/photos/zyJyB9S1oZs" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>。</p></figure><p id="ad81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，我使用<a class="ae ky" href="https://www.robots.ox.ac.uk/~vgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津Pet数据集</a>(参考文献中的引用)进行了一个多任务深度学习项目。虽然我能够通过使用Colab的GPU来克服极其缓慢的训练速度，但在加载数据时，我很快就遇到了瓶颈。尽管使用了一个庞大的多任务模型，对于一个32的批量，训练速度大约是0.5秒，而数据加载过程大约是12…</p><p id="0482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在互联网上深入搜索后，我发现这是许多人(参见<a class="ae ky" href="https://discuss.pytorch.org/t/dataloader-when-num-worker-0-there-is-bug/25643/2" rel="noopener ugc nofollow" target="_blank"> PT论坛</a>、<a class="ae ky" href="https://stackoverflow.com/questions/61915325/is-there-a-more-efficient-way-of-retrieving-batches-from-a-hdf5-dataset" rel="noopener ugc nofollow" target="_blank"> S/O #1 </a>和<a class="ae ky" href="https://stackoverflow.com/questions/68705944/reading-h5-file-is-extremely-slow" rel="noopener ugc nofollow" target="_blank"> S/O #2 </a>)面临的问题，也是尚未得到充分解决的问题。我想到了一个PyTorch解决方案，我认为它适用于大多数情况。这使得我在Colab上的加载时间从12秒减少到0.4秒左右。本文将首先解释瓶颈，然后介绍解决方案。随后是实证结果和对理论保证的评论。我假设您了解PyTorch数据集，但是对Python中的类有足够的工作知识就足够了。</p><h2 id="70f2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">瓶颈从哪里来？</h2><p id="a96d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">PyTorch中创建标准数据集的方式基于<code class="fe mt mu mv mw b">torch.utils.data.Dataset</code>类。这些定制数据集有一个<code class="fe mt mu mv mw b">__getitem__</code>方法，在调用该方法时，它会加载一组给定索引的数据。我写的数据集是这样的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">完整的数据加载器可以在GitHub资源库中找到，<a class="ae ky" href="https://github.com/namiyousef/multi-task-learning/blob/main/data/data.py" rel="noopener ugc nofollow" target="_blank">这里</a>。在数据集初始化时调用<strong class="ak"> _load_h5_file_with_data </strong>方法，将. h5文件预加载为生成器对象，以防止每次调用<strong class="ak"> __getitem__ </strong>时被调用、保存和删除。</p></figure><p id="e2e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，使用<code class="fe mt mu mv mw b">torch.utils.data.Dataloader</code>类，我定义了一个<code class="fe mt mu mv mw b">trainloader</code>来批量处理我的数据用于训练目的。这就是问题所在。</p><p id="25b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每次迭代<code class="fe mt mu mv mw b">trainloader</code>时，数据集中的<code class="fe mt mu mv mw b">__getitem__</code>方法被调用<code class="fe mt mu mv mw b">batch_size</code>次，以生成一批数据。这对于. h5数据集来说是有问题的，因为从它们中查询数据的时间复杂度与对它们的调用次数成线性比例。因此，如果查询单个索引的开销是<strong class="lb iu"> x </strong>，那么在我们的例子中，我们期望每批的总时间是<strong class="lb iu"> 32*x </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里，列车循环的每一对(输入，目标)将由<strong class="ak">列车装载器</strong>查询数据集32次(由于<strong class="ak"> shuffle=True </strong>)而创建。<strong class="ak"> __getitem__ </strong>方法被调用了32次，每次都使用不同的索引。然后，<strong class="ak"> trainloader </strong>后端聚集来自<strong class="ak"> __getitem__ </strong>方法的单个(输入，目标)返回，返回一个包含32项的(输入，目标)对用于训练循环。</p></figure><h2 id="7697" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用弱洗牌提高速度</h2><p id="6db1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">要认识到的关键点是，. h5文件的瓶颈来自于寻找数据所在的索引。找到随后出现的索引几乎要花上10分钟的时间！</p><p id="9ce5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着查询索引<code class="fe mt mu mv mw b">i</code>处的数据的时间复杂度与查询索引<code class="fe mt mu mv mw b">i:i+batch_size</code>处的数据的时间复杂度几乎相同。因此，如果我们进行批处理，然后对批处理进行混洗(我称之为‘弱混洗’)，那么我们将通过<code class="fe mt mu mv mw b">batch_size</code>因子来加快数据加载过程。下图显示了这种方法与标准洗牌方式的不同之处。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/4212ae8e5502f91e74c275e3bd98cff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDtzE_3eNrZEXaFMx89Ebw.png"/></div></div></figure><p id="0110" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法的明显含义是，模型中引入了一些偏差，因为我们不是在点的水平上洗牌。虽然人们可能试图在批次级别洗牌后逐点洗牌以减少偏差，但这是徒劳的，因为损失函数是可交换的。如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/9e7e68004c077b531d30b20db8a27a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iZ2a96vIXDSp-pgjTaHjw.png"/></div></div></figure><p id="ce20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> m &gt;</strong></p><h2 id="6757" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">弱洗牌实现</h2><p id="2d29" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">数据集不需要更改。这是因为来自<code class="fe mt mu mv mw b">__getitem__</code>的<code class="fe mt mu mv mw b">index</code>参数可以是一个索引列表。因此，如果我们能够传递一个列表<code class="fe mt mu mv mw b">list(range(i, i+batch_size))</code>，那么我们就能够通过对. h5文件的一次查询获得整批数据。</p><p id="287b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着需要改变的是数据加载器。为此，必须使用<code class="fe mt mu mv mw b">sampler</code>参数创建一个自定义加载方案。如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="ea07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几个要点:</p><ul class=""><li id="7fb8" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><code class="fe mt mu mv mw b">RandomBatchSampler</code>是生成索引<code class="fe mt mu mv mw b">i:i+batch_size</code>的自定义采样器</li><li id="d2cd" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><code class="fe mt mu mv mw b">BatchSampler</code>类批量采样<code class="fe mt mu mv mw b">RandomBatchSampler</code></li><li id="4322" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><code class="fe mt mu mv mw b">Dataloader</code>的<code class="fe mt mu mv mw b">batch_size</code>参数必须设置为<code class="fe mt mu mv mw b">None</code>。这是因为<code class="fe mt mu mv mw b">batch_size</code>和<code class="fe mt mu mv mw b">sampler</code>不能同时设置</li></ul><h2 id="e565" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">理论保证</h2><p id="dc21" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">迄今为止，就我们在模型中诱导的偏差而言，我还没有找到弱改组方法的理论保证。我已经在<a class="ae ky" href="https://stats.stackexchange.com/questions/562050/bias-introduced-when-using-weak-shuffling" rel="noopener ugc nofollow" target="_blank">交叉验证</a>和<a class="ae ky" href="https://math.stackexchange.com/questions/4367356/theoretical-bounds-for-error-rate-in-online-learning-when-using-weakly-shuffled" rel="noopener ugc nofollow" target="_blank">数学交流</a>上问过这个问题，如果我得到他们的回复，我会更新这篇文章。在此之前，我会感谢任何建议！</p><h2 id="c5af" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">关键要点</h2><ul class=""><li id="efc9" class="nb nc it lb b lc mo lf mp li np lm nq lq nr lu ng nh ni nj bi translated">本文描述的方法适用于无法将全部数据加载到内存中的情况</li><li id="d9fb" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">使用标准加载方案从. h5文件中加载批处理很慢，因为时间复杂度与对文件进行的查询数量成比例</li><li id="a0e2" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">瓶颈来自于定位第一个索引，任何后续的索引(顺序排列，中间没有间隔！)可以加载，几乎不需要额外的费用</li><li id="2df9" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">通过使用批量采样，我们可以批量加载数据，从而将加载时间减少一倍<code class="fe mt mu mv mw b">batch_size</code></li><li id="566b" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">这意味着数据将被弱混洗。<code class="fe mt mu mv mw b">batch_size</code>的选择仅限于<strong class="lb iu">batch _ size&lt;T21【m】T7。因此，这种解决方案不适合瓶颈来自训练时间的情况。</strong></li></ul><p id="0905" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多任务项目的完整代码可以在<a class="ae ky" href="https://github.com/namiyousef/multi-task-learning" rel="noopener ugc nofollow" target="_blank">这里</a>找到。在不久的将来，我会发布更多关于这个项目的文章，敬请关注！</p></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><h2 id="2d59" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="49d8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] <a class="ae ky" href="https://discuss.pytorch.org/t/recommend-the-way-to-load-larger-h5-files/32993/9" rel="noopener ugc nofollow" target="_blank">推荐加载较大h5文件的方式</a>。PyTorch论坛。</p><p id="e27a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" href="https://pytorch.org/docs/stable/data.html#disable-automatic-batching" rel="noopener ugc nofollow" target="_blank"> torch.utils.data </a>。PyTorch文档。</p><p id="65ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://stackoverflow.com/questions/61458305/how-to-use-a-batchsampler-within-a-dataloader" rel="noopener ugc nofollow" target="_blank">如何在Dataloader </a>中使用Batchsampler。销售订单</p><p id="f815" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] <a class="ae ky" href="https://stackoverflow.com/questions/61915325/is-there-a-more-efficient-way-of-retrieving-batches-from-a-hdf5-dataset" rel="noopener ugc nofollow" target="_blank">是否有更有效的方法从hdf5数据集中检索批次？</a>销售订单</p><p id="20f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] <a class="ae ky" href="https://stackoverflow.com/questions/68705944/reading-h5-file-is-extremely-slow" rel="noopener ugc nofollow" target="_blank">读取. h5文件极其缓慢</a>。销售订单</p><p id="1983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] <a class="ae ky" href="https://discuss.pytorch.org/t/random-batch-sampling-from-dataloader/113457" rel="noopener ugc nofollow" target="_blank">从数据加载器</a>随机批量取样。PyTorch论坛。</p><p id="1619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] <a class="ae ky" href="https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d" rel="noopener">在PyTorch </a>中创建一个定制的数据加载器。中等。</p><p id="0b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8] <a class="ae ky" href="https://stackoverflow.com/questions/33900486/shuffle-hdf5-dataset-using-h5py" rel="noopener ugc nofollow" target="_blank">使用h5py </a>混洗HDF5数据集。销售订单</p><p id="2bd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9]<a class="ae ky" rel="noopener" target="_blank" href="/hdf5-datasets-for-pytorch-631ff1d750f5">py torch的HDF5数据集</a>。中等，</p><h2 id="637d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据集</h2><p id="420a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] <a class="ae ky" href="https://www.robots.ox.ac.uk/~vgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津宠物数据集</a>。许可:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-共享4.0国际许可</a>。</p><p id="b536" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nz">所有图片均来自作者，除非另有说明</em></p></div></div>    
</body>
</html>