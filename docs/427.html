<html>
<head>
<title>Topic Modeling with Political Texts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">政治文本的话题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-with-political-texts-4a9b20b5e91#2022-01-14">https://towardsdatascience.com/topic-modeling-with-political-texts-4a9b20b5e91#2022-01-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="862d" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">政治文本的话题建模</h1></div><div class=""><h2 id="2cb7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自然语言处理和文本分析系列的第二部分——利用LDA</h2></div><p id="44ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">欢迎来到我们的自然语言处理和文本分析系列的第二部分！如果你还没有机会，请阅读本系列的第一部分<a class="ae lb" rel="noopener" target="_blank" href="/using-nlp-and-text-analytics-to-cluster-political-texts-41fdb2c3a325">这里</a>。为了总结我们迄今为止的进展，我们从政治著作的文集开始，如马基雅维利的《君主论》、汉密尔顿/麦迪逊/杰伊的《联邦党人文集》和马克思/恩格斯的《共产党宣言》。从这些作品中，我们导出了文本表格，并使用余弦相似性度量来确定各个作品的聚类，如下所示。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/285a930f1f012c6f164cc1ffe8420cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvjWmbNi5IQ6aTOrN-28Hw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><p id="3c0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一部分中，我们将在先前结果的基础上，尝试为这些集群找到最相关的主题。我们确定的三个集群将被称为“Old_West ”,代表较老的西方政治哲学(红色部分),“US”代表美国政治哲学(绿色部分),而“Communist”代表共产主义政治哲学(橙色部分)。我们还将构建在本系列第1部分中创建的令牌表，下面显示了一个示例。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ls"><img src="../Images/f99503725bfa6048c85ffc0d900efc45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7b7VT1_7OWmoae0kUx2ZA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><p id="1229" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">主题建模<br/> </strong>首先我们应该定义什么是主题建模，以及我们希望通过进行这种分析达到的目标。根据维基百科的定义，主题建模是“一种用于发现出现在文档集合中的抽象‘主题’的统计模型”。考虑到我们使用的是政治作品，很明显，每部作品的主题都与政治理想相关，但是通过使用主题建模，我们将寻找在每个聚类的顶级主题中找到的特定单词，并寻找我们不同聚类之间的潜在关系。</p><p id="b9f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的第一步将是把我们的令牌表做成进行建模所需的格式。为此，我们首先为标记表中的每个标记添加聚类和作者名称作为主要的分层术语，然后我们将各个标记和句子聚合成一个字符串，形成完整的段落字符串。如果我们聚集了太多，比如整个章节的字符串，我们的许多主题将会完全相同，如果我们使用的样本太少，比如一个句子或单个术语，我们的模型将无法识别正确的主题。我们的新层级应反映以下内容:</p><pre class="ld le lf lg gt lt lu lv lw aw lx bi"><span id="b9fb" class="ly lz iq lu b gy ma mb l mc md"><em class="me"># Create Topic Model OHCO</em><br/>OHCO_TM <strong class="lu ir">=</strong> ['clusters', 'authors', 'book_id', 'chap_num', 'para_num']</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/c6a3e89db7e3290ac897e6c5844ba79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*k2cIYVeWJ5GvvnHSrCY6tQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><p id="79e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经准备好了表，我们可以开始应用我们的主题建模算法了。</p><pre class="ld le lf lg gt lt lu lv lw aw lx bi"><span id="eaa1" class="ly lz iq lu b gy ma mb l mc md">tfv <strong class="lu ir">=</strong> CountVectorizer(max_features<strong class="lu ir">=</strong>4000, stop_words<strong class="lu ir">=</strong>'english')<br/>tf <strong class="lu ir">=</strong> tfv<strong class="lu ir">.</strong>fit_transform(PARAS<strong class="lu ir">.</strong>para_str)<br/>TERMS <strong class="lu ir">=</strong> tfv<strong class="lu ir">.</strong>get_feature_names()<br/><strong class="lu ir">def</strong> model(PARAS <strong class="lu ir">=</strong> PARAS, tf <strong class="lu ir">=</strong> tf, TERMS <strong class="lu ir">=</strong> TERMS):<br/>    lda <strong class="lu ir">=</strong> LDA(n_components<strong class="lu ir">=</strong>25, max_iter<strong class="lu ir">=</strong>10, learning_offset<strong class="lu ir">=</strong>50)<br/>    THETA <strong class="lu ir">=</strong> pd<strong class="lu ir">.</strong>DataFrame(lda<strong class="lu ir">.</strong>fit_transform(tf), index<strong class="lu ir">=</strong>PARAS<strong class="lu ir">.</strong>index)<br/>    THETA<strong class="lu ir">.</strong>columns<strong class="lu ir">.</strong>name <strong class="lu ir">=</strong> 'topic_id'<br/>    <br/>    <em class="me">#PHI</em><br/>    PHI <strong class="lu ir">=</strong> pd<strong class="lu ir">.</strong>DataFrame(lda<strong class="lu ir">.</strong>components_, columns<strong class="lu ir">=</strong>TERMS)<br/>    PHI<strong class="lu ir">.</strong>index<strong class="lu ir">.</strong>name <strong class="lu ir">=</strong> 'topic_id'<br/>    PHI<strong class="lu ir">.</strong>columns<strong class="lu ir">.</strong>name  <strong class="lu ir">=</strong> 'term_str'<br/>    <br/>    <strong class="lu ir">return</strong> THETA, PHI</span></pre><p id="0b58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个代码块中，我们从使用sklearn模块中的CountVectorizer函数开始，它遍历我们新创建的表“PARAS”，为前4000个术语的计数创建一个稀疏矩阵。可以在传递给CountVectorizer函数的参数“max_feature”中调整项数。我们最终将使用潜在的狄利克雷分配(LDA)模型来创建我们的主题。LDA模型通常用于主题建模，并且在我们的上下文中通过基于所使用的术语识别文档语料库中的主题，然后将这些文档分配给最合适的主题来工作。</p><p id="1353" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的LDA函数可以在sklearn库中找到。我们传递的第一个参数“n_components”设置我们选择创建的主题数量，在本例中是25个。第二项“max_iter”指的是在训练数据上的迭代次数，而“learning_offset”用于避免由训练数据中的早期学习引起的错误。</p><p id="f946" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为LDA函数的结果，我们得到了θ和φ的输出。Theta矩阵将用于确定给定文档中主题的分布，而Phi将用于表示特定主题中单词的分布。使用下面的代码块，我们可以做到这一点，这意味着我们可以识别组成每个主题的单词，以及每个集群中最流行的主题。</p><pre class="ld le lf lg gt lt lu lv lw aw lx bi"><span id="96c3" class="ly lz iq lu b gy ma mb l mc md"><em class="me"># Create topics</em><br/>TOPICS <strong class="lu ir">=</strong> PHI<strong class="lu ir">.</strong>stack()<strong class="lu ir">.</strong>to_frame()<strong class="lu ir">.</strong>rename(columns<strong class="lu ir">=</strong>{0:'weight'})\<br/>    <strong class="lu ir">.</strong>groupby('topic_id')\<br/>    <strong class="lu ir">.</strong>apply(<strong class="lu ir">lambda</strong> x: <br/>           x<strong class="lu ir">.</strong>weight<strong class="lu ir">.</strong>sort_values(ascending<strong class="lu ir">=False</strong>)\<br/>               <strong class="lu ir">.</strong>head(10)\<br/>               <strong class="lu ir">.</strong>reset_index()\<br/>               <strong class="lu ir">.</strong>drop('topic_id',1)\<br/>               <strong class="lu ir">.</strong>term_str)</span><span id="dfd6" class="ly lz iq lu b gy mg mb l mc md"><em class="me"># Add label column to TOPICS table</em><br/>TOPICS['label'] <strong class="lu ir">=</strong> TOPICS<strong class="lu ir">.</strong>apply(<strong class="lu ir">lambda</strong> x: str(x<strong class="lu ir">.</strong>name) <strong class="lu ir">+</strong> ' ' <strong class="lu ir">+</strong> ' '<strong class="lu ir">.</strong>join(x), 1)<br/><br/><em class="me"># Set topics by Doc weight</em><br/>TOPICS['doc_weight_sum'] <strong class="lu ir">=</strong> THETA<strong class="lu ir">.</strong>sum()</span><span id="5e42" class="ly lz iq lu b gy mg mb l mc md"><em class="me"># Topics by book cluster given top 25 topics</em><br/>topic_cols <strong class="lu ir">=</strong> [t <strong class="lu ir">for</strong> t <strong class="lu ir">in</strong> range(25)]<br/>CLUSTERS <strong class="lu ir">=</strong> THETA<strong class="lu ir">.</strong>groupby('clusters')[topic_cols]<strong class="lu ir">.</strong>mean()<strong class="lu ir">.</strong>T                                            <br/>CLUSTERS<strong class="lu ir">.</strong>index<strong class="lu ir">.</strong>name <strong class="lu ir">=</strong> 'topic_id'<br/>CLUSTERS<strong class="lu ir">.</strong>T</span><span id="ec8d" class="ly lz iq lu b gy mg mb l mc md">CLUSTERS['topterms'] <strong class="lu ir">=</strong> TOPICS[[i <strong class="lu ir">for</strong> i <strong class="lu ir">in</strong> range(10)]]<strong class="lu ir">.</strong>apply(<strong class="lu ir">lambda</strong> x: ' '<strong class="lu ir">.</strong>join(x), 1)</span></pre><p id="cf54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以对三个集群中的每一个进行排序，以确定哪些主题是最受欢迎的，以及组成这些主题的单词。如下面可以看到的，每个聚类用不同的主题和相关术语集合来最大程度地标识。如前所述，大多数话题都属于政治思想的范畴，这在我们的领域中并不奇怪，但在这些话题中，我们可以看到一些不同的词汇选择。例如，在共产主义集群的主题17中，我们可以看到，这个主题通常是关于劳动者和/或劳动条件的，根据我们对这些作品的了解，这也是有意义的，而美国集群中最普遍的主题似乎反映了我们的共和制度的基础词汇。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mh"><img src="../Images/9568aedc27ef4833c409561098e2ce63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPJ1WreGKrXvsTR2Y_Z-ug.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mi"><img src="../Images/1cac1b3fe7563b8ba390cc36282b8c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eXMrq087i2We4euZlGc8A.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mj"><img src="../Images/d49603d79eb16971b1544a9fbf14275d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gqkz_Szf9RGPJATqvnLU5g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片</p></figure><p id="3122" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论:</strong> <br/>如上图所示，我们可以使用主题建模来发现哪些主题在我们的文本聚类中最普遍。主题建模可以是一个强大的工具，用于区分不同的文本数据集，同时识别与数据相关的主要主题和单词。然而，主题建模也有一些缺点，例如潜在的需要专家给每个主题ID一个整体的身份，以及主题中单词的重复性质，尤其是那些彼此密切相关的主题。这里获得的结果是基于我们为LDA模型选择的特定参数。我鼓励您试验这些参数，看看调优如何改变您的结果。这篇文章是LDA和主题建模的基本演示，应该如此对待，而不是模型准确性/成功的决定性证明。一如既往地感谢你花时间阅读这篇文章，如果你想阅读更多，请关注我的频道！</p><p id="54d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这个项目的完整代码可以在我的GitHub页面上找到:</strong><a class="ae lb" href="https://github.com/nbehe/NLP_texts/blob/main/Beheshti_project_code.ipynb" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/nbehe/NLP _ texts/blob/main/Beheshti _ project _ code . ipynb</a></p><p id="02f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*这个项目是我的文本分析课程的一个大项目的一小部分。来自这个项目的代码是我自己写的，由讲师提供的，以及通过课程材料提供的作品的混合物。*</p></div></div>    
</body>
</html>