<html>
<head>
<title>How to Find Weaknesses in Your Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在你的机器学习模型中找到弱点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-find-weaknesses-in-your-machine-learning-models-3bcce3c7d71e#2022-01-19">https://towardsdatascience.com/how-to-find-weaknesses-in-your-machine-learning-models-3bcce3c7d71e#2022-01-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="ea06" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">如何在你的机器学习模型中找到弱点</h1></div><div class=""><h2 id="eb34" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">IBM的FreaAI的一种可能实现</h2></div><p id="98e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">任何时候使用汇总统计简化数据，都会丢失信息。模型精度也不例外。当将模型简化为汇总统计数据时，您将无法确定哪里的性能最低/最高以及原因。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/b30998dcf13da57247e386b36d2ec1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6eH3fwxdpTxyubTBwgawQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图1:使用IBM FreaAI的POC训练的最低精度决策树。图片作者。</p></figure><p id="f69d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们讨论了IBM的FreaAI 背后的代码，这是一种识别低精度数据切片的有效方法。在以前的帖子中，我们在高层次上讨论了<a class="ae lr" rel="noopener" target="_blank" href="/how-to-find-weaknesses-in-your-machine-learning-models-ae8bd18880a3">方法</a>，并深入利用<a class="ae lr" rel="noopener" target="_blank" href="/highest-prior-density-estimation-for-diagnosing-black-box-performance-c16447a96b7"> HPD来找到模型弱点的区域</a>。在这里，我们将通过一个二进制分类器论文的MVP实现。</p><p id="d8c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，我们开始吧。</p><h1 id="7c7d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">0 —技术概述</h1><p id="87fd" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">FreaAI是确定表现不佳的数据区域的有效方法。它通过利用<a class="ae lr" rel="noopener" target="_blank" href="/highest-prior-density-estimation-for-diagnosing-black-box-performance-c16447a96b7">最高先验密度(HPD)方法</a>和决策树来搜索我们的特征空间，并找到精确度最低的地方。从那里，可解释的数据切片被返回给工程师。</p><p id="c674" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">FreaAI提出了一个解决方案来模拟可解释性、偏差和概化。</p><h1 id="a74f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1 —设置</h1><p id="1dfb" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在第1部分，我们将快速回顾一下我们在之前的<a class="ae lr" rel="noopener" target="_blank" href="/highest-prior-density-estimation-for-diagnosing-black-box-performance-c16447a96b7">帖子</a>中所涵盖的内容。</p><h2 id="7628" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">1.1 —获取数据</h2><p id="4709" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">首先，我们需要在这里下载我们的分类数据<a class="ae lr" href="https://github.com/mberk06/DS_academic_papers/tree/master/DiabetesData" rel="noopener ugc nofollow" target="_blank">。这是</a><a class="ae lr" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> CC0授权</a>的，属于公有领域。数据如下图2所示…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/0fccfa9e741fe55fb8eb2c5504573627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DqSuw7yE9h0hlZ_W.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图2:数据集的最后4列— <a class="ae lr" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><pre class="lc ld le lf gt nc nd ne nf aw ng bi"><span id="a1b4" class="mp lt iq nd b gy nh ni l nj nk">import pandas as pd<br/>df = pd.read_csv('DiabetesData/pima-indians-diabetes.data.csv', header=None)</span></pre><p id="4d8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还将创建一些虚拟分类列，它们应该会产生非常嘈杂的结果…</p><pre class="lc ld le lf gt nc nd ne nf aw ng bi"><span id="5aeb" class="mp lt iq nd b gy nh ni l nj nk"># add some more columns for fun (one hot encoded categorical)<br/>np.random.seed(0)<br/>enc_df = pd.DataFrame(dict(<br/>        color=['red' if x &gt; 0.25 else 'green' for x in np.random.rand(len(df.index))],<br/>        gender=['male' if x &gt; 0.55 else 'female' for x in np.random.rand(len(df.index))]<br/>))</span><span id="dea8" class="mp lt iq nd b gy nl ni l nj nk">enc = OneHotEncoder(handle_unknown='ignore')<br/>enc_df = pd.DataFrame(enc.fit_transform(enc_df[['color','gender']]).toarray())</span><span id="aee0" class="mp lt iq nd b gy nl ni l nj nk">df = pd.concat([df, enc_df], ignore_index=True, axis=1)</span><span id="c51a" class="mp lt iq nd b gy nl ni l nj nk">df.columns = ['num_pregnancies','glucose','blood_pressure','skin_thickness','insulin','bmi','diabetes_pedigree','age','outcome', 'color_red','color_green','gender_male','gender_female']</span></pre><h2 id="f5e4" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">1.2 —训练一个黑盒模型</h2><p id="4c2c" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">接下来，我们训练一个XGBoost模型…</p><pre class="lc ld le lf gt nc nd ne nf aw ng bi"><span id="b911" class="mp lt iq nd b gy nh ni l nj nk"># split data into X and y<br/>mask = np.array(list(df)) == 'outcome'<br/>X = df.loc[:,~mask]<br/>Y = df.loc[:,mask]</span><span id="0a82" class="mp lt iq nd b gy nl ni l nj nk"># split data into train and test sets<br/>seed = 7<br/>test_size = 0.33<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>                                     X, Y,<br/>                                     test_size=test_size, <br/>                                     random_state=seed)</span><span id="f245" class="mp lt iq nd b gy nl ni l nj nk"># fit model no training data<br/>model = XGBClassifier()<br/>model.fit(X_train, y_train)</span><span id="583d" class="mp lt iq nd b gy nl ni l nj nk"># make predictions for test data<br/>y_pred = model.predict(X_test)<br/>predictions = [round(value) for value in y_pred]</span><span id="1665" class="mp lt iq nd b gy nl ni l nj nk"># evaluate predictions<br/>accuracy = accuracy_score(y_test, predictions)<br/>print("Accuracy: %.2f%%" % (accuracy * 100.0))# add accuracy and return<br/>out = pd.concat([X_test, y_test], axis=1, ignore_index=True)<br/>out.columns = list(df)accuracy_bool = (np.array(y_test).flatten() ==<br/>                 np.array(predictions))</span><span id="f9f5" class="mp lt iq nd b gy nl ni l nj nk">out['accuracy_bool'] = accuracy_bool<br/>out</span></pre><p id="6514" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用上述配置，该模型实现了约71%的准确性。为了使这种准确性具有可操作性，我们还创建了一个列<code class="fe nm nn no nd b">accuracy_bool</code>，如果标签预测正确，它就是<code class="fe nm nn no nd b">True</code>的布尔值，否则就是<code class="fe nm nn no nd b">False</code>。</p><p id="ede3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了，我们都赶上了。</p><h1 id="5ae0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2-训练可解释的决策树</h1><p id="619a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">好了，让我们慢一点，真正理解我们代码中的决策树(DT)部分。与最高先验密度方法一样，我们希望找出数据中表现出异常低准确性的区域。因此，我们训练了一个决策树，其中我们的…</p><ul class=""><li id="f067" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated"><strong class="kh ir">特性</strong>是XGBoost模型中使用的一个或两个特性。</li><li id="3956" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated"><strong class="kh ir">目标变量</strong>是上面创建的<code class="fe nm nn no nd b">accuracy_bool</code>列。</li></ul><p id="d806" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们限制一个或两个特性的原因是为了确保数据切片是可解释的，从而是可用的。让我们看一个例子…</p><p id="4429" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们创建一个简单的助手来训练我们的决策树。</p><pre class="lc ld le lf gt nc nd ne nf aw ng bi"><span id="6e7b" class="mp lt iq nd b gy nh ni l nj nk">def fit_DT(df, predictors = ['age']):<br/>    """ Fit binary classifier"""</span><span id="c6bf" class="mp lt iq nd b gy nl ni l nj nk">    X = df[predictors] <br/>    y = df['accuracy_bool']</span><span id="8598" class="mp lt iq nd b gy nl ni l nj nk">    model = DecisionTreeClassifier(max_depth=3, <br/>                                   criterion='entropy',<br/>                                   random_state=1)<br/>    model.fit(X, y)</span><span id="cb9d" class="mp lt iq nd b gy nl ni l nj nk">    preds = model.predict(X)<br/>    acc = accuracy_score(y, preds)</span><span id="eded" class="mp lt iq nd b gy nl ni l nj nk">    return model, preds, acc, X</span></pre><p id="0740" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这个助手，我们能够将感兴趣的特性传递给<code class="fe nm nn no nd b">predictors</code>参数，并(希望)得到低精度的分割。</p><h2 id="a015" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">2.1 —单变量决策树</h2><p id="ff00" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">作为一个例子，让我们使用数字列<code class="fe nm nn no nd b">age</code>，然后使用sklearn中的<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html" rel="noopener ugc nofollow" target="_blank"> export_graphviz </a>函数将其可视化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi od"><img src="../Images/d7f8c22bce90e3b0ab2328f47feba6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qo2h32s5zR3aLOyYYLFNuw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3:根据年龄训练的决策树。图片作者。</p></figure><p id="f9cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图3中，每个节点(蓝框)对应一个数据片。叶子最有可能有“小”的<code class="fe nm nn no nd b">sample</code>大小和最极端的<code class="fe nm nn no nd b">entropy</code>，所以我们关注它们。更具体地说，我们在寻找…</p><ol class=""><li id="c355" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la oe nv nw nx bi translated">具有低熵(高节点纯度)</li><li id="07df" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la oe nv nw nx bi translated">有足够的样本量是有用的</li><li id="fa83" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la oe nv nw nx bi translated">相对于我们71%的基线显示出较低的准确性</li></ol><p id="8b4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贯穿以上逻辑，我们可以看到节点3、4、12有熵&lt; 0.3. Node 12 we can immediately throw out because it has 2 samples, making it pretty useless. Nodes 3 and 4 have sufficient sample size, however their accuracy is 94.7% and 95.8% respectively, indicating they <strong class="kh ir">突出模型优势而不是弱势。</strong></p><p id="071f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许我们选择了一个非常健壮的列，所以让我们继续看另一个例子。</p><h2 id="65a6" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">2.2 —二元决策树</h2><p id="e150" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这里，我们将利用决策树处理多个特性的能力，这有助于我们理解<strong class="kh ir">特性交互</strong>。在这个例子中，感兴趣的特征是<code class="fe nm nn no nd b">diabetes_pedigree</code>和<code class="fe nm nn no nd b">blood_pressure</code>。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi of"><img src="../Images/889bfea352ceb068983ee3923330b9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vp9gVp-cXJTgb-77Xy67w.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4:在血压和葡萄糖上训练的决策树。图片作者。</p></figure><p id="eb6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用sklearn的graph_viz库中的<code class="fe nm nn no nd b">filled=True</code>参数，我们可以看到，在图4中，橙色节点在第一个索引中的<code class="fe nm nn no nd b">values</code>比第二个索引中的多，这表明我们的错误分类比分类多。</p><p id="fd3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，通过上述标准，我们正在寻找具有足够样本量和低精度的纯叶。节点10、11和12清楚地显示了低准确度的区域，但是它们具有相对较低的样本大小。使用<code class="fe nm nn no nd b">values[1] / sum(values)</code>计算的节点9的准确率为55%,比我们的基线71%低16个百分点。应该进一步探索这些节点。</p><p id="3f1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在深入之前，让我们停下来欣赏一下这个决策树中模型/特性的可解释性。</p><ul class=""><li id="e79f" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated">对于<code class="fe nm nn no nd b">glucose</code> ≤ 104.5的人，我们展示了88%的准确率，如我们的根节点所示。如果<code class="fe nm nn no nd b">glucose</code> ≤ 87.5，我们展示100%的准确性，如节点2所示。<strong class="kh ir">葡萄糖在其分布的左侧是一个极其有用的特征。</strong></li><li id="3030" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">对于<code class="fe nm nn no nd b">glucose</code> &gt; 104.5、<code class="fe nm nn no nd b">blood_pressure</code> &gt; 91.0的人，我们的准确率下降到18%。<strong class="kh ir">无论是高</strong> <code class="fe nm nn no nd b"><strong class="kh ir">blood_pressure</strong></code> <strong class="kh ir">还是</strong> <code class="fe nm nn no nd b"><strong class="kh ir">glucose</strong></code> <strong class="kh ir">都是我们数据的噪音区。</strong></li></ul><p id="e41e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我不是糖尿病专家，但理想情况下，如果你正在做这样的项目，你(或你的队友)可以评估这些结论，看看它们是否有意义。如果没有，可能是数据有问题。</p><p id="e613" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，尽管我们缺乏糖尿病知识，但我们可以通过利用来自<a class="ae lr" href="https://github.com/parrt/dtreeviz/blob/master/testing/gen_samples.py" rel="noopener ugc nofollow" target="_blank"> dtreeviz </a>库的可视化来继续分析…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi og"><img src="../Images/0d6c558b46b36bf88a3569a505969e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_x1-Muax6Wbj-l5oTZDLhA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图5:数据绘图决策树适合血压和葡萄糖。图片作者。</p></figure><p id="dcc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图5中，我们有与图4完全相同的决策树，但是这次我们在每个节点/叶绘制数据分布。</p><p id="1267" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很快，我们就能发现一些异常。对于我们最右边的决策路径，我们可以看到我们最右边的叶子对应于葡萄糖非常高的5个错误分类，这与先前树的结论一致。此外，我们在最左边的叶子上获得了一张非常好的高精度区域的图片——这只是根分布的一部分，其中<code class="fe nm nn no nd b">glucose</code> ≤ 87.5。</p><p id="a5a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！现在我们已经讨论了关键的可视化，让我们讨论这个二元特征空间的下一步。我们的<a class="ae lr" href="https://github.com/parrt/dtreeviz/blob/master/testing/gen_samples.py" rel="noopener ugc nofollow" target="_blank">代码</a>返回我们有问题的数据片的索引。</p><p id="66db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，对于节点11，这6个数据点的行索引是:<em class="oh">【58，87，105，108，222，230】</em>。从那里，好的老式EDA将有望产生更多的见解。</p><p id="ba61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简洁起见，我们不会涉及这些步骤，但希望您能有所了解。</p><h1 id="fc89" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3-识别精确度差的切片</h1><p id="bbc7" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在，我们已经走过了一个非常具体的例子，让我们把所有的东西放在一个元分析我们的模型。</p><p id="828e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本节中，我们将为每个变量以及变量的所有组合创建一个可视化的树。然而，为了减少查看每个可视化的手动负担，我们将精度输出到一个可排序的数据框中。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oi"><img src="../Images/d8b01f27b65a8725459f5bc3dc210aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srhNi7JJ5Tl3-qCfUjwHAA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图6:我们的元分析的输出。图片作者。</p></figure><p id="1060" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图6所示，我们的输出包括4列:</p><ol class=""><li id="a8ea" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la oe nv nw nx bi translated"><code class="fe nm nn no nd b">names</code>:HPD的特征名称或百分位数</li><li id="876a" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la oe nv nw nx bi translated"><code class="fe nm nn no nd b">indices</code>:该数据切片的行索引</li><li id="fb59" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la oe nv nw nx bi translated"><code class="fe nm nn no nd b">accuracies</code>:数据切片的准确性</li><li id="99b7" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la oe nv nw nx bi translated"><code class="fe nm nn no nd b">method</code>:我们是否使用了决策树(DT)或最高先验密度(HPD)</li></ol><p id="0359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，你会注意到最低精度的分割来自二元决策树。我们创建了一些虚拟列(<code class="fe nm nn no nd b">gender</code>和<code class="fe nm nn no nd b">color</code>)来测试分类变量拟合，正如所料，它们显示了许多低精度区域。</p><p id="8824" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者第一个“真正的”有问题的交互来自于<code class="fe nm nn no nd b">blood_pressure</code>和<code class="fe nm nn no nd b">num_pregnancies</code>。如果我们有兴趣深入研究，我们可以研究数据框中返回的索引，看看决策树认为哪些行有问题。</p><p id="f9bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以看到，HPD的当前实现对于确定低精度切片不是非常有效——最弱的切片表现出66%的精度。虽然HPD超级有效，但被训练来根据节点纯度进行分割的决策树会发现较弱的切片是有道理的。</p><p id="9a8a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，这只是一个拙劣地拼凑成POC的虚拟示例。这个基线可以得到显著的改进，IBM的FreaAI实现无疑有更多的功能和优化。随意使用<a class="ae lr" href="https://github.com/mberk06/DS_academic_papers/blob/master/32_freaai_potential_implementation.py" rel="noopener ugc nofollow" target="_blank">这个代码</a>作为起点:)</p><h1 id="354a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4 —总结和后续步骤</h1><p id="40f8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">概括地说，在这篇文章中，我们介绍了IBM的FreaAI的一个潜在实现。该方法在黑盒模型性能较低的地方找到可解释的数据片段。然后将这些返回给工程师进行纠正。</p><p id="f191" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个实现对于我们的玩具例子是有效的，但是还有许多可能的改进，这里列出了其中的一些…</p><ul class=""><li id="a0bf" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated">支持多类分类</li><li id="3605" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">支持回归</li><li id="b780" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">数据切片上的统计显著性计算(带FDR校正)</li><li id="641a" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">模块化成类</li></ul></div><div class="ab cl oj ok hu ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="ij ik il im in"><p id="8b98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="oh">感谢阅读！我会再写19篇文章，把学术研究带到DS行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>