# 计算机视觉和黑色素，DEI 案例研究

> 原文：<https://towardsdatascience.com/computer-vision-and-melanin-a-dei-case-study-df4a6f304a01>

## 为了纪念黑人历史月，我想写关于机器学习的进步和错误，特别是计算机视觉，以及拥有一个大而多样的数据集的必要性

![](img/24325f4102ed2d24e514dae0842b156f.png)

**照片由** [**奥拉迪梅吉**](https://unsplash.com/@oladimeg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) **上** [**下**](https://unsplash.com/s/photos/black-history?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

我这个月开始以兼职教授的身份教书，这让我非常兴奋。由于我教授数据科学 101，我希望我的学生在他们早期的数据科学之旅中带走的一点是建立道德模型的责任，以及即使是大公司也没有做好这一点，以及成为努力做好这一点的工人阶级的一部分是多么重要。

让我们谈谈解决谷歌等科技巨头多年来缺乏训练数据多样性的许多机器学习算法之一的最新进展。最近，我通过广告听说谷歌推出了智能手机上最具包容性的摄像头。据说谷歌的 Pixel 6 拍摄了数千张照片，使他们的训练数据因其 [Real Tone](https://youtu.be/gGZx0RrL-ow) 技术而变得更加多样化 25 倍。《华尔街日报》的这篇文章[展示了这些对比，确实表明谷歌的 Pixel 6 看起来最清晰，而且未经编辑；它不会像这些照片中的三星 Galaxy S21 那样提亮较暗的皮肤。](https://www.wsj.com/articles/google-built-the-pixel-6-camera-to-better-portray-people-with-darker-skin-tones-does-it-11635177665)

在培训数据方面，他们的产品营销经理在 Pixel 6 发布会上指出，他们的培训数据现在多样化了 25 倍。我不知道他们是否在将其与他们的原始训练数据进行比较。让我们假设这是因为他们提到他们拍了几千张照片，而不是几万张。所以假设他们加了 9000 张图片。这可能意味着最初他们的有色人种训练数据集可能有数百个。他们是否认为深色皮肤是一种二项式分类，他们只需要数百张黑人的照片，而不考虑肤色的多样性？蕾哈娜的化妆品牌 Fenty Beauty 推出时，推出了 40 种不同色调的化妆品，最近又增加了 10 种，这在化妆行业引起了轰动和破坏(以一种好的方式)。自从人们认识到人的肤色可以表现为大范围的色调分布后，他们觉得更多的人看到了她的化妆品品牌。现在很难想象人们的化妆需求最终得到了满足，他们会回到不那么包容的东西上去。

在我们考虑需要多少数据样本来支持这种色调分布之前，让我们先从高层次上了解一下计算机视觉是如何工作的。与我们相似，计算机视觉算法也可以检测模式，只是我们接近模式的方式与计算机发现模式的方式非常不同。计算机对像素进行数学运算。像素是代表图像的最小单位。像素以值的形式保存信息，值的范围从 0 到 255，分别表示黑到白。

![](img/8f6b16b50b0de825362eb5619724370a.png)

[瓦迪姆·博古洛夫](https://unsplash.com/@franku84?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/pixels?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

计算机视觉的一些领域包括:

*   边缘检测检测图像的边缘。[这篇文章](https://aryamansharda.medium.com/how-image-edge-detection-works-b759baac01e2)展示了索贝尔公式背后的数学原理来证明边缘，下面的视频也是如此。

*   对象检测将边界框应用于感兴趣的对象
*   对象分割对属于每一类的所有像素进行分类
*   图像分类将分类标签应用于整个图像。比如这个图像里有猫吗？

下面的视频展示了对象检测与对象分割和图像分类之间的区别。

当你开始学习数据科学时，他们展示的最受欢迎的图像分类数据集之一是 [MNIST 数据集](http://yann.lecun.com/exdb/mnist/)，这是一个手写数字的大型数据集。参考链接还展示了自 1998 年以来的不同算法及其测试错误率。正如您在该链接的表格中看到的，直到我们进入多层神经网络、卷积等不同方法和更大的模型，测试错误率才会下降。这些类是数字 0 到 9 的手写数字。训练数据集是 60，000 个样本，测试数据集是 10，000 个样本。这是每个数字平均 6000 个训练样本。

尽管有人可能会争辩说，技术和计算每年都有更大的进步，更多的训练数据解决所有问题还不够。然而，与推进实际技术相比，这是一个良好的开端，而不是沉重的负担。

回到这个问题，我们需要多少数据样本才能开始使用人脸作为算法的输入？深度神经网络(DNNs)已经证明它们具有低错误率*(当正确使用时)。挑战在于它需要非常大的数据集。萨利赫·沙欣法尔、保罗·米克、格雷格·法尔松写道:“我需要多少张照片？”虽然不是关于人，但这是一项“[了解每类样本量如何影响自主野生动物监测中平衡设计的深度学习模型性能指标”的研究。](https://www.sciencedirect.com/science/article/abs/pii/S1574954120300352)在他们的研究中，你会看到他们的错误率随着数据集规模的增加而下降。

感觉就像一个能够接触到最聪明的人和资源的大型科技巨头花了一分多钟来解决他们的机器学习产品中的偏见。2015 年[当谷歌的技术将黑人的图像分类为大猩猩](https://www.wsj.com/articles/BL-DGB-42522?mod=article_inline)时，2018 年的[“修复”是完全删除大猩猩的标签作为一种预测。技术和数据规模并不是唯一的挑战，还有伦理方面的挑战。我们需要问自己这样一个问题，“这是弊大于利吗？”。就像帕累托原理一样，80%的结果来自 20%的努力。目前，领先的科技巨头对这项技术的应用有着最大的影响力。2018 年 5 月，](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)[亚马逊的 Rekognition 算法错误地将大约 30 名国会议员与面部照片](https://www.aclu.org/blog/privacy-technology/surveillance-technologies/amazons-face-recognition-falsely-matched-28)关联起来，这一臭名昭著的事件引发了人们的关注。在接下来的几年里，从旧金山开始，该市禁止使用面部识别软件，这将限制执法机构利用它进行监控。其他城市在以下领域效仿；圣地亚哥、波特兰、波士顿、马萨诸塞州，以及 2021 年 7 月的弗吉尼亚州。

我想到的最好的研究例子之一是 Joy Buloamwini 博士在麻省理工学院的论文陈述*性别差异*，它放大了这些科技巨头的面部识别软件的差距，尤其是在有色人种女性中。她指出的问题还是缺乏训练数据。她发现，当将 IBM、微软和亚马逊的分类器应用于一个更加平衡和多样化的人物图像数据集时，她基于来自世界各地 3 个非洲国家和 3 个欧洲国家的 1270 名政治家构建了这个数据集。在她的论文中，她举例展示了来自试点议会基准(PPB)数据集的图像样本，其中 44%为女性，47%为深色皮肤。相比之下，在她的论文中，她发现用于训练这家科技巨头的性别分类器的数据集不成比例地大多是浅色皮肤，最小的一组深色皮肤女性约占数据集的 4%。

当她将性别分类器应用于 PPB 数据集时，她发现最佳分类器的错误率在肤色较深的女性中比肤色较浅的女性高 32 倍。

就像我们数据科学家如何通过查看各种分类分数来执行模型选择一样，我们也必须在多维空间中查看它们，尤其是在处理人口统计数据时。在她的论文中，你会发现 Buloamwini 博士提出的记分卡的一个例子，以促进未来面部识别数据集中的多样性。

面部识别在执法部门以外也有使用案例，如生物信息学，如苹果的 Face ID。还有拉纳·埃尔·卡利欧比博士的作品，我喜欢读她的新书《T2 女孩解码》。她在麻省理工学院时是人工智能情感领域的先驱。最初，她的想法是用情商来增强技术，以帮助被诊断患有自闭症的人进行社交互动。患有自闭症的人在社交场合很难区分情绪，2006 年，Rana 和麻省理工学院媒体实验室一起创造了智能眼镜，帮助自闭症患者识别他们正在交往的人的情绪。在她的书中，她谈到了作为妻子、母亲、计算机科学家、她的公司、研究和文化的生活。在一次事件中，她一直在全力处理这件事，她遭遇了一场车祸，她写道，如果她的车能在开车前判断她的情绪状态，这可能会有助于防止车祸的发生，

因此，我留给读者的问题是:除了增加训练数据，我们如何确保我们生活中使用的算法是合乎道德和负责任的？即使这项技术是可行的，我们还应该利用它吗？使用或不使用它的含义是什么？