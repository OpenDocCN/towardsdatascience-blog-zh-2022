<html>
<head>
<title>Ensemble Learning in sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">sklearn中的集成学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ensemble-learning-in-sklearn-587f21246e8d#2022-01-04">https://towardsdatascience.com/ensemble-learning-in-sklearn-587f21246e8d#2022-01-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="f29e" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">sklearn中的集成学习</h1></div><div class=""><h2 id="d4a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何在sklearn中与pipelines和GridSearchCV一起执行集成学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dad2aa1575811a51631794941810f5c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XjxLtFoQM1usHG2S"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尼克·费因斯</a>拍摄</p></figure><p id="9cc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我之前的文章中，我谈到了在sklearn中使用管道:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/using-sklearn-pipelines-to-streamline-your-machine-learning-process-a27721fdff1b"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">使用Sklearn管道简化您的机器学习过程</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">了解Pipeline类如何简化和自动化您的机器学习工作流</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="396c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那篇文章中，您学习了如何在sklearn中使用管道来简化您的机器学习工作流。您还了解了如何使用<code class="fe mn mo mp mq b">GridSearchCV()</code>和管道来为您的数据集寻找最佳估计器。</p><p id="5468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果能够利用多个机器学习模型(估计器)对数据集进行单独预测，而不是仅仅依赖于一个估计器，这将非常有用。然后，可以将这些结果结合起来进行最终预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d09c441f2eaf145a3bddbc084383a6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*HqyXbCdCvihajqi7vpf7Fg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是所谓的<em class="ms">合奏学习</em>。总的来说，集成学习模型的性能通常比使用单个估计器要好。</p><p id="e10e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在本文中，我将向您展示如何在sklearn中使用集成学习来为您的数据做出更好的预测。对于代码的前几个部分，我将使用我在上一篇文章中创建的管道。因此，如果您不熟悉管道，请务必在继续之前先阅读这篇文章。</p><h1 id="e86a" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">加载数据</h1><p id="0a0a" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">对于本文，我们将使用流行的泰坦尼克号数据集，我们将从ka ggle(【https://www.kaggle.com/c/titanic/data?select=train.csv】)下载:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="56ff" class="nu mu it mq b gy nv nw l nx ny">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split</span><span id="98c1" class="nu mu it mq b gy nz nw l nx ny">df = pd.read_csv('train.csv')<br/>df = df[['Survived','Pclass','Sex','Age','Fare','Embarked']]</span><span id="1a12" class="nu mu it mq b gy nz nw l nx ny">X = df.iloc[:,1:]<br/>y = df.iloc[:,0]</span><span id="2b52" class="nu mu it mq b gy nz nw l nx ny">X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                                  test_size = 0.3, <br/>                                                  stratify = y, <br/>                                                  random_state = 0)</span></pre><blockquote class="oa ob oc"><p id="32f9" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">数据来源</em> </strong> <em class="it">:本文数据来源于</em><a class="ae ky" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"><em class="it">https://www.kaggle.com/c/titanic/data.</em></a></p></blockquote><h1 id="ae18" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">定义管道</h1><p id="2f85" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">接下来的两节将定义转换数据集中的列的管道。</p><h2 id="59df" class="nu mu it bd mv og oh dn mz oi oj dp nd li ok ol nf lm om on nh lq oo op nj oq bi translated"><strong class="ak">定义变压器</strong></h2><p id="f9a7" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我们将首先为不同的列定义不同的转换器:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="3582" class="nu mu it mq b gy nv nw l nx ny">from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.utils.validation import check_is_fitted<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.impute import SimpleImputer<br/>from sklearn.preprocessing import StandardScaler, OneHotEncoder</span><span id="ce08" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># custom transformer to select specific columns</strong><br/>class FeatureSelector(BaseEstimator, TransformerMixin):<br/>    def __init__(self, feature_names):<br/>        self._feature_names = feature_names <br/>    <br/>    def fit(self, X, y = None):<br/>        return self</span><span id="fee5" class="nu mu it mq b gy nz nw l nx ny">    def transform(self, X, y = None):<br/>        return X[self._feature_names]</span><span id="b2e7" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># define the transformer for numeric columns<br/># for 'Age' and 'Fare'<br/></strong>numeric_transformer = Pipeline(steps=[<br/>    ('imputer', SimpleImputer(strategy='median')),<br/>    ('scaler', StandardScaler())<br/>])</span><span id="b085" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># define the transformer for categorical columns<br/># for 'Sex' and 'Embarked'<br/></strong>categorical_transformer1 = Pipeline(steps=[<br/>    ('imputer', SimpleImputer(strategy='most_frequent')),<br/>    ('onehot', OneHotEncoder(handle_unknown='ignore'))<br/>])</span><span id="9666" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># define the transformer for categorical columns<br/># for 'Pclass'<br/></strong>categorical_transformer2 = Pipeline(steps=[<br/>    ('imputer', SimpleImputer(strategy='most_frequent'))<br/>])</span></pre><p id="fdd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您不熟悉自定义变压器，请参考本文:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/creating-custom-transformers-for-sklearn-pipelines-d3d51852ecc1"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">为sklearn管道创建自定义转换器</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">了解如何创建适合并转换数据的自定义转换器</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="or l mj mk ml mh mm ks ly"/></div></div></a></div><h2 id="d81c" class="nu mu it bd mv og oh dn mz oi oj dp nd li ok ol nf lm om on nh lq oo op nj oq bi translated">转换列</h2><p id="54a0" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">接下来我们将使用<code class="fe mn mo mp mq b">ColumnTransformer</code>类来转换所有需要的列:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="5d57" class="nu mu it mq b gy nv nw l nx ny">from sklearn.compose import ColumnTransformer</span><span id="47a7" class="nu mu it mq b gy nz nw l nx ny">features_preprocessor = ColumnTransformer(<br/>    transformers=[<br/>        ('numeric',     numeric_transformer,       ['Age','Fare']),<br/>        ('categorical1', categorical_transformer1, ['Sex',<br/>                                                    'Embarked']),<br/>        ('categorical2', categorical_transformer2, ['Pclass'])<br/>    ], remainder='passthrough')</span></pre><h1 id="4e52" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">使用GridSearchCV为每个分类器寻找最佳估计量</h1><p id="ec31" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">现在我们已经定义了转换列的管道，是时候为我们的预测尝试不同的算法了。让我们一步一步来。</p><p id="c49d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，导入所需的模块:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="3ed1" class="nu mu it mq b gy nv nw l nx ny">from sklearn.model_selection import GridSearchCV<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn import svm</span><span id="6346" class="nu mu it mq b gy nz nw l nx ny">import warnings<br/>warnings.filterwarnings('ignore')</span></pre><p id="da4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，创建一个列表来存储要使用的算法(分类器)列表:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="d3b7" class="nu mu it mq b gy nv nw l nx ny"><strong class="mq iu"># the list of classifiers to use</strong><br/># use random_state for reproducibility<br/>classifiers = [<br/>    LogisticRegression(random_state=0), <br/>    KNeighborsClassifier(), <br/>    RandomForestClassifier(random_state=0)<br/>]</span></pre><blockquote class="oa ob oc"><p id="10f2" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">为了再现性，我将第一个和最后一个分类器的random_state设置为0。</p></blockquote><p id="188b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本例，我们将使用:</p><ul class=""><li id="88b2" class="os ot it lb b lc ld lf lg li ou lm ov lq ow lu ox oy oz pa bi translated">逻辑回归</li><li id="e401" class="os ot it lb b lc pb lf pc li pd lm pe lq pf lu ox oy oz pa bi translated">k-最近邻(KNN)</li><li id="b51d" class="os ot it lb b lc pb lf pc li pd lm pe lq pf lu ox oy oz pa bi translated">随机森林</li></ul><p id="cb1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们为每个分类器指定我们想要调整的各种超参数:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="8906" class="nu mu it mq b gy nv nw l nx ny"><strong class="mq iu"># parameter grids for the various classifiers</strong><br/>logregress_parameters = {<br/>    '<strong class="mq iu">classifier__</strong>penalty' : ['l1','l2'],     <br/>    '<strong class="mq iu">classifier__</strong>C'       : np.logspace(-3,3,7),<br/>    '<strong class="mq iu">classifier__</strong>solver'  : ['newton-cg', 'lbfgs', 'liblinear'],<br/>}</span><span id="bc68" class="nu mu it mq b gy nz nw l nx ny">knn_parameters = {<br/>    '<strong class="mq iu">classifier__</strong>n_neighbors': np.arange(1, 25, 2)<br/>}</span><span id="c71f" class="nu mu it mq b gy nz nw l nx ny">randomforest_parameters = {<br/>    '<strong class="mq iu">classifier__</strong>n_estimators': [50, 100, 200, 300]<br/>}</span></pre><blockquote class="oa ob oc"><p id="4570" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">注意每个超参数的前缀<code class="fe mn mo mp mq b">classifier__</code>。这个前缀与您稍后将在管道中使用的分类器的名称相关联。</p></blockquote><p id="2609" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，将所有这些超参数存储在一个列表中:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="40ed" class="nu mu it mq b gy nv nw l nx ny"><strong class="mq iu"># stores all the parameters in a list </strong><br/>parameters = [<br/>    logregress_parameters, <br/>    knn_parameters, <br/>    randomforest_parameters<br/>]</span></pre><p id="8b20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建一个名为<code class="fe mn mo mp mq b">estimators</code>的列表，它将存储所有调优的估计器。该列表将包含以下格式的元组集合:(<em class="ms"> name_of_classifier </em>，<em class="ms"> tuned_estimator </em></p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="303d" class="nu mu it mq b gy nv nw l nx ny">estimators = []</span></pre><p id="4f2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，创建一个循环来调整每个分类器。对于每个优化的分类器，打印出优化的超参数及其精度。此外，将分类器和调优的估计器的名称添加到<code class="fe mn mo mp mq b">estimators</code>列表中:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="5f1b" class="nu mu it mq b gy nv nw l nx ny"><strong class="mq iu"># iterate through each classifier and use GridSearchCV</strong><br/>for i, classifier in enumerate(classifiers):</span><span id="2183" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu">    # create a Pipeline object</strong><br/>    pipe = Pipeline(steps=[<br/>        ('preprocessor', features_preprocessor),<br/>        ('classifier', classifier)<br/>    ])</span><span id="9ace" class="nu mu it mq b gy nz nw l nx ny">    clf = GridSearchCV(pipe,              # model<br/>              param_grid = parameters[i], # hyperparameters<br/>              scoring='accuracy',         # metric for scoring<br/>              cv=10)                      # number of folds</span><span id="166e" class="nu mu it mq b gy nz nw l nx ny">    clf.fit(X, y)<br/>    print("Tuned Hyperparameters :", clf.best_params_)<br/>    print("Accuracy :", clf.best_score_)</span><span id="0390" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu">    # add the clf to the estimators list</strong><br/>    estimators.append((classifier.__class__.__name__, clf))</span></pre><p id="c200" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个代码块如下所示:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="f291" class="nu mu it mq b gy nv nw l nx ny">from sklearn.model_selection import GridSearchCV<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn import svm</span><span id="890c" class="nu mu it mq b gy nz nw l nx ny">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="874d" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># the list of classifiers to use</strong><br/># use random_state for reproducibility<br/>classifiers = [<br/>    LogisticRegression(random_state=0), <br/>    KNeighborsClassifier(), <br/>    RandomForestClassifier(random_state=0)<br/>]</span><span id="d1ab" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># parameter grids for the various classifiers</strong><br/>logregress_parameters = {<br/>    'classifier__penalty' : ['l1','l2'],     <br/>    'classifier__C'       : np.logspace(-3,3,7),<br/>    'classifier__solver'  : ['newton-cg', 'lbfgs', 'liblinear'],<br/>}</span><span id="aedb" class="nu mu it mq b gy nz nw l nx ny">knn_parameters = {<br/>    'classifier__n_neighbors': np.arange(1, 25, 2)<br/>}</span><span id="03bf" class="nu mu it mq b gy nz nw l nx ny">randomforest_parameters = {<br/>    'classifier__n_estimators': [50, 100, 200, 300]<br/>}</span><span id="768b" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># stores all the parameters in a list </strong><br/>parameters = [<br/>    logregress_parameters, <br/>    knn_parameters, <br/>    randomforest_parameters<br/>]</span><span id="c4b5" class="nu mu it mq b gy nz nw l nx ny"># estimators is a list of tuple -&gt; <br/>#   [(name_of_classifier, tuned_estimator)]<br/>estimators = []</span><span id="7550" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu"># iterate through each classifier and use GridSearchCV</strong><br/>for i, classifier in enumerate(classifiers):</span><span id="806b" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu">    # create a Pipeline object</strong><br/>    pipe = Pipeline(steps=[<br/>        ('preprocessor', features_preprocessor),<br/>        ('classifier', classifier)<br/>    ])</span><span id="ea1c" class="nu mu it mq b gy nz nw l nx ny">    clf = GridSearchCV(pipe,              # model<br/>              param_grid = parameters[i], # hyperparameters<br/>              scoring='accuracy',         # metric for scoring<br/>              cv=10)                      # number of folds</span><span id="8cea" class="nu mu it mq b gy nz nw l nx ny">    clf.fit(X, y)<br/>    print("Tuned Hyperparameters :", clf.best_params_)<br/>    print("Accuracy :", clf.best_score_)</span><span id="84eb" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu">    # add the clf to the estimators list</strong><br/>    estimators.append((classifier.__class__.__name__, clf))    <br/></span></pre><p id="7957" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行这段代码后，您将看到以下输出:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="fdd5" class="nu mu it mq b gy nv nw l nx ny">Tuned Hyperparameters : {'classifier__C': 0.1, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}<br/>Accuracy : 0.7934956304619226<br/>Tuned Hyperparameters : {'classifier__n_neighbors': 5}<br/>Accuracy : 0.8204619225967539<br/>Tuned Hyperparameters : {'classifier__n_estimators': 50}<br/>Accuracy : 0.8148564294631709</span></pre><p id="7078" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，性能最好的估计器是邻居为5的KNN，精度为0.82。</p><h1 id="3b85" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">集成学习</h1><p id="6b9f" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">现在您已经用<code class="fe mn mo mp mq b">GridSearchCV()</code>调好了三个分类器的超参数，现在您可以使用<code class="fe mn mo mp mq b">VotingClassifier</code>类将它们放在一起用于集成学习。使用这个类，您可以传入您的调优估计器，并选择如何使用它们进行预测:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="feb7" class="nu mu it mq b gy nv nw l nx ny">from sklearn.ensemble import VotingClassifier</span><span id="0ad3" class="nu mu it mq b gy nz nw l nx ny"><strong class="mq iu">ensemble = VotingClassifier(estimators, voting='hard')</strong></span></pre><p id="35ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mn mo mp mq b">voting</code>参数有两个可能的值:</p><ul class=""><li id="57e8" class="os ot it lb b lc ld lf lg li ou lm ov lq ow lu ox oy oz pa bi translated"><code class="fe mn mo mp mq b">hard</code> —也称为<strong class="lb iu"> <em class="ms">多数表决</em> </strong>，这意味着每个估计器将继续做出自己的预测。在预测结束时，由大多数估计器预测的标签将是最终预测的标签:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e8fab5047fcb1f3a8e7c667c7c497402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*0PnRdR8wft7E-2oqI0mCvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">参数投票= '硬'(图片由作者提供)</p></figure><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="2418" class="nu mu it mq b gy nv nw l nx ny">ensemble = VotingClassifier(estimators, <strong class="mq iu">voting='hard'</strong>)  # default is<br/>                                                        # 'hard'</span></pre><blockquote class="oa ob oc"><p id="3371" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">如果出现平局，预测的类标签将按升序排序，并将选择顶部的标签。</p></blockquote><ul class=""><li id="b7d6" class="os ot it lb b lc ld lf lg li ou lm ov lq ow lu ox oy oz pa bi translated"><code class="fe mn mo mp mq b">soft</code> —也称为<strong class="lb iu"> <em class="ms">加权平均概率投票</em> </strong>，这意味着每个估计器将生成每个类别的概率。</li></ul><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="3ed3" class="nu mu it mq b gy nv nw l nx ny">ensemble = VotingClassifier(estimators, <strong class="mq iu">voting='soft'</strong>)</span></pre><blockquote class="oa ob oc"><p id="600e" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">要使用<code class="fe mn mo mp mq b"><em class="it">soft</em></code>投票，您的所有评估人员必须支持<code class="fe mn mo mp mq b"><em class="it">predict_proba()</em></code>方法。</p></blockquote><p id="9996" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在该过程结束时，对每个类的所有概率进行求和，并且具有最高求和概率的类将是最终的预测标签:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/c146cf1634aaad3b4fc8f628e8b6ee07.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*8EFm3k3SOqDBieEQtuCDGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">参数投票= '软'(图片由作者提供)</p></figure><p id="0be7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还可以为每个评估者分配权重:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="2cfa" class="nu mu it mq b gy nv nw l nx ny">ensemble = VotingClassifier(estimators, <br/>                            voting='soft', <br/>                            <strong class="mq iu">weights=[1,1,1]</strong>)  # n-estimators</span></pre><p id="5fb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您为每个估计值分配权重时，它们将应用于每个类的计算概率。然后计算每个类别的加权平均值，具有最高加权平均值的类别是最终预测的标签:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/fe6ac85ac5edb19cc9e488f971af0b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*lw4SlgdDfNrscqUg7DaB0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将权重应用于集合模型(图片由作者提供)</p></figure><blockquote class="oa ob oc"><p id="c195" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">权重也可以应用于<code class="fe mn mo mp mq b">hard</code>投票。</p></blockquote><p id="0902" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以用您的训练数据来调整整体:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="ff02" class="nu mu it mq b gy nv nw l nx ny">ensemble.fit(X_train, y_train)</span></pre><p id="915a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后用你的测试集来评分:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="49ef" class="nu mu it mq b gy nv nw l nx ny">ensemble.score(X_test, y_test)</span><span id="09db" class="nu mu it mq b gy nz nw l nx ny"># 0.8507462686567164</span></pre><blockquote class="oa ob oc"><p id="6c0e" class="kz la ms lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">正如你所观察到的，集合模型的精度高于每个单独的估计量。</p></blockquote><p id="85fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，您可以使用自己的数据进行预测:</p><pre class="kj kk kl km gt nq mq nr ns aw nt bi"><span id="3816" class="nu mu it mq b gy nv nw l nx ny"># test data for 2 passengers<br/>test_data = {<br/>    'Pclass'   : [2,1],<br/>    'Sex'      : ['male','female'],<br/>    'Age'      : [35,15],<br/>    'Fare'     : [90,20],<br/>    'Embarked' : ['S','Q']<br/>}</span><span id="6d4f" class="nu mu it mq b gy nz nw l nx ny">ensemble.predict(pd.DataFrame.from_dict(test_data))<br/># array([0, 1])</span></pre><p id="8a51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的输出中，第一个乘客没有幸存(<code class="fe mn mo mp mq b">0</code>)，而第二个乘客幸存(<code class="fe mn mo mp mq b">1</code>)。</p><h1 id="6db5" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">摘要</h1><p id="6afc" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">这篇短文解释了集成学习是如何工作的。特别是，我将它与pipelines和<code class="fe mn mo mp mq b">GridSearchCV()</code>结合起来，使你的机器学习工作流程正式化。如果你在你的机器学习项目中使用集成学习，请在评论中告诉我。</p><div class="lv lw gp gr lx ly"><a href="https://weimenglee.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">加入媒介与我的介绍链接-李伟孟</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">weimenglee.medium.com</p></div></div><div class="mh l"><div class="pj l mj mk ml mh mm ks ly"/></div></div></a></div></div></div>    
</body>
</html>