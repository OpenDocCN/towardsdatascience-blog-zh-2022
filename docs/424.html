<html>
<head>
<title>An Introductory Primer to Bayesian Statistics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯统计入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introductory-primer-to-bayesian-statistics-3415ffa28488#2022-01-14">https://towardsdatascience.com/an-introductory-primer-to-bayesian-statistics-3415ffa28488#2022-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="7488" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated"><strong class="ak">贝叶斯统计入门</strong></h1></div><div class=""><h2 id="20e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">贝叶斯统计世界的数学介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9aa73954c7cb8046388c6092e34e3526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6kasoezqLON45CC-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@mediamodifier?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">媒体修改器</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b5ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1748年，哲学家大卫·休谟写了一篇名为《论奇迹》的文章，认为过去写的不可信的奇迹故事不应该被相信，因为它们违背了自然法则。15年后，托马斯·贝叶斯牧师的遗作提出了一个更重要的问题，“我们需要多少证据来支持任何主张？”</p><p id="a0b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然他的论文确实有神学目的，但它也有许多数学含义，可以说在今天甚至更相关。</p><p id="9f98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">250年后，他的简单公式仍然广泛适用，甚至创造了一个全新的统计学分支。它不仅是任何概率入门课程中的一个基本概念，也是我们今天拥有的许多重要算法和技术的动力。</p><h1 id="5b64" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">贝叶斯公式的数学直觉</strong></h1><p id="5951" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们以一个简单的参数估计问题为背景来看看贝叶斯公式。我们收集一些数据<strong class="lb iu"> <em class="ms"> x </em> </strong>并且我们确定这些数据是基于某个参数<strong class="lb iu"> <em class="ms"> θ生成的。</em>T11】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1712c992246f08965ce81f37ef18c635.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*0U5W-GYrWLXxhPcg8GcCOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯公式</p></figure><p id="0fa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，概率不是发生的概率，而是确定的程度。如果概率为0，我们确定该主张为假，而如果概率为1，我们完全确定该主张为真。</p><p id="4c16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着视角的改变，P(x|θ)是我们对θ值的确定性。请注意，这是一个以x为条件的概率。因此，后验概率本质上陈述了<strong class="lb iu">“假设我们已经收集了数据x，我们对θ的确定性如下”</strong></p><p id="ce5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个后部由3部分组成</p><ul class=""><li id="b7e5" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu"> P(x|θ) —似然</strong>:在给定当前模型的情况下，观察到证据的概率。这个模型可以看作是对数据设置了某些假设。例如，如果我们假设山洪暴发的速率为泊松分布，速率参数为<strong class="lb iu"> λ </strong>，则可能性将基于泊松分布的<strong class="lb iu">概率质量函数</strong>。</li><li id="9a3f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> P(θ) —先验</strong>:我们参数的初始分布。使用前面的泊松例子，我们可以近似地得出速率参数λ遵循从1到3的均匀分布</li><li id="1335" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated"><strong class="lb iu"> P(X) —模型证据</strong>:所有参数观察到证据<strong class="lb iu">的概率。在大多数情况下，它的主要目的是作为一个标准化常数的后验。有效的概率分布的总和必须等于1，而归一化常数有助于确保这一点。</strong></li></ul><p id="ce85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个公式的有用之处在于，当额外的证据被引入时，我们可以将我们的后验概率作为新的先验概率。这就产生了一个反复的过程，在这个过程中，我们慢慢地使我们的后验概率适应证据，以评估我们对某一主张的确定性程度。</p><p id="2ed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<strong class="lb iu"> P(X) </strong>是一个归一化常数，在大多数情况下，我们可以将后验概率表示为一个更简单的比例方程。这允许我们将贝叶斯公式改写如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/67687be06b24b9cf85223748dc2baa4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*ctSo4GmY9jtrXEG1yERQxw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯公式——比例</p></figure><p id="b7ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经理解了贝叶斯公式，让我们重新回顾一下frequentist线性回归，看看它与贝叶斯方法有什么不同。</p><h1 id="f7e9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">频率主义者线性回归</strong></h1><p id="b24e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在普通(简单)线性回归中，我们有一个模型，可以表述为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/38541a51b5c77ee48ccca40c8a1147d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*FI_pIVY3djcimrOCcUaNbQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归模型</p></figure><p id="ae02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标变量y可以分解成确定性分量<strong class="lb iu"> <em class="ms"> β x +α </em> </strong>和被建模为随机噪声的随机分量ϵ。</p><p id="6ca6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此，我们还可以得到y的条件分布，即<strong class="lb iu"> P(Y|X，β) ~ N(β x +α，ϵ) </strong></p><blockquote class="nk nl nm"><p id="cea0" class="kz la ms lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">在频率统计中，我们只关心可能性。在这种情况下，我们已经标记了数据，它略有不同，因为我们关注的是<strong class="lb iu">条件似然P(Y|X，β) </strong></p></blockquote><p id="49ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于似然性是以X和β为条件的，这个条件似然性就是正态分布的概率密度函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/54a042686573163645e847d678811c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*bIdxlNI6QcXdCP9zoajzOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">条件似然的概率分布</p></figure><p id="b37c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像往常一样，我们将其转换为更容易优化的对数似然。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/92e92b182e2f6cb550eb204d92fa5e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PexafMPb9T16pWxqEL3IgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">条件对数似然</p></figure><p id="5db7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了对数似然函数，我们可以执行优化，并找到使该似然函数最大化的β。注意，前两项与β无关，可以省略。同样，由于<strong class="lb iu"> 1/2σ </strong>是常数，所以不会影响优化，可以省略。剩下的就是下面的优化目标了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/04893bab43601bb072127799ec3cc9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*jW1_J-m8Udaa3haXQpMjdA.png"/></div></figure><p id="732b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果这个目标看起来很熟悉，那是因为它是我们都非常熟悉的最小二乘目标。</p><p id="3a1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对线性回归数据集执行最小二乘法就是进行最大似然估计。本质上，这给了我们一个线性回归的统计解释！</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="6102" class="lv lw it bd lx ly oa ma mb mc ob me mf jz oc ka mh kc od kd mj kf oe kg ml mm bi translated"><strong class="ak">贝叶斯线性回归</strong></h1><p id="76a0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在频率统计中，我们相信每个未知参数都有一个“真值”或“正确答案”。然后使用最大似然估计来获得这些参数的最佳估计。在贝叶斯统计中，参数本身是一个随机变量，我们试图从观测值中获得这个随机变量的分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0940ba281ca19699211d2448880dd48a.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*YIQLaPDHT8VupdS8XGyhgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一般线性回归方程</p></figure><p id="e8b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于贝叶斯回归，我们将展示一般情况，从方程<strong class="lb iu"> Y = Xβ </strong>开始。对于一个有<strong class="lb iu"> <em class="ms"> k </em> </strong>特征和<strong class="lb iu"> <em class="ms"> n </em> </strong>数据点的回归问题，β是一个k+1大小的向量(包括截距)，X是一个<strong class="lb iu"> <em class="ms"> n × k+1 </em> </strong>矩阵。</p><p id="1646" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这与前一种情况有一个关键的区别。我们的参数<strong class="lb iu"> β </strong>不再是点估计，但是<strong class="lb iu">有自己的分布</strong>。我们首先将其初始化为先验分布或<strong class="lb iu"> P(θ)。</strong>为了简化我们的计算，我们将从正态分布开始，以均值向量<strong class="lb iu"> μ₀ </strong>作为我们的先验，我们的协方差矩阵将是单位矩阵<strong class="lb iu"> <em class="ms"> I </em> </strong>。</p><p id="0015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此，我们现在可以求解后验分布。这是通过对模型中的所有变量应用贝叶斯公式来求解后验概率来实现的。</p><h2 id="daf2" class="og lw it bd lx oh oi dn mb oj ok dp mf li ol om mh lm on oo mj lq op oq ml or bi translated"><strong class="ak">为后路求解</strong></h2><p id="7184" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">由于我们先前的假设，我们可以推导出这个特殊问题的解析解。回想一下，我们能够将后验概率表示为一个比例方程，它将先验概率和似然概率结合在一起。</p><p id="7e1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的似然和先验都是多元正态分布，可以这样表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d0bbd8fc34af58efdfcb42efd60aece7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*t2rr1j1rWBmKXMLXV3IpBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">似然和先验分布</p></figure><p id="f225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们把重点放在指数项上，把可能性和先验结合起来。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/8b2195e40e32a4a355da13ac9f350709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKXygYN-9nXhSjRVw7UtJg.png"/></div></div></figure><p id="c0cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以去掉不包含β的项，因为它们是常数，可以被吸收到归一化常数项中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/afc9e33bc9edf86ef7bf484d880d9104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7Dbr86yjRmahnDIGZnWEg.png"/></div></div></figure><p id="2ec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里我们看到后验可以表示为一般形式的<br/> <strong class="lb iu"> C exp(-Q(β)) </strong>其中<strong class="lb iu"> Q(β) </strong>是二次多项式。这意味着后验概率必须是正态分布。现在我们的目标是将指数部分转换成(β—μ<strong class="lb iu">ₙ</strong>)ᵀσ⁻(β—μₙ).)的形式</p><p id="8912" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">二次部分可以分解成<strong class="lb iu"> βᵀ(XᵀX+I)β </strong>。这给了我们<strong class="lb iu">∑</strong>一定是<strong class="lb iu">【βᵀ(xᵀx+i)β】</strong>⁻</p><p id="ed89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一部分是<strong class="lb iu">-2μₙᵀσ⁻β</strong>。这对应于<strong class="lb iu"> 2(YᵀX + μ₀)β </strong>。通过一些操作和匹配，我们也可以导出一个表达式。</p><blockquote class="nk nl nm"><p id="17c7" class="kz la ms lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">注意:使用这个因式分解会有一个μₙᵀσ⁻μₙ项，但是它可以被去掉，因为它不是β的函数</p></blockquote><p id="e22f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了我们最后的PDF</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0f17876604c7a2e8c70c163448120bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*0ri3PVAjBtqCWPoYST0mbg.png"/></div></figure><p id="60de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> μₙ </strong>和<strong class="lb iu">σ</strong>分别是均值向量和协方差矩阵。</p><p id="9260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，如果我们将初始均值<strong class="lb iu"> μ₀ </strong>设为0向量，我们会得到一个非常熟悉的解。如果我们添加一个正则项，<strong class="lb iu"> μₙ </strong>的值将是(<strong class="lb iu"> XᵀX + I)⁻ XᵀY </strong>，这是<a class="ae ky" href="https://en.wikipedia.org/wiki/Ridge_regression" rel="noopener ugc nofollow" target="_blank">岭回归解</a>。这意味着，从贝叶斯的角度来看，岭回归可以被视为具有以0为中心的先验！</p><p id="4cd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">贝叶斯统计告诉我们如何在证据面前调整我们先前的信念。如果我们的观察值不是正态分布的，或者我们想要不同的起始参数，我们可以自由地这样做。贝叶斯统计为我们提供了一种计算(或至少近似)参数<strong class="lb iu"> <em class="ms"> θ </em> </strong>分布的方法</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="9802" class="lv lw it bd lx ly oa ma mb mc ob me mf jz oc ka mh kc od kd mj kf oe kg ml mm bi translated"><strong class="ak">为什么要贝叶斯学习？</strong></h1><ol class=""><li id="2b82" class="mu mv it lb b lc mn lf mo li ow lm ox lq oy lu oz na nb nc bi translated"><strong class="lb iu">合并先前信息</strong></li></ol><p id="d070" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先验是非常有用的，因为它帮助我们根据数据之外的一些知识来引导我们的模型达到不同的目标。贝叶斯统计为我们提供了一种整合这些信息的数学方法。事实上，流行的正则化技术，如L2权重惩罚，可以从贝叶斯的角度来看，具有特定类型的先验。</p><p id="c55b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。获得参数值的分布<br/> </strong>贝叶斯技术的一个巨大优势是这些模型返回参数值的分布。然后，这些参数值可用于计算预测的置信区间。在错误预测会产生巨大后果的情况下，拥有这些置信区间可以帮助决策者更好地估计错误预测的概率。</p><p id="df67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。数据不足<br/> </strong>虽然MLE被证明是给定足够大数量样本的最佳估计量，但在许多情况下，我们没有足够的数据点。</p><p id="7007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种可能性是，由于难以获得样本，数据很少。另一个更常见的可能性是数据是高维的。随着维数的增加，我们需要的样本数呈指数增长，导致我们的MLE估计不够精确。</p><p id="c83c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对参数进行点估计会将所有的概率分布折叠成一个点，这会导致模型对错误的预测非常有把握。</p><p id="930b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用贝叶斯方法有助于模型在观察外来数据点时降低可信度，并降低产生高可信度错误预测的可能性。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="4913" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，贝叶斯技术有一个很大的弱点，那就是它们很难计算。在许多情况下，获得P(X)在数学上是困难的，我们不能解析地求解后验概率。在这些情况下，贝叶斯推理是通过<strong class="lb iu">逼近</strong>后验来完成的。在下一篇博文中，我将深入研究这些方法，并展示它们是如何工作的。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="026b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，请在Medium上关注我！<br/>在LinkedIn上连接:<a class="ae ky" href="https://www.linkedin.com/in/reo-neo/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/reo-neo/</a></p><h2 id="dba5" class="og lw it bd lx oh oi dn mb oj ok dp mf li ol om mh lm on oo mj lq op oq ml or bi translated">参考</h2><p id="5333" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">I .古德费勒、y .本吉奥和a .库维尔，2016年。<em class="ms">深度学习</em>。剑桥(EE。UU。):麻省理工学院出版社，pp .第5.6节贝叶斯统计。</p><p id="ad9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">珀尔朱迪亚和丹娜麦肯齐。<em class="ms">为什么之书</em>。企鹅图书，2019。</p></div></div>    
</body>
</html>