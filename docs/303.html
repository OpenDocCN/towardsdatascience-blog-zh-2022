<html>
<head>
<title>Decision Tree and Random Forest from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的决策树和随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-and-random-forest-from-scratch-4c12b351fe5e#2022-01-11">https://towardsdatascience.com/decision-tree-and-random-forest-from-scratch-4c12b351fe5e#2022-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="13a4" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">从零开始的决策树和随机森林</h1></div><div class=""><h2 id="0af6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对决策树和随机森林算法的全面外行介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c69fcc8520a46c61234c023fb6c079f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5uIcvtxvtIB3Y90mYK2wg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@evgenit" rel="noopener ugc nofollow" target="_blank">叶夫根尼·切尔卡斯基</a>在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d6e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树和随机森林可能是机器学习领域中最经典的算法之一。在本教程中，我们将带读者在公园里漫步，欣赏这些雄伟树木背后的算法之美。除了为您提供这些算法结构背后的简单直觉之外，我们将带您从头开始理解这些代码，这样在本教程结束时，您将基本掌握强大的决策树和随机森林算法。</p><p id="4c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">页（page的缩写）s:这篇文章受到了由<a class="ae ky" href="https://medium.com/@joachimvalente" rel="noopener">约阿希姆·瓦伦蒂</a>撰写的关于<a class="ae ky" rel="noopener" target="_blank" href="/decision-tree-from-scratch-in-python-46e99dfea775">决策树</a>和由<a class="ae ky" href="https://medium.com/@radecicdario" rel="noopener">达里奥·拉德契奇</a>撰写的<a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a">随机森林</a>的早期文章的启发，但是为了更好的算法和更容易理解，这些想法被进一步微调和提炼。</p><h1 id="651e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.决策树算法</h1><p id="7956" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然决策树可以用于回归(预测连续的实值目标，例如预测汽车价格，给定特征)，但在本教程中，我们将只考虑分类决策树(预测目标的离散类别，例如预测水果类型，给定特征)。</p><p id="4cee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在关于特征-目标关系的<strong class="lb iu">模型训练</strong>期间，从<strong class="lb iu">根(父)节点</strong>(包含特征-目标关系的所有数据)生长一棵树，然后以二进制方式递归地分裂成<strong class="lb iu">子节点</strong>(整个数据的子集)。在父节点中的单个特征上，在该特征的期望阈值<strong class="lb iu">处执行每次分割。例如，在父节点的每次分裂期间，如果特征小于阈值，我们转到左节点(具有相应的数据子集)，否则转到右节点。但是我们如何决定分割呢？进入基尼杂质。</strong></p><h2 id="892e" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.1.基尼杂质</h2><p id="ead8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">决定父节点分裂的成本函数被称为基尼不纯度，这基本上是一个量化节点的同质性或“纯净”程度的概念，与节点中目标的分布有关。如果节点中的所有<strong class="lb iu">训练样本属于同一类，则节点被认为是纯的(G=0 ),而具有来自许多不同类的许多训练样本的节点将具有接近1的Gini杂质。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3575b89bc80d190c9c6e4259cef0c8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*j5ngW9gOMKqGbrkTp9VhIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="67a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树向基尼杂质递减的方向生长，根节点最不纯。因此，<strong class="lb iu">在每个节点，搜索并选择最优特征和相应的最优阈值</strong>，使得2个子节点的加权平均基尼系数最小。</p><p id="b441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果最小可能加权平均Gini杂质仍大于父节点的Gini杂质，则不会发生二进制分裂，父节点将被视为<strong class="lb iu">终端节点</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/27c030b92d3788ff44741d7efc1a7498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*KwiYAaf44xXzxbKhGgo3uQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="d9bd" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.2.水果分类示例</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4cf0cd5b0ae47710701c4b41746462c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*IDVBFRfbE6Eq02PkjOywfQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">水果表及其特点。图片作者。</p></figure><p id="f36f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以<code class="fe ng nh ni nj b">fruit_name</code>为对象、<code class="fe ng nh ni nj b">mass</code>、<code class="fe ng nh ni nj b">width</code>、<code class="fe ng nh ni nj b">height</code>、<code class="fe ng nh ni nj b">color_score</code>为特征的表格数据为例。当我们将这些数据训练到一个这样的模型上时，决策树(最大深度=2)看起来会是什么样子？</p><p id="b0e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">页（page的缩写）为了演示的简单，二进制分割被认为是随机发生的，而不应该被认为是最佳分割。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b76bd40ad9afd11d8e42c1637be81c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*3b_XfdDjG3jrpjRmzYTfhQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最大深度=2的决策树。图片作者。</p></figure><p id="7f32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解，我们鼓励你勾画出上述决策树，并对其进行必要的基尼系数计算。我们也邀请你勾画出决策树的其他排列。</p><h2 id="1fc4" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.3.推理时的预测</h2><p id="6bde" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">给定一个新的水果，具有其指定特征的苹果，预测的水果将从其根节点遍历决策树，遵循每个节点给定的规则，到达终端节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/08a51361fdcbe2d85cf4b2204efa9622.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*m37yQe3q5aHo0mYbLkKCaw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过模型进行推理的测试示例。图片作者。</p></figure><p id="bde2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的测试示例中，因为mass=160&gt;155，所以决策路径从根节点到右边的子节点。从这一点开始，由于height=7.4≤7.7，所以它进入左侧子节点，该子节点是没有进一步分割的终端节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/868ef99bae2c0f08d22fe8d8b1818629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*XCeI4EgMJIXL4EJ-2oZ99Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用决策树对单个测试示例进行推理。图片作者。</p></figure><p id="a339" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自决策树的预测类然后由终端节点处的多数类给出，这是在训练过程中确定的。</p><h2 id="0dd4" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">1.4.从头开始编写决策树</h2><p id="a083" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们对决策树的工作原理有了基本和直观的了解之后，让我们开始构建一个决策树吧！从头开始构建一个决策树可能看起来令人生畏，但是随着我们一步一步地构建它的组件，情况可能会变得简单得多。在这一节中，我们将按模块介绍代码。</p><p id="bd60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.1。实例化节点类</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="0c85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在导入必要的库之后，我们创建一个带有几个伴随属性的类<code class="fe ng nh ni nj b">Node</code>。这通常被认为是父节点。</p><p id="9bd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，<code class="fe ng nh ni nj b">self.predicted_class</code>将在每个(父)节点中存储预测的类。是的，这是正确的，每个节点都应该有多数类作为预测类，即使是根节点。并且该属性在代码中特别重要，尤其是对于终端节点，当在推断期间应该检索预测的类时。</p><p id="4a30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni nj b">self.feature_index</code>和<code class="fe ng nh ni nj b">self.threshold</code>主要记录父节点处的最优特征和该特征对应的最优阈值，将与父节点发生分裂形成子节点。</p><p id="0640" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni nj b">self.left</code>和<code class="fe ng nh ni nj b">self.right</code>是非常重要的属性，用于递归实例化子节点。因此，这些属性本质上只是另一个类<code class="fe ng nh ni nj b">Node</code>，它又可能有<code class="fe ng nh ni nj b">self.left</code>和<code class="fe ng nh ni nj b">self.right</code>属性作为子节点。</p><p id="1c03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.2。实例化决策树类</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="7797" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树模型可以有几个超参数，可以调整这些参数以获得特定数据集的最佳性能模型。在我们的算法中，我们对这种定制进行了编码。虽然<code class="fe ng nh ni nj b">self.max_depth</code>确定了树生长的最大深度，但为了在较低深度实现终端节点(通常是为了防止过度拟合)，<code class="fe ng nh ni nj b">self.max_features</code>允许我们仅考虑有限数量的特征来搜索最佳分割，这允许对特征进行一些正则化效果。</p><p id="714d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni nj b">self.random_state</code>控制估计器的随机性，特别是特征置换。每次分割时，特征总是随机排列。当<code class="fe ng nh ni nj b">max_features</code> &lt; <code class="fe ng nh ni nj b">n_features</code>时，算法将在每次分割时随机选择<code class="fe ng nh ni nj b">max_features</code>，然后在其中找到最佳分割。但是最佳发现分割可能在不同的运行中有所不同，即使<code class="fe ng nh ni nj b">max_features</code> = <code class="fe ng nh ni nj b">n_features.</code>是这种情况，如果标准的改进对于几个分割是相同的，并且必须随机选择一个分割。</p><p id="b0b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<code class="fe ng nh ni nj b">self.tree</code>是根<code class="fe ng nh ni nj b">Node</code>，包含所有其他的子<code class="fe ng nh ni nj b">Node</code>作为属性，当生长树的函数被调用时，这些属性将被递归生长。</p><p id="28a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.3。拟合方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="70a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni nj b">DecisionTree</code>类的<code class="fe ng nh ni nj b">fit</code>方法接受Pandas DataFrame/Series或Numpy数组形式的训练数据集，然后确定类的数量、特征的数量和最大特征。最后，调用函数<code class="fe ng nh ni nj b">self.grow_tree</code>递归构建决策树。</p><p id="a44e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.4。</em> <em class="nk">种树的方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="0893" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在将自身实例化为父节点<code class="fe ng nh ni nj b">Node</code>并确定其父节点的预测类之前，<code class="fe ng nh ni nj b">grow_tree</code>方法首先接收来自<code class="fe ng nh ni nj b">fit</code>方法的数据。当满足<code class="fe ng nh ni nj b">self.max_depth</code>条件时，调用函数<code class="fe ng nh ni nj b">self.best_split</code>来找到最优特征ID和相应的最优阈值，以分裂成子节点。</p><p id="cd1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果存在最佳特征ID和最佳阈值(子节点的Gini杂质的加权平均值小于父节点的Gini杂质)，则将为左子节点和右子节点过滤在特征阈值处分裂的对比数据子集。</p><p id="f5ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，各个数据子集被递归调用到同一个<code class="fe ng nh ni nj b">grow_tree</code>函数中，并作为父<code class="fe ng nh ni nj b">Node</code>的属性存储为<code class="fe ng nh ni nj b">node.left</code>和<code class="fe ng nh ni nj b">node.right</code>。</p><p id="3318" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后返回父对象<code class="fe ng nh ni nj b">Node</code>。</p><p id="6004" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.5。最佳拆分方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="2cbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni nj b">best_split</code>方法从<code class="fe ng nh ni nj b">grow_tree </code>方法中的父节点获取数据，然后检查父节点的Gini杂质。如果父节点已经是纯的(G=0)，<code class="fe ng nh ni nj b">best_feat_id</code>和<code class="fe ng nh ni nj b">best_threshold</code>作为<code class="fe ng nh ni nj b">None</code>返回。否则，该方法继续基于最大特征数量对特征索引进行加扰和随机化。</p><p id="77d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，该方法继续搜索最佳特征索引和相应的阈值。在迭代该特征的连续值之间的每个平均值之前，首先过滤和排序每个特征的唯一数据值。</p><p id="5ff7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每对特征指数和特征阈值，计算平均基尼系数。对应于最低平均Gini杂质(如果它低于父节点Gini杂质)的特征索引和特征阈值对作为<code class="fe ng nh ni nj b">best_feat_id</code>和<code class="fe ng nh ni nj b">best_threshold</code>返回。</p><p id="59f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.6。预测方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="376e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在训练集上训练决策树之后，可以通过<code class="fe ng nh ni nj b">predict</code>方法对测试集进行推理。使用<code class="fe ng nh ni nj b">predict_example</code>方法，通过合适的决策树运行测试集中的每个测试示例，一次一个。</p><p id="d0b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 1.4.7。预测示例方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="2105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个测试示例都是特征值的向量。在<code class="fe ng nh ni nj b">predict_example</code>方法中，测试示例根据每个特性的阈值遍历基于规则的节点，直到到达终端节点，在那里存储并返回预测的类。</p><p id="8f81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树算法的完整代码显示在<a class="ae ky" href="https://github.com/tanpengshi/ML_Algorithms_from_Scratch/blob/master/decision_tree.py" rel="noopener ugc nofollow" target="_blank"> Github </a>上。精度性能与scikit-learn的实现非常接近，我们邀请您试用。</p><h1 id="800c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.随机森林算法</h1><p id="afc3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随机森林，顾名思义，是决策树的集合，也用于回归和分类任务。同样，我们在这里只考虑随机森林进行分类。</p><p id="787d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林算法建立在“弱”学习者(决策树)投票的思想上，给出了树组成森林的类比。随机性元素有几个方面:</p><ol class=""><li id="0e32" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">每棵树都适合整个数据集的子集，因此每棵树的生长方式不同，规则也不同</li><li id="1d0c" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">如果<strong class="lb iu">整个数据集</strong>用替换随机引导<strong class="lb iu">，对于每棵树，每棵树仍将具有略微不同的数据分布，因此将以不同的规则不同地生长。</strong></li><li id="f8c5" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">即使<strong class="lb iu">整个数据集</strong>在没有替换的情况下被引导<strong class="lb iu">，每个树仍然可能不同地生长，这是由于在决策树中被考虑用于最佳分裂的特征的随机顺序或子集。</strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/570515e8e97bc64d5f699ea2670ad60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2ZqFMiqDX-1uSCfS.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树与随机森林。图片由<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Decision_Tree_vs._Random_Forest.png" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="c53a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种随机森林算法被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">自举聚合</a>或bagging。本质上，它是一个结合多个模型(决策树)的概念，在数据集上引导，通过减少方差和避免单个模型的过度拟合来提高性能。</p><p id="679a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，</p><ul class=""><li id="b990" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu oc nt nu nv bi translated">在训练<em class="nk">期间，从整个数据集顺序获得N个</em>自举数据集。</li><li id="1f57" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated">每个决策树(总共<em class="nk"> N </em>个决策树)都是从每个自举数据集构建的。</li><li id="c981" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated">在推理过程中，每个决策树都会做出一个预测，随机树的最终预测将作为多数投票返回。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/830286a3bc68d9d56369e2b2aeb5d4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*rnJ0JvZGJfksnAsW.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对随机森林进行推断时的预测。图片由<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png" rel="noopener ugc nofollow" target="_blank">来源</a>提供。</p></figure><h2 id="a274" class="ms lw it bd lx mt mu dn mb mv mw dp mf li mx my mh lm mz na mj lq nb nc ml nd bi translated">2.1.从头开始编码随机森林</h2><p id="faaa" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如您所见，随机森林与决策树算法密切相关。因此，从某种意义上来说，它是上述决策树算法代码的结转。同样，我们将按模块介绍代码。</p><p id="b888" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 2.1.1。实例化随机森林类</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="ca3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在导入必要的库(包括决策树算法的模块)之后，我们创建一个带有相应属性的类<code class="fe ng nh ni nj b">RandomForest</code>:</p><ul class=""><li id="054d" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.num_trees</code>:用于分类的投票决策树分类器的数量。</li><li id="29b6" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.subsample_size</code>:用于训练每个决策树的总训练样本的比例。</li><li id="ff2a" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.max_depth</code>:每个决策树的最大深度。如果没有，那么节点被扩展，直到所有的叶子都是最纯的。</li><li id="7942" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.max_features</code>:对于每个决策树，在从父节点到子节点的每次分裂中，仅考虑“最大特征”来寻找阈值分裂。</li><li id="fee6" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.bootstrap</code>:训练样本的Bootstrap抽样，有无替换。</li><li id="aa34" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.random_state</code>:控制估计器的随机性。在每个决策树的每次分裂中，特征总是随机排列的，并且引导抽样是随机排列的。</li><li id="9be3" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oc nt nu nv bi translated"><code class="fe ng nh ni nj b">self.decision_tree</code>:包含构建后的决策树对象的列表。最初，列表是空的。</li></ul><p id="e025" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 2.1.2。拟合方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="d77f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练期间，<code class="fe ng nh ni nj b">RandomForest</code>类的<code class="fe ng nh ni nj b">fit</code>方法接受Pandas DataFrame/Series或Numpy数组形式的训练数据集，并在训练数据集的随机引导样本上顺序构建每个决策树(导入的模块)，然后将其作为对象附加到<code class="fe ng nh ni nj b">RandomForest</code>类的<code class="fe ng nh ni nj b">self.decision_tree</code>属性。通过<code class="fe ng nh ni nj b">sample</code>方法获取随机引导样本。</p><p id="40d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 2.1.3。取样方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="8155" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe ng nh ni nj b">fit</code>方法中，训练数据集被摄取到被调用的<code class="fe ng nh ni nj b">sample</code>方法中，以返回基于整个数据集的随机引导子集。</p><p id="b542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nk"> 2.1.4。预测方法</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="cb81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在推理过程中，每个决策树将所有测试实例的结果预测到预测的<strong class="lb iu">列</strong>中。这些预测列在所有决策树中并排附加在Numpy数组中。此后，对于每个测试示例，在每个<strong class="lb iu">行</strong>中采用<strong class="lb iu">模式或大多数</strong>预测。最后一列作为最终预测结果返回。</p><p id="d688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林算法的完整代码显示在<a class="ae ky" href="https://github.com/tanpengshi/ML_Algorithms_from_Scratch/blob/master/random_forest.py" rel="noopener ugc nofollow" target="_blank"> Github </a>上。同样，我们的精度性能与scikit-learn的实现非常接近。</p><h1 id="dc58" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.结论</h1><p id="87e0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">祝贺你成功来到这里！给你一些编码和机器学习的经验，从这本简单的指南中，你将会对决策树和随机森林如何从头开始工作有一个原始的和完整的理解。这种理解对于任何严肃的数据科学家的学习课程都是必不可少的。</p><p id="d7ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在未来，我期待以最浅显易懂的方式从零开始介绍其他机器学习算法，如香草神经网络(多层感知器)、CNN和RNNs。敬请关注，关注我在<a class="ae ky" href="https://www.linkedin.com/in/tanpengshi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>和<a class="ae ky" href="https://github.com/tanpengshi" rel="noopener ugc nofollow" target="_blank"> Github </a>的报道。</p><p id="107c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新(2022年7月28日):我已经从头开始完成了关于逻辑回归和神经网络的3篇文章系列。查看它们以了解更多:</p><div class="oe of gp gr og oh"><a href="https://medium.com/mlearning-ai/neural-networks-from-scratch-logistic-regression-part-1-d8cfc4a2fb3b" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">从零开始的神经网络:逻辑回归—第1部分</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">单层神经网络与裸基础的逻辑回归</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">medium.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ks oh"/></div></div></a></div><p id="a85c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">干杯！_/\_</p><blockquote class="ow ox oy"><p id="ea0c" class="kz la nk lb b lc ld ju le lf lg jx lh oz lj lk ll pa ln lo lp pb lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">支持我！</em> </strong> —如果你<em class="it">没有</em>订阅Medium，并且喜欢我的内容，请考虑通过我的<a class="ae ky" href="https://tanpengshi.medium.com/membership" rel="noopener">推荐链接</a>加入Medium来支持我。</p></blockquote><div class="oe of gp gr og oh"><a href="https://tanpengshi.medium.com/membership" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">加入我的介绍链接媒体-谭师鹏阿尔文</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">阅读谭·师鹏·阿尔文(以及媒体上成千上万其他作家)的每一个故事。您的会员费直接…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">tanpengshi.medium.com</p></div></div><div class="oq l"><div class="pc l os ot ou oq ov ks oh"/></div></div></a></div></div></div>    
</body>
</html>