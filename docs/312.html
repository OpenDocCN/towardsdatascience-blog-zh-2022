<html>
<head>
<title>Using NLP and Text Analytics to Cluster Political Texts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自然语言处理和文本分析对政治文本进行聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-nlp-and-text-analytics-to-cluster-political-texts-41fdb2c3a325#2022-01-11">https://towardsdatascience.com/using-nlp-and-text-analytics-to-cluster-political-texts-41fdb2c3a325#2022-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="24ea" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用自然语言处理和文本分析对政治文本进行聚类</h1></div><div class=""><h2 id="6e1a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">NLTK和古根海姆项目文本的scipy</h2></div><p id="3ef0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在文本分析的保护伞下，有许多python包可以帮助我们以产生有趣结果的方式分析当前和历史文本。在这个项目中，我试图用余弦相似性聚类法对跨越数千年的政治著作进行分类。我使用的具体标题是:</p><p id="f9b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马基雅维利的《君主论》</p><p id="cd95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">汉密尔顿、麦迪逊和杰伊的《联邦党人文集》</p><p id="c1c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">恩格斯的《英国工人阶级的状况》</p><p id="65f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">摩尔的《乌托邦》</p><p id="7936" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">戈德曼的《无政府主义和其他论文》</p><p id="8044" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">霍布斯的《利维坦》</p><p id="0c17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">《美国宪法》</p><p id="5029" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">密尔的《代议制政府的思考》</p><p id="b9a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马克思和恩格斯的《共产党宣言》</p><p id="2115" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">亚里士多德的《政治学》</p><p id="d2e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">洛克的《政府第二论》</p><p id="8cc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步是下载文本语料库(直接从古根海姆项目的网站下载)。然后，我需要导入文件，并在文本文件中找到每项工作的开始和结束行。* *古根海姆项目文件下载为。txt文件，并在实际工作的上下有额外的行。**</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="a738" class="lk ll iq lg b gy lm ln l lo lp"><em class="lq"># For loop to find the start and end points of the books</em><br/><strong class="lg ir">for</strong> i <strong class="lg ir">in</strong> book_id:<br/>    ...<br/>    a <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>line_str<strong class="lg ir">.</strong>str<strong class="lg ir">.</strong>match(r"\*\*\*\s*START OF (THE|THIS) PROJECT")<br/>    b <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>line_str<strong class="lg ir">.</strong>str<strong class="lg ir">.</strong>match(r"\*\*\*\s*END OF (THE|THIS) PROJECT")<br/>    ...</span></pre><p id="dee1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用for循环和其他字符串操作，我匹配了它们的公共开始和结束元素。txt文件，以便确定每个文学作品的起点和终点。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/4381a69a567b02da853c325e97f8a191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*4TMQAPh0r0f-w3tuSFABow.png"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="30eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">Doc&amp;Library Tables:</strong><br/>下面的regex部分搜索每本书以分离出章节，从而开始创建OCHO(内容对象的有序层次结构)样式表。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="e812" class="lk ll iq lg b gy lm ln l lo lp"><br/>OHCO <strong class="lg ir">=</strong> ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']</span><span id="122b" class="lk ll iq lg b gy lz ln l lo lp">roman <strong class="lg ir">=</strong> '[IVXLCM]+'<br/>caps <strong class="lg ir">=</strong> "[A-Z';, -]+"<br/>chap_pats <strong class="lg ir">=</strong> {<br/>    5: {<br/>        'start_line': 40,<br/>        'end_line': 680,<br/>        'chapter': re<strong class="lg ir">.</strong>compile("^ARTICLE\s+{}$"<strong class="lg ir">.</strong>format(roman))<br/>    },<br/>    61: {<br/>        'start_line': 19,<br/>        'end_line': 1529,<br/>        'chapter': re<strong class="lg ir">.</strong>compile("^\s*Chapter\s+{}.*$"<strong class="lg ir">.</strong>format(roman))<br/>    },<br/>    1232: {<br/>        'start_line': 1000, <em class="lq"># This was mannually found through trial and error becuase the previous start had issues</em><br/>        'end_line': 4828,<br/>        'chapter': re<strong class="lg ir">.</strong>compile("^\s*CHAPTER\s+{}\."<strong class="lg ir">.</strong>format(roman))<br/>    },<br/>    ...<br/>}</span></pre><p id="6c1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经有了book_id和章节的分类，我们可以填充每个文本层次结构的其余部分。我们按照熊猫和字符串操作将每一章分解成段落。生成的表称为Doc表，将在后续步骤中使用。我们还能够分离出标题和作者来完成库表。</p><p id="4388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的图片中，我们可以看到一个显示图书id、章节、段落编号和字符串的doc表示例。以及完整的库表。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ma"><img src="../Images/4f436599b477b839f7a4c8247f901bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TijfztxDjt9Uk4IgmhtvVA.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mf"><img src="../Images/6502969bee650b703b22a5c76b6d6357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d89z-rfL_UlrntdqBPKUNw.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="35ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Token &amp; Vocab表</strong> <br/>为了进一步分解Doc表，我们需要定义每个单独的术语。下面的函数接收我们的Doc表，将每个段落的字符串解析成句子，然后将这些句子解析成单独的术语。这个操作主要是通过nltk包使用nltk.sent_tokenize、nltk.pos_tag、NLTK来完成的。WhitespaceTokenizer和nltk.word_tokenize。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="73fe" class="lk ll iq lg b gy lm ln l lo lp"><em class="lq"># Tokenize doc table to derive TOKEN table</em><br/><strong class="lg ir">def</strong> tokenize(doc_df, OHCO<strong class="lg ir">=</strong>OHCO, remove_pos_tuple<strong class="lg ir">=False</strong>, ws<strong class="lg ir">=False</strong>):<br/>    <br/>    <em class="lq"># Paragraphs to Sentences</em><br/>    df <strong class="lg ir">=</strong> doc_df<strong class="lg ir">.</strong>para_str\<br/>        <strong class="lg ir">.</strong>apply(<strong class="lg ir">lambda</strong> x: pd<strong class="lg ir">.</strong>Series(nltk<strong class="lg ir">.</strong>sent_tokenize(x)))\<br/>        <strong class="lg ir">.</strong>stack()\<br/>        <strong class="lg ir">.</strong>to_frame()\<br/>        <strong class="lg ir">.</strong>rename(columns<strong class="lg ir">=</strong>{0:'sent_str'})<br/>    <br/>    <em class="lq"># Sentences to Tokens</em><br/>    <em class="lq"># Local function to pick tokenizer</em><br/>    <strong class="lg ir">def</strong> word_tokenize(x):<br/>        <strong class="lg ir">if</strong> ws:<br/>            s <strong class="lg ir">=</strong> pd<strong class="lg ir">.</strong>Series(nltk<strong class="lg ir">.</strong>pos_tag(nltk<strong class="lg ir">.</strong>WhitespaceTokenizer()<strong class="lg ir">.</strong>tokenize(x)))<br/>        <strong class="lg ir">else</strong>:<br/>            s <strong class="lg ir">=</strong> pd<strong class="lg ir">.</strong>Series(nltk<strong class="lg ir">.</strong>pos_tag(nltk<strong class="lg ir">.</strong>word_tokenize(x)))<br/>        <strong class="lg ir">return</strong> s<br/>            <br/>    df <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>sent_str\<br/>        <strong class="lg ir">.</strong>apply(word_tokenize)\<br/>        <strong class="lg ir">.</strong>stack()\<br/>        <strong class="lg ir">.</strong>to_frame()\<br/>        <strong class="lg ir">.</strong>rename(columns<strong class="lg ir">=</strong>{0:'pos_tuple'})<br/>    <br/>    <em class="lq"># Grab info from tuple</em><br/>    df['pos'] <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>pos_tuple<strong class="lg ir">.</strong>apply(<strong class="lg ir">lambda</strong> x: x[1])<br/>    df['token_str'] <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>pos_tuple<strong class="lg ir">.</strong>apply(<strong class="lg ir">lambda</strong> x: x[0])<br/>    <strong class="lg ir">if</strong> remove_pos_tuple:<br/>        df <strong class="lg ir">=</strong> df<strong class="lg ir">.</strong>drop('pos_tuple', 1)<br/>    <br/>    <em class="lq"># Add index</em><br/>    df<strong class="lg ir">.</strong>index<strong class="lg ir">.</strong>names <strong class="lg ir">=</strong> OHCO<br/>    <br/>    <strong class="lg ir">return</strong> df</span><span id="e405" class="lk ll iq lg b gy lz ln l lo lp">TOKEN <strong class="lg ir">=</strong> tokenize(DOC, ws<strong class="lg ir">=True</strong>)</span><span id="f218" class="lk ll iq lg b gy lz ln l lo lp"><em class="lq"># Creating VOCAB table and adding columns to TOKEN data</em><br/>...</span></pre><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mg"><img src="../Images/979b67a54ca7df2146e5f4dde398f2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wYKeeLHYvXfFpcQegfa4VA.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="46fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面有我们的令牌表并不意味着我们完成了。我们现在需要在这个表的基础上创建一个Vocab表来索引我们的文本语料库中使用的每个术语。使用字符串和矩阵操作，以及NLTK模块的PorterStemmer()函数，我们进一步构建我们的令牌表并创建我们的Vocab表。</p><p id="a509" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面分别是令牌表和Vocab表的示例。从我们的令牌表中可以看出，每个术语现在都对应于一个编目的term_id，可以使用我们新的Vocab表来引用它。此表提供了术语id、术语、术语词干、在整个语料库中出现的次数、停用词区别以及最常见的词性。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mh"><img src="../Images/f99503725bfa6048c85ffc0d900efc45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7b7VT1_7OWmoae0kUx2ZA.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d5141dd889495bdcbf3dd9a288035d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*wkEnsgRHQmUEUsV76VoT9Q.png"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="7545" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> TFIDF表:</strong> <br/>接下来，我们需要使用这些表来创建一个TFIDF表。TFIDF表根据每个文本内的使用频率和语料库中文档之间的使用频率来衡量每个术语。此表的目的是为我们提供最有影响力和最有用的分析单词的结果，同时减少停用词或其他常用词的影响，否则这些词会一直使用，如“and”、“of”、“The”等。请注意，我们在代码块的开头定义了不同的包组织，这些可以用来在不同的层次元素上创建TFIDF表。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="9821" class="lk ll iq lg b gy lm ln l lo lp">SENTS <strong class="lg ir">=</strong> OHCO[:4]<br/>PARAS <strong class="lg ir">=</strong> OHCO[:3]<br/>CHAPS <strong class="lg ir">=</strong> OHCO[:2]<br/>BOOKS <strong class="lg ir">=</strong> OHCO[:1]</span><span id="c287" class="lk ll iq lg b gy lz ln l lo lp"><strong class="lg ir">def</strong> tfidf(TOKEN, bag, count_method, tf_method, idf_method):<br/>    <br/>    <em class="lq">#Create Bag of Words and DTCM</em><br/>    BOW <strong class="lg ir">=</strong> TOKEN<strong class="lg ir">.</strong>groupby(bag<strong class="lg ir">+</strong>['term_id'])<strong class="lg ir">.</strong>term_id<strong class="lg ir">.</strong>count()\<br/>        <strong class="lg ir">.</strong>to_frame()<strong class="lg ir">.</strong>rename(columns<strong class="lg ir">=</strong>{'term_id':'n'})<br/>    BOW['c'] <strong class="lg ir">=</strong> BOW<strong class="lg ir">.</strong>n<strong class="lg ir">.</strong>astype('bool')<strong class="lg ir">.</strong>astype('int')<br/>    DTCM <strong class="lg ir">=</strong> BOW[count_method]<strong class="lg ir">.</strong>unstack()<strong class="lg ir">.</strong>fillna(0)<strong class="lg ir">.</strong>astype('int')<br/>    <br/>    <em class="lq"># TF calculations, will comment out all but 'sum' given that will be what I use</em><br/>    <strong class="lg ir">if</strong> tf_method <strong class="lg ir">==</strong> 'sum':<br/>        TF <strong class="lg ir">=</strong> DTCM<strong class="lg ir">.</strong>T <strong class="lg ir">/</strong> DTCM<strong class="lg ir">.</strong>T<strong class="lg ir">.</strong>sum()<br/><br/>    TF <strong class="lg ir">=</strong> TF<strong class="lg ir">.</strong>T<br/>    DF <strong class="lg ir">=</strong> DTCM[DTCM <strong class="lg ir">&gt;</strong> 0]<strong class="lg ir">.</strong>count()<br/>    N <strong class="lg ir">=</strong> DTCM<strong class="lg ir">.</strong>shape[0]<br/>    <br/>    <em class="lq"># IDF calculations, will comment out all but 'standard' given that will be what I use</em><br/>    <strong class="lg ir">if</strong> idf_method <strong class="lg ir">==</strong> 'standard':<br/>        IDF <strong class="lg ir">=</strong> np<strong class="lg ir">.</strong>log10(N <strong class="lg ir">/</strong> DF)<br/>    <br/>    <em class="lq"># create TFIDF table</em><br/>    TFIDF <strong class="lg ir">=</strong> TF <strong class="lg ir">*</strong> IDF<br/>    <strong class="lg ir">return</strong> TFIDF</span><span id="cd97" class="lk ll iq lg b gy lz ln l lo lp">TFIDF <strong class="lg ir">=</strong> tfidf(TOKEN, BOOKS, 'n', 'sum', 'standard')</span><span id="5465" class="lk ll iq lg b gy lz ln l lo lp"><em class="lq"># Add tfidf_sum column to Vocab table</em><br/>VOCAB['tfidf_sum'] <strong class="lg ir">=</strong> TFIDF<strong class="lg ir">.</strong>sum()</span></pre><p id="4121" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在下面的TFIDF表的图像中所看到的，我们得到了一个非常稀疏的结果，因为我们当前包含了在语料库中找到的每个单词。在尝试最终的聚类分析之前，我们需要进一步减少这种情况。</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi mj"><img src="../Images/23043db4a1bcaa6777c1a9df4db5e7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cmgSf51BPA7aEq3EAAs9Dw.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="9d61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，在将该TFIDF表的求和原则添加到Vocab表，并对最高tfidf_sum值的结果进行排序之后，我们得到了具有最具影响力的单词的表:</p><figure class="lb lc ld le gt ls gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/8047a8f6c62f999d15abd3d0573475c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*bfSrRJCjLJi-KdhyRPIaiQ.png"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="42b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不出所料，最有影响力的词与政治术语有关，这符合我们的预期，因为语料库由各种政治著作组成，像“and”或“the”这样的正常使用的词可能无关紧要。</p><p id="0fed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了帮助简化我们的TFIDF表，我们将把number列减少到最常见的4000，这是由上面的Vocab表中的tfidf_sum列排序的。</p><p id="b4c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">聚类:</strong> <br/>现在我们已经得到了最终的表，我们可以开始聚类过程并查看我们的结果。下一个代码块在每个作品之间创建配对，比较它们的距离和相似性度量，将文档聚类为最相似/最近的距离。出于本文的目的，我将只展示余弦相似性集群，但是您也可以运行这个代码块中包含的其他测试(cityblock、euclidean、jaccard、dice、correlation和jensenshannon)。实际的相似性/距离计算是使用scipy的空间距离模块和pdist函数运行的。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="eca9" class="lk ll iq lg b gy lm ln l lo lp"><em class="lq"># Normalize the TFIDF table, create PAIRS and lists for PAIRS testing</em><br/><strong class="lg ir">def</strong> tab_doc_tab(TFIDF_table, Doc_table):<br/>    L0 <strong class="lg ir">=</strong> TFIDF_table<strong class="lg ir">.</strong>astype('bool')<strong class="lg ir">.</strong>astype('int')<br/>    L1 <strong class="lg ir">=</strong> TFIDF_table<strong class="lg ir">.</strong>apply(<strong class="lg ir">lambda</strong> x: x <strong class="lg ir">/</strong> x<strong class="lg ir">.</strong>sum(), 1)<br/>    L2 <strong class="lg ir">=</strong> TFIDF_table<strong class="lg ir">.</strong>apply(<strong class="lg ir">lambda</strong> x: x <strong class="lg ir">/</strong> norm(x), 1)<br/>    PAIRS <strong class="lg ir">=</strong> pd<strong class="lg ir">.</strong>DataFrame(index<strong class="lg ir">=</strong>pd<strong class="lg ir">.</strong>MultiIndex<strong class="lg ir">.</strong>from_product([Doc_table<strong class="lg ir">.</strong>index<strong class="lg ir">.</strong>tolist(), Doc_table<strong class="lg ir">.</strong>index<strong class="lg ir">.</strong>tolist()]))<strong class="lg ir">.</strong>reset_index()<br/>    PAIRS <strong class="lg ir">=</strong> PAIRS[PAIRS<strong class="lg ir">.</strong>level_0 <strong class="lg ir">&lt;</strong> PAIRS<strong class="lg ir">.</strong>level_1]<strong class="lg ir">.</strong>set_index(['level_0','level_1'])<br/>    PAIRS<strong class="lg ir">.</strong>index<strong class="lg ir">.</strong>names <strong class="lg ir">=</strong> ['doc_a', 'doc_b']<br/>    tfidf_list <strong class="lg ir">=</strong> ['cityblock', 'euclidean', 'cosine']<br/>    l0_list <strong class="lg ir">=</strong> ['jaccard', 'dice','correlation']<br/>    l1_list <strong class="lg ir">=</strong> ['jensenshannon']<br/>    <strong class="lg ir">for</strong> i <strong class="lg ir">in</strong> tfidf_list:<br/>        PAIRS[i] <strong class="lg ir">=</strong> pdist(TFIDF_table, i)<br/>    <strong class="lg ir">for</strong> i <strong class="lg ir">in</strong> l0_list:<br/>        PAIRS[i] <strong class="lg ir">=</strong> pdist(L0, i)<br/>    <strong class="lg ir">for</strong> i <strong class="lg ir">in</strong> l1_list:<br/>        PAIRS[i] <strong class="lg ir">=</strong> pdist(L1, i)<br/>    <strong class="lg ir">return</strong> PAIRS</span><span id="8ec5" class="lk ll iq lg b gy lz ln l lo lp">PAIRS <strong class="lg ir">=</strong> tab_doc_tab(TFIDF_cluster, DOC)</span></pre><p id="279e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们运行聚类函数并显示余弦相似性测试的结果。我们使用scipy模块的scipy.cluster.hierarchy函数来创建我们的聚类图的框架，并使用matplotlib来显示下面的实际聚类。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="99b7" class="lk ll iq lg b gy lm ln l lo lp"><em class="lq"># Create Clusters</em><br/><strong class="lg ir">def</strong> hca(sims, linkage_method<strong class="lg ir">=</strong>'ward', color_thresh<strong class="lg ir">=</strong>.3, figsize<strong class="lg ir">=</strong>(10, 10)):<br/>    tree <strong class="lg ir">=</strong> sch<strong class="lg ir">.</strong>linkage(sims, method<strong class="lg ir">=</strong>linkage_method)<br/>    labels <strong class="lg ir">=</strong> list(DOC<strong class="lg ir">.</strong>title<strong class="lg ir">.</strong>values)<br/>    plt<strong class="lg ir">.</strong>figure()<br/>    fig, axes <strong class="lg ir">=</strong> plt<strong class="lg ir">.</strong>subplots(figsize<strong class="lg ir">=</strong>figsize)<br/>    dendrogram <strong class="lg ir">=</strong> sch<strong class="lg ir">.</strong>dendrogram(tree, <br/>                                labels<strong class="lg ir">=</strong>labels, <br/>                                orientation<strong class="lg ir">=</strong>"left", <br/>                                count_sort<strong class="lg ir">=True</strong>,<br/>                                distance_sort<strong class="lg ir">=True</strong>,<br/>                                above_threshold_color<strong class="lg ir">=</strong>'.75',<br/>                                color_threshold<strong class="lg ir">=</strong>color_thresh<br/>                               )<br/>    plt<strong class="lg ir">.</strong>tick_params(axis<strong class="lg ir">=</strong>'both', which<strong class="lg ir">=</strong>'major', labelsize<strong class="lg ir">=</strong>14)</span><span id="a9cb" class="lk ll iq lg b gy lz ln l lo lp">hca(PAIRS<strong class="lg ir">.</strong>cosine, color_thresh<strong class="lg ir">=</strong>1)</span></pre><figure class="lb lc ld le gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="gh gi ml"><img src="../Images/285a930f1f012c6f164cc1ffe8420cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvjWmbNi5IQ6aTOrN-28Hw.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">作者图片</p></figure><p id="77eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的图片是我们看到每个作品的聚类以及每个作品之间的紧密联系的结果。基于我们的结果，我们可以看到余弦相似性聚类提供了相当准确的结果。有三大类，我称之为西方政治哲学、美国政治哲学和共产主义政治哲学。每个文本都与相似的文本适当地聚集在一起，来自相同作者的一些文本更紧密地聚集在一起，例如恩格斯的文本。</p><p id="39af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论:<br/>我们从。txt文件，创建了各种文本表，如Library、Token、Vocab和TFIDF表。一旦我们得到了我们需要的所有表格，我们运行余弦相似性度量，并根据相似的作品对文本进行聚类。这个项目最终能够找到我们所期望的三个集群，给定我们对作者的了解，以及每个政治文本的主题。</strong></p><p id="0f3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这个项目的完整代码可以在我的GitHub页面上找到:</strong><a class="ae mm" href="https://github.com/nbehe/NLP_texts/blob/main/Beheshti_project_code.ipynb" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/nbehe/NLP _ texts/blob/main/Beheshti _ project _ code . ipynb</a></p><p id="e834" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*这个项目是我的文本分析课程的一个大项目的一小部分。我将为该项目的其他部分撰写额外的文章，其中我们将研究其他非监督模型，如主成分分析、主题建模和情感分析。与文本分析一样，尤其是本文，大部分代码都致力于创建运行这些模型所需的表。我选择保留所有代码，而不是只保留一部分，这样其他人就可以看到我的所有步骤。来自这个项目的代码是我自己写的，由讲师提供的，以及通过课程材料提供的作品的混合物。*</p></div></div>    
</body>
</html>