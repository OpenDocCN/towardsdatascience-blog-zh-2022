<html>
<head>
<title>Multi-Dimensional Decision Boundary: Why Current Approaches Fail and How to Make It Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多维决策边界:当前方法失败的原因及如何使其有效</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-dimensional-machine-learning-decision-boundary-how-to-get-it-to-work-7122dca3b3a#2022-01-13">https://towardsdatascience.com/multi-dimensional-machine-learning-decision-boundary-how-to-get-it-to-work-7122dca3b3a#2022-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="c394" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">多维决策边界:当前方法失败的原因及如何使其有效</h1></div><div class=""><h2 id="1440" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">决策边界是模型评估的一个非常重要的可视化工具。了解如何让它处理复杂的数据集。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1a3e05d10859560125d4014f5219988b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVWVXOFN5cIOLCFfvOVgyQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">丹尼尔·林肯在<a class="ae ky" href="https://unsplash.com/s/photos/person?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9118" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有方法在玩具数据集(如Iris数据集)上都运行良好，看起来很棒。然而，当您在真实数据集上尝试您的方法时，事情就崩溃了。决策边界或决策面就是这种情况的一个很好的例子。</p><p id="ad3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策边界或决策表面是理解机器学习分类器结果的一种很好的方式。在开源软件包网站上有各种各样的例子来解释如何制定决策界限。这些例子是理解这个概念的一个很好的来源，然而，它们在实践中是不够的。</p><p id="d580" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种不足的主要原因是现有的例子是玩具数据集，如虹膜数据集，其维度有限。在实际应用中，数据集有非常多的维度。</p><p id="b144" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个故事中，我将展示一种多维决策边界的方法。</p><p id="0c0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先回顾一下决策边界或决策面的目标。</p><h1 id="ef2e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">决策边界或决策面的目标</h1><p id="1ac5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">机器学习分类器的视觉理解非常重要。有许多基于性能的可视化技术，如ROC曲线或精度和召回图。然而，基于性能的可视化不足以理解机器学习分类器。</p><p id="5030" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，在许多应用中，仅基于性能的评估并不是最佳的。例如，机器学习分类器可以具有极好的ROC，然而，底层分类器模型可能太复杂。模型的复杂性不能用性能指标来判断。</p><p id="367d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策边界或决策面有助于我们通过以下方式更好地理解机器学习模型:</p><ul class=""><li id="c53d" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">它帮助我们理解底层模型的复杂性。复杂的决策边界或决策面可能意味着模型复杂或过度拟合。</li><li id="15f0" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">它告诉我们，在预测新数据时，我们应该有多大的信心。如果有太多的点非常接近决策边界的边缘，那么对新的和看不见的数据的预测就会出错。</li></ul><h1 id="5598" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">当前确定多维决策边界的方法</h1><p id="ad51" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以下是当前用于制定多维决策边界的不同方法。</p><h2 id="2023" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">取所有特征的两两组合</h2><p id="4188" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这种方法中，采用所有特征的成对组合。在下面的例子中，数据有4个特征-萼片宽度、萼片长度、萼片宽度、花瓣长度。当我们对所有特征进行成对组合时，这给了我们6个可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/856e47b20588b3f5348ffb9c5e346c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jfy7H36vPCuJEQjNGTkMjA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/tree/plot _ iris _ DTC . html # sphx-glr-auto-examples-tree-plot-iris-DTC-py</a></p></figure><p id="a53c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用网格方法生成决策边界。对于每个可视化，X轴和Y轴被分成小方框。这就形成了一个网格。例如，在下图中，网格有9个(在X轴上)* 12个(在Y轴上)= 108个框。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/0ba1c229436e3e7367548891d8ca34f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rAzS_seSMsL_UXGN6TuSgg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网格法—作者图片</p></figure><p id="5e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个框可以代表萼片长度和萼片宽度的值。对于每个盒子，然后使用训练的分类器来预测类别。框的颜色基于预测的类。</p><p id="0ad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦获得彩色网格，来自测试数据集的数据点就绘制在网格上。</p><h2 id="dafa" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">使用主成分分析(PCA)</h2><p id="b37c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这种方法中，主成分分析用于将多维数据集简化为2D数据集。如上所述，网格方法用于将2D空间切割成许多盒子。一个<strong class="lb iu"> <em class="nu">新的</em> </strong>机器学习分类器以X为PCA 2D值，Y为类进行训练。</p><p id="67dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，该机器学习分类器将预测网格中每个点的类别。然后，预测的类可以用于制作网格栅格框的颜色。下图给出了一个说明性的例子。</p><p id="d10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦获得彩色网格，来自测试数据集的数据点就绘制在网格上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/67dc4539cb4cc07294241ecd0a4250b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Umh5lxUQRewNpvlC6eLLBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">主成分分析和网格方法—图片由作者提供</p></figure><h1 id="40a4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">够了艾瑞斯。欢迎来到现实世界。</h1><p id="803d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正如您到目前为止所看到的，决策边界就像在Iris数据集上散步一样。现在让我们看看上述方法是如何用于真实世界的多维数据集的。让我们来看一个电信公司客户的数据集。该数据集包含人口统计信息、服务、账单信息以及客户是否有过交易。</p><p id="8a11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PS:数据集引用在文末说明</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/ac030ca853e8820faa69872030f94674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lmcy6-LBYbmu4gbenI06jQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">电信客户流失数据集(图片由作者提供)</p></figure><p id="e6d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据大约有20个字段。几乎50%的字段是分类的(非数字)。机器学习的任务是开发一个分类器来预测客户流失。现在让我们看看上述决策边界的方法是否可行。</p><h2 id="6ea0" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">对所有特征进行成对组合的方法对于高维数据不是可行的方法。</h2><p id="c515" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">成对组合方法仅适用于少量维度。对于大量的特征，例如我们的电信数据集中的特征，它将悲惨地失败。会产生太多的可视化效果，同时查看许多可视化效果会变得很麻烦。此外，您将错过所有分类特征，因为这种方法只对数字特征有效。</p><h2 id="df09" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">PCA方法的问题在于…PCA技术本身。</h2><p id="c9fa" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">PCA方法的问题是PCA技术的使用。PCA技术通过基于最高变化分离点来工作。这意味着从数据的角度来看，PCA空间中的两个相邻点可能非常不同。</p><p id="53f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解为什么PCA不适合，让我们比较一下PCA和TSNE。TSNE(t-分布式随机邻居嵌入)也是一种降维技术。下图显示了五氯苯甲醚和SNE霸王龙之间的对比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/fab5c737984616f21e74cc2d2d74d58c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9f6P_nTpteNqcTPHZDhymA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA vs TSNE(图片由作者提供)</p></figure><p id="1f42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以观察到两种可视化效果看起来完全不同。主成分分析只有两种聚类结构，而SNE霸王龙有多种聚类结构。该数据集根据人口统计、服务和账单信息拥有各种各样的客户。如主成分分析所示，只有两个集群结构是不现实的。另一方面，T-SNE是电信客户数据集的更现实的表示。</p><p id="beb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">区别的原因是PCA本质上是通过基于最高变化场<em class="nu">将<strong class="lb iu"/><strong class="lb iu">点</strong>尽可能远地<strong class="lb iu">分离来工作的。</strong></em> TSNE基本上是通过<strong class="lb iu">分组</strong> <strong class="lb iu">点</strong> <strong class="lb iu">根据点的特性尽可能接近</strong>来工作的。</p><p id="3363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA是一种很好的降维方法，但如果你想重用结果来制定决策边界，它就不好了。T-SNE是一种更好的决策边界方法，因为我们希望实际数据中接近的点在决策表面上也接近。</p><h2 id="848e" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">要不要网格…这是个问题</h2><p id="80cd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这里是一个网格着色工作原理的回顾。首先，我们基于对应于训练数据集(如X)和实际标签(如Y)的TSNE点创建分类器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/808524ee402ed4a22bd836cc60f0116d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIkgj2Wjf8y0Kr11E09VMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网格步骤1(图片由作者提供)</p></figure><p id="5f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们训练了分类器，我们就可以用它来预测网格中某个点的颜色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/7585ea0b47e166d2817fddb2de6d40d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*voEe13fPkQ_vrqQvhcvhwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网格步骤2(图片由作者提供)</p></figure><p id="fb8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法适用于简单的数据集。然而，对于复杂的数据集，mesh grid并不理想。下面显示的是两个网格。左边的是基于逻辑回归分类器的颜色。右边的是基于决策树分类器的颜色。这些点对应于测试数据集。点的颜色基于流失预测。蓝点是测试数据集中没有搅动的客户。绿色的点是测试数据集中的顾客。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/0df2fe39b15883d4705e59ae95dbde17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJfC8d1uw71W4WuS3EQ4eQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策面(图片由作者提供)</p></figure><p id="0c2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看决策面，它给人的印象是，我们预测客户流失的分类器非常不准确，因为决策面和测试点之间有很多差异。然而，预测流失的分类器具有83%的ROC。那么，问题出在哪里？</p><p id="5812" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题在于预测网格颜色的分类器。数据集太复杂了，不能只用两个TSNE点来预测客户流失与否。</p><p id="a40d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，网格方法不适用于非常复杂和高维的数据集。</p><h1 id="380b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">多维数据集的决策边界方法</h1><p id="9ab1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">因此，基于目前的观察，这里给出了我们可以适用于多维数据集的方法。</p><ul class=""><li id="3788" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">不要使用成对散点图方法。</li><li id="d50c" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">不要使用PCA。用TSNE代替</li><li id="4b84" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">放下网格。专注于创建一个正确的决策面，而不是一个带有网格的不正确的决策边界。如果我们直接使用训练数据集来生成决策面，就可以做到这一点。</li></ul><p id="82b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是基于训练数据集<strong class="lb iu">的<strong class="lb iu">决策面</strong>。</strong>没有使用网格。颜色基于直接获取训练数据集的TSNE点及其相应的标签(流失或不流失)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/ae6b804f0436d9cf2848fe45907e7d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CN8fJJACv_ieKgfioHJMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于训练数据的决策图(图片由作者提供)</p></figure><p id="ee90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以叠加我们的测试点，得到如下所示的最终结果。回想一下，蓝点是测试数据集中没有搅动的客户。绿色的点是测试数据集中的顾客。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/165b1df33ae56782e14d3a32ab02e579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7uoaLiPOWRH1bGBExGP9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有测试数据覆盖的决策表面(图片由作者提供)</p></figure><p id="f8a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是上面的同一个图像以及所有的颜色代码注释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/77817c7e1fdee52215174b3a7d106f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbDcK2E9AXQyqaC6J6iH9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有测试数据覆盖的决策表面(图片由作者提供)</p></figure><p id="0b0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这看起来好多了，我们可以理解为什么分类器的准确率是83%。大多数客户流失的测试点都落在预测流失的决策面上。此外，客户没有流失的大多数测试点都落在没有预测流失的决策面中。</p><p id="78cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法适用于复杂的多维数据集。但是，有一些限制:</p><ul class=""><li id="6c8a" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">训练数据集需要非常大(至少超过10000行)。原因是我们使用从训练数据集得到的决策表面，而不是网格方法。因此，这需要大量的训练数据来制作一个好的决策表面。</li><li id="05f2" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">测试数据集应该是训练数据集的子集。尽可能地去除异常值。这确保了测试数据的TSNE点被正确地映射到2D网格。</li><li id="ab0d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">一般来说，TSNE在非稀疏数据上工作得很好。所以这种方法通常用于非稀疏数据集。</li></ul><p id="7985" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你喜欢这篇文章，现在准备在复杂和多维数据集上做决策表面。</p><p id="79e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为<strong class="lb iu">的额外奖励</strong>，这里有不同颜色和风格的决策面。享受吧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/9478fa592aa2265adf3d8ade585d77ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GNoYsgIspsAWkUhMN66VMQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策表面的不同颜色和样式(图片由作者提供)</p></figure><h1 id="766a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">数据集引用</strong></h1><p id="7a09" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">电信数据集可在此获得。允许与引文一起使用</p><p id="7614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/blastchar/telco-customer-churn</a></p><p id="6109" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Blastchar (2018)。Kaggle数据集。[<a class="ae ky" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">www.kaggle.com</a>]<a class="ae ky" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/blastchar/telco-customer-churn</a></p><h1 id="0aee" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">附加资源</strong></h1><h1 id="1c20" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">网站(全球资讯网的主机站)</h1><p id="42f5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">你可以访问我的网站进行零编码分析。<a class="ae ky" href="https://experiencedatascience.com/" rel="noopener ugc nofollow" target="_blank">https://experiencedatascience.com</a></p><p id="465b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每当我发布一个新的故事，请订阅保持通知。</p><div class="oe of gp gr og oh"><a href="https://pranay-dave9.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">每当Pranay Dave发表文章时，您都会收到电子邮件。</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">每当Pranay Dave发表文章时，您都会收到电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">pranay-dave9.medium.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov ks oh"/></div></div></a></div><p id="bbf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以通过我的推荐链接加入Medium</p><div class="oe of gp gr og oh"><a href="https://pranay-dave9.medium.com/membership" rel="noopener follow" target="_blank"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd iu gy z fp om fr fs on fu fw is bi translated">通过我的推荐链接加入Medium—Pranay Dave</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">pranay-dave9.medium.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov ks oh"/></div></div></a></div><h2 id="cb82" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">Youtube频道</h2><p id="e3da" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是我的YouTube频道<br/><a class="ae ky" href="https://www.youtube.com/c/DataScienceDemonstrated" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/c/DataScienceDemonstrated</a>的链接</p></div></div>    
</body>
</html>