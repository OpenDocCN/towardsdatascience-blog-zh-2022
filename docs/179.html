<html>
<head>
<title>Synthetic Data with Gumbel-Softmax Activations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Gumbel-Softmax激活的合成数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/synthetic-data-with-gumbel-softmax-activations-49e990168565#2022-01-06">https://towardsdatascience.com/synthetic-data-with-gumbel-softmax-activations-49e990168565#2022-01-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="04c3" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">Gumbel-Softmax激活的合成数据</h1></div><div class=""><h2 id="2bc0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">高质量分类数据合成实践</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3e5304bcc38473ab8460b37e365aece3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjSI_zi0T-itKEW3US7gYA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/s/photos/categories?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@v2osk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> v2osk </a>拍摄的照片</p></figure><p id="b605" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在分类中，一个中心问题是如何有效地从离散数据格式(分类或顺序特征)中学习。大多数数据集给我们带来了这个问题，所以我想公平地说，几乎所有的数据科学家都在某个时候面临过这个问题，对吗？)。</p><p id="02f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管存在不同的技术可以减轻它，但这是一个需要妥协才能解决的问题。选择留给了数据科学家，无论是否意识到，一个糟糕的选择会对结果产生负面影响。</p><p id="a090" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Gumbel-Softmax是最近推出的一个非常有趣的激活层，可以帮助我们做到这一点。我写这篇文章是为了演示它试图解决的问题，向您介绍Gumbel-Softmax的核心思想，并帮助您使用开源代码在自己的管道中实现它。如题所示，我们将采用数据合成方法，并在GANs上进行试验。</p><p id="f73c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们开始吧！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="dd6c" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">分类特征合成的快速回顾</h1><p id="8947" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在日益以数据为中心的<a class="ae kv" href="https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf" rel="noopener ugc nofollow" target="_blank">人工智能社区中，合成数据正成为一个热门话题。如果这个话题在某个时候引起了你的注意，那么你可能也听说过不到10年前引入的</a><a class="ae kv" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成对抗网络</a>，但从那以后在输出质量和应用范围方面取得了很大进展。事实上，有大量跨各种任务的真实合成数据生成示例，例如:</p><ul class=""><li id="30fe" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nc nd ne nf bi translated"><a class="ae kv" href="https://arxiv.org/pdf/2104.00567.pdf" rel="noopener ugc nofollow" target="_blank">文本到图像</a></li><li id="3f7a" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">图像风格转换</a></li><li id="9907" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1805.11973.pdf" rel="noopener ugc nofollow" target="_blank">图形结构化数据</a></li><li id="c3de" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated"><a class="ae kv" href="https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf" rel="noopener ugc nofollow" target="_blank">时间序列数据</a></li></ul><p id="34b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1811.11264.pdf" rel="noopener ugc nofollow" target="_blank">表格数据</a>合成是另一个有趣的应用，它更加普遍，并且与本文的重点——分类数据密切相关。</p><p id="80c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练合成器之前，我们可以预处理我们的特征。在分类特征的情况下，使用一键编码将离散特征转换为1和0的稀疏块。将分类特征等符号输入转换为稀疏数组允许神经网络(NN)模型以类似于数字连续特征等非常不同的特征格式来处理数据。</p><p id="653e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个例子:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在一次性编码之前，我们的分类数据可能看起来像这样</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在应用了一键编码后，我们现在每个类别都有了新的列</p></figure><p id="331f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GAN生成器试图从真实数据中合成稀疏分类输入。然而，就像<a class="ae kv" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">多层感知器</a>一样，在没有适当的激活函数的情况下进行预测时，经常输出的是<a class="ae kv" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank">逻辑</a>，即可能看起来像这样的非归一化概率分布:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">logits输出示例</p></figure><p id="7dcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看这个。混乱，在这种情况下，我们在寻找二进制输出，不是吗？生成可用的记录需要我们推断出合理的特征值:我们必须以某种方式对逻辑进行采样(例如，使用激活度最高的类)；此外，这看起来像是GAN鉴别器识别假样本的潜在标志。我承认我不确定最后一点是否如此简单，但如果鉴别器能够根据<em class="ls">和</em>的样子(浮点向量而不是单热向量)来区分真实和合成样本，那么它将成为一个完美的鉴别器。完美的鉴别器将使发生器基于<a class="ae kv" href="https://developers.google.com/machine-learning/gan/loss" rel="noopener ugc nofollow" target="_blank">无限最小最大损失</a>更新其参数(从等式中可以看出，如果鉴别器对真实样本的估计为1，对虚假样本的估计为0，将会发生什么)。预期结果？发电机组不可能汇合！</p><p id="6c28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像Softmax和Gumbel-Softmax这样的激活函数使我们能够将logits的输出转换成易于采样的分类分布。以上示例的Softmax输出如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过应用Softmax，我们将对数转换为分类分布</p></figure><p id="ee72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们更详细地了解一下什么是Softmax和Gumbel-Softmax，以及它们对我们的合成样品的分类特征有何作用！</p><h1 id="29b4" class="ma mb iq bd mc md nn mf mg mh no mj mk jw np jx mm jz nq ka mo kc nr kd mq mr bi translated">Softmax和Gumbel-Softmax激活</h1><p id="a86a" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">Softmax是一个可微的函数族。正如我们所看到的，他们将一个logits数组映射到分类分布，即值在[0，1]范围内且总和为1的数组。然而，这些样本无法帮助我们进行梯度下降模型学习，因为它们是从随机过程中获得的(与模型参数无关)。我们的生成器执行的操作需要是可微分的和确定的，以便我们能够将梯度从一端流向另一端。</p><p id="ac96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Gumbel-Softmax (GS)是一种特殊的Softmax函数，于2016年推出(有趣的事实:巧合的是，它是由两个独立的团队同时提出的)<a class="ae kv" href="https://arxiv.org/abs/1611.00712" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae kv" href="https://arxiv.org/abs/1611.01144" rel="noopener ugc nofollow" target="_blank">【2】</a>。它的工作方式类似于Softmax的连续逼近。不是直接使用logits<a class="ae kv" href="https://en.wikipedia.org/wiki/Gumbel_distribution" rel="noopener ugc nofollow" target="_blank">Gumbel分布</a>而是在Softmax操作之前添加噪声，以便来自生成器的样本成为确定性成分(由分类分布的均值和方差参数化)和随机成分(gum bel噪声)的组合，这有助于我们在不增加过程偏差的情况下进行采样。</p><p id="32e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">温度参数，通常称为tau或lambda，定义在]0，<em class="ls"> inf </em>中，作为旋钮来定义Softmax的指数运算的基数。这个参数通常保持接近0。较高的基数接近均匀分布，接近0的基数接近真实的分类分布。</p><p id="c4bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图显示了三个不同的流试图做同样的事情，检索分类样本并计算所有模型参数的可区分损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2376405d44cb02943e85e0ec906e66d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*VO7eKMnKu0_0Z5BQc5K9hg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">流程1 —可区分和确定性采样策略(p . e . arg max)；流程2 —可区分的随机抽样(对分类分布进行抽样)；流程3——gum bel-soft max采样，可区分随机和确定性成分。图片作者，改编自<a class="ae kv" href="https://arxiv.org/pdf/1611.01144.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="5592" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在流程1中，我们使用类似于<em class="ls"> x </em>中的<a class="ae kv" href="https://en.wikipedia.org/wiki/Arg_max" rel="noopener ugc nofollow" target="_blank"> argmax </a>的<a class="ae kv" href="https://github.com/MWPainter/cvpr2019/blob/master/stitched/soft_argmax.py" rel="noopener ugc nofollow" target="_blank">软版本</a>的操作来检索单热样本(基本argmax是不可微的)。反向传播工作正常，红色操作允许梯度从样本损失流向发生器参数。不方便的部分是，argmax是一个赢家采取所有功能。最有可能的类别被采样，并且我们的合成样本很有可能永远不会包括少数类。<em class="ls">嘘！</em></p><p id="5eba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在流程2中，我们没有发现赢家采取所有行为，我们实际上可以根据我们的真实分类分布获得真实的样本。太好了！对吗？不幸的是，不，抽样过程的随机性质将不允许梯度流回我们的模型参数，这意味着没有学习。还有一些替代策略，如直通策略，通过假设采样操作的梯度不变来解决这一问题。这将允许梯度流动，但会有偏差。<a class="ae kv" href="https://arxiv.org/pdf/1611.01144.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>有更多关于这个估算器的信息。</p><p id="ae45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">流3是Gumbel-Softmax，也许你听说过<a class="ae kv" href="https://arxiv.org/pdf/1312.6114.pdf" rel="noopener ugc nofollow" target="_blank">重新参数化技巧</a>。在这里，我们可以看到它的巨大应用效果！关键是在Gumbel噪声比例保持不变的情况下，通过改变logit比例来调整logit和Gumbel噪声比例。基于对其自身逻辑的信任，模型将倾向于均匀分布(降低比率)或其分类分布的预测(增加比率)。由于节点<em class="ls"> y </em>仍然不是一个独热编码向量，我们可以通过两个计算来解决这个问题。我们使用<em class="ls"> y </em>在训练时间完成我们的梯度计算图，并在预测时间从中检索分类样本。鉴别器将对这种一热样品进行操作，但梯度将通过<em class="ls"> y </em>操作。</p><p id="5fa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经看到了Gumbel-Softmax激活背后的理论，让我们转向实际部分，分类数据综合！</p><h1 id="90b0" class="ma mb iq bd mc md nn mf mg mh no mj mk jw np jx mm jz nq ka mo kc nr kd mq mr bi translated">综合分类特征比较</h1><p id="055f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">为了生成合成数据，我们将使用<a class="ae kv" href="https://github.com/ydataai/ydata-synthetic" rel="noopener ugc nofollow" target="_blank"> YData合成包</a>。在最近的更新中，所有常规数据合成器都引入了<a class="ae kv" href="https://github.com/ydataai/ydata-synthetic/blob/dev/src/ydata_synthetic/utils/gumbel_softmax.py" rel="noopener ugc nofollow" target="_blank"> Gumbel-Softmax激活</a>。以下是基本Gumbel-Softmax层实现的一个片段:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Gumbel-Softmax层实现。请注意调用方法的返回:hard_sample是采样的one-hot输出，soft_sample是softmax分类分布</p></figure><p id="cff5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们用两个版本的<a class="ae kv" href="https://github.com/ydataai/ydata-synthetic/blob/feat/pate_methods/src/ydata_synthetic/synthesizers/regular/wgangp/model.py" rel="noopener ugc nofollow" target="_blank"> Wasserstein GAN用梯度惩罚实现</a><a class="ae kv" href="https://arxiv.org/pdf/1704.00028.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>。包含Gumbel-Softmax的标准版本和不使用它的覆盖版本。这两个版本本质上是相同的，您可以在下面的代码片段中看到:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="c4b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个简单的基本合成器覆盖移除了传递的<em class="ls"> activation_info </em>参数，这样我们就可以拥有一个没有Gumbel-Softmax激活的生成器。也就是说，没有Gumbel-Softmax生成器永远不会执行第12行。</p><p id="96e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">合成器使用<a class="ae kv" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人数据集</a>进行训练，下面您可以看到workclass功能的一些生成器样本输出(我只选择了一个功能，因为完整的样本有120列):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/31849ae6cbca3c9a79c62d9dcc778529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EE_aBKxzXoBXtaHhBpas4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">无Gumbel-Softmax激活的生成器工作类特征输出。作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/a290a5e40f7c7fea153ec273ecff15e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WoJAUwhC2xpRzUK514u96A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带有Gumbel-Softmax激活工作类特征输出的生成器。作者图片</p></figure><p id="f3f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来看看原始样本的统计分布，有和没有GS实现:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/70eea5d02688e6cbd074962f0a7637ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xOHAZej1YPziGijiYImj2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">成人分类特征的等级分布条形图。从左至右1)原始样品，2)带Gumbel-Softmax激活的WGAN-GP，3)不带Gumbel-Softmax的WGAN-GP。作者图片</p></figure><p id="f77d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望此时你能对上面的结果得出一些结论。无论如何，以下是我的:</p><ol class=""><li id="1736" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nw nd ne nf bi translated">没有一个合成样本能够很好地捕捉分类分布。但是，公平地说，模型或训练条件并没有针对这种情况进行优化，我相信这些变化会有很大的帮助。</li><li id="d21e" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nw nd ne nf bi translated">带有Gumbel-Softmax的合成器产生几乎一致的样本。</li><li id="5681" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nw nd ne nf bi translated">没有Gumbel-Softmax的合成器产生的样本接近于winner的所有行为。</li></ol><h1 id="5f6e" class="ma mb iq bd mc md nn mf mg mh no mj mk jw np jx mm jz nq ka mo kc nr kd mq mr bi translated">包扎</h1><p id="7ab5" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们总结了一些关键点，这些关键点与我们走过的Gumbel-Softmax和分类特征综合理论中的预期一致。</p><p id="b99b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">非常欢迎您使用我用来产生结果的示例笔记本，您可以在这里找到它<a class="ae kv" href="https://github.com/ydataai/ydata-synthetic/blob/dev/examples/regular/gumbel_softmax_example.ipynb" rel="noopener ugc nofollow" target="_blank"/>！如果您想尝试增强原始示例，也可以随意克隆或派生该项目并摆弄源代码。调整每个示例的Gumbel-Softmax激活的参数<em class="ls">τ</em>可以帮助我们近似真实的分类分布。</p><p id="aa23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一点，如果你想加入一群合成数据爱好者，讨论论文、数据集、模型或解决你在途中发现的一些问题，你也可以加入<a class="ae kv" href="https://syntheticdata.community/" rel="noopener ugc nofollow" target="_blank">合成数据社区</a>。我和我的同事会很高兴看到你在那里！</p><p id="f46f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我在《走向数据科学》中的第一篇文章，感谢您阅读它，我希望您发现它很有用，并且您设法学到了一些新东西。</p><h1 id="6a4a" class="ma mb iq bd mc md nn mf mg mh no mj mk jw np jx mm jz nq ka mo kc nr kd mq mr bi translated">参考资料:</h1><p id="a9e4" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">[1] C. J .马迪森，A. Mnih和Y. W. Teh，具体分布:离散随机变量的连续松弛(2016)，arXiv</p><p id="c3f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] E. Jang，S. Gu，B. Poole，Gumbel-Softmax (2016年)的分类重新参数化，arXiv</p><p id="08d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]I . gullajani，F. Ahmed，M. Arjovsky，V. Dumoulin，a .库维尔，Wasserstein GANs的改进培训(2017年)，arXiv</p></div></div>    
</body>
</html>