<html>
<head>
<title>How to Utilize Machine Learning to Automatically Detect Patterns in Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用机器学习自动检测文本中的模式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-text-with-k-means-c2953c8a9772#2022-01-09">https://towardsdatascience.com/clustering-text-with-k-means-c2953c8a9772#2022-01-09</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""><h1 id="c4ae" class="pw-post-title is it iu bd iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq bi translated">如何利用机器学习自动检测文本中的模式</h1></div><div class=""><h2 id="f5a8" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">用k-Means聚类葡萄酒评论</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/2c67156ca9afc3f15c955bd10f0c30d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*58jhGKRBDinlpgJj"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Maksym Kaharlytskyi 在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="650b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么文本聚类？</h1><p id="2923" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在上一篇文章中，我们谈到了<a class="ae kz" href="https://medium.com/data-knows-all/topic-models-with-gensim-97f358ad4151" rel="noopener">主题建模</a>或者从文档语料库中识别几个主题的方法。这里使用的方法是潜在的狄利克雷分配或LDA。在本文中，我们将执行类似的任务，但通过<strong class="lu iv">无监督的机器学习</strong>方法<strong class="lu iv">聚类</strong>。虽然方法不同，但结果是几组(或主题)彼此相关的单词。</p><p id="b1d4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于这个例子，我们将使用来自<a class="ae kz" href="https://www.kaggle.com/zynicide/wine-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的葡萄酒评论数据集。它包含了超过100，000种不同的全球葡萄酒评论。作为品尝笔记的葡萄酒描述是基于文本的变量，我们将使用它来聚类和解释结果。</p><p id="731c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">从导入所需的库开始。</p><p id="9f99" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">注</strong>:这里不打算展示文本的预处理。你可以在<a class="ae kz" href="https://github.com/broepke/TextClustering" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上看到完整的代码。还有一篇关于<a class="ae kz" href="https://medium.com/data-knows-all/text-cleaning-for-nlp-in-python-2716be301d5d" rel="noopener">文字清理</a>的完整文章可以参考。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="8959" class="my lb iu mu b be mz na l nb nc">from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_score<br/><br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span></pre><h1 id="666c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">文本矢量化</h1><p id="d728" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">向量化文本是将文本<strong class="lu iv">文档</strong>转换成数字表示<strong class="lu iv">的过程。这有不同的版本，例如<strong class="lu iv">单词袋</strong> (BoW)，以及<strong class="lu iv">术语频率-逆文档频率</strong> (TF-IDF)，我们将在这里使用。</strong></p><ul class=""><li id="c409" class="nd ne iu lu b lv mo ly mp mb nf mf ng mj nh mn ni nj nk nl bi translated"><strong class="lu iv"> BoW或TF </strong>:表示每个文档中每个单词的计数。在这种情况下，文档记录了我们所针对的列的数据集中的观察结果。</li><li id="e4e4" class="nd ne iu lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">TF-IDF :它不是只计算单词的数量，而是反过来，给那些出现频率较低的单词更高的权重。常见单词具有较低的权重，而可能更特定于领域且出现较少的单词将具有较高的权重。</li></ul><p id="320c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">创建<strong class="lu iv"> TF-IDF </strong>，创建矢量器的一个实例，然后<code class="fe nr ns nt mu b">fit-transform</code>数据框的列是非常容易的。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="7aa6" class="my lb iu mu b be mz na l nb nc">vectorizer = TfidfVectorizer()<br/>X = vectorizer.fit_transform(df['description_clean'])</span></pre><h1 id="6669" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">确定最佳聚类数</h1><p id="ce00" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我上面提到聚类是一种<strong class="lu iv">无监督</strong>的机器学习方法。无监督意味着我们的数据集中没有告诉我们正确答案的信息；这通常被称为<strong class="lu iv">标记数据</strong>。在我们的例子中，我们不知道文本中有多少不同类型的酒或不同的主题。然而，仅仅因为我们不知道这些信息，并不意味着我们不能找到正确的集群数。</p><p id="f396" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">有了聚类，我们需要初始化几个<strong class="lu iv">聚类中心</strong>。这个数字被输入到模型中，然后在结果输出后，具有数据知识的人可以解释这些结果。然而，有一些方法可以评估哪一个是正确的聚类中心数量，我将介绍两种常用的方法。</p><h1 id="05fc" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">肘法</h1><p id="7648" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">第一种称为<strong class="lu iv">肘法</strong>。该名称源自运行此分析后的图形外观。理想情况下，我们在寻找曲线开始变平的点。这种方法使用<code class="fe nr ns nt mu b">inertia</code>来确定簇的数量。惯性是从每个点到聚类中心的距离的平方的总和。我们可以对一系列不同的聚类值进行计算，绘制它们，并寻找<strong class="lu iv">弯头</strong>。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="0664" class="my lb iu mu b be mz na l nb nc">Sum_of_squared_distances = []<br/>K = range(1,10)<br/>for k in K:<br/>    km = KMeans(init="k-means++", n_clusters=k)<br/>    km = km.fit(X)<br/>    Sum_of_squared_distances.append(km.inertia_)<br/><br/>ax = sns.lineplot(x=K, y=Sum_of_squared_distances)<br/>ax.lines[0].set_linestyle("--")<br/><br/># Add a vertical line to show the optimum number of clusters<br/>plt.axvline(2, color='#F26457', linestyle=':')<br/><br/>plt.xlabel('k')<br/>plt.ylabel('Sum of Squared Distances')<br/>plt.title('Elbow Method For Optimal k')<br/>plt.show()</span></pre><p id="2277" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在绘制数据后，肘形并不明显，但最佳近似是在2个集群处，我们在曲线中看到一个<em class="nu">轻微扭结</em>。我画了一条垂直线来识别它。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nv"><img src="../Images/00d94925d94559321231ed1084f37d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F0Q0jC5XzMro23GD.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><h1 id="e16f" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">剪影分数</h1><p id="ee0e" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">另一种计算最佳聚类中心的方法是<strong class="lu iv">轮廓系数</strong>。使用每个样本的平均聚类内距离和平均最近聚类距离来计算轮廓系数。换句话说，样本和样本不属于的最近聚类之间的距离。</p><p id="b2cb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">最好的值是<code class="fe nr ns nt mu b">1</code>，最差的是<code class="fe nr ns nt mu b">-1</code>。<code class="fe nr ns nt mu b">0</code>附近的值表示重叠的簇。负值通常表示样本被分配到了错误的分类，因为不同的分类与其被分配的分类更相似。让我们也为不同的聚类中心值计算这些分数。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="d8a7" class="my lb iu mu b be mz na l nb nc">def get_silhouette_score(X, k):<br/>    for n_clusters in range(2, k):<br/>        clusterer = KMeans(init="k-means++", n_clusters=n_clusters, random_state=42)<br/>        y = clusterer.fit_predict(X)<br/><br/>        message = "For n_clusters = {} The average score is: {}"<br/>        print(message.format(n_clusters, silhouette_score(X, y)))<br/><br/>get_silhouette_score(X, 10)</span></pre><pre class="nw mt mu mv bn mw mx bi"><span id="dcce" class="my lb iu mu b be mz na l nb nc">For n_clusters = 2 The average score is: 0.00821919113279018<br/>For n_clusters = 3 The average score is: 0.006522933295313797<br/>For n_clusters = 4 The average score is: 0.006237960319271207<br/>For n_clusters = 5 The average score is: 0.006266850309331783<br/>For n_clusters = 6 The average score is: 0.006381665959703946<br/>For n_clusters = 7 The average score is: 0.005549433908077499<br/>For n_clusters = 8 The average score is: 0.005962146586290015<br/>For n_clusters = 9 The average score is: 0.00632540099660495</span></pre><p id="f1e2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">根据定义，最接近<code class="fe nr ns nt mu b">1</code>的数字是最好的，在我们的例子中是<code class="fe nr ns nt mu b">2</code>簇。但是，这些值接近于零，这意味着我们的聚类有很高的重叠。</p><p id="3c8b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">虽然这两种方法都不能让我们理想地了解集群的数量，但是我们应该使用，两者都指向<code class="fe nr ns nt mu b">2</code>作为最佳值。</p><h1 id="34c6" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">k-均值聚类</h1><p id="903e" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">好吧！我们准备好构建模型并检查结果。这个过程非常简单，我们将重复上面所做的大部分工作，但是只对我们选择的集群数量进行一次运行。</p><p id="1ac4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于这个例子，我们将使用k-Means。k-Means是最常见的聚类算法之一，如果不是最常见的话。通常，k-Means会随机初始化聚类中心，然后迭代，直到找到理想的位置。指定<code class="fe nr ns nt mu b">init="k-means++"</code>意味着我们将使用k-means++算法来初始化集群中心，这是2007年提出的一种减少随机初始化问题的方法。首先，我建议使用这个工具，并稍微阅读一下它是如何工作的。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="7fbb" class="my lb iu mu b be mz na l nb nc"># Set the number of clusters<br/>k = 2<br/># Vectorize the text<br/>vectorizer = TfidfVectorizer()<br/>X = vectorizer.fit_transform(df['description_clean'])<br/># Fit our Model<br/>model = KMeans(init="k-means++", n_clusters=k, max_iter=25, n_init=1)<br/>model.fit(X)</span></pre><p id="e3bf" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">您可以将聚类分配保存为数据框中的新列，其中包含聚类编号，以供将来参考。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="a163" class="my lb iu mu b be mz na l nb nc"># Get the cluster labels<br/>clust_labels = model.predict(X)<br/>cent = model.cluster_centers_<br/><br/>kmeans_labels = pd.DataFrame(clust_labels)<br/>df.insert((df.shape[1]),'clusters',kmeans_labels)</span></pre><p id="18d9" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">最后，让我们构建一个快速数据框，显示两个集群中的前<code class="fe nr ns nt mu b">15</code>个单词，看看我们得到了什么。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="5c60" class="my lb iu mu b be mz na l nb nc">order_centroids = model.cluster_centers_.argsort()[:, ::-1]<br/>terms = vectorizer.get_feature_names_out()<br/><br/>results_dict = {}<br/><br/>for i in range(k):<br/>    terms_list = []<br/><br/>    for ind in order_centroids[i, :15]:  <br/>        terms_list.append(terms[ind])<br/><br/>    results_dict[f'Cluster {i}'] = terms_list<br/><br/>df_clusters = pd.DataFrame.from_dict(results_dict)<br/>df_clusters</span></pre><pre class="nw mt mu mv bn mw mx bi"><span id="19af" class="my lb iu mu b be mz na l nb nc">Cluster 0   Cluster 1<br/>0   pineapple      cherry<br/>1      flavor      flavor<br/>2     acidity         dry<br/>3     vanilla  blackberry<br/>4       fruit      tannin<br/>5         oak        cola<br/>6       crisp   raspberry<br/>7        pear     currant<br/>8       apple        good<br/>9       peach        rich<br/>10       lime        soft<br/>11     butter       spice<br/>12      toast        show<br/>13      sweet         oak<br/>14      lemon       sweet</span></pre><p id="4cd5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">看看集群<code class="fe nr ns nt mu b">0</code>，我们看到的单词通常与<strong class="lu iv">白</strong>酒相关，集群<code class="fe nr ns nt mu b">1</code>与<strong class="lu iv">红</strong>酒相关。</p><h1 id="f1e8" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">预测新文档</h1><p id="9224" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">接下来，我们可以看到该模型如何对过去没有的新葡萄酒评论进行聚类。我用白葡萄酒和红葡萄酒可能与之相关的词造了几个句子。</p><pre class="kk kl km kn gu mt mu mv bn mw mx bi"><span id="4e85" class="my lb iu mu b be mz na l nb nc">new_docs = ['Rich deep color of oak and chocolate.',<br/>            'Light and crisp with a hint of vanilla.',<br/>            'Hints of citrus and melon.',<br/>            'Dark raspberry and black cherry flavors.']<br/><br/>pred = model.predict(vectorizer.transform(new_docs))<br/>print(pred)</span></pre><pre class="nw mt mu mv bn mw mx bi"><span id="684c" class="my lb iu mu b be mz na l nb nc">[1 0 0 1]</span></pre><p id="4b5b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">正如我们所料，红酒被归类为<code class="fe nr ns nt mu b">1</code>，白酒被归类为<code class="fe nr ns nt mu b">0</code>！你可以试试其他的弦，看看它的表现如何。</p><h1 id="552a" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="c442" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">第一次在这个数据集上执行聚类时，我真的被震惊了。显而易见，通过<code class="fe nr ns nt mu b">2</code>聚类，算法可以识别和聚类红葡萄酒和白葡萄酒。这感觉有点像魔术，但最终，这只是数学！享受这个过程。它功能强大，可以帮助你识别文本中相关的主题组！</p><p id="39aa" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，可以考虑报名成为一名媒体成员。一个月5美元，让你可以无限制地访问成千上万篇文章。如果你使用<a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener">我的链接</a>注册，我会赚一小笔佣金，不需要你额外付费。</p></div></div>    
</body>
</html>