<html>
<head>
<title>I-Scores: Intuition about the Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">I-Scores:对算法的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/i-scores-intuition-about-the-algorithm-56717330077a#2022-01-25">https://towardsdatascience.com/i-scores-intuition-about-the-algorithm-56717330077a#2022-01-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="b8da" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">I-Scores:对算法的直觉</h1></div><div class=""><h2 id="fe8f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于插补分数算法的一些细节，一种选择最佳插补的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4cd47b6bb4f1358aee0c63867c31354b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vX7FlEQ4A5zQWBIf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@pgreen1983?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">保罗·格伦</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="cb86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">撰稿人:<a class="lv lw ep" href="https://medium.com/u/ca780798011a?source=post_page-----56717330077a--------------------------------" rel="noopener" target="_blank">杰佛里·纳夫</a>，<a class="lv lw ep" href="https://medium.com/u/1e04db6f8e30?source=post_page-----56717330077a--------------------------------" rel="noopener" target="_blank">梅塔莉娜·斯波</a>，<a class="lv lw ep" href="https://medium.com/u/f562dceaeb63?source=post_page-----56717330077a--------------------------------" rel="noopener" target="_blank">洛里斯·米歇尔</a></p><p id="13e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据一些读者的建议，这篇文章提供了关于我们的I-Scores算法的更多细节，该算法在早先的帖子中介绍过。</p><p id="7e62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">评估插补方法质量的一个常用评分标准是均方根误差(RMSE)，它要求知道缺失值背后的真实值，编码为NA..这在模拟环境中肯定是可行的，但在数据确实缺失的真实场景中却不可行。</p><p id="981d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与常用的RMSE相比，I-Score看起来几乎不可思议:它不需要访问NA背后的真实数据，不需要屏蔽数据(人为地将观察值设置为NA ),并且在没有完整病例(没有NA的观察值)时也有效。尽管如此，它避免了RMSE和其他绩效指标遇到的陷阱，即使它们被允许使用真实的基础数据。当然，付出的代价是方法复杂性的增加。尽管如此，尽管实现细节可能很快变得相当复杂，但主要原理非常简单:我们使用<a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8">kull back-lei bler Divergence</a>(KL-Divergence)将估算分布与数据集的完全观察部分的分布进行比较。如果插补是完美的(好的)，则偏差应该为零(接近零)，如果插补不好，则偏差应该很大。</p><p id="adde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们依次关注I-Scores算法的<strong class="lb iu"> 3个主要步骤</strong>。下图提供了对它们的直观总结:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/eedd092cc89494b3f8faae96e3a94327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMuaPgX3JAPk18vhBAlTWw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="a963" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">步骤1:找到足够多的完整案例</h1><p id="3255" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">在大多数合理规模的数据集中，很难找到完整的案例。原因很简单:假设数据集中的每个值都随机缺失(完全随机缺失，MCAR ),概率为5%。不是很多，对吧？但是如果你在20个维度上有500个观察值，那么完整案例的预期数量只有(1–0.05)⁰*500，大约等于179。</p><p id="d929" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们采用一个简单的方法来解决这个问题:<em class="mv">随机投影</em>。虽然在20维中可能没有太多的完整案例，但在较低维中观察到更多完整案例的机会更大。这就像将几盏聚光灯照射到数据空间的不同部分。当然，这种解决方案是有代价的，因为看投影会丢失信息。但是，应用足够多的投影通常可以解决这个问题。</p><h1 id="7edb" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">步骤2:KL-散度估计</h1><p id="0549" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">出于我们的目的，我们需要两个密度f，h的<em class="mv">负</em> KL散度的以下表示，其中f是完全观察分布的密度，h是插补分布的密度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/bf2a3bdaa161f353ec17d9800fb0ff59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lUzkzwV6iOnbFv3csA1U3w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="394d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获得一般分布的KL散度是困难的:我们需要估计观察分布和估算分布之间的密度比(根据上面的等式)。因此，我们使用一个技巧:通过使用分类器来估计完全观察的分布和估算的分布之间的密度比。更具体地说，我们</p><p id="f472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1)给完全观察的观察值分配标签1；<br/> 2)给估算的观测值分配一个标签0，</p><p id="4b0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且估计被标注为1的概率，我们姑且称这个估计量为p(例如我们建议使用随机森林(RF))。这个估计的概率p通过p/(1-p)提供了密度比f/g估计，基于此我们获得了KL散度的估计。</p><p id="a257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么会这样？想象p是完美的估计，即观察到1的真实条件概率。它由贝叶斯公式给出为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/42039e54393d517272f2588a369301c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w2H7-TjlYsUwFuN_EZQRcA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="87d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们假设观察到两个类的概率是1/2。这实际上是在I-Scores算法中强制执行的。那么密度比可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/805d22d4b432a82d085853243abd2868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*3bB5ntrtFG6tD8LijQj0Vg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7413" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，很好地近似类别1的概率的估计p也应该导致密度比的良好估计。</p><p id="4214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个巧妙的小技巧实际上已经被使用了很多。它已经在2003年被《随机森林》的发明者雇佣，并在<a class="ae ky" href="https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f" rel="noopener">甘文学</a>中崭露头角。</p><h1 id="258f" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">第三步:形成一个简洁的单一I-Score</h1><p id="76db" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">然后，根据多个随机投影的平均值，从估计的KL偏差中构建插补的最终得分。选择分数的方向，使得较高的值表示较好的插补性能。如果有多个插补可供选择，可以对每个插补应用I-Scores算法，并选择得分最高的一个。因此，I-Scores可以指导选择手头数据集的最佳插补。</p><h1 id="ece1" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated"><strong class="ak">结论</strong></h1><p id="7b9a" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ms lk ll lm mt lo lp lq mu ls lt lu im bi translated">这篇文章希望能阐明I-Scores算法的内部工作原理。</p><p id="3b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，该算法采用数据的随机投影，并在这些投影上，将估算分布h与具有估计(负)KL散度的完全观测分布f进行比较。因此，好的插补将得到高值，而坏的插补将得到低值。</p><p id="b320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在获得分数的过程中，我们使用了一个方便的技巧，即KL散度的估计可以从概率估计中获得，这在许多情况下都是有用的。</p><p id="aca5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然，我们遗漏了一些问题。最重要的一点是，在随机缺失(MAR)情况下，估算分布与完全观测分布f不必相同。这是因为在MAR下，数据的观察部分可以改变其分布。然而，这是一个有点微妙的话题，我们只是在这里提到，分数仍然有效，即使数据是MAR。</p><p id="757f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="mv">免责声明和资源:</em> </strong> <em class="mv">我们意识到我们遗漏了许多细节，但希望能对算法的直觉有所澄清。</em> M <em class="mv">矿石的详细算法可以在我们的</em> <a class="ae ky" href="https://arxiv.org/abs/2106.03742" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="mv">论文</em> </strong> </a> <em class="mv">中找到，关于R-package Iscores的使用指南，连同一个小的说明性例子，可以在我们的</em><a class="ae ky" href="https://github.com/missValTeam/Iscores" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="mv">github知识库</em> </strong> </a> <strong class="lb iu"> <em class="mv">中找到。</em>T25】</strong></p></div></div>    
</body>
</html>