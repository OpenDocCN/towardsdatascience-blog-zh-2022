<html>
<head>
<title>Linear Regression basics in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的线性回归基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-in-python-for-data-scientists-16caef003012#2022-01-13">https://towardsdatascience.com/linear-regression-in-python-for-data-scientists-16caef003012#2022-01-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="16c1" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">Python中的线性回归基础</h1></div><div class=""><h2 id="6d34" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">UCL数据科学学会研讨会10:什么是线性回归、数据探索、Scikit学习实施和多元线性回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7233478f2e11edd1625e1a622dcd7845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7PakrHB6WCcS0_cA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾萨克·史密斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="168e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今年，作为UCL数据科学协会的科学负责人，该协会的目标是在整个学年举办一系列20场研讨会，涵盖的主题包括Python、数据科学家工具包和机器学习方法的介绍。每一篇文章的目标都是创建一系列的小博客，这些小博客将概述要点，并为任何希望跟进的人提供完整研讨会的链接。所有这些都可以在我们的<a class="ae ky" href="https://github.com/UCL-DSS" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中找到，该资源库将在全年更新新的研讨会和挑战。</p><p id="4ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第十次研讨会介绍了Python中的线性回归，并介绍了我们的数据科学与Python研讨会系列。这个特定的研讨会将涵盖什么是线性回归，探索数据，实施模型和模型评估。虽然亮点将在这篇博文中呈现，但完整的研讨会可以在我们的GitHub账户<a class="ae ky" href="https://github.com/UCL-DSS/linear-regression-workshop" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3313" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了我们之前的任何研讨会，可以在这里找到:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-sql-for-data-scientists-e3bb539decdf"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">数据科学家的SQL介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会工作坊9:什么是SQL，选择数据，查询数据，汇总统计，分组数据和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/git-and-github-basics-for-data-scientists-b9fd96f8a02a"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的Git和GitHub基础知识</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学研讨会8:什么是Git，创建本地存储库，提交第一批文件，链接到远程…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-plotting-with-matplotlib-in-python-6d983b9ba081"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中Matplotlib绘图简介</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL数据科学学会研讨会7:创建一个基本的图表，在同一图表上绘制不同的信息…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="775d" class="mw mx it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">什么是线性回归？</h1><p id="96c6" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">线性回归是一种统计方法，用于模拟两个或更多数量之间的相关性或关系。这样做的目的是能够更好地理解现有的关系，或者能够预测我们目前没有数据的点的行为。</p><p id="ff58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一组数据点x和y，我们可以将我们要建模的线的方程写成:</p><blockquote class="nt nu nv"><p id="ffea" class="kz la nw lb b lc ld ju le lf lg jx lh nx lj lk ll ny ln lo lp nz lr ls lt lu im bi translated"><em class="it"> y(x) = mx + c </em></p></blockquote><p id="d6ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中y(x)是模型的预测y值，梯度(m)和y截距(c)称为拟合参数。通过使用线性回归方法(也称为最小二乘拟合)，我们可以计算两个参数的值，并绘制最佳拟合线，以实现我们更好地理解关系或找到未知点的估计值的目标。</p><p id="e9b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们必须能够计算斜率(m)和截距(c ),以给出数据的最佳拟合线。我们可以通过以下等式来实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7ccd9a74ce00d565d5275b88fee29389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/0*o9FvDu9e4yIjomEJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d864" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中x和y代表数据的平均值。然而，这通过已经实现的库变得简单，例如Scikit-Learn和Statsmodels Api，它们具有内置的线性回归功能。我们将首先展示如何在Python中实现这些公式，然后展示如何使用Scikit Learn库执行更复杂的分析。</p><h1 id="4931" class="mw mx it bd my mz ob nb nc nd oc nf ng jz od ka ni kc oe kd nk kf of kg nm nn bi translated">数据探索</h1><p id="5433" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">我们拥有的数据是一个生成的数据集，我们首先只关注X和Y两个变量来执行简单的线性回归。我们可以使用以下代码绘制数据，以了解我们可能期望看到的关系:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="c741" class="ol mx it oh b gy om on l oo op"># Generates data frame from csv file<br/>df = pd.read_csv("RegressionData.csv")</span><span id="94e5" class="ol mx it oh b gy oq on l oo op"># Turning the columns into arrays<br/>x = df["x"].values<br/>y = df["y"].values</span><span id="5a17" class="ol mx it oh b gy oq on l oo op"># Plots the graph from the above data<br/>plt.figure()<br/>plt.grid(True)<br/>plt.plot(x,y,'r.')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/52713f93822fc2b846f5a7569145dbf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4YmMVfDL3887HySE.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d8d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，X和Y变量显示了一种关系，Y变量似乎随着X的变化而线性变化。这意味着我们可以尝试用线性拟合来模拟，所以我们可以用线性回归。</p><p id="2851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们可以使用上面详述的等式，尝试根据数据计算梯度和y轴截距，如下所示:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="4525" class="ol mx it oh b gy om on l oo op"># calculating the means of the x and y data<br/>mean_x = np.mean(x) <br/>mean_y = np.mean(y)</span><span id="9df3" class="ol mx it oh b gy oq on l oo op"># calculating the slope<br/>slope = np.sum((y - mean_y)*x) / np.sum((x - mean_x)*x) <br/>print ("Gradient:", slope)</span><span id="e56e" class="ol mx it oh b gy oq on l oo op"># calculating the intercept<br/>intercept = mean_y - slope*mean_x <br/>print ("Intercept", intercept)</span><span id="dfbd" class="ol mx it oh b gy oq on l oo op">#out:<br/>Gradient: 0.9773554490236186<br/>Intercept 0.33323427076670953</span></pre><p id="14f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以用它来绘制数据的最佳拟合线:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="c88e" class="ol mx it oh b gy om on l oo op">x_max = max(x)<br/>plt.figure("") # start a new figure<br/>plt.grid(True) # add a grid<br/># generate two points for the fitted line<br/>x_points = np.linspace(0, x_max*2., 2) <br/>y_points = slope*x_points + intercept<br/>plt.plot(x, y,'r.') # plotting data as points<br/>plt.line = plt.plot(x_points, y_points, 'b-') #plotting the line of best fit<br/># setting limits for the axes<br/>plt.xlim(-10,110) <br/>plt.ylim(-10,110)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/4781197e0db313e209d3c7f87531c8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZVXpsmC-NPk0HNLG.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4a8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此我们可以看到我们在第一个数据图中确定的预期线性关系。我们还可以看到，截距约为0.3，斜率接近1，几乎是单位线性关系。</p><h1 id="cd92" class="mw mx it bd my mz ob nb nc nd oc nf ng jz od ka ni kc oe kd nk kf of kg nm nn bi translated">Scikit学习实现</h1><p id="2ce6" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">然而，更有效和有用的方法是使用Python的Scikit学习库，该库具有更深入评估线性回归的广泛功能。虽然我们已经计算了梯度<code class="fe ot ou ov oh b">m</code>和截距<code class="fe ot ou ov oh b">c</code>，但是我们可以使用Scikit Learn来完成这项工作，然后使用这个库来评估模型性能。</p><p id="49fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要为模型准备数据。为此，我们可以将模型从当前格式重新调整为数组，然后将数据拆分为训练和测试数据集。我们这样做的原因是，我们可以看到我们的模型如何适应看不见的数据，而不是让它过度适应我们所有的数据。我们可以用下面的代码做到这一点，并想象它是什么样子:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="9f10" class="ol mx it oh b gy om on l oo op"># Independant variable or features<br/>X = x.reshape(-1,1)</span><span id="9e60" class="ol mx it oh b gy oq on l oo op"># Dependant variable or labels<br/>y = y.reshape(-1,1)</span><span id="7928" class="ol mx it oh b gy oq on l oo op"># Seperates the data into test and training sets <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)</span><span id="9436" class="ol mx it oh b gy oq on l oo op"># Plotting the training and testing splits<br/>plt.scatter(X_train, y_train, label = "Training Data", color = 'r')<br/>plt.scatter(X_test, y_test, label = "Testing Data", color = 'b')<br/>plt.legend()<br/>plt.grid("True")<br/>plt.title("Test/Train Split")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/56744ab28e18a0734ec605d08a2c772e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WZsAyRjPPJKRARw8.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明，我们已经将大部分数据(80%)作为模型将要训练的数据，同时保留了可用于评估模型性能的少数数据(20%)。我们在这里可以看到，测试数据虽然很小，但覆盖了大范围的整体数据，因此可以作为很好的测试数据来查看模型的表现。</p><p id="fbc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经将数据分为测试和训练数据集，我们就可以实现线性回归模型了。为了通过Scikit Learn实现线性回归，我们首先通过调用<code class="fe ot ou ov oh b">LinearRegression</code>函数来定义一个<code class="fe ot ou ov oh b">regressor</code>变量。然后我们传递<code class="fe ot ou ov oh b">regressor.fit(X_train, y_train)</code>，它将训练数据传递给回归，以便训练它。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="be41" class="ol mx it oh b gy om on l oo op"># Defining our regressor<br/>regressor = LinearRegression()</span><span id="7bd6" class="ol mx it oh b gy oq on l oo op"># Train the regressor<br/>fit = regressor.fit(X_train, y_train)</span></pre><p id="8e0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和以前一样，我们感兴趣的是从我们的数据中提取的梯度和截距，我们可以通过调用我们训练的回归对象的属性<code class="fe ot ou ov oh b">.coef_</code>和<code class="fe ot ou ov oh b">.intercept_</code>来提取这些数据。由此我们可以得到:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="64a0" class="ol mx it oh b gy om on l oo op"># Returns gradient and intercept<br/>print("Gradient:",fit.coef_)<br/>print("Intercept:",fit.intercept_)</span><span id="f403" class="ol mx it oh b gy oq on l oo op">#out<br/>Gradient: [[0.98952479]]<br/>Intercept: [-0.1328508]</span></pre><p id="0907" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，虽然梯度与之前的相似，但截距不同。这主要是因为我们将数据分成了训练数据和测试数据，因此我们不是在处理整个数据集。</p><p id="6eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着我们可以根据我们的测试数据来评估我们的模型，看看它的表现如何。为此，像以前一样，我们想提取最佳拟合线，现在我们可以使用<code class="fe ot ou ov oh b">regressor.predict(X_test)</code>方法，而不是像以前一样计算直线。这意味着我们可以这样实现:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="ae17" class="ol mx it oh b gy om on l oo op"># Predicted values <br/>y_pred = regressor.predict(X_test)</span><span id="c21f" class="ol mx it oh b gy oq on l oo op"># Plot of the data with the line of best fit<br/>plt.plot(X_test,y_pred)<br/>plt.plot(x,y, "rx")<br/>plt.grid(True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/e4a3b29235ecce9f2d627ff8728bf10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-A3gI6v6McQYBDkP.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="f76e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，通过创建一个数据框架，将预测值与实际测试结果进行比较，如下所示:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="28d0" class="ol mx it oh b gy om on l oo op"># Converts predicted values and test values to a data frame<br/>df = pd.DataFrame({"Predicted": y_pred[:,0], "Actual": y_test[:,0]})<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c588b3269b70ed19482de792c134b00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/0*Nqiic9veQrCSWE0n.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c39d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以通过传递<code class="fe ot ou ov oh b">regressor.score(X_test, y_test)</code>为我们的模型提供一个分数。正如我们之前提到的，这是回归器没有训练过的数据，这意味着它可以为模型在数据本身上的表现提供良好的基础。这是R分数，它显示了模型捕获的目标变量的变化量。这方面的最佳得分为1，其中大于0.8通常被视为模型拟合良好的指标，而0则被视为最差的性能。</p><p id="83b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们当前的模型，结果如下所示:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="7241" class="ol mx it oh b gy om on l oo op"># Determines a score for our model<br/>score = regressor.score(X_test, y_test)<br/>print(score)</span><span id="d868" class="ol mx it oh b gy oq on l oo op">#out:<br/>0.9763002831471521</span></pre><p id="788e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明我们有一个很好的模型。</p><h1 id="7d44" class="mw mx it bd my mz ob nb nc nd oc nf ng jz od ka ni kc oe kd nk kf of kg nm nn bi translated">Scikit学习多元线性回归</h1><p id="dd20" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">我们上面使用的例子只涉及两个变量，这在我们自己的代码中相对容易实现，但在现实世界中，单独测量或使用一个特征或独立变量是极不可能的。这使得实现和等式更加复杂，因此我们可以依靠Scikit了解更多来实现这一点。</p><p id="16f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有多个独立变量的新方程采用以下形式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/e87e1209655140ca945e9a4c53e35307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*oYNMECzyDzVzP0YR.png"/></div></figure><p id="2fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着实际上我们有多个梯度值，还有一个额外的𝜖项，这就是误差。实际上，𝜖考虑了任何可能不适合线性模型的潜在点。</p><p id="b0e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们使用生成的广告数据及其对销售的影响。这里，自变量由<code class="fe ot ou ov oh b">TV</code>、<code class="fe ot ou ov oh b">Radio</code>和<code class="fe ot ou ov oh b">Newspaper</code>列给出，而<code class="fe ot ou ov oh b">Sales</code>是我们的自变量。这方面的数据如下:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="a986" class="ol mx it oh b gy om on l oo op"># Converts advertising csv to a data frame<br/>df = pd.read_csv("advertising.csv")<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ae7fb87d7d3b9d5374326a2d47b1a7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*VVDyTd_j7PMBehL3.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8ea3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后在Scikit Learn library下应用线性回归算法的过程采取了类似的路径，我们获取X和Y变量，对其进行整形，分成训练和测试数据，实施模型，预测未知数据，然后提取分数。这都可以通过以下方式实现:</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="3ad3" class="ol mx it oh b gy om on l oo op"># Independent variables<br/>X = df.drop("Sales",axis=1)</span><span id="5d03" class="ol mx it oh b gy oq on l oo op"># Dependent variable<br/>y = df["Sales"].values.reshape(-1,1)</span><span id="fb5f" class="ol mx it oh b gy oq on l oo op"># Splitting into test and training data<br/>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)</span><span id="1d8a" class="ol mx it oh b gy oq on l oo op"># Defining regressor<br/>regressor = LinearRegression()</span><span id="0c2c" class="ol mx it oh b gy oq on l oo op"># Training our regressor<br/>fit = regressor.fit(X_train,y_train)</span><span id="696d" class="ol mx it oh b gy oq on l oo op"># Predicting values<br/>y_pred = fit.predict(X_test)</span><span id="f693" class="ol mx it oh b gy oq on l oo op"># Scoring our regressor<br/>fit.score(X_test,y_test)</span><span id="aad1" class="ol mx it oh b gy oq on l oo op">#out:<br/>0.9174440089730509</span></pre><p id="b0b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，我们的模型通常与我们拥有的数据非常吻合，得分为0.917，实施相对简单。</p><p id="082b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们可以创建一个更复杂的模型，并通过使用Scikit Learn库中内置的其他指标或评估方法以更详细的方式对其进行评估，更多信息可在此处找到<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank"/>。此外，除了Scikit Learn之外，您还可以使用Statsmodels Api库，该库为您内置的回归实现提供了更详细的摘要，可用于评估您的模型，其实现可在<a class="ae ky" href="https://www.statsmodels.org/stable/regression.html" rel="noopener ugc nofollow" target="_blank">此处</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="61d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想了解我们协会的更多信息，请随时关注我们的社交网站:</p><p id="d216" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.facebook.com/ucldata<a class="ae ky" href="https://www.facebook.com/ucldata" rel="noopener ugc nofollow" target="_blank">脸书</a></p><p id="15cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">insta gram:<a class="ae ky" href="https://www.instagram.com/ucl.datasci/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/ucl.datasci/</a></p><p id="cd20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">领英:<a class="ae ky" href="https://www.linkedin.com/company/ucldata/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/company/ucldata/</a></p><p id="d1c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解UCL数据科学协会和其他优秀作者的最新信息，请使用我下面的推荐代码注册medium。</p><div class="mc md gp gr me mf"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mo l"><div class="pb l mq mr ms mo mt ks mf"/></div></div></a></div><p id="8be7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者看看我写的其他故事:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/bias-and-variance-for-machine-learning-in-3-minutes-4e5770e4bf1b"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">3分钟机器学习的偏差和方差</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">什么是偏差和方差，这对你的机器学习模型意味着什么？</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pc l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-object-oriented-programming-for-data-scientists-879106d90d89"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">面向对象的基础知识，适合那些以前可能没有接触过这个概念或者想知道更多的人</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pd l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/univariate-outlier-detection-in-python-40b621295bc5"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的单变量异常检测</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">从数据集中检测异常值的五种方法</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pe l mq mr ms mo mt ks mf"/></div></div></a></div></div></div>    
</body>
</html>