# 道德和会话助理

> 原文：<https://towardsdatascience.com/ethics-and-conversational-assistants-68adcec20ca>

## 意见

## 近年来，信任和对伦理规则的考虑已经成为人工智能领域的一个重要组成部分。早期的滥用导致立法者在事后监管该行业。本文讨论的道德问题的会话助理考虑这些新的规定。

![](img/a1840906a94fa76838e24902c2efea68.png)

[廷杰伤害律师事务所](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

2018 年 9 月 28 日，加州州长批准第 6 章命令，机器人必须明确表示它们只是软件。

2020 年 9 月 7 日，CNIL ( *国家信息和自由委员会* — *法国组织，负责确保保护计算机文件和处理或纸张(公共和私人*)中包含的个人数据)发布了一份关于语音助手的白皮书，并提出了道德等问题。

同年 10 月 20 日，欧洲议会通过了一项关于人工智能、机器人和相关技术伦理方面框架的决议。

作为该决议和其他工作的延续，同一个议会和欧洲委员会于 2021 年 4 月 21 日提交了一份关于建立人工智能统一规则和修订欧盟某些立法法案的法规提案。

2021 年 9 月 15 日，cn pen(*“commitéNational Pilote d ' ethique du numérique”是一个法国委员会，成立于 2019 年，旨在全面解决数字世界的道德问题*)通过了一项关于对话代理的道德风险的意见，并制定了十三项设计建议。

这些最近的出版物告诉了我们什么？

## 伦理学

伦理学是哲学的一个分支，研究道德的基础和社会中个人的行为。伦理学研究道德，研究什么是对或错，什么是正义或不正义。

即使对话助手是简单的计算机程序，最近的技术进步和聊天机器人的广泛使用也对它们在社会中的作用以及它们对人类的影响提出了质疑。

## 出版物

加利福尼亚州是美国第一个对“机器人”(“非人类”执行动作或发布消息)的道德规范进行立法的州。州长认为掩盖机器人的人工身份，不清楚地表明它是简单的软件，而不是提供建议或信息的人是非法的。对这种虚拟性的提及必须“*清晰、显眼、设计合理*”[1]。

尽管这项法律是 2016 年美国大选期间“假新闻”传播的间接后果，并且具有明显的限制性(拥有超过一千万访问者的平台，煽动购买，在选举期间影响投票)，但它现在已经扎根于这个领域，并可以在事实上应用于对话助理。这条定律给我们上了第一课:我们不应该欺骗公众，把一个严格的人性层面归因于一个会话助手。

2020 年 9 月 7 日，CNIL 发布了一份 88 页的白皮书，探讨语音助手面临的伦理、技术和法律挑战[2]。这份报告指出，语音辅助设备通过改变我们使用数字工具的方式，提出了一个重大的范式转变。报告指出，一方面，这些终端的设计带来了不透明性，没有屏幕，因此无法可视化过去的活动。另一方面，这些设备只提供一个答案，这就带来了选择的可靠性和中立性的问题。对于这个问题，“谁是最好的水管工？”，系统可以很好地给出支付最多的人的地址，以首先被定位，而无需机器人这样说。

除了隐私和数据保护的问题(*比伦理部分*更详细)，拟人化的问题引起了争论，设计师有时会推动助手的拟人化以增加承诺。该报告提到了“宜家综合症”，即当用户预见到个性化助理的可能性时，会导致用户对物品产生更强烈的依恋，从而赋予物品更大的价值，因为他为物品的构造做出了贡献[3]。即使参数化仅仅在于选择助手的声音，该附件也会出现。通过这个简单的动作，机器人变成了一个私人助理，一个它帮助建造的存在。

关于国家笔会对话代理人的第 3 号意见在伦理领域走得更远[4]。第一，它占用了不欺骗用户的原则。该意见然后建议在选择代理人性别时选择公平原则。令人惊讶的是，该意见建议能够个性化助理，从而为用户诱发“宜家综合症”。

> …聊天机器人不应该被用户认为是一个负责任的人，即使是通过投影。总的来说，这不是一个让拟人化自由发展的问题，也不是一个想不惜一切代价消除它的问题，而是在具体情况下确定其限度的问题。

该报告还处理了对助手的侮辱，指出他们没有什么不道德的，因为他们只是计算机程序。同时考虑到这种行为对说出这些话的人来说是有辱人格的。最后，CNPEN 建议通过允许对侮辱做出反应来处理这些行为，但建议不要寻求升级。另一方面，他们建议不要在学习语料库中包括这样的句子，这与检测的概念奇怪地矛盾…

对于“助理可以故意撒谎吗？”答案是“没有”(迄今为止)。助手没有道德判断，没有邪恶的意图，因此责任在于制造商。我要补充的是，训练机器人的人也有责任。

今天，助手的作用是显而易见的，特别是对弱势群体(=“未成年人或由于年龄、疾病、虚弱、身体或精神缺陷或怀孕状态而无法保护自己的人”——《法国刑法典》第 434-3 条)。这种有用性与这些系统的弹性、对所说内容的判断缺失以及它们在任何时候的可用性密切相关。尽管如此，从设计上来说，还是有必要防止责任的混淆，以及例如助手和医务人员之间可能存在的责任混淆。最后要注意的一点是用户和他的机器人之间可能的依赖。

## “死穴”

2017 年，微软申请了一项专利，解释了他们如何计划将个人的个性数据复制到对话机器人中[5]。该系统使用个人数据(例如，图像、语音数据、社交媒体帖子、电子邮件、书面信件等。)来创建或修改帮助训练机器人的“索引”。

最令人不安的用途是这种系统允许实现所谓的“死机器人”的能力，也就是说，模拟一个已故的个人的形象及其对话能力。正如国家预防和控制网络的报告所指出的，死者存在的这种延长"首先质疑方法本身"。他们最后指出，首先，对这些助手的存在进行社会反思是可取的。

## 欧洲伦理框架

人工智能、机器人和相关技术的伦理方面的框架以及欧洲议会和理事会制定人工智能协调规则(人工智能立法)和修订某些联盟立法法案的提案[6]汇集了一系列涵盖人工智能广阔领域的文章。它对会话助手(聊天机器人)的应用赋予我们以下义务:

*   人工监督必须始终完整。
*   告知与人工智能系统交互的责任。请注意，该提案规定"*这一义务不适用于法律授权检测、预防、调查和起诉刑事犯罪的人工智能系统，除非这些系统可供公众报告刑事犯罪。*
*   能够暂时禁用某些功能，只保留最安全的功能。
*   反偏见和反歧视的方法。
*   在使用情绪识别系统的情况下，该文本建议用户有义务了解系统的功能。

## 综合

总之，助理创作者需要记住的道德设计原则如下:

*   不要通过表明它是一个计算机程序来欺骗用户。有义务从互动一开始就明确，并定期重复这种状态。
*   从算法的设计阶段或对话的创建阶段就考虑伦理问题，以避免偏见或歧视。
*   描述用户助手的用途及其目标。告知对过去的对话进行的操作，特别是如果拟人原则被应用于改善未来的对话。展示系统在语言处理方面的能力和局限性。如果适用，解释如何管理情绪以及系统将如何做出相应的反应。
*   将对话保留必要的时间来处理它们，并且能够解释*、后验*以及(如果必要的话)机器人在对话期间的行为。
*   让机器人的个性适应用户的文化。

因为使用语言作为交流的媒介，所以在称呼会话助手时排除任何形式的拟人化都是空想。因此，设计者必须通过实施这些设计规则来限制这些缺点，从而降低欺骗和依赖的风险，并对这些系统给予信任。

## 参考

[1] [参议院第 1001 号法案——第 6 章。bots](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1001)—[https://leginfo . legislation . ca . gov/faces/billtextclient . XHTML？bill_id=201720180SB1001](https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1001)

[2][法语][Livre blanc sur les assistants vocaux de la CNIL](https://www.cnil.fr/sites/default/files/atoms/files/cnil_livre-blanc-assistants-vocaux.pdf)—探讨语音助手的伦理、技术和法律问题—[https://www . cnil . fr/sites/default/files/atoms/files/cnil _ Livre-blanc-assistants-vocaux . pdf](https://www.cnil.fr/sites/default/files/atoms/files/cnil_livre-blanc-assistants-vocaux.pdf)

[3][法语] [合成之声:人机交互的大众传播。对话 avec le monde](https://www.cairn.info/revue-communication-et-langages1-2017-3-page-63.htm)——[https://www . cairn . info/revue-communication-et-langages 1-2017-3-page-63 . htm](https://www.cairn.info/revue-communication-et-langages1-2017-3-page-63.htm)

[4][法语] [Avis n 3 代理商 conversationnels enjeux d ' ethique](http://www.ccne-ethique.fr/sites/default/files/2022-02/Avis%20n%C2%B03%20agents%20conversationnels%20enjeux%20d%27%C3%A9thique.pdf)—[https://www . ccne-ethique . fr/sites/default/files/2022–02/Avis % 20n % C2 % B03 % 20 代理商% 20 conversationnels % 20 enjeux % 20d % 27% C3 % a9thique . pdf](https://www.ccne-ethique.fr/sites/default/files/2022-02/Avis%20n%C2%B03%20agents%20conversationnels%20enjeux%20d%27%C3%A9thique.pdf)

[5] [创建一个特定人的对话聊天机器人](https://patents.google.com/patent/US10853717B2/en?oq=us10853717b2)——[https://patents.google.com/patent/US10853717B2/en?oq=us10853717b2](https://patents.google.com/patent/US10853717B2/en?oq=us10853717b2)

[6] [关于欧洲议会和理事会制定人工智能统一规则(人工智能法案)并修正某些欧盟法案的法规提案](https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF)—[https://eur-lex.europa.eu/resource.html?uri = cellar:e 0649735-a372-11eb-9585-01aa 75 ed 71 a 1.0001 . 02/DOC _ 1&format = PDF](https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF)

感谢您的阅读！如果你想了解更多关于聊天机器人、自然语言处理和人工智能的信息，请关注我的[媒体](https://pemey.medium.com/)、[领英、](https://www.linkedin.com/in/patrickmeyer/)和[推特](https://twitter.com/pemey)。

## 与会话助手相关的其他文章

[](/conversational-ai-trends-and-predictions-for-2022-8be05e15e713)  [](/is-your-chatbot-sensitive-575ad0217707)  [](https://ai.plainenglish.io/is-your-chatbot-accessible-6b89a5e300f1)  [](/14-criteria-for-well-choosing-a-chatbots-solution-2e788aace3b8)  [](/an-overview-of-the-global-market-for-chatbot-solutions-in-2020-820aa9366efc)  [](https://ai.plainenglish.io/conversational-ai-for-builders-the-4-levels-of-complexity-scale-4eb482a862d9) 