<html>
<head>
<title>How to Use Pandas for Big Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用熊猫进行大数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-pandas-for-big-data-50650945b5c6#2022-01-25">https://towardsdatascience.com/how-to-use-pandas-for-big-data-50650945b5c6#2022-01-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="ac24" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">如何利用熊猫进行大数据</h1></div><div class=""><h2 id="8177" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过Spark上的Pandas运行分布式工作负载</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/399a546ac26412b94938cd7b6b53d6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*asySatVycgapvByrPkjn6g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/illustrations/background-show-graphic-art-panda-3330215/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="19f2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动机</h1><p id="973b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于其直观的数据结构和丰富的API，Pandas已经成为数据科学家和分析师事实上的python库。Pandas使用内存计算，这使它成为中小型数据集的理想选择。然而，由于内存不足的错误，Pandas处理大数据集的能力是有限的。熊猫的替代品有很多，其中之一就是阿帕奇Spark。</p><p id="d8b8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Apache Spark是一个开源的分布式计算引擎，用于处理和分析大量数据，方法是在集群之间分发数据并并行处理数据。虽然Pyspark(python中的Apache Spark接口)非常适合繁重的数据工作负载，但是学习新的Pyspark语法并将代码从Pandas重构到py Spark可能会很乏味。</p><p id="4f6e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">幸运的是随着<a class="ae ky" href="https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html" rel="noopener ugc nofollow" target="_blank"> Spark 3.2更新</a>，我们现在可以在Spark上运行熊猫API了。这允许我们在Spark中利用分布式处理的能力，同时使用Pandas中熟悉的语法。让我们来看一个例子，看看它是如何处理来自UCI数据库[1]的银行营销数据集的。代码在<a class="ae ky" href="https://databricks.com/try-databricks" rel="noopener ugc nofollow" target="_blank"> Databricks社区版</a>运行时版本10.2ML上执行</p><h1 id="cb5f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">例子</h1><p id="71d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了使用本地熊猫，我们通常会以下列方式进口熊猫</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7e54" class="mx la it mt b gy my mz l na nb">import pandas as pd</span></pre><p id="4720" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要在Pyspark中使用Pandas API，我们只需进行以下导入，其他一切都是一样的。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="146e" class="mx la it mt b gy my mz l na nb">import pyspark.pandas as ps</span></pre><p id="fe88" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">读取CSV文件</strong></p><p id="effd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">得到的数据帧是Pyspark Pandas数据帧。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a73b" class="mx la it mt b gy my mz l na nb">df = ps.read_csv('/FileStore/tables/bank_full.csv')</span><span id="2a1b" class="mx la it mt b gy nc mz l na nb">type(df)<br/>&gt;&gt; pyspark.pandas.frame.DataFrame</span></pre><p id="5b50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">检查数据帧</strong></p><p id="39e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该数据与一家葡萄牙银行机构的直接营销活动(电话)相关。分类的目标是预测客户是否会认购定期存款(变量y)。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="133b" class="mx la it mt b gy my mz l na nb">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/e54986bade615c10bb013459f0836673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_YQsnyYS5YaY_6ZW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fa77" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">查看栏目信息</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2f98" class="mx la it mt b gy my mz l na nb">df.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/20831bde448c5a5ee73c1342290aeaf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/0*e__UHFZhUoieSdxM"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c2d2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">分组依据和聚合</strong></p><p id="b512" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过目标变量(y)找出平均年龄</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9b25" class="mx la it mt b gy my mz l na nb">df.groupby('y', as_index = False).agg({'age':'mean'}).sort_values('age')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3af45a341b2c70b64e09a5591a98432b.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/0*6n6K97GaYC1e2V1-"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6237" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">应用Lambda函数</strong></p><p id="6d9b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们创建一个指示器列来指示客户的年龄是否超过40岁。我们可以通过使用Pandas <code class="fe ng nh ni mt b">.apply</code>和<code class="fe ng nh ni mt b">lambda</code>函数对age列应用lambda函数来实现这一点。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="87b5" class="mx la it mt b gy my mz l na nb">df['age_above_40'] = df['age'].apply(lambda x: 'yes' if x &gt; 40 else 'no')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/1d729d50e8c4702f7e9ca560b49f5e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_tgfMrSZM4nQn7ZN"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="93a1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">绘图</strong></p><p id="7599" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还可以用熊猫<code class="fe ng nh ni mt b">.plot</code>功能绘制图表。图表是用Plotly绘制的，我们可以使用Plotly的<code class="fe ng nh ni mt b">.update_layout</code>来调整图表属性。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f223" class="mx la it mt b gy my mz l na nb">fig = df.groupby('marital').agg({'age':'mean'}).plot.bar()<br/>fig.update_layout(xaxis_title = 'Marital Status', yaxis_title = 'Average Age', width = 500, height = 400)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/920416bb967fc25a631228352866c820.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/0*qx7qQLVqMGXZkfmZ"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="78dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">用SQL查询数据帧</strong></p><p id="d9c3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Pyspark Pandas DataFrame也可以用SQL查询。这是本土熊猫所没有的。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2921" class="mx la it mt b gy my mz l na nb">ps.sql("SELECT y, mean(age) AS average_age FROM {df} GROUP BY y")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/eadf5e837ff1833b645d3f2ccdd5c734.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/0*z0UwNqIvlhXi90CF"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="55e7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">从Spark Pandas转换到Pyspark DataFrame </strong></p><p id="5ea3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们也可以选择在Pyspark中工作，将Pyspark Pandas数据帧转换成Pyspark数据帧。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3d53" class="mx la it mt b gy my mz l na nb">spark_df = df.to_spark()</span><span id="6b81" class="mx la it mt b gy nc mz l na nb">type(spark_df)<br/>&gt;&gt; pyspark.sql.dataframe.DataFrame</span></pre><p id="732a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">从Pyspark数据帧转换到Pyspark熊猫数据帧</strong></p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="d7a2" class="mx la it mt b gy my mz l na nb">psdf = spark_df.to_pandas_on_spark()</span><span id="2d92" class="mx la it mt b gy nc mz l na nb">type(psdf)<br/>&gt;&gt;pyspark.pandas.frame.DataFrame</span></pre><h1 id="d0c1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="1040" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们看了如何在Spark上使用Pandas API，它帮助我们使用熟悉的Pandas语法以分布式方式处理大数据集。Apache Spark只是Pandas在Python中处理大数据集的众多替代品之一。查看这篇关于熊猫处理大数据集的其他<a class="ae ky" href="https://medium.com/@edwin.tan/8-alternatives-to-pandas-for-processing-large-datasets-928fc927b08c" rel="noopener">替代品的文章。</a></p><h1 id="5261" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="ed1a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]来自<a class="ae ky" href="https://archive-beta.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank"> UCI知识库</a>的数据集。由4.0在CC下授权。c .萨卡尔&amp;金尤美·卡斯特洛创建的数据集。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="43b5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/@edwin.tan/membership" rel="noopener">加入Medium </a>阅读更多这样的故事。</p></div></div>    
</body>
</html>