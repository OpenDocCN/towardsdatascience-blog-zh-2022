<html>
<head>
<title>Explainable AI (XAI) Methods Part 3 — Accumulated Local Effects (ALE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能(XAI)方法第三部分——累积局部效应(ALE)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-xai-methods-part-3-accumulated-local-effects-ale-cf6ba3387fde#2022-01-19">https://towardsdatascience.com/explainable-ai-xai-methods-part-3-accumulated-local-effects-ale-cf6ba3387fde#2022-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="6b14" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">可解释的人工智能(XAI)方法第三部分——累积局部效应(ALE)</h1></div><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于累积局部效应(ALE)的教程，侧重于它的使用、解释和利弊</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/a141db704c801a910b40070b6fd95913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*TO2t8lrKWnVsayPj"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在<a class="ae ku" href="https://www.pexels.com/ko-kr/photo/7191988/" rel="noopener ugc nofollow" target="_blank">像素点</a>免费使用照片。</p></figure><h1 id="749b" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">对以前职位的审查</h1><p id="5bb3" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">可解释的机器学习(XAI)指的是努力确保人工智能程序在目的和工作方式上是透明的。[1]这是我打算写的XAI系列文章中的第三篇。</p><p id="4023" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在我的第一篇<a class="ae ku" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d">帖子</a>中，我介绍了部分相关(PD)和<strong class="lp iu">部分相关图(PDP) </strong>的概念，这是一种使用那些PD值来显示特征对目标变量的边际平均影响的可视化。[2]我建议在继续下一步之前先看一下<a class="ae ku" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d">的帖子</a>！</p><p id="fe30" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在我的第二篇<a class="ae ku" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-2-individual-conditional-expectation-ice-curves-8fe76919aab7">帖子</a>中，我介绍了<strong class="lp iu">个人条件期望(ICE) </strong>曲线，它在与PDP联合使用时特别有效。[3]是因为我们可以同时观察到特性的整体/平均效果和特性的观察级效果！</p><h1 id="6c35" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated"><strong class="ak">累积局部效应(ALE)与标准PDP </strong></h1><p id="08f4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">累积局部效应(ALE)与部分相关(PD)的概念相似，都旨在描述特征如何影响模型的平均预测。ALE确实比PD具有竞争优势，因为它解决了当感兴趣的特征与其他特征高度相关时PD中出现的偏差。让我们看看，由于计算部分相关性的方式，部分相关性图(PDP)中的特征之间的相关性是如何成为问题的。</p><p id="50e7" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">假设我们有一个简单的回归模型，它根据其他身体特征，如体重、性别、父亲的身高等，预测一个人的身高。这里，我们感兴趣的是理解体重对目标变量的边际平均效应，在本例中，目标变量是身高。对于权重的每个网格值，我们将用该网格值替换整个权重列，计算预测值，并对它们进行平均。这将是权重网格值的部分相关值。然而，为了计算部分依赖图，我们可能不得不容忍一些不现实的情况。例如，如果体重的网格值是30kg，我们将所有观察的体重替换为30kg，即使是像{性别:男性，父亲身高:185cm …}这似乎不太可能。</p><p id="18b6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">ALE plots解决了这个问题，它计算预测值的<strong class="lp iu">差值，而不是平均值。<strong class="lp iu"> <em class="mo">用“差异”代替“平均值”可以让我们屏蔽相关特征的影响。</em></strong>【4】</strong></p><p id="6482" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">Molnar的可解释机器学习书用两个简短的句子很好地对PDP和ALE进行了比较。</p><blockquote class="mp mq mr"><p id="bcf2" class="ln lo mo lp b lq mj ju ls lt mk jx lv ms ml ly lz mt mm mc md mu mn mg mh mi im bi translated"><strong class="lp iu"> PDP </strong>:“让我向您展示当每个数据实例都具有该特性的值v时，模型平均预测的结果。我忽略了值v是否对所有数据实例都有意义。”</p><p id="9ae6" class="ln lo mo lp b lq mj ju ls lt mk jx lv ms ml ly lz mt mm mc md mu mn mg mh mi im bi translated">v.s</p><p id="c84c" class="ln lo mo lp b lq mj ju ls lt mk jx lv ms ml ly lz mt mm mc md mu mn mg mh mi im bi translated"><strong class="lp iu"> ALE图</strong>:“让我向您展示模型预测如何在v 周围的一个小“窗口”中针对该窗口中的数据实例改变<strong class="lp iu">。”[4]</strong></p></blockquote><h1 id="247c" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">理论和解释</h1><p id="2074" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">Molnar的<a class="ae ku" href="https://christophm.github.io/interpretable-ml-book/ale.html" rel="noopener ugc nofollow" target="_blank">可解释机器学习书籍</a>涵盖了ALE的理论，非常深入和详细。我将把重点放在解释和应用上！</p><p id="826a" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">以下节选自这篇<a class="ae ku" href="https://www.enjine.com/blog/interpreting-machine-learning-models-accumulated-local-effects/" rel="noopener ugc nofollow" target="_blank">博客文章</a>，很好地总结了ALE works如何与其名称相匹配。</p><blockquote class="mp mq mr"><p id="7a95" class="ln lo mo lp b lq mj ju ls lt mk jx lv ms ml ly lz mt mm mc md mu mn mg mh mi im bi translated">一个足够小的窗口允许我们对这段时间内的变化做出相当准确的估计。然后通过<em class="it">累加</em>所有的局部区域，我们就能够全面了解我们的输入对输出的影响。因此，如果我们想知道20摄氏度的一天对我们的跑步者有什么影响，我们会找到21度的影响，然后减去19度的差异。通过<strong class="lp iu">平均</strong>预测中的变化，我们可以确定该窗口的特征的<strong class="lp iu">效果</strong>。然后，我们对数据重复这个过程，并<strong class="lp iu">累加</strong> …</p></blockquote><h2 id="8268" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">数字特征的解释…</h2><p id="ba58" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">对于单个数值特征，ALE值可以解释为与数据 的平均预测相比，该特征在某一值的<strong class="lp iu"> <em class="mo">主效应。这种解释是可能的，因为ALE图以零为中心，所以ALE曲线的每个点都代表与平均值预测的差异。[4]例如，特征值12.2的ALE值为5将意味着如果感兴趣的特征等于12.2，则它将产生的预测比平均预测高5。</em></strong></p><h2 id="c4c4" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">两个数字特征的解释…</h2><p id="dbd4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">二阶效应是在我们考虑了特征的主要效应之后，特征的附加交互效应。[4]</p><h2 id="d7e3" class="mv kw it bd kx mw mx dn lb my mz dp lf lw na nb lh ma nc nd lj me ne nf ll ng bi translated">对分类特征的解释…</h2><p id="d3c1" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">顾名思义，效果朝着某个方向累积，因此特征值需要有一个“<strong class="lp iu">顺序</strong>”。对于分类特征，在计算ALE值之前，应首先设置变量中值的顺序。</p><p id="f234" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">确定分类变量中值的顺序的一种方法是，根据其他特征将具有高相似性的值视为连续顺序或彼此接近的顺序。为了计算数值特征之间的相似性，使用Kolmogorov-Smirnov距离，对于分类特征，使用相对频率表。[4]</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/d95fd800ac60171e66fb1d6fec6fd97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K28U2xpLN-EzE7ja.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">可解释机器学习书籍中的自行车租赁数量预测示例—分类变量“月”的ALE图</p></figure><p id="8457" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">可解释的机器学习书提供了一个预测自行车租赁数量的例子。[4]从上面分类变量的ALE图中可以看出，性质相似的月份值排列在一起或彼此接近。例如，被认为是冬季或早春的月份，如12月、11月、3月和1月被聚集在一起。同样，高温的夏季月份在x轴上彼此相邻。这是有意义的，因为我们根据表征每个月的其他特征的相似性分数给这些值赋予了顺序。</p><p id="5b55" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">至于解释，这类似于我们解释一个数值变量。12月份的ALE值约为-660，这意味着在12月份，预测的自行车租赁数量将比平均预测值低660辆。我们也可以通过观察每个条形图的绝对长度来说明影响的大小。例如，一月、三月和四月的柱线非常短，这意味着与其他月份相比，它们对自行车租赁预测数量的影响。</p><h1 id="bc0c" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated"><strong class="ak">麦酒地块的利与弊</strong></h1><p id="2e8c" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">如前所述，在我们比较PDP和ALE图的部分，即使感兴趣的特征与其他特征相关，我们的ALE图也是无偏的。但是PDP肯定比ALEs更容易理解。另一方面，ALE图在解释图时需要注意细节，这取决于您正在处理的特征类型(如数值、分类)以及您正在查看的交互特征的数量。</p><p id="89bb" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">尽管如此，对ALE图的解释仍然相当简单。它是“在给定值的条件下改变特征对预测的相对影响”。[4]此外，ALE图以零为中心，这使得很容易理解ALE曲线中的每个点都代表平均预测值的差异。</p><p id="a9fe" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">然而，ALE图也有其局限性。它比PDP或ICE图更难解释。这种解释也通常被限制在所定义的“窗口”或“间隔”内。如果特征高度相关(这是经常发生的情况)，解释跨区间的影响是不可能的。请记住，影响是在每个区间内计算的，并且每个区间包含不同的数据点集合。这些计算出的效应被累加(因此命名为“累加的”局部效应),只是为了使线平滑。</p><p id="523e" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">ALE图的另一个麻烦之处是需要确定最佳间隔数。如果定义了太多的间隔，图形中的起伏会使绘图变得嘈杂。减少间隔的数量将使图更加稳定，但这是有代价的-它可能会掩盖模型中存在的一些复杂性或相互作用。</p><h1 id="79e5" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">履行</h1><p id="004d" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">ALE图可以用R和Python实现。</p><p id="9983" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果你正在使用R…</p><ul class=""><li id="b4c5" class="nm nn it lp b lq mj lt mk lw no ma np me nq mi nr ns nt nu bi translated">ALEPlot包</li><li id="cbe5" class="nm nn it lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">iml包</li></ul><p id="f792" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">都是值得一看的好地方！</p><p id="6240" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果你正在使用Python…</p><ul class=""><li id="f1d0" class="nm nn it lp b lq mj lt mk lw no ma np me nq mi nr ns nt nu bi translated">ALEPython包</li><li id="0861" class="nm nn it lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">Alibi套餐</li></ul><p id="15ab" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">是最受欢迎的。</p><p id="2bd0" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">这里有一些很好的文档和博客帖子，它们使用了上面的包来实现ALE图，所以请查看它们！</p><div class="oa ob gp gr oc od"><a href="https://github.com/blent-ai/ALEPython/blob/dev/examples/regression_iris.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">ALEPython/regression _ iris . ipynb at dev blent-ai/ALEPython</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">Python累积的局部效果包。通过在…上创建帐户，为blent-ai/ALEPython开发做出贡献</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">github.com</p></div></div><div class="om l"><div class="on l oo op oq om or ko od"/></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://www.analyticsvidhya.com/blog/2020/10/accumulated-local-effects-ale-feature-effects-global-interpretability/" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">累积局部效应(ALE)-特征重要性技术</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">这篇文章恰好是我上一篇文章“Black…的全局模型可解释性技术”的延续</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="om l"><div class="os l oo op oq om or ko od"/></div></div></a></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">ALIBI包文档</p></figure><h1 id="213e" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">参考</h1><p id="c117" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">[1] <a class="ae ku" href="https://www.techopedia.com/definition/33240/explainable-artificial-intelligence-xai" rel="noopener ugc nofollow" target="_blank">可解释的人工智能(XAI) </a> (2019)，Technopedia</p><p id="29fa" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">[2] S. Kim，<a class="ae ku" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d">《可解释的人工智能(XAI)方法第一部分—部分依赖图(PDP) </a> (2021)，走向数据科学</p><p id="ee8f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">[3] S. Kim，<a class="ae ku" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d">《可解释的人工智能(XAI)方法》第2部分—部分依赖图(PDP) </a> (2021)，走向数据科学</p><p id="9455" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">[4] C. Molnar <em class="mo">，</em> <a class="ae ku" href="https://christophm.github.io/interpretable-ml-book/ice.html" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a> (2020)</p></div></div>    
</body>
</html>