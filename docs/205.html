<html>
<head>
<title>Practical Guide to DQN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DQN实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-guide-for-dqn-3b70b1d759bf#2022-01-07">https://towardsdatascience.com/practical-guide-for-dqn-3b70b1d759bf#2022-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="a723" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">DQN实用指南</h1></div><div class=""><h2 id="e80e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">DQN在强化学习中的实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e560eb8a3902595847e685486f2385a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2dPkQcidjw3euwNF"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><blockquote class="kz la lb"><p id="0a8d" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">“实践你所知道的，这将有助于弄清楚你现在所不知道的”<br/> <strong class="lf iu">伦勃朗</strong></p></blockquote><h1 id="5636" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">概观</h1><p id="b690" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">Mnih等人[2015]提出的深度Q-网络是许多深度强化学习算法的跳转起点和构建点。然而，尽管它表面上很简单，然而，当实现它和寻找问题的解决方案时，它代表了一些挑战。</p><p id="7100" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">本文将使用<a class="ae ky" href="https://www.tensorflow.org/js" rel="noopener ugc nofollow" target="_blank"> Tensorflow.js </a>作为实现技术，原因很明显:<br/>——你需要安装的东西越多，你就越不可能努力编写真正的算法。<br/> - Javascript是一种很好的流行语言，对于熟悉C++、C#、Java等的人来说非常直观……<br/>-在网上分享演示既简单又直接。</p><h1 id="4231" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">该算法</h1><p id="8772" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">如前所述，该算法本身非常简单，可以通过下图进行总结</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/06db49f44a710dc25008b728ea799dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uLtcNBImDEo1qcUaOK5niw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DQN工作流程(图片由作者提供)</p></figure><p id="8b55" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">以下步骤描述了该算法的实际工作原理:</p><p id="acc4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">1-创建Q-网络和目标-网络<br/> 2-使用Q-网络<br/>用数据填充经验缓冲区3-重复以下步骤足够的次数<br/> 4-从经验缓冲区获得随机样本<br/> 5-将样本作为输入馈送到Q-网络和目标网络<br/> 6-使用目标网络的输出来训练Q-网络(即，目标网络的输出将在 标准的监督学习场景)<br/> 7-应用探索/开发策略(例如:ε贪婪)<br/> 8-如果选择了探索，则生成随机动作，否则，如果选择了开发，则向Q网络馈送当前状态，并从输出中推断动作。 <br/> 9-将动作应用于环境，获得奖励，以及新状态<br/> 10-将旧状态、动作、奖励和新状态存储在经验缓冲区(也称为重放记忆)<br/> 11-每隔几集，将权重从Q-网络复制到目标-网络</p><h1 id="de9e" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">为什么是两个网络？</h1><p id="df89" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">很明显，我们想知道，为什么我们需要两个网络？<br/>实际上，Q学习公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/70cc8d87fd573c45677bfa795f3276a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYrSz2WS3103a6_yzCLSmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q-学习:非策略TD控制(摘自萨顿和巴尔托的书:强化学习，介绍)</p></figure><p id="c865" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">我们注意到，为了使Q(S，A)达到它的最终值，它必须变得几乎等于R' + γ max Q(S '，A)。因此，为了使Q(S，A)向R' + γ max Q(S '，A)移动，我们将问题转化为某种监督学习，其中我们估计Q(S，A)和R' + γ max Q(S '，A)成为“标签”。<br/>用于估计Q(S，A)的网络权重将通过损失函数和反向传播的组合来更新。</p><p id="eb69" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">但是我们不知道最大Q(S '，a)，我们简单地使用相同的神经网络来估计它，然而，该网络的权重不断地变化，这在每次迭代中给出不同的最大Q(S '，a)值。这使得对Q(S，A)的估计可以追踪一个移动目标。</p><p id="45cc" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">为了解决这个问题，我们创建了一个相同的神经网络，我们称之为目标网络，而计算Q(S，A)所需的第一个网络将被称为在线网络。目标网络的目的是在一段时间内具有固定的权重，以便稳定max Q(S’，a)的计算。然后，在一些迭代之后，目标网络通过在线网络的权重的简单拷贝来更新其权重，在线网络有机会学习这些权重。</p><h2 id="5d32" class="nb ma it bd mb nc nd dn mf ne nf dp mj mt ng nh ml mv ni nj mn mx nk nl mp nm bi translated">当心一些实现陷阱</h2><ul class=""><li id="dff3" class="nn no it lf b lg mr lj ms mt np mv nq mx nr ly ns nt nu nv bi translated">标准化输入。这可能是显而易见的，但是初学者可能很容易忘记这个重要的特性。这将对输出产生重大影响，因为一些输入可能具有较大的值范围，而其他输入则具有较小的值范围。这并不意味着后者不如前者重要。然而，从计算的角度来看，大值会压倒小值。因此，如果您看到一些明显的输入有奇怪的值，这是一个很好的条件反射，检查输入是否已经被规范化。</li><li id="e6c8" class="nn no it lf b lg nw lj nx mt ny mv nz mx oa ly ns nt nu nv bi translated">同步两个神经网络。DQN使用两个神经网络，一个是“在线”的，这意味着在每次迭代中计算Q值，另一个是“目标”网络，它计算Q值，但基于一组稳定的权重。<br/>“目标”网络表示表达式R + γ max Q(St+1)。“在线”和“目标”网络的输出用于计算均方误差(MSE ),然后通过反向传播更新“在线”网络的权重。<br/>每一定次数的迭代，“在线”网络的权重应该被复制到“目标”网络，以便“目标”获得由“在线”网络学习的经验。<br/>这里的重要值是设置复制权重的迭代次数的正确值。太小的值，使系统不稳定，太大的值使系统无法学习。所以当事情没有朝着正确的方向发展时，检查这个参数是很重要的。</li><li id="d9af" class="nn no it lf b lg nw lj nx mt ny mv nz mx oa ly ns nt nu nv bi translated">填补经验缓冲。在过程开始时，重要的是通过让代理与环境交互来填充经验缓冲区，并在经验缓冲区中存储当前状态、动作、奖励、下一状态、终端状态，以便当学习过程从该经验中生成随机样本时。</li><li id="14df" class="nn no it lf b lg nw lj nx mt ny mv nz mx oa ly ns nt nu nv bi translated">抽样规模。将采样大小设置为经验缓冲区大小的一半左右是一个很好的做法，因为小的采样大小会使从经验中学习的效率低下，而大的采样大小(最大值附近)会在学习中引入您不想要的偏差。例如，如果你有一辆自动驾驶汽车，并且由于大多数时间道路是直的，过度地从直路经验中采样将使AI倾向于最小化转弯的影响。</li></ul><h1 id="63dc" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">扁担例子</h1><p id="d4da" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">DQN的一个经典实现是CartPole游戏，在这个游戏中，人工智能代理应该以某种方式移动手推车，以保持杆子接近垂直。<br/>电杆与垂直方向的夹角不得超过30 °,手推车不得触及边缘。每当人工智能代理收集到300个奖励时，就取得了胜利。</p><p id="8fda" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">你可以在https://rl-lab.com/cartpole/查看游戏的真实实现</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/cd164d5ea1f6e8505d8ed331468951cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lY7SdoRT5dZ4EGE9vf62uA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片显示了弹球游戏的配置(图片由作者提供)</p></figure><p id="ad2d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">以下是算法主要部分的代码片段。它包含主循环、播放步骤和基于批量样本的训练。</p><p id="6415" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">train()方法包含训练的主循环。它首先初始化代理，并用经验填充重放内存缓冲区，然后开始循环:训练，然后单步执行。</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="164b" class="nb ma it od b gy oh oi l oj ok">train(agent, maxIterations, batchSize, gamma, learningRate, syncEveryFrames) {</span><span id="4671" class="nb ma it od b gy ol oi l oj ok">  // init the agent<br/>  agent.reset();</span><span id="a635" class="nb ma it od b gy ol oi l oj ok">  // at first fill the replay memory with experience by playing<br/>  // without any training<br/>  for (let i = 0; i &lt; agent.replayBufferSize; ++i) {<br/>    agent.playStep();<br/>  }</span><span id="0de5" class="nb ma it od b gy ol oi l oj ok">  // create optimizer for the training  <br/>  const optimizer = tf.train.adam(learningRate);</span><span id="937e" class="nb ma it od b gy ol oi l oj ok">  // loop until all iterations are done<br/>  let counter = maxIterations;</span><span id="8d18" class="nb ma it od b gy ol oi l oj ok">  while (counter &gt; 0) {</span><span id="3cda" class="nb ma it od b gy ol oi l oj ok">    // train the agent by extracting samples from replay memory and <br/>    // use them as input for the DQN   <br/>    agent.trainOnReplayBatch(batchSize, gamma, optimizer);</span><span id="42a5" class="nb ma it od b gy ol oi l oj ok">    // after each training, play one step of the game    <br/>    agent.playStep();</span><span id="eecd" class="nb ma it od b gy ol oi l oj ok">    if (agent.frameCount % syncEveryFrames === 0) {<br/>       copyWeights(agent.targetNN, agent.onlineNN);<br/>    }</span><span id="6f12" class="nb ma it od b gy ol oi l oj ok">    counter--;<br/>  }<br/>}</span></pre><p id="440c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">playStep()方法使代理只玩一步游戏。它使用ε贪婪策略，使它在部分时间选择随机动作，在其余时间选择返回最大Q值的动作。</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="dfd0" class="nb ma it od b gy oh oi l oj ok">// play one step of the game and store result in replay memory<br/>playStep() {</span><span id="6a21" class="nb ma it od b gy ol oi l oj ok">...<br/>  let action;<br/>  let actionIndex = 0;</span><span id="9ecc" class="nb ma it od b gy ol oi l oj ok">  const state = this.game.getState();</span><span id="7924" class="nb ma it od b gy ol oi l oj ok">  if (Math.random() &lt; this.epsilon) {<br/>    // Pick an action at random.<br/>    actionIndex = this.getRandomAction();<br/>    action = ALL_ACTIONS[actionIndex];<br/>  } else {<br/>  // Greedily pick an action based on online DQN output.<br/>  // use tf.tidy() to clean up after finishing<br/>     tf.tidy(() =&gt; {<br/>       const stateTensor =  getStateTensor(state);<br/>       actionIndex = this.onlineNN.predict(stateTensor)<br/>                              .argMax(-1).dataSync()[0];</span><span id="017c" class="nb ma it od b gy ol oi l oj ok">       action = ALL_ACTIONS[actionIndex];<br/>     });<br/>  }</span><span id="5036" class="nb ma it od b gy ol oi l oj ok">  // play one step of the game and get the results<br/>  const result = this.game.step(action);</span><span id="5dd6" class="nb ma it od b gy ol oi l oj ok">  // store experience in replay Memory<br/>  this.replayMemory.append([state, actionIndex, result.reward,<br/>                                 result.state, result.status]);</span><span id="3ba5" class="nb ma it od b gy ol oi l oj ok">  // if terminal state, reset the game<br/>  if (r.status != 0) {this.reset();}</span><span id="c814" class="nb ma it od b gy ol oi l oj ok">}</span></pre><p id="0481" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">trainOnReplayBatch()从replayMemory中提取一个样本，并将其作为DQN的输入，然后使用均方误差作为损失函数来更新权重。</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="6295" class="nb ma it od b gy oh oi l oj ok">// This method is used to train the online network by using batch<br/>// samples from the memory replay</span><span id="91b9" class="nb ma it od b gy ol oi l oj ok">trainOnReplayBatch(batchSize, gamma, optimizer){</span><span id="4d5c" class="nb ma it od b gy ol oi l oj ok">// Get a batch of examples from the replay buffer.<br/>const batch = this.replayMemory.sample(batchSize);</span><span id="6db1" class="nb ma it od b gy ol oi l oj ok">//define the loss function<br/>const lossFunction = () =&gt; tf.tidy(() =&gt; {</span><span id="b1b9" class="nb ma it od b gy ol oi l oj ok">  // example[0] is the state<br/>  // example[1] is the action<br/>  // example[2] is the reward<br/>  // example[3] is the next state<br/>  // example[4] indicates if this is a terminal state</span><span id="3415" class="nb ma it od b gy ol oi l oj ok">  const stateTensor = getStateTensor(batch.map(<br/>                                     example =&gt;   example[0]));<br/>  const actionTensor = tf.tensor1d(batch.map(<br/>                                   example =&gt; example[1]), 'int32');<br/>  <br/>  // compute Q value of the current state<br/>  // note that we use apply() instead of predict<br/>  // because apply() allow access to the gradient<br/>  const online = this.onlineNN.apply(stateTensor, {training: true});<br/>  const oneHot = tf.oneHot(actionTensor, NUM_ACTIONS);<br/>  const qs = online.mul(oneHot).sum(-1);<br/>  </span><span id="362d" class="nb ma it od b gy ol oi l oj ok">  // compute the Q value of the next state. <br/>  // it is R if the next state is terminal<br/>  // R + max Q(next_state) if the next state is not terminal</span><span id="191e" class="nb ma it od b gy ol oi l oj ok">  const rewardTensor = tf.tensor1d(batch.map(<br/>                                         example =&gt; example[2]));<br/>  const nextStateTensor = getStateTensor(batch.map(<br/>                                         example =&gt; example[3]));<br/>  const nextMaxQTensor =  this.targetNN.predict(<br/>                                         nextStateTensor).max(-1);<br/>  const status = tf.tensor1d(batch.map(<br/>                          example =&gt; example[4])).asType('float32');</span><span id="cbaa" class="nb ma it od b gy ol oi l oj ok">  // if terminal state then status = 1 =&gt; doneMask = 0<br/>  // if not terminal then status = 0 =&gt; doneMask = 1<br/>  // this will make nextMaxQTensor.mul(doneMask) either 0 or not<br/>  const doneMask = tf.scalar(1).sub(status);<br/>  const targetQs = rewardTensor.add(<br/>                           nextMaxQTensor.mul(doneMask).mul(gamma));<br/>  </span><span id="2d76" class="nb ma it od b gy ol oi l oj ok">  // define the mean square error between Q value of current state<br/>  // and target Q value<br/>  const mse = tf.losses.meanSquaredError(targetQs, qs);<br/>  return mse;<br/>});</span><span id="a203" class="nb ma it od b gy ol oi l oj ok">// Calculate the gradients of the loss function with respect <br/>// to the weights of the online DQN.</span><span id="3407" class="nb ma it od b gy ol oi l oj ok">const grads = tf.variableGrads(lossFunction);</span><span id="0765" class="nb ma it od b gy ol oi l oj ok">// Use the gradients to update the online DQN's weights.optimizer.applyGradients(grads.grads);</span><span id="db17" class="nb ma it od b gy ol oi l oj ok">tf.dispose(grads);<br/>}</span></pre><p id="df16" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll mt ln lo lp mv lr ls lt mx lv lw lx ly im bi translated">最后，这里是一个视频，其中DQN代理人通过达到300英镑的奖励来玩并赢得了掷骰子游戏。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">展示DQN算法玩钢管舞的视频(视频由作者提供)</p></figure><h1 id="6439" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">结论</h1><p id="8704" class="pw-post-body-paragraph lc ld it lf b lg mr ju li lj ms jx ll mt mu lo lp mv mw ls lt mx my lw lx ly im bi translated">DQN是最流行的深度强化学习算法之一。它第一次在Atari游戏上实现了超人水平的性能。<br/>随着时间的推移，它经历了多次改进，这使得它仍然是DRL表现最好的代理商之一。</p></div></div>    
</body>
</html>