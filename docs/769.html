<html>
<head>
<title>How and Why to Perform a K-Fold Cross Validation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何以及为什么执行K倍交叉验证</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-and-why-to-perform-a-k-folds-cross-validation-adf88665893b#2022-01-27">https://towardsdatascience.com/how-and-why-to-perform-a-k-folds-cross-validation-adf88665893b#2022-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4a350dd3661a85e7c18651f0587da2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NbqCMJ76ox72iaU3yEwNg.png"/></div></div></figure><div class=""><h1 id="2c18" class="pw-post-title jc jd je bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">如何以及为什么执行K倍交叉验证</h1></div><div class=""><h2 id="e00a" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">帮助您在处理较小的数据集时避免过度拟合等情况</h2></div><p id="1e31" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你好，朋友们！一年多前，我开始了一系列的短文，我称之为<strong class="kv jf">数据科学快速提示</strong>。我已经有一段时间没做了，这一个与我正在做的另一个系列特别相关，我正在创建一个围绕电影评级的预测模型<a class="ae lp" href="https://medium.com/tag/dkhundley-movie-model" rel="noopener"/>。为了回顾我们在该系列中所做的事情，我创建了一个预测模型，试图预测我最喜欢的播客之一可能给他的下一部电影的电影评级。</p><p id="9e7d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然我一直在通过收听播客的备份目录来收集有关他评级的电影的数据，但我发现的现实是，他根本没有给那么多电影评级。(这个播客的主要目的完全是别的东西；电影评论只是偶尔的奖励。)我仍然有办法查阅过去的目录，但迄今为止，我只收集了大约<strong class="kv jf"> 125条影评</strong>。</p><p id="5f14" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这可能看起来很多，但…不是。差远了。大多数大公司在数千甚至数百万条记录上训练模型，所以125条记录与像这样大得多的数据集相比几乎不算什么。因此，在我们进入什么是k倍验证之前，让我们快速回顾一下数据科学家通常如何用大型数据集训练他们的预测模型。如果你想继续，你可以在我的GitHub库<a class="ae lp" href="https://github.com/dkhundley/ds-quick-tips/blob/master/014_kfolds_validation/notebooks/kfolds-validation.ipynb" rel="noopener ugc nofollow" target="_blank">中找到我将在这篇文章中分享的代码，链接在这里</a>。</p><h1 id="25f4" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">传统的“训练测试”分裂</h1><p id="cbeb" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">为了简单起见，在这篇文章中，我们将使用虹膜数据集<a class="ae lp" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank">。Iris数据集可以非常容易地用Scikit-Learn库直接加载。以下是加载该数据集的代码:</a></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="99c6" class="mw lr je ms b gy mx my l mz na"># Importing the necessary Python libraries<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn import datasets<br/>from sklearn.model_selection import train_test_split, KFold<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import accuracy_score, confusion_matrix</span><span id="3681" class="mw lr je ms b gy nb my l mz na"># Getting the Iris dataset from Scikit-Learn<br/>iris = datasets.load_iris()</span><span id="5ced" class="mw lr je ms b gy nb my l mz na"># Loading the predictor value (y) and remainder of the training dataset (X) as Pandas DataFrames<br/>X = pd.DataFrame(data = iris['data'], columns = iris['feature_names'])<br/>y = pd.DataFrame(data = iris['target'], columns = ['target'])</span></pre><p id="2ba8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Iris数据集是一个非常简单的小数据集，有三个目标类，这意味着这是一个很好的小数据集，可以用来练习创建<strong class="kv jf">多类分类模型</strong>。虹膜数据集也已经处于这样一个位置，如果我们不想做任何特征工程，我们在技术上不必做任何特征工程。鉴于这篇文章的重点不是创建一个完美的模型，我也不会费心去做任何特性工程。此外，我将使用Scikit-Learn的<strong class="kv jf"> RandomForestClassifer </strong>算法来创建一个基本的预测模型。因为我不想创建一个完全精确的模型，你会注意到我根本不想调整算法的超参数。</p><p id="0cd4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当数据科学家创建预测模型时，他们通常会留出一部分带标签的训练数据集，称为<strong class="kv jf">验证数据集</strong>。(它有时被称为测试数据集，但由于我个人认为这有点用词不当，因为你有时会在一个模型被创建后传递一个未标记的测试数据集，我认为“测试数据集”术语应该保留给它。)验证数据集通过根据训练数据集训练的模型来计算度量，以确保模型的准确性之类的事情。</p><p id="b838" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，不要把验证数据集和训练数据集混在一起是一个绝对的基本规则。这是因为您希望确保模型能够从模型尚未看到的数据集中得出准确的推断。在机器学习领域，我们将外部数据对训练数据集的这种影响称为<strong class="kv jf">数据泄漏</strong>。</p><p id="60a4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在数据科学家有足够数量的记录进行训练的典型情况下，他们将执行典型的<strong class="kv jf">训练测试分割</strong>。(再说一次，我并不热衷于这种命名法，但是考虑到Scikit-Learn这样的库会进行这种拆分，我们将这样称呼这种拆分。)执行这种训练测试分割的最流行的方法之一是使用Scikit-Learn的内置函数<code class="fe nc nd ne ms b">train_test_split</code>。下面是我们如何在X和y数据集上执行该函数:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6760" class="mw lr je ms b gy mx my l mz na"># Performing a train_test_split on the dataset<br/>X_train, X_val, y_train, y_val = train_test_split(X, y)</span></pre><p id="ba07" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，我们已经在训练和验证数据集之间执行了拆分，我们准备好执行模型训练和验证。下面是实现这一点的代码:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="2d32" class="mw lr je ms b gy mx my l mz na"># Instantiating a RandomForestClassifier model<br/>rfc_model = RandomForestClassifier()</span><span id="1556" class="mw lr je ms b gy nb my l mz na"># Fitting the X_train and y_train datasets to the RandomForestClassifier model<br/>rfc_model.fit(X_train, y_train)</span><span id="1f07" class="mw lr je ms b gy nb my l mz na"># Generating validation metrics by comparing the inferential predictions (val_preds) to the actuals (y_val)<br/>val_accuracy = accuracy_score(y_val, val_preds)<br/>val_confusion_matrix = confusion_matrix(y_val, val_preds)</span><span id="7a7e" class="mw lr je ms b gy nb my l mz na"># Printing out the validation metrics<br/>print(f'Accuracy Score: {val_accuracy}')<br/>print(f'Confusion Matrix: \n{val_confusion_matrix}')</span></pre><p id="eaf4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我打印出这些验证指标时，我在个人电脑上看到的是:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/8d67ab8dfb76ba9ccc9cee0021a3efac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVSPTfgV6uhQyhcEpbMKUg.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者截图</p></figure><p id="ec27" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">老实说，92%并不坏，因为我们既没有特征工程也没有超参数调整，但这也是非常简单的虹膜数据集。我们面临的最大风险是，像这样的小数据集的验证准确率通常要低得多。其原因是，在针对非常小的一组数据进行训练时，我们很可能会面临<strong class="kv jf">过度拟合</strong>的风险。也就是说，因为模型在训练数据集中没有太多的信息，所以模型倾向于针对狭窄的训练数据集的外观生成非常具体的推断。</p><p id="60f8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这显然是不理想的，那么我们如何确保我们的模型不会过度拟合呢？正如您所猜测的，这就是k倍验证的用处所在！</p><h1 id="21f3" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">k倍验证</h1><p id="f534" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">在上面的例子中，我们对数据集进行了一次训练测试分割。如果您避免数据泄漏，这意味着您的验证数据集将永远不会被视为模型训练过程的一部分。</p><p id="752c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">所以问题是，有没有一种安全的方法来利用完整的数据集，同时小心数据泄漏？答案是肯定的，一种流行的方法是使用<strong class="kv jf"> k倍验证</strong>。k-fold验证所做的是将数据分成许多批次(或折叠),并对数据集进行洗牌，每次留出一个折叠用于验证目的。下图有助于更清楚地说明这一点。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/d628b19cb114f9d76806164d17184640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rT7cHxlgzy-q6aKC0MtW8g.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者创作的图形</p></figure><p id="864f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在上图中，数据集被分成五个不同的部分，当我们遍历每一行时，我们用所有的浅灰色框进行训练，然后用一个深灰色框进行验证。请注意，我们实际上是以这种方式对模型进行五次不同的训练。虽然这看起来并不理想，但它仍然是有帮助的，因为如果验证度量标准有些不同，我们将能够判断出模型是否被过度拟合。换句话说，如果您的验证度量对于每个折叠都是不同的，这是一个很好的指标，表明您的模型过度拟合了。</p><p id="c13f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，让我们从上面的代码开始，对其进行一点重构，以执行k倍验证:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="4648" class="mw lr je ms b gy mx my l mz na"># Instantiating the K-Fold cross validation object with 5 folds<br/>k_folds = KFold(n_splits = 5, shuffle = True, random_state = 42)</span><span id="8d70" class="mw lr je ms b gy nb my l mz na"># Iterating through each of the folds in K-Fold<br/>for train_index, val_index in k_folds.split(X):<br/>    <br/>    # Splitting the training set from the validation set for this specific fold<br/>    X_train, X_val = X.iloc[train_index, :], X.iloc[val_index, :]<br/>    y_train, y_val = y.iloc[train_index], y.iloc[val_index]<br/>    <br/>    # Instantiating a RandomForestClassifier model<br/>    rfc_model = RandomForestClassifier()<br/>    <br/>    # Fitting the X_train and y_train datasets to the RandomForestClassifier model<br/>    rfc_model.fit(X_train, y_train)<br/>    <br/>    # Getting inferential predictions for the validation dataset<br/>    val_preds = rfc_model.predict(X_val)<br/>    <br/>    # Generating validation metrics by comparing the inferential predictions (val_preds) to the actuals (y_val)<br/>    val_accuracy = accuracy_score(y_val, val_preds)<br/>    val_confusion_matrix = confusion_matrix(y_val, val_preds)<br/>    <br/>    # Printing out the validation metrics<br/>    print(f'Accuracy Score: {val_accuracy}')<br/>    print(f'Confusion Matrix: \n{val_confusion_matrix}')</span></pre><p id="bcfe" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当我运行这段代码时，输出如下:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d146a26823813da289fc25bb9625e12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*8CJ1yChPw-DTtEbEe0zn0Q.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">作者截图</p></figure><p id="c184" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果您还记得，来自原始训练测试分割数据(92%)的准确性分数或多或少与我们从k倍验证准确性度量中看到的一致。虽然所有这些可能有一些过度拟合，但没有什么特别明显的迹象表明我们在这里有严重的过度拟合问题。这是一件好事！</p><p id="2436" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在，这是一个很好的评估过度拟合的技术，但是你可能会发现，这不是一个纠正过度拟合的方法。您必须采取额外的措施来实际纠正过度拟合的情况，无论是以不同的方式设计数据，解决某种目标类不平衡，还是其他什么。这超出了这篇文章的范围，但我可能会在以后写一些更具体的东西。</p><p id="5370" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是现在，这个帖子就写到这里吧！这次要保持简短甜蜜。希望在处理小数据集时，你们都觉得这很有价值。我知道我个人会在我的电影分级模型项目中使用它！感谢您的阅读，我们将在下一篇文章中再见。</p></div></div>    
</body>
</html>