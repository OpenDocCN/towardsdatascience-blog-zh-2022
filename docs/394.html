<html>
<head>
<title>Explaining Face Mask Image Classification Model Using LIME</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用石灰解释口罩图像分类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-face-mask-image-classification-model-using-lime-8f423c601ff9#2022-01-13">https://towardsdatascience.com/explaining-face-mask-image-classification-model-using-lime-8f423c601ff9#2022-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="ea1b" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">用石灰解释口罩图像分类模型</h1></div><div class=""><h2 id="3752" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于解释影像分类模型的LIME的不同组件的概述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4a8618aefc0ea14efccbdfb134e422b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7sPPLV_BdYzW2b325JcNsQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Rinke Dohmen 在<a class="ae kv" href="https://unsplash.com/s/photos/facemask?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b82f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着新冠肺炎的持续高涨和新变种的出现，不同的监管机构已经声明了佩戴口罩的重要性，尤其是在公共场所。已经开发了不同的面罩检测机器，并且正在被一些组织使用。所有这些机器都有某种图像分类和检测算法在后台工作。但是，可悲的是，这些算法大多是“黑箱”。这意味着模型正在后台做一些事情，因为它的行为方式对我们来说是模糊的。因此，在这些高度敏感的场景中，是否信任该模型是一个值得关注的问题。</p><p id="4e50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，可以使用LIME(局部可解释模型不可知解释)来解释图像分类模型的行为方式。它可以提供关于负责进行某种预测的超像素(或特征)的清晰想法。在这篇文章中，我将一步一步地分享如何使用<code class="fe ls lt lu lv b">lime</code>来解释图像分类模型所做的预测。要了解不同可解释性工具的工作机制，请阅读本文</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/explainable-ai-an-illuminator-in-the-field-of-black-box-machine-learning-62d805d54a7a"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd ir gy z fp me fr fs mf fu fw ip bi translated">可解释的人工智能:黑盒机器学习领域的一个亮点</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">不同的机器学习可解释性工具如何用于解释的概述</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn kp lz"/></div></div></a></div></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><p id="54d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据描述</strong></p><p id="a998" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据收集自<a class="ae kv" href="https://www.kaggle.com/prasoonkottarathil" rel="noopener ugc nofollow" target="_blank"><em class="mv">Prasoon Kottarathil</em></a><em class="mv">的“面膜简装数据集”</em>，可在此处<a class="ae kv" href="https://www.kaggle.com/prasoonkottarathil/face-mask-lite-dataset/" rel="noopener ugc nofollow" target="_blank"/>【4】。为了进行分析，我从数据集来源中随机选择了大约10，000张图像。该数据用于区分图像中的人是否戴着面具(类别0)或是否戴着面具(类别1)。图像分类模型的目的是检测那些人没有戴面罩的图像，因此类别“没有面罩”被认为是肯定类别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/68c3726e135c24daa907110bf227bca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*1OFlS3AksVBPBfFJT4ZU3w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像分类模型的类别(按作者分类的图像)</p></figure><p id="984e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的主要目的是解释图像分类模型所做的预测。因此，我不打算详细介绍我用于分类的CNN模型的架构。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><p id="cd52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">建立CNN模型</strong></p><p id="0754" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据存储在包含两个文件夹的目录中:一个包含带面具的图像，另一个包含不带面具的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c9577670747e538cbc2ce5e83d7ad57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*gRO6wXQtxMPxdm7Jdxe34w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c4de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<code class="fe ls lt lu lv b">ImageDataGenerator()</code>从路径生成数据。它采用目录的路径并生成批量的扩充数据。由于用于分析的图像是3d图像，因此它们包含3个通道(<code class="fe ls lt lu lv b">color_mode = ‘rgb'</code>)。图像被调整到(高度，宽度):<code class="fe ls lt lu lv b">target_size = (256,256)</code>。<code class="fe ls lt lu lv b">class_mode = ‘categorical'</code>被选中，这决定了类标签数组将作为二维热编码标签返回。图像从这两个文件夹中流出，其类别标签与它们来自的文件夹名称一致，batch_size为32。</p><p id="162b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mv">现在让我们创建CNN模型！！</em></p><p id="a5a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所使用的模型是一个普通的CNN模型，它采用3-D输入图像并将它们传递到第一个<em class="mv"> Conv2D </em>层，该层具有16个大小为3×3的滤波器和激活函数ReLU。它返回特征图，然后将这些特征图传递给最大filer大小为2x2的<em class="mv"> MaxPooling2D </em>层，从而降低特征向量的维数。然后它移动到第二个<em class="mv"> Conv2D </em>层(32个滤镜)、<em class="mv"> MaxPooling2D </em>层、第三个<em class="mv"> Conv2D </em>层(64个滤镜)和最后一个<em class="mv"> MaxPooling2D </em>层。</p><p id="6d98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，输出传递到展平层(展平输出)、具有512个单元的密集隐藏层和输出大小为2的向量(这是两个类的概率向量)的全连接层。</p><p id="dfe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Adam优化器用于优化二元交叉熵损失函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="10c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">检查CNN模型完成的预测</strong></p><p id="ee1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们看看这个模型是如何处理一个看不见的数据的。为此，我用自己的照片来看看这个模型是否能够正确地对图像进行分类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/2ccafa7281c1a2c5f0282fb0d7af4613.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*rfTviA0m9fzv_Qbqmihwew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="16cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太神奇了！！该模型已经为该图像预测了合适的类别。</p><p id="04cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们能相信我们的模型吗？哪些特征影响了我的模型以这种方式进行预测？？</p><p id="38f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据模型的准确性来判断模型是不够的。理解“黑箱”模型的行为非常重要。为此，我们可以使用石灰。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="fd37" class="nb nc iq bd nd ne nf ng nh ni nj nk nl jw nm jx nn jz no ka np kc nq kd nr ns bi translated"><strong class="ak">用石灰解释</strong></h1><p id="c3ff" class="pw-post-body-paragraph kw kx iq ky b kz nt jr lb lc nu ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">在做任何事情之前，我们首先要知道图像的特征是什么。像素值可用作图像的特征，但在时间上<a class="ae kv" href="https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08" rel="noopener">超像素</a>用作特征。超像素是具有一些共同特征(例如像素强度)的一组像素。它们包含比像素更多的有用信息。有几种方法可以分割图像的像素，如SLIC(简单线性迭代聚类)、Quickshift等。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="fc0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用Quickshift算法，我们获得了46个超像素，它们作为图像的特征，输出如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/88b937eb4e3aea03048434eaa4256184.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*AtMfuuGHjt7Oj1aBdgdDRw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像的超像素(作者提供的图像)</p></figure><p id="f639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">扰动样本的生成</strong></p><p id="b4ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，通过随机打开和关闭一些超像素，在原始数据实例的邻域周围创建一些扰动样本。这里，在这个例子中，我们已经在邻域中生成了1000个扰动样本，并且已经通过使用已经拟合的CNN模型为这些扰动样本预测了类别。下图显示了这种扰动样本的一个示例。注意，每个扰动样本的长度是原始图像中超像素(或特征)的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/97f874e0351afd734955a5362aa3a635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*q5g9ywtW6m1fqW8-lgUtvQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扰动样本示例(图片由作者提供)</p></figure><p id="abaa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在扰动的样本中，<code class="fe ls lt lu lv b">1</code>表示超像素打开，<code class="fe ls lt lu lv b">0</code>表示该位置的超像素关闭。</p><p id="fefa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，现在我们已经看到哪些超像素(或特征)出现在扰动的样本中，但我们很想知道这些扰动的图像会是什么样子。不是吗？？</p><p id="5b80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看。</p><p id="ea7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于所有关闭的超像素或特征，原始图像的像素被名为fudged_image的图像的像素替换。通过取平均像素值或取特定颜色(隐藏颜色)的超像素来创建伪造图像。在这个例子中，fudged_image是通过使用颜色为0的超像素(<code class="fe ls lt lu lv b">hide_color=0</code>)创建的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/67e9f2d754944c9c95069e7cd4c981c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*J912DErQSpQCFF2Mj-n_HA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原始图像和前3个最接近的扰动图像(作者提供的图像)</p></figure><p id="fdbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">计算原始图像和扰动图像之间的距离</strong></p><p id="6058" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算原始图像和扰动图像之间的余弦距离，上图显示了前3个最接近的扰动图像。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扰动图像的创建和距离计算</p></figure><p id="692b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">计算扰动样本的权重</strong></p><p id="14d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在邻域中创建扰动样本后，权重(0和1之间的值)被赋予这些样本。与远离原始图像的样本相比，离原始图像近的样本被给予更高的权重。内核宽度为25的指数内核用于给出这些权重。基于内核宽度，我们可以定义原始实例周围的“局部性”。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e657" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">重要特征或超像素的选择</strong></p><p id="ec60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过学习局部线性模型来选择重要的特征。加权线性模型是根据我们从前面的步骤中获得的数据(扰动的样本、它们来自CNN模型的预测和权重)拟合的。由于我们想要解释类‘无_掩码’，所以从预测向量中，对应于‘无_掩码’的列被用于局部线性模型中。有几种方法来选择顶部特征，如向前选择，向后消除，甚至我们可以使用正则化的线性模型，如Lasso和Ridge regression。在这个例子中，我对数据拟合了一个岭回归模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="fe38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我得到的前10个特征或超像素如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7eb9cfa5e0e207a6b99187fb9b8ea30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*DgsBmVy72scw9y1U9mIypA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前10个超像素(图片由作者提供)</p></figure><p id="1804" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">局部线性模型预测</strong></p><p id="5556" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在选择了前10个特征之后，加权的局部线性模型被拟合以解释由“黑盒”模型在原始图像的局部邻域中所做的预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="cda0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们已经看到了LIME是如何一步一步地工作的，并且局部地解释了一个由黑盒模型完成的预测。那么，让我们看看LIME的最终输出是什么样子，以及如何解释它。</p><p id="e842" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">石灰解释最终输出</strong></p><p id="4ddb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此示例中使用的图像的最终石灰输出如下所示。在这里，我们希望看到图像中前10个最重要的超像素。用绿色标记的超像素是对标签“无_掩码”的预测有积极贡献的特征，用红色标记的超像素是对标签的预测有消极贡献的特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/20311043bec75fa794bc786e926c07be.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*J8G-TdlJ4N9bHE7-I758nw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终解释权归LIME所有(图片由作者提供)</p></figure><p id="6d31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的解释中，我们可以看到鼻子和嘴附近的超像素对预测“无_掩模”的类别有积极的贡献。因为鼻子和嘴附近的超像素在该图像中不包含遮罩，所以它们对“无遮罩”的预测有积极的贡献。这是有道理的。类似地，耳朵附近的一些超像素对“无_掩码”类的预测有负面影响(模型认为可能是掩码的一些胡须片，如果这是我们需要在训练数据中添加更多变化以正确训练模型的原因)。因此，这是石灰优化拟合模型行为的一个好处。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="feef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这让我们走到了尽头。</p><p id="fd8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要获得本文使用的完整Jupyter笔记本，请访问我的<a class="ae kv" href="https://github.com/kunduayan/LIME_image_classification/tree/main" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库。关于我未来的博客，请在<a class="ae kv" href="https://www.linkedin.com/in/ayan-kundu-a86293149/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae kv" href="https://medium.com/@ayan.kundu09" rel="noopener"> Medium </a>关注我。</p><p id="0e73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="58f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我试图解释图像数据的最终结果，以及整个解释过程是如何一步一步地进行的。对于表格和文本数据也可以进行类似的解释。</p><p id="5e79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><ol class=""><li id="d2a2" class="od oe iq ky b kz la lc ld lf of lj og ln oh lr oi oj ok ol bi translated">GitHub石灰库:<a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a></li><li id="35c1" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">超像素和SLIC:<a class="ae kv" href="https://darshita1405.medium.com/superpixels-and-slic-6b2d8a6e4f08" rel="noopener">https://darshita 1405 . medium . com/super pixels-and-SLIC-6 B2 D8 a6 E4 f 08</a></li><li id="fb1c" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">Scikit-image图像分割:<a class="ae kv" href="https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.quickshift" rel="noopener ugc nofollow" target="_blank">https://scikit-image . org/docs/dev/API/skim age . segmentation . html # skim age . segmentation . quick shift</a></li><li id="de5b" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">数据集来源:<a class="ae kv" href="https://www.kaggle.com/prasoonkottarathil/face-mask-lite-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/prasoonkottarathil/face-mask-lite-dataset</a>(License:CC BY-SA 4.0)</li><li id="e066" class="od oe iq ky b kz om lc on lf oo lj op ln oq lr oi oj ok ol bi translated">数据集许可链接(CC BY-SA 4.0):【https://creativecommons.org/licenses/by-sa/4.0 T4】</li></ol><p id="a052" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mv">感谢阅读，快乐学习！</em>T9】</strong></p></div></div>    
</body>
</html>