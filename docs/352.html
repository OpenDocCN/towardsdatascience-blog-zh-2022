<html>
<head>
<title>Highest Prior Density Estimation for Diagnosing Black Box Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于诊断黑盒性能的最高先验密度估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/highest-prior-density-estimation-for-diagnosing-black-box-performance-c16447a96b7#2022-01-12">https://towardsdatascience.com/highest-prior-density-estimation-for-diagnosing-black-box-performance-c16447a96b7#2022-01-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="6053" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">用于诊断黑盒性能的最高先验密度估计</h1></div><div class=""><h2 id="d2fa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何搜索精确度较差的数字要素</h2></div><p id="5066" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型可解释性是机器学习的一个领域，在过去几年里越来越受欢迎。更好的理解导致利益相关者更大的信任和更好的推广。但是你怎么能偷看黑盒呢？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d629320df0c0fb40dbd8b59531df81e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JkDPD65uR3IFW51K.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图1:蓝色的50%最高密度区域(HDR)的示例。图片作者— <a class="ae lr" href="https://mathematica.stackexchange.com/questions/173282/computing-credible-region-highest-posterior-density-from-empirical-distributio" rel="noopener ugc nofollow" target="_blank"> src </a>。</p></figure><p id="3a65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在之前的一篇文章中，我们介绍了IBM的解决方案<a class="ae lr" href="https://arxiv.org/abs/2108.05620" rel="noopener ugc nofollow" target="_blank"> FreaAI </a>。用一句技术话来说，FreaAI使用最高先验密度方法和决策树来识别模型性能较低的数据区域。在对统计显著性和一些其他标准进行过滤之后，可解释的数据切片呈现给工程师。</p><p id="bba0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">FreaAI不是开源的，所以在这篇文章中，我们将做一个最高先验密度方法的代码演练。</p><p id="4bb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，我们开始吧！</p><h1 id="4ed5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1-虚拟示例</h1><p id="38d9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们正在寻找开发代码，有效地搜索我们的功能，并寻找低准确度的领域。对于单变量数值数据，我们使用最高先验密度(HPD)。</p><p id="bdf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">HPD地区是我们数据的密集区域。根据IBM从事FreaAI的研究人员的说法，高密度区域更有可能有可解释和可纠正的不准确性。为了确定这些区域，我们…</p><ol class=""><li id="ab63" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">近似概率密度函数(PDF)。</li><li id="2846" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la mu mv mw mx bi translated">求一条水平线的y值，这条水平线将我们的PDF分为α和1-α。</li></ol><p id="9d72" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们慢慢来，一步一步来。首先，我们需要创建一些虚拟数据…</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="d7e0" class="ni lt iq ne b gy nj nk l nl nm">samples = np.random.normal(loc=[-4,4], size=(1000, 2)).flatten()</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nn"><img src="../Images/1aadcf4f23381511ef7c18a098f7c544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKmgnkBpD3X5D7zmi7DwSA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图2:虚拟双模态数据的直方图。图片作者。</p></figure><p id="bf4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在的数据只是一个向量。取而代之的是，我们想要近似一条平滑的曲线来表示我们在任何给定位置的数据密度。换句话说，我们正在寻找近似的<strong class="kh ir">概率密度函数(PDF) </strong>。</p><p id="ee1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一种估计概率密度函数的方法叫做核密度估计。我们没有时间讨论细节，但是如果你有兴趣的话，这里有一个<a class="ae lr" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.13-kernel-density-estimation.html" rel="noopener ugc nofollow" target="_blank">深入的演练</a>。</p><p id="1c1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的代码中，我们利用高斯核来开发上述数据的平滑近似值。</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="a30b" class="ni lt iq ne b gy nj nk l nl nm">import plotly.express as px<br/>import scipy.stats.kde as kde</span><span id="efd5" class="ni lt iq ne b gy no nk l nl nm"># 1. get upper and lower bounds on space<br/>l = np.min(sample)<br/>u = np.max(sample)</span><span id="9482" class="ni lt iq ne b gy no nk l nl nm"># 2. get x-axis values<br/>x = np.linspace(l, u, 2000)</span><span id="822a" class="ni lt iq ne b gy no nk l nl nm"># 3. get kernel density estimate<br/>density = kde.gaussian_kde(sample)<br/>y = density.evaluate(x)</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi np"><img src="../Images/68a29b1ad49f155fa7ea4ccd0d119a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktNkcWnR0xuMep1QKPkJEw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3:使用高斯核的双模式数据的核密度估计。图片作者。</p></figure><p id="eaa7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经有了一条接近PDF的平滑曲线(图3)，我们就可以开始第二步了。</p><p id="4a9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们希望找到我们的PDF中包含了我们数据的百分之<em class="nq"> p </em>的区域。例如，当p<em class="nq">为0.05时，我们希望5%的数据包含在我们的区域中。这样做的代码有点冗长，所以我们不打算展示它，但是在图4中我们可以看到一个图表，它将我们所有的工作联系在一起。</em></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nr"><img src="../Images/6c2b97c13586a0c2faa1a812aee85aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWZVNU-eIPLIjKadggW1Ew.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4:50% HPD区域的图(阴影区域)。图片作者。</p></figure><p id="02d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分解图4…</p><ul class=""><li id="3c52" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la ns mv mw mx bi translated">蓝色的<strong class="kh ir">框是我们原始数据的直方图。</strong></li><li id="2ff3" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated">橙色的T4曲线是我们对PDF的核密度估计。</li><li id="4ab0" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated"><strong class="kh ir">红色</strong>垂直线是我们PDF中密度最高的点。</li><li id="c227" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated"><strong class="kh ir">灰色</strong>框对应于x轴区域，该区域包含我们50%的数据。</li><li id="4e57" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated"><strong class="kh ir">绿色</strong>水平线用于确定<strong class="kh ir">灰色</strong>框的宽度——绿色<strong class="kh ir">线</strong>和橙色<strong class="kh ir">线</strong>的交点为50%对应的宽度。</li></ul><p id="373b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个图表看起来很复杂，但是我们想要关注的是绿线和它与灰框的关系。如果我们将绿线下移，灰框会变宽，从而容纳更多的数据。相反，如果我们向上移动线条，灰色区域会变得更窄，从而容纳更少的数据。</p><p id="ec12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> FreaAI反复向上移动绿线，直到模型精度有足够的变化。每当这种情况发生时，区域(x轴上的值)被返回给工程师。</strong></p><p id="bf62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过这样做，我们以非常有效的方式接收可解释的数据切片，这可以扩展到大型数据集。</p><h1 id="b043" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2 —训练我们的模型</h1><p id="4709" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在我们已经了解了HPD方法，让我们在一个真实的模型上测试一下。</p><h2 id="a46d" class="ni lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">2.1 —获取数据</h2><p id="1bd9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">所有的模型都需要数据，我们就从这里开始吧。</p><p id="f2ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本帖中，我们将使用一个可以从<a class="ae lr" href="https://github.com/mberk06/DS_academic_papers/tree/master/DiabetesData" rel="noopener ugc nofollow" target="_blank">这里</a>下载的<a class="ae lr" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">糖尿病数据集</a>。行对应于人，列对应于这些人的特征，例如年龄和身体质量指数。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oe"><img src="../Images/7ffc7945377a14ab657d0193839f8aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSuyterRjv_DI4uP4XS8YQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图5:我们数据集的最后4列— <a class="ae lr" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="dac6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图5所示，我们的结果是对应于给定个体是否患有糖尿病的二元标签。</p><p id="3c84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，数据已经被清理了，所以我们只需要将数据帧读入pandas就可以开始了…</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="806c" class="ni lt iq ne b gy nj nk l nl nm">import pandas as pd<br/>df = pd.read_csv('DiabetesData/pima-indians-diabetes.data.csv', header=None)</span></pre><h2 id="dc55" class="ni lt iq bd lu nt nu dn ly nv nw dp mc ko nx ny me ks nz oa mg kw ob oc mi od bi translated">2.2—训练模型</h2><p id="cef1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">有了熊猫数据框架，让我们来训练我们的黑盒模型。我们选择适合的具体模型并不重要，但是我们将使用XGBoost，因为它的速度、开箱即用的准确性和受欢迎程度。</p><p id="1afe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个代码块中的模型训练…</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="17b4" class="ni lt iq ne b gy nj nk l nl nm"># split data into X and y<br/>mask = np.array(list(df)) == 'outcome'<br/>X = df.loc[:,~mask]<br/>Y = df.loc[:,mask]</span><span id="7a3a" class="ni lt iq ne b gy no nk l nl nm"># split data into train and test sets<br/>seed = 7<br/>test_size = 0.33<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>                                     X, Y,<br/>                                     test_size=test_size, <br/>                                     random_state=seed)</span><span id="1168" class="ni lt iq ne b gy no nk l nl nm"># fit model no training data<br/>model = XGBClassifier()<br/>model.fit(X_train, y_train)</span><span id="ccbd" class="ni lt iq ne b gy no nk l nl nm"># make predictions for test data<br/>y_pred = model.predict(X_test)<br/>predictions = [round(value) for value in y_pred]</span><span id="7fbb" class="ni lt iq ne b gy no nk l nl nm"># evaluate predictions<br/>accuracy = accuracy_score(y_test, predictions)<br/>print("Accuracy: %.2f%%" % (accuracy * 100.0))</span><span id="e048" class="ni lt iq ne b gy no nk l nl nm"># add accuracy and return<br/>out = pd.concat([X_test, y_test], axis=1, ignore_index=True)<br/>out.columns = list(df)</span><span id="8fa7" class="ni lt iq ne b gy no nk l nl nm">accuracy_bool = (np.array(y_test).flatten() ==<br/>                 np.array(predictions))<br/>out['accuracy_bool'] = accuracy_bool</span><span id="9c81" class="ni lt iq ne b gy no nk l nl nm">out    </span></pre><p id="4944" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了最后一步，所有的步骤都应该是不言自明的；在训练模型之后，我们返回带有一个<code class="fe of og oh ne b">accuracy_bool</code>列的测试数据帧，该列对应于我们是否正确预测了标签。这用于确定不同数据分割的准确性。</p><p id="84a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们继续之前，让我们回到我们的高层次目标。上面的XGBoost模型展示了71%的样本外准确性，但是数据的某些部分显示了非常不同的准确性水平，这难道不合理吗？</p><p id="68ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嗯，这是我们的假设——我们的错误分类(可能)有系统性。</p><p id="a743" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看是否能识别这些系统区域。</p><h1 id="4b94" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3 —真实的东西</h1><p id="fb8b" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">如上所述，我们利用HPD方法的方式是，我们将反复改变我们正在寻找的数据的百分比，以寻找准确性的“显著”下降。</p><p id="f944" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简洁起见，我们很遗憾不能做到超级科学，我们将不得不省去助手。然而，这里的代码改编自<a class="ae lr" href="https://github.com/aloctavodia/BAP/blob/master/first_edition/code/Chp1/hpd.py" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="c25b" class="ni lt iq ne b gy nj nk l nl nm">import numpy as np</span><span id="7c60" class="ni lt iq ne b gy no nk l nl nm"># 1. Get percents to iterate over<br/>start, end, increment = 0.5, 0.95, 0.05<br/>percents = np.arange(start, end, increment)[::-1]</span><span id="8c96" class="ni lt iq ne b gy no nk l nl nm"># 2. Create prior vars<br/>prior_indices = {} <br/>prior_acc = None<br/>prior_p = None</span><span id="0681" class="ni lt iq ne b gy no nk l nl nm">for p in percents:<br/>  # 3. run HDP and get indices of data in percent<br/>  *_, indices = hpd_grid(col, percent=p)</span><span id="c295" class="ni lt iq ne b gy no nk l nl nm">  # 4. Calculate accuracy<br/>  acc = get_accuracy(indices)</span><span id="4ff0" class="ni lt iq ne b gy no nk l nl nm">  # 5. Save if accuracy increases by &gt; 0.01<br/>  if prior_acc is not None and acc - prior_acc &gt; 0.01:<br/>    out[f'{p}-{prior_p}'] = (prior_indices - set(indices), acc - prior_acc)</span><span id="2188" class="ni lt iq ne b gy no nk l nl nm">  # 6. Reset<br/>  prior_indices = set(indices)<br/>  prior_acc = acc<br/>  prior_p = p</span></pre><p id="7e12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这有用吗？嗯，代码很简单(这就是为什么它没有被链接)，但是我们确实看到我们的8个预测器列中有5个有低精度区域。一些快速击球手…</p><ul class=""><li id="817d" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la ns mv mw mx bi translated">没有误差的<strong class="kh ir">栏是<em class="nq">皮肤厚度</em>、<em class="nq"> bmi </em>、<em class="nq">糖尿病血统</em></strong></li><li id="a4d6" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated">准确度下降范围从<strong class="kh ir"> 1.00到3.18% </strong></li><li id="d666" class="mp mq iq kh b ki my kl mz ko na ks nb kw nc la ns mv mw mx bi translated">多达<strong class="kh ir"> 4 </strong>个百分比变化显示精确度下降</li></ul><p id="03ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了给出更多的上下文，下面是列<em class="nq">胰岛素</em>的输出。</p><pre class="lc ld le lf gt nd ne nf ng aw nh bi"><span id="0736" class="ni lt iq ne b gy nj nk l nl nm">60-65%<br/>&gt; indices: {65, 129, 131, 226, 137, 170, 46, 81, 52, 26, 155}<br/>&gt; accuracy drop: 0.017309115</span><span id="6521" class="ni lt iq ne b gy no nk l nl nm">65-70%<br/>&gt; indices: {33, 67, 134, 8, 76, 143, 175}<br/>&gt; accuracy drop: 0.015932642</span><span id="36c2" class="ni lt iq ne b gy no nk l nl nm">80-85%<br/>&gt; indices: {193, 98, 35, 196, 109, 142, 176, 210, 21, 86}<br/>&gt; accuracy drop: 0.010059552<br/></span></pre><p id="14cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们知道了哪些指数返回了准确性的巨大变化，我们可以做一些探索性的分析，并尝试诊断问题。</p><p id="51f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，如果您想探索原始的最高先验密度函数，如下所示。请注意，它很快从<a class="ae lr" href="https://github.com/aloctavodia/BAP/blob/master/first_edition/code/Chp1/hpd.py" rel="noopener ugc nofollow" target="_blank">这个回购</a>改编而来，需要更多的工作才能量产。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="7b9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下周我计划发布决策树逻辑的完整代码。敬请期待！</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="5611" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nq">感谢阅读！我会再写20篇文章，把学术研究带到DS行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>