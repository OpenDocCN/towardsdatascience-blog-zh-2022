<html>
<head>
<title>Disease Informed Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">疾病通知神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/disease-informed-neural-networks-aa1f17f598a4#2022-01-07">https://towardsdatascience.com/disease-informed-neural-networks-aa1f17f598a4#2022-01-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="dfb5" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">疾病通知神经网络</h1></div><div class=""><h2 id="eca3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络能够学习疾病如何传播，预测其进展，并找到其独特的参数(如死亡率)。</h2></div><p id="fd52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我介绍了疾病通知神经网络(DINNs)。在<a class="ae lb" href="https://arxiv.org/abs/2110.05445" rel="noopener ugc nofollow" target="_blank">论文</a>中，我们使用DINNs来确定11种高传染性和致命性疾病的动态。这些系统的复杂性和参数数量各不相同。这些疾病包括COVID，炭疽，艾滋病毒，寨卡，天花，结核病，肺炎，埃博拉病毒，登革热，脊髓灰质炎和麻疹。完整的代码&amp;实验可以在<a class="ae lb" href="https://github.com/Shaier/DINN" rel="noopener ugc nofollow" target="_blank">这里</a>找到，具体的教程笔记本可以在<a class="ae lb" href="https://github.com/Shaier/DINN/blob/master/tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/5fca5a0d771492e76b1b259dc1c081de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEiqlXwr5FU13gy9e9DwzQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">DINN示例架构。输入是不同的时间点(第1天、第2天、…、第100天)，输出是每个时间点每个疾病区室(易感、感染、死亡等)的值。图片作者。</p></figure><p id="ac29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">疾病在它们影响的有机体、它们的症状和它们传播的速度方面有很大的不同。为了更好地理解疾病，就像大多数机器学习问题一样，我们需要数据。在本教程中，为了简化，我们将生成数据，而不是从真实世界环境中收集数据(尽管在本文中，我们确实使用了来自<a class="ae lb" href="https://github.com/CSSEGISandData/COVID-19" rel="noopener ugc nofollow" target="_blank">这里</a>的真实COVID数据)。</p><p id="c9c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于那些没有流行病学和微分方程背景的人来说，我会在这里保持简单。DINN输入时间步长(第1天、第2天、…、第100天)，每天输出每个疾病区间中的人数。“车厢”只是对不同群体的一个花哨称呼。例如，我们有“受感染”组，它代表人口中有多少人被感染(它实际上是一个计数，比如5个人)。“易感”群体代表有多少人没有被感染，但可以被感染。诸如此类。这些区间/组是由数学家/其他人决定的，他们对疾病如何传播进行建模，并在数学上写成一组微分方程。</p><p id="203e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，这就是背景，让我们开始编码。</p><h1 id="ca0b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">考虑下面的微分方程系统</h1><p id="08b9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">dS/dt =-(α/N)S I</p><p id="5214" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">dI/dt =(α/N)S I-βI-γI</p><p id="bf7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">dD/dt =γI</p><p id="c11b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">dR/dt =βI</p><p id="6e1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><p id="6b4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">beta =“有效/明显”每日回收率</p><p id="034d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">gamma = "有效/明显"每日死亡率</p><p id="54e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">α=感染率</p><p id="7878" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">N =人口规模</p><blockquote class="mp mq mr"><p id="7f69" class="kf kg ms kh b ki kj jr kk kl km ju kn mt kp kq kr mu kt ku kv mv kx ky kz la ij bi translated">他的系统代表了一个COVID模型，有4个区间:易感(ds/dt)、感染(dI/dt)、死亡(dD/dt)和康复(dR/dt)。同样为了简化，这些车厢只是对每个车厢里有多少人的计数。</p></blockquote><p id="9e86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦有了方程式，生成数据就非常简单:</p><p id="ba23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">导入用于生成和可视化数据的库</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="8d61" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">import</strong> numpy <strong class="mx ir">as</strong> np<br/><strong class="mx ir">from</strong> scipy.integrate <strong class="mx ir">import</strong> odeint<br/><strong class="mx ir">import</strong> matplotlib.pyplot <strong class="mx ir">as</strong> plt</span></pre><p id="0ca4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们首先设定一些从文献中获得的信息</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="9367" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># Initial conditions (from the literature)</em><br/>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/><br/>S0 <strong class="mx ir">=</strong> N <strong class="mx ir">-</strong> 1 <em class="ms">#everyone starts out as susceptible, except for 1 person that is infected</em><br/>I0 <strong class="mx ir">=</strong> 1 <em class="ms">#1 infected person</em><br/>D0 <strong class="mx ir">=</strong> 0<br/>R0 <strong class="mx ir">=</strong> 0<br/><br/><em class="ms"># A grid of time points (in days)</em><br/>t <strong class="mx ir">=</strong> np<strong class="mx ir">.</strong>linspace(0, 500, 100) <em class="ms">#from day 0 to day 500, generate 100 points</em><br/><br/><em class="ms">#parameters (from the literature)</em><br/>alpha <strong class="mx ir">=</strong> 0.191<br/>beta <strong class="mx ir">=</strong> 0.05<br/>gamma <strong class="mx ir">=</strong> 0.0294</span></pre><h1 id="21a5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">然后，我们用一个函数写出方程组，该函数将计算每个时间步长(例如一天)下每个区间的值</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="f831" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># The SIDR model differential equations.</em><br/><strong class="mx ir">def</strong> deriv(y, t, alpha, betta, gamma):<br/>    S, I, D, R <strong class="mx ir">=</strong> y<br/>    dSdt <strong class="mx ir">=</strong> <strong class="mx ir">-</strong> (alpha <strong class="mx ir">/</strong> N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I<br/>    dIdt <strong class="mx ir">=</strong> (alpha <strong class="mx ir">/</strong> N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> beta <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> gamma <strong class="mx ir">*</strong> I <br/>    dDdt <strong class="mx ir">=</strong> gamma <strong class="mx ir">*</strong> I<br/>    dRdt <strong class="mx ir">=</strong> beta <strong class="mx ir">*</strong> I<br/><br/>    <strong class="mx ir">return</strong> dSdt, dIdt, dDdt, dRdt</span></pre><h1 id="0b1d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">然后，我们将初始条件传递给函数，以获得我们之前选择的长度(500天，每个区间100个数据点)的每个区间的值</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="b30a" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># Initial conditions vector</em><br/>y0 <strong class="mx ir">=</strong> S0, I0, D0, R0<br/><em class="ms"># Integrate the SIR equations over the time grid, t.</em><br/>ret <strong class="mx ir">=</strong> odeint(deriv, y0, t, args<strong class="mx ir">=</strong>(alpha, beta, gamma))<br/>S, I, D, R <strong class="mx ir">=</strong> ret<strong class="mx ir">.</strong>T</span></pre><h1 id="e2ad" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我们现在可以画出结果</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="a513" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># Plot the data</em><br/>fig <strong class="mx ir">=</strong> plt<strong class="mx ir">.</strong>figure(figsize<strong class="mx ir">=</strong>(12,12))<br/>ax <strong class="mx ir">=</strong> fig<strong class="mx ir">.</strong>add_subplot(111, facecolor<strong class="mx ir">=</strong>'#dddddd', axisbelow<strong class="mx ir">=True</strong>)<br/>ax<strong class="mx ir">.</strong>set_facecolor('xkcd:white')<br/><br/>ax<strong class="mx ir">.</strong>plot(t, S, 'violet', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Susceptible', linestyle<strong class="mx ir">=</strong>'dashed')<br/>ax<strong class="mx ir">.</strong>plot(t, I, 'darkgreen', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Infected', linestyle<strong class="mx ir">=</strong>'dashed')<br/>ax<strong class="mx ir">.</strong>plot(t, D, 'blue', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Dead', linestyle<strong class="mx ir">=</strong>'dashed')<br/>ax<strong class="mx ir">.</strong>plot(t, R, 'red', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Recovered', linestyle<strong class="mx ir">=</strong>'dashed')<br/><br/>ax<strong class="mx ir">.</strong>set_xlabel('Time /days')<br/>ax<strong class="mx ir">.</strong>set_ylabel('Number')<br/>ax<strong class="mx ir">.</strong>yaxis<strong class="mx ir">.</strong>set_tick_params(length<strong class="mx ir">=</strong>0)<br/>ax<strong class="mx ir">.</strong>xaxis<strong class="mx ir">.</strong>set_tick_params(length<strong class="mx ir">=</strong>0)<br/>ax<strong class="mx ir">.</strong>grid(b<strong class="mx ir">=True</strong>, which<strong class="mx ir">=</strong>'major', c<strong class="mx ir">=</strong>'black', lw<strong class="mx ir">=</strong>0.2, ls<strong class="mx ir">=</strong>'-')<br/>legend <strong class="mx ir">=</strong> ax<strong class="mx ir">.</strong>legend()<br/>legend<strong class="mx ir">.</strong>get_frame()<strong class="mx ir">.</strong>set_alpha(0.5)<br/><strong class="mx ir">for</strong> spine <strong class="mx ir">in</strong> ('top', 'right', 'bottom', 'left'):<br/>    ax<strong class="mx ir">.</strong>spines[spine]<strong class="mx ir">.</strong>set_visible(<strong class="mx ir">False</strong>)<br/>plt<strong class="mx ir">.</strong>show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ng"><img src="../Images/aaa9dc25b1c47c9b166d32d647483a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wGjr90VQcVf4eBhXNoU-g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">COVID生成的数据。图片作者。</p></figure><h1 id="3379" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">并将结果保存为csv文件</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="c641" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms">#save to csv file</em><br/>COVID_Data <strong class="mx ir">=</strong> np<strong class="mx ir">.</strong>asarray([t, S, I, D, R]) </span><span id="c141" class="nb lt iq mx b gy nh nd l ne nf">np<strong class="mx ir">.</strong>savetxt("COVID_Tutorial.csv", COVID_Data, delimiter<strong class="mx ir">=</strong>",")</span></pre><h1 id="7d5c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">太好了！现在我们有数据可以处理了。开始训练神经网络。</h1><p id="ce7d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我将把整个训练过程分成几个部分。这些部分本身可能是不可运行的(也就是说，你不能运行代码——这就是为什么我要把它们注释掉)。关键是，我认为首先理解每个部分在做什么，然后看看它们是如何一起工作的，这将更容易理解这个过程。我将在主函数前写“&gt; &gt; &gt; &gt;”，这样更容易理解(因为我会在后面添加一些部分)。</p><h1 id="3c1e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.导入库</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="7da2" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">import</strong> torch<br/><strong class="mx ir">from</strong> torch.autograd <strong class="mx ir">import</strong> grad<br/><strong class="mx ir">import</strong> torch.nn <strong class="mx ir">as</strong> nn<br/><strong class="mx ir">from</strong> numpy <strong class="mx ir">import</strong> genfromtxt<br/><strong class="mx ir">import</strong> torch.optim <strong class="mx ir">as</strong> optim<br/><strong class="mx ir">import</strong> matplotlib.pyplot <strong class="mx ir">as</strong> plt<br/><strong class="mx ir">import</strong> torch.nn.functional <strong class="mx ir">as</strong> F</span><span id="6909" class="nb lt iq mx b gy nh nd l ne nf">torch<strong class="mx ir">.</strong>manual_seed(1234) <em class="ms">#set seed (optional)</em></span></pre><h1 id="024f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.加载数据</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="e7a3" class="nb lt iq mx b gy nc nd l ne nf">covid_data <strong class="mx ir">=</strong> genfromtxt('COVID_Tutorial.csv', delimiter<strong class="mx ir">=</strong>',') <em class="ms">#in the form of [t,S,I,D,R]</em></span></pre><h1 id="041f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.定义类别</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="66b6" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt; MAIN FUNCTION<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms"># remember that the data was saved as [t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <em class="ms"># here all the "loading the data" and training is happening</em><br/>        <strong class="mx ir">pass</strong></span></pre><h1 id="3ad6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.现在我们需要定义一些初始条件</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="1a3b" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em></span><span id="7a3e" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)</span><span id="22cd" class="nb lt iq mx b gy nh nd l ne nf">        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em></span></pre><h1 id="08ab" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.这部分有两个选项。要么你知道参数(α，β，γ)的值，要么你不知道。</h1><p id="4780" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">如果您想知道，如果我们只是使用它们生成数据，我们怎么可能不知道它们的值，请记住，我们也可以从环境中获取数据，因此我们可能没有参数</p><h1 id="6f58" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如果我们不知道它们的值，我们让神经网络来学习</h1><p id="a7e1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">注意“蒂尔达”部分的原因很快就清楚了。基本上就是把变量限制在一定的范围内(想象一下告诉神经网络学习alpha，但不是从负无穷大到无穷大，而是从-100到100)</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="8451" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))</em><br/><em class="ms"># self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))</em><br/><em class="ms"># self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))</em></span></pre><h1 id="6656" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如果我们知道它们的值，我们只需设置它</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="81d9" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># self.alpha_tilda = torch.tensor(0.191)</em><br/><em class="ms"># self.beta_tilda = torch.tensor(0.05)</em><br/><em class="ms"># self.gamma_tilda = torch.tensor(0.0294)</em></span></pre><h1 id="b48b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">假设我们不认识他们:</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="3f70" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em></span><span id="8f58" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)</span><span id="42b5" class="nb lt iq mx b gy nh nd l ne nf">        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em></span><span id="963d" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))</span></pre><h1 id="9f1b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">6.标准化数据</h1><p id="cfe2" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">还记得我们的人口规模是5900万吗？还记得只有1名感染者和几乎5900万易感人群吗？这些价值观的巨大变化对网络学习来说相当具有挑战性。因此，为了便于训练，我们将每个区间标准化为0到1之间</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="bbb4" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># find values for normalization</em></span><span id="b0bc" class="nb lt iq mx b gy nh nd l ne nf"><em class="ms"># max values</em><br/><em class="ms"># self.S_max = max(self.S)</em><br/><em class="ms"># self.I_max = max(self.I)</em><br/><em class="ms"># self.D_max = max(self.D)</em><br/><em class="ms"># self.R_max = max(self.R)</em></span><span id="67bf" class="nb lt iq mx b gy nh nd l ne nf"><em class="ms"># min values</em><br/><em class="ms"># self.S_min = min(self.S)</em><br/><em class="ms"># self.I_min = min(self.I)</em><br/><em class="ms"># self.D_min = min(self.D)</em><br/><em class="ms"># self.R_min = min(self.R)</em></span><span id="1a6b" class="nb lt iq mx b gy nh nd l ne nf"><em class="ms"># create new normalized parameters (which is why the "hat" parts)</em><br/><em class="ms"># self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)</em><br/><em class="ms"># self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)</em><br/><em class="ms"># self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)</em><br/><em class="ms"># self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)</em></span><span id="db01" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em></span><span id="e6e5" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)</span><span id="d08d" class="nb lt iq mx b gy nh nd l ne nf">        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em></span><span id="f196" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))</span><span id="4b12" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)</span><span id="0d1d" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)</span></pre><h1 id="b9bb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">7.稍后计算梯度的一个有点“黑客”的方法</h1><p id="8b1a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">PyTorch似乎没有计算梯度的简单方法。问题是“grad”只知道如何从标量张量传播梯度(我们网络的输出不是)，这就是为什么我必须计算雅可比矩阵。所以我没有计算整个雅可比矩阵，而是用这种“黑客”的方式</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="7937" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># matrices (x4 for S,I,D,R) for the gradients</em><br/><em class="ms"># What's important here:</em><br/><em class="ms"># We have 4 compartments, hence the value 4 in "torch.zeros((len(self.t), 4))". </em><br/><em class="ms"># If we had 20 compartments we would write torch.zeros((len(self.t), 20))</em><br/><em class="ms"># Also, we're setting each specific column in the formed matrices to 1</em></span><span id="7c4b" class="nb lt iq mx b gy nh nd l ne nf"><em class="ms"># self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1</em><br/><em class="ms"># self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1</em><br/><em class="ms"># self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1</em><br/><em class="ms"># self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1</em></span><span id="9b5f" class="nb lt iq mx b gy nh nd l ne nf"><em class="ms"># See (https://stackoverflow.com/questions/67472361/using-pytorchs-autograd-efficiently-with-tensors-by-calculating-the-jacobian) for more details</em></span><span id="4104" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em></span><span id="307d" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)</span><span id="e8b5" class="nb lt iq mx b gy nh nd l ne nf">        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em></span><span id="b656" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))</span><span id="6741" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)</span><span id="bcda" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        </span><span id="ec24" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1</span></pre><h1 id="755d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">8.让我们初始化网络和可学习的参数</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="71a4" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># # Initializing the neural network</em><br/><em class="ms"># self.net_sidr = self.Net_sidr()</em><br/><br/><em class="ms"># # adding the parameters (alpha, beta, gamma) to the list of learnable parameters (basically, without this part only the neural network's weights will be updated, so we're telling the model to learn alpha, beta, and gamma as well)</em><br/><em class="ms"># self.params = list(self.net_sidr.parameters())</em><br/><em class="ms"># self.params.extend(list([self.alpha_tilda, self.beta_tilda, self.gamma_tilda]))</em></span><span id="e553" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))</span></pre><h1 id="7432" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">9.迫使参数在一定范围内</h1><p id="8e51" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">正如我之前提到的，我们可以让模型学习从负无穷大到无穷大的参数，但我们为什么要这样做呢？这些参数通常有一些合理的范围。这里我们强制这些范围</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="45cc" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms">#force parameters to be in the range of (-1, 1)</em><br/>@property<br/><strong class="mx ir">def</strong> alpha(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <br/><br/>@property<br/><strong class="mx ir">def</strong> beta(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <br/><br/>@property<br/><strong class="mx ir">def</strong> gamma(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <br/><br/><br/><em class="ms">#note that you can easily play with that:</em><br/><br/><em class="ms">#force parameters to be in various ranges</em><br/>@property<br/><strong class="mx ir">def</strong> alpha(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <strong class="mx ir">*</strong> 0.5 <em class="ms"># range of (-0.5, 0.5)</em><br/><br/>@property<br/><strong class="mx ir">def</strong> beta(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <strong class="mx ir">*</strong> 0.01 <strong class="mx ir">+</strong> 1 <em class="ms"># range of (-0.99, 1.01)</em><br/><br/>@property<br/><strong class="mx ir">def</strong> gamma(self):<br/>    <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <strong class="mx ir">*</strong> 100 <em class="ms"># range of (-100, 100)</em><br/><br/><br/><em class="ms"># Also note that we call these alpha, beta, and gamma (in comparison to "alpha_tilda", etc. from before)</em><br/><br/><em class="ms"># If you know the values of the parameters you just need to change this to:</em><br/>@property<br/><strong class="mx ir">def</strong> alpha(self):<br/>    <strong class="mx ir">return</strong> self<strong class="mx ir">.</strong>alpha_tilda<br/><br/>@property<br/><strong class="mx ir">def</strong> beta(self):<br/>    <strong class="mx ir">return</strong> self<strong class="mx ir">.</strong>beta_tilda<br/><br/>@property<br/><strong class="mx ir">def</strong> gamma(self):<br/>    <strong class="mx ir">return</strong> self<strong class="mx ir">.</strong>gamma_tilda</span><span id="be0d" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda)</span></pre><h1 id="cbb8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">10.创建神经网络</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="1de2" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>    <strong class="mx ir">def</strong> __init__(self):<br/>        super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>        self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>        self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>        self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>    <strong class="mx ir">def</strong> forward(self, t_batch):<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>        sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>        sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>        <strong class="mx ir">return</strong> sidr</span><span id="0417" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr</span></pre><h1 id="dfbc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">11.现在有点复杂的部分，我们创建另一个函数，它采用时间步长批处理，并将其传递给神经网络</h1><p id="007e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们主要是想优化神经网络，还有这个有方程组的函数</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="7b87" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">def</strong> net_f(self, t_batch):<br/>        <br/>        <em class="ms">#pass the timesteps batch to the neural network</em><br/>        sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>        <br/>        <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>        S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]</span><span id="80be" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr<br/><br/>    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]</span></pre><h1 id="ce83" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">12.我们现在想要得到每个隔间对时间的导数(这就是为什么我们必须做“hacky”雅可比部分)</h1><p id="d1f9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们这样做是为了插入方程组(你将在下一步看到)</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="df90" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># #S_t</em><br/><em class="ms"># sidr_hat.backward(self.m1, retain_graph=True)</em><br/><em class="ms"># S_hat_t = self.t.grad.clone()</em><br/><em class="ms"># self.t.grad.zero_()</em><br/><br/><em class="ms"># #I_t</em><br/><em class="ms"># sidr_hat.backward(self.m2, retain_graph=True)</em><br/><em class="ms"># I_hat_t = self.t.grad.clone()</em><br/><em class="ms"># self.t.grad.zero_()</em><br/><br/><em class="ms"># #D_t</em><br/><em class="ms"># sidr_hat.backward(self.m3, retain_graph=True)</em><br/><em class="ms"># D_hat_t = self.t.grad.clone()</em><br/><em class="ms"># self.t.grad.zero_()</em><br/><br/><em class="ms"># #R_t</em><br/><em class="ms"># sidr_hat.backward(self.m4, retain_graph=True)</em><br/><em class="ms"># R_hat_t = self.t.grad.clone()</em><br/><em class="ms"># self.t.grad.zero_()</em></span><span id="2824" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr<br/><br/>    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]<br/><br/>            <em class="ms">#S_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m1, retain_graph<strong class="mx ir">=True</strong>)<br/>            S_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#I_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m2, retain_graph<strong class="mx ir">=True</strong>)<br/>            I_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#D_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m3, retain_graph<strong class="mx ir">=True</strong>)<br/>            D_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#R_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m4, retain_graph<strong class="mx ir">=True</strong>)<br/>            R_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()</span></pre><h1 id="ecfe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">13.我们现在取消了区间的标准化，因为我们实际上不希望方程改变，我们只是希望网络学习得更快</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="03b3" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># #unnormalize</em><br/><em class="ms"># S = self.S_min + (self.S_max - self.S_min) * S_hat</em><br/><em class="ms"># I = self.I_min + (self.I_max - self.I_min) * I_hat</em><br/><em class="ms"># D = self.D_min + (self.D_max - self.D_min) * D_hat      </em><br/><em class="ms"># R = self.R_min + (self.R_max - self.R_min) * R_hat</em></span><span id="28be" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr<br/><br/>    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]<br/><br/>            <em class="ms">#S_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m1, retain_graph<strong class="mx ir">=True</strong>)<br/>            S_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#I_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m2, retain_graph<strong class="mx ir">=True</strong>)<br/>            I_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#D_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m3, retain_graph<strong class="mx ir">=True</strong>)<br/>            D_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#R_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m4, retain_graph<strong class="mx ir">=True</strong>)<br/>            R_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_() <br/><br/>            <em class="ms">#unnormalize</em><br/>            S <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_hat<br/>            I <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_hat<br/>            D <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_hat      <br/>            R <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_hat</span></pre><h1 id="81c8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">14.最后(几乎)，我们写出我们想要学习的方程组</h1><p id="88ef" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这些几乎与最初的方程组相同，除了我们在这里有另一个规范化组件(例如"/ (self。S_max —自我。S_min)")，并且我们在这里使用偏导数(例如“S”相对于时间)。我们基本上是把每个隔间的右边移到左边(因此导数后面有负号)</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="84ff" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># f1_hat = S_hat_t - (-(self.alpha / self.N) * S * I)  / (self.S_max - self.S_min)</em><br/><em class="ms"># f2_hat = I_hat_t - ((self.alpha / self.N) * S * I - self.beta * I - self.gamma * I ) / (self.I_max - self.I_min)</em><br/><em class="ms"># f3_hat = D_hat_t - (self.gamma * I) / (self.D_max - self.D_min)</em><br/><em class="ms"># f4_hat = R_hat_t - (self.beta * I ) / (self.R_max - self.R_min)</em></span><span id="540f" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em></span><span id="c58f" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)</span><span id="3154" class="nb lt iq mx b gy nh nd l ne nf">        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em></span><span id="25b1" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))</span><span id="6b50" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)</span><span id="0cdb" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        </span><span id="3944" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1</span><span id="3009" class="nb lt iq mx b gy nh nd l ne nf">        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))</span><span id="60e7" class="nb lt iq mx b gy nh nd l ne nf">    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em></span><span id="a67f" class="nb lt iq mx b gy nh nd l ne nf">    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em></span><span id="dc7f" class="nb lt iq mx b gy nh nd l ne nf">    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()</span><span id="d586" class="nb lt iq mx b gy nh nd l ne nf">            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em></span><span id="e4c7" class="nb lt iq mx b gy nh nd l ne nf">        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr</span><span id="a336" class="nb lt iq mx b gy nh nd l ne nf">    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]</span><span id="a3c2" class="nb lt iq mx b gy nh nd l ne nf">            <em class="ms">#S_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m1, retain_graph<strong class="mx ir">=True</strong>)<br/>            S_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()</span><span id="a938" class="nb lt iq mx b gy nh nd l ne nf">            <em class="ms">#I_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m2, retain_graph<strong class="mx ir">=True</strong>)<br/>            I_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()</span><span id="d983" class="nb lt iq mx b gy nh nd l ne nf">            <em class="ms">#D_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m3, retain_graph<strong class="mx ir">=True</strong>)<br/>            D_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()</span><span id="c102" class="nb lt iq mx b gy nh nd l ne nf">            <em class="ms">#R_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m4, retain_graph<strong class="mx ir">=True</strong>)<br/>            R_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_() </span><span id="c5f8" class="nb lt iq mx b gy nh nd l ne nf">            <em class="ms">#unnormalize</em><br/>            S <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_hat<br/>            I <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_hat<br/>            D <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_hat      <br/>            R <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_hat        </span><span id="8ce7" class="nb lt iq mx b gy nh nd l ne nf">            f1_hat <strong class="mx ir">=</strong> S_hat_t <strong class="mx ir">-</strong> (<strong class="mx ir">-</strong>(self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I)  <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>            f2_hat <strong class="mx ir">=</strong> I_hat_t <strong class="mx ir">-</strong> ((self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>            f3_hat <strong class="mx ir">=</strong> D_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>            f4_hat <strong class="mx ir">=</strong> R_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)</span></pre><h1 id="c2cc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">15.最后，我们返回我们学到的想要优化的值— — S、I、D、R和每个系统的区间(例如f1_hat)</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="dd0e" class="nb lt iq mx b gy nc nd l ne nf"><em class="ms"># return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat</em></span><span id="6d7c" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr<br/><br/>    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]<br/><br/>            <em class="ms">#S_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m1, retain_graph<strong class="mx ir">=True</strong>)<br/>            S_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#I_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m2, retain_graph<strong class="mx ir">=True</strong>)<br/>            I_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#D_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m3, retain_graph<strong class="mx ir">=True</strong>)<br/>            D_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#R_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m4, retain_graph<strong class="mx ir">=True</strong>)<br/>            R_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_() <br/><br/>            <em class="ms">#unnormalize</em><br/>            S <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_hat<br/>            I <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_hat<br/>            D <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_hat      <br/>            R <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_hat        <br/><br/>            f1_hat <strong class="mx ir">=</strong> S_hat_t <strong class="mx ir">-</strong> (<strong class="mx ir">-</strong>(self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I)  <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>            f2_hat <strong class="mx ir">=</strong> I_hat_t <strong class="mx ir">-</strong> ((self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>            f3_hat <strong class="mx ir">=</strong> D_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>            f4_hat <strong class="mx ir">=</strong> R_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>            <strong class="mx ir">return</strong> f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat</span></pre><h1 id="57ae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">16.培训过程:</h1><p id="c12a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这里，我们只创建一个名为“train”的函数，它将需要多个历元来训练，并将训练网络</p><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="549a" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">def</strong> train(self, n_epochs):<br/>      <em class="ms"># train</em><br/>      print('\nstarting training...\n')<br/>      <br/>      <strong class="mx ir">for</strong> epoch <strong class="mx ir">in</strong> range(n_epochs):<br/>        <em class="ms"># lists to hold the output (maintain only the final epoch)</em><br/>        S_pred_list <strong class="mx ir">=</strong> []<br/>        I_pred_list <strong class="mx ir">=</strong> []<br/>        D_pred_list <strong class="mx ir">=</strong> []<br/>        R_pred_list <strong class="mx ir">=</strong> []<br/><br/>        <em class="ms"># we pass the timesteps batch into net_f</em><br/>        f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_f(self<strong class="mx ir">.</strong>t_batch) <em class="ms"># net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat</em><br/>        <br/>        self<strong class="mx ir">.</strong>optimizer<strong class="mx ir">.</strong>zero_grad() <em class="ms">#zero grad</em><br/>        <br/>        <em class="ms">#append the values to plot later (note that we unnormalize them here for plotting)</em><br/>        S_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_pred)<br/>        I_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_pred)<br/>        D_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_pred)<br/>        R_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_pred)<br/><br/>        <em class="ms">#calculate the loss --- MSE of the neural networks output and each compartment</em><br/>        loss <strong class="mx ir">=</strong> (torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">-</strong> S_pred))<strong class="mx ir">+</strong> <br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">-</strong> I_pred))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">-</strong> D_pred))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">-</strong> R_pred))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f1))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f2))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f3))<strong class="mx ir">+</strong><br/>                torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f4))<br/>                ) <br/><br/>        loss<strong class="mx ir">.</strong>backward()<br/>        self<strong class="mx ir">.</strong>optimizer<strong class="mx ir">.</strong>step()<br/>        self<strong class="mx ir">.</strong>scheduler<strong class="mx ir">.</strong>step() <br/><br/>        <em class="ms"># append the loss value (we call "loss.item()" because we just want the value of the loss and not the entire computational graph)</em><br/>        self<strong class="mx ir">.</strong>losses<strong class="mx ir">.</strong>append(loss<strong class="mx ir">.</strong>item())<br/><br/>        <strong class="mx ir">if</strong> epoch <strong class="mx ir">%</strong> 1000 <strong class="mx ir">==</strong> 0:          <br/>          print('\nEpoch ', epoch)<br/><br/>          print('alpha: (goal 0.191 ', self<strong class="mx ir">.</strong>alpha)<br/>          print('beta: (goal 0.05 ', self<strong class="mx ir">.</strong>beta)<br/>          print('gamma: (goal 0.0294 ', self<strong class="mx ir">.</strong>gamma)<br/><br/>          print('#################################')                <br/><br/>      <strong class="mx ir">return</strong> S_pred_list, I_pred_list, D_pred_list, R_pred_list</span><span id="ccb3" class="nb lt iq mx b gy nh nd l ne nf"><strong class="mx ir">#&gt;&gt;&gt;&gt;<br/>class</strong> DINN(nn<strong class="mx ir">.</strong>Module):<br/>    <strong class="mx ir">def</strong> __init__(self, t, S_data, I_data, D_data, R_data): <em class="ms">#[t,S,I,D,R]</em><br/>        super(DINN, self)<strong class="mx ir">.</strong>__init__()<br/>        <br/>        self<strong class="mx ir">.</strong>N <strong class="mx ir">=</strong> 59e6 <em class="ms">#population size</em><br/>        <br/>        <em class="ms">#for the time steps, we need to convert them to a tensor, a float, and eventually to reshape it so it can be used as a batch</em><br/>        self<strong class="mx ir">.</strong>t <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(t, requires_grad<strong class="mx ir">=True</strong>)<br/>        self<strong class="mx ir">.</strong>t_float <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>float()<br/>        self<strong class="mx ir">.</strong>t_batch <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>reshape(self<strong class="mx ir">.</strong>t_float, (len(self<strong class="mx ir">.</strong>t),1)) <em class="ms">#reshape for batch </em><br/><br/>        <em class="ms">#for the compartments we just need to convert them into tensors</em><br/>        self<strong class="mx ir">.</strong>S <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(S_data)<br/>        self<strong class="mx ir">.</strong>I <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(I_data)<br/>        self<strong class="mx ir">.</strong>D <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(D_data)<br/>        self<strong class="mx ir">.</strong>R <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>tensor(R_data)<br/><br/>        self<strong class="mx ir">.</strong>losses <strong class="mx ir">=</strong> [] <em class="ms"># here I saved the model's losses per epoch</em><br/><br/>        <em class="ms">#setting the parameters</em><br/>        self<strong class="mx ir">.</strong>alpha_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>beta_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/>        self<strong class="mx ir">.</strong>gamma_tilda <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>nn<strong class="mx ir">.</strong>Parameter(torch<strong class="mx ir">.</strong>rand(1, requires_grad<strong class="mx ir">=True</strong>))<br/><br/>        <em class="ms">#find values for normalization</em><br/>        self<strong class="mx ir">.</strong>S_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_max <strong class="mx ir">=</strong> max(self<strong class="mx ir">.</strong>R)<br/>        self<strong class="mx ir">.</strong>S_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>S)<br/>        self<strong class="mx ir">.</strong>I_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>I)<br/>        self<strong class="mx ir">.</strong>D_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>D)<br/>        self<strong class="mx ir">.</strong>R_min <strong class="mx ir">=</strong> min(self<strong class="mx ir">.</strong>R)<br/><br/>        <em class="ms">#normalize</em><br/>        self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>S <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>        self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>        self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>D <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>        self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">=</strong> (self<strong class="mx ir">.</strong>R <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>        <em class="ms">#matrices (x4 for S,I,D,R) for the gradients</em><br/>        self<strong class="mx ir">.</strong>m1 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m1[:, 0] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m2 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m2[:, 1] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m3 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m3[:, 2] <strong class="mx ir">=</strong> 1<br/>        self<strong class="mx ir">.</strong>m4 <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>zeros((len(self<strong class="mx ir">.</strong>t), 4)); self<strong class="mx ir">.</strong>m4[:, 3] <strong class="mx ir">=</strong> 1<br/><br/>        <em class="ms">#NN</em><br/>        self<strong class="mx ir">.</strong>net_sidr <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>Net_sidr()<br/>        self<strong class="mx ir">.</strong>params <strong class="mx ir">=</strong> list(self<strong class="mx ir">.</strong>net_sidr<strong class="mx ir">.</strong>parameters())<br/>        self<strong class="mx ir">.</strong>params<strong class="mx ir">.</strong>extend(list([self<strong class="mx ir">.</strong>alpha_tilda, self<strong class="mx ir">.</strong>beta_tilda, self<strong class="mx ir">.</strong>gamma_tilda]))<br/><br/>    <em class="ms">#force parameters to be in a range</em><br/>    @property<br/>    <strong class="mx ir">def</strong> alpha(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>alpha_tilda) <em class="ms">#* 0.1 + 0.2</em><br/><br/>    @property<br/>    <strong class="mx ir">def</strong> beta(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>beta_tilda) <em class="ms">#* 0.01 + 0.05</em><br/>    <br/>    @property<br/>    <strong class="mx ir">def</strong> gamma(self):<br/>        <strong class="mx ir">return</strong> torch<strong class="mx ir">.</strong>tanh(self<strong class="mx ir">.</strong>gamma_tilda) <em class="ms">#* 0.01 + 0.03</em><br/><br/>    <strong class="mx ir">class</strong> Net_sidr(nn<strong class="mx ir">.</strong>Module): <em class="ms"># input = [[t1], [t2]...[t100]] -- that is, a batch of timesteps </em><br/>        <strong class="mx ir">def</strong> __init__(self):<br/>            super(DINN<strong class="mx ir">.</strong>Net_sidr, self)<strong class="mx ir">.</strong>__init__()<br/><br/>            self<strong class="mx ir">.</strong>fc1<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(1, 20) <em class="ms">#takes 100 t's</em><br/>            self<strong class="mx ir">.</strong>fc2<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc3<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc4<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc5<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc6<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc7<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>fc8<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 20)<br/>            self<strong class="mx ir">.</strong>out<strong class="mx ir">=</strong>nn<strong class="mx ir">.</strong>Linear(20, 4) <em class="ms">#outputs S, I, D, R (100 S, 100 I, 100 D, 100 R --- since we have a batch of 100 timesteps)</em><br/><br/>        <strong class="mx ir">def</strong> forward(self, t_batch):<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc1(t_batch))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc2(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc3(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc4(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc5(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc6(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc7(sidr))<br/>            sidr<strong class="mx ir">=</strong>F<strong class="mx ir">.</strong>relu(self<strong class="mx ir">.</strong>fc8(sidr))<br/>            sidr<strong class="mx ir">=</strong>self<strong class="mx ir">.</strong>out(sidr)<br/>            <strong class="mx ir">return</strong> sidr<br/><br/>    <strong class="mx ir">def</strong> net_f(self, t_batch):<br/>            <br/>            <em class="ms">#pass the timesteps batch to the neural network</em><br/>            sidr_hat <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_sidr(t_batch)<br/>            <br/>            <em class="ms">#organize S,I,D,R from the neural network's output -- note that these are normalized values -- hence the "hat" part</em><br/>            S_hat, I_hat, D_hat, R_hat <strong class="mx ir">=</strong> sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]<br/><br/>            <em class="ms">#S_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m1, retain_graph<strong class="mx ir">=True</strong>)<br/>            S_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#I_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m2, retain_graph<strong class="mx ir">=True</strong>)<br/>            I_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#D_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m3, retain_graph<strong class="mx ir">=True</strong>)<br/>            D_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_()<br/><br/>            <em class="ms">#R_t</em><br/>            sidr_hat<strong class="mx ir">.</strong>backward(self<strong class="mx ir">.</strong>m4, retain_graph<strong class="mx ir">=True</strong>)<br/>            R_hat_t <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>clone()<br/>            self<strong class="mx ir">.</strong>t<strong class="mx ir">.</strong>grad<strong class="mx ir">.</strong>zero_() <br/><br/>            <em class="ms">#unnormalize</em><br/>            S <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_hat<br/>            I <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_hat<br/>            D <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_hat      <br/>            R <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_hat        <br/><br/>            f1_hat <strong class="mx ir">=</strong> S_hat_t <strong class="mx ir">-</strong> (<strong class="mx ir">-</strong>(self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I)  <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min)<br/>            f2_hat <strong class="mx ir">=</strong> I_hat_t <strong class="mx ir">-</strong> ((self<strong class="mx ir">.</strong>alpha <strong class="mx ir">/</strong> self<strong class="mx ir">.</strong>N) <strong class="mx ir">*</strong> S <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min)<br/>            f3_hat <strong class="mx ir">=</strong> D_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>gamma <strong class="mx ir">*</strong> I) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min)<br/>            f4_hat <strong class="mx ir">=</strong> R_hat_t <strong class="mx ir">-</strong> (self<strong class="mx ir">.</strong>beta <strong class="mx ir">*</strong> I ) <strong class="mx ir">/</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min)        <br/><br/>            <strong class="mx ir">return</strong> f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat<br/><br/>    <strong class="mx ir">def</strong> train(self, n_epochs):<br/>        <em class="ms"># train</em><br/>        print('\nstarting training...\n')<br/>        <br/>        <strong class="mx ir">for</strong> epoch <strong class="mx ir">in</strong> range(n_epochs):<br/>            <em class="ms"># lists to hold the output (maintain only the final epoch)</em><br/>            S_pred_list <strong class="mx ir">=</strong> []<br/>            I_pred_list <strong class="mx ir">=</strong> []<br/>            D_pred_list <strong class="mx ir">=</strong> []<br/>            R_pred_list <strong class="mx ir">=</strong> []<br/><br/>            <em class="ms"># we pass the timesteps batch into net_f</em><br/>            f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred <strong class="mx ir">=</strong> self<strong class="mx ir">.</strong>net_f(self<strong class="mx ir">.</strong>t_batch) <em class="ms"># net_f outputs f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat</em><br/>            <br/>            self<strong class="mx ir">.</strong>optimizer<strong class="mx ir">.</strong>zero_grad() <em class="ms">#zero grad</em><br/>            <br/>            <em class="ms">#append the values to plot later (note that we unnormalize them here for plotting)</em><br/>            S_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>S_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>S_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>S_min) <strong class="mx ir">*</strong> S_pred)<br/>            I_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>I_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>I_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>I_min) <strong class="mx ir">*</strong> I_pred)<br/>            D_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>D_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>D_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>D_min) <strong class="mx ir">*</strong> D_pred)<br/>            R_pred_list<strong class="mx ir">.</strong>append(self<strong class="mx ir">.</strong>R_min <strong class="mx ir">+</strong> (self<strong class="mx ir">.</strong>R_max <strong class="mx ir">-</strong> self<strong class="mx ir">.</strong>R_min) <strong class="mx ir">*</strong> R_pred)<br/><br/>            <em class="ms">#calculate the loss --- MSE of the neural networks output and each compartment</em><br/>            loss <strong class="mx ir">=</strong> (torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>S_hat <strong class="mx ir">-</strong> S_pred))<strong class="mx ir">+</strong> <br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>I_hat <strong class="mx ir">-</strong> I_pred))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>D_hat <strong class="mx ir">-</strong> D_pred))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(self<strong class="mx ir">.</strong>R_hat <strong class="mx ir">-</strong> R_pred))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f1))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f2))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f3))<strong class="mx ir">+</strong><br/>                    torch<strong class="mx ir">.</strong>mean(torch<strong class="mx ir">.</strong>square(f4))<br/>                    ) <br/><br/>            loss<strong class="mx ir">.</strong>backward()<br/>            self<strong class="mx ir">.</strong>optimizer<strong class="mx ir">.</strong>step()<br/>            self<strong class="mx ir">.</strong>scheduler<strong class="mx ir">.</strong>step() <br/><br/>            <em class="ms"># append the loss value (we call "loss.item()" because we just want the value of the loss and not the entire computational graph)</em><br/>            self<strong class="mx ir">.</strong>losses<strong class="mx ir">.</strong>append(loss<strong class="mx ir">.</strong>item())<br/><br/>            <strong class="mx ir">if</strong> epoch <strong class="mx ir">%</strong> 1000 <strong class="mx ir">==</strong> 0:          <br/>                print('\nEpoch ', epoch)<br/><br/>                print('alpha: (goal 0.191 ', self<strong class="mx ir">.</strong>alpha)<br/>                print('beta: (goal 0.05 ', self<strong class="mx ir">.</strong>beta)<br/>                print('gamma: (goal 0.0294 ', self<strong class="mx ir">.</strong>gamma)<br/><br/>                print('#################################')                <br/><br/>        <strong class="mx ir">return</strong> S_pred_list, I_pred_list, D_pred_list, R_pred_list</span></pre><h1 id="f281" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">17.训练网络</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="127f" class="nb lt iq mx b gy nc nd l ne nf"><strong class="mx ir">%%</strong>time</span><span id="2a1d" class="nb lt iq mx b gy nh nd l ne nf">dinn <strong class="mx ir">=</strong> DINN(covid_data[0], covid_data[1], covid_data[2], covid_data[3], <br/>            covid_data[4]) <em class="ms">#in the form of [t,S,I,D,R]</em></span><span id="594a" class="nb lt iq mx b gy nh nd l ne nf">learning_rate <strong class="mx ir">=</strong> 1e-6<br/>optimizer <strong class="mx ir">=</strong> optim<strong class="mx ir">.</strong>Adam(dinn<strong class="mx ir">.</strong>params, lr <strong class="mx ir">=</strong> learning_rate)<br/>dinn<strong class="mx ir">.</strong>optimizer <strong class="mx ir">=</strong> optimizer</span><span id="5d82" class="nb lt iq mx b gy nh nd l ne nf">scheduler <strong class="mx ir">=</strong> torch<strong class="mx ir">.</strong>optim<strong class="mx ir">.</strong>lr_scheduler<strong class="mx ir">.</strong>CyclicLR(dinn<strong class="mx ir">.</strong>optimizer, base_lr<strong class="mx ir">=</strong>1e-5, max_lr<strong class="mx ir">=</strong>1e-3, step_size_up<strong class="mx ir">=</strong>1000, mode<strong class="mx ir">=</strong>"exp_range", gamma<strong class="mx ir">=</strong>0.85, cycle_momentum<strong class="mx ir">=False</strong>)</span><span id="a731" class="nb lt iq mx b gy nh nd l ne nf">dinn<strong class="mx ir">.</strong>scheduler <strong class="mx ir">=</strong> scheduler</span><span id="905f" class="nb lt iq mx b gy nh nd l ne nf">S_pred_list, I_pred_list, D_pred_list, R_pred_list <strong class="mx ir">=</strong> dinn<strong class="mx ir">.</strong>train(50000) <em class="ms">#train</em></span></pre><blockquote class="mp mq mr"><p id="ff97" class="kf kg ms kh b ki kj jr kk kl km ju kn mt kp kq kr mu kt ku kv mv kx ky kz la ij bi">……..……..</p></blockquote><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="eaba" class="nb lt iq mx b gy nc nd l ne nf">starting training...<br/></span><span id="3cbd" class="nb lt iq mx b gy nh nd l ne nf">Epoch  0<br/>alpha: (goal 0.191  tensor([0.0290], grad_fn=&lt;TanhBackward&gt;)<br/>beta: (goal 0.05  tensor([0.3816], grad_fn=&lt;TanhBackward&gt;)<br/>gamma: (goal 0.0294  tensor([0.2541], grad_fn=&lt;TanhBackward&gt;)</span></pre><blockquote class="mp mq mr"><p id="8764" class="kf kg ms kh b ki kj jr kk kl km ju kn mt kp kq kr mu kt ku kv mv kx ky kz la ij bi">……..……..</p></blockquote><h1 id="8a06" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">18.绘制损失和价值</h1><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="4e20" class="nb lt iq mx b gy nc nd l ne nf">plt<strong class="mx ir">.</strong>plot(dinn<strong class="mx ir">.</strong>losses[0:], color <strong class="mx ir">=</strong> 'teal')<br/>plt<strong class="mx ir">.</strong>xlabel('Epochs')<br/>plt<strong class="mx ir">.</strong>ylabel('Loss')</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/db8ba2af02045ddcd809360b82018e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*UG5MUiWjW7WJ8QgOQKXhWw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">损失与时代。图片作者。</p></figure><pre class="ld le lf lg gt mw mx my mz aw na bi"><span id="b032" class="nb lt iq mx b gy nc nd l ne nf">fig <strong class="mx ir">=</strong> plt<strong class="mx ir">.</strong>figure(figsize<strong class="mx ir">=</strong>(12,12))<br/>ax <strong class="mx ir">=</strong> fig<strong class="mx ir">.</strong>add_subplot(111, facecolor<strong class="mx ir">=</strong>'#dddddd', axisbelow<strong class="mx ir">=True</strong>)<br/>ax<strong class="mx ir">.</strong>set_facecolor('xkcd:white')<br/><br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], covid_data[1], 'pink', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Susceptible')<br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], S_pred_list[0]<strong class="mx ir">.</strong>detach()<strong class="mx ir">.</strong>numpy(), 'red', alpha<strong class="mx ir">=</strong>0.9, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Susceptible Prediction', linestyle<strong class="mx ir">=</strong>'dashed')<br/><br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], covid_data[2], 'violet', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Infected')<br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], I_pred_list[0]<strong class="mx ir">.</strong>detach()<strong class="mx ir">.</strong>numpy(), 'dodgerblue', alpha<strong class="mx ir">=</strong>0.9, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Infected Prediction', linestyle<strong class="mx ir">=</strong>'dashed')<br/><br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], covid_data[3], 'darkgreen', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Dead')<br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], D_pred_list[0]<strong class="mx ir">.</strong>detach()<strong class="mx ir">.</strong>numpy(), 'green', alpha<strong class="mx ir">=</strong>0.9, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Dead Prediction', linestyle<strong class="mx ir">=</strong>'dashed')<br/><br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], covid_data[4], 'blue', alpha<strong class="mx ir">=</strong>0.5, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Recovered')<br/>ax<strong class="mx ir">.</strong>plot(covid_data[0], R_pred_list[0]<strong class="mx ir">.</strong>detach()<strong class="mx ir">.</strong>numpy(), 'teal', alpha<strong class="mx ir">=</strong>0.9, lw<strong class="mx ir">=</strong>2, label<strong class="mx ir">=</strong>'Recovered Prediction', linestyle<strong class="mx ir">=</strong>'dashed')<br/><br/><br/>ax<strong class="mx ir">.</strong>set_xlabel('Time /days')<br/>ax<strong class="mx ir">.</strong>set_ylabel('Number')<br/>ax<strong class="mx ir">.</strong>yaxis<strong class="mx ir">.</strong>set_tick_params(length<strong class="mx ir">=</strong>0)<br/>ax<strong class="mx ir">.</strong>xaxis<strong class="mx ir">.</strong>set_tick_params(length<strong class="mx ir">=</strong>0)<br/>ax<strong class="mx ir">.</strong>grid(b<strong class="mx ir">=True</strong>, which<strong class="mx ir">=</strong>'major', c<strong class="mx ir">=</strong>'black', lw<strong class="mx ir">=</strong>0.2, ls<strong class="mx ir">=</strong>'-')<br/>legend <strong class="mx ir">=</strong> ax<strong class="mx ir">.</strong>legend()<br/>legend<strong class="mx ir">.</strong>get_frame()<strong class="mx ir">.</strong>set_alpha(0.5)<br/><strong class="mx ir">for</strong> spine <strong class="mx ir">in</strong> ('top', 'right', 'bottom', 'left'):<br/>    ax<strong class="mx ir">.</strong>spines[spine]<strong class="mx ir">.</strong>set_visible(<strong class="mx ir">False</strong>)<br/>plt<strong class="mx ir">.</strong>show()</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nj"><img src="../Images/934f275f3a5fb10c1d480de9675b449c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kLKaJnjxvcjke3CAvjukOA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">COVID数据和DINNs COVID预测。图片作者。</p></figure><p id="1230" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给你。我们从不同群体计数形式的数据(即多少人易感、感染、死亡等)开始，训练一个神经网络来预测疾病将如何传播。不仅如此，我们还使用神经网络来预测疾病参数(如死亡率)。</p><p id="c873" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，你可以探索我们研究过的其他疾病(在页面顶部的github链接中)，或者你可以针对一种新的疾病训练你自己的DINN。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><blockquote class="mp mq mr"><p id="96de" class="kf kg ms kh b ki kj jr kk kl km ju kn mt kp kq kr mu kt ku kv mv kx ky kz la ij bi translated">如果你对我的更多作品感兴趣，你可以看看我的<a class="ae lb" href="https://github.com/shaier" rel="noopener ugc nofollow" target="_blank"> Github </a>，我的<a class="ae lb" href="https://scholar.google.com/citations?user=paO-O00AAAAJ&amp;hl=en&amp;oi=sra" rel="noopener ugc nofollow" target="_blank">学者页面</a>，或者我的<a class="ae lb" href="https://shaier.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a></p></blockquote></div></div>    
</body>
</html>