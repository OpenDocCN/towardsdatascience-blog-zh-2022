<html>
<head>
<title>Quantitative Input Influence (QII) — an ML explainability measure beyond SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数量输入影响(QII)——超越SHAP的最大似然解释度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantitative-input-influence-qii-an-ml-explainability-measure-f16debc9ed10#2022-01-24">https://towardsdatascience.com/quantitative-input-influence-qii-an-ml-explainability-measure-f16debc9ed10#2022-01-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="4b54" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated"><strong class="ak">量化输入影响(QII)——超越SHAP的ML可解释度</strong></h1></div><div class=""><h2 id="76f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">功能的独立性不再是一个约束</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/39001890438293dfbf2c6fd1c9351f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ijWSivf_GiWdJRZf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奥拉夫·阿伦斯·罗特内在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1b76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">任何数据科学家都会大声证明这一说法——“真实世界的数据至少违背了一些众所周知的方法的假设”。机器学习毕竟不简单。</p><p id="e844" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在ML模型的可解释性领域，SHAP和莱姆是两种值得信赖的方法。然而，它们中的每一个都有一些限制。在我的上一篇文章中，我讨论了SHAP的一些惊人的局限性。如果您想参考，以下是快速参考:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/using-shap-for-explainability-understand-these-limitations-first-1bed91c9d21"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">使用SHAP解释——首先理解这些限制！！</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">可解释性做对了。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="b11d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有的SHAP局限性中，我认为两个(尽管有些关联)最令人担忧的是:</p><p id="ffe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1)假设特征不相互依赖。在现实世界中，不可能找到特征相互独立的数据。在输出上，总是存在一定程度的相互依赖和共同依赖。</p><p id="71ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2)相关性而非因果关系<strong class="lb iu"> : </strong>重要的是要承认，SHAP只是“解释”了根据模型结构定义的变量“相关性”。这并不意味着定义的变量也有因果关系。</p><p id="4b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在寻找问题的潜在解决方案的过程中，我看到了一篇研究论文(“通过量化输入影响的算法透明度”)，展示了一种叫做“量化输入影响”(QII)的方法。QII与SHAP的方法有一些相似之处，也有一些有意义的不同。</p><p id="bf87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键的区别在于使用了“干预技术”。相比之下，SHAP使用“有条件的方法”。</p><p id="15a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入研究干预技术之前，先快速回顾一下模型中变量的边际贡献是如何计算的。对于给定的模型，特性的边际贡献计算如下:</p><p id="d83b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过包含和排除特征来分析输出的差异</p><p id="8ee1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">求所有N的平均值！可能的订单</p><p id="88fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有剩余特征所有子集。</p><p id="69a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">“介入”技术有什么帮助？</strong></p><p id="0f6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑几个例子来解释这个概念。</p><p id="22db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.为了<strong class="lb iu">确定公司的增长</strong>(分类问题:1如果增长&gt; 10%，0如果增长&lt; = 10%)，在许多变量中有2个与销售相关的变量:1)销售人员的数量2)销售额。现在，这两个变量是相互关联的，也就是说，更多的销售人员意味着更高的销售额。为了评估因果关系，在边际贡献计算中，我们可以首先用一组随机值替换销售人员的数量，并查看模型结果如何变化，然后我们可以用一组随机值替换销售额，并评估模型结果的变化。两者改变模型输出的程度有助于理解输出中两个变量的“因果关系”(不仅仅是相关性)。这里用一组随机值代替一个值的想法是“随机干预”。</p><p id="bf9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.想象一个<strong class="lb iu">收入分类问题</strong>，输出是收入大于50k (GT)还是小于50k(LT)。再次假设有两个变量:</p><p id="def9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">-“教育”的价值有高、中、低</p><p id="0c42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">——价值观蓝领、白领、粉领的“职业”</p><p id="cb30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设模型将输出标记为GT，如果“教育程度”是“高”，而“职业”是“白领”，对于所有其他情况，它将输出标记为LT。</p><p id="b8d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，对于一个“教育程度”低而“职业”为蓝领的数据点，变量的影响/贡献是什么？在这里，根据敏感性机制，仅仅使用“教育”或“职业”的随机值不会产生任何结果差异——因为在所有情况下，结果只会是LT。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/103838e1296cd7de8201ffecb432ba96.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*bZv6RV3LzynaOd1j7tg6zg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="eb83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以承认，这并不意味着“教育”和“职业”对产出没有影响。这仅仅意味着只有一起移动的值才会给输出带来敏感性，并且可以推断每个变量的影响。这正是QII所做的。QII不仅解释了个体特征的变化，也解释了“特征集合”的变化。利用这一点，QII计算了一组功能的影响，然后使用合作博弈论的概念(与SHAP使用的方法相同)计算边际贡献。从逻辑上讲，这里QII打破了特征之间的相关性，以衡量特征对模型输出的单独贡献。</p><p id="5866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你不熟悉SHAP使用的合作博弈理论，请参考以下文章:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/can-shap-trigger-a-paradigm-shift-in-risk-analytics-c01278e4dd77"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">SHAP能引发风险分析的范式转变吗？</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">由Chandan Durgia和Prasun Biswas撰写</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mo l mj mk ml mh mm ks ly"/></div></div></a></div><p id="699b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在解决SHAP带来的两个关键限制方面，QII无疑已经向前迈出了一大步。此外，QII据称比SHAP计算量少。</p><p id="e4b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我个人认为，随着数据科学家继续揭示可解释性，QII肯定会走上舞台中央。</p><p id="7bdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">时间会证明一切！！</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="b6d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">免责声明:本文中表达的观点是作者以个人身份发表的意见，而不是他们各自雇主的意见。</p></div></div>    
</body>
</html>