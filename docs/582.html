<html>
<head>
<title>Training Neural Networks to Create Text Like a Human</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练神经网络像人类一样创造文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-neural-networks-to-create-text-like-a-human-23bfdc23c28#2022-01-20">https://towardsdatascience.com/training-neural-networks-to-create-text-like-a-human-23bfdc23c28#2022-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="41f0" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">训练神经网络像人类一样创造文本</h1></div><div class=""><h2 id="f36b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">递归神经网络可以生成与人类书写难以区分的文本。这里有一个亚马逊产品评论数据的例子。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bbd6dd93baba0a0826480e97e133222f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YNZYraxwaVnDJ2WL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由mauricio SANTOS在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="737d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语言建模使用各种技术来确定特定语言的句子中单词序列的概率。它在很大程度上借鉴了Yoshua Bengio及其合作者的工作(例如，参见<a class="ae ky" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> Bengio等人，2003 </a>)中的神经概率语言模型)。</p><p id="d836" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，语言模型被用于人们日常使用的应用程序中。语音识别系统，如亚马逊的Alexa，将语音转换为文本，其关键组件之一是其语言模型(<a class="ae ky" href="https://www.amazon.science/blog/how-to-make-neural-language-models-practical-for-speech-recognition" rel="noopener ugc nofollow" target="_blank">拉朱，2019 </a>)。此外，目前帮助在所有主要语言之间直接翻译句子的谷歌翻译(Google Translate)由深度学习语言模型支持(【吴等人，2016 ，展示了其原始架构)。</p><p id="e86b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将说明神经语言模型如何生成人们无法从人类书写中识别的文本。我们将探索TensorFlow中的实现，并将使用<strong class="lb iu"> </strong>和<strong class="lb iu"/><strong class="lb iu"/>神经<strong class="lb iu"> </strong>网络编写产品<strong class="lb iu"> </strong>评论<strong class="lb iu"> </strong>。产品评论可以作为非常好的训练数据，因为它们通常比书籍或诗歌在语法上更简单，这样，深度神经网络应该可以很好地预测文本。</p><h1 id="a406" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><p id="c2e9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">朱利安·麦考利从他的<a class="ae ky" href="http://jmcauley.ucsd.edu/data/amazon/" rel="noopener ugc nofollow" target="_blank">网站</a>收集的亚马逊产品评论数据被用作训练数据。通过在亚马逊上填写评论，客户授予亚马逊权利<em class="ms">“使用、复制、修改、改编、出版、表演、翻译、创作衍生作品、在全球范围内以任何媒体发布和展示这些内容。”</em> ( <a class="ae ky" href="https://www.amazon.com/gp/help/customer/display.html?nodeId=508088" rel="noopener ugc nofollow" target="_blank">数据使用条件</a>编于<a class="ae ky" href="https://arxiv.org/abs/1602.01585" rel="noopener ugc nofollow" target="_blank">何、麦考利，2016 </a>)。</p><p id="0fc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有几个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/944256e407b2376f6b8ecbefe0f7dd4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xF84P-nsBXn5uILt43JnIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><blockquote class="mu mv mw"><p id="34d2" class="kz la ms lb b lc ld ju le lf lg jx lh mx lj lk ll my ln lo lp mz lr ls lt lu im bi translated">当生成文本时，我们不需要验证数据集，并将使用100 %的数据进行训练。我们将仅使用训练数据来预测单词序列。没有过度拟合的风险，没有一般化，不需要训练测试分割。</p></blockquote><p id="bf05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文使用TensorFlow和Pandas版本，此处指定<a class="ae ky" href="https://github.com/PetrKorab/Training-Neural-Networks-to-Create-Text-Like-a-Human/blob/main/requirements.txt" rel="noopener ugc nofollow" target="_blank"/>。要复制文章，请在虚拟环境中按照这些要求运行代码。</p><h1 id="7942" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">建立语料库</h1><p id="bd41" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">从实现开始，有必要参考一下<a class="ae ky" href="https://www.youtube.com/watch?v=ZMudJXhsUpY" rel="noopener ugc nofollow" target="_blank"> Laurence Moroney在TensorFlow </a>开设的NLP基础课程。我遵循了他的指导方针，但是在必要的地方对python代码做了一些修改。</p><p id="b039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将遵循这个方案:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/34bac2a91a02c2fd24d6990d1ed4203f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RckdQdyCRzBCXCldr7UqLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者(来源:draw.io)</p></figure><p id="4926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不会过多地讨论几个概念的细节。请使用<a class="ae ky" href="https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这篇文章</strong> </a>来刷新这些术语的含义:标记化、填充、文本到序列。</p><p id="7bb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此功能删除特殊字符、标点和数字，并准备一个干净的评论列表。我们需要熊猫和熊猫。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="5789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们<strong class="lb iu">通过创建一个tokenizer对象并使其适合清理后的数据，对<strong class="lb iu"> </strong>数据<strong class="lb iu"> </strong>进行标记化。Tokenizer将文本语料库矢量化，将其转换为单词及其索引的字典。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="c3ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个评论，我们准备<strong class="lb iu"> n-grams </strong>来使用<strong class="lb iu"> </strong>作为训练的输入序列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="a135" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将训练模型来预测所有可能的单词组合；因此，单个有序审查的n元语法如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/9d31a93fc0f8f7b63e0046d702aa9bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xfWirlJ--PxBZcA8L1eng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dc44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们填充测序数据并定义<strong class="lb iu">预测值</strong>和<strong class="lb iu">标签</strong>。我们使用预测器来猜测序列中的下一个单词是什么，并使用标签来纠正模型的预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="7346" class="lv lw it bd lx ly nl ma mb mc nm me mf jz nn ka mh kc no kd mj kf np kg ml mm bi translated">模特培训</h1><p id="4c2f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了训练模型，我们需要<em class="ms"> TensorFlow </em>和<em class="ms"> Keras </em>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="8a4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺序模型包括六层:第一层将文本输入矢量化到240个维度。第二层是一个LSTM，然后是一个辍学层做一些正规化。两个密集层跟随另一个LSTM，在最后一个中具有softmax激活功能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="31ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这项任务需要对模型进行多次训练，以获得良好的准确性。让我们也在准确度达到某个阈值(在我们的例子中是93 %)时设置一个回调，并将其传递给<code class="fe nq nr ns nt b">fit</code>方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="9cd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在大约140个时期内达到了93 %的准确率。这是沿着训练时期的准确度和损失的图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/14d7031c74044fcb06943940b9991087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tvAguu1XT3cnm_TOGtdwQQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="fbba" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">训练数据与生成的文本</h1><p id="12f5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，让我们看看客户评论和用神经网络生成的评论，看看我们是否能发现任何差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/383c3f52d21a1c041d3a8864fb899c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IOobUpITbMvJ0fUakdbWIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="39d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的评论中有一些不完美之处:</p><ol class=""><li id="4414" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated">没有逗号，但训练数据中的许多“真正的”评论也漏掉了逗号。</li><li id="48aa" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">除了句子中的第一个单词，预测不处理大写字母。同样，在“我”的地方有“我”。</li></ol><p id="09e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">否则，很难发现任何差异。</p><p id="070b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ms">我们是怎么做到的？</em> </strong></p><p id="24eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型基于我们指定为输入的种子文本来预测下一个单词。下面是我们要求神经网络完成的句子的五个开头:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="faf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们之前适合训练数据的标记器现在用于对种子文本(前两个单词)进行排序，然后填充种子文本并使用<code class="fe nq nr ns nt b">model.predict_classes.</code>预测下一个单词。然后，循环将预测的单词附加到一个句子中，并以相同的方式继续，直到我们获得一个包含12个单词的大写句子(种子文本+ 10个预测的单词)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="2781" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想解决“I”的问题，我们可以在代码中用“I”替换“I ”,如下所示:</p><pre class="kj kk kl km gt ok nt ol om aw on bi"><span id="dd3c" class="oo lw it nt b gy op oq l or os">if output_word == ‘i’:<br/> output_word == ‘I’</span></pre><h1 id="d20c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="4eee" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这篇文章展示了一个简单的神经网络如何产生一个很难从人类书写中识别的文本。亚马逊评论是很好的训练范例。当然，一个主要的区别是，递归神经网络只是一种应用的统计方法，并且不理解它被训练的文本。但是，将人类编写的文本和程序生成的文本进行比较真的很令人兴奋，这两者看起来非常相似。</p><p id="aad8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">jupyter笔记本的完整代码可以在我的<a class="ae ky" href="https://github.com/PetrKorab/Training-Neural-Networks-to-Create-Text-Like-a-Human" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="db9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ms"> PS:你可以订阅我的</em> <a class="ae ky" href="https://medium.com/subscribe/@petrkorab" rel="noopener"> <em class="ms">邮件列表</em> </a> <em class="ms">在我每次写新文章的时候得到通知。如果你还不是中等会员，你可以在这里加入</em><a class="ae ky" href="https://medium.com/@petrkorab/membership" rel="noopener"><em class="ms"/></a><em class="ms">。</em></p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="4fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ms">免责声明:</em> </strong> <em class="ms">本文使用的数据仅作为TensorFlow中教学文本生成的训练示例，适合这种特定的深度学习应用。作者不同意在生产中使用生成的数据，或者暗示任何不道德或法律禁止的行为。</em></p><h2 id="02ed" class="oo lw it bd lx ot ou dn mb ov ow dp mf li ox oy mh lm oz pa mj lq pb pc ml pd bi translated">参考资料:</h2><p id="b2ef" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一个神经概率语言模型。 J <em class="ms">机器学习研究杂志</em>，2003年第33期，第1137–1155页。</p><p id="d50a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拉朱，A. <a class="ae ky" href="https://www.amazon.science/blog/how-to-make-neural-language-models-practical-for-speech-recognition" rel="noopener ugc nofollow" target="_blank">如何使神经语言模型在语音识别中实用</a>。<em class="ms">亚马逊科学博客</em>，2019年8月29日。</p><p id="610f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">何，r .，麦考利，J. <a class="ae ky" href="https://arxiv.org/abs/1602.01585" rel="noopener ugc nofollow" target="_blank">沉浮:用一类协同过滤建模流行趋势的视觉演变</a>。<strong class="lb iu"> </strong> <em class="ms">国际万维网大会。</em>2016年4月11日至15日，加拿大魁北克省蒙特利尔。</p><p id="0418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练人工智能创作诗歌。<a class="ae ky" href="https://www.youtube.com/watch?v=ZMudJXhsUpY" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> Youtube NLP零到英雄课程，第6部分。</em> </a></p><p id="2dc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Mayo，M. <a class="ae ky" href="https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html" rel="noopener ugc nofollow" target="_blank">使用TensorFlow &amp; Keras进行标记化和文本数据准备。</a> <em class="ms"> KDnuggets教程。</em></p></div></div>    
</body>
</html>