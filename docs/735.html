<html>
<head>
<title>A Simple Introduction to Gaussian Mixture Model (GMM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型(GMM)简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-introduction-to-gaussian-mixture-model-gmm-f9fe501eef99#2022-01-26">https://towardsdatascience.com/a-simple-introduction-to-gaussian-mixture-model-gmm-f9fe501eef99#2022-01-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="ac25" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">高斯混合模型(GMM)简介</h1></div><div class=""><h2 id="7deb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这篇文章将向您展示——使用简单的词语——在Python中聚类数据的另一种选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e0dcc322864cc558789a6a88fa0c1e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*szDevlKG53CRRTmh52nucQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马里奥·普里西克在<a class="ae ky" href="https://unsplash.com/s/photos/groups?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="8ad6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">高斯分布也就是我们所知道的正态分布。众所周知，钟形曲线概念以平均值和中间值为中心点。</p><p id="65a5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">鉴于此，如果我们观察一个数据集，即使它不遵循正态分布，我们也有办法让它看起来像正态分布。或者，更好的是，我们可以说在一个数据集中，每个数据点可能来自许多正态分布。</p><p id="489a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在假设我们正在获取一个数据集来创建三个集群。一种方法是假设其中有三个高斯(正态)分布。然后，我们将查看每个数据点，并分析该数据点更适合哪种分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/515e230b445a6eb6679ae54e01eb4c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Cmg-hGcNOOIZBM1xhPuoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">三个正态分布。图片由作者提供。</p></figure><p id="64e7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">看上面的图，我来说明一下概念。GMM算法将每个高斯分布视为一个聚类。因此，它将获取每个数据点，并检查该点在3个分布中的概率。为其选择的集群就越高。</p><blockquote class="md"><p id="0745" class="me mf it bd mg mh mi mj mk ml mm mb dk translated">GMM认为每个集群是不同的高斯分布。然后它会根据概率告诉你数据点是从哪个分布中出来的。</p></blockquote><p id="1786" class="pw-post-body-paragraph lg lh it li b lj mn ju ll lm mo jx lo lp mp lr ls lt mq lv lw lx mr lz ma mb im bi translated">可能最著名和最常用的聚类算法是K-Means。但它有其局限性。其中之一是，聚类将根据聚类中心的最佳半径值进行分离，该半径值是根据到该点的欧几里德距离计算的。因此，如果你的集群没有被定义为一个圆形，你就很难正确地分离它。GMM，另一方面，与其他格式，椭圆形状是最常见的。</p><h2 id="d830" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">GMM是如何运作的</h2><p id="5799" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">好的，在我们完成了一些初始概念之后，让我们基本上理解算法是如何在幕后工作的。</p><p id="f1b7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">就像K-Means算法一样，GM模型必须从某个地方开始，使用一个随机参数并以此为基础构建。一旦选择了初始参数，该算法就开始一系列计算，试图找到收敛估计的高斯分布以满足具有初始参数的高斯分布所需的最佳权重和手段。换句话说，选择每个数据点的分布。</p><p id="2cfb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为此，有一个名为<code class="fe nq nr ns nt b">init_params</code>的超参数，你可以在<code class="fe nq nr ns nt b">‘random’</code>点之间进行选择，或者使用默认的<code class="fe nq nr ns nt b">'kmeans'</code>。使用K-Means可以帮助收敛发生得更快，因此我会保留默认值。我将在最后为那些想要/需要深入研究这个算法背后的疯狂数学的人建议一些参考帖子。</p><p id="2297" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里下一个有趣的参数是<code class="fe nq nr ns nt b">covariance_type</code>，在这里你可以选择'<em class="nu">满'、'绑'、'诊断'、'球形'。</em> <strong class="li iu"> </strong>为了帮助你理解这个参数，下面是我能找到的对它的最好解释。但是在你去那里之前，记住K-Means只使用圆形来聚集点。</p><ul class=""><li id="e5c7" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated"><strong class="li iu">全</strong>表示部件可以独立采用任何位置和形状。</li><li id="fafc" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">并列</strong>表示它们形状相同，但形状可能是任何东西。</li><li id="4d22" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">对角线</strong>表示轮廓轴沿坐标轴定向，但除此之外，组件之间的偏心率可能不同。</li><li id="c7d4" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">球形</strong>是圆形轮廓的“对角线”情况(更高维度的球形，因此得名)。</li></ul><h2 id="fd70" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">让我们开始编码吧！</h2><p id="a32e" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">说得够多了，现在让我们做点什么吧。</p><p id="3a50" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从数据集开始，我将使用Seaborn原生的玩具数据集:<em class="nu">提示</em>。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="ca9c" class="ms mt it nt b gy on oo l op oq">import seaborn as sns<br/>df = sns.load_dataset('tips')</span></pre><p id="609e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本例需要的其他导入。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="9ba1" class="ms mt it nt b gy on oo l op oq"># Basic<br/>import pandas as pd</span><span id="66e6" class="ms mt it nt b gy or oo l op oq"># Viz<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="0705" class="ms mt it nt b gy or oo l op oq"># KMeans<br/>from sklearn.cluster import KMeans</span><span id="2f58" class="ms mt it nt b gy or oo l op oq"># Gaussian Misture Model (GMM)<br/>from sklearn.mixture import GaussianMixture</span></pre><p id="f538" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我将把变量<em class="nu"> total_bill </em>和<em class="nu"> tip </em>输入到算法中，看看K-Means和GMM如何进行聚类，这样我们就可以评估差异。</p><p id="6632" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">注意，它们都不是正态分布的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/82d91c359e61bcbdea005ba92f70712b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVZjyQShdQpUVMQaCHiuiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tip和total_bill变量的分布。图片由作者提供。</p></figure><p id="dc04" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">创建用于聚类的输入数据集。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="000a" class="ms mt it nt b gy on oo l op oq">X = df[['total_bill', 'tip']].copy()</span></pre><p id="0e9a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">用K-均值聚类。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="98db" class="ms mt it nt b gy on oo l op oq">kmeans = KMeans(n_clusters=2, max_iter=600)<br/>fitted = kmeans.fit(X)<br/>prediction = kmeans.predict(X)</span></pre><p id="fa5e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">高斯混合模型聚类。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="e67f" class="ms mt it nt b gy on oo l op oq">gmm = GaussianMixture(n_components=2, covariance_type='full').fit(X)<br/>prediction_gmm = gmm.predict(X)</span></pre><p id="9dd5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在让我们绘制两个结果并进行比较。</p><h2 id="63e3" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">GMM满了</h2><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="6ae9" class="ms mt it nt b gy on oo l op oq"># Add predictions to the original dataset<br/>df['kmeans_cluster'] = prediction<br/>df['gmm_cluster'] = prediction_gmm</span><span id="7280" class="ms mt it nt b gy or oo l op oq"># Plot K-Means<br/>sns.scatterplot(data=df, y='tip', x='total_bill', hue='kmeans_cluster');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b540840d1ae2f11fc25bd1c4515ae0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*IjKMsTsNAKqUW0urLOEaqQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-表示结果。图片由作者提供。</p></figure><p id="0c41" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">K-Means按预期工作，圆形分割。我们可以看到一串高达23美元左右，小费高达5.5美元。其余为总账单较高的集群。</p><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="e5d7" class="ms mt it nt b gy on oo l op oq">#Plot GMM<br/>sns.scatterplot(data=df, y='tip', x='total_bill', hue='gmm_cluster');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/98aa8aa406c293f2aa2c0a69d57bcd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*ks51X-Ic_0oUY213uFW6eA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM全部结果。图片由作者提供。</p></figure><p id="f3c3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">“完整”协方差类型为我们提供了一个更紧密的分类1和一个分类0，分类1中的小费与总账单非常成比例，分类0中有更多的分布值。它们有不同的形状。</p><p id="2ce7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们来看看GMM的其他协方差类型的图形。</p><h2 id="6ba6" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">GMM打平了</h2><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="e0bf" class="ms mt it nt b gy on oo l op oq"># Rerun the model<br/>gmm = GaussianMixture(n_components=2, covariance_type='tied').fit(X)<br/>prediction_gmm = gmm.predict(X)</span><span id="04dc" class="ms mt it nt b gy or oo l op oq"># Replace the predictions<br/>df['gmm_cluster'] = prediction_gmm</span><span id="cfc2" class="ms mt it nt b gy or oo l op oq"># Plot<br/>sns.scatterplot(data=df, y='tip', x='total_bill', hue='gmm_cluster');</span></pre><p id="3b78" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">捆绑意味着我希望集群具有相同的形状。有趣的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/671ad7cac21d728fe37755c2f79dc739.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*GugSaRKPaE4L1vSxz6Ig5Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM潮汐结果。图片由作者提供。</p></figure><h2 id="a8ba" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">GMM对角线</h2><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="21da" class="ms mt it nt b gy on oo l op oq"># Rerun the model<br/>gmm = GaussianMixture(n_components=2, covariance_type='diag').fit(X)<br/>prediction_gmm = gmm.predict(X)</span><span id="f654" class="ms mt it nt b gy or oo l op oq"># Replace the predictions<br/>df['gmm_cluster'] = prediction_gmm</span><span id="1608" class="ms mt it nt b gy or oo l op oq"># Plot<br/>sns.scatterplot(data=df, y='tip', x='total_bill', hue='gmm_cluster');</span></pre><p id="d5bb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这里，形状遵循坐标轴，但一些异常值可能因组件而异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/18dc48903f97ded45b2f12d71ad37101.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*Ol-ciNzmZxPPXkEXTFezGA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM对角线结果。图片由作者提供。</p></figure><h2 id="837e" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">GMM球形</h2><pre class="kj kk kl km gt oj nt ok ol aw om bi"><span id="7a6f" class="ms mt it nt b gy on oo l op oq"># Rerun the model<br/>gmm = GaussianMixture(n_components=2, covariance_type='spherical').fit(X)<br/>prediction_gmm = gmm.predict(X)</span><span id="7739" class="ms mt it nt b gy or oo l op oq"># Replace the predictions<br/>df['gmm_cluster'] = prediction_gmm</span><span id="a86a" class="ms mt it nt b gy or oo l op oq"># Plot<br/>sns.scatterplot(data=df, y='tip', x='total_bill', hue='gmm_cluster');</span></pre><p id="cc8c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这与K-Means结果非常相似，因为聚类被定义为球形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a8882bd0ec21ebe5cb4e7109b817c400.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*9p6UegevDQCvuKgrHXdymw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM球形结果。图片由作者提供。</p></figure><h2 id="2c3e" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">在你走之前</h2><p id="d1d5" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">这个问题并不简单。它可能看起来很简单，但它有更多的深度。我建议你阅读我留下的帖子，作为更好地理解GMMs的参考。</p><p id="acae" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">底线是，没有容易的路。你必须尝试不同类型的模型，以确定哪种模型最适合你的情况。</p><p id="be70" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你愿意，不要忘记关注我的博客或者使用我的推荐链接订阅Medium。</p><div class="oy oz gp gr pa pb"><a href="https://gustavorsantos.medium.com/" rel="noopener follow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">古斯塔沃·桑托斯-中等</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">阅读古斯塔夫·桑托斯在媒介上的作品。数据科学家。我从数据中提取见解，以帮助个人和公司…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">gustavorsantos.medium.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ks pb"/></div></div></a></div><h2 id="8330" class="ms mt it bd mu mv mw dn mx my mz dp na lp nb nc nd lt ne nf ng lx nh ni nj nk bi translated">参考</h2><p id="9958" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html" rel="noopener ugc nofollow" target="_blank"> Sklearn文档:GMM </a></p><p id="dbda" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><a class="ae ky" href="https://stats.stackexchange.com/questions/326671/different-covariance-types-for-gaussian-mixture-models" rel="noopener ugc nofollow" target="_blank">解释堆栈交换的协方差类型</a></p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">高斯混合模型解释</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">在机器学习领域，我们可以区分两个主要领域:监督学习和非监督学习。主要的…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/how-to-code-gaussian-mixture-models-from-scratch-in-python-9e7975df5252"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">如何用Python从头开始编写高斯混合模型</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">使用NumPy的GMMs和最大似然优化</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-modelling-gmm-833c88587c7f"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">高斯混合模型(GMM)</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">使用无监督学习理解文本数据</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="ps l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-d13a5e915c8e"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">高斯混合模型聚类算法讲解</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">高斯混合模型可以像k-means一样用于聚类未标记的数据。</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pt l pm pn po pk pp ks pb"/></div></div></a></div></div></div>    
</body>
</html>