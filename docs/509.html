<html>
<head>
<title>Cliff Walking With Monte Carlo Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蒙特卡洛强化学习的悬崖行走</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cliff-walking-with-monte-carlo-reinforcement-learning-587e9d3bc4e7#2022-01-17">https://towardsdatascience.com/cliff-walking-with-monte-carlo-reinforcement-learning-587e9d3bc4e7#2022-01-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="c07a" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">蒙特卡洛强化学习的悬崖行走</h1></div><div class=""><h2 id="0674" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TD(0)和TD(1)学习的比较，包括完整的Python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f433224e009019d302326bb06dcac8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9zVywl4hOwyIrKYe"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">玛丽安·布兰德在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="5d73" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">新的一年，新的悬崖行走算法！这一次，<strong class="li iu">蒙特卡洛强化学习</strong>将被部署。可以说，这是强化学习最简单、最直观的形式。本文将该算法与时间差分方法(如Q学习和SARSA)进行了对比。此外，还提供了完整的Python实现，并显示了一些数值结果。</p><h1 id="0009" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">悬崖漫步问题</h1><p id="9f02" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">悬崖行走问题是一个教科书问题(萨顿和巴尔托，2018年)，其中一个代理试图从左下瓦片移动到右下瓦片，旨在最大限度地减少步骤数，同时避免悬崖。一集在走进悬崖(大负奖励)或在目标瓦片上(正奖励)时结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/5f4d13cbe56bfec804201001f9a70090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M79MspX4MckRX3wqKCgAxA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">悬崖漫步世界[图片由作者提供]</p></figure><h1 id="3816" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">蒙特卡洛强化学习</h1><p id="a2c8" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">蒙特卡洛RL还有其他几个名称(技术上有一些细微的区别，但我们可以忽略不计)，包括:</p><ul class=""><li id="b3f5" class="na nb it li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated"><strong class="li iu">双重传递</strong>:首先向前传递直到剧集结束，同时收集奖励，然后向后传递以更新所有遇到的状态的值函数。</li><li id="5ac0" class="na nb it li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated"><strong class="li iu">向后传递</strong>:这个术语强调向后机制，在情节中遇到的所有状态上循环返回。</li><li id="620c" class="na nb it li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated"><strong class="li iu">时间差1或TD(1) </strong> : TD(0)指的是在每个时间步长之后发生的更新，TD(1)在整个轨迹之后更新。</li></ul><p id="7348" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我并不偏爱这两个名字，但是在本文的剩余部分，我将坚持使用蒙特卡罗RL (MC-RL)。</p><p id="7168" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">无论如何，核心概念是首先完成<strong class="li iu">完整体验轨迹</strong>——在这种情况下，这些步骤一直走到达到目标或跌入悬崖——然后才继续更新查找表。</p><p id="481f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">MC-RL和<strong class="li iu">时间差分方法</strong>如Q-learning和SARSA的根本区别在于，后者在每个时间步长后更新查找表，使用<em class="no">估计值</em> <code class="fe np nq nr ns b">Q(s_t+1,a)</code>更新<code class="fe np nq nr ns b">Q(s_t,a_t)</code>。这个过程也称为引导；一个估计用于更新另一个。作为复习，以下是更新功能:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/e82094cf1c40316ac5ec008ad7e01aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3tsU3mRBB0wWlwOeIcH3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q-学习更新公式(非政策)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d4ba4e1b3b5f87a254bdbae1060769bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSr8_pMUBaSu5A2jt_gskQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SARSA更新公式(根据政策)</p></figure><p id="4d21" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相比之下，MC-RL仅利用轨迹的<em class="no">实际</em>观测值来更新查找表。关键的好处是使程序<strong class="li iu">无偏</strong>；为了直观起见，假设观察值是从相应的概率分布中随机抽取的。特别是当Q值由近似函数表示时——当问题变大时，这通常是不可避免的——偏差会严重影响性能。然而，MC-RL奖励轨迹表现出更大的<strong class="li iu">方差</strong>，因此通常学习(慢很多)。</p><p id="f011" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">价值函数<code class="fe np nq nr ns b">V(s_t)</code>的更新函数非常简单，简单地计算观察到的回报轨迹<code class="fe np nq nr ns b">G_t</code>和价值函数<code class="fe np nq nr ns b">V(s_t)</code>之间的误差，用权重<code class="fe np nq nr ns b">α</code>进行更新:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/5dfb1e806a1065b8257203e912436ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*caWJawfpdz2oulUnmTWCow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蒙特卡罗强化学习的更新函数</p></figure><p id="3b2e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其中累积奖励轨迹<code class="fe np nq nr ns b">G_t</code>(下游奖励除以<code class="fe np nq nr ns b">γ</code>)由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/63de12bd2010b98eacb3cabaea14e886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZymijv8u8P4RuKWmhd1Eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">累积奖励函数</p></figure><p id="c48e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">机制非常全面和直观，但让我们考虑一些并发症。看看下面两个轨迹示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/2999647ea1a4df4638a8f9be1086dc63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cxrY4vURMjK_nqTFk7wCvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">成功轨迹。在目标状态下产生的积极回报会反向传播到轨迹中的所有状态。[图片由作者提供]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/de41abc2be10ce8c9d27c2d3e81b06c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfVMTljRoE2LTuKOWutbCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">失败的轨迹。在悬崖状态产生的负回报会反向传播到轨迹中的所有状态。[图片由作者提供]</p></figure><p id="7ff6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管轨迹几乎相同，但一个轨迹会导致所有遇到的状态的正奖励更新，而另一个轨迹会导致负奖励更新。如前所述，蒙特卡罗方法的<strong class="li iu">方差</strong>往往非常高，因为累积奖励可能会有很大差异。上述情况对TD(0)学习的影响要小得多。</p><p id="1126" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，以次优解决方案告终的风险也在增加。对于这个特定的问题，代理可能只坚持一条路径，仅仅因为它曾经发现过这条路径。由于这些图块具有较高的值，代理可能会一遍又一遍地遵循相同的路径。</p><h1 id="baf3" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数值实验</h1><p id="03ef" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">这里介绍的蒙特卡罗方法是<strong class="li iu">政策上的</strong>，这意味着它使用实际观察到的奖励来更新估计值。因此，一个公平的基准是SARSA，它也是根据政策原则运作的。像往常一样，Python源代码可以在我的<a class="ae ky" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。</p><p id="986e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">注意到的第一件事是，使用标准探索率<code class="fe np nq nr ns b">ϵ=0.05</code>，MC-RL通常不会在10，000集内收敛到令人满意的解决方案。我把它提高到<code class="fe np nq nr ns b">ϵ=0.10</code>，为了公平起见，对SARSA也做同样的事情。</p><p id="9bd6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">事不宜迟，该出结果了。显然，蒙特卡罗RL比SARSA表现差得多，需要更长的时间来收敛，并且经常坚持更差的路径。急剧下降意味着代理继续遵循第一条成功的路径，之后几乎没有改善。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/47240239e9a8a0910c425846f5a0576f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*cPpFCQ1iwxeSM1gSe-1enA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SARSA和蒙特卡罗RL的比较。SARSA通常收敛得更快，并且经常找到更好的解决方案。[图片由作者提供]</p></figure><p id="53d8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相应的解决方案也往往表现出一些奇特的模式。上了政策，自然要绕道悬崖(由于正面<code class="fe np nq nr ns b">ϵ</code>)。然而，也有一些模式显然没有理性基础。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e87f7c2cd37454de2818dd46d9ef2000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*XMFjozx7__lMB0l9TREvFw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用MC-RL学习路径的示例[图片由作者提供]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/6818b2428502a7c2e4f925caeba62f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*vLXLEuhA6dAjpiStOjwvGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用MC-RL学习路径的示例[图片由作者提供]</p></figure><p id="21eb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管直觉很吸引人，但方差问题确实阻碍了MC-RL快速识别好的解决方案。如果有的话，这些结果说明了为什么时间差异学习方法已经成为基于价值的强化学习的标准。</p><h1 id="6bdb" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">外卖食品</h1><ul class=""><li id="479e" class="na nb it li b lj mu lm mv lp nz lt oa lx ob mb nf ng nh ni bi translated">蒙特卡洛强化学习(或TD(1)，双通)基于观察到的<strong class="li iu">完全回报轨迹</strong>更新价值函数。</li><li id="1bcd" class="na nb it li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">与时间差异学习方法(如Q-learning和SARSA)相比，MC-RL是<strong class="li iu">无偏的、</strong>，即，值更新不受值函数的不正确先验估计的影响。</li><li id="b22c" class="na nb it li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">由于奖励轨迹的<strong class="li iu">高方差</strong>，MC-RL通常比时间差异学习花费更长的时间来收敛。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="55a8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="no">蒙特卡洛强化学习算法的完整代码可以在我的</em> <a class="ae ky" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> <em class="no"> GitHub资源库</em> </a> <em class="no">上找到。</em></p><p id="93de" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="no">也来看看我这个系列的其他文章吧！</em></p><p id="e2bc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"><em class="no">-Q-learning</em></strong><em class="no">:</em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">用非策略强化学习走下悬崖</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">政策外强化学习和政策内强化学习的深入比较</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><p id="e149" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> <em class="no">离散政策梯度</em> </strong> <em class="no"> : </em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">基于离散策略梯度算法的悬崖行走问题</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明内部…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ou l oq or os oo ot ks of"/></div></div></a></div><p id="2bb7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> <em class="no">深度政策梯度</em> </strong> <em class="no"> : </em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/deep-policy-gradient-for-cliff-walking-37d5014fd4bc"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">悬崖漫步的深度政策梯度</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">用Python实现TensorFlow 2.0。在这个解决方案中，参与者由一个神经网络来表示，它是…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ov l oq or os oo ot ks of"/></div></div></a></div><p id="e9dc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"> <em class="no">深度Q-学习</em> </strong> <em class="no"> : </em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">TensorFlow 2.0中深度Q学习的最小工作示例</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">一个多臂土匪的例子来训练一个Q网络。使用TensorFlow，更新过程只需要几行代码</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ow l oq or os oo ot ks of"/></div></div></a></div><h1 id="a9df" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="a508" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">鲍威尔(2007年)。<em class="no">近似动态规划:解决维数灾难</em>(第703卷)。约翰·威利的儿子们。</p><p id="d1be" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">萨顿和巴尔托(2018年)。<em class="no">强化学习:简介</em>。麻省理工出版社。</p></div></div>    
</body>
</html>