<html>
<head>
<title>How to Create and Train a Multi-Task Transformer Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何创建和训练多任务转换器模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a146240#2022-01-03">https://towardsdatascience.com/how-to-create-and-train-a-multi-task-transformer-model-18c54a146240#2022-01-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="f1d9" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">如何创建和训练多任务转换器模型</h1></div><div class=""><h2 id="184d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何使用Huggingface Transformers创建和训练多任务transformer模型的分步教程。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c425bd1f1c7b801be5e376c1dd7599fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tC3yxThBtA2P6yPS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弗拉德·扎伊采夫在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">当我在做一个人工智能聊天机器人项目时，我对提供NLP模型服务的公司做了一个简短的回顾。我对一些提供商对基本意图分类模型收取的费用感到惊讶，甚至对那些既提供意图分类又提供令牌分类任务(例如部分速度标记)的提供商收取的费用更高。</p><p id="4e60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我看来，这些服务的唯一附加值就是部署和维护。因为开源的<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> Huggingface Transformer库</a>(训练这种模型的当前标准)已经成熟，并且可以被任何没有广泛的Transformer模型知识的人使用。</p><p id="c1a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道部署和维护是任何生产应用程序的关键方面(知道维护平均占任何软件成本的70%)。然而，走这条路也有一些缺点:</p><ul class=""><li id="9eb6" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">你对引擎盖下使用的模型控制较少。您将模型视为通过API访问的黑盒。此外，您总是受到发布周期的约束，以获得最新的模型架构(如果它们被添加的话)。</li><li id="9939" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">对于纯云服务，您需要将您的数据发送给该第三方。</li><li id="868f" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">您受到任何服务限制。比如API访问方法(REST vs. gRPC)、延迟、特性等。</li></ul><p id="fa3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果您可以构建和训练您的模型，您只需要支付成本更低的部署服务，尤其是从建模的角度来看没有任何优势，因为数据的质量决定了您的模型的性能。</p><p id="19a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章是关于构建和训练多任务模型的分步教程，该模型执行序列分类(即意图分类)和标记分类(即命名实体识别)。然而，对于任何使用<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> Huggingface Transformer库</a>的MTL模型来说，这也是一个很好的起点。最后，如果您必须为您的NLP服务选择一个外部提供者，至少您可以使用这个例子创建一个基线来比较不同提供者的性能。</p><h1 id="665e" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">多任务数据集</h1><p id="4f52" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">首先，我们将创建一个多任务数据集，它结合了来自两个任务的样本(即序列和标记分类)。</p><p id="53f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于序列分类，我们将使用公开可用的<a class="ae ky" href="https://metatext.io/datasets/microsoft-research-paraphrase-corpus-(mrpc)" rel="noopener ugc nofollow" target="_blank"> MRPC </a>数据集和<a class="ae ky" href="https://www.clips.uantwerpen.be/conll2003/" rel="noopener ugc nofollow" target="_blank"> Conll2013数据集</a>来完成令牌分类任务。然而，在实际的用例中，您可能会将相同的数据标记为令牌和序列分类。</p><p id="4cae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要创建多任务数据集，我们需要:</p><ul class=""><li id="88c6" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">下载或加载每个数据集。</li><li id="1764" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">将每个数据集标记化。</li><li id="3c0e" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">组合标记化的集合。</li></ul><h2 id="b75d" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">令牌分类数据集</h2><p id="8f7e" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">以下是加载令牌分类数据集的代码片段。这个片段的灵感来自Huggingface中的<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/examples/pytorch/token-classification/run_ner.py" rel="noopener ugc nofollow" target="_blank"> run_ner.py示例</a>，并做了一些修改以处理多任务设置:</p><ul class=""><li id="95e6" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">我们添加了一个名为<code class="fe ob oc od oe b">task_ids</code>的新列，包含每个样本的任务id<strong class="lb iu">(第59行)</strong>。模型将使用任务id来正确处理来自每个任务的样本。</li><li id="8a26" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">在标记化之后删除不使用的列，只保留模型<strong class="lb iu">(第70行)</strong>使用的特性。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="84e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用了Huggingface数据集<code class="fe ob oc od oe b">load_dataset</code>函数<strong class="lb iu">(第79行)</strong>。但是，如果您有数据，您仍然可以使用相同的函数并提供文件路径而不是数据集名称。</p><p id="12e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ob oc od oe b">Task</code>数据类是一个助手，用来记录MTL模型所需的关于特定任务的所有信息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><ul class=""><li id="9589" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">id</code>:唯一的任务id。</li><li id="d87b" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">name</code>:任务名称。用于打印日志消息。</li><li id="7384" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">type</code>:任务类型(<code class="fe ob oc od oe b">seq_classification</code>或<code class="fe ob oc od oe b">token_classification</code>)。</li><li id="d900" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">num_labels</code>:标签的数量(如二进制分类为2)。</li></ul><h2 id="6532" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">序列分类数据集</h2><p id="fe59" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">以下是加载序列分类数据集的代码片段。它受Huggingface的<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py" rel="noopener ugc nofollow" target="_blank"> run_glue.py示例</a>的启发，并做了一些修改以处理多任务设置:</p><ul class=""><li id="c7ef" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">我们添加了类似于令牌分类数据集<strong class="lb iu">(第30行)</strong>的<code class="fe ob oc od oe b">task_ids</code>列。</li><li id="b52a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">将<code class="fe ob oc od oe b">label</code>列重命名为<code class="fe ob oc od oe b">labels</code>，以匹配令牌分类数据集<strong class="lb iu">(第29行)</strong>。</li><li id="36c5" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">仅为训练数据集填充标签<strong class="lb iu">(第36行)</strong>。</li></ul><p id="f9b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解我们为什么需要填充标签，让我们以下面的例子为例:“我想开一个账户。”</p><ul class=""><li id="5b5d" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">标签将是序列分类任务的类id(单个整数)。</li><li id="1077" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">标签将是用于令牌分类任务的每个单词的标签列表，</li></ul><p id="a348" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们需要在同一批中包装两种样品，所以标签的形状应该相同。我们只需要稍后为序列分类样本去除填充。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="de85" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">合并数据集</h2><p id="97fa" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">既然我们可以分别加载每个数据集，那么最后一步就是创建一个单独的数据集来提供给培训师。</p><ul class=""><li id="cf79" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">load_seq_classification_dataset</code>和<code class="fe ob oc od oe b">load_token_classification_dataset</code>是我们之前定义的函数。</li><li id="c2b7" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">为了合并训练数据集，我们使用pandas格式的原始数据来避免处理数据集库中的一些检查，因为这些数据集格式最初是不兼容的(我们添加的填充)<strong class="lb iu">(第14行)</strong>。</li><li id="ff8e" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">训练数据集被混洗，以便单个批次包含来自多个任务<strong class="lb iu">(第18行)</strong>的样本。</li><li id="75e1" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">因为我们分别评估每个任务<strong class="lb iu">(第21行)</strong>，所以验证数据集被附加到一个列表中。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="2846" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会问自己，为什么验证集没有像训练集一样合并。由于计算性能指标和避免改变默认训练者的不同方式，在验证期间单独处理每个数据集(一个接一个)要容易得多。</p><h1 id="3d15" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">多任务模型</h1><h2 id="1714" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">概观</h2><p id="6aff" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">我们将使用硬参数共享多任务模型[1],因为这是最广泛使用的技术，也是最容易实现的。在硬参数共享中，所有的任务共享一组隐藏层，每个任务都有其输出层，通常称为输出头，如下图所示。在这种设置中，模型学习对所有任务建模的共享特征表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/fb849ccaef1131fbe88fa5c9afa9e69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms8E2_8JvxgbTPZIVqxHtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。灵感来源于[1]。</p></figure><h2 id="1ea1" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">多任务模型</h2><p id="e0ad" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">对于我们的用例，共享层将是一个转换器(例如，BERT、RoBERTa等。)，输出头将是线性层，有落差，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/6683480567f6c5f178fd17fbb3791c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EgUtGOq-XKRD46tMpeMPMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。</p></figure><p id="1e32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建多任务模型时有两个主要考虑事项:</p><ul class=""><li id="db98" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">模型应该是Pytorch模块。意味着一个从<code class="fe ob oc od oe b">torch.nn.Model</code>继承并实现<code class="fe ob oc od oe b">forward</code>方法的类。</li><li id="595c" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">forward</code>方法应该处理输入参数并返回类似于任何Huggingface模型的输出。</li></ul><p id="4005" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> __init__方法</strong></p><p id="336f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码片段为每个任务创建了编码器和输出头。</p><ul class=""><li id="78fb" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">使用<code class="fe ob oc od oe b">AutoModel</code>创建编码器，以使用任何变压器<strong class="lb iu">(第5行)</strong>。</li><li id="1aad" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">torch.nn.ModuleDict</code>中的输出标题帮助我们使用任务id <strong class="lb iu">(第7–11行)</strong>访问每个标题。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="f7a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">前进</strong>前进<strong class="lb iu">方法</strong></p><p id="2ae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了forward方法的高级流程。如前所述，批次首先通过编码器。然后，使用任务id将每个样本重定向到适当的输出头。最后，我们计算平均损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/bb3af7adef357689cacfa99f4abd36fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXtTttGCNnMjheQ345PF3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。</p></figure><p id="ea11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码片段是<code class="fe ob oc od oe b">forward</code>方法的实现。</p><ul class=""><li id="d111" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><code class="fe ob oc od oe b">**kwargs </code>参数用于避免声明未使用的参数<strong class="lb iu">(第23行)</strong>。</li><li id="1758" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">使用每个样本的任务id对编码器输出进行过滤，然后馈送到适当的解码器<strong class="lb iu">(第29–37行)</strong>。</li><li id="d50e" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">总损失是批次中每个任务损失的平均值<strong class="lb iu">(第48行)</strong>。</li><li id="9c01" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">与其他huggingface模型<strong class="lb iu">(第56–60行)</strong>类似，返回损耗、逻辑和编码器输出。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="10b1" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">令牌分类头</h2><p id="fdf1" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">正如上面概述中所讨论的，令牌分类是一个简单的线性层，与<code class="fe ob oc od oe b">BERTForTokenClassification</code>类似。</p><p id="2410" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是这个输出头的代码片段。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h2 id="b3ec" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">序列分类头</h2><p id="adc6" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">序列分类头类似于令牌分类头，但有以下区别:</p><ul class=""><li id="3617" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">使用<code class="fe ob oc od oe b">pooled_output</code>代替<code class="fe ob oc od oe b">sequence_output</code> <strong class="lb iu">(第11行)</strong>。</li><li id="7a0c" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">在标记化过程中添加的填充被删除<strong class="lb iu">(第18行)</strong>。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="b806" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">韵律学</h1><p id="e239" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">我们需要为每种任务类型定义指标。这里，我们使用seqeval进行标记分类，使用accuracy进行序列分类。</p><p id="6046" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了区分每个任务，我们在预测中使用维度的数量。</p><ul class=""><li id="cc36" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">序列分类任务的维数是2，因为整个序列只有一个标签<strong class="lb iu">(第4行)</strong>。</li><li id="38f9" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">对于令牌分类任务，维数是3，因为每个令牌都有一个标签<strong class="lb iu">(第8行)</strong>。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="9c80" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">一起</h1><p id="c7da" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">下面的代码片段显示了一个训练循环的工作示例。</p><p id="a2f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ob oc od oe b">model_args</code>、<code class="fe ob oc od oe b">data_args</code>和<code class="fe ob oc od oe b">training_args</code>是类似于huggingface <a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py" rel="noopener ugc nofollow" target="_blank"> run_glue.py示例</a>中定义的数据类，只是<code class="fe ob oc od oe b">model_name_or_path</code>被重命名为<code class="fe ob oc od oe b">encoder_name_or_path</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><h1 id="d282" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">进一步的考虑</h1><p id="421d" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">这篇文章的目的不是为你的用例提供一个现成的代码。它旨在通过一个简单的例子向您展示构建和训练MTL模型是多么容易。但是，如果您想要扩展这项工作，以下是您需要考虑的一些事项。</p><h2 id="3fbb" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated"><strong class="ak">采样</strong></h2><p id="76fc" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">为了简化，我们对训练数据集进行了洗牌，以确保每一批都包含来自不同任务的样本。然而，如果数据集的大小不一致，低资源任务可能会饿死，这可能会导致灾难性的遗忘。你可能想检查其他MTL抽样技术，如MTL主动学习抽样[2]。</p><h2 id="d3a9" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated"><strong class="ak">损失</strong></h2><p id="4a2a" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在这个例子中，我们选择平均每个任务的损失。如果损失不在同一个范围内，那么在这种情况下，一个单一的任务可以控制梯度下降。</p><h1 id="4bfb" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">结论</h1><p id="6b19" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在这篇文章中，我用huggingface Transformers库演示了一个创建和训练多任务模型的例子。</p><p id="9eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我简化了这个例子以便于理解，但它仍然是一个很好的开始例子。</p><h2 id="527a" class="np mt it bd mu nq nr dn my ns nt dp nc li nu nv ne lm nw nx ng lq ny nz ni oa bi translated">在你走之前</h2><p id="2168" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在<a class="ae ky" href="https://twitter.com/amine_elhattami" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我，在那里我定期发关于软件开发和机器学习的推特。</p><h1 id="9fa4" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">参考</h1><ul class=""><li id="8e65" class="me mf it lb b lc nk lf nl li oi lm oj lq ok lu mj mk ml mm bi translated">[1]深度神经网络中的多任务学习概述，“CoRR，vol. abs/1706.05098，2017。【在线】。可用:<a class="ae ky" href="http://arxiv.org/abs/1706.05098" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1706.05098</a></li><li id="8839" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">[2] J. Pilault，A. E. hattami，C. Pal，“条件自适应多任务学习利用较少的参数和较少的数据改善NLP中的迁移学习”，2019，arXiv:2009.09139。【在线】。可用:<a class="ae ky" href="https://arxiv.org/abs/2009.09139" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2009.09139</a></li></ul></div></div>    
</body>
</html>