<html>
<head>
<title>Machine Learning 101: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习101:线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-101-linear-regression-72ba6c04fbf1#2022-01-12">https://towardsdatascience.com/machine-learning-101-linear-regression-72ba6c04fbf1#2022-01-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="73a5" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">机器学习101:线性回归</h1></div><div class=""><h2 id="652a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">回到数据科学的基础</h2></div><p id="406b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自从被称为“21世纪最性感的工作”以来，机器学习和数据科学已经取得了很大进展——我们现在有非常强大的深度学习模型，能够自动驾驶汽车，或者在不同语言之间无缝翻译。所有这些强大的深度学习模型的基础是一个不起眼的模型，我今天想深入探讨它的细节。那个模型是线性回归！</p><p id="4ec5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管简单，但对线性回归的良好理解是理解更高级模型如何工作的先决条件！毕竟，一个人必须先学会走，然后才能跑！今天，我将在这篇文章中详细探讨:</p><ul class=""><li id="3c68" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">线性回归背后的基本数学。</li><li id="9658" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">损失函数。</li><li id="a068" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">普通最小二乘回归的解析解。</li><li id="142a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">如何创建Python普通最小二乘求解器🐍</li></ul><p id="092d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你刚刚开始学习机器学习和数据科学，或者你只是想修改一些你在高级职业生涯中留下的基本概念，请继续阅读，直到本文结束！</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/d9a27890c25e7bd638be98f53578b4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUOVbatNI5srLJN-UZtsFA.jpeg"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated"><a class="ae lb" href="https://unsplash.com/s/photos/mount-taranaki%2C-egmont-national-park%2C-new-zealand" rel="noopener ugc nofollow" target="_blank">新西兰埃格蒙特国家公园塔拉纳基山</a>。由<a class="ae lb" href="https://unsplash.com/@dav_billings?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">大卫·比林斯</a>在<a class="ae lb" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h1 id="677a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">数据中的数学线性关系</h1><p id="ae70" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">线性回归模型假设因变量<em class="nd"> d </em>与<em class="nd"> m </em>解释变量<em class="nd"> g </em> ₁、<em class="nd"> g </em> ₂、… <em class="nd"> gₘ </em>呈线性关系:</p><p id="1f2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">d</em>=<em class="nd">c</em>₁<em class="nd">g</em>₁+<em class="nd">c</em>₂<em class="nd">g</em>₂+…<em class="nd">+cₘgₘ</em>，</p><p id="0eaf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="nd"> c </em> ₁、<em class="nd"> c </em> ₂、… <em class="nd"> cₘ </em>为与解释变量相关的系数。例如，因变量<em class="nd"> d </em>可以是房价，解释变量<em class="nd"> g </em> ₁、<em class="nd"> g </em> ₂、… <em class="nd"> gₘ </em>可以是地理坐标、年龄、建筑面积等。那栋房子。</p><p id="1afc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的线性方程可以简洁地写成下面的内积:</p><p id="67d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">d</em>=σ<em class="nd">ᵢcᵢgᵢ=</em><strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong><em class="nd">，</em></p><p id="4255" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> m </strong>是长度为<em class="nd">m</em>:<strong class="kh ir">m</strong>=<strong class="kh ir"/>[<em class="nd">c</em>₁、<em class="nd"> c </em> ₂、… <em class="nd"> cₘ </em> ]ᵀ、<strong class="kh ir"> g </strong>也是长度为<em class="nd"> m </em>的向量:<strong class="kh ir">g</strong>=<em class="nd">g</em>g<em class="nd">，g </em></p><p id="595a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于<strong class="kh ir"> m </strong>包含允许我们使用<strong class="kh ir"> g </strong> <em class="nd">对<em class="nd"> d </em>进行预测的系数向量，</em> <strong class="kh ir"> m </strong>也被称为定义<em class="nd"> d </em>和<strong class="kh ir"> g </strong> <em class="nd">之间线性关系的模型参数向量。</em></p><p id="bb2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在数据分析中，我们通常不得不处理一组因变量，而不是单个值。因此，我们通常使用一个包含<em class="nd"> n </em>因变量的向量:<strong class="kh ir">d</strong>=【<em class="nd">d</em>₁、<em class="nd"> d </em> ₂、… <em class="nd"> dₙ </em> ]ᵀ，其中<strong class="kh ir"> d </strong>中的<em class="nd"> u </em> th元素与相应的一组<em class="nd"> m </em>解释变量<em class="nd"> Gᵤ </em> ₁ <em class="nd">、Gᵤ</em></p><p id="21d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">dᵤ</em>=<em class="nd">gᵤ</em>₁<em class="nd">c</em>₁<em class="nd">+gᵤ</em>₂<em class="nd">c</em>₂+…+<em class="nd">gᵤₘcₘ</em>=σ<em class="nd">g \u\u g \u\u c。</em></p><p id="75be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">向量<strong class="kh ir">d</strong>=【<em class="nd">d</em>₁、<em class="nd"> d </em> ₂、… <em class="nd"> dₙ </em> ]ᵀ中所有<em class="nd"> n </em>元素的这种线性关系可以简洁地写成以下矩阵乘积:</p><p id="a576" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> d </strong> = <strong class="kh ir"> Gm </strong>，</p><p id="a85f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> G </strong>是大小为<em class="nd"> n </em> × <em class="nd"> m </em>的矩阵，结构如下:</p><p id="2022" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">g</strong>=[<em class="nd">g</em>₁₁，<em class="nd"> G </em> ₁₂，… <em class="nd"> G </em> ₁ <em class="nd"> ₘ </em>，<br/>。…….【<em class="nd"> G </em> ₂₁、<em class="nd"> G </em> ₂₂、… <em class="nd"> G </em> ₂ <em class="nd"> ₘ </em>、<br/>。…….… <br/>。…….[ <em class="nd"> Gₙ </em> ₁、<em class="nd"> Gₙ </em> ₂、… <em class="nd"> Gₙₘ </em> ]]，</p><p id="48c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> G </strong>的第<em class="nd"> u </em>行包含<strong class="kh ir"> d </strong>中第<em class="nd"> u </em>元素的解释变量向量。<strong class="kh ir"> </strong>和上面一样，<strong class="kh ir"> m </strong>是一个长度为<em class="nd"> m </em>的向量:<strong class="kh ir">m</strong>=<strong class="kh ir"/>[<em class="nd">c</em>₁，<em class="nd"> c </em> ₂，……<em class="nd">cₘ</em>]ᵀ包含模型参数。</p><p id="fc24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，给定一组解释变量<strong class="kh ir"> G </strong>和相应的因变量<strong class="kh ir"> d </strong>，线性回归的目标是确定最能描述<strong class="kh ir"> d </strong>和<strong class="kh ir"> G </strong>之间线性关系的<strong class="kh ir"> m </strong>。</p><h1 id="e907" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">上下确定线性系统</h1><p id="1ec0" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">现在，如果<em class="nd"> n，</em>d，中的数据点数大于<em class="nd"> m </em>中的模型参数数<strong class="kh ir"> m </strong>，那么数据提供了足够的信息来唯一地确定所有的模型参数。我们说这个系统被过度决定了。</p><p id="7a07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果<em class="nd"> m </em>大于<em class="nd"> n </em>，那么不幸的是，数据没有提供足够的信息来唯一地确定所有的模型参数。我们说这个系统是不确定的。</p><p id="7018" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于本文的其余部分，我们将只处理超定线性系统，这通常是大多数数据科学项目中的情况。对于确定系统下的<em class="nd"> </em>，需要<a class="ae lb" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日乘子</a>来获得解，使得所涉及的数学稍微复杂一些。</p><h1 id="191c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">预测误差和损失函数</h1><p id="d53f" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">为了最好地确定<strong class="kh ir"> m </strong>，我们需要最小化<strong class="kh ir"> d </strong>和<strong class="kh ir"> Gm </strong>之间的预测误差(或损失函数):</p><p id="efd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">e</strong>=<strong class="kh ir">d</strong>-<strong class="kh ir">Gm</strong>。</p><p id="894e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<strong class="kh ir"> m </strong>完美地描述了<strong class="kh ir"> d </strong>和<strong class="kh ir"> G </strong>之间的关系，那么<strong class="kh ir"> Gm </strong>将等于<strong class="kh ir"> d </strong>，预测误差<strong class="kh ir"> e </strong>将为<strong class="kh ir"> 0 </strong>。</p><p id="9ff2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步量化跨越<strong class="kh ir"> d </strong>中所有<em class="nd"> n </em>元素的整体预测误差，我们将误差向量<strong class="kh ir"> e </strong>减少到一个标量。这通常是通过使用向量的某种范数来完成的。常用的规范有:</p><ol class=""><li id="9e9f" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la ne li lj lk bi translated">L1常模:σ<em class="nd">ᵤ</em>|<em class="nd">eᵤ</em>|</li><li id="9c90" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ne li lj lk bi translated">L2范数:√(σ<em class="nd">ᵤeᵤ</em>)</li></ol><p id="cefc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">L1规范假设<strong class="kh ir"> d </strong>中的数据来自<a class="ae lb" href="https://en.wikipedia.org/wiki/Exponential_distribution" rel="noopener ugc nofollow" target="_blank">指数分布</a>，而L2规范假设数据来自<a class="ae lb" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">高斯分布</a>。</p><p id="43a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如下图所示，指数分布比高斯分布有更长的尾部。这意味着数据点远离平均值的概率在指数分布中比在高斯分布中更高。这个结果反过来意味着指数分布对于远离平均值的异常数据点比高斯分布更稳健。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b21762c2b2dd6b14e105755598f3439f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*oHXelOnkat_3LUUh7y_xFg.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">指数分布比高斯分布有更长的尾部，这使得它对异常值更稳健。作者创建的图像。</p></figure><p id="868b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当确定使用哪个范数时，理解哪个概率分布控制<strong class="kh ir"> d </strong>很重要，因为这将极大地影响<strong class="kh ir"> m </strong>的估计值。然而，令人欣慰的是，由于<a class="ae lb" href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="noopener ugc nofollow" target="_blank">中心极限定理</a>，高斯分布非常频繁地出现，并且在大多数情况下，使用L2范数应该会导致模型参数<strong class="kh ir"> m </strong>的合理的良好估计。</p><p id="eca2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，更复杂的损失函数可以通过使用L1和L2规范的组合来构建，例如<a class="ae lb" href="https://en.wikipedia.org/wiki/Huber_loss" rel="noopener ugc nofollow" target="_blank">胡伯损失</a>，或者通过引入<a class="ae lb" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化项</a>使最优解唯一。可能性是无限的！</p><h1 id="37dc" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">超定普通最小二乘解</h1><p id="4052" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">假设<strong class="kh ir"> d </strong>中的数据由高斯分布控制，我们希望最小化与预测误差<strong class="kh ir">e</strong>=<strong class="kh ir">d</strong>-<strong class="kh ir">Gm</strong>的L2范数相关的损失函数<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>):</p><p id="29c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">l</em>(<strong class="kh ir">m</strong>)=σ<em class="nd">ᵤeᵤ</em>=<strong class="kh ir">e</strong>ᵀ<strong class="kh ir">e</strong>=(<strong class="kh ir">d</strong>-<strong class="kh ir">GM</strong>)ᵀ(<strong class="kh ir">d</strong>-<strong class="kh ir">GM</strong>)=(<strong class="kh ir">d</strong>ᵀ-<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong> =<strong class="kh ir">d</strong>ᵀ<strong class="kh ir">d</strong>-<strong class="kh ir">d</strong>ᵀ<strong class="kh ir">GM-</strong><strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">d</strong>+<strong class="kh ir">m</strong>ᵀ<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">GM</strong>。</p><p id="26b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>)是由误差的平方组成的，这个问题也被称为普通最小二乘问题。我们通过对<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>)相对于<strong class="kh ir"> m </strong>求微分并将结果等于零，来最小化相对于模型参数<strong class="kh ir"> m </strong>的损失函数<em class="nd"> L </em> ( <strong class="kh ir"> m </strong>):</p><p id="dda9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">∂</strong><em class="nd">l</em>(<strong class="kh ir">m</strong>)/<strong class="kh ir">∂m</strong>= 0-<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">d</strong>-<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">d+</strong>2<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">GM</strong>= 0，<br/><strong class="kh ir">g</strong>ᵀ<strong class="kh ir">GM-g<br/></strong></p><p id="6843" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(如果你不太熟悉向量的微分，你可以参考杨、曹、钟和莫里斯的这组笔记<a class="ae lb" href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471705195.app3" rel="noopener ugc nofollow" target="_blank">。)</a></p><p id="c008" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">于是，<strong class="kh ir"> d </strong> = <strong class="kh ir"> Gm </strong>的超定普通最小二乘解就简单了:<strong class="kh ir">m</strong>=【<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">g</strong>]⁻<strong class="kh ir">g</strong>ᵀ<strong class="kh ir">d</strong>！</p><h1 id="d531" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">普通最小二乘Python求解器</h1><p id="f048" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">数学够了！现在让我们用Python实现上面导出的普通最小二乘解！</p><pre class="lr ls lt lu gt ng nh ni nj aw nk bi"><span id="79b9" class="nl mh iq nh b gy nm nn l no np">import numpy as np</span><span id="0ced" class="nl mh iq nh b gy nq nn l no np">def least_squares(G, d):<br/>    """<br/>    Over determined ordinary least squares solver.</span><span id="80ec" class="nl mh iq nh b gy nq nn l no np">    Inputs<br/>    G: np.array of nxm kernel matrix of explanatory variables. <br/>    d: np.array of n observations. <br/>    Returns<br/>    M: np.array of m estimated model parameters<br/>    """<br/>    M = np.dot(np.linalg.inv(np.dot(G.T, G)), G.T)<br/>    M = np.dot(M, d)<br/>    return M</span></pre><p id="ec28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们也创建一个一维线性系统来解决使用我们的代码。我们假设有10个无噪声测量值，由一个非常简单的线性系统生成，该系统有两个模型参数:</p><p id="440b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">d</em>=<em class="nd">c</em>₀+<em class="nd">c</em>₁<em class="nd">g，</em></p><p id="95f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们定义<em class="nd">c</em>₁=π<strong class="kh ir"><em class="nd"/></strong>和<em class="nd"> c </em> ₀ = exp(1)。</p><pre class="lr ls lt lu gt ng nh ni nj aw nk bi"><span id="f620" class="nl mh iq nh b gy nm nn l no np">g = np.arange(0, 10)<br/>d = np.exp(1) + np.pi * g</span></pre><p id="cf2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在需要准备解释变量的矩阵<strong class="kh ir"> G </strong>。为了做到这一点，我们注意到上面的等式相当于<em class="nd"> c </em> ₀是一个解释变量的系数，其值始终为1:</p><p id="21f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">d</em>=<em class="nd">c</em>₀<em class="nd">g</em>⁰+<em class="nd">c</em>₁<em class="nd">g =</em>1<em class="nd">c</em>₀+<em class="nd">c</em>₁<em class="nd">g .</em></p><p id="b0c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">矩阵<strong class="kh ir"> G </strong>因此将具有对应于2个模型参数<em class="nd"> c </em> ₀和<em class="nd"> c </em> ₁.的2列第一列包含<em class="nd"> g </em>的值，而第二列仅包含1。该矩阵<strong class="kh ir"> G </strong>可以使用二阶<a class="ae lb" href="https://en.wikipedia.org/wiki/Vandermonde_matrix" rel="noopener ugc nofollow" target="_blank">范德蒙矩阵</a>创建，使用<code class="fe nr ns nt nh b">np.vander</code>:</p><pre class="lr ls lt lu gt ng nh ni nj aw nk bi"><span id="cb28" class="nl mh iq nh b gy nm nn l no np"># The 2 here means: create a 2nd order Vandermonde matrix from g.<br/>G = np.vander(g, 2)<br/>print(G)</span><span id="5ced" class="nl mh iq nh b gy nq nn l no np">[[0 1]  <br/> [1 1]  <br/> [2 1]  <br/> [3 1]  <br/> [4 1]  <br/> [5 1]  <br/> [6 1] <br/> [7 1] <br/> [8 1]  <br/> [9 1]]</span></pre><p id="97ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在需要做的就是将<strong class="kh ir"> G </strong>和<strong class="kh ir"> d </strong>传递给最小二乘解算器来求解模型参数:</p><pre class="lr ls lt lu gt ng nh ni nj aw nk bi"><span id="8e54" class="nl mh iq nh b gy nm nn l no np">print(least_squares(G, d))</span><span id="6355" class="nl mh iq nh b gy nq nn l no np">[3.14159265, 2.71828183]</span></pre><p id="a3fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它正确地返回了<em class="nd">c</em>₁=π<strong class="kh ir"><em class="nd"/></strong>和<em class="nd"> c </em> ₀ = exp(1)的值！</p><p id="0d24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然这可能是一个非常简单的例子(使用我们事先已经知道模型参数的数据)，但它准确地显示了线性回归是如何工作的，并且可以应用到更复杂的问题中。</p><h1 id="7c70" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">更高级的线性回归</h1><p id="968c" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我们今天做了大量的数学和一些编码，最终达到这一点！然而，线性回归并没有完全完成！还有其他更复杂的方法来获得上面获得的最小二乘解，例如使用矩阵<strong class="kh ir"/><strong class="kh ir"/>的<a class="ae lb" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">奇异值分解</a>或者通过<a class="ae lb" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然法</a>！每种方法都有适合不同场景的特点。</p><p id="eeae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们还没有探索欠定的情况下，需要一个稍微复杂的数学来获得解决方案！我们也没有试图为L1范数损失函数创建求解器，这需要更高级的数值方法来求解，例如<a class="ae lb" href="https://en.wikipedia.org/wiki/Linear_programming" rel="noopener ugc nofollow" target="_blank">线性规划方法</a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>方法。</p><p id="5d9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归可能是最简单的回归模型，但它也非常深奥，充满了复杂的细节！一定要花时间探索这些更高级的主题，以便更好地理解线性回归是如何工作的！</p><h1 id="515b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">摘要</h1><p id="cfa1" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">今天我们探索了线性回归背后的数学，并用Python创建了一个普通的最小二乘解算器。今天探讨的概念，如最小化损失函数可能是基本的，但适用于所有其他更高级的模型，如神经网络！我希望您能够从这篇文章中更好地理解回归是如何工作的。感谢您的阅读！</p><h1 id="b60c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="cfe7" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">[1] W. M. Menke (2012)，<em class="nd">地球物理数据分析:离散逆理论MATLAB版</em>，Elsevier。<br/>【2】杨，曹，钟，莫理斯(2007)，<em class="nd">利用MATLAB应用数值方法</em>，Wiley .</p></div></div>    
</body>
</html>