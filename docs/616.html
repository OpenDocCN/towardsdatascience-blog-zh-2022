<html>
<head>
<title>Easily get high-quality embeddings with SentenceTransformers!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SentenceTransformers轻松获得高质量的嵌入！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/easily-get-high-quality-embeddings-with-sentencetransformers-c61c0169864b#2022-01-21">https://towardsdatascience.com/easily-get-high-quality-embeddings-with-sentencetransformers-c61c0169864b#2022-01-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="02ed" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">使用SentenceTransformers轻松获得高质量的嵌入！</h1></div><div class=""><h2 id="b8e7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍向量表示的概念，并比较TF-IDF向量和SentenceTransformers向量！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c5b764f29c9bba822d72dffaddc88b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SfXErdExImYWjb004oPQg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><h1 id="d6f0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">背景</h1><p id="89c2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在我加入<a class="ae mj" href="https://www.hk01.com/" rel="noopener ugc nofollow" target="_blank"> HK01 </a>之前，它的数据团队已经利用大量的文本数据建立了几个数据项目。虽然能够将NLP(自然语言处理)技术应用于真实世界的数据以对业务产生影响是件好事，但我注意到这些数据项目都在使用<a class="ae mj" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>来学习HK01上发布的内容的嵌入(向量表示)。这些嵌入是这些数据项目的核心，因为它们在机器学习模型中用于进行预测。这种方法有几个问题:</p><ol class=""><li id="a408" class="mk ml iq lp b lq mm lt mn lw mo ma mp me mq mi mr ms mt mu bi translated"><strong class="lp ir">性能欠佳<br/> </strong>虽然TF-IDF做得很好，但它是一种相对较老的计算文本嵌入的方法。最近有很多关于NLP的研究，现在变形金刚模型(例如<a class="ae mj" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae mj" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>、<a class="ae mj" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>)是NLP任务的最新模型。</li><li id="b12d" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated"><strong class="lp ir">难以维护</strong> <br/>升级或调试效率低下，我们需要对这些项目中的每一个都进行更改。</li><li id="bb35" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated"><strong class="lp ir">高成本</strong> <br/>由于这些数据项目中的每一个都是独立计算嵌入，因此其成本远远高于由一个集中式服务来计算全部嵌入。</li></ol><h2 id="f0cd" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">我们为什么要关心这些嵌入呢？</h2><p id="f5f9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">嵌入只是一些东西的表示，可以是文本、图像，甚至是语音，通常是矢量形式。计算文本嵌入的最简单方法是使用<a class="ae mj" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋(BOW)表示法</a>。</p><p id="3146" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">假设你有很多关于你在网上销售的产品的用户评论。要获得BOW表示或用户评论，您需要做的就是将这些评论分解成单词。然后创建一个表格(或者更像一个矩阵),它的列是注释中的单词。索引将是用户评论的id。该值是该单词是否出现在用户评论中。看起来是这样的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="0211" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">现在每一行都是一个注释的向量表示，所以注释ID 1的嵌入向量是<code class="fe nr ns nt nu b">[1, 0, 1, 1, 0, ...]</code>。由于第一行在单词“好”、“爱”和“令人惊叹”中的值为1，我们已经可以看出这个评论在情感方面是积极的。所以嵌入向量告诉我们注释的上下文含义。</p><p id="0fc5" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">一旦我们有了所有用户评论的嵌入，我们就可以做许多不同的事情。例如:</p><ol class=""><li id="81ef" class="mk ml iq lp b lq mm lt mn lw mo ma mp me mq mi mr ms mt mu bi translated">我们可以训练一个分类器来预测每个评论的情绪。</li><li id="27c9" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated">我们可以通过计算余弦相似度或欧氏距离来找到上下文相似的评论。</li><li id="b259" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated">我们可以将评论分组，然后找到这些组中出现的最常见的单词。例如，我们可能会在聚类中找到像<code class="fe nr ns nt nu b">'fast', 'delivery'</code>或<code class="fe nr ns nt nu b">'overpriced'</code>这样的词。这有助于我们了解需要改进什么。</li></ol><p id="6f93" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">想象一下，如果我们对HK01上发表的文章这样做，我们就可以做一些事情，比如主题建模(类似于第3点)，推荐用户读过的类似文章(类似于第2点)，等等！</p><h2 id="92c9" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">旁注</h2><p id="3f23" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在实践中，我们会将停用词(例如' the '，' a '，' and ')和词干词放入它们的基本形式(例如' flying '到' fly '，' cars '到' car '，' am '到' be ')。这里不解释涉及的许多数据预处理技术。</p><h2 id="e532" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">为什么弓不够好？</h2><p id="b684" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在这一点上，你可能会意识到我们的向量的维数很高，这是因为用户评论中可能出现的单词数量非常大。所以每个向量在维度上可以是<code class="fe nr ns nt nu b">1x10000</code>甚至是<code class="fe nr ns nt nu b">1x100000</code>。当我们使用这些向量时，这会降低计算速度！</p><p id="705c" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">此外，BOW(单词袋)表示法可能不是一个很好的表示法。如果我们有两个注释，例如<code class="fe nr ns nt nu b">Exactly what I wanted! 5 stars!</code>和<code class="fe nr ns nt nu b">Absolutely amazed by the product! great price!</code>。虽然我们可以知道它们都是正面的评论，但是这两个评论的BOW表示没有相似之处，因为没有重叠的单词。这就是为什么我们需要更好的东西！</p><h1 id="90cc" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">句子变压器</h1><div class="nv nw gp gr nx ny"><a href="https://www.sbert.net/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd ir gy z fp od fr fs oe fu fw ip bi translated">句子变压器文件-句子-变压器文件</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">编辑描述</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">www.sbert.net</p></div></div></div></a></div><p id="20db" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">SentenceTransformers是一个Python框架，用于最先进的句子、文本和图像嵌入。它的API使用起来非常简单:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d789" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">就这么简单，这就是我们需要编码来获得任何文本的嵌入的全部内容！</p><pre class="kg kh ki kj gt oh nu oi oj aw ok bi"><span id="dcf1" class="na kw iq nu b gy ol om l on oo">[-5.09571552e-01  6.48085847e-02  7.05999061e-02 -1.10023748e-02<br/> -2.93584824e-01 -6.55062944e-02  7.86340162e-02  1.02539763e-01<br/> -3.80028449e-02  1.71354219e-01  1.69140086e-01 -9.65721607e-02<br/>  2.04715624e-01  3.12998295e-01 -1.77713439e-01  1.53584480e-02<br/>  9.74370390e-02  2.11779386e-01  1.24351427e-01 -1.38820678e-01<br/>  4.35955316e-01 -1.17388457e-01 -3.58339213e-02 -1.36476666e-01<br/>... # hidden by me as it's too long :)</span><span id="3d87" class="na kw iq nu b gy op om l on oo">(1, 384)</span></pre><p id="37d9" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">与使用BOW得到的1和0不同，我们得到了浮点值和固定尺寸(本例中为1 x 384)。这是因为嵌入是从变压器模型中的隐藏层提取的，维度与该隐藏层的神经元数量相同。因为嵌入是由transformer模型学习的，所以前一节中的两个示例注释现在是相似的。在这篇博文中，我不打算详细讨论这个问题。现在，只需要知道转换器是一个神经网络，它的本质是可以将高维数据压缩到低维。</p><h2 id="9ffd" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">它在幕后做什么？</h2><p id="5bdc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为SentenceTransformers做出贡献的人们在<a class="ae mj" href="https://huggingface.co/sentence-transformers" rel="noopener ugc nofollow" target="_blank"> HuggingFace模型中心</a>上托管许多不同的预训练的变形金刚模型。托管预训练模型已经在大量数据(100多万甚至1B+训练对)上进行训练，因此被称为“预训练”模型。模型看到的数据样本量如此之大，以至于我们可以简单地使用它<strong class="lp ir">开箱即用</strong>来获得高质量的嵌入。当API被调用时，它从HuggingFace Model Hub下载所选择的预训练模型(或者如果给定了本地路径，则下载load)。然后，它对你的输入句子进行标记，并将它们放入模型中计算它们的嵌入。</p><p id="515c" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">API极大地简化了这个过程，以至于我们不再需要为标记化编写代码，改变PyTorch模型类中定义的<code class="fe nr ns nt nu b">predict()</code>等等。如果你使用了HuggingFace甚至纯PyTorch来做这件事，你就会知道这种痛苦了。现在，它变成了一行代码！</p><pre class="kg kh ki kj gt oh nu oi oj aw ok bi"><span id="7be0" class="na kw iq nu b gy ol om l on oo">embeddings = model.encode(sentences) # very simple, isn't it?</span></pre><h2 id="2966" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">我的文字超级长怎么办？</h2><p id="dc06" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">考虑SentenceTransformers上可用的每个预训练模型的最大长度很重要。例如，<code class="fe nr ns nt nu b">paraphrase-multilingual-mpnet-base-v2</code>型号的最大序列长度为128。这意味着对于任何超过128个标记的输入句子，都将被修剪，即只有前128个标记将被放入模型。想象一下，如果我们在HK01上发表了一篇包含数千个汉字的文章。如果我们把它放到API中，我们将只能得到代表前几个句子的嵌入！</p><p id="1427" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">克服这一点的一个简单技巧是将文章分成一系列句子，然后取句子嵌入的平均值作为文章嵌入。像这样:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="358d" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">虽然这听起来可能太简单，但我们发现它工作得很好。让我们在下一节看看它的表现。</p><h2 id="99d3" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">旁注</h2><p id="31d7" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">正如你所看到的，理解它是如何工作的非常重要。这正是为什么即使训练机器学习模型的API已经简化了这么多，公司仍然需要数据科学家和机器学习工程师的原因:)！</p><h2 id="d93a" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">与TF-IDF相比，它的性能如何？</h2><p id="4e9b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为了了解它如何优于TF-IDF矢量表示，我们:</p><ol class=""><li id="6359" class="mk ml iq lp b lq mm lt mn lw mo ma mp me mq mi mr ms mt mu bi translated">随机抽样10，000篇发表在HK01上的文章。</li><li id="923b" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated">使用句子变形器(<code class="fe nr ns nt nu b">paraphrase-multilingual-MiniLM-L12-v2</code>和平均句子嵌入)和TF-IDF计算它们的嵌入。对于TF-IDF，我使用<code class="fe nr ns nt nu b">sklearn.feature_extraction.text.TfidfVectorizer</code>和<code class="fe nr ns nt nu b">TfidfVectorizer(analyzer='word', min_df=0.001, ngram_range=(1, 3)</code>来更好地捕捉汉字。</li><li id="1525" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated">通过运行t-SNE将嵌入维数减少到2，<a class="ae mj" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> t-SNE是一种降维算法，以防你不知道</a>。</li><li id="6765" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi mr ms mt mu bi translated">将文章类别映射到数据，并将结果可视化。</li></ol><p id="242a" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">以下是可视化效果，您可以通过单击图例与图表进行交互以放大和过滤数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">请注意，由于有太多不同的类别，一些不同的类别使用相同的颜色。</p></figure><p id="8812" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">我们可以看到，在顶部(红色的)和底部(紫色的)周围只有一些小簇。这个中心根本没有清晰的星团。想象一下，如果我们完全没有类别，我们很难对同一主题的文章进行分组。现在让我们来看看句子变形器生成的嵌入:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">请注意，由于有太多不同的类别，一些不同的类别使用相同的颜色。</p></figure><p id="6b99" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi">At first glance, you can already see data points with the same category are sticking closer together. Not only that, categories that are somewhat related (e.g. movie (電影), games &amp; anime (遊戲動漫) and entertainment (即時娛樂)) are also closer together at the bottom right.</p><p id="46f8" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">想象一下，如果我们有一篇关于喜剧电影的文章，那么我们可以使用上述嵌入来找到最近的N个数据点，并将它们列在HK01上作为相关文章，最近的N个数据点也将与“电影”、“喜剧”或至少“娱乐”相关。</p><h1 id="c576" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">摘要</h1><p id="f91d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在，我们已经看到SentenceTransformers非常容易使用，并且它生成高质量的嵌入。</p><p id="9789" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">为了克服HK01的问题，我们正在创建一个新项目，目的是使用SentenceTransformers计算HK01上发布的任何内容的嵌入。目前，我们正在将每一个数据源集成到这个新项目中，以建立一个cronjob来计算新内容或编辑内容的嵌入。我们期待看到我们的推荐系统和下游数据项目的改进！</p><h2 id="4082" class="na kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">如果你喜欢这篇博文，请点击下面按钮并鼓掌！<a class="ae mj" href="https://emojipedia.org/clapping-hands-light-skin-tone/" rel="noopener ugc nofollow" target="_blank">👏🏻也在Github上关注我吧！</a></h2><div class="nv nw gp gr nx ny"><a href="https://github.com/thomas-tf" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd ir gy z fp od fr fs oe fu fw ip bi translated">托马斯-tf -概述</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">thomas-tf有13个可用的存储库。在GitHub上关注他们的代码。</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">github.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow kp ny"/></div></div></a></div><h1 id="b25d" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><ul class=""><li id="bc40" class="mk ml iq lp b lq lr lt lu lw ox ma oy me oz mi pa ms mt mu bi translated">变形金刚—<a class="ae mj" href="https://www.sbert.net/index.html#" rel="noopener ugc nofollow" target="_blank">https://www.sbert.net/index.html#</a></li><li id="003e" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi pa ms mt mu bi translated">TF-IDF—<a class="ae mj" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Tf%E2%80%93idf</a></li><li id="5336" class="mk ml iq lp b lq mv lt mw lw mx ma my me mz mi pa ms mt mu bi translated">鞠躬—<a class="ae mj" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Bag-of-words_model</a></li></ul></div></div>    
</body>
</html>