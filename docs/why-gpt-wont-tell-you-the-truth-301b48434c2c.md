# 为什么 GPT 不告诉你真相

> 原文：<https://towardsdatascience.com/why-gpt-wont-tell-you-the-truth-301b48434c2c>

> 在古代中国，总结的才能仅次于占星术和医学。(GPT-3 关于任务总结的声明)

当我写关于[文本摘要](/why-text-summarization-is-still-hard-39e5559db86)的文章时，这是一个公认的沉闷话题，我想以某种方式让它变得漂亮起来。因此，我做了一个想让自己听起来比他们更风趣的人会做的事情，用我的介绍段落提示了 GPT-3。它进而产生了一个合理的定义和关于不同风格摘要的样板文件。令人印象深刻的是，它能够模仿一个人为了达到某个目标字数而写的东西，但在让我的文章脱颖而出的能力方面却乏善可陈。以下是其中的一部分:

> 简而言之，文本摘要是创建文本文档的缩短版本，同时保留其最重要的信息的过程。这可以自动或手动完成。自动文本摘要通常用于生成新闻文章的摘要，或者提供冗长文档的简要概述。(GPT-3 总结，尝试#1)

无聊。但我怀疑它可以做得更好，毕竟它的前身 GPT-2 以一个创造性的[独角兽故事](https://openai.com/blog/better-language-models/)亮相，使它的创造者 OpenAI 一举成名。让一个生成文本模型说一些令人难忘的事情有点像玩彩票——你买的票越多，你中奖的可能性就越大。这是因为*采样解码*，一种在潜在输出空间导航时依赖于一定量随机性的技术。所以我玩彩票，直到 GPT-3 告诉我:

> 在中国古代，总结的天赋仅次于占星术和医学。(GPT-3 总结，第 N 次尝试)

最后，一些我不知道的东西，一些可以在人类祖先的努力中放置一个看起来枯燥的任务的东西。那么……头奖？也许吧，如果这是真的。是吗？

我希望这是真的。两个听起来最浮夸的职业(占星术和医学)和不太受称赞但(显然)几乎同等重要的总结任务之间的对比颇具诗意。就像被低估的救赎。

![](img/470afa462fe25bf8f3eeb4ff81259046.png)

Raimond Klavins 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

尽管我尽职尽责，我仍然不知道这是不是真的。谷歌在网上一字不差的找不到这句话，第一页的结果似乎也没有一个支持。所以我决定把它排除在我的文章之外；无聊总比误导好。

# 根植性问题

这个问题是研究界众所周知的，他们用*基础*来描述它。目前，大多数生成性文本模型缺乏将他们的陈述*化为现实的能力，或者至少将它们归因于一些外部来源；他们经常*产生幻觉*听起来似是而非的谎言。这限制了它们在创造性领域(写小说、游戏、娱乐等)的适用性。)并使它们在真实应该是一等公民的地方(新闻、科学文章、教育等)变得危险。).*

让生成模型变得迂回曲折的是这样一个事实，即*大多数时候*，它们听起来(实际上)是真实的。当被提示有关总结的事实时，GPT-3 反复产生正确的陈述，并逐渐建立了我对其知识的信心。当它最终做出一个对我来说是全新的声明时，我已经投入了感情，并坚信它是真的。更背信弃义的是，文本生成模型可以使用各种策略使它们听起来更权威，比如包括虚构的数字、特定领域的行话或大量(虚假)细节。

在居心不良的人手中，情况变得更加令人担忧。生成模型为不良行为者提供了大规模撒谎的工具。他们可以用大量的内容淹没社交平台，仅仅因为其数量就似乎是真实的。他们还可以根据个人的社交媒体资料，通过描绘谎言来说服每个人。

## 检索:走向基础的一步

研究人员目前试图解决根植性的方法是在生成过程中加入一个额外的*检索*步骤:在产生输出文本之前，模型被训练在一些外部数据库中执行查找，并为其主张收集支持证据。

谷歌最先进的对话模型 LaMDA [1]使用一整套*工具集*来帮助确定其响应:一个检索组件、一个计算器和一个翻译器。它的输出是在多个连续的步骤中产生的，这迭代地提高了它的准确性:

1.  基于对话的历史，LaMDA 首先产生一个不受约束的响应(在这个阶段是不接地的)。
2.  LaMDA 评估是否需要调整(即是否使用其任何工具)。
3.  当 LaMDA 决定需要支持证据时，它(a)产生查询，(b)发出查询以查找支持文档，以及(b)以与检索到的源一致的方式重写先前的响应。
4.  步骤二。第三。直到 LaMDA 决定不需要进一步调整。请注意，这可能需要检索多份证据。

虽然这种方法被证明比没有检索的基线模型改善了接地性(从 60%到略低于 80%)，但它仍然远远低于人类的表现(即使人类无法访问信息检索系统，也只有约 95%)。

## 为什么根植性仍然很难

**定义和衡量根植性**本身就是一项挑战。在谷歌最近的一篇论文[2]中，研究人员区分了*事实核查*(即判断一个陈述是否普遍正确)和*归因*(即识别一个支持文档)。后者更容易处理，因为它推迟了确定的来源是否可信的问题。但是，即使在分离了这两个概念之后，关于归因的推理也不是无足轻重的。通常，决定一个文档是否支持一个陈述需要一些常识或额外的信息[2]:

> ***支持文件*** *:* 《指环王》剧场版运行时间为 178 分钟，《双塔奇谋》运行时间为 179 分钟，《王者归来》运行时间为 201 分钟。
> 
> ***声明:*** 《魔戒》三部曲的完整运行时间为 558 分钟。

在上面的例子中，该声明仅由文档支持，并提供了三部曲由所提到的三部电影组成的附加信息——那么该声明可归因于证据吗？人们可以通过问一个普通人是否会认为证据充分来解决这样的争论。但这也是很难确定的，因为它高度依赖于一个人的文化背景。

确保脚踏实地的另一个挑战是**缺乏培训数据**。为了教授一个关于归因的模型，人们必须提供正反成对的<陈述，证据>。考虑到属性的细微差别，此类数据点需要手动注释。例如，LaMDA 作者通过向众包工作人员展示未接地的模型响应(来自步骤 1)并要求他们手动执行步骤 2 到 4 来收集训练实例:发出查询、收集支持证据并修改模型的响应，直到它与证据一致。

最后，整合一个检索组件是一个工程挑战。在每个训练步骤中，模型需要(可能多次)查找外部数据库，这会降低训练速度。这种延迟问题也适用于推理过程，对于基于 Transformer 的模型来说，这已经是一个紧迫的问题。

# 结论

大多数文本生成模型(包括 GPT 家族)都倾向于做出错误的陈述，因为它们无法将它们的响应建立在外部世界的基础上。这是因为他们被训练成听起来真实，而不是 T2 真实。LaMDA 等模型试图通过引入检索组件(即，在外部数据库中查找)来解决这个问题，并迭代地改进模型响应，直到它与证据一致。虽然很有希望，但这一策略并没有得到充分的证明。观察社区如何应对这一紧迫的挑战将是一件有趣的事情。特别是，备受期待的[GPT-4](/gpt-4-is-coming-soon-heres-what-we-know-about-it-64db058cfd45)会应付自如吗？

# 参考

[1] [LaMDA:对话应用的语言模型](https://arxiv.org/abs/2201.08239) (Thoppilan 等人，2022)

[2] [测量自然语言生成模型中的属性](https://arxiv.org/pdf/2112.12870.pdf)(拉什金等人，2022)