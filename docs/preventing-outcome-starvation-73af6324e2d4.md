# 防止结果饥饿

> 原文：<https://towardsdatascience.com/preventing-outcome-starvation-73af6324e2d4>

## 为什么在操作机器学习模型时需要紧急处理退化的反馈回路

![](img/f090d23c9a509fb17d15c124e871f3ca.png)

图片由 [Vertigo3D](https://www.istockphoto.com/photo/big-data-network-abstract-concept-with-abstract-binary-code-gm1288922463-384746197#:~:text=Credit%3A-,Vertigo3d,-Stock%20photo%20ID) 提供，通过 iStock 授权

预测模型很少是静态的——可操作模型通常具有更新节奏。例如，在 Mobilewalla，我们的模型每 30-180 天更新一次。在每个更新周期结束时，基于对自上次更新以来其输出的保真度的评估来修改模型。这是标准模型维护实践的重要组成部分，被称为*反馈回路*。

当先前的输出不公平地影响未来的结果时，就会出现*退化反馈回路* (DFL)。我最喜欢的关于 DFL 的解释是在 Chip Hyuen 的课堂笔记中，她在斯坦福大学教授 CS329S (ML 系统设计)课程(在这里找到 T8)。为了说明 DFLs，让我直接引用其中的一段话:*假设你构建了一个系统，向用户推荐他们可能喜欢的歌曲。系统排名高的歌曲首先显示给用户。因为它们首先被显示，所以用户更多地点击它们，这使得系统更加确信这些推荐是好的。一开始，两首歌曲 A 和 B 的排名可能只有微小的不同，但是因为 A 最初的排名稍微高一些，所以 A 得到了更多的点击，这使得系统对 A 的排名更高。过了一段时间，A 的排名变得比 b 高得多。退化的反馈循环是流行电影、书籍或歌曲不断变得更受欢迎的原因之一，这使得新项目很难进入流行列表。*

DFLs 在推荐系统中得到了广泛的研究。最近，随着机器学习中公平和偏见的概念变得流行(参见[此处](/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)的教程)，DFL 尤其被认为是 ML 偏见的关键驱动因素[1]。然而，除了在某些特定的环境中，例如驱动公共/社会服务的模型(例如，跨社区的犯罪预测和财富预测)，公平以及 DFL 还没有成为主流。大多数构建推动商业成果的模型的数据科学从业者通常不会在其建模工作流中优先处理 DFL。如果一个模型最终受到了公平性的挑战，只要它提供的输出具有可接受的预测准确性，大多数建模者和 MLOps 专业人员就不会过分担心。这种缺乏关注也反映在通常用于模型构建的软件实用程序中，其中不提供防止 DFL 的特征。总之，退化的反馈回路现象，以及随之而来的模型中的不公平性，对于广大数据科学家来说，还没有被优先考虑为具有重要实际意义的问题。

在这样的背景下，我来到了本文的主题:为什么这个视角需要改变。事实证明，虽然我们可以忍受大多数模型中某种程度的不公平，但由于 DFL 对预测质量产生负面影响，还会出现另一个更紧迫的实际问题——***结果饥饿*** 。我们首先解释这种现象，然后提供两个真实世界的例子来说明它的范围。

# 什么是结果饥饿？

任何预测模型都必须通过三项测试才能实际使用:

1.它的预测精度是否超出了可接受的阈值？

2.模型有弹性吗？

*3。它能产生有意义的结果吗？*

前两个广为人知，但第三个对大多数读者来说是新的——我们从未在文学作品中看到过讨论。为了理解这个问题，考虑机器学习中的*分类*问题，其中模型的目标是将输入映射到多个预先确定的桶或类中。分类构成了机器学习中最广泛的一类实际问题——大量的实际预测问题都归结为分类场景，如下表所示。

![](img/cda64313e1f327a093f61e9884e13646.png)

分类方案

在上述每种情况下，模型结果(*所需预测*栏)关键地告知相关业务流程(*实际/商业用例*栏)。模型不仅需要有弹性，并产生准确的预测结果，*这些结果还需要以商业上有意义的数量出现*。以上面的个性化用例为例，考虑一个电子商务网站。一旦一个匿名用户登陆该酒店，ML 模型会预测用户的人口统计数据，以便随后的参与可以个性化。假设该模型遵循预定的更新节奏，并且具有弹性。进一步假设其结果具有可接受的预测保真度，例如，它以 80%以上的准确度预测性别。*然而，对于模型中的每个输入，它只能成功分类 10%* 。换句话说，对于每个出现的匿名用户，该模型只能在 10%的情况下有足够的把握确定他们的年龄/性别。很明显，尽管具有弹性，并且高度准确，但是该模型是没有用的，因为它不能产生足够有意义的结果来驱动它所支持的用例。在这种情况下，模型是**结果饥饿**。在 Mobilewalla，我和我的同事构建了支持各种行业和使用案例的模型——电信行业的用户流失、拼车和食品配送行业的高价值用户获取、“先买后付”(BNPL)公司的信誉预测、消费品行业的股票/库存预测等等——我们发现结果饥饿是一种普遍存在的现象。我们生产的 120 多种型号中，超过 50%的型号都随着时间的推移而出现问题。有趣的是，像弹性一样，结果饥饿是一种“渐进疾病”——模型从来不会一开始就饥饿，但随着时间的推移会变得饥饿。说到本文的关键，结果饥饿的主要原因是退化反馈循环(DFL)的存在。

# 退化的反馈回路如何导致结果饥饿

我们现在提供两个真实世界的例子来说明 DFL 如何导致结果饥饿。在这些项目中，我们直接参与相关模型的构建和操作。

**示例 1:拼车中的高价值用户获取**

拼车公司的主要运营成本是获取用户。然而，获得的用户转变为“有价值的客户”的可能性很低。根据我们与许多全球共乘服务提供商的合作经验，只有 12–14%的收购用户会转化为高价值客户(HVC)。因此，这些公司投入了大量的数据科学资源来开发拼车消费倾向模型。这些模型将一个潜在客户(及其特征)作为输入，返回一个“消费倾向得分”作为输出。这个分数代表潜在客户成为 HVC 的可能性。使用这些分数，提供商试图通过试图优先获得那些更有可能变成 HVC 的用户来在大的潜在客户群中进行区分。这些模式对这些提供商至关重要——即使被收购资产池中的 HVC 比例出现 1%或 2%的有利波动，也能带来每年数亿美元的额外收入。

现在让我们考虑这种模型的一个实例。更具体地说，让我们考虑锚定该模型的预测性消费者特征。虽然有许多这样的特征，但事实证明具有高预测能力的一个是*在过去 90 天内在非国内机场观察到的 10 次以上的前景* —让我们简称为 OBSGT10。这是一个真实世界的预测器，我们发现它在这些模型中有很大的实际用途。为了说明 DFL 对该模型的影响，进一步假设 OBSGT10 是我们模型中最具预测性的特征。这通过模型函数中的“特性权重”得到反映——obsg T10 开始时的权重略高于许多其他特性。随着模型的运行，它会稍微优先考虑(这是它的本意)满足 OBSGT10 的潜在客户，但也会关注其他特征。一切顺利——该模型产生了预期的良好结果——并提高了用户获取的投资回报率。

当模型更新时间到来时，新的训练集现在充满了来自上一个周期的反馈，与原始训练集相比，满足 OBSGT10 的用户略多，并且这一次 OBSGT10 功能的权重比前一个版本稍高。这种循环继续下去，很快，OBSGT10 的权重明显高于其同类，模型变得明显偏向 OBS gt 10——它在前景集中渴望这一特性。事实上，在现实世界中，当这种情况发生时，我们已经看到特征集发生了变化——开始时权重稍低但完全可以预测的属性开始完全脱离特征集。

现在我们来看问题的症结所在——我们的模型很快就用完了在目标池中满足 OBSGT10 的潜在客户，并且在其改变的状态下，无法再为有意义规模的收购集提供动力。请注意，在最初的版本中，当第一次运行时，它被赋予了各种各样的功能，仍然可以驱动完全可接受的规模。然而，由于 OBSGT10 导致的 DFL，它最终成为结果饥饿。

**示例 2:评估“先买后付”( BNPL)公司的申请人信用风险**

我们现在关注一个完全不同的行业中完全不同的用例。

BNPL 公司是金融科技公司的一个重要类别，它们主要是在新兴市场发展起来的，以应对两种社会现象交织在一起造成的局面:

“大众富裕阶层”的崛起

大多数大众富裕阶层缺乏传统的信用记录(信用卡持有情况、信用记录)

以印度为例，在超过 2.5 亿的中产阶级中，只有大约 5700 万张信用卡(2021 年[https://www . statista . com/statistics/1203267/India-number-of-credit-cards/](https://www.statista.com/statistics/1203267/india-number-of-credit-cards/))。

BNPL 供应商向传统信贷保护伞之外的消费者提供信贷。因为这些消费者中的大多数没有信用评分，或创建信用评分所需的原始数据，BNPLs 依赖于使用机器学习创建自己的信用风险模型。这些模型将一个潜在客户(和她的特征)作为输入，返回一个“信用评分”作为输出。这些模型对这些提供商至关重要，代表了他们核保策略的支柱。

和前面的例子一样，让我们考虑锚定信用风险模型的预测特征。因为传统的信用数据对于典型的贷款申请人来说是不可用的，所以特征是从“替代数据”设计的。一个具有特别高预测能力的是*居住在手机平均价值超过$Threshold ( )* 的家庭——让我们简称为 HHPHVAL。按照示例 1 中描述的完全相同的模型操作和反馈序列，使用 HHPHVAL 创建了一个退化的反馈环路，导致该特性在几个更新周期后被不公平地加重。因此，满足这一特征的消费者得到优先考虑，并且不值得推荐的应用的量很快减缓到涓涓细流，因为只有那么多家庭具有足够高的 HHPHVAL。该模型现在缺乏结果。

# 摘要

DFL 被认为是 ML 模型中公平和偏差问题的主要原因，但没有受到从业者的重视，因为大多数构建商业模型的数据科学家都愿意忍受某种程度的偏差。然而，DFLs 的一个结果和由此产生的不公平是结果饥饿，这是 ML 模型的实用性的实质性实际减损，否则 ML 模型是高质量的。令人惊讶的是，这种现象在文献中并没有得到太多的关注，至少在我们的经验中，它的发生是普遍的。在本文中，我介绍了结果饥饿的概念，以及它与 DFL 的关系。在未来的一篇文章中，我将讨论 Mobilewalla 如何使用 [Anovos](http://www.anovos.ai) ，一个开源特性工程[库](https://github.com/anovos)，通过减少 DFL 的影响来解决结果饥饿问题。

[1] Dana Pessach & Erez Shmueli，**关于机器学习公平性的评论，** [计算调查，第 55I 卷](https://dl.acm.org/toc/csur/2023/55/3) (3)，2023 年 4 月，第 1–44 页

( )不同地区的$Threshold 值不同。在我们的模型中，它在印度尼西亚的实例化价格是 400 美元，在印度是 225 美元