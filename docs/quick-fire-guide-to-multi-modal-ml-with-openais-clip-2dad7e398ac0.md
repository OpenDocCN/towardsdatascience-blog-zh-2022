# 多模态 ML 快速入门指南

> 原文：<https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0>

## 了解如何使用剪辑和矢量嵌入在文本和图像之间进行转换

![](img/6ee01431494a2c61fc6d2655e1b901db.png)

作者修改，背景照片由 [Maximalfocus](https://unsplash.com/@maximalfocus?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/future?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

经过短短几年的生活，儿童可以理解简单单词背后的概念，并将它们与相关图像联系起来。他们能够识别物理世界的形状和纹理与书面语言的抽象符号之间的联系。

这是我们认为理所当然的事情。世界上很少有人(如果有的话)会记得这些“基本”技能超出他们能力的时候。

电脑不一样。他们可以计算出火箭穿越太阳系所需的参数。但是如果你让一台电脑找到一张“公园里的一只狗”的图像，你最好向美国国家航空航天局索要一张去空间站的免费机票。

至少，直到最近还是如此。

在这篇文章中，我们将看看 OpenAI 的剪辑。能够理解文本和图像之间的关系和概念的“多模态”模型。正如我们将会看到的，剪辑不仅仅是一个花哨的客厅把戏。它能力惊人。

# 对比学习？

**C**ontrastive**L**language-**I**mage**P**再训练(CLIP)由两个模型并行训练而成。用于图像嵌入的视觉变换器(ViT) *或* ResNet 模型和用于语言嵌入的变换器模型。

在训练期间，*(图像，文本)*对被馈送到相应的模型中，并且都输出表示矢量空间中相应图像/文本的 512 维矢量嵌入。

*对比*组件采用这两个向量嵌入，并计算模型损失作为两个向量之间的差异(例如，对比度)。然后优化两个模型以最小化这种差异，并因此学习如何将相似的*(图像，文本)*对嵌入到相似的向量空间中。

在这个对比预训练过程之后，我们剩下 CLIP，一个能够通过共享向量空间理解语言和图像的多模态模型。

# 使用夹子

OpenAI 开发并发布了`clip`库，可以在 GitHub 这里找到。然而，Hugging Face 的`transformers`库托管了 CLIP 的另一个实现(也是由 OpenAI 构建的)，这个实现更常用。

拥抱脸实现*不*使用 ResNet 进行图像编码。它使用与文本转换器配对的 ViT 模型的替代设置。我们将通过把一个简单的文本-图像搜索脚本串在一起来学习如何使用这个实现，这个脚本可以适用于图像-图像、文本-文本和图像-文本模态。

## 加载数据和剪辑

首先，我们将安装演示所需的库，下载数据集，并初始化 CLIP。

```
pip install -U torch datasets transformers
```

我们将使用*“imagenette”*数据集，这是由拥抱脸托管的～10K 图像的集合。

这给了我们从收音机到狗的各种图像。所有这些图像都作为 PIL 图像对象存储在`'image'`特征中。

现在我们通过`transformers`库初始化 CLIP，如下所示:

这里发生了一些事情:

*   整个`device`部分是设置我们的实例，以使用我们可用的最快的硬件(M1 芯片上的 MPS，否则 CUDA)。
*   我们设定了`model_id`。这是在这里找到的剪辑型号[的名称](https://huggingface.co/openai/clip-vit-base-patch32)。
*   然后我们初始化一个用于预处理文本的`tokenizer`，一个用于预处理图像的`processor`，以及一个用于生成矢量嵌入的剪辑`model`。

现在我们准备开始创建文本和图像嵌入。

## 创建文本嵌入

文本转换器模型将我们的文本编码成有意义的矢量嵌入。为此，我们首先对文本进行标记化，将其从人类可读的文本转换为转换器可读的标记。

然后使用`get_text_features`方法将这些令牌输入模型。

这里我们有一个 512 维的向量来表示短语“雪中的狗”的语义含义。这是我们文本图像搜索的一半。

## 创建图像嵌入

下一步是创建图像嵌入。同样，这非常简单。我们将`tokenizer`换成`processor`，这将给我们一个叫做`pixel_values`的调整后的图像张量。

我们仍然可以看到处理过的图像。它已被调整大小，像素“激活”值不再在 Matplotlib 可以读取的 0–255 的典型 RGB 范围内，因此颜色无法正确显示。尽管如此，我们可以看到这是我们之前看到的同一台索尼收音机。

接下来，我们用 CLIP 处理这些输入，这次使用的是`get_image_features`方法。

在此基础上，我们用剪辑为文本和图像构建了矢量嵌入。有了这些嵌入，我们可以使用像欧几里德距离、余弦相似性或点积相似性这样的度量来比较它们的相似性。

然而，我们不能只拿一个例子来比较，所以让我们继续，在更大的图像样本上测试。

我们将从`imagenette`数据中随机抽取 *100 张*图像。为此，我们首先随机选择 100 个索引位置，并使用它们来构建图像列表。

现在我们遍历这 100 张图片，用 CLIP 创建图片嵌入。我们将它们全部添加到一个名为`image_arr`的 Numpy 数组中。

在底部的单元格中，我们可以看到我们的图像嵌入中的最小值和最大值分别是`-7.99`和`+3.15`。我们将使用*点积相似度*来比较我们的向量。如果我们想准确地将它们与点积进行比较，我们需要将它们归一化。我们这样做:

现在我们准备比较和搜索我们的向量。

## 文本图像搜索

如前所述，我们将使用点积来比较矢量。文本嵌入将充当“查询”,我们将使用它来搜索最相似的图像嵌入。

我们首先计算查询和图像之间的点积相似度:

这给了我们 100 分，例如，每个文本嵌入到图像嵌入对的一对多得分。我们所知道的是按照降序对这些分数进行排序，并返回各自得分最高的图像。

在 1 号位置，我们有一只狗在雪地里，这是一个很好的结果！这是*很可能是*在我们的 100 张图片样本中*唯一的*一张狗在雪中的图片。这就是我们如何使用 CLIP 执行文本图像搜索。

CLIP 是一个惊人的模型，可以以任何顺序或组合应用于语言图像领域。我们可以使用相同的方法执行文本-文本、图像-图像和图像-文本搜索。

事实上，我们可以通过简单地将图像和文本向量添加到一个存储中，然后使用图像或文本进行查询，来同时完成所有这些工作。

如果你坚持使用较少的搜索项目，我们的方法是很好的。然而，当我们开始搜索更多的记录时，这是缓慢的，甚至是不可能的。为此，我们需要一个[向量数据库](https://www.pinecone.io/learn/vector-database/)。允许我们将它扩展到数百万甚至数十亿条记录。

如果你有兴趣了解更多关于多模态模型、自然语言处理或矢量搜索的知识，可以查看我的 [YouTube 频道](https://www.youtube.com/c/jamesbriggs)，联系 [Discord](https://discord.gg/c5QtDB9RAP) ，或者跟随我的一门免费课程(下面的链接)。

感谢阅读！

<https://www.pinecone.io/learn/nlp/>  <https://www.pinecone.io/learn/image-search/> 