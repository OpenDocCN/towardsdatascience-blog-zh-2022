<html>
<head>
<title>Don’t waste your unlabeled data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要浪费你未标记的数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dont-waste-your-unlabeled-data-9bb778743b41#2022-01-03">https://towardsdatascience.com/dont-waste-your-unlabeled-data-9bb778743b41#2022-01-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4fde" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""><h1 id="7502" class="pw-post-title jb jc it bd jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy bi translated">不要浪费你未标记的数据</h1></div><div class=""><h2 id="8b8a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">评估您的模型的弱点，并在可用数据中确定最具信息性的数据样本。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4023cf5649b61ae499fbff6a1b1e308d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QMbf1z90AKuXp3he"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">不确定性之光。迈克尔将抱在<a class="ae lh" href="https://unsplash.com/photos/jPhXEmOxt0w" rel="noopener ugc nofollow" target="_blank">的Unsplash </a>上。</p></figure><h1 id="bb7b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">以数据为中心的方法</h1><p id="1482" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在过去的十年里，人工智能的主要趋势是专注于建立创造性的复杂模型。但是最近几年，任何人都可以用Python等流行语言和Pytorch、Tensorflow、JAX等库来使用最先进的架构。<strong class="mc jd">现在，人工智能的最高优先级是让这些模型对每个特定的用例都有价值</strong>。这需要对可用的精选数据有深刻的理解，不幸的是，数据分析和如何建立一个良好的训练集是经典ML课程中很少教授的技能，而是通过时间和汗水学习的。</p><p id="d791" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">到目前为止，数据并不被认为是一个主要问题:可变大小的标准研究数据集是免费的(<em class="nb"> ImageNet，COCO，GLUE等</em>，以及预训练模型的权重(<em class="nb"> ModelZoo，Huggingface </em>等)，作为对特定数据子集进行微调的先验知识。除此之外，在小规模数据集上训练本地模型成为一种商品。但在实践中，在生产阶段成功制作ML模型是一个难题。</p><p id="79a8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">失败的主要原因之一是研究模板上的模型和现实环境中部署的模型之间的准确性损失。事实上，这些公司中有13%的模型能够投入生产[7]。</p><p id="7186" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当我们知道随着AutoML平台的兴起，甚至模型搜索也已经自动化时，这似乎很令人惊讶。瓶颈是什么？</p><p id="0211" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">瓶颈是数据</strong>。下一个需要强大框架和标准化的挑战是处理数据的管道。行业中的人工智能可能需要从以模型为中心的方法后退一步，转向以数据为中心的方法。以数据为中心的方法试图将模型开发与训练数据集的创建和模型性能的监控协调起来。</p><blockquote class="nc"><p id="211b" class="nd ne it bd nf ng nh ni nj nk nl mv dk translated">“并非所有数据都是生而平等的”——詹妮弗·普伦斯基</p></blockquote><h2 id="c98b" class="nm lj it bd lk nn no dn lo np nq dp ls mj nr ns lu mn nt nu lw mr nv nw ly iz bi translated">不确定性的量化</h2><p id="5632" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">因为真实世界的数据是杂乱的、嘈杂的、错误的、充满离群值的，并且不遵循正态分布，所以量化不确定性对于构建可信的人工智能模型至关重要。选择好你输入的数据。剔除可疑数据。避免致命的垃圾进，垃圾出，节省时间和金钱。除非我们可以信任模型预测，否则它几乎毫无用处。为了发现这些病态数据，我们将探索作为数据质量代理的模型的不确定性。<strong class="mc jd">我们想确定数据在哪里混淆了模型。</strong></p><h2 id="8224" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated"><strong class="ak">不确定性的原因有哪些？</strong></h2><p id="1c88" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">不确定性有许多原因。当训练和测试数据集的分布不匹配时，推理中可能出现不确定性。当数据类重叠或数据本身有噪声时，它也会出现。一般来说，我们应该意识到数据集中不确定性的原始来源，以便创建一个有效的策略来尽可能地减少它。</p><p id="c689" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在主动学习文献中，这些不确定性被分为两大类:<strong class="mc jd">认知</strong>和<strong class="mc jd">任意</strong>不确定性。一方面，随机不确定性是由数据的噪声产生的，并且在同一实验的每次运行中都会有所不同。这种不确定性是不可减少的，因为它是数据的固有属性。另一方面，认知的不确定性可以通过学习来减少。通过增加数据集大小或使模型复杂化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/2b260315df9d127761c80b7d69782b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B55KWJc7Qv2loHHN3Y4rrQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">两种不确定性的示意图。图自开篇:【https://arxiv.org/pdf/2011.06225.pdf T4】</p></figure><h2 id="888b" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">从理论到实践</h2><p id="d402" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">默认情况下，我们经常错误地将softmax函数输出的每个标签的概率视为每个标签的模型置信度。这种误解可以通过使用一种简单的技术来避免:考虑每个softmax输出相对于其他标签或其他实例的相对值。</p><p id="38eb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在本文中，我将分享扫描所有可用数据(标记的或未标记的)的标准技术，以帮助对数据和模型的当前状态做出以数据为中心的结论。这些技术根据模型对其预测标签的置信度来排列每个数据点。我们可以使用不同的函数，将“不确定性”分数归属于每个数据点。高度不确定性意味着模型对该数据点不确定，推断很可能是错误的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/b378e66ea75c1975db76c6b2ccf5e325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfb8XY_ksJtbQnKOcpxK5w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用不确定性分数识别类边界附近的实例。图片由作者提供，灵感来自[1]。</p></figure><h2 id="8ba1" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">为什么我们应该尝试自动化每个可用数据点的评分？</h2><ul class=""><li id="204f" class="oe of it mc b md me mg mh mj og mn oh mr oi mv oj ok ol om bi translated">利用现有数据和模型</li><li id="ab80" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">在数据集中查找边缘案例</li><li id="ccb7" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">查找质量差的数据以重新标记或从训练数据集中排除</li><li id="e12a" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">通过追踪歧义来澄清标签说明</li><li id="7193" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">找到当前模型的弱点</li><li id="4444" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">识别新传入数据的数据漂移</li><li id="04b5" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">误差分析，以找出下一个要标记的样本</li><li id="fdea" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">查找异常值</li></ul><p id="d75c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">考虑到我们将模型投入生产的不断发展的世界，持续改进数据是必要的一步。我听说过一个保龄球图片分类器的故事，当COVID开始时，因为社交媒体在这件事上的趋势，它将空卫生纸架上的图像标记为保龄球图片。我们永远不知道未来的数据会是什么样子，我们应该始终意识到我们让数据溜进我们的模型预测。</p><h2 id="445e" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">我们的实验</h2><p id="8b8f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本文中，我们评估了为CNN模型提供分类的图像。我们将使用EMNIST进行实验。</p><p id="9b2e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们使用softmax根据最后一个激活函数计算的输出逻辑创建概率分布。Softmax无法提供模型确定性的概率，并且正在丢失信息，因为它使用指数，所以它丢失了数字的比例([1，4，2，1]将产生与[101，104，102，101]相同的分布)。但我们仍然可以使用softmax作为模型可信度的代理，并从高模型可信度到低模型可信度对我们的数据样本进行排序。</p><blockquote class="nc"><p id="fa93" class="nd ne it bd nf ng nh ni nj nk nl mv dk translated">“没有算法能在坏数据中存活”——罗伯特·莫纳克</p></blockquote><h1 id="612c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki os kj lu kl ot km lw ko ou kp ly lz bi translated">不确定性评分</h1><p id="ef38" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们希望将数据标注在我们的模型最混乱的地方——不确定性。因此，对于每个数据点，我们将为所有未标记的数据点给出单个<strong class="mc jd">不确定性分数</strong>，并<strong class="mc jd">将它们从最不确定性到最不确定性</strong>进行排序。</p><p id="6b4a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">该方法是不确定性量化的最简单、最便宜的方法。<strong class="mc jd">您可以识别位于模型决策边界附近的数据点</strong>。</p><p id="fad7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于一个数据点，您的模型:</p><ul class=""><li id="72ec" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated">给不同的类一个相似的softmax概率(例如，输入图像是狗，输出softmax是:狗33.3%，猫33.3%，鸟33.3%)</li><li id="875c" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">将低概率赋予最高概率(例如，在十个等级中，最高的softmax是“青蛙”, softmax为10.5%)</li></ul><p id="bfba" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们将按照从简单到复杂的顺序列出评分技术。</p><p id="ec4b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们称<em class="nb"> p_max </em>为针对一幅图像推断出的类的最高softmax分数，称<em class="nb"> p_2nd </em>为针对一幅图像的类的第二高置信度。<em class="nb"> n </em>为分类的类数。</p><p id="97e6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于下面的每个函数，输入<em class="nb"> prob_dist </em>是一个数据点的softmax输出数组，输出是该数据点的模型的0到1之间的不确定性分数。如果数据集是具有三个类的多类分类，<em class="nb"> prob_dist </em>的形状为(3，)。</p><h2 id="4805" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">评分类型:</h2><ul class=""><li id="67f9" class="oe of it mc b md me mg mh mj og mn oh mr oi mv oj ok ol om bi translated"><strong class="mc jd">最小置信度</strong>(最大置信度预测和100%置信度之间的差值)</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f9d01cd80067ddf195c2fab548b05453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*jPuLey84keuaq9gGZkckaA.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="072d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们希望选择在最可能的类别中可信度最低的实例。假设我们有两个图像A和B，模型的最高softmax logit分别是A的0.9和B的0.3。在这种情况下，我们可能希望了解B看起来是什么样子，处理，(重新)标记或删除它。</p><p id="995c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们使用为每个数据点计算的标准化“最小”不确定性分数(公式(1))。由于最有可能的实例的最低置信水平不能低于<em class="nb"> 1/n </em>，我们添加了标准化分数，以将不确定性分数从0调整到1。</p><p id="b36b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">该分数将给出预测的等级顺序，其中我们希望对预测标签置信度最低的项目进行采样。</p><p id="843a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种评分技术只取决于最有信心项的信心，而不关心第二个或第n个信心分。</p><ul class=""><li id="fb30" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated"><strong class="mc jd">置信区间</strong></li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/95474ad3819e2cc66f16684b01d77ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*72ZJCkQOkFvq0oZl2_wYjw.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="8682" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">边际置信度取一个数据点的两个最有把握的预测之间的差值。我们可以通过将结果减去1(公式(2))将该差值转换为[0–1]范围。这次你只关心两个最高类别中两个最高分的差。这个差值越小，越接近1，你就越不确定。与最小置信度得分相比，该算法对softmax基数不太敏感，但仍然敏感。</p><ul class=""><li id="c57b" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated"><strong class="mc jd">置信度</strong>(两个最有把握的预测之间的比率)</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4b3c44c16049d211c419f8392890ac80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*r0hu3bjpuGmk6q9lhzwSxA.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="4efa" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">比率评分函数是置信区间评分的变体。这个不确定性分数看两个最高分数的比率(3)。该比率作为用于softmax的基数的乘数给出，因此与前两种方法不同，它对softmax基数不敏感。它捕捉到第一个标签比第二个标签更有可能。</p><ul class=""><li id="9233" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated"><strong class="mc jd">基于熵的</strong>(所有预测之间的差异)</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/45f6c4fbbde1f43af79b56c11c84bbd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*8kyPzlWLKDN5K5nYZqVGNA.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="a34d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">熵分数考虑了所有类别的预测。它是通过将概率的负和乘以其log_2概率来计算的。</p><p id="9299" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们除以log_2(n)来归一化0和1之间的熵(等式(4))。</p><p id="7e8c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">2作为对数的基数是不改变排名的任意基数，即使它将单调地改变绝对不确定性分数。</p><h1 id="4509" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">实验设置</strong></h1><p id="0e3c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们举例说明两个评分函数的结果(<strong class="mc jd">最小</strong>和<strong class="mc jd">熵</strong>)。我们首先在一半的<strong class="mc jd"> EMNIST </strong>数据集(130，106张图片)上训练一个简单的两层CNN。然后，我们预测所有看不见的数据点的类别，并根据它们的不确定性得分对它们进行排序。我们对前1000幅图像进行采样，给出它们的不确定性分数。</p><p id="356a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们的CNN由两个卷积层组成，每个卷积层之后是RELU激活，然后是两个完全连接的层。在验证准确度开始降低之前，我们在第6个时期停止训练。我们在验证集上获得了0.79的F1分数。</p><p id="c84e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">您可以在以下位置找到培训代码:<a class="ae lh" href="https://github.com/bledem/active-learning/blob/master/run_experiment.py" rel="noopener ugc nofollow" target="_blank">https://github . com/ble DEM/active-learning/blob/master/run _ experiment . py</a></p><p id="effb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">以及用于采样前1000个图像作为新数据集，位于:<a class="ae lh" href="https://github.com/bledem/active-learning/blob/master/create_new_set.py" rel="noopener ugc nofollow" target="_blank">https://github . com/ble DEM/active-learning/blob/master/create _ new _ set . py</a></p><h2 id="965c" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">结果</h2><p id="d99e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们查看在验证集上每个类的平均F1分数最高和最低的类。</p><p id="f930" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">F1分数最低的类别(F1 &lt;50%) are:</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="0dee" class="nm lj it pd b gy ph pi l pj pk">'k' 'o' 'i' 'r' 'Y' 'q' 'b' 'l' 'h' 'e' 'm'</span></pre><p id="33a8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">and the classes with the highest F1 score (F1&gt; 95%)是:</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="2f05" class="nm lj it pd b gy ph pi l pj pk">'n' 'Z' 'A' 'a' '3'</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/18e9d4267e2cc96c914dece7b3aaad85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_OdllCnN9_tGHfU7ygO2w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每级F1-验证数据集中的分数。</p></figure><p id="5a32" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们来看看根据不确定性得分排名的前1k张图片。为了便于比较，我们还随机抽取了1k的样本。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/028183f162cb3b53a7a4553e2203c4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*sl3fbmzNytobd-BVHZp_gA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用“最少”评分技术从看不见的数据中分配1k个采样数据点。</p></figure><p id="9d60" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">从上图可以看出，我们对容易混淆的字符进行了过采样:“I”、“1”、“l”或“O”、“O”、“0”。</p><p id="0f45" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在下图中，我们将每个类的F1分数与使用熵不确定性评分函数采样的1k的不同类之间的实例分布进行了比较。我们在“容易混淆”的类中发现一个高峰，这在随机采样的数据集中不会发生。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/a54bd40bd396c4f26c3e9da02cddcf31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*tLfteiCSHHqTQ4zGBxwNmw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">验证数据集(蓝色)上的F1分数与“最少”技术得出的样本集中每个类的百分比的叠加。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/2cf0e9ab5bb52fb1d7c86c486e3c589f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*xJUiTzyhfKcebb2E--yANw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">验证集(蓝色)和随机采样的1k(橙色)上F1分数的叠加。</p></figure><p id="153a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们在下面展示了一些EMNIST图像作为前1k不确定推理的例子。当我们查看基于“最少”或“熵”的样本时，我们可以清楚地看到标签中的噪声(例如S标记为0)和图像本身(模糊、不可读的字符)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/0c1b17b672e2cf08212e15c5d51ed184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbKSBkHe8HFBe_DzcRrftg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">随机采样的图像(右)和模型置信度最低的图像(左)之间的比较。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/ec6c2042a266adc0396586d9146557d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jonIM3bU9stWVRGPDB0t3Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">给定“熵”不确定性分数的高不确定性图像(低模型置信度)的示例。</p></figure><h1 id="9429" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">在实践中使用不确定性评分</h1><p id="104a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">使用不确定性分数对新标注任务的数据批次进行采样是标记未知数据集中最具信息量的数据点的有效方法。然而，正如我们所看到的，我们应该注意不要系统地对过度表示的类及其近邻中的相似数据点进行过采样。</p><p id="c953" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们在这里列出了使用上面介绍的评分方法的利弊:</p><p id="3bcc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">优点:</strong></p><ul class=""><li id="525c" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated">使用现有/标准模型</li><li id="43dd" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">快速且易于计算和解释。</li><li id="0b94" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">如果您的模型使用传统的机器学习(而不是深度学习)，请创建一批新的信息数据样本进行标记</li></ul><p id="8ca4" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">缺点:</strong></p><ul class=""><li id="880d" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated">这些简单的评分函数不应用于为深度学习的新批量任务选择数据点。由于潜在的采样偏差，在对这些采样数据进行训练后，模型的性能可能比随机采样差[5]。</li><li id="e344" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">使用贝叶斯深度学习创建高维数据(图像、文本等)的采样策略更安全。).</li></ul></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><h2 id="847a" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">复制不确定样品的注意事项</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4c0fa401065974e330d76304a5db7db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*juQcfbnBH4BaR5mW"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="https://unsplash.com/@ahsanjaya" rel="noopener ugc nofollow" target="_blank">穆罕默德·道迪</a>拍摄</p></figure><p id="2a3b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用不确定性量化方法可以非常有效地在未标记的数据中找到位于模型边界上的样本。但是，在为下一批注释选择要采样的数据点时要小心。该抽样单独评估每个未标记点的信息量。如果您只选择最高不确定性得分数据点，您将检索到一个信息数据点和许多几乎相同的拷贝。如果你天真地获得了前k个最有信息的点，你可能最终也会问k个几乎相同的点。这将使您的样本批次的贡献低于随机抽样批次。例如，在EMNIST实验中，如果你观察熵采样技术，你可以看到字符“I”，“1”，“l”被大量采样。</p><p id="64d7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了避免这种重复采样效应，一些研究论文提出了补充方法，如BatchBALD等批量感知采样技术解决方案[2]。在本文中，作者不仅关注每个样本的不确定性得分，还关注为下一批训练样本提供最丰富的信息。</p><p id="a958" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用wise采样构建注释的智能批处理[1]:</p><ul class=""><li id="193e" class="oe of it mc b md mw mg mx mj ov mn ow mr ox mv oj ok ol om bi translated">最小化样本量，以确保从每个数据点获得最大收益。</li><li id="003f" class="oe of it mc b md on mg oo mj op mn oq mr or mv oj ok ol om bi translated">最大化样本量，以避免过于频繁的重新训练。</li></ul><h1 id="e049" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">关于智能采样的更多信息</strong></h1><h2 id="cc2e" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">1)异议评分:</h2><p id="6d04" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">评估每个未标记数据点的不确定性的另一种流行的启发式方法被称为委员会的<strong class="mc jd">查询</strong>方法。如果您有一组模型，您也可以通过获取每个模型的每个数据点的所有分数来调整不确定性评分。您在初始训练集上训练几个不同的模型，您可以查看它们在未标记数据上的分歧。</p><p id="d51d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">例如，我可以在相同的训练集上用不同的超参数训练相同的CNN，然后将它们应用于未标记的数据，并使用它们的不一致来选择下一个要标记的样本。</p><p id="71bd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当您想要评估预测中softmax置信度不可用的回归任务的不确定性分数时，此方法特别有用。</p><p id="397e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">辍学策略</strong></p><p id="0274" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你没有一个集合模型，你可以为每个数据点画出不同的预测，其中每个新的预测是在每次丢弃不同的随机选择的神经元之后做出的。正常情况下，退出仅在训练期间应用，所有神经元在测试期间都被激活。但是如果我们在测试中也应用dropout，我们可以为每个实例推断出一系列不同的预测。每个样本的不确定性可以计算为所有预测的变化:不一致程度越高，不确定性就越大[1]。这种技术通常被称为<em class="nb">蒙特卡洛退出</em>技术。蒙特卡洛是一种计算技术，它从真实分布中随机取样，以获得该分布的估计值。在不确定性估计过程中，我们将来自漏失模型的每个预测视为蒙特卡罗样本。</p><p id="08b6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">面向MC辍学和其他主动学习技术的开源库:<a class="ae lh" href="https://github.com/ElementAI/baal/" rel="noopener ugc nofollow" target="_blank">https://github.com/ElementAI/baal/</a><a class="ae lh" href="https://arxiv.org/pdf/2006.09916.pdf" rel="noopener ugc nofollow" target="_blank">【论文】</a></p><h2 id="0e6f" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">2)贝叶斯模型</h2><p id="23ee" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">您可以使用生成贝叶斯模型对每个类的分布进行建模。在这种情况下，您不需要使用概率代理来推断模型预测的可信度，因为您可以从模型输出本身直接读取每个数据点属于每个类的概率。</p><p id="acd0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">您可以使用贝叶斯神经网络来估计神经网络的不确定性。在贝叶斯神经网络框架中，在训练期间，你不尝试学习单个函数f，而是学习类似于集成模型的多个函数。训练之后，我们可以对每个数据点进行平均预测，并计算这些预测之间的方差。可以为从回归到分割的各种任务建立贝叶斯神经网络。它将模型参数视为正态分布以及每个模型参数分布的均值和方差。这些参数通常在训练过程中学习。</p><h2 id="feed" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">3)主动学习</h2><p id="d084" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我一直专注于给每个未标记的样本分配一个不确定性分数。但是，不确定性量化只占更广泛的主动学习领域的极小一部分。AL致力于通过标记尽可能少的样本来获得尽可能多的性能增益[3]。理论上，理想的系统会查询位于模型的决策边界上的实例和远离决策边界的实例(离群值、多样性采样、要定义的新类)。不幸的是，还没有一种标准的方法来构建最佳的管道来查询最小的数据样本以获得最高的性能。</p><h2 id="0d74" class="nm lj it bd lk nn nx dn lo np ny dp ls mj nz ns lu mn oa nu lw mr ob nw ly iz bi translated">4)课程学习</h2><p id="719e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">另一个非常有趣的研究领域试图通过向模型提供适当的数据点来优化学习。它不是关注未标记的数据，而是关注已经标记的数据。学习任务需要一些时间来为模型提供简单的样本，并在训练过程中一点一点地增加样本的难度，类似于人类。对于不平衡的数据集，相反的策略工作得很好:我们为硬(罕见)实例分配更高的惩罚，以迫使模型学习它们。在实践中，这种先易后难的学习策略是由损失函数的选择来设定的。我鼓励你阅读这篇文章[4]来学习更多关于标记数据中的抽样技术。</p><h1 id="bd99" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">本文的代码</h1><p id="36bc" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">https://github.com/bledem/active-learning<a class="ae lh" href="https://github.com/bledem/active-learning" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="40fa" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">参考</h1><p id="6035" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">[1] <em class="nb">人在回路机器学习:以人为中心的人工智能的主动学习和注释</em>，Robert Monarch。</p><p id="a83f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[2]巴特包勒德:<a class="ae lh" href="https://oatml.cs.ox.ac.uk/blog/2019/06/24/batchbald.html" rel="noopener ugc nofollow" target="_blank">https://oatml.cs.ox.ac.uk/blog/2019/06/24/batchbald.html</a></p><p id="fe2c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">【3】<em class="nb">深度主动学习的调查</em>。、任&amp;肖、昀&amp;常、肖军&amp;黄、王宝耀&amp;李、陈志辉&amp;陈、肖江&amp;王、辛。(2020).<a class="ae lh" href="https://arxiv.org/pdf/2009.00236.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2009.00236.pdf</a></p><p id="119e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[4] <em class="nb">先学哪些样本:容易还是难？</em><a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou%2C+X" rel="noopener ugc nofollow" target="_blank"/>，<a class="ae lh" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+O" rel="noopener ugc nofollow" target="_blank">欧武</a>，2021。<a class="ae lh" href="https://openreview.net/pdf?id=pSbqyZRKzbw" rel="noopener ugc nofollow" target="_blank">https://openreview.net/pdf?id=pSbqyZRKzbw</a></p><p id="6782" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[5] <em class="nb">通过期望改进最大化在CNN中主动学习。纳格帕尔，乌代。(2020).<a class="ae lh" href="https://openaccess.thecvf.com/content_WACV_2020/papers/Mayer_Adversarial_Sampling_for_Active_Learning_WACV_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ WACV _ 2020/papers/Mayer _ Adversarial _ Sampling _ for _ Active _ Learning _ WACV _ 2020 _ paper . pdf</a></em></p><p id="26f0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[6]<a class="ae lh" href="https://www.youtube.com/watch?v=Yqj7Kyjznh4&amp;t=1260s&amp;ab_channel=DeepLearningAI" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Yqj7Kyjznh4&amp;t = 1260s&amp;ab _ channel = deep learning ai</a></p><p id="9be0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[7]<a class="ae lh" href="https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/" rel="noopener ugc nofollow" target="_blank">https://venturebeat . com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/</a></p></div></div>    
</body>
</html>