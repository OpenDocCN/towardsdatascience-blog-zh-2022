<html>
<head>
<title>Implementing Support Vector Machine From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-svm-from-scratch-784e4ad0bc6a#2022-01-24">https://towardsdatascience.com/implementing-svm-from-scratch-784e4ad0bc6a#2022-01-24</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="b0f2" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">从头做起</h2><div class=""><h1 id="f3c0" class="pw-post-title jc jd iu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">从头开始实现支持向量机</h1></div><div class=""><h2 id="4080" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">通过从头推导理解具有梯度下降和铰链损失的最大间隔分类器</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/93c7b35716356752ee777241ec9e024d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*67IA2OsmsNXtckdF"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">阿曼德·库利在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ac08" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi mf translated">当我决定学习一种机器学习算法时，我总是想知道它是如何工作的。</p><blockquote class="mo"><p id="81d1" class="mp mq iu bd mr ms mt mu mv mw mx me dk translated">我想知道引擎盖下是什么。想知道是怎么实施的？我想知道它为什么有效。</p></blockquote><p id="38d4" class="pw-post-body-paragraph lj lk iu ll b lm my ke lo lp mz kh lr ls na lu lv lw nb ly lz ma nc mc md me in bi translated">从头开始实现机器学习算法迫使我们寻找所有这些问题的答案——这正是我们在本文中试图做的。</p><p id="88b6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在接下来的小节中，我们将使用Python和NumPy一步步实现<strong class="ll je">支持向量机</strong> <em class="nd"> </em>。我们还将学习基本的数学原理，铰链损失函数，以及如何应用梯度下降。</p><p id="c01b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是在深入实现细节之前，让我们建立一些关于支持向量机的基本直觉。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="2569" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">支持向量机</h1><p id="c369" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">由计算机科学界在20世纪90年代开发的支持向量机(SVM)是一种常用的监督学习算法，最初用于二进制分类设置。</p><blockquote class="mo"><p id="d76d" class="mp mq iu bd mr ms mt mu mv mw mx me dk translated">它通常被认为是最好的“开箱即用”分类器之一。</p></blockquote><p id="b424" class="pw-post-body-paragraph lj lk iu ll b lm my ke lo lp mz kh lr ls na lu lv lw nb ly lz ma nc mc md me in bi translated">SVM是一种简单而优雅的算法，称为最大间隔分类器的推广。然而，这种分类器不能应用于所有情况，因为它严重依赖于数据集是线性可分的假设，因此，存在几种扩展。</p><blockquote class="oi oj ok"><p id="6dc1" class="lj lk nd ll b lm ln ke lo lp lq kh lr ol lt lu lv om lx ly lz on mb mc md me in bi translated"><strong class="ll je">注意:</strong>在下文中，我们将只讨论最大间隔分类器，有意避免不同的扩展。这使我们能够对基本原理和学习算法的主要目标有更深的理解。</p></blockquote><p id="89ee" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们假设，我们有一个数据集，其中每个数据点属于两个类中的一个。假设数据集是线性可分的——最大间隔分类器试图找到<strong class="ll je">最佳分离超平面</strong>，顾名思义——最大化间隔。</p><p id="0b9d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">换句话说，我们只是通过最大化到任一类的最近数据点的距离来寻找分隔两类的决策边界。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/3fb8cccf932cfc41ed012368580bfd6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*Anqz5lIxp0X7rzL6_7tpqg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">最大间隔分类器对超平面的可视化[图片由作者提供]</p></figure><p id="7442" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，一个基本问题浮现在脑海中——我们如何找到最佳超平面？</p><p id="fa43" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在回答这个问题之前，我们需要做一些基础工作。更准确地说，我们需要知道什么是超平面，它是如何定义的，以及如何通过计算和最大化边际来选择最优解。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="6154" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">了解超平面</h1><p id="a04b" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">一般来说，<em class="nd"> p </em>维的超平面可以描述为一个维数为<em class="nd"> p-1 </em>的平坦仿射子空间。</p><p id="93a9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">例如，在二维空间中，超平面可以由平坦的一维子空间来定义，或者换句话说，可以由一条简单的直线来定义。类似地，在三维中，超平面可以描述为二维子空间或平面。</p><p id="a813" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，假设我们有一个二维空间的数据集，可以线性分离。因此，将数据点按其类别{-1，1}划分的超平面<em class="nd"> (H₀) </em>是一条直线。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/061a5a0e945d6061e15fdfab12ab3f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*cNFo1F6VwTNvfY4cxnxiXw.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">一个分离超平面的例子[图片由作者提供]</p></figure><p id="d1c6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">更正式地说，超平面由满足以下等式的点集<em class="nd"> x </em>定义:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj op"><img src="../Images/43d966c30635ab1c9356ef5efce740d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/0*q5ymv6JkZPnE0iCt"/></div></figure><p id="113e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这个定义特别有用，因为它允许我们把超平面看作一个决策边界。不满足上述等式的任何点<em class="nd"> x </em>，必定位于超平面的一侧或另一侧。</p><p id="6419" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">重新构建之前的情况，我们也可以认为这是选择另外两个超平面<em class="nd"> (H₁，H₂) </em>，它们与<em class="nd"> H₀、</em>的距离相同，确保每个数据点都位于正确的一侧，中间没有数据。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/17d95de850e90cc42dbb322661a486ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*13OzQUcoMdyJkQNIWbcvdA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">H2 H1等距超平面的一个例子</p></figure><p id="85c9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">超平面<em class="nd"> (H₁，H₂) </em>因此可以定义为</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/5488b4b935eed3ed616e59f715ed3728.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/0*yTNq0IPmIIJfE2nV"/></div></figure><p id="fcd8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以将它组合成一个用于分离超平面的<strong class="ll je">单一方程:</strong></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/00f5262eb4a337394867eccf7764bb5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/0*WlX0kCTy5S-QYv47"/></div></figure><p id="0315" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们知道分离超平面是如何定义的，但如何找到最优解的问题仍然没有答案。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="6da9" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">寻找最优分离超平面</h1><p id="65ec" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">由于我们的数据集可以使用超平面完美地分离，并且任何超平面都可以通过微小的推动来移动和旋转，因此我们可以从无限量的可能解决方案中进行选择。</p><p id="82c7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">因此，我们需要一种合理的方法来决定使用哪个可能的超平面。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj or"><img src="../Images/afb9284c2aa57643ea09a8143a29ecc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*2Q870UIoZIM8zuwd4Vbqjg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">可能的分离超平面的例子[图片由作者提供]</p></figure><p id="aa89" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">不严格地说，<strong class="ll je">最优分离超平面</strong>是离最近数据点最远的解——或者换句话说，是<strong class="ll je">最大化了余量</strong>。</p><p id="49f6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们也可以把它想象成另外两个超平面<em class="nd">(支持向量)</em>，它们之间的距离最大化。最佳分离超平面可以想象成一条将边缘分成两半的平行线。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/2b0e143c260bbff9868748afd09d8f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*d36R6LJu_FxS5Em6XTtrTg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">最大化边距[图片由作者提供]</p></figure><p id="4066" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了最大化两个超平面之间的距离，我们需要找到一种计算边距的方法。</p><p id="6e0a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">边缘也可以解释为垂直于任何超平面的向量，其大小等于边缘。因此，我们只需要定义这个向量，然后求解它的大小。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj os"><img src="../Images/52c0bea87583ceb06b635d9a3f0545da.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*g4LaFAzVyxzQUpzeXuAcjA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">计算边距[图片由作者提供]</p></figure><p id="c3fe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于我们的权重向量<em class="nd"> w </em>已经垂直于超平面，我们可以用下面的等式计算与<em class="nd"> w </em>方向相同的单位向量<em class="nd"> u </em>:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ot"><img src="../Images/347697fa326b577ed4f81b4393d9e8e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/0*TuqomDUDztMARVGO"/></div></figure><p id="4228" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们只需要将单位向量<em class="nd"> u </em>缩放一个标量<em class="nd"> m </em>就可以得到一个方向和大小与边距相同的向量<em class="nd"> k </em>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ou"><img src="../Images/443415fa0d312f6d8c481ad5a2de9fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/0*Sb4ZA3dxLm2Voa4p"/></div></figure><p id="365c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们想象一个任意的点<em class="nd"> x₀ </em>并加上我们的向量<em class="nd"> k </em>，得到<em class="nd"> z₀ = x₀ + k </em>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ov"><img src="../Images/bd92818329a14770ef8779769e0d4c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*2YfkuIfcVXQtnPcNsM-rCg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">导出边距[图片由作者提供]</p></figure><p id="d6a6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于<em class="nd"> z₀ </em>和<em class="nd"> x₀ </em>分别位于超平面<em class="nd"> H₁ </em>和<em class="nd"> H₂ </em>上，我们可以如下求解余量:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ow"><img src="../Images/7c368075baf40a28636bcce2fbe4ae45.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/0*S66mRFbgM6pvRlte"/></div></figure><p id="6408" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">通过仔细研究我们的最终等式(8)，我们可以看到，为了最大化裕量，我们只需最小化分母——向量<em class="nd"> w </em>的范数。</p><p id="e858" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">就是这样，我们最终收集了计算最优分离超平面所需的所有成分。剩下唯一要做的就是把所有的成分组合成一个单一的目标函数。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="adcb" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">主要目标函数</h1><p id="f96d" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">有了如何定义分离超平面以及如何计算两个超平面之间的距离的知识，我们可以导出主目标函数。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ox"><img src="../Images/7f92f5c462e7f4293f6f651d42cdbadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/0*M_j5Giv9Whw3ToPc"/></div></div></figure><p id="25d9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">到目前为止，这个等式的某些部分对我们来说应该很熟悉。</p><p id="f066" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第一项基本上负责最大化裕度，表示为具有附加正则化参数λ的最小化问题。</p><blockquote class="oi oj ok"><p id="02e6" class="lj lk nd ll b lm ln ke lo lp lq kh lr ol lt lu lv om lx ly lz on mb mc md me in bi translated"><strong class="ll je">注</strong>:当导出梯度时，简单地增加乘以1/2是为了方便。</p></blockquote><p id="a01e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第二项——包含我们对分离超平面的定义——是一个称为<a class="ae li" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">铰链损失</a>的损失函数。不严格地说，这个术语负责确保我们以足够的余量预测正确的类标签。</p><p id="0979" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">例如，如果<em class="nd"> yᵢ = 1 </em>和<em class="nd"> xᵢ </em>被正确分类，则计算铰链损耗的结果为零，因为<em class="nd"> max(0，1–1)= 0</em>。但是，如果错误地预测了类别标签，则铰链损耗将导致大于零的值。</p><p id="d21e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于我们要用<a class="ae li" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>优化损失函数，我们需要计算函数相对于权重和偏差的偏导数。一旦我们获得梯度，我们可以简单地在相反的方向采取小步骤，以尽量减少损失。</p><p id="998f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了获得<em class="nd">(子)</em>梯度，我们必须区分两种情况下的目标函数:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oy"><img src="../Images/d079570ace236852a3993bd55d4eef63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*QvmKW0sl_1A_bQ9n"/></div></figure><p id="6ad5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对第一种情况取偏导数，得到以下梯度:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/cf16baef4c8f51d4a2bc80da31aa8fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/0*7hFsLzeYVICByRXr"/></div></figure><p id="5428" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对第二种情况重复相同的步骤得到:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pa"><img src="../Images/7f792a45b4691a3d6b898fe48629175e.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*ZTtoHdezSBaf5P_9"/></div></figure><p id="99f6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们应该把梯度和导数放在手边，因为它们在实现梯度下降和下一节的主要算法时会派上用场。</p></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="1581" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">介绍算法</h1><p id="c838" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">在直接进入实现细节之前，我们将快速看一下算法的主要计算步骤，以提供一个高层次的概述以及一些基本结构。</p><p id="b577" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">主算法</strong>基本上可以分解为4个步骤:</p><ol class=""><li id="e1e6" class="pb pc iu ll b lm ln lp lq ls pd lw pe ma pf me pg ph pi pj bi translated">权重和偏差的基本设置和初始化</li><li id="08e3" class="pb pc iu ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">将类别标签从{0，1}映射到{-1，1}</li><li id="1542" class="pb pc iu ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">执行<em class="nd"> n次</em>迭代的梯度下降，这涉及梯度的计算并相应地更新权重和偏差。</li><li id="1c27" class="pb pc iu ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">做最后的预测</li></ol><p id="2c1c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于第三步由多个动作组成，我们将把它分解成几个辅助函数。</p><p id="112b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">该算法将用Python和Numpy在一个类中实现。下面，我们可以看一下<em class="nd">骨架类</em>，它可以解释为某种蓝图，指导我们完成下一节的实现。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="1891" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">从头开始实施</h1><h2 id="3e5a" class="pr nm iu bd nn ps pt dn nr pu pv dp nv ls pw px nx lw py pz nz ma qa qb ob ja bi translated">基本设置</h2><p id="ad40" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">我们从一些简单的内务开始，通过<strong class="ll je">初始化参数</strong>(学习率，迭代次数)并定义用于初始化权重和偏差的辅助函数。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure><h2 id="7730" class="pr nm iu bd nn ps pt dn nr pu pv dp nv ls pw px nx lw py pz nz ma qa qb ob ja bi translated">主算法</h2><p id="8b17" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">基本设置完成后，我们现在能够实现算法的核心计算步骤。</p><p id="1d06" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">首先，我们需要<strong class="ll je">初始化权重和偏差</strong>并<strong class="ll je">映射</strong>我们的二进制<strong class="ll je">类标签</strong>从{0，1}到{-1，1}，这使我们能够检查数据点是否位于分离超平面的正确一侧。</p><p id="971a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">可以借助内置的NumPy函数<code class="fe qc qd qe qf b">np.where()</code>来执行类映射，该函数简单地将0的每个类标签变成-1。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure><p id="f4b2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，继续进行<strong class="ll je">梯度下降</strong>——在单次迭代中，我们必须检查是否满足分离超平面的约束，计算权重和偏差的梯度，并相应地更新参数。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure><p id="539a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了计算正确的梯度，我们需要知道目标函数的哪种情况与特定的数据点相关。因此，我们需要检查数据点<strong class="ll je">是否满足约束</strong></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qg"><img src="../Images/08002397b17e484c400628db45fed94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/0*APe3Srb0p1Y-Lqrk"/></div></figure><p id="6664" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">一旦我们知道是否满足约束，我们就可以相应地<strong class="ll je">计算梯度</strong>。我们应该能够识别出前面部分的导数。一个简单的<em class="nd"> if语句</em>完成了决定计算哪组渐变的工作。</p><p id="7b5b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">有了手中的梯度，我们只需再做一件事——我们必须<strong class="ll je">更新权重和偏差</strong>，允许我们向相反的方向迈出一小步，使损失函数最小化。</p><h2 id="a326" class="pr nm iu bd nn ps pt dn nr pu pv dp nv ls pw px nx lw py pz nz ma qa qb ob ja bi translated">预测并把它们放在一起</h2><p id="efd6" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">进行预测非常简单，因为它只需要我们计算训练数据和权重的点积，并在顶部添加偏差。</p><p id="0a5a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们还计算类标签的符号，并在返回预测标签之前，将它们从{-1，1}映射回原始值{0，1}。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure><p id="aacf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">到目前为止，我们基本上完成了所有的艰苦工作——我们只需要将所有的东西放在一起，完成我们的实现。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="2fdb" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">测试分类器</h1><p id="acc4" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">我们已经完成了最大间隔分类器的实现，我们仍然需要测试它。</p><p id="e1f6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">出于测试目的，我们将简单地使用<code class="fe qc qd qe qf b">sklearn.datasets.make_blobs()</code>并创建一个包含250个样本的基本数据集，其中只有两个特征。我们可以将数据可视化如下:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qh"><img src="../Images/0ca49c741fee8c9abf8732dded4ff954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vHnfCxUMYAEvN0UZfjHCw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">简单数据集的可视化[图片由作者提供]</p></figure><p id="9641" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们可以分割数据集，实例化并拟合我们的分类器来进行预测。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="pp pq l"/></div></figure><p id="9792" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">将预测的类别标签与真实的类别标签进行比较，得到100 %的准确度分数。我们还可以通过绘制超平面来可视化结果，这验证了我们的算法是正确工作的。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qh"><img src="../Images/02c84b8c26fdd78ebbfa732b8e69c762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcrYsUIL7g7dcj0D1Vm5NQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">结果超平面的可视化[图片由作者提供]</p></figure></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><h1 id="dc39" class="nl nm iu bd nn no np nq nr ns nt nu nv kj nw kk nx km ny kn nz kp oa kq ob oc bi translated">结论</h1><p id="da1b" class="pw-post-body-paragraph lj lk iu ll b lm od ke lo lp oe kh lr ls of lu lv lw og ly lz ma oh mc md me in bi translated">在本文中，我们以循序渐进的方式从头开始实现了最大间隔分类器。我们还学习了基本的数学概念，如何实现铰链损失函数，以及如何应用梯度下降。</p><p id="f820" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，最大间隔分类器仅覆盖数据集是线性可分的简单情况。因此，存在几个扩展，人们可以并且可能应该更深入地探索。</p><p id="e7d3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">从零开始实现最大间隔分类器使我们能够对基本的和潜在的概念建立更深的理解。当探索一些建立在最大间隔分类器之上的更复杂的扩展时，这些知识将被证明是有用的。</p><p id="76ef" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">你可以在我的GitHub上的这里找到<a class="ae li" href="https://github.com/marvinlanhenke/DataScience/tree/main/MachineLearningFromScratch" rel="noopener ugc nofollow" target="_blank">的完整代码。</a></p><div class="qi qj gq gs qk"><div role="button" tabindex="0" class="ab bv gw cb fq ql qm bn qn lc ex"><div class="qo l"><div class="ab q"><div class="l di"><img alt="Marvin Lanhenke" class="l de bw qp qq fe" src="../Images/5b1b337a332bf18381aa650edc2190bd.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*iEJ9qn9i9dTw3uyXG2jsTA.jpeg"/><div class="fb bw l qp qq fc n aw fd"/></div><div class="hi l fp"><p class="bd b dl z fq fr fs ft fu fv fw fx dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://medium.com/@marvinlanhenke?source=post_page-----784e4ad0bc6a--------------------------------" rel="noopener follow" target="_top">马文·兰亨克</a></p></div></div><div class="qt qu gx l"><h2 class="bd je wl pb fq wm fs ft rt fv fx jd bi translated">从零开始的ML算法</h2></div><div class="ab q"><div class="l fp"><a class="bd b be z bi wn au wo wp wq td wr an eh ei ws wt wu el em eo de bk ep" href="https://medium.com/@marvinlanhenke/list/ml-algorithms-from-scratch-7621d01922ad?source=post_page-----784e4ad0bc6a--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l fp"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="rg dh rh fq ab ri fp di"><div class="di qy bv qz ra"><div class="dh l"><img alt="" class="dh" src="../Images/230471643f4baf6e91fca0422ddfb56b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qT41zaPnyJHQeGKz0FvkQA.png"/></div></div><div class="di qy bv rb rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/ca1fa6207e26d895ef8ba30bb66ef3a2.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*67IA2OsmsNXtckdF"/></div></div><div class="di bv re rf rd"><div class="dh l"><img alt="" class="dh" src="../Images/33c295e3ac2a12346fba6df057ae448b.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HXXyDFWGhV7pvo8E"/></div></div></div></div></div></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><p id="a2f4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="nd">喜欢这篇文章吗？成为</em> <a class="ae li" href="https://medium.com/@marvinlanhenke/membership" rel="noopener"> <em class="nd">中等会员</em> </a> <em class="nd">继续无限学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="qi qj gq gs qk rn"><a href="https://medium.com/@marvinlanhenke/membership" rel="noopener follow" target="_blank"><div class="ro ab fp"><div class="rp ab rq cl cj rr"><h2 class="bd je gz z fq rs fs ft rt fv fx jd bi translated">通过我的推荐链接加入Medium-Marvin Lanhenke</h2><div class="ru l"><h3 class="bd b gz z fq rs fs ft rt fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="rv l"><p class="bd b dl z fq rs fs ft rt fv fx dk translated">medium.com</p></div></div><div class="rw l"><div class="rx l ry rz sa rw sb lc rn"/></div></div></a></div></div><div class="ab cl ne nf hy ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="in io ip iq ir"><p id="fe8a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">参考资料/更多资料:</strong></p><ul class=""><li id="ec38" class="pb pc iu ll b lm ln lp lq ls pd lw pe ma pf me sc ph pi pj bi translated"><a class="ae li" href="https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/" rel="noopener ugc nofollow" target="_blank"> SVM教程</a></li><li id="cef7" class="pb pc iu ll b lm pk lp pl ls pm lw pn ma po me sc ph pi pj bi translated">加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼。《统计学习导论:在r .纽约的应用》: Springer，2013年。</li></ul></div></div>    
</body>
</html>