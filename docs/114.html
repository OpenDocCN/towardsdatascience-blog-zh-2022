<html>
<head>
<title>Bayesian method (1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯方法(1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-method-1-1cbdb1e6b4#2022-01-05">https://towardsdatascience.com/bayesian-method-1-1cbdb1e6b4#2022-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="b541" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">贝叶斯方法(1)</h1></div><div class=""><h2 id="353e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">先验分布</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/36099223984e5166cd97b5633b09f433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*w-T0ct36uGxw97TOEw_D4Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图0.1不同参数下的贝塔分布。(图片由作者提供)</p></figure><p id="29ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">很容易找到大量介绍贝叶斯统计的好文章。然而，它们中的大多数只介绍了什么是贝叶斯统计以及贝叶斯推理是如何工作的，而没有涉及许多数学细节。此外，这也是一个充满乐趣和挑战的探索领域。</p><p id="6b16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我打算写一系列文章来分享更多关于贝叶斯统计的理论，包括先验的选择，贝叶斯推断中的损失函数以及贝叶斯统计和一些频率主义方法之间的关系。本帖将介绍贝叶斯统计中使用的<strong class="kw iu">先验分布</strong>。为什么我们需要学习这个？因为选择先验分布是我们需要使用贝叶斯推理的第一步。更多地了解它们有助于选择。</p><h1 id="9207" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">基础知识</h1><p id="74d5" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">在这里，我们从贝叶斯统计如何工作的简要概述开始，并且在这里也介绍了我们稍后将使用的一些符号。在贝叶斯统计中，我们假设一个先验概率分布，然后使用我们拥有的数据更新先验。这种更新给出了后验概率分布。我们将后验概率表示为<em class="mn">π(θ|</em><strong class="kw iu">T5】x</strong><em class="mn">)</em>(<em class="mn">π</em>可能看起来很烦人，因为它也是数学中很常见的常数，但在这种情况下，π提醒我们该分布与总体分布的参数有关)，其计算方法如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/1e95946c946f1ca480e1ed8e2f74664f.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*s1yo_e1Quf38c6q60DnKSA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1.1计算后验概率的公式</p></figure><p id="d8f8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中，θ是所有可能的参数值的空间(这里的“空间”，我们指的是“样本空间”)，并且<em class="mn">π(</em><strong class="kw iu"><em class="mn">x</em></strong><em class="mn">|θ)</em>是<a class="ae mp" href="https://en.wikipedia.org/wiki/Likelihood_function#:~:text=is%20the%20likelihood%20function%2C%20given%20the" rel="noopener ugc nofollow" target="_blank">可能性</a>——观察到给定真实参数值为<em class="mn"> θ </em>，输出<strong class="kw iu"> <em class="mn"> x </em> </strong>的条件概率。由于<em class="mn">θ</em>∈θ是与<em class="mn">先验分布</em> n相关的参数，而不是总体的分布，我们可以<em class="mn"> θ </em>超参数<strong class="kw iu"/><em class="mn">。</em>和往常一样，我们使用粗体(<strong class="kw iu"> <em class="mn"> x </em> </strong>)来表示向量。分母又称为<em class="mn">证据，</em>是使后验概率<em class="mn">π(θ|</em><strong class="kw iu"><em class="mn">x</em></strong><em class="mn">)</em>成为概率分布(总和为1)的归一化因子(常数)。这很容易验证</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0aa6a20f540a509f004f03092fa48fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*wN6_O1UePbqLTIJEzHYMEg.png"/></div></figure><p id="bf02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在推理中可以忽略归一化因子，我们可以在一些文献中看到这一点，如[1]，因为省略常数不会改变曲线的形状。</p><p id="fba5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么先验概率可以写成这样的形式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4b48a0ed0ef496f34eacc0fdafec5e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*Ae3aENOyFZgmyXswRBdxIQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1.2不带分母的后验概率</p></figure><p id="6feb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们有了后验分布，也就是参数的分布(记住这一点)，我们就可以计算<strong class="kw iu">预测分布</strong>。是条件概率，是观察<em class="mn"> y </em>，给定数据<strong class="kw iu">T5】x</strong>的概率分布。其计算方法如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/d97bd09721a9b626d7498ac80ff5f9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*xVsFmEkQi_vZpBxEX2xY-w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1.3预测分布</p></figure><p id="4b26" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中红色部分是新观测值的概率密度函数，给定参数<em class="mn"> θ </em>。等式1.3一开始可能看起来有点混乱，但仔细观察后，我们可以看到它实际上是使用总概率的<a class="ae mp" href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener ugc nofollow" target="_blank">定律计算的(就像加权平均一样简单)，它是给定参数值<em class="mn"> θ </em>和参数取值<em class="mn"> θ </em>给定数据<strong class="kw iu"> <em class="mn"> x </em> </strong>的概率分布的乘积的积分</a></p><h1 id="20c7" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">选择先验</h1><p id="0c80" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">先验有时被描述为关于数据的“信念”。[2]这意味着我们根据我们对数据的了解来选择先验。当然，这并不像“信念”这个词所暗示的那样完全是一个主观的问题。</p><h2 id="5f71" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">先验的性质</h2><p id="41e2" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">请注意，参数的分布可以是无界的，这意味着它的概率密度是非负的，但它们的总和或积分是无限的。我们称参数<strong class="kw iu">的这种分布为不恰当的先验分布</strong>。</p><p id="835c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">根据维基百科的说法，一个信息性的先验表达了关于一个变量的具体、明确的信息。通常，无论如何都要避免，但是如果先验信息是可用的，信息先验是将信息引入模型的合适方式。[7]</p><p id="9f40" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当我们不太了解我们的数据和参数的分布时，选择所谓的“模糊先验”是有意义的，它反映了最少的知识。所以我们需要一个没有人口基数的先验分布，这就很难构造，对后验分布的作用微乎其微。这样的先验密度称为<strong class="kw iu">无信息</strong>先验，或<em class="mn">扩散</em>先验。而<a class="ae mp" href="https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/" rel="noopener ugc nofollow" target="_blank">有些人</a>宁愿认为先验分布<strong class="kw iu">总是</strong>包含一些信息。有时不适当的先验被用来代表这种模糊的先验。稍后我们将看到这样的例子。</p><p id="2841" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一个相关术语是<strong class="kw iu">弱信息</strong>先验，它包含部分信息，这意味着它足以给出后验分布的合理界限，但不能完全捕捉一个人关于参数的科学知识。[3]</p><p id="74b5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">先验的一个非常有趣的性质是<strong class="kw iu">共轭</strong>，这意味着后验分布与先验分布具有相同的参数形式。我们可以看到，这种先验与后验密切相关，因此我们说它包含了<em class="mn">强先验知识</em>。[5]共轭先验的好处是显而易见的——后验分布将是一个已知的分布。</p><h2 id="4138" class="mt lr it bd ls mu mv dn lw mw mx dp ma ld my mz mc lh na nb me ll nc nd mg ne bi translated">一些常见前科的例子</h2><ol class=""><li id="49f1" class="nf ng it kw b kx mi la mj ld nh lh ni ll nj lp nk nl nm nn bi translated"><em class="mn">制服在先</em></li></ol><p id="9536" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果参数值是有界的，最直观和最容易的先验是<strong class="kw iu">均匀的</strong>先验分布。该先验是非信息性的(有时也称为“低信息先验”[2])，它假设参数空间θ中的所有参数都是同等可能的。比如我们要用伯努利分布对数据进行建模(就像著名的例子——抛硬币)，参数<em class="mn"> p </em>是一个概率，落在区间<em class="mn">【0，1】</em>内。在这种情况下，先验概率变为<em class="mn"> π(θ) = 1 </em>，对于<em class="mn">【0，1】</em>中的<em class="mn"> θ </em>。</p><p id="a189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">2.<em class="mn">霍尔丹在先</em></p><p id="4344" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最少的知识不一定意味着所有的参数都是同样可能的。许多其他无信息的前科也是可能的。无信息先验的另一个例子是J. B. S .霍尔丹提出的用于估计罕见事件的<strong class="kw iu">霍尔丹</strong>先验。霍尔丹先验实际上是参数<em class="mn"> α=0，β=0的贝塔分布。</em>因此霍尔丹的先验是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f4d854c2c5b931b22cb2e5395f4fbaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*zIhS1PWSNMVQiJciHBrEng.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">情商。2.1霍尔丹先验</p></figure><p id="9022" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="mn"> B(α，β) </em>为贝塔函数。提醒一下，Beta函数是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f71bef77bede1f247766fc752761d2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*Pi7Hs44OvNMIoFDTPlP0rQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.3贝塔函数</p></figure><p id="abe6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也可以写成伽玛函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bd3346744c50c1aa8618a60779b3f6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*Dyz5ElIt5uCYlsBwQS2KHA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.4根据伽马函数的贝塔函数</p></figure><p id="f419" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，<em class="mn"> Beta(0，0) </em>没有定义，但是我们可以考虑它在点<em class="mn"> (0，0) </em>的近似值，如下图所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c98569e0d49a9c6201bdd717a8e20102.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*YdMaZ2PYdFlNYYEvtOJ19A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.5参数<em class="ns"> α=0，β=0 </em>的贝塔分布的极限行为</p></figure><p id="125a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">后验分布<em class="mn">π(θ|</em><strong class="kw iu"><em class="mn">x</em></strong><em class="mn">)</em>与<em class="mn"> θ⁻ (1-θ)⁻ </em>)成正比(回想一下贝叶斯定理可以写成公式1.2的形式)，也就是说</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/100750cb7b815c84dad9de0f34b103b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*jlyAHhXfVnNnW2aMGFmoZA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">方程2.6霍尔丹先验没有归一化系数</p></figure><p id="94b2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该先验赋予<em class="mn"> θ=1 </em>和<em class="mn"> θ=0 </em>最大权重。使用[5]中的例子可以清楚地说明这一点:考虑我们正在观察一种未知化合物是否会溶解于水的情况。起初，我们对结果一无所知。因此，在观察到一个小样本溶解后，我们立即假设所有的样本都会这样做；如果没有，我们假设没有样品可以溶解。</p><p id="9ab9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">3.<em class="mn">共轭先验—贝塔分布</em></p><p id="4eca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第三个例子是<strong class="kw iu">贝塔分布</strong>，它是二项式分布的共轭先验。并且注意，由于<em class="mn">伯努利分布是二项式分布</em>(与<em class="mn"> B(1，1) </em>相同)的特例，贝塔分布也是伯努利分布的共轭先验。是共轭先验的典型例子(在<a class="ae mp" href="https://en.wikipedia.org/wiki/Conjugate_prior#:~:text=usual%20conjugate%20prior%20is%20the%20beta%20distribution%20with%20parameters" rel="noopener ugc nofollow" target="_blank">维基百科</a>、【3】和上出现过)。</p><p id="ee03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里我们将展示为什么贝塔分布与二项式分布共轭。首先回忆一下，二项分布的概率质量函数是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ec62eff2d8bb030887e5d3aeebfa7cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*bSSlOVVvGDyDbBL9Y2Bz9Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.7二项式分布的概率质量函数(pmf)</p></figure><p id="4f80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="mn"> n </em>为试验总次数，<em class="mn"> k </em>为成功次数，<em class="mn"> p </em>为成功概率。因此，可能性是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/78d678916f9e66fec46e9340e844553d.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*AaKLc73mjk8IvG97VX3Ikw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.8可能性</p></figure><p id="4223" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">参考我们在基础知识一节看到的，似然性表示为<em class="mn">π(</em><strong class="kw iu"><em class="mn">x</em></strong><em class="mn">|θ)</em>，其中<strong class="kw iu"> <em class="mn"> x </em> </strong>为观测值，所以<strong class="kw iu"> <em class="mn"> x </em> </strong> = (k，n-k)。这意味着</p><blockquote class="nw nx ny"><p id="5183" class="ku kv mn kw b kx ky ju kz la lb jx lc nz le lf lg oa li lj lk ob lm ln lo lp im bi translated">二项分布的参数成为观察值，这种可能性中的“参数”是超参数。</p></blockquote><p id="42cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们选择贝塔分布作为先验。我们要做的是证明后验分布与先验分布是同一类型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f63f8dff98ad7022e21fb43f0038a61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*0LpW1uzOEXV9MATGOIC3mg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.9选择贝塔分布作为先验分布</p></figure><p id="237a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">后验分布推导如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi od"><img src="../Images/69521b0f433888043f0ff92fed5e70e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fWoAT0UANd3i7kTNlK1OQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.10后验分布</p></figure><p id="61ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到后验分布也是贝塔分布。</p><p id="1fef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="mn"> 4。杰弗里斯先验</em></p><p id="62fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Jeffreys先验是根据Fisher信息矩阵的行列式的平方根定义的非信息先验。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e955e38d8300182d947985cc604817d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*CtRP9PaD6kx_9-eHZDBc3A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Def 2.11杰弗里斯先验的定义</p></figure><p id="5860" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">费希尔信息和费希尔信息矩阵在这里<a class="ae mp" rel="noopener" target="_blank" href="/maximum-likelihood-estimation-mle-and-the-fisher-information-1dd53faa369">介绍过</a>，但是为了方便起见，我们在这里也再次提及。最初，费希尔信息被定义为分数的方差</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ae298a07e7e6495836ad62eb955c8b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*S05uakuUfopFBrJiW-2w_w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Def 2.12费希尔信息的定义</p></figure><p id="ad30" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中较低的指数<em class="mn"> θ </em>表示期望值是关于θ的，矩阵形式写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9de2112913afcac27070f77663478565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*J9fXiTEXQLMa2iqvYZko7w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.13费希尔信息矩阵</p></figure><p id="ce02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是在某些特定的条件下(密度函数f是二阶可微的和正则的条件，可以在<a class="ae mp" href="https://stats.stackexchange.com/questions/101520/what-are-the-regularity-conditions-for-likelihood-ratio-test" rel="noopener ugc nofollow" target="_blank">这里</a>找到很好的总结)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1f92151a64a48138857319dca2647300.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*mAP2kkNfL7IjE1ZpVy8V4w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2.14特定条件下的费希尔信息</p></figure><p id="6c36" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等式2.14是单变量情况下的公式(当有多个参数时，我们使用矩阵形式)。让我们试着计算伯努利试验的杰弗里斯先验，这是一个单变量的例子。我们使用这个分布进行演示是有原因的，我们将在后面看到。我们知道伯努利分布的概率分布是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b7b71bfed007b6767ab34ea44168418d.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mbaKEBn0ocIom6upLiS-qw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">方程2.15伯努利分布的密度函数</p></figure><p id="b485" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们需要计算密度函数的费希尔信息(方程2.15)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi on"><img src="../Images/019c1a864aaed88e98df66aaa61646fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQ3i6VYO1b3P__ew4fw2LQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1.26伯努利试验的费希尔信息</p></figure><p id="84fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于参数只是一维的(单变量)，费雪信息只是一个数，也是行列式，我们有先验分布</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e37f8ab62a78a6dca037549b3b34b611.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*1VriVBwKN-H1PiEY1yg-6w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1.27</p></figure><p id="8ac0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">仔细看方程1.27，我们会发现杰弗里斯先验和霍尔丹先验类似。但不像霍尔丹先验，杰弗里斯先验是适当的。方程式1.27的曲线如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/b4b9277c4c2c845559101417ba8ed7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*ALjYiozjZCC66S498HTijA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1.28(作者图片)</p></figure><p id="1d6f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它还与β分布有关，因为等式1.27等于β(1/2，1/2)。</p><p id="889e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">摘要</p><p id="1c22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个帖子主要是关于贝叶斯推断中的先验分布。首先，简单介绍了贝叶斯推理的基础知识。然后我们看看先验分布的类型，然后选择一些常见的先验分布。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="3791" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">参考文献</strong>:</p><p id="c02a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[1]李，T. S .，&amp;芒福德，D. (2003)。<em class="mn">视觉皮层中的分层贝叶斯推理</em>。<em class="mn"> JOSA A </em>，<em class="mn"> 20 </em> (7)，1434–1448。</p><p id="1114" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] Surya，Tokdar，<a class="ae mp" href="https://www2.stat.duke.edu/courses/Spring13/sta732.01/priors.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mn">选择先验分布</em> </a>，2021年12月4日获取。</p><p id="0f89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3]赫尔曼、卡林、斯特恩、H. S .、鲁宾(1995年)。<a class="ae mp" href="https://www.taylorfrancis.com/books/mono/10.1201/9780429258411/bayesian-data-analysis-andrew-gelman-john-carlin-hal-stern-donald-rubin" rel="noopener ugc nofollow" target="_blank"> <em class="mn">贝叶斯数据分析</em> </a>。查普曼和霍尔/CRC。</p><p id="119e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4] Etz，a .，&amp; Wagenmakers，E. J. (2017年)。JBS霍尔丹对贝叶斯因子假设检验的贡献。<em class="mn">统计科学</em>，313–329。</p><p id="6418" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]杰恩斯，E. T. (1968年)。先验概率。<em class="mn"> IEEE系统科学与控制论汇刊</em>，<em class="mn"> 4 </em> (3)，227–241。</p><p id="0450" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]斯坦福大学法学博士和瓦德曼大学(1994年)。<em class="mn">物理科学统计方法</em>(第28卷)。学术出版社。</p><p id="2ed6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7]戈尔奇，S. (2016年10月)。<a class="ae mp" href="https://ieeexplore.ieee.org/abstract/document/7796966?casa_token=RgQ527gQM6cAAAAA:lvcjiPhN82UF3AmhNjPg4tEc-2nKYRkD5WtD-NVmNXNMqwcp6gJw8aeV-FY9ddqQAKgMCHD-eTU" rel="noopener ugc nofollow" target="_blank">信息先验和贝叶斯计算</a>。在<em class="mn"> 2016 IEEE数据科学与高级分析国际会议(DSAA) </em>(第782–789页)。IEEE。</p><p id="f1e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[8] Nicenboim，b .，Schad，D. J .，&amp; Vasishth，S. (2021年)。<a class="ae mp" href="https://vasishth.github.io/bayescogsci/book/sec-analytical.html" rel="noopener ugc nofollow" target="_blank"> <em class="mn">认知科学贝叶斯数据分析导论</em> </a>。</p><p id="d3a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[9]杰里米·奥尔洛夫和乔纳森·布鲁姆，<a class="ae mp" href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading15a.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mn">共轭先验:贝塔和正常</em> </a>，2021年12月11日获取。</p><p id="ce93" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mp" href="http://halweb.uc3m.es/esp/Personal/personas/mwiper/docencia/English/PhD_Bayesian_Statistics/ch5_2009.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mn"/></a>【先验分布】，2022年1月1日获取。</p><p id="8aa9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">延伸阅读</strong>:</p><p id="fe96" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有关概率论的更多信息:</p><div class="ox oy gp gr oz pa"><a rel="noopener follow" target="_blank" href="/measure-theory-in-probability-c8aaf1dea87c"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd iu gy z fp pf fr fs pg fu fw is bi translated">概率论中的测度论</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">概率毕竟不简单。</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ko pa"/></div></div></a></div><p id="d31e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">关于贝叶斯和频率主义方法的比较:</p><div class="ox oy gp gr oz pa"><a href="https://medium.com/science-and-philosophy/subjectivism-in-decision-science-926c29feb7bb" rel="noopener follow" target="_blank"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd iu gy z fp pf fr fs pg fu fw is bi translated">决策科学中的主观主义</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">关于贝叶斯主义的一点注记</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">medium.com</p></div></div><div class="pj l"><div class="pp l pl pm pn pj po ko pa"/></div></div></a></div><p id="fdeb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">补充</strong>:</p><p id="3c4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用于生成图0.1的代码</p><pre class="kj kk kl km gt pq pr ps pt aw pu bi"><span id="6ae6" class="mt lr it pr b gy pv pw l px py">x.v &lt;- seq(0, 1, by=0.01)<br/>n &lt;- length(x.v)<br/>m &lt;- matrix(nrow=n, ncol=1)</span><span id="8181" class="mt lr it pr b gy pz pw l px py">for (i in seq(0.1, 5, 0.3)) {<br/>  y.v &lt;- dbeta(x.v, shape1=i, shape2=15)<br/>  m &lt;- cbind(y.v, m)<br/>}<br/>for (j in seq(0.1, 5, 0.3)) {<br/>  y.v &lt;- dbeta(x.v, shape1=15, shape2=j)<br/>  m &lt;- cbind(y.v, m)<br/>}<br/>for (i in seq(0.1, 10, 1)) {<br/>  y.v &lt;- dbeta(x.v, shape1=i, shape2=i)<br/>  m &lt;- cbind(y.v, m)<br/>}<br/>n.c &lt;- ncol(m)<br/>n.c<br/># remove last column with Nas<br/>m &lt;- m[,-n.c]<br/>par(mar = c(3, 4, 1, 2))<br/>matplot(x=x.v, y=m, type="l", col="black", ylim=c(0,11), xlab="", ylab="PDF")</span></pre></div></div>    
</body>
</html>