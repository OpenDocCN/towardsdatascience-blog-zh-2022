<html>
<head>
<title>Four Deep Learning Papers to Read in January 2022</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2022年1月要读的四篇深度学习论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-deep-learning-papers-to-read-in-january-2022-fbe183e6bf6c#2022-01-10">https://towardsdatascience.com/four-deep-learning-papers-to-read-in-january-2022-fbe183e6bf6c#2022-01-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="ab33" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">【2022年1月要读的四篇深度学习论文</h1></div><div class=""><h2 id="74dc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">从自举元学习到深度学习的时间序列预测，外推&amp;泛化与利用Ridge Rider探索多样性最优的关系</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b4dac16062850cd68156910f632df74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lt9NSP8Z5L0hRAbfbQT3SQ.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ku kv l"/></div></figure><p id="2bf1" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">欢迎来到一月份的【T4:机器学习-拼贴】系列，在这里我提供了不同深度学习研究流的概述。那么什么是ML拼贴呢？简单地说，我为我最近最喜欢的一篇论文起草了一张幻灯片的视觉摘要。每一周。在月底，所有由此产生的视觉拼贴都被收集在一个摘要博客帖子中。因此，我希望给你一个视觉和直观的深入了解一些最酷的趋势。所以，废话不多说:这里是我最近阅读的四篇最喜欢的论文，以及为什么我认为它们对深度学习的未来很重要。</p><h2 id="116f" class="lt lu it bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">“自举元学习”</strong></h2><p id="17de" class="pw-post-body-paragraph kw kx it ky b kz mm ju lb lc mn jx le lf mo lh li lj mp ll lm ln mq lp lq lr im bi translated"><em class="mr">作者:Flennerhag等人(2021) </em> |📝<a class="ae ls" href="https://arxiv.org/pdf/2109.04504.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="e150" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated"><strong class="ky iu">一段总结:</strong>元学习算法旨在自动发现归纳偏差，这允许跨许多任务的快速适应。经典的例子包括MAML或rl2。通常，这些系统在双层优化问题上被训练，其中在快速内部循环中，人们仅考虑单个任务实例化。在第二个更慢的外部循环中，系统的权重然后通过在许多这样的单独任务上分批来更新。因此，系统被迫发现并利用任务分布的底层结构。大多数时候，外部更新必须通过内部循环更新过程来传播梯度。这会导致两个问题:如何选择内环的长度？目光短浅允许更容易的优化，同时可能是短视的。此外，元目标可能表现不稳定，遭受消失和爆炸梯度。那么，我们如何克服这种短视的优化困难呢？自举元学习(Bootstrapped meta-learning)提出通过将内部循环运行得更长一点来构建所谓的自举目标。然后，我们可以使用由此产生的网络作为短期学生的老师。类似于DQNs，引导目标从计算图中分离出来，并简单地作为损失计算中的固定量。因此，我们本质上是向前推动元代理。用于比较专家和学生的度量可以进一步控制元目标的曲率。在一组toy RL实验中，作者表明自举可以允许快速探索适应，尽管时间跨度较短，并且它优于时间跨度较长的普通元梯度。与STACX元梯度代理一起，自举元梯度提供了一个新的雅达利SOTA，也可以应用于多任务少镜头学习。总之，这项工作为如何积极地操纵元学习问题公式化打开了许多新的视角。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/aac92affa2b7290b83c380ed0b3eb14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-fSGJwyA2IlDLxL9nKQ8w.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">ML-Collage [37]:作者的数字。|📝<a class="ae ls" href="https://arxiv.org/abs/1905.10437" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="b6e3" class="lt lu it bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">“N-Beats:可解释时间序列预测的神经基础扩展分析”</strong></h2><p id="8b7c" class="pw-post-body-paragraph kw kx it ky b kz mm ju lb lc mn jx le lf mo lh li lj mp ll lm ln mq lp lq lr im bi translated"><em class="mr">作者:Oreshkin等人(2020) </em> |📝<a class="ae ls" href="https://arxiv.org/abs/1905.10437" rel="noopener ugc nofollow" target="_blank">纸张</a> |🤖<a class="ae ls" href="https://github.com/ElementAI/N-BEATS" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="e52d" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated"><strong class="ky iu">一段话总结:</strong>传统的时间序列预测模型，如ARIMA，来自金融计量经济学领域，依靠拟合的移动平均线来预测趋势和季节性成分。它们往往只有很少的参数，同时保持清晰的可解释性。最近，混合模型，结合递归神经网络和微分预测已经变得越来越流行。这允许灵活的函数拟合，同时保持更经典方法的归纳偏差。但是，是否也有可能训练有竞争力的预测者，这些预测者基于纯粹的深度学习方法？在N-Beats中，作者介绍了一种用于单变量时间序列预测的新网络架构，该架构在M4 M3&amp;旅游基准上建立了一个新的SOTA。该体系结构由多个残差块栈组成，它们同时执行预测和回溯。单个堆栈的部分预测被组合成所考虑的时间范围的最终预测。此外，单个块预测的基础可以被学习或固定为合适的和可解释的函数形式。例如，这可以是捕捉趋势的低维多项式或季节性成分的周期性泛函。作者将他们的方法与集成技术相结合，融合了基于不同度量、输入窗口和随机初始化训练的模型。此外，它们还表明，随着更多叠加的增加，性能增益达到饱和，并直观地分析了固定基础叠加预测确实是可解释的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/9f70bf269b7b7e806a83a1a2599adb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDHL4AGLlY5_jvys-Ws-wA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">ML-Collage [38]:作者的数字。|📝<a class="ae ls" href="https://arxiv.org/abs/1905.10437" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="e827" class="lt lu it bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">‘高维学习总是等于外推’</strong></h2><p id="71d5" class="pw-post-body-paragraph kw kx it ky b kz mm ju lb lc mn jx le lf mo lh li lj mp ll lm ln mq lp lq lr im bi translated"><em class="mr">作者:Balestriero等人(2021) </em> |📝<a class="ae ls" href="http://arxiv.org/abs/2110.09485" rel="noopener ugc nofollow" target="_blank">论文</a> | 🗣 <a class="ae ls" href="https://twitter.com/MLStreetTalk/status/1478360446264610821?s=20" rel="noopener ugc nofollow" target="_blank">播客</a></p><p id="05d4" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated"><strong class="ky iu">一段话总结:</strong>神经网络(NNs)只能学习插值吗？Balestriero等人认为，为了解决高维任务，神经网络必须进行外推。他们的推理依赖于插值的简单定义，也就是说，每当数据点落入观察到的训练数据的凸包中时，就会发生插值。随着原始输入空间的维数线性增长，该空间的体积以指数速率增长。我们人类挣扎于三维空间之外的几何直觉的可视化，但是这种现象通常被称为维度的诅咒。但如果数据位于一个更低维度的流形上呢？那么，是否有可能避开维数灾难，只用几个样本就能获得插值？在一组合成实验中，作者表明，实际上重要的不是流形的原始维度，而是所谓的内在维度，即包含数据流形的最小仿射子空间。他们表明，对于常见的计算机视觉数据集；随着所考虑的输入维数的增加，测试集样本包含在训练集的凸包中的概率迅速降低。作者还强调，这种现象存在于神经网络嵌入或不同的降维技术中。在所有情况下，插值百分比随着更多输入维度的考虑而降低。那么这能告诉我们什么呢？为了使无核武器国家成功地解决一项任务，他们必须在“外推法”制度下运作！但并不是所有的人都像其他人一样一般化。所以这就提出了新的问题，关于外推和更普遍的概括之间的关系。例如，数据扩充和规范化扮演什么角色？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/ba5a1dbd44898f6257518ef81fead87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0b-sTk3lyEDr8KZhBWCFg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">ML-Collage [39]:作者的数字。|📝<a class="ae ls" href="http://arxiv.org/abs/2110.09485" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="ceb9" class="lt lu it bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">“脊骑士:通过遵循黑森的特征向量寻找不同的解决方案”</strong></h2><p id="ea2a" class="pw-post-body-paragraph kw kx it ky b kz mm ju lb lc mn jx le lf mo lh li lj mp ll lm ln mq lp lq lr im bi translated"><em class="mr">作者:帕克-霍尔德等人(2020) </em> |📝<a class="ae ls" href="https://arxiv.org/abs/2011.06505" rel="noopener ugc nofollow" target="_blank">论文</a> | 🗣 <a class="ae ls" href="https://youtu.be/MHz5zG9DDIY?t=2095" rel="noopener ugc nofollow" target="_blank">谈话</a> |🤖<a class="ae ls" href="https://colab.research.google.com/drive/1RTwd7IOgOC7Meky1jCekbyQNG8fu_Stg?usp=sharing" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="62fd" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated"><strong class="ky iu">一段话总结:</strong>现代深度学习问题往往要处理很多局部最优。梯度下降已被证明偏向于简单的高曲率解。这个问题的经典例子包括计算机视觉中的形状与纹理优化，或者不能推广到新玩家的自我游戏策略。其中优化过程结束的局部最优可能取决于许多任意因素，例如初始化、数据排序或诸如正则化的细节。但是，如果我们不是试图获得一个单一的最优值，而是同时探索一组不同的最优值，那会怎么样呢？脊骑士算法旨在这样做，通过迭代地跟随具有负特征值的Hessian的特征向量——所谓的脊。作者证明，只要特征向量沿轨迹平滑变化，这个过程就是局部损失减少的。通过遵循这些不同的脊，脊骑士能够在列表RL和MNIST分类的上下文中覆盖许多不同的局部最优。作者表明，脊骑士也可以帮助发现最佳零射击协调政策，而不必访问潜在的问题对称性。总之，Rider将连续优化问题转化为在不同山脊上的离散搜索。它为稳健优化开辟了一个有希望的未来方向。但是在该方法的可扩展性方面还存在许多未解决的问题，包括有效的特征分解和多个特征向量的同时探索。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/3b94c1b73f9ac871c1abb2671a98afe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGiaJjzKF6_w01Irqq5BlA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">ML-Collage [40]:作者的数字。|📝<a class="ae ls" href="https://arxiv.org/abs/2011.06505" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="6d11" class="pw-post-body-paragraph kw kx it ky b kz la ju lb lc ld jx le lf lg lh li lj lk ll lm ln lo lp lq lr im bi translated">这是这个月的🤗让我知道你最喜欢的论文是什么。如果你想获得一些每周的ML拼贴画输入，查看Twitter标签<a class="ae ls" href="https://twitter.com/hashtag/mlcollage" rel="noopener ugc nofollow" target="_blank"> #mlcollage </a>，你也可以在最后的摘要中看到更多的拼贴画📖博客帖子:</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/four-deep-learning-papers-to-read-in-december-2021-e28f31e6aab4"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">2021年12月要读的四篇深度学习论文</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">从感官替代到决策转换器、持续进化策略和敏锐感知最小化</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ks nj"/></div></div></a></div></div></div>    
</body>
</html>