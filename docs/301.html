<html>
<head>
<title>Analyzing Document Layout with LayoutParser</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LayoutParser分析文档布局</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyzing-document-layout-with-layoutparser-ed24d85f1d44#2022-01-11">https://towardsdatascience.com/analyzing-document-layout-with-layoutparser-ed24d85f1d44#2022-01-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="f3c7" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">使用LayoutParser分析文档布局</h1></div><div class=""><h2 id="40e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用LayoutParser库检测布局并从文档图像中提取文本</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7f16bab179eaf45fda01e6fc34ae6bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeyJDOo8zo_vdbT94CYsyw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安妮·斯普拉特在<a class="ae ky" href="https://unsplash.com/s/photos/document?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="34bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言处理的应用经常需要我们从输入文档中提取文本作为先决条件。</p><p id="5970" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题是，有时我们需要做额外的工作来从输入文档中提取文本，因为它们通常是PDF、JPEG或PNG格式的。而这就是我们通常使用OCR引擎的地方。它帮助我们将图像或扫描文档中的书面文本转换为机器可读的文本数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/6906cd8f0aad2acc7f44c70e4b5655a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ltT43FbMf35gLjtx8T6qfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dd95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，在使用OCR提取文本之前，我们需要注意一个问题。有时，我们的输入文档不仅包含一堆文本，还包含一个标题、一个图像和一个表格，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/5b36c474b0299b356a5e0b460be448e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trPTNcYURXm6fSom-AHYEw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a749" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设对于我们的用例，我们只想从上面输入文档的每一段中提取文本。这意味着我们希望省略表格、标题和图像区域中的文本。</p><p id="c7f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们怎么做呢？在使用OCR之前，我们需要在这里对输入文档的每个部分进行分类。我们将使用LayoutParser来完成这项工作。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="591b" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">什么是LayoutParser？</h1><p id="09fb" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><a class="ae ky" href="https://github.com/Layout-Parser/layout-parser" rel="noopener ugc nofollow" target="_blank"> LayoutParser </a>是一个Python库，它提供了广泛的预训练深度学习模型来检测文档图像的布局。</p><p id="3752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用LayoutParser的好处是它真的很容易实现。您实际上只需要几行代码就可以检测到文档图像的布局。我们将在下一节中看到这些步骤。</p><p id="ab24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了LayoutParser，你可以利用一些预先训练好的深度学习模型，这些模型已经在各种数据集上进行了训练，比如<a class="ae ky" href="https://developer.ibm.com/exchanges/data/all/publaynet/" rel="noopener ugc nofollow" target="_blank">publilaynet</a>、<a class="ae ky" href="https://dell-research-harvard.github.io/HJDataset/" rel="noopener ugc nofollow" target="_blank"> HJDataset </a>、<a class="ae ky" href="https://www.primaresearch.org/dataset/" rel="noopener ugc nofollow" target="_blank"> PrimaLayout </a>、<a class="ae ky" href="https://news-navigator.labs.loc.gov/" rel="noopener ugc nofollow" target="_blank">报纸导航器</a>和<a class="ae ky" href="https://doc-analysis.github.io/tablebank-page/index.html" rel="noopener ugc nofollow" target="_blank"> TableBank </a>。</p><p id="bae6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您的文档图像看起来类似于上面提到的任何数据集，那么您将有很好的机会使用LayoutParser获得非常好的布局检测结果。</p><p id="853d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们直接进入LayoutParser实现。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="e8cc" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">使用LayoutParser进行布局检测</h1><p id="0de2" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">要使用LayoutParser来检测我们的文档图像的布局，我们需要通过pip install安装软件包和Detectron2模型，如下所示:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="461a" class="ng mf it nc b gy nh ni l nj nk">pip install layoutparser torchvision &amp;&amp; pip install "detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2"</span></pre><p id="a930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您的文档是PDF格式，您需要将其转换为PNG文件。要用Python进行这种转换，我们可以使用pdf2img库。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="b920" class="ng mf it nc b gy nh ni l nj nk">pip install pdf2img</span></pre><p id="a7df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在本文中用作示例的文档仍然是PDF格式的，因此这个pdf2img库对于将文档转换为PNG文件是必不可少的。我们实际上只需要两行代码就可以做到这一点:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="0e28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。现在我们的文档已经可以用于布局检测了。</p><p id="cb83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LayoutParser使用基于Detectron2的预训练模型(如fast R-CNN、RetinaNet和Mask R-CNN)来检测输入文档的布局。要初始化预训练模型，我们可以执行以下操作:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="f4c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我们在实例化<code class="fe nn no np nc b">Detectron2LayoutModel</code>时提供了三个参数:</p><ul class=""><li id="522d" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated"><code class="fe nn no np nc b">model_path</code>:预训练模型的配置路径。要查看LayoutParser当前支持的各种预训练模型及其对应的配置路径，请查看<a class="ae ky" href="https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html#model-catalog" rel="noopener ugc nofollow" target="_blank">它们的文档页面</a>。</li><li id="2661" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe nn no np nc b">extra_config</code>:这是一个可选参数。但是，您可以提供一个来调整Detectron2型号的默认配置。在上面的示例中，我们将每个检测到的布局的阈值更改为0.5。这意味着，如果检测到的布局的置信度低于0.5，相应的边界框将不会显示。</li><li id="e531" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated"><code class="fe nn no np nc b">label_map</code>:从模型预测的id映射到实际的字符串表示。此标签映射取决于您选择的预训练模型以及该模型已训练的数据集。您可以在此页面中看到关于标签映射<a class="ae ky" href="https://layout-parser.readthedocs.io/en/latest/notes/modelzoo.html#model-label-map" rel="noopener ugc nofollow" target="_blank">的完整信息。</a></li></ul><p id="2818" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以使用<code class="fe nn no np nc b">model</code>中的<code class="fe nn no np nc b">detect</code>方法来检测输入文档的布局，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="a0b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们基本上完成了。现在如果你打印出<code class="fe nn no np nc b">layout_result</code>里面的内容，你会得到如下结果:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="310f" class="ng mf it nc b gy nh ni l nj nk">Layout(_blocks=[</span><span id="f102" class="ng mf it nc b gy oe ni l nj nk">TextBlock(block=Rectangle(x_1=126.12479400634766,       y_1=1335.8980712890625, x_2=806.6560668945312, y_2=1578.486328125), text=None, id=None, type=Text, parent=None, next=None, score=0.9993358254432678),</span><span id="9c11" class="ng mf it nc b gy oe ni l nj nk">TextBlock(block=Rectangle(x_1=854.9361572265625, y_1=259.9295654296875, x_2=1530.5875244140625, y_2=592.3228149414062), text=None, id=None, type=Text, parent=None, next=None, score=0.9992992877960205),</span><span id="5e64" class="ng mf it nc b gy oe ni l nj nk">....</span></pre><p id="9b9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这基本上是一个由检测到的布局列表组成的对象。在每个检测到的布局中，您会获得以下重要信息:</p><ul class=""><li id="3e12" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated">每个检测到的布局的边界框(<strong class="lb iu"> <em class="of"> x1，y1，x2，y2 </em> </strong>)的坐标</li><li id="69b8" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">检测到的布局类型(即文本、图像、表格、列表或标题)</li><li id="5efa" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">检测到的布局的id</li><li id="2d8b" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">每个检测到的布局中的文本</li><li id="a4b5" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">每个检测到的布局的置信度得分</li></ul><p id="5c5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要进一步调整或优化布局检测的结果，所有这些信息都将派上用场，你将在这篇文章的后面看到。</p><p id="ee5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想要可视化布局检测的结果，您可以使用LayoutParser中的<code class="fe nn no np nc b">draw_box</code>方法，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="015d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会得到下面的视觉效果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/9b86927e8aae7adeee0152d83628e732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxPC83n2IFrAs438ucVp-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7313" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们只想检测文本区域并忽略图像和表格区域，那么我们可以使用相应的标签映射来过滤结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="22e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在过滤过程之后，我们可以再次用<code class="fe nn no np nc b">draw_box</code>方法可视化结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/31871b40377d2445183e0f930406e86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPy7Gr6nwklSdZg2z8cvMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="2098" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">使用LayoutParser的OCR</h1><p id="6191" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">不仅是布局检测，我们还可以用LayoutParser提取每个检测到的布局中的文本。为此，您需要通过pip install安装一个附加的依赖项:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="bf44" class="ng mf it nc b gy nh ni l nj nk">pip install "layoutparser[ocr]"</span></pre><p id="a340" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LayoutParser目前支持两个OCR引擎:Tesseract和Google Cloud Vision。在这篇文章中，我们将使用Tesseract作为OCR引擎，从检测到的布局中提取文本。</p><p id="69f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您使用Tesseract，那么您可能还需要安装引擎本身。<a class="ae ky" href="https://tesseract-ocr.github.io/tessdoc/Installation.html" rel="noopener ugc nofollow" target="_blank">参考他们的文档</a>了解如何根据您的平台安装Tesseract引擎。</p><p id="b495" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但在此之前，我们需要对检测到的布局的元素ID进行排序，因为我们的OCR引擎将根据布局的元素ID顺序提取文本。从上面的可视化可以看出，检测到的布局的元素ID还没有排序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/924e26f7c335ce4b84a116be4158a8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lg4-WNK61yDcQVBBofVssw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="cede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们准备好用OCR提取每个检测到的布局的文本。首先，我们需要用LayoutParser中的<code class="fe nn no np nc b">TesseractAgent</code>初始化Tesseract OCR代理对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="9837" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你从上一节已经知道的，我们的<code class="fe nn no np nc b">text_blocks</code>变量基本上是一个带有几个有用信息的<code class="fe nn no np nc b">Layout</code>对象，包括每个检测到的布局内的文本，如下所示:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f788" class="ng mf it nc b gy nh ni l nj nk">Layout(_blocks=[</span><span id="7806" class="ng mf it nc b gy oe ni l nj nk">TextBlock(block=Rectangle(x_1=126.12479400634766,       y_1=1335.8980712890625, x_2=806.6560668945312, y_2=1578.486328125), text=None, id=0, type=Text, parent=None, next=None, score=0.9993358254432678),</span><span id="1e8e" class="ng mf it nc b gy oe ni l nj nk">....</span></pre><p id="fc4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果仔细观察，每个检测到的布局的<code class="fe nn no np nc b">text</code>仍然有一个值<code class="fe nn no np nc b">None</code>。我们将用Tesseract OCR将这个<code class="fe nn no np nc b">None</code>值转换成实际的文本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="ddd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在上面的代码中所做的基本如下:</p><ul class=""><li id="be26" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated">迭代每个检测到的布局</li><li id="7e3e" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">在每个检测到的布局中，我们只将原始图像裁剪到该检测到的布局的区域</li><li id="b3a6" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">对裁剪的图像执行OCR</li><li id="f0fb" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">将从<code class="fe nn no np nc b">None</code>检测到的每个布局的<code class="fe nn no np nc b">text</code>值设置为OCR产生的实际文本</li></ul><p id="a3c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以获取每个检测到的布局的文本，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="04ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我只显示了从前三个文本区域提取的文本:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="d611" class="ng mf it nc b gy nh ni l nj nk">Since the dataset is considered imbalanced, then any kind of alternative methods such as oversampling technique and the choice of different Machine Learning algorithms are implemented as well. The last step would be observing the accuracy and the F1 score of the model.   <br/>--- <br/>The first step that should be done in an image classification task is ingesting the training and validation data, and then preprocess them straight away such that they have a consistent size. Next, the pixel value in each of the images needs to be normalized as well.   --- <br/>After the preprocessing step is done, then an appropriate CNN model needs to be built. For this project, there are two CNN models that have been implemented. The first model is a custom model, meaning that the CNN model is built from the scratch. Meanwhile, the second model is built by utilizing the transfer learning method from InceptionV3 model, in which its weight has been trained on ImageNet dataset.</span></pre><p id="8cb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！现在，您可以将输出保存到一个文本文件、一个CSV文件中，或者直接对其进行预处理，以将其用作您想要执行的任何NLP任务的输入。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="7495" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">调整LayoutParser的结果</h1><p id="0d7d" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">由于LayoutParser利用了在特定数据集上训练过的预训练模型，当然，最终的布局检测有时会与我们预期的有一点偏差。</p><p id="2dd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的例子工作得很好，因为提供的文档非常类似于PubLayNet数据集中的典型科学文档，这是我们选择的模型的训练数据集。</p><p id="a9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当输入文档的布局略有不同时，假设文档只有一列，而不是典型的两列格式，我们可能会得到稍微不准确的结果。</p><p id="58ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本节中，我将向您展示一个布局检测结果略有偏差的示例，以及一种我们如何进行调整以提高结果质量的可能方法。</p><p id="6452" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用与上一节完全相同的模型配置(阈值0.5)，我们从一列格式的输入文档中得到以下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/c843811bc83ecf1d42e5ab6fddab8a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQNLMKbNiRgrpYzpBg9-DQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4549" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述结果有两个问题:</p><ol class=""><li id="1afd" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu og nw nx ny bi translated">在另一个边界框中有许多冗余的边界框，这对于我们的OCR过程来说不是理想的。</li><li id="b0a2" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu og nw nx ny bi translated">文本'<em class="of">直径厚度</em>'不应被检测到，因为它是表格区域的一部分。</li></ol><p id="4825" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缓解这些问题的一个可能的方法是在我们初始化模型时增加<code class="fe nn no np nc b">extra_config</code>参数中的阈值。假设我们将它从0.5增加到0.8</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/9e5a43ee3ce6f2e4ff8891cea4c64e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcL1dwgvjAXc_tW2TsLq3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dac8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的结果可以看出，我们在调整阈值的时候是有取舍的。较低的阈值意味着我们会得到很多噪声，而较高的阈值意味着丢失一个或多个文本区域的风险较高。</p><p id="84b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们不想错过很多文本区域，那么我们可以将阈值设置为一个较低的值(在本例中我们使用0.5)。然后，我们通过计算一个边界框与另一个边界框的交集(IoU)来移除位于边界框内部的边界框。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="2173" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码是这样做的:</p><ul class=""><li id="abd9" class="nq nr it lb b lc ld lf lg li ns lm nt lq nu lu nv nw nx ny bi translated">计算每个边界框相对于另一个边界框的IoU。</li><li id="beb7" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">如果IoU高于某个阈值，那么我们计算两个边界框的面积。</li><li id="5e90" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">将面积较小的边框类型从<code class="fe nn no np nc b">Text</code>更改为<code class="fe nn no np nc b">None</code></li><li id="8d2c" class="nq nr it lb b lc nz lf oa li ob lm oc lq od lu nv nw nx ny bi translated">最后，我们过滤检测到的布局，只包含类型为<code class="fe nn no np nc b">Text</code>的边界框</li></ul><p id="4120" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是优化检测到的布局后的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/db758861324c4cb949849aea33b299ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lwvF5fNNIl5vG4M-12OgWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在结果看起来好多了，因为我们已经移除了位于边界框内的边界框。</p><p id="b4ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是还有一个问题。检测到的带有文本'<em class="of">直径厚度</em>'的布局不应该在那里，因为它是表格区域的一部分。移除该布局的一个简单方法是查看其索引，然后将其<code class="fe nn no np nc b">type</code>从<code class="fe nn no np nc b">Text</code>设置为<code class="fe nn no np nc b">None</code>，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/0dd55eb97dbce92f363711b55a6bd810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WhsBssvZkWSq8Jx_uowbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8a3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅此而已。现在，我们可以继续使用OCR从每个布局中提取文本，正如您在上一节中看到的那样。</p><p id="9d27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，上面的例子只是调整LayoutParser结果的许多可能性之一。决定什么方法最适合你的用例，这完全取决于你的创造力。</p><p id="8a4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果结果对您的数据非常不利，以至于调整输出不再是一个可行的选择，该怎么办呢？</p><p id="7219" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用LayoutParser，您实际上可以在自己的定制数据集上训练LayoutParser的model zoo上可用的模型。为此，你可以<a class="ae ky" href="https://github.com/Layout-Parser/layout-model-training" rel="noopener ugc nofollow" target="_blank">遵循他们的GitHub页面</a>上提到的步骤。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="ddcb" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="5d32" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">LayoutParser是一个很棒的库，只需几行代码就可以检测文档图像的布局。不仅检测布局，我们还可以用OCR提取每个检测到的布局的文本。</p><p id="acab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据您的用例，您实际上可以调整或细化LayoutParser的布局检测结果。但是，如果结果很差，以至于无法再对其进行调整，则可以在自定义数据集上训练LayoutParser上可用的模型。</p><p id="e0ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章能够帮助您开始探索LayoutParser！你可以在<a class="ae ky" href="https://github.com/marcellusruben/medium-resources/blob/main/Layout_Parser/layout_parser_ex.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这本笔记本</strong> </a>里看到这篇文章的代码。</p></div></div>    
</body>
</html>