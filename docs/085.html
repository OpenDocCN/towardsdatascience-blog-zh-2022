<html>
<head>
<title>How machine learning can help visually impaired people</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习如何帮助视障人士</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-can-machine-learning-help-visually-impaired-people-4fcdc76816b2#2022-01-04">https://towardsdatascience.com/how-can-machine-learning-help-visually-impaired-people-4fcdc76816b2#2022-01-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="5edc" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">机器学习如何帮助视障人士</h1></div><div class=""><h2 id="0e86" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于DeepLabv3+的盲人室外障碍物识别方案</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/0de3ab781aa7c54df5f88f3587645704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*aayydJi_93qXxk7eKd-Znw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="4493" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">多年来，我一直是视障人士的志愿者。在刚刚过去的新年前夜，我和一位盲人女士一起出去，这位女士生活了几十年，对她的邻居非常熟悉，但她仍然需要一双眼睛，以防人行道上出现意想不到的障碍:节日后被遗弃的圣诞树，非法停放的汽车，新的工地等。不幸的是，我们的志愿者比需要的少，因此一些盲人不得不等待几次才能有人陪同行走。</p><p id="1288" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我一直在想，当我的视障朋友单独使用机器学习时，我如何建议一种解决方案来帮助他们识别障碍。</p><p id="d615" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我提出了一种方法:</p><ol class=""><li id="d98a" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">使用智能手机捕捉实时图像作为用于对象识别的深度学习模型的输入；</li><li id="eac4" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">提供语音向导，让视障人士意识到道路上可能存在的障碍。</li></ol><p id="25d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我现在将为这个项目的重要部分提供更多细节:具有深度学习的对象识别系统。本文用到的所有代码都可以在这个<a class="ae mb" href="https://colab.research.google.com/drive/13kvfII2amzhcjc1AB40yk1nH8D9dJu3i" rel="noopener ugc nofollow" target="_blank">笔记本</a>里找到。</p><h1 id="e838" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">资料组</h1><p id="38ec" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">物体识别系统可以通过语义分割来实现，即对输入图像中的每一个像素赋予语义标签(比如一棵树，一条人行道)。</p><p id="81fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">准备适当的数据集是这项任务的一个重要步骤。</p><p id="819f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我在巴黎做志愿者时，我将在<a class="ae mb" href="https://www.kaggle.com/dansbecker/cityscapes-image-pairs" rel="noopener ugc nofollow" target="_blank">城市景观图像</a>的数据集上训练深度学习模型。原始cityscapes数据集可从cityscapes-dataset.com<a class="ae mb" href="http://cityscapes-dataset.com" rel="noopener ugc nofollow" target="_blank">获得，该数据集免费提供给学术和非学术实体用于非商业目的，如学术研究、教学、科学出版物或个人实验。</a></p><p id="a843" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">它包含带有2975个训练图像文件和500个验证图像文件的城市街道场景的标记图像。每个图像文件都是256x512像素，每个文件都是原始照片在图像左半部分的合成，在右半部分的旁边是标记的图像(语义分割的输出)。</p><p id="d601" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是一个图像的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5b87d5d63c9905076a76543ba680f672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*zQZaZYGlMYR85gr4uG0b2Q.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">数据集<a class="ae mb" href="https://www.kaggle.com/dansbecker/cityscapes-image-pairs" rel="noopener ugc nofollow" target="_blank">中的样本城市景观图像</a></p></figure><p id="ca0d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">数据集对不同的物体有不同的颜色，我们可以通过快速浏览来快速识别一些颜色:蓝色代表汽车，红色代表行人，绿色代表树木，灰色代表建筑物，等等。</p><p id="6d86" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们从图像预处理开始。第一项调查得出的结论是，即使是单个图像蒙版也有不止10K种不同的独特颜色，因此我们需要一个“getlabel”函数来查找给定调色板中每个像素的最近颜色和有限的颜色值，并将每个颜色与一个标签关联为一个整数。关于更多信息，这个<a class="ae mb" href="https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py" rel="noopener ugc nofollow" target="_blank"> Github库</a>提供了颜色标签和相关对象的完整列表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/981884becb09b8399b2d754a5a09c0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*i9VKZ2oVno_pIKj1gj5vKg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片:有29个颜色值的调色板</p></figure><p id="3e51" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以下函数通过以下方式为模型创建训练和验证数据集:</p><p id="502e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">1.分离输入和输出图像；</p><p id="34f4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">2.归一化输入图像；</p><p id="fbeb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">3.将输出图像中的每个像素关联一个整数作为标签。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="31a6" class="ng md iq nc b gy nh ni l nj nk">train_images=[]<br/>train_masks=[]<br/>val_images=[]<br/>val_masks=[]<br/><br/>def getlabel(img):<br/>    <em class="nl">"""<br/>    turn a 3 channel RGB image to 1 channel index image<br/>    """<br/>    </em>height, width, ch = img.shape<br/>    m_lable = np.zeros((height, width, 1), dtype=np.uint8)<br/>    for w in range(width):<br/>        for h in range(height):<br/>            b,g,r=img[h, w, :]<br/>m_lable[h,w,:]=np.argmin(np.linalg.norm(np.array([r,g,b])-color_palette,axis=1),axis=0)<br/>    return m_lable<br/><br/>def load_images(path):<br/>    temp_img,temp_masks=[],[]<br/>    images=glob(os.path.join(path,<strong class="nc ir">'*.jpg'</strong>))<br/>    for i in tqdm(images):<br/>        i = cv2.imread(i)<br/>        img = i[:, :256]<br/>        img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_32F)<br/>        msk = i[:, 256:]<br/>        label = getlabel(msk)<br/>        temp_masks.append(label)<br/>        temp_img.append(img)<br/>    return np.array(temp_img),np.array(temp_masks)</span><span id="b64e" class="ng md iq nc b gy nm ni l nj nk">train_images,train_masks=load_images(train_path)<br/>val_images,val_masks=load_images(val_path)</span></pre><h1 id="c3f4" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">训练模型</h1><p id="6313" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">在本节中，我们将在cityscapes数据集上使用DeepLabV3+ 构建一个<strong class="kt ir">语义分割模型。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/b88b9f85b3cea5a3b5a24f890160f47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyNUJJ4yW9Xpuazprzlkhw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><a class="ae mb" href="https://arxiv.org/pdf/1802.02611.pdf" rel="noopener ugc nofollow" target="_blank">DeepLabV3+</a>架构</p></figure><p id="cb03" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">DeepLab是一个语义分割架构<strong class="kt ir">，它基于DeepLab </strong>和阿特鲁空间金字塔池(ASPP)方案。使用ASPP而不是经典池方法的原因是，随着采样率变大，有效过滤器权重的数量变小。</p><p id="c0b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本文给出了DeepLabV3+的结果，它是DeepLab的最新版本，通过添加编码器-解码器结构扩展了它的祖先DeepLabV3。下面的代码通过构造扩展卷积块来构建DeepLabV3+,扩展卷积块包含一个卷积层，后跟一个批处理归一化层和ASPP，后者包含一个平均池、一个1*1卷积块、一个上采样层，后跟一系列具有递增扩展速率的卷积块。</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="cf8d" class="ng md iq nc b gy nh ni l nj nk">def convolution_block(<br/>    block_input,<br/>    num_filters=256,<br/>    kernel_size=3,<br/>    dilation_rate=1,<br/>    padding=<strong class="nc ir">"same"</strong>,<br/>    use_bias=False,<br/>):<br/>    x = layers.Conv2D(<br/>        num_filters,<br/>        kernel_size=kernel_size,<br/>        dilation_rate=dilation_rate,<br/>        padding=<strong class="nc ir">"same"</strong>,<br/>        use_bias=use_bias,<br/>        kernel_initializer=keras.initializers.HeNormal(),<br/>    )(block_input)<br/>    x = layers.BatchNormalization()(x)<br/>    return tf.nn.relu(x)<br/><br/><br/>def DilatedSpatialPyramidPooling(dspp_input):<br/>    dims = dspp_input.shape<br/>    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)<br/>    x = convolution_block(x, kernel_size=1, use_bias=True)<br/>    out_pool = layers.UpSampling2D(<br/>        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=<strong class="nc ir">"bilinear"</strong>,<br/>    )(x)<br/><br/>    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)<br/>    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)<br/>    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)<br/>    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)<br/><br/>    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])<br/>    output = convolution_block(x, kernel_size=1)<br/>    return output<br/><br/>def DeeplabV3Plus(image_size, num_classes):<br/>    model_input = keras.Input(shape=(image_size, image_size, 3))<br/>    resnet50 = keras.applications.ResNet50(<br/>        weights=<strong class="nc ir">"imagenet"</strong>, include_top=False, input_tensor=model_input<br/>    )<br/>    x = resnet50.get_layer(<strong class="nc ir">"conv4_block6_2_relu"</strong>).output<br/>    x = DilatedSpatialPyramidPooling(x)<br/><br/>    input_a = layers.UpSampling2D(<br/>        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),<br/>        interpolation=<strong class="nc ir">"bilinear"</strong>,<br/>    )(x)<br/>    input_b = resnet50.get_layer(<strong class="nc ir">"conv2_block3_2_relu"</strong>).output<br/>    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)<br/><br/>    x = layers.Concatenate(axis=-1)([input_a, input_b])<br/>    x = convolution_block(x)<br/>    x = convolution_block(x)<br/>    x = layers.UpSampling2D(<br/>        size=(image_size // x.shape[1], image_size // x.shape[2]),<br/>        interpolation=<strong class="nc ir">"bilinear"</strong>,<br/>    )(x)<br/>    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=<strong class="nc ir">"same"</strong>)(x)<br/>    return keras.Model(inputs=model_input, outputs=model_output)<br/><br/><br/>model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)<br/>model.summary()<br/><br/>loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br/>model.compile(<br/>    optimizer=keras.optimizers.Adam(learning_rate=0.001),<br/>    loss=loss,<br/>    metrics=[<strong class="nc ir">"accuracy"</strong>],<br/>)<br/><br/>history = model.fit(train_dataset,validation_data=val_dataset, epochs=15)<br/></span></pre><p id="efc0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是15个训练时期的训练准确度和验证准确度的曲线图。经过15次训练，验证准确率达到84%。</p><div class="kg kh ki kj gt ab cb"><figure class="ns kk nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><img src="../Images/20a91bbe13c8dbfd35ef273bce04cba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xerhIcZWQoYCgQMaDhpxlw.png"/></div></figure><figure class="ns kk nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><img src="../Images/d04677411a65473b299fe8cf17579950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*0swKNVGmWGOBfVkM3APAAw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk ny di nz oa translated">作者图片:准确性(左)与验证n准确性(右)</p></figure></div><p id="ef00" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以用这个模型来识别照片中的物体。本文开头有一个模型预测的图像遮罩，有下图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/53b6bdb70715bc3a05f4f73d86802684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JVr2ttdm6qrumY-7M4ZTpQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者的图像:作为模型输入的原始图像</p></figure><h1 id="561a" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">后续步骤</h1><p id="c737" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">为了说明这个想法，我在非常著名的城市景观数据集上训练了一个模型，该数据集取自驾驶车辆，这在实践中可能是不够的，因为视障者以不同的行为行走。为了避免偏见，最好直接从盲人的智能手机上收集数据。</p><p id="98cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有了深度学习提供的客观识别，我建议在系统中加入语音引导，只要设备检测到障碍物，就向用户发出警告。</p><p id="4fc1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，对于盲人来说，知道他们离障碍物有多远也是有用的。我们当然可以用标记数据训练另一个深度神经工作。缺少这样的数据集，我给她一个简单的代数计算，作为利用<a class="ae mb" href="https://en.wikipedia.org/wiki/Pinhole_camera_model" rel="noopener ugc nofollow" target="_blank">针孔相机模型</a>对用户与物体距离的第一次估算:即图像上物体的大小与现实生活中物体的大小之比，与焦距和到物体的距离之比相同。因此，我们可以通过焦距*实际高度/图像高度来估计距离。</p><h1 id="d909" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论</h1><p id="3e25" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">在这篇文章中，我给出了一个可能非常幼稚的项目建议，关于我们如何通过深度学习来帮助视障人士。</p><p id="520c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为一项重要的任务，外出永远不会是盲人的唯一需求，日常生活中的其他活动有时也具有挑战性:阅读电子邮件、购物、医疗访问等。不幸的是，我们与环境的大多数互动都是通过视觉实现的，我相信人工智能至少可以为盲人消除一部分这种限制。我愿意接受更多的建议和讨论。</p><h1 id="1da1" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">参考</h1><p id="8952" class="pw-post-body-paragraph kr ks iq kt b ku mu jr kw kx mv ju kz la mw lc ld le mx lg lh li my lk ll lm ij bi translated">[1]使用DeepLabV3+的多类语义分割:【https://keras.io/examples/vision/deeplabv3_plus/ T2】</p><p id="48e8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2]用于语义图像分割的阿特鲁可分离卷积编解码器:【https://arxiv.org/pdf/1802.02611.pdf】T4</p><p id="94d5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]复习:DeepLabv3 —阿特鲁卷积(语义分割):<a class="ae mb" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">https://towardsdatascience . com/Review-DeepLabv3-atrous-卷积-语义-分割-6d818bfd1d74 </a></p></div></div>    
</body>
</html>