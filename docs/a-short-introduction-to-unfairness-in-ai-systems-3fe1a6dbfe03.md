# 人工智能系统中的不公平现象简介

> 原文：<https://towardsdatascience.com/a-short-introduction-to-unfairness-in-ai-systems-3fe1a6dbfe03>

## 为什么 AI 系统很难做到公平？

![](img/f04edba3f36a11e6d68aee658480dc87.png)

亚历山大·辛恩在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 您的旅程概述

1.  [人工智能系统中的不公平是什么？](#b9e3)
2.  [案例研究:亚马逊的简历扫描仪📓](#e64e)
3.  [我们如何对抗人工智能系统中的不公平？](#bc22)
4.  [现代问题需要现代解决方案](#5a11)
5.  [包装](#20eb)

# 1 —人工智能系统中的不公平是什么？

总的来说，人工智能和机器学习已经改变了现代世界。仅在几年前，许多商业 AI 系统，如智能助手、自动驾驶汽车和准确的实时字幕，似乎都是不可能的。现在我们正在进入一个人工智能系统将变得越来越普遍的时代。这适用于从医药到营销的几乎每个领域。

现在，人工智能系统比以往任何时候都被用来以多种方式评估人类。基于性别或种族的人口群体有被人工智能系统不公平对待的历史。

2016 年有报道称，美国各地法庭使用的人工智能系统 COMPAS 非常不公平。它认为非裔美国人被告未来犯罪的高风险是白人被告的两倍。这严重影响了被告是否应该面临监禁或服刑的决定。

这些人工智能系统因此可以延续现有的社会不平等。很明显，弄清为什么会发生这种情况以及我们如何阻止这种情况发生是非常重要的。评估和减轻人工智能系统中的不公平和偏见的过程是负责任的人工智能的一部分。负责任的人工智能是一个更广泛的话题，包括安全性、透明性和可访问性。

在这篇博文中，我将尝试介绍人工智能系统中的不公平性，以及为什么它既困难又有趣。没有先决条件，所以让我们开始吧👍

# 2-案例研究:亚马逊的简历扫描仪📓

在解决这个普遍问题之前，让我们更仔细地看看一个具体案例，即亚马逊的简历扫描仪。像许多公司一样，亚马逊收到了大量求职申请。通读它们非常耗时。一个可能的解决方案是创建一个人工智能系统来解析简历，并返回简历“好”的程度。这样，可以显著减少人工阅读。

虽然理论上这是一个有趣的想法，但很快就发现它并没有达到预期的效果。在大约 10 年的时间里，人工智能系统是根据以前提交给公司的简历进行训练的。由于技术是一个男性占主导地位的行业，该系统审查的大多数申请人来自男性。

这造成了非常不幸的后果。在招聘技术职位时，简历扫描仪系统地偏向男性而非女性。这实际上是对包含“女性”一词的简历的惩罚。如果有人在五年前加入了国家女子游泳队，人工智能系统会认为这是负面的。如果把这个写进简历，那个人会自动得到一个较低的分数。

简历扫描仪很快就被解散了，据亚马逊的招聘人员称，它从未被单独用来评估候选人。即使亚马逊的团队最终设法消除了系统中对女性的负面偏见，人们也不能确定是否存在其他偏见。

尽管简历扫描仪从未完全投入生产，但它表明**创造公平的人工智能系统是一个棘手的问题**。更令人不安的问题是，现在现实世界中的人工智能系统越来越多。我们真的很难理解这些系统到底有多公平，公司通常没有经济动机去仔细调查他们自己的系统是否有偏见。

# 3 —我们如何对抗人工智能系统中的不公平？

上面关于亚马逊简历扫描仪的例子说明了两件事:

1.  人工智能系统中的不公平是典型的非故意的**。一个主要由女性人工智能工程师组成的团队仍然可以创建歧视女性的人工智能系统。虽然出于许多原因，拥有多样化的团队是好的，但这不足以阻止不公平的人工智能系统的创建。拥有多样化的团队应该是一个基线，而不是最终目标。**
2.  **人工智能系统中的不公平可能需要**很长时间才能检测到**。假设你已经创建了一个评估贷款申请的人工智能系统。你如何知道系统不会歧视申请者的性别或种族？你可以设置实验，使用控制组和足够的样本量来仔细地、科学地评估人工智能系统。然而，这是非常昂贵和耗时的。**

**上面最后一点可能也有心理学的成分。再次想象你已经把你的灵魂投入到创造一个人工智能系统中。也许这个系统做了一些真正令人钦佩的事情，比如检测皮肤癌。这个项目看起来非常成功，每个人都对你的工作非常满意。当在实践中观察人工智能系统时，它确实通过早期检测癌症来帮助人们，从而增加他们的生存机会。它正在改善这个世界，你得到了广泛的赞誉。**

**在这种情况下，不去想太多 AI 系统中的不公平可能很有诱惑力。你认为最坏的情况会是什么？这显然是在帮助人们，对吗？另一种选择是去你的公司，要求他们提供大量资金来评估你的模式是否不公平。这是你公司的利益相关者可能甚至没有考虑过的事情。你真的想成为坏消息的传递者吗？你真的希望所有人都开始质疑你的工作吗？**

**从客观的角度来看，这个难题的答案很简单。当然，你要评估你的 AI 系统是否不公平。但也不难想象这样一个场景，当人工智能工程师简单地开始一个新项目时，确信一切都很好。**

**为了对抗人工智能系统中的不公平，我们需要人工智能工程师把他们的可信度放到线上，并提出困难的问题。我设计的制度真的公平吗？是否有一些弱势群体受到我的解决方案的负面影响？问这样的问题是发现一个人工智能系统是否公正的第一步。如果没有人大声问这个问题，人工智能中的公平甚至无法实现。**

# **4 —即将出现的解决方案？**

**足够的厄运和黑暗。与其更多地关注人工智能中不公平的困难，我想谈一谈过去几年人工智能公平方面的进展。负责任的人工智能的一般领域在过去几年里得到了更多的关注，这可能是由于人工智能系统中几个现实生活中的不公平现象的出现。**

**最近的一项进展是引入了用于评估人工智能系统的偏见和公平性的开源软件。其中之一是[费尔勒恩](https://fairlearn.org/)。它旨在检测和评估人工智能系统中的偏差。Fairlearn 强调，人工智能系统中的公平既是一个社会学问题，也是一个技术问题。这里是 Python 中其他人工智能公平工具的[集合。在未来，我希望像 Fairlearn 这样的工具会被更多地用来批评人工智能系统的公平性。](https://techairesearch.com/most-essential-python-fairness-libraries-every-data-scientist-should-know/)**

**另一个积极的消息是，许多公司开始更加认真地对待人工智能(以及整个负责任的人工智能)中的不公平。在微软 Azure 入门认证 [*微软认证:Azure AI 基础*](https://learn.microsoft.com/en-us/certifications/azure-ai-fundamentals/) 他们已经将负责任的 AI 原则作为关键技能之一纳入其中。通过这样做，更多的人知道人工智能中的不公平，并可以在必要时提出困难的问题🔥**

# **5 —总结**

**![](img/f5dbcffb73fb3afc21037b9f3a42193c.png)**

**照片由[斯潘塞·伯根](https://unsplash.com/@spencerbergen?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上拍摄**

**希望你现在理解了人工智能公平性的一些挑战。如果你对数据科学、编程或任何介于两者之间的东西感兴趣，那么请随意在 LinkedIn 上加我，并向✋问好**

****喜欢我写的？**查看我的其他帖子，了解更多 Python 内容:**

*   **[用漂亮的类型提示使你罪恶的 Python 代码现代化](/modernize-your-sinful-python-code-with-beautiful-type-hints-4e72e98f6bf1)**
*   **[用 Python 可视化缺失值非常简单](/visualizing-missing-values-in-python-is-shockingly-easy-56ed5bc2e7ea)**
*   **[使用 PyOD 在 Python 中引入异常/异常值检测🔥](/introducing-anomaly-outlier-detection-in-python-with-pyod-40afcccee9ff)**
*   **5 个能在紧要关头救你的牛逼数字功能**
*   **[5 个专家提示，让你的 Python 字典技能一飞冲天🚀](/5-expert-tips-to-skyrocket-your-dictionary-skills-in-python-1cf54b7d920d)**