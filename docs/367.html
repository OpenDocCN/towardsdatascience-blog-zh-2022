<html>
<head>
<title>Serve hundreds to thousands of ML models — architectures from industry</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">服务数百到数千个ML模型——来自行业的架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serve-hundreds-to-thousands-of-ml-models-architectures-from-industry-bf3d9474d427#2022-01-13">https://towardsdatascience.com/serve-hundreds-to-thousands-of-ml-models-architectures-from-industry-bf3d9474d427#2022-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ffd6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""><h1 id="be18" class="pw-post-title iy iz iq bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">服务数百到数千个ML模型——来自行业的架构</h1></div><p id="cfad" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">当您只有一两个模型要部署时，您可以简单地将您的模型放在一个服务框架中，并将您的模型部署在几个实例/容器上。然而，如果您的ML用例增长或者您在您的数据的许多部分上构建单独的模型(像每个客户的模型)，您可能最终需要服务于大量的模型。这篇文章将探讨这个问题:如何设计一个可以实时服务数百甚至数千个模型的系统？</p><p id="6cbf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">第一，当你需要部署成百上千的在线模型时，会有什么变化？TLDR:更加自动化和标准化。通常，这意味着对模型服务平台的投资。</p><p id="cd42" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">像Reddit、DoorDash、Grab和Salesforce这样的公司已经建立了模型服务平台来:</p><ol class=""><li id="f902" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt kz la lb lc bi translated">满足他们的规模需求，包括模型数量和每秒请求数。</li><li id="7caf" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">让整个组织的ML团队更容易部署模型。</li><li id="d337" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">提供像影子模式、逐步展开和监控这样的功能。</li><li id="b07f" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">确保模型服务的一致SLA，这可能涉及到实现对平台上所有模型都有利的优化。</li></ol><p id="b69f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">尽管每个组织都独立地构建了自己的系统，但是高层设计惊人地相似。下面是我综合的一个总体设计。</p><p id="8deb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这篇文章将主要关注服务于大量模型的模型服务架构，而不会涉及其他方面，如模型CI/CD或比较服务框架。</p><h1 id="857c" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">开发者体验</h1><p id="7129" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">让我们从数据科学家/ML工程师如何在平台上部署新模型开始。他们</p><ol class=""><li id="1a40" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt kz la lb lc bi translated">将训练好的模型保存到模型存储/注册表中。</li><li id="9e23" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">提供一个模型配置文件，其中包含模型的输入特性、模型位置、需要运行的内容(比如对Docker映像的引用)、CPU和内存请求以及其他相关信息。</li></ol><p id="095b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">基本上——这是我的模型工件，以及如何使用它。这两个步骤可以通过内部工具来简化。在自动化再培训的情况下，部署新模型甚至可能是自动化的。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/c3e30d554dcd6becffe45993bec5eade.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*0u-itU2IioT2Gc25sep6rQ.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图1:平台用户将模型工件和模型配置存储到模型服务平台期望的地方。作者图解。</p></figure><h1 id="616f" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">服务架构</h1><p id="7fa2" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">一旦平台有了模型工件及其配置，它就处理模型加载、特性获取和其他关注点。一般服务系统的高级组件如图2所示。为了简单起见，我假设所有服务都作为容器部署。</p><p id="ad6e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在一般情况下，对推理服务的请求可能要求来自多个模型的多个预测。如图2所示，服务请求的步骤如下:</p><ol class=""><li id="4be4" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt kz la lb lc bi translated"><strong class="jy ja">读取请求</strong>中模型的模型配置。然后使用配置来确定要获取的特性。</li><li id="af87" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated"><strong class="jy ja">从特征库中检索特征</strong>。有些功能可能包含在请求中，不需要提取。如果功能存储中缺少某个功能或该功能过于陈旧，请使用默认值。</li><li id="3329" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated"><strong class="jy ja">向各自的模型发送特征以获得预测</strong>。</li><li id="dc43" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated"><strong class="jy ja">将预测返回给客户端</strong>。</li></ol><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/88aa462a605b42629806b653666eeb82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3oubb0dRRiCQ-BleWSSgGA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图2:支持许多模型的模型服务平台的架构。重叠的矩形显示组件可以水平缩放。作者图解。</p></figure><p id="3629" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">架构的组成部分</strong>:</p><ul class=""><li id="dce4" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt nc la lb lc bi translated"><strong class="jy ja">推理服务</strong> —提供服务API。客户端可以向不同的路线发送请求，以从不同的模型获得预测。推理服务统一了跨模型的服务逻辑，并提供了与其他内部服务更容易的交互。因此，数据科学家不需要考虑这些问题。此外，推理服务调用ML服务容器来获得模型预测。这样，推理服务可以专注于I/O相关的操作，而模型服务框架则专注于计算相关的操作。每组服务都可以基于其独特的性能特征进行独立扩展。</li><li id="72b1" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated"><strong class="jy ja">型号配置</strong> —来自平台用户。可以从另一个服务中读取某个模型的配置，或者在推理服务启动时将所有配置加载到内存中。</li><li id="e28b" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated"><strong class="jy ja">特征存储</strong> —是一个或多个包含特征的数据存储。</li><li id="4207" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated"><strong class="jy ja"> ML serving framework </strong> —是一个容器，其中包含了运行某种模型所需的所有依赖关系。例如，具有Tensorflow服务的特定版本的容器。容器可以服务于一个或多个模型。为了确保图像的可重用性，模型通常不会被放入容器图像中。更确切地说，容器在启动时或者收到对模型的请求时加载模型。一段时间不活动后，模型可能会从内存中被逐出。</li><li id="fc14" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated">模型商店——听起来确实如此。这可以是像AWS S3这样的对象存储，服务可以通过模型注册服务与它交互。</li></ul><h2 id="f8c4" class="nd lj iq bd lk ne nf dn lo ng nh dp ls kh ni nj lw kl nk nl ma kp nm nn me iw bi translated">图中未显示</h2><p id="a4f2" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">图2并不全面，仅代表核心组件。所有生产系统都应该对运营指标进行监控。此外，模型度量可能会流入另一个系统进行漂移检测。可能有一个组件将业务逻辑应用于模型预测，例如，增加产品推荐的排名。每个企业都需要确定自己的需求。</p><h2 id="fee7" class="nd lj iq bd lk ne nf dn lo ng nh dp ls kh ni nj lw kl nk nl ma kp nm nn me iw bi translated">系统存在于何处？</h2><p id="ab9c" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">上面提到的许多组织选择在Kubernetes上部署平台。一些原因:</p><ul class=""><li id="85de" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt nc la lb lc bi translated">它帮助你管理大量的容器！</li><li id="f8a2" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated">资源虚拟化。容器获得资源请求和限制，所以没有人可以独占所有的资源，这对于从同一个实例服务多个模型很有用。</li><li id="5c41" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt nc la lb lc bi translated">基于CPU利用率和其他指标的水平自动扩展易于配置。</li></ul><p id="4b59" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">您可能不会选择Kubernetes进行容器编排，但您可能会想要类似的东西。</p><p id="bff3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">举个例子，Reddit的服务平台是这样的:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fddddfbc09f32ff0df2cd69a060ed9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*08jgaSxJmfZpH4WNUaSOHA.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图3: Reddit的推理服务，Gazette。来源:[2]。该架构类似于图2。绿色方框用于张量流模型。他们使用Memcached Daemonset作为缓存来加速特征检索。</p></figure><h2 id="e666" class="nd lj iq bd lk ne nf dn lo ng nh dp ls kh ni nj lw kl nk nl ma kp nm nn me iw bi translated">扩展以服务更多型号</h2><p id="9168" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">首先，最简单的方法是为每组ML服务框架容器提供一个模型。图4是一个例子。欺诈模型在一组XGBoost容器中，定价模型在一组TorchServe容器中，推荐者模型在另一组TorchServe容器中。有了一个好的容器编排框架，您可以扩展到数百个模型。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi np"><img src="../Images/8b8e46106c13caf5d0b11ed56c4e9cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YI1fyzi3rYMOy_gmaHj8Tg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图4:每套ML服务容器一个模型。作者图表</p></figure><p id="d85d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">随着时间的推移，团队产生了更多的模型和同一模型类型的多个版本，这导致了许多容器。您可能会倾向于为每个容器提供多个模型，因为:</p><ol class=""><li id="e22c" class="ku kv iq jy b jz ka kd ke kh kw kl kx kp ky kt kz la lb lc bi translated">您想要更高的资源利用率。如果定价模型是空闲的，仍然有2个或更多的运行容器来确保模型是可用的。这些容器占用了所请求的资源量。但是，如果TorchServe容器同时服务于推荐模型和定价模型，您可以使用更高百分比的资源。当然，代价是你可能会有吵闹的邻居，其中一个模型变慢了，因为另一个正在争夺资源。</li><li id="b290" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">你预计你会遇到一些容器数量的限制。Kubernetes有每个集群的限制，AWS弹性容器服务也是如此。或者，可能集群是共享的，服务平台的容器数量有限制。</li></ol><p id="4571" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">一些像Tensorflow Serving和TorchServe这样的框架支持本地服务多个模型[1，5]。Salesforce Einstein为每个容器提供多个模型，因为他们需要为超级大量的模型提供服务。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nq"><img src="../Images/43c0148f9d33b564df52973e6bfe5499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrAtJOFdOyvxhQJLyXQnKw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图5:每组ML框架容器的多个模型。XGBoost容器服务于同一模型类型的两个版本。TorchServe容器同时服务于定价和推荐模型。作者图解。</p></figure><h1 id="1f5f" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">最佳化</h1><p id="a8d5" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">如果图2中的系统性能不够好，有很多优化的机会。</p><p id="2f72" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">首先是添加缓存。您可以在要素存储前添加缓存，以加快要素检索。如果服务框架仅在有请求时将模型加载到内存中，那么第一个请求可能会很慢。加快速度的一个方法是在模型存储之前添加一个缓存，就像共享文件系统一样。如果您经常收到相同的预测请求，您甚至可以为模型预测添加缓存。</p><p id="30f6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">另一个优化是用C++服务模型。DoorDash的服务平台支持lightGBM和PyTorch模型，并且都在C++中运行[7]。</p><p id="6d64" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">如果某些应用程序需要一次预测多个输入，请求批处理会很有用。例如，在单个请求中对100个推荐项目进行排名。一些服务框架支持批处理多个预测请求，以获得更好的吞吐量。在GPU上运行神经网络时，批处理预测可能特别有益，因为批处理可以更好地利用硬件。</p><h1 id="d035" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">阴影模式和模型卷展栏</h1><p id="5d33" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">实现影子模式的一种方法是将一定比例的生产模型请求转发给影子模型。发送请求而不等待响应，并设置一个监视系统，将阴影模型输入和输出写到一个离线存储，如雪花或AWS红移。然后，数据科学家可以离线分析模型性能。给予影子模型比生产模型更低的优先级以防止停机是至关重要的。在Kubernetes中，您可以指定pod调度和驱逐的优先级[4]。</p><p id="2efb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">推出新版本模型的安全方法是首先将一小部分生产流量转移到新版本。有两种方法可以做到这一点。第一种是让客户选择使用哪个版本的模型。他们可以调用不同的API路由，或者在请求中包含模型版本。另一种让平台选择的方法是——开发人员在模型配置或其他配置文件中指定向新模型发送多少流量。然后，推理服务负责将正确百分比的请求路由到新模型。我更喜欢后一种方法，因为客户不需要改变他们的行为来使用新的模型。</p><h1 id="f710" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">服务于超大量的模型</h1><p id="b713" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">Salesforce有一个独特的使用案例，他们需要服务100K-500K模型，因为Salesforce Einstein产品为每个客户构建模型。他们的系统在每个ML服务框架容器中服务多个模型。为了避免嘈杂的邻居问题，并防止一些容器比其他容器承担更多的负载，他们使用shuffle sharing[8]为容器分配模型。我就不赘述了，推荐看他们在[3]的精彩呈现。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi nr"><img src="../Images/df87b4431170b5550882e2680d4df8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pL3v27Blm0c46dYj"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图Manoj Agarwal关于Salesforce Einstein如何为模型服务的演示截图。Manoj在这张幻灯片上解释了洗牌分块。“路由器”服务是推理服务。“版本管理”服务告诉“路由器”要服务哪个版本的模型。特征被发送到AutoML 2.0容器进行预测。来源:[3]。</p></figure><h1 id="1392" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论</h1><p id="6028" class="pw-post-body-paragraph jw jx iq jy b jz mg kb kc kd mh kf kg kh mi kj kk kl mj kn ko kp mk kr ks kt ij bi translated">我们已经讨论了一个被多个组织使用的架构，它服务于大量的模型，并为整个组织中的团队提供了一条铺平的道路。我确信有替代设计，因为任何设计的细节都取决于组织的需求。请让我知道我应该考虑的其他架构！</p><h1 id="b9d1" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">参考</h1><ol class=""><li id="93ab" class="ku kv iq jy b jz mg kd mh kh ns kl nt kp nu kt kz la lb lc bi translated">"12.运行TorchServe — PyTorch/Serve主文档。<em class="nv"> PyTorch </em>，py torch . org/serve/server . html # serving-multi-models-with-torch serve。访问时间为2022年1月11日。</li><li id="7091" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">霍夫曼加勒特。"发展Reddit的ML模型部署和服务架构."<em class="nv">Reddit</em>2021年10月4日<a class="ae nw" href="http://www.reddit.com/r/RedditEng/comments/q14tsw/evolving_reddits_ml_model_deployment_and_serving" rel="noopener ugc nofollow" target="_blank">www . Reddit . com/r/RedditEng/comments/q 14 tsw/evolving _ reddits _ ml _ model _ deployment _ and _ serving</a>。</li><li id="1380" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">马诺基·阿加瓦尔。“以低延迟大规模提供ML模型// Manoj Agarwal // MLOps Meetup #48。”<em class="nv"> YouTube </em>，由MLOps.community上传，2021年1月22日<a class="ae nw" href="http://www.youtube.com/watch?v=J36xHc05z-M" rel="noopener ugc nofollow" target="_blank">www.youtube.com/watch?v=J36xHc05z-M</a>。</li><li id="c760" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">更多，普拉东斯。"猫步:大规模服务于机器学习模型——走向数据科学."<em class="nv">中</em>，2021年12月12日，朝sdadascience . com/catwalk-serving-machine-learning-models-at-scale-221d 1100 aa2b。</li><li id="9e87" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">"调度、抢占和驱逐."2022年1月11日，kubernetes.io/docs/concepts/scheduling-eviction.<em class="nv">库伯内特斯</em>进入。</li><li id="a10f" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">"张量流服务配置| TFX | . "<em class="nv"> TensorFlow </em>，<a class="ae nw" href="http://www.tensorflow.org/tfx/serving/serving_config#model_server_configuration." rel="noopener ugc nofollow" target="_blank">www . tensor flow . org/tfx/serving/serving _ config # model _ server _ configuration。于2022年1月11日访问。</a></li><li id="a182" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">曾，科迪。"认识Sibyl——door dash的新预测服务——了解它的构思、实施和推广."<em class="nv"> DoorDash工程博客</em>，2020年10月6日，door dash . Engineering/2020/06/29/door dash-new-prediction-service。</li><li id="ba48" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">左，奎因。“在贝宝部署大规模欺诈检测机器学习模型。”<em class="nv">Medium</em>2022年1月4日Medium . com/paypal-tech/machine-learning-model-ci-CD-and-shadow-platform-8c4f 44998 c78。</li><li id="e5a0" class="ku kv iq jy b jz ld kd le kh lf kl lg kp lh kt kz la lb lc bi translated">MacCárthaigh，Colm。"使用混洗分片的工作负载隔离."<em class="nv">亚马逊网络服务公司</em>，AWS . Amazon . com/builders-library/workload-isolation-using-shuffle-sharding。于2022年1月13日访问。</li></ol></div></div>    
</body>
</html>