# MLOps 的最基础层—所需的基础架构

> 原文：<https://towardsdatascience.com/the-most-fundamental-layer-of-mlops-required-infrastructure-bafd111db4c3>

## 拥有实施 MLOps 解决方案的合适基础架构

在我之前的帖子中，我讨论了构建端到端 MLOps 解决方案的三个关键组件，即数据和功能工程管道、ML 模型培训和再培训管道 ML 模型服务管道。你可以在这里找到文章:[学习 MLOPS 的核心——构建 ML 管道。](/learn-the-core-of-mlops-building-machine-learning-ml-pipelines-7242b77520b7?source=friends_link&sk=0a3006eed886f1071082ac1b5a485785)在我上一篇文章的最后，我简要地谈到了一个事实，即 MLOps 解决方案的复杂性可能会因 ML 项目的性质而有很大的不同，更重要的是，取决于所需的底层基础设施的变化。

因此，在今天的帖子中，我将解释不同级别的基础架构是如何要求的，如何确定 MLOps 解决方案的复杂性，以及如何将 MLOPS 解决方案分为不同的级别。

更重要的是，在我看来，将 mlop 分为不同的级别可以使任何规模的组织更容易采用 mlop。原因是，并不是每个层次的 MLOps 都需要 Kubernetes 这样的大规模在线推理基础设施，Apache Spark 这样的并行和分布式数据处理框架，以及结构化流和 Apache Flink 这样的低延迟和流数据管道解决方案。因此，拥有小规模数据集和批量推理 ML 项目的组织不需要招聘具有这些专业技能的人员，也不需要设置复杂的底层存储和计算基础架构，但仍然可以利用现有的技能组合和大大简化的基础架构来正确执行 MLOps。

对于每个级别，我将在以后的博客中分享一些参考架构和实现指南。如果你想在这些新博客发表时得到通知，请随时关注我。

首先，让我们讨论一下运行端到端 MLOps 解决方案所需的基础架构，包括 3 个关键组件:

*   数据和特征工程管道；
*   ML 模型培训管道；
*   ML 模型推理管道；

我将介绍每个组件可能需要的所有基础设施。然后，我将根据所需的基础设施将它们分为不同的级别。

![](img/9bbd792021ab53e5324f41be497f406d.png)

由[瑞安·昆塔尔](https://unsplash.com/@ryanquintal?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

## 数据和特征工程管道所需的基础设施

根据数据量和数据延迟，运行数据和特征工程管道所需的基础设施如下:

*   第 1 级—当数据量可以由单台机器处理，并且数据延迟为批量频率时，所需的基础架构可以像本地笔记本电脑或公共云上的虚拟机一样简单。此外，您可以利用云平台即服务(PaaS)产品，如 AWS Batch、AWS Lambda 或 Azure 功能，进一步简化基础架构管理；
*   第 2 级—当数据量无法由单台机器处理且需要并行和分布式数据处理，但数据延迟仍可保持批处理频率时，所需的基础架构将需要超越单台机器成为计算集群，以便安装和管理 Apache Spark 等分布式计算框架。Apache Spark 是一个开源解决方案。组织可以运行自己的计算集群，并使用开源 Spark 来管理他们的数据和功能工程管道。但是，大多数企业仍然选择托管服务(如 Databricks)作为大规模数据和功能工程工作负载的底层数据基础架构。公共云提供商也为 Spark 提供服务，如 AWS EMR 和 GCP 数据处理。
*   级别 3 —在前两个场景中，数据延迟保持在批处理级别。然而，当数据延迟需要非常低时，就需要完全不同的基础设施集合。至少需要一个事件驱动的消息队列和一个流引擎。为了实现更低的延迟，通常需要消息队列服务来动态捕获流数据，而不是将数据保存到存储系统中。对于消息队列服务，有开源的解决方案，比如 Apache Kafka 还有商业托管服务，比如 Azure Event Hub、AWS Kinesis 数据流和 Confluent。除了消息队列服务之外，为了实现下游数据消耗的低频率，健壮的流引擎也是非常必要的。开源流媒体引擎包括 Apache Spark 结构化流媒体和 Apache Flink 以及 Apache Beam。当然，也有针对流媒体引擎的商业产品，如 Databricks、AWS Kinesis 数据分析以及 GCP 数据流。

如您所见，根据数据量和数据延迟要求，运行数据和功能工程管道的基础设施可能会有很大差异。实际上，这对于 ML 模型训练管道和 ML 模型推理管道都是相同的。这就是为什么在基础设施级别进行澄清是至关重要的，以避免 MLOPS 的印象(或误解)是 ***总是*** 令人生畏且复杂。对于某些级别，MLOps 也可以非常简单，我将在这篇博客的后面解释。现在让我们继续解释运行 ML 模型训练管道所需的基础设施。

## ML 模型培训管道所需的基础设施

根据训练数据的大小和将训练好的模型用于生产环境所需的时间(SLA ),模型训练的基础结构可以划分如下:

*   级别 1 —当训练数据大小适合单台计算机的内存，并且总训练时间不超过生产环境所需的 SLA 时，使用单台计算机进行模型训练就足够了。根据训练数据的格式，可能需要一台 GPU 机器。比如你的训练数据是结构化的，数值型的，一般一个 CPU 机器就够了。然而，如果您的训练数据是非结构化的，如图像，首选的训练基础设施将是 GPU 机器。
*   第 2 级—当训练数据太大而无法容纳在单台机器的内存中，或者即使训练数据大小可以容纳在单台机器的内存中，但完成一项训练工作所需的时间长于所需的 SLA 时，这是公司需要启动训练集群以跨多个节点进行并行和分布式 ML 模型训练的时候。然而，在多个节点上运行分布式 ML 模型训练引入了许多新的复杂性，例如跨多台机器调度任务、高效地传输数据以及从机器故障中恢复。幸运的是，已经有一些开源库来处理多节点训练带来的这些额外的复杂性，并使数据科学家的训练工作相对简单，即使他们需要分发这些工作。这些开源库包括用于扩展 Python ML 工作负载的 Ray，用于 TensorFlow、Keras、PyTorch 和 Apache MXNet 的分布式深度学习培训框架的 Horovod，以及用于扩展 Python 库(包括 Python ML 库)的 Dask。

我将单独发表一篇关于分布式培训的博客。如果您想在发布博客时得到通知，请随时关注我。

众所周知，ML 是一个极其动态的领域。要运行一个模型训练工作，数据科学家需要安装相当多的开源库，包括 Pandas、Numpy、Matplotlib、Seaborn、Plotly、Scikit Learn、Tensorflow、Keras、Pytorch、mlflow 等等。因此，大多数公共云供应商或特定的数据+人工智能供应商(如 Databricks)提供预配置的 ML 运行时，包括所有这些流行的 ML 库，以节省数据科学家安装和维护这些库的大量时间。因此，大多数组织通过利用云服务来构建他们的 ML 培训基础设施。云上流行的 ML 服务有 AWS Sagemaker、Azure 机器学习工作空间、GCP Vertex AI 以及 Databricks 机器学习运行时。

## ML 模型推理管道所需的基础设施

根据模型推理频率和推理请求量，运行 ML 模型推理管道所需的基础设施如下:

*   级别 1-当模型推理频率是批量的，并且用于模型推理的数据量能够由一台单独的机器处理时，通过对数据调用预测函数，可以将训练的模型加载到用于批量预测的单独的机器中，该数据通常存储为 Pandas 数据帧；
*   第 2 级—当模型推理频率为批处理，但数据量无法在单台机器内管理时，需要建立一个集群来利用分布式计算框架，如 Apache Spark。例如，经过训练的 ML 模型可以作为 Spark 用户定义函数(UDF)加载，并且可以将 UDF 应用于 Spark 数据帧以进行并行模型预测。
*   级别 3 —当模型推理频率是低延迟的，并且数据量相当大时，流推理就变得必要了。与第 2 级类似，也需要一个计算集群。此外，还需要使用流引擎进行模型预测，以满足低延迟要求。在这种情况下，使用的流行流媒体引擎是 Apache Spark 和 Apache Flink 的结构化流媒体。
*   第 4 级—当模型推理在线时，这意味着模型通常被打包为 REST API 端点，但 API 请求量很小，可以由单台机器处理，所需的基础架构通常是云上的单节点 CPU 虚拟机。公共云提供商和数据/人工智能供应商都有针对此类模型服务的托管服务。例如，Databricks 有无服务器端点，客户不必担心设置服务基础设施，他们需要做的只是实例化一个模型端点。其他公司也有类似的产品。

在我们进入第 5 级之前，需要注意一点——在线推理不同于流式推理，在流式推理中，经过训练的 ML 模型仍然作为 Python 函数加载，而不是作为 REST 端点。它们都有低延迟，但是在线推理应该是实时的。

*   级别 5 —当模型推理是在线的，并且 API 请求量是大规模的(意味着每秒查询数(QPS)对于单个端点来说是非常大的)时，就需要设置一个像 Kubernetes 这样的集群基础设施来进行分布式推理。通常使用的流行方法是，将训练好的模型打包成容器映像，并在容器注册表中注册，如 AWS 弹性容器注册表、Azure 容器注册表或 GCP 容器注册表。然后，这些经过训练的模型的注册图像将被拉入并部署到 Kubernetes 中，用于大规模和分布式模型推理。每个公共云都有自己的托管 Kubernetes 服务。

## 结论

到目前为止，我们已经涵盖了完整的端到端 MLOps 解决方案的 3 个关键管道中的每一个所需的不同级别的基础设施。很明显，不同级别的基础架构的复杂性有很大不同。

在下一篇博客中，我将根据基础设施的复杂性和实现模式将 MLOps 分为不同的级别，对于每个级别，我还将分享一些参考架构和代码示例，其中将包括 MLOps 解决方案的其他部分，如编排、模型版本控制、数据版本控制、漂移检测、数据质量检查和监控。

我希望你喜欢阅读这篇博客。如果你想在有新博客发表时得到通知，请随时关注我。

如果想看到更多围绕现代高效数据+AI 栈的指南、深度潜水、见解，请订阅我的免费简讯— [***高效数据+AI 栈***](https://yunnawei.substack.com/) ，谢谢！

注:以防万一你还没有成为媒体会员，并希望获得无限制的媒体访问，你可以使用我的[推荐链接](https://medium.com/@weiyunna91/membership)注册！我可以免费给你一点佣金。非常感谢你的支持！