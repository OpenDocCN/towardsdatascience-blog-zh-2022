<html>
<head>
<title>Liar’s Dice by Self-Play</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">靠自我发挥的骗子的骰子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lairs-dice-by-self-play-3bbed6addde0#2022-01-09">https://towardsdatascience.com/lairs-dice-by-self-play-3bbed6addde0#2022-01-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="edc3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""><h1 id="4abf" class="pw-post-title iy iz iq bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">靠自我发挥的骗子的骰子</h1></div><div class=""><h2 id="5269" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用反事实后悔和神经网络</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0821d11739ab5ce7f78ae24436c05614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJxxOprc21FlJcB1YL71qw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">与人工智能对战是如此有趣，以至于我不得不创建一个小网站来与世界分享它。—<a class="ae le" href="https://dudo.ai/" rel="noopener ugc nofollow" target="_blank">dudo . ai</a>截图。作者图片</p></figure><p id="7561" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一段时间以来，我一直想学习像扑克或骗子骰子这样的<a class="ae le" href="https://en.wikipedia.org/wiki/Perfect_information" rel="noopener ugc nofollow" target="_blank">游戏的人工智能。最近在读Deepmind的文章</a><a class="ae le" href="https://arxiv.org/abs/2112.03178" rel="noopener ugc nofollow" target="_blank">游戏玩家</a>时，我想我也许能做出一些真正简单的东西，足够好，足够有趣。“应该不会超过一个下午，”我想。当然，我最终在这上面花了更多的时间，但这确实很有趣，也很简单。</p><p id="4395" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个项目让我能够深入到反事实后悔最小化、强化学习、在浏览器中提供PyTorch模型和一些其他有趣的主题的令人兴奋的概念中，所以在这篇博客文章中有一堆事情要涵盖。</p><h1 id="6a53" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">骗子的骰子怎么玩？</h1><blockquote class="mu"><p id="f6fd" class="mv mw iq bd mx my mz na nb nc nd ma dk translated">我明白。这是一个欺骗的游戏。但是你的赌注包括所有的骰子，而不仅仅是你自己的。——威尔·特纳，《加勒比海盗》<a class="ae le" href="https://pirates.fandom.com/wiki/Liar%27s_Dice" rel="noopener ugc nofollow" target="_blank"/></p></blockquote><p id="f8b2" class="pw-post-body-paragraph lf lg iq lh b li ne ka lk ll nf kd ln lo ng lq lr ls nh lu lv lw ni ly lz ma ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Liar%27s_dice#Single_hand" rel="noopener ugc nofollow" target="_blank">骗子的骰子</a>是我个人喜欢在我们还能去酒吧的时候玩的游戏。它比扑克简单，但有许多相同的下注、虚张声势和隐藏信息的概念，这使它成为人工智能的一个有趣挑战。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/325e642b9fcf2c8484ca15fbf421d6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*FqGbtvl-uITLXaJK6ZokQQ.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">供六个人玩的骗子的骰子，有用来隐藏的骰盅。在这篇博文中，我们只考虑2人版本。<a class="ae le" href="https://www.flickr.com/photos/dicemanic/12902825/" rel="noopener ugc nofollow" target="_blank">图片由Dicemanic </a> (CC BY 4.0)</p></figure><p id="9e80" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">游戏也是从<a class="ae le" href="https://pirates.fandom.com/wiki/Liar%27s_Dice" rel="noopener ugc nofollow" target="_blank">加勒比海盗</a>开始出名的，不过不了解游戏的话我会快速勾画一下规则。</p><ul class=""><li id="5935" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma np nq nr ns bi translated">每个玩家都有一套只有他们自己知道的骰子。玩家轮流对玩家认为显示给定值的最小骰子数出价。</li><li id="a39e" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">每回合，玩家必须对任何特定的牌面叫牌数量更高，或者对更高的牌面叫牌数量相同。例如，“2 ⚃”和“3 ⚂”都是“3 ⚁”之后的有效赌注，但“2 ⚂”不是。</li><li id="fb25" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated">或者，玩家可以挑战先前的出价。如果挑战成功——例如，先前的出价是虚张声势——挑战者赢得这一轮；否则，下注的玩家获胜。</li></ul><p id="36d0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，⚀'s被认为是爱开玩笑的人，在最后合计骰子点数时，他会考虑任何一张脸。</p><p id="9c88" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">爱丽丝和鲍勃可能有⚀ ⚁和⚂ ⚃.的手爱丽丝出价“2 ⚁”(她可以独自用自己的手支撑)，鲍勃说“2 ⚂”(预计爱丽丝有一张⚂或⚀(百搭))。艾丽丝打电话给鲍勃，但是鲍勃是对的并且赢了。</p><h1 id="445a" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">理论是什么</h1><p id="a556" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">解释这个算法需要一点理论——尽管它最终非常简单。如果您想直接跳到神经网络部分或模型的ELO评级和<em class="mb">可开发性</em>评估，请随意跳过以下三个部分。</p><blockquote class="mu"><p id="fb21" class="mv mw iq bd mx my mz na nb nc nd ma dk translated">为什么不完全信息博弈如此困难？毕竟，我们有深蓝，我们有AlphaGo。为什么我们不能把这些技术直接应用到像扑克这样的游戏中呢？— <a class="ae le" href="https://www.youtube.com/watch?v=tRiaGahlyy4" rel="noopener ugc nofollow" target="_blank">诺姆·布朗·尼普斯2017年最佳论文演讲</a>。</p></blockquote><p id="c778" class="pw-post-body-paragraph lf lg iq lh b li ne ka lk ll nf kd ln lo ng lq lr ls nh lu lv lw ni ly lz ma ij bi translated">骗子的骰子是一个有限的游戏:你只能掷出有限的骰子组合，以及有限的下注顺序。(我们称这种组合为<em class="mb">状态，</em>从技术上来说，它通常被称为<em class="mb">信息集</em>)然而，与像国际象棋这样的<a class="ae le" href="https://en.wikipedia.org/wiki/Perfect_information" rel="noopener ugc nofollow" target="_blank">完全信息博弈</a>不同，我们无法为每种状态计算出单一的完美走法(或<em class="mb">动作</em>)。相反，<a class="ae le" href="https://en.wikipedia.org/wiki/Nash_equilibrium" rel="noopener ugc nofollow" target="_blank">如约翰·冯·诺依曼和约翰·纳西</a>所示，我们需要一个“混合策略”:对于每个状态，我们从一组动作的<strong class="lh ja">概率分布</strong>中选择一个动作。</p><p id="56c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">直觉上，这是因为有一个固定的策略会泄露我们私人骰子的信息。如果我们在掷骰子时总是下大注，我们的对手就会明白这一点，开始跟我们虚张声势。如果我们一半时间虚张声势，另一半时间稳扎稳打，我们的对手就更难识破我们。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/25ef26dd6be8bf66be2c3495661d3a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J-dJIJihI23eV0Jq"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">据报道，约翰·冯·诺依曼在T4曼哈顿计划T5和其他科学家玩扑克时发明了现代博弈论。照片由<a class="ae le" href="https://unsplash.com/@keenangrams?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Keenan Constance </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e407" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">纳什均衡可以使用<a class="ae le" href="https://en.wikipedia.org/wiki/Linear_programming" rel="noopener ugc nofollow" target="_blank">线性规划</a>来计算，例如<a class="ae le" href="http://www.sciencedirect.com/science/article/pii/089982569290035Q" rel="noopener ugc nofollow" target="_blank">达芙妮·柯勒和尼姆罗德·迈吉多</a>的方法，该方法在状态数(或更常见的术语“信息集”)上花费多项式时间。)几年前我其实就是用这个方法<a class="ae le" href="https://en.wikipedia.org/wiki/Linear_programming" rel="noopener ugc nofollow" target="_blank">解开骗子的骰子的。这是可能的，因为每个人有1个骰子，有意义的最大出价是2 ⚅'s.，总共有12个可能的出价。结合骰子信息，那就是6*2 ~40万个状态。这只是现代线性代数解算器可能做到的事情，比如谷歌或者工具。</a></p><p id="b269" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不幸的是，这种方法不能扩展到多个骰子。每个6个骰子，我们得到大约10个⁶状态，这相当于国际象棋中的<a class="ae le" href="https://github.com/tromp/ChessPositionRanking" rel="noopener ugc nofollow" target="_blank">总合法位置。虽然国际象棋理论上可以通过简单的深度优先搜索来解决——在状态数中采用恒定的空间和线性时间——但骗子的骰子要难得多。这就是为什么人工智能在1998年可以很好地下棋，但直到最近才在扑克上达到同样的水平。</a></p><h1 id="6e12" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">反事实后悔最小化</h1><p id="33a8" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">好了，是时候我开始解释这个人工智能实际上是如何工作的，以及机器学习是如何发挥作用的。</p><blockquote class="mu"><p id="d499" class="mv mw iq bd mx my mz na nb nc nd ma dk translated">在过去，很多人会通过最大化你的收益来模拟决策。你想尽最大努力赢得比赛，对吗？但是人们发现你真正想做的是尽量减少你的遗憾。— <a class="ae le" href="https://www.cardplayer.com/poker-news/25778-team-polk-s-bryan-pellegrino-talks-about-his-ai-research-and-how-it-helped-formulate-strategies-to-win-1-2-million" rel="noopener ugc nofollow" target="_blank"> <em class="oe">布莱恩·佩莱格里诺</em> </a>、<em class="oe"> </em>扑克职业和人工智能研究员</p></blockquote><p id="7729" class="pw-post-body-paragraph lf lg iq lh b li ne ka lk ll nf kd ln lo ng lq lr ls nh lu lv lw ni ly lz ma ij bi translated">解决不完美信息博弈的算法被称为反事实后悔最小化。我在这篇文章的结尾添加了一些相关链接，但在我的休闲版中，它是这样工作的:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/6febbe3b4064f848d94852d3b4ba50d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7T4nXBvhXIsR0Try"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我们应该生活和玩耍来最大化我们的快乐还是最小化我们的遗憾？马科斯·保罗·普拉多在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><ol class=""><li id="0d6e" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma og nq nr ns bi translated">如果开始的玩家赢了，给所有的最终状态(其中一个玩家称另一个玩家说谎)赋值+1，如果第二个玩家赢了，赋值-1。</li><li id="1811" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">对于所有其他状态，我们将保持一个值<em class="mb"> v(state) </em>(在-1和1之间)，该值近似于在通过给定状态的先前游戏中观察到的<em class="mb">平均值</em>。<br/> <em class="mb">(在真实的CFR中，这些值应该由玩家进入状态的概率进行反向加权，但为了简单起见，由于使用Adam和momentum来训练神经网络已经有点乱了加权，所以我跳过了这一部分。)</em></li><li id="154f" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">定义“反事实遗憾”<em class="mb"> r(state，I)= v(state+I)</em>—<em class="mb">v(state)</em>，这是如果我们在给定的状态下每次都采取行动<em class="mb"> i </em>，我们的得分(平均)会差(或好)多少。<br/>(此处<em class="mb">状态+i </em>是采取动作<em class="mb"> i </em>后的结果状态。)</li><li id="aaf5" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">最后，通过以下方式定义策略(在给定状态下选择动作<em class="mb"> i </em>的概率)</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/d2dbae4cf81a1175d1adbfacd5e229e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKHIAKbVGRqOm6-_-UUxCw.png"/></div></div></figure><p id="753c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中r <em class="mb">(状态)</em><em class="mb">= max(r(状态)，0)。这确保了不存在负概率。</em></p><p id="16f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总是玩具有最高期望值<em class="mb"> v(state+i)的动作可能很有诱惑力。然而，<em class="mb"> </em>这将是一种确定性策略，很可能被我们的对手所利用。相反，随着我们对状态值的估计越来越精确，上述算法(称为后悔匹配)肯定会给出纳什均衡。</em></p><h1 id="16f0" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">蒙特卡洛和神经网络的懒惰</h1><p id="cdaf" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">在真实反事实后悔最小化(简称CFR)中，我们通过在整个博弈树上运行深度优先搜索并存储迄今为止看到的平均分数，来反复改进我们的<em class="mb"> v(状态)</em>估计。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9dc7975d59987376345d9463447c43bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*qgyMAFdL0RhMocoltPgaog.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">蒙特卡洛近似法可用于测量物体的面积，方法是对随机点进行采样，并计算感兴趣的物体内部有多少陆地。类似地，通过重复游戏，它可以接近游戏的期望值。图片由<a class="ae le" href="https://commons.wikimedia.org/wiki/User:CaitlinJo" rel="noopener ugc nofollow" target="_blank">凯特琳·乔·拉姆齐</a></p></figure><p id="ed92" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这有两个主要问题:</p><ol class=""><li id="7f54" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma og nq nr ns bi translated">它用了太多的时间。</li><li id="11cb" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">它占用了太多的空间。</li></ol><p id="d124" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了解决时间问题，我们用游戏样本代替深度优先搜索(也称为蒙特卡罗CFR)。基本上，我们不用更新每个子状态——包括那些概率为0的动作——我们只需玩一个游戏，然后根据游戏结果用+1或<strong class="lh ja">—</strong>1更新游戏树中的路径。换句话说，我们在做强化学习。</p><p id="8936" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了确保我们探索所有的选项，我们在玩的时候增加了一些噪音。噪音还教会了人工智能利用对手的糟糕举动，因为它不是在对手总是做出完美选择的情况下训练的。理想情况下，我们应该减少训练期间随着时间增加的噪音量——如果我们想最终获得纳什均衡的话——但我没有这样做。</p><p id="751e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了解决内存问题，我们不存储每个可能状态的精确平均分数。相反，我们使用神经网络<em class="mb"> </em>来计算平均值<em class="mb"> v(状态)的良好近似值。</em>为了模拟CFR更新规则，我们使用1/T的学习率，其中T是当前时间步长。这确保了<em class="mb"> v(state) </em>学习训练游戏的平均值，而不仅仅是最近的结果。<em class="mb">(我实际上尝试了其他学习率，比如1/sqrt(T)和1/T，令人惊讶的是1/T是唯一一个收敛到纳什均衡附近的学习率。)</em></p><p id="5c12" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">完整的训练代码可以在<a class="ae le" href="https://github.com/thomasahle/liars-dice/blob/main/code/train.py" rel="noopener ugc nofollow" target="_blank">github.com/thomasahle/liars-dice/blob/main/code/train.py</a>中找到，而最关键的部分——自我游戏代码——只有27行。</p><p id="3a9f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">代码将三元组(私有状态、公共状态、游戏结果)保存到一个名为<em class="mb"> replay_buffer </em>的列表中，该列表被传递给PyTorch用于训练神经网络(在<em class="mb"> game.sample_action </em>中调用)。在一个梯度下降的步骤之后，我们用改进的策略玩另一批游戏并重复。</p><h1 id="02b6" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">模型</h1><p id="ee9a" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">神经网络是一个函数<em class="mb"> v(private，public) </em>，它返回一个介于<strong class="lh ja">-</strong>1和1之间的值。我们可以用很多方法来定义它，所以我只选择了我能想到的最简单的方法，它看起来足够好了。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/bc79c0c7ddb8abc0fe1420f45c59ca77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVlieAfwZ4ec6ChZS16zaA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我能想到的最简单的模型是:私有和公共状态被编码、组合，并通过几个完全连接的层传递。作者图片</p></figure><p id="0087" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">私人输入是玩家手牌的一个6 <strong class="lh ja"> × </strong>“骰子数”的一键表示。我们可以把它看作是每个面上有多少骰子的直方图。该编码取自<a class="ae le" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank">dou zero</a>【2021】。</p><p id="ccb2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由于在两个玩家的游戏中，我们最多可以调用2个<strong class="lh ja"> × </strong>“骰子数”，<br/>公共状态是2个<strong class="lh ja"> × </strong> 6个<strong class="lh ja"> × </strong> (2个<strong class="lh ja"> × </strong> num-dice)的一键表示，表示每个玩家到目前为止所做的调用。</p><p id="12d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">每个独热输入被嵌入为500维向量，并且两者相乘。其他的池化方式，比如连接，也可以工作得很好，但是元素式乘法模拟了<a class="ae le" href="https://en.wikipedia.org/wiki/Tensor_sketch#:~:text=Compact%20multilinear%20pooling%5Bedit%5D" rel="noopener ugc nofollow" target="_blank">压缩双线性池</a>，<a class="ae le" href="https://thomasahle.com/#paper-tensorsketch-joint" rel="noopener ugc nofollow" target="_blank">我个人倾向于</a>。</p><p id="ccae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在结合了私有和公共状态之后，我们运行4个完全连接的层，100个单元宽，在每层中具有校正的线性单元。最终，我们减少到一个标量值，使用tanh映射到[1，+1](像AlphaZero一样。)</p><p id="d3c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">训练损失简单来说就是MSE: ( <em class="mb"> v(私，公)——game _ result)。再次复制AlphaZero。</em></p><h1 id="ffad" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">有用吗？</h1><p id="1db1" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">有时候我会打败它。有时候我也不知道。说谎者的骰子是一个涉及很多机会的游戏，需要很多游戏才能确定谁是更好的玩家。所以我们来玩很多游戏吧！</p><p id="0a4a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在第一周在<a class="ae le" href="https://dudo.ai" rel="noopener ugc nofollow" target="_blank"> dudo.ai </a>与人在线对战时，人工智能赢了5031回合，输了4416回合。这相当于骰子点数的<strong class="lh ja"> 53.3% </strong>胜率。骗子骰子游戏由多轮不同数量的骰子组成。AI赢了846场完整的比赛，输了640场，对应于56.9%的胜率。</p><p id="61f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以使用经典的ELO公式将这些百分比转换成评级差异:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/285d79b4dd27a65034eff7550f8d5dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cp3ANpMurfljTYfcVtrKsg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">玩家1战胜玩家2的概率被认为是他们的评级e1和e2之间的差的函数。</p></figure><p id="9a09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">加上上面的胜率，人工智能比人类对手有23到48个ELO点的优势。</p><p id="304f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一个有趣的测试是观察算法在自我对抗时提高的速度。我建立了一个锦标赛，不同版本的模型与其他版本的模型进行10，000场比赛。然后我用最大似然估计来计算每个玩家的ELO。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/514310b69decc4d6109f070814391503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVT4NdpVID2OMi9p3stSUA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">不同迭代的n骰子模型的ELO，假设迭代1000的评级为0。作者图片</p></figure><p id="4c83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">模型显然在开始时改进得更快，但它们都在长时间内保持改进。将5骰子模型训练到230，000次迭代大约需要24小时。训练本身不需要太多时间，但是玩23，000，000个游戏需要一段时间，因为我没有实现任何并行化。</p><p id="8b88" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有趣的一点是，ELO通常用于不需要运气的游戏，比如国际象棋。我们可以在ELO公式中加入如下的偶然性因素:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/03edc32842926f112efaf8453af28572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEvLhdNtBUS9HlfLg0jxPg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ELO公式假设骗子的骰子是概率为r的技巧游戏，概率为1-r的纯运气游戏</p></figure><p id="4762" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们使用这个公式来估计不同版本之间的比赛的ELO收视率，结果是最好地解释数据的模型具有<em class="mb"> r </em> ~ 0.20。这意味着骗子的骰子大约80%是纯粹的运气。🤷</p><h1 id="0018" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">它能工作吗？2 —可开发性</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/08b8eaa265fb6e3dfa5ea4a4c04fea11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_f8aYNSyA8u_TgCu7IMGGw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我对1v1模型进行了3次训练，并计算了不同迭代的可开发性。粗线表示运行的中值可开发性，虚线表示理论最小值。作者图片</p></figure><p id="2c95" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有一个更有原则的方法来计算我们的人工智能离纳什均衡有多近。这被称为测量战略的<em class="mb">可开发性</em>。</p><p id="4ae8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可利用性是任何人在对抗人工智能时能够赢得的最大数量<em class="mb">，即使知道人工智能使用的概率是多少，并且拥有无限的计算能力。</em></p><p id="4a4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">实际的CFR保证最终产生一个不可利用的策略。我们不能期望我们的方法有那么好，因为神经网络甚至不能代表这个完美的策略。然而，如果我们接近0，这意味着人工智能客观上非常好，而不仅仅是在线上与自己和人类玩得很好。</p><p id="48a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">计算可利用性是通过计算对手的最优反应策略来完成的。我们告诉我们的对手我们的策略(我们每个状态的概率)，这个策略将游戏转变成一个<a class="ae le" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程</a> (MDP)。对手可以解决这个问题(在深度优先搜索的线性时间内)，MDP的值是任何对手使用任何策略都无法战胜我们的。</p><p id="4da1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然可利用性可以在状态数中以线性时间计算，但对于具有4个或更多骰子的游戏来说，这仍然太昂贵了。然而，我设法计算了1v1的可利用性(上图)，以及1v2和2v1(下图)。巧合的是，这也是我5年前设法计算出<a class="ae le" href="https://github.com/thomasahle/snyd" rel="noopener ugc nofollow" target="_blank"/>(使用线性规划)的精确纳什均衡的游戏。结果如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/012c108c2438359c253de1e184392711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOPOUOsnCjRr_7fuT6qqeQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">1v2和2v1游戏中的可利用性，其中玩家1有1个骰子，玩家2有2个骰子，反之亦然。在这两种情况下，玩家2的可利用性都相当好(低)，尽管不是最低可能。然而，当人工智能是参与人1时，它是令人惊讶地可利用的。例如，在1v2中，玩家2有优势，我们应该仍然只能赢得大约54%的比赛(每场0.09分)，但有一种策略可以在近65%的时间里击败它(每场0.3分)。作者图片</p></figure><p id="9b12" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">显然，我的模型很擅长学习参与人2的最佳策略。这里的可开发性离最佳值不远。另一方面，在这两个游戏中，参与人1的策略都没那么好。似乎人工智能更喜欢在做出自己的决定之前判断人类的呼唤。</p><p id="d968" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是否意味着作为一号玩家玩好游戏要困难得多？在1v1设置中，这似乎不是一个问题…是不是有一个bug🐛在我的算法、架构或最佳反应开发器中的某个地方，只会为参与人1提供更多骰子？如果我想通了，我会更新这个帖子。</p><p id="680f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">总的来说，该算法似乎很好地学习了骗子的骰子&amp;非常快。鉴于代码和架构如此简单，我对此感到非常鼓舞。它当然足够强大，可以在大部分时间里赢过网上的人。<a class="ae le" href="https://github.com/thomasahle/liars-dice" rel="noopener ugc nofollow" target="_blank">请随意使用Github </a>上的代码，让它变得更好！</strong></p><h1 id="c5ff" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">为模型服务</h1><p id="e067" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">我认为只有终端界面不会鼓励我的许多朋友去测试这个模型。我需要一些基于浏览器的东西，但是我不想创建一个完整的服务器/客户机来运行这个模型。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="b39a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，我发现了关于<a class="ae le" href="https://github.com/microsoft/onnxruntime" rel="noopener ugc nofollow" target="_blank"> ONNX </a>，尤其是埃利奥特·韦特的这个伟大的视频。在两行代码中，PyTorch模型可以转换成onnx文件，可以直接从javascript中使用:</p><pre class="kp kq kr ks gt op oq or os aw ot bi"><span id="7eee" class="ou md iq oq b gy ov ow l ox oy">torch.onnx.export(model, path=’out.onnx’)</span></pre><p id="1b09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我创建了一个小的静态javascript前端，买了一个域名，你现在可以在<a class="ae le" href="http://dudo.ai" rel="noopener ugc nofollow" target="_blank"> dudo.ai </a>上与之对战。</p><h1 id="78f6" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">附录</h1><p id="9b20" class="pw-post-body-paragraph lf lg iq lh b li ny ka lk ll nz kd ln lo oa lq lr ls ob lu lv lw oc ly lz ma ij bi translated">这篇文章的灵感来自于Erik Bernhardsson的深度学习...2014年的象棋。除了Deepmind的游戏玩家，我还从其他令人兴奋的扑克论文中获得了灵感，如DouZero、DeepStack和ReBeL。</p><p id="988c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更高级的方法有两个主要改进:</p><ol class=""><li id="3215" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma og nq nr ns bi translated">他们使用两个神经网络而不是一个:一个网络(像我们的一样)用于预期的游戏价值，另一个预测最佳政策/概率分布。由于另一项创新，第二个网络可以训练得特别好:</li><li id="49b5" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">他们使用<a class="ae le" href="https://arxiv.org/abs/2111.05884" rel="noopener ugc nofollow" target="_blank">搜索</a>。搜索意味着给定状态的策略(概率分布)随着给AI时间“思考”而迭代改进。经典游戏(如国际象棋)的人工智能一直都是这样做的，但在CFR中，策略完全是在训练时计算的，游戏中更多的“思考时间”对人工智能没有帮助。直到最近几年才发现如何将这一点应用到类似《骗子的骰子》这样的游戏中，方法比我以为一个下午就能做到的要复杂得多。</li></ol><p id="aab2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有关CFR的更多信息，您可以查看</p><ol class=""><li id="9f18" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma og nq nr ns bi translated">原文<a class="ae le" href="https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf" rel="noopener ugc nofollow" target="_blank">不完全信息博弈中的后悔最小化</a>摘自NIPS 2007。</li><li id="3104" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">来自NIPS 2009的<a class="ae le" href="http://mlanctot.info/files/papers/nips09mccfr_techreport.pdf" rel="noopener ugc nofollow" target="_blank">蒙特卡洛版本</a>。</li><li id="6fa1" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">一篇关于<a class="ae le" href="https://openreview.net/forum?id=ByebT3EYvr" rel="noopener ugc nofollow" target="_blank">深度反事实后悔最小化</a>的更有原则性的论文，来自ICLR 2020。</li><li id="86a7" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma og nq nr ns bi translated">2021年<a class="ae le" href="https://link.springer.com/book/10.1007/978-3-030-60990-0" rel="noopener ugc nofollow" target="_blank">强化学习和控制手册</a>中关于基于政策的方法的12.4.2.2章节。</li></ol><p id="ffeb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">感谢<a class="ae le" href="https://twitter.com/kgourg" rel="noopener ugc nofollow" target="_blank"> Kosti </a>阅读本文的早期草稿，感谢Troels Bjerre Lund批评我轻率地使用CFR术语。</p></div></div>    
</body>
</html>