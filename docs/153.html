<html>
<head>
<title>Stop Explaining Black-Box Machine Learning Models For High Stakes Decisions And Use Interpretable Models Instead</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止解释高风险决策的黑盒机器学习模型，而是使用可解释的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions-and-use-interpretable-771298d1f3a7#2022-01-05">https://towardsdatascience.com/stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions-and-use-interpretable-771298d1f3a7#2022-01-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4ab7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习中的可解释性</h2><div class=""><h1 id="11d1" class="pw-post-title iy iz iq bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">停止解释高风险决策的黑盒机器学习模型，而是使用可解释的模型</h1></div><div class=""><h2 id="7b38" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">理解模型解释前后的区别</h2></div><p id="d973" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">深度学习模型通常被视为黑盒。这是因为他们对于如何达到预测的方式并不透明。人类无法直接解释具有数百万个参数的模型。选择无知会导致不可预见的危险。这本质上是一种不好的做法，应该尽可能减少。当前的深度学习可解释性工具旨在将过程简化为结果，但并没有真正解释模型遵循的“思维过程”。这对于医疗保健等高风险企业尤为重要。我们不应该试图开发解释机器学习模型的工具，而是应该致力于开发内在可解释的模型。当我们使用工具来解释模型时，我们正是这样做的:解释模型。我们不解释世界是如何运转的。</p><p id="058d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对可解释模型的第一个批评是性能。可解释的模型比不可解释的模型准确性低，这是不对的。通过正确的预处理，两个模型可以达到相同的精度。移除更复杂模型的某些部分以实现可解释性当然会降低准确性，但这并不代表真实世界的分析。模型的开发应该考虑到可解释性，而不是事后的。</p><p id="e0b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">自然可解释的模型可以帮助纠正数据，揭示错误的假设。最佳预测器是通过迭代过程构建的。否则，垃圾进垃圾出的风险会增加。黑匣子似乎发现的“隐藏模式”可能并不真实。这些模式可能来自数据工件。然而，如果它们是真实的信号，一个可解释的模型应该能够正确地提取和使用它们。使用可解释工具可能会产生误导。使用正确的标签来解释模型的预测路径隐藏了工具的一些工件。例如，显著性图为同一幅图像提供了相似的解释，而不考虑图像的类别(图1)。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/da14f3248ee0d8e26a7cbfd99c21ba9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WmT4LarZ19S1PU7P.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">基于类别的显著图判别[1]</p></figure><p id="d03a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">黑盒更难复制，因此可以作为商业模式的护城河。可以开发一个重症监护室死亡率的模型。如果那个模型是黑匣子，可以卖给医院。然而，如果这是一个有10个参数的回归，赚钱可能会更难。如果医院购买黑盒解决方案，他们会假设模型训练所依据的数据与医院的数据相似。这意味着人口遵循与输入数据相似的年龄和性别分布，医生遵循相似的诊断和治疗程序。如果这些假设不成立，该模型很可能无法达到类似的精度。该模型将对与其被训练的情况大相径庭的情况做出预测。</p><p id="2952" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就引发了一场问责讨论。人工智能提供商不想为他们模型的预测失败负责。这导致了不同于假设的动机。该公司的目标是致力于晦涩和适销性，而不是模型的简单性。私人模型的表现应该用真实世界的数据来更新，而不仅仅是用训练数据来更新，以真实地反映实际表现。</p><p id="316f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有人可能会说，开放模型可以被逆向工程。但这应该不是问题。首先，如果这个模型可以被游戏化，那么它就不是被正确设计的。第二，一个真正的系统整合可以带来更好的结果。让我们考虑一家预测死亡率的保险公司。如果你戒烟，你的死亡风险会降低，你的保险费也会降低。恭喜你成功地玩了这个模型！那是件坏事吗？还是双赢？</p><p id="333e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">透明模型很难构建。它们需要领域知识，并且非常特定于应用程序。通常更容易的做法是将所有东西放入一个盒子中，并尽可能优化最高性能。与计算相比，领域专家分析师的成本很高。</p><p id="c71a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[1] <em class="ma">鲁丁，C. (2019)。停止解释高风险决策的黑盒机器学习模型，而是使用可解释的模型。自然机器智能，1(5)，206–215。</em></p><p id="05b1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你喜欢吗？我也在我的 <a class="ae mb" href="https://rocreguant.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">个人博客</em> </a> <em class="ma">上发表。你有问题吗？在</em><a class="ae mb" href="https://twitter.com/rocreguant" rel="noopener ugc nofollow" target="_blank"><em class="ma">Twitter</em></a><em class="ma">或</em><a class="ae mb" href="https://www.linkedin.com/in/rocreguant/" rel="noopener ugc nofollow" target="_blank"><em class="ma">LinkedIn</em></a><em class="ma">上打我。这个故事最初发表在</em><a class="ae mb" href="https://rocreguant.com/stop-explaining-black-box-machine-learning-models-for-high-stakes-decisions-and-use-interpretable-models-instead-paper-summary/2084/" rel="noopener ugc nofollow" target="_blank">https://rocreguant . com/stop-explaining-black-box-machine-learning-models-for-high-stages-decisions-and-use-interpretable-models-instead-paper-summary/2084/</a></p></div></div>    
</body>
</html>