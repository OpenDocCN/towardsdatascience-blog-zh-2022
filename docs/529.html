<html>
<head>
<title>Deep Learning Reviews the Literature</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习回顾文献</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-reviews-the-literature-3abd90027c90#2022-01-18">https://towardsdatascience.com/deep-learning-reviews-the-literature-3abd90027c90#2022-01-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="aef2" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">深度学习回顾文献</h1></div><div class=""><h2 id="290d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用变形器对复杂文本进行分类的指南</h2></div><p id="dbe0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由<a class="lb lc ep" href="https://medium.com/u/e393a0d6684?source=post_page-----3abd90027c90--------------------------------" rel="noopener" target="_blank"> Jaren R Haber </a>(乔治城大学)<a class="lb lc ep" href="https://medium.com/u/1865a1d2f0b2?source=post_page-----3abd90027c90--------------------------------" rel="noopener" target="_blank"> Thomas Lu </a>(加州大学伯克利分校)<a class="lb lc ep" href="https://medium.com/u/9c797a119b78?source=post_page-----3abd90027c90--------------------------------" rel="noopener" target="_blank"> Nancy Xu </a>(加州大学伯克利分校)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/cbbb51be597807450bd553c202a3fb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MKVnCBEY-65vqhvW"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lt" href="https://unsplash.com/@jareddc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jaredd Craig </a>拍照</p></figure><p id="6192" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于研究人员和科学家来说，有太多的东西需要阅读:更多的博客文章、期刊文章、书籍、技术手册、评论等等。等。比以往任何时候都多。然而，我们必须阅读，因为从过去学习和避免重复发明轮子——或者更糟的是，爆胎——的主要方法是通过历史悠久的文献综述。当我们的书桌和书架上堆满了知识，提供了太多的内容让一个人消化不了的时候，我们如何才能跟上邻居的步伐——也就是说，学术的最新发展及其随着时间的推移而发生的许多变化？</p><p id="f8ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数字时代最受欢迎的消遣来了:计算机阅读东西，或者自然语言处理<em class="lu"> </em> (NLP)。我们提供NLP工具和示例代码来帮助解析数量呈指数级增长的学术期刊文章——尤其是交叉于学术领域的跨学科研究。作为一个案例研究，我们开发了阅读和跟踪一个特定的跨学科领域的方法:社会学和管理学学科中的组织理论。我们使用深度学习来帮助大规模阅读这些文献，我们称之为<em class="lu">计算文献综述</em>的管道。</p><p id="173b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在JSTOR 提供的约70，000篇期刊文章<a class="ae lt" href="https://about.jstor.org/whats-in-jstor/text-mining-support/" rel="noopener ugc nofollow" target="_blank">的语料库中，我们具体追踪了现代社会学和管理学研究</a>中的三大视角——人口统计学、关系学和文化学。我们的模型通过利用一套强大的NLP工具:预训练的Transformer模型来确定在学术文本中使用这些视角中的哪些(如果有的话)。变形金刚可以处理大量文本，使用<a class="ae lt" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力层</a>和神经网络在高层次上“理解”其内容，并生成有用的输出，如预测。它们是用于训练机器学习分类器的最先进、细致入微的工具，我们构建这些工具是为了帮助审阅学术文献。</p><p id="45b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">读完这篇博客后，你应该能够:</p><ul class=""><li id="09ed" class="lv lw iq kh b ki kj kl km ko lx ks ly kw lz la ma mb mc md bi translated">了解我们在数字文本语料库中跟踪想法的工作流程，从预处理到模型训练</li><li id="5ec9" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">比较几种深度学习算法在获取组织理论方面的性能</li><li id="6949" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">想象一下随着时间的推移，学术文本中的观点是如何转变的</li><li id="5118" class="lv lw iq kh b ki me kl mf ko mg ks mh kw mi la ma mb mc md bi translated">找到(并在此基础上构建！)我们的计算文献综述代码</li></ul><h1 id="7ca7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">工作流程</h1><p id="787c" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们团队中的专家组织学者手工编写了大约700篇文章，为每个观点的出现提供了基础事实。我们使用这些带标签的训练数据，在来自JSTOR的预处理过的全文数据上训练基于Transformer的分类器。这些模型为组织理论中的三个观点输出二元预测，我们根据交叉验证的准确性选择了最好的一个。然后，我们对未标记的数据进行预测，并分析社会学和管理学学科中每个角度的趋势。</p><p id="0e95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标是对学术文章进行分类，这些文章平均长度为7640个单词。然而，传统的基于变压器的模型通常无法处理长序列:<a class="ae lt" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>依赖于<a class="ae lt" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">自我关注</a>，这与序列长度成二次比例，并且在处理更长的序列时产生难以维持的计算需求。这个限制在这里很重要，因为我们的分析单位通常是长篇文章。此外，BERT预先接受了ebook scorpus(800万字)和英语维基百科(2500万字)的培训，这可能会限制其在科学文献领域的可移植性。出于这些原因，我们将我们的关注点缩小到两种变形的Transformer模型，以及用于比较的BERT本身:SciBERT和Longformer。<a class="ae lt" href="https://arxiv.org/abs/1903.10676v3" rel="noopener ugc nofollow" target="_blank"> SciBERT </a>基于BERT，但在与我们的语料库更相似的学术期刊文章上进行训练:具体来说，它的语料库是语义学者的114万篇论文(18%来自计算机科学，82%来自生物医学领域)。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c28fcdeeb4d48e346069b1aa601e35f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*9KGBv0rC3rBdYUe_"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">我们的计算文献综述工作流程。作者图片。</p></figure><p id="480c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相比之下，<a class="ae lt" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank"> Longformer </a>是一个基于Transformer的模型，在与BERT相同的原始数据集上训练，但它最多可以接受16K个令牌(与BERT和SciBERT的512个相反)。Longformer通过不执行所有可能的比较，而只执行与相关单词的比较，最大限度地减少了注意力的计算负担。Longformer提供了许多策略来做到这一点(见下图)，例如让大多数令牌只关注它们的邻居，而只有少数令牌关注每个令牌(并且每个令牌也关注它们)。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nh"><img src="../Images/3716a7cb3856caf5ab3e52bf14b14b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bq3_PTcb2aJeUUWbYSyycQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><em class="ni">变形金刚的高效注意力模式。资料来源:I. Beltagy，M. Peters，A. Cohan，</em><a class="ae lt" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank">Long former:The Long-Document Transformer</a>(<em class="ni">2020)，arXiv </em></p></figure><p id="5612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种更长的序列长度允许我们的深度学习分类器捕捉组织观点，这些观点不仅嵌入在学术文章的摘要或介绍中，还嵌入在文本的主体中(如理论部分)。我们用最大令牌长度1024(出于资源管理的原因)训练Longformer模型，远低于16K的限制。即便如此(剧透警告)，在我们的案例中，这种更长的序列长度被证明是Longformer的一个关键优势，与BERT相比，准确性提高了2%左右。</p><h1 id="9a58" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">我们工作流程的代码</h1><h2 id="ed8d" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">预处理</h2><p id="7ff1" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">因为像BERT这样的转换器利用句子的结构，并根据单词周围的单词(相当于每个单词的上下文唯一向量，这比像<a class="ae lt" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> word2vec </a>这样的单词嵌入更有优势)为单词分配意义，所以停用词和语法结构对于模型训练很重要。因此，我们对文本进行最低限度的预处理，用Apache Lucene tokenizer对文本进行标记，只删除HTML标记、页面格式标记和其他各种垃圾。以下简单的正则表达式替换清理了JSTOR用OCR对文章文本进行数字化后留下的页面格式和XML标签。我们完整的文本预处理代码可以在<a class="ae lt" href="https://github.com/h2researchgroup/classification/blob/main/preprocess/preprocess_article_text_full.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="f124" class="nj mk iq nw b gy oa ob l oc od">def remove_tags(article):<br/>    """<br/>    Cleans page formatting and XML tags from JSTOR articles.<br/>    """</span><span id="6dcf" class="nj mk iq nw b gy oe ob l oc od">    article = re.sub(‘&lt;plain_text&gt; &lt;page sequence=”1"&gt;’, ‘’, article)<br/>    article = re.sub(r’&lt;/page&gt;(\&lt;.*?\&gt;)’, ‘ \n ‘, article)<br/>    # remove xml tags<br/>    article = re.sub(r’&lt;.*?&gt;’, ‘’, article)<br/>    article = re.sub(r’&lt;body.*\n\s*.*\s*.*&gt;’, ‘’, article)</span><span id="e42c" class="nj mk iq nw b gy oe ob l oc od">    return article</span></pre><h2 id="aeda" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">过采样</h2><p id="b348" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">因为我们的二元分类器只预测两个类，所以我们的训练数据中的最佳类平衡将是1:1: 50%真实正例(本文中引用了视角)，50%真实负例(未引用视角)。然而，我们手工标注的数据中，阴性病例是阳性病例的两倍。为了解决训练和测试样本中的这种类不平衡，我们通过从少数类(正例)进行引导来进行过采样，以达到1:1的类比率。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="db81" class="nj mk iq nw b gy oa ob l oc od">from imblearn.over_sampling import RandomOverSampler<br/>from imblearn.under_sampling import RandomUnderSampler<br/><br/>def oversample_shuffle(X, y, random_state=52):<br/>    """<br/>    Oversamples X and y for equal class proportions.<br/>    """<br/>    <br/>    fakeX = np.arange(len(X), dtype=int).reshape((-1, 1))<br/>    ros = RandomOverSampler(random_state=random_state, sampling_strategy=1.0)<br/>    fakeX, y = ros.fit_resample(fakeX, y)<br/>    p = np.random.permutation(len(fakeX))<br/>    fakeX, y = fakeX[p], y[p]<br/>    X = X.iloc[fakeX.reshape((-1, ))]</span><span id="8cfe" class="nj mk iq nw b gy oe ob l oc od">    return X.to_numpy(), y.to_numpy()</span></pre><h2 id="dc41" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">列车测试分离</h2><p id="f962" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">模型训练需要单独的训练集和测试集来提供准确的预测。为了创建用于训练的数据集，我们执行训练-测试分割，然后对两个数据集进行过采样，以解决前面提到的类别不平衡问题。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="403c" class="nj mk iq nw b gy oa ob l oc od">def create_datasets(texts, scores, test_size=0.2, random_state=42):<br/>    """<br/>    Splits the texts (X variables) and the scores (y variables) <br/>    then oversamples from the minority class to achieve a balanced sample.<br/>    """ <br/>  <br/>    X_train1, X_test1, y_train1, y_test1 = train_test_split(<br/>        texts, scores, test_size=test_size, <br/>        random_state=random_state)</span><span id="246d" class="nj mk iq nw b gy oe ob l oc od">    X_train1, y_train1 = oversample_shuffle(<br/>        X_train1, y_train1, random_state=random_state)<br/>    X_test1, y_test1 = oversample_shuffle(<br/>        X_test1, y_test1, random_state=random_state)<br/>  <br/>    return X_train1, y_train1, X_test1, y_test1</span></pre><h2 id="0cfb" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">模特培训</h2><p id="5938" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们在HuggingFace Transformers库中的Longformer模型的基础上构建我们的分类器，并使用一个线性层将Longformer表示转换为每个标签的一组预测。下面是BERTClassifier类，它包含用于加载和初始化具有指定最大令牌长度的预训练long former模型的代码，以及用于模型训练和交叉验证的“get_batches()”、“forward()”和“evaluate()”等帮助函数。<a class="ae lt" href="https://github.com/h2researchgroup/classification/blob/main/modeling/Azure%20Files/Longformer-CV.py" rel="noopener ugc nofollow" target="_blank">这是模特训练的完整代码。</a></p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="efd7" class="nj mk iq nw b gy oa ob l oc od">class BERTClassifier(nn.Module):<br/>    """ <br/>    Initializes the max length of Longformer, the pretrained model, and its tokenizer.<br/>    """</span><span id="267f" class="nj mk iq nw b gy oe ob l oc od">    def __init__(self, params):<br/>        super().__init__()<br/>        self.tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')<br/>        self.bert = LongformerModel.from_pretrained("allenai/longformer-base-4096", gradient_checkpointing=True)<br/>        self.fc = nn.Linear(768, self.num_labels)</span></pre><p id="bb2c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们在“BERTClassifier”类中定义“get_batches()”辅助函数，该函数使用BERT标记器标记文本。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="3aa3" class="nj mk iq nw b gy oa ob l oc od">    def get_batches(self, all_x, all_y, batch_size=10):<br/>        """<br/>        Tokenizes text using a BERT tokenizer. <br/>        Limits the maximum number of WordPiece tokens to 1096.<br/>        """</span><span id="3a8d" class="nj mk iq nw b gy oe ob l oc od">        batch_x = self.tokenizer.batch_encode_plus(<br/>            x, max_length=self.max_length, truncation=True, <br/>            padding='max_length', return_tensors="pt")</span><span id="5d8b" class="nj mk iq nw b gy oe ob l oc od">        batch_y=all_y[i:i+batch_size]</span><span id="f4a5" class="nj mk iq nw b gy oe ob l oc od">        ## code omitted</span><span id="28d5" class="nj mk iq nw b gy oe ob l oc od">        return batches_x, batches_y</span></pre><p id="6fd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了运行该模型，我们在“BERTClassifier”中定义了一个“forward()”函数。这将获取模型输出，并通过完全连接的线性图层运行最后一个图层，以通过执行所有输出类别的softmax来生成预测，这在logit模型中很常见。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="ce3d" class="nj mk iq nw b gy oa ob l oc od">    def forward(self, batch_x):<br/>        """<br/>        Gets outputs from the pretrained model and generates predictions.<br/>        Note that Longformer is a RoBERTa model, so we don't need to pass in input type ids.<br/>        """    </span><span id="3bee" class="nj mk iq nw b gy oe ob l oc od">        # First, get the outputs of Longformer from the pretrained model<br/>        bert_output = self.bert(input_ids=batch_x["input_ids"],<br/>                        attention_mask=batch_x["attention_mask"],<br/>                        output_hidden_states=True)<br/>    <br/>        # Next, we represent an entire document by its [CLS] embedding (at position 0) and use the *last* layer output (layer -1).<br/>            bert_hidden_states = bert_output['hidden_states']<br/>            out = bert_hidden_states[-1][:,0,:]<br/>            if self.dropout:<br/>                out = self.dropout(out)<br/>            out = self.fc(out)</span><span id="f62b" class="nj mk iq nw b gy oe ob l oc od">        return out.squeeze()</span></pre><p id="a232" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们为“BERTClassifier”定义一个“evaluate()”函数，以基于批次y确定批次x的预测的准确性</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="41b2" class="nj mk iq nw b gy oa ob l oc od">    def evaluate(self, batch_x, batch_y):<br/>        """<br/>        Evaluates the model during training by getting predictions (batch x) and comparing to true labels (batch y).<br/>        """</span><span id="a11a" class="nj mk iq nw b gy oe ob l oc od">        self.eval()<br/>        corr = 0.<br/>        total = 0.</span><span id="816d" class="nj mk iq nw b gy oe ob l oc od">        # disable gradient calculation<br/>        with torch.no_grad():<br/>            for x, y in zip(batch_x, batch_y):<br/>        # for each batch of x and y, get the predictions<br/>        # and compare with the true y labels<br/>                y_preds = self.forward(x)<br/>                for idx, y_pred in enumerate(y_preds):<br/>                    prediction=torch.argmax(y_pred)<br/>                    if prediction == y[idx]:<br/>                        corr += 1.<br/>                    total+=1    <br/>                     <br/>       return corr/total</span></pre><p id="0bb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们训练模型并确定最佳超参数。我们使用了微软Azure的机器学习服务器来搜索超参数空间，并最大化交叉验证的准确性；下面是完成这项工作的完整代码。此代码中的主要部分是下面的“train_bert()”函数，它使用“create_datasets()”来准备训练和测试数据，然后从“BERTClassifier”类中“get_batches()”来获取用于模型构建的批。接下来，我们定义超参数变量、Adam优化器及其调度程序。然后，我们训练和评估该模型，记录它的最佳开发精度和时期，用于模型比较。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="e457" class="nj mk iq nw b gy oa ob l oc od">def train_bert(texts, scores, params, test_size=0.2):<br/>    """<br/>    This deep learning workhorse function trains a BERT model <br/>    with the given hyperparameters, <br/>    returning the best accuracy out of all epochs.<br/>    (Code is simplified for ease of reading.)</span><span id="9ace" class="nj mk iq nw b gy oe ob l oc od">    Inputs:<br/>        texts: a list of article texts for training<br/>        scores: a list of labels for the texts (0 or 1)<br/>        test_size: proportion of input data to use for model testing (rather than training)<br/>        params: a dict of parameters with optional keys like batch_size, learning_rate, adam_beta, weight_decay, max_memory_size, and num_epochs (full list omitted).<br/>    """</span><span id="59f1" class="nj mk iq nw b gy oe ob l oc od">    # cross entropy loss is used for binary classification<br/>    cross_entropy=nn.CrossEntropyLoss()<br/>    best_dev_acc = 0.<br/>    best_dev_epoch = 0.</span><span id="7e42" class="nj mk iq nw b gy oe ob l oc od">    for epoch in tqdm(range(num_epochs)): # loop over epochs<br/>        bert_model.train()<br/>        # Train model<br/>        optimizer.zero_grad() # Clear any previously accumulated gradients<br/>        for i, (x, y) in enumerate(zip(batch_x, batch_y)):<br/>            # Find the loss for the next training batch<br/>            y_pred = bert_model.forward(x)<br/>            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))<br/>            loss.backward() # Accumulate gradients for back-propagation</span><span id="91b0" class="nj mk iq nw b gy oe ob l oc od">            if (i + 1) % (batch_size // max_memory_size) == 0:<br/>                # Check if enough gradient accumulation steps occurred. If so, perform a step of gradient descent<br/>                optimizer.step()<br/>                scheduler.step()<br/>                optimizer.zero_grad()<br/><br/>       # Evaluate model and record best accuracy and epoch <br/>        dev_accuracy=bert_model.evaluate(dev_batch_x, dev_batch_y)<br/>        if epoch % 1 == 0:<br/>            if dev_accuracy &gt; best_dev_acc:<br/>                best_dev_acc = dev_accuracy<br/>                best_dev_epoch = epoch<br/>      <br/>    print("\nBest Performing Model achieves dev accuracy of : %.3f" % (best_dev_acc))<br/>    print("\nBest Performing Model achieves dev epoch of : %.3f" % (best_dev_epoch))</span><span id="40ed" class="nj mk iq nw b gy oe ob l oc od">    return best_dev_acc</span></pre><h2 id="cadd" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">模型评估</h2><p id="f441" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们用“cross_validate()”函数将上述所有内容联系在一起，该函数接受一个预处理文本列表、一个分类分数列表(如果文章是关于给定的视角，则为1，否则为0)和一个要优化的超参数列表。然后，该函数训练该模型并返回交叉验证精度，或者跨数据的几个(这里是5个)切片或<em class="lu">折叠</em>中的每一个的模型的平均精度。与单一训练/测试分割相比，<a class="ae lt" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank"><em class="lu">k</em>-折叠交叉验证</a>提供了一种更通用的模型性能测量方法，最大限度地降低了模型过度拟合输入数据的风险。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="dc78" class="nj mk iq nw b gy oa ob l oc od">def cross_validate(texts, scores, params, folds=5, seed=42):<br/>    """<br/>    Trains BERT model and returns cross-validation accuracy.<br/>    Communicates directly with Azure to run model <br/>    and record accuracy of given hyperparameters.<br/>    """<br/><br/>    np.random.seed(seed)<br/>    states = np.random.randint(0, 100000, size=folds)<br/>    accs = []<br/>    for state in states:<br/>        print(state)<br/>        accs.append(train_bert(<br/>            texts, scores, params, test_size=round(1/folds, 2),<br/>            random_state=state, save=False))<br/>    acc = np.mean(accs)<br/>    run.log('accuracy', acc) # works with Azure</span><span id="da62" class="nj mk iq nw b gy oe ob l oc od">    return acc</span></pre><p id="6474" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是我们代码的一个用法示例，使用“orgs”Pandas数据框架作为训练数据。每行是一篇社会学文章，列“全文”包含文章的全文，而“组织分数”表示文章是关于组织的(1)还是关于组织的(0)。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="7fe6" class="nj mk iq nw b gy oa ob l oc od">orgs['text_no_tags'] = [remove_tags(art) for art in orgs['full_text']]<br/>orgs = orgs[orgs['orgs_score']!=0.5]<br/><br/>cross_validate(orgs['text_no_tags'], orgs['orgs_score'],{'batch_size': 16, 'lr': 1e-4, 'dropout': 0.2, 'weight_decay':0.005, 'num_warmup_steps': 60})</span></pre><p id="7ba4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后可以使用Azure的专业机器学习仪表板来可视化结果。下图显示了我们训练的模型的超参数组合(批量大小、学习率、预训练学习率、重量衰减、热身步骤数和辍学率)。最精确的模型(紫色线)对除辍学率之外的所有超参数都有相对较低的值。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi of"><img src="../Images/0279f49658ff01459929b4490a0cbad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xQaKRWYD6S-PYZuA3lkhQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">跨超参数的模型性能。图片由作者提供，使用Microsoft Azure平台制作。</p></figure><p id="7d7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与BERT和SciBERT相比，我们发现基于Longformer的优化二元分类器在所有三个角度和组织社会学方面表现最佳。下表比较了我们测试的不同变压器型号的精度。最佳模型超参数见附录A，Longformer性能替代指标见附录B。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi og"><img src="../Images/c61845a803b94fcc3151100d10d71377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ilcYQSPU-y8EaHi3d68Jag.png"/></div></div></figure><h2 id="22bf" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">做出并可视化预测</h2><p id="1523" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">由于其卓越的性能，我们使用Longformer为65K左右尚未标记的JSTOR文章生成二元和连续预测(是/否以及参与每个视角的概率)。你可以在这里找到生成预测的完整代码。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="1bf0" class="nj mk iq nw b gy oa ob l oc od"><strong class="nw ir">def</strong> get_preds(model, batch_x, batch_y):<br/>    """<br/>    Using input model, outputs predictions (1,0) and <br/>    scores (continuous variable ranging from 0 to 1) for batch_x<br/>    Note that batch_y contains dummy labels to take advantage of pre-existing batching code<br/>    """</span><span id="c5eb" class="nj mk iq nw b gy oe ob l oc od">    model<strong class="nw ir">.</strong>eval()<br/>    predictions <strong class="nw ir">=</strong> []<br/>    scores <strong class="nw ir">=</strong> []</span><span id="d4e5" class="nj mk iq nw b gy oe ob l oc od"><strong class="nw ir">    with</strong> torch<strong class="nw ir">.</strong>no_grad():<br/>        <strong class="nw ir">for</strong> x, y <strong class="nw ir">in</strong> tqdm(zip(batch_x, batch_y)):<br/>            y_preds <strong class="nw ir">=</strong> model<strong class="nw ir">.</strong>forward(x)<br/>            <strong class="nw ir">for</strong> idx, y_pred <strong class="nw ir">in</strong> enumerate(y_preds):<br/>                predictions<strong class="nw ir">.</strong>append(torch<strong class="nw ir">.</strong>argmax(y_pred)<strong class="nw ir">.</strong>item())<br/>                sm <strong class="nw ir">=</strong> F<strong class="nw ir">.</strong>softmax(y_pred, dim<strong class="nw ir">=</strong>0)<br/>            <strong class="nw ir">try</strong>:<br/>                scores<strong class="nw ir">.</strong>append(sm[1]<strong class="nw ir">.</strong>item())<br/>            <strong class="nw ir">except</strong>:<br/>                print(len(scores))<br/>                print(sm<strong class="nw ir">.</strong>shape, sm)</span><span id="79ef" class="nj mk iq nw b gy oe ob l oc od"><strong class="nw ir">    return</strong> predictions, scores</span></pre><p id="5dfd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将JSTOR语料库分为两个主要主题(社会学和管理&amp;组织行为)，并从三个角度(人口统计、文化和关系)分析参与度。我们过滤掉了预测概率低于0.7的关于组织的社会学文章。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="16fd" class="nj mk iq nw b gy oa ob l oc od"><em class="lu"># filter to only organizational sociology articles using threshold of 0.7<br/></em>minority_threshold_orgs <strong class="nw ir">=</strong> 0.7<br/>df_orgsoc <strong class="nw ir">=</strong> merged[merged['primary_subject']<strong class="nw ir">==</strong>'Sociology'][merged['org_score'] <strong class="nw ir">&gt;</strong> minority_threshold_orgs]</span></pre><p id="d081" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们创建一个辅助函数“cal_yearly_prop()”来计算涉及每个视角的文章的年度比例。这有助于随着时间的推移，可视化参与每个视角的文章比例。</p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="ef56" class="nj mk iq nw b gy oa ob l oc od">def cal_yearly_prop(df_category):<br/>    """<br/>    For graphing purposes, calculate the proportion of articles <br/>    engaging a perspective (cultural, relational, or demographic) <br/>    for a given discipline (sociology or management studies) over years.</span><span id="d8a8" class="nj mk iq nw b gy oe ob l oc od">    Arg:<br/>        df_category: DataFrame of article predictions with columns ‘publicationYear’ indicating year published and ‘article_name’ as identifier</span><span id="1085" class="nj mk iq nw b gy oe ob l oc od">    Returns:<br/>        A list of proportion of articles engaging in a perspective over the years<br/>    """<br/>    <br/>    yearly_grouped = df_category.groupby('publicationYear').count()<br/>    yearly_total = list(yearly_grouped[‘article_name’])<br/>    years = list(yearly_grouped.index)</span><span id="7f12" class="nj mk iq nw b gy oe ob l oc od">    # get the yearly counts of articles <br/>    counts = df_category.groupby('publicationYear').count()[['article_name']].reset_index()<br/>    not_present = list(set(years) - set(counts.publicationYear))</span><span id="231d" class="nj mk iq nw b gy oe ob l oc od">    # create a dataframe for the article counts of missing years (0)<br/>    df2 = pd.DataFrame.from_dict({'publicationYear':not_present, 'article_name': [0]*len(not_present)})<br/>    <br/>    # concatenate the counts df with df of missing years, and divide the yearly counts by yearly total to get a list of yearly proportions<br/>    return np.array(pd.concat([counts, df2]).reset_index().sort_values('publicationYear')['article_name'])/yearly_total</span></pre><p id="b939" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的堆积条形图显示了一段时间内社会学和管理学文章比例的变化。这一指标的分子是在给定年份从事文化社会学、关系社会学或人口社会学的文章数量；分母是当年社会学的文章总数。管理研究中可视化趋势的代码非常相似。<a class="ae lt" href="https://github.com/h2researchgroup/classification/blob/main/modeling/longformer_sampling_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">你可以在这里找到可视化预测的完整代码。</a></p><pre class="le lf lg lh gt nv nw nx ny aw nz bi"><span id="19ee" class="nj mk iq nw b gy oa ob l oc od"># Visualize engagement across perspectives in organizational sociology<br/>relt_fig = plt.figure()<br/>relt_fig.set_figwidth(10)<br/>relt_fig.set_figheight(8)<br/><br/>yearly_grouped = df_orgsoc.groupby('publicationYear').count()<br/>years = list(yearly_grouped.index)<br/><br/>ax = plt.gca()</span><span id="b325" class="nj mk iq nw b gy oe ob l oc od"># the start and end years are omitted because they are outliers<br/>demog_fig = plt.bar(years[1:-1], demog_soc_minority_prop_os[1:-1], color = 'yellowgreen')<br/>cult_fig = plt.bar(years[1:-1], cult_soc_minority_prop_os[1:-1], bottom = demog_soc_minority_prop_os[1:-1], color = 'gold')<br/>relt_fig = plt.bar(years[1:-1], relt_soc_minority_prop_os[1:-1], bottom =  demog_soc_minority_prop_os[1:-1] + cult_soc_minority_prop_os[1:-1], color = 'lightblue')<br/><br/>plt.legend([ 'demog_soc','cult_soc', 'relt_soc' ])<br/><br/>plt.title('Changes in Proportion of Articles in Organizational Sociology from 1971 - 2015')</span></pre><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3f6dfbb20d4581055f34441e9ac3361d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Jp4-sWHN6zljW-KEAUrdUw.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">作者图片。</p></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9dd80643ea113339308cdf4390269f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*qxlb3LQiZHqWi8GG5SLLBA.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">作者图片。</p></figure><p id="7474" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，我们看到人口统计学的观点在(组织)社会学的文章中是最常见的，而关系学的观点在管理和组织行为学中是最常见的。人口统计<strong class="kh ir"> </strong>管理类文章从2010年开始逐渐下降，文化管理类文章在观察时间段内逐渐增长。所有其他类别多年来有所波动，但没有显示出随着时间的推移而普遍增长或下降。</p><h1 id="fcc7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">结论</h1><p id="832b" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们希望您喜欢了解我们的深度学习工作流，该工作流用于跟踪复杂文本中随时间变化的想法，并应用于我们对学术文章中的组织理论的案例研究。回顾:我们准备了一个完整的出版文献语料库，使用几个transformer模型来分析和预测文章内容，然后绘制跨时间和学科的特定理论的参与度。我们的总体目标是通过使用数据科学工具阅读和解释大量文本来提高文献综述的可访问性。我们认为变形金刚非常适合这种应用，激发我们的技术洞察力，将变形金刚应用于各种社会科学学科的学术文章——这是我们以前从未见过的练习。</p><p id="2c11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们希望您发现我们的演示很有趣，并对您自己的工作有所帮助。请关注我们未来探索其他采样方法、手工编码模型验证和更长序列长度的工作(关于这方面的具体想法，请参见附录C)。我们还计划分析引用模式和组织理论中不断变化的词汇代表观点。敬请期待，感谢阅读！</p><h1 id="2f37" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">附录</h1><h2 id="2b83" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">A.最佳模型超参数</h2><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oi"><img src="../Images/32426245338db430b125baaef908c837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAL5nPYVTxqN_fFS0u79jg.png"/></div></div></figure><p id="dba1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为增加输入序列长度的另一种方法，我们还利用BERT探索了“分块”技术:我们将文本分成更小的部分，评估每个块上的模型，并组合结果(<a class="ae lt" href="https://arxiv.org/pdf/1910.10781.pdf" rel="noopener ugc nofollow" target="_blank">引用</a>)。我们发现使用BERT进行分块和使用其默认的512个标记的序列长度之间没有显著差异。</p><h2 id="22c0" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">B.备选Longformer指标</h2><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oj"><img src="../Images/9c5f1d042e60a9b1e25893cab3651b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAIfeOZ41iJDTgC3nWYm7g.png"/></div></div></figure><h2 id="e55d" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">C.未来的工作</h2><h2 id="11ac" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">手动编码的模型验证</h2><p id="afc3" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">为了验证我们的模型，我们使用似然阈值来选择六组样本(跨两个学科的三个视角)，包括少数和多数类以及那些具有低置信度预测的类(用于可能的主动学习应用)。我们的专家合作者对这些样本进行手工编码将有助于验证模型，确保过去大量假阳性的经验不会重复。</p><h2 id="8350" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">分层抽样</h2><p id="58e4" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">在数据预处理过程中，除了对少数类进行过采样之外，使用分层采样可能是有益的。总的来说，从1970年到2005年，吸引我们观点的JSTOR文章总数稳步增加，在2005年达到顶峰，从2005年到2016年开始急剧下降。除了观点的潜在变化之外，上述文章频率和比例的变化可能是由于某些期刊改变了发表文章的数量。为了确保样本中代表了某些观点，JSTOR文章的总体可以根据出版年份和期刊等特征划分为子总体。然而，这也取决于JSTOR提供的文章数量。</p><h2 id="5c31" class="nj mk iq bd ml nk nl dn mp nm nn dp mt ko no np mv ks nq nr mx kw ns nt mz nu bi translated">长模型输入长度</h2><p id="c75c" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">由于时间约束和计算限制，我们用最大令牌长度= 1024来训练长成形器。然而，继续用更大的标记长度训练长成形器并观察准确性的任何变化将是有见地的。截取一个长文本的中间部分(<a class="ae lt" href="https://arxiv.org/pdf/1905.05583.pdf" rel="noopener ugc nofollow" target="_blank">参考</a>)也是值得探索的。此外，在预处理过程中，我们还可以删除摘要或元数据信息，如开头的作者姓名和电子邮件。</p></div></div>    
</body>
</html>