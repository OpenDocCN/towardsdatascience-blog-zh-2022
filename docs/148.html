<html>
<head>
<title>A Complete Guide to Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-guide-to-decision-trees-ac8656a0b4bb#2022-01-05">https://towardsdatascience.com/a-complete-guide-to-decision-trees-ac8656a0b4bb#2022-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="35f2" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">决策树完全指南</h1></div><div class=""><h2 id="dd78" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习关于决策树的所有知识，包括Python示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f3bd3d43ad917bc56c8f60acd87e3858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vFLr-c_sEidQES_FfmPDQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">西蒙·威尔克斯在<a class="ae ky" href="https://unsplash.com/s/photos/tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4bf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是一种机器学习算法，其名称来自其树状结构，用于表示多个决策阶段和可能的响应路径。决策树为分类任务或回归分析提供了良好的结果。</p><h1 id="fa48" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我们用决策树做什么？</h1><p id="862e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">借助于树形结构，不仅可以将不同的决策层可视化，还可以将它们按一定的顺序排列。对于单个数据点，可以进行预测，例如，通过得出目标值以及分支中的观察值进行分类。</p><p id="349c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树用于根据目标变量进行分类或回归。如果树的最后一个值可以映射到一个连续的尺度上，我们称之为回归树。另一方面，如果目标变量属于一个类别，我们称之为分类树。</p><p id="0f79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这种简单的结构，这种类型的决策非常受欢迎，并广泛应用于各种领域:</p><ul class=""><li id="a7a5" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><strong class="lb iu">业务管理</strong>:不透明的成本结构可以借助树状结构来说明，并清楚地表明哪些决策需要多少成本。</li><li id="3f63" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><strong class="lb iu">医学</strong>:决策树有助于患者发现自己是否应该寻求医疗帮助。</li><li id="e6b0" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated"><strong class="lb iu">机器学习和人工智能</strong>:在这个领域，决策树被用来学习分类或回归任务，然后做出预测。</li></ul><h1 id="9465" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">决策树的结构</h1><p id="1fbc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">树基本上由三个主要部分组成:根、分支和节点。为了更好地理解这些组成部分，让我们仔细看看一个示例树，它可以帮助我们决定今天是否在户外锻炼。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/2a5e759005a3825bb115ea7035aaf128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ru0tIhJBdv__T0LfYxL9Gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树示例|作者照片</p></figure><p id="b716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顶层节点“天气”就是所谓的根节点，作为决策的依据。决策树总是只有一个根节点，因此所有决策的入口点都是相同的。在这个节点上挂着所谓的具有决策可能性的分支。在我们的情况下，天气可以是多云、晴天或雨天。其中两个分支(“晴天”和“雨天”)挂着所谓的节点。此时，必须做出新的决定。只有分支“多云”直接导致一个结果(叶)。所以从我们的树上，我们已经可以读到，当天气多云时，我们应该总是去户外运动。</p><p id="69c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，在晴天或雨天，我们必须考虑第二个因素，这取决于我们的天气结果。对于节点“湿度”，我们可以在“高”和“正常”之间选择。如果湿度高，我们最终会得到“不”叶子。因此，在阳光充足、湿度较高的天气里，不宜在户外运动。</p><p id="0e6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果天气下雨，我们就在决策树的另一个分支。那么我们就要在“风”这个节点做出决定了。这里的决策选项是“强”或“弱”。还是那句话，我们可以读两条规则:如果下雨但风力较弱，我们可以在外面做运动。另一方面，如果下雨并伴有大风，我们应该呆在家里。</p><p id="a381" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个非常简单的例子当然可以进一步扩展和完善。例如，对于节点“湿度”和“风”，可以考虑用具体的规则(强风=风速&gt; 10公里/小时)来代替主观的决策选项，或者将分支细分得更细。</p><h1 id="3b27" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">所谓的修剪是什么？</h1><p id="c16f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在现实世界的用例中，决策树会很快变得复杂和混乱，因为在大多数情况下，需要两个以上的决策才能得到一个结果。为了防止这种情况，经过训练的决策树经常被修剪。</p><p id="2db0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">减少错误剪枝</strong>是一种自下而上的算法，从叶子开始，逐渐向根发展。这包括取出整个决策树，并省去包括决策的一个节点。然后，进行比较以查看截断树的预测精度是否已经恶化。如果不是这种情况，树被这个节点缩短，决策树的复杂度降低。</p><p id="5122" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了在训练之后缩短树的可能性之外，还有在训练之前或期间保持低复杂度的方法。一个流行的算法是所谓的<strong class="lb iu">提前停止规则</strong>。在训练期间，在每个创建的节点之后，决定是否在该点继续该树，即它是否是决策节点，或者它是否是结果节点。在许多情况下，所谓的基尼系数被用作一个标准。</p><p id="9df3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，它表达了如果标签被简单地随机分配，即基于在该节点的分布，标签将在该节点被不正确地设置的概率。这个比率越小，我们就越有可能在这一点上对树进行修剪，而不必担心模型准确性的巨大损失。</p><h1 id="8c8c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">决策树的优点和缺点</h1><p id="4332" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">简单易懂的结构使决策树成为许多用例中的流行选择。但是，在使用该模型之前，应该权衡以下优点和缺点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/2542e1353f2145e1e977a575be833ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHwMeiy4_RXSLdJJsE93aw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树的优点和缺点|作者照片</p></figure><h1 id="f3b7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">决策树是随机森林的一部分</h1><p id="4600" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随机森林是由个体决策树组成的<a class="ae ky" href="https://databasecamp.de/en/ml/supervised-learning-models" rel="noopener ugc nofollow" target="_blank">监督</a>机器学习算法。这种类型的模型被称为集合模型，因为独立模型的“集合”被用于计算结果。在实践中，这种算法用于各种分类任务或回归分析。其优点是通常较短的培训时间和程序的可追溯性。</p><p id="d841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-random-forest-algorithm-fed4b8c8e848">随机森林</a>由大量这些决策树组成，它们作为一个所谓的整体一起工作。每个单独的决策树做出预测，例如分类结果，并且森林使用大多数决策树支持的结果作为整个集合的预测。为什么多个决策树比单个决策树好得多？</p><p id="7441" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林之所以奏效，是因为所谓的<a class="ae ky" href="https://de.wikipedia.org/wiki/Die_Weisheit_der_Vielen" rel="noopener ugc nofollow" target="_blank">的多数人</a>的智慧原则。它说，许多决策树的决策优于一个独特的树的结果。这是一个适用于各种用例的原则，并在fair上首次得到认可。</p><p id="8c73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在20世纪，在集市上出售牛是很常见的，必须确定它们的重量。1906年，这样的一头牛被展示给800个不同的人，让他们猜这头牛的重量。最终，八百个人猜测的中值距离最终重量只有1 %左右。没有一个估计与实际结果如此接近，这意味着个人的总和比任何其他人都有更好的估计。</p><p id="c35b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一发现可以转化为其他领域，如随机森林，这意味着几个决策树及其聚合预测优于单个树。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/279ec70f139d31ed5a5dbade96bd55eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIbfZHfOJr-rzVsqNRV5tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林的结构|作者照片</p></figure><p id="acb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这是有一个先决条件的。决策不能与<a class="ae ky" href="https://databasecamp.de/en/statistics/correlation-and-causation" rel="noopener ugc nofollow" target="_blank">相关</a>，否则单个树的错误不会被另一个树补偿。让我们回到我们公平的例子。</p><p id="b009" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果所有的参与者都没有达成任何形式的一致，也就是说他们是不相关的，那么权重的中值估计将会比单一的猜测要好。否则，几个人的估计会影响其他人，而这不会产生多数人的智慧。</p><h1 id="8575" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">用Python训练决策树</h1><p id="2659" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">skic it-Learn</a><a class="ae ky" href="https://databasecamp.de/en/python-coding" rel="noopener ugc nofollow" target="_blank">Python</a>模块提供了数据分析所需的各种工具，包括决策树。其中，它基于Numpy已知的数据格式。为了用Python创建决策树，我们使用了来自<a class="ae ky" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank">文档</a>的模块和相应的例子。</p><p id="c582" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所谓的<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>是用于创建分类算法的流行训练数据集。这是一个生物学的例子，涉及到所谓的鸢尾属植物的分类。关于每朵花的长度和宽度的花瓣和所谓的萼片是可用的。基于这四条信息，然后就可以知道这种特定的花是三种虹膜类型中的哪一种。</p><p id="f856" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Skicit-Learn的帮助下，只需几行代码就可以训练出一个决策树:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="4c5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以通过定义输入变量X和要预测的类Y来相对容易地训练决策树，并根据它们来训练决策树。利用函数“predict_proba”和具体值，可以进行分类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj ni l"/></div></figure><p id="0d06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，根据我们的决策树，具有虚构值的这朵花属于第一类。这个属叫做“刚毛鸢尾”。</p><h1 id="ede2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">如何解读决策树？</h1><p id="bf12" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在MatplotLib的帮助下，可以画出训练好的决策树。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk ni l"/></div></figure><p id="9351" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们数据的最佳决策树共有五个决策级别:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/662de521c4dc493df7feafadb19ef7f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xTjFOLpluQrPkcz8R9vgGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树虹膜数据集示例|作者照片</p></figure><p id="fb64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个树的简单解释，我们感兴趣的是第一行和最后一行的值。树是从上到下读的。这意味着，在第一个决策层，我们检查花瓣的长度是否小于或等于2.45厘米。条件总是被公式化，使得在左分支中只有“真”，在右分支中只有“假”。</p><p id="5270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，如果一朵混凝土花有一个小于或等于2.45厘米的花瓣，我们在左边的分支(橙色瓷砖中)，这也是一片结果叶。因此我们知道，在这种情况下，花属于“Setosa”类。</p><p id="8401" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，如果花瓣更长，我们沿着正确的分支前进，并面临另一个决定，即花瓣是否有1.75厘米的最大宽度。我们遍历树，直到得到一个提供分类信息的结果表。</p><h1 id="a90b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">这是你应该带走的东西</h1><ul class=""><li id="41aa" class="ms mt it lb b lc mn lf mo li nl lm nm lq nn lu mx my mz na bi translated">决策树是另一种机器学习算法，主要用于分类或回归。</li><li id="7838" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">树由起点、所谓的根、代表决策可能性的分支和具有决策级别的节点组成。</li><li id="3bc9" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">为了降低树的复杂性和大小，我们应用所谓的修剪方法来减少节点的数量。</li><li id="ecb3" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">决策树非常适合生动地表现决策，并使其可以解释。</li><li id="1b69" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">然而，在训练时，为了获得有意义的模型，必须注意许多细节。</li></ul></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="3a7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">如果你喜欢我的作品，请在这里订阅</em><a class="ae ky" href="https://medium.com/subscribe/@niklas_lang" rel="noopener"><em class="nv"/></a><em class="nv">或者查看我的网站</em> <a class="ae ky" href="http://www.databasecamp.de/en/homepage" rel="noopener ugc nofollow" target="_blank"> <em class="nv">数据大本营</em> </a> <em class="nv">！还有，medium允许你每月免费阅读</em> <strong class="lb iu"> <em class="nv"> 3篇</em> </strong> <em class="nv">。如果你希望有</em><strong class="lb iu"><em class="nv">无限制的</em> </strong> <em class="nv">访问我的文章和数以千计的精彩文章，请不要犹豫，点击我的推荐链接:</em><a class="ae ky" href="https://medium.com/@niklas_lang/membership" rel="noopener">【https://medium.com/@niklas_lang/membership】</a>每月花$<strong class="lb iu"><em class="nv">5</em></strong><em class="nv">获得会员资格</em></p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><div class="kj kk kl km gt nw"><a rel="noopener follow" target="_blank" href="/learn-coding-13-free-sites-to-help-you-do-it-9b2c1b92e573"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">学习编码:13个免费网站帮助你开始</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">一旦你决定要学习编码，你会被众多的在线工具宠坏，这些工具可以帮助你…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/introduction-to-random-forest-algorithm-fed4b8c8e848"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">随机森林算法简介</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">算法是如何工作的，我们可以用它来做什么</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok ks nw"/></div></div></a></div><div class="ol om gp gr on nw"><a rel="noopener follow" target="_blank" href="/understanding-mapreduce-with-the-help-of-harry-potter-5b0ae89cc88"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">借助《哈利·波特》理解MapReduce</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">MapReduce是一种允许并行处理大型数据集的算法，例如，在多台计算机上…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="op l oh oi oj of ok ks nw"/></div></div></a></div></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><div class="kj kk kl km gt nw"><a href="https://medium.com/@niklas_lang/membership" rel="noopener follow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">通过我的推荐链接加入媒体- Niklas Lang</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">medium.com</p></div></div><div class="of l"><div class="oq l oh oi oj of ok ks nw"/></div></div></a></div><p id="c5c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">原载于2022年1月5日</em><a class="ae ky" href="https://databasecamp.de/en/ml/decision-trees" rel="noopener ugc nofollow" target="_blank"><em class="nv">https://database camp . de</em></a><em class="nv">。</em></p></div></div>    
</body>
</html>