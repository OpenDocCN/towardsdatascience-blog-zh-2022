# 论文解释:无监督视觉表征学习的动量对比

> 原文：<https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb>

## 查看 MoCo v1 和 v2 文件中的原则

在这个故事中，我们将回顾 MoCo 论文，这是一种使用动量对比(MoCo)对计算机视觉模型进行预训练的无监督方法，已经由作者从版本 1 到版本 3 进行了迭代改进。

于 2020 年由何等人在论文 [*【动量对比无监督视觉表征学习】*](https://arxiv.org/pdf/1911.05722.pdf) 中首次提出，并在 [*【改进的基线与动量对比学习】*](https://arxiv.org/pdf/2003.04297.pdf) *中进一步改进为 v2。*在陈等人的[“训练自监督视觉变形器的实证研究”](https://arxiv.org/pdf/2104.02057.pdf)中有另一个称为 v3 的迭代，它介绍了训练过程和模型架构的基本适应，我们将在未来的故事中介绍。

由于这种方法一直在不断改进，我们将首先回顾本文背后的最初想法，然后概述新版本引入的改进。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！

![](img/5f6ec6d7a04d4f0e8951896923ed5131.png)

对最初的 MoCo 论文背后的关键思想的简单说明。来源:[【1】](https://arxiv.org/pdf/1911.05722.pdf)

# 先决条件:计算机视觉的自我监督和非监督预训练

在我们深入研究 MoCo 的论文之前，有必要快速回顾一下自我监督的职前培训到底是怎么回事。**如果你一直在阅读我的其他自我监督学习故事，或者你熟悉自我监督预培训，请随意跳过这一部分。**

传统上，计算机视觉模型总是使用**监督学习**来训练。这意味着人类看着这些图像并为它们创建各种标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。

相比之下，**自我监督学习不需要任何人为创造的标签**。顾名思义，**模型学会自我监督**。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，**我们让模型知道这些图像仍然包含相同的视觉信息**，即相同的对象。**这导致模型学习相同对象的相似潜在表示(输出向量)。**

我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在 10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。

# 使用动量对比进行无监督学习

在 MoCo 的论文中，无监督学习过程被框定为字典查找:**每个视图或图像被分配一个关键字**，就像在字典中一样。**该密钥通过使用卷积神经网络对每个图像**进行编码而生成，输出是图像的矢量表示。现在，如果一个查询以另一个图像的形式呈现给这个字典，这个查询图像也被编码成矢量表示，并且将属于字典中的一个关键字，具有最小距离的关键字。

![](img/26c72a5d58d9ff56990497a7b6d9a6bf.png)

左边是查询图像和编码器，右边是小批量密钥队列和 momentum 编码器。来源:[【1】](https://arxiv.org/pdf/1911.05722.pdf)

训练过程设计如下:编码器网络选择并处理查询图像，以计算编码的查询图像 *q* 。由于该模型的目标是学习区分大量不同的图像，因此该查询图像编码不仅与一个小批量的编码关键图像相比较，而且与它们中的多个相比较。为了实现这一目标，MoCo 形成了一个由动量编码器网络编码的小批量队列。当选择新的小批量时，其编码被排队，并且数据结构中最旧的编码被出列。**这将由队列表示的字典大小与批处理大小分离，并允许查询更大的字典。**

如果查询图像的编码与字典中的关键字匹配，则这两个视图被认为来自同一图像(例如，多个不同的裁剪)。

训练目标由信息损失函数表示:

![](img/942b535f666b8b55def5248a58a52242.png)

信息对比损失函数。来源:[【1】](https://arxiv.org/pdf/1911.05722.pdf)

该损失函数允许模型学习来自同一图像的视图的较小距离以及不同图像之间的较大距离。为了实现这一点，损失类似于基于 softmax 的分类器损失，其目的是将 *q* 分类为 *k+* 。对查询 *q* 的一个正键和 *K* 个负键计算总和。

最后，**如第一幅图所示，通过编码器网络执行反向传播，但不通过动量编码器。**

作者试图用动量编码器对每幅图像实现相对一致的编码。由于保持权重冻结不会反映模型的学习进度，**他们建议使用动量更新**而不是复制编码器的权重来获得一致的动量编码器结果。为了实现这一点，将动量编码器网络设置为由查询编码器的**基于动量的移动平均值来更新。每次训练迭代都执行这种更新。**

# MoCo v2 的进一步改进

在模型的第二个版本中， **MoCo v2** ，在他们的论文 [*“改进的基线与动量对比学习”*](https://arxiv.org/pdf/2003.04297.pdf) *，*中，作者将 [SimCLR 论文](/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703)中的一些贡献转移到他们的模型中。最初，为了在预训练 MoCo 之后训练线性分类器，他们向模型添加了单个线性层。在 MoCo v2 中，该层已被 MLP 投影头取代，从而提高了分类性能。另一个显著的改进是引入了更强的数据扩充，类似于 SimCLR 中的数据扩充。他们通过添加模糊和更强的颜色失真来扩展 v1 增强。

最新版本的 **MoCo v3** ，在[“训练自我监督视觉变形者的实证研究”](https://arxiv.org/pdf/2104.02057.pdf)论文中介绍，将该模型改编为不同的训练过程。由于这些变化不仅仅是简单的扩展，我们将在以后的文章中讨论这篇文章。

# 包装它

在本文中，您已经了解了 MoCo，这是一篇最受欢迎的自我监督框架论文，为该领域更有前途的论文铺平了道路。

通常在我的论文故事中，我会在结尾介绍这种方法的结果和表现。这一次，我们将跳过这一部分，因为这两种方法都不是最先进的了，但 MoCo v3 是。尽管如此，我发现了解 MoCo 背后的原理作为最先进的 MoCo v3 论文的基础是非常重要的。

虽然我希望这个故事能让你对 MoCo 有一个很好的初步了解，但仍有很多东西有待发现。因此，即使你是这个领域的新手，我也会鼓励你自己阅读这些论文。你必须从某个地方开始；)

如果你对论文中介绍的方法有更多的细节感兴趣，请随时在 Twitter 上给我留言，我的账户链接在我的媒体简介上。

我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。

**最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者**。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。

参考资料:

[1]何，，等.“用于无监督视觉表征学习的动量对比”*IEEE/CVF 计算机视觉和模式识别会议论文集*。2020.[https://arxiv.org/pdf/1911.05722.pdf](https://arxiv.org/pdf/1911.05722.pdf)