# ML 可解释性是错误的目标吗？

> 原文：<https://towardsdatascience.com/is-ml-explainability-the-wrong-goal-d2e94e1f4bf2>

## 在高风险的应用中，可解释性甚至可能适得其反

![](img/d9965a3b936a0f4151a53345a437bf61.png)

照片由 [Yosef Futsum](https://unsplash.com/@yosef_fxum?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

在他 2006 年的经典作品[](https://en.wikipedia.org/wiki/The_Shock_of_the_Old)**中，大卫·艾顿认为历史学家对技术史的理解过于受发明的支配。我们理所当然地怀念和钦佩伟大的发明家和科学家。事实上，David Edgerton 详细描述道，技术的采用和使用通常和发明本身一样重要，如果不是更重要的话。技术投入使用的方式在历史上有很大影响。在这种背景下，ML 的可解释性和可解释性成为非常重要和有争议的概念就不足为奇了，有时会误入滥用的口号中。ML 是一项关键技术，目前正被集成到基础设施和决策过程的关键部分，因此采用的方式无疑是重要的。具体来说，一个已部署的 ML 系统可解释或可说明的程度决定性地影响了人在该系统操作中的角色。**

****可解释性对可解释性****

**“可解释性”是指用户/接收者证明人工智能模型所做预测的能力。这通常是一种用于深入了解复杂模型的技术。例如，由于所用算法的复杂性，人类可能无法理解发生在数据上的转换(尽管他们会理解该过程在高层次上是如何工作的)。在这种情况下，可解释性技术提供了一些为什么做出复杂预测的建议。可解释性指的是他们从因果角度解释为什么做出预测的能力。**

**从这个意义上来说，可解释性是可解释性的更强版本(对模型输出的更彻底的基于因果关系的解释)。)可解释性通常被用来证明黑盒模型所做的预测是合理的，而黑盒模型是不可解释的。例如，通过置换输入或用代理模型拟合黑盒模型的预测，我们也许可以更好地解释预测过程中发生的事情，但不能从因果上证明为什么会做出决策。**

****有‘可解释性’就够了吗？****

**然而，有些模型可能永远无法解释，尤其是深度学习(DL)模型。这是因为对于这些模型，输入通过训练过程被不可识别地转换。DL 的核心原则之一是[‘表示学习’](https://arxiv.org/abs/1206.5538)，这意味着模型将它接收到的输入迭代地转换成新的表示(当输入通过神经网络的连续层时)。这种转换旨在最大化数据中的信号，为算法提供更大的牵引力，以进行准确预测。换句话说，这个输入转换过程允许机器在输入上获得更多购买，同时限制了人类分析师理解相同输入的能力。这种权衡是神经网络所固有的，也是这套强大的模型存在问题的原因之一。这些是附加了特别解释工具的黑盒模型(例如，计算机视觉中的显著性图，表格数据的 SHAP 值等)。)以抵消人类固有的无法理解这些转换输入的能力。**

**事实上，已经表明，一些流行的用于证明深度学习模型预测的可解释性技术是不可靠的。显著图是理解卷积神经网络中预测的常用方法，旨在揭示哪些图像像素在进行预测时最重要。然而，已经证明这些方法[并不总是能够](https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v1)识别用于分类的图像的关键区域，这让我们质疑它们的实用性。一个可能不正确的可解释性方法有什么用？事实上，这可能会给用户一种错误的自信感。**

****我们如何定义“高风险”？****

**因此，一个关键的问题是，在人工智能系统实现之前，可解释性何时是必要的先决条件。Yoshia Bengio 对“系统 1”和“系统 2”的有影响力的区分可能有助于理解仍然使用黑盒和可解释性的辩护。他认为，系统 1 DL(构成“快速”感知式思维)已经由 ML 系统实现，如计算机视觉。然而，系统 2 DL(“慢”逻辑推理，可能涉及训练集数据分布之外的概括)还没有实现。本吉奥本人并没有提出这个论点，但是基于这种想法，有些人可能会认为系统 1 DL 不需要可解释性。我们大多数人都无法向朋友解释为什么我们会看到某个物体，或者为什么我们能够以某种方式闻到某样东西。**

**然而，在系统 1 的实现中(用大卫·埃哲顿的话来说，这是技术变革的*创新*部分),应用程序中 DL 的感知能力有时会取代人类的推理和逻辑，即使模型本身并不执行逻辑或推理。例如，以胸部 x 光片作为输入并预测相应患者哪里患有急性疾病的计算机视觉模型正在取代放射科医师可能用于根据 x 光片进行诊断的推理。与此同时，像这样的应用程序可以[通过排除模型以高置信度预测扫描正常的扫描，从而给放射科医生更多的时间来诊断棘手的问题，从而显著改善患者的结果](https://pubmed.ncbi.nlm.nih.gov/34623478/)。**

**这显然是一个使用黑盒模型做出高风险决策的例子。但是，其他一些实现更难分类。使用谷歌搜索是高风险决策的一个例子吗？网飞的建议是高风险决策吗？可能需要对“高风险”的含义进行严格的定义，以便就每个用例所需的可解释性水平达成共识。与此同时，我们应该非常小心地调用可解释的方法，特别是当它们给予模型预测背后的推理以信心的时候。**

****参考文献****

**[1] C. Rudin，[停止解释高风险决策的黑盒机器学习模型，转而使用可解释模型(2019)](https://arxiv.org/pdf/1811.10154.pdf?fbclid=IwAR01WIlfIiC1cgM99nhwIjAT0tHWYxHk7ZA_o9nEK9jJ75KdFMNZlv5Y0AU) ，《自然机器智能》第 1 卷，206–215 页。**

**[2]萨波塔等人。al，[‘深度学习显著图没有准确地突出医学图像解释的诊断相关区域’](https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v1)(2021)，medRxiv。**