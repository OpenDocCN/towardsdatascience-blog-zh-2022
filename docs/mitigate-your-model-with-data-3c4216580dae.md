# 用数据减轻你的模型

> 原文：<https://towardsdatascience.com/mitigate-your-model-with-data-3c4216580dae>

## 用这些简单的技巧让你的模型脱离生命支持系统，让你的数据更优秀

![](img/d782d9926fe0d926ebc39bba26b4eabc.png)

(图片由 [geralt](https://pixabay.com/images/id-5140055/) 在 [Pixabay](http://pixabay.com) 上提供)

# 介绍

预测建模的世界可能是复杂而可怕的。机器学习模型可能有点复杂，了解什么破坏了模型的输出可能是一个非常难以解决的问题。通常，预测模型的问题实际上根本不是模型本身的问题。相反，这个模型建立在一个糟糕的基础上——糟糕的数据。

修复数据并准备将其放入模型中有很多细节和细微差别。这通常包括探索特征、移动、删除、清理和数据测试等任务。准备训练和测试集，工程特性，标准化，以及更多的东西。今天，我想从一个绝对混乱的模型开始，它有糟糕的数据，糟糕的功能选择，然后试图通过操纵测试和训练数据来扭转这个模型。在这样做的时候，我也将展示人们可能用来解决这类问题的各种技术。我还将根据我在这里概述的一步一步的建模过程来概述这个项目:

</my-predictive-modeling-and-learning-step-process-technique-f0521ee76d90>  

此外，如果您想让笔记本跟随或自己尝试这些代码，那么可以在 Github 上找到这个笔记本，它还克隆了一个完整的示例数据存储库，包括这个模型中使用的数据集:

<https://github.com/emmettgb/Emmetts-DS-NoteBooks/blob/master/Python3/Bring%20a%20model%20back%20with%20data.ipynb>  

# 第 1 部分:我们的模型

对于我们的模型，我去了五点三十八分去获取[关于航空安全](https://github.com/fivethirtyeight/data/tree/master/airline-safety)的数据。FiveThirtyEight 也是一个很好的平台，可以获得一些数据来进行实践，所以对于那些正在学习或不断改进的人来说，这些数据集足够干净，可以使用，但又不至于干净到不需要任何工作。也就是说，我认为它们是很好的练习。

## 数据争论

对于我们的第一个实例，我们不会按照我在那篇文章中概述的过程进行建模。这只是为了让我们可以得到一个特别糟糕的模型，并通过修改特性使它起死回生。现在让我们开始争论步骤，这是我们在这里实际要做的几个步骤之一，从导入我们的依赖项和通过 Bash 用 Git 获得我们的 CSV 文件的克隆开始:

```
# deps
import pandas as pd
import numpy as np!git clone [https://github.com/fivethirtyeight/data](https://github.com/fivethirtyeight/data)
```

> 这些数据是由 [FiveThirtyEight](https://fivethirtyeight.com/) 根据[知识共享——归属许可](https://github.com/fivethirtyeight/data/blob/master/LICENSE)授权的，所以你可以在任何地方免费使用。

现在，我们将继续通过 read_csv()读取熊猫的 CSV 文件:

```
df = pd.read_csv("data/airline-safety/airline-safety.csv")
df.head(5)
```

![](img/b30947f9f1b82fb735c37cb21afa2513.png)

(图片由作者提供)

好吧，我不知道 Aroflot*是谁，但是

> 我不会上他们的飞机。

## 预处理

这个处理步骤更像是预处理，因为我们的任务只是让数据帧足够可读。一旦它足够易读，我们将跳过我将采取的下几个步骤，因为我们将看到这些步骤对于做出一个伟大的预测是多么重要和有效。对于这个数据框，我要做的第一件事就是通过获得空值的观察值的总和来检查缺失值。

```
df.isnull().sum()airline                   0
avail_seat_km_per_week    0
incidents_85_99           0
fatal_accidents_85_99     0
fatalities_85_99          0
incidents_00_14           0
fatal_accidents_00_14     0
fatalities_00_14          0
dtype: int64
```

多么干净方便，没有缺失值。也就是说，我将直接进入功能选择，但是我不会根据它们的优势来分离这些功能，因为我们将在获得针对此数据的模型的初始“轻而易举”指标后进行分离。

## 特征和目标选择

既然我们知道我们实际上可以将这些特征通过一些数据处理，让我们直接选择我们的特征和我们的目标。由于这是我的低努力示例，我们没有关注数据，所以我不会在这里花太多精力。相反，我会选择一个目标，然后继续前进。

所以…我控制不住自己，因为这里的特色很吸引人。

![](img/fd91c1e453266b957e38bf75eea9c9d6.png)

(图片由作者提供)

根据 README.md，以及我对该数据的假设，我们有事故、致命事故(导致 1 人或更多人死亡的事故)，然后我们有死亡人数。我的看法是，我们可以制作一些不同的建模示例，这将非常酷。我最初的一个想法是，我们可以根据我们的飞机平均每周飞行多少公里来预测我们可能遇到的死亡人数。我的另一个很酷的想法是提供一些其他值，并让一个二元分类器尝试预测它是否发生在 1985-1999 年或 2000-2014 年之间，鉴于 1999 年是我出生的年份，我认为找出飞机以前是否是死亡陷阱会很有趣。

2001 年 9 月还发生了一个现实世界的变化，我认为这很有可能改变政府对这些航空公司的监管方式。我认为，随着对这些公司更多的安全、更多的责任和更密切的关注，想象一下这样的事件对不相关和不相关的事情有多大的影响可能会很有趣。我认为在 2021 年的类似数据中也可以观察到同样的现象，我认为这将是一项有趣的研究。

> 看看我的遭遇。

这就是数据科学如此有趣的原因，但让我们现在只选择一个目标来研究，也许将来当我有空时，我会在这个数据集上尝试不同的目标，也许我会在我的博客上的另一篇文章中使用它！无论如何，今天我走的是安全路线，因为这个数据集的一个不幸之处是…

```
len(df)56
```

这么少的观测数据，分散在两个独立的年份中，这将有一个严重的风险，与我们在这里保留的较少特征不匹配。考虑到这一点，我认为这里最好的方法可能是二进制分类器。二进制分类器将要求我们设计一个目标，然而，这是有问题的，因为这应该是一个非常低工作量的模块。然而，正如我在许多类似文章的结论中所讨论的那样，数据永远不会是相同的——所以总有一些情况下我们需要做一些不寻常的事情。这是做这件事如此吸引人的部分原因。然而，对于这个观察长度，我们只有 56 个总观察计数，所以我们可能想要以不同的方式处理事情。例如，如果我们对此进行训练/测试/val 分割，训练数据将是

```
len(df) * .75 * .7531.5
```

长时间观察。换句话说，我们将训练 31 个观察值，预测 11 个验证观察值和 14 个测试观察值，同时用少量数据训练我们的模型。更好的选择可能是在这里进行常规的训练/测试。让我们首先设计我们的目标，它基本上只是一个布尔值，但是我们将把它存储为一个整数。我们还需要设计一些功能来配合它。这将有助于我们的无数据问题的实质。鉴于我们的长度如此之短，通过选择这个目标，我们可以有效地将数据翻倍。首先，我将把我们的功能放入一系列标签中:

```
features = ["incidents", "fatal_accidents", "fatalities"]
```

请注意，这些功能不包括年份。这是因为我现在将使用一行 for-loop 来连接每个特性的所有内容，这样特性名称中就不再包含年份。我们需要将这些名称转换成数据框中的一个新列。这个简洁的 for 循环可以做到这一点，你也可以同时使用 lambda 和 map 来达到同样的目的。

```
z = [list(df[feature + "_85_99"]) + list(df[feature + "_00_14"])for feature in features]
```

现在，当我们把这些放在一个数据框架中时，我们有效地加倍了我们的观察:

```
len(z[1])112 features = pd.DataFrame({"indicidents" : z[0], "fatal_accidents" : z[1], 
                   "fatalities" : z[2]})
```

我写了一个简单的函数来生成 y，这只是一个简单的条件，根据它所代表的值返回一个或另一个数字集合。因为所有超过原始数据帧长度的值(没有变化)都是我们附加的值，我们可以利用这一点。不幸的是，这意味着许多函数对此不起作用，因为我们**必须**知道值的索引。考虑到这一点，我编写了这个初等函数来完成所有这些工作:

```
def yearcategory(x):
    z = []
    for count, i in enumerate(x):
        if count > len(df):
            z.append(2014)
        else:
            z.append(1985)
    return(z)years = yearcategory(features["fatalities"])
```

现在我们有了建模这个目标所需要的一切！一路走来，我们也做了一些光特征工程。我们会回来，并在这方面进行迭代，以使我们的模型在未来变得更好。另一个我确实去掉了的特性是 km traveled 特性，但可能不应该去掉，因为在这个例子中我们可能根本不需要去掉任何特性。然而，这显然不重要，就像航空公司的标签一样。它们对研究不重要，因此我们不妨把它扔掉。

对我来说，不处理数据很难建立一个模型。很可能完全不可能做到完全诚实，但希望这只是进一步证明，即使做这么多也不总是足够的。

## 随机元素

每一步的大纲中有一些元素通常做得有点乱。例如，分割数据通常被认为是特征工程的一部分，但是通常将训练 X 和测试 X 一起处理是有意义的，这样它们接受相同类型的处理。我们将从把年份放入我们的特征数据框开始。

```
features["years"] = years
```

假设我们的其他特征都是连续的，我们可以很容易地将所有这些值放入一个模型中并得到一个预测，不需要任何处理。因为这是我们的演习，我们当然要这样做。这是我们的训练测试分割，然后将这些值转换成适当的格式，以便我们的模型进行解释:

```
from sklearn.model_selection import train_test_splittarget = 'years'
train, test = train_test_split(features)
trainX = train.drop(target, axis = 1)
trainy = train[target]
testX = test.drop(target, axis = 1)
testy = test[target]
```

## 建模

好消息是。我们仍然不太了解这些特征。甚至不知道这些价值观的含义是什么，并在我们说话的时候模拟它，这似乎很奇怪，但不管怎样，这是我选择的人生道路。我们需要一个分类器模型，在预测像这样的一组二元特征时，它将非常有效。我们基本上可以使用任何分类器，但是如果它是一个通用分类器，例如通过随机森林分类器的基尼指数，求解器会有过度思考的倾向。在整个过程中，特别是由于我们的数据量较小以及我们对目标的各种限制，我们可能希望选择一个更适合具有二元集的目标的模型。

```
from sklearn.linear_model import LogisticRegression
```

我最终选择了逻辑回归。我之所以选择这个，是因为它们相当擅长预测二进制值。

```
model = LogisticRegression
model.fit(trainX, trainy)
yhat = model.predict(testX)
```

现在让我们看看这个模型做得有多好:

```
from sklearn.metrics import accuracy_score
accuracy_score(testy, yhat)0.4642857142857143
```

> 哇，这个模型做得太糟糕了。

这并不奇怪，但我们现在可以通过演示这种情况下的一些处理和缓解技术来提高该模型的准确性！

# 第 2 部分:减轻我们的模型

现在我们有了一个表现不佳的模型的例子，让我们通过在数据中投入更多的工作来扭转这种局面。当然，最初我们这里的数据形式非常有限，所以我们的偏见总是会做出弱假设，在这种情况下，我们对此真的无能为力。有一种潜在的方法可以帮助解决这个问题，那就是使用 boosting 算法，甚至改变我们的模型来解决这个问题。让我们从分析致命事故特征开始。获得 Pandas 中连续值的良好汇总的一个好方法是使用 Series.describe()方法:

```
features["fatal_accidents"].describe()count    112.000000
mean       1.419643
std        2.236625
min        0.000000
25%        0.000000
50%        1.000000
75%        2.000000
max       14.000000
Name: fatal_accidents, dtype: float64
```

我已经立刻发现了一些东西。75%的数据低于 3，这意味着最大值 14 可能是这个数据中的一个巨大的异常值。我们可能应该将这样的值映射为平均值。除此之外，为了让我们的数据在数字上更接近，我们可以通过一个标准的定标器来运行它。然而，在我们做任何事情之前，我们可能想要检查这个值对于我们的模型在统计上有多重要。当我们有这样的数据时，我们将使用一种典型的方法来这样做，即来自 SciPy.stats 的标准独立 t-test。另外，如果您希望学习 Python 中的一般科学知识，这也是一个很好的学习库。

## 一些测试

要在 Python 中设置一个测试，我们需要从数据帧中提取一些值，以找到符合某些标准的样本。要做到这一点，最好的方法可能是条件屏蔽。条件屏蔽允许我们使用迭代或映射创建一个位数组，然后我们可以用这些条件索引我们的数据帧，在我们的位数组中过滤掉错误值。

```
from scipy.stats import ttest_indmask = [val > np.mean(features["fatal_accidents"]) for val in features["fatal_accidents"]]
```

这里我只是创建了一个迭代循环。这个循环的每次调用都会将 false 返回到我们的 mask 类型中，这是一个通过这个语法构造的数组。现在，我们只需将该掩码放在数据框上，这将删除掩码数组中的假值。

```
onlyabovemu = features[mask]
```

现在我们可以使用 scipy.stats ttest_ind()方法来测试我们的数据。我们会想要比较我们的目标之间的差异。

```
ttest_ind(features["years"], onlyabovemu["years"])
```

现在我们将检查 P 值:

```
Ttest_indResult(statistic=1.538691906112917, pvalue=0.12605892096661567)
```

> 我的天啊。

这里有一个潜在的因素可能会影响这些值，我们将在查看数据本身后检查这一点。我有一个痛苦的怀疑，也许我们的平均值被一些异常值打乱了，这甚至给这个测试带来了问题，因为我们所有的样本都在平均值的上半部分，谁知道我们整个样本是基于什么样的数据。让我们检查一下长度:

```
len(onlyabovemu)
37
```

与我们数据中的 112 个值相比，这是相当低的。我们真正的计数应该是观测值的一半左右。这表明该数据在一个方向上有异常值。数据需要比这集中得多，所以我们必须去除这些异常值。不过，在此之前，我们先来看看其他特性。这是对其他特征的描述，至少看起来**甚至不如**有统计学意义。

```
features["fatalities"].describe()count    112.000000
mean      83.964286
std      132.741701
min        0.000000
25%        0.000000
50%        3.500000
75%      109.250000
max      537.000000
Name: fatalities, dtype: float64features["incidents"].describe()count    112.000000
mean       5.651786
std        8.540006
min        0.000000
25%        1.750000
50%        3.000000
75%        7.000000
max       76.000000
Name: incidents, dtype: float64
Name: fatalities, dtype: float64
```

在死亡的例子中，致命性通常为 0，这可能是一个完全无用的特征。由于 fatal_accidents 特性，这也是部分正确的。我们可能希望完全摆脱这个特性，因为它给我们带来的问题比解决方案更多。这是因为观察结果主要保留在数据的上层，它并不真正影响每年有多少人死于飞机故障——仅此而已。不是愤世嫉俗，但通常如果一架飞机坠毁，每个人都会死。至于事件，这可能是我们见过的最一致的，尽管在这种情况下，0 可能会提交很多对我们的均值的权重。记住我们测试和获得显著性的整个方法就是这个意思。也就是说，如果我们想让这个模型很快脱离生命维持系统，我们肯定需要修复这些装置。

我还对以下特性进行了 T 检验:

```
ttest_ind(features["years"], onlyabovemu["years"])
mask = [val > np.mean(features["fatalities"]) for val in features["fatalities"]]
onlyabovemu = features[mask]
ttest_ind(features["years"], onlyabovemu["years"])mask = [val > np.mean(features["incidents"]) for val in features["incidents"]]
```

我只是一遍又一遍地运行相同的代码，如果你真的想这样做，你可以有条不紊地进行自动化测试，尤其是当你有很多特性的时候。以下是事件的 P 值:

```
Ttest_indResult(statistic=1.1895326000264201, pvalue=0.23614792418482572)
```

这是死亡人数的 P 值:

```
Ttest_indResult(statistic=0.9387567984962254, pvalue=0.3494167340587091)
```

正如我所怀疑的，死亡特性真的一点也不重要。

在我们移除一些值之后，我们将标准地缩放所有这些连续的特征。这将在数字上缩小差距，并使我们更容易过滤值。此外，它还会为我们的模型做类似的事情，所以我们为使数据更好而做的处理也在改进模型，

> 谁能想到呢？

## 预处理

为了消除这些明显高于我们真实平均值的值，我们将根据 z 值对数据进行归一化处理。为此，我们将利用 scipy.stats 中的 zscores 函数:

```
z_scores = stats.zscore(features)
```

为了做到这一点，我们将使用分位数定标器。分位数定标器相当于标准定标器，但用于分位数。这只是看待数据的不同方式。我们可以利用分位数，而不是只关注平均值。这可以减少离群值的影响，并且我们在扩展数据后仍然可以删除它们。所有这些工作的现实是，如果我们一开始就这样做，现在就会容易得多。现在，我们将只使用绝对距离，确保绝对距离小于 3，小于平均值的 3 个标准偏差就是这意味着什么。

```
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
df = features[filtered_entries]
```

> 这就是为什么条件掩蔽是不可思议的。

无论如何，我们还可以做更多的事情，但是这篇文章很快就会成为一本书，所以让我们看看这些小小的变化对我们的准确性产生了什么影响。

# 结论

现在我们几乎已经完成了这个项目，让我们在得到最终预测之前最后一次反思我们的冒险和我们模型的缺点。真的，这么多的数据很难处理。这个功能的数据翻倍确实表现不错，但是，我们的模型显然还需要更多的观察。我希望我选择了一个更大的数据集，但是所有的数据都是不同的，你不可能总是得到你想要的数据。这是我们最后的预测:

```
target = 'years'
train, test = train_test_split(features)
trainX = train.drop(target, axis = 1)
trainy = train[target]
testX = test.drop(target, axis = 1)
testy = test[target] model.fit(trainX, trainy)accuracy_score(testy, yhat)0.7857142857142857
```

仅仅使用这些简单的技术，我们就将这个模型的精确度提高了近一倍。您仍然可以帮助处理这些数据的一些方法包括某种增强算法，或者添加更多的数据。另一个可能有帮助的方法是设计一个新的特性或者放弃不太重要的特性。我不确定它会对诚实产生什么样的影响，但我认为这是一个有趣的项目。

谢谢你读我的文章，它对我来说意味着整个世界。我希望这很好地证明了立即关注你的数据是多么好。如果我一开始就把时间花在数据上，有很多事情可以做得更好。祝数据科学快乐！