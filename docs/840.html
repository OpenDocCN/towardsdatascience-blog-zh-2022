<html>
<head>
<title>Training Hidden Markov Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练隐马尔可夫模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-hidden-markov-models-831c1bdec27d#2022-01-31">https://towardsdatascience.com/training-hidden-markov-models-831c1bdec27d#2022-01-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="5cd3" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">训练隐马尔可夫模型</h1></div><div class=""><h2 id="5dc7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Baum-Welch和向前向后算法</h2></div><p id="eb0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的<a class="ae lb" rel="noopener" target="_blank" href="/hidden-markov-models-an-overview-98926404da0e">上一篇文章</a>中，我介绍了隐马尔可夫模型(HMMs)——对有噪声的序列数据建模的最强大(但未被充分重视)的工具之一。如果你有一个HMM来描述你的过程，维特比算法可以把一个嘈杂的观察流变成一个对每个时间步发生的事情的高度可信的猜测。</p><p id="c02a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这是假设您已经有了一个所有参数都已经调好的HMM。有时候情况就是这样——我记得我在谷歌的时候，我们有一个非常奇特的模型来模拟英语的词序——但更多时候你只有原始数据，你必须自己拟合一个HMM。这篇文章将讨论这种情况。</p><p id="e9f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TLDR是这样的:如果你<em class="lc">真的</em>没有标记数据，也不知道任何事情，你可以使用Baum-Welch算法来拟合HMM。但是由于技术原因，Baum-Welch算法并不总是给出正确的答案。另一方面，如果你<em class="lc">确实</em>有一些知识和一点点时间，那么有无数种方法可以破解培训过程。</p><h1 id="8622" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">训练的两个部分:马尔可夫链和观察值</h1><p id="a05c" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">回想一下上一篇文章，HMM有两个部分:</p><ul class=""><li id="6f36" class="ma mb iq kh b ki kj kl km ko mc ks md kw me la mf mg mh mi bi translated">描述您在不同状态之间转换(或停留在同一状态)的可能性的底层马尔可夫链。通常这种潜在的状态是你真正感兴趣的东西。如果在HMM中有k个状态，那么马尔可夫链由1)k*k矩阵和2)k长度向量组成，k * k矩阵表示你从S1状态转移到S2状态的可能性，k长度向量表示你从每个状态开始的可能性。</li><li id="abc3" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">一个允许您计算Pr[O|S]的概率模型-如果我们假设基础状态为S，则看到观察值O的概率。与具有固定格式的马尔可夫链不同，Pr[O|S]的模型可以是任意复杂的。但是在许多hmm中，Pr[O|S]非常简单:每个状态S是一个不同的装载骰子，Pr[O|S]是它在每一边着陆的概率。</li></ul><p id="1ef7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在很大程度上，这两个运动部分可以独立考虑。你甚至可能有外部知识告诉你其中一个是什么，而不是另一个。例如，假设您正在尝试制作一份音频记录的抄本；状态是英语单词，观察是声音的模糊。您可以使用现有的英语文本语料库来训练您的马尔可夫模型，然后只在训练HMM时调整Pr[O|S]的参数。</p><p id="9e58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有大量的标记数据——你有一系列的观察结果和一个潜在状态的知识——训练HMM实际上分解成两个独立的问题。首先，您使用标签来训练马尔可夫链。然后，根据观察值所处的状态对它们进行划分，并为每个状态S训练P[O|S]。如果我们的数据有可靠的状态标签，那么训练HMM就很简单。</p><p id="2d93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但实际上，我们通常只有一系列的观察结果，而没有系统处于何种状态的可靠知识。直觉是这样的:我们<em class="lc">猜测</em>州标签是什么，并使用这些猜测训练一个HMM。这将是一个相当可怜的嗯，但它可能会有<em class="lc">一些</em>位真正的模式，即使只是运气不好。然后，我们使用训练好的HMM在这些状态下做出更好的猜测，并根据这些更好的猜测重新训练HMM。这个过程一直持续到训练好的HMM稳定下来。这种来回——在使用HMM猜测状态标签和使用这些标签拟合新的HMM之间——是Baum-Welch算法的本质。</p><h1 id="4c1a" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">鲍姆-韦尔奇算法:细则</h1><p id="2673" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我给你的关于Baum-Welch (BW)算法的直觉需要澄清两点。</p><p id="c68f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们通常从猜测HMM的参数开始，而不是从数据中的潜在状态开始。这让我们可以确保HMM中存在模式，并且后续的迭代可以确定哪些模式是真实的。如果你愿意，你可以从猜测状态开始，但是这样会收敛得更快。此外，通常您会对HMM参数有很好的猜测，您可以使用这些参数进行明智的初步猜测。</p><p id="50f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二点比较微妙。一旦我们有了一个HMM，我们所说的“猜测状态”是什么意思呢？在上一篇文章中，我们谈到了维特比算法，该算法采用一系列观察值，并为我们提供一个最佳状态序列。但问题是，其中一些状态可能会被可靠地猜测出来，而其他状态可能会非常模糊——维特比算法无法区分高置信度和低置信度的猜测。</p><p id="5fb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决第二个问题，我们放弃了维特比算法和它的假置信度。相反，我们使用其不太为人所知的表亲向前向后算法，它给我们提供了概率猜测，可以用来衡量信心。</p><h1 id="18f6" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">向前向后算法</h1><p id="1f41" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">让我们谈一分钟技术问题。维特比算法不只是“解码观察”。它解决了一个非常具体的数学问题:给定o o …个观察值序列，它会找到最大化这些状态和观察值的组合概率的单个状态序列s …:</p><p id="462f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Pr[s…]* Pr[o o…| s…]</p><p id="0823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它针对整个状态序列进行优化，而不是针对任何一个特定的状态。但是让我们说，我们只对一个特定的州感兴趣，s⁵.可能是状态的单个最佳序列——由维特比算法找到的序列——具有s⁵=0，但是有许多稍微差一些的序列，对于这些序列，s⁵=1.在这种情况下，稍微差一点的序列相加，孤立的s⁵的最佳猜测是1。</p><p id="a864" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">向前-向后算法为每个时间步长寻找最佳猜测状态。一般来说，这些状态将不同于维特比状态，尽管实际上它们通常非常接近(例如:在我现在处理的数据中，它们有5%的时间不一致，我很惊讶它会那么高)。</p><p id="0b0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了我们的目的，前向-后向算法的价值不在于它与维特比算法的不同之处。事实是，它严格计算每个时间步在每个状态的概率。<em class="lc">这个</em>就是我们用于鲍姆-韦尔奇算法的！我们并不在一系列我们确定知道的状态上训练我们的HMM:我们训练我们的HMM，使得它平均起来是所有这些概率猜测的最佳拟合。</p><p id="3dcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里就不给出Baum-Welch算法的伪代码了。相反，我将注意到这是EM算法的一个特例，当存在未知的“隐藏变量”(在这种情况下，哪个状态在哪个时间戳)时，EM算法用于拟合模型，这些未知的“隐藏变量”可以通过概率进行猜测。Baum-Welch算法的致命弱点与困扰EM算法的问题是一样的:解决方案只是局部最优的。</p><h1 id="60de" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">大恶魔:局部最优</h1><p id="b861" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我提到Baum-Welch算法的第一步是猜测HMM的参数。这意味着随机初始化参数(对于hmm来说，dirichlet分布是一种流行的选择，但这超出了我们的范围)。但问题是，不同的随机起点可以汇聚到不同的最终hmm上——不管起点如何，都没有唯一的最佳答案。</p><p id="f2d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可能收敛到的各种hmm被称为“局部最优”。取一个局部最优X——你可以把它想象成高维向量空间中的一个点，其中维度是hmm的不同参数。X比在它的一定距离内的任何其他HMM都好，并且如果您的初始猜测足够接近X，那么Baum-Welch算法将收敛于X。但是很容易存在比X好但远离它的另一个X′。</p><p id="0cda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我最近在自己的工作中遇到了一个有趣的例子。我正在模拟一个人在每个时间点使用7个桌面应用程序中的哪一个，并试图用一个双态HMM来拟合它。我的希望是找到一种“自然”的行为模式，让他们在几个应用之间快速切换。相反，我安装了两次HMM，每次都做了一个饼状图，显示用户在每个州的每个应用上花费的时间。以下是两次运行的结果:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/a0d5c3c33f1416a4d0ab67d2b15f762d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iolNut7fbg4FYdXcOSYhCQ.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">Baum-Welch算法中不同的初始条件，即使是对相同的数据进行训练，由于存在许多局部最优值，也会产生非常不同的最终输出。</p></figure><p id="42d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以看到，一次紫色应用程序与棕色和粉色应用程序合并在一个州，而另一次它与橙色/蓝色/红色/绿色应用程序合并在一起。额外的运行产生了其他结果。我一直发现某个应用程序几乎只在一个州内使用。但是有7个应用程序要在两个州之间分配，每种分配方式都有自己的局部最优方案。</p><p id="f819" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有通用的方法来避免这个问题——拟合HMM是所谓的“非凸”问题，并且通常会遭遇局部最优。但是，有几种方法可以减轻这种影响:</p><ul class=""><li id="35fa" class="ma mb iq kh b ki kj kl km ko mc ks md kw me la mf mg mh mi bi translated">你可以简化你的HMM。局部最优值的数量会随着HMM中参数的数量呈指数增长。如果减小参数大小，就可以减少陷入局部最优的次数。这可能意味着使用2态HMM而不是3态HMM。如果你的观察值是多项式，这可能意味着减少可能的观察值。</li><li id="72f7" class="ma mb iq kh b ki mj kl mk ko ml ks mm kw mn la mf mg mh mi bi translated">利用商业直觉得到一个与你预期的最终结果相似的初始HMM。不同的局部最优解之间通常有很大的质量差异，就像我遇到的例子一样，所以从总体上看，你最终收敛的HMM很可能类似于你开始时的那个。不过，一定要确保你的猜测是部分随机的——可能有几个与你的商业直觉一致的局部最优值，如果是这样，你就想知道它。</li></ul><p id="8870" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，局部最优与过拟合是不同的问题(这当然也适用！).在我遇到的例子中，无论我有多少训练数据，各州之间的每个应用程序分发都将是其自己的局部最优。</p><h1 id="12d8" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">离别赠言</h1><p id="4cea" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我不想撒谎:使用一个经过训练的HMM比一开始就去拟合它要简单得多。另一方面，至少如果你像我一样，这也是乐趣的一部分！在对客户数据应用hmm时，我们在<a class="ae lb" href="http://www.zeitworks.com" rel="noopener ugc nofollow" target="_blank"> Zeitworks </a>经常遇到这种情况。我们需要一个足够简单的模型来减轻局部最优，但又足够复杂来描述我们正在研究的过程。开箱即用的工具通常不能处理像部分已知的基本事实这样的边缘情况，我们经常发现自己在修补核心算法。</p><p id="d58f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，这种复杂性与你可能考虑的其他技术相比相形见绌，比如递归神经网络。此外，hmm更容易理解业务，更容易调试，如果训练数据有限，它们通常表现得更好。它们不能解决所有问题(没有什么能)，但它们是武器库中不可替代的工具。</p></div></div>    
</body>
</html>