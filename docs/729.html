<html>
<head>
<title>Adding Custom Layers on Top of a Hugging Face Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在拥抱脸模型上添加自定义层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd#2022-01-26">https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd#2022-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="66db" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">在拥抱脸模型上添加自定义层</h1></div><div class=""><h2 id="f88f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何从拥抱脸模型体中提取隐藏状态，在其上修改/添加特定于任务的层，并使用PyTorch端到端地训练整个自定义设置</h2></div><p id="8142" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在开始之前，这篇文章假设对拥抱脸有基本的了解(使用开箱即用的模型)。此外，在<a class="ae lb" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">拥抱脸</strong> </a>的人们大声喊出来，建立一个对初学者友好的学习环境！</p><h1 id="e25c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">你会从这个博客中学到什么？</h1><ol class=""><li id="489c" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated">从拥抱面部中枢使用特定于任务的模型，并使它们适应你手头的任务。</li><li id="d733" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">将模型的头部与身体分离，并使用身体来利用特定领域的知识。</li><li id="3542" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">在PyTorch中构建一个定制的头部并将其连接到HF模型的主体上，并对系统进行端到端的训练。</li></ol><h1 id="7981" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">拥抱脸模型的剖析</h1><p id="6718" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">这是一个典型的高频模型的样子</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0f4f1a29f4ce9eb86b8c6bc9e94b077c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*7JDSKluZfSSI0O1yRWUIOQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">作者图片</p></figure><h1 id="9cbc" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">为什么我需要分别使用头部和身体？</h1><p id="1a03" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">拥抱脸的一些模型在问答或文本分类等下游任务中接受训练，并包含关于它们在权重中接受训练的数据的知识。</p><p id="04aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">有时，特别是当我们手头的任务包含非常少的数据或者是特定领域的任务(如医疗或体育特定任务)时，我们可以利用</strong> <a class="ae lb" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="mz">中枢</em> </strong> </a> <strong class="kh ir">上接受过任务<em class="mz">(不一定与我们手头的任务相同，但属于同一领域，如体育或医疗)</em>训练的其他模型，并利用这些模型的一些预先训练的知识来提高我们自己任务的性能。</strong></p><ol class=""><li id="c25a" class="lu lv iq kh b ki kj kl km ko na ks nb kw nc la mb mc md me bi translated">一个非常简单的例子是，假设我们<strong class="kh ir">有一个小数据集</strong>来分类一些财务报表在情绪方面是积极的还是消极的。<strong class="kh ir">然而，我们去了中心，发现很多模型已经被训练用于与金融相关的QA。我们可以从这些模型中使用<strong class="kh ir">T21s</strong>来改进我们自己的任务。</strong></li><li id="75a8" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">另一个简单的例子是，当某个特定于领域的模型已经学会从一个巨大的数据集中将文本分类成5个类别时，它是在上被训练的。<strong class="kh ir">假设我们有一个相似的分类任务，一个完全不同的数据集在同一个域中，我们只想将数据分为2类而不是5类。我们可以再次使用模特的身体，并添加我们自己的头部，试图在我们自己的任务中增加特定领域的知识。</strong></li></ol><p id="2a63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">概略地说，这就是我们正在努力做的</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a07871667b9e56758f6c2b2d4fdc6dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*5h3h7WtxAZpmmjfem3eoUQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">作者图片</p></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/ca57b7bf209bdea3b3142f42ffa7a505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*Zz_QpVlAPF0Jkgd02948Yg.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">作者图片</p></figure><h1 id="6451" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">跳入代码！</h1><p id="b155" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated"><strong class="kh ir">我们的任务很简单，讥讽检测来自Kaggle的</strong><a class="ae lb" href="https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir"/></a><strong class="kh ir">数据集。</strong></p><p id="2f90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">你可以在这里</strong>  <strong class="kh ir">查看完整代码</strong> <a class="ae lb" href="https://jovian.ai/rajbsangani/emotion-tuned-sarcasm" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">。出于时间的考虑，我没有包括下面的预处理和一些训练细节，所以请确保查看笔记本以获得完整的代码。</strong></a></p><p id="f96a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">我将使用一个在巨大的推特语料库上训练的具有5个分类输出的模型来分类5种不同的情绪，提取身体并在PyTorch中为我们的任务添加自定义层(2个标签，讽刺和非讽刺)，并端到端地训练新模型。</em></p><p id="693f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">注意:您可以使用本例中的任何模型(不一定是为分类而训练的模型)，因为我们将只使用该模型的身体，而不使用头部。</strong></p><p id="6b12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是我们的工作流程</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e8d5330a06261911926613c246ffc973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*vBXL8SiUl9lPLvZkUIoGUQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">作者图片</p></figure><p id="85ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将跳过数据预处理步骤，直接跳到主类，但是您可以在本节开头的链接中查看完整的代码。</p><h2 id="103b" class="nk ld iq bd le nl nm dn li nn no dp lm ko np nq lo ks nr ns lq kw nt nu ls nv bi translated">标记化和动态填充</h2><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="68a8" class="nk ld iq bd le nl nm dn li nn no dp lm ko np nq lo ks nr ns lq kw nt nu ls nv bi translated">提取身体并添加我们自己的层</h2><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="5725" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">如你所见，我们首先从PyTorch中派生出nn模块的子类，使用AutoModel(从transformers中)提取模型主体，并向我们想要使用其主体的模型提供检查点。</strong></p><p id="5c7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">注意，返回一个</em><strong class="kh ir"><em class="mz">token classifier output(来自变形金刚库)</em> </strong> <em class="mz">，这确保我们的输出与中枢上的拥抱人脸模型的输出格式相似。</em></p><h2 id="5792" class="nk ld iq bd le nl nm dn li nn no dp lm ko np nq lo ks nr ns lq kw nt nu ls nv bi translated">端到端培训新模型</h2><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="5a5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，我们使用这种方法获得了不错的性能。<strong class="kh ir">请记住，这篇博客的目的不是分析这个特定数据集的性能，而是学习如何使用预先训练好的身体并添加自定义头部。</strong></p><h1 id="0436" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="68e5" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">我们看到了如何使用Hugging Face Hub向预训练模型的身体添加自定义层。</p><p id="56f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些要点:</p><ol class=""><li id="d32d" class="lu lv iq kh b ki kj kl km ko na ks nb kw nc la mb mc md me bi translated">这种技术在以下情况下特别有用:我们有特定于领域的小型数据集，并且希望利用在同一领域(任务不可知)的大型数据集上训练的模型来提高小型数据集的性能。</li><li id="c808" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">我们可以选择已经在不同于我们自己的任务的下游任务上训练过的模型，并且仍然使用来自该模型主体的知识。</li><li id="4516" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">如果您的数据集足够大且通用，这可能完全没有必要，在这种情况下，您可以使用AutoModelForSequenceClassification或任何其他您必须使用类似BERT的检查点来解决的任务。事实上，如果是这样的话，我会强烈建议不要构建自己的头部。</li></ol><p id="0283" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看我的<a class="ae lb" href="https://github.com/rajlm10" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> GitHub </strong> </a>其他一些项目。可以联系我<a class="ae lb" href="https://rajsangani.me/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="mz">这里</em> </strong> </a> <strong class="kh ir"> <em class="mz">。</em> </strong>感谢您的配合！</p><p id="8ef3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这个，这里还有一些！</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/interpreting-an-lstm-through-lime-e294e6ed3a03"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">透过莱姆解读LSTM</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">了解如何通过LIME解释一个Keras LSTM，并深入到文本LIME库的内部工作…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op mt ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/locality-sensitive-hashing-in-nlp-1fb3d4a7ba9f"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">自然语言处理中的位置敏感哈希算法</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">关于如何通过区分位置来减少搜索空间以加快文档检索的实践教程…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="oq l om on oo ok op mt ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">处理具有高基数的要素</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">一个简单的实用程序，我用来处理具有许多唯一值的分类特征</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="or l om on oo ok op mt ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/powerful-text-augmentation-using-nlpaug-5851099b4e97"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">使用NLPAUG的强大文本增强！</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">通过文本增强技术处理NLP分类问题中的类别不平衡</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="os l om on oo ok op mt ob"/></div></div></a></div><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/regex-essential-for-nlp-ee0336ef988d"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">正则表达式对NLP至关重要</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">理解各种正则表达式，并将其应用于自然语言中经常遇到的情况…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ot l om on oo ok op mt ob"/></div></div></a></div><h1 id="abd2" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考</h1><ol class=""><li id="7544" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated"><a class="ae lb" href="https://huggingface.co/course/chapter3/1?fw=pt" rel="noopener ugc nofollow" target="_blank">令人惊叹的拥抱脸示例部分！</a></li><li id="9f91" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">数据集的引用和数据集<a class="ae lb" href="https://rishabhmisra.github.io/publications/" rel="noopener ugc nofollow" target="_blank">的引用</a></li></ol><pre class="mo mp mq mr gt ou ov ow ox aw oy bi"><span id="9153" class="nk ld iq ov b gy oz pa l pb pc">@article{misra2019sarcasm,<br/>  title={Sarcasm Detection using Hybrid Neural Network},<br/>  author={Misra, Rishabh and Arora, Prahal},<br/>  journal={arXiv preprint arXiv:1908.07414},<br/>  year={2019}<br/>}</span></pre></div></div>    
</body>
</html>