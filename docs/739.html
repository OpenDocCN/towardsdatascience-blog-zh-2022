<html>
<head>
<title>Integer-Only Inference for Deep Learning in Native C</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">原生C语言中深度学习的纯整数推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/integer-only-inference-for-deep-learning-in-native-c-e57f29a20adc#2022-01-26">https://towardsdatascience.com/integer-only-inference-for-deep-learning-in-native-c-e57f29a20adc#2022-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="d6c8" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">原生C语言中深度学习的纯整数推理</h1></div><div class=""><h2 id="5bf8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">教程:通过统一量化和定点表示，转换深度神经网络以部署在低延迟、低计算设备上。</h2></div><p id="53f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">纯整数推理允许压缩深度学习模型，以部署在低计算和低延迟设备上。许多嵌入式设备使用本机C编程，不支持浮点运算和动态分配。然而，通过统一量化和定点表示，小型深度学习模型可以部署到具有纯整数推理流水线的这种设备。</p><p id="3529" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们采用这些方法在网络接口卡(NIC)上部署深度强化学习(RL)模型(Tessler等人2021[1])。在不支持浮点运算的设备上成功部署RL模型需要<em class="lb"> O(微秒)</em>的推理延迟。小延迟限制抑制了通过GPU运行模型。为了实现我们的目标，我们使用了郝等人2020[2]指定的训练后量化(PTQ)技术和TensorRT的pytorch量化包。</p><p id="e9d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我们将学习郝等人2020[2]之后的量子化和PTQ技术。此外，我们将了解定点表示，并给出一个详细的示例，将在MNIST上训练的PyTorch分类器转换为原生C语言，以最小的性能损失进行纯整数推理。</p><p id="0a46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">目录</strong></p><p id="771a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Q <a class="ae lc" href="#452f" rel="noopener ugc nofollow">量化</a> <br/> <a class="ae lc" href="#e27f" rel="noopener ugc nofollow">训练后量化</a> <br/> ∘ <a class="ae lc" href="#e4f1" rel="noopener ugc nofollow">校准</a> <br/> <a class="ae lc" href="#039c" rel="noopener ugc nofollow">唯整数推理</a> <br/> ∘ <a class="ae lc" href="#03e6" rel="noopener ugc nofollow">定点表示</a> <br/> ∘ <a class="ae lc" href="#5d91" rel="noopener ugc nofollow">定点运算</a> <br/> <a class="ae lc" href="#59d3" rel="noopener ugc nofollow">教程—唯整数推理在原生c中用于MNIST分类</a> <br/> ∘ <a class="ae lc" href="#0321" rel="noopener ugc nofollow">模型训练和量化在Python中</a> <br/></p><h1 id="452f" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">量化</h1><p id="ab48" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">量化是将数值从一个大集合(通常是连续的)映射到一个较小集合的过程。在我们的例子中，我们感兴趣的是减小神经网络的大小和加快计算速度，以便它可以在低计算量、低内存的设备上运行推理。我们将集中讨论均匀整数量化，这样我们可以在整数域中执行矩阵乘法和卷积。更具体地说，我们将量化网络的输入和参数，从float32到int8。</p><p id="1754" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">均匀量化— </strong>在均匀量化中，输入<em class="lb">x∈【β，α】被变换到范围</em></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/551954ff6f3f616066ea12f821adc839.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*Efg8sgjMi0w7soYHAPPFCA.png"/></div></figure><p id="ffdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> b </em>是比特数<em class="lb">。</em>输入范围之外的值被剪切到最近的界限。在我们的例子中，变换后的范围是[-128，127]，为了对称，我们将使用[-127，127]。</p><p id="f03b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">均匀量化通过两个函数完成:</p><ol class=""><li id="862e" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated"><strong class="kh ir">量化- </strong>将float32值映射到int8范围，然后进行舍入和限幅(float32 → int8)</li><li id="5d7f" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated"><strong class="kh ir">反量化— </strong>将int32值映射到float32</li></ol><p id="558c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们想要最小化运算的数量，我们将使用<strong class="kh ir">比例量化</strong>来实现这些运算。</p><p id="df1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在标度量化中，输入范围为<em class="lb"> x ∈ [-α，α]，</em>并且校准最大α值(amax)以最大化精度。校准α后，通过乘以/除以比例因子<strong class="kh ir"> <em class="lb"> s </em> </strong>进行映射。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/e82c7d17530173b9603f99be173ab46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*Goon-NXvG6RIT_FF0PtitQ.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">比例因子计算，其中α最大值(a max)在校准期间计算，b=8位。</p></figure><p id="3145" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于比例量化，我们的操作定义如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/7af41ea6a606d823434e47bc5adbed13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*W58Pcfz5qhfwKcVlZaG-Mw.png"/></div></figure><p id="2569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">张量量化粒度</strong></p><p id="6e7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">张量粒度指的是共享量化参数(例如，比例因子)。在精度和计算成本之间存在权衡，其中用不同的比例因子缩放每个张量元素将导致最高的精度和计算成本，相反，用单个因子缩放整个张量导致最低的精度和计算成本。</p><p id="d931" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">线性层量化</strong></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/46f6b7fe0a8bcf87c90eefb568098f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*i5HBySSFbROn4rHsewVKng.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">PyTorch中实现的线性层，X是NxK输入张量，W是MxK权重张量，b是M长度偏移向量，Y是NxM输出张量。</p></figure><p id="812f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们用每个张量的粒度来量化层输入(所有张量元素共享相同的比例因子)。相比之下，层的权重以每行粒度进行量化(每行有不同的比例因子)。这种选择允许分解比例因子，并执行整数的矩阵乘法。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f9ccac6d0b941a7f52c4b156d34b6f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*izyVp232geD-LxK4H6mtEA.png"/></div></figure><p id="ee43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过将一个向量连接到输入并向图层权重添加另一个维度，可以轻松实现偏差量化。</p><p id="ee22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">卷积层量化</strong></p><p id="1e1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与线性情况一样，我们可以从分解比例因子中获得计算上的好处。这是通过为层输入选择每张量粒度和为层权重选择每通道粒度来实现的。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/a1524c08f0f71df48b918e5bf0fbe772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9L96Eg8rjHFln1IoQ1qGA.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">在PyTorch的实现之后，输入张量<em class="nj"> X </em>具有维度<em class="nj"> N，C_in，L </em>，输出张量<em class="nj"> Y </em>具有维度<em class="nj"> N，C_out，L_prime </em>，然后是具有步长1、膨胀1和<strong class="bd nk">无</strong>填充的1D卷积。</p></figure><h1 id="e27f" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">训练后量化</h1><p id="549d" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在训练后量化中，我们通过构建绝对值的直方图(一个用于层输入，一个用于参数)来校准每个模型层的比例因子。推理在图层输入的样本数据集上运行，而图层参数校准可以离线完成。</p><h2 id="e4f1" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">校准</h2><p id="66bc" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">校准部分选择量化精度损失最小的α。有三种主要方法来为每个直方图选择合适的α:</p><ol class=""><li id="f60b" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated">最大值—使用绝对值分布的最大值进行校准</li><li id="6972" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">熵——最小化量化分布和原始分布之间的KL散度</li><li id="7df0" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">百分点-选择与绝对值分布的第k个百分点相对应的条柱</li></ol><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nx"><img src="../Images/d8cd1072e3f214f6cf01c454225b16a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3UGYBOBrtfa4EnWfTrUnRQ.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">ResNet 50第三层输入和校准范围的绝对值直方图。图片摘自(郝等2020) [2]</p></figure><p id="d768" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦校准完成，我们可以在推理中构建一个量化的线性层。</p><h1 id="039c" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">纯整数推理</h1><p id="d2a9" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">到目前为止，我们讨论了用于执行线性运算的量化，例如卷积和整数的矩阵乘法。另一方面，非线性激活不适合均匀量化，应该近似。非线性函数可以用查找表、分段函数和其他方法来近似，这些方法值得专门撰写一篇文章。相反，我们将不讨论非线性近似，而是使用ReLU激活函数，因为它<strong class="kh ir">不需要近似。</strong></p><p id="a23b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，我们仍然将float32作为<em class="lb">量化/去量化</em>操作的输入/输出。当我们在<em class="lb">量化</em> / <em class="lb">去量化</em>操作期间用比例因子乘/除数值时，我们<strong class="kh ir">无法</strong>避免处理分数。为了处理这种整数运算，我们将使用定点表示法。</p><h2 id="03e6" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">定点表示法</h2><p id="bad7" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">定点表示法是一种用整数表示分数的方法。我们可以拆分组成整数的K位来表示一个数的整数部分和小数部分。使用符号幅度格式，我们保留1位表示符号，其他<em class="lb"> </em>位表示分数。基数将剩余的<em class="lb"> K-1 </em>位拆分为代表整数值的<em class="lb"> M </em>个最高有效位(MSB)和代表分数的<em class="lb"> N </em>个最低有效位(LSB)。<em class="lb"> M </em>和<em class="lb"> N </em>的选择导致表示范围和精度之间的折衷。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/721c21e8914e4d16ded143d578e21c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*1eIRe8sW7lyevt7utGCJBg.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">32位有符号整数的定点16位表示形式。16 LSB表示小数，15 MSB表示整数。</p></figure><p id="e43e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">浮点和它的定点表示之间的转换是通过乘以/除以2的定点值的幂来完成的。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a4fd7f4b1cbd41d8296efc3f455f20cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*TfT5KvDvDMtTDyO9a7OAuw.png"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">浮点数与其定点16位表示形式之间的转换。</p></figure><h2 id="5d91" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">定点算法</h2><p id="f973" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">定点运算的三条经验法则:</p><ol class=""><li id="e7c6" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated">两个定点数之和就是定点数。</li></ol><p id="ec65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.整数与定点数的乘积就是定点数。</p><p id="b5c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.两个定点数的乘积<strong class="kh ir">除以2的定点数的幂</strong>就是一个定点数。</p><p id="7abb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">定点表示用整数代替剩余的浮点运算。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oa"><img src="../Images/02e1f14754030861adf054791b5adebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6aDo2OJVohVzyuuovMRZQ.png"/></div></div><p class="mx my gj gh gi mz na bd b be z dk translated">具有量化图层输入和权重的纯整数线性图层的方案</p></figure><h1 id="59d3" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">教程—针对MNIST分类的纯整数C推理</h1><p id="4277" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我们将在PyTorch的MNIST数据集上训练一个简单的分类器。接下来，我们将网络参数量化为int8，并校准其比例因子。最后，我们将用原生c语言编写一个纯整数的推理代码。</p><h2 id="0321" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">Python中的模型训练和量化</h2><p id="1a19" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated"><strong class="kh ir">模型</strong> —一个多层感知器(MLP)，具有两个隐藏层(分别为128、64)，并在MNIST数据集上进行ReLU训练。MLP不包含使模型参数和计算开销最小化的偏差。</p><p id="13dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们在PyTorch中定义一个简单的MLP模型。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3ee4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并训练它十个纪元</p><pre class="mb mc md me gt od oe of og aw oh bi"><span id="a282" class="nl le iq oe b gy oi oj l ok ol">Epoch: 1 — train loss: 0.35650 validation loss: 0.20097<br/>Epoch: 2 — train loss: 0.14854 validation loss: 0.13693<br/>Epoch: 3 — train loss: 0.10302 validation loss: 0.11963<br/>Epoch: 4 — train loss: 0.07892 validation loss: 0.11841<br/>Epoch: 5 — train loss: 0.06072 validation loss: 0.09850<br/>Epoch: 6 — train loss: 0.04874 validation loss: 0.09466<br/>Epoch: 7 — train loss: 0.04126 validation loss: 0.09458<br/>Epoch: 8 — train loss: 0.03457 validation loss: 0.10938<br/>Epoch: 9 — train loss: 0.02713 validation loss: 0.09077<br/>Epoch: 10 — train loss: 0.02135 validation loss: 0.09448<br/>Evaluate model on test data<br/>Accuracy: 97.450%</span></pre><p id="9826" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">模型量化— </strong> TensorRT是一款用于高性能深度学习推理的SDK。它包含pytorch-quantization，允许pytorch模型的直接量化(假的，量化感知训练，PTQ)。</p><p id="b4bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从导入相关模块开始。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><ul class=""><li id="9370" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la om mo mp mq bi translated"><code class="fe on oo op oe b">quant_nn</code>包含PyTorch层的量化版本，如nn.Linear。</li><li id="ee2f" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la om mo mp mq bi translated"><code class="fe on oo op oe b">calib</code>是用于构建直方图和计算比例因子的校准模块。</li><li id="0266" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la om mo mp mq bi translated"><code class="fe on oo op oe b">quant_modules</code>允许用量化版本动态替换PyTorch层。</li><li id="7408" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la om mo mp mq bi translated"><code class="fe on oo op oe b">QuantDescriptor</code>定义如何量化张量。</li></ul><p id="6248" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一步是使用直方图校准器定义量化。然后，我们用期望的量化描述来设置我们的量化线性层。最后，通过调用<em class="lb"> initialize()，用量化版本为PyTorch模块打补丁。</em></p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="a98f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们只在<strong class="kh ir"> </strong>定义量化方案和猴子修补PyTorch模块之后加载训练好的MLP模型<strong class="kh ir">。</strong></p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="c3e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们定义一个函数，在推断过程中输入用于校准的统计数据。首先，我们禁用量化来从浮点值构建直方图。然后，我们执行推理，并在我们希望使用量化模型进行推理的情况下重新启用量化。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="7b23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们定义一个函数来计算amax值(比例因子= 127 / amax)。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="5d71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了减少推理时的运算次数，我们离线计算比例因子值。为了避免定点除法，我们可以将它们反转，并在c中乘以反转后的值。</p><p id="d555" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">可以将</em> <strong class="kh ir"> <em class="lb">反量化</em> </strong> <em class="lb">比例因子组合成一个变量。尽管如此，它可能会降低精度，因为对于定点16，结果数可能非常小(参见教程中的</em> <strong class="kh ir"> <em class="lb">反量化</em> </strong> <em class="lb">部分的示例)。</em></p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3d7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们定义了函数，我们就可以运行代码了。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="1537" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">本机C代码中的模型推理</h2><p id="fcd8" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">实施MLP模型需要定义:</p><ol class=""><li id="6990" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated">矩阵乘法</li><li id="c848" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">定点乘法</li><li id="c8cd" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">数字转换</li><li id="94a9" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">去量化</li><li id="3caf" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">热卢</li><li id="77fc" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">argmax(代替softmax进行推理)</li><li id="7c2e" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">线性层</li></ol><p id="9fb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设神经网络的架构和参数是预先确定的，并且我们不能使用动态分配，我们将不定义矩阵和张量的一般结构。相反，在本教程中，我们将把矩阵/张量视为展平的1D阵列，并使用它们的形状来应用运算。</p><p id="cc5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">矩阵乘法</strong></p><p id="7600" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在两个平坦的int8矩阵之间执行矩阵乘法。为了避免溢出，结果存储在一个比int8大的整数中。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">Y = XW，其中X是NxK矩阵，W是KxM矩阵，结果Y是NxM矩阵。乘法和累加运算以大整数类型存储，以避免溢出</p></figure><p id="73c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">定点乘法</strong></p><p id="439d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两个定点16数之间的简单乘法如下所示:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f3d899184ed5d1d3e3bd7af938fcb850.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*QpDfd1_OlOHpVvQDCs0XMQ.png"/></div></figure><p id="ace8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们需要将结果除以2的16次方。</p><p id="55c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在C中，左移运算符可以看作是乘以2的幂，而右移运算符可以看作是除以2的幂。我们可以在C中定义一个定点乘法函数，其中两个输入都用相同的定点值表示，如下所示:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="c02d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当乘以定点值时有两个主要问题</p><ol class=""><li id="29dd" class="mi mj iq kh b ki kj kl km ko mk ks ml kw mm la mn mo mp mq bi translated">舍入</li><li id="65d1" class="mi mj iq kh b ki mr kl ms ko mt ks mu kw mv la mn mo mp mq bi translated">泛滥</li></ol><p id="73f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">舍入— </strong>默认情况下，使用左移和右移的乘法/除法会导致截断(地板)。一种简单的舍入方法是通过round half up方法舍入到最接近的整数。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9b88353bf63ee40cfb83728c5c714ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*LhmC-8fwsucjIneBMyKE5A.png"/></div></figure><p id="aa20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">带舍入的定点乘法如下所示:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">采用四舍五入法的定点乘法。</p></figure><p id="d37e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">溢出— </strong>很直观的看到，两个大定点数的乘积在右移之前很容易溢出。一个简单的解决方案是将值存储在更大的类型中(如int64)，但在某些情况下，定点数已经用最大的可用类型表示了。一种替代方法是按部分执行定点乘法。这意味着我们在两个整数的乘积不会溢出的假设下，将定点数分成一个整数和一个小数部分。</p><p id="1fca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">部分乘的定点乘法实现如下所示:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3891" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经讨论了定点乘法，我们可以编写量化函数了。</p><p id="c02a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">量化</strong></p><p id="13c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">量化函数将两个定点数、层的输入和比例因子相乘，然后裁剪到int8范围[-127，127]。在执行定点乘法之前，我们通过将输入与反转比例因子的乘积和我们的限幅范围的边界进行比较来降低溢出的风险。与标准的定点乘法相反，在量化中，我们希望乘积是一个四舍五入的整数。因此，我们不会将输出转换为定点。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div><p class="mx my gj gh gi mz na bd b be z dk translated">定点量化函数的实现。FXP值= 16，INT8最大值= 127，舍入常量= (1 &lt;&lt; FXP值— 1)。注意——我们不像在定点乘法实现中那样右移最终值，因为(input*scale_factor)的定点表示代表一个四舍五入的整数。</p></figure><p id="a30e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">反量化</strong></p><p id="1017" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">反量化操作需要将一个整数(矩阵与int8相乘的乘积)与图层输入(标量)和图层权重(按每行粒度量化的向量)的反向比例因子相乘。这意味着我们需要混合常规和定点乘法。一般来说，整数和定点数相乘有很高的溢出风险。另一方面，当定点数表示小分数时(在我们的例子中确实如此)，将它们相乘会导致精度损失。当他们右移前的乘积可能小于我们右移的值时，就会出现这种情况。因此，它们的输出会被错误地归零。<br/>为了说明这一点，在下面的例子中，我们用两个整数乘以不同大小的值:50，500。我们比较右移前或右移后的乘法。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><pre class="mb mc md me gt od oe of og aw oh bi"><span id="7e57" class="nl le iq oe b gy oi oj l ok ol">a: 0.001000 b: 0.000400 int: 50, c_before: 1 c_after: 0<br/>a: 0.001000 b: 0.000400 int: 500, c_before: 13 c_after: 0<br/>a: 0.100000 b: 0.000400 int: 50, c_before: 130 c_after: 150<br/>a: 0.100000 b: 0.000400 int: 500, c_before: 1300 c_after: 1500<br/>a: 0.001000 b: 0.400000 int: 50, c_before: 1300 c_after: 1300<br/>a: 0.001000 b: 0.400000 int: 500, c_before: 13000 c_after: 13000<br/>a: 0.100000 b: 0.400000 int: 50, c_before: -14 c_after: 131050<br/>a: 0.100000 b: 0.400000 int: 500, c_before: -140 c_after: 1310500</span></pre><p id="cf5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，对于小的<em class="lb"> a </em>和<em class="lb"> b </em>值，我们受益于右移前的乘法，但当它们的值较大时，我们会溢出。另一方面，当我们在(精度损失)之后乘时，我们得到稍微不同的结果。最后，只有当<em class="lb"> a=0.001 </em>和<em class="lb"> b=0.4 </em>时，两种方法才输出相同的结果。</p><p id="85fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个简单的解决方案是计算比例因子的乘积，并将其与1(定点)进行比较。当它们大于1时，我们有过度拟合的风险，我们应该在定点乘法后乘以矩阵值。否则，我们在定点乘法中乘以矩阵值。</p><p id="eb90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将反量化定义为:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="00a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> ReLU </strong></p><p id="6d3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ReLU的C实现很简单</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="0799" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> argmax </strong></p><p id="4382" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们通过NxM矩阵的列来计算argmax(其中N是批次大小，M是标签的数量)。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="dd4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">线性图层</strong></p><p id="5e08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性层由所有先前定义的部分组成:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="6d52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> MLP </strong> —将几个线性图层组合在一起</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="19ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于本教程，模型参数和比例因子编码在一个头文件中，并且使用C类型从python调用<em class="lb"> run_mlp </em>函数。</p><h2 id="b6b0" class="nl le iq bd lf nm nn dn lj no np dp ln ko nq nr lp ks ns nt lr kw nu nv lt nw bi translated">模型评估</h2><pre class="mb mc md me gt od oe of og aw oh bi"><span id="a805" class="nl le iq oe b gy oi oj l ok ol">Evaluating integer-only C model on test data</span><span id="f7ce" class="nl le iq oe b gy os oj l ok ol">Accuracy: 97.27%</span></pre><p id="f1fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这比python中的浮点模型少了0.18%。</p><h1 id="45a3" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">结论</h1><p id="7b5a" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在本文中，我们讨论了训练后量化、定点表示，以及如何使用PyTorch中完全精确训练的分类器在本机C中运行纯整数推理。</p><p id="8586" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的代码，包括卷积神经网络版本，请访问Github知识库:<a class="ae lc" href="https://github.com/benja263/Integer-Only-Inference-for-Deep-Learning-in-Native-C" rel="noopener ugc nofollow" target="_blank">https://Github . com/benja 263/Integer-Only-Inference-for-Deep-Learning-in-Native-C</a></p><p id="d212" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您的阅读！</p><h1 id="34ff" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">参考</h1><p id="4096" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">[1] Tessler，c .，Shpigelman，y .，Dalal，g .，Mandelbaum，a .，卡萨科夫，D. H .，Fuhrer，b .，Chechik，g .，和Mannor，S. (2021)。<em class="lb">数据中心拥塞控制的强化学习</em>。【http://arxiv.org/abs/2102.09337 T4】</p><p id="8dd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]吴，h .，贾德，p .，张，x .，伊塞耶夫，m .，&amp;米契克维丘斯，P. (2020)。<em class="lb">深度学习推理的整数量化:原理与实证评估</em>。<a class="ae lc" rel="noopener ugc nofollow" target="_blank" href="/&lt;div class=&quot;csl-entry&quot;&gt;Wu, H., Judd, P., Zhang, X., Isaev, M., &amp;#38;%20Micikevicius,%20P.%20(2020).%20%3Ci%3EInteger%20Quantization%20for%20Deep%20Learning%20Inference:%20Principles%20and%20Empirical%20Evaluation%3C/i%3E.%20http://arxiv.org/abs/2004.09602%3C/div%3E">http://arxiv.org/abs/2004.09602</a></p></div></div>    
</body>
</html>