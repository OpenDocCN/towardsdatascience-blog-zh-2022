# 梯度下降 VS 正则化:使用哪一个？

> 原文：<https://towardsdatascience.com/gradient-descent-or-regularization-which-one-to-use-f02adc5e642f>

## 为了更好地理解，对梯度下降和正则化进行概述

![](img/a6837b940759efd490199dbfc96c88a2.png)

巴勃罗·加西亚·萨尔达尼亚在 [Unsplash](https://unsplash.com/s/photos/ways?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

W 当迈出机器学习的第一步时，有很多东西需要学习和理解。此外，一些 ML 模型可能看起来彼此非常相似；有时候，很难真正理解它们之间的区别。这是我第一次接触正则化和梯度下降时的情况:它们对我来说太相似了，以至于不能理解它们之间的区别。事实上，正则化和梯度下降都使用成本函数，但它们之间的差异是成本函数的类型和我们对这些方法的使用。

在这篇文章中，我将试图阐明它们的区别，目的是帮助你更好地理解这些方法。

# 1.正规化

在机器学习中，我们通常应用套索和脊正则化方法(也分别称为 L1 和 L2)，但当“标准”模型过度拟合时，我们会使用它们。

例如，考虑一个简单的线性回归模型；如果在训练集上得到一个好的决定系数值(例如，接近 1 ),但在测试集上得到一个差的决定系数值(例如，接近 0 ),那么您将面临过度拟合，应用 Lasso 或 Ridge 回归模型甚至可以在测试集上得到好的结果。此外，如果你不知道是否使用套索或脊方法，你可以在这里阅读我的文章:

</understanding-l1-and-l2-regularization-93918a5ac8d0>  

在机器学习的背景下，正则化是将系数缩小到零的过程，“阻止”学习复杂的问题；这个过程是通过所谓的成本函数来实现的。

套索和岭方法具有“预先固定的”成本函数；这意味着它们是“内置”成本函数的这意味着 Lasso 是用线性回归模型的方程加上等于系数大小绝对值的成本函数构建的。相反，岭是用线性回归模型的方程加上等于系数大小的平方值的成本函数构建的。

由于这些是监督学习技术(如果你想了解更多关于监督和非监督学习的知识，你可以在这里阅读我的文章)，我们的目标是找到最佳超参数以避免过度拟合，这个超参数可以是 0(因此，我们“返回”到简单的线性回归模型)或无穷大(意味着模型已经被算法高度惩罚)。

# 2.梯度下降

梯度下降是一种学习算法，通过最小化给定的成本函数来工作；所以梯度下降和正则化之间的主要区别是:

*   正则化方法有一个“预定义的”成本函数，不像梯度下降(它有一个“给定的”成本函数，但我们将在后面看到它是如何工作的)
*   与梯度下降不同，当模型过拟合时，使用正则化

有一个重要的概念要记住:**模型通过最小化成本函数来学习，**这就是为什么梯度下降是有用的。

你可能知道，在数学中，当我们需要找到一个函数的最小值时，我们会用到导数。当我们有一个多变量的问题时，我们必须进行多重求导:简化很多，这就是梯度。

因此，梯度下降算法使用梯度(也称为多维导数)来寻找函数的最小值，并通过多次迭代来实现，直到找到函数的最小值。

比如，假设我们有一个函数 f(x)，其中 x 是一个多元的元组:x = (x1，x2，…，xn)。假设 f(x)的梯度由∇f(x 给出)(倒三角形是标识梯度的符号)。在任何迭代 t，我们将用 x[t]表示元组 x 的值。对于每个迭代 t，梯度下降的训练工作如下:

x[t] = x[t-1] — 𝜂∇f(x[t-1])

其中𝜂被称为学习率(这是一个超参数)。在这个过程的最后，算法将找到使函数 f(x)最小的变量(x1，x2，…，xn)的值(更多细节参见[这里](https://machinelearningmastery.com/a-gentle-introduction-to-gradient-descent-procedure/))。

但是，最后，我们发现了什么？像在机器学习中一样，我们找到了最适合给定数据的函数，但我们发现它最小化了一个成本函数。

# **结论**

本文的目的是阐明梯度下降和正则化之间的区别；概括地说，这些差异是:

*   正则化方法有一个“预定义”的成本函数，不像梯度下降法，成本函数是给定函数的梯度。
*   当模型过度拟合时，使用正则化，这与梯度下降不同，梯度下降不考虑过度拟合(我们可能会在验证后发现模型过度拟合，但**我们不使用梯度下降来防止过度拟合**)。

*让我们连在一起！*

[*中等*](https://federicotrotta.medium.com/)

[*LINKEDIN*](https://www.linkedin.com/in/federico-trotta/) *(向我发送连接请求)*

*如果你愿意，你可以* [*订阅我的邮件列表*](https://federicotrotta.medium.com/subscribe) *这样你就可以一直保持更新了！*

考虑成为会员:你可以免费支持我和其他像我一样的作家。点击 [*这里的*](https://federicotrotta.medium.com/membership) *成为会员。*