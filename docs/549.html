<html>
<head>
<title>Multi-class Text Classification using BERT and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT和张量流的多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d#2022-01-19">https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d#2022-01-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="64d1" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">基于BERT和张量流的多类文本分类</h1></div><div class=""><h2 id="c56a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从数据加载到预测的分步教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/97a844acea1c0cb2627a75e6eea72fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tt0vAnphtFI6JtTp"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@alfonsmc10?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿尔方斯·莫拉莱斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="fd53" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ol class=""><li id="ab3d" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><a class="ae ky" href="#a31f" rel="noopener ugc nofollow">简介</a></li><li id="982e" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#5418" rel="noopener ugc nofollow">数据准备</a> <br/> 2.1 <a class="ae ky" href="#850b" rel="noopener ugc nofollow">加载数据集</a> <br/> 2.2 <a class="ae ky" href="#5a24" rel="noopener ugc nofollow">【可选】观察随机样本</a> <br/> 2.3 <a class="ae ky" href="#ca58" rel="noopener ugc nofollow">在训练和测试集中拆分</a></li><li id="d443" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#98ee" rel="noopener ugc nofollow">数据建模</a> <br/> 3.1 <a class="ae ky" href="#f7ca" rel="noopener ugc nofollow">用TensorfFlow Hub加载BERT</a><br/>3.2<a class="ae ky" href="#6476" rel="noopener ugc nofollow">【可选】观察语义文本相似度</a> <br/> 3.3 <a class="ae ky" href="#2126" rel="noopener ugc nofollow">创建并训练分类模型</a> <br/> 3.4 <a class="ae ky" href="#3349" rel="noopener ugc nofollow">预测</a> <br/> 3.5 <a class="ae ky" href="#253d" rel="noopener ugc nofollow">盲集评估</a></li><li id="dd60" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#0d89" rel="noopener ugc nofollow">[可选]保存并加载模型以备将来使用</a></li><li id="3639" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated"><a class="ae ky" href="#779c" rel="noopener ugc nofollow">参考文献</a></li></ol><h1 id="a31f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.介绍</h1><p id="ee4d" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">在这篇文章中，我们将开发一个多类文本分类器。</p><p id="700b" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">分类的任务是指对一个给定的观察值预测一个类别。由于这个原因，训练这样一个模型唯一需要的输入是一个数据集，它包括:</p><ul class=""><li id="c638" class="lr ls it lt b lu nb lw nc ly ng ma nh mc ni me nj mg mh mi bi translated">文本示例</li><li id="3abd" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nj mg mh mi bi translated">关联标签</li></ul><p id="3f58" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">出于这篇文章的目的，我们需要知道<strong class="lt iu">BERT</strong><strong class="lt iu"/>(<strong class="lt iu">B</strong>I direction<strong class="lt iu">E</strong>n coder<strong class="lt iu">R</strong>presentations from<strong class="lt iu">T</strong>transformers)是一个基于transformers的机器学习模型，即能够学习单词之间上下文关系的注意力组件。更多细节可在参考文献中找到。</p><p id="2f74" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">有趣的是，我们将为非英语文本开发一个分类器，我们将展示如何通过从TensorFlow Hub导入不同的BERT模型来处理不同的语言。</p><h1 id="5418" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.数据准备</h1><h2 id="850b" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">2.1加载数据集</h2><p id="10fa" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">在之前的一篇文章中，我们分析了罗马图书馆的读者评论公开数据集，该数据集由“<a class="ae ky" href="https://www.bibliotechediroma.it/" rel="noopener ugc nofollow" target="_blank"> <em class="nw">罗马图书馆研究院</em></a>”⁴.公开提供在没有标签的情况下，我们使用主题建模(一种无监督的技术)来发现读者评论中重复出现的主题，从而通过推理确定所借书籍的主题和读者的兴趣。</p><p id="c788" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们现在将使用前面分析中出现的主题作为标签来对用户评论进行分类。</p><p id="38d2" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">值得注意的是，<strong class="lt iu">之后的步骤可以应用于任何包含<strong class="lt iu">至少两列</strong>的数据集</strong>，分别用于<strong class="lt iu">文本样本</strong>及其<strong class="lt iu">标签</strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/f203f9b1b712eec73275f9b3ed119451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8mRxTGlFVytcjYRFpmM4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="51e9" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">由于数据集包含来自罗马图书馆的用户评论，他们的语言是意大利语。我们从之前的分析中随机抽取了五个主题，每个主题对应一个标签。它们的分布如下:</p><ol class=""><li id="d8ed" class="lr ls it lt b lu nb lw nc ly ng ma nh mc ni me mf mg mh mi bi translated">关于女性在社会中的地位的评论，或者有着强势女性主角的小说(n=205，25.5%)</li><li id="de32" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">专辑和音乐会的评论，或音乐家的传记(n=182，22.64%)</li><li id="72d0" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">关于经济和社会政治状况的书籍和论文的评论(n=161，20.02%)</li><li id="dca3" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">与日本或日本文化相关的评论(n=134，13.67%)</li><li id="bda2" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">科技泄密论文综述(122篇，15.17%)</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/de1930fcdff8a1e6d693f19c57968b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*52fsv6Fxl1ByMRBvbS8S4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="77b3" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">出于分类的目的，我们需要数字标签。因此，我们将主题描述映射到整数，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4b245ed0ebc7f2a53240c73058b688d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*m1v0cdxPQ6iEDwkea0FoUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="5a24" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">2.2[可选]观察随机样本</h2><p id="aa72" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">虽然对文本分类的目标来说，这不是绝对必要的，但是我们可能想要检查来自不同主题的一些随机样本，以便更好地理解数据。</p><p id="edc7" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">因此，我们定义了一个函数，它将一个列上的过滤条件作为输入，并打印一个满足该条件的随机评论，以及一个可读性更好的英文翻译:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3f49" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">现在，让我们观察一些样品。通过拨打<code class="fe oc od oe of b">print_rand_example(df, "Labels", 1)</code>，我们可以看到一些顾客对女性及其社会地位的评论:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/a4e2200c96aea4f75a1f85c6762b8666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L4nb3SyUaY0_c6JU0vqrhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者输出的<code class="fe oc od oe of b">print_rand_example(df, "Labels", 1). </code>图像。</p></figure><p id="13c1" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">当阅读评论的第一句话时，之前通过无监督方法制造的主题描述似乎是合理的。我们可以从同一个主题看另一个随机样本，同样调用<code class="fe oc od oe of b">print_rand_example(df, "Labels", 1)</code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/b52dc6aa10934e57050b530191a41afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHHO2I9iuzA2l0fvLAcG2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="1218" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">日语相关的话题呢？让我们通过<code class="fe oc od oe of b">print_rand_example(df, "Labels", 3)</code>了解一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/a44b9cbfb37a68c987429ff7059be58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2QJJ6uhXIU6tRcXpsO-Rg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="ca58" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">2.3在训练和测试装置中的分离</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h1 id="98ee" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.数据建模</h1><h2 id="f7ca" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">3.1用TensorFlow Hub加载BERT</h2><p id="1284" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">TensorFlow Hub是经过训练的机器学习models⁵.的存储库</p><p id="8c82" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">数据科学家可以方便地从TensorFlow Hub加载大型复杂的预训练模型，并在需要时重用它们。</p><p id="2145" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">有趣的是，当我们在TensorFlow Hub上搜索“<em class="nw"> bert </em>”时，我们可能还会应用问题域(分类、嵌入、…)、架构、语言等过滤器，以方便检索更适合我们需求的模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/a08c065900bf5efc435377a789cf3c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hC5bghgAS1FPuGZRw0FPdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在TensorFlow Hub⁵.网站上搜索“bert”的结果图片作者。</p></figure><p id="790a" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">在这个例子中，我们使用了<code class="fe oc od oe of b">universal-sentence-encoder-cmlm/multilingual-base</code> ⁶模型，这是一个支持100多种语言的通用句子编码器。如参考paper⁷.中所述，使用条件屏蔽语言模型来训练它</p><p id="bc29" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们要实现的是把文本变成捕捉句子级语义的高维向量。因此，我们从TensorFlow Hub提供的端点加载<code class="fe oc od oe of b">preprocessor</code>和<code class="fe oc od oe of b">encoder</code>层，并定义一个简单的函数从输入文本中获取嵌入。</p><p id="2df2" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">重要的一点是，可以根据任务和输入语言选择导入任何偏好的模型。</p><p id="7748" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">由于模型基于BERT transformer架构，它将生成形状为<code class="fe oc od oe of b">[batch size, 768]</code>的<code class="fe oc od oe of b">pooled_output</code>(整个序列的输出嵌入)，如下例所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/96318705ae3dfbf9ab965efdf928db20.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*u92AA5qBTxZRbBvq_-5ecg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="6476" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">3.2[可选]观察语义文本相似性</h2><p id="14e8" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">由于嵌入提供了句子级语义的向量表示，我们可能想要观察不同文本序列之间的相似性。</p><p id="b95b" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">为此，我们绘制了通过<code class="fe oc od oe of b">cosine similarity</code> ⁸计算的不同文本样本的语义文本相似度作为follows⁹:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="65a7" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">例如，我们期望两个几乎相同的句子，如:</p><ul class=""><li id="dfe6" class="lr ls it lt b lu nb lw nc ly ng ma nh mc ni me nj mg mh mi bi translated">这本书很有趣</li><li id="7364" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me nj mg mh mi bi translated"><em class="nw"> Il romanzo è interessante </em>(小说有趣)</li></ul><p id="3d21" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">将显示出高度的语义相似性，并且我们还期望这两个句子将与具有不同含义的第三个句子共享可比较的相似性度量，并且事实正是如此:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/3cdefab88e3df75f7232fd8236ad3716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DNny2o2jGwDDV50m9NLfoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="6ed6" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">由于模型的性质，我们可以有效地估计不同语言中句子之间的语义相似度:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/94fc8b61aa8366690e9abc02a572ff13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSAPtgH7bXMhSRxA9jf3Tw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="2126" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">3.3创建和训练分类模型</h2><p id="97da" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">由于我们面临着一个多类分类问题，并且我们之前注意到我们的主题分布略有不平衡，我们可能希望在模型训练期间观察不同的度量。</p><p id="6c3c" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">由于这个原因，我们定义了函数来分别计算训练期间每个类的<code class="fe oc od oe of b">precision</code>、<code class="fe oc od oe of b">recall</code>和<code class="fe oc od oe of b">F1 score</code>，然后返回这些类的平均值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/c4a5530dd1fdcd5a063e55774ea32660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cft8Kg-NpqPgOjuPqB6e_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">精确度、召回率和F1分数的定义。图片修改自⁰.维基百科</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="c226" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们现在将模型定义为<code class="fe oc od oe of b">preprocessor</code>和<code class="fe oc od oe of b">encoder</code>层，后面是<code class="fe oc od oe of b">dropout</code>和<code class="fe oc od oe of b">dense</code>层，具有<code class="fe oc od oe of b">softmax</code>激活函数和等于我们想要预测的类别数量的输出空间维度:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="7a1c" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">一旦我们定义了模型的结构，我们就可以编译并适应它。我们选择训练20个时期的模型，但是我们也使用<code class="fe oc od oe of b">EarlyStopping</code>回调以便在训练期间监控<code class="fe oc od oe of b">validation loss</code>:如果度量在至少3个时期内没有改善(<code class="fe oc od oe of b">patience = 3</code>)，则训练被中断，并且来自<code class="fe oc od oe of b">validation loss</code>显示最佳值(即最低值)的时期的权重被恢复(<code class="fe oc od oe of b">restore_best_weights = True</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/0140b34edefbdd62c28835378bf95d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiQaLFn6dto73JkbyqTwog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练日志。图片作者。</p></figure><p id="4ce8" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">最后，我们可以绘制训练过程中每个受监控指标的假设值，并比较训练和验证曲线:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/9899cdde52eab79f07bfab68ad0c3c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7QmS74MqoIlRTvfd9-T-Iw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练历史。图片作者。</p></figure><h2 id="3349" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">3.4预测</h2><p id="b876" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">模型现在已经准备好了，是时候测试一些预测了:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/ff7d3d251e09608b2bbe9a570fffdddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySqTxcGufyi2_fFVyLoJig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="fbdf" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">给定这三个输入句子，我们期望模型分别预测主题id<strong class="lt iu">3</strong>(<code class="fe oc od oe of b">Japan/Japanese/..</code>)、<strong class="lt iu"> 1 </strong> ( <code class="fe oc od oe of b">Woman/Women/..</code>)和<strong class="lt iu"> 0 </strong> ( <code class="fe oc od oe of b">Economy/Politics/..</code>)。</p><p id="9bfb" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">为了测试我们的假设，我们定义了一个简单的函数包装<code class="fe oc od oe of b">model.predict</code>。特别是，当我们试图在五个可能的标签之间进行分类时，<code class="fe oc od oe of b">model.predict</code>将返回一个大小为五的<code class="fe oc od oe of b">numpy.ndarray</code>。我们在模型的最后一层使用的<code class="fe oc od oe of b">softmax</code>激活函数实际上提供了目标类的离散概率分布。因此，我们可以简单地采用与较高概率(<code class="fe oc od oe of b">np.argmax</code>)相关联的指数来推断预测的标签:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="5bff" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">上面的代码片段产生了预期的结果:<code class="fe oc od oe of b">[3,1,0]</code>。</p><h2 id="253d" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">3.5盲集评估</h2><p id="287c" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">我们最初将数据集分为训练集和测试集，但在训练和验证过程中同时使用了这两个数据集。</p><p id="9369" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">为了公正地评估我们的性能，我们在一个新的数据集上评估预测的质量，该数据集包含模型在训练(盲集)期间没有“T22”看到的观察值:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/4788c514d6efe14069a26c5aa6895f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8-yThqeeaI1Zz2S35PlcA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">盲评估测试集。图片作者。</p></figure><p id="4748" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们可以通过使用数据准备部分中使用的相同代码来观察主题分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/f8e89277db1179fa6c8baf0789344245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upCkFoXKCSBAw_0FfOjylg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="b762" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">通过遵循用于准备训练集和验证集的相同步骤，我们可以将主题描述映射到数字标签，并检查一些随机样本。</p><p id="cd35" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">例如，让我们检查来自id为1的主题的评论:<code class="fe oc od oe of b">print_rand_example(test_set, "Labels", 1)</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/4ecdbba77475bed20b6299f979c26994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFeiv20HV4DR1yUPSytEEA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="af03" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">让我们也观察来自id为3的主题的评论:<code class="fe oc od oe of b">print_rand_example(test_set, "Labels", 3)</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/f2b5482147cad21592cfba79ecc02b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3eYcSbaDs2EazROJicdSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="1024" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们现在可以测试盲集性能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/76048dc1f13bed3e058be5871617ee8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*NLErfUn_ALDLmXFU9EJpPA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h1 id="0d89" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.[可选]保存并加载模型以备将来使用</h1><p id="d752" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">这项任务对于文本分类模型的开发来说不是必不可少的，但它仍然与机器学习问题有关，因为我们可能希望保存模型并在需要时加载它以用于未来的预测。</p><p id="19de" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">通过调用<code class="fe oc od oe of b">model.save</code>，可以将模型保存为<code class="fe oc od oe of b">SavedModel</code>格式，其中包括模型架构、权重和调用函数的追踪张量流子图。这使Keras能够恢复内置层和自定义对象:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="2541" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">我们现在可以根据需要加载模型以供将来使用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0a1e" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated"><em class="nw">就是这样！</em></p><h1 id="779c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.参考</h1><p id="542b" class="pw-post-body-paragraph mo mp it lt b lu lv ju mq lw lx jx mr ly ms mt mu ma mv mw mx mc my mz na me im bi translated">[1]德夫林，雅各布；张明伟；李，肯顿；图塔诺娃、克里斯蒂娜、<em class="nw">伯特:语言理解的深度双向变形金刚</em>前期训练2018、<a class="ae ky" href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" rel="noopener ugc nofollow" target="_blank">arXiv</a>:<a class="ae ky" href="https://arxiv.org/abs/1810.04805v2" rel="noopener ugc nofollow" target="_blank">1810.04805 v2</a></p><p id="e2bf" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[2]阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯、卢卡兹·凯泽、伊利亚·波洛舒欣，“<em class="nw">关注是你所需要的一切</em>”，2017年，<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762 </a></p><p id="ccdd" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[3]<a class="ae ky" rel="noopener" target="_blank" href="/romes-libraries-readers-comments-analysis-with-deep-learning-989d72bb680c">https://towards data science . com/romes-libraries-readers-comments-analysis-with-deep-learning-989d 72 bb 680 c</a></p><p id="2ab0" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[4]<a class="ae ky" href="https://www.bibliotechediroma.it/it/open-data-commenti-lettori" rel="noopener ugc nofollow" target="_blank">https://www . bibliotechediroma . it/it/open-data-commenti-lettori</a></p><p id="0423" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated"><a class="ae ky" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/</a></p><p id="4ecc" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[6]<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1" rel="noopener ugc nofollow" target="_blank">https://tfhub . dev/Google/universal-sentence-encoder-cmlm/multilingual-base/1</a></p><p id="84aa" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[7]，，，杨，Daniel Cer，Jax Law，Eric Darve，<em class="nw">用条件掩蔽语言模型学习通用句子表征</em>，2021，<a class="ae ky" href="https://arxiv.org/abs/2012.14388" rel="noopener ugc nofollow" target="_blank"> arXiv:2012.14388 </a></p><p id="240c" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cosine_similarity</a></p><p id="da0b" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated"><a class="ae ky" href="https://www.tensorflow.org/hub/tutorials/bert_experts" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/hub/tutorials/bert_experts</a></p><p id="0c64" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Precision_and_recall</a></p><p id="c97c" class="pw-post-body-paragraph mo mp it lt b lu nb ju mq lw nc jx mr ly nd mt mu ma ne mw mx mc nf mz na me im bi translated">[11]<a class="ae ky" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/saved_model</a></p></div></div>    
</body>
</html>