<html>
<head>
<title>Hypertuning for Both TensorFlow &amp; PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow和PyTorch的超调</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hypertuning-for-tensorflow-pytorch-a6e117e4655e#2022-01-13">https://towardsdatascience.com/hypertuning-for-tensorflow-pytorch-a6e117e4655e#2022-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="30b7" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">TensorFlow和PyTorch的超调</h1></div><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d89ff40f4abd6b03e497ece9509c1db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KnhiPkXlyzAscdBSIpElUQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">超参数调整就像用数据打碟一样——照片来源(<a class="ae kc" href="https://unsplash.com/photos/ZfCVTJ30yoc" rel="noopener ugc nofollow" target="_blank">unsplash.com/photos/ZfCVTJ30yoc</a>)</p></figure><blockquote class="kd ke kf"><p id="76e5" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><em class="iq">深度学习库的根本问题是，它们是为单次内存运行而设计的，其唯一目的是最小化损失——而在现实中，调整一个架构需要多次运行，工作流需要持续，训练损失只是模型评估的开始。</em></p></blockquote></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><figure class="ln lo lp lq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lm"><img src="../Images/49505c35ad8b5500724148007160421f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nA-k6HRURTdu5XDHWNPm4Q.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">红色非常受欢迎，尽管较少的图层类型功能较少-图片由作者提供</p></figure><h1 id="c24b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">物以类聚。</h1><p id="bc18" class="pw-post-body-paragraph kg kh iq kj b kk mp km kn ko mq kq kr mr ms ku kv mt mu ky kz mv mw lc ld le ij bi translated">一年前，我开始构建一个库，让<strong class="kj ir"> TensorFlow </strong>和<strong class="kj ir"> PyTorch </strong>的实验跟踪和参数调整更加容易。这些库共享相同的底层概念，因此包装它们很容易:</p><ul class=""><li id="e74f" class="mx my iq kj b kk kl ko kp mr mz mt na mv nb le nc nd ne nf bi translated"><em class="ki"> fn_build — </em>为架构。</li><li id="ce2d" class="mx my iq kj b kk ng ko nh mr ni mt nj mv nk le nc nd ne nf bi translated"><em class="ki"> fn_train — </em>定义回路。</li><li id="7364" class="mx my iq kj b kk ng ko nh mr ni mt nj mv nk le nc nd ne nf bi translated"><em class="ki"> fn_lose </em> —计算损耗。</li><li id="7f0c" class="mx my iq kj b kk ng ko nh mr ni mt nj mv nk le nc nd ne nf bi translated"><em class="ki"> fn_optimize — </em>用于学习设置。</li><li id="0e0c" class="mx my iq kj b kk ng ko nh mr ni mt nj mv nk le nc nd ne nf bi translated"><em class="ki"> fn_predict — </em>用于运行它。</li></ul><p id="e24f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">很有趣！我用简单的<em class="ki"> if </em>语句(例如<em class="ki"> concave_convex:[True，False] </em>)测试了完全不同的架构，甚至将Torch损失函数与Keras模型混合在一起。</p><p id="2ed8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi nl translated">根据我正在进行的分析类型(例如回归、二元分类和多标签分类)，我开始注意到我正在使用相同的<em class="ki">丢失-优化-预测</em>组合。所以我能够为大多数组件设置可覆盖的默认值。</p><blockquote class="nu"><p id="f4ae" class="nv nw iq bd nx ny nz oa ob oc od le dk translated">♻️类似于Keras如何抽象一个训练循环，我抽象了一个训练循环的循环，同时保持了工作流的可定制性。</p></blockquote><p id="360b" class="pw-post-body-paragraph kg kh iq kj b kk oe km kn ko of kq kr mr og ku kv mt oh ky kz mv oi lc ld le ij bi translated">以下是一个基本的多标签分类示例:</p><pre class="ln lo lp lq gt oj ok ol om aw on bi"><span id="67de" class="oo ls iq ok b gy op oq l or os"># Unique param combos will get passed into functions as `hp**`.<br/><strong class="ok ir">hyperparameters</strong> = {<br/>      "neuron_count":  [9, 12]<br/>    , "batch_size":    [3, 5]<br/>    , "epoch_count":   [30, 60]<br/>}<br/></span><span id="60d0" class="oo ls iq ok b gy ot oq l or os"><strong class="ok ir">def fn_build</strong><em class="ki">(features_shape, label_shape, **hp):</em><br/>    model = Sequential()<br/>    model.add(Input(shape=features_shape))<br/>    model.add(Dense(units=hp['neuron_count'], activation='relu'))<br/>    model.add(Dense(units=label_shape[0], activation='softmax'))<br/>    return model<br/></span><span id="e685" class="oo ls iq ok b gy ot oq l or os"><strong class="ok ir">def fn_train</strong><em class="ki">(<br/>    model, loser, optimizer, <br/>    samples_train, samples_evaluate, **hp<br/>):</em><br/>    model.compile(<br/>          loss      = loser<br/>        , optimizer = optimizer<br/>        , metrics   = ['accuracy']<br/>    )<br/>    model.fit(<br/>          samples_train["features"]<br/>        , samples_train["labels"]<br/>        , validation_data = (<br/>            samples_evaluate["features"]<br/>            , samples_evaluate["labels"]<br/>        )<br/>        , verbose    = 0<br/>        , batch_size = hp['batch_size']<br/>        , epochs     = hp['epoch_count']<br/>        , callbacks  = [History()]<br/>    )<br/>    return model</span></pre><p id="e921" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">这些组件用于组装培训工作的<em class="ki">队列</em>:</p><pre class="ln lo lp lq gt oj ok ol om aw on bi"><span id="466a" class="oo ls iq ok b gy op oq l or os"><strong class="ok ir">queue = aiqc.Experiment.make</strong>(<br/>      <strong class="ok ir"># --- Analysis type ---</strong><br/>      <em class="ki">library</em>         = "keras"<br/>    , <em class="ki">analysis_type</em>   = "classification_multi"<br/><br/>      <strong class="ok ir"># --- Model functions ---</strong><br/>    , <em class="ki">fn_build</em>        = fn_build<br/>    , <em class="ki">fn_train</em>        = fn_train<br/>    , <em class="ki">fn_lose</em>         = None <em class="ki">#auto CatCrossEnt.</em><br/>    , <em class="ki">fn_optimize</em>     = None <em class="ki">#auto Adamax &lt;3.</em><br/>    , <em class="ki">fn_predict</em>      = None <em class="ki">#returns `preds, probs`.</em><br/><br/>      <strong class="ok ir"># --- Training options ---</strong><br/>    , <em class="ki">repeat_count</em>    = 2<br/>    , <em class="ki">hyperparameters</em> = hyperparameters<br/><br/>      <strong class="ok ir"># --- Data source ---</strong><br/>    , <em class="ki">splitset_id</em>     = splitset.id #scroll down.<br/>    , <em class="ki">hide_test</em>       = False<br/>)</span><span id="4cfb" class="oo ls iq ok b gy ot oq l or os"><strong class="ok ir">queue.run_jobs()</strong><br/>#🔮 Training Models 🔮: 100%|███████████████████████| 16/16</span></pre><p id="9526" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">然而，当到了用指标和图表评估每个模型的时候，我意识到一些关键问题被掩盖了。</p><figure class="ln lo lp lq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ou"><img src="../Images/5ca597b78d583f5df218d7617fb385e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kMC6irHP8aCFJftiTzWKpQ.gif"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">AIQC的内置可视化——作者图片</p></figure></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><figure class="ln lo lp lq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ov"><img src="../Images/940e61c7caed31ce8fde2e32f4224be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mso3Ojxqty3dniPjBLTs7g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">探索超维度特征空间以最小化损失——作者图片</p></figure><h1 id="0c32" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">深度学习，浅范围。</h1><blockquote class="kd ke kf"><p id="6750" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">深度学习库做好一件事；高效地将损失降至最低。非常快—使用20，000列650个样本，每个时期1秒—非常快。</p></blockquote><p id="8e10" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">离计算机科学繁重的任务越远，这些库的帮助就越少。其他的事情——上游的<em class="ki">数据准备</em>和下游的<em class="ki">模型评估</em>都转移到了用户的肩上。核心深度学习库不是:</p><p id="c054" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">🤔<strong class="kj ir"> <em class="ki">分析感知</em> </strong> —算法不知道您正在进行什么类型的分析，因此它不知道评估给定模型需要什么指标&amp;图表。</p><p id="51bf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated"><a class="ae kc" href="https://emojipedia.org/floppy-disk/" rel="noopener ugc nofollow" target="_blank">💾</a> <strong class="kj ir"> <em class="ki">数据感知</em> </strong> —程序不知道你的数据集是如何构造的。这不仅适用于(a)需要评估的分割/折叠中的特征/标签等子集，还适用于(b)用于预处理/后处理的形状。</p><p id="96dc" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">📂<strong class="kj ir"><em class="ki"/></strong>—模型、指标和任何预处理步骤只存在于内存中。保存和组织它们取决于从业者。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><h1 id="fa7d" class="lr ls iq bd lt lu ow lw lx ly ox ma mb mc oy me mf mg oz mi mj mk pa mm mn mo bi translated">面向对象的方法。</h1><figure class="ln lo lp lq gt jr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/78cad7239deaac4026ee35e4f5a0ff41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Eb9ENRWgGJp_kV7_Y6c9OQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">AIQC高级和低级原料药的要点-图片由作者提供。</p></figure><blockquote class="nu"><p id="4c02" class="nv nw iq bd nx ny pc pd pe pf pg le dk translated">算法只是希望你以正确的格式显示正确的数据。我厌倦了互联网上任意食谱中的这种“X_train，y_test”数学术语。就叫它们<strong class="ak">特性</strong>和标签吧！等等…就是这样！</p></blockquote><p id="222e" class="pw-post-body-paragraph kg kh iq kj b kk oe km kn ko of kq kr mr og ku kv mt oh ky kz mv oi lc ld le ij bi translated">基于所涉及的数据和分析的类型，有一些规则来管理这些对象应该如何相互交互。所有这些信息都需要持久化。这个标准使得机器学习成为对象关系模型(ORM)抽象的完美候选，也就是关系数据库的API。</p></div><div class="ab cl lf lg hu lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="ij ik il im in"><p id="cfe5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">在构建<strong class="kj ir">低级</strong> <em class="ki">特征时，标签、编码器、分割、折叠</em>等。—我意识到我每次都在用不同设置的相同对象。所以我将所有的摄取&amp;预处理步骤组合成一个<strong class="kj ir">高级</strong> <em class="ki">流水线</em>，并将所有的训练&amp;评估对象组合成一个<em class="ki">实验</em>。</p><p id="d49a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mr kt ku kv mt kx ky kz mv lb lc ld le ij bi translated">Iris数据集的多标签分类问题的数据准备如下:</p><pre class="ln lo lp lq gt oj ok ol om aw on bi"><span id="7020" class="oo ls iq ok b gy op oq l or os">import aiqc<br/># Creates or finds SQLite db for persisting workflow.<br/>aiqc.setup()</span><span id="f458" class="oo ls iq ok b gy ot oq l or os"># Built-in example datasets.<br/>from aiqc import datum<br/>df = datum.to_pandas('iris.tsv')</span><span id="5a14" class="oo ls iq ok b gy ot oq l or os"><strong class="ok ir"><br/>splitset = aiqc.Pipeline.Tabular.make</strong>(<br/>      <strong class="ok ir"># --- Data source ---</strong><br/>      <em class="ki">df_or_path</em> = df<br/><br/>      <strong class="ok ir"># --- Label preprocessing ---</strong><br/>    , <em class="ki">label_column</em>  = 'species'<br/>    , <em class="ki">label_encoder</em> = dict(<br/>          sklearn_preprocess=OneHotEncoder()<br/>    )<br/><br/>      <strong class="ok ir"># --- Feature preprocessing ---</strong><br/>    , <em class="ki">feature_cols_excluded</em> = 'species'<br/>    , <em class="ki">feature_encoders</em>      = [<br/>          dict(<br/>              sklearn_preprocess   = StandardScaler()<br/>              # Encode a list of `dtypes` or `columns`              <br/>              , dtypes = ['float64'] <br/>          )<br/>    ]</span><span id="991a" class="oo ls iq ok b gy ot oq l or os">      <strong class="ok ir"># --- Stratification ---</strong><br/>    , <em class="ki">size_test</em>       = 0.22<br/>    , <em class="ki">size_validation</em> = 0.12<br/>    , <em class="ki">fold_count</em>      = None<br/>    , <em class="ki">bin_count</em>       = None<strong class="ok ir"><br/></strong>)</span></pre><h2 id="01f6" class="oo ls iq bd lt ph pi dn lx pj pk dp mb mr pl pm mf mt pn po mj mv pp pq mn pr bi translated">AIQC是面向对象MLOps的开源框架。</h2><p id="cf98" class="pw-post-body-paragraph kg kh iq kj b kk mp km kn ko mq kq kr mr ms ku kv mt mu ky kz mv mw lc ld le ij bi translated"><em class="ki">高级API </em>允许从业者专注于他们的数据科学工作流程，而不是将脚本和拼凑的工具集粘在一起。它抽象出了使机器学习变得如此不可接近的数据争论。</p><blockquote class="nu"><p id="12e0" class="nv nw iq bd nx ny nz oa ob oc od le dk translated"><a class="ae kc" href="https://github.com/aiqc/aiqc" rel="noopener ugc nofollow" target="_blank"><em class="ps"/></a><em class="ps">(别忘了⭐ ) </em></p></blockquote></div></div>    
</body>
</html>