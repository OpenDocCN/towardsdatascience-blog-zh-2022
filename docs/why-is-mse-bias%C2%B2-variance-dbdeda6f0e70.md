# 为什么 MSE =偏差+方差？

> 原文：<https://towardsdatascience.com/why-is-mse-bias%C2%B2-variance-dbdeda6f0e70>

## “好”统计估计量及其性质简介

# 偏差-方差权衡

*“偏差-方差权衡”*是你在 [ML/AI](http://bit.ly/quaesita_emperor) 中会遇到的一个流行概念。为了让它变得直观，我想我应该给你们当中的公式爱好者一个关于这个关键公式来源的简单解释:

> MSE =偏差+方差

嗯，这篇文章不仅仅是要证明这个公式——那只是一个[的意思](http://bit.ly/quaesita_gistlist)(呵)到此结束。我用它作为一个借口，给你一个幕后的视角，看看[统计学家](http://bit.ly/quaesita_pointofstats)如何和为什么操纵一些核心构件，以及我们如何思考是什么让一些估算者比其他人更好，但要注意:这是关于这里的技术。

![](img/d7738444c9f4dc22d97d2713bd3ceae7.png)

作者创造的形象。

我的博客不喜欢探究公式和一般化的事实，所以许多读者可能想借此机会赶紧离开。如果证明的想法让你充满了存在主义的恐惧，这里有一篇有趣的文章供你欣赏。不要担心，你仍然可以遵循即将到来的偏差-方差权衡文章，但你必须相信这个公式是准确的。这篇文章是写给那些要求证明的人的！(以及关于装饰华丽的希腊字母的讨论。)

# 先决条件

还在吗？很好。如果你对一些核心概念有所了解的话，这些东西会更容易理解，所以这里有一个快速的清单:

**偏向；分销；Estimand 估计；估计量；期望值 E(X)；损失函数；卑鄙；模型；观察；参数；人口；概率；随机变量；样本；统计；方差 V(X)**

如果你遗漏了一个概念，我会在我的[统计术语表](http://bit.ly/quaesita_gistlist)中帮你找到。

# E(X)和 V(X)

为了确保您对我们讨论的构建模块感到满意，让我们从我的[发行版参数](http://bit.ly/quaesita_lemur)领域指南中摘录一段:

## 期望值 E( *X* )

一个期望值，写成 *E(X)* 或 *E(X = x)* ，就是[随机变量 *X*](http://bit.ly/quaesita_distributions) *的理论概率加权**均值**(这个字读作“平均”)。*

你可以通过加权(乘以)每个潜在值[*X**X*可以取](http://bit.ly/quaesita_distributions)的相应概率 [*P(X = x)*](http://bit.ly/quaesita_distributions) 然后组合它们(用积分∫表示像身高这样的[连续](http://bit.ly/quaesita_datatypes)变量，或者用和表示像身高四舍五入到最近的英寸这样的[离散](http://bit.ly/quaesita_datatypes)变量): ***E(X) = 【T18***

![](img/e02c22799cdd1b91a8265a3dadf6b815.png)

米洛斯·托马舍维奇在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

如果我们正在处理一个公平的六面骰子， *X* 可以以 1/6 的相等概率取{1，2，3，4，5，6}中的每个值，所以:

e(*X*)=(1)(1/6)+(2)(1/6)+(3)(1/6)+(4)(1/6)+(5)(1/6)+(6)(1/6)= 3.5

换句话说，3.5 是 *X* 的概率加权平均值，没人关心 3.5 甚至不是掷骰子的允许结果。

## 方差 V(X)

将上面的 E( *X* )公式中的 *X* 替换为 *(X - E(X))* ，得到一个[分布](http://bit.ly/quaesita_distributions)的方差。让我授权你在冲动袭来时计算它:

v(*X*)= E[*(X-E(X))*=**∑【*X-*E(X)】*P(X = X)***

那是一个定义，所以这部分没有证明。让我们兜一圈，得到一个公平骰子的方差:V(X)=**∑[*X-*E(X)]*P(X = X)***=**∑(*X-3.5)**P(X = X)=***(1–3.5)(1/6)+(2–3.5)(1/6)

如果你处理的是连续的数据，你会用一个积分来代替一个和，但这是同样的想法。

## 备选 V(X)公式

在我们下面的证明中，我们将对方差公式做一点改变，用最右边的位替换中间的位:

v(*X*)= E[*(X-E(X))*]= E[(*X*)]*-*【E(*X*)]

我欠你一个解释，它是从哪里来的，所以让我们快速地覆盖它:

v(*X*)= E[*(X-E(X))*]
= E[*X**-*2*X*E(*X*)+E(*X*)]
= E(*X*)*-*2e(*X*)E(【T22

这是如何发生的，为什么会发生？关键的一点是从第 2 行到第 3 行…我们可以用括号这样做的原因是，期望值是求和/积分，所以无论我们允许用常数和括号来做什么，我们也允许用期望值来做。这就是为什么如果 **a** 和 **b** 都是常数，那么 E[**a***X*+**b**=**a**E(*X*)+**b**。哦，而且 E(X)本身也是一个常数——算出来之后就不是随机的了——所以 E(E(*X*)= E(*X*)。很高兴事情解决了。

# 关于符号和 bling 的注释

[**Estimands**](http://bit.ly/quaesita_vocab) (你要 [*估计*](http://bit.ly/quaesita_vocab) 的东西)往往用不加修饰的希腊字母表示，最常见的是θ。(这是英文中的字母“theta ”,如果我们觉得“th”应该有自己的字母；“th”与“pffft”非常接近，使得θ成为统计中标准占位符的绝佳选择。)

Estimands θ是[参数](http://bit.ly/quaesita_vocab)，所以它们是(未知的)常数:E(θ) = θ，V(θ) = 0。

[**估算者**](http://bit.ly/quaesita_vocab) (你为了 [*估算*](http://bit.ly/quaesita_vocab)[*估算者*](http://bit.ly/quaesita_vocab) 而使用的公式)通常用在希腊字母上加上 bling 来表示，比如在θ上加一个小帽子，就像这样:

![](img/6bb676f20d29ce4c14db384120066976.png)

既然让这篇博文在一篇中等的帖子里很好地渲染一个带着帽子的θ是一件痛苦的事情，那我就请你发挥你的想象力，每当我键入“θhat”的时候，就能看到这个整洁的小家伙。此外，无论如何，你都要用笔和纸来完成这个过程——你不会像疯子一样，只是通过阅读来学习公式，对吧？这样你就不会被我的符号弄糊涂了。你将抄下用上面漂亮的帽子格式化的公式，然后阅读你自己的笔记，如果你迷路了，瞥一眼我闲聊的解释来帮助你。

# 推导 MSE 公式

估值器是随机变量，直到你插入你的 T2 数据，得到 T4 估值。估计值是一个常数，所以你可以把它当作一个简单的数字。再说一遍，这样我们就不会混淆了:

*   Estimand ，θ，我们试图估计的东西，一个常数。
*   **估计量**，θhat，我们用来得到估计值的公式，一个取决于你得到的数据的随机变量。抽签的运气！
*   **估算**，一旦我们将数据插入估算器，最终会得出一些数字。

现在为了知道我们的估计量θhat 是否笨得像砖头一样，我们要检查我们是否能 ***期望*** 它接近估计量θ。所以随机变量 X = (θhat - θ)的 E()是我们要处理的第一个变量。

*E(X)= E((θhat-θ))= E(θhat)-E(θ)= E(θhat)-E(θ)= E(θhat)-θ*

这个量在统计学上有一个专门的名字:偏差。

无偏估计量是 E(θhat) = θ的估计量，这是一个很好的性质。这意味着我们可以 ***期望*** 我们的估计量是正确的(平均而言)。在我的[温和介绍博客文章](http://bit.ly/quaesita_bivar)中，我解释了偏见指的是*“系统性偏离目标的结果。”我应该更恰当地说，偏差是我们的估计量(θhat)给出的结果与我们的目标(θ)之间的预期距离，换句话说:*

> **Bias = E(θhat) - θ**

# 选择“最佳”评估者

如果你喜欢无偏估计量，那么你会喜欢一些 UMVUEs。这个缩写代表一致最小方差无偏估计量，它指的是无偏估计量中最佳选择的标准:如果它们都是无偏的，选择方差最小的一个！(现在我已经带你到了硕士水平统计推断教材的第七章[。不客气)](https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf)

![](img/675241c9d785f711f669dc221e841bca.png)

UMVUE，不是悍马。瑞安在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

“你给我提供了两个具有相同偏差的估值器，所以我选择了方差较小的那个，duh”的花哨术语是 ***效率*** 。

当然，有许多不同的方法来选择一个“最佳”的评估者。要寻找的好的属性包括无偏性、相对效率、一致性、渐近无偏性和渐近效率。前两个是 ***小样本属性*** ，后三个是 ***大样本属性*** ，因为它们处理的是当样本量增加时估计量的表现。随着样本量的增加，如果一个估计量最终达到目标，那么它就是 ***一致的*** 。(没错，是时候极限了！读[这个](https://www.unil.ch/files/live/sites/issr/files/shared/8._Telechargement/Cours_MA_quantiBasel/Chapter_7_Point_Estimation.pdf)如果*你的*时间- >无限。)

效率是一个非常重要的属性，因为没有人希望他们的评估者到处都是。(恶心。)因为效率与方差有关，所以让我们尝试将 X = (θhat — θ)代入方差公式:

> *方差*V(X)= E[(X)]-[E(X)]
> *变成*V(θhat-θ)= E[(θhat-θ)]-[E(θhat-θ)]

方差衡量随机变量的分布，因此减去一个常数(可以将参数θ视为一个常数)只会移动所有值，而不会改变分布，V(θhat - θ) = V(θhat)，因此:

> v(θhat)= E[(θhat-θ)]-[E(θhat)-E(θ)]

现在我们重新排列术语，记住常数 E(θ) = θ:

> E[(θhat - θ) ] = [E(θhat) - θ] + V(θhat)

现在我们来看看这个公式，因为它有一些特殊的东西，有特殊的名字在里面。提示:还记得偏见吗？

> Bias = E(θhat) — θ

我们能在公式中找到吗？当然可以！

> e[(θhat-θ)]=[偏差] + V(θhat) =偏差+方差

那么左边的东西到底是什么？这是一个有用的量，但我们在给它命名时并不太有创意。因为“误差”是描述我们的射击点(θhat)和我们的瞄准点(θ)之间的差异(通常记为ε)的恰当方式，所以 E[(θhat - θ) ] = E(ε)。

E(ε)名为，等一下，均方误差！简称 MSE。是的，它的字面意思是 E(ε):我们取误差平方ε的平均值(期望值的另一种说法)。统计学家们，创造力加分。

MSE 是模型的[损失函数](http://bit.ly/quaesita_msefav)中最受欢迎的(也是最普通的)选择，它往往是你第一次被教授的(在我自己的[机器学习课程](http://bit.ly/mfml_006)中)。

所以我们有:

> MSE =偏差+方差

现在你已经完成了数学，你已经准备好理解机器学习中的偏差-方差权衡是怎么回事了。我们将在我的下一篇文章中讨论这个问题——点击*跟随*按钮继续关注。

[](/stats-gist-list-an-irreverent-statisticians-guide-to-jargon-be8173df090d)  

# 感谢阅读！YouTube 课程怎么样？

如果你在这里很开心，并且你正在寻找一个为初学者和专家设计的有趣的完整的应用人工智能课程，这里有一个我为你制作的娱乐课程:

# 寻找动手 ML/AI 教程？

以下是我最喜欢的 10 分钟演练:

*   [AutoML](https://console.cloud.google.com/?walkthrough_id=automl_quickstart)
*   [顶点 AI](https://bit.ly/kozvertex)
*   [人工智能笔记本](https://bit.ly/kozvertexnotebooks)
*   [表格数据的 ML](https://bit.ly/kozvertextables)
*   [文本分类](https://bit.ly/kozvertextext)
*   [图像分类](https://bit.ly/kozverteximage)
*   [视频分类](https://bit.ly/kozvertexvideo)