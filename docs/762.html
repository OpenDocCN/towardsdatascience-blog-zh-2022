<html>
<head>
<title>When Is Bayesian Machine Learning Actually Useful?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯机器学习实际上什么时候有用？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-is-bayesian-machine-learning-actually-useful-1fd8af456e4c#2022-01-27">https://towardsdatascience.com/when-is-bayesian-machine-learning-actually-useful-1fd8af456e4c#2022-01-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="8085" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">贝叶斯机器学习实际上什么时候有用？</h1></div><div class=""><h2 id="0264" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于一个有点争议的范式的个人想法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fac103de75890172a6dafee3dbf67901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKxVYiu3fVePHu2RyhNaow.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Javier Allegue Barros 在<a class="ae ky" href="https://unsplash.com/s/photos/thinking?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="eaae" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="f532" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当谈到贝叶斯机器学习时，你可能要么喜欢它，要么更喜欢与任何贝叶斯保持安全距离。鉴于当前最先进的模型几乎从未提及贝叶斯，后者背后可能至少有一些基本原理。</p><p id="64c4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一方面，许多高知名度的研究小组几十年来一直在研究贝叶斯机器学习工具。现在仍然如此。</p><p id="570a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，这个领域也不太可能完全是骗人的。正如生活中经常发生的那样，真相可能介于两个极端之间。在这篇文章中，我想分享我对这个问题的看法，并指出贝叶斯机器学习何时会有所帮助。</p><p id="f26e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了避免混淆，让我们简要定义一下<em class="ms">贝叶斯机器学习</em>在本文中的含义:</p><blockquote class="mt mu mv"><p id="bb57" class="lr ls ms lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated"><em class="it">“机器学习的贝叶斯框架指出，你首先列举数据的所有合理模型，并为这些模型中的每一个分配你的先验信念P(M)。然后，在观察数据D的基础上，你评估这些数据在每个模型下计算P(D|M)的可能性有多大。”</em></p><p id="9b59" class="lr ls ms lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated"><a class="ae ky" href="http://mlg.eng.cam.ac.uk/zoubin/bayesian.html#:~:text=The%20Bayesian%20framework%20for%20machine,P(D%7CM)." rel="noopener ugc nofollow" target="_blank">T5【邹斌】格拉马尼T7】</a></p></blockquote><p id="5117" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不太正式的是，我们将来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_statistics" rel="noopener ugc nofollow" target="_blank">贝叶斯统计</a>的工具和框架应用于机器学习模型。我会在最后提供一些参考，以防你还不熟悉贝叶斯统计。</p><p id="cd73" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，我们的讨论基本上将机器学习等同于神经网络和一般的可微分模型(特别是线性回归)。由于深度学习目前是现代机器学习的基石，这似乎是一种公平的方法。</p><p id="0adf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为最后的免责声明，我们将区分frequentist和贝叶斯机器学习。前者包括你可能已经熟悉的标准ML方法和损失函数。</p><p id="cff6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，事实上‘贝叶斯’是用大写字母写的，而‘frequentist’不是，没有判断的意思。</p><p id="debe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们从一个令人惊讶的结果开始:</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="337e" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">你的频率主义模型可能已经是贝叶斯的了</h1><p id="83ce" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这种说法听起来可能令人惊讶和奇怪。然而，贝叶斯和频率主义学习范式之间有一个清晰的联系。</p><p id="9cb0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们从贝叶斯公式开始</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/650942beac08ac59148bfa58b3359cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*HbevR5AlQmYTVJbmCpaqrw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="bb55" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并将其应用于贝叶斯神经网络:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8680cedf16cf5a7aa2cb804cdd18d7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*EKHmSqBrRMi_YyrgoawGZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1eea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为一个示例，我们可以</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/758aa1ae2280b7dd0e272853238fe07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*zI0EHNl8q5xNNRpyQL9q1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c82d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">即网络输出定义了目标变量的平均值，该平均值可能遵循正态分布。对于网络上的先验权重——假设我们总共有<em class="ms"> K </em>个权重——我们可以选择</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2e728d0cd3c02e13ab9f0c564e8b305c.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*9aRhY1YPvra_WovzEUCi_w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5350" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">具有<strong class="lt iu"> (2) </strong>和<strong class="lt iu"> (3) </strong>的设置对于贝叶斯神经网络来说是相当标准的。为<strong class="lt iu"> (1) </strong>寻找后验权重分布在贝叶斯网络的任何合理设置中都是徒劳的。这对于贝叶斯模型来说也不是什么新鲜事。</p><p id="432b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们现在有几个选项来处理这个问题，例如<a class="ae ky" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">变分贝叶斯</strong> </a>或<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">最大后验估计</strong> </a> <strong class="lt iu"> (MAP) </strong>。从技术上讲，后者只给出了后验最大值的点估计，而不是完整的后验分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0d281185faa8bbfa221022d7d0af071c.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*Y06dTuDKC-v1AiwcdhVp6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="aec6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">假设概率密度在其定义域上严格为正，我们可以引入对数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9d02543760bdb9d6033795bd94c81426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*vDLCgCoLNZy-RL_Z0qz_dw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5b5e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于最后一个被加数不依赖于模型参数，它不影响<em class="ms"> argmax </em>。因此，我们可以不考虑它。</p><h2 id="8bcb" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">从贝叶斯映射到正则化均方误差</h2><p id="2c76" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">方程式<strong class="lt iu"> (2) </strong>和<strong class="lt iu"> (3) </strong>的目的仅仅是说明性的。让我们用下面的内容代替可能性和先验:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4b0676eff5fe348149093a728ab00f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Veq4b1_2DMO8YCOqdjKaHw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8048" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Z项是归一化常数，其唯一目的是产生有效的概率密度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/8ab4df0825160dcd624b44a5b4346063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqPvoLB4ZJbayYEEtbBYCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="2bfd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当一个函数的最大值等于反函数的最小值时，我们可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fea5c9ab23ea168f069997f5fc25c624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*hgid3cUzGYek8ANgSR9JLQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1261" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，<em class="ms"> argmax </em>也不受与常数相乘的影响:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9f4761155502ccf57d64ddbed83d439e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*x1WrjmfqCEBYBSqPgf3Yug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b4eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后一项只不过是带有正则化的标准MSE目标。因此，给定特定的先验和似然性，正则化的MSE目标等价于贝叶斯映射目标。</p><p id="2894" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你反过来，你可以找到几乎任何频率损失函数的先验概率对。然而，常客目标通常只给你一个点的解决方案。</p><p id="70b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">贝叶斯方法，另一方面，给你一个完整的后验分布，上面有不确定性区间。使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">无信息先验分布</strong> </a>你也可以在必要时移除正则项。然而，我们不会在这里讨论推导。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="5eef" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">贝叶斯机器学习在现实世界中的代价</h1><p id="a16f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因此，理论上，贝叶斯机器学习比频率主义方法产生了更完整的图像。然而需要注意的是后验分布的困难性，因此需要对其进行近似或估计。</p><p id="c96b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，近似和估计都不可避免地导致精度损失。以流行的贝叶斯神经网络的<a class="ae ky" href="http://krasserm.github.io/2019/03/14/bayesian-neural-networks/" rel="noopener ugc nofollow" target="_blank">变分推理方法为例。为了优化模型，我们需要通过从变分分布中取样来近似一个所谓的<strong class="lt iu"> ELBO </strong>目标。</a></p><p id="336d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通常的ELBO物镜看起来是这样的——如果您不理解所有内容，也不要担心:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3bd4b45ef1d2dff87df105a6ed7e7ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*wRL2F4kriMCKOF0tfUR2kQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="97b1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们的目标是最大化<strong class="lt iu"> (7) </strong>或者最小化它的负值。然而，期望项通常不是难以处理的。这个问题最简单的解决方法是应用<a class="ae ky" href="https://gregorygundersen.com/blog/2018/04/29/reparameterization/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">重新参数化技巧</strong> </a>。这使我们能够估计厄尔巴维亚</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/bc6f0815742e1131ba92c703cc4b86ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*7zUoyvZay5m34SdkZ6tUAQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="227c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">梯度运算的线性允许我们通过<strong class="lt iu"> (8) </strong>估算<strong class="lt iu"> (7) </strong>的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/df0e0b66e5f6b76d529005a4a89578bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*w9d-4FPbCfc_cnt9Tcu_tA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8fb8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用这个公式，我们基本上是从一个重新参数化的分布中采样<em class="ms"> M </em>个梯度。此后，我们使用这些样本来计算“平均”梯度。</p><h2 id="729a" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">ELBO的无偏梯度估计</h2><p id="9d13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如<a class="ae ky" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank"> Kingma和Welling (2013) </a>的里程碑式论文所示，我们已经</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f6237d83969f77bd20c839efcb47fc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*ArQYb2KHfS2lDUCGgh7oXw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1274" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">本质上，等式<strong class="lt iu"> (9) </strong>告诉我们，平均而言，我们基于采样的梯度等于真实梯度。<strong class="lt iu"> (9) </strong>的问题在于我们对采样梯度的高阶统计矩一无所知。</p><p id="4eb7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">例如，我们的估计可能有一个非常大的方差。因此，虽然我们平均在正确的方向上梯度下降，但我们的效率低得不合理。</p><p id="b21d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于重采样导致的额外随机性也使得难以找到梯度下降的正确停止时间。再加上非凸损失函数的问题，你在非贝叶斯网络中表现不佳的几率相当高。</p><p id="91bb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">总之，贝叶斯世界中的真实后验可以给我们一个关于最优参数的更完整的描述。然而，由于我们需要求助于近似值，我们很可能以比标准方法更差的性能结束。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="9a3b" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">贝叶斯机器学习实际上什么时候有意义？</h1><p id="c543" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">以上考虑引出了一个问题:你到底想不想使用贝叶斯机器学习。就我个人而言，我看到了两种特殊的情况。请记住，以下是经验法则。支持或反对贝叶斯机器学习的实际决定应该基于手头的具体问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ca112a7b2ea7b673a983d47c3fe1a565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*_sBm7mVEbxc6VIY1zC25dA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯机器学习实际上什么时候有用？一个简单的指导决策树—始终适应您的具体问题。(图片由作者提供)</p></figure><h2 id="946e" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">小数据集和丰富的先验知识</h2><p id="93f3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们回到之前的正则化MSE推导。我们的目标是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/a91028d66eaec1cf2f48c14fc5448f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*ph6Y-tODxrXGuUjrj2dudQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="68d2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在数据集的大小<em class="ms"> N </em>和模型参数的数量<em class="ms"> K </em>之间存在明显的权衡。对于足够大的数据集，右侧的先验与地图解决方案无关，反之亦然。这通常也是后验推理的一个关键特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/472f7853bd6212683a3b6af679de22f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8lONnEMTRrr2wiQvrH9ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">后验分布的形象化表现。更多的数据使后验更接近数据。更少的数据使得它越来越类似于先验分布。(图片由作者提供)</p></figure><p id="4403" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当数据集相对于模型参数的数量很小时，后验分布将非常类似于先验分布。因此，如果先验分布包含关于问题的合理信息，我们可以在一定程度上缓解数据的缺乏。</p><p id="ade4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以简单的线性回归模型为例。根据信噪比，在参数收敛到“最佳”值之前，可能需要大量数据。如果你有一个关于“最佳”解决方案的合理的先验分布，你的模型实际上可能收敛得更快。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/d4c8d6f98fe788f22fbfbb87e9226cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*625B2muI7wlmbfSzIXCxJQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">1)由于小数据集和随机噪声，普通OLS回归线是关闭的(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/237a986fcd7e3d464c754250e3734a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*reUcu2Rih-b_I6K0Q8jDVg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">2)在任何模型拟合发生之前，一个消息灵通的先验已经产生了一个合理的范围(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/60ccae9e952138704fc92ac8c8fd3be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CcGAKF3NsVJqXptOkv82Tw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">3)在“训练”之后，贝叶斯模型产生一条预测平均回归线，该预测平均回归线更加接近数据生成回归线(图片由作者提供)</p></figure><p id="4006" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，恰恰相反，你同样可以通过使用错误的先验知识来搞乱你的模型。结果，收敛到“最佳”解可能比没有任何先验知识更慢。贝叶斯统计中有更多的工具来减轻这种合理的异议，我渴望在未来的文章中讨论这个主题。</p><p id="c8cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，即使你不想完全贝叶斯，地图估计仍然是有用的给定高质量的先验知识。</p><h2 id="d135" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">现代贝叶斯机器学习的功能先验</h2><p id="4238" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一个不同的问题可能是在神经网络权重上表达有意义的先验知识的困难。如果模型是一个黑盒，你如何告诉它该做什么？除了一些模糊的零均值正态分布？</p><p id="30b8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">解决这个问题的一个有希望的方向可能是<a class="ae ky" href="https://arxiv.org/pdf/1903.05779.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">功能贝叶斯</strong> </a>。在这种方法中，我们只告诉网络，基于我们的先验知识，对于给定的输入，我们期望什么样的输出。因此，我们只关心我们的模型能够表达的后验函数。确切的参数后验分布只是次要的。</p><p id="91a7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">简而言之，如果你只有有限的数据，但有充分的先验知识，贝叶斯机器学习实际上可以有所帮助。</p><h2 id="2556" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">不确定性量化的重要性</h2><p id="48b5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于大型数据集，您的先验知识的影响变得越来越不相关。在这种情况下，获得一个frequentist解决方案可能是完全足够的和优越的方法。然而，在某些情况下，全貌——也称为后验不确定性——可能很重要。</p><p id="131d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">考虑基于某种机器学习模型做出医疗决策的情况。此外，该模型并不产生最终决定，而是支持医生的判断。如果贝叶斯点精度至少接近一个frequentist等效，不确定性输出可以作为有用的额外信息。</p><p id="510c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">假设专家的时间有限，他们可能想更仔细地研究高度不确定的模型输出。对于低不确定性的输出，快速的健全性检查可能就足够了。这当然首先要求不确定性估计是有效的。</p><p id="f078" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，贝叶斯方法也带来了监控不确定性性能的额外成本。当决定是否使用贝叶斯模型时，这当然应该被考虑在内。</p><p id="c27f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">贝叶斯不确定性可以发挥作用的另一个例子是时间序列预测问题。如果您以自回归方式对时间序列建模，即您估计</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/f0c4b00129efdd6b996fc7729e73484a.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*T4CCGxwk_A3yF4KzftOALg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="44a5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你的模型误差越大，你就越想预测未来。</p><p id="bece" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">即使对于高度简单的自回归时间序列，如</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/a236d9d2cf7d0d1be30cd837f69c7266.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*lpyAnyJsG9osOh34vL4V8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7696" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你估计模型中的微小偏差</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/c16fc71401395ee0518cebc12b156d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*frCgwgbZzZi4WtqgcoGXkw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="3c4e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从长远来看，会导致灾难性的误差积累:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/050a5a518d8bbd9d4bd9d51b12b36d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*ZBsGLbnB1ZWfzv4fF4EsLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b1ac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个完整的事后分析可以更清晰地描绘出其他类似的可能情况会如何发展。即使模型平均来说可能是错误的，我们仍然可以了解不同的参数估计如何影响预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/8f550f52d8ff6341cb13d171b1e310b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOTXGwJqyDZSzqdArTZ0yg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有不确定性的贝叶斯预测(紫色)与频率点预测(红色)。虽然从长期来看，frequentist预测稍微更准确，但贝叶斯不确定性区间正确地包含了已实现的平均轨迹。(图片由作者提供)</p></figure><p id="d40e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，如果最后一段让你对尝试贝叶斯机器学习感兴趣，这里有一个很好的方法可以开始:</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="59ab" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">MC辍学的贝叶斯深度学习之光</h1><p id="cb3b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">幸运的是，如今训练贝叶斯模型的成本并不太高。除非您正在处理非常大的模型，否则贝叶斯推理增加的计算需求应该不会有太大问题。</p><p id="af55" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">贝叶斯推理的一个特别直接的方法是<strong class="lt iu"> MC dropout </strong>。后者是由<a class="ae ky" href="http://proceedings.mlr.press/v48/gal16.pdf" rel="noopener ugc nofollow" target="_blank"> Gal和Ghahramani </a>引入的，并且已经成为一个相当流行的工具。总之，作者表明在模型的训练和推断过程中使用辍学是明智的。事实上，这被证明等同于具有特定的、基于伯努利的变分分布的变分推断。</p><p id="8cf9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，MC辍学可以是一个很好的起点，使你的神经网络贝叶斯化，而不需要太多的前期工作。</p><h2 id="2bfb" class="nr la it bd lb ns nt dn lf nu nv dp lj ma nw nx ll me ny nz ln mi oa ob lp oc bi translated">利弊</h2><p id="1e34" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一方面，MC dropout使得建立现有的贝叶斯模型变得非常简单。您可以简单地用dropout重新训练它，并在推断过程中保持dropout打开。您以这种方式创建的样本可以被看作是从贝叶斯后验预测分布中抽取的。</p><p id="e486" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一方面，MC丢失中的变分分布是基于伯努利随机变量的。理论上，这应该使它比普通的高斯变分分布更不精确。然而，构建和使用这个模型非常简单，只需要普通的Tensorflow或Pytorch。</p><p id="6030" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，对这种方法的有效性也有一些更深层次的批评。这里有一个有趣的争论，涉及到作者之一<a class="ae ky" href="https://twitter.com/ianosband/status/1014466510885216256" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="10db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">无论你从这种批评中得到什么，MC Dropout都可以成为更复杂方法的有用基线。一旦你掌握了贝叶斯机器学习，你就可以尝试用更复杂的模型来提高性能。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="0427" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">摘要</h1><p id="98eb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我希望这篇文章能让你对贝叶斯机器学习的有用性有所了解。当然，它不是灵丹妙药，在很多情况下，它可能不是正确的选择。如果你仔细权衡利弊，贝叶斯方法可能是一个非常有用的工具。</p><p id="a472" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另外，我很高兴就这个话题进行更深入的讨论，无论是在评论中还是通过私人渠道。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="ffb7" class="kz la it bd lb lc ng le lf lg nh li lj jz ni ka ll kc nj kd ln kf nk kg lp lq bi translated">贝叶斯统计参考文献</h1><p id="af7a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] Gelman，Andrew等，“贝叶斯数据分析”。CRC出版社，2013年。</p><p id="e30e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2]约翰·克鲁施克。"做贝叶斯数据分析:与R，JAGS和斯坦的教程."学术出版社，2014。</p><p id="d22a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3]西维亚、德文德吉特和约翰·斯基林。"数据分析:贝叶斯教程."OUP牛津大学，2006年。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="670d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">原载于2022年1月27日</em><a class="ae ky" href="https://sarem-seitz.com/blog/when-is-bayesian-machine-learning-actually-useful/" rel="noopener ugc nofollow" target="_blank"><em class="ms">【https://sarem-seitz.com】</em></a><em class="ms">。</em></p></div></div>    
</body>
</html>