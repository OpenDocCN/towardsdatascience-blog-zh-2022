<html>
<head>
<title>Combating the curse of dimensionality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗维度的诅咒</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/combating-the-curse-of-dimensionality-62b0c2bf3472#2022-01-03">https://towardsdatascience.com/combating-the-curse-of-dimensionality-62b0c2bf3472#2022-01-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="bfdb" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated">对抗维度的诅咒</h1></div><div class=""><h2 id="61a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">方法来减少数据集的维数，以减少过度拟合并加速该过程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6f928a6eeeec7243e0b40c27d25528cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WP67FEFGQ_9Zu2iU16IIw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@moren?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">莫仁·许</a>在<a class="ae kv" href="https://unsplash.com/s/photos/minimalist?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="b9be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当您处理高维数据时，您很可能会遇到过拟合、较长的计算时间和/或模糊的见解等问题。在本文中，我们将探讨一些高维数据的降维方法。但在此之前，我们先说几件在深入降维之前需要了解的重要事情。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="2a73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">降维有两种方式，即特征选择和特征提取。<strong class="ky ir"> <em class="lz">特征选择</em> </strong>是指从数据集中选取最重要的原始形式的特征进行建模。而<strong class="ky ir"> <em class="lz">特征提取</em> </strong>是指在不丢失任何重要信息的情况下，从现有特征中创建新特征。</p><p id="865d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集中要素的数量必须小于记录的总数，如果要增加要素，观测值的数量应呈指数增长。具有更多特征和更少记录的数据集将导致模型过度拟合，这是我们一直想要避免的。因此，我们丢弃对决策没有太大帮助的不重要的特征，只保留重要的特征。但是，我们如何知道哪些特性对我们来说是重要的呢？幸运的是，Python库使降维过程变得更加容易。那么，让我们来了解一下降低数据维数的不同方法，以及如何在Python中实现它们。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="3765" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">具有方差阈值的特征选择</h2><p id="dcad" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">顾名思义，方差代表数据集中的可变性，表明分布有多分散。如果一个特征的方差很低，那么这个特征就不那么重要。假设我们有一个狗的健康数据集，它具有不同的特征，指示狗的体重、身高、身体质量指数等。如果权重特性始终只有一个或两个值，则权重的方差为零，因此对我们来说不重要。由于所有记录中的权重都是相同的，因此很明显，该特征对于我们产生未知见解或对预测模型的训练没有太多帮助。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/88eb7dc31fb0f0f6aa11858bc41eb715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*B5hE8qtd5FivacELjbOCOA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者创建的样本数据集</p></figure><p id="0f5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Python的scikit-learn库提供了一种使用<em class="lz">variance threshold</em>估算器删除低方差特性的简单方法。我们可以将阈值参数设置为等于我们想要设置的任何方差阈值。方差低于阈值的要素会被自动丢弃。</p><h2 id="c9be" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">方差阈值的Python实现</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="aac5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第1行和第2行中，我们导入并定义了阈值等于1的估计量。在第5行中，我们将估计量与数据集相匹配。在第8行中，我们创建了一个具有方差等于1或更大的特征的变量掩码，在第9行中，我们通过将掩码应用于原始数据集来获得缩减的数据集。</p><h2 id="8c88" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">具有成对相关性的特征选择</h2><p id="b04d" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">相关性定义了两个特征之间的关系。正相关意味着如果一个特性增加，另一个也会增加，负相关意味着如果一个特性增加，另一个会减少。为了清楚地理解，假设相同的狗物种数据集，但这次我们将着眼于两个特征，即体重和身高。如果这两个变量之间的相关性为0.99，则它们具有很强的正相关意义，如果狗的体重增加，其高度也增加，反之亦然。因此，使用这两个特征进行建模只会导致过度拟合和资源/时间开销，因为一个特征足以映射特征和目标之间的关系。</p><h2 id="6c21" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">成对关联的Python实现</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="eb0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第1行中，我们计算数据集中所有要素的相关性。在第4行和第5行，我们创建了一个相关矩阵的布尔掩码，以消除重复的相关值，并将该掩码应用于原始的相关矩阵。在第8、9和10行中，我们对简化的矩阵列运行一个循环，以获得相关性大于0.9的列，并打印出我们需要删除的列名。</p><h2 id="fb2e" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">递归特征消除</h2><p id="2dc4" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">递归特征消除或RFE是一种使用监督机器学习算法的特征选择技术。该算法帮助它获得每个特征的系数，并在每次迭代中消除具有最低有效系数的特征，直到它达到期望的特征数量。</p><h2 id="939a" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">RFE的Python实现</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="8fa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第1行和第2行，我们导入了决策树分类器和RFE。在第5–7行，我们将决策树定义为dt，将RFE定义为rfe，其中estimator设置为等于dt，n_features_to_select设置为5，这意味着我们希望从数据集中获得5个最重要的特征。在第10行，我们将重要的特性定义为selected_features，在第11行，我们打印selected_features来查看我们最重要的特性。</p><h2 id="f9b6" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">用主成分分析或PCA进行特征提取</h2><p id="5122" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">PCA使用线性代数在协方差矩阵、特征值和特征向量的帮助下将原始变量转换成新变量。它从寻找协方差矩阵的特征值和特征向量开始，并使用特征值和特征向量来计算主分量。较高的特征值是重要的，其形成主分量，并且通过在其中存储具有重要信息的变量来获得特征矩阵。通过将原始数据集的转置与所获得的特征向量的转置相乘，获得最终的数据矩阵。</p><h2 id="32bc" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">PCA的Python实现</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="2427" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第1行，我们导入PCA。在第4行中，我们将pca定义为PCA，并传递了等于5的n_components，表示我们需要5个提取的特征。在第7行和第8行，我们创建了一个新的数据集，包含我们新提取的特征。</p><h2 id="d468" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">结论</h2><p id="b30c" class="pw-post-body-paragraph kw kx iq ky b kz mt jr lb lc mu ju le lf mv lh li lj mw ll lm ln mx lp lq lr ij bi translated">在本文中，我们讨论了维数灾难，并理解了特征选择和特征提取之间的区别。然后，我们学习了四种降维技术以及如何在Python中实现它们。</p><p id="2a17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我真的希望，这有助于你学习新的技能或消除对降维的疑虑。如果你有任何问题，请在评论中联系我。点击拍手，分享文章，如果你喜欢它。感谢您的阅读！</p></div></div>    
</body>
</html>