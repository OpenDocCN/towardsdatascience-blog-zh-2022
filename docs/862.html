<html>
<head>
<title>Are You Aware How Difficult Your Regression Problem Is?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你知道你的回归问题有多难吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-you-aware-how-difficult-your-regression-problem-is-b7dae830652b#2022-01-31">https://towardsdatascience.com/are-you-aware-how-difficult-your-regression-problem-is-b7dae830652b#2022-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""><h1 id="61b9" class="pw-post-title ir is it bd iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp bi translated">你知道你的回归问题有多难吗？</h1></div><div class=""><h2 id="083b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一半的解决方案是理解你的问题，但一半的理解是知道它有多复杂</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/896e7afcf9a4fc49cb3219cb98ee65d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RCLHdecd0L3IQzvS"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Jaromír Kavan 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5e8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很多ML的帖子都讨论过分类问题的复杂性，比如他们的<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/01/class-sensitivity-machine-learning-classification-problems/" rel="noopener ugc nofollow" target="_blank">类敏感度</a>和<a class="ae ky" href="https://machinelearningmastery.com/imbalanced-classification-is-hard/" rel="noopener ugc nofollow" target="_blank">不平衡标签</a>。即使回归问题同样普遍，也没有通用的初步分析来评估给定回归问题的复杂性。幸运的是，ML研究人员[1，2]已经采用了众所周知的分类复杂性度量，以数字量化基于特征相关性、线性或平滑度度量拟合回归数据集的固有困难。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="8484" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将系统地描述每个复杂性度量的直觉、定义和属性，以及我如何用Python实现它们。这篇文章旨在通过方便的方法帮助ML从业者区分简单的线性问题和更复杂的变化。出于可读性的考虑，我偶尔会使用我在<a class="ae ky" href="https://gist.github.com/hoss-bb/bcbae7a3315d10b6727e98baaf1fb843" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上分享的助手模块中的函数。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="ce06" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">特征相关性度量(C)</h1><h2 id="8787" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">C1/C2:最大/平均特征相关性</h2><p id="ba54" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>一个特征和一个目标之间的相关度的绝对值反映了它们之间关系的强度，它表明了该特征能够在目标上提供多少信息(从0到1的标度)。</p><p id="710f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong><em class="nl">C1</em>和<em class="nl"> C1 </em>度量分别表示与目标输出相关的特征的最大值和平均值。通过Spearman相关性的绝对值来估计特征相关性[ <a class="ae ky" rel="noopener" target="_blank" href="/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8"> 3 </a> ]，这是一种非参数度量。</p><p id="5374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">性质:</strong> <em class="nl"> C1 </em>和<em class="nl"> C2 </em>在[0，1]的范围内，它们的高值暗示更简单的回归问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="9c4e" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">C3:个体特征效率</h2><p id="9224" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>数据中包含不同的对象或样本，因此特征和目标之间的关系在某些情况下可能很强，而在其他情况下可能很弱。在大多数情况下，当特征与目标高度相关时，回归被简化。</p><p id="36c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong><em class="nl">C3</em>度量估计为了产生一个子数据集(其中一个特征与目标输出高度相关)而应该移除的样本的最小比例。因此，有必要固定一个相关阈值来确定某个特征是否可以被视为与输出高度相关。对于每个特征，可以迭代地推断出在与输出的相关性超过预定阈值之前必须移除的样本的比例。因此，<em class="nl"> C3 </em>由所有计算比例的最小值组成。</p><p id="a22b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> C3 </em>产生[0，1]内的值，更简单的回归问题会有低值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="74e7" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">C4:集体特征效率</h2><p id="d5ea" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>以前，当应该移除较少的行时，我们查看与目标高度相关的特征。在这里，我们考虑特征的贡献来共同解释目标方差。</p><p id="3546" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong><em class="nl">C4</em>度量由可以用任何特征解释的剩余例子的比例组成。首先，根据要素与输出的相关性，从高值开始迭代要素。然后，排除使用所选特征的线性模型[ <a class="ae ky" rel="noopener" target="_blank" href="/understanding-linear-regression-94a6ab9595de"> 4 </a> ]可以解释其输出的所有示例。事实上，解释示例输出的能力是由小于预定阈值的残差值决定的。因此，当所有特征都已被分析或没有样本留下时，迭代过程结束，并且<em class="nl"> C4 </em>度量等于剩余样本的比率。</p><p id="f1f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> C4 </em>返回值在[0，1]以内<em class="nl"> C4 </em>值较低的回归问题更简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f93b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">线性测量(L)</h1><h2 id="9bbd" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">L1/L2:线性模型的平均/绝对误差</h2><p id="6a63" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>如果一个线性函数可以近似拟合数据，那么回归问题可以看做简单的涉及线性分布的数据点。</p><p id="34cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong> <em class="nl"> L1 </em>或<em class="nl"> L2 </em>测度分别代表多元线性回归模型的绝对残差[ <a class="ae ky" href="https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e" rel="noopener"> 5 </a> ]或平方残差[ <a class="ae ky" href="https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e" rel="noopener"> 5 </a> ]的和。</p><p id="6543" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong><em class="nl">L1</em>和<em class="nl"> L2 </em>是简单回归问题得分较低的正指标，这些问题几乎可以用线性函数来解决。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="51ca" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">L3:线性模型的非线性</h2><p id="739e" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>如果原始数据点是线性分布的，那么一个拟合的线性模型可以预测它们在分布内的插值变量。</p><p id="5076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong>原始数据分布中的随机插值可用于生成合成数据。具有接近输出(低<code class="fe no np nq nr b">|y_i-y_j|</code>)的一对数据点<code class="fe no np nq nr b">x_i</code>和<code class="fe no np nq nr b">x_j</code>，产生新的测试输入:<code class="fe no np nq nr b">(x_k,y_k)=(alpha*x_i + (1 — alpha)*x_j, alpha*y_i + (1 — alpha)*y_j)</code>，其中<code class="fe no np nq nr b">alpha</code>是在[0，1]内随机采样的实数。使用在原始数据上训练的线性模型，预测导出的随机输入的输出。因此，<em class="nl"> L3 </em>等于所有合成输入的均方误差。</p><p id="3a73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">性质:</strong> <em class="nl"> L3 </em>总是正的，只要回归问题简单就保持低。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a234" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">平滑度测量值</h1><h2 id="6893" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">S1:产量分布</h2><p id="6eb6" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>如果输入空间中相邻的数据点也具有接近的输出，那么基于输入特征预测目标将会更简单。</p><p id="5138" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong> <em class="nl"> S1 </em>衡量一对相邻的数据点在多大程度上会有接近的输出值。首先，从数据中生成最小生成树(MST) [ <a class="ae ky" href="https://medium.com/@pp7954296/minimum-spanning-tree-940b80568ecb" rel="noopener"> 6 </a>。因此，每个数据点对应于图形的一个顶点。根据数据点之间的欧几里德距离对边进行加权。MST技术包括贪婪地将更近的顶点连接在一起。因此，<em class="nl"> S1 </em>测量在构建的MST中连接的数据点之间的绝对输出差异的平均值。</p><p id="f299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> S1 </em>始终为正，对于相邻输入要素的输出也很接近的简单回归问题，该值趋于零。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="59ea" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">S2:投入分配</h2><p id="4c83" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>如果输出接近的数据点在输入空间中也接近，则根据输入特征预测目标会更容易。</p><p id="d389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong>首先，根据输出值对数据点进行排序，然后估计每对相邻行之间的距离。此后，<em class="nl"> S2 </em>返回输出关闭输入之间的平均距离。</p><p id="d104" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> S2 </em>始终为正，对于输入空间中输入要素接近的输入要素也是相邻要素的简单回归问题，该值趋于零。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="d701" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">S3:最近邻模型的平均误差</h2><p id="4eee" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>在一个简单的高密度回归问题中，一个例子的最近邻可以告诉我们很多。</p><p id="e898" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong> <em class="nl"> S3 </em>使用留一法计算最近邻回归量(kNN [ <a class="ae ky" rel="noopener" target="_blank" href="/intro-to-scikit-learns-k-nearest-neighbors-classifier-and-regressor-4228d8d1cba6"> 7 </a> ] with <code class="fe no np nq nr b">k=1</code>)的均方误差。换句话说，当数据点的预测仅仅是基于欧几里德距离的最近邻的输出时，S3返回平均误差。</p><p id="9760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> S3 </em>是一个正测度，其值越低，表示回归越简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="04d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> S4:最近邻模型的非线性</strong></p><p id="471e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">直觉:</strong>对于具有高密度的简单回归问题，数据分布内的插值合成数据点将从原始数据中找到有信息的最近邻。</p><p id="99fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定义:</strong> <em class="nl"> S4 </em>表示原始最近邻相对于线性导出的合成输入的信息量。可以通过在原始数据的分布内随机插值来生成一组合成的输入。具有接近输出(低<code class="fe no np nq nr b">|y_i-y_j|</code>)的每一对数据点<code class="fe no np nq nr b">x_i</code>和<code class="fe no np nq nr b">x_j</code>，产生新的测试输入<code class="fe no np nq nr b">(x_k,y_k)=(alpha*x_i + (1-alpha)*x_j, alpha*y_i + (1 — alpha)*y_j)</code>，其中<code class="fe no np nq nr b">alpha</code>是在[0，1]内随机采样的实数。基于最近邻回归模型，预测导出的随机输入的输出。因此，S4指数等于所有合成输入的均方误差。</p><p id="b499" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性:</strong> <em class="nl"> S4 </em>返回正值，但对于较简单的回归问题，它们较低。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><blockquote class="ns"><p id="e209" class="nt nu it bd nv nw nx ny nz oa ob lu dk translated">太棒了，你坚持到了最后。请随意使用这些指标来衡量您下一个回归问题的内在复杂性，并在评论中告诉我们您的想法。</p></blockquote><h1 id="056e" class="mc md it bd me mf oc mh mi mj od ml mm jz oe ka mo kc of kd mq kf og kg ms mt bi translated">参考</h1><p id="4111" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">[1] Maciel等，测量回归问题的复杂性(2016)，神经网络国际联合会议(温哥华)。</p><p id="9968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] Lorena等人，《回归问题的数据复杂性元特征》(2018)，《机器学习》(第107卷第1期，第209–246页)。</p><p id="4af4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Juhi Ramzai，明确解释:Pearson V/S Spearman相关系数(2020)，<a class="ae ky" rel="noopener" target="_blank" href="/clearly-explained-pearson-v-s-spearman-correlation-coefficient-ada2f473b8"> TDS </a>。</p><p id="70d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]饶彤彤，了解线性回归(2020)，<a class="ae ky" rel="noopener" target="_blank" href="/understanding-linear-regression-94a6ab9595de"> TDS </a>。</p><p id="1819" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] Akshita Chugh，MAE，MSE，RMSE，决定系数，调整后的R平方——哪种度量更好？(2020)，<a class="ae ky" href="https://medium.com/analytics-vidhya/mae-mse-rmse-coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e" rel="noopener">中</a>。</p><p id="a6e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] Payal Patel，最小生成树(2019)，<a class="ae ky" href="https://medium.com/@pp7954296/minimum-spanning-tree-940b80568ecb" rel="noopener">中</a>。</p><p id="42a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] Bex T .，Scikit-learn的k最近邻分类器和回归器介绍(2021)，<a class="ae ky" rel="noopener" target="_blank" href="/intro-to-scikit-learns-k-nearest-neighbors-classifier-and-regressor-4228d8d1cba6"> TDS </a>。</p></div></div>    
</body>
</html>