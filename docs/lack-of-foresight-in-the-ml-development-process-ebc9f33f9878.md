# 在 ML 开发过程中缺乏远见

> 原文：<https://towardsdatascience.com/lack-of-foresight-in-the-ml-development-process-ebc9f33f9878>

## 通过前瞻性的组织规划来解决潜在的问题

![](img/c569f061e3d075a15e10f3a51a5a5e6e.png)

[斯科特·格雷厄姆](https://unsplash.com/@homajob?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/machine-planning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍照

Y 您已经收集并准备了数据，设计了出色的特性，训练、评估并部署了您的模型。您和您的组织都很高兴:它在实践中显示出了巨大的成果，并成功地推进了您组织的业务目标。你终于完成了！

嗯……不完全是。

# 什么会出错？

几个星期，也许几个月，一切都很好，但是最终，有人意识到你的模型没有你想象的那么好。这种现象叫做**模型漂移**。模型漂移很大程度上是由*数据漂移*和*概念漂移*造成的。

*   ***数据漂移*** 是当模型的预测因子(自变量)发生变化时。例如，在垃圾邮件预测模型中，假设我们使用出站电子邮件的比率作为一个特征。如果在我们训练了我们的模型之后，电子邮件服务现在对出站电子邮件的比率实施了一个上限，那么这个独立变量的分布就发生了根本的变化。
*   ***概念漂移*** 是当模型的预测目标(因变量)发生变化时。使用前面的例子，这可能是由于用户对“垃圾邮件”的理解发生了变化。随着时间的推移，一个鲜为人知的出版物可能会变得更受欢迎和可靠，因此将来自该领域的电子邮件归类为垃圾邮件是不合适的，因为它们以前可能就是垃圾邮件。

随着时间的推移，这两种类型的漂移都会导致模型性能下降。如果不进行监控和纠正，部署的模型很快就会变得不准确和不可靠。

# 典型解决方案

这是很好的信息，但这是一个已经解决的问题。这很容易解决，我们的监控工具将检测模型漂移，然后我们只需收集更多数据，重新训练和重新部署…对吗？

## 问题

在与初始开发阶段相同的条件下，这是一个有效的假设。但是随着时间的推移，尤其是几个月之后，可能会出现以下问题:

*   谁负责数据收集、特征工程、模型创建、评估和部署？他们还在组织里吗？
*   数据存储在哪里？我们如何知道模型是在哪个版本的数据上被训练的？
*   模特住在哪里？ *"model_1_best_weights"* 还是 *"updated_model_1_v2"* 是生产中部署的模型？
*   数据处理和模型开发的代码在哪里？代码还存在吗？为什么看代码会让我想哭？

这些问题可能看起来很激烈。事实上，他们确实应该如此。但是这篇文章的灵感来自于答案:他们几个月前就离开了，数据丢失了，模型消失了，代码不可读了。祝你向你的委托人展示这个的时候好运。

# 为什么会出现这些问题？

我很幸运地在许多组织中工作过，看到了 ML 开发过程的各个阶段。我见过一些非常有问题的情况，和一些体面的情况，但从未见过这个过程做得非常好。这是为什么呢？

责怪工程师、数据科学家和开发团队很容易。但实际上，在大多数情况下，这些问题在组织和文化中根深蒂固得多。

> 难以纠正的模型退化问题源于**组织缺乏远见**。基本的长期问题在目光短浅的组织中激增。

## 反对主动行动的理由是什么？

我已经注意到了以下倾向于产生这些问题的开发实践的论点，特别是在较小的、较新的初创企业中。

> 这个问题不是问题。开发功能性的、可部署的模型的过程远没有模型本身重要。

对于一次性系统或分析，我同意这一点。临时系统不需要完美，只需要临时工作就能得出结论。然而，许多人错误地认为 ML 开发过程是临时的，导致了这种观点。相反，这个过程应该与传统软件工程的基本实践相当。

> 我们需要快速迭代来推出产品。

虽然这可能是真的，但是低于标准的开发实践实际上会**增加发货时间**。*“如果代码质量不好，错误和有问题的边缘案例很容易被忽视。这导致后来耗时的错误修复，最糟糕的是，生产失败。高质量的代码让你* ***早失败，快失败*** *。”[1]* 与直觉相反，在过程中放慢速度会让组织加快取得成果。

> 这只是一个概念验证，没有必要考虑可维护性。

许多“快节奏”组织的方法是从关注高速、**低质量**概念验证开始。这产生了快速但目光短浅的结果，不能很好地转化为 MVP(最低可行产品)。虽然这一过程在不确定其数据需求的组织中非常有效，但对于以数据驱动为目标的组织来说，我们已经知道这些项目是核心业务的一个必要方面。

这些论点的结合经常会导致我们所描述的问题。

# 我们能做些什么呢？

希望到目前为止，您已经意识到这是一个可能在组织中悄悄发生的重大问题。我将提出一套指导方针，指导组织采取先发制人的措施，在问题发生之前就加以预防。

## 1.监视

最起码的步骤是简单地监控模型的性能。虽然这并不能让我们*修复*问题，但它允许对问题进行初步检测。如果我们不知道一个问题存在，我们怎么知道去纠正它呢？

监控的目标是*“确保模型在应用于信心测试集时生成合理的性能度量。”[2]* 此外，置信集应定期更新，以说明上述分布变化。

## 2.迭代过程的重要性

一个组织应该强调 ML 开发过程的迭代性质的重要性，给团队足够的时间来考虑这个问题。不应低估维护周期。

> “大多数生产型号必须定期更新。该比率取决于几个因素:
> 
> 它出错的频率以及错误的严重程度，
> 
> 模型应该有多“新鲜”才能有用，
> 
> 新培训数据变得可用的速度，
> 
> 重新培训一个模型需要多长时间，
> 
> 部署模型的成本有多高，以及
> 
> 型号更新对产品和用户目标的实现有多大贡献。" [2]

## 3.数据版本控制

许多数据版本控制工具将自己标榜为*“数据 git”*。任何数据版本化工具的主要目的是同步不同版本的代码和数据(训练数据、测试数据、模型等)。).当一个模型需要更新时，我们可以在最后一次更新时获得开发状态的完美副本。模型更新后，如果我们的监控工具显示性能下降，我们可以快速轻松地恢复到以前的部署。我是 DVC 的支持者，但是还有很多其他的解决方案。

## 4.实验跟踪

实验跟踪工具允许所有实验相关数据(超参数、模型配置、结果等)的跟踪和可视化。)跨越多次运行。像[权重&偏差](https://wandb.ai/site)、 [MLflow](https://mlflow.org/) 和 [Neptune](https://neptune.ai/) 等工具都是很好的选择。这将允许不同模型版本之间的分离。

## 5.证明文件

开发人员最不喜欢的消遣。这很好地反映在 Jupyter 笔记本上杂乱无章的零星评论和许多项目中未完成的自述中。不幸的是，为了未来工程师的健康，模型架构的选择、复制步骤、结论和所有其他相关信息都应该被很好地记录下来。

# 结论

我们已经看到了有缺陷的开发过程是如何导致难以纠正的模型退化的。这个问题的根源不是缺少监控工具，而是导致这些长期问题的短视的组织行为。我提出了一套组织准则来解决上述一系列问题。

我希望你现在可以避免让我写这篇文章的痛苦。

这只是开始！如果你喜欢这篇文章，请关注我以获得下一篇文章的通知！我感谢❤️的支持

[](https://medium.com/@brandenlisk)  

**来源**

[1] E. Berge，[《作为数据科学家如何写出高质量的 Python》](/how-to-write-high-quality-python-as-a-data-scientist-cde99f582675)(2022)，走向数据科学

[2] A .布尔科夫，[机器学习工程](https://mlebook.com/) (2020)，加拿大魁北克:真正公司。