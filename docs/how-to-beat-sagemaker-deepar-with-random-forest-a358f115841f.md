# 用随机森林击败萨格马克·迪帕

> 原文：<https://towardsdatascience.com/how-to-beat-sagemaker-deepar-with-random-forest-a358f115841f>

## 建模变得简单

## 借助速度提高 3 倍的免费可解释模型，将您的 KPI 提高 15%

![](img/9883c7feb19597947713eb59cabe519a.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Victoire Joncheray](https://unsplash.com/@victoire_jonch?utm_source=medium&utm_medium=referral) 拍摄的照片

如果你正在阅读这篇文章，你要么考虑对你的**多时间序列**数据集使用[亚马逊 SageMaker DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) 算法，要么你已经被一个算法“卡住”——不知道如何进一步改进它。

你的数据集可能是什么样子。Airtable(数据由作者提供)

在深入研究尼提格里提技术之前，你应该考虑以下几点:

## DeepAR pros

1.  ***不需要数据科学专业知识*** *—如果你对数据科学一无所知，就不要继续阅读了。使用 DeepAR，它几乎不需要任何处理数据和创建预测模型的知识。*
2.  ***稳定性*** *—指标的方差很低，因为结果随着时间的推移相当稳定。你(一开始)看到的，就是你(将来)得到的。*
3.  **——你会使用一种最先进的算法，很可能是亚马逊自己在自己的销售数据上使用的。因此，向你的非技术同事推销可能更容易。**

## *DeepAR cons*

1.  ****SageMaker 所需专业知识****—*[*SageMaker 手册*](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-dg.pdf#whatis) *长达 3447 页。虽然 DeepAR 只有 16 页，但你必须仔细阅读它们，并遵循以下说明:格式化你的数据(即类别标签)，手动和自动调整超参数，回答 AWS 实例需求，并正确地将数据输入模型。**
2.  ****刚性&黑盒****——要改进一个现有的 DeepAR 模型并不是一件容易的事情。有一个小的、固定的超参数列表，如果您已经使用了超参数优化(您应该使用它),那么更改它们的值是没有用的！).此外，没有足够好的解释来说明模型的这些特征的重要性，所以很难在*[*CRISP-DM*](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)*阶段重申，或者只是向客户解释他的模型中到底发生了什么。**
3.  ****成本*** *—模型在 AWS 上的训练运行时间既耗费时间又耗费金钱*。*

*这就是你如何为你的 DeepAR 模型提供数据和超参数。是不是很奇怪？*

*如果你还和我在一起，你要么相信(像我一样)利大于弊，要么只是好奇想看看未来会怎样。无论哪种方式，我的经验告诉我，使用 3 倍的速度、相对免费和更易解释的模型，可以将 DeepAR 得分提高 15%，甚至可以在相当稀疏的销售数据上做到这一点。但这并不是一项容易的任务:我能给你的只有热血、汗水和一小时的编码，一直到胜利。*

*你准备好了吗？*

1.  *[了解你的工具](https://medium.com/p/a358f115841f#700e)*
2.  *[介意差距](https://medium.com/p/a358f115841f#614a)*
3.  *你在这里的目标是什么？*
4.  *[改变你的视角](https://medium.com/p/a358f115841f#9bff)*
5.  *[丰富你的故事](https://medium.com/p/a358f115841f#f2ad)*
6.  *[建模，正确的方法](https://medium.com/p/a358f115841f#15aa)*
7.  *[多余的边](https://medium.com/p/a358f115841f#1841)*

# *1.了解你的工具*

## *分组，分组，别忘了分组*

*首先，用正确的工具集来解决这个问题是至关重要的。由于数据的本质是有序序列，**很容易使用*对循环*** 预处理数据，这些循环一个接一个地迭代日期和产品(如[machinelingmastery](https://machinelearningmastery.com/xgboost-for-time-series-forecasting/)所示)。尽管如此，我还是劝你——不要那样做！如果你下了那个兔子洞，你可能再也出不来了，因为你的管道需要几个小时才能运行。*

*相反，去认识你的新好朋友: [pandas Groupby operation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) ，以及它所有的奇妙小伙伴:rolling，expanding，cumsum，cumcount，diff，sum，min，max，mean，pct_change，rank，head，tail，first，last，bfill，ffill，shift，nunique 等…当然，还有**group by****queen**:***apply。(lambda x:your_function(x))****

## *把事情理清楚*

*在将 Groupby 结果连接回 DataFrame 之前，不要忘记按正确的顺序排序 sort _ values()*，并且记住 *bfill* 和 *ffill* 函数很容易在不同的组之间溢出。使用 groupby + merge 更安全，因为它将确保那些丢失的值被正确组上计算的值填充。**

## ****维护自己****

**测试您所做操作的输入和输出的正确性总是一个好主意，即使只是通过键入一个简单的 ***断言*** 命令。通过使用 *assert* ，您可以确保您对数据所做的假设(稍后用于您的计算)是正确的。相信我，当数据变得复杂时，如果没有那些有用的小断言，就很难搞清楚到底发生了什么。**

**只是你的定期-正常-每天 5 天滚动窗口的最低，最高，平均和总和的产品销售在整个国家，为每种产品。很简单，对吧？**

**->还有输出。Airtable(数据由作者提供)**

# **2.小心（列车与站台之间的）空隙**

**如果数据集是完整的，即:**

> **行数=时间戳数 X 索引数**

**这一点可以跳过。否则，您可能应该填充那些缺失的观察值，这样您就可以正确理解目标在时间戳上的真实分布。**

**您的数据集不完整的一个常见原因也在于我们的多指标时间序列数据集的性质:**销售数据往往包括实际销售的信息，而没有任何零售额**。如果精度很重要，并且您希望您的模型知道销售何时不会发生，那么就用零行来填充它。**

**另一件要记住的事情是**有时候时间戳可能不是以日期的形式出现**。考虑一个预测学生毕业的模型。在这个模型中，时间特征可能是学生到目前为止获得的学术学分，而不仅仅是一些无用的日历日期。**

**零很重要！Airtable(数据由作者提供)**

# **3.你在这里的目标是什么？**

**DeepAR 将预测您的定量需求。但是如果时机比数量更重要呢？例如，您的最终目标是对每天每个城市的每种产品进行合理的销售预测，因为您必须为库存产品规划空间。但对你来说，更重要的是正确预测**任何数量**的产品将被出售的日期，因为任何数量大于零意味着你将向芝加哥的仓库发送一辆满载香蕉的新卡车。你不介意在这条赛道上装载一些额外的香蕉，但在根本没有香蕉出售的时候派出一辆新的卡车，可能是对你金钱的巨大浪费。如果对你来说是这样，时间比数量更重要，你应该考虑把你的目标转换成二进制的:大于 0 的数量为 1，否则为 0。**

**![](img/faae2d8d190da176ce8ef03e65bdb806.png)**

**有时候，仅仅一根香蕉就能改变一切。照片由[在](https://unsplash.com/@mockupgraphics?utm_source=medium&utm_medium=referral) [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的实体模型图形拍摄**

**但是我们如何在同一个模型中既有二元预测又有定量预测呢？简单——我们将训练一个两步模型:第一步预测销售是否会发生，第二步将**预测仅在预测销售的时间内** **销售多少数量。**不要担心我们在这里添加的额外工作，因为即使这是两个目标不同的不同型号，它们也可以使用完全相同的功能。**

**然后…瞧！我们现在有一个分类问题。Airtable(数据由作者提供)**

**最后，关于物质的目标，如何解决任何挑战的一个重要建议:**花一些时间浏览文献，最好是学术文献。你可能会从那些已经在这方面花费了大量时间并决定分享他们的成果的好心的研究人员那里得到一些关于如何定义和解决你的问题的好主意。所以，站在那些[巨人的肩膀上。](https://scholar.google.com/)****

# **4.改变你的视角**

**像 ARIMA 或 RNN 这样的时间序列算法知道如何处理你的时间序列数据集，一个索引一个索引。另一方面，像 Random Forest 和 XGBoost 这样的监督算法不会。它们将每一行与其他行分开处理，您不应该向它们提供时间戳列。因此，我们必须将我们的数据结构转换成[截面](https://en.wikipedia.org/wiki/Cross-sectional_data) ( *Groupby.shift* 将会很方便)。但是，如果您真的走这条路，请记住，对于日期、城市和产品的每种组合，您必须只有一行，所以要提前分组。**

**将它输入到没有日期列的模型中，行的顺序并不重要。Airtable(数据由作者提供)**

## **警告信息**

*   **添加这些滞后要素将会在每个组的第一个时间戳中添加缺少数据的像元，请考虑将这些行一起移除或为它们赋予一些有意义的值。你**唯一不能**做的就是用原始销售数据填充它们，因为那只是直接的目标泄露。**
*   **注意不要将行移动到错误的方向，从而创建未来销售特征而不是历史销售特征，这将再次导致泄漏。**

# **5.丰富你的故事**

**数据完整性、项目目标和问题视角非常重要，但是如果没有一套好的特性来辅助故事情节，预测可能会做得很差。我们的故事有两种特征:**(独立的):****

*   ******时间固定特征*** 往往来源于类别(即地点、名称、品牌)，可以用通常的方式处理， ***目标编码*** 是一种降低稀疏性的方法。只需记住在训练中计算，保存计算的统计数据，并在测试中重新应用它们，而无需再次计算。***
*   ***的时间相关特性既可以用[点 4](https://medium.com/p/a358f115841f#9bff) 中提到的移位来丰富，也可以用 ***滚动窗口*** 聚合器来丰富，如[点 1](https://medium.com/p/a358f115841f#700e) 的示例输出。与时间相关的功能也可能包含一些有用的数据，如 ***扩展*聚合器**，即对于每个时间戳，直到该时间戳的平均销售额。最后，还有 ***全列车*统计**，比如在每个时间戳中有整个列车期间的销售总和(或加权总和)。这些特性包含一些产品“永远正确”的数据，就像一个*时间固定特性*。例如，随着时间的推移，香蕉在总销售额中的权重为 50%，因此在 *total_sales* 特性中，包含香蕉的每一行将得到 0.5。***

**![](img/d869ca66e678d57f0ff55487fbb45ad7.png)**

**投资这个故事，它会回报你的投资。[农 V](https://unsplash.com/@californong?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照**

**这里的一个关键问题是数据管道。**你应该有一个有效的功能管道** —记住，管道将为训练和测试运行两次，它们之间有一些差异(因为一些计算将只发生在训练中，并复制到测试)。确保每个功能完全独立于其他功能，这样您就可以添加和删除功能，而不必担心它们对其他功能的影响。**

# **6.建模，正确的方法**

**最后，面包和黄油。以下是一些特定于时间序列的建模建议:**

*   **像在[点 2](https://medium.com/p/a358f115841f#614a) 中那样填充缺失的时间戳后，您的数据集可能会变得完全不平衡。**对销售行**进行上采样，使其等于零销售额的行数。这将增加目标的信号，可以帮助您的模型更好地学习。**
*   ****不要随意拆分你的数据**。在时间上分割它(训练=过去，测试=最后的时间戳)，并且在部署之前总是记住在你的整个数据集上重新训练你的模型(训练+测试)。这样，部署的模型将根据尽可能接近新数据的数据进行训练。**
*   **记住**删除任何带有未来信息的特性**，或者任何不再出现的时间戳(具体日期不行，具体月份可以接受)。**

**基本上与每个 ML 模型相关的其他要点:**

*   ****去掉高度相关的特征。**它们的贡献很小，但会对相关特征的系数产生偏差。**
*   ****根据客户的 KPI 优化您的决策门槛！**这可能会带来巨大的改进，而且应用起来相当简单:计算 0 到 1 之间的 100 个不同阈值的 KPI 值，并选择使训练数据的 KPI 最大化的阈值。**
*   **为不同的算法尝试不同的超参数，绘制衡量指标对模型复杂性的改进的训练与测试图。**确保你足够接近“刚好合适”**:一方面，你的模型没有过度训练和学习你的训练数据的随机性，另一方面，你的模型训练得足够努力，以达到令人满意的结果。**

# **7.额外的优势**

## **多个组的多个模型**

**有时，您的数据非常庞大和复杂，模型很难收敛到最佳解决方案。例如，香蕉和苹果的销售模式确实不同，但是不同城市的苹果销售模式也确实不同。它可能不会就此停止，因为同一座城市的一些商店在周末和节假日会卖出更多的苹果，而且，出于某种未知的原因，他们可能还会在每个月的第三周卖出更多的苹果。**

**在这种情况下，以及在任何其他情况下，努力**在数据中找到有意义的组，然后在每个不同的组上训练独立的模型**(从[点 5](https://medium.com/p/a358f115841f#f2ad) 开始的所有规则适用于每个模型)。你会惊讶地发现这一步是如何进一步改进你的模型的，即使在你已经将特征工程和超参数优化挤压到骨骼之后。更不用说，DeepAR 可能不容易分成不同的模型，因为每个模型需要的观察最少，或者只是训练更多模型所需的额外时间和金钱，所以您将拥有与生俱来的优势。**

**![](img/ca504e46573330556bb3afbb420ea141.png)**

**当有许多锁着的门并且很难找到主人时，使用许多钥匙。照片由 [Alp Duran](https://unsplash.com/@alpduran?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄**

**你如何找到这些群体？您可以使用分类特征(例如每个城市、每个项目、每个季度的模型)，或者——这可能与时间序列数据更相关——您可以**将具有相似销售模式的组聚集在一起**。例如，创建一个在训练中的最后一个时间戳之前的最后 6 天中某个城市中某个产品的二进制销售额总和的特征。这个特性的范围自然是从 0 到 6，因此您可以为这 7 组中的每一组训练一个单独的模型。因此，如果在最后一次约会之前的最后 6 天里，休斯顿的枣椰被卖了 3 次，而洛杉矶的接骨木莓被卖了 5 次，那么休斯顿的枣椰和洛杉矶的接骨木莓将被放入不同的模型中。最后，**使用统计学**你总能找到有意义的群体:记住不同群体可能的分裂列表，对这些群体进行目标编码，并查看分裂的平均值是否“足够”不同。您也可以使用[卡方检验](https://en.wikipedia.org/wiki/Chi-squared_test)来完成这项任务。**

# **摘要**

**针对多个时间序列数据的亚马逊 SageMaker DeepAR 模型是一种最先进的算法，由一家科技巨头开发。不需要 DS 知识，相当稳定可靠。尽管如此，它并不总是交付最好的结果，即使结果是合理的，它们也很难被解释，并且总是耗费大量的时间和金钱。**

**我认为，技术娴熟的数据科学家可以做得更好。如果她:[使用 Groupby 命令](https://medium.com/p/a358f115841f#700e)，[确保数据完整且充分](https://medium.com/p/a358f115841f#614a)，[正确定义目标](https://medium.com/p/a358f115841f#9d65)，[将时间序列操纵成横截面](https://medium.com/p/a358f115841f#9bff)，[工程师洞察力特征](https://medium.com/p/a358f115841f#f2ad)，[训练模型推进并达到最佳可实现的精度](https://medium.com/p/a358f115841f#15aa)，甚至[为不同的组训练不同的模型](https://medium.com/p/a358f115841f#1841) —
**她能够并将打败 DeepAR****

> **欢迎分享您的反馈，并通过 LinkedIn 联系我。**
> 
> **感谢您的阅读，祝您好运！🍀**