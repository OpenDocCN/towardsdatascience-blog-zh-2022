<html>
<head>
<title>Building a Random Forest Classifier to Predict Neural Spikes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建随机森林分类器来预测神经尖峰</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-random-forest-classifier-for-neural-spike-data-8e523f3639e1#2022-01-30">https://towardsdatascience.com/building-a-random-forest-classifier-for-neural-spike-data-8e523f3639e1#2022-01-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""><h1 id="2911" class="pw-post-title io ip iq bd ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm bi translated"><strong class="ak">构建随机森林分类器来预测神经尖峰信号</strong></h1></div><div class=""><h2 id="2cb8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">在Python中构建随机森林分类器以预测真实神经细胞外尖峰的子类型的分步指南。</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/5054fdd9991494b6a8be60f14f954667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ss_LylfOEI_oHiRaBTQw_A.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/photos/58Z17lnVS4U" rel="noopener ugc nofollow" target="_blank">未喷涂</a>上的Fakurian设计的“Braintree”</p></figure><p id="6efc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> G </span> <em class="mc">考虑到人脑本身神经元的异质性，分类工具通常被用来将电活动与不同的细胞类型和/或形态相关联。</em>这是神经科学界的一个长期问题，在不同物种、病理、大脑区域和层次之间可能有很大差异。幸运的是，随着快速增长的计算能力允许机器学习和深度学习算法的改进，神经科学家获得了进一步探究这些重要问题的工具。然而，正如Juavinett等人所述，在大多数情况下，编程技能在社区中的代表性不足，教授这些技能的新资源对于解决人类大脑的复杂性至关重要。</p><p id="9f80" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因此，为了提供一个相关的用例，在本文中，我们将为神经细胞外锋电位波形数据集构建一个随机森林分类器算法。随机森林分类器是广泛应用的监督学习模型，是解决分类问题的有效而强大的算法。可以说，它们位于分类器层次结构的顶端，旁边还有其他算法，如:逻辑回归、支持向量机、朴素贝叶斯分类器和决策树。</p><p id="2f42" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在此过程中，我们还将完成多维降维和聚类步骤(带有示例代码)，以建立一个有监督的学习问题。一旦完成，任何级别的程序员将能够(1)识别和绘制独特的细胞外波形,( 2)实施随机森林算法对它们进行分类。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="ae5d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">什么是决策树？</h1><p id="9580" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">返璞归真！首先，为了理解随机森林图，掌握决策树是很重要的(这将很快有意义)。<em class="mc">简而言之，决策树是一种由“节点”和“分支”组成的树状模型，是一种可视化展示决策及其结果的工具。</em></p><p id="18a8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如<a class="nh ni ep" href="https://medium.com/u/8418ab50405a?source=post_page-----8e523f3639e1--------------------------------" rel="noopener" target="_blank"> Angel Das </a>、<a class="nh ni ep" href="https://medium.com/u/1fd70d25ff14?source=post_page-----8e523f3639e1--------------------------------" rel="noopener" target="_blank"> z_ai </a>所总结的，并且如下图所示，节点可以分类如下:</p><ul class=""><li id="6e49" class="nj nk iq kz b la lb ld le lg nl lk nm lo nn ls no np nq nr bi translated"><strong class="kz ir">根节点:</strong>起始节点。在决策树中，它通常评估最能分割数据的变量。</li><li id="f066" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated"><strong class="kz ir">决策节点:</strong>根据条件分支的子节点。</li><li id="1f42" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated"><strong class="kz ir">叶节点:</strong>树的最终节点，不能再分支。</li></ul><p id="1d07" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从根节点开始，按照if-else结构在决策节点对一个独立变量(连续变量或分类变量)进行拆分。沿着树向下移动，数据不断被“分割”,直到到达可以进行分类的叶节点。如果不是这种情况，则重复该过程，直到达到结果。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/b5e6f0f7a43bbc294dc2962c878a9964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJyByQvx160sqkQ31Q9BYQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">带有定义的决策树插图。图片作者。</p></figure><h2 id="f866" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">决定分裂</h2><p id="307f" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">决策树使用多种算法来决定问题的最佳分割，但<strong class="kz ir">“基尼指数”和信息增益的“熵”</strong>是最常见的标准。两者都确定了节点处标签的<strong class="kz ir">杂质</strong>，这是在判定节点处异质或混合值程度的度量。这是至关重要的，因为决策树算法找到了最佳标准，使集合成为更同质的子集(即↓杂质)而不是异质的子集(即↑杂质)。</p><p id="bef9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">熵</strong>是对一组数据的杂质的一种度量，可以通过以下公式表示:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/68cca76494b55901ac84073f280d9eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*Zegxm4rIFAKlNOurO7b0ng.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">熵公式。其中“<strong class="bd ol"> Pi </strong>表示数据集中类别“I”的概率。</p></figure><p id="a350" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通过使用熵计算，可以计算节点处的随机性或无序度。<strong class="kz ir">信息增益</strong>使用熵来量化哪个特征通过减少熵来提供关于分类的最大信息，并因此做出相应的分割:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7146a4280cdeb231739f3216fb912897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*PWSg3CiV6jWxSvQQZ03k_Q.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">信息增益公式。从类“<strong class="bd ol"> X </strong>”上的“<strong class="bd ol"> Y </strong>”的熵中减去类“<strong class="bd ol"> Y </strong>的熵。</p></figure><p id="375e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">与熵相似，<strong class="kz ir">基尼系数</strong>(也称为<strong class="kz ir">基尼系数或基尼系数</strong>)在0和1之间变化。它通过测量目标属性值的概率分布之间的差异来计算分类的杂质。0的输出是同质的，而1是不纯的，这表明新的随机数据被错误分类的可能性很大。一旦实现，就进行节点分裂，这减少了这种计算的杂质。对于“C”类不同阶层，基尼公式将为:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9309a9195b952838ee67003db24b4756.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*9x70BLJN9VIsxQ-PVm34Lw.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">基尼系数公式，其中"<strong class="bd ol"> C" </strong>表示数据子集中的总类别数，而"<strong class="bd ol"> i" </strong>是从C中选择的类别。</p></figure><h2 id="7b49" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">决策树的衰落</h2><p id="82d3" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">虽然决策树可以为分类问题提供一个可视化的解决方案，但是使用该算法的缺点<strong class="kz ir">应该被考虑在内:</strong></p><ul class=""><li id="2823" class="nj nk iq kz b la lb ld le lg nl lk nm lo nn ls no np nq nr bi translated">它们将继续形成分支，直到每个节点都是同质的。如果测试一个小样本，这就产生了一个过度拟合的问题(鲁棒性)。</li><li id="ea81" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated">一次仅使用一个独立变量来确定分类，这可能会导致性能和准确性问题。</li><li id="af24" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated">不处理缺少的值和数值属性。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="feab" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">随机森林分类算法是如何工作的？</h1><p id="2993" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">理解分类和回归树(CART)的基本概念的重要性在这里发挥了作用，因为<strong class="kz ir">随机森林</strong>使用许多决策树的<strong class="kz ir">集合</strong>而不是一个来做出最终决策(集合技术)。这是特别强大的，因为模型的集合将胜过单个模型。最终，这解决了与决策树相关的一些缺点，因为它提高了性能和健壮性。值得注意的是，集合中的每棵树都是相对不相关的，这很重要，因为正如<a class="nh ni ep" href="https://medium.com/u/840a3210fbe7?source=post_page-----8e523f3639e1--------------------------------" rel="noopener" target="_blank">饶彤彤</a>所概述的:</p><blockquote class="oo op oq"><p id="f4e7" class="kx ky mc kz b la lb jr lc ld le ju lf or lh li lj os ll lm ln ot lp lq lr ls ij bi translated">这些树保护彼此免受各自错误的影响(只要它们不总是在同一个方向出错)。虽然有些树可能是错误的，但许多其他的树将是正确的，因此作为一个群体，这些树能够朝着正确的方向移动。</p></blockquote><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/0c2e0491467c01dd8571b563985b5f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jM6Z2SEgo3lrbQ1j6o6aYg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">一种简化的随机森林分类算法的图示。图片作者。</p></figure><h2 id="2690" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">随机森林算法如何构建多棵树？</h2><p id="99b3" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">随机森林算法实现了<strong class="kz ir"> (1)引导聚合和(2)特征随机性</strong>的组合，以使用相同的数据集构建许多决策树。总体而言，它们在这样做的同时保持相对不相关，这是一个重要的特性:</p><ol class=""><li id="07f0" class="nj nk iq kz b la lb ld le lg nl lk nm lo nn ls ou np nq nr bi translated"><strong class="kz ir"> bootstrap聚合(又名Bootstrap或bagging) </strong>是一种技术，涉及在给定迭代次数和变量(Bootstrap样本)的基础上随机抽样数据子集，并进行替换。来自所有迭代和样本的预测通常被平均以获得最可能的结果。重要的是要理解，它不是将数据“分块”成小的大小，并在其上训练单个树，而是仍然保持初始数据大小。这是一个应用集合模型的例子。</li><li id="7992" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls ou np nq nr bi translated"><strong class="kz ir">特征随机性</strong>主要作用是降低决策树模型之间的相关性。与可以利用所有特征来辨别最佳节点分裂的决策树相比，随机森林算法将随机选择这些来进行决策。最终，这允许训练也在不同的特征上发生。</li></ol><blockquote class="oo op oq"><p id="ffed" class="kx ky mc kz b la lb jr lc ld le ju lf or lh li lj os ll lm ln ot lp lq lr ls ij bi translated"><strong class="kz ir">总的来说，通过这些方法，随机森林算法可以在相同数据的不同集合上进行训练(bootstrapping ),同时还可以利用不同的特征来生成预测结果。这是一个比决策树更强大的分类工具。</strong></p></blockquote><h2 id="f857" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">特征重要性</h2><p id="8b50" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">最后，随机森林算法还将使用<strong class="kz ir">基尼系数</strong>(或平均减少杂质)来评估数据集中每个特征在人工分类问题上的重要性。现在知道了随机森林算法是使用决策树的集合来构建的，直观地，每个内部节点是使用基尼不纯或信息增益来选择的(如上所述)。对于每个特征，计算杂质的减少，并在集合中的所有决策树上平均，以确定特征重要性。该方法在随机森林的<code class="fe ov ow ox oy b"><a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener ugc nofollow" target="_blank">scikit-learn</a></code> <a class="ae kw" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_" rel="noopener ugc nofollow" target="_blank">实现</a>中可用(分类器和回归器都适用)。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="8fb2" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">将随机森林分类器应用于细胞外神经记录:</h1><p id="93a0" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">现在，申请！我们将使用一个公开可用的数据集，这个数据集是在授权<strong class="kz ir">Attribution 4.0 International(CC by 4.0)</strong>下<a class="ae kw" href="https://www.kaggle.com/joseguzman/spike-classification-based-on-waveforms/data" rel="noopener ugc nofollow" target="_blank">提供的。这是一组取自哺乳动物大脑的细胞外记录。</a></p><blockquote class="oo op oq"><p id="720d" class="kx ky mc kz b la lb jr lc ld le ju lf or lh li lj os ll lm ln ot lp lq lr ls ij bi translated">就上下文而言，<strong class="kz ir">细胞外记录</strong>是由细胞产生的电位的记录，或者在感兴趣的细胞附近的细胞外液中，或者无创地。</p></blockquote><h2 id="644a" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">加载库和数据集</h2><p id="dd8b" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">首先，我们将导入以下库和load required数据集。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="5046" class="ny ml iq oy b gy pd pe l pf pg"># Install the following libraries<br/>import pandas as pd<br/>import numpy as np<br/>from umap import UMAP<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar</span><span id="0629" class="ny ml iq oy b gy ph pe l pf pg">from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.cluster import AgglomerativeClustering<br/>from sklearn.neighbors import kneighbors_graph<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_samples, silhouette_score<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix</span></pre><p id="0e12" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了熟悉数据集，首先可视化数据类是很重要的。这使我们能够更好地理解数据集和组织数据的可能方式。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="e275" class="ny ml iq oy b gy pd pe l pf pg"># Load waveforms<br/>mypath = '*insert file path/waveforms.csv'<br/>data = pd.read_csv(mypath, index_col = 'uid')</span><span id="019a" class="ny ml iq oy b gy ph pe l pf pg"># Get an overview of the data<br/>print(f'{data.shape[0]} unique experiment identifiers (uid), recorded with a sampling frequency (KHz) of {((data.shape[1]-1)/5)}')</span><span id="95d9" class="ny ml iq oy b gy ph pe l pf pg"># Class breakdown<br/>data.organoid.value_counts()</span></pre><p id="c642" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">看到我们有多个类，如<code class="fe ov ow ox oy b">value_counts()</code>所示，我们将用唯一的颜色代码标记每个类。当在进一步的数据分析中可视化这些类时，这将被证明是有用的。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="886f" class="ny ml iq oy b gy pd pe l pf pg"># Define custom colors for visualization<br/>mycolors = {'Data_D':     '#FFA500', # orange<br/>           'Data_G':       '#4169E1', # royalblue<br/>           'Data_F':       '#FF4500', # orange red<br/>           'Data_C':       '#9400D3', # darkviolet<br/>           'Data_A':       '#32CD32', # limegreen <br/>           'Data_E':       '#228B22', # forestgreen<br/>           'Data_G_V2' :   '#006400', # darkgreen <br/>           'Data_H':       '#00BFFF', # deepskyblue <br/>           'Data_E_V2':    '#DC143C', # crimson<br/>           'Data_F_V2':    '#0000FF', # blue<br/>           'Data_B':       '#000000', # black<br/>           }</span><span id="61df" class="ny ml iq oy b gy ph pe l pf pg"># Add color to the DataFrame<br/>data['color'] = data['organoid'].apply(lambda orgID: mycolors[orgID])</span></pre><p id="3ed1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，我们可以想象！我在下面展示了一个简单的柱状图，总结了每节课的录音数量。然而，更多的Python数据可视化工具可以在<a class="ae kw" rel="noopener" target="_blank" href="/top-python-libraries-for-visualization-a-starting-guide-73402178811b">这里</a>的上一篇文章中找到。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="1117" class="ny ml iq oy b gy pd pe l pf pg"># Visualizing the unique experiment identifiers<br/>fig, ax = plt.subplots(figsize=(15, 8))<br/>sns.barplot(x=data.organoid.value_counts().index, y=data.organoid.value_counts(), palette=mycolors)</span><span id="eacf" class="ny ml iq oy b gy ph pe l pf pg"># Customizing the graph<br/>plt.xticks(rotation=30,fontsize=14)<br/>plt.yticks(fontsize=14)<br/>ax.set_xlabel('Class type', fontsize=16)<br/>ax.set_ylabel('Number of waveforms', fontsize=16)<br/>plt.rcParams["font.family"] = "Arial"</span><span id="efd5" class="ny ml iq oy b gy ph pe l pf pg"># Despine<br/>right_side = ax.spines["right"]<br/>right_side.set_visible(False)<br/>top_side = ax.spines["top"]<br/>top_side.set_visible(False)</span><span id="e5ce" class="ny ml iq oy b gy ph pe l pf pg">plt.savefig('Figures/barplot.png', dpi = 300, bbox_inches="tight")<br/>plt.show()</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/0bdebf23eeee191a0dcb19b6e8fe1df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aL3aY0KYtV_9r-Jv5Cv98A.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。一个条形图，突出显示了每种类型记录的波形数量的变化。</p></figure><h2 id="0c98" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">可视化细胞外波形</h2><p id="5cd4" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">假设我们正在分析细胞外记录，我们将可视化每个数据集类产生的平均波形。这将为手头的分类问题提供进一步的见解。下面，我通过计算每个数据集类的平均轨迹，并叠加到它们的输入轨迹上来实现这一点。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="7ab8" class="ny ml iq oy b gy pd pe l pf pg"># Isolating the waveforms for each class <br/>class_names = data['organoid'].unique()</span><span id="02ac" class="ny ml iq oy b gy ph pe l pf pg"># Plotting mean traces for each organoid class<br/>fig, ax = plt.subplots(1,9, figsize=(24,4.5))<br/>for unique_class in class_names:    <br/>        df_new = data[data['organoid'] == unique_class] #isolating </span><span id="06b2" class="ny ml iq oy b gy ph pe l pf pg"># np.array conversion<br/>        df_new = df_new.iloc[:,:-2].to_numpy() #dropped last column</span><span id="ee51" class="ny ml iq oy b gy ph pe l pf pg"># Averaging across samples per organoid class <br/>        data_mean_perclass = np.mean(df_new, axis=0)</span><span id="1b9c" class="ny ml iq oy b gy ph pe l pf pg"># Sampling frequency for plot generation<br/>        sampling_freq = np.linspace(0, 5, 150) #recording length is 5ms per 150 samples</span><span id="aaa1" class="ny ml iq oy b gy ph pe l pf pg">for i in range(class_names.shape[0]): <br/>            if unique_class == class_names[i]:<br/>                <br/>                # Plotting all traces behind in light color<br/>                for row_num in range(df_new.shape[0]): <br/>                    ax[i].plot(sampling_freq, df_new[row_num,:], color = 'lightgray')<br/>                <br/>                # Plotting each mean waveform into a separate subplot <br/>                ax[i].plot(sampling_freq,data_mean_perclass, color=mycolors[unique_class], linewidth=3)<br/>                ax[i].set_ylim([-1.8, 1.8])<br/>                ax[i].grid()<br/>                ax[i].axis('off')<br/>                ax[i].title.set_text(unique_class)  <br/>                plt.rcParams["font.family"] = "Arial"<br/>                <br/>            else: <br/>                continue<br/>        <br/>        # Scale bar<br/>        scalebar = AnchoredSizeBar(ax[8].transData, 1, "1 ms", 'lower right', frameon=False, size_vertical=0.02, pad=0.1)<br/>        ax[8].add_artist(scalebar)<br/>        <br/>plt.savefig('Figures/spikes.png', dpi = 300)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/abaeb6c7721fe21244cb70f305b3a499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2arEuKXIdsP2x8fTLZQwA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">每一类的平均细胞外波形以彩色显示，并与其子图标题相对应。浅灰色表示用于产生平均输出阵列的输入信号。作者提供的图片，显示尺寸已重新格式化。</p></figure><h2 id="ca8a" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">多维缩减</h2><p id="70c9" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">从视觉上看，波形轨迹突出显示了每个数据集类都有许多来自不同神经元细胞类型的单个尖峰波形。平均起来，可以说这些类中有些看起来很像，有些则不像。</p><p id="57d3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了更好地验证这一点，我们将使用统一流形近似和降维投影(UMAP)对细胞外波形数据进行多维降维。最后，还可以进行主成分分析或t分布随机邻居嵌入(t-SNE)。西瓦卡尔·西瓦拉贾 <a class="ae kw" rel="noopener" target="_blank" href="/dimensionality-reduction-for-data-visualization-pca-vs-tsne-vs-umap-be4aa7b1cb29">在这里</a>概述了这些方法之间的区别。出于本文的考虑，我们将只关注前两个部分，以获取大部分方差。</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="8856" class="ny ml iq oy b gy pd pe l pf pg"># UMAP calculation<br/>umap_2d = UMAP(n_components=2, random_state = 43)<br/>projections = umap_2d.fit_transform(data.iloc[:,:-2])</span><span id="2834" class="ny ml iq oy b gy ph pe l pf pg"># Concat dataframes for seaborn scatter plot<br/>UMAP_df = pd.DataFrame(projections, columns = ['UMAP 1','UMAP 2'])<br/>UMAP_df_concat = pd.concat([UMAP_df,data['organoid'].reset_index(),data['color'].reset_index()], axis=1)</span><span id="3bda" class="ny ml iq oy b gy ph pe l pf pg"># Figure plotting<br/>sns.set(font_scale=1.2)<br/>sns.set_style("whitegrid", {'axes.grid' : False})<br/>fig1 = sns.relplot(data = UMAP_df_concat, x = "UMAP 1", y = "UMAP 2", hue="organoid", kind="scatter", palette=mycolors, height=5, aspect=1.5)<br/>fig2 = sns.relplot(data = UMAP_df_concat, x = "UMAP 1", y = "UMAP 2", hue="organoid", kind="scatter", palette=mycolors, height=3, aspect=1.5, col='organoid', col_wrap=3)</span><span id="b91f" class="ny ml iq oy b gy ph pe l pf pg"># Figure export<br/>fig1.savefig('Figures/UMAP.png', dpi = 300)<br/>fig2.savefig('Figures/UMAP_2.png', dpi = 300)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/0791d9ed8a306dbdbd3ea6841c63ec84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsELYKAFtSFzPnbh8z9fSg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">细胞外波形时间序列数据的UMAP图，n_components = 2。颜色对应于每个数据集类。图片作者。</p></figure><p id="9d28" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从空中俯瞰，UMAP上正在形成不同的星团。每个聚类没有同质的类别类型(由颜色变化显示)，而是来自不同类别的不同细胞外记录的组合。因此，很可能每个有机类类型都具有与其他类相似的波形比例。</p><p id="3d35" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了更好地查看每个数据集对UMAP的贡献以及它们是如何聚类的，我们将隔离它们的数据:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/5b5fca1fd390010d4e28d525c21f4a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0cjEgc8PXZFr-7Ba5g0Bg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">代表来自每个数据集类的独特细胞外波形时间序列数据的一系列UMAP子图。图片作者。</p></figure><p id="c238" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">的确，来自特定数据集类的一些记录看起来更相似地聚类，而其他记录则不相似。这与我们的波形轨迹一致。接下来，我们将对数据进行聚类，为我们的随机森林算法建立一个分类问题。</p><h2 id="637e" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">k均值聚类</h2><p id="0cb7" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">为了使用前两个分量对我们的数据进行无偏聚类，我们将利用K-means。这种聚类方法属于聚类的划分类，用于最小化总的类内变化。为了选择最佳聚类数，<em class="mc">我们将利用(1)肘和(2)剪影方法:</em></p><ol class=""><li id="ac8c" class="nj nk iq kz b la lb ld le lg nl lk nm lo nn ls ou np nq nr bi translated"><strong class="kz ir"> Elbow方法:</strong>计算总的类内平方和(wss)，它衡量聚类的紧密性。目的是尽可能减少这种情况。</li><li id="ee08" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls ou np nq nr bi translated"><strong class="kz ir">剪影法:</strong>我们将使用剪影法回测肘法。这种方法通过确定对象在其中的位置来测量群集的质量。高轮廓值表示“好”的聚类，而低轮廓值表示“差”的聚类。</li></ol><p id="5b99" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是这两种方法之间的微妙平衡，可以为您的数据确定最佳的聚类数。所以，让我们一步一步来:</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="9daf" class="ny ml iq oy b gy pd pe l pf pg"># Set range of possible clusters<br/>range_n_clusters = range(2, 11) #set as 2 to 10</span><span id="ddae" class="ny ml iq oy b gy ph pe l pf pg"># Elbow method &amp; Silhouette scores for optimal number of cluster<br/>wcss = [] #within cluster sum of square <br/>silhouette_values = []<br/>for i in range_n_clusters:<br/>    <br/>    kmeans = KMeans(n_clusters = i, random_state = 42)<br/>    kmeans.fit(data.iloc[:,:-2])<br/>    wcss.append(kmeans.inertia_)<br/>    <br/>    cluster_labels = kmeans.fit_predict(data.iloc[:,:-2])<br/>    silhouette_avg = silhouette_score(data.iloc[:,:-2], cluster_labels)<br/>    silhouette_values.append(silhouette_avg)</span><span id="153d" class="ny ml iq oy b gy ph pe l pf pg"># Isolating highest Silhouette value calculation<br/>max_silhouette = max(silhouette_values)<br/>max_index = silhouette_values.index(max_silhouette)<br/>print(f'Optimum number of clusters is {range_n_clusters[max_index]}')<br/>    <br/># Figure plotting<br/>fig, ax = plt.subplots(1,2, figsize=(18,4.5))<br/>ax[0].plot(range_n_clusters, wcss)<br/>ax[1].plot(range_n_clusters, silhouette_values)<br/>ax[0].title.set_text('The Elbow Method')<br/>ax[1].title.set_text('Optimal Cluster Number')<br/>ax[0].set(xlabel='Number of Clusters', ylabel='WCSS')<br/>ax[1].set(xlabel='Number of Clusters', ylabel='Silhouette score')</span><span id="5bcd" class="ny ml iq oy b gy ph pe l pf pg"># Line to indicate optimum cluster number<br/>ax[0].axvline(x=range_n_clusters[max_index], color='black', label='axvline - full height', ls='--')<br/>ax[1].axvline(x=range_n_clusters[max_index], color='black', label='axvline - full height', ls='--')</span><span id="1bc1" class="ny ml iq oy b gy ph pe l pf pg"># Export figures<br/>fig.savefig('Figures/Silhouette_elbow_method.png', dpi = 300, bbox_inches="tight")</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pj"><img src="../Images/cabd422d0429fc0a41f4e6780e33f093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lws_LmqZpPa_FbSWVrpmTQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">肘(左)和侧影方法(右)对神经尖峰数据集的结果。虚线突出显示了使用K均值聚类的最佳聚类数。图片作者。</p></figure><p id="bde4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">正如图中虚线所示，<em class="mc">我们数据集的最佳聚类数是3 </em>。原因是对于这个数字，轮廓得分最高，而组内平方和(WCSS)减少。我们将通过使用以下代码在我们的组合UMAP图上可视化聚类来确认这一点:</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="be34" class="ny ml iq oy b gy pd pe l pf pg"># Agglomerative clustering with optimal cluster number on UMAP<br/>def clustering_agglo(df, n_clusters):<br/>    X = df.to_numpy()<br/>    connectivity = kneighbors_graph(X, int(len(df.index)/10), include_self=False)<br/>    agc = AgglomerativeClustering(linkage='ward', connectivity=connectivity, n_clusters=n_clusters)<br/>    agc.fit(X)<br/>    print(f'Labelling {len(np.unique(agc.labels_))} clusters on original UMAP projection')<br/>    return agc.labels_</span><span id="b5df" class="ny ml iq oy b gy ph pe l pf pg">labelsAgglo = clustering_agglo(data.iloc[:,:-2],range_n_clusters[max_index])</span><span id="a16e" class="ny ml iq oy b gy ph pe l pf pg"># UMAP plotting<br/>sns.set(font_scale=1.2)<br/>sns.set_style("whitegrid", {'axes.grid' : False})<br/>fig = sns.relplot(data = UMAP_df_concat, x = "UMAP 1", y = "UMAP 2", hue=labelsAgglo, kind="scatter", palette=['red','blue','green'], height=5, aspect=1.5)</span><span id="6666" class="ny ml iq oy b gy ph pe l pf pg"># Add labels to original dataframe: <br/>data['cluster_group'] = labelsAgglo</span><span id="6c1a" class="ny ml iq oy b gy ph pe l pf pg"># Figure export<br/>fig.savefig('Figures/UMAP_clusters.png', dpi = 300)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/b8af7f225076905e9337d937a025d94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7kLT4EFROnnRZPY2BarTw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">跨数据集类别记录的时序细胞外数据的UMAP图。颜色对应于使用K-means聚类确定的聚类，并使用剪影和肘方法进行验证。图片作者。</p></figure><p id="e198" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从这个图中，我们可以看到(在很大程度上)K-means聚类有一个合理的工作！值得注意的是，UMAP图上的聚类之间有一些重叠，但看到这是一个使用神经尖峰数据的真实数据集，这是可以预期的。现在，让我们进一步调查这些？</p><h2 id="6c9c" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">每个集群的类别和波形细分</h2><p id="3620" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated"><em class="mc">如前所述，从特定数据集类记录的波形聚类更相似，而其他的则不相似。</em>为了更好地理解这些差异，让我们来看看每个聚类的平均波形以及组成它们的数据集类别:</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="3ad4" class="ny ml iq oy b gy pd pe l pf pg">#Cluster group colour<br/>cluster_colors=['red','blue','green']</span><span id="a9cd" class="ny ml iq oy b gy ph pe l pf pg">#Breakdown of each waveform cluster type by organoid class<br/>for cluster_group in range(0,3):<br/>    df_group = data[data['cluster_group'] == cluster_group] #isolating each cluster group<br/>    breakdown_perclust = ((df_group.organoid.value_counts())/(df_group.shape[0]))*100<br/>    <br/>    # Getting idx for each cluster for color matching<br/>    breakdown_perclust_df = breakdown_perclust.to_frame()<br/>    breakdown_perclust_df['index'] = breakdown_perclust_df.index<br/>    breakdown_perclust_df['color'] = breakdown_perclust_df['index'].apply(lambda orgID: mycolors[orgID])<br/>    breakdown_perclust_df = breakdown_perclust_df.rename(columns = {'organoid' : 'class_breakdown_percentage'})<br/>    <br/>    # Computing mean waveforms for each cluster<br/>    df_group_new = df_group.iloc[:,:-3].to_numpy() #dropped last columns<br/>    data_mean_group_percluster = np.mean(df_group_new, axis=0)</span><span id="a241" class="ny ml iq oy b gy ph pe l pf pg"># Sampling frequency for plot generation<br/>    sampling_freq = np.linspace(0, 5, 150) #recording length is 5ms per 150 samples<br/>    <br/>    # Piechart plotting<br/>    fig, ax = plt.subplots(1,2, figsize=(16,4), gridspec_kw={'width_ratios': [2.5, 1]})<br/>    ax[0].pie(breakdown_perclust_df['class_breakdown_percentage'], colors = breakdown_perclust_df['color'], labels = breakdown_perclust_df['index'],\<br/>              autopct='%1.0f%%', pctdistance=0.5, labeldistance=1.1)<br/>    title_pie = ("Cluster number: " + str(cluster_group))<br/>    ax[0].title.set_text(title_pie)<br/>    <br/>    # Draw inner circle<br/>    centre_circle = plt.Circle((0,0),0.70,fc='white')<br/>    <br/>    # Equal aspect ratio ensures that pie is drawn as a circle<br/>    ax[0].add_patch(centre_circle)  <br/>    ax[0].axis('equal')  <br/>    <br/>    # Mean waveform plotting<br/>    ax[1].plot(sampling_freq,data_mean_group_percluster, linewidth=3, color = cluster_colors[cluster_group])<br/>    title_waveform = ("Mean waveform for cluster number: " + str(cluster_group))<br/>    ax[1].title.set_text(title_waveform)<br/>    ax[1].set_ylim([-1.3, 1.3])<br/>    ax[1].grid()<br/>    ax[1].axis('off')<br/>    plt.rcParams["font.family"] = "Arial"<br/>    <br/>    # Scale bar<br/>    scalebar = AnchoredSizeBar(ax[1].transData, 1, "1 ms", 'lower right', frameon=False, size_vertical=0.02, pad=0.1)<br/>    ax[1].add_artist(scalebar)<br/>    <br/>    # N-values<br/>    n_value = ("N-value: " + str(df_group.shape[0]))<br/>    plt.figtext(0.36, -0.05, n_value, ha="center", fontsize=12, bbox={"facecolor":"orange", "alpha":0.2, "pad":5})<br/>    <br/>    # Export figures<br/>    fig_name = ("Figures/Piechart_waveform_clust" + str(cluster_group) + ".png")<br/>    plt.savefig(fig_name, dpi = 300, bbox_inches="tight")<br/>    plt.tight_layout()<br/>    plt.show()</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/7ccddccc1c8eebdce7bfc9b127c00687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5acBmO6j1-BdAKdc5tZ1A.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">饼图(左)突出显示了每个数据集类的波形百分比及其对应的平均波形(右)。图片作者。</p></figure><p id="9f95" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">乍一看，可以看出不同数据集聚类之间的平均波形存在相当大的差异。此外，来自不同数据集类别的波形对聚类的贡献百分比似乎也不同。</p><blockquote class="oo op oq"><p id="695f" class="kx ky mc kz b la lb jr lc ld le ju lf or lh li lj os ll lm ln ot lp lq lr ls ij bi translated">现在，我们的随机森林分类器有了一个明确的分类问题！我们可以验证集群之间的这些差异。</p></blockquote><h2 id="2bbe" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">将数据分成单独的训练集和测试集</h2><p id="0374" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">为了有效地训练随机森林算法，我们将把数据分成训练集和测试集。我们预测的期望结果<code class="fe ov ow ox oy b">y</code>是聚类类型，而我们的细胞外尖峰时间序列数据是<code class="fe ov ow ox oy b">X</code>。</p><p id="8798" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">测试和训练的原因将其设置为<strong class="kz ir">不允许训练数据集中有足够的数据供模型学习输入到输出的有效映射</strong>。测试集中也没有足够的数据来有效地评估模型性能。这可以使用以下代码来完成:</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="d756" class="ny ml iq oy b gy pd pe l pf pg"># Split data into training and testing sets<br/>X = data.iloc[:,:-3] #drop last 3 columns<br/>y = data['cluster_group']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</span><span id="f8db" class="ny ml iq oy b gy ph pe l pf pg"># check the shape of X_train and X_test<br/>print(f'Number of entries for X_train is {X_train.shape[0]}, and the number of entries for X_test is {X_test.shape[0]}')</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pk"><img src="../Images/d89d0c13dd760930715d80278b7e7d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opMceRsVGxAqv8nCmOJ72g.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">打印输出。图片作者。</p></figure><h2 id="42f0" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">随机森林分类</h2><p id="9d0b" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">现在，让我们训练我们的随机森林分类器！我们将在许多决策树(迭代)上这样做，并测试它们的预测准确性。这里，<strong class="kz ir"> y_test </strong>是真实类标签，<strong class="kz ir"> y_pred </strong>是测试集中的预测类标签:</p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="a6fe" class="ny ml iq oy b gy pd pe l pf pg"># Instantiate the Random Forest classifier <br/>number_int = [1,2,3,4,5,10,20,50,70,100,200,400,500]<br/>accuracy = []<br/>for i in number_int: <br/>    rfc = RandomForestClassifier(n_estimators=i, random_state=41)</span><span id="9b3c" class="ny ml iq oy b gy ph pe l pf pg"># Fit the model<br/>    rfc.fit(X_train, y_train)</span><span id="222e" class="ny ml iq oy b gy ph pe l pf pg"># Predict the test set results<br/>    y_pred = rfc.predict(X_test)<br/>    <br/>    # Append to list<br/>    accuracy.append('{0:0.4f}'.format(accuracy_score(y_test, y_pred)))</span><span id="1684" class="ny ml iq oy b gy ph pe l pf pg"># Check accuracy score <br/>    print(f'Model accuracy score with {i} decision-trees is {format(accuracy_score(y_test, y_pred))}')</span><span id="f985" class="ny ml iq oy b gy ph pe l pf pg"># Figure plotting<br/>fig, ax = plt.subplots(figsize=(7,4))<br/>ax.plot(number_int, accuracy)<br/>ax.title.set_text('Number of interations vs accuracy')<br/>ax.set(xlabel='Number of iterations', ylabel='Accuracy')<br/>plt.savefig('Figures/Iterations_accuracy.png', dpi = 300, bbox_inches="tight")</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/8f3b4c72f046d1e8d3d708bd8394688a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fSmEnD3Q6O8vlOqbZwtmOg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">随机森林分类器的“精确度”和“迭代次数”之间的关系。图片作者。</p></figure><p id="2081" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">尽管决策树的数量更多，但是在达到稳定状态之前，模型的准确度分数确实在1到100之间增加。对于2个决策树，它是0.93877，而对于500个决策树，它是0.9632。因此，随着模型中决策树数量的增加，预期精度也会增加。</p><h2 id="c0b7" class="ny ml iq bd mm nz oa dn mq ob oc dp mu lg od oe mw lk of og my lo oh oi na oj bi translated">混淆矩阵</h2><p id="8e77" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated">最后，让我们用分类报告构建一个混淆矩阵来总结我们的随机森林分类算法的性能，并评估准确性、精确度和召回指标。</p><p id="d572" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">混淆矩阵将通过总结正确和不正确的预测提供我们的模型性能和产生的错误类型的指示。这些可以分为以下四类:<strong class="kz ir">(1)</strong><strong class="kz ir">(TP)【2】真阳性(TN)</strong><strong class="kz ir">(3)</strong><strong class="kz ir">假阳性(FP) </strong>和<strong class="kz ir"> (4)假阴性(FN)。</strong> <a class="nh ni ep" href="https://medium.com/u/2489258d6569?source=post_page-----8e523f3639e1--------------------------------" rel="noopener" target="_blank"> Giulio Laurenti博士</a>在这里更详细地解释了所述读数<a class="ae kw" href="https://medium.com/swlh/confusion-matrix-and-classification-report-88105288d48f#:~:text=The%20confusion%20matrix%20is%20an,predictions%20of%20the%20classification%20model.&amp;text=The%20rows%20of%20the%20matrix,columns%20represent%20the%20predicted%20classes." rel="noopener">。</a></p><pre class="kh ki kj kk gt oz oy pa pb aw pc bi"><span id="1740" class="ny ml iq oy b gy pd pe l pf pg"># Confusion matrix<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(classification_report(y_test, y_pred))</span><span id="843d" class="ny ml iq oy b gy ph pe l pf pg"># Figure plotting<br/>cmap = sns.color_palette("Greys", as_cmap=True) # color map<br/>fig, ax = plt.subplots(figsize=(8, 8))<br/>cm_percent = ((cm/cm.sum(axis=0))*100)<br/>cm_percent = np.nan_to_num(cm_percent, copy=True, nan=0.0, posinf=None, neginf=None)<br/>sns.heatmap(cm_percent, cmap = cmap, annot=True, fmt=".2f", linewidth=1, cbar_kws={"label": "Cluster group classified (%)", "shrink": .8}, annot_kws={"size": 16}) #selecting for percentage only column <br/>ax.set_title('Confusion matrix')<br/>plt.show()</span><span id="f37d" class="ny ml iq oy b gy ph pe l pf pg"># Figure export<br/>fig.savefig("Figures/Confusion_matrix.png", dpi=300, bbox_inches='tight') </span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/5cdabded2d22c7aa2db0b927b251af1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8IR_9qyN3cT7_2WIizoXw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">输出的混淆矩阵分类报告(截图)。图片作者。</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pi"><img src="../Images/f1e75ff895ef4cb2ef5e7879fde2a4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z96MCCxiK8mrbkU9YSsl7w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">0-2群组的混淆矩阵。不同的阴影突出显示分类为TP、TN、FN或FP的预测数相对于每个分类的总可能分类数(%)。图片作者。</p></figure><h1 id="640f" class="mk ml iq bd mm mn pl mp mq mr pm mt mu jw pn jx mw jz po ka my kc pp kd na nb bi translated">结论</h1><p id="542e" class="pw-post-body-paragraph kx ky iq kz b la nc jr lc ld nd ju lf lg ne li lj lk nf lm ln lo ng lq lr ls ij bi translated"><em class="mc">我们做到了！我们现在已经建立了一个随机森林算法来准确和精确地分类神经尖峰数据的子集。</em></p><p id="60f4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我们不仅介绍了一个涉及神经细胞外数据的真实例子，还介绍了决策树和随机森林分类器背后的理论，以及在我们的工作流程中实现的多维缩减和K-means聚类。</p><p id="4771" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">虽然我们在这里讨论了使用随机森林解决分类问题，但重要的是要认识到有许多方法可以解决这些问题。因此，在未来的文章中，我们将通过覆盖使用深度学习算法和逻辑回归的分类来进一步讨论这一点。</p><p id="0063" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你可以通过<a class="ae kw" href="https://www.linkedin.com/in/michael-zabolocki/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我，通过<a class="ae kw" href="https://twitter.com/mzabolocki1" rel="noopener ugc nofollow" target="_blank"> Twitter </a>关注我，或者在<a class="ae kw" href="https://github.com/mzabolocki" rel="noopener ugc nofollow" target="_blank"> GitHub </a>和<a class="ae kw" href="https://www.kaggle.com/michaelzabolocki" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上加入我。</p></div></div>    
</body>
</html>