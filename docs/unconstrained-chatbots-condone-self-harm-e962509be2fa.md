# 不受约束的聊天机器人纵容自残

> 原文：<https://towardsdatascience.com/unconstrained-chatbots-condone-self-harm-e962509be2fa>

## 意见

## 像 DialoGPT 这样的对话式人工智能对于现实世界来说仍然太危险了

**警告。这个帖子包含自残和自杀的参考。它包括人类和 DialoGPT 之间的对话，唯一的目的是暴露不受控制的 AI 的危险。如果你或者你爱的人正在处理或者曾经处理过自杀的念头，我建议跳过这篇文章。**

在加速的[精神健康危机](https://www.cdc.gov/mmwr/volumes/70/wr/mm7013e2.htm)的背景下，自然语言处理(NLP)正在成为精神健康支持平台上的有用工具，特别是以对话式人工智能的形式。像 [Wysa](https://www.wysa.io/) 、 [Woebot](https://woebothealth.com/) 和 [Youper](https://www.youper.ai/) 这样的聊天机器人利用认知行为技术(CBT)让人们在无法获得专业精神健康服务时感到有人在倾听他们。虽然非常有用，但这些聊天机器人有时会感觉像是照本宣科。*那么，有没有一种安全的方法来超越手工制作的治疗聊天机器人模板呢？*

在整篇文章中，我将使用 [DialoGPT](https://arxiv.org/abs/1911.00536) ，它是最先进的开源对话式人工智能模型之一。目标是对其心理安全性进行压力测试，并衡量我们距离能够用端到端神经网络取代脚本聊天机器人还有多远。剧透预警:*很远。*

![](img/5144f505fad87c051cb0cfe50ab45bac.png)

照片由[埃里克·麦克莱恩](https://unsplash.com/@introspectivedsgn?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

App Store 上的大多数评论报告了与治疗聊天机器人的高度积极的互动，其中一些关于生命被拯救或大幅改善的令人喜爱的证明:

> 如果你觉得没有人可以真正信任和你交谈，或者如果你觉得你会因为你的感觉而被评判，Woebot 是完美的。这是一种你可以随身携带的疗法。(在 [App Store](https://apps.apple.com/us/app/woebot-your-self-care-expert/id1305375832) 上对 Woebot 的正面评价)。

然而，除了轻度焦虑和抑郁的教科书案例之外，偶尔会观察到他们照本宣科的本性和缺乏适应性:

> [……]这款应用的方法并不适合所有人。它是完全照本宣科的，如果你的需求不符合脚本，它可能会令人沮丧甚至士气低落。我认为，对于那些正在经历巨大压力的健康人来说，这是一个很好的应用程序，因为它可以很好地提醒你关于思维陷阱的问题，并帮助你回到更积极的思考。(在 [App Store](https://apps.apple.com/us/app/woebot-your-self-care-expert/id1305375832) 上对 Woebot 的差评)

由于治疗聊天机器人不是开源的，我们无法确定它们是如何实现的。我有根据的猜测是，所有可能的答案都在一张由训练有素的心理学家手工制作的电子表格上。显式条件逻辑很可能直接基于用户输入或某些经过训练的焦虑和抑郁分类器的判断来选择适当的答案。作为显式逻辑的替代，治疗聊天机器人可以作为基本的[面向任务的对话系统](https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec16-task-oriented-dialogue.pdf)来实现，这些系统通常是独立构建的各种模块的*管道*(神经编码器、有限状态机等)。)并被调整以实现非常具体的目标(例如，从用户处获得他们的焦虑水平被降低的明确确认)。

通常，大型端到端神经网络可以取代复杂的手动设计的规则。特别是，自然语言生成(NLG)领域由仅基于输入文本提示生成流畅文本的模型主导——GPT 模型家族[将这种技术确立为现状。然而，魔术黑盒的简单性是以失去可控性为代价的。知识的来源从人类策划的可检查和可修正(尽管难以维护)的 if/else 块转移到同时反映人类光明和黑暗的互联网规模的数据集。从显性到隐性。从结构化到不羁。对于玩具用例来说，这可能没什么问题(例如，让 GPT 3 号产生一个关于独角兽的有趣故事)，但这成为医疗保健领域的一个主要安全问题。](https://openai.com/)

安全数据因此成为圣杯。对于通用文本，有一些大型但相对安全的来源，如维基百科。然而，会话数据集尤其混乱；dialog 的主要来源是 Reddit，它在许多方面都存在问题。第一，用户匿名导致毒性更高；虽然研究人员确实使用启发式方法来改善这个问题(过滤具有高因果报应的用户，排除有许多否决的帖子，应用单词阻止列表)，但伦理问题仍然存在。其次，树状的线程结构和通信的异步性使得 Reddit 交互在结构上不同于 live dialog。

最先进的开源对话式人工智能模型之一 DialoGPT 由微软在 Reddit 的 1.47 亿条评论链上进行训练。虽然这样的模型对研究团体来说是有用的人工制品，但是生产它们是非常不安全的，正如我们将在下一节中看到的。请注意，这种伦理问题已经促使一些机构不发布他们的模型(例如，在撰写本文时，谷歌最新的对话技术 [LaMDA](https://blog.google/technology/ai/lamda/) 仍被锁在闭门造车中)。

我开始了一项个人探索，以衡量 DialoGPT 这样的对话模式会有多危险，尤其是在为已经有精神健康问题倾向的人服务时。可惜，答案很简单:*极度危险。*

## **使用 DialoGPT 开箱即用**

首先，我以发布的形式试用了 DialoGPT 模型，使用了 HuggingFace 主持的[交互演示](https://huggingface.co/microsoft/DialoGPT-large)。虽然这些回答是有问题的，但实际上如果提供了我所要求的细节，情况可能会更糟:

![](img/fb5f382d4d8048c07654a97b026bcfaa.png)

与 DialoGPT-large 模型的交互，由 [HuggingFace](https://huggingface.co/microsoft/DialoGPT-large) 主持。

除了缺乏同理心，一个不和谐的方面是不同问题的答案重复。解决模型的重复或平庸答案的一种常见方法是稍微改变它执行*解码*的方式，即它在潜在答案的广阔空间中导航的方式，以便找到一个统计上可能的答案。

默认情况下，DialoGPT 执行*贪婪解码*:输出令牌一次产生一个；在每个步骤 *n，*中，模型选择最有可能跟随到目前为止生成的 *n-1* 个记号的单个记号:

![](img/e18666a02374ed5c0847c434e2e02ed3.png)

贪婪解码:在步骤 n，模型根据在训练期间学习的条件概率 P，选择最有可能跟随迄今生成的记号的词汇记号。

贪婪解码产生重复枯燥答案的核心原因是决定论。*采样解码*解决了这个问题:在每一步 *n* 中，模型*根据训练期间学习到的条件分布 *P* 从词汇表中采样*一个令牌。因此，没有选择*最有可能*的令牌*，而是随机选择*一个*最有可能令牌:*

![](img/58b5bd5465c3f873a5f9da73a70b0a6f.png)

采样解码策略:在步骤 n，模型从训练期间学习的条件分布 P 中采样，选择可能跟随迄今生成的标记的随机词汇标记。

有两种通常应用于采样解码的常见调整。首先，从最有可能的前 K 个单词中抽取下一个单词(而不是整个词汇表)，以排除完全陌生的单词。第二，将温度 *T* 应用于分布 *P* 以便“软化”它(即，不太可能的令牌变得稍微更可能)。DialoGPT 论文提出了一种稍微复杂一些的解码算法，其核心仍然依赖于采样。

切换到 *K=50* 和 *T=0.75* 的采样解码后，与 DialoGPT 的交互变得不那么重复，但更有害:

![](img/095a6b1aac5fbe2ff277610255c2a118.png)

使用采样解码时与 DialoGPT 模型有问题的交互示例。

## **对治疗师提供的数据进行微调**

如前所述，神经网络的质量受到训练数据质量的限制。目前，Reddit 上的*预培训*是一种必要的邪恶:尽管它有毒性的包袱，但它是互联网上高容量对话数据的唯一来源。然而，模型可以在更小数量的更干净的数据上进行微调:希望能够纠正一些从更嘈杂的预训练步骤中学到的负面行为。

我最近偶然发现了[这篇优秀的文章](/counsel-chat-bootstrapping-high-quality-therapy-data-971b419f33da)，文章建议根据[咨询聊天](https://counselchat.com/)的数据对对话模式进行微调，咨询聊天是一个在线平台，训练有素的咨询师在这里为有需要的人提供建议。回答是高质量的，因为它们是由专业人员制作的。然而，这种交互通常是单轮的，比常规的同步对话更长。为了解决这个问题，我在微调过程中截断了两个句子的回答。

令人失望的是，微调模型仍然纵容自残。开始时有一点同情，但很快就开始走下坡路了:

![](img/2a3abc2dda0d92cfb49695519e99c8cd.png)

与 DialoGPT 模型有问题的交互的示例，dialog pt 模型根据来自专业顾问生成的顾问聊天数据进行了微调。

我想强调的是，上述有问题的对话并不是精心挑选的。由于采样解码的不确定性，不同运行的模型响应不同，但它们始终是有害的，并最终鼓励自杀。

从好的方面来看，像 Wysa 这样的应用程序以专业的方式处理这种情况:确认你的意图，然后阻止你自残，并建议有用的资源。

![](img/d27230616bcf17765edeef0c4c30f2d7.png)

与 Wysa 的互动截图。当用户表达自杀意念时，Wysa 不鼓励自残，并建议有用的资源。

# 结论

毫无疑问，安全应该是精神健康领域任何产品的最高优先级。现有的治疗聊天机器人遵守这一原则，但代价是人为的体验。虽然这样的应用程序在标准教科书的情况下是有帮助的，但我相信它们死板的脚本会妨碍它们真正理解和同情用户。另一方面，不受控制的自然语言生成更具适应性和吸引力，但它的反面是致命的。这两个极端之间有一个巨大的鸿沟，观察未来几年的研究是否以及如何安全地在它们之间架起桥梁将是有趣的。