<html>
<head>
<title>Stable diffusion using Hugging Face — Variations of Stable Diffusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用拥抱面的稳定扩散——稳定扩散的变化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265#2022-11-15">https://towardsdatascience.com/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265#2022-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e6a9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用<a class="ae kf" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">拥抱面部扩散器库</a>的负面提示和图像到图像稳定扩散管道的介绍</h2></div><p id="7604" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这是我上一篇文章的续篇——<a class="ae kf" href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8" rel="noopener">使用拥抱脸的稳定扩散|作者:Aayush agr awal | 2022 年 11 月|迈向数据科学(medium.com)</a>。</p><p id="2262" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在前一篇文章中，我回顾了稳定扩散的所有关键组成部分，以及如何让<code class="fe lc ld le lf b">prompt to image</code>管道工作。在这篇文章中，我将展示如何编辑<code class="fe lc ld le lf b">prompt to image</code>函数来为我们的稳定扩散管道添加额外的功能，即<code class="fe lc ld le lf b">Negative prompting</code>和<code class="fe lc ld le lf b">Image to Image</code>管道。希望这将提供足够的动力来玩这个函数并进行您的研究。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/26c1ceb006b5d462fec26fa14dd52628.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*-MV6xVxqQeRdMxZl.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 1:使用 prompt - <br/>“在两个不同方向分叉的道路”的稳定扩散生成的图像</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3ced" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">1.变体 1:否定提示</h1><h2 id="dfc9" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">1.1 什么是负面提示？</h2><p id="e8da" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">否定提示是我们可以添加到模型中的附加功能，用来告诉稳定扩散模型我们不希望在生成的图像中看到什么。这个特性很受欢迎，可以从原始生成的图像中删除用户不想看到的任何内容。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/a5f0cf0d035bcbb82819e9bf25b20e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-ZdRTLNHCqwSarUA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 2:否定提示示例</p></figure><h2 id="bd20" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">1.2 通过代码理解负面提示</h2><p id="f59f" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">让我们从导入所需的库和助手函数开始。所有这些都已经在之前的<a class="ae kf" href="https://medium.com/towards-data-science/stable-diffusion-using-hugging-face-501d8dbdd8" rel="noopener">帖子</a>中使用和解释过了。</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="0e15" class="nr ma iq lf b be ns nt l nu nv">import torch, logging<br/><br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/><br/>## Imaging  library<br/>from PIL import Image<br/>from torchvision import transforms as tfms<br/><br/><br/>## Basic libraries<br/>from fastdownload import FastDownload<br/>import numpy as np<br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from IPython.display import display<br/>import shutil<br/>import os<br/><br/>## For video display<br/>from IPython.display import HTML<br/>from base64 import b64encode<br/><br/><br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler<br/><br/>## Initiating tokenizer and encoder.<br/>tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")<br/><br/>## Initiating the VAE<br/>vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/><br/>## Initializing a scheduler and Setting number of sampling steps<br/>scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)<br/>scheduler.set_timesteps(50)<br/><br/>## Initializing the U-Net model<br/>unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")<br/><br/>## Helper functions<br/>def load_image(p):<br/>    '''<br/>    Function to load images from a defined path<br/>    '''<br/>    return Image.open(p).convert('RGB').resize((512,512))<br/><br/>def pil_to_latents(image):<br/>    '''<br/>    Function to convert image to latents<br/>    '''<br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0<br/>    init_image = init_image.to(device="cuda", dtype=torch.float16) <br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215<br/>    return init_latent_dist<br/><br/>def latents_to_pil(latents):<br/>    '''<br/>    Function to convert latents to images<br/>    '''<br/>    latents = (1 / 0.18215) * latents<br/>    with torch.no_grad():<br/>        image = vae.decode(latents).sample<br/>    image = (image / 2 + 0.5).clamp(0, 1)<br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()<br/>    images = (image * 255).round().astype("uint8")<br/>    pil_images = [Image.fromarray(image) for image in images]<br/>    return pil_images<br/><br/>def text_enc(prompts, maxlen=None):<br/>    '''<br/>    A function to take a texual promt and convert it into embeddings<br/>    '''<br/>    if maxlen is None: maxlen = tokenizer.model_max_length<br/>    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") <br/>    return text_encoder(inp.input_ids.to("cuda"))[0].half()</span></pre><p id="3829" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在我们要通过传递一个额外的函数<code class="fe lc ld le lf b">neg_prompts</code>来改变<code class="fe lc ld le lf b">prompt_2_img</code>函数。否定提示的工作方式是在采样时使用用户指定的文本代替空字符串进行无条件嵌入(<code class="fe lc ld le lf b">uncond</code>)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nw"><img src="../Images/8caf58ac1ca0908894f9afc5a3052e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*okDcT2Tk3rfKUMKF.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 3:负提示代码变化</p></figure><p id="83d3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">所以，让我们做这个改变并更新我们的<code class="fe lc ld le lf b">prompt_2_img</code>函数。</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="66a2" class="nr ma iq lf b be ns nt l nu nv">def prompt_2_img(prompts, neg_prompts=None, g=7.5, seed=100, steps=70, dim=512, save_int=False):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    <br/>    # Defining batch size<br/>    bs = len(prompts) <br/>    <br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding negative prompt condition<br/>    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])<br/>    # Adding an unconditional prompt , helps in the generation process<br/>    else: uncond =  text_enc(neg_prompts, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Initiating random noise<br/>    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Adding noise to the latents <br/>    latents = latents.to("cuda").half() * scheduler.init_noise_sigma<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(scheduler.timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/>        <br/>        # Conditioning  the latents<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>        <br/>        # Saving intermediate images<br/>        if save_int: <br/>            if not os.path.exists(f'./steps'): os.mkdir(f'./steps')<br/>            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')<br/>            <br/>    # Returning the latent representation to output an image of 3x512x512<br/>    return latents_to_pil(latents)</span></pre><p id="a929" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">让我们看看这个函数是否如预期的那样工作。</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="3c67" class="nr ma iq lf b be ns nt l nu nv">## Image without neg prompt<br/>images = [None, None]<br/>images[0] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=[""],steps=50, save_int=False)[0]<br/>images[1] = prompt_2_img(prompts = ["A dog wearing a white hat"], neg_prompts=["White hat"],steps=50, save_int=False)[0]<br/>    <br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate(images): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"A dog wearing a white hat")<br/>    else: axs[c].set_title(f"Neg prompt - white hat")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nx"><img src="../Images/e73a2278b4876ba5048031be3769a31f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wnoQYUZJiCwhtUTg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 4:负面提示的可视化。左侧 SD 生成提示“戴白帽子的狗”,右侧相同标题生成否定提示“白帽子”</p></figure><p id="719b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">正如我们所见，这是一个非常方便的功能，可以根据您的喜好微调图像。你也可以用它来生成一张非常逼真的脸，就像这个<a class="ae kf" href="https://www.reddit.com/r/StableDiffusion/comments/yqnh2c/closeup_photo_of_a_face_just_txt2img_and_lsdr/" rel="noopener ugc nofollow" target="_blank"> Reddit 帖子</a>一样。让我们试试-</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="d4bf" class="nr ma iq lf b be ns nt l nu nv">prompt = ['Close-up photography of the face of a 30 years old man with brown eyes, (by Alyssa Monks:1.1), by Joseph Lorusso, by Lilia Alvarado, beautiful lighting, sharp focus, 8k, high res, (pores:0.1), (sweaty:0.8), Masterpiece, Nikon Z9, Award - winning photograph']<br/>neg_prompt = ['lowres, signs, memes, labels, text, food, text, error, mutant, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, made by children, caricature, ugly, boring, sketch, lacklustre, repetitive, cropped, (long neck), facebook, youtube, body horror, out of frame, mutilated, tiled, frame, border, porcelain skin, doll like, doll']<br/>images = prompt_2_img(prompts = prompt, neg_prompts=neg_prompt, steps=50, save_int=False)<br/>images[0]</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/7088aff5ea5ea90d218f2242ec3733c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*VoO-iaVaD8rcqwad.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 5:使用负面提示生成的图像。</p></figure><p id="93f2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">相当整洁！我希望这能给你一些想法，关于如何开始你自己的稳定扩散的变化。现在让我们看看稳定扩散的另一种变化。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="2e97" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">2.变体 2:图像到图像管道</h1><h2 id="c872" class="mr ma iq bd mb ms mt dn mf mu mv dp mj kp mw mx ml kt my mz mn kx na nb mp nc bi translated">2.1 什么是图像到图像管道？</h2><p id="ad61" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">如上所述，<code class="fe lc ld le lf b">prompt_2_img</code>函数开始从随机高斯噪声中生成图像，但是如果我们输入一个初始种子图像来引导扩散过程会怎么样呢？这正是图像到图像管道的工作方式。我们可以使用初始种子图像将它与一些噪声混合(这可以由一个<code class="fe lc ld le lf b">strength</code>参数来引导)，然后运行扩散循环，而不是纯粹依赖于输出图像的文本调节。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ny"><img src="../Images/09b08c3cc5482fb3c937cf121e064efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U34RCDrNa6qKj2rp.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 6:图像到图像管道示例。</p></figure><h1 id="2d1d" class="lz ma iq bd mb mc nz me mf mg oa mi mj jw ob jx ml jz oc ka mn kc od kd mp mq bi translated">2.2 通过代码理解图像到图像的提示</h1><p id="8fbd" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">现在我们要改变上面定义的<code class="fe lc ld le lf b">prompt_2_img</code>函数。我们将为我们的<code class="fe lc ld le lf b">prompt_2_img_i2i</code>函数- <br/> 1 引入另外两个参数。<code class="fe lc ld le lf b">init_img</code>:它将是包含种子图像<br/> 2 的<code class="fe lc ld le lf b">Image</code>对象。<code class="fe lc ld le lf b">strength</code>:该参数取 0 到 1 之间的值。值越高，最终图像看起来就越不像种子图像。</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="fa38" class="nr ma iq lf b be ns nt l nu nv">def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512, save_int=False):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompt) <br/><br/>    # Adding negative prompt condition<br/>    if not neg_prompts: uncond =  text_enc([""] * bs, text.shape[1])<br/>    # Adding an unconditional prompt , helps in the generation process<br/>    else: uncond =  text_enc(neg_prompts, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/>        <br/>        # Conditioning  the latents<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>        <br/>        # Saving intermediate images<br/>        if save_int: <br/>            if not os.path.exists(f'./steps'):<br/>                os.mkdir(f'./steps')<br/>            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')<br/>            <br/>    # Returning the latent representation to output an image of 3x512x512<br/>    return latents_to_pil(latents)</span></pre><p id="8f2c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">你会注意到，我们没有使用随机噪声，而是使用<code class="fe lc ld le lf b">strength</code>参数来计算添加多少噪声以及运行扩散循环的步骤数。通过将强度(默认值= 0.8)乘以第 10(50-50 * 0.8)步的步数(默认值= 50)并运行剩余 40(50*0.8)步的扩散循环来计算噪波量。让我们加载一个初始图像，并通过<code class="fe lc ld le lf b">prompt_2_img_i2i</code>函数传递它。</p><pre class="lh li lj lk gt nn lf no bn np nq bi"><span id="b4e2" class="nr ma iq lf b be ns nt l nu nv">p = FastDownload().download('https://s3.amazonaws.com/moonup/production/uploads/1664665907257-noauth.png')<br/>image = Image.open(p).convert('RGB').resize((512,512))<br/>prompt = ["Wolf howling at the moon, photorealistic 4K"]<br/>images = prompt_2_img_i2i(prompts = prompt, init_img = image)<br/><br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate([image, images[0]]): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"Initial image")<br/>    else: axs[c].set_title(f"Image 2 Image output")</span></pre><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nx"><img src="../Images/41bf4b29ac8c6fff3986b6a8719ad84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jIRRyIbGiUh8KEdw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图 7:图像到图像管道的可视化。左边是 img2img 管道中传递的初始图像，右边是 img2img 管道的输出。</p></figure><p id="12ea" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以看到，我们的<code class="fe lc ld le lf b">prompt_2_img_i2i</code>函数从提供的初始草图中创建了一个漂亮的史诗图像。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="53b9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">3 结论</h1><p id="f8f6" class="pw-post-body-paragraph kg kh iq ki b kj nd jr kl km ne ju ko kp nf kr ks kt ng kv kw kx nh kz la lb ij bi translated">我希望这能很好地概述如何调整<code class="fe lc ld le lf b">prompt_2_img</code>函数，为你的稳定扩散循环增加额外的能力。对这个低级函数的理解对于尝试你自己的想法来改善稳定扩散或实现我可能在下一篇文章中涉及的新论文是有用的。</p><p id="95a4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我希望你喜欢阅读它，并随时使用我的代码，并尝试生成您的图像。此外，如果对代码或博客帖子有任何反馈，请随时联系 aayushmnit@gmail.com 的 LinkedIn 或发电子邮件给我。你也可以在我的网站上阅读博客的早期发布<a class="ae kf" href="https://aayushmnit.com/blog.html" rel="noopener ugc nofollow" target="_blank">Aayush agr awal-博客(aayushmnit.com)</a>。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4d9d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">4 参考文献</h1><ul class=""><li id="6ec6" class="oe of iq ki b kj nd km ne kp og kt oh kx oi lb oj ok ol om bi translated"><a class="ae kf" href="https://www.fast.ai/posts/part2-2022-preview.html" rel="noopener ugc nofollow" target="_blank"> Fast.ai 课程——《从深度学习基础到稳定扩散》前两节</a></li><li id="baa8" class="oe of iq ki b kj on km oo kp op kt oq kx or lb oj ok ol om bi translated"><a class="ae kf" href="https://huggingface.co/blog/stable_diffusion" rel="noopener ugc nofollow" target="_blank">🧨扩散器的稳定扩散</a></li><li id="cbe0" class="oe of iq ki b kj on km oo kp op kt oq kx or lb oj ok ol om bi translated"><a class="ae kf" href="https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">进入稳定扩散的世界</a></li></ul></div></div>    
</body>
</html>