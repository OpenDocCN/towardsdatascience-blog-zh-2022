<html>
<head>
<title>Risk Implications of Excessive Multiple Local Minima during Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整期间过多多个局部最小值的风险含义</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/risk-implications-of-excessive-multiple-local-minima-during-hyperparameter-tuning-fe6f517e57e8#2022-10-12">https://towardsdatascience.com/risk-implications-of-excessive-multiple-local-minima-during-hyperparameter-tuning-fe6f517e57e8#2022-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="944d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们的认识论局限和知识幻觉</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/300caba1932d136fa3fd519637c6baeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*t-K6xmXF-ZkUJJmwza44fA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="2534" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">超参数调整期间过多的多个局部最小值是模型性能对超参数值的微小变化高度敏感的症状，如上图所示。</p><p id="d31e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我在本机XGBoost API的超参数对reg_alpha和reg_lambda上执行网格搜索调优时，我遇到了这种非常崎岖不平的性能状况，有多次起伏。这只是我对房地产价格预测回归分析的一个组成部分。然而，在很大程度上，我全神贯注于以下问题。</p><ul class=""><li id="6b7f" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">那些多个局部极小值是否意味着任何风险？T3】</li><li id="e4e6" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><strong class="kt ir"> <em class="lw">如果有的话，那些是什么？</em> </strong></li></ul><p id="08ba" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这篇阅读中，我想分享我基于我的项目案例研究对这些问题的观察和思考。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="4e7c" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">A.超参数调整概述</h1><p id="815b" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">这里，我只想对我在原始项目中执行的超参数调优过程做一个非常简要的概述。</p><p id="97ca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lw"> a)超参数</em> </strong></p><p id="35ab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我使用了Python的原生XGBoost API<a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="510a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank">在原生XGBoost API </a>中内置的许多超参数中，由于给定的计算资源可用性有限，本分析主要关注以下8个超参数。</p><ul class=""><li id="b30f" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> eta </strong> </a></li><li id="1019" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> max_depth </strong> </a></li><li id="e585" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">子样</strong> </a></li><li id="df8e" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">col sample _ bytree</strong></a></li><li id="1b90" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">最小_子_体重</strong> </a></li><li id="bde3" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"/>伽马</a></li><li id="6794" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank">T5】阿尔法T7】</a></li><li id="b021" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated"><a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"/></a></li></ul><p id="7b9f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于这些超参数的定义，请参考原生<a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> XGBoost API </a>的文档。</p><p id="6516" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了这8个超参数，我在调优过程中使用<a class="ae ng" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=cv()#xgboost.train" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">early _ stopping _ round</strong></a>调优了迭代次数<a class="ae ng" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=cv()#xgboost.train" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">num _ boost _ round</strong></a>。</p><p id="8ea0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于XGBoost超参数的更多内容，这里有一个Jason Brownlee写的很好的总结:<a class="ae ng" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="lw">机器学习的梯度提升算法的温和介绍</em> </a> <em class="lw">。</em></p><p id="730b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lw"> b)调谐方法</em> </strong></p><p id="afce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">理想地，在所有这8个超参数上的单个联合调谐将准确地捕获这些超参数之间所有可能的相互作用。然而，联合调谐具有巨大的搜索网格数据点，因此在计算上将是昂贵的。</p><p id="f96d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了节约分析的计算成本，我从8个参数对中选择了4个:即</p><ul class=""><li id="fb13" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">(最大深度，eta)</li><li id="2286" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">(子样本，列样本_字节树)</li><li id="f127" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">(最小_孩子_体重，伽玛)，以及</li><li id="8cfb" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">(寄存器_alpha，寄存器_lambda)。</li></ul><p id="7626" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我对这4个超参数对中的每一个进行了成对调整。每次调整后，最佳调整结果用于替换超参数对的旧值。通过这种方式，每次调优都会逐步提高调优模型的性能。</p><p id="2b12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这总体上减少了搜索网格数据点的数量，并以网格搜索的公正性为代价节约了调整过程的计算成本。当然，这里的基本假设是，通过4个成对调节获得的结果不会在本质上偏离在所有8个超参数上的理想单个联合调节。</p><p id="0b5a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在调优方法方面，我想强调几个重要的脚注。</p><ul class=""><li id="3c8e" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">我在超参数调整的训练数据集上运行<a class="ae ng" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=cross%20validation#xgboost.cv" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> <em class="lw"> k重交叉验证</em> </strong> </a>。</li><li id="8d64" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">我使用网格搜索法进行参数调整。</li><li id="3130" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">我使用3D可视化和三角插值技术<a class="ae ng" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank">来填充搜索网格数据点之间的空隙，并渲染出一个平滑的表演场景表面。</a></li></ul><p id="d059" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lw"> c)第一次调谐结果</em> </strong></p><p id="8dcf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里，我们有4个成对超参数调整结果的3D可视化。每个图表显示了每个成对超参数调整的性能情况。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ad71e324b276543b39e5bb701d6c80cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*aDlQwhofqmWb-DzS8SrsTw.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/f3d01c0c134d8a3d5db6c553b8df475e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*ASGF0DxXwuORHxGMLOoLzA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/5ba78326cd9b502eef2aeb5e495687f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*VZu7CEdH7Zjb4ZYWjEXGYQ.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/0d83c6700ead74fb84b082fd60ccfe8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*Tx3rIfLJq99a2VvmOD0_ew.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="f861" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如您所见，最后一个性能场景展示了与其他三个性能场景截然不同的行为。它产生了多个局部最小值和多个凹陷和凸起，并对超参数对的值的微小变化表现出极大的敏感性。相反，前3种情况呈现了相对一致的性能景观。</p><p id="4cce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面，我想介绍超参数对的第一个搜索网格设置。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="839e" class="nm mk iq ni b gy nn no l np nq">for reg_alpha in [0, 1e-2, 0.1, 1, 2, 3, 4, 8, 10, 12, 14] </span><span id="f6be" class="nm mk iq ni b gy nr no l np nq">for reg_lambda in [0, 1e-2, 0.1, 1, 2, 3, 4, 8, 10, 12, 14] ]</span></pre><p id="7a15" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">数据点之间的间隔故意设置得不均匀，以观察搜索网格的粒度对性能前景的影响。对于这两个超参数，我在0和1之间设置了一个更精细的粒度间隔，在4之后的范围内设置了一个更粗糙的间隔。</p><p id="7df7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下表列出了搜索网格上的前10个最佳性能数据点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/3c6efcee53940d73d8f8cb552ccefa68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*_rOgQizjKjpPsLr4vTNOgg.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="d474" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该表确定了在搜索网格数据点(reg_alpha =0.1，reg_lambda=8.0)时的最佳性能；第二好的是(reg_alpha =3.0，reg_lambda=0.1)。</p><p id="d825" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了更深入地了解(reg_alpha和reg_lambda)的超参数对的不稳定性能状况，我沿着reg_alpha的前2个数据点值(即reg_alpha = 0.1和3.0)对性能状况进行了切片。</p><p id="09aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于每个切片，这里我沿着reg_alpha绘制了切片性能曲线，并在切片性能曲线上绘制了前10个最佳性能搜索网格数据点的表格。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/48b428115d6183294526881c4f48f213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ielpqG3vC-FzlpZ_ztIdSA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/998b95fb7b8a080f6075b8df129b730f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*luM-drsPQolMTxjPmGCKsg.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="0c14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">沿着上面的reg_alpha=0.1切片的性能曲线确认了在(reg_lambda = 0.01、1.00和8.00)的3个局部最小值。在0和2之间有明显的下降和上升。在某种程度上，reg_lambda = 0.01处的局部最小值和那些凹陷和凸起不会被捕获，如果我们没有在0和1之间的粒度局部搜索网格设置的话。</p><p id="593d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，让我们看看另一条沿reg_alpha =3.0切片的性能曲线及其前10位性能搜索网格数据点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nz"><img src="../Images/06eefc9d0412b568ca2a8d880f8ea1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEyoq8OZ8XUpvk6pGbIjDQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4f49df4db36ee7a9430ae845ab2aa762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*7Uy8uYX65jiMPm2GazuSPA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="c8de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">沿着(reg_alpha = 3.0)的性能状况的第二部分确定了在(reg_lambda = 0.01)和(reg_lambda = 1.0)之间形成的尖锐深渊。深渊形状的强烈凹凸不平(沿reg_lambda = 0.01、0.1和1.00)由局部搜索网格设置捕获，该设置比搜索网格的任何其他部分都精细得多。换句话说，如果我们在搜索网格中错过了(reg_lambda = 0.1)的单个数据点，我们将无法捕捉到(reg_lambda = 0.01)和(reg_lambda = 1.0)之间的局部最小值的存在。</p><p id="c38a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lw"> d)网格搜索的盲点与可视化</em> </strong></p><p id="62b3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这两条切片的性能曲线揭示了网格搜索的一个普遍缺陷:性能前景捕获的只是在所选搜索网格上预测的性能结果。称之为搜索网格性能；我们没有任何关于搜索网格之外的任何数据点的性能的信息。称之为脱离搜索网格的性能。</p><p id="593f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，可视化使情况变得更糟，通过平滑地插入搜索网格上的性能来推测那些不可见的搜索网格外的性能。在非搜索网格间隔内平滑的图像只是一个假象，但根本不是事实。这个人工制品可能会在我们不稳定的感知中造成一种知识幻觉。换句话说，我们可以通过盲目地用平滑的视觉假象来控制我们的视觉来欺骗自己。</p><p id="aaff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">搜索网格设置中更精细的粒度可能会揭示更粗糙的性能环境。也就是说，通过更精细的搜索网格设置，我们可能会发现更多的凹陷和凸起。它可能会发现许多其他隐藏的局部最小值的存在，这些最小值在当前的性能环境中是不可见的。</p><p id="7ddb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，地面实况全局最小值很可能出现在当前搜索网格之外，并且位于与由当前搜索网格设置识别的当前最小值相距很远的地方。</p><p id="5f77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，当前的性能状况可能不仅捕捉到了其粗糙的部分图像，还欺骗了我们的视觉。</p><p id="98d5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"><em class="lw">e)reg _ alpha和reg_lambda的第二轮调整。</em> </strong></p><p id="10b0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了测试假设的视图，我做了一个更细粒度的搜索网格设置，如下所示，并为特定的超参数对重新运行成对调优。</p><pre class="kg kh ki kj gt nh ni nj nk aw nl bi"><span id="fd78" class="nm mk iq ni b gy nn no l np nq">for reg_alpha in [0, 1e-2, 2e-2, 3e-2, 0.1, 0.5, 1, 1.5, 1.8, 1.9, 2, 2.1, 2.2, 2.5, 3, 3.5, 4]</span><span id="a4ba" class="nm mk iq ni b gy nr no l np nq">for reg_lambda in [0, 1e-2, 2e-2, 3e-2, 0.1, 0.5, 1, 1.5, 1.8, 1.9, 2, 2.1, 2.2, 2.5, 3, 4, 5, 6, 7, 7.5, 8, 8.5, 9, 9.5, 10, 10.5, 11, 11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5]</span></pre><p id="1fb4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是第二轮调整结果的三维可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/300caba1932d136fa3fd519637c6baeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*t-K6xmXF-ZkUJJmwza44fA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">用Michio Suginoo制作的Matplotlib的plot_trisurf进行三维可视化</p></figure><p id="58e0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如预期的那样，超参数对(reg_alpha和reg_lambda)的第二轮调整比第一轮调整呈现出更加崎岖的性能状况，有更多的下降和上升。</p><p id="0813" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是第二轮调优的前10个性能搜索网格数据点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b7506e2a34f7f5182aa26dc347f2e5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*etsrHmSzaIGB7PijEcBqVw.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="4588" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，这证实了我之前的担心是正确的。</p><ul class=""><li id="e346" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">首先，第一个搜索网格的粗略粒度低估了性能状况对超参数对值的微小变化的实际敏感度。</li><li id="f4a9" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">top 10 performance search-grid data point表显示，粒度更细的第二轮调整在与第一轮调整结果(reg_alpha = 0.1，reg_lambda = 8.0)完全不同的位置(reg_alpha = 0.5，reg_lambda = 13.5)发现了其最佳性能数据点。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="c6c7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> <em class="oc"> B .正在使用的多个本地最小值和方法的风险影响</em> </strong></h1><p id="3707" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">该分析使用网格搜索来进行超参数调整，并使用三角插值来渲染超参数值上的性能景观的3D可视化。像任何其他方法一样，这些方法在它们自己的设计中有其固有的局限性。以及reg_alpha和reg_lambda的超参数对的特定成对调谐的结果，展示了多个局部最小值，这是高灵敏度/易变性能的症状，暴露了那些固有设计限制的风险。在某种程度上，这种极度敏感的表现放大并暴露了使用这些方法的风险。</p><p id="c1a4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">以下是这些局限性的概述。</p><p id="363c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">局限性1)网格搜索的盲点:</strong></p><ul class=""><li id="fc7d" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">搜索网格被设置为所选离散超参数数据点的集合。称之为搜索网格数据点。网格搜索只计算选定搜索网格中那些离散超参数数据点的性能结果，或搜索网格性能。</li><li id="8a12" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">调优并没有告诉我们在相邻的两个搜索网格数据点之间发生了什么。我们对所选搜索网格之外或搜索网格之外的性能值(评估指标)一无所知。因此，调优结果最多只能让我们看到底层性能状况的一部分。</li></ul><p id="4cae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">限制2)虚幻的视觉化</strong></p><ul class=""><li id="83e6" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">3D可视化仅捕获搜索网格上的结果。</li><li id="a497" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">并且3D可视化平滑地内插那些离散的数据点，以创建估计的脱离搜索网格的评估度量值的人工制品。该人工制品产生了关于脱离搜索网格的评估度量值的知识幻觉。</li></ul><p id="f2de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果当前的搜索网格仅捕获其灵敏度的部分图像，则脱离搜索网格的地面实况性能可能比其在平滑的可视化上出现的更敏感/不稳定，并且更崎岖，有更多的下降和颠簸。换句话说，它的高灵敏度表明<strong class="kt ir"> <em class="lw">地面实况脱离网格性能</em> </strong>可能实质上偏离投影在3D可视化上的插值估计的假象。</p><p id="d81f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，业绩前景的高度敏感性可能会进一步带来风险，即地面实况全局最小值可能出现在搜索网格之外，并且与搜索网格上的全局最小值相距甚远。在这种意义上，当与仅具有单个局部最小值的3个其他情况相比时，多个局部最小值放大了遭受可视化的网格搜索和三角测量插值技术的那些固有限制的风险。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="eaf2" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> <em class="oc"> C .偏差- </em> </strong>方差<strong class="ak"> <em class="oc">模型选择中的取舍</em> </strong></h1><p id="16a7" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">现在，在这个阶段，考虑到这两轮调整的结果，我们想对模型选择做出一个试探性的决定来回答下面的问题。我们应该选择哪个调整模型:第一次调整结果还是第二次调整结果？</p><p id="bc2d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们是否可以说，与第一轮结果相比，第二轮调整提供了更好的解决方案，因为它在调整期间产生了更好的性能(k倍交叉验证)？</p><p id="0956" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">可惜没那么简单。到目前为止，我们只使用训练数据集进行k-fold交叉验证。</p><p id="fa90" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了回答这个问题，我们需要考虑<a class="ae ng" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">偏差-方差权衡</a>或欠拟合-过拟合权衡的角度。换句话说，我们需要在测试数据集 上运行调整后的模型，并比较它们的性能。</p><p id="b901" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的表格比较了交叉验证数据集和测试数据集之间的性能，分别为“调整(验证MAE)”和“测试性能(MAE)”三个阶段:预调整阶段、第一次成对调整后以及第二次成对调整后。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi od"><img src="../Images/61a8edbdcbf1925b068bcd0bed839920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1cjX1MJwnDOrB2smFU_nA.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="eece" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该表表明，最佳测试性能实际上是在第一轮调优时产生的，而不是在第二轮调优时产生的。这表明第二轮调整期间较好的交叉验证性能结果说明了<strong class="kt ir"> <em class="lw">方差</em> </strong>，或<strong class="kt ir"> <em class="lw">过拟合</em> </strong>。</p><p id="03f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了解决偏差-方差权衡问题，我们必须选择第一轮结果，拒绝第二轮结果用于模型选择。</p><p id="6a67" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于数据驱动的机器学习，模型不稳定性是由偏差-方差权衡或欠拟合-过拟合权衡引起的关键问题。当新类型的数据集出现在部署域中并进入优化模型时，在开发阶段优化的模型可能会表现出意外的行为。模型性能的稳定性需要在模型风险管理的整个生命周期中进行评估。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="0a65" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> <em class="oc"> D .讨论</em>T3】</strong></h1><p id="319f" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">尽管如此，第一轮调整结果是否是绝对意义上的最佳解决方案仍不确定。考虑到模型的异常敏感性，当前搜索网格的粒度可能不够精细，无法捕捉到真实的全局最小值。在第一轮和第二轮中使用的搜索网格之外，可能存在更好的解决方案(甚至对于测试数据集)。</p><p id="2c32" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们要不要设置一个新的搜索网格来测试这个疑问？</p><p id="b89b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这将是一个永无止境的故事。</p><p id="7fb7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最重要的是，当我们使用网格搜索参数调整时，我们永远无法知道绝对意义上的最佳解决方案。网格搜索最多只能捕捉到仅投射在搜索网格上的性能状况的一部分。我们没有任何关于搜索范围外性能的信息。</p><p id="9cf6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从这个意义上来说，我们并不了解整个表演领域的全貌。原则上，我们的知识仅限于那些离散的搜索网格上的数据点。更糟糕的是，三角插值可视化技术呈现了连续性能景观的假象，并在我们对整个性能景观的不确定感知中产生了错觉。总的来说，如果我们认为我们可以获得真实性能的全貌，也就是真实性能的全局最小值，那就太天真了。</p><p id="d0ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了前面反复提到的两个限制之外，所选择的调优方法还有另一个明显的限制，即成对调优。</p><p id="457f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">限制3)成对调谐的偏好</strong></p><ul class=""><li id="a7b1" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">所有超参数的单个联合调整将更好地捕捉所有超参数之间的实际性能交互。可能会得出完全不同的结论。从这个意义上说，成对调整捕获的全局最小值可能是不准确的，并且不同于真实的全局最小值。</li><li id="cdb4" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">只有4个成对调整，我们没有覆盖8个超参数的所有可能的组合对(28=8×7/2)。这些缺失组合之间的相互作用不能被4个成对调节捕获。</li></ul><p id="00dd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对本地XGBoost API的reg_alpha和reg_lambda进行成对调整，证明了模型性能对超参数对值的微小变化高度敏感，并揭示了网格搜索和插值3D可视化的认识论限制。</p><ul class=""><li id="591d" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">网格搜索只能捕获搜索网格上的模型性能，而不会给我们留下任何关于搜索网格外性能的信息。</li><li id="0782" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">更糟糕的是，三角形插值通过平滑性能景观，蒙蔽了我们的视觉，并产生了关于实际模型性能的错觉。</li></ul><p id="af52" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简而言之，这两种限制都存在于这些方法的设计中。因此，它们是体系结构固有的，无法解决。这设置了一个认识论的限制:我们永远不会达到我们的地面真实性能景观的完美知识，因此，地面真实的全球最小值。这将引出一个相关的问题:</p><ul class=""><li id="78cf" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">如果在认识论上不可能找到基本事实的全局最小值，有可能建立它的强大代理吗？什么构成了全局最小值的可靠代理？</li></ul><p id="ab6a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，在超参数对上使用更细粒度的搜索网格设置进行的第二轮调整揭示了一个复杂的偏差-方差权衡问题。虽然提高了训练成绩，却恶化了考试成绩。不幸的是，更细粒度的网格搜索只能使模型过度适应训练数据集，而不能优化一般的模型性能。这提出了另一个相关的问题:</p><ul class=""><li id="9052" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">从调优模型的总体优化角度来看，基本事实全局最小值有什么关系吗？</li></ul><p id="4925" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一点是:在分析中，认识论的限制似乎对3个其他性能场景没有问题，即对于以下3对:(max_depth，eta)，(subsample，colsample_bytree)，(min_child_weight，gamma)，因为它们相对一致。很明显，只有在reg_alpha和reg_lambda的高灵敏度性能环境下，我们才注意到认识论的限制。</p><p id="f859" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这告诉我们一些事情:只要性能状况没有表现出对这些超参数值的微小变化的高度敏感性，三角形插值与网格搜索的结合似乎已经成为一种稳健的解决方案。</p><p id="d16c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这转化为另一个视角:除了这些方法的一般认识论限制，还有一个过度敏感的性能景观的更严重的问题。需要解决过多局部最小值的风险影响。一个相关的问题是:</p><ul class=""><li id="9cee" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">多个局部最小值是否表明部署域中调优模型的模型不稳定性，其中新类型的数据集可能被引入到模型中？</li></ul><p id="5073" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你怎么想呢?</p><p id="c986" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这些问题似乎都没有不言自明的答案。</p><h1 id="70ed" class="mj mk iq bd ml mm oe mo mp mq of ms mt jw og jx mv jz oh ka mx kc oi kd mz na bi translated">D.结论</h1><p id="a5e2" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">现在，我将关闭这个职位，同时保持这些问题的答案。展望未来，我希望继续关注过多多重最小值的风险影响，并探索那些尚未解答的问题。此外，我欢迎读者联系我，分享他们对此事的看法。</p><p id="600e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">总的来说，这篇文章揭示了在超参数调优过程中，我们在寻找性能全局最小值的基本事实答案时的认知局限性。</p><p id="3f12" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">网格搜索只确定了搜索网格上的模型性能，搜索网格外的性能未知。让情况变得更糟的是，3D可视化渲染了脱离搜索网格的数据点上的三角插值的假象，并创造了一种知识的幻觉。</p><p id="80b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，从偏差-方差权衡的角度来看，我们是否需要获得关于基本事实全局最小值的知识并不是不证自明的。</p><p id="a3bb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">感谢你阅读这篇文章。</p><p id="3de6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你对原分析的整个过程感兴趣，可以访问<a class="ae ng" href="https://github.com/deeporigami/Portfolio/blob/main/XGBoost/Version%201%20XGBoost%20Hyperparameter%20Tuning%20and%20Performance%20Landscape.ipynb" rel="noopener ugc nofollow" target="_blank"> my Github repository </a>查看代码实现。作为脚注，为了生成所有这些图像，我使用了python和Matplotlib。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="b8fa" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">确认</h1><p id="abe6" class="pw-post-body-paragraph kr ks iq kt b ku nb jr kw kx nc ju kz la nd lc ld le ne lg lh li nf lk ll lm ij bi translated">我要感谢TDS的编辑团队，特别是<a class="ae ng" href="https://www.linkedin.com/in/ACoAAArEpWwBHvFKeyTA3JKk6gApzplqwM2hjKY" rel="noopener ugc nofollow" target="_blank">凯瑟琳·普雷里</a>，感谢他们在编辑阶段提供的宝贵意见。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="6bf7" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">参考</h1><ul class=""><li id="4041" class="ln lo iq kt b ku nb kx nc la oj le ok li ol lm ls lt lu lv bi translated">布朗利(Brownlee)，J. <a class="ae ng" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="lw">机器学习的梯度推进算法温和介绍</em> </a> <em class="lw">。</em> (2016)。检索自机器学习掌握:<a class="ae ng" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://Machine Learning Mastery . com/gentle-introduction-gradient-boosting-algorithm-Machine-Learning/</a></li><li id="05ce" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">维基百科。<a class="ae ng" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank"> <em class="lw">偏差-方差权衡</em> </a> <em class="lw">。</em> (2022)。检索自维基百科:<a class="ae ng" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Bias % E2 % 80% 93 variance _ trade off #:~:text = In % 20 statistics % 20 and % 20 machine % 20 learning，Bias % 20 In % 20估计% 20参数。</a></li><li id="fa0f" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">维基百科。<a class="ae ng" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank"> <em class="lw">【交叉验证(统计)</em> </a> <em class="lw">。</em> (2022)。从维基百科检索:<a class="ae ng" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Cross-validation _ % 28 statistics % 29 # k-fold _ Cross-validation</a></li><li id="b96c" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">维基百科。<a class="ae ng" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank"> <em class="lw">曲面三角剖分</em> </a> <em class="lw">。</em> (2022)。从维基百科检索:<a class="ae ng" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Surface_triangulation</a></li><li id="c38f" class="ln lo iq kt b ku lx kx ly la lz le ma li mb lm ls lt lu lv bi translated">xgboost开发人员。<a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost参数</a>。(2021).检索自<a class="ae ng" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthe docs . io/en/stable/parameter . html # parameters-for-tree-booster</a></li></ul></div></div>    
</body>
</html>