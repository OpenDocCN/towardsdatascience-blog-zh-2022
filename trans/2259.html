<html>
<head>
<title>Intriguing Properties of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的有趣特性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intriguing-properties-of-neural-networks-57fef936adc7#2022-05-18">https://towardsdatascience.com/intriguing-properties-of-neural-networks-57fef936adc7#2022-05-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="86ad" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络是如何工作的？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/36219d242634221b7a03151514e636d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6YGMlExzFveE_lg5"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">莫里茨·金德勒在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="11b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">十多年来，神经网络一直是机器学习算法中的佼佼者。他们在不同领域的众多任务中创造了奇迹。然而，关于它们实际上是如何工作的，一直存在疑问。</p><p id="0a9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络可以拟合任何训练数据。它试图通过调整分配给每个输入的权重和随后的中间步骤(隐藏层)来学习输入和输出之间的映射。这种调整是通过监督学习的反向传播自动完成的。因此，很难解释这些权重是如何对映射做出贡献的，并且它还表现出一些反直觉的特性。<br/> T <strong class="ky ir"> wo </strong>这些特性在我们将在本文中讨论的'<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">神经网络的有趣特性</a>中有所涉及。虽然它可以追溯到2014年，但它仍然是相关的，并丰富了w.r.t .对神经网络工作的理解。</p><p id="55b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是所述属性:</p><ol class=""><li id="3945" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">与经验结果相反，是<strong class="ky ir">而不是</strong>单个神经元(单元)而是<strong class="ky ir">整个向量</strong>对输入的语义信息进行编码。</li><li id="fd3d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将<strong class="ky ir">不易察觉的</strong>、<strong class="ky ir">非随机</strong> <strong class="ky ir">噪声</strong>添加到输入中，甚至可以使最先进的深度学习模型对它们进行错误分类，证明该模型对小扰动敏感，并对它们的稳定性提出质疑。</li></ol><p id="4c32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这听起来可能有点模糊和混乱，但是我们将在接下来的章节中更详细地探讨这些。</p><h1 id="11bf" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">内容</h1><ol class=""><li id="6d0d" class="ls lt iq ky b kz my lc mz lf na lj nb ln nc lr lx ly lz ma bi translated"><strong class="ky ir">符号</strong></li><li id="9d59" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">术语</strong></li><li id="6d61" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">神经元v/s神经元</strong></li><li id="3fec" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">神经网络的盲点<br/> →实验结果<br/> →不稳定性分析</strong></li><li id="7648" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">结论</strong></li><li id="add4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">参考文献</strong></li></ol><h1 id="9b7d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">符号</strong></h1><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="847e" class="ni mh iq ne b gy nj nk l nl nm"><strong class="ne ir">x ∈ ℝᵐ </strong>is an input image (a vector of real numbers of length m).</span><span id="6e6b" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir"><em class="no">I</em></strong><em class="no">      </em>is the test-set of images.</span><span id="07ef" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">𝜙(x)   </strong>denotes<strong class="ne ir"> </strong>activations for an entire layer for the given image.</span><span id="1e0e" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">𝜙₁(x), 𝜙₂(x), 𝜙₃(x), ... </strong>denotes activations of individual neurons.</span><span id="b61a" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">φ(x)</strong>   denotes the output of an entire network of <strong class="ne ir">K</strong> layers.</span><span id="11de" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">φ<em class="no">₁</em>(x; <em class="no">W₁</em>), ..., φ<em class="no">ₖ</em>(x; <em class="no">Wₖ</em>)</strong> denotes the outputs of individual layers.</span><span id="3f46" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">φ(x) = φₖ ( φₖ₋₁ (... φ</strong><strong class="ne ir">₁ </strong><strong class="ne ir">( x; <em class="no">W₁ </em>); <em class="no">W₂ </em>) ...; <em class="no">Wₖ </em>)</strong></span><span id="bead" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">ρ(x)   </strong>ReLU, i.e., <strong class="ne ir">max(0, x)</strong></span><span id="f8a8" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">σ₁     </strong>Largest singular value of a matrix</span></pre><h1 id="ef3d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">术语</h1><p id="4b00" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">编号对应于正文中引用这些术语的上标。</p><ol class=""><li id="f046" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">激活:</strong>一个神经元(或一整层)的最终输出，包括激活函数。</li><li id="5231" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">单位:</strong>单个神经元。</li><li id="4fb6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">基:</strong>向量空间中的一组向量<code class="fe ns nt nu ne b"><strong class="ky ir">B</strong></code>，使得该空间中的每一个向量都可以唯一地表示为<code class="fe ns nt nu ne b"><strong class="ky ir">B</strong></code>中向量的一个<strong class="ky ir">线性组合</strong>。例如，在三维欧几里德向量空间中，向量<code class="fe ns nt nu ne b"><strong class="ky ir">{<em class="no">î, ĵ, k̂</em>}</strong></code> <strong class="ky ir"> </strong>是基向量，因为该空间中的所有向量都可以表示为线性组合:<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">xî + yĵ + zk̂</em></strong></code>。这些也是欧氏向量空间的<strong class="ky ir">标准基</strong>。标准基是一组向量，它们的分量都是<code class="fe ns nt nu ne b">zero</code>，除了一个等于<code class="fe ns nt nu ne b">1</code>，即<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">ĵ = (0, 1, 0)</em></strong></code>。</li><li id="3b63" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">方向:</strong>在这个上下文中是指随机向量<code class="fe ns nt nu ne b"><strong class="ky ir">𝑣</strong></code> <strong class="ky ir"> </strong>会指向某个方向，与这个向量的最大点积会给我们指向<code class="fe ns nt nu ne b"><strong class="ky ir">𝑣</strong></code>方向最多的向量。现在，因为据说共享相似特征的图像具有相似的点积值，所以他们说“方向对该特征是敏感的。”</li><li id="72d4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://qr.ae/pvA9ml" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">非局部泛化</strong> </a> <strong class="ky ir"> : </strong>神经网络的深层隐藏层本质上是一个对空间进行建模的函数，使得相似的输入更接近于这个潜在空间(例如，具有相似特征的同一类的图像)。神经网络将很好地概括以正确预测附近的输入类别。这叫做<strong class="ky ir">局部泛化</strong>。然而，确实存在一些输入，这些输入可能属于相同的类别，但是由于某些特征使得它们与其他输入不太相似而远离空间中的其他输入(例如，狗的图像可能在附近，但是狗的侧视图可能在空间中远离，尽管它仍然是狗)。神经网络被认为也能够推广这些远距离的输入。这叫做<strong class="ky ir">非局部泛化</strong>。</li><li id="7b3c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">框式约束优化:</strong>这是一个唯一的约束是变量上下限(可以是<code class="fe ns nt nu ne b"><strong class="ky ir">±∞</strong></code>)的问题。<a class="ae kv" href="https://math.stackexchange.com/questions/3965637/what-is-box-constrained-mathematical-optimization-problem" rel="noopener ugc nofollow" target="_blank">看到这里</a>。</li></ol><h1 id="2fba" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">神经元v/s神经元</h1><p id="b13a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">过去的研究表明，特定的神经元对特定的特征做出反应。他们表明，单个神经元的激活对输入具有某种语义意义。为此，他们在图像中寻找相似之处，使同一神经元的激活值最大化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/7bc821b99c7551b68acb78661dc04e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2e4qPvPCbw_W9La-wqQa1w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">证明过去研究有效性的MNIST实验— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><p id="34d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果你想检查某个<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">iᵗʰ</em></strong></code>神经元的激活<code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="no">ᵢ</em>(x)</strong></code>，</p><p id="c658" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">首先，</strong>您可以在测试图像<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">I</em></strong></code> <strong class="ky ir"> <em class="no"> </em> </strong>集合中找到<code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="no">ᵢ</em>(x)</strong></code> <strong class="ky ir"> </strong>值最大的图像，使用:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/455ab8165d6fa70bc983d17a5b0dc5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*-4HDzdXLb71MiHYZ6-8P4w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用单个神经元的激活来寻找语义相关的图像— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">赛格迪等人</a></p></figure><p id="18fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本上，您可以找到所有<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">I</em></strong></code> <strong class="ky ir"> <em class="no"> </em> </strong>的<code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="no">ᵢ</em>(x)</strong></code> <strong class="ky ir"> </strong>的最大值，然后挑选值接近此最大值的图像。</p><blockquote class="nx ny nz"><p id="4d17" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="iq">注意</em> </strong> <em class="iq">作者使用了</em><code class="fe ns nt nu ne b"><strong class="ky ir"><em class="iq">⟨</em>𝜙<em class="iq">(x), </em>𝑒<em class="iq">ᵢ⟩</em></strong></code><strong class="ky ir"><em class="iq"/></strong><em class="iq">的表达方式为激活</em> <code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="iq">ᵢ</em>(x)</strong></code> <em class="iq">。这里，</em> <code class="fe ns nt nu ne b"><strong class="ky ir">𝑒<em class="iq">ᵢ</em></strong></code> <em class="iq">是带有</em> <code class="fe ns nt nu ne b"><strong class="ky ir">iᵗʰ</strong></code> <em class="iq">值</em> <code class="fe ns nt nu ne b"><strong class="ky ir"><em class="iq">1</em></strong></code>的一键编码(基)向量。<em class="iq">和尖括号</em><code class="fe ns nt nu ne b"><strong class="ky ir"><em class="iq">⟨⟩</em></strong></code><strong class="ky ir"><em class="iq"/></strong><em class="iq">分别代表</em><code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="iq">(x)</em></strong></code><strong class="ky ir"><em class="iq"/></strong><em class="iq">和</em> <code class="fe ns nt nu ne b"><strong class="ky ir">𝑒<em class="iq">ᵢ</em></strong></code> <em class="iq">的点积。所以我们最终做的是:</em></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/acbf65fbb55f768e7503d44826be9858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FgSLk0icr_VQ0b_xtfsCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe"> <em class="of"> ⟨𝜙(x)，𝑒ᵢ⟩ = 𝜙ᵢ(x) </em> </strong></p></figure><p id="1dc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">然后，</strong>你可以目视检查这些图像，找出它们之间的相似之处(比如图像有一条对角线的直线)。</p><p id="a0df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，基于他们的观察，作者表明，只有特定神经元对特定特征做出反应的结论是不正确的。</p><blockquote class="og"><p id="7231" class="oh oi iq bd oj ok ol om on oo op lr dk translated">这就对神经网络将坐标间的变异因素分开的概念提出了质疑。</p><p id="5487" class="oh oi iq bd oj ok ol om on oo op lr dk translated">— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">塞格迪等人</a></p></blockquote><p id="eaf5" class="pw-post-body-paragraph kw kx iq ky b kz oq jr lb lc or ju le lf os lh li lj ot ll lm ln ou lp lq lr ij bi translated">他们认为整个激活空间包含了大量的语义信息。为了证明这一点，他们采用了一个随机向量<code class="fe ns nt nu ne b"><strong class="ky ir"> 𝑣</strong></code> <strong class="ky ir"> </strong>，而不是一键(标准基)向量<code class="fe ns nt nu ne b"><strong class="ky ir">𝑒</strong></code>。即</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ff6194d7af5f797524c87d002b2aa2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Zz9TG1KiD_vpsBVXYNLkbg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用整个层的激活找到语义相关的图像— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><p id="b013" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/96eb118d3896cb4bf4674a155bb4faba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lcKeL5p2md0RBHyPpKyxEA.png"/></div></div></figure><p id="1f45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以在这里，不是只有一个<code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="no">ᵢ</em>(x)</strong></code>，他们考虑所有<code class="fe ns nt nu ne b"><strong class="ky ir">𝜙<em class="no">ᵢ</em>(x)</strong></code> <strong class="ky ir"> </strong>的一个<strong class="ky ir">线性组合</strong>，并挑选点积值接近最大值的图像。</p><p id="9b49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，他们证明了给予一层中所有神经元高值的图像在语义特征上也有相似之处。这表明似乎没有对特定特征作出反应的优选神经元或神经元集，但该层作为一个整体编码了大量语义信息:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/b9a02c0570067478bd9c32a84d6f22e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9stESqa_bJ8jI9HVvxX_KQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MNIST实验证明层作为一个整体编码语义信息— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><p id="fe4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你仍然困惑，想想如果只有特定的神经元对特定的特征做出反应,<code class="fe ns nt nu ne b">Dropout</code>会如何工作。</p><blockquote class="nx ny nz"><p id="f80b" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">另外，<strong class="ky ir">请注意</strong>经验主义的说法并没有错。有些神经元确实对某些特征比其他的更敏感。但是这些结果并不比用整个层获得的结果更好，因此，可以说神经网络没有在不同的神经元之间分配不同的特征。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/9d4e7655143928fc26ec5ddee68c0ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-Oqnp3T7WffmzXwF8_c7w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ImageNet上的实验。<strong class="bd oe">左</strong>:演示编码信息的单个神经元；<strong class="bd oe">右图</strong>:展示整个图层编码信息— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><h1 id="9631" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">神经网络中的盲点</h1><p id="36b8" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">研究表明，神经网络的输入和输出层之间的隐藏层的深度堆栈能够在输入空间上实现<strong class="ky ir">非局部推广</strong> ⁵。因此，这意味着<strong class="ky ir">本地</strong> <strong class="ky ir"> generalization⁵ </strong>应该按预期工作。简而言之，对于足够小的半径<code class="fe ns nt nu ne b"><strong class="ky ir">ϵ &gt; 0</strong></code>，在给定输入<code class="fe ns nt nu ne b"><strong class="ky ir">x</strong></code>附近，满足<code class="fe ns nt nu ne b"><strong class="ky ir">‖r‖ &lt; ϵ</strong></code>的输入<code class="fe ns nt nu ne b"><strong class="ky ir">x + r</strong></code>将预测正确的类别。</p><p id="fb33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该论文的作者认为这一假设不成立，他们通过生成具有难以察觉的小扰动的对立样本来证明这一点，这些小扰动使模型误预测类别。</p><p id="4db0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑将输入图像<code class="fe ns nt nu ne b"><strong class="ky ir">x ∈ ℝᵐ</strong></code>映射到一组标签<code class="fe ns nt nu ne b"><strong class="ky ir">{1 . . . <em class="no">k</em>}</strong></code> <strong class="ky ir"> — </strong> <code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em> : ℝᵐ → {1 . . . <em class="no">k</em>}</strong></code>的神经网络分类器<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em></strong></code> <strong class="ky ir"> <em class="no"> </em> </strong>。<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em></strong></code>具有由<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">lossf</em> : ℝᵐ × {1 . . . <em class="no">k</em>} → ℝ⁺</strong></code>表示的损失函数。</p><p id="6943" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们想要找到给出不正确预测<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">l</em></strong></code>的最接近<code class="fe ns nt nu ne b"><strong class="ky ir">x</strong></code>的图像。<em class="no"> </em>设此像为<code class="fe ns nt nu ne b"><strong class="ky ir">x + r</strong></code>。所以为了让<code class="fe ns nt nu ne b"><strong class="ky ir">x + r</strong></code> <strong class="ky ir"> </strong>最接近<code class="fe ns nt nu ne b"><strong class="ky ir">x</strong></code>，我们需要<code class="fe ns nt nu ne b"><strong class="ky ir">r</strong></code>越小越好。因此，我们可以构造一个<strong class="ky ir">框约束优化</strong> ⁶问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/7e8a72896658bbc1343eb7ed3a37db4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*lw643ANFMKep4pWMC2fQng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对立范例生成问题的形式定义— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等</a></p></figure><p id="ba06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有可能<code class="fe ns nt nu ne b"><strong class="ky ir">r</strong></code>不是唯一的。因此我们将由失真函数<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">D(x, l)</em></strong></code>选择的任意<code class="fe ns nt nu ne b"><strong class="ky ir">r</strong></code> <strong class="ky ir"> </strong>表示为<code class="fe ns nt nu ne b"><strong class="ky ir">x + r</strong></code>。<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">D(x, l) </em></strong></code>的精确计算是一个难题。因此，作者使用框约束L-BFGS近似这一点。</p><blockquote class="nx ny nz"><p id="8980" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">具体地说，我们通过执行线搜索来找到下面问题的极小值<code class="fe ns nt nu ne b"><strong class="ky ir">r</strong></code>满足<code class="fe ns nt nu ne b"><strong class="ky ir">f(x + r) = l</strong></code>的最小值<code class="fe ns nt nu ne b"><strong class="ky ir">c &gt; 0</strong></code>，从而找到<code class="fe ns nt nu ne b"><strong class="ky ir">D(x, l)</strong></code>的近似值。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/857e24e4dd2b2152dbba1a63f34743fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nW18lFoFQYlR0uDkVEJHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">近似“硬”优化问题— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><p id="da98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，这篇论文没有详细说明他们的具体方法。因此，很难围绕这一点获得更多的直觉。</p><p id="89d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">L-BFGS和线搜索超出了本文的范围；我将在以后讨论这些内容。这里有几个解释<a class="ae kv" rel="noopener" target="_blank" href="/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504">【BFGS】</a>和<a class="ae kv" href="https://www.youtube.com/watch?v=8tqaXIM6kEE" rel="noopener ugc nofollow" target="_blank">线搜索</a>的好资源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/1095375691b9f0aebf0ae81853744873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wl_nRZ4gbGh1Ca2Fd_VIsQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为AlexNet生成的对抗性示例。<strong class="bd oe">左</strong>:原图；<strong class="bd oe">中心</strong>:添加小扰动(生成图像与原始图像之间的差异，放大)；<strong class="bd oe">右</strong>:被错误分类的反例——<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">Szegedy等人</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/6299972f5881449bf737697e052ce1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qxFVCoBnBqnKAsi8SHl0-w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为QuocNet生成的对抗性示例。<strong class="bd oe">左</strong>:原图；<strong class="bd oe">中心</strong>:被错误分类的反例；<strong class="bd oe">右</strong>:添加了小扰动(生成图像和原始图像之间的差异，放大)——<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">塞格迪等人</a></p></figure><h2 id="8e09" class="ni mh iq bd mi pd pe dn mm pf pg dp mq lf ph pi ms lj pj pk mu ln pl pm mw pn bi translated">实验结果</h2><p id="4980" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">有人可能会认为这些错误分类是由于在一组固定的输入上过度拟合模型造成的。为了证明事实并非如此，作者们提出了一些概括实验:</p><ul class=""><li id="e4ed" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated"><strong class="ky ir">跨模型推广</strong>:与用于生成对立样本的模型相比，使用不同的超参数(如层数、正则化或初始权重)从零开始训练的模型会对图像产生明显的错误分类。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/42fe4944f04ecc3055f3cd4fa7189450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skzHUGXH6xujOO_mqpUjjA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">第一列表示通过添加最小扰动生成对立示例的模型，使得测试集中的所有示例都被错误分类。最后一列显示了实现误预测所需的原始训练集的平均失真。这些例子被提供给不同的模型(跨列提及)，相应的诱发误差列于上表中— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><ul class=""><li id="3a4a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated"><strong class="ky ir">交叉训练集泛化</strong>:在与用于生成对抗样本的模型完全不同的训练集上训练的模型，会对图像产生明显的错误分类。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/e3a4d713441c43146f42d096d2d72c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CMWz1R7HoEHqxjVwDtIHVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于交叉训练集泛化的原始模型训练细节。MNIST样本分为两部分——P1和P2。最后一列指示测试集中所需的平均最小失真，使得该集中的所有图像被相应的模型错误分类— <a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pr"><img src="../Images/cd1f1a8d3a4346f17320e11e6f5f2ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5cr2lrWblmK3KVQYlW8OQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">针对一个模型生成的对立示例的错误率，并将其提供给在不同训练集上训练的另一个模型(见上表)——<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank">Szegedy等人</a></p></figure><h2 id="1ae4" class="ni mh iq bd mi pd pe dn mm pf pg dp mq lf ph pi ms lj pj pk mu ln pl pm mw pn bi translated">不稳定性分析</h2><p id="5ddd" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">从上一节中，我们了解到，尽管已知神经网络可以在局部和非局部点上进行推广，但如果添加正确，它仍然对微小的扰动高度敏感。这就提出了一个关于它们稳定性的问题，也就是说，如果输入的微小变化改变了模型的预测，以至于它变得不正确，那么模型可能根本就不稳定。</p><p id="63a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了研究这一点，我们将使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Lipschitz连续性的概念</strong> </a> <strong class="ky ir"> : </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/8c027a823b79c9a850223e1d5407991a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/1*hJv0MTFmms9HgtmINyy0-w.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对于Lipschitz连续函数，我们可以画一个双锥(白色区域),这样如果我们沿着函数移动它的原点，函数就不会进入白色区域— <a class="ae kv" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="dc2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Lipschitz连续函数的变化速度有限。对于Lipschitz连续函数，连接该函数上任意两点的直线的斜率大于一个实数。这个实数称为李普希茨常数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/711691d9160b653444513010f733f4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*WIJZ7PVRr5lnzKL3OtDl_Q.png"/></div></figure><p id="5caf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这确实非常直观。例如，斜率<strong class="ky ir"> <em class="no"> </em> </strong>给出了<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f(x)</em></strong></code> <strong class="ky ir"> <em class="no"> </em> </strong>随<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">x</em></strong></code>变化多少的度量。因此，我们可以说，如果斜率以一个小值为界，那么<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">x</em></strong></code>中的“小扰动”不会对<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f(x)</em></strong></code>产生大的影响。</p><p id="420b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在考虑两个函数<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em></strong></code>和<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">g</em></strong></code>，设<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">h</em></strong></code>为<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em></strong></code>和<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">g</em></strong></code>的组合，即<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">h = f ∘ g</em></strong></code>。为此，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pu"><img src="../Images/caed1f2f2027c647b18b88b90cedc1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MrcA7Uwlp_dyjdk_QEr5w.png"/></div></div></figure><p id="357a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，对于<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">n</em></strong></code>功能的组合，即<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">h = f₁ ∘ f₂ ∘ ... ∘ fₙ</em></strong></code>，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pv"><img src="../Images/1b7546376a31b0c5f5382d87f2b16179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cC_ej70Ai_e3PcMckRV9vQ.png"/></div></div></figure><p id="9d26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，在我们的例子中，对于一个<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">kᵗʰ</em></strong></code>层，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/ca1234c17a219cb04daeac27f6a00a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R81vmV-hyeWdqZYfHmCTww.png"/></div></div></figure><p id="687d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">Lₖ</em></strong></code>是<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">kᵗʰ</em></strong></code>层的Lipschitz常数。所以对于整个<code class="fe ns nt nu ne b"><strong class="ky ir">K</strong></code>层的网络，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi px"><img src="../Images/100d58a1f5c2da39e72b56d488286dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQ9YKFAJMQsWQyFT9CTdjA.png"/></div></div></figure><p id="4b4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们想找到我们的神经网络的李普希茨常数。给定上面的表达式，可以有把握地说，我们可以为每一层找到<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">L</em></strong></code>，然后取乘积。但是首先，我们需要弄清楚如何找到所有层的<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">L</em></strong></code>:</p><ul class=""><li id="9fad" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated"><strong class="ky ir">为ReLU ( </strong> <code class="fe ns nt nu ne b"><strong class="ky ir">ρ</strong></code> <strong class="ky ir"> ) </strong>:</li></ul><p id="c6c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要找到<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">Lᵣₑₗᵤ</em></strong></code>，我们需要以下内容:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="deea" class="ni mh iq ne b gy nj nk l nl nm">Given any differentiable function <strong class="ne ir"><em class="no">f</em> : ℝ → ℝ</strong> and points <strong class="ne ir">x, y ∈ ℝ</strong>, if the maximum derivative of <strong class="ne ir"><em class="no">f</em></strong>, <strong class="ne ir"><em class="no">dₘₐₓ</em> = max{∣<em class="no">f'</em>(z)∣ | z ∈ ℝ}</strong> exists, then</span><span id="f9b8" class="ni mh iq ne b gy nn nk l nl nm"><strong class="ne ir">∣<em class="no">f</em>(x) - <em class="no">f</em>(y)∣ ≤ <em class="no">dₘₐₓ</em>∣x - y∣</strong></span><span id="07f9" class="ni mh iq ne b gy nn nk l nl nm">So, <strong class="ne ir"><em class="no">dₘₐₓ </em></strong>is the Lipschitz constant of <strong class="ne ir"><em class="no">f</em></strong>. For proof, refer <a class="ae kv" href="https://www.duo.uio.no/bitstream/handle/10852/69487/master_mathialo.pdf?sequence=1" rel="noopener ugc nofollow" target="_blank">this link</a> (Page 48).</span></pre><p id="8dd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，ReLU由<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em>(a) = max(0, a)</strong></code>定义，即它可以取2个值:<code class="fe ns nt nu ne b"><strong class="ky ir">{0, a}</strong></code>。对这个w.r.t. <code class="fe ns nt nu ne b"><strong class="ky ir">a</strong></code>求导我们得到— <code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f’</em>(a)</strong></code>可以取2个值:<code class="fe ns nt nu ne b"><strong class="ky ir">{0, 1}</strong></code>。所以<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">dₘₐₓ</em></strong></code>是<code class="fe ns nt nu ne b"><strong class="ky ir">1</strong></code>，由此，<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">Lᵣₑₗᵤ </em>= 1</strong></code>。因此，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi py"><img src="../Images/210988742d32ca198d4bfb4514e861fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*sh_4QlRnT7HC2fhbM9W84A.png"/></div></figure><ul class=""><li id="1547" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated">同样，我们可以证明对于<strong class="ky ir"> max-pooling层</strong> <code class="fe ns nt nu ne b"><strong class="ky ir">φ(x)</strong></code>，</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/94fdf313764a536af03ef6af93923ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*FnUh0IoDw60d78jKVtUbdQ.png"/></div></figure><ul class=""><li id="561e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated"><strong class="ky ir">为</strong> <strong class="ky ir">密集层</strong>:</li></ul><p id="3d5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">连续性由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/2ca63998d57aab655f4fa29f31fecab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDnuENMIqMILlsjmNpi5pg.png"/></div></div></figure><p id="66f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由上可知，李普希兹常数是稠密层<code class="fe ns nt nu ne b"><strong class="ky ir">‖<em class="no">Wₖ</em>‖</strong></code>的<strong class="ky ir">算子范数</strong>。可以证明这个范数是以<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">Wₖ</em></strong></code>的最大奇异值为界的。你可以在这里找到证明<a class="ae kv" href="https://www.duo.uio.no/bitstream/handle/10852/69487/master_mathialo.pdf?sequence=1" rel="noopener ugc nofollow" target="_blank">(第44页)</a>。</p><p id="a76f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你不明白，回想一下对于<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">h = f ∘ g</em></strong></code> <strong class="ky ir"> <em class="no">，</em> </strong>李普希兹常数的值是<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">f</em></strong></code>和<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">g</em></strong></code>的李普希兹常数的乘积。所以在这里，我们可以把<code class="fe ns nt nu ne b"><strong class="ky ir">ρ</strong></code>乘以它的<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">Lᵣₑₗᵤ</em></strong></code> <strong class="ky ir"> <em class="no"> </em> </strong>值，也就是<code class="fe ns nt nu ne b"><strong class="ky ir">1</strong></code>。</p><ul class=""><li id="ff94" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated">类似地，对于<strong class="ky ir">对比度归一化层</strong> <code class="fe ns nt nu ne b"><strong class="ky ir">φ(x)</strong></code>，</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/96fb9443e4c99eb09742366b92724ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*XpSrtIEshMk19qqAy6gfQw.png"/></div></figure><ul class=""><li id="81b2" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr po ly lz ma bi translated"><strong class="ky ir">对于卷积层:</strong></li></ul><blockquote class="nx ny nz"><p id="95d9" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">就我个人而言，我并不觉得这篇论文对此的解释特别直观。根据我的理解，他们已经使用了Toeplitz矩阵理论及其与傅立叶分析的联系来寻找卷积层的算子范数的上界。</p><p id="40f9" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated">然而，这只是一篇关于神经网络和不稳定性的介绍性文章，所以我们不会在卷积理论上纠缠太深。相反，我们将研究卷积层的不同解释，我在这里遇到了<a class="ae kv" href="https://www.duo.uio.no/bitstream/handle/10852/69487/master_mathialo.pdf?sequence=1" rel="noopener ugc nofollow" target="_blank">(从第44页开始)</a>。</p></blockquote><p id="b2d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们现在来看看卷积运算实际上是如何实现的。</p><p id="ba86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然在图像上滑动内核是直观的，但我们的python库在执行矩阵乘法时更加舒适和高效。因此，他们不是滑动内核，而是"<strong class="ky ir">将图像展开</strong>成平坦的张量，用平坦的内核做<strong class="ky ir">矩阵乘法</strong>，并将其"<strong class="ky ir">折叠</strong>"到所需的输出维度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/222605fbb5425f93a6f4890320ba1ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/1*ZUylCJjJBRnK0TvgERiqVg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开和折叠操作</p></figure><p id="8eaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">展开操作</strong>例如，如果一幅图像有3个通道，那么展开矩阵的行数将是<code class="fe ns nt nu ne b">3 x kernel size</code>。展开图像中的列数就是内核幻灯片的数量。另外，请注意展开操作包括相邻幻灯片中的重复像素。</p><p id="5670" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相反，<strong class="ky ir">折叠操作</strong> <code class="fe ns nt nu ne b"><strong class="ky ir">F(a)</strong></code>采用展开的矩阵，并在给定内核和输出尺寸的情况下构建图像。</p><p id="6658" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以卷积层可以概括为:<br/> <code class="fe ns nt nu ne b"><strong class="ky ir">(w ★ a) = F(WᵀU(a))<br/></strong></code>现在我们知道了矩阵乘法运算的Lipschitz常数。让我们试着找到展开和折叠的方法。</p><p id="51b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于足够小的内核(<code class="fe ns nt nu ne b"><strong class="ky ir">k x k</strong></code>)和相对较大的图像，展开操作符最多可以重复像素<code class="fe ns nt nu ne b"><strong class="ky ir">k²</strong></code>次。除此之外，它只是重塑了图像矩阵。折叠运算符也是如此，只是它最多在<code class="fe ns nt nu ne b"><strong class="ky ir">k²</strong></code>次合并一个给定的像素。因此，我们可以说，不管输入的变化如何，展开/折叠操作的变化都受到恒定内核大小<code class="fe ns nt nu ne b"><strong class="ky ir">k²</strong></code>的限制。<br/>让我们来证明这一点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qd qe l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qd qe l"/></div></figure><p id="715d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用<code class="fe ns nt nu ne b"><strong class="ky ir">1</strong></code>通道创建一个任意的<code class="fe ns nt nu ne b"><strong class="ky ir">4 x 4</strong></code>张量(图像)。首先，我们打开它，然后折叠它。在这样做的时候，我们检查两个操作的导数。展开和折叠的<code class="fe ns nt nu ne b"><strong class="ky ir"><em class="no">dₘₐₓ</em></strong></code>为<code class="fe ns nt nu ne b"><strong class="ky ir">k²</strong></code>。因此，<code class="fe ns nt nu ne b"><strong class="ky ir">U(a)</strong></code>和<code class="fe ns nt nu ne b"><strong class="ky ir">F(a)</strong></code> <strong class="ky ir"> </strong>的李普希兹常数受<code class="fe ns nt nu ne b"><strong class="ky ir">k²</strong></code>限制。</p><p id="c82e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，卷积层的连续性由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qf"><img src="../Images/344b347b4052b4ff5c5725532aadc2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7G6CmGtykfYiPExpOzEZQ.png"/></div></div></figure><p id="fa1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经看到，算子范数<code class="fe ns nt nu ne b"><strong class="ky ir">‖<em class="no">Wₖ</em>‖</strong></code> <strong class="ky ir"> </strong>以核矩阵<code class="fe ns nt nu ne b"><strong class="ky ir">σ₁</strong></code>的最大奇异值为界。</p><blockquote class="nx ny nz"><p id="b852" class="kw kx no ky b kz la jr lb lc ld ju le oa lg lh li ob lk ll lm oc lo lp lq lr ij bi translated"><strong class="ky ir">注意</strong>虽然卷积层的这种解释的想法取自<a class="ae kv" href="https://www.duo.uio.no/bitstream/handle/10852/69487/master_mathialo.pdf?sequence=1" rel="noopener ugc nofollow" target="_blank">这篇论文的文章</a>，但它没有考虑matmul之后的折叠运算符。它也没有解释重复操作是如何给出展开的Lipschitz常数的。为了提供更好的直觉，我试图填补这些空白。我很乐意看到和解决任何关于相同的评论。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/d71a404ebb6a3aea80664c9677228cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*oEwSn4k1KAtMqIXrPVKa8A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人</a>的网络各层的界限</p></figure><p id="8266" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以清楚地看到，所有层的上限值都相当高，表明它们不稳定。</p><h1 id="f4a8" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="c52b" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">在这篇文章中，我们浏览了<a class="ae kv" href="https://arxiv.org/abs/1312.6199" rel="noopener ugc nofollow" target="_blank"> Szegedy等人的</a>，并试图对这篇论文涉及的神经网络的各种属性有一个直观的了解。</p><p id="0f9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们发现神经网络在归纳方面可能并不稳定。显而易见，我们应该考虑如何让它们稳定下来。这是一个独立的研究分支，致力于通过正则化各层来推广神经网络，使其成为<a class="ae kv" href="https://en.wikipedia.org/wiki/Contraction_mapping" rel="noopener ugc nofollow" target="_blank">收缩</a>(基本上是<code class="fe ns nt nu ne b"><strong class="ky ir">0 ≤ <em class="no">Lₖ</em> &lt; 1</strong></code>的连续性)。</p><p id="03e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在未来尝试触及一些这方面的研究。</p><h1 id="a5a0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><div class="qh qi gp gr qj qk"><a href="https://arxiv.org/abs/1312.6199" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">神经网络的有趣特性</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">深度神经网络是高度表达的模型，最近在语音方面取得了最先进的表现…</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">arxiv.org</p></div></div><div class="qt l"><div class="qu l qv qw qx qt qy kp qk"/></div></div></a></div><div class="qh qi gp gr qj qk"><a href="https://stats.stackexchange.com/questions/371564/intriguing-properties-of-neural-networks" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">神经网络的有趣特性</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">它可能有助于你研究一个具体的例子。作者通过…表示整个激活层</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">stats.stackexchange.com</p></div></div><div class="qt l"><div class="qz l qv qw qx qt qy kp qk"/></div></div></a></div><p id="59b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.duo.uio.no/bitstream/handle/10852/69487/master_mathialo.pdf" rel="noopener ugc nofollow" target="_blank">https://www . duo . uio . no/bitstream/handle/10852/69487/master _ mathialo . pdf</a></p><div class="qh qi gp gr qj qk"><a href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">李普希茨连续性-维基百科</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">在数学分析中，以德国数学家鲁道夫·李普希茨命名的李普希茨连续性是一种强形式的…</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">en.wikipedia.org</p></div></div><div class="qt l"><div class="ra l qv qw qx qt qy kp qk"/></div></div></a></div><div class="qh qi gp gr qj qk"><a href="https://www.youtube.com/watch?v=zVDDITt4XEA" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">PyTorch -引擎盖下的卷积(展开/折叠)</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">我将讨论如何使用折叠和展开操作从头开始实现类似卷积的操作。这是如何…</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">www.youtube.com</p></div></div><div class="qt l"><div class="rb l qv qw qx qt qy kp qk"/></div></div></a></div><p id="6a87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://math.berkeley.edu/~hutching/teach/54-2017/svd-notes.pdf" rel="noopener ugc nofollow" target="_blank">https://math . Berkeley . edu/~ hutching/teach/54-2017/SVD-notes . pdf</a></p><div class="qh qi gp gr qj qk"><a href="https://yeephycho.github.io/2016/08/03/normalizations_in_neural_networks/" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">神经网络中的标准化</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">在图像处理领域，术语“归一化”有许多其他名称，如对比度拉伸、直方图拉伸等</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">yeephycho.github.io</p></div></div><div class="qt l"><div class="rc l qv qw qx qt qy kp qk"/></div></div></a></div><div class="qh qi gp gr qj qk"><a href="https://arxiv.org/abs/2006.08391" rel="noopener  ugc nofollow" target="_blank"><div class="ql ab fo"><div class="qm ab qn cl cj qo"><h2 class="bd ir gy z fp qp fr fs qq fu fw ip bi translated">用Toeplitz矩阵理论研究卷积层的Lipschitz正则化</h2><div class="qr l"><h3 class="bd b gy z fp qp fr fs qq fu fw dk translated">本文研究卷积神经网络的Lipschitz正则化问题。李普希茨正则性是…</h3></div><div class="qs l"><p class="bd b dl z fp qp fr fs qq fu fw dk translated">arxiv.org</p></div></div><div class="qt l"><div class="rd l qv qw qx qt qy kp qk"/></div></div></a></div></div></div>    
</body>
</html>