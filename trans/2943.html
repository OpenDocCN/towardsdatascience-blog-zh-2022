<html>
<head>
<title>Understanding Self-Organising Map Neural Network with Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python代码理解自组织映射神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985#2022-06-28">https://towardsdatascience.com/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985#2022-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2b6c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过竞争、合作和适应的大脑启发的无监督机器学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/85aaa40a6a1d9b65bea58cc76c7839e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*X10Usd9AgRGfdQqvmVR2VQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自组织映射的进化。作者图片</p></figure><h1 id="9ebe" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">1.介绍</h1><p id="f50a" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">自组织映射(SOM)是一种无监督的机器学习算法，由Teuvo Kohonen于20世纪80年代提出[1]。顾名思义，地图在没有任何他人指导的情况下自行组织。这是一个受大脑启发的模型。我们大脑中大脑皮层的不同区域负责特定的活动。视觉、听觉、嗅觉和味觉等感官输入通过突触以自组织的方式映射到相应皮层区域的神经元。还已知具有相似输出的神经元在附近。SOM通过竞争神经网络进行训练，这是一种类似于这些大脑机制的单层前馈网络。</p><p id="9ae6" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">SOM的算法相对简单，但是乍一看可能会有一些混乱，并且很难弄清楚如何在实践中应用它。可能是因为SOM可以从多个角度来理解。它类似于用于降维和可视化的主成分分析(PCA)。SOM也可以被认为是一种处理非线性降维的流形学习。SOM还因其矢量量化特性而用于数据挖掘[2]。训练可以将高维可观测数据表示到较低维的潜在空间上，通常在2D正方形网格上，同时保留原始输入空间的拓扑。但是该地图也可以用于投影新的数据点，并查看哪个集群属于该地图。</p><p id="7b3e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">本文解释了自组织映射的基本结构及其算法，重点放在它的自组织方面。我们用Python编写了SOM来解决一个聚类问题，使用了UCI机器学习知识库[3]中的数据集。然后，我们将看到在线(连续)训练中地图是如何组织自己的。最后，我们对训练好的SOM进行了评估，并讨论了它的优点和局限性。SOM不是最流行的ML技术，在学术文献之外也不是很常见；然而，这并不意味着SOM不是解决所有问题的有效工具。训练一个模型是相对容易的，来自训练好的模型的可视化可以用来有效地向非技术审计人员解释。我们将会看到，该算法所面临的问题在其他无监督的方法中也经常出现。</p><h1 id="5558" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">2.架构和学习算法</h1><p id="18ff" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">自组织映射的神经网络具有一个输入层和一个输出层。第二层通常由一个二维网格组成，网格中有<em class="mo"> m </em> x <em class="mo"> n </em>个神经元。地图层的每个神经元与输入层的所有神经元紧密相连，拥有不同的权重值。</p><p id="b48c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在竞争学习中，输出层的神经元相互竞争被激活。竞赛获胜的神经元是唯一可以被解雇的神经元；因此，它被称为赢家通吃神经元。在SOM中，竞争过程是寻找与输入模式最相似的神经元；获胜者被称为最佳匹配单位(BMU)。</p><p id="e462" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">相似性作为获胜的标准可以用几种方法来衡量。最常用的度量是欧几里德距离。与输入信号距离最短的神经元成为BMU。</p><p id="d189" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在SOM中，学习不仅是为了胜利者，也是为了在地图上物理上接近它的神经元。BMU与其邻国共享共同学习的特权。邻域的定义由网络设计者确定，而最佳邻近度取决于其他超参数。如果邻域范围太小，则训练的模型将遭受过拟合，并且存在一些死亡神经元永远没有机会改变的风险。</p><p id="3442" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在适应阶段，BMU及其邻国会调整自身的权重。学习的效果是将获胜和相邻神经元的权重移动到更接近输入模式。举个例子，如果输入信号是蓝色的，而BMU神经元是浅蓝色的，胜出者会变得比浅蓝色更蓝。如果邻居是黄色的，他们会在当前的颜色上增加一点蓝色。</p><p id="2829" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">自组织映射学习算法(在线学习)可以用以下4个步骤来描述。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="ada0" class="mu kw iq mq b gy mv mw l mx my">1. Initialisation<br/>   Weights of neurons in the map layer are initialised.<br/><br/>2. Competitive process<br/>   Select one input sample and search the best matching unit among all neurons in n x m grid using distance measures.<br/><br/>3. Cooperative process<br/>   Find the proximity neurons of BMU by neighbourhood function.<br/><br/>4. Adaptation process<br/>   Update the BMU and neighbours' weights by shifting the values towards the input pattern.</span><span id="4b95" class="mu kw iq mq b gy mz mw l mx my">   If the maximum count of training iteration is reached, exit. If not, increment the iteration count by 1 and repeat the process from 2.</span></pre><h1 id="0aa2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">3.履行</h1><p id="8d0a" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在本文中，SOM使用在<a class="ae na" href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication" rel="noopener ugc nofollow" target="_blank"> UCI ML Repository </a>网站上提供的钞票认证数据集进行训练。数据文件包含1372行，每行有4个特征和1个标签。所有4个特征都是没有空值的数值。标签是一个二进制整数值。</p><p id="737e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">首先，我们将数据随机分为训练数据和测试数据。我们使用所有4个特征通过在线训练算法来训练SOM。然后，我们通过使用训练数据的标签可视化地图来评估训练的SOM。最后，我们可以使用训练好的映射，使用测试数据来预测标签。</p><p id="cd82" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">因此，我们可以证明，以无监督方式训练的SOM可以应用于使用标记数据集的分类。</p><h2 id="7e3a" class="mu kw iq bd kx nb nc dn lb nd ne dp lf lw nf ng lh ma nh ni lj me nj nk ll nl bi translated">Python代码</h2><p id="3a58" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="mo"> 1。导入库</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="ad72" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 2。导入数据集</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="ed9e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">CSV文件从网站下载并存储在一个目录中。我们使用前4列表示x，最后一列表示y。</p><p id="b191" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 3。训练和测试数据分割</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="37f3" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">数据以0.8:0.2的比例分割用于训练和测试。我们可以看到分别有1097个和275个观察值。</p><p id="6bd4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 4。助手功能</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="ffd2" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> minmax_scaler </strong>用于在0和1之间标准化输入数据。因为算法会计算距离，所以我们应该将每个要素的值缩放到相同的范围，以避免其中任何一个要素对距离计算的影响大于其他要素。</p><p id="93c8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> e_distance </strong>计算两点之间的欧氏距离。<strong class="lp ir"> m_distance </strong>用于获取网格上两点之间的曼哈顿距离。在我们的例子中，欧几里德距离用于搜索获胜的神经元，而曼哈顿距离用于限制邻域范围。它通过应用矩形邻域函数来简化计算，其中位于距BMU的拓扑位置一定曼哈顿距离内的神经元在相同水平上被激活。</p><p id="5194" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir"> winning_neuron </strong>在BMU中搜索样本数据<em class="mo"> t </em>。计算输入信号和地图层中每个神经元之间的距离，并返回具有最短距离的神经元的网格的行和列索引。</p><p id="4124" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">衰减</strong>返回使用当前训练步、最大训练步数、最大邻域范围和学习率应用线性衰减后的学习率和邻域范围。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/bafd5ffffd0b1d2c68fd79e722344dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*VpI2hcP0zY4BElcEcAEsQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–4–1邻域范围衰减</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ff50da5c8180a9a21bd489da6840250c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*AW2pMjdervdfSZzQeTSaxw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–4–2邻域范围衰减</p></figure><p id="db18" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 5。超参数</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="1525" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">超参数是不可训练的参数，需要在训练算法之前选择。它们是神经元的数量、SOM网格的尺寸、训练步骤的数量、学习速率和来自BMU的邻域范围。</p><p id="3725" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在本例中，我们为网格设置了较小的数字(10*10 ),但是对于超参数的选择有启发性。我们可以使用[5 * sqrt(训练样本数)]公式[4]来选择神经元的数量。我们有1097个训练样本，因此可以在网格上创建5* sqrt(1097) = 165.60个神经元。因为我们有一个2D正方晶格，这个数的平方根暗示了我们在每个维度上可以有多少个神经元。sqrt的上限(165.40) = 13。，所以地图的尺寸可以是13*13。</p><p id="bf22" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">训练步骤的数量可能需要至少(500 * <em class="mo"> n </em>行* <em class="mo"> m </em>列)来收敛。首先，我们可以将步数设置为500 *13*13 = 84，500。学习率和邻域范围可以设置为较大的数字，并逐渐减小。建议使用不同的超参数集进行改进试验。</p><p id="1dca" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">最大邻域范围和学习率的初始值可以设置为一个较大的数。如果比率太小，可能会导致过度拟合，并需要更多的学习训练步骤。</p><p id="42b9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 6。培训</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="1081" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在应用输入数据标准化之后，我们用0和1之间的随机值为网格上的每个神经元初始化映射。然后使用衰减函数计算学习率和邻近范围。从训练数据中随机选择样本输入观察，并搜索最佳匹配单元。基于曼哈顿距离准则，选择包括获胜者在内的邻居进行学习，并调整权重。</p><p id="52ad" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 7。向经过训练的SOM显示标签</em></p><p id="8734" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在上一步中，我们完成了培训。因为这是无监督学习，但我们的问题有一个标签数据，我们现在可以将标签投影到地图上。这一步有两个部分。首先，收集每个神经元的标签。其次，将单个标签投影到每个神经元，构建标签图。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="c104" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们创建与SOM相同的网格。对于每个训练数据，我们搜索获胜的神经元，并将观察的标签添加到每个BMU的列表中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="70f8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">为了构建标签图，我们通过多数投票给图上的每个神经元分配一个标签。在没有选择BMU的神经元的情况下，我们将类值2指定为不可识别的。图3–7–1和3–7–2显示了第一次和最后一次迭代创建的标签映射。开始时，许多神经元既不是0也不是1，类别标签看起来是随机分散的；最终的迭代清楚地显示了类0和1之间的区域分离，尽管我们在最终的迭代中看到几个不属于任何一个类的单元。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/08c8652f98d8fb0dc6a6a7fe1b1d7c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NIuasZZ1zaiQcA1MdPG8AQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–7–1第一次迭代时训练图上的标签</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/2be7cf74fb2e5132c6a715d6e2b2da68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PwAtT4GeH-VKySf7jZk0qA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–7–2最大迭代时训练图上的标签</p></figure><p id="5bfb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">figure 3–7–3是一个动画gif，显示了SOM从第一步到最大75，000步的演变。我们可以看到地图清楚地组织了自己。</p><p id="32a7" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">要生成动画gif，您可以参考我以前的文章，使用Matplotlib库为Python片段进行粒子群优化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/85aaa40a6a1d9b65bea58cc76c7839e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*X10Usd9AgRGfdQqvmVR2VQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–7–3 SOM培训动画</p></figure><p id="7ba4" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="mo"> 8。预测测试集标签</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="4436" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">最后，我们可以使用训练好的映射对测试数据进行二进制分类。我们对测试x数据进行归一化，并搜索每个观察值t的MBU。返回与神经元相关联的标签。返回了准确性结果，我们的示例获得了非常好的结果，如图3–8所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/5826924e85b9c2a60024951fb1ad4224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SadaE_PvvHI40HcgsIQiWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3–8预测结果</p></figure><h1 id="c152" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">4.估价</h1><p id="4719" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在上一节中，演示了如何针对分类问题实现无监督的自组织映射。我们使用没有标注的数据集来训练地图，并通过将标注投影到地图上来确认训练的结果。正如预期的那样，我们可以观察到每个类别都有清晰的区域，具有相似属性的神经元彼此靠近。最后，我们用一个不可预见的测试数据集测试了地图，并测量了预测的准确性。</p><p id="0057" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我们在示例中使用的数据是一个小而干净的数据集，具有有限数量的观察值和特征。在现实生活中，数据科学家面临的问题在维度上要高得多，并且带标签的数据集不完全可用。即使有，它们的质量也不一定可靠。例如，在为一家银行检测恶意交易时，不期望所有交易都对照阳性案例进行检查可能是明智的；可能只有少数阳性病例在真实数据集中被标记。</p><p id="d774" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">当把自组织地图应用到现实世界的场景中时，我们会遇到一些挑战。首先，如果我们没有带标签的数据集，我们就无法测量损失。我们无法验证训练好的地图有多可靠。地图的质量在很大程度上取决于数据本身的特征。通过归一化进行的数据预处理对于基于距离的算法是必不可少的。对数据集的预先分析对于理解数据点的分布也很重要。特别是对于无法可视化的高维数据，我们可以使用其他降维技术，如PCA和奇异值分解(SVD)。</p><p id="2f64" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">此外，如果拓扑图的形状与潜在空间中数据点的分布不相关，则训练可能不成功。虽然我们在例子中使用了正方形网格，但是我们必须仔细设计地图的结构。一个值得推荐的方法是使用主成分分析的前两个主成分的解释方差的比率。但是，如果时间允许，尝试不同的超参数进行微调是值得的。</p><p id="03e9" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在算法的计算代价方面，训练时间复杂度取决于迭代次数、特征数和神经元数。最初，SOM是为顺序学习而设计的，但在某些情况下，批量学习方法是首选。随着大数据的数据量增加，可能有必要研究更有效的学习算法。在我们的例子中，我们使用了矩形邻域函数，为简化起见，称为气泡。对于训练迭代，我们可以监控自组织映射的形成，并检查在循环过程中如何学习映射。训练步骤数量的减少直接影响计算的数量。</p><p id="f3bb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">最后，本文没有涉及批量学习算法。如果问题的数据有限，使用批处理算法是一个不错的选择。事实上，Kohonen指出，批处理算法是有效的，对于实际应用是值得推荐的[5]。我们想通过留下链接到Kohonen的文章“自组织地图的本质”来结束这篇文章，以供进一步阅读。</p><div class="nr ns gp gr nt nu"><a href="https://www.sciencedirect.com/science/article/pii/S0893608012002596?via%3Dihub" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd ir gy z fp nz fr fs oa fu fw ip bi translated">自组织映射的本质</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">自组织映射(SOM)是一种自动数据分析方法。它被广泛应用于聚类问题和数据挖掘</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">www.sciencedirect.com</p></div></div><div class="od l"><div class="oe l of og oh od oi kp nu"/></div></div></a></div><h1 id="c00a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><p id="7070" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1] T. Kohonen，“拓扑正确特征图的自组织形成”，生物控制论，第43卷，第1期，第59–69页，1982年，doi: 10.1007/bf00337288。</p><p id="30e8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">[2] de Bodt，e .，Cottrell，m .，Letremy，p .和Verleysen，m .，2004年。利用自组织映射加速矢量量化。神经计算，第56期，第187-203页。</p><p id="f633" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">[3]杜瓦和格拉夫(2019年)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</p><p id="4a7c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">[4]田军，M. H. Azarian，M. Pecht，“利用基于自组织映射的K-最近邻算法进行异常检测”，《PHME_CONF》，第2卷第1期，2014年7月。</p><p id="479e" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">[5] T. Kohonen，“自组织映射的本质”，神经网络，第37卷，第52–65页，2013年1月，doi:10.1016/j . neu net . 2012 . 09 . 018。</p><p id="2d30" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi">‌</p></div></div>    
</body>
</html>