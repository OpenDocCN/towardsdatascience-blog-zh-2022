<html>
<head>
<title>Hierarchical Clustering and K-means Clustering on Country Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">国家数据的层次聚类和K-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hierarchical-clustering-and-k-means-clustering-on-country-data-84b2bf54d282#2022-05-04">https://towardsdatascience.com/hierarchical-clustering-and-k-means-clustering-on-country-data-84b2bf54d282#2022-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b79" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概述两种聚类技术并比较结果</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ea23a7a5f4973f302a94228bd2506cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*li-6kJ_3Q_MuwnHERZTcwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7fb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">聚类分析是一种无监督的技术，它试图在某些特征方面彼此相似的观察值中识别亚组。像<a class="ae lu" rel="noopener" target="_blank" href="/principal-component-analysis-fbce2a22c6e0">主成分分析</a>一样，聚类通过使用数据的某些摘要来简化数据。虽然PCA减少了数据的维数(特征),但是聚类识别了观察值中的相似组。</p><p id="cfe0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有聚类技术都旨在将观测值分组为子组/聚类，以便:</p><ul class=""><li id="df0a" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">每个聚类包含彼此相似的观察值。就某些群集特征而言，群集应该是同质的。</li><li id="dbeb" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">一个集群的观测值不同于其他集群的观测值。每个集群在相同的特征方面不同于其他群组。</li></ul><p id="754c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相似性的定义必须针对特定的聚类任务来定义，并且通常需要一些关于数据的领域特定的知识。两个相似的观察结果意味着什么也取决于研究的目的。</p><p id="72cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将使用k-means进行两种类型的聚类，一种是层次聚类，一种是非层次聚类，并比较结果。分层聚类不需要事先知道适当的聚类数。它创建了一个称为树状图的类似树的聚类可视化，并提供了整个聚类的良好概览。把树砍到一定的高度会产生不同数量的簇。另一方面，K-means要求我们指定聚类的数量。</p><p id="0f12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用了在<a class="ae lu" href="https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>可获得的国家数据集。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="53c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在分别研究层次聚类和k-means聚类之前，我想提一下聚类分析的总体步骤，以及无论选择哪种方法都需要考虑的几个重要方面。</p><h2 id="286f" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">进行聚类分析的4个步骤</h2><ol class=""><li id="7465" class="lv lw it la b lb nj le nk lh nl ll nm lp nn lt no mb mc md bi translated">选择相似性/距离(有些人更喜欢说不相似性)和缩放的度量。</li><li id="c1c5" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt no mb mc md bi translated">选择聚类技术的类型，分层或非分层。</li><li id="c38d" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt no mb mc md bi translated">选择聚类方法的类型(即层次聚类中的平均或完全链接方法)。</li><li id="10b1" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt no mb mc md bi translated">决定一些集群。</li></ol><h2 id="1c10" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">选择距离/相似性和缩放的度量</h2><p id="d32a" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">距离/相似性的度量以及数据的缩放对结果有很大的影响。因此，根据研究的目的和数据的类型，我们需要考虑如何衡量相似性以及如何调整数据。</p><ul class=""><li id="4d7f" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">欧几里德距离:平方差之和的平方根，这是最常见的。曼哈顿、最大值和闵可夫斯基是您可以研究的其他指标。</li><li id="3d32" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">基于相关性的距离:测量相关性将强调观察的形状，而不是它们的大小。特征高度相关的两个观察值将被认为是相似的，即使它们以欧几里德距离度量相距很远。</li><li id="d272" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">变量的缩放:决定标准差/方差是否应该为1。具体选择取决于您希望按何种模式/相似性对观察值进行聚类。如果你想给每个特性同等的重要性，你应该让std/var等于1。如果在不同的比例下测量要素，您可能希望缩放数据，使其具有std/var 1。</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="d0ca" class="ns mr it bd ms nt nu nv mv nw nx ny my jz nz ka nb kc oa kd ne kf ob kg nh oc bi translated">分层聚类</h1><p id="d4bc" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">分层聚类以分层的方式形成子组。分层聚类可以以聚集或分割的方式执行。</p><ul class=""><li id="bd3f" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">聚集</strong>(“自下而上”)聚类从每个观察值成为自己的聚类开始。当我们沿着树向上移动时，它们合并成子群。</li><li id="86be" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">分裂</strong>(“自上而下”)聚类从所有观察值的一个聚类开始。当我们沿着树向下移动时，集群被分成子组。</li></ul><h2 id="4206" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">层次聚类的步骤:</h2><ul class=""><li id="0de6" class="lv lw it la b lb nj le nk lh nl ll nm lp nn lt ma mb mc md bi translated">选择距离/相似性和缩放的度量。</li><li id="fb22" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">选择链接方法。</li><li id="ffdb" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">n个观测值中的每一个本身都被视为一个聚类。彼此最相似的聚类形成一个聚类，在第一次迭代之后留下n-1个聚类。该算法迭代进行，直到所有的观察值都属于一个聚类，该聚类用树状图表示。</li><li id="7833" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">决定集群的数量</li></ul><h2 id="2bf4" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">链接方法:</h2><ul class=""><li id="cdf8" class="lv lw it la b lb nj le nk lh nl ll nm lp nn lt ma mb mc md bi translated"><strong class="la iu">最近邻法或单链法</strong>:两个亚组之间的距离用这两个亚组中所有可能的观测值对之间的最小距离来表示。</li><li id="dea0" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">最近邻法或完全连锁法</strong>:与上述方法相反。两个子组之间的距离由两个子组中所有可能的观察值对之间的最大距离表示。</li><li id="4da8" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">平均连锁法</strong>:在这种方法中，两个亚组之间的距离用两个亚组中所有可能的观察值对的平均值来表示。</li><li id="f52d" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">质心法</strong>:每个聚类被一个作为该组质心的平均主体所代替。</li><li id="4909" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">沃德方法</strong>:该方法通过最大化类内同质性来形成类，测量类内平方和。</li></ul><h2 id="931d" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">如何评估结果/选择聚类数</h2><ul class=""><li id="6ccc" class="lv lw it la b lb nj le nk lh nl ll nm lp nn lt ma mb mc md bi translated">类内平方和</li><li id="4b66" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">观察结果在树状图(树)和散点图中的可视化表示</li></ul><p id="793e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看如何使用r进行层次聚类。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="8057" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">读出数据</h2><p id="9815" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">我们将对两种聚类技术使用相同的数据集，您可以看到数据的第一行，以及下面变量的平均值和方差表。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="441d" class="mq mr it oe b gy oi oj l ok ol">library(tidyverse) # data manipulation and ggplot2 <br/>library(factoextra) # multivariate package</span><span id="b54c" class="mq mr it oe b gy om oj l ok ol"># Read data<br/>data_raw &lt;- read.csv("Country-data.csv") # keeping countries as column<br/>countries &lt;- data_raw[, 1]<br/>data &lt;- read.csv("Country-data.csv", row.names = "country") # set countries as index</span><span id="91be" class="mq mr it oe b gy om oj l ok ol"># View first rows<br/>head(data)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/16d5718c8f96f946b8789b68803ccacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dTs84IrkaSShzxOT4J9lA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="19b0" class="mq mr it oe b gy oi oj l ok ol"># Check variance and mean of the numeric variables<br/>variance &lt;- as.data.frame(apply(data_raw[, 2:10], MARGIN = 2, FUN = var))<br/>means &lt;- as.data.frame(apply(data_raw[,2:10], 2, mean))</span><span id="ab8e" class="mq mr it oe b gy om oj l ok ol"># Store mean and variance in one data frame<br/>mean_variance_df &lt;- cbind(means, variance)<br/>colnames(mean_variance_df) &lt;- c('Mean', 'Variance')<br/>round(mean_variance_df, digits = 2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1ecd00061e232d29d369a12922131030.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*FzvAGjHcfWzuqrPHlIxalw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="6329" class="mq mr it oe b gy oi oj l ok ol"># Standardize the data (mean 0, std 1)<br/>data_std &lt;- as.data.frame(scale(data))</span></pre><h2 id="c7ee" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">创建集群</h2><p id="92f0" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">用<code class="fe op oq or oe b">hclust()</code>函数进行层次聚类。它以距离矩阵为输入数据，采用链接法。距离矩阵可以用<code class="fe op oq or oe b">dist()</code>函数或<code class="fe op oq or oe b">get_dist()</code>函数计算。后者允许相关性作为距离度量。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="1b3c" class="mq mr it oe b gy oi oj l ok ol">### Hierarchical clustering with hclust()<br/># hclust() takes a distance matrix as data input, use dist() or get_dist() for this<br/># Choose linkage method with method argument</span><span id="3908" class="mq mr it oe b gy om oj l ok ol"># Hierarchical clustering on standardized data using Euclidean distance measure<br/>complete &lt;- hclust(get_dist(data_std, method = 'euclidean'), method = 'complete')</span></pre><h2 id="5195" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">显示树状图</h2><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="f74c" class="mq mr it oe b gy oi oj l ok ol"># Plot the dendrogram, showing the entire clustering result</span><span id="1e02" class="mq mr it oe b gy om oj l ok ol"># Show dendrogram, pass cluster object as first argument<br/>plot(complete, main = 'Complete Linkage', xlab = "",<br/>     sub = "", cex = 0.8)</span><span id="e0c8" class="mq mr it oe b gy om oj l ok ol"># Add rectangle around clusters, choose how many clusters with k, color in border<br/>rect.hclust(complete, k = 5, border = 3:6)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/a9f2f039410ce32d7fb58f69506798d1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6CwRedFfZbgYJqC5GfjgIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b5fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">旁注:</strong>我尝试了几种聚类方法(完全、平均、单一、沃德)，在所有聚类中，尼日利亚、海地和卡塔尔单独表现突出，卢森堡、马耳他和新加坡也很接近。这表明这些国家在某些方面不同于所有其他国家。此外，平均和单一的联系将几乎所有国家放在一个大的集群中，考虑到这些变量以及这些国家在这些变量上的差异，这是不合适的。</p><h2 id="31bc" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">显示树状图的特定分支/簇</h2><p id="f22c" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">对于较大的数据集，树状图很快变得难以概括，但是可以通过在一定高度切割树并绘制它们来查看特定的分支/簇。高度是距离测量和连接方法的结果。低于特定高度的聚类的观测值在其聚类内具有小于该特定高度的最大距离和关联。</p><p id="8f64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，如果我们想要查看用蓝色标记的聚类，我们在高度8处切割树，这将导致五个聚类。我们可以通过绘制切口上方的分支来查看簇的数量。为了显示每个分支/簇包含的内容，我们在切割下绘制树状图，并指定哪个分支/簇。看看哪些国家属于第二组，它们似乎是欠发达国家。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="b261" class="mq mr it oe b gy oi oj l ok ol"># Store cluster object as a dendrogram<br/>complete_dend &lt;- as.dendrogram(complete)</span><span id="309a" class="mq mr it oe b gy om oj l ok ol"># Plot full dendrogram<br/>plot(complete_dend, main="Full Dendrogram")<br/>rect.hclust(complete, k = 5, border = 3:6) </span><span id="8803" class="mq mr it oe b gy om oj l ok ol"># Plot branches above the cut<br/>plot(cut(complete_dend, h=8)$upper, <br/>     main="Upper tree of cut at h=10")</span><span id="f401" class="mq mr it oe b gy om oj l ok ol"># Plot selected branch below the cut<br/>plot(cut(complete_dend, h=8)$lower[[2]], # Change value in 'lower[]' to view different branches/clusters<br/>     main="Second branch of lower tree with cut at h=8")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/efe27acb93180a8c612a668fe1888c55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*lpcMzWzd3qiBKxQngK9l2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="1d3d" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">如何解读树状图</h2><p id="bcb9" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">在树的底部，每个观察值是一片树叶，当我们沿着树向上移动时，观察值融合成树枝。沿着树向上延伸，树枝与其他树枝或树叶融合在一起。叶/枝融合越早，观察结果越相似。重要的是要记住，相似性是基于水平轴，而不是垂直轴。在树的底部早期融合的观察结果比在树的更高处融合的观察结果/分支更相似(后者实际上可能非常不同)。</p><h2 id="aa9a" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">使用cutree()提取集群分配</h2><p id="fcb9" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">我们可以在一个选定的高度砍下这棵树，这将给我们k个簇。然后，我们可以将聚类id存储在数据框中，并在散点图中绘制聚类。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="0818" class="mq mr it oe b gy oi oj l ok ol"># Cut the dendrogram at a certain height giving a specific number of clusters with cutree(). <br/>clusters_complete &lt;- cutree(complete, k = 5) # ok cluster with 6 too</span><span id="7472" class="mq mr it oe b gy om oj l ok ol"># Append the cluster result to the dataset so that we can plot the clusters with different colors <br/>complete_df &lt;- data_raw %&gt;% <br/>                mutate(cluster = clusters_complete) %&gt;%<br/>                relocate(cluster, .after = country)</span><span id="859e" class="mq mr it oe b gy om oj l ok ol"># Filter cluster of choice too see the countries in that cluster<br/>complete_df %&gt;% filter(cluster == 3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/c77a0f3ef116791ad41544809ce57acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59Syy-dgBvhepfH1sWwIKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="54b2" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">聚类散点图</h2><p id="074b" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated"><code class="fe op oq or oe b">fviz_cluster()</code>函数将聚类可视化，如果变量的数量超过两个，则应用主成分。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="4d8b" class="mq mr it oe b gy oi oj l ok ol"># Plot the clusters. Number of clusters as chosen in cutree() and stored in clusters_complete</span><span id="c811" class="mq mr it oe b gy om oj l ok ol"># Print clusters. The clusters make sense<br/>fviz_cluster(list(data = data_std, cluster = clusters_complete), <br/>             main = 'Complete Linkage, euclidean distance')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/99d9d8d316a680af7172f45c28e7ceac.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DCS2SvPnWbX8su6x9VNqvA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0418" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以根据一些变量绘制国家图，并用颜色分类，看看这些分类对特定变量是否有意义。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="1d7e" class="mq mr it oe b gy oi oj l ok ol"># Plot gpdp and life expectancy and color by clusters. As expected, life expectancy increases as gdp increases. <br/># Clusters ok. The outliers integrates with other clusters for these variables. </span><span id="ec4b" class="mq mr it oe b gy om oj l ok ol">ggplot(complete_df, aes(x=gdpp, y = life_expec, col = as.factor(cluster), label = country)) + <br/>        geom_point() +<br/>        geom_text() +<br/>        ggtitle('GDP and Life Expectancy') +<br/>        xlab('GDPP') +<br/>        ylab('Life Expectancy') +<br/>        scale_x_log10() </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/d9cf7705ffee481622a1bc4f4acfa96e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*okrb6m6n8P59NmNVUGP88w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="88dd" class="mq mr it oe b gy oi oj l ok ol"># Plot gpdp and child_mort. Child mortality decreases as dgp increases.</span><span id="51db" class="mq mr it oe b gy om oj l ok ol">ggplot(complete_df, aes(x=gdpp, y = child_mort, col = as.factor(cluster), label = country)) + <br/>        geom_point() +<br/>        geom_text() +<br/>        ggtitle('GDPP and Child Mortality') +<br/>        xlab('GDPP') +<br/>        ylab('Child Mortality') +<br/>        scale_x_log10() +<br/>        scale_y_log10()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/1745a0576d9887931b6a67b936f680df.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vMnjZHSiTkJlkYN06u3yhw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="fcb4" class="ns mr it bd ms nt nu nv mv nw nx ny my jz nz ka nb kc oa kd ne kf ob kg nh oc bi translated">k-means(非层次聚类)</h1><p id="fd42" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">非层次聚类要求聚类的起始分区/数目是先验已知的。我们希望将数据点划分为k个聚类，以便所有聚类的类内变化尽可能小。有不同的非层次聚类方法，大多数主要在以下方面有所不同:</p><ul class=""><li id="af25" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">用于设置初始聚类质心的方法。</li><li id="8fd4" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">在每次迭代中重新分配观测值的规则。</li></ul><p id="dafd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用K-means方法进行非层次聚类。</p><h2 id="40ac" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">k均值聚类的步骤</h2><ul class=""><li id="a60b" class="lv lw it la b lb nj le nk lh nl ll nm lp nn lt ma mb mc md bi translated">选择k个集群。</li><li id="13e6" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">通过将每个观察值随机分配给k个聚类中的一个来初始化。</li><li id="4772" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">重复以下两个步骤，直到集群分配停止变化:</li><li id="70ac" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">计算每个聚类的质心</li><li id="579a" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">将每个观察值分配/重新分配给质心最近的聚类。</li></ul><h2 id="c78c" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">如何评估结果/选择聚类数</h2><p id="d649" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">为了帮助决定使用多少个集群，我们可以使用以下两种方法。</p><ul class=""><li id="1132" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated"><strong class="la iu">肘图</strong>:针对不同数量的聚类，绘制每个聚类的平方和内图。随着每一个额外的聚类，平方和内将减少，但是我们将寻找肘部，在那里平方和内从先前的聚类数减少最多，然后对于额外的聚类变平。</li><li id="7999" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><strong class="la iu">剪影分析</strong>:查看每个观察值被分配到一个集群的情况。</li></ul><p id="6494" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在将看到k-means是如何使用r实现的。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="cf04" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">使用k-means创建任意数量的聚类</h2><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="dd24" class="mq mr it oe b gy oi oj l ok ol"># Perform k-means with arbitrary number of clusters to obtain the object<br/>k_means &lt;- kmeans(data_std, centers = 3)</span><span id="8f8c" class="mq mr it oe b gy om oj l ok ol"># Look at what k_means model object contains<br/>str(k_means)</span><span id="b19a" class="mq mr it oe b gy om oj l ok ol">List of 9<br/> $ cluster     : Named int [1:167] 3 1 1 3 1 1 1 1 1 1 ...<br/>  ..- attr(*, "names")= chr [1:167] "Afghanistan" "Albania" "Algeria" "Angola" ...<br/> $ centers     : num [1:3, 1:9] -0.649 0.483 0.941 0.281 -0.278 ...<br/>  ..- attr(*, "dimnames")=List of 2<br/>  .. ..$ : chr [1:3] "1" "2" "3"<br/>  .. ..$ : chr [1:9] "child_mort" "exports" "health" "imports" ...<br/> $ totss       : num 1494<br/> $ withinss    : num [1:3] 625 34.7 306.3<br/> $ tot.withinss: num 966<br/> $ betweenss   : num 528<br/> $ size        : int [1:3] 98 3 66<br/> $ iter        : int 2<br/> $ ifault      : int 0<br/> - attr(*, "class")= chr "kmeans"</span><span id="fa86" class="mq mr it oe b gy om oj l ok ol"># After getting cluster assignment we can append them to data frame<br/>data_kmeans &lt;- data_raw %&gt;% <br/>                   mutate(cluster = k_means$cluster)</span></pre><h2 id="7474" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">每个聚类的平方和内肘图</h2><p id="dbdf" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">这里我们创建了一个函数，它对1到8个聚类执行k-means聚类，并存储每个聚类的平方和，以便我们可以绘制肘图。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="eebb" class="mq mr it oe b gy oi oj l ok ol"># Use sapply to get wihtin sum of squares for k = 1:8<br/>tot_withinss &lt;- sapply(1:8, function(k){<br/>    km_mod &lt;- kmeans(x = data_std, centers = k)<br/>    km_mod$tot.withinss<br/>})</span><span id="840e" class="mq mr it oe b gy om oj l ok ol"># Run function and store within sum-of-squares in a dataframe<br/>tot_withinss_df &lt;- data.frame(k = 1:8, tot_withinss = tot_withinss)</span><span id="b06c" class="mq mr it oe b gy om oj l ok ol"># Plot elbow plot<br/>ggplot(tot_withinss_df, aes(x = k, y = tot_withinss)) +<br/>    geom_point() +<br/>    geom_line() +<br/>    scale_x_continuous(breaks = 1:8) +<br/>    ggtitle('Elbow plot of Within Sum of Squares') + <br/>    ylab('Within Sum of Squares') + <br/>    xlab('k = number of clusters')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/d33d7f1740571ca787403b729dfd4fdb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*HqWPQPgr0OnbJL4Q6f8I8Q.png"/></div></figure><p id="feb4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">肘形图没有给出明确的聚类数。也许三个或六个？我们稍后将绘制聚类图，并评估有多少聚类给出了合理的结果。</p><h2 id="c9aa" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">轮廓分析—观察水平</h2><p id="2a40" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">我们也可以创建一个小函数，手动绘制轮廓图，但是factoextra包有一个用于轮廓图的内置函数。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="131e" class="mq mr it oe b gy oi oj l ok ol"># Quickest to use factoextra package and use this plot<br/>fviz_nbclust(data_std, kmeans, method= 'silhouette')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/922e4fa011daa96e927f1ae44deb19ef.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AHC3p9w1eIsIjfqErA3gkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="72d4" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">评估3-6个聚类的k均值聚类</h2><p id="b056" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">肘部和轮廓图没有给出明确的答案，因此我们可以对3-6个聚类进行k-means聚类，并查看每个聚类的结果，以确定哪个数量的聚类有意义。我们还会将结果与层次聚类进行比较。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="feca" class="mq mr it oe b gy oi oj l ok ol"># Run k-means clustering with 3-6 clusters<br/>k_means_3 &lt;- kmeans(data_std, centers = 3)<br/>k_means_4 &lt;- kmeans(data_std, centers = 4)<br/>k_means_5 &lt;- kmeans(data_std, centers = 5)<br/>k_means_6 &lt;- kmeans(data_std, centers = 6)</span><span id="9bf4" class="mq mr it oe b gy om oj l ok ol"># Add cluster assignments to data frame<br/>data_kmeans_3 &lt;- data_raw %&gt;% <br/>                   mutate(cluster = k_means_3$cluster) </span><span id="2020" class="mq mr it oe b gy om oj l ok ol">data_kmeans_4 &lt;- data_raw %&gt;% <br/>                   mutate(cluster = k_means_4$cluster) </span><span id="210d" class="mq mr it oe b gy om oj l ok ol">data_kmeans_5 &lt;- data_raw %&gt;% <br/>                   mutate(cluster = k_means_5$cluster) </span><span id="98b7" class="mq mr it oe b gy om oj l ok ol">data_kmeans_6 &lt;- data_raw %&gt;% <br/>                   mutate(cluster = k_means_6$cluster)</span></pre><h2 id="10ae" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">可视化k均值聚类</h2><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="15a7" class="mq mr it oe b gy oi oj l ok ol"># Plot the clusters from k-means with x clusters</span><span id="71dd" class="mq mr it oe b gy om oj l ok ol"># Print clusters. Here 3 clusters. <br/># Change digit in "k_means_x$clusters" to look at different number of clusters<br/>fviz_cluster(list(data = data_std, cluster = k_means_3$cluster), main = 'K-means, 4 clusters')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/406f14ac83cc65bc4ffb5712bac4bbd2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*tsC2Yibszep9Fqa4l5A51g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="9ed5" class="mq mr it oe b gy oi oj l ok ol"># Plot the clusters from k-means with 4 clusters<br/>fviz_cluster(list(data = data_std, cluster = k_means_4$cluster), main = 'K-means, 4 clusters')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/8090330e77b61c0b3cec51a71b598ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyCLuNS2C-ot-ie9n1fy3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="9710" class="mq mr it oe b gy oi oj l ok ol"># Plot the clusters from k-means with 5 clusters<br/>fviz_cluster(list(data = data_std, cluster = k_means_5$cluster), main = 'K-means, 4 clusters')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/bdcf79373c6fb6e31275eba86d6768fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pvz4JNbBn_o_cRKRhYNniQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="06ad" class="mq mr it bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">用k均值聚类分析变量</h2><p id="fbe9" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">正如我们对层次聚类所做的那样，我们可以在聚类后观察一些变量和颜色。</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="0227" class="mq mr it oe b gy oi oj l ok ol"># Plot gpdp and life expectancy and color by clusters. Life_expectancy increases as gdp increases. <br/># Clusters ok. The outliers integrates with other clusters for these variables. <br/>ggplot(data_kmeans_4, aes(x = gdpp, y = life_expec, col = as.factor(cluster), label = country)) + <br/>        geom_point() +<br/>        geom_text() +<br/>        ggtitle('GDP and Life Expectancy') +<br/>        xlab('GDPP') +<br/>        ylab('Life Expectancy') +<br/>        scale_x_log10() <br/>        #scale_y_log10()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/b27a6ccabb707bd914ba7549623c3e66.png" data-original-src="https://miro.medium.com/v2/format:webp/1*25mKT7yJoPYLuy-sMUgjuA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="b5c1" class="mq mr it oe b gy oi oj l ok ol"># Plot gpdp and child_mort. As expected, child mortality decreases as dgp increases. Clusters ok.<br/>ggplot(data_kmeans_4, aes(x=gdpp, y = child_mort, col = as.factor(cluster), label = country)) + <br/>        geom_point() +<br/>        geom_text() +<br/>        ggtitle('GDPP and Child Mortality') +<br/>        xlab('GDPP') +<br/>        ylab('Child Mortality') +<br/>        scale_x_log10() +<br/>        scale_y_log10()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl os"><img src="../Images/efa802806bfe94a780ad8e062c763cfb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6h-OD8jCFqLHkB_ZqNG3TA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="bcdf" class="ns mr it bd ms nt nu nv mv nw nx ny my jz nz ka nb kc oa kd ne kf ob kg nh oc bi translated">比较层次聚类和k-means聚类的结果</h1><p id="e585" class="pw-post-body-paragraph ky kz it la b lb nj ju ld le nk jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">用3、4、5和6个聚类对k-means进行聚类和可视化会得到不同的结果。人们可以更仔细地研究不同变量中的分组，但总体而言，考虑到国家通常被分为“欠发达”、“发展中”和“发达”三类，我认为三四个分组是有意义的，因为至少有一些现有变量被用于对这些国家进行分类。</p><p id="9fb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用四个群集可以将马耳他、卢森堡和新加坡保持在自己单独的群集中。如果我们使用三个集群，这三个国家被合并到包含发达国家的集群中，这也是有意义的。</p><p id="4425" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种聚类技术之间也有区别。相同数量的聚类使用不同的方法，层次和k-means，给出非常不同的结果。例如，具有k均值的四个聚类与使用分层聚类的四个聚类非常不同。但是，四个k-means聚类与五个层次聚类非常相似，因为层次聚类将尼日利亚分配到它自己的聚类中。其余的四个聚类类似于四个k均值聚类。来自k-means的聚类更明显，重叠更少。使用k-means将更多的国家分配到发达国家组。</p><p id="81ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种方法各有利弊，选择哪种方法取决于研究的目的。请记住，它们可以被视为补充方法，在继续进行非层次聚类之前，可以在探索意义上使用层次聚类。</p></div></div>    
</body>
</html>