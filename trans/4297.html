<html>
<head>
<title>Google’s PaLI: language-image learning in 100 languages</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌的巴利文:100种语言的语言图像学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/googles-pali-language-image-learning-in-100-languages-31d32f9b74fe#2022-09-22">https://towardsdatascience.com/googles-pali-language-image-learning-in-100-languages-31d32f9b74fe#2022-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1fdb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">令人印象深刻的新型号，能够在复杂任务中达到最先进水平</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ebb7a51d4afc2413e1fe45d9eb6d1f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sJZvdAuXOF_Y52ek"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">unsplash.com<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">的<a class="ae ky" href="https://unsplash.com/@grantritchie" rel="noopener ugc nofollow" target="_blank">格兰特·里奇</a>的图片</a></p></figure><p id="12b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">近年来的趋势是增加神经网络的容量。一方面，越来越多的参数的不同模型被公布，另一方面，用于训练的数据量增加。无论输入和任务是文本的还是视觉的，这种观点都是普遍的。</p><p id="917a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于文本模型，增加的容量显示了改进的性能和有趣的涌现行为:<a class="ae ky" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT </a>、<a class="ae ky" href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html" rel="noopener ugc nofollow" target="_blank"> GLaM </a>、<a class="ae ky" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank"> PaLM </a>、<a class="ae ky" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a>等等。而对于图像，在卷积神经网络主导该领域之前，近年来的趋势一直是<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">视觉变压器</a>。</p><p id="dfd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有两个领域(文本和视觉)中的类似进展已经允许在这两个领域的交叉点上打开一个新的领域:<a class="ae ky" href="https://lilianweng.github.io/posts/2022-06-09-vlm" rel="noopener ugc nofollow" target="_blank">视觉语言模型</a>。</p><p id="52b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">什么是视觉语言模型？</strong>将图像和文本都视为输入的模型可用于各种任务。今年，我们惊讶于DALL-E或T21这样的模型是如何从文字描述中创造出详细的图像的。另一方面，这只是视觉语言模型可以解决的任务之一:</p><ul class=""><li id="56cb" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">可视字幕(VC) </strong>:生成可视输入(图像、视频)的描述。该模型分析图像并提供表示该图像的文本描述。</li><li id="d86d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">视觉问题回答(VQA) </strong>:为视觉输入提供答案。</li><li id="38c6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">视觉常识推理(VCR) </strong>:从视觉输入中推断常识信息和认知理解。</li><li id="b12d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">视觉生成(VG) </strong>:从文本输入(提示)生成视觉输出。</li><li id="fc14" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">上下文光学字符识别(OCR) </strong> : OCR是将包含文本(打印、手写或印刷)的图像转换成计算机可以理解的文本输出。</li><li id="05f1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">物体识别</strong></li></ul><p id="a272" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，许多模型仅使用英语进行训练，但是有数千种语言(<a class="ae ky" href="https://en.wikipedia.org/wiki/Language" rel="noopener ugc nofollow" target="_blank">估计有7000种语言</a>)并且重要的是其他语言被表示和包括在内。</p><p id="84da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本周展示了一个语言视觉模型，可以用100种语言执行任务。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="d649" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的模型参数是170亿(语言模型计数为13 B，视觉组件为4)。该模型由一个处理文本的<a class="ae ky" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">变压器</a>编码器和一个生成测试输出的n自回归变压器解码器组成。图像输入由连接到编码器的<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">视觉转换器(Vit) </a>处理。有趣的是，这些组件取自早期型号:</p><blockquote class="ml mm mn"><p id="3879" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">PaLI模型的一个关键组成部分是重用，其中我们用来自先前训练的单模态视觉和语言模型的权重来播种模型，例如<a class="ae ky" href="https://arxiv.org/abs/2010.11934" rel="noopener ugc nofollow" target="_blank"> mT5-XXL </a>和大<a class="ae ky" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank"> ViTs </a>。这种重用不仅能够转移单模态训练的能力，而且还节省了计算成本。— <a class="ae ky" href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能</a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/bda70894fa9d02a22d1ce76812e2d5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xqdOSmFUsavQazVFe_Oqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型建筑。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><p id="2b18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，作者创建了三个不同的模型，它们的参数越来越多，但架构相似(最终的模型容量最大)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/39a66f9e61c48d4b4e8aeb056a0e7138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bB3dFMJWXgOXdpEsCqOpGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><p id="67c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者专门为这个模型构建了一个数据集: WebLI。该数据集是通过从网上收集图像和文本构建的(它们不限于英语，而是收集了109种语言的数据)。他们因此收集了120亿张图像和120亿条<a class="ae ky" href="https://en.wikipedia.org/wiki/Alt_attribute" rel="noopener ugc nofollow" target="_blank">替代文本</a>(描述图像的文本)。他们还使用GCP视觉API提取了图像中的文本(OCR注释)，产生了290亿个图像-OCR对。最终，只有10亿张图像被用于训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/d278c74192078b0806d30fe7dc037ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7lFq_HLgfKNDzcTUzUn0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“与多语言替代文本(可用)和光学字符识别(使用GCP视觉API5计算)相关的样本图像4”。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bee0090aaf7a551061adbd12d3947d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FPsnSQ66zjXpzzXt.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">WebLI中alt-text和OCR识别语言的统计。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/395d635add7de249cb70bdb9f594ec0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*uoyExkmvEqvH2E6C.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">WebLI等大规模视觉语言数据集的图文对计数、<a class="ae ky" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> CLIP </a>、<a class="ae ky" href="https://arxiv.org/abs/2102.05918" rel="noopener ugc nofollow" target="_blank"> ALIGN </a>和<a class="ae ky" href="https://arxiv.org/abs/2111.07991" rel="noopener ugc nofollow" target="_blank"> LiT </a>。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><blockquote class="ml mm mn"><p id="bf77" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">视觉语言任务需要不同的能力，有时会有不同的目标。一些任务本质上需要对象的本地化来精确地解决任务，而另一些任务可能需要更全局的视图。类似地，不同的任务可能需要长的或紧凑的答案。— <a class="ae ky" href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能</a></p></blockquote><p id="1e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正是因为这个原因，他们开发了一种特殊的训练策略，目的是每次为模型提供一个输入(图像+文本)并接收一个输出(文本)。事实上，该模型伴随着一系列预训练任务(仅文本损坏、本地和翻译的替代文本数据上的字幕、分割字幕等)，但仍保持相同的系统(输入:图像+文本；输出:文本)。因此，允许以相同的方式训练模型，同时仍然保持模型能够概括和能够执行其他任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/8120b1b75df8da327dd34288eb8953da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsjpd5Ai1W_3AaGpYMs2Gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">training.image来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><blockquote class="ml mm mn"><p id="25b5" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">模型在<a class="ae ky" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX </a>与<a class="ae ky" href="https://github.com/google/flax" rel="noopener ugc nofollow" target="_blank">亚麻</a>使用开源的<a class="ae ky" href="https://github.com/google-research/t5x" rel="noopener ugc nofollow" target="_blank"> T5X </a>和<a class="ae ky" href="https://github.com/google/flaxformer" rel="noopener ugc nofollow" target="_blank"> Flaxformer </a>框架进行训练。对于视觉组件，我们使用开源的<a class="ae ky" href="https://github.com/google-research/big_vision" rel="noopener ugc nofollow" target="_blank"> BigVision </a>框架引入并训练了一个大型<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> ViT </a>架构，名为ViT-e，具有4B参数。</p></blockquote><p id="e1e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦模型被训练，作者将它与其他最先进的方法(包括<a class="ae ky" href="https://arxiv.org/abs/2108.10904" rel="noopener ugc nofollow" target="_blank"> SimVLM </a>、<a class="ae ky" href="https://arxiv.org/abs/2205.01917" rel="noopener ugc nofollow" target="_blank"> CoCa </a>、<a class="ae ky" href="https://arxiv.org/pdf/2205.14100.pdf" rel="noopener ugc nofollow" target="_blank"> GIT2 </a>、<a class="ae ky" href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model" rel="noopener ugc nofollow" target="_blank"> Flamingo </a>、<a class="ae ky" href="https://arxiv.org/abs/2208.10442" rel="noopener ugc nofollow" target="_blank"> BEiT3 </a>)进行比较，以完成多种视觉和语言任务:</p><ul class=""><li id="11ba" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">图像字幕。作者在三个数据集上测试了该模型的能力:COCO(标准基准)、NoCaps(类似于COCO，但目标有更多的视觉概念)和TextCaps(图像包含文本)。此外，他们使用了另一个数据集XM-3600(与其他三个不同，它是多语言的)</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/36862a6497b3a4048f2c9c56de274d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Alv-u6TUo4n7Xd6VeZjKeg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">纯英文基准数据集上的图像字幕结果。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c0aee62cefe81ad8c498db31ff575bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*xyCNCiiRYH0mMlCrP4mQng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多语言数据集上的图像字幕结果(此处显示:英语、法语、印地语、希伯来语、罗马尼亚语、泰语和中文以及35种语言的平均值)。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><ul class=""><li id="0db5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">视觉问答</strong>。正如作者指出的，这是一项艰巨的任务，因为答案必须完全匹配才能被认为是准确的，而巴利文的词汇有100种语言。尽管如此，他们在几个数据集上取得了非常好的结果:VQAv2、OKVQA(其中需要外部知识来回答它的问题，因为所有知识都不存在于输入图像中，必须进行推断)，TextVQA &amp; VizWiz-QA(模型必须使用图像中存在的文本来回答)。他们还使用了跨语言和多语言数据集(xGQA和MaXM)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5aec45fc0f85286683fd83cbc39d16d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*b7Y4RNYpSynYj2c4LukvCA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">英语数据集上的可视化问答结果。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b5a5900bfc1ef12e74bea8f2bb07e5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*n35tdTAydmMnLoLGKdPkbw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">跨语言和多语言数据集的可视化问答。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></figure><ul class=""><li id="2273" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">语言理解能力。作者想知道模型一旦被训练用于多模态任务(图像+文本)是否会忘记它的语言建模能力。他们在英语和几种语言的数据集上测试了这些功能。该模型，尽管培训设置有利于多模态，保持了高水平的英语语言理解能力。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/62120745aaef9b5ea48f6fd6a74788c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*ztCeSvoaFbJhiPW3gUzIfw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语言理解能力的结果。图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原创文章</a></p></figure><ul class=""><li id="eb7b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">零镜头图像分类。在这种情况下，作者没有通过在原始模型上添加分类器来修改模型。相反，他们调整了数据集。该模型显示了良好的性能，但仍低于最先进水平</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/4492c2d9185a02312cca86c377e552b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*TEnjn4avQ6csUZg7cYQJLQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://arxiv.org/pdf/2209.06794.pdf" rel="noopener ugc nofollow" target="_blank">原文</a></p></figure><p id="7d3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然结果令人惊讶，但模型并不完美，<strong class="lb iu">也有一些局限性</strong>:</p><ul class=""><li id="3b04" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">该模型不能描述有许多对象的情况，因为训练数据集无论如何都没有复杂的注释。</li><li id="ac33" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">当模型针对纯英语数据进行微调时，会丢失一些多语言功能。</li><li id="7cf6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">正如作者所言，这种评估存在固有的局限性:“模型可能会生成一个正确的响应，它是目标响应的同义词或释义，并且与目标不完全匹配。”</li></ul><p id="35fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，他们试图减少潜在的偏差，并在文章的附录中提供了所用模型和数据集的详细表格(<a class="ae ky" href="https://modelcards.withgoogle.com/about" rel="noopener ugc nofollow" target="_blank">模型</a>和<a class="ae ky" href="https://arxiv.org/abs/2204.01075" rel="noopener ugc nofollow" target="_blank">数据卡</a>)。尽管正如作者在文章中指出的:</p><blockquote class="ml mm mn"><p id="f67a" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">大型模型可能会产生更广泛的社会影响。虽然这些模型在公共基准上表现强劲，但它们可能包含未知的偏见或成见，或者传播不准确或扭曲的信息。虽然我们已经努力衡量其中的一些问题，但在用于特定目的之前，需要对这些模型进行仔细的重新评估。</p></blockquote><p id="c5d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="5a8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PaLI表明，它在几项迄今为止被认为具有挑战性的任务中达到了最先进的水平。而且，不仅仅是英语，还有几种语言。</p><p id="754b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然它已经达到了最先进的水平，但很可能更多的视觉语言模型将很快发布(希望是开源的)。它还展示了一些趋势:</p><ul class=""><li id="a7af" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">性能更高但不一定能力更强的型号(它比<a class="ae ky" href="https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model" rel="noopener ugc nofollow" target="_blank"> Flamingo </a>更好，但只有18个B对80个B参数)</li><li id="8fa9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">除了英语之外，还接受多种语言训练的模特(如<a class="ae ky" href="https://pub.towardsai.net/no-language-left-behind-579afea29e52" rel="noopener ugc nofollow" target="_blank">梅塔的NLLB </a>或<a class="ae ky" href="https://pub.towardsai.net/a-new-bloom-in-ai-why-the-bloom-model-can-be-a-gamechanger-380a15b1fba7" rel="noopener ugc nofollow" target="_blank">布鲁姆</a>)</li><li id="3f7a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">这种情况下是多模态、图像和文本，但也有视频、音乐等…</li><li id="c16d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">专注于避免偏见</li></ul><h1 id="a880" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">如果你觉得有趣:</h1><p id="d305" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">你可以寻找我的其他文章，你也可以<a class="ae ky" href="https://salvatore-raieli.medium.com/subscribe" rel="noopener"> <strong class="lb iu">订阅</strong> </a>在我发表文章时得到通知，你也可以在<strong class="lb iu"/><a class="ae ky" href="https://www.linkedin.com/in/salvatore-raieli/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">LinkedIn</strong></a><strong class="lb iu">上连接或联系我。感谢您的支持！</strong></p><p id="2c4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我的GitHub知识库的链接，我计划在这里收集代码和许多与机器学习、人工智能等相关的资源。</p><div class="oa ob gp gr oc od"><a href="https://github.com/SalvatoreRa/tutorial" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">GitHub - SalvatoreRa/tutorial:关于机器学习、人工智能、数据科学的教程…</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">关于机器学习、人工智能、数据科学的教程，包括数学解释和可重复使用的代码(python…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">github.com</p></div></div><div class="om l"><div class="on l oo op oq om or ks od"/></div></div></a></div><p id="b8a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者随意查看我在Medium上的其他文章:</p><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/a-critical-analysis-of-your-dataset-2b388e7ca01e"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">对数据集的批判性分析</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">停止微调你的模型:你的模型已经很好了，但不是你的数据</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="os l oo op oq om or ks od"/></div></div></a></div><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/machine-learning-to-tackle-climate-change-7911e004c3a2"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">机器学习应对气候变化</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">人工智能如何帮助对抗全球变暖并从人类手中拯救世界</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="ot l oo op oq om or ks od"/></div></div></a></div><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/speaking-the-language-of-life-how-alphafold2-and-co-are-changing-biology-97cff7496221"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">说生命的语言:AlphaFold2和公司如何改变生物学</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">人工智能正在重塑生物学研究，并开辟治疗的新领域</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="ou l oo op oq om or ks od"/></div></div></a></div><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/how-science-contribution-has-become-a-toxic-environment-6beb382cebcd"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">科学贡献如何变成一个有毒的环境</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">计算机科学是如何继承了其他学科同样的错误的</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="ov l oo op oq om or ks od"/></div></div></a></div></div></div>    
</body>
</html>