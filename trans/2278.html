<html>
<head>
<title>Support Vector Machines (SVM) and the Multi-Dimensional Wizardry</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)和多维魔法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-svm-and-the-multi-dimensional-wizardry-b1563ccbc127#2022-05-19">https://towardsdatascience.com/support-vector-machine-svm-and-the-multi-dimensional-wizardry-b1563ccbc127#2022-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="945f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">数据科学|机器学习|算法</h2><div class=""/><div class=""><h2 id="312f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">了解支持向量机内核函数背后的魔力</h2></div><p id="053b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">支持向量机(SVM) </strong>是一种监督学习算法，用于回归、分类和离群点检测，但广泛用于分类。SVM的目标是在n维空间中创建一条线或一个超平面，将数据清楚地分成几类。</p><h1 id="316b" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">线性支持向量机</h1><p id="c16f" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">我们用一个例子来理解这个。假设您有如下所示的数据点，您需要将这些数据点分为两类。关键的想法是找到一条线来分隔这两个类。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/5d26f94a301d2b8b5857beb4c252a926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XsKVehDhECy7s-KdQbGZYQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="c477" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这种情况下，画一条线把这两个阶层分开并不是很难。然而，你可以画多条线来做这件事。那么，你怎么知道这几行中哪一行是你的问题的最佳答案呢？</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/de40ecfda51185debf0e9a51d6b4d78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6dk5nHG2dpa2UxpAQGoIsg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="9573" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从上面显示的线来看，黑线最好地分隔了两个类，因为与其他线相比，它离数据点的任一侧相对更远。对于蓝色和红色的线，我们不太确定新的、看不见的数据点的性能。</p><p id="58cc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">显然，我们不能总是想象和选择会得出最佳分类的线条。你可以用支持向量机来完成这项任务。SVM从两个类中查找最接近直线的点。这些点称为支持向量。直线和支持向量之间的距离称为边距。SVM试图最大化这一距离。边缘最大的超平面是SVM的最优超平面。在我们的例子中，上面的最优超平面是黑线。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mh"><img src="../Images/b17de35601f91842fecc0260e303d6b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQbCsvXR6WUybpXwUym6xg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="e835" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有两种类型的边距:</p><ol class=""><li id="f2e9" class="mx my iq kq b kr ks ku kv kx mz lb na lf nb lj nc nd ne nf bi translated"><strong class="kq ja">硬保证金:</strong></li></ol><p id="1d81" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在硬边界方法中，决策边界确保所有数据点都被正确分类。在训练数据中没有出错的余地。它也可能会减少保证金的大小来实现这一点，挫败了使用SVM的整个目的。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ng"><img src="../Images/6c733f911eedbd5b38c93e7996a338e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtx9a3tLx6zHRgSQWb_Dxg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="17a7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> 2。软边距:</strong></p><p id="e607" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">大多数真实世界的数据不是线性可分的。在软裕度方法中，我们允许在裕度中出现样本的一些错误分类。我们的想法是尽可能保持较大的利润。它试图在找到一条最大限度地增加边界的线和最小化错误分类之间进行权衡。</p><p id="785d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">硬边际方法对异常值敏感，而软边际方法对异常值稳健，并能很好地概括看不见的数据。</p><h1 id="ae5d" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">非线性支持向量机</h1><p id="210e" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">我们上面看到的样本数据是线性可分的。但是，并不是所有的数据都好到可以线性分离，这使得SVM的工作很困难。在这种情况下，我们不能用一条简单的直线来分隔数据。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/5f6f696ff8a3a468311b670c318218eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*8dOjieVggEUjdloKr6wK1Q.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="8040" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当难以分离非线性类时，我们可以应用一种称为<strong class="kq ja">内核技巧</strong>的技术来帮助重塑数据。Kernel trick提供了一种高效且廉价的数据转换方式。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1be1a2c35d37a23a569ce86dfe8b7fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*stErcCvhE1_SogRB_Azk7w.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="ec51" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上图中，在应用二次多项式核后，在一个维度上不可分的数据一旦转换为两个维度，就会变得可分。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nj"><img src="../Images/db02d6b596f59f5d1726b414ebb76818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vjYjGdVTetHvm7s9XcidBQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者创造的形象</p></figure><p id="2671" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">类似地，二次多项式核被应用于上述2D数据，并且通过使用线性平面它变得可分离。</p><p id="13f8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">关于如何工作的更多细节，我建议检查我的<a class="ae nk" href="https://github.com/swapnilin/SVM-Demo-and-Kernel-Functions/blob/main/SVM_Demo.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库上的代码。</p><h1 id="609e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated"><strong class="ak">内核函数</strong></h1><p id="4e9a" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">核函数是SVM模型中的一个超参数。它负责消除计算需求，以实现高维空间和处理线性不可分的数据。两个最广泛使用的内核函数是:</p><ol class=""><li id="458a" class="mx my iq kq b kr ks ku kv kx mz lb na lf nb lj nc nd ne nf bi translated">多项式核</li></ol><p id="4fae" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用具有“k”次的多项式函数，通过将非线性数据变换到更高维度来分离非线性数据。</p><p id="0f26" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.径向基函数核</p><p id="4792" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个核函数也称为高斯核函数。它能够产生无限数量的维度来分离非线性数据。它取决于一个超参数‘γ’(γ)。超参数的值越小，偏差越小，方差越大，反之亦然。</p><p id="3c97" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得注意的是，RBF内核不会在数据集中创建新的变量或任何转换。计算在内部进行，就好像这些点在更高维度中一样。您不必为RBF内核创建z变量来施展它的魔法。</p><p id="4478" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了实现SVM并理解内核函数的作用，我建议检查一下我的<a class="ae nk" href="https://github.com/swapnilin/SVM-Demo-and-Kernel-Functions/blob/main/SVM_Demo.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库上的代码。</p><p id="c6fe" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">感谢您的阅读。如果您有更多问题，请通过LinkedIn联系。</p><div class="nl nm gp gr nn no"><a href="https://swapnilin.github.io/" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ja gy z fp nt fr fs nu fu fw iz bi translated">Swapnil Kangralkar</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">Swapnil Kangralkar。我是加拿大渥太华的一名数据科学家。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">swapnilin.github.io</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc mr no"/></div></div></a></div></div></div>    
</body>
</html>