<html>
<head>
<title>Harder, Better, Faster, Lighter Deep Learning with Direct Feedback Alignment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更难、更好、更快、更轻的深度学习，具有直接反馈一致性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/harder-better-faster-lighter-deep-learning-with-direct-feedback-alignment-9e837b076baa#2022-06-16">https://towardsdatascience.com/harder-better-faster-lighter-deep-learning-with-direct-feedback-alignment-9e837b076baa#2022-06-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="10a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过随机性和激光提高深度学习效率的温和指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/751165b467b1b92d0305bd36a0eca690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*orv34O1Xgopp3Q2D"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">托拜厄斯·科奈尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="daf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将谈论自从我发现深度学习以来，我所面临的最有趣的主题之一:直接反馈对齐(DFA)。这种方法是标准深度学习训练方法(反向传播)的一种令人难以置信和令人难以置信的替代方法，同时允许更有效的训练。在这篇文章结束时，你会明白混合神经网络，激光和随机性可以提供更多的力量！</p><p id="aaba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章不是神经网络的介绍。尽管我们将对神经网络背后的数学做一个简要的回顾，但它是为了共享符号并介绍反向传播和DFA之间的区别。如果你想在深入研究之前了解更多关于神经网络和深度学习的知识，你可以阅读<a class="ae kv" href="https://medium.com/towards-data-science/deep-learnings-mathematics-f52b3c4d2576" rel="noopener">这篇文章</a>。</p><h1 id="bb5d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">神经网络101概述</h1><p id="0315" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">神经网络被广泛使用，因为它们能够很好地逼近每一个可能的函数，并且可以扩展到高维空间。它们的主要用途是监督学习，在监督学习中，它们被用于使用数据示例来推断输入和输出之间的关系，我们试图减少预测和目标之间的误差。然而，它们超出了这个范围，可以用来最小化或最大化任何给定的数字目标。</p><p id="7b9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络由相互连接的神经元层组成。这种结构允许使用一系列非线性变换将输入信号处理成输出信号，最终产生全局复变换。给定神经网络执行的转换由其参数(也称为权重和偏差)和激活函数来定义。为了清楚起见，本文中我们只考虑全连通网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/6464a426ff325821566d6277b96a7ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-G4Urde_MBAv1Ret2AZOOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="4853" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">前进传球</h2><p id="1c92" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在让我们深入研究它背后的数学原理。</p><p id="8b5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，神经网络如何处理输入:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/4c8d8a6982ef5a9ee7ab5242076eefa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ro9xIy40XTodf66gTAdFsQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正向传播方案(注意:最后一层的激活不同于其他层)。作者图片</p></figure><p id="93f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输入被馈送到第一层，在那里使用权重和偏差应用线性变换。然后，通过激活函数修改线性变换的输出，从而允许引入非线性(使用sigmoid、tanh或swish等函数)或线性修改(如ReLU及其变体)。然后，使用激活函数的结果作为下一层的输入，通过其他层重复该过程。</p><p id="05cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过程开始时，由于权重已经被随机初始化(这是神经网络中最常见的初始化类型)，正向传递的结果输出(以及网络的预测)本质上是随机的。</p><p id="823d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络的基本问题是:</p><blockquote class="nd"><p id="5538" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">我如何改变权重，使神经网络做我想要它做的事情？</p></blockquote><h2 id="af41" class="mq lt iq bd lu mr nn dn ly mt no dp mc lf np mw me lj nq my mg ln nr na mi nb bi translated">目标(损失)</h2><p id="4ca7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">知道如何改变权重取决于我们所说的“我希望网络做什么”是什么意思。这个目标的详细说明是通过一个目标函数来完成的，通常称为损失。损失(表示为L或J)是神经网络参数(权重和偏差，表示为θ)的函数，其描述了如何将我们的神经网络的输出与预期输出进行比较。它本质上是对网络错误程度的量化，并因此被最小化。例如，在二进制分类中，一个常见的损失是二进制交叉熵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/cf49b367e50b0e2c6a91abdda61a9f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AidBO2FUm8ZwppRS_757dQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">二进制交叉熵，其中m和n是输出单元和小批量索引(符号保持与DFA原始论文相同)。作者图片</p></figure><p id="2921" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在这种情况下，目标是最小化这个损失函数，以使网络做我们想要的。我们现在可以将问题更新为:</p><blockquote class="nd"><p id="77c3" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">我如何改变权重，使神经网络最小化损失？</p></blockquote><h2 id="ce89" class="mq lt iq bd lu mr nn dn ly mt no dp mc lf np mw me lj nq my mg ln nr na mi nb bi translated">反向传播</h2><p id="c2de" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">损失告诉我们离我们的目标有多远，我们希望有一种方法通过网络传播此信息，将权重(和偏差)更改为一个值，从而导致更低的损失。这意味着揭示每个个体重量对输出的贡献(以及损失)，以便能够适当地调整它们。让我们从调整最后一层的权重开始。为此，我们首先想知道我们的损耗(或误差)如何随最后一层的输出而变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fea07a999de27a5f510d73ddff17058c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*YIC6wNbWbR88843nJdjWNg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用二进制交叉熵和逻辑激活，损耗相对于最后一层激活的导数被简化为输出和标签之间的差。作者图片</p></figure><p id="ba3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们知道了损耗如何随输出层变化，我们想利用这一知识使激活以降低损耗的方式变化。因此，我们想知道如何以这样一种方式改变权重，以引起激活的期望变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/4a3883f94acfc6a43962d1a9e317dd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38wMM-88aKVBvC2GEGn4NA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左:减少损失所需的重量变化，右:重量更新(增加了学习率α)。作者图片</p></figure><p id="3e3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将相同的方案应用于其他层:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/d962586faee6379ce5f38545516d83bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qUFynwYuXuy3BehUFcuESQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隐藏层和输入层的权重更新方案。“带点的o”运算符是Hadamard乘积(逐元素乘法)。作者图片</p></figure><p id="4a52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后…瞧！我们更新了权重(类似的过程也适用于偏差)以减少损失。我们现在可以对新的示例重复该过程，以逐渐降低损失，直到收敛。</p><h1 id="fb6a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">那么反向传播有什么问题呢？</h1><p id="f004" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管这一过程可行，但也并非没有缺陷:</p><ul class=""><li id="2d60" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated"><strong class="ky ir">反向传播是一个连续的过程:</strong>在计算前一层的梯度之前，我们需要计算最后一层的梯度，然后需要计算前一层的梯度，等等。对于更深的网络，这使得过程非常慢</li><li id="018a" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir">梯度计算需要矩阵转置</strong>:这是一种昂贵的计算(具有O(n^2的复杂性)，并且需要大量内存。对于较大的网络，这可能很快成为一个问题。</li></ul><p id="f030" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个问题，根据<a class="ae kv" href="https://arxiv.org/abs/1411.0247" rel="noopener ugc nofollow" target="_blank">原始论文</a>的作者，问题不太大但仍然相关，就是反向传播不是“生物学上合理的”。这是因为在真实的神经元中，一致认为没有信号反向传播。</p><blockquote class="ok ol om"><p id="ae55" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">反向传播学习算法是强大的，但是需要生物学上难以置信的单个突触权重信息的传输。对于反向传播，神经元必须知道彼此的突触权重。[……]在计算机上，在正向和反向计算中使用突触权重很简单，但大脑中的突触单向传递信息</p><p id="6d79" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/1411.0247" rel="noopener ugc nofollow" target="_blank"> <em class="iq">来源</em> </a> <em class="iq">:随机反馈权重支持深度神经网络中的学习，T. P. Lillicrap，D. Cownden，D. B. Tweed，C. J. Akerman，2014 </em></p></blockquote><p id="3b63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们不打算在本文中讨论这种说法的真实性，但我们仍然会注意到，这种信念导致了旨在更具生物学合理性的方法的诞生。思路是，如果我们更接近地模仿大脑，我们将有更好的方法，因为大脑比现有的方法更有效。其中一种更符合生物学原理的方法是我们今天感兴趣的。</p><blockquote class="nd"><p id="f8eb" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">那么，我们能解决反向传播的缺陷吗？</p></blockquote><h1 id="752e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw or jx me jz os ka mg kc ot kd mi mj bi translated">直接反馈校准</h1><p id="6d10" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们从反向传播开始，在没有顺序计算和矩阵转置的情况下构造我们的梦更新。我们不在乎它是否有意义…现在还不在乎。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/eff57224c1d3a780923c64aee0a3d79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXxTUnMMqCxoUtin0zoroQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从反向传播(BP)到DFA的第一层更新修改。图片作者。</p></figure><p id="154b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了消除对前一层的依赖性，并因此消除顺序性以允许并行计算，<strong class="ky ir">我们使所有层使用相同的反馈</strong>:我们将跨所有层使用最后一层的误差(在上面的等式中表示为e ),而不是使用损耗相对于前一层的激活的梯度(上面的等式中的δ2)。<br/>然后，为了移除矩阵转置，<strong class="ky ir">我们用适当大小的固定随机矩阵</strong>替换转置的权重矩阵。没有转置计算来运行每一个更新了，只是生成一个随机矩阵每一层，并使用它进行所有的计算。</p><p id="4489" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧，我们有一个表达式，它不需要任何顺序计算，因为各层之间没有依赖关系，一旦我们计算了最后一层的误差，我们就可以同时计算所有其他层的更新。我们还删除了矩阵转置，这将进一步加速计算！这种更新称为直接反馈校准(DFA):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/2698b8f997b2dfc7314d46d251aae102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Dy5739VcxVBDzjkusoDMQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">与同时计算几个示例的更新但仍以顺序方式进行的并行化不同，DFA同时处理几个示例，同时更新所有层。红色是第一批，蓝色是第二批，箭头表示不同输入的向前和向后传递。作者图片，灵感来自<a class="ae kv" href="https://slideplayer.com/slide/12651467/" rel="noopener ugc nofollow" target="_blank">https://slideplayer.com/slide/12651467/</a></p></figure><p id="d901" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DFA是类似技术家族的一部分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/999df0c37d83522833559ab31fab6ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OcgltO1nKPueLCb4jya-kQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同反馈校准方法的表示。来源:<a class="ae kv" href="https://arxiv.org/pdf/1609.01596.pdf" rel="noopener ugc nofollow" target="_blank">直接反馈对齐提供深度神经网络中的学习</a>，Arild nkland，2016年</p></figure><ul class=""><li id="e804" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">反馈对准是该过程的第一步，仅去除矩阵转置，但保留反向信号。</li><li id="97ec" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">直接反馈调整，我们已经讨论过了。</li><li id="f3e5" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">间接反馈对齐，它消除了换位，并将最后一层的误差直接传播到第一层，然后继续向前传播。这允许使用单个随机矩阵而不是每层一个矩阵，从而节省存储器。实际上，这个版本没有DFA有趣，除非你的内存很紧张，所以我们不会关注它。</li></ul><h2 id="cc84" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">深度魔法</h2><p id="5d3e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">但是等一下…有了这个“<em class="on">更好地更新</em>”我们基本上已经破坏了这个过程中的反向传播，不是吗？我们的新更新如何允许训练神经网络？<strong class="ky ir">随机矩阵如何在有意义的更新中发挥作用？</strong></p><p id="af7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个非常好的问题！首先，让我们看看它与反向传播有何不同:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/c39da4fe7e70700b8111397d650dd1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHSm0aHJSisome2hgNbifg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们可以看到，尽管进行了修改，DFA仍然与反向传播(BP)结果相匹配。来源:<a class="ae kv" href="https://arxiv.org/pdf/1609.01596.pdf" rel="noopener ugc nofollow" target="_blank">直接反馈对齐提供深度神经网络中的学习</a>，Arild nkland，2016</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/b1703492db486f0974fd26db4b7f7f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9d2-ulvNh2eVFJgU1-cWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们可以看到，在几个深度学习任务上，DFA匹配甚至优于BP的结果。来源:<a class="ae kv" href="https://arxiv.org/pdf/2006.12878.pdf" rel="noopener ugc nofollow" target="_blank">现代深度学习任务和架构的直接反馈校准量表</a>，j .劳奈、I .里坡、f .博尼法斯和f .克尔扎卡拉，2020</p></figure><p id="bfe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">令人吃惊不是吗？</p><p id="ca9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们获得了与BP类似的结果(更好的结果会令人震惊，因为我们只是使信号变得更糟)，同时允许更简单的计算。我们可以更快地得到相同的结果！</p><h2 id="9cbf" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">但是…怎么做？</h2><p id="f680" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">太好了，但是……<strong class="ky ir">如何</strong>？让我们试着理解为什么我们看似破碎的更新仍然为我们的网络传递着一个有意义的信号。</p><p id="276c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原论文的作者解释说，网络“<em class="on">学习如何学习</em>”。Mhhh，我们见过更清晰的。他们指出，由于反馈现在是一个固定的和随机修改的误差，前馈权重必须自己适应。反馈不再是明确的，而是隐含的。</p><blockquote class="ok ol om"><p id="3f16" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">网络学习如何学习——它逐渐发现如何使用B(随机矩阵),然后允许有效地修改隐藏单元。起初，对隐藏层的更新没有帮助，但通过改变W的隐式反馈过程，它们很快得到改善，使得eT WBe &gt; 0。为了揭示这一点，我们绘制了由反馈对齐和反向投影规定的隐藏单元更新之间的角度，最初角度平均约为90°。但随着算法开始采取更接近反向投影的步骤，它们很快就会缩小。h的这种排列意味着b已经开始像W^T一样行动。因为b是固定的，所以这种排列是由前向权重w的变化驱动的。这样，随机反馈权重开始向网络深处的神经元传输有用的教学信号。<em class="iq">T11】</em></p><p id="e29b" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">来源:<a class="ae kv" href="https://arxiv.org/pdf/1411.0247.pdf" rel="noopener ugc nofollow" target="_blank">随机反馈权重支持深度神经网络中的学习</a>，T. P. Lillicrap、D. Cownden、D. B. Tweed和C. J. Akerman，2014年</p></blockquote><p id="79bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们试着解释一下，好吗？基本上，信号在有意义的信息方面要差得多，然而，更新方向大致是正确的，尽管如此，允许学习。更新方向与反向传播大致相同，因此称为“对齐”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/69dee43cc0472c95ec334be646acb7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*Rkdx-3gjh54KqUIjFt_0JQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DFA和BP更新之间的角度。随着训练的进行，DFA的更新几乎与BP的更新一致。来源:<a class="ae kv" href="https://arxiv.org/pdf/1411.0247.pdf" rel="noopener ugc nofollow" target="_blank">随机反馈权重支持深度神经网络中的学习</a>，T. P. Lillicrap、D. Cownden、D. B. Tweed和C. J. Akerman，2014年</p></figure><p id="f086" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DFA的更新围绕真实BP更新“振荡”。</p><h2 id="bce0" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">直觉</h2><p id="c045" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们尝试建立一些关于为什么会发生这种情况的直觉(这种直觉大多来自<a class="ae kv" href="https://www.youtube.com/watch?v=Hdo81GtLC_4&amp;ab_channel=YannicKilcher" rel="noopener ugc nofollow" target="_blank"> Yannic Kilcher的视频</a>，关于这篇文章的视频带有我自己理解的包装)。</p><p id="d310" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">即使反向传播的信号被随机矩阵调制，它也不会从误差中去除所有信息。这是由于向量(在我们的例子中是误差e)乘以一个随机矩阵来表示一个保持角度和距离的投影的性质(为什么会发生这种情况与Jonhson-Lidenstrauss引理有关，但我不能100%确定为什么会发生这种情况，所以我们将在本文中假设它是正确的)。换句话说，通过将我们的信号乘以随机矩阵，我们只是应用了随机旋转，而没有其他修改。</p><p id="1448" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们试着从最后一层开始想象。为了举例，我们将假设在一个分类问题中有3个可能的类别:c1、c2和c3。我们的网络对每个示例(y hat)进行预测，更新告诉我们应该在哪个方向上改变多少权重，以正确地将该示例归属于其类。换句话说，我们希望将预测(y hat，不同类别的概率向量，例如[0.7，0.1，0.2])与正确的类别(例如c1:[1，0，0])对齐。在这种情况下，要进行的修改将是[0.3，-0.1，-0.2]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/bf40dc77203cc5ca4120d96a46261851.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*tiXsP4GkMzAw5DSV1IETmg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表示将预测(y hat)正确映射到其类(此处:c1)的更新。作者图片</p></figure><p id="85a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在对于前面的层，我们必须处理随机乘法。这意味着我们的坐标系被随机旋转，同时保持角度和距离不变。在这个系统中，误差不传达我们应该如何修改预测以使其更接近目标，而是传达如何使其更接近目标的一些随机旋转。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/2e68688979f6d1372b8539f13f637d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Aixp0ig5_Mn_b-7UMU9JSw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">随机矩阵乘法使更新随机旋转，从而不再处于原始方向。作者图片</p></figure><p id="f06a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从单个例子的角度来看，这是不好的，因为更新似乎朝着一个随机错误的方向进行。然而，如果我们考虑多个示例，由于旋转保持不变，属于同一类的示例仍将被拉近，因为它们将被投影到同一旋转的类上。结果，传播的信号不允许指定权重应该被更新的绝对方向。然而，我们仍然可以用相对论的方式做到这一点:不是每次更新都在正确的绝对方向上移动权重，而是网络学习将应该靠近的例子聚集在一起。从某种意义上来说，这可以归结为对示例进行聚类，然后对聚类进行分类，而不是单独对它们进行分类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/825a8088fbc8e266821264776c448ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*L8wg1ZUnXSkVM1iU4THyMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隐藏层过程的表示:相同类的例子(绿色+蓝色和绿色+红色)被分组到相同的随机投影类上。作者图片</p></figure><p id="585d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，我们确实传播了错误，但是我们没有传播它应该如何解决最终任务。我们只宣传例子应该如何分组在一起。这使得最后一层对示例进行分类的任务大大简化了。为了证实这一点，让我们看看BP和DFA通过网络各层对输入到网络的示例的内部表示进行比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/dda45481faf2670e214f4093f2bd4417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rzQmvm_2xDaX4CmX_ZBMHQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/pdf/1609.01596.pdf" rel="noopener ugc nofollow" target="_blank">直接反馈对齐提供深度神经网络中的学习</a>，Arild nkland，2016</p></figure><p id="e88f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看这些表示证实了这种“聚类”方法导致了与BP构建的表示类似的结果。换句话说，我们以不同且更简单的方式实现了相同的结果，因为看起来隐藏层不需要显式地将示例映射到空间的特定部分，它们只需要对它们进行聚类。隐藏层构造的变换允许在两种方法中轻松分离最后一层。<strong class="ky ir">重要的是隐藏的特征，而不是我们获得它们的方式。DFA是BP更新的一个较弱版本，它保留了聚类特性，因此也保留了学习特性。DFA“足够好”可以工作。</strong></p><p id="15c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们对它的工作原理有了更清晰的直觉，让我们回到它的性能上来。</p><h1 id="243a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我为什么要使用DFA？</h1><p id="03e4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如我们所看到的，对于相似数量的例子，DFA收敛得和反向传播一样快(就例子而言)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/67ae01a3a729051e998d03fbc4f5659c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GvMeC8f8VF23t-gxZF06bQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">各种更新机制中的错误减少。来源:<a class="ae kv" href="https://arxiv.org/pdf/1411.0247.pdf" rel="noopener ugc nofollow" target="_blank">随机反馈权重支持深度神经网络中的学习</a>，T. P. Lillicrap，D. Cownden，D. B. Tweed和C. J. Akerman，2014</p></figure><p id="28b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，由于缺少转置和并行化所有层更新的能力，使用DFA的网络更新在计算上应该比使用反向传播快得多。虽然我们将在后面看到确切的数字，但由于下一点，很难在速度方面比较DFA和BP。</p><blockquote class="nd"><p id="14e4" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">如果DFA在保持性能的同时速度更快，那么我们为什么不使用它呢？</p></blockquote><p id="ba4f" class="pw-post-body-paragraph kw kx iq ky b kz pf jr lb lc pg ju le lf ph lh li lj pi ll lm ln pj lp lq lr ij bi translated">自1986年以来，反向传播一直存在并被大量研究。基本上，所有现有的深度学习都围绕着它。作者声称，缺乏DFA的优化实现以及研究人员和实践者的习惯阻碍了DFA的采用。然而，另一个技巧应该有助于DFA获胜…</p><h1 id="4775" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">london and south eastern regional library system 伦敦与东南地区图书系统</h1><p id="53a9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">激光，是的，激光。</p><p id="1736" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们一直在标准硬件(CPU或GPU)上比较DFA和BP。然而，依赖于随机矩阵乘法的DFA更新的特异性允许我们利用一个非常有趣的物理现象:<strong class="ky ir">动态光散射</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/06e0d5bfaa048a70232762f171966174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ar2j5gnx_QhCU9tGYWIfsg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">动态光散射的示意图。来源:<a class="ae kv" href="https://arxiv.org/pdf/1510.06664.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="pl">通过多重光学散射的随机投影:光速下的近似核</em> </a> <em class="pl">，a .萨阿德、f .卡尔塔基龙、I .卡隆、l .道代特、a .德雷莫、s .盖刚、f .克尔扎卡拉，2015 </em></p></figure><blockquote class="ok ol om"><p id="12d8" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">532nm的单色<strong class="ky ir">激光</strong>被望远镜放大，然后照射数字微镜装置(DMD)<strong class="ky ir">，该装置能够通过振幅调制对光束</strong>上的数字信息进行空间编码，如第三节b所述。然后，携带信号的光束<strong class="ky ir">通过透镜聚焦在随机介质</strong>上。这里，介质是一层厚(几十微米)的T iO2(二氧化钛)纳米粒子(沉积在显微镜载玻片上的白色颜料)。<strong class="ky ir">透射光由第二透镜在远侧</strong>收集，穿过偏振器，并由标准单色CCD摄像机测量。</p><p id="82bc" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated"><em class="iq">来源:</em> <a class="ae kv" href="https://arxiv.org/pdf/1510.06664.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="iq">通过多重光学散射的随机投影:光速下的近似核</em> </a> <em class="iq">，a .萨阿德、f .卡尔塔基龙、I .卡隆、l .道代、a .德雷莫、s .盖刚、f .克尔扎卡拉，2015 </em></p></blockquote><p id="dc37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说:</p><blockquote class="nd"><p id="8b47" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">我们正在以光速计算随机矩阵乘法。</p></blockquote><p id="933e" class="pw-post-body-paragraph kw kx iq ky b kz pf jr lb lc pg ju le lf ph lh li lj pi ll lm ln pj lp lq lr ij bi translated">你可能会说这种技术只限于实验室和研究论文…但不，它已经存在了！法国初创公司<a class="ae kv" href="https://lighton.ai/" rel="noopener ugc nofollow" target="_blank"> LightOn </a>已经设计并销售了光学处理单元(OPUs ),可以精确地做到这一点(随机矩阵乘法也可以用于其他应用)。</p><blockquote class="ok ol om"><p id="ee2e" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">LightOn成立于2016年，与总部位于巴黎的云计算服务提供商OVH集团合作，<strong class="ky ir">声称某些机器学习任务</strong>的性能有所提高。</p><p id="24c8" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated"><strong class="ky ir"> OPU使用激光</strong>照射到数字微镜装置(DMD) <strong class="ky ir">上，用光</strong>对1和0进行编码，然后通过透镜和<strong class="ky ir">随机散射介质组件</strong>进行重定向，然后被偏振，并由传统相机读取<strong class="ky ir">。<strong class="ky ir">这允许并行处理非常大的矩阵</strong>。可以做的一个操作就是内核分类。<strong class="ky ir">通常，DMD可以处理1k乘1k数量级的矩阵。</strong></strong></p><p id="95b1" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">对于一个名为<strong class="ky ir">转移学习</strong>的任务，<strong class="ky ir"> OPU显示出比基于GPU的解决方案快6倍的速度和5倍的能效</strong>。这意味着耗电量减少了30倍。另一个基于<strong class="ky ir">时间序列分析的基准测试显示，递归神经网络的速度比采用大内存的传统CPU快200倍</strong>。</p><p id="7394" class="kw kx on ky b kz la jr lb lc ld ju le oo lg lh li op lk ll lm oq lo lp lq lr ij bi translated">来源:<a class="ae kv" href="https://www.eenewsanalog.com/en/startup-integrates-optical-processing-within-data-center-2/" rel="noopener ugc nofollow" target="_blank">https://www . eenewsanalog . com/en/startup-integrated-optical-processing-within-data-center-2/</a></p></blockquote><p id="a74b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，这项技术允许更快地运行随机矩阵乘法计算，极大地改善了缩放，并允许更大的输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/7995f77a4c02cd839c34077346cd1b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*9W_ZyzskahYnmGzFoT4QqA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://arxiv.org/pdf/2104.14429.pdf" rel="noopener ugc nofollow" target="_blank">HPC中的光子协处理器:使用LightOn OPUs进行随机数值线性代数</a>，Daniel Hesslow、Alessandro Cappelli、Igor Carron、Laurent Daudet、Raphael Lafargue、Kilian Muller、Ruben Ohana、Gustave Pariente和Iacopo里坡，2021</p></figure><h1 id="25f7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="6784" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">通过这篇文章，我们发现了反向传播的一种替代方法，它允许通过巧妙的数学技巧，用更简单的计算表达式来近似相同的更新。尽管在传输的信号中不太明确，但是该表达式仍然允许传播重要的信息，从而允许以隐含的方式进行正确的更新。此外，这种计算更新的方式为通过专用硬件大大加速计算打开了大门，专用硬件利用光物理现象以光速进行计算。</p><p id="42b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，一旦我们获得了通用库中的高效实现以及负担得起且易于使用的专用硬件，直接反馈对准必将在计算速度、内存需求以及降低深度学习及其所有应用的能耗方面带来巨大的改进。</p></div><div class="ab cl pn po hu pp" role="separator"><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps pt"/><span class="pq bw bk pr ps"/></div><div class="ij ik il im in"><p id="5ea9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您通读了整篇文章。我希望你和我第一次遇到这个算法时一样对它感到惊讶。如果你有一个替代的直觉命题，我会很高兴知道它们。敬请关注更多内容！</p><h1 id="1f04" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">资源</h1><p id="e003" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">扬尼克·基尔彻在DFA上的视频:<a class="ae kv" href="https://www.youtube.com/watch?v=Hdo81GtLC_4&amp;ab_channel=YannicKilcher" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Hdo81GtLC_4&amp;ab _ channel =扬尼克·基尔彻</a></p><p id="1c68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的FA和DFA论文:<a class="ae kv" href="https://arxiv.org/pdf/1411.0247.pdf" rel="noopener ugc nofollow" target="_blank">随机反馈权重支持深度神经网络中的学习</a>，T. P. Lillicrap，D. Cownden，D. B. Tweed和C. J. Akerman，2014</p><p id="af65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一篇关于DFA的有趣论文:<a class="ae kv" href="https://arxiv.org/pdf/1609.01596.pdf" rel="noopener ugc nofollow" target="_blank">直接反馈对齐提供深度神经网络中的学习</a>，Arild nkland，2016</p><p id="b459" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于DFA性能的后续论文:<a class="ae kv" href="https://arxiv.org/pdf/2006.12878.pdf" rel="noopener ugc nofollow" target="_blank">直接反馈校准适用于现代深度学习任务和架构</a>，j .劳奈、I .里坡、f .博尼法斯和f .克尔扎卡拉，2020年</p><p id="a6af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于动态光散射的论文:<a class="ae kv" href="https://arxiv.org/pdf/1510.06664.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="on">通过多重光学散射的随机投影:光速下的近似核</em> </a> <em class="on">，A. Saade，F. Caltagirone，I. Carron，L. Daudet，A. Drémeau，s .盖刚，F. Krzakala，2015 </em></p><p id="17af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">莱顿的作品网站:<a class="ae kv" href="https://lighton.ai/photonic-computing-for-ai/" rel="noopener ugc nofollow" target="_blank">https://lighton.ai/photonic-computing-for-ai/</a></p><p id="a37c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别感谢Benjamin Farcy审阅本文。</p></div></div>    
</body>
</html>