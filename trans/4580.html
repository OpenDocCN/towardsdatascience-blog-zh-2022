<html>
<head>
<title>The Slow Mo Guys now Matched by AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">缓慢的莫家伙现在由人工智能匹配</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-slow-mo-guys-now-matched-by-ai-dba5e9e177c7#2022-10-11">https://towardsdatascience.com/the-slow-mo-guys-now-matched-by-ai-dba5e9e177c7#2022-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bc82" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用深度学习和视频帧插值在5分钟内提高您的慢动作视频的质量</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/ef855a3538067fa8f84fb8d44f25b0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*DfUYQS15V3h6Pa-f4ZTrfw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">克洛德·莫内画的相机——稳定扩散生成的图像</p></figure><p id="a36c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们都同意慢动作视频创造了这种戏剧效果，这无疑为特定场景增添了额外的风味。另一方面，降低视频速度可能会产生一些不想要的伪像，这可能会被认为是不值得观看的。但是是什么让慢动作视频变得伟大呢？</p><blockquote class="lq"><p id="8697" class="lr ls it bd lt lu lv lw lx ly lz lp dk translated">让我们使用深度学习来产生能够以高帧率记录的高端相机的平滑结果。</p></blockquote><p id="cea5" class="pw-post-body-paragraph ku kv it kw b kx ma ju kz la mb jx lc ld mc lf lg lh md lj lk ll me ln lo lp im bi translated">除了其他方面，我认为最重要的方面是它的FPS数:每秒有多少帧被渲染到屏幕上。太低，视频会断断续续，造成不愉快的观看体验。<strong class="kw iu">因此，我们需要一台昂贵的摄像机，能够以高帧率进行拍摄。或者，我们可以使用深度学习，尝试在两个连续的帧之间合成中间图像。</strong>我有没有提到这会让我们多花0美元？</p><p id="e2ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">超级慢动作只是计算机视觉领域中众所周知的视频帧插值或VFI的应用之一。这个故事介绍了<em class="mf"> RIFE:视频帧插值的实时中间流估计。</em>我们介绍了它带来了什么，并使用我们自己的视频进行了快速试驾。让我们开始吧！</p><blockquote class="mg mh mi"><p id="fba6" class="ku kv mf kw b kx ky ju kz la lb jx lc mj le lf lg mk li lj lk ml lm ln lo lp im bi translated"><a class="ae mm" href="https://mailchi.mp/d2d2d4a109b5/learning-rate-newsletter" rel="noopener ugc nofollow" target="_blank"> Learning Rate </a>是为那些对AI和MLOps的世界感到好奇的人准备的时事通讯。你会在每周五收到我关于最新人工智能新闻和文章的更新和想法。订阅<a class="ae mm" href="https://mailchi.mp/d2d2d4a109b5/learning-rate-newsletter" rel="noopener ugc nofollow" target="_blank">这里</a>！</p></blockquote></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="8941" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">什么是VFI？</h1><p id="af52" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated"><strong class="kw iu">视频帧内插(VFI)的最终目的是提高帧速率，增强视频的视觉质量。</strong>为了实现这一点，它试图合成两个连续帧之间的中间图像，这两个连续帧在空间和时间上都与现有上下文一致。</p><p id="fefe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在其核心，我们发现另一个视觉任务命名为光流。所以，让我们快速绕一圈，看看什么是光流，以及它如何帮助我们处理VFI。然后，我们将准备好观看<em class="mf"> RIFE </em>的运行，并使用我们自己的视频来玩它。</p><h2 id="ec0b" class="nr mv it bd mw ns nt dn na nu nv dp ne ld nw nx ng lh ny nz ni ll oa ob nk oc bi translated">光流</h2><p id="4943" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated"><strong class="kw iu">广义而言，光流是理解事物在像素级图像中如何运动。</strong>这对于许多下游视频编辑任务非常有用，例如视频分析、图像稳定和视频对齐。</p><p id="022b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一幅图像由许多像素组成，当我们移动相机时，这些像素会发生某种变化。光流试图找出这种运动是如何影响每个像素的。所以，<strong class="kw iu">我们试图给图像中的每个像素附加一个运动矢量，指示这个像素要移动到哪里。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">光流—计算机爱好者</p></figure><p id="e5cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些信息是理解图像中正在发生的事情、物体如何移动以及移动方向的关键。然后，下一步是在这些像素变化之间生成合理的帧。这就是VFI发挥作用的地方。</p><h1 id="3107" class="mu mv it bd mw mx of mz na nb og nd ne jz oh ka ng kc oi kd ni kf oj kg nk nl bi translated">RIFE简介</h1><p id="8249" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">我们看到了什么是VFI，以及它如何提高视频的视觉质量。我们还看到了什么是光流，为什么它被认为是VFI的核心元素，以及它如何帮助我们解决这个问题。现在，是时候介绍RIFE了。</p><p id="5d73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">RIFE使用一个名为IFNet的神经网络，可以直接从图像中估计中间流。这项新工作的主要贡献总结如下:</p><ul class=""><li id="2049" class="ok ol it kw b kx ky la lb ld om lh on ll oo lp op oq or os bi translated">设计一种新颖高效的网络架构来简化基于流的VFI方法。因此，给定两个输入帧，所提出的模型可以从头开始训练，并直接逼近中间流。</li><li id="343b" class="ok ol it kw b kx ot la ou ld ov lh ow ll ox lp op oq or os bi translated">一个新的泄漏蒸馏损失函数，它导致更稳定的收敛和大的性能改善。</li><li id="ad24" class="ok ol it kw b kx ot la ou ld ov lh ow ll ox lp op oq or os bi translated">首款基于流的实时VFI算法，能够以30FPS的速度处理720p视频</li></ul><p id="2dff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们可以看到一些用RIFE处理的视频的例子。左边是原始视频，右边是增强版。看看处理后的版本有多平滑。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oe l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">RIFE:视频帧插值</p></figure><p id="7311" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可以阅读ArXiv上的<a class="ae mm" href="https://arxiv.org/pdf/2011.06294.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>并浏览该项目的<a class="ae mm" href="https://github.com/megvii-research/ECCV2022-RIFE" rel="noopener ugc nofollow" target="_blank">页面</a>了解更多细节。让我们进入有趣的部分:在我们自己的视频上使用RIFE！</p><h1 id="2db0" class="mu mv it bd mw mx of mz na nb og nd ne jz oh ka ng kc oi kd ni kf oj kg nk nl bi translated">辅导的</h1><p id="9ad1" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">在我们自己的视频上使用RIFE真的很容易。然而，如果我们真的想让它发光，我们需要一个Nvidia GPU。为此，我们用谷歌Colab吧！</p><p id="c8d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">去<a class="ae mm" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">colab.research.google.com</a>用GPU运行时创建一个新的笔记本。如果您以前没有这样做过，从<code class="fe oy oz pa pb b">Runtime</code>菜单中选择<code class="fe oy oz pa pb b">Change Runtime Type</code>，然后选择<code class="fe oy oz pa pb b">GPU</code>。</p><p id="1d1a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们准备克隆RIFE repo。在单元格中，键入以下内容:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="4b03" class="nr mv it pb b gy pg ph l pi pj">!git clone <a class="ae mm" href="https://github.com/hzwer/arXiv2020-RIFE" rel="noopener ugc nofollow" target="_blank">https://github.com/hzwer/arXiv2020-RIFE</a></span></pre><p id="93f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，在目录中创建一个名为<code class="fe oy oz pa pb b">train_log</code>和<code class="fe oy oz pa pb b">cd</code>的新文件夹:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="4089" class="nr mv it pb b gy pg ph l pi pj">!mkdir /content/arXiv2020-RIFE/train_log<br/>%cd /content/arXiv2020-RIFE/train_log</span></pre><p id="1f48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下载预先训练好的模型，为视频帧生成预测。为了实现这一点，我们使用了一个Python模块，使得从Google Drive下载对象变得容易:<code class="fe oy oz pa pb b">gdown</code></p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="9d84" class="nr mv it pb b gy pg ph l pi pj">!gdown --id 1APIzVeI-4ZZCEuIRE1m6WYfSCaOsi_7_<br/>!7z e RIFE_trained_model_v3.6.zip</span></pre><p id="6419" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们应该安装项目的依赖项。<code class="fe oy oz pa pb b">cd</code>到项目的文件夹中，使用<code class="fe oy oz pa pb b">pip</code>安装必要的包:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="abe9" class="nr mv it pb b gy pg ph l pi pj">%cd /content/arXiv2020-RIFE/<br/>!pip3 install -r requirements.txt</span></pre><p id="3a23" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您想快速尝试该模型，请下载作者提供的<code class="fe oy oz pa pb b">demo.mp4</code>视频:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="5cda" class="nr mv it pb b gy pg ph l pi pj">!gdown --id 1i3xlKb7ax7Y70khcTcuePi6E7crO_dFc</span></pre><p id="f55d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行下面的脚本为<code class="fe oy oz pa pb b">demo.mp4</code>生成一个增强的视频:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="c8a4" class="nr mv it pb b gy pg ph l pi pj">!python3 inference_video.py --exp=2 --video=demo.mp4 --montage</span></pre><p id="87b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，如果您想在自己的视频上使用该模型，只需更改<code class="fe oy oz pa pb b">--video</code>标志指向的视频的名称:</p><pre class="kj kk kl km gt pc pb pd pe aw pf bi"><span id="2742" class="nr mv it pb b gy pg ph l pi pj">!python3 inference_video.py --exp=2 --video=mydemo.mp4 --montage</span></pre><p id="d028" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个脚本将使视频中的帧数增加四倍。输出将存储在相同的位置，其名称将类似于<code class="fe oy oz pa pb b">mydemo_4X_120fps.mp4</code>。恭喜你！您的新的、增强的视频现在准备好了！</p><p id="04ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意，上传带音频的视频的时候好像有个bug。为了获得最佳体验，请使用没有音频的视频，或者尝试使用<code class="fe oy oz pa pb b">--fps</code>标志，直到问题得到解决。</p><h1 id="15c1" class="mu mv it bd mw mx of mz na nb og nd ne jz oh ka ng kc oi kd ni kf oj kg nk nl bi translated">结论</h1><p id="1abb" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">在这个故事中，我们看到了什么是光流，以及它与视频帧插值的关系。我们研究了这两种方法的目标，以及一种新技术RIFE:VFI实时中间流量估计如何用于生成令人印象深刻的新内容。</p><p id="391e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们玩了RIFE和我们自己的视频。我把最好的部分留到了最后:一个新的Windows应用程序已经使用RIFE为任何GPU提供快速视频插值:<a class="ae mm" href="https://nmkd.itch.io/flowframes" rel="noopener ugc nofollow" target="_blank"> Flowframes </a>。然而，使用python，您可以在任何支持GPU的机器上获得RIFE的好处！</p><h1 id="b701" class="mu mv it bd mw mx of mz na nb og nd ne jz oh ka ng kc oi kd ni kf oj kg nk nl bi translated">关于作者</h1><p id="f5fb" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">我叫<a class="ae mm" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> Dimitris Poulopoulos </a>，我是为<a class="ae mm" href="https://www.arrikto.com/" rel="noopener ugc nofollow" target="_blank"> Arrikto </a>工作的机器学习工程师。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲央行、经合组织和宜家等主要客户设计和实施过人工智能和软件解决方案。</p><p id="0e97" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的帖子，请关注我的<a class="ae mm" href="medium.com/@dpoulopoulos/follow" rel="noopener ugc nofollow" target="_blank"> Medium </a>、<a class="ae mm" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或Twitter上的<a class="ae mm" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"> @james2pl </a>。</p><p id="1f7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。</p></div></div>    
</body>
</html>