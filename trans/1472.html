<html>
<head>
<title>My Three Go-To Outlier Detection Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的三种离群点检测方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-three-go-to-outlier-detection-methods-49d74dc3fc29#2022-04-11">https://towardsdatascience.com/my-three-go-to-outlier-detection-methods-49d74dc3fc29#2022-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">异常值检测至关重要，尤其是对于数据质量评估</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/1ec71075deda10c5589f9754b30a3326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*iWZ-E-zIQoaDp9Sc"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">来自<a class="ae ku" href="https://www.pexels.com/ko-kr/photo/5935788/" rel="noopener ugc nofollow" target="_blank">像素</a>的免费使用照片</p></figure><h1 id="4e62" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="c6d4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">离群点检测在许多不同的方面都是至关重要的。如果一家公司想要了解异常/非典型的客户行为，它需要首先识别这样的客户。离群点检测技术在这种情况下发挥了作用。离群点检测对于检查数据集的数据质量也非常有用。在这里，我们来看看常用于检测异常值的三种主流方法。</p><h1 id="985b" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">IQR /标准差方法</h1><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="ca8f" class="mo kw it mk b gy mp mq l mr ms">Outliers are defined as:</span><span id="7494" class="mo kw it mk b gy mt mq l mr ms">Points that are &lt; mean/median - n * IQR/Std or &gt; mean/median + n * IQR/Std</span></pre><p id="3784" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">IQR代表四分位距。要理解这个概念，首先需要知道四分位数是什么意思。在统计学中，四分位数是将数据分成四份的值，因此自然会有四个四分位数。[1]它们中的每一个通常被表示为Q1、Q2、Q3和Q4。四分位范围(IQR)是数据集的中间五十，可表示为Q3- Q1。[2]它经常被用来代替标准偏差(Std)来衡量分布和变化，因为后者更不稳定，对异常值更敏感。</p><p id="c87d" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">使用IQR和标准偏差检测异常值的方法非常简单，位于n倍IQR或标准偏差定义的特定范围之外的数据点可以被视为异常值。但是，请注意，这种方法对于单变量类型的数据是有效的。</p><p id="9774" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">记住高斯分布的一个特性。偏离平均值3个标准偏差之外的点仅占分布的1%。这意味着，与大多数其他点相比，构成1%分布的那些点是非典型的，并且可能是异常值。当然，现实世界中的数据很少是完美的高斯分布，但是这个更大的概念仍然成立，即远离平均值或中值的点很可能是异常值。</p><p id="f909" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">没有用于设置阈值n设置规则。这取决于异常值检测的目的以及用户希望异常值检测是保守的还是全面的。因此，在一些测试数据上修改阈值将是一个好主意。</p><h1 id="6456" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">k表示聚类</h1><p id="33ec" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">k表示聚类是数据科学领域中使用的最经典、最传统的聚类方法之一。在这里，我不会深入讨论聚类算法本身是如何工作的。请回顾<a class="ae ku" rel="noopener" target="_blank" href="/k-means-clustering-explained-4528df86a120">一篇</a>解释这种算法如何工作的文章。[3]</p><p id="d2ab" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">尽管是一个聚类算法，它也可以用于离群点检测！</p><p id="e90e" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">一种方法是将簇的数量设置为K = 1。然后，质心将成为数据中所有点的平均值。然后，计算所有点的欧几里德距离。根据要标记为异常值的点的数量，可以选择距离质心最远的前n个点。k意味着可以通过Python的scikit-learn库轻松实现集群。参考下面的示例代码，假设存储在变量df中的数据有两个数字列V1和V2。(如果包括分类变量，记得对变量进行编码)。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="1efc" class="mo kw it mk b gy mp mq l mr ms"><strong class="mk iu">import </strong>pandas <strong class="mk iu">as </strong>pd<br/><strong class="mk iu">import </strong>numpy <strong class="mk iu">as </strong>np<br/><strong class="mk iu">from</strong> sklearn.cluster <strong class="mk iu">import</strong> KMeans</span><span id="9278" class="mo kw it mk b gy mt mq l mr ms">### Assume you already read in data in pandas in the variable called df (with two numerical columns V1 and V2)</span><span id="41fa" class="mo kw it mk b gy mt mq l mr ms">X = df.to_numpy() # change the dataframe to numpy matrix</span><span id="727c" class="mo kw it mk b gy mt mq l mr ms">kmeans <strong class="mk iu">=</strong> KMeans(n_clusters<strong class="mk iu">=</strong>1)<br/>kmeans<strong class="mk iu">.</strong>fit(X)<br/>distances <strong class="mk iu">=</strong> kmeans<strong class="mk iu">.</strong>transform(X) # apply kmeans on data</span><span id="b60e" class="mo kw it mk b gy mt mq l mr ms"># If you want to flag 50 points as outliers, grab indexes flagged as outliers<br/>sorted_idx <strong class="mk iu">=</strong> np<strong class="mk iu">.</strong>argsort(distances<strong class="mk iu">.</strong>ravel())[::<strong class="mk iu">-</strong>1][:50]</span></pre><p id="2cd0" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">另一种方法是使用k &gt; 1个聚类，并标记最小大小的聚类中的所有点。当然，您必须首先通过使用elbow方法或剪影评分方法来确定最佳聚类数，我不会在这里详细介绍这两种方法。</p><h1 id="ce11" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">隔离森林</h1><p id="ad2c" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">使用尽可能少的技术术语的隔离森林是一种算法，它不使用随机森林进行预测，而是如其名称所示“隔离”点。</p><p id="62dd" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">主要思想如下-它试图“通过随机选择一个特征，然后随机选择所选特征的最大值和最小值之间的分割值来隔离观察值。”[4]我们把这种隔离过程称为“分割”。然后，每一轮划分可以被认为是“一棵随机树”，然后所有这些树的集合将是“随机森林”。分离一个样本所需的分裂数就是“树的深度”或“从根到终端节点的路径长度”。然后，在随机森林中的所有树上对该路径长度进行平均，并成为算法的度量。该算法假设使用相对较少数量的分区更容易隔离离群点(因此平均而言隔离在树的较浅深度)。</p><p id="2818" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">请参考再次使用scikit学习包的隔离森林的Python实现示例。</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="aefb" class="mo kw it mk b gy mp mq l mr ms"><strong class="mk iu">import </strong>pandas <strong class="mk iu">as </strong>pd<br/><strong class="mk iu">import </strong>numpy <strong class="mk iu">as </strong>np<br/><strong class="mk iu">from</strong> sklearn.ensemble <strong class="mk iu">import</strong> IsolationForest</span><span id="ff58" class="mo kw it mk b gy mt mq l mr ms">data = df[['V1','V2','V3']] # assume there is data with three numerical columns V1, V2, and V3</span><span id="b5f9" class="mo kw it mk b gy mt mq l mr ms">min_max_scaler = preprocessing.StandardScaler()</span><span id="a6ca" class="mo kw it mk b gy mt mq l mr ms">data_scaled = min_max_scaler.fit_transform(data)</span><span id="a5dd" class="mo kw it mk b gy mt mq l mr ms"><em class="mz"># train isolation forest<br/>outliers_fraction = 0.2 # you can set how much would be flagged as outliers</em></span><span id="dc8d" class="mo kw it mk b gy mt mq l mr ms">model =  IsolationForest(contamination = outliers_fraction)</span><span id="cb81" class="mo kw it mk b gy mt mq l mr ms">model.fit(data)</span><span id="2c46" class="mo kw it mk b gy mt mq l mr ms"><em class="mz"># add the anomaly flags to the data</em></span><span id="4808" class="mo kw it mk b gy mt mq l mr ms">data['anomaly'] = pd.Series(model.predict(data_scaled))</span><span id="3c75" class="mo kw it mk b gy mt mq l mr ms"># Flagged outlier points are labelled as -1 and non-outlier points are labelled as 1 and so relabel them as binary outcomes (1 and 0) </span><span id="a965" class="mo kw it mk b gy mt mq l mr ms">data['anomaly'] = data['anomaly'].map( {1: 0, -1: 1} )</span></pre><p id="e3bf" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">还有其他多种异常检测算法，包括DBSCAN、局部异常因子(LOF)等。我希望在其他一些帖子中讨论这个问题！如果你感兴趣，请关注我并订阅我的帖子: )</p><h1 id="063d" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">参考</h1><p id="1deb" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">[1]统计学如何，<a class="ae ku" href="https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/what-are-quartiles/" rel="noopener ugc nofollow" target="_blank">什么是四分位数？定义</a></p><p id="beee" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">[2]统计学如何，<a class="ae ku" href="https://www.statisticshowto.com/probability-and-statistics/interquartile-range/" rel="noopener ugc nofollow" target="_blank">四分位距(IQR):它是什么以及如何找到它</a></p><p id="f6d9" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">[3] S .耶尔德勒姆，<a class="ae ku" rel="noopener" target="_blank" href="/k-means-clustering-explained-4528df86a120">走向数据科学，K-均值聚类—解释(2020) </a></p><p id="bcad" class="pw-post-body-paragraph ln lo it lp b lq mu ju ls lt mv jx lv lw mw ly lz ma mx mc md me my mg mh mi im bi translated">[4] <a class="ae ku" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank"> Python Scikit学习文档</a></p><h1 id="7bff" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">关于作者</h1><p id="85a0" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><em class="mz">数据科学家。在密歇根大学刑事司法行政记录系统(CJARS)经济学实验室担任副研究员。Spotify前数据科学实习生。Inc .(纽约市)。即将入学的信息学博士生。他喜欢运动，健身，烹饪美味的亚洲食物，看kdramas和制作/表演音乐，最重要的是崇拜耶稣基督。结账他的</em> <a class="ae ku" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="mz">网站</em> </a> <em class="mz">！</em></p></div></div>    
</body>
</html>