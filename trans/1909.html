<html>
<head>
<title>Named Entity Recognition with BERT in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中基于BERT的命名实体识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a#2022-05-03">https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a#2022-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b905" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何利用定制数据的预训练BERT模型来预测文本中每个单词的实体</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/807a90ea4514e288b15fef39aa1587b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3jFHHVU7M-YJqHPrCIW7w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@aaronburden?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在<a class="ae ky" href="https://unsplash.com/s/photos/dictionary-highlight?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="5b95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当涉及到处理NLP问题时，BERT经常作为一个机器学习模型出现，我们可以依靠它的性能。事实上，它已经预先训练了超过2500万个单词，它从一系列单词中学习信息的双向性质使它成为一个强大的模型。</p><p id="b8a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我以前写过如何利用BERT进行文本分类，在本文中，我们将更多地关注如何使用BERT进行命名实体识别(NER)任务。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4526" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">什么是NER？</h1><p id="13af" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">NER是自然语言处理中的一项任务，用于识别和提取句子或文本中有意义的信息(或者我们可以称之为实体)。一个实体可以是一个单词，甚至是指同一类别的一组词。</p><p id="63f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，让我们说我们下面的句子，我们想从这个句子中提取关于一个人的名字的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/ec5990d493cd347f5915e74571ea23e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1R3_XyaUMQW2ChmIDb2vw.png"/></div></div></figure><p id="d4b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NER任务的第一步是检测实体。这可以是指同一类别的一个单词或一组单词。举个例子:</p><ul class=""><li id="ca44" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">➡️由一个单词组成的实体</li><li id="260a" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">由两个词组成的实体，但它们指的是同一个范畴。</li></ul><p id="109d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了确保我们的BERT模型知道实体可以是单个单词或一组单词，我们需要通过所谓的内部-外部-开始(IOB)标记在我们的训练数据上提供关于实体的开始和结束的信息。我们将在本文后面的数据集上看到更多这方面的内容。</p><p id="164c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检测到实体后，NER任务的下一步是对检测到的实体进行分类。实体的类别可以是任何东西，这取决于我们的用例。以下是实体类别的示例:</p><ul class=""><li id="ed71" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">人物:邦德、詹姆斯·邦德、山姆、安娜、弗兰克、莱昂纳多·迪卡普里奥</li><li id="3156" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><strong class="lb iu">地点</strong>:纽约、维也纳、慕尼黑、伦敦</li><li id="f08d" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><strong class="lb iu">组织</strong>:谷歌、苹果、斯坦福大学、德意志银行</li><li id="4460" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><strong class="lb iu">地点</strong>:中央公园，勃兰登堡门，时代广场</li></ul><p id="f5b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的BERT模型的训练过程中，这些实体基本上是我们的数据的标签，我们将在下面的部分中详细讨论。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d788" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">NER的伯特</h1><p id="c3ae" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如前所述，BERT是一个基于变形金刚的机器学习模型，如果我们想解决NLP相关的任务，它会非常方便。</p><p id="7145" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您还不熟悉BERT，我建议您在阅读本文之前先阅读我以前的一篇关于BERT文本分类的文章。在那里，您可以找到关于BERT实际上是什么、模型期望哪种输入数据以及您将从模型获得的输出的信息。</p><div class="np nq gp gr nr ns"><a rel="noopener follow" target="_blank" href="/text-classification-with-bert-in-pytorch-887965e5820f"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">PyTorch中基于BERT的文本分类</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">如何利用来自拥抱脸的预先训练的BERT模型来分类新闻文章的文本</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div><p id="6e9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于文本分类的BERT和NER问题之间的区别在于我们如何设置模型的输出。对于一个文本分类问题，我们只使用从特殊的<strong class="lb iu">【CLS】</strong>标记输出的嵌入向量，正如你在下面的可视化中看到的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/5f9ee647e33eac9eff5ad3f16384a9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARJzucvPAmzdaB_y3J3ngg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b97b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同时，如果我们想要使用BERT来完成NER任务，我们需要使用来自所有令牌的嵌入向量输出，正如您在下面的可视化中所看到的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/3f07e58c403ee74c565596b81bcc23e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98osfdBNVYyl-M_NtvuNyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dab0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用从所有标记输出的嵌入向量，我们可以在标记级别对文本进行分类。这正是我们想要的，因为我们希望我们的伯特模型预测每个令牌的实体。现在事不宜迟，我们去实现吧。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="681d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">关于数据集</h1><p id="118c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将在本文中使用的数据集是CoNLL-2003数据集，这是一个专门用于NER任务的数据集。你可以通过下面的链接下载Kaggle上的数据。</p><div class="np nq gp gr nr ns"><a href="https://www.kaggle.com/datasets/rajnathpatel/ner-data" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">NER数据</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">命名实体识别数据</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">www.kaggle.com</p></div></div><div class="ob l"><div class="oi l od oe of ob og ks ns"/></div></div></a></div><p id="279b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据集是在开放数据库1.0版许可下分发的，因此我们可以出于自己的目的自由共享和使用这个数据集。现在让我们看看数据集是什么样子的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2a0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在上面看到的，我们有一个由文本和标签组成的数据框架。标签对应于文本中每个单词的实体类别。</p><p id="908f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总共有9个实体类别，它们是:</p><ul class=""><li id="ab5b" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><code class="fe ol om on oo b">geo</code>为地理实体</li><li id="e417" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">org</code>为组织实体</li><li id="c91c" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">per</code>为人身实体</li><li id="e7aa" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">gpe</code>对于地缘政治实体</li><li id="b143" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">tim</code>为时间指示实体</li><li id="1bc7" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">art</code>为工件实体</li><li id="5154" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">eve</code>为事件实体</li><li id="2cb8" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">nat</code>为自然现象实体</li><li id="ddeb" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">如果单词不属于任何实体，则分配<code class="fe ol om on oo b">O</code>。</li></ul><p id="0ead" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看数据集中可用的唯一标签:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="e75b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会注意到，每个实体类别前面都有字母<code class="fe ol om on oo b">I</code>或<code class="fe ol om on oo b">B</code>。这对应于前面提到的IOB标记。<code class="fe ol om on oo b">I</code>表示<strong class="lb iu"> <em class="nj">中间</em></strong><code class="fe ol om on oo b">B</code>表示<strong class="lb iu"> <em class="nj">开始</em> </strong>。让我们来看看下面的句子，以便更好地理解IOB标记的概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/3f889bb6779301b664ec991fdd9e72b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AujtffwNyoychjkHOiYHQ.png"/></div></div></figure><ul class=""><li id="e185" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">“凯文”有<code class="fe ol om on oo b">B-pers</code>的标签，因为它是一个人实体的开始</li><li id="bce9" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">“杜兰特”有<code class="fe ol om on oo b">I-pers</code>的标签，因为它是一个人实体的延续</li><li id="4505" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">‘布鲁克林’有<code class="fe ol om on oo b">B-org</code>因为它是一个组织实体的开始</li><li id="f270" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">“网络”有<code class="fe ol om on oo b">I-org</code>的标签，因为它是一个组织实体的延续</li><li id="8839" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">其他单词被分配<code class="fe ol om on oo b">O</code>标签，因为它们不属于任何实体</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6a11" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据预处理</h1><p id="9184" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">当然，在我们能够使用BERT模型对标记的实体进行分类之前，我们需要首先进行数据预处理，这包括两个部分:标记化和调整标签以匹配标记化。先从标记化说起。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="099f" class="oq md it bd me or os dn mi ot ou dp mm li ov ow mo lm ox oy mq lq oz pa ms pb bi translated">标记化</h2><p id="a02c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">使用BERT可以很容易地实现标记化，因为我们可以使用带有HuggingFace的预训练BERT基础模型中的<code class="fe ol om on oo b">BertTokenizerFast</code>类。</p><p id="67bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了举例说明BERT tokenizer是如何工作的，让我们来看看数据集中的一个文本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2a8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用<code class="fe ol om on oo b">BertTokenizerFast</code>标记上面的文本非常简单:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当从上面的<code class="fe ol om on oo b">BertTokenizerFast</code>类调用<code class="fe ol om on oo b">tokenizer</code>方法时，我们提供了几个参数:</p><ul class=""><li id="ac54" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><code class="fe ol om on oo b">padding</code>:用一个特殊的<strong class="lb iu">【PAD】</strong>标记将序列填充到我们指定的最大长度。BERT模型的最大序列长度为512。</li><li id="583b" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">max_length</code>:序列的最大长度。</li><li id="bc50" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">truncation</code>:这是一个布尔值。如果我们将该值设置为True，那么将不会使用超过最大长度的令牌。</li><li id="6376" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">return_tensors</code>:返回的张量类型，取决于我们使用的机器学习框架。既然我们使用PyTorch，那么我们使用<code class="fe ol om on oo b">pt</code>。</li></ul><p id="3bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是标记化过程的输出:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="b299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我们从标记化过程中获得的输出是一个字典，其中包含三个变量:</p><ul class=""><li id="9787" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><code class="fe ol om on oo b">input_ids</code>:序列中记号的id表示。在BERT中，id 101为特殊<strong class="lb iu">【CLS】</strong>令牌保留，id 102为特殊<strong class="lb iu">【SEP】</strong>令牌保留，id 0为<strong class="lb iu">【PAD】</strong>令牌保留。</li><li id="7c58" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">token_type_ids</code>:标识令牌所属的序列。因为每个文本只有一个序列，所以<code class="fe ol om on oo b">token_type_ids</code>的所有值都是0。</li><li id="09ee" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated"><code class="fe ol om on oo b">attention_mask</code>:识别令牌是真令牌还是填充符。如果它是一个真实的令牌，则该值为1，如果它是一个<strong class="lb iu">【PAD】</strong>令牌，则该值为0。</li></ul><p id="3120" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的<code class="fe ol om on oo b">input_ids</code>中，我们可以用下面的<code class="fe ol om on oo b">decode</code>方法将ids解码回原始序列:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="df5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实现了<code class="fe ol om on oo b">decode</code>方法之后，我们恢复了原来的序列，增加了来自BERT的特殊标记，例如序列开头的<strong class="lb iu">【CLS】</strong>标记，序列结尾的<strong class="lb iu">【SEP】</strong>标记，以及一堆<strong class="lb iu">【PAD】</strong>标记，以满足所需的最大长度512。</p><p id="a956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个标记化过程之后，我们需要进行下一步，即调整每个标记的标签。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="aa91" class="oq md it bd me or os dn mi ot ou dp mm li ov ow mo lm ox oy mq lq oz pa ms pb bi translated">标记化后调整标签</h2><p id="c277" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这是我们在标记化过程之后需要做的一个非常重要的步骤。这是因为在标记化过程之后，序列的长度不再匹配原始标签的长度。</p><p id="52b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT记号赋予器使用所谓的词块记号赋予器，它是一个子词记号赋予器。这意味着BERT tokenizer可能会将一个单词拆分成一个或多个有意义的子单词。</p><p id="5010" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设我们有以下序列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/3527664dc12d3b9abbaaefe5d9f31f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n66Vk78mSHXrxh45b3xlSw.png"/></div></div></figure><p id="f973" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的序列总共有13个标记，因此它也有13个标签。然而，在BERT标记化之后，我们得到以下结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="b179" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在令牌化过程之后，我们需要解决两个问题:</p><ul class=""><li id="1951" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">增加了来自BERT的特殊令牌，如<strong class="lb iu">【CLS】</strong><strong class="lb iu">【SEP】</strong><strong class="lb iu">【PAD】</strong></li><li id="a9eb" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu nf ng nh ni bi translated">一些记号被分裂成子单词的事实。</li></ul><p id="65af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为子词标记化，词块标记化将生僻字拆分成它们的子词，例如上面示例中的'<em class="nj"> Geir </em>'和'<em class="nj"> Haarde </em>'。这种子词标记化有助于BERT模型学习相关词的语义。</p><p id="ecbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种单词片段标记化和添加来自BERT的特殊标记的结果是标记化后的序列长度不再匹配初始标签的长度。</p><p id="2c11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的例子中，现在在标记化之后，序列中总共有512个标记，而标签的长度仍然与之前相同。此外，序列中的第一个标记不再是单词'<em class="nj"> Prime </em>，而是新添加的<strong class="lb iu">【CLS】</strong>标记，因此我们也需要移动我们的标签。</p><p id="0131" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这个问题，我们需要调整标签，使其与标记化后的序列具有相同的长度。为此，我们可以利用来自标记化结果的<code class="fe ol om on oo b">word_ids</code>方法，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7284" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的代码片段中可以看出，每个被拆分的令牌共享同一个<code class="fe ol om on oo b">word_ids</code>，其中来自BERT的特殊令牌如<strong class="lb iu">【CLS】</strong><strong class="lb iu">【SEP】</strong><strong class="lb iu">【PAD】</strong>都没有特定的<code class="fe ol om on oo b">word_ids</code>。</p><p id="55e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些<code class="fe ol om on oo b">word_ids</code>将非常有助于通过应用以下两种方法中的任何一种来调整标签的长度:</p><ol class=""><li id="c2a1" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu pd ng nh ni bi translated">我们只为每个分裂令牌的第一个子词提供一个标签。子词的延续将简单地用“-100”作为标签。所有没有<code class="fe ol om on oo b">word_ids</code>的代币也将标上“-100”。</li><li id="bd07" class="na nb it lb b lc nk lf nl li nm lm nn lq no lu pd ng nh ni bi translated">我们在属于同一单词的所有子单词中提供相同的标签。所有没有<code class="fe ol om on oo b">word_ids</code>的令牌都将被标记为“-100”。</li></ol><p id="30e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面代码片段中的函数将执行上面定义的步骤。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="a41e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果要应用第一种方法，将<code class="fe ol om on oo b">label_all_tokens</code>设置为假。如果您想应用第二种方法，请将<code class="fe ol om on oo b">label_all_tokens</code>设置为True，如下面的代码片段所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2f70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的其余部分，我们将实现第一种方法，在这种方法中，我们将只为每个令牌中的第一个子词提供一个标签，并将<code class="fe ol om on oo b">label_all_tokens</code>设置为False。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="80ff" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">数据集类</h1><p id="adb8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在我们为NER任务训练BERT模型之前，我们需要创建一个数据集类来批量生成和获取数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="9349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码片段中，我们用<code class="fe ol om on oo b">__init__</code>函数中的<code class="fe ol om on oo b">tokenizer</code>变量调用<code class="fe ol om on oo b">BertTokenizerFast</code>类来标记我们的输入文本，用<code class="fe ol om on oo b">align_label</code>函数在标记过程后调整我们的标签。</p><p id="7105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将数据随机分为训练、变异和测试。但是，请注意，数据总数是47959。因此，为了演示和加快培训过程，我将只选取其中的1000个。当然，你可以将所有的数据用于模型训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8e5f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">模型结构</h1><p id="a22e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们将使用来自HuggingFace的预训练BERT基本模型。既然我们要在标记级别对文本进行分类，那么我们需要使用<code class="fe ol om on oo b">BertForTokenClassification</code>类。</p><p id="767a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ol om on oo b">BertForTokenClassification</code> class是一个模型，它包装了BERT模型，并在BERT模型之上添加了线性层，这些层将充当令牌级分类器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c7ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码片段中，首先，我们实例化模型，并将每个令牌分类器的输出设置为数据集上唯一实体的数量，在我们的示例中是17。</p><p id="6cc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将为训练循环定义一个函数。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="0ff2" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">训练循环</h1><p id="05b7" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们的BERT模型的训练循环是标准PyTorch训练循环，增加了一些内容，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="356c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的训练循环中，我只训练了5个时期的模型，然后使用SGD作为优化器。每批的损耗计算已经由<code class="fe ol om on oo b">BertForTokenClassification</code>级负责。</p><p id="6ed4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练循环的每个时期，我们还需要做一个重要的步骤。在模型预测之后，我们需要忽略所有标签为'-100 '的标记，正如您在第36、37、62和63行中看到的。</p><p id="2e60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是我们训练5个时期的BERT模型后的训练输出示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/3446bf93a62a95dce6c27f7fea23a3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yK49KiF49AUezmqpomY6KA.png"/></div></div></figure><p id="6487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，当您训练自己的BERT模型时，您将看到的输出可能会有所不同，因为在训练过程中存在随机性。</p><p id="b97c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以做很多事情来提高我们模型的性能。如果你注意到了，我们有一个数据不平衡的问题，因为有很多带有“O”标签的令牌。例如，我们可以通过在训练过程中应用类权重来改进我们的模型。</p><p id="80a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，您可以尝试不同的优化器，如权重衰减正则化的Adam优化器。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8cae" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">根据测试数据评估模型</h1><p id="9fb2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">既然我们已经训练了我们的模型，我们可以用下面的代码片段来评估它在看不见的测试数据上的性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="80df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的例子中，经过训练的模型在测试集上达到了平均92.22%的准确率。当然，您可以将指标更改为F1得分、精确度或召回率。</p><p id="5a1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，我们可以使用经过训练的模型来预测文本或句子中每个单词的实体，代码如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="beaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一切都完美，那么我们的模型将能够相当好地预测一个看不见的句子的每个单词的实体，正如你在上面看到的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a414" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="a388" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在这篇文章中，我们实现了命名实体识别(NER)任务的伯特。这意味着我们已经训练了BERT模型来在标记级别预测定制文本或定制句子的IOB标记。</p><p id="2e46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章能帮助你开始使用NER任务的BERT。你可以在<a class="ae ky" href="https://github.com/marcellusruben/medium-resources/tree/main/NER_BERT" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这本笔记本</strong> </a>中找到本文实现的所有代码。</p></div></div>    
</body>
</html>