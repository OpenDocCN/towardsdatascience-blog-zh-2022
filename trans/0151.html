<html>
<head>
<title>Polynomial Regression — An Alternative For Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多项式回归——神经网络的替代方案？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-an-alternative-for-neural-networks-c4bd30fa6cf6#2022-02-07">https://towardsdatascience.com/polynomial-regression-an-alternative-for-neural-networks-c4bd30fa6cf6#2022-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5742" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多项式和神经网络的讨论，理论上它们都能无限逼近连续函数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5824160f1c8405510f56a77d96335c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*_ukNbg00_Zm5zK9y2g2aIg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用三种不同次数的多项式对数据集执行多项式回归的示例。理论上，多项式可以实现无限紧密的配合。[图片来自<a class="ae ku" href="https://upload.wikimedia.org/wikipedia/commons/9/90/Poly-reg-3.png" rel="noopener ugc nofollow" target="_blank">维基媒体</a>作者<a class="ae ku" href="https://commons.wikimedia.org/wiki/File:Poly-reg-3.png" rel="noopener ugc nofollow" target="_blank">肖巴</a></p></figure></div><div class="ab cl kv kw hx kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="im in io ip iq"><p id="9171" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">首先，这更像是一篇讨论文章。虽然我将尝试比较多项式和神经网络，但没有大量的数学严谨性或文献支持来支持这篇文章。它也不是教程:这次没有 Python 实现或数值实验(至少我自己没有)。最后，我也不是试图说服你放弃神经网络，接受多项式回归。</p><p id="f479" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">说完这些，让我们说说我<em class="ly">想用这篇文章做什么</em>。几乎每个人都在生活中的某个时候学会了设计多项式，他们拟合任何函数的能力在很久以前就被证明了。添加这两个应该会产生有趣的用例，但是对数据科学中的多项式回归的关注是相当不深刻的。如果这篇文章成功地引发了一些思考，并恢复了对古老的多项式设计艺术的关注，那就足够了。</p><h2 id="06f5" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">神经网络</h2><p id="dc8c" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">先说神经网络，以及我们为什么喜欢它。从数学的角度来看，神经网络的巨大吸引力在于它们可以无限逼近任何连续函数。原则上，即使是简单的单层网络也能完成这项任务。是的，有严格的理论要求，你可能永远无法在实践中达到完美的契合，但它仍然是一个令人放心的属性。</p><p id="a9e8" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">【提供一些</em> <strong class="le iu"> <em class="ly">泛逼近定理的背景</em></strong><em class="ly">:George Cybenko(1989)证明了具有 sigmoid 激活函数的单层网络可以无限逼近任何连续函数。Hornik 等人(1989)表明，该结果也适用于多层网络中的其他激活函数。从那时起，定理已经扩展到其他方向，例如，任意层宽度和深度，各种激活函数，非连续函数等。同样，这些结果不能提供任何实际的保证，因为条件可能是不现实的。】</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/e11d0084ea0e9d153798ad3e3616a8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QCNpM70r-fJy3Do2DvBnWA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">神经网络的示例。多个层通常捕获数据集的特定特征。[图片由<a class="ae ku" href="https://freesvg.org/nn1" rel="noopener ugc nofollow" target="_blank">open clipbart</a>在<a class="ae ku" href="https://freesvg.org/" rel="noopener ugc nofollow" target="_blank"> FreeSVG </a>上提供]</p></figure><p id="14a3" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我知道这不是对神经网络最详尽的描述，但是我们还有很多材料要介绍。我们继续吧。</p><h2 id="e394" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated"><strong class="ak">多项式</strong></h2><p id="e69d" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">另一类能够拟合几乎任何数据集的函数似乎几乎被遗忘了——至少在数据科学界——但与神经网络具有相似的属性(Stone-Weierstrass 定理)。正如你可能已经从标题中猜测到的，那就是多项式类<strong class="le iu"/>。</p><p id="cbed" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">很可能，你高中的时候有过一些多项式。然后，取决于你之后的方向，你可能研究了很多，也可能根本没有研究。无论如何，它们在当今的数据科学中似乎并不太受欢迎。</p><p id="c02c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了提供一个简短的复习，多项式被定义为:</p><blockquote class="my mz na"><p id="1070" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">由不定项(也称为变量)和系数组成的表达式，只涉及变量的<strong class="le iu">加</strong>、<strong class="le iu">减</strong>、<strong class="le iu">乘</strong>和非负整数<strong class="le iu">取幂</strong>运算—维基百科</p></blockquote><p id="b6b9" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">多项式的一个例子可能是:</p><p id="a864" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe ne nf ng nh b">f(x) = (x − 3)(x − 2)(x − 1)(x)(x + 1)(x + 2)(x + 3)</code></p><p id="37c5" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">相应的图如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/ecc7ef5a3b5a95c0b1402ac594f5d697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRKXQkGj8dHDpWNM7dzirA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">7 次多项式的示例。高次多项式可以拟合非常复杂的模式。[图片来自<a class="ae ku" href="https://commons.wikimedia.org/w/index.php?curid=25264233" rel="noopener ugc nofollow" target="_blank">维基媒体</a>作者<a class="ae ku" href="https://commons.wikimedia.org/wiki/User:Melikamp" rel="noopener ugc nofollow" target="_blank">梅利坎普</a></p></figure><p id="9ceb" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">请注意，多项式可以通过将各项相乘来写出。多项式总是可以改写成标准形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nn"><img src="../Images/59aa226540747bc9edb8ddb34ca037d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBYJszkIPNw--mklpXnbbw.png"/></div></div></figure><p id="4225" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">注意，虽然特征<code class="fe ne nf ng nh b">x^k, k&gt;1</code>是非线性的，但是作为统计估计问题<strong class="le iu">是线性的</strong>(特征的加权线性组合)，并且可以用基本的线性回归技术来解决。这是一个强大的属性，使生活变得容易得多。</p><p id="690d" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在数据科学中，我们通常处理多个特征(例如，一个以上的解释变量，比如说<code class="fe ne nf ng nh b">x_1, x_2, x_3</code>)，转化为一个<strong class="le iu">多变量多项式</strong>。例如，像这样的术语可能代表一个特征:</p><p id="ce0f" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe ne nf ng nh b">x_1x_2x_3²</code></p><p id="295f" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">或者如果你喜欢更具描述性的东西:</p><p id="51ea" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><code class="fe ne nf ng nh b">x_years_experience * x_salary_last_year * x_num_past_jobs²</code></p><p id="aa55" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这只是众多可能特征中的一个。可以想象，多项式很快变得相当混乱和复杂。两个方面决定了多项式的复杂性:</p><ul class=""><li id="27c7" class="no np it le b lf lg li lj ll nq lp nr lt ns lx nt nu nv nw bi translated"><strong class="le iu">变量的数量</strong>。表达式<code class="fe ne nf ng nh b">2x_1²x_2⁴ - 3x_1x_2² + x_1 + 8</code>包含两个解释变量，<code class="fe ne nf ng nh b">x_1</code>和<code class="fe ne nf ng nh b">x_2</code>。具有两个变量的多项式可以在 3D 绘图中可视化，在更高的维度上，我们人类的大脑往往会有所欠缺。</li><li id="5cfe" class="no np it le b lf nx li ny ll nz lp oa lt ob lx nt nu nv nw bi translated">多项式的<strong class="le iu">次</strong>。这是标准形式表达式中的最高幂(乘以变量时指数的总和)。例如，在<code class="fe ne nf ng nh b">2x_1²x_2⁴ - 3x_1x_2² + x_1 + 8</code>示例中，度数将是 6(即 2+4)。</li></ul><p id="ce6d" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">此时，您可能会发现一个潜在的问题。带有一个变量的 5 次多项式(如<code class="fe ne nf ng nh b">w_5x⁵ + w_4x⁴ + w_3x³ + w_2x² + w_1x + w_0</code>)似乎是可行的，带有五个变量的 1 次多项式(如<code class="fe ne nf ng nh b">w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_0</code>)也是可行的。</p><p id="d87b" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">但是，如果我们同时增加次数和变量的数量，特征的数量可能会爆炸。当我们转向像<code class="fe ne nf ng nh b">x_1⁴x_2x_3³x_4²</code>这样的装置时，我们意识到<em class="ly">有很多</em>的<strong class="le iu">高度多变量特征</strong>我们可以设计来捕捉复杂的交互。首先，我们不知道哪些特性是相关的——不管我们有多少领域知识。在精神上，我们可以概念化基本的相互作用和二次效应，但除此之外就没什么了。</p><p id="4418" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">总之，我们有两个问题要处理:(1)包括哪些特征，以及(2)我们应该在多大程度上捕捉相关效应。在面对这些挑战之前，让我们暂时回到神经网络。</p><h2 id="63a9" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">多项式和神经网络之间的联系</h2><p id="1c44" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">我们很少费心以数学形式明确写出神经网络，但我们可以。本质上，每个隐藏节点将前一层的加权组合作为输入，根据某个激活函数(例如，sigmoid 或 ReLU)对其进行变换，并将输出传递给下一层。</p><p id="393b" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">就像一个高次多项式取一些初始解释变量<code class="fe ne nf ng nh b">x_1,x_2,x_3</code>并嵌入到类似<code class="fe ne nf ng nh b">x_1²x_2x_3⁴</code>的复杂特征中一样，一个神经网络也转换输入并隐式提取特征。你看这是怎么回事。根据加州大学和斯坦福大学的研究人员(Cheng et al .，2021)，<strong class="le iu">神经网络实际上是一类特殊的多项式</strong>。每一层都增加了多项式的复杂性。</p><blockquote class="my mz na"><p id="4204" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">如果激活函数是任何多项式，或者由一个多项式实现，则 NN 精确地执行多项式回归。</p><p id="44ba" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">而且，多项式的次数会逐层递增。”—程等，2019</p></blockquote><p id="a796" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这是一个迷人的结果。事实证明，我们并没有真正讨论对立的解决方法。如果有什么不同的话，神经网络为多项式回归的总体解决方案类增加了特定的角度和概念化。</p><p id="a12c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了避免问题过于复杂:在剩余部分，我将继续使用术语<strong class="le iu">多项式</strong>表示典型的多项式回归，术语<strong class="le iu">神经网络</strong>表示神经网络。</p><p id="67fa" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">【程等论文注:仅讨论简单人工神经网络；特殊口味，如循环、通用、卷积等。不包括在内。同样值得指出的是，这篇论文没有提供严格的数学证明。最后，到目前为止，这份手稿还没有被同行评审或发表。] </em></p><h2 id="4e68" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">多项式回归-特征选择和数据拟合</h2><p id="d004" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">回到多项式。我们现在知道了它们的样子，但是我们如何在数据科学环境中使用它们呢？为了解决这个问题，让我们假设我们使用一个给定的数据集来构建一个既适合训练数据又能很好地解释未知数据的模型。</p><p id="cf72" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">虽然有很多方法可以将变量组合成高级术语，但是现在<strong class="le iu">创建特性</strong>并不太复杂。虽然我不会在这里深入探讨，但是类似于<code class="fe ne nf ng nh b">sklearn.PolynomialFeatures</code>的东西可以在一定程度上生成所有的特性。例如，对于两个变量和 2 度，我们将生成<code class="fe ne nf ng nh b">[1, x_1, x_2, x_1², x_1x_2, x_2²]</code>。</p><p id="ee03" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le iu">拟合多项式</strong>本质上也不具有挑战性。例如，<code class="fe ne nf ng nh b">numpy.Polynomial</code>功能包含许多常见的多项式(切比雪夫多项式、埃尔米特多项式、勒让德多项式等。)并且拟合是在一行代码中完成的:</p><pre class="kj kk kl km gt oc nh od oe aw of bi"><span id="cbcc" class="lz ma it nh b gy og oh l oi oj">output = <!-- -->Polynomial<!-- -->.fit(x_data, y_data, deg=3)</span></pre><p id="5fa9" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">真正的挑战在于找到正确的<strong class="le iu">特征子集</strong>。也许<code class="fe ne nf ng nh b">x_1⁷x_2³x_3²</code>证明了一个深刻的预测，或者也许<code class="fe ne nf ng nh b">x_1⁵x_2⁴x_3²</code>是。即使对于更合理的程度，也经常存在如此多的可能性，以至于对所有模型(即，特征的组合)进行穷举搜索是不可行的。</p><p id="587e" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">一些技术可能有助于处理大量特征，例如:</p><ul class=""><li id="bbe2" class="no np it le b lf lg li lj ll nq lp nr lt ns lx nt nu nv nw bi translated"><strong class="le iu">主成分分析</strong> (PCA)以在设计多项式之前降低特征集的维数；</li><li id="fd6e" class="no np it le b lf nx li ny ll nz lp oa lt ob lx nt nu nv nw bi translated"><strong class="le iu">卡方检验</strong>检验特征在预测结果中的显著性；</li><li id="3edd" class="no np it le b lf nx li ny ll nz lp oa lt ob lx nt nu nv nw bi translated"><strong class="le iu">决策树</strong>(或随机森林)选择成功特征；</li><li id="c0ae" class="no np it le b lf nx li ny ll nz lp oa lt ob lx nt nu nv nw bi translated"><strong class="le iu">弹性网回归</strong>(结合套索和脊线)剔除低显著性特征。</li></ul><p id="c1b2" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我承认:不一定容易。然而，请记住，神经网络训练中的认真努力还需要特征选择、架构选择和超参数调整。这两种解决方法都不会立即产生一个完美的模型。</p><p id="716a" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">另一个挑战在于<strong class="le iu">过度拟合</strong>。一个 20 次多项式可能捕捉到许多特殊的效果、异常值等等。尽管我们不一定想要对训练数据有一个完美的拟合；我们希望这个模型能够很好地处理以前没有见过的数据。前述的<em class="ly">弹性网</em>有所帮助，避免高次多项式也是如此。与神经网络不同——神经网络也容易过度拟合——对于多项式回归，我们可以计算<em class="ly">方差膨胀因子— </em>,因为我们仍在执行普通最小二乘(OLS)回归——当我们即将在模型中引入<strong class="le iu">多重共线性</strong>时会发出警告。</p><p id="3ae7" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">总之:我们有所有必要的工具来将复杂的多项式拟合到复杂的数据集中。这并不是说多项式是数据科学的灵丹妙药。这并不是说拟合一个合适的多项式将是一件轻而易举的事。它绝对没有说我们会找到理论上可以达到的完美匹配。这也不是令人难以置信的黯淡前景。</p><h2 id="12e1" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">多项式和神经网络的优势</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ok"><img src="../Images/49c9ec99235899cfa35d2bf020194a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7nlikHVuspvNANU8"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">神经网络的最大好处之一是它们可以有效地捕捉层次结构。[照片由<a class="ae ku" href="https://unsplash.com/@eugene_tkachenko?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尤金·特卡琴科</a>在<a class="ae ku" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄]</p></figure><p id="22e7" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">多项式的起源可以追溯到几个世纪以前；到目前为止，它们已经被很好地研究和理解了。显然，它们确实有一些缺点——否则，一开始就没有人会费心研究新技术。神经网络在该领域赢得了应有的地位。</p><p id="ec47" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">也许神经网络最大的吸引力在于它们捕捉层级效应的能力。这是他们在计算机视觉上工作得如此好的原因之一，特定的层专用于大形状、小形状、颜色等。由于分层架构，他们通常能够<strong class="le iu">很好地概括</strong>，引入处理新数据所需的<em class="ly">归纳偏差</em>。</p><p id="ec28" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">此外，神经网络在许多方面都便于使用。是的，当网络变得更先进时，你必须研究层和节点的数量、学习速率和额外的超参数。找到合适的架构需要时间和精力。</p><p id="1d7c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然而，就<strong class="le iu">功能设计</strong>而言，很多工作是从你手中拿走的。基本上，你输入一些解释变量，网络就会自己计算出相关的相互作用和高阶效应。有了所有可用的库和复杂的<strong class="le iu">梯度下降算法</strong>，定制一个神经网络是完全可行的。</p><p id="e12f" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">相比之下，对于<em class="ly">多项式回归</em>，<strong class="le iu">训练本身是直接的</strong>(基本回归)。挑战在于选择合适的特征并将它们组合成一个合理的模型。这里有一个小小的帮助，就是前面提到的<strong class="le iu">多重共线性警告</strong>，它比神经网络更容易避免过度拟合。离群点检测有点问题。在线性回归中，离群值很容易识别；在这里，它们可能被认为是某种奇怪的非线性模式的一部分。</p><p id="2c95" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在库的帮助下，这两种解决方法现在都很容易管理。拟合多项式不需要成为数学家，训练神经网络也不需要成为机器学习工程师。</p><h2 id="cf2a" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">一些结果(来自程等人)</h2><p id="670a" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">最终，相关的问题是<em class="ly">‘哪一个更好？’</em>。希望你不要期望在这里找到一个结论性的答案。</p><p id="e988" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然而，值得注意的是，Cheng 等人(2019)在多个数据集(分类和回归任务)上比较了两者的性能。有趣的是，他们发现多项式回归的表现与神经网络一样好，甚至更好。此外，他们的实验表明多项式很少需要高于二阶或三阶。因此，它们也是可以解释的。</p><blockquote class="my mz na"><p id="499e" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">“由于这个和其他原因，例如数据边缘的大拟合值，许多作者建议不要使用高于 2 或 3 次的多项式模型，事实上，在本文的实证实验中，我们很少发现有必要使用更高的次数。”—程等，2019</p></blockquote><p id="4200" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">直觉上，这是有道理的。<strong class="le iu">相互作用效应</strong>在高阶时会急剧减弱。将两个或三个变量相乘会产生相关的见解，捕捉线性和二次效应也是如此。但是在现实生活中，我们真的经常遇到有意义的 20 次方效应或十几个变量之间的相互作用吗？如果有什么不同的话，这个结果意味着神经网络对于手头的任务来说经常是不必要的复杂，导致了臭名昭著的过度拟合效应。</p><blockquote class="my mz na"><p id="98eb" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">“最重要的是，我们已经表明，公关应该是一个有效的替代无核武器国家。我们使用各种类型的各种数据进行了实验，在所有情况下，PR 的性能与 NN 相似，或者明显优于 NN，而没有 NN 试图找到调整参数的良好组合的麻烦。</p><p id="360f" class="lc ld ly le b lf lg ju lh li lj jx lk nb lm ln lo nc lq lr ls nd lu lv lw lx im bi translated">在某些情况下，PR 实际上优于 nn 的事实反映了 nn 通常被过度参数化的事实，本质上是拟合比它们应该拟合的更高次的多项式。”—程等，2019</p></blockquote><p id="02d5" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">尽管测试了各种数据集，但声称多项式回归优于神经网络拟合还是言过其实。当然——特别是在充分微调的情况下——很容易找到神经网络优于多项式回归的反例。我还怀疑多项式在完成超级马里奥游戏或识别人脸(即更专业化的神经网络)方面是否同样成功。</p><p id="b9c4" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于典型的数据科学任务——估计工资、预测旅行时间、按流派对歌曲进行分类——多项式显然比神经网络更合理。由于现实的过度简化，线性回归往往不尽人意，然而深度神经网络可能会寻找根本不存在的高度复杂的模式。</p><h2 id="f9f2" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">结束语</h2><p id="a442" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">正如本文开头所提到的，这更多的是一篇讨论文章。它不打算是完整的或严格的，也不主张多项式回归取代神经网络。</p><p id="5b7b" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">尽管如此，看到多项式回归与神经网络的竞争有多激烈肯定是很有趣的。后者取得的进展非常有趣，但大多数数据科学任务都不在一般的 DeepMind 挑战水平上。</p><p id="db2c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">虽然深度学习对许多人有强烈的吸引力，但它是否总是最适合这项工作的方法是值得怀疑的。多项式提供了非常丰富的现实表现；即使低度数和交互效果也能捕捉大部分真实世界的效果。在那种形式下，它们相对紧凑和全面，避免了神经网络容易遭受的<em class="ly">【黑箱】</em>效应。</p><p id="ab7c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现代工具包消除了过去多项式回归遇到的许多实际问题。理论上，多项式可以实现与神经网络相同的“完美”拟合，同时只需要比基本线性回归多一点的知识。</p><p id="d541" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">总而言之，也许是时候给多项式第二次机会了。</p></div><div class="ab cl kv kw hx kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="im in io ip iq"><h2 id="63f4" class="lz ma it bd mb mc md dn me mf mg dp mh ll mi mj mk lp ml mm mn lt mo mp mq mr bi translated">参考</h2><p id="3041" class="pw-post-body-paragraph lc ld it le b lf ms ju lh li mt jx lk ll mu ln lo lp mv lr ls lt mw lv lw lx im bi translated">Cheng，x .，Khomtchouk，b .，Matloff，n .，&amp; Mohanty，P. (2019)。多项式回归作为神经网络的替代。<em class="ly"> ArXiv 预印本。</em><a class="ae ku" href="https://arxiv.org/abs/1806.06850v2" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1806.06850v2</a></p><p id="b34f" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">乔治·西本科(1989 年)。sigmoidal 函数的叠加逼近。<em class="ly">控制、信号与系统的数学</em>，<em class="ly"> 2 </em> (4)，303–314。</p><p id="4299" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">霍尼克，k .，斯廷奇科姆，m .，，怀特，H. (1989)。多层前馈网络是通用逼近器。<em class="ly">神经网络</em>，<em class="ly"> 2 </em> (5)，359–366。</p><p id="a11e" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">M. H .斯通(1948 年)。广义维尔斯特拉斯逼近定理。<em class="ly">数学杂志</em>，<em class="ly"> 21 </em> (5)，237–254。</p><p id="821f" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">魏尔斯特拉斯，K. (1885)。因此，分析师们将库利尔的工作放在一个更低的位置。<em class="ly">柏林科学研究院院长会议</em>，<em class="ly"> 2 </em>，633–639。</p><div class="ol om gp gr on oo"><a href="https://en.wikipedia.org/wiki/Polynomial" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">多项式-维基百科</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在数学中，多项式是由不定项(也称为变量)和系数组成的表达式</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">en.wikipedia.org</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://en.wikipedia.org/wiki/Polynomial_regression" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">多项式回归-维基百科</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在统计学中，多项式回归是回归分析的一种形式，其中独立变量之间的关系…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">en.wikipedia.org</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://en.wikipedia.org/wiki/Variance_inflation_factor" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">方差膨胀因子—维基百科</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在统计学中，方差膨胀因子(VIF)是估计某个变量的方差的比率(商)</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="ol om gp gr on oo"><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通用逼近定理—维基百科</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">在人工神经网络的数学理论中，通用逼近定理是建立…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="ol om gp gr on oo"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">sklearn.preprocessing .多项式功能</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">使用 sk learn . preprocessing . polynomial features 的示例:scikit 的发布亮点-learn 0.24 发布亮点…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">scikit-learn.org</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://numpy.org/doc/stable/reference/routines.polynomials.package.html#module-numpy.polynomial" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">便利类-NumPy 1.22 版手册</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一个有效处理多项式的子程序包。在该子包的文档中,“有限功率……</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">numpy.org</p></div></div></div></a></div></div></div>    
</body>
</html>