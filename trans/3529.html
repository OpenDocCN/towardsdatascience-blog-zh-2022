<html>
<head>
<title>OCR-Free Document Understanding with Donut</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用甜甜圈实现无OCR文档理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ocr-free-document-understanding-with-donut-1acfbdf099be#2022-08-05">https://towardsdatascience.com/ocr-free-document-understanding-with-donut-1acfbdf099be#2022-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="df93" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用最近发布的Transformers模型来生成文档数据的JSON表示</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/07aad42891e6b3936d82bbc298428a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D7mD51u7270ye9XB"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@romaindancre?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">罗曼·丹斯克</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9742" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">视觉文档理解(VDU)是深度学习和数据科学中一个经过大量研究的新领域，特别是因为pdf或文档扫描中存在大量非结构化数据。最近的模型，如<a class="ae kv" href="https://arxiv.org/abs/1912.13318" rel="noopener ugc nofollow" target="_blank"> LayoutLM </a>，利用<a class="ae kv" rel="noopener" target="_blank" href="/transformers-141e32e69591"> transformers </a>深度学习模型架构来标记单词或回答基于文档图像的给定问题(例如，您可以通过注释图像本身来突出显示和标记帐号，或者询问模型，“帐号是什么？”).像HuggingFace的<code class="fe ls lt lu lv b"><a class="ae kv" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">transformers</a></code>这样的库使得使用开源变形金刚模型变得更加容易。</p><p id="e9b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VDU问题的大多数传统解决方案依赖于解析该图像的OCR输出以及视觉编码，但是OCR的计算成本很高(因为它通常需要安装像Tesseract这样的OCR引擎),并且在完整的管道中包含另一个模型会导致必须训练和微调另一个模型，并且不准确的OCR模型会导致VDU模型中的错误传播。</p><p id="8388" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，来自<a class="ae kv" href="https://clova.ai/en/research/research-areas.html" rel="noopener ugc nofollow" target="_blank"> Naver CLOVA </a> <a class="ae kv" href="https://arxiv.org/abs/2111.15664" rel="noopener ugc nofollow" target="_blank">的研究人员提出了一个端到端的VDU解决方案</a>【1】，该方案使用了一个编码器-解码器变压器模型架构，并且<a class="ae kv" href="https://github.com/clovaai/donut" rel="noopener ugc nofollow" target="_blank">最近将</a>与HuggingFace <code class="fe ls lt lu lv b">transformers</code>库结合使用。换句话说，它使用BART解码器模型将图像(使用Swin Transformer分割成小块)编码成令牌向量，然后可以解码或翻译成数据结构形式的输出序列(然后可以进一步解析成JSON ),并在多语言数据集上进行公开预训练。在推理时输入到模型中的任何提示也可以在相同的架构中解码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/8d8a0f8fe25781552639bc8923338a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sIR3l8QOkIr99hqlkRIOOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">甜甜圈作者的图片(麻省理工学院许可)</p></figure><p id="5b59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在<a class="ae kv" href="https://github.com/clovaai/cord" rel="noopener ugc nofollow" target="_blank">脐带收据数据集</a> <a class="ae kv" href="https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2" rel="noopener ugc nofollow" target="_blank">这里</a>看到甜甜圈的演示。他们提供了一个样本收据图像来进行测试，但是您也可以在许多其他文档图像上进行测试。当我在这张图片上测试时:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/0329d90aacad79043e1e357901162215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x0lXq3IvzKDLA4pP"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由甜甜圈的作者提供</p></figure><p id="8a2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我得到了结果:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="cac3" class="md me iq lv b be mf mg l mh mi">{<br/>    nm: "Presentation"<br/>}</span></pre><p id="bf3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这表明它检测到“演示”标题是菜单或收据上的项目名称。</p><p id="fb47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者还提供了培训和测试脚本，因此我们可以演示如何在实践中实际使用这些模型(我将使用<a class="ae kv" href="https://github.com/zzzDavid/ICDAR-2019-SROIE" rel="noopener ugc nofollow" target="_blank"> SROIE数据集</a> [2]，一个标记收据和发票的数据集，来演示对定制数据集的微调)。我建议在GPU上运行代码，因为推理和训练都需要花费CPU相当长的时间。Google Colab提供免费的GPU访问，应该足以进行微调(转到<strong class="ky ir">运行时</strong> &gt; <strong class="ky ir">更改运行时类型</strong>从CPU切换到GPU)。</p><p id="3de8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们确保我们有GPU访问。</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="7e5a" class="md me iq lv b be mf mg l mh mi">import torch<br/>print("CUDA available:", torch.cuda.is_available())<br/>!nvcc --version</span></pre><p id="dde7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以下载相关的文件和库。下面几行代码应该会安装所有的依赖项，包括donut库(虽然您可以使用<code class="fe ls lt lu lv b">pip install donut-python</code>手动安装，但是从Github克隆的代码库包括重要的培训和测试脚本)。</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="a40f" class="md me iq lv b be mf mg l mh mi">!git clone https://github.com/clovaai/donut.git<br/>!cd donut &amp;&amp; pip install .</span></pre><h2 id="faf2" class="mj me iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">使用CORD微调模型进行推断</h2><p id="740d" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">首先，我们将演示模型的基本用法。</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="90b6" class="md me iq lv b be mf mg l mh mi">from donut import DonutModel<br/>from PIL import Image<br/>import torch<br/>model = DonutModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")<br/>if torch.cuda.is_available():<br/>    model.half() <br/>    device = torch.device("cuda") <br/>    model.to(device) <br/>else: <br/>    model.encoder.to(torch.bfloat16)<br/>model.eval() <br/>image = Image.open("./donut/misc/sample_image_cord_test_receipt_00004.png")<br/>    .convert("RGB")<br/>output = model.inference(image=image, prompt="&lt;s_cord-v2&gt;")<br/>output</span></pre><p id="9c2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<code class="fe ls lt lu lv b">DonutModel.from_pretrained()</code>调用中，我已经简单地从HuggingFace Hub中指定了预训练模型的名称(此时下载了必要的文件)，尽管我也可以指定模型文件夹的本地路径，我们将在后面演示。Donut代码库还包括一个示例图像(如下所示)，这是我传递到模型中的，但是您可以用任何您喜欢的图像来测试模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/837c93a3e3b60bd8bab934c88f316f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sksoYvZkCpu4wuLKK81iMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">甜甜圈作者提供的收据样本图片</p></figure><p id="611b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您应该得到如下输出</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="6934" class="md me iq lv b be mf mg l mh mi">{'predictions': [{'menu': [{'cnt': '2', 'nm': 'ICE BLAOKCOFFE', 'price': '82,000'}, <br/>    {'cnt': '1', 'nm': 'AVOCADO COFFEE', 'price': '61,000'}, <br/>    {'cnt': '1', 'nm': 'Oud CHINEN KATSU FF', 'price': '51,000'}],<br/>    'sub_total': {'discount_price': '19,400', 'subtotal_price': '194,000'}, <br/>    'total': {'cashprice': '200,000', <br/>    'changeprice': '25,400', <br/>    'total_price': '174,600'}}]}</span></pre><p id="8a06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(注意:如果你像我一样好奇，想知道预训练的<code class="fe ls lt lu lv b">donut-base</code>主干会给你什么输出，我继续测试了一下。因为占用了太多内存，所以在崩溃之前需要很长时间才能产生输出。)</p><h1 id="14b7" class="nh me iq bd mk ni nj nk mn nl nm nn mq jw no jx mt jz np ka mw kc nq kd mz nr bi translated">在自定义数据集上微调甜甜圈</h1><p id="8aa5" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">为了演示微调，我将使用SROIE数据集，这是一个收据和发票扫描的数据集，带有JSON形式的基本信息以及单词级的边界框和文本。它包含626张图片，但我将只在100张图片上进行训练，以展示Donut的有效性。这是一个比CORD(包含大约1000张图片)更小的数据集，并且标签也更少(只有公司、日期、地址和总数)。</p><h2 id="b672" class="mj me iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">下载和解析SROIE</h2><p id="953d" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">要下载数据集，您只需从<a class="ae kv" href="https://github.com/zzzDavid/ICDAR-2019-SROIE" rel="noopener ugc nofollow" target="_blank">主存储库</a>中下载<em class="lw">数据</em>文件夹。您可以通过克隆整个存储库或者使用类似于<a class="ae kv" href="https://download-directory.github.io" rel="noopener ugc nofollow" target="_blank">下载目录</a>的东西来下载单个文件夹。</p><p id="29c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是现在我们需要将数据集解析为HuggingFace <code class="fe ls lt lu lv b">datasets</code>库所要求的格式，这是Donut在幕后用来将自定义数据集作为图像字符串表加载的。(如果你在寻找文档，Donut使用<code class="fe ls lt lu lv b">imagefolder</code>加载脚本。)</p><p id="6964" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是所需的数据集格式:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="2db8" class="md me iq lv b be mf mg l mh mi">dataset_name<br/>├── test<br/>│   ├── metadata.jsonl<br/>│   ├── {image_path0}<br/>│   ├── {image_path1}<br/>│             .<br/>│             .<br/>├── train<br/>│   ├── metadata.jsonl<br/>│   ├── {image_path0}<br/>│   ├── {image_path1}<br/>│             .<br/>│             .<br/>└── validation<br/>    ├── metadata.jsonl<br/>    ├── {image_path0}<br/>    ├── {image_path1}<br/>              .<br/>              .</span></pre><p id="907d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> metadata.jsonl </strong>是一个类似于</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="013d" class="md me iq lv b be mf mg l mh mi">{"file_name": {image_path0}, "ground_truth": "{\"gt_parse\": {ground_truth_parse}, ... {other_metadata_not_used} ... }"}<br/>{"file_name": {image_path1}, "ground_truth": "{\"gt_parse\": {ground_truth_parse}, ... {other_metadata_not_used} ... }"}</span></pre><p id="fdf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，我们希望将每个文档的注释(在<em class="lw">键</em>文件夹中找到)转换成类似于<code class="fe ls lt lu lv b">"{\"gt_parse\": {actual JSON content}"}"</code>的JSON转储字符串。下面是一个注释示例:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="464e" class="md me iq lv b be mf mg l mh mi">{<br/>    "company": "BOOK TA .K (TAMAN DAYA) SDN BHD",<br/>    "date": "25/12/2018",<br/>    "address": "NO.53 55,57 &amp; 59, JALAN SAGU 18, TAMAN DAYA, 81100 JOHOR BAHRU, JOHOR.",<br/>    "total": "9.00"<br/>}</span></pre><p id="d422" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是我用来将数据转换成JSON lines文件以及将图像复制到各自的文件夹中的脚本:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="10b5" class="md me iq lv b be mf mg l mh mi">import os<br/>import json<br/>import shutil<br/>from tqdm.notebook import tqdm<br/>lines = []<br/>images = []<br/>for ann in tqdm(os.listdir("./sroie/key")[:100]):<br/>  if ann != ".ipynb_checkpoints":<br/>    with open("./sroie/key/" + ann) as f:<br/>      data = json.load(f)<br/>images.append(ann[:-4] + "jpg")<br/>    line = {"gt_parse": data}<br/>    lines.append(line)<br/>with open("./sroie-donut/train/metadata.jsonl", 'w') as f:<br/>  for i, gt_parse in enumerate(lines):<br/>    line = {"file_name": images[i], "ground_truth": json.dumps(gt_parse)}<br/>    f.write(json.dumps(line) + "\n")<br/>shutil.copyfile("./sroie/img/" + images[i], "./sroie-donut/train/" + images[i])</span></pre><p id="e630" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我简单地运行这个脚本三次，每次都改变文件夹和列表片的名称(<code class="fe ls lt lu lv b">[:100]</code>)，这样我在<em class="lw">训练</em>中有100个例子，在<em class="lw">验证</em>和<em class="lw">测试</em>中各有20个例子。</p><h2 id="8dbd" class="mj me iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">训练模型</h2><p id="9489" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">Donut的作者提供了一个非常简单的方法来训练模型。首先，我们需要在<em class="lw"> donut/config </em>文件夹中创建一个新的配置文件。您可以将已经存在的示例(<em class="lw"> train_cord.yaml </em>)复制到一个名为<strong class="ky ir"> train_sroie.yaml </strong>的新文件中。这些是我更改的值:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="d5be" class="md me iq lv b be mf mg l mh mi">dataset_name_or_paths: ["../sroie-donut"]<br/>train_batch_sizes: [1]<br/>check_val_every_n_epochs: 10<br/>max_steps: -1 # infinite, since max_epochs is specified</span></pre><p id="7c19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您已经本地下载了<code class="fe ls lt lu lv b">donut-base</code>模型，您也可以在<code class="fe ls lt lu lv b">pretrained_model_name_or_path</code>中指定它的路径。否则HuggingFace会直接从Hub下载。</p><p id="ed01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我在Google Colab上发现CUDA内存不足错误时，我将批处理大小从8减小，并将<code class="fe ls lt lu lv b">check_val_every_n_epochs</code>增加到10以节省时间。</p><p id="b67e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是你应该用来训练你的模型的线:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="f67e" class="md me iq lv b be mf mg l mh mi">cd donut &amp;&amp; python train.py --config config/train_sroie.yaml</span></pre><p id="1497" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我花了大约一个小时的时间在Google Colab提供的GPU上完成了训练。</p><h2 id="5ba9" class="mj me iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">使用微调模型进行推理</h2><p id="e911" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">使用与上面的CORD演示类似的脚本，我们可以使用</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="b8af" class="md me iq lv b be mf mg l mh mi">from donut import DonutModel<br/>from PIL import Image<br/>import torch<br/>model = DonutModel.from_pretrained("./donut/result/train_sroie/20220804_214401")<br/>if torch.cuda.is_available():<br/>    model.half()<br/>    device = torch.device("cuda")<br/>    model.to(device)<br/>else:<br/>    model.encoder.to(torch.bfloat16)<br/>model.eval()<br/>image = Image.open("./sroie-donut/test/099.jpg").convert("RGB")<br/>output = model.inference(image=image, prompt="&lt;s_sroie-donut&gt;")<br/>output</span></pre><p id="cf65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们已经在<code class="fe ls lt lu lv b">DonutModel.from_pretrained()</code>调用中更改了模型路径，并且我们还将推理<code class="fe ls lt lu lv b">prompt</code>更改为<code class="fe ls lt lu lv b">&lt;s_{dataset_name}&gt;</code>的格式。这是我用的图片:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/f341dff03756fb14be2e04282ced3df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HxAHYDlV9bO1QVCWVRLmqA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://github.com/zzzDavid/ICDAR-2019-SROIE" rel="noopener ugc nofollow" target="_blank"> SROIE数据集</a></p></figure><p id="c4b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是我的结果:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="8c14" class="md me iq lv b be mf mg l mh mi">{'predictions': [{'address': 'NO 290, JALAN AIR PANAS. SETAPAK. 53200, KUALA LUMPUR.',<br/>   'company': 'SYARIKAT PERNIAGAAN GIN KEE',<br/>   'date': '04/12/2017',<br/>   'total': '47.70'}]}</span></pre><h1 id="e3c0" class="nh me iq bd mk ni nj nk mn nl nm nn mq jw no jx mt jz np ka mw kc nq kd mz nr bi translated">最后的想法</h1><p id="36da" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">我注意到，使用Donut的伪OCR的最终输出比传统的现成OCR方法要准确得多。作为一个极端的例子，下面是使用Tesseract的OCR引擎进行OCRed的演示中的同一个CORD文档:</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="8d7c" class="md me iq lv b be mf mg l mh mi">*' il " i<br/>- ' s ' -<br/>W =<br/>o o<br/>ok S<br/>?ﬂﬁ (€<br/>rgm"f"; o ;<br/>L i 4</span></pre><p id="d0cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图像模糊，对比度低，甚至对人来说也难以阅读，所以不太可能有人会期望模型能够识别角色。令人印象深刻的是，甜甜圈能够用自己的技术做到这一点。即使对于高质量的文档，尽管其他商业OCR模型比开源OCR引擎(如Tesseract)提供的结果更好，但它们通常成本很高，并且因为商业数据集的强化训练和更强的计算能力而更好。</p><p id="e039" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解析给定文档的OCR输出的模型的替代方案包括仅利用计算机视觉技术来突出显示各种文本块、解析表格、或者识别图像、图形和数学等式，但是如果可以导出有意义的数据，则再次要求用户对包围盒输出进行OCR。库包括<a class="ae kv" href="https://layout-parser.github.io/" rel="noopener ugc nofollow" target="_blank"> LayoutParser </a>和<a class="ae kv" href="https://github.com/deepdoctection/deepdoctection" rel="noopener ugc nofollow" target="_blank">deep doc detection</a>，两者都连接到<a class="ae kv" href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/" rel="noopener ugc nofollow" target="_blank"> Detectron2 </a>计算机视觉模型的模型动物园以交付结果。</p><p id="759c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，Donut的作者还提供了一个测试脚本，您可以使用它为您的微调模型开发评估指标，该脚本位于Donut代码库中的<strong class="ky ir"> test.py <em class="lw"> </em> </strong>文件中。它提供F1准确度分数，该分数是基于对基本事实解析的准确通过或失败来测量的，以及由树编辑距离算法给出的准确度分数，该算法确定最终JSON树与基本事实JSON的接近程度。</p><pre class="kg kh ki kj gt lz lv ma bn mb mc bi"><span id="1cf9" class="md me iq lv b be mf mg l mh mi">cd ./donut &amp;&amp;<br/>python test.py --dataset_name_or_path ../sroie-donut --pretrained_model_name_or_path ./result/train_sroie/20220804_214401 --save_path ./result/train_sroie/output.json</span></pre><p id="f544" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用我的SROIE-finetuned模型，我的所有20个测试图像的平均准确率为94.4%。</p><p id="a300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Donut还与SynthDoG打包在一起，synth dog是一个模型，可用于以四种不同的语言生成额外的伪造文档以进行数据扩充。它在英文、中文、日文和韩文维基百科上接受了培训，以便更好地解决传统OCR/VDU方法的问题，这些方法通常因缺乏除英语之外的大量语言数据而受到限制。</p><p id="8b48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] Kim，Geewook等，“无OCR文档理解转换器”(2021).麻省理工许可证。</p><p id="a69a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]黄征等，“ICDAR2019扫描收据OCR和信息提取竞赛”<em class="lw"> 2019文档分析与识别国际会议(ICDAR) </em>。IEEE，2019。麻省理工许可证。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="9da3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Neha Desaraju是德克萨斯大学奥斯丁分校学习计算机科学的学生。你可以在网上的</strong><a class="ae kv" href="https://estaudere.github.io/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">estau dere . github . io</strong></a><strong class="ky ir">找到她。</strong></p></div></div>    
</body>
</html>