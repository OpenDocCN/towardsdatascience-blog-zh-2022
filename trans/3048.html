<html>
<head>
<title>How to Create a Graph Neural Network in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python创建图形神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-create-a-graph-neural-network-in-python-61fd9b83b54e#2022-07-05">https://towardsdatascience.com/how-to-create-a-graph-neural-network-in-python-61fd9b83b54e#2022-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9510" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Pytorch几何图形和OGB创建GNN</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/90a3a44a46c69b7cceb3a1c8cbfafe37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RgcjAU7CajtsBMqx"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="1b70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习为在非结构化数据上进行预测开辟了一个全新的可能性世界。今天，通常对图像数据使用卷积神经网络(CNN ),对文本使用递归神经网络(RNNs ),等等。</p><p id="598a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过去的几年中，出现了一类新的令人兴奋的神经网络:图神经网络(GNNs)。顾名思义，这个网络类侧重于处理图形数据。</p><p id="078e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，你将学习图形神经网络如何工作的基础知识，以及如何使用Pytorch几何(PyG)库和开放图形基准(OGB)库在Python中实现它。</p><p id="8542" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的<a class="ae kv" href="https://github.com/TNanukem/paper_implementations/blob/main/Graph%20Neural%20Networks%20with%20OGB%20and%20Pytorch%20Geometric.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae kv" href="https://www.kaggle.com/tiagotoledojr/gnns-with-ogb-and-pytorch-geometric" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上有这篇文章的代码。</p><h1 id="9430" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一个普通的GNN是如何工作的</h1><p id="2709" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">随着图形卷积网络(GCN) [1]的引入，GNNs开始变得流行起来，它从CNN借用了一些概念到图形世界。这种网络的主要思想，也称为消息传递框架，多年来成为该领域的黄金标准，这就是我们将在这里探讨的概念。</p><p id="6afa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">消息传递框架声明，对于图中的每个节点，我们将做两件事:</p><ul class=""><li id="8e93" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">聚集来自其邻居的信息</li><li id="8d55" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">用来自其先前层及其邻居聚集的信息来更新节点信息</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/90c860fe4bac0be3a6046182e8eb540d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A65sNmF4UpRFn8dO.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">消息传递框架。来源:<a class="ae kv" href="https://en.wikipedia.org/wiki/Graph_neural_network#/media/File:Message_Passing_Neural_Network.png" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="fbea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图1展示了这个通用框架是如何工作的。GCN之后开发的许多架构都专注于定义聚集和更新数据的最佳方式。</p><h1 id="cc1d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">关于派格和OGB</h1><p id="70ce" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">PyG是Pytorch库的扩展，它允许我们使用研究中已经建立的层快速实现新的GNNs架构。</p><p id="aa62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">OGB [2]是作为一种提高该领域研究质量的方法而开发的，因为它提供了可供使用的精选图表，也是评估给定架构结果的标准方法，从而使方案之间的比较更加公平。</p><p id="1470" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一起使用这两个库的想法是，人们可以更容易地提出一个体系结构，而不必担心数据获取和评估机制。</p><h1 id="ad5e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">实施GNN</h1><p id="0555" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，让我们安装所需的库。请注意，您必须安装PyTorch:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b275" class="nj lt iq nf b gy nk nl l nm nn">pip install ogb<br/>pip install torch_geometric</span></pre><p id="badf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们导入所需的方法和库:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="25a5" class="nj lt iq nf b gy nk nl l nm nn">import os<br/>import torch<br/>import torch.nn.functional as F</span><span id="f12a" class="nj lt iq nf b gy no nl l nm nn">from tqdm import tqdm<br/>from torch_geometric.loader import NeighborLoader<br/>from torch.optim.lr_scheduler import ReduceLROnPlateau<br/>from torch_geometric.nn import MessagePassing, SAGEConv<br/>from ogb.nodeproppred import Evaluator, PygNodePropPredDataset</span></pre><p id="57f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步是从OGB下载数据集。我们将使用ogbn-arxiv网络，其中每个节点是arxiv上的一篇计算机科学论文，每个有向边代表一篇论文引用了另一篇论文。任务是将每个节点分类到一个纸类中。</p><p id="dc20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下载它非常简单:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7ce4" class="nj lt iq nf b gy nk nl l nm nn">target_dataset = 'ogbn-arxiv'</span><span id="83b0" class="nj lt iq nf b gy no nl l nm nn"># This will download the ogbn-arxiv to the 'networks' folder<br/>dataset = PygNodePropPredDataset(name=target_dataset, root='networks')</span></pre><p id="b6a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集变量是一个名为PygNodePropPredDataset()的类的实例，该类特定于OGB。要将数据集作为可在Pytorch几何图形上使用的数据类进行访问，我们只需:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="d453" class="nj lt iq nf b gy nk nl l nm nn">data = dataset[0]</span></pre><p id="908e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们看一下这个变量，我们会看到这样的情况:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="f809" class="nj lt iq nf b gy nk nl l nm nn">Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])</span></pre><p id="9a40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有节点数、邻接表、网络的特征向量、每个节点的年份和目标标签。</p><p id="7c60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个网络已经有了一个用于训练、验证和测试的分离集。这是由OGB提供的，旨在提高该网络研究的可重复性和质量。我们可以通过以下方式提取:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="86cc" class="nj lt iq nf b gy nk nl l nm nn">split_idx = dataset.get_idx_split() <br/>        <br/>train_idx = split_idx['train']<br/>valid_idx = split_idx['valid']<br/>test_idx = split_idx['test']</span></pre><p id="2e1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将定义两个数据加载器，在我们的培训中使用。第一个将只加载训练集中的节点，第二个将加载网络上的所有节点。</p><p id="e5c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用Pytorch几何图形中的邻居加载器。该数据加载器对每个节点的给定数量的邻居进行采样。这是一种避免具有数千个节点的节点的RAM和计算时间爆炸的方法。对于本教程，我们将在训练加载器上对每个节点使用30个邻居。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="01b9" class="nj lt iq nf b gy nk nl l nm nn">train_loader = NeighborLoader(data, input_nodes=train_idx,<br/>                              shuffle=True, num_workers=os.cpu_count() - 2,<br/>                              batch_size=1024, num_neighbors=[30] * 2)</span><span id="3ba5" class="nj lt iq nf b gy no nl l nm nn">total_loader = NeighborLoader(data, input_nodes=None, num_neighbors=[-1],<br/>                               batch_size=4096, shuffle=False,<br/>                               num_workers=os.cpu_count() - 2)</span></pre><p id="490f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们打乱了训练数据加载器，而不是整个加载器。此外，训练加载器的邻居数量被定义为网络每层的数量。因为我们这里要使用两层网络，所以我们用两个值30将其设置到列表中。</p><p id="348b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是时候创造我们的GNN建筑了。对于任何熟悉Pytorch的人来说，这应该不会太可怕。</p><p id="3cad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用鼠尾草层。这些层在一篇非常好的论文[3]中进行了定义，该论文介绍了邻域采样的概念。Pytorch几何库已经为我们实现了这一层。</p><p id="f6cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，与每一个PyTorch架构一样，我们必须用我们将要使用的层定义一个类:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="4b09" class="nj lt iq nf b gy nk nl l nm nn">class SAGE(torch.nn.Module):<br/>    def __init__(self, in_channels,<br/>                 hidden_channels, out_channels,<br/>                 n_layers=2):<br/>        <br/>        super(SAGE, self).__init__()<br/>        self.n_layers = n_layers</span><span id="6600" class="nj lt iq nf b gy no nl l nm nn">    self.layers = torch.nn.ModuleList()<br/>        self.layers_bn = torch.nn.ModuleList()</span><span id="4493" class="nj lt iq nf b gy no nl l nm nn">    if n_layers == 1:<br/>            self.layers.append(SAGEConv(in_channels, out_channels,   normalize=False))<br/>        elif n_layers == 2:<br/>            self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))<br/>             self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))<br/>            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))<br/>        else:<br/>           self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))<br/>              self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))</span><span id="ea44" class="nj lt iq nf b gy no nl l nm nn">    for _ in range(n_layers - 2):<br/>                self.layers.append(SAGEConv(hidden_channels,  hidden_channels, normalize=False))<br/>                 self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))<br/>            <br/>            self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))<br/>            <br/>        for layer in self.layers:<br/>            layer.reset_parameters()</span><span id="1077" class="nj lt iq nf b gy no nl l nm nn">    def forward(self, x, edge_index):<br/>        if len(self.layers) &gt; 1:<br/>            looper = self.layers[:-1]<br/>        else:<br/>            looper = self.layers<br/>        <br/>        for i, layer in enumerate(looper):<br/>            x = layer(x, edge_index)<br/>            try:<br/>                x = self.layers_bn[i](x)<br/>            except Exception as e:<br/>                abs(1)<br/>            finally:<br/>                x = F.relu(x)<br/>                x = F.dropout(x, p=0.5, training=self.training)<br/>        <br/>        if len(self.layers) &gt; 1:<br/>            x = self.layers[-1](x, edge_index)</span><span id="9fbb" class="nj lt iq nf b gy no nl l nm nn">        return F.log_softmax(x, dim=-1), torch.var(x)<br/>    <br/>    def inference(self, total_loader, device):<br/>        xs = []<br/>        var_ = []<br/>        for batch in total_loader:<br/>            out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))<br/>            out = out[:batch.batch_size]<br/>            xs.append(out.cpu())<br/>            var_.append(var.item())<br/>        <br/>        out_all = torch.cat(xs, dim=0)<br/>        <br/>        return out_all, var_</span></pre><p id="894b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们一步一步地分解它:</p><ul class=""><li id="5bcc" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">我们必须定义网络的输入通道数，这将是数据集中的要素数。out_channels将是我们试图预测的类的数量。隐藏通道参数是一个我们可以自己定义的值，代表隐藏单元的数量。</li><li id="42d7" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">我们可以设置网络的层数。对于每个隐藏层，我们添加一个批量标准化层，然后我们重置每个层的参数。</li><li id="fc5b" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">forward方法运行正向传递的单次迭代。它将获取特征向量和邻接表，并将其传递给SAGE层，然后将结果传递给批量规范化层。我们还应用了一个ReLU非线性和一个用于正则化目的的漏失层。</li><li id="3a2d" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">最后，推理方法将为数据集中的每个节点生成一个预测。我们将使用它进行验证。</li></ul><p id="78c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们定义模型的一些参数:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="2427" class="nj lt iq nf b gy nk nl l nm nn">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</span><span id="2716" class="nj lt iq nf b gy no nl l nm nn">model = SAGE(data.x.shape[1], 256, dataset.num_classes, n_layers=2)<br/>model.to(device)<br/>epochs = 100<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.03)<br/>scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)</span></pre><p id="45e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将创建测试函数来验证我们所有的预测:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7aaa" class="nj lt iq nf b gy nk nl l nm nn">def test(model, device):<br/>    evaluator = Evaluator(name=target_dataset)<br/>    model.eval()<br/>    out, var = model.inference(total_loader, device)</span><span id="2c8e" class="nj lt iq nf b gy no nl l nm nn">    y_true = data.y.cpu()<br/>        y_pred = out.argmax(dim=-1, keepdim=True)</span><span id="e36f" class="nj lt iq nf b gy no nl l nm nn">    train_acc = evaluator.eval({<br/>        'y_true': y_true[split_idx['train']],<br/>        'y_pred': y_pred[split_idx['train']],<br/>    })['acc']<br/>    val_acc = evaluator.eval({<br/>        'y_true': y_true[split_idx['valid']],<br/>        'y_pred': y_pred[split_idx['valid']],<br/>    })['acc']<br/>    test_acc = evaluator.eval({<br/>        'y_true': y_true[split_idx['test']],<br/>        'y_pred': y_pred[split_idx['test']],<br/>    })['acc']</span><span id="2e93" class="nj lt iq nf b gy no nl l nm nn">return train_acc, val_acc, test_acc, torch.mean(torch.Tensor(var))</span></pre><p id="d356" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个函数中，我们从OGB实例化了一个验证器类。这个类将负责对我们之前检索到的每个分割进行模型验证。这样，我们将看到每个时期的训练、验证和测试集的分数。</p><p id="1473" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们创建我们的培训循环:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="827f" class="nj lt iq nf b gy nk nl l nm nn">for epoch in range(1, epochs):<br/>    model.train()</span><span id="d3ff" class="nj lt iq nf b gy no nl l nm nn">    pbar = tqdm(total=train_idx.size(0))<br/>    pbar.set_description(f'Epoch {epoch:02d}')</span><span id="eea4" class="nj lt iq nf b gy no nl l nm nn">    total_loss = total_correct = 0</span><span id="6c21" class="nj lt iq nf b gy no nl l nm nn">    for batch in train_loader:<br/>        batch_size = batch.batch_size<br/>        optimizer.zero_grad()</span><span id="96c2" class="nj lt iq nf b gy no nl l nm nn">        out, _ = model(batch.x.to(device), batch.edge_index.to(device))<br/>        out = out[:batch_size]</span><span id="ec68" class="nj lt iq nf b gy no nl l nm nn">        batch_y = batch.y[:batch_size].to(device)<br/>        batch_y = torch.reshape(batch_y, (-1,))</span><span id="fc08" class="nj lt iq nf b gy no nl l nm nn">        loss = F.nll_loss(out, batch_y)<br/>        loss.backward()<br/>        optimizer.step()</span><span id="1228" class="nj lt iq nf b gy no nl l nm nn">        total_loss += float(loss)<br/>        total_correct += int(out.argmax(dim=-1).eq(batch_y).sum())<br/>        pbar.update(batch.batch_size)</span><span id="9ba8" class="nj lt iq nf b gy no nl l nm nn">    pbar.close()</span><span id="b4d4" class="nj lt iq nf b gy no nl l nm nn">    loss = total_loss / len(train_loader)<br/>    approx_acc = total_correct / train_idx.size(0)</span><span id="86a4" class="nj lt iq nf b gy no nl l nm nn">    train_acc, val_acc, test_acc, var = test(model, device)<br/>    <br/>    print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Var: {var:.4f}')</span></pre><p id="1d54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个循环将训练我们的GNN 100个代，并且如果我们的验证分数连续七个代没有增长，将提前停止它。</p><h1 id="8d81" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="2c7e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">gnn是一类令人着迷的神经网络。今天，我们已经有几个工具来帮助我们开发这种解决方案。正如你所看到的，使用Pytorch几何和OGB可以很容易地实现一些图形的GNN。</p><p id="abcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]基普夫，托马斯&amp;韦林，马克斯。(2016).基于图卷积网络的半监督分类。</p><p id="f841" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]胡，魏，费伊，米，齐特尼克，米，董，y，任，刘，b，卡塔斯塔，米和莱斯科维奇(2020)。开放图基准:图的机器学习数据集<em class="np">。arXiv预印本arXiv:2005.00687 </em>。</p><p id="3c31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]汉密尔顿，威廉&amp;英，雷克斯&amp;莱斯科维奇，Jure。(2017).大型图上的归纳表示学习。</p></div></div>    
</body>
</html>