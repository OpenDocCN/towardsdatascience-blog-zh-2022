<html>
<head>
<title>A Guide to Find the Best Boosting Model using Bayesian Hyperparameter Tuning but without Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用贝叶斯超参数调整但不过度拟合找到最佳推进模型的指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8#2022-08-29">https://towardsdatascience.com/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8#2022-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f698" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用增强的决策树算法，如XGBoost、CatBoost和LightBoost，您可能会胜过其他模型，但过度拟合是一个真正的危险。了解如何使用HGBoost库拆分数据、优化超参数，以及在不过度训练的情况下找到性能最佳的模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3aa515b2dd0ff0b7053561f98c0d1236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tepkj5NPqypeej26uXDV4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者。</p></figure><p id="ad36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">近年来，梯度推进技术在分类和回归任务中得到了广泛的应用。一个重要的部分是调整超参数，以获得最佳的预测性能。这需要搜索数千个参数组合，这不仅是一项计算密集型任务，而且会很快导致模型过度训练。因此，模型可能无法对新的(看不见的)数据进行归纳，并且性能准确性可能比预期的差。幸运的是，有<em class="lu">贝叶斯优化</em>技术可以帮助优化网格搜索并减少计算负担。但是还有更多的原因，因为优化的网格搜索仍然可能导致模型训练过度。在优化超参数时，仔细地将数据分成一个训练集、一个测试集和一个独立的验证集是应该包含的另一个重要部分。<strong class="la iu"><em class="lu">HGBoost库进场了！</em> </strong> <em class="lu"> HGBoost代表超优化梯度提升，是一个针对XGBoost、LightBoost、CatBoost进行超参数优化的Python包。它会小心地将数据集分成训练集、测试集和独立的验证集。在训练测试集中，有一个使用贝叶斯优化(使用hyperopt)优化超参数的内环，以及一个基于k-fold交叉验证对性能最佳的模型的概括程度进行评分的外环。因此，它将尽最大努力选择具有最佳性能的最稳健的模型。</em> <strong class="la iu"> <em class="lu">在这篇博客中，我将首先简单讨论boosting算法和超参数优化。然后，我将讨论如何使用优化的超参数来训练模型。我将演示如何解读和直观地解释优化的超参数空间，以及如何评估模型性能。</em>T11】</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="58ed" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">升压算法的简单介绍。</h1><p id="59dd" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">极端梯度增强(<em class="lu"> XGboost </em>)、轻度梯度增强(<em class="lu"> Lightboost </em>)和<em class="lu"> CatBoost </em>等梯度增强算法是强大的集成机器学习算法，用于预测建模，可应用于表格和连续数据，以及用于<a class="ae mz" href="https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d" rel="noopener">分类</a>和回归任务。</p><p id="c426" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">增强决策树算法非常受欢迎并不奇怪，因为在Kaggle [ <a class="ae mz" href="https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]举办的机器学习挑战中，这些算法参与了超过一半的获胜解决方案。<em class="lu">梯度推进和决策树的结合在许多应用中提供了最先进的结果。</em>也正是<em class="lu">这种</em>组合使得<em class="lu"> XGboost、CatBoost和Lightboost </em>之间产生了差异。共同的主题是，每个boosting算法都需要为每片叶子找到最佳分裂，并且需要最小化<a class="ae mz" href="https://arxiv.org/abs/1706.08359" rel="noopener ugc nofollow" target="_blank">计算成本</a>。高计算成本是因为模型需要为每片叶子找到精确的分裂点，这需要对所有数据进行迭代扫描。这个过程很难优化。</p><p id="4d65" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">粗略地说，有两种不同的策略来计算树:<strong class="la iu"> <em class="lu">逐层和逐叶。逐级策略</em> </strong> <em class="lu">逐级增长树。在这种策略中，每个节点分割数据，并对靠近树根的节点进行优先级排序。</em> <strong class="la iu"> <em class="lu">逐叶策略</em> </strong> <em class="lu">通过在具有最高损失变化的节点处分割数据来增长树。</em>级别式增长通常对较小的数据集更好，而叶式增长在较小的数据集中往往会过拟合。然而，叶方向的增长<a class="ae mz" href="http://researchcommons.waikato.ac.nz/handle/10289/2317" rel="noopener ugc nofollow" target="_blank">倾向于在更大的数据集</a>中表现出色，在那里它比水平方向的增长<a class="ae mz" href="https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html" rel="noopener ugc nofollow" target="_blank"> 2 </a>要快得多。让我们从树分裂和计算成本的角度总结一下<em class="lu"> XGboost、LightBoost和CatBoost </em>算法。</p><h2 id="3314" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">XGBoost。</h2><p id="e0b6" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">极端梯度提升<em class="lu"> (XGboost) </em>是最流行的梯度提升技术类型之一，其中提升的决策树在内部由弱<a class="ae mz" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">决策树</a>的集合组成。最初<em class="lu"> XGBoost </em>基于逐层增长算法，但最近增加了一个逐叶增长选项，使用直方图实现分裂近似。<strong class="la iu">优点</strong>是<em class="lu"> XGBoost </em>学习有效，泛化性能强。此外，它还可以捕捉非线性关系。<strong class="la iu">缺点</strong>是超参数调整可能会很复杂，当使用稀疏数据集时，它可能会很快导致高计算成本(内存方面和时间方面)，因为当使用非常大的数据集时需要许多树[ <a class="ae mz" href="https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p><h2 id="88b9" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">灯光增强。</h2><p id="f8eb" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated"><em class="lu"> LightBoost </em>或<em class="lu"> LightGBM </em>是一个梯度提升框架，它使用基于树的学习算法，对树进行垂直分割(或逐叶分割)。当生长相同的叶子时，与逐层算法相比，这种方法可以更有效地减少损失。此外，<em class="lu"> LightBoost </em>使用基于直方图的算法，将连续的特征值存储到离散的容器中。这加快了训练速度，减少了内存使用。它被设计成高效的，并具有许多<strong class="la iu">优点</strong>，例如快速训练、高效率、低内存使用、更好的准确性、支持并行和GPU学习、能够处理大规模数据<a class="ae mz" href="https://lightgbm.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> 3 </a>。计算和内存高效的优势使<em class="lu"> LightBoost </em>适用于大量数据。<strong class="la iu">缺点</strong>是由于逐叶分裂，对过拟合敏感，并且由于超参数调整，复杂度高。</p><h2 id="16cf" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">CatBoost。</h2><p id="84a1" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated"><em class="lu"> CatBoost </em>也是决策树上梯度提升的高性能方法。<em class="lu"> Catboost使用</em> <a class="ae mz" href="https://en.wikipedia.org/wiki/Oblivious_data_structure" rel="noopener ugc nofollow" target="_blank"> <em class="lu">不经意</em> </a> <em class="lu">树或对称树生成平衡树，以加快执行速度</em>。这意味着通过创建特征分割对，将pér特征、值划分到桶中，例如(temperature，&lt; 0)、(temperature，1–30)、(temperature，&gt; 30)，等等。在不经意树的每一层中，带来最低损失(根据惩罚函数)的特征分裂对被选择并用于所有层节点。据描述，<em class="lu"> CatBoost </em>使用默认参数提供了很好的结果，因此在参数调整上需要较少的时间[ <a class="ae mz" href="http://CatBoost is a high-performance open source library for gradient boosting on decision trees" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。另一个<strong class="la iu">优势</strong>是<em class="lu"> CatBoost </em>允许你使用非数字因子，而不是必须预处理你的数据或者花费时间和精力把它变成数字。<strong class="la iu">的一些缺点</strong>是:它需要构建深度决策树来恢复数据中的依赖性，以防具有高基数的特性，并且它不能处理缺失值。</p><p id="0af5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个提升决策树都有自己的优势(劣势),因此，结合您要开发的应用程序，很好地理解您正在使用的数据集是非常重要的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f50a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">超参数还是参数？</h1><p id="7589" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">简单介绍一下<strong class="la iu"> <em class="lu">超参数</em> </strong>的定义，以及它们与<strong class="la iu"> <em class="lu">正常参数</em> </strong>的区别。总的来说，我们可以说<em class="lu">正常参数</em>是由机器学习模型本身优化的，而超参数<strong class="la iu"> <em class="lu">不是</em> </strong>。在回归模型的情况下，输入特征和结果(或目标值)之间的关系是已知的。在回归模型的训练过程中，<em class="lu">通过调整权重来优化回归线的斜率(假设与目标值的关系是线性的)</em>。或者换句话说，在训练阶段学习<em class="lu">模型参数</em>。</p><blockquote class="nm"><p id="411a" class="nn no it bd np nq nr ns nt nu nv lt dk translated">参数由机器学习模型本身来优化，而超参数在训练过程之外，需要一个元过程来调整。</p></blockquote><p id="039e" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated"><strong class="la iu">然后还有另外一组参数叫做<em class="lu">超参数</em> </strong>(在统计学<em class="lu"/>中又称为<em class="lu">滋扰参数</em>)。这些值必须在训练过程之外指定，并且是模型的输入参数。一些模型没有任何超参数，其他模型有一些(两个或三个左右)仍然可以手动评估。然后是有很多超参数的模型。我是说，真的很多。增强的决策树算法，如<em class="lu"> XGBoost、CatBoost和LightBoost </em>都是有很多超参数的例子，考虑期望的深度、树中的叶子数量等。您可以使用默认的超参数来训练模型，但是调整超参数通常会对训练模型的最终预测精度产生很大影响[ <a class="ae mz" href="https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/ch04.html" rel="noopener ugc nofollow" target="_blank"> 8 </a> ]。此外，不同的数据集需要不同的超参数。这很麻烦，因为一个模型可以很容易地包含几十个超参数，这随后会产生(几万)个超参数组合，需要对这些组合进行评估以确定模型性能。这被称为<em class="lu">搜索空间</em>。因此，计算负担(时间方面和内存方面)可能是巨大的。因此，优化<em class="lu">搜索空间</em>是有益的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="aab0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">仔细分割数据集的情况。</h1><p id="4cf8" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">对于受监督的机器学习任务，重要的是将数据分成单独的部分，以避免在学习模型时<a class="ae mz" href="https://www.techtarget.com/whatis/definition/overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>。过度拟合是指模型对数据拟合(或学习)得太好，然后无法预测(新的)看不见的数据。最常见的方式是将数据集分成<strong class="la iu">训练集</strong>和独立的<strong class="la iu">验证集</strong>。<strong class="la iu"> </strong>然而，当我们也执行超参数调优时，比如在boosting算法中，也需要一个<strong class="la iu">测试集</strong>。模型现在可以<strong class="la iu"> <em class="lu">看到</em> </strong>的数据，<strong class="la iu"> <em class="lu">从数据中学习</em> </strong>，最后，我们可以<strong class="la iu"> <em class="lu">在看不见的数据上评估</em> </strong>的模型。这种方法不仅可以防止过度拟合，而且有助于确定模型的<em class="lu">鲁棒性</em>，即使用<em class="lu"> k </em>折叠交叉验证方法。因此，当我们需要调整超参数时，我们应该将数据分成三部分，即:<em class="lu">训练集、测试集和验证集。</em></p><ol class=""><li id="9b6c" class="ob oc it la b lb lc le lf lh od ll oe lp of lt og oh oi oj bi translated"><strong class="la iu">训练集</strong>:这是模型看到并从数据中学习的部分。它通常由70%或80%的样本组成，以确定数千个可能的超参数中的最佳拟合(例如，使用<em class="lu">k</em>-折叠交叉验证方案)。</li><li id="12d6" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><strong class="la iu">测试集:</strong>该集通常包含20%的样本，可用于评估模型性能，如特定的超参数集。</li><li id="6bf0" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><strong class="la iu">验证集:</strong>该集通常也包含数据中20%的样本，但在最终模型被训练之前保持不变。现在可以以公正的方式评估该模型。<em class="lu">请务必意识到，本器械包只能使用一次。</em>或者换句话说，如果模型在获得对验证集的见解后得到进一步优化，那么您需要另一个独立的集来确定最终的模型性能。</li></ol><p id="3704" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">建立嵌套的交叉验证方法可能很耗时，甚至是一项复杂的任务，但如果您想要创建一个健壮的模型，并防止过度拟合，这是非常重要的。<strong class="la iu"> <em class="lu">自己做出这样的做法是很好的</em>练习<em class="lu">，不过这些也在</em></strong><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="lu">HGBoost库</em></strong></a><strong class="la iu"/><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">1</strong></a><strong class="la iu"><em class="lu">中实现。</em> </strong> <em class="lu">在我讨论HGBoost的工作原理之前，我将首先简要描述一下针对具有不同超参数组合的模型的大规模优化的贝叶斯方法。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a571" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">超参数优化是一项复杂的任务。</h1><p id="740d" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">当使用提升决策树算法时，可能很容易有几十个输入超参数，这可能随后导致需要评估(几万)个超参数组合。这是一项重要的任务，因为超参数的特定组合可以为特定数据集带来更准确的预测。尽管有许多超参数需要优化，但有些参数比其他参数更重要。此外，一些超参数可能对结果影响很小或没有影响，但是如果没有智能方法，需要评估超参数的所有组合，以找到最佳执行模型。这使得它成为计算负担。</p><blockquote class="nm"><p id="3d72" class="nn no it bd np nq nr ns nt nu nv lt dk translated">超参数优化可以大大提高机器学习模型的准确性。</p></blockquote><p id="3110" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">通常使用<em class="lu">网格搜索</em>来搜索参数组合。一般有两种类型:<strong class="la iu"> <em class="lu">网格搜索</em> </strong>和<strong class="la iu"> <em class="lu">随机搜索</em> </strong>可用于参数整定。<strong class="la iu"> <em class="lu">网格搜索</em> </strong>将遍历整个搜索空间，因此非常有效，但也非常非常慢。另一方面，<strong class="la iu"> <em class="lu">随机搜索</em> </strong>很快，因为它会在搜索空间中随机迭代，虽然这种方法已经被证明是有效的，但它很容易错过搜索空间中最重要的点。<strong class="la iu"> <em class="lu">幸运的是，还有第三种选择:基于序列模型的优化，也称为贝叶斯优化</em>。</strong></p><p id="3f67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">贝叶斯优化</em> </strong> <em class="lu"> </em>进行参数整定是在几次迭代内确定最佳的超参数集。<em class="lu">贝叶斯</em>优化技术是一种高效的函数最小化方法，在<a class="ae mz" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">hyperpt</a>库中实现。该效率使其适合于优化训练缓慢的机器学习算法的超参数[ <a class="ae mz" href="https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。关于Hyperopt的深入细节可以在这个博客中找到[ <a class="ae mz" href="https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce" rel="noopener"> 6 </a> ]。总的来说，它首先对参数的随机组合进行采样，然后使用交叉验证方案来计算性能。在每次迭代期间，样本分布被更新，并且以这种方式，它变得更有可能对具有良好性能的参数组合进行采样。传统的<em class="lu">网格搜索</em>、<em class="lu">随机搜索、</em>和<em class="lu">远视</em>之间的巨大对比可以在这个<a class="ae mz" href="https://www.kaggle.com/code/ilialar/hyperparameters-tunning-with-hyperopt/notebook" rel="noopener ugc nofollow" target="_blank">博客</a>中找到。</p><blockquote class="nm"><p id="0cf8" class="nn no it bd np nq nr ns nt nu nv lt dk translated"><strong class="ak"><em class="op"><strong class="ak"><em class="op">贝叶斯优化</em> </strong>的</em> </strong>效率使其适合于优化训练缓慢的机器学习算法的超参数<a class="ae mz" href="https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf" rel="noopener ugc nofollow" target="_blank"> 5 </a>。</p></blockquote><p id="64ea" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated"><a class="ae mz" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">远视</em> </a> <em class="lu">被并入</em> <a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> HGBoost </em> </a> <em class="lu">的方法来做超参数优化。在下一节中，我将描述如何处理分割数据集和超参数优化的不同部分，包括目标函数、搜索空间和所有试验的评估</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d01a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">超优化梯度增强库。</h1><p id="2c1b" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">超优化的梯度增强库(<a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> HGBoost </em> </a>)，是一个针对<em class="lu"> XGBoost </em>、<em class="lu"> LightBoost </em>和<em class="lu"> CatBoost </em>进行超参数优化的Python包。它将把数据集分成<em class="lu">系列</em>、<em class="lu">测试、</em>和<em class="lu">独立验证集。</em>在训练测试集中，有使用贝叶斯优化(使用<a class="ae mz" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">超点)</em> </a> <em class="lu"> </em>来优化超参数的内部循环，以及使用<em class="lu"> k </em>折叠交叉验证来对表现最佳的模型能够概括得多好进行评分的外部循环。这种方法的优点在于，不仅可以选择精度最高的模型，还可以选择最能概括的模型。</p><p id="55e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> HGBoost </em> </a> <em class="lu">有很多优点，比如；它尽最大努力检测最能概括的模型，从而降低选择过度训练模型的可能性。它通过提供深刻的情节提供可解释的结果，例如:对超参数空间的深入检查、所有评估模型的性能、k倍交叉验证的准确性、验证集以及最后但同样重要的最佳决策树可以与最重要的特征一起绘制。</em></p><p id="0795" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu"> hgboost </em>提供的好处更多:</p><ol class=""><li id="9151" class="ob oc it la b lb lc le lf lh od ll oe lp of lt og oh oi oj bi translated">它由最流行的决策树算法组成；<em class="lu"> XGBoost、LightBoost和Catboost </em>。</li><li id="6d04" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">它包括最流行的贝叶斯优化的超参数优化库；<em class="lu">远视</em>。</li><li id="9adf" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">一种将数据集分为训练测试和独立验证以对模型性能评分的自动化方式。</li><li id="6e11" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">流水线有一个嵌套方案，内循环用于超参数优化，外循环使用<em class="lu"> k </em> -fold交叉验证来确定最健壮和性能最好的模型。</li><li id="c644" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">它可以处理<em class="lu">分类</em>和<em class="lu">回归</em>任务。</li><li id="ac3d" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">创建一个<em class="lu">多类模型</em>或增强决策树模型的<em class="lu">集合是很容易的。</em></li><li id="d715" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">它处理不平衡的数据集。</li><li id="7ca3" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">它为超参数搜索空间创建可解释的结果，并通过创建有洞察力的图来模拟性能结果。</li><li id="495d" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">它是开源的。</li><li id="a4e2" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">有很多例子证明了这点。</li></ol></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="88fc" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">走向超优化和稳健模型的步骤。</h1><p id="e2a3" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated"><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> HGBoost </em> </a>方法由流水线中的各个步骤组成。<em class="lu">图1 </em> 中描绘了示意图。让我们来看看这些步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/d07709d8e67ddf2067eee47c7f0e4eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsKi4Re8IhV2KzhRnEjQQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。HGBoost的示意图。(图片来自作者)</p></figure><p id="34b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">第一步</em> </strong>是将数据集拆分成<em class="lu">训练集</em>，一个<em class="lu">测试集，</em>和一个<em class="lu">独立验证集</em>。<em class="lu">请记住，验证集在整个训练过程中保持不变，并在最后用于评估模型性能。</em>在<em class="lu">训练测试集</em> ( <strong class="la iu"> <em class="lu">图1A </em> </strong>)中，有使用贝叶斯优化来优化超参数的内循环，以及使用外部<em class="lu">k</em>-折叠交叉验证来测试最佳性能模型在未发现的测试集上的概括程度的外循环。搜索空间取决于模型类型的可用超参数。需要优化的模型超参数见<strong class="la iu"> <em class="lu">代码段1 </em> </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分1。为XGBoost、LightBoost和CatBoost的分类和回归任务搜索空间。</p></figure><p id="7501" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在贝叶斯优化部分，我们通过<em class="lu">探索</em>整个空间来最小化超参数空间上的成本函数。与传统的<em class="lu">网格搜索</em>不同，<em class="lu">贝叶斯方法</em>搜索最佳参数集，并在每次迭代后优化搜索空间。使用<em class="lu"> k </em>折叠交叉验证方法对模型进行内部评估。或者换句话说，我们可以评估固定数量的函数评估，并采取最佳尝试。默认设置为250次评估，因此产生250个模型。<em class="lu">完成此步骤后，您现在可以选择表现最佳的模型(基于AUC或任何其他指标),然后停止。</em>然而，这是非常不鼓励的，因为我们只是在整个搜索空间中搜索了一组现在似乎最适合训练数据的参数，而不知道它概括得有多好。或者换句话说，表现最好的模型可能会，也可能不会过度训练。<em class="lu">为了避免找到过度训练的模型，k-fold交叉验证方案将继续我们寻找最稳健模型的探索。我们进行第二步。</em></p><p id="23be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">第二步</em> </strong>是根据指定的评估指标(默认设置为AUC)对模型(本例中为250个)进行排名，然后取表现最好的前<strong class="la iu"> <em class="lu"> p </em> </strong>模型(默认设置为<strong class="la iu"><em class="lu">p</em></strong><em class="lu">= 10</em>)。以这种方式，我们不依赖于具有最佳超参数的单个模型，即<em class="lu">可能或可能不会</em>过度拟合。<em class="lu">请注意，可以评估所有250个模型，但这是计算密集型的，因此我们选择最好的(性能)模型。</em>为了确定前10名执行模型如何概括，使用了<strong class="la iu"><em class="lu">k</em></strong>-折叠交叉验证方案进行评估。<strong class="la iu"> <em class="lu"> k </em> </strong> <em class="lu">的缺省值为</em> 5折，这意味着对于每一款<strong class="la iu"><em class="lu">【p】</em></strong>车型，我们都将检查跨越<strong class="la iu"> <em class="lu"> k </em> </strong> =5折的性能。为了确保前<strong class="la iu"><em class="lu">p</em></strong><em class="lu">= 10</em>模型之间的交叉验证结果具有可比性，抽样以分层方式进行。总的来说，我们会评价<strong class="la iu"><em class="lu">p</em></strong><em class="lu">x</em><strong class="la iu"><em class="lu">k</em></strong>；10x5=50个模型。</p><p id="fd51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">第三步，</em> </strong>我们有了表现最好的<strong class="la iu"> <em class="lu"> p </em> </strong>模型，我们用一种<strong class="la iu"><em class="lu">k</em></strong>-折叠交叉验证的方法计算了它们的性能。我们现在可以计算跨越<strong class="la iu"><em class="lu"/></strong>-褶皱的平均准确度(例如AUC)，然后对模型进行排序，最后选择排序最高的模型<em class="lu">。</em>交叉验证方法将有助于选择最稳健的模型，即也能在不同数据集之间推广的模型。<em class="lu">请注意，可能存在其他模型，这些模型在没有交叉验证方法的情况下会产生更好的性能，但这些模型可能会被过度训练。</em></p><p id="c2d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">第四步，</em> </strong>我们将在独立验证集上检验模型的准确性。这一套到目前为止还没有接触过，因此将给出一个很好的估计。由于我们广泛的模型选择方法(模型是否可以一般化，并限制选择过度训练模型的可能性)，我们不应该期望与我们容易看到的结果相比，在性能上有很大的差异。</p><p id="c735" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">在最后一步</em> </strong>中，我们可以使用整个数据集上的优化参数重新训练模型(<strong class="la iu"> <em class="lu">图1C </em> </strong>)。下一步(<strong class="la iu"> <em class="lu">图1D </em> </strong>)就是对结果的解读，为其创造有见地的情节。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="355c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">导入并初始化。</h1><p id="2bf8" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">在我们可以使用<em class="lu"> HGBoost </em>之前，我们需要先使用命令行<em class="lu">安装</em>:</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="4561" class="na md it ou b gy oy oz l pa pb"><strong class="ou iu">pip install hgboost</strong></span></pre><p id="39f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">安装完成后，我们可以导入<em class="lu"> HGBoost </em>并初始化。我们可以保持输入参数的默认值(在<strong class="la iu"> <em class="lu">代码段2 </em> </strong>中描述)或相应地改变它们。在接下来的部分中，我们将讨论一组参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分2。导入并初始化HGBoost。</p></figure><p id="ce80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个任务(<em class="lu">分类、回归、多类或集成</em>)和每个模型(<em class="lu"> XGBoost、LightBoost和CatBoost </em>)的初始化保持一致，这使得在不同决策树模型甚至任务之间切换变得非常容易。<em class="lu">输出也保持一致</em>是一个包含六个键的字典，如<strong class="la iu"> <em class="lu">代码段3 </em> </strong>所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分3:输出参数的描述。</p></figure><h1 id="1b5b" class="mc md it bd me mf pc mh mi mj pd ml mm jz pe ka mo kc pf kd mq kf pg kg ms mt bi translated">读取和预处理数据集。</h1><p id="74d3" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了演示，让我们使用可以使用函数<code class="fe ph pi pj ou b">import_example(data=’titanic’)</code>导入的<a class="ae mz" href="https://www.kaggle.com/c/titanic/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">泰坦尼克号</em>数据集</a>【10】。这个数据集是免费使用的，是Kaggle竞赛<a class="ae mz" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank"> <em class="lu">机器从灾难中学习</em> </a>的一部分。可以使用依赖于<a class="ae mz" href="https://github.com/erdogant/df2onehot" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="lu"> df2onehot库</em></strong></a><em class="lu">【11】的函数<code class="fe ph pi pj ou b">.preprocessing()</code>来执行预处理步骤。</em>该函数将分类值编码成一个hot，并保持连续值不变。我手动去掉了一些特性，比如<em class="lu"> PassengerId </em>和<em class="lu">名字</em> <strong class="la iu"> <em class="lu">(代码段4) </em> </strong> <em class="lu">。请注意，在正常的用例中，建议仔细进行探索性的数据分析、特性清理、特性工程等等。最大的性能提升通常来自这些最初的步骤。</em></p><p id="c840" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请记住，当使用<em class="lu"> XGBoost </em>、<em class="lu"> LightBoost、</em>或<em class="lu"> CatBoost </em>训练模型时，预处理步骤可能会有所不同。例如，模型是否需要对特征进行编码，或者它能否处理非数字特征？模型是否可以处理缺失值，或者是否应该移除这些特征？参见第<strong class="la iu"> <em class="lu">节升压算法的简要介绍</em> </strong>以了解三种模型之间优势(劣势)的更多详细信息，并相应地进行预处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分4:输入参数以及如何使用XGBoost、LightBoost和Catboost模型进行分类和回归任务以及多类和集成模型的概述。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="3318" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">训练一个模特。</h2><p id="22bf" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">初始化之后，我们可以使用<em class="lu"> XGBoost </em>决策树来训练一个模型。<em class="lu">注意，您也可以使用LightBoost或CatBoost模型，如代码部分3所示。</em>在训练过程中(运行第4节<strong class="la iu"><em class="lu"/></strong>中的<strong class="la iu"> <em class="lu">代码</em> </strong>)，它将遍历搜索空间并创建250个不同的模型(<code class="fe ph pi pj ou b">max_eval=250</code>)，为这些模型评估特定决策树模型可用的一组超参数的模型准确性(在这种情况下为<em class="lu"> XGBoost，代码第1节</em>)。接下来，对模型的准确性进行排名，选出前10个表现最好的模型<code class="fe ph pi pj ou b">top_cv_evals=10</code>。现在使用5折交叉验证方案(<code class="fe ph pi pj ou b">cv=5</code>)对这些进行进一步研究，以对模型的概括程度进行评分。在这个意义上，我们的目标是防止找到一个过度训练的模型。</p><p id="f2bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">返回不同模型的所有测试超参数，可进一步检查(手动)或使用绘图功能。例如，<em class="lu"> HGBoost </em>的输出在<strong class="la iu"> <em class="lu">代码段5 </em> </strong>中描述，更多细节在<strong class="la iu"> <em class="lu">图2 </em> </strong>中描述。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分HGBoost模型返回的结果。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/c87067f41222d2afdae870b25c0624b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIp8bBGXrcHwjNsel1_Bzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。结果输出['summary']显示了所有已执行评估的摘要。(图片来自作者)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c6b4" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">使用绘图功能创建可解释的结果。</h1><p id="12b9" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">返回最终模型后，可以创建各种图来更深入地检查模型性能和超参数搜索空间。这有助于获得更好的直觉和关于模型参数相对于性能如何表现的可解释的结果。<strong class="la iu"> <em class="lu">可以创建以下情节:</em> </strong></p><ol class=""><li id="8bb9" class="ob oc it la b lb lc le lf lh od ll oe lp of lt og oh oi oj bi translated"><em class="lu">为了更深入地研究超参数空间。</em></li><li id="3bd5" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><em class="lu">总结所有被评估的模型。</em></li><li id="6247" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><em class="lu">使用交叉验证方案显示性能。</em></li><li id="65ff" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><em class="lu">独立验证集上的结果</em>。</li><li id="4743" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><em class="lu">最佳模型和最佳性能特征的决策树图。</em></li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码部分6:对模型结果的深入研究。</p></figure><h2 id="99f8" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">超参数调谐的解释。</h2><p id="8a2c" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了更深入地研究超参数空间，可以使用功能<code class="fe ph pi pj ou b">.plot_params()</code><strong class="la iu"><em class="lu">(图3和图4) </em> </strong>。让我们从研究在贝叶斯优化过程中如何调整超参数开始<em class="lu">。</em>两个图都包含多个直方图(或核密度图)，其中每个子图是在250次模型迭代期间优化的单个参数。直方图底部的小条描绘了250个评估，而黑色垂直虚线描绘了前10个最佳性能模型中使用的特定参数值。绿色虚线描绘了没有 交叉验证方法的最佳表现模型<strong class="la iu"> <em class="lu">，红色虚线描绘了具有</em> </strong>交叉验证的最佳表现模型<strong class="la iu"> <em class="lu">。</em></strong></p><p id="c542" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看<strong class="la iu"> <em class="lu">图3 </em> </strong>。左上角有参数<strong class="la iu"><em class="lu">col sample _ bytree</em></strong>，取值范围从0到1.2。<strong class="la iu"><em class="lu">col sample _ bytree</em></strong><em class="lu">的平均值为</em> ~0.7，这表明针对该参数使用贝叶斯优化的优化样本分布已移至该值。我们表现最好的模型的值正好是<strong class="la iu"><em class="lu">col sample _ bytree =</em></strong>0.7，(<em class="lu">红色虚线</em>)。但是还有更多值得关注的。当我们现在查看<strong class="la iu"> <em class="lu">图4 </em> </strong>时，我们也有<strong class="la iu"><em class="lu">col sample _ bytree</em></strong>，其中每个点都是根据迭代次数排序的250个模型之一。水平轴是迭代，垂直轴是优化的参数值。对于该参数，在优化过程的迭代期间有向上的趋势；接近0.7的值。这表明在迭代期间，当增加<strong class="la iu"><em class="lu">col sample _ bytree</em></strong><em class="lu">的值时，优化了<strong class="la iu"><em class="lu">col sample _ bytree</em></strong>的搜索空间，并且看到了更多具有更好准确性的模型。以这种方式，所有的超参数可以相对于不同的模型被解释。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/551f564fff317bd3f422406aa54f00a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LORWaxe874H8rlZ9Qj2iZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。在回归模型的250次迭代中调整超参数。七个参数的分布。(图片来自作者)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/7216331b76fd107a64f678017748bc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XStalCvNLrmQLKYjrWavUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。在回归模型的250次迭代中调整超参数。每个点是一次迭代，为其选择特定的值。(图片来自作者)</p></figure><h2 id="6fcb" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">解释所有评估模型的模型性能。</h2><p id="203b" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">通过绘图功能<code class="fe ph pi pj ou b">.plot()</code>，我们可以深入了解250款车型的性能(本例中为AUC)(<strong class="la iu"><em class="lu">图4 </em> </strong>)。这里，绿色虚线再次描绘了没有 交叉验证方法的表现最好的模型<strong class="la iu"> <em class="lu">，而红色虚线描绘了具有</em> </strong>交叉验证的表现最好的模型<strong class="la iu"> <em class="lu">。<strong class="la iu"> <em class="lu">图5 </em> </strong>描绘了与具有默认参数的模型相比，具有优化超参数的最佳表现模型的ROC曲线。</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/ffc449c77c360dd767abf8fbf8964990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjCFGwzaifl1Gq7YlHylew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。250次评估和交叉验证的模型准确性。(图片来自作者)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/a05d285b0df7c76565ed35727939fa88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CGe11InPHVmhsNhpGkl1Vw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。与具有默认参数的模型相比，具有优化超参数的最佳性能模型的ROC曲线。(图片来自作者)</p></figure><h2 id="8436" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated"><em class="op">最佳模型的决策树图。</em></h2><p id="d358" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有了决策树图(<strong class="la iu"> <em class="lu">图6 </em> </strong>)我们可以更好的理解模型是如何工作的。它也可以给我们一些直觉，这样一个模型是否可以推广到其他数据集。注意，默认情况下会返回最佳树<code class="fe ph pi pj ou b">num_tree=0</code>，但是通过指定输入参数<code class="fe ph pi pj ou b">.treeplot(num_trees=1)</code>可以创建许多可以返回的树。此外，我们还可以绘制出性能最佳的特性(此处未显示)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/1e91e29b66c3456b26ecc86790cd0110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYo-FrUT5a9piEyD9EgS2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。最佳模型的决策树图。(图片来自作者)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="d9b6" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">做预测。</h2><p id="92cb" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">有了最终的训练模型后，我们现在可以用它对新数据进行预测。假设<strong class="la iu"> <em class="lu"> X </em> </strong>是新数据，并且在训练过程中进行了类似的预处理，那么我们可以使用<code class="fe ph pi pj ou b">.predict(X)</code>函数进行预测。该函数返回分类概率和预测标签。示例见<strong class="la iu"> <em class="lu">代码第7节</em> </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码第7部分。预测。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ac10" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">保存并加载模型。</h2><p id="b629" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">保存和加载模型会变得很方便。为了完成这个，有两个函数:<code class="fe ph pi pj ou b"><strong class="la iu">.save()</strong></code>和函数<code class="fe ph pi pj ou b"><strong class="la iu">.load()</strong></code> <strong class="la iu"> <em class="lu">(代码段8) </em> </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or os l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码第8部分。保存和加载模型。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f4d5" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结束了。</h1><p id="3b37" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我演示了如何通过将数据集分为训练集、测试集和独立验证集来训练具有优化的超参数的模型。在训练测试集中，有使用贝叶斯优化来优化超参数的内部循环，以及基于k-fold交叉验证来对表现最好的模型的概括程度进行评分的外部循环。以这种方式，我们尽最大努力选择能够概括和具有最佳准确性的模型。</p><p id="3e72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练任何模型之前，一个重要的部分就是做典型的建模工作流程:<em class="lu">首先做探索性数据分析(EDA)，迭代做清洗，特征工程，特征选择。这些步骤通常会带来最大的性能提升。</em></p><p id="276e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> HGBoost </em> </a> <em class="lu"> </em>支持学习<a class="ae mz" href="https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d" rel="noopener"> <em class="lu">分类模型</em> </a> <em class="lu">，回归模型</em>，<em class="lu">多类模型</em>，甚至是boosting树模型的<em class="lu">集合。对于所有任务，应用相同的程序以确保选择最佳性能和最稳健的模型。如果你需要更多关于如何训练一个可解释的梯度推进分类模型的细节，请阅读这个博客<em class="lu">【12】</em>。</em></p><p id="2490" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，<em class="lu"> HGboost </em>依靠<a class="ae mz" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">hyperpt</a>进行贝叶斯优化，这是最流行的超参数优化库之一。最近开发的另一种贝叶斯优化算法叫做<a class="ae mz" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a>。如果你想阅读更多的细节，以及两种方法之间的比较，试试这个博客。</p><p id="1450" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是了。我希望你喜欢读它！如果有不清楚的地方或者你有其他建议，请告诉我。</p><p id="5fc2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意安全。保持冷静。</p><p id="e580" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">欢呼，E. </em> </strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="89f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">如果你觉得这篇文章很有帮助，</em> <a class="ae mz" href="https://erdogant.medium.com/" rel="noopener"> <em class="lu">关注我</em> </a> <em class="lu">，Star the</em><a class="ae mz" href="https://github.com/erdogant/hgboost" rel="noopener ugc nofollow" target="_blank"><em class="lu">HGBoost Github Repo</em></a><em class="lu">，和/或支持我的内容，并通过使用我的</em> <a class="ae mz" href="https://erdogant.medium.com/membership" rel="noopener"> <em class="lu">推荐链接</em> </a> <em class="lu">注册一个媒体会员来访问类似的博客。</em></p><h2 id="dc75" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">软件</h2><ul class=""><li id="20fb" class="ob oc it la b lb mu le mv lh pq ll pr lp ps lt pt oh oi oj bi translated">HGBoost Github </li><li id="79be" class="ob oc it la b lb ok le ol lh om ll on lp oo lt pt oh oi oj bi translated"><a class="ae mz" href="https://erdogant.github.io/hgboost/" rel="noopener ugc nofollow" target="_blank"> HGBoost文档页面</a></li><li id="f2e9" class="ob oc it la b lb ok le ol lh om ll on lp oo lt pt oh oi oj bi translated"><a class="ae mz" href="https://erdogant.github.io/hgboost/pages/html/Blog.html" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li></ul><h2 id="b8d8" class="na md it bd me nb nc dn mi nd ne dp mm lh nf ng mo ll nh ni mq lp nj nk ms nl bi translated">我们连线吧！</h2><ul class=""><li id="ff1c" class="ob oc it la b lb mu le mv lh pq ll pr lp ps lt pt oh oi oj bi translated">让我们在LinkedIn上联系吧</li><li id="73b8" class="ob oc it la b lb ok le ol lh om ll on lp oo lt pt oh oi oj bi translated"><a class="ae mz" href="https://github.com/erdogant" rel="noopener ugc nofollow" target="_blank">在Github上关注我</a></li><li id="8af0" class="ob oc it la b lb ok le ol lh om ll on lp oo lt pt oh oi oj bi translated"><a class="ae mz" href="https://erdogant.medium.com/" rel="noopener">在媒体上跟随我</a></li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9523" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><ol class=""><li id="9f32" class="ob oc it la b lb mu le mv lh pq ll pr lp ps lt og oh oi oj bi translated">南珠等，<a class="ae mz" href="https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> XGBoost:在Spark和Flink </em> </a>中实现Winningest Kaggle算法。</li><li id="229a" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">米盖尔·菲耶罗等人<a class="ae mz" href="https://www.kdnuggets.com/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html" rel="noopener ugc nofollow" target="_blank"> <em class="lu">从基准测试快速机器学习算法中吸取的教训</em> </a> <em class="lu">。</em></li><li id="097f" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><a class="ae mz" href="https://lightgbm.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"> <em class="lu">欢迎来到LightGBM的文档</em> </a> <em class="lu">！</em></li><li id="4fa1" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><a class="ae mz" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> CatBoost是一个高性能的开源库，用于决策树</em> </a> <em class="lu">上的梯度提升。</em></li><li id="233c" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">James Bergstra等人，2013，<a class="ae mz" href="https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> Hyperopt:一个优化机器学习算法超参数的Python库。</em>T57】</a></li><li id="786e" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">Kris Wright，2017，<a class="ae mz" href="https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce" rel="noopener"> <em class="lu">用超视进行参数调谐。</em> </a></li><li id="a23b" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><em class="lu"> E.Taskesen，2019，</em> <a class="ae mz" href="https://github.com/erdogant/classeval" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> Classeval:两类和多类分类器的监督预测评估</em> </a> <em class="lu">。</em></li><li id="caf3" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated"><a class="ae mz" href="https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/ch04.html" rel="noopener ugc nofollow" target="_blank">爱丽丝一怔，<em class="lu">第四章。超参数调谐</em>。</a></li><li id="8151" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">E.Taskesen，2020，<a class="ae mz" href="https://github.com/erdogant/hgboost" rel="noopener ugc nofollow" target="_blank"> <em class="lu">超优化梯度提升</em> </a> <em class="lu">。</em></li><li id="217f" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">卡格尔，<a class="ae mz" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank"> <em class="lu">机器从灾难中学习</em> </a> <em class="lu">。</em></li><li id="0374" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">E.Taskesen，2019，<a class="ae mz" href="https://github.com/erdogant/df2onehot" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> df2onehot:将非结构化数据帧转换为结构化数据帧。</em> </a></li><li id="9c67" class="ob oc it la b lb ok le ol lh om ll on lp oo lt og oh oi oj bi translated">E.Taskesen，2022，<a class="ae mz" href="https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d" rel="noopener"> <em class="lu">使用贝叶斯超参数优化创建可解释的梯度推进分类模型的实践指南。</em> </a></li></ol></div></div>    
</body>
</html>