<html>
<head>
<title>Deep Video Inpainting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度视频修复</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-video-inpainting-756e60ddcaaf#2022-03-23">https://towardsdatascience.com/deep-video-inpainting-756e60ddcaaf#2022-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2e19" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用深度神经网络去除视频中不需要的对象。问题设置和最先进的审查。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0c9caa71650cf0b96a9b3a11cf73cf4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmGbXBbvGc_eFpFtVH-tFA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c340" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然抹去记忆的技术，如著名电影<em class="lr"/><a class="ae ls" href="https://www.imdb.com/title/tt0338013/" rel="noopener ugc nofollow" target="_blank"><em class="lr">【一尘不染的心灵的永恒阳光】</em></a><em class="lr"/>(还)不存在，但我们在图像和视频方面取得了良好的进展。把这个拿着啤酒的陌生人从假日视频中移除，破坏了令人惊叹的日落，这不是很酷吗？</p><p id="8f5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">视频修复使我们能够屏蔽视频中不想要的部分。一段时间以前，这项工作需要动画师和图形设计师花费数百个小时来一帧一帧地手工编辑视频。</p><p id="f181" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是随着机器学习的出现，我们经常可以实现结果，如果不是更好，但肯定非常接近艺术家的能力——用无缝适合视频的内容替换不需要的对象。</p><p id="89f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随便看看下面的一个小片段，是不是印象深刻？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/2b6c3dc09cfd8e0ec8bcbe6ceab3460d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MgiLkhtQUeJIih_7BsLiMQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<a class="ae ls" href="http://chengao.vision/FGVC/" rel="noopener ugc nofollow" target="_blank">流引导视频完成方法(FGVC) </a>制作的视频。GIF 由<a class="ae ls" href="http://chengao.vision/" rel="noopener ugc nofollow" target="_blank">高晨</a>制作。</p></figure><p id="ab91" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对我来说，上面的动画看起来难以置信！注意，例如，跳舞女孩前景中的草的高频纹理是如何被保留下来的。为了了解我们是如何得到这些结果的，下面我设置了问题，讨论了可用的解决方案和基准。在最底部，我还包含了每个讨论过的方法的代码链接。说完这些，让我们开始吧！</p><h1 id="b028" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">我们要解决什么？</h1><p id="8548" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">从图像修复的例子开始更容易，目标是用背景替换场景的一部分，例如 Adobe Photoshop 为照片提供了这个选项。</p><div class="kg kh ki kj gt ab cb"><figure class="mr kk ms mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a8998f9b78d3de9632e5625ef0203530.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*wAijNu1fZA7c8VLQ0297lA.png"/></div></figure><figure class="mr kk mx mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9cbb0e2a76afe648a7fab6cba86d14cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EwnNcjIb1HsQTdlFxYh2aw.png"/></div></figure><figure class="mr kk my mt mu mv mw paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/bb7fea2c128c99c3719bcbe7974598f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*2A1Wvk-QR9xzsKIR7_auOQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk mz di na nb translated">用 Adobe Photoshop 修复图像的例子。图像、选定区域、结果。图片作者。</p></figure></div><p id="ea25" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在蒙版这个图像时，我指定了我想要修补的区域。Photoshop 使用“<em class="lr">内容感知填充</em>”将前景(所选区域)替换为背景纹理。</p><p id="f948" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于一个视频来说，这个过程会更加费力，因为我们需要标记物体出现的所有帧。</p><p id="1bb8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">视频修补在概念上类似于图像修补，但是有一点复杂——需要满足整个视频的时间一致性。随着时间逐渐变化的视频不应该具有闪烁的伪像或者在修补区域中对象的颜色或形状的突然变化。并且有许多变量会影响实现时间一致性的难度——场景的复杂性、摄像机位置的改变、场景的改变或用于修补的选定区域的移动(例如移动的对象)。</p><p id="2544" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然与图像修复相比，视频修复更具挑战性，但由于需要满足帧之间的时间一致性，它固有地具有更多线索，因为一帧中缺失区域的有效像素可能出现在其他帧中。</p><h1 id="85f0" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">可用的解决方案</h1><p id="c43a" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">通常可用的方法可以通过修补机制来区分。它们可以是<em class="lr">复制和粘贴</em>——在附近的帧中搜索丢失像素的信息，一旦找到就复制到指定的位置，或者是<em class="lr">生成</em>，使用某种形式的生成模型根据整个视频的内容在该区域中产生像素信息。</p><p id="8b4d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两种方法各有利弊。复制和粘贴方法在可以通过视频跟踪像素的情况下工作良好，但在无法检索信息时显然会失败，例如删除了静止区域的静止视频。在这种情况下，创成式模型会更合适。与此同时，生成式方法在重建中可能不太准确，通常会产生模糊的、跨可能选项的平均解。</p><p id="37f4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常提出的解决方案采用<a class="ae ls" rel="noopener" target="_blank" href="/cnn-cheat-sheet-the-essential-summary-for-a-quick-start-58820a14d3b4?source=your_stories_page-------------------------------------">卷积神经网络(CNN)</a>、视觉转换器或 3D CNNs。后者由于内存和时间的限制而不太受欢迎。</p><h2 id="3f42" class="nc lv iq bd lw nd ne dn ma nf ng dp me le nh ni mg li nj nk mi lm nl nm mk nn bi translated">复制并粘贴</h2><p id="bb2b" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">为了从邻近的帧中复制丢失像素的内容，我们首先需要追踪该像素的位置。这通常通过<a class="ae ls" href="https://nanonets.com/blog/optical-flow/" rel="noopener ugc nofollow" target="_blank">光流</a>来完成。</p><p id="f95e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">光流提供了关于背景和物体跨帧位移的信息。如果计算的流量误差很小，我们可以从第一帧到最后一帧跟踪特定像素的精确位置。光流可以向前和向后，即从第一帧向前计算位移，或者以相反的顺序从最后一帧开始计算位移。</p><p id="00d4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">寻找光流是具有挑战性的，因为可能有多种可能的解决方案。这就是为什么后向流和前向流经常不匹配，并且经常两者都被计算，并且联合信息被用于改进估计。关于光流方法的快速解释，请查看这篇<a class="ae ls" rel="noopener" target="_blank" href="/understanding-optical-flow-raft-accb38132fba">文章</a>。</p><p id="833d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们不能借用内容来从附近的帧中恢复当前帧，则仍然未知的区域可以与单个帧的单个图像修补合成，然后传播到视频的其余部分。</p><p id="f0d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面我将按照时间顺序介绍复制和粘贴架构。由于用于流量估计的基于优化的方法的计算复杂性很高，下面描述的许多方法或者找到替代、简化或者近似它的机制。</p><p id="d898" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1905.01639.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">深度视频修复</strong> </a> <strong class="kx ir">，2019 </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/316bb6c63a3363743abaa68b26abbe96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hH4b73fv00qgDKRJNP3awg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/1905.01639.pdf" rel="noopener ugc nofollow" target="_blank">大婚金</a></p></figure><p id="3533" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该论文的叙述围绕确保修补视频的时间一致性的方法而构建。作者为第 t 帧的生成设置了三个要求。它应该与时空相邻帧 X(t+/-N)一致，其中 N 表示时间半径；先前生成的帧 yt1；以及之前的所有历史。</p><p id="3cae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一种机制是基于反馈上一步生成的帧以及当前步骤中的帧——循环反馈。在组合之前，源特征点和参考特征点是对齐的。这种策略有助于模型从相邻帧借用可追踪的特征。为了实现这一点，流量子网络被用来估计四个空间尺度(1/8、1/4、1/2 和 1)的特征图之间的流量。</p><p id="e08f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第二种机制包括在具有<a class="ae ls" href="https://arxiv.org/abs/1612.01925" rel="noopener ugc nofollow" target="_blank">流网 2 </a>的<em class="lr"> </em>两个连续帧之间的最细尺度的显式流监控。</p><p id="a701" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然循环反馈连接了连续的帧，但填补大洞需要更多的长期知识。第三种机制基于时间记忆层，有助于长期连接不同时间步长的内部特征。这里，作者采用了卷积 LSTM ( <a class="ae ls" href="https://proceedings.neurips.cc/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> ConvLSTM </a>)层和翘曲损耗。</p><p id="60f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型用三种损失训练——重建损失(L1 + SSIM)、流量估计损失和扭曲损失。</p><p id="98dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1908.11587.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">复制粘贴网络进行深度视频修复</strong> </a> <strong class="kx ir">，2019 </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/0e9a1f845b83d839a965ceaa869696a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*asZ6w5mRQv1tKjc0VgrIOg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由李成浩提供。</p></figure><p id="1880" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该网络基于三个阶段—对齐、复制和粘贴。</p><p id="196c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在对齐步骤中，参考帧与测试帧对齐，以匹配像素之间的对应关系。</p><p id="831c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">复制阶段由编码器和上下文匹配模块组成。在编码器模块中，对齐的帧及其掩码通过特征编码器。所得的嵌入然后被传递到上下文匹配模块——帧特征和它们的可见性图之间的余弦相似性的组合。这个步骤产生了每个像素对丢失区域的贡献的权重。</p><p id="175e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解码器(粘贴模块)获取权重和连接的帧特征，并输出结果图像。</p><p id="9f3c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用 L1、风格、感知和总变差损失的组合来训练整个网络。</p><p id="cff1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1905.02884.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">深度流引导图像修复</strong> </a> <strong class="kx ir">，2019 (DFC-Net) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/c698a917a7afd4d9370df41276d43b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QTTEt5KzEprQKuyScqMeg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/1905.02884.pdf" rel="noopener ugc nofollow" target="_blank">徐睿</a>拍摄。</p></figure><p id="db16" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文通过一个由粗到细的深度流完井网络(DFC-Net)解决了光流的高计算复杂度问题，该网络由三个小的子网(DFC-S)组成。这三个子网中的每一个都接受大小调整为原始大小的 1/2、2/3 和 1 的输入。</p><p id="65d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个 DFC-S 接受两种类型的输入:用<a class="ae ls" href="https://arxiv.org/abs/1612.01925" rel="noopener ugc nofollow" target="_blank"> FlowNet2 </a>估计的流图的串联，以及相关的二进制掩码序列，指示每个流图中缺失的区域。网络输出流量的精确估计。</p><p id="991d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">估计的流场用于引导像素的传播以填充缺失的区域。具体而言，DFC-Net 遵循由粗到细的细化来完成流场，同时通过硬流示例挖掘来进一步提高其质量。</p><p id="b216" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">优化目标是最小化预测和地面真实流量之间的 L1 距离。然而，因为视频序列中的大部分流动区域是平滑的，所以使用 L1 损失直接导致在训练期间平滑区域占优势，而预测中的边界区域是模糊的。为了克服这一点，所有像素按照损失的降序排序。前 p 个百分比的像素被标记为硬样本。它们的损失被加权以迫使模型更多地关注那些区域。</p><p id="7558" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/abs/2009.01835" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">【流边引导】视频完成</strong> </a> <strong class="kx ir">、2020 (FGVC) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/098276956271882ed73a46a9557b51a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKWJW4iOPzQFCWAQWCN7ag.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/2009.01835.pdf" rel="noopener ugc nofollow" target="_blank">高晨</a>提供。</p></figure><p id="bcb9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">流边缘引导的视频完成(FGVC)由流完成、时间传播和融合步骤组成。</p><p id="a7c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在流完成步骤中，用流网 2 计算相邻和非相邻帧之间的反向和正向流。为了提供尽可能多的信息，该方法使用本地和非本地帧之间的流。由于流中的一些信息丢失(由于掩码),该方法首先使用 Canny 边缘检测器和 EdgeConnect 在流中找到边缘，以连接不相交的区域。基于边缘，执行边缘引导的流动补偿。对于非局部帧，使用单应变形来补偿大的运动。</p><p id="52c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在时间传播期间，遵循流动轨迹来传播每个缺失像素的一组候选像素。分配五个候选像素——两个来自前向和后向流动通道，三个来自远处的非局部帧。</p><p id="c5f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在无缝融合步骤中，通过置信度加权平均来融合候选对象。在梯度域中执行融合，以避免可见的颜色接缝。最终融合是通过泊松优化获得的。</p><p id="be80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/abs/2108.06765" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"/></a><strong class="kx ir">，2021 (VOIN) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/d96e0520636e87038655d97e567e766b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHXtJR1iQmkdLyiIY0OJqg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/94b5f820adf72252244ceb8c6d61e501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sON14uNhuidjFbqdHnIZbw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/abs/2108.06765" rel="noopener ugc nofollow" target="_blank">雷科</a></p></figure><p id="63d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">VOIN 基于这样的假设，即关于被遮罩对象遮挡的形状的知识可以提高算法的性能。</p><p id="0529" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型由三个模块组成——(I)基于变换器的形状完成模块，其学习从可见掩模区域推断完整的对象形状；(ii)遮挡感知流完成模块，用于通过加强流一致性来捕捉移动对象并跨甚至在时间上相距很远的帧传播内容；(iii)流动引导的视频对象修补</p><p id="cb9c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在形状完成模块中，对象轮廓引导流动预测过程。在整个对象区域内实施流动平滑(流动梯度通常很小，除非沿着不同的对象运动边界)。</p><p id="2408" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了填充剩余像素，作者训练了一个遮挡感知门控生成器来修复视频对象的遮挡区域。该模型采用残差时间移位模块(TSM)作为构建模块，将部分通道沿时间维度移位，进行联合时空特征学习。</p><p id="93a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型使用两个鉴别器进行训练——一个用于感知质量和时间一致性，另一个用于对象语义。为了调整流完成网络，采用了 L1 流损失、图像梯度损失、拉普拉斯金字塔损失和幻觉内容的扭曲损失。用二元交叉熵和<a class="ae ls" href="https://arxiv.org/pdf/1606.04797.pdf" rel="noopener ugc nofollow" target="_blank">骰子</a>损失来训练形状完成模块。</p><p id="2c66" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了这些方法，作者还提供了一个新的数据集 YouTube-VOI-包含 5305 个视频，一个 65 类标签集，包括常见对象，如人、动物和车辆，以及超过 200 万个用于移动视频对象的遮挡和可见遮罩</p><h2 id="9b34" class="nc lv iq bd lw nd ne dn ma nf ng dp me le nh ni mg li nj nk mi lm nl nm mk nn bi translated">生成模型</h2><p id="f736" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">标准的图像生成模型，例如基于 2D CNN 的生成敌对网络(GANs ),尽管在图像修复方面取得了成功，但对于视频修复却不太适用。其主要原因是无法说明像素之间的时间关系。因此，使用 3D CNNs 或<a class="ae ls" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">视觉转换器</a>。由于这种类型中表现最好的模型是基于变形金刚的，所以我把重点放在它们身上。作为视觉变形金刚的回顾，看看<a class="ae ls" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">的博客</a>。</p><p id="777b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，变形金刚的一个大缺点是对注意力层的内存要求非常高，这限制了它们的输入大小。不同的模型以不同的方式处理这个问题。</p><p id="561c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/2007.10247.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">学习联合时空变换进行视频修复</strong> </a> <strong class="kx ir">，2020 (STTN) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/233a34ec1975ee00399611ff10f19d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqFG_gltUgjwGXk6iej5bA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/2007.10247.pdf" rel="noopener ugc nofollow" target="_blank">曾</a>提供。</p></figure><p id="1bfc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型的设计基于多尺度多头变压器。STTN 将以目标帧为中心的帧窗口和从视频的其余部分均匀采样的帧作为输入。该模型使用不同比例的小块，而不是处理完整的帧。</p><p id="19a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">STTN 由三部分组成——帧级编码器、多层多头时空转换器和帧级解码器。帧级编码器是 CNN，它为每一帧编码深度特征。类似地，帧级解码器将特征解码回帧。</p><p id="80ee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">时空转换器是核心组件，学习所有缺失区域的联合时空转换。不同的变换头计算不同尺度的空间块的相似性。转换器运行“嵌入-匹配-参与”算法。在嵌入步骤中，用编码器或前层变换器提取的特征被映射到查询和存储器(键值)。在匹配步骤中，通过匹配从所有帧中提取的空间碎片中的查询和关键字来计算区域相似性。在关注步骤中，检测相关区域并对缺失区域进行变换，其输出通过对相关面片进行加权求和来计算。</p><p id="d283" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型是用 L1 和对抗性损失训练的。</p><p id="857d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/2104.06637.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">解耦空间变压器用于视频修复</strong> </a> <strong class="kx ir">，2021 (DSTT) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/7a9be685113cc97880e01391990fdde6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUNmPyDnnVbmexAqkXWbJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/2109.02974.pdf" rel="noopener ugc nofollow" target="_blank">刘锐</a>拍摄。</p></figure><p id="fe37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">DSTT 是不同于 STTN 的建筑，因为它分别处理空间和时间特征。该方法基于几个架构模块——堆叠特征编码器(<a class="ae ls" rel="noopener" target="_blank" href="/cnn-cheat-sheet-the-essential-summary-for-a-quick-start-58820a14d3b4?source=your_stories_page-------------------------------------">卷积神经网络</a>)、一个空间解耦和一个临时解耦的变换器以及一个解码器。</p><p id="bd48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">分层特征编码器是卷积层的堆叠，底层特征附加到更深的层，从而从一层到另一层传播低级细节。</p><p id="47e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，单个帧的聚集特征被分割成小块，并通过空间解耦的变换器。暂时解耦的变换器使用在相同空间位置的几个连续帧的特征片作为输入嵌入。然后，输出被汇总并通过解码器。</p><p id="a06e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型是用 L1 和对抗性损失训练的。</p><p id="a7b8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/2109.02974.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> FuseFormer:融合变形金刚中的细粒度信息进行视频修复</strong> </a> <strong class="kx ir">，(2021) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/a092e3931b394291938887bbe18b9ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-0RVRd2Xsn2zC8YQdbBlQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae ls" href="https://arxiv.org/pdf/2109.02974.pdf" rel="noopener ugc nofollow" target="_blank">刘锐</a>提供。</p></figure><p id="c84c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇论文是由研究 DSTT 的同一个小组提出的。然而，这部作品并没有提到 DSTT，只是将自己与 STTN 进行了对比。作者依赖于最初提出的<a class="ae ls" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">视觉变形器</a> (ViT)的单一尺度架构，而不是在变形器之前引入基于不同尺度补丁的基于 CNN 的特征提取器。</p><p id="24c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">新模型引入了对原始 ViT 的两个修改——贴片软分裂和软贴片组合，以及融合形成器块。正如这篇论文所说，所有这些都提高了基线模型的性能。</p><p id="c48c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">软分割模块将图像分割成具有重叠区域的小块，并且相应地，软合成将这些重叠的小块组合回图像。</p><p id="bd25" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">融合前馈网络(F3N)取代了标准变压器模型中的两层 MLPs。在 F3N 中，在两个完全连接的层之间，每个 1D 令牌都以其原始的空间形状被整形回 2D 面片，然后被软合成为一幅完整的图像。重叠区域中像素的重叠特征将来自所有相邻块的对应值相加，用于进一步的细粒度特征融合。然后，面片被软分割和平坦化成 1D 向量，这些向量被馈送给第二个 MLP。</p><p id="bcbc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型是在 L1 和对抗性损失的组合上训练的。</p><h1 id="bd50" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">基准测试—如何比较模型</h1><p id="a861" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">评估视频生成模型的困难在于找到一个好的基准来测试:空间质量、时间一致性和真实性。视频修复是一项特别具有挑战性的评估任务，因为它取决于多种因素，如相机运动、对象运动和遮罩大小，并且在评估性能时应该跟踪所有这些因素。</p><p id="55c4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些方法性能的最佳评判者是人类观察者。然而，用人类参与者进行实验既昂贵又耗时。我在两篇文章中详细讨论了改进当前数据收集方法的挑战和方法:<a class="ae ls" rel="noopener" target="_blank" href="/deep-image-quality-assessment-30ad71641fac">深度图像质量评估</a>和<a class="ae ls" rel="noopener" target="_blank" href="/active-sampling-for-pairwise-comparisons-476c2dc18231">用于成对比较的主动采样</a>。</p><p id="f44e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">代替人类观察者的是客观的质量度量。最近的一篇论文有一个吸引人的名字:<a class="ae ls" href="https://arxiv.org/pdf/2105.05332.pdf" rel="noopener ugc nofollow" target="_blank">魔鬼在细节中:视频修复的诊断评估基准</a>在一个新的详尽的基准数据集上比较了最先进的方法，该数据集涵盖了视频修复的各种场景的过多组合:</p><ul class=""><li id="b037" class="nx ny iq kx b ky kz lb lc le nz li oa lm ob lq oc od oe of bi translated">面具尺寸小/大</li><li id="5402" class="nx ny iq kx b ky og lb oh le oi li oj lm ok lq oc od oe of bi translated">掩模形状变化缓慢/快速</li><li id="dc81" class="nx ny iq kx b ky og lb oh le oi li oj lm ok lq oc od oe of bi translated">掩模移动速度慢/快</li><li id="293e" class="nx ny iq kx b ky og lb oh le oi li oj lm ok lq oc od oe of bi translated">背景运动高/低</li><li id="8696" class="nx ny iq kx b ky og lb oh le oi li oj lm ok lq oc od oe of bi translated">摄像机运动高/低</li></ul><p id="be30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该基准包括九个图像质量指标，包括典型的图像质量和逼真度 FID、LPIPS、PSNR 和 FSIM，以及考虑时间一致性的指标——VFID、PVCS 和扭曲误差。</p><h1 id="f6f1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">摘要</h1><p id="4670" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">创建一个功能齐全的视频修复应用程序是一个巨大的工程挑战。虽然有一些非常令人印象深刻的结果，但所有方法仍然会产生明显的伪像，而获得实时速度是一项巨大的工程工作。</p><p id="f583" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一个需要解决的现象是与修补对象相关联的帧部分，例如阴影或反射，它们有时很难被发现，但是仍然需要被去除。处理这类问题的方法之一是最近由谷歌、牛津和魏茨曼科学研究所发表的一篇 Omnimate 论文。</p><p id="38f2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，和往常一样，巨大的进步伴随着技术的误用。视频修复最明显的例子之一就是深度赝品。这在以持续不断的信息流为基础的现代世界中可能会在更大范围内严重损害个人生活和社会。想象一下法庭上的伪造证据或社交媒体上的错误信息，这些信息的来源往往未经核实。这就是为什么有论文通过开发修复区域检测的方法来解决这个问题，就像<a class="ae ls" href="https://www.bmvc2021-virtualconference.com/assets/papers/0398.pdf" rel="noopener ugc nofollow" target="_blank">的那篇</a>。</p><p id="1bcb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我谈到了以下论文中的深度视频修复列表作品:</p><p id="87f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">复制粘贴:</strong> <a class="ae ls" href="https://arxiv.org/pdf/1905.01639.pdf" rel="noopener ugc nofollow" target="_blank">深度视频修复</a>，2019，(<a class="ae ls" href="https://github.com/mcahny/Deep-Video-Inpainting" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em></a>)；<a class="ae ls" href="https://arxiv.org/pdf/1908.11587.pdf" rel="noopener ugc nofollow" target="_blank">复制粘贴网络进行深度视频修复</a>，2019，(<a class="ae ls" href="https://github.com/shleecs/Copy-and-Paste-Networks-for-Deep-Video-Inpainting" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em></a>)；<a class="ae ls" href="https://arxiv.org/pdf/1905.02884.pdf" rel="noopener ugc nofollow" target="_blank">深度流引导图像修复</a> (DFC-Net)，2019，(<a class="ae ls" href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em></a>)；<a class="ae ls" href="https://arxiv.org/abs/2009.01835" rel="noopener ugc nofollow" target="_blank">流边引导视频完成</a> (FGVC)，2020，(<a class="ae ls" href="https://github.com/vt-vl-lab/FGVC" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代号</em></a>)；<a class="ae ls" href="https://arxiv.org/abs/2108.06765" rel="noopener ugc nofollow" target="_blank">遮挡感知视频对象修复</a> (VOIN)，2021 ( <a class="ae ls" href="http://www.kelei.site/voin/" rel="noopener ugc nofollow" target="_blank"> <em class="lr">项目-页面</em> </a>)。</p><p id="594a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">生成式:</strong> <a class="ae ls" href="https://arxiv.org/pdf/2007.10247.pdf" rel="noopener ugc nofollow" target="_blank">学习联合时空变换进行视频修复</a> (STTN)，2020，(<a class="ae ls" href="https://github.com/researchmm/STTN" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em></a>)；<a class="ae ls" href="https://arxiv.org/pdf/2104.06637.pdf" rel="noopener ugc nofollow" target="_blank">用于视频修复的解耦空间变换器</a> (DSTT)，2021，(<a class="ae ls" href="https://github.com/ruiliu-ai/DSTT" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em></a>)；<a class="ae ls" href="https://arxiv.org/pdf/2109.02974.pdf" rel="noopener ugc nofollow" target="_blank"> FuseFormer:融合变形金刚中的细粒度信息进行视频修复</a>，2021，(<a class="ae ls" href="https://github.com/ruiliu-ai/FuseFormer" rel="noopener ugc nofollow" target="_blank"> <em class="lr">代码</em> </a>)。</p><p id="3c58" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望你喜欢这篇文章，如果是的话，让我们一起关注这个领域——与朋友分享吧！</p><p id="546e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要<em class="lr">阅读更多关于机器学习和图像处理的</em><em class="lr">按订阅</em>！</p><h2 id="8c5e" class="nc lv iq bd lw nd ne dn ma nf ng dp me le nh ni mg li nj nk mi lm nl nm mk nn bi translated">喜欢作者？保持联系！</h2><p id="983b" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我错过了什么吗？不要犹豫，直接在<a class="ae ls" href="https://www.linkedin.com/in/aliakseimikhailiuk/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ls" href="https://twitter.com/mikhailiuka" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上给我留言、评论或发消息吧！</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/deep-image-quality-assessment-30ad71641fac"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">深度图像质量评估</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">深入研究全参考图像质量评估。从主观画质实验到深层客观…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/perceptual-losses-for-image-restoration-dd3c9de4113"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">深度图像恢复的感知损失</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">从均方误差到 GANs——什么是好的感知损失函数？</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc kp oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/cnn-cheat-sheet-the-essential-summary-for-a-quick-start-58820a14d3b4"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">卷积神经网络——概要</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">如何使用、何时使用以及提高性能的有用技巧</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc kp oo"/></div></div></a></div></div></div>    
</body>
</html>