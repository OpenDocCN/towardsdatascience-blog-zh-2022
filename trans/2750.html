<html>
<head>
<title>Data Wrangling with PySpark, for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark的数据争论，适合初学者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-wrangling-with-pyspark-for-beginners-3f2197c81511#2022-06-14">https://towardsdatascience.com/data-wrangling-with-pyspark-for-beginners-3f2197c81511#2022-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1890" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从熊猫用户的角度看PySpark初级读本</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9f996b50efa821a75dc499c0b83fc171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qj2fa6tZqy8lyqKQ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">戈兰·艾沃斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f381" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫图书馆是数据科学家的主要宝库，由于其功能和易用性，许多人开始依赖该模块进行数据处理。</p><p id="945b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，当谈到处理大数据时，Pandas是不够的，随着大数据变得更加普遍，这提出了一个问题。</p><p id="51a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> PySpark </strong>，一个用于Apache Spark的Python API，是处理大量数据的绝佳选择。</p><p id="3e34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，PySpark并没有获得和熊猫一样的吸引力，尽管它有着巨大的效用。由于PySpark不是一个纯Python框架，它的学习曲线更长，这可能会阻碍其他人学习使用该工具。</p><p id="8bbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们从一个熊猫用户的角度探索PySpark作为一个数据争论的工具。</p><h2 id="0def" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">为什么PySpark适合大数据？</h2><p id="6aae" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">PySpark在处理大型数据集方面优于Pandas，这源于它的两个最明显的特性。</p><ol class=""><li id="921b" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu"> PySpark采用并行处理</strong></li></ol><p id="d325" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与在单台机器上运行所有操作的Pandas不同，PySpark利用并行处理，这需要在多台机器上并行运行操作，从而以更快的处理速度获得结果。</p><p id="a3f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。PySpark实现懒惰评估</strong></p><p id="b53f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark还通过引入惰性评估来优化其操作。简单来说，它只会在必要的时候导出运算的结果。这种方法有助于最小化任何数据处理过程的计算和运行时间。</p><p id="3fa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与熊猫图书馆形成对比，熊猫图书馆使用热切的评价。Pandas中的所有计算都会在操作被调用后立即执行，结果会立即存储在内存中。虽然这对于小数据集是可行的，但当可伸缩性成为一个问题时，这是一个障碍。</p><h2 id="32a7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">PySpark vs熊猫:语法</h2><p id="ceeb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">幸运的是，PySpark数据帧的语法与Pandas的语法非常相似。</p><p id="193b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过对使用<a class="ae ky" href="https://www.mockaroo.com/" rel="noopener ugc nofollow" target="_blank"> Mockaroo </a>生成的模拟数据集执行一些操作来展示PySpark语法。</p><p id="13d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得更好的视角，我们将在适当的时候对熊猫执行相同的操作。</p><p id="fe79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1。创建Spark会话</strong></p><p id="392d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">spark会话充当创建和操作数据帧的入口点。它方便了PySpark中的所有后续操作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="dbe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，创建的会话将使用所有可用的核心。</p><p id="6164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。加载数据集</strong></p><p id="ed17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用Pandas和PySpark加载第一个模拟数据集。</p><p id="dcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="48f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0217" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在加载数据时，Pandas会自动从它读取的数据中做出推断。例如，具有数值的要素将被赋予int或float数据类型。</p><p id="8fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，PySpark没有这样的推论。默认情况下，PySpark会将标题视为第一行，将所有列视为字符串变量。为了防止PySpark做出任何错误的假设，用户必须给<code class="fe ne nf ng nh b">header</code>和<code class="fe ne nf ng nh b">inferSchema</code>参数赋值。</p><p id="4925" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。查看数据集</strong></p><p id="08a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/47476c8ce0070d515b0e58ee0a7899ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*Y5XbB7SZYoF2QYqPDyIizA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出预览(由作者创建)</p></figure><p id="bfa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2532cdc58934e79bd4b1731585c94374.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*hJruXK24wtoqRvwVB6o3Xg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出预览(由作者创建)</p></figure><p id="043d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为PySpark实现了延迟执行，所以它需要一个触发器来导出任何操作的结果。在这种情况下，<code class="fe ne nf ng nh b">show</code>函数充当触发器，让用户查看加载的数据集。</p><p id="ccdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 4。选择列</strong></p><p id="7171" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="4f4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7444" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 5。描述数据集的特征</strong></p><p id="e1f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/233c675de51b5336ae806d5692af162f.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*32ffRyI_5AeOABzfU_5Tdg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="5675" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6bac07a77718335f31d6a0f6d8973fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*AXmO0bNLRBt0jggw-7Kcfg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="bc41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然Pandas和PySpark使用相同的<code class="fe ne nf ng nh b">describe</code>函数，但是两个包的代码输出略有不同。</p><p id="e7a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，PySpark没有透露列的第25、50和75百分位值。其次，PySpark与Pandas不同，它包含分类特征的描述性统计数据(在本例中为<code class="fe ne nf ng nh b">gender</code>列)。</p><p id="d475" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 6。重命名列</strong></p><p id="f2b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Pandas和PySpark，我们将<code class="fe ne nf ng nh b">id</code>列重命名为“person_id”。</p><p id="6f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="c288" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="8bb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 7。添加列</strong></p><p id="d8fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们添加一个显示10年内受试者年龄的列。</p><p id="2063" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="1c25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="93c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 8。移除立柱</strong></p><p id="b84b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，我们删除新添加的列。</p><p id="1f86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="27e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="8472" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas和PySpark都用<code class="fe ne nf ng nh b">drop</code>功能删除列。唯一的区别是PySpark不包含<code class="fe ne nf ng nh b">axis</code>参数。</p><p id="f209" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 9。删除缺失值</strong></p><p id="fcd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="41c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="c722" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">10。过滤</p><p id="5ea8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个过滤操作中，我们只保留50岁以上的人的记录。</p><p id="cd73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="b2ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="db33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark非常像熊猫，允许使用括号来过滤记录。它还允许用户使用<code class="fe ne nf ng nh b">filter</code>功能过滤数值。</p><p id="90a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">11。聚合</p><p id="65b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过找出每个性别组的所有特征的平均值来执行聚合。</p><p id="2b3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/eb41dbdc1315fdd760f767dfb52958a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*rBDavDz2sZ_tlsYQMe5-Hw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="57c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0b74b9b3aac313c8f8aae1a1549c28a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*TE0J7KgdjL6SM0q2b6cdfQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="761a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个包之间唯一的区别是PySpark在输出的列名中包含了聚合类型(类似于SQL)。</p><p id="38fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 12。加入</strong></p><p id="9615" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在有了额外的数据，显示了我们希望与当前数据集合并的每个id的工作和收入。</p><p id="6ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">熊猫:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8d267762fa2783a52866bda7d5f0b820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*Y4jnT1U9Zu2g0zepVFHnig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="e0f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3114ddbc264313961c882a3c55518cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*klQw4M6NYwXtqwDY1-2HXA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="6f71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 13。转换成熊猫数据帧</strong></p><p id="7da6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们通过使用<code class="fe ne nf ng nh b">toPandas</code>函数将合并的PySpark数据帧转换为Pandas数据帧来结束练习。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4bcd74755eb763eab0c24d73e4e55ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*tI4irxhVzrIc17LZ3Udqzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码输出(由作者创建)</p></figure><h2 id="2d95" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">PySpark的缺点(对于熊猫用户)</h2><p id="d0ea" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">尽管Pandas和PySpark数据帧之间的语法相似，但Pandas用户可能仍然会发现适应新软件包的困难。</p><p id="2f44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas用户在使用Pandas很长一段时间后，可能会对数据处理有明显的感觉。不幸的是，与熊猫数据框争论的一些原则可能不适用于PySpark。</p><p id="49fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark的一些特性可能会妨碍熊猫用户。</p><ol class=""><li id="c391" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu"> PySpark不支持行索引</strong></li></ol><p id="fd50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">行索引(即每一行被分配一个索引号)在Pandas中可能很常见，但在PySpark中却没有用武之地。习惯于通过索引访问行或使用Pandas遍历行的用户可能会发现很难导航PySpark。</p><p id="0bac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。PySpark不具备熊猫的功能</strong></p><p id="8c9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas库提供了大量可以增强项目的工具(例如可视化)。不幸的是，可以使用Pandas数据框执行的某些任务无法使用PySpark数据框执行。</p><p id="3811" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:对于这种情况，用户可以使用<code class="fe ne nf ng nh b">toPandas</code>函数将PySpark数据框转换为Pandas数据框。</p><p id="8bc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。PySpark错误更难调试</strong></p><p id="a2a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark错误消息可能很难解释和解决。此外，PySpark并不像熊猫那样拥有大量的社区支持。因此，在PySpark中调试可能是一项艰苦的工作。</p><h2 id="154e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/62a8e5d3445384eabfab25c666510a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mkJEXaLNs4rH4Zz1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@prateekkatyal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Prateek Katyal </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e7c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管数据科学家可以单独使用Pandas模块成功完成许多项目，但如果他们缺乏处理更大数据集的适当方法，他们就不应该满足于自己的技能。毕竟，随着行业转向大数据解决方案，像PySpark这样的工具将不可避免地成为必需品。</p><p id="f208" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，尽管Pandas和PySpark之间存在差异，但两个包共享相似的语法，因此从一个包转换到另一个包应该是可行的。</p><p id="4874" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我祝你在数据科学的努力中好运！</p></div></div>    
</body>
</html>