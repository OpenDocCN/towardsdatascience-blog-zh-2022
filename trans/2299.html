<html>
<head>
<title>6 Dimensionality Reduction Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">6种降维技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/6-dimensionality-reduction-techniques-how-and-when-to-use-them-e4891c10b5db#2022-05-20">https://towardsdatascience.com/6-dimensionality-reduction-techniques-how-and-when-to-use-them-e4891c10b5db#2022-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d488" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何以及何时使用它们</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/71a67560392ad554778c177b1776eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PSXa6viWCRyXhAvDtQIf0w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://www.pexels.com/photo/yellow-and-white-3-d-cube-9436715/" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae kv" href="https://www.pexels.com/@frostroomhead/" rel="noopener ugc nofollow" target="_blank"> Rodion Kutsaiev </a>摄影</p></figure><p id="6be1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在大数据时代，数据科学家正在使用拥有越来越多功能的数据集。这导致了一个众所周知的效应:<strong class="ky ir">维度的诅咒。</strong>当特征数量增加时，在某一点之后，模型的性能会下降。这是因为数据点的密度将随着维度的增加而降低(没有添加任何样本)。其中一个主要的后果是，模型变得非常容易过度拟合。</p><p id="ed3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了克服由于高维导致的过拟合、训练时间和存储的问题，一种流行的方法包括对原始数据集应用降维技术。</p><p id="df7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将描述你在做数据科学项目时必须知道的六种降维方法。我将展示这些方法在著名的<a class="ae kv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>上的应用。最后，我会在最后一部分比较和详细说明何时使用哪种方法。</p><h1 id="5fba" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">主成分分析</h1><p id="df28" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">该算法是最著名的特征提取算法之一。PCA是一种无监督的线性变换算法，通过确定数据的最大方差来产生新特征。算法如下:</p><ul class=""><li id="df7d" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">构建数据的协方差矩阵</li><li id="40c0" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">对矩阵应用特征分解，并按降序对特征值进行排序</li><li id="39c6" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过投影前k个特征值的子集来变换数据</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/d210d2a69541886c051a5ee94880e448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PeBa2a3CA_yfnwyRR9F8zw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">主成分分析应用于MNIST数据集</p></figure><p id="13b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PCA有几个好处，比如不迭代，因此耗时较少。此外，它很好地减少了过拟合。然而，它仅限于线性投影，因此不能很好地处理非线性数据。这就是为什么没有一个数字在投影的PCA上形成真正分开的簇的原因。</p><h1 id="56dc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">线性判别分析(LDA)</h1><p id="690b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与前一种方法一样，这种方法是一种线性特征提取算法，但这次是有监督的。该方法创建了一个新的特征空间来投影数据，目标是最大化类的可分性。它通过创建两个距离矩阵来做到这一点:类间的<strong class="ky ir">和类内的<strong class="ky ir"/>。第一个计算每个类的平均值之间的距离。第二个函数计算每个类的平均值与该类中的数据之间的距离。</strong></p><p id="7821" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">算法如下:</p><ul class=""><li id="ff92" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">构建两个距离矩阵</li><li id="a8d0" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">计算这些矩阵组合的特征值</li><li id="10f6" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">对特征向量进行排序并构建top-k矩阵</li><li id="e5fa" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">投影新子空间的数据</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/41e7932e467c034335e3b14ba5c1108a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMw_E0vVXTb8O71HM9-H_A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">应用于MNIST数据集的LDA</p></figure><p id="d399" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">选择前k个特征向量使得LDA类似于PCA。然而，该算法的一个主要不便之处在于，在二元分类的情况下，在LDA之后将只有一个可用特征，而与最初可用特征的数量无关。</p><h1 id="ee56" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">独立成分分析</h1><p id="5bf5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在介绍一些非线性算法之前，我想强调一下独立分量分析方法。这是一种线性的监督特征提取算法，可生成统计上独立的新特征。这是通过减少给定数据集中的二阶和高阶相关性来实现的。</p><p id="ad69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">算法是这样的:</p><ul class=""><li id="9bc0" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">将数据分解成混合矩阵<strong class="ky ir"> A </strong>和新的基矩阵<strong class="ky ir"> S </strong></li><li id="0118" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">选择前k个组件</li><li id="0ce3" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过在那些k组件上投影来构建新的特征</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/640732aee3bc8dfe14da29f4a03a3909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g23lcTNnO44x8ahy95R-mQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">IDA应用于MNIST数据集</p></figure><p id="aef7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与前两种算法相比，ICA搜索统计上独立的特征。它通过搜索非高斯特征来做到这一点。这种算法常用于<em class="ng">盲源分离</em>问题。该算法的一个限制是它不能分离高斯特征。</p><h1 id="373c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">多维标度(MDS)</h1><p id="6ebe" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">接下来我将介绍的三种方法是非线性降维技术。第一种是非线性、无监督的维数约简技术。它侧重于多维空间中数据之间的关系，如相似性或不相似性。MDS将相似的数据放在一起表示，将不太相似的数据放在一起表示。它通过找到一个输出来最大化原始要素和修改后的要素的距离矩阵之间的相似性。</p><p id="c32d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">算法是:</p><ul class=""><li id="102f" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">计算数据的相异矩阵</li><li id="80b1" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过将数据居中来计算核矩阵<strong class="ky ir"> K </strong>。</li><li id="1e36" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">应用特征分解</li><li id="7755" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过选择前k个特征向量获得新的特征空间</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/917c01d19781188f273bff82c2818900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2TMsYtNC8d_1ndS2Wk8BrQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MDS应用于MNIST数据集</p></figure><p id="2046" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">小心使用这种方法，因为它计算量很大。</p><h1 id="fc0e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">等距映射</h1><p id="3a85" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这种方法来自于这样一个事实，即经典的缩放方法不能捕捉数据集中可能的非线性模式。这种非线性无监督方法通过在低维空间中保持数据之间的<em class="ng">成对测地线距离</em>来解决这个问题。它计算邻域图中所有数据对之间的最短路径，以近似它们之间的测地线距离。</p><p id="631d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些步骤是:</p><ul class=""><li id="0833" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">构建数据的邻域图</li><li id="03d1" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">计算测地线距离矩阵</li><li id="df77" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">将MDS应用于此测地线矩阵</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/fba45d82d4541e28f0ca18cc23796184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iY2TF_6koKYA76UkpwVSSA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ISOMAP应用于MNIST数据集</p></figure><p id="49bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们可以观察到的将该方法应用于MNIST数据集，一些数字被很好地分离，但是对于类似的数字，例如2和7，操作更复杂。这是因为ISOMAP是一种流形学习算法(最早的算法之一)，但由于它使用邻域图，有时会在元素之间建立错误的连接。非凸流形也很难用这种算法来分析。</p><h1 id="4198" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">t分布随机邻居嵌入(t-SNE)</h1><p id="bb35" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我将介绍的最后一个算法是基于流形的降维技术。它是一种非线性的无监督方法，由于保留了数据的结构，因此便于可视化。该算法首先将数据点之间的欧几里德距离转换为表示相似性的条件概率。之后，t-SNE最小化来自低维和高维空间的概率之间的差异(使用Kullback-Leibner散度)。</p><p id="1251" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">概括的算法是:</p><ul class=""><li id="214e" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">应用随机邻居嵌入来获得条件概率</li><li id="75f2" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">通过最小化概率之间的距离，将高维空间映射到低维空间</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/544d0641208c4846aef259d7057b379a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ON6Rb7XbDXTgE6_dFnOg7A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">t-SNE应用于MNIST数据集</p></figure><p id="032a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过保持局部结构，该算法特别适用于低维空间中的数据可视化，如可以证明上面的MNIST数据，其中大多数数字类被很好地分离。</p><h1 id="a03f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="6cd8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这六种降维技术代表了可用于此目的的各种方法。每种方法都有其特殊性和弱点，其中一些方法主要用于数据可视化，如t-SNE。所有这些都与Scikit-Learn集成在一起，Scikit-Learn提供了一种在机器学习管道中实现它们的简单方法。</p><p id="cf3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过下面的两个表格，我提供了整篇文章的有用摘要，详细说明了何时以及如何使用这些不同的特征提取算法。我们确实有一套工具来降低数据的维度，但每一种工具都必须在特定的环境中使用，才能真正有效。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure></div></div>    
</body>
</html>