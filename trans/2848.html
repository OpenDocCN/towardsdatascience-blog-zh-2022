<html>
<head>
<title>Object Detection Neural Network: Building a YOLOX Model on a Custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对象检测神经网络:在自定义数据集上构建YOLOX模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-neural-network-building-a-yolox-model-on-a-custom-dataset-77d29d85ae7f#2022-06-21">https://towardsdatascience.com/object-detection-neural-network-building-a-yolox-model-on-a-custom-dataset-77d29d85ae7f#2022-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a655" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">综合指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/868cfb57e694f8dcc0ee234a05a75d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vn9PSAih21ZaJoGsbI9tWw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://burst.shopify.com/@matthew_henry" rel="noopener ugc nofollow" target="_blank">马太·亨利</a>拍摄</p></figure><p id="892c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算机视觉中，对象检测是检测图像或视频中的对象的任务。目标检测算法工作有两个方面</p><ul class=""><li id="7f39" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><em class="me">一个定位任务</em> —输出包围盒(x，y坐标)。本质上，定位任务是一个回归问题，它输出表示边界框坐标的连续数字。</li><li id="23de" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated"><em class="me">一个分类任务</em>——对一个物体(人与车等)进行分类。).</li></ul><p id="5122" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标检测算法可以基于传统的计算机视觉方法或神经网络方法[1]。在基于神经网络的方法中，我们可以将算法分为两大类:单阶段和两阶段对象检测器。</p><p id="a79f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">单级物体检测器</strong>中，定位和分类任务一次完成，这意味着没有额外的网络来“帮助”物体检测器的定位和分类过程。单级检测器产生更高的推理速度，更适合移动和边缘设备。YOLO系列物体检测算法是单级物体检测器。</p><p id="46e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">两阶段检测器</strong>中——除了定位和分类网络，我们还有一个额外的网络，称为区域建议网络(RPN)。RPN用于决定“在哪里”寻找，以便降低整个对象检测网络的计算要求。RPN使用锚点—固定大小的参考边界框，均匀地放置在整个原始图像中。在区域提案阶段[2] —我们问</p><ul class=""><li id="f306" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这个锚包含相关对象吗？</li><li id="1a60" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">我们如何调整锚点以更好地适应相关对象？</li></ul><p id="9d03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们从上述过程中得到一个列表，本地化和分类网络的任务就变得简单明了。两级检测器通常具有更高的定位和分类精度。RCNN、fast-RCNN等。是两级物体检测器的几个例子。</p><h1 id="53a2" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">物体检测器的一般结构</h1><p id="9d2c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">图像是对象检测网络的输入<em class="me">。<em class="me">输入</em>通过卷积神经网络(CNN) <em class="me">中枢</em>从中提取特征(嵌入)。<em class="me">颈部</em>阶段的工作是混合和组合CNN主干中形成的特征，为<em class="me">头部</em>步骤做准备。<em class="me">颈部</em>的组件通常在层之间上下流动，并且仅连接【7】末端的几层。<em class="me">头</em>负责预测——aka。分类和本地化。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/5b4002787dd7e3edfa59e1b7380c0e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVhp8ILQZfhqKsYTKBS2uw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:基于深度学习的对象检测通用架构(来源:<a class="ae ky" href="https://arxiv.org/pdf/2004.10934v1.pdf" rel="noopener ugc nofollow" target="_blank"> Bochkovskiy等人，2020 </a>，第2页，图2)</p></figure><p id="18b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">YOLO系列物体探测器[3]是最先进的(SOTA)单级物体探测器。YOLO家族从2016年的YOLOv1开始发展到2021年的YOLOX。在本文中，我们将重点关注YOLOX。</p><h1 id="645f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">YOLOx建筑</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/62297b598f3a99a56fcae16e69ddd91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qg618svWeRTGqb-E2mNQug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:耦合头与解耦头(资料来源:郑等，2021，第3页，图3)</p></figure><p id="ad91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看YOLOX的一些架构组件。</p><h2 id="a200" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">去耦头</h2><p id="0c3d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">对于YOLOv3-v5，探测头保持耦合。这带来了挑战，因为检测头基本上执行两种不同的任务——分类和回归(边界框)。用一个解耦的头代替YOLO的头——一个用于分类，另一个用于包围盒回归。</p><h2 id="001b" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">强大的数据增强</h2><p id="259c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">YOLOX增加了两种数据增强策略——镶嵌和混合。</p><ul class=""><li id="f6aa" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">马赛克最初是在YOLOv3中引入的，随后在v4和v5中使用。镶嵌数据增强将4幅训练图像以一定的比例组合成一幅。这允许模型学习如何在比正常情况下更小的尺度上识别对象。这在培训中也是有用的，以显著减少对大的小批量的需求[4]。</li><li id="ecb7" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">混合——是一种数据扩充技术，从训练数据中生成随机图像对的加权组合[5]。</li></ul><h2 id="c323" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">无锚架构</h2><p id="331a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">YOLOv3-5是基于锚点的管道——这意味着RPN风格的固定大小的参考边界框被均匀地放置在整个原始图像中，以检查该框是否包含预期的类。锚定框允许我们在同一个网格中找到多个对象。但是基于锚的机制也有一些问题—</p><ul class=""><li id="297f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在基于锚的机制中，目标检测需要确定最优的锚盒。为此，我们需要在训练之前通过聚类分析找到最佳锚盒。这是增加训练时间和复杂性额外步骤。</li><li id="ba50" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">另一个问题是基于锚的机制增加了对每个图像进行预测的数量。这样会增加推断时间。</li><li id="15cf" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ma mb mc md bi translated">最后，基于锚的机制显著增加了检测头和整个网络的复杂性。</li></ul><p id="97c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">近年来，无锚机制得到了改进，但直到YOLOX才被引入YOLO家族。</p><p id="0f2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无锚点YOLOX将每个图像单元的预测数量从3减少到1。</p><h2 id="c5f4" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">西蒙塔</h2><p id="f93c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">SimOTA是一种先进的标签分配技术。什么是标签分配？它为每个基本事实对象定义正/负训练样本。YOLOX将这个标签分配问题公式化为最优运输(OT)问题[6]。</p><p id="f404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表显示了这些不同的架构组件如何帮助提高模型的平均精度(AP)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/583d9555cc6940438c3aa1e193d09f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2hrnRWrT822wXHQ589yhA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:基于不同技术的YOLOX性能(资料来源:<a class="ae ky" href="https://arxiv.org/pdf/2107.08430.pdf" rel="noopener ugc nofollow" target="_blank">郑等</a>，2021，第4页，表1)。2)</p></figure><h1 id="f5f1" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">在自定义数据集上训练YOLOX</h1><h2 id="544b" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">装置</h2><p id="ce43" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">你可以在这里找到YOLOX <a class="ae ky" href="https://github.com/Megvii-BaseDetection/YOLOX/" rel="noopener ugc nofollow" target="_blank">的开源代码。按照安装部分，您可以从源代码安装</a></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ffd3" class="nj ml it nx b gy ob oc l od oe">git clone <a class="ae ky" href="mailto:git@github.com" rel="noopener ugc nofollow" target="_blank">git@github.com</a>:Megvii-BaseDetection/YOLOX.git<br/>cd YOLOX<br/>pip3 install -v -e .  # or  python3 setup.py develop</span></pre><h2 id="5705" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">数据集转换</h2><p id="af1c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">确保您的自定义数据集为COCO格式。如果您的数据集是darknet或yolo5格式，您可以使用<a class="ae ky" href="https://github.com/RapidAI/YOLO2COCO" rel="noopener ugc nofollow" target="_blank"> YOLO2COCO </a>存储库将其转换为COCO格式。</p><h2 id="8434" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">下载预先训练的重量</h2><p id="22e9" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">当在自定义数据集上训练我们的模型时，我们更喜欢从预训练基线开始，并在其上训练我们的数据。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ad7a" class="nj ml it nx b gy ob oc l od oe">wget https://github.com/Megvii-BaseDetection/storage/releases/download/0.0.1/yolox_s.pth</span></pre><h2 id="a6c0" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">模特培训</h2><p id="7513" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">YOLOX模型培训的一个关键组成部分是拥有正确的实验文件——这里有一些示例自定义实验文件<a class="ae ky" href="https://github.com/Megvii-BaseDetection/YOLOX/tree/main/exps/example/custom" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="8753" class="nj ml it nx b gy ob oc l od oe">#!/usr/bin/env python3<br/># -*- coding:utf-8 -*-<br/># Copyright (c) Megvii, Inc. and its affiliates.<br/>import os<br/><br/>from yolox.exp import Exp as MyExp<br/><br/><br/>class Exp(MyExp):<br/>    def __init__(self):<br/>        super(Exp, self).__init__()<br/>        self.depth = 0.33<br/>        self.width = 0.50<br/>        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(".")[0]<br/><br/>        # Define yourself dataset path<br/>        self.data_dir = "datasets/coco128"<br/>        self.train_ann = "instances_train2017.json"<br/>        self.val_ann = "instances_val2017.json"<br/><br/>        self.num_classes = 71<br/><br/>        self.max_epoch = 300<br/>        self.data_num_workers = 4<br/>        self.eval_interval = 1</span></pre><p id="80a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以根据您拥有的类的数量来试验变量，如<code class="fe of og oh nx b">self.num_classes</code>,更改数据集路径等。</p><p id="398f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里看到另一个自定义实验文件<a class="ae ky" href="https://dicksonneoh.com/portfolio/how_to_10x_your_od_model_and_deploy_50fps_cpu/#-modeling-with-yolox" rel="noopener ugc nofollow" target="_blank">的例子</a></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6017" class="nj ml it nx b gy ob oc l od oe">import os<br/>from yolox.exp import Exp as MyExp<br/><br/>class Exp(MyExp):<br/>    def __init__(self):<br/>        super(Exp, self).__init__()<br/>        self.depth = 0.33<br/>        self.width = 0.50<br/>        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(".")[0]<br/><br/>        # Define yourself dataset path<br/>        self.data_dir = "data"<br/>        self.train_ann = "train.json"<br/>        self.val_ann = "val.json"<br/><br/>        self.num_classes = 1<br/>        self.data_num_workers = 4<br/>        self.eval_interval = 1<br/><br/>        # --------------- transform config ----------------- #<br/>        self.degrees = 10.0<br/>        self.translate = 0.1<br/>        self.scale = (0.1, 2)<br/>        self.mosaic_scale = (0.8, 1.6)<br/>        self.shear = 2.0<br/>        self.perspective = 0.0<br/>        self.enable_mixup = True<br/><br/>        # --------------  training config --------------------- #<br/>        self.warmup_epochs = 5<br/>        self.max_epoch = 300<br/>        self.warmup_lr = 0<br/>        self.basic_lr_per_img = 0.01 / 64.0<br/>        self.scheduler = "yoloxwarmcos"<br/>        self.no_aug_epochs = 15<br/>        self.min_lr_ratio = 0.05<br/>        self.ema = True<br/><br/>        self.weight_decay = 5e-4<br/>        self.momentum = 0.9</span></pre><p id="d3c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要训练模型(确保您在YOLOX目录中)，您可以使用下面的命令。注意<code class="fe of og oh nx b">d</code>和<code class="fe of og oh nx b">b</code>的参数。<code class="fe of og oh nx b">d</code>表示GPU的数量，在我这里是<code class="fe of og oh nx b">1</code>。<code class="fe of og oh nx b">b</code>是训练时的批量<code class="fe of og oh nx b">8x</code>是<code class="fe of og oh nx b">d</code>的值。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="23bd" class="nj ml it nx b gy ob oc l od oe">python tools/train.py -f exps/example/custom/yolox_tiny.py -d 1 -b 8 --fp16 -o -c yolox_tiny.pth</span></pre><h2 id="b941" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">模型评估</h2><p id="136c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在模型评估期间，您必须指定用于训练的相同(或其副本)实验文件</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="657b" class="nj ml it nx b gy ob oc l od oe">python -m yolox.tools.eval -n  yolox_s -c YOLOX_outputs/yolox_s/best_ckpt.pth -f exps/example/custom/yolox_s.py -b 1 -d 1 --conf 0.001 --fp16 --fuse</span></pre><h2 id="1980" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">模型检验</h2><p id="272d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">为了在测试图像上运行YOLOX推理，我们可以做下面的[8]</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ccf2" class="nj ml it nx b gy ob oc l od oe">TEST_IMAGE_PATH = "test_image.jpg"<br/><br/>python tools/demo.py image -f exps/example/custom/yolox_s.py -c <!-- -->YOLOX_outputs/yolox_s/best_ckpt.pth<!-- --> --path {TEST_IMAGE_PATH} --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu</span></pre><p id="c69e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设输出的预测图像是<code class="fe of og oh nx b">prediction.jpg</code>。你可以像下面这样想象这个图像</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ac14" class="nj ml it nx b gy ob oc l od oe">from PIL import Image<br/>Image.open('prediction.jpg')</span></pre><p id="97b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要检查您的模型在视频上的表现，您可以执行以下操作</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="b6e4" class="nj ml it nx b gy ob oc l od oe">python tools/demo.py video -n yolox-s -c /path/to/your/yolox_s.pth <!-- -->-f exps/example/custom/yolox_s.py<!-- --> --path /path/to/your/video --conf 0.25 --nms 0.45 --tsize 640 --save_result --device [cpu/gpu]</span></pre><p id="b782" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意这里的参数<code class="fe of og oh nx b">conf</code>——它表示置信区间，意味着在输出边界框之前，您希望您的模型有多自信。拥有更高的<code class="fe of og oh nx b">conf</code>分数通常会提高检测的质量——移除不需要的边界框等。</p><h2 id="f3be" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">前进路径:</h2><p id="5607" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">既然我们已经学会了如何在自定义数据集上训练YOLOX前面还有许多可能性。根据您的使用情况，您可能想要一个更轻或更重的YOLOX版本。您可以使用`<code class="fe of og oh nx b">exps/example/custom/yolox_s.py</code>中定义的实验文件中的参数进行实验。一旦模型准备好，根据您的使用情况，您可能需要修剪或量化模型，特别是如果您正在边缘设备上部署它。</p><p id="17fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇博文。如果你有任何问题，请在这里随意评论。</p><h2 id="5456" class="nj ml it bd mm nk nl dn mq nm nn dp mu li no np mw lm nq nr my lq ns nt na nu bi translated">参考资料:</h2><ol class=""><li id="b58c" class="lv lw it lb b lc nc lf nd li oi lm oj lq ok lu ol mb mc md bi translated">20年来的物体探测:一项调查<a class="ae ky" href="https://arxiv.org/pdf/1905.05055.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1905.05055.pdf</a></li><li id="e70b" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">一阶段对两阶段物体检测器<a class="ae ky" href="https://stackoverflow.com/questions/65942471/one-stage-vs-two-stage-object-detection" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/65942471/一阶段对两阶段物体检测</a></li><li id="1383" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">YOLO家族简介<a class="ae ky" href="https://pyimagesearch.com/2022/04/04/introduction-to-the-yolo-family/" rel="noopener ugc nofollow" target="_blank">https://pyimagesearch . com/2022/04/04/introduction-to-the-yolo-family/</a></li><li id="8451" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">YOLOv4数据增强<a class="ae ky" href="https://blog.roboflow.com/yolov4-data-augmentation/" rel="noopener ugc nofollow" target="_blank">https://blog.roboflow.com/yolov4-data-augmentation/</a></li><li id="bb0e" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">https://paperswithcode.com/method/mixup</li><li id="455f" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">物体检测的最佳运输分配<a class="ae ky" href="https://arxiv.org/pdf/2103.14259.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.14259.pdf</a></li><li id="0761" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">约洛夫4<a class="ae ky" href="https://blog.roboflow.com/a-thorough-breakdown-of-yolov4/" rel="noopener ugc nofollow" target="_blank">https://blog.roboflow.com/a-thorough-breakdown-of-yolov4/</a>彻底崩溃</li><li id="cc6f" class="lv lw it lb b lc mf lf mg li mh lm mi lq mj lu ol mb mc md bi translated">如何训练一个定制的YOLOX模型<a class="ae ky" href="https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset/" rel="noopener ugc nofollow" target="_blank">https://blog . robo flow . com/how-to-training-YOLOX-on-a-custom-dataset/</a></li></ol></div></div>    
</body>
</html>