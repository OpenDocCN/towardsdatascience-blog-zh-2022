<html>
<head>
<title>An Introduction to Long Short-Term Memory Networks (LSTM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆网络导论(LSTM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-long-short-term-memory-networks-lstm-27af36dde85d#2022-06-04">https://towardsdatascience.com/an-introduction-to-long-short-term-memory-networks-lstm-27af36dde85d#2022-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解长短期记忆的概念和问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fac647f8d0ce6e3718d159202ff929a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IUZWGWfdYlEMVW3I"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@invictar1997?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Soragrit Wongsa </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c743" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">长短期记忆(简称:LSTM)模型是<a class="ae ky" href="https://databasecamp.de/en/ml/recurrent-neural-network" rel="noopener ugc nofollow" target="_blank">循环神经网络</a> (RNN)的一个亚型。它用于识别数据序列中的模式，例如出现在传感器数据、股票价格或自然语言中的模式。rnn之所以能够做到这一点，是因为除了实际值之外，它们还在预测中包括其在序列中的位置。</p><h1 id="5501" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是递归神经网络？</h1><p id="78ec" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了理解递归神经网络如何工作，我们必须再看一下常规的<a class="ae ky" href="https://databasecamp.de/en/ml/artificial-neural-networks" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>是如何构造的。其中，隐藏层的神经元与前一层的神经元和后一层的神经元相连。在这样的网络中，一个神经元的输出只能向前传递，而绝不会传递给同一层甚至上一层的神经元，因此得名“前馈”。</p><p id="4b50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这对于递归神经网络是不同的。神经元的输出可以很好地用作前一层或当前层的输入。这比前馈神经网络的构建方式更接近我们大脑的工作方式。在许多应用中，我们还需要在改善整体结果之前立即了解计算的步骤。</p><h1 id="73ea" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">RNNs面临哪些问题？</h1><p id="aede" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">递归神经网络是<a class="ae ky" href="https://databasecamp.de/en/ml/deep-learning-en" rel="noopener ugc nofollow" target="_blank">深度学习</a>领域的一个真正突破，因为第一次，最近过去的计算也包括在当前计算中，显著提高了语言处理的结果。尽管如此，在训练过程中，他们也带来了一些需要考虑的问题。</p><p id="9e25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在关于<a class="ae ky" href="https://databasecamp.de/en/ml/gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度法</a>的文章中已经解释过的，当用梯度法训练神经网络时，梯度可能会呈现非常小的接近0的值或者非常大的接近无穷大的值。在这两种情况下，我们都不能在<a class="ae ky" href="https://databasecamp.de/en/ml/backpropagation-basics" rel="noopener ugc nofollow" target="_blank">反向传播</a>过程中改变神经元的权重，因为权重要么根本不变，要么我们不能用这么大的值乘以数字。由于递归神经网络中的许多互连以及用于它的反向传播算法的稍微修改的形式，这些问题发生的概率比正常的前馈网络高得多。</p><p id="9e17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">常规rnn非常擅长记忆上下文，并将其纳入预测。例如，这允许RNN认识到在句子“clouds are at the _ _”中，需要单词“sky”来在该上下文中正确完成句子。另一方面，在一个较长的句子中，保持上下文变得困难得多。在稍加修改的句子“部分流入彼此并低悬的云在__”中，一个递归神经网络推断“天空”这个词变得困难得多。</p><h1 id="f20c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">长短期记忆模型是如何工作的？</h1><p id="b924" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">递归神经网络的问题在于，它们有一个短期记忆来保留当前神经元中以前的信息。然而，对于较长的序列，这种能力下降得非常快。作为对这一点的补救，LSTM模型被引入，以便能够更长时间地保留过去的信息。</p><p id="7378" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">递归神经网络的问题在于，它们只是将之前的数据存储在它们的“短期记忆”中。一旦其中的内存耗尽，它就简单地删除保留时间最长的信息，并用新数据替换它。LSTM模型试图通过在短期记忆中只保留选定的信息来避免这个问题。</p><p id="464c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，LSTM架构总共包括三个不同的阶段:</p><ol class=""><li id="196d" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">在所谓的<strong class="lb iu">遗忘门</strong>中，决定哪些当前和先前的信息被保留，哪些被丢弃。这包括上次运行的隐藏状态和当前状态。这些值被传递到一个sigmoid函数中，该函数只能输出0到1之间的值。值0意味着所有以前的信息都被遗忘，1意味着所有以前的信息都被保留。</li><li id="da0d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">在<strong class="lb iu">输入门</strong>中，决定当前输入对解决任务有多大价值。为此，当前输入乘以隐藏状态和上次运行的权重矩阵。</li><li id="65bd" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">在<strong class="lb iu">输出门</strong>中，计算LSTM模型的输出。取决于应用，例如，它可以是补充句子意思的单词。</li></ol><h1 id="49cb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">这是你应该带走的东西</h1><ul class=""><li id="cc39" class="ms mt it lb b lc mn lf mo li ng lm nh lq ni lu nj my mz na bi translated">LSTM模型是递归神经网络的一个亚型。</li><li id="1337" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu nj my mz na bi translated">它们用于识别数据序列中的模式，例如出现在传感器数据、股票价格或自然语言中的模式。</li><li id="deec" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu nj my mz na bi translated">一种特殊的结构允许LSTM模型决定是在短期记忆中保留先前的信息还是丢弃它。因此，序列中更长的依赖性也被识别。</li></ul></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="696d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nr">如果你喜欢我的作品，请在这里订阅</em><a class="ae ky" href="https://medium.com/subscribe/@niklas_lang" rel="noopener"><em class="nr"/></a><em class="nr">或者查看我的网站</em> <a class="ae ky" href="http://www.databasecamp.de/en/homepage" rel="noopener ugc nofollow" target="_blank"> <em class="nr">数据大本营</em> </a> <em class="nr">！还有，medium允许你每月免费阅读</em> <strong class="lb iu"> <em class="nr"> 3篇</em> </strong> <em class="nr">。如果你想让</em><strong class="lb iu"><em class="nr"/></strong><em class="nr">无限制地访问我的文章和数以千计的精彩文章，不要犹豫，通过点击我的推荐链接:</em><a class="ae ky" href="https://medium.com/@niklas_lang/membership" rel="noopener">【https://medium.com/@niklas_lang/membership】</a>每月花$<strong class="lb iu"><em class="nr">5</em></strong><em class="nr">获得会员资格</em></p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ns"><a rel="noopener follow" target="_blank" href="/comprehensive-guide-to-principal-component-analysis-bb4458fff9e2"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">主成分分析综合指南</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">主成分分析的理论解释</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og ks ns"/></div></div></a></div><div class="oh oi gp gr oj ns"><a rel="noopener follow" target="_blank" href="/why-you-should-know-big-data-3c0c161b9e14"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">为什么您应该了解大数据</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">定义大数据及其潜在威胁</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="ok l od oe of ob og ks ns"/></div></div></a></div><div class="oh oi gp gr oj ns"><a rel="noopener follow" target="_blank" href="/what-are-deepfakes-and-how-do-you-recognize-them-f9ab1a143456"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">什么是Deepfakes？</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">如何应对人工智能制造的误传</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="ol l od oe of ob og ks ns"/></div></div></a></div></div></div>    
</body>
</html>