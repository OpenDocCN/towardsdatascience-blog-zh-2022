<html>
<head>
<title>A Journey into Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习之旅</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-journey-into-deep-reinforcement-learning-41b6ee5e860e#2022-05-27">https://towardsdatascience.com/a-journey-into-deep-reinforcement-learning-41b6ee5e860e#2022-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="678f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深Q网解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8dc24137a98ceeb23c96471bd9ccbaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0aJhVRJ7Uv3_vVjTzn0p7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">必须抓住所有的概念💫</p></figure><p id="4659" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">深度强化学习</strong> (DRL)在过去几年里一直是人工智能领域的焦点。在游戏世界中，几个机器人(在本文的其余部分也称为代理人或模型)如围棋游戏的<a class="ae lu" href="https://www.youtube.com/watch?v=WXuK6gekU1Y" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>或星际争霸的<a class="ae lu" href="https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii" rel="noopener ugc nofollow" target="_blank"> AlphaStar </a>和Dota视频游戏的<a class="ae lu" href="https://openai.com/blog/openai-five/" rel="noopener ugc nofollow" target="_blank"> Open AI Five </a>，仅举几个著名的例子，它们表明，通过将<strong class="la iu">深度神经网络代表性力量</strong>和<strong class="la iu">强化学习框架</strong>巧妙结合，有可能主宰职业游戏玩家联盟。</p><p id="07a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">DRL不仅限于游戏，它还在更实际的问题上表现出强大的能力，如物流、生物、电子设计，甚至在聚变反应堆中指导等离子体操纵，这表明以前难以解决的问题可以由“超人”代理来解决。</p><p id="0243" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">知道了这一点，我非常好奇，想知道这样的结果是如何得到的。一如既往，从学科的最底层开始帮助我掌握了基本概念，并在挖掘更复杂的架构之前建立了对DRL的理解。因此，这篇帖子的目的是专注于理论，并解释DRL基金会算法之一的<strong class="la iu">深度Q-网络</strong> (DQN)的一些主要概念，以了解深度学习&amp;强化学习(RL)如何联合起来构建这样的超人代理。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="8060" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在继续这篇文章之前，我假设读者已经熟悉RL的基本概念和词汇。如果没有，你可以先看看RL 上的这个很棒的<a class="ae lu" href="https://www.davidsilver.uk/teaching/" rel="noopener ugc nofollow" target="_blank"> UCL课程，或者看看你真实的介绍:</a></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/playing-cards-with-reinforcement-learning-1-3-c2dbabcf1df0"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">强化学习扑克牌</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">概念和Python代码🐍</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e200" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👾 🎮<strong class="la iu">太空入侵者</strong> (S-I)是一款来自著名的雅达利2600主机的视频游戏(图1)。这款视频游戏被广泛用于DRL模型的基准测试。在《星际迷航》中，你驾驶一艘小飞船，在外星人摧毁你之前，你必须通过射击摧毁尽可能多的外星人飞船。我们将使用S-I游戏作为一个玩具环境来说明我们在这篇文章中涉及的RL概念。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a109191f8efe0cd64ec8c65afba1507a.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/1*FZ_ZW-iRxRCQEXKgcfSREA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图一</strong>。扮演太空入侵者的随机特工</p></figure><p id="170c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们已经提到的，球员经纪人将由DRL砖块制成。它将通过<strong class="la iu">观看游戏的原始图像</strong>和<strong class="la iu">从所有可能的动作中选择一个动作做出相应的反应</strong>来直接学习玩S-I(就像人类会做的那样)。</p><p id="50c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解代理背后的DRL砖块，我们剖析了DeepMind的三篇主要论文(按出版时间顺序排列如下)，它们是许多现代DRL方法的核心:</p><ul class=""><li id="f20d" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><a class="ae lu" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">玩雅达利用深度强化学习</strong> </a> <strong class="la iu">(一)</strong></li><li id="ceae" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">深度强化学习与双Q学习</strong> </a> <strong class="la iu">(二)</strong></li><li id="e032" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1511.05952.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"/></a><strong class="la iu">(三)</strong></li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="c06e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从直接查看狮穴开始我们的旅程(下面的算法1👇).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/8f3e435197d8257db21dd8d5824701ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oqk9y_3Vv3MgO3cRiifsgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">算法1 </strong>。下面我们来解开那些<strong class="bd mv">伪代码</strong> <strong class="bd mv">步骤</strong></p></figure><p id="6496" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">天啊😱！乍一看，这看起来很可怕…但是不要担心，我们将一行一行地检查它，并解开每个概念。</p><h1 id="febd" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">⛰️ Q学习</h1><p id="27b4" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">首先，在深度<strong class="la iu"> Q </strong>网络中我们有一个<strong class="la iu"> Q </strong>。并且，在一开始<a class="ae lu" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">萨顿&amp;巴尔托</a>创造了<strong class="la iu"> Q函数</strong>。Q函数(也称为状态-动作值函数)将几个&lt;状态、动作&gt;映射到一个值(或预期回报)。</p><p id="546c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单地说，Q函数告诉你，在你所处的特定状态下，你想采取的行动有多好。下面的图2用S-I环境中的一个具体例子说明了这个定义。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/94aa1af8def612dd39e328d1e51baacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GwqCRJqbynrgMseeIJLwg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图二</strong>。S-I中给定情况下4种潜在行动的4 Q函数输出图</p></figure><p id="eb99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦与环境相关的Q函数已知，代理总是可以选择导致最大期望回报的行动，因此在该环境中遵循所谓的<strong class="la iu">最优策略</strong>。正是对最优策略的了解，使得智能体达到了超人的性能。</p><p id="9d4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">S-I博弈类似于一个<a class="ae lu" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程</a> (MDP)环境。RL研究人员提出了在这种环境下寻找最优策略的几种方法。在DQN代理中，<strong class="la iu"> Q学习</strong>方法用于寻找最佳Q函数。因此，让我们深入研究一下这种方法背后的数学原理，分解将我们引向算法1核心部分的不同步骤。</p><p id="9c58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据定义，Q函数方程已经被<a class="ae lu" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼</a>提出为一个递归(动态规划)问题:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d6e94fc5c1246309532473a6dca4b9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oy4XqfFPx24LPDc6JeD2ig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv"> Eq 1 </strong>。定义Q函数的贝尔曼方程</p></figure><p id="7286" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝尔曼方程表明，处于状态<strong class="la iu"><em class="ok"/></strong>的动作<strong class="la iu"><em class="ok"/></strong>的Q函数可以分解成两部分:</p><ul class=""><li id="85ca" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated">一份立即的奖励</li><li id="64aa" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">一个贴现的<strong class="la iu">未来的</strong>预期报酬</li></ul><p id="4bb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过将期望函数发展为所有可能的下一个状态上的状态转移概率的加权平均值，以及遵循我们的当前策略在特定状态中选择任何可能动作的概率的加权平均值，可以在下面的等式(2)中导出第一个等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/d4d58106490374c7f762ecb317266511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZNZDj2jMpxVMxUocIcDbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv"> Eq 2。</strong>Q函数的扩展定义</p></figure><p id="eae5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，我们的S-I环境可以看作是一个<strong class="la iu">确定性</strong>环境。的确，按下左键时向左移动是肯定的。因此，它使我们能够摆脱状态转移的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/fc5c594d4f538e8f01e1eddc1b52abc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1DiNidPXUIykAfBiBBrLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">情商3。</strong>Q函数的确定性方程</p></figure><p id="8a0b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们已经推导出了Q函数方程，让我们记住我们要寻找的是<strong class="la iu">最优</strong> <strong class="la iu"> Q函数</strong>(具有最高值)。这意味着我们不想再考虑所有可能的行动，而只考虑最大化预期回报的行动。一个自然的替代是查看最大预期回报，因此是下面的(4):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/cf29a597fd4ab808595459f2892e0081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhXHBzorIEYHR1qya8dWNA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv"> Eq 4。</strong>最优Q-函数方程</p></figure><p id="a605" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从数学上来说，最佳Q函数应该遵守(4)中的等式。</p><p id="51df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从理论上的最佳Q-函数定义(4)，Q-learning使用以下更新步骤推导出收敛到最佳Q-函数的<strong class="la iu">估计值</strong>的迭代算法:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/21ef76a9c238cab43203f2203ca50614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKXZ-FqKynE5rdJt1epWMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">情商5。</strong><strong class="bd mv">Q-学习步骤</strong>。</p></figure><ul class=""><li id="dd61" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated">红色的<a class="ae lu" href="https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=TD%20algorithm%20in%20neuroscience,-The%20TD%20algorithm&amp;text=The%20error%20function%20reports%20back,the%20expected%20and%20actual%20reward." rel="noopener ugc nofollow" target="_blank"><strong class="la iu">TD-误差</strong> </a> <strong class="la iu"> </strong>:是两个时间上不同的估计值之间的差距。越接近0，越接近最优，因此更新越小；越大越接近最优，因此更新越大。</li><li id="f173" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">绿色部分显示的是<strong class="la iu">学习速率</strong> <strong class="la iu"> λ </strong>，表示我们希望更新多少旧的估计Q值。</li><li id="dc0f" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">蓝色表示更新后的旧估计Q值。</li></ul><p id="2a2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用简单的英语总结最后一个等式(5): Q学习迭代地朝着最优方向更新估计的Q值。在这个迭代过程结束时，它收敛到<strong class="la iu">最优Q函数</strong>。</p><h1 id="9c2e" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">💧深度Q学习</h1><p id="ca7f" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">你看到它来到这里(或者深度Q网络不够明确🙃).这确实是深度神经网络(DNN)的用途，以<a class="ae lu" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">通用函数逼近器</strong> </a> <strong class="la iu">而闻名。</strong>最重要的是，DNN在处理图像等非结构化数据时特别有效(与更传统的ML方法相比)。请注意，在这篇文章的其余部分，我们交替使用DQN和DNN术语来指代代理的模型(假设DQN是一个DNN)。</p><p id="437c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们记住，我们直接使用S-I视频游戏的图像流，让代理学习最优Q函数。所以问题如下:</p><blockquote class="op"><p id="91eb" class="oq or it bd os ot ou ov ow ox oy lt dk translated">为什么不用DNN作为Q函数的近似值呢？</p></blockquote><p id="0e08" class="pw-post-body-paragraph ky kz it la b lb oz ju ld le pa jx lg lh pb lj lk ll pc ln lo lp pd lr ls lt im bi translated">让我们看看如何在Q函数近似任务中利用DNN的代表性力量。因为一张图片胜过千言万语，所以下面是(如图3所示👇2)DNN如何处理S-I游戏图像以输出与可能的游戏动作相关的值(准确地说是Q值)的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/c9274553c7ff806781a0f053d15f355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhkdhMyCjcVmphxs5_HVbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图3 </strong>。DQN向前传球，从S-I真实游戏画面流中学习(详细图例如下)</p></figure><ul class=""><li id="2b4d" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><strong class="la iu">跳过帧</strong>:非常接近的状态(或转换)不会携带太多信息，我们更喜欢探索更多不同的模式，而不是收集高度相关的转换，这将输出非常接近的Q值。此外，运行更少帧的游戏引擎比计算所有这些高度相关的转换的Q值更便宜。因此，使用这种技术可以运行更多不同的剧集。</li><li id="2fd3" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu">向下采样，灰度调整&amp;归一化</strong>:在ML中，为了减少从图像中学习所需的计算量，并保持像素表示在可比的尺度上，遵循这些预处理步骤是一种常见的做法。</li><li id="32e7" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu"> DQN输入</strong>:来自S-I的4帧作为DNN输入，它们被称为一个状态或转换。</li><li id="3d5e" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu">卷积层</strong>:从原始图像中学习信息特征恰恰属于神经网络的范畴，更具体地说是控制计算机视觉领域的c <a class="ae lu" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> (CNN)。</li><li id="2677" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu">输出层</strong>:DNN的最后一层给出与每4个潜在动作相关的4个分数(分数越高，动作越好)。</li></ul><p id="e022" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是如何处理图像的一个很好的可视化，但要让DNN学会最后一个要素是一个损失函数。这里没有什么特别的，所用的损失是一个自适应的均方误差(TD-MSE，见下面图4中的等式(6 )),它使用估计的最佳Q函数。最后，经典的<a class="ae lu" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>被用于向等式(7)上的损失最小值更新DNN网络权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/2a15af218e920dc6d4f5c1a09da0a745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZFJAIJLoUJlqRTquObM_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图4 </strong>。深度Q网络随机梯度下降步长</p></figure><p id="957c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止一切顺利。现在让我们看看为什么在梯度下降更新步骤中区分损失时，红色参数(图4 ⬆️上)保持冻结。</p><h1 id="345c" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">⚡在线与目标网络</h1><p id="feed" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">如果我们正确理解了等式(6)，我们在损失中使用的目标/标签来自我们更新的同一模型。换句话说，模型正在从自己的预测中学习…这难道不像蛇咬尾巴一样奇怪吗？♾️</p><p id="6aff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">乍听起来确实很奇怪。但是如果我们想到它，它就像使用我们对估计的最优Q函数的最佳猜测(到目前为止)。并且假设Q学习迭代地收敛到最优Q函数，我们可以期望我们的噪声估计在开始时变得越来越一致，一次又一次迭代。<br/>理论上，当且仅当模型在训练时给出越来越相关的目标时，这才会起作用<strong class="la iu">。</strong></p><p id="195b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，实际上它并不那么有效。事实上，在每个学习步骤中，模型的权重都会更新，这可能会导致两个相似状态的目标预测发生巨大变化，从而阻止DNN有效学习。<br/>例如，对于两个连续的学习步骤，给定一个非常相似的输入状态，DNN可能会选择不同的最优行动……这意味着我们的目标不一致。😢</p><p id="3e77" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种不一致性被称为<strong class="la iu">移动目标</strong>问题，导致学习不稳定。这就是使用<strong class="la iu"> 2网络</strong>的想法的来源:</p><ul class=""><li id="156e" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><strong class="la iu">在线网络</strong>:每步更新的学习网络</li><li id="b9cc" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu">目标网络</strong>:在线网络的副本，在几个步骤中保持冻结，以确保目标保持稳定</li></ul><p id="5655" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该在线/目标网络工作流在⬇️下面的图5中解释</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/a85dbaf495df5a2f25e8deb9d3f22e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlqHARpu8uIFE68FsZ5bLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图5 </strong>。目标网络产生目标，而在线网络向它学习。在线网络的权重被不时地复制到目标网络。</p></figure><ul class=""><li id="07f4" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><strong class="la iu">步骤1 </strong>:目标网络产生给定过渡的目标</li><li id="5590" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated"><strong class="la iu">步骤2 </strong>:在线模型从目标网络产生的目标中学习</li></ul><p id="7a1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">遵循在线/目标网络方法保证了DNN学习的稳定目标。</p><p id="c017" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们开始看到这一点，要将深度学习和强化学习结合起来，我们必须确保DNN有一个高效的学习机制。所以，让我们看看释放全部DNN能量所必需的其他工程技巧吧！💥</p><h1 id="b255" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">🍃双(深度)Q学习</h1><p id="c668" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">再一次，另一个障碍落在DQN学习和收敛的路上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/3177654a6ccaf671d7d21ff230b4f8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXWKJ_w9t4qxGeun9qAhjQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">方程式(4) </strong></p></figure><p id="8984" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回顾Q-学习最优(上面的等式(4))，我们可以看到<state action="">对的Q值取决于下一状态的所有可能动作的<strong class="la iu">最大Q值</strong>(等式(4)上的红色项)。这意味着，在Q学习中，所有可能动作的最大值是估计状态值的相关代理。</state></p><p id="8305" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如此确信最大值是对一个状态值的正确估计被认为是<strong class="la iu">偏向最大值</strong>。这就是下图6中的玩具例子所展示的👇</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/883ad848f9f1931bde47c9218e261a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFpAkWZCcDYo5wqeDRMFSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图六。</strong>盲目相信<strong class="bd mv">嘈杂的Q值估计，</strong>与隐藏的现实不同，使智能体探索错误的方向:<strong class="bd mv">选择动作A </strong>而不是<strong class="bd mv">动作B </strong>。</p></figure><p id="03ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简而言之，偏向最大值会使代理选择不感兴趣的动作并探索错误的方向，这可能会导致模型收敛更慢，并恶化其学习机制。</p><blockquote class="pj pk pl"><p id="52b9" class="ky kz ok la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated"><strong class="la iu">注意</strong>:那些噪声估计<strong class="la iu">不是随机的</strong>，因为它们来自单一模型，该模型总是高估给定相似状态的相同动作。否则就不会产生这种偏见。</p></blockquote><p id="ec00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"> Hado van Hasselt </a>纠正这种偏差的想法是并行使用<strong class="la iu"> 2个函数逼近器</strong>(因此得名<strong class="la iu">双</strong> Q-learning):</p><ul class=""><li id="874e" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated">一个用于给定状态下的<strong class="la iu">最大</strong>动作选择<strong class="la iu"/></li><li id="75cc" class="mw mx it la b lb nf le ng lh nh ll ni lp nj lt nb nc nd ne bi translated">另一个用于与所选最大动作相关的Q值计算</li></ul><p id="7499" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，使用两个智能体给我们嘈杂的估计带来了渴望已久的随机性。让我解释一下🤓<br/>两个模型不会高估给定相似状态的相同动作，因为它们被不同地训练，所以使用一个选择动作，另一个计算关联的&lt;状态，选择的动作&gt;值以随机方式混合两个模型的噪声，因此<strong class="la iu">去偏置值估计</strong>。</p><p id="e4bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免有太多的网络和混淆整个培训架构(我们已经有了在线和目标网络😅！)，已经表明使用在线网络作为Q值计算机和目标网络作为最大动作选择器对于代理学习能力来说工作良好。</p><h1 id="33e6" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">🌀体验回放</h1><p id="64ca" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">到目前为止，人们理所当然地认为DQN正在从一系列连续的转变中学习...我们再想想。</p><p id="7937" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们采取一系列连续的转变，大多数情况下，它们看起来是一样的。在ML中，我们可以说<strong class="la iu">连续转换是高度相关的</strong>……向DNN输入相关输入会使DNN连续过度拟合少数相关样本，从而降低其学习速度。事实上，我们都知道改组数据集对模型收敛更好🙏</p><p id="cb53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，<strong class="la iu">经验重放</strong>程序(在下面的图7中解释)来解除输入到DNN的输入转换的相关性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/338476ef01a5adf61b59ebd6e652008d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M-rTjP8NtjwlQmNfBI0kKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图7。</strong>不是直接从连续的过渡流中学习，而是将图像存储在一个缓冲区中，并从中随机采样以提供给代理。</p></figure><blockquote class="pj pk pl"><p id="bf3a" class="ky kz ok la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated"><strong class="la iu">注意</strong>:重放记忆是一种真正的神经科学现象，发生在海马体(至少对啮齿动物来说是这样的<em class="it">🐭</em>)。</p></blockquote><h1 id="dec5" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">🔥优先体验重放(PER)</h1><p id="6fba" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">对于这一个，一切都在名字里。事实上，不是从重放缓冲区统一采样，而是根据<em class="ok">对代理的</em>转换有多感兴趣，以<strong class="la iu">优先级</strong>采样转换。简而言之，更多的<em class="ok">有趣的</em>跃迁被采样的概率更高。</p><p id="434b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一、<em class="ok">有趣的</em>对代理来说意味着什么？有时，对于某些转换，代理的表现要比其他的差得多。这意味着代理的TD误差对于这些转换来说更高。因此，PER背后的想法是关注那些预测不佳的转换(也称为<em class="ok">有趣的</em>转换),以提高代理的学习。</p><blockquote class="op"><p id="5528" class="oq or it bd os ot ou ov ow ox oy lt dk translated">永远针对你的弱点进行训练🦾#保护</p></blockquote><p id="518c" class="pw-post-body-paragraph ky kz it la b lb oz ju ld le pa jx lg lh pb lj lk ll pc ln lo lp pd lr ls lt im bi translated">下面的图8显示了如何计算转换的优先级分数。一旦一个转换被采样，它的TD误差(以及它的优先级分数)立即被学习代理更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/6e69d317e32cc4cebd5fb7992ace3394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWb1FTLHeOkvQUkNs2xSRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图8。</strong>优先级分数计算</p></figure><p id="1f89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，采样策略中的这种优先化也导致了对代理学习机制的干扰。事实上，一遍又一遍地重复具有高TD误差的转换使得代理由于缺乏多样性而过度适应那些预测不佳的例子。除此之外，优先化使输入跃迁分布偏离真实跃迁分布，因此引入了干扰DNN收敛的偏差。</p><p id="2e8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是不用担心，像往常一样一些聪明的咒语被发现来对付这些诅咒🧙‍♂️</p><ul class=""><li id="4ef0" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><strong class="la iu">随机优先级</strong>通过<strong class="la iu">平滑</strong>原始优先级分数分布，允许对一些低TD误差的转换进行采样，从而避免了缺乏多样性的问题(参见下图9)</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/ff480c60f89e7d0860249e13244bdd41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e56FG04PV0JFBV4hD2FrXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图九。</strong>α<em class="ps">越接近1，采样优先级越高，越接近0，越均匀。</em></p></figure><ul class=""><li id="c643" class="mw mx it la b lb lc le lf lh my ll mz lp na lt nb nc nd ne bi translated"><strong class="la iu">重要性-采样</strong>进入环中<strong class="la iu">重新权衡</strong>在深度Q学习中代理的参数更新了多少，以补偿由优先采样引起的分布偏移。<em class="ok"> β </em>越接近1，补偿越多。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/1f350c4114ea271539a2a60a83f2aa9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pNn5tgyYpnBh7j77UJ_Cmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mv">图10。</strong>给定一个特定的<em class="ps"> α(本例中α=0.5)，β补偿优先采样引入的偏差(当β → 1时完全补偿)。</em></p></figure><p id="29e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">能够在训练期间调整两个参数<em class="ok"> α </em>和<em class="ok">β增强了代理的学习能力和其更快地收敛到最优策略</em></p><blockquote class="pj pk pl"><p id="b880" class="ky kz ok la b lb lc ju ld le lf jx lg pm li lj lk pn lm ln lo po lq lr ls lt im bi translated"><strong class="la iu"> NB </strong>:重复优先级化采样和更新那些优先级具有线性复杂度O(n)。然而，在DRL，代理的学习必须探索大量的转换，因此这种线性采样的复杂性在整个训练过程中产生了瓶颈。因此，为了克服这一工程问题，一种特定的数据结构被用于优先化缓冲器:<a class="ae lu" href="https://www.fcodelabs.com/2019/03/18/Sum-Tree-Introduction/#:~:text=What%20is%20a%20Sum%20Tree,the%20values%20of%20its%20children." rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> SumTree </strong> </a> <strong class="la iu"> </strong>，这将采样复杂度从O(n)提高到O(log(n))。</p></blockquote></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f091" class="nl nm it bd nn no pu nq nr ns pv nu nv jz pw ka nx kc px kd nz kf py kg ob oc bi translated">结论</h1><p id="321e" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated">干得好！我们已经完成了一件很好的工作，🥵.</p><p id="b578" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们有足够的徽章来理解DQN的初始算法1。让我们快速回顾一下不同的关键思想，并将它们映射到算法1伪代码的行中:</p><p id="dcc4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">⛰️ Q-learning:第11行<br/>💧深度q-learning:11线、13线&amp; 15线<br/> ⚡在线vs目标网:11线&amp; 16线<br/>🍃双(深度)Q学习:第11行<br/>🌀体验回放:第6行&amp;第9行<br/>🔥优先体验回放:6、9、10线&amp; 12</p><p id="b60d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所见，需要无数数学/工程/计算<strong class="la iu">技巧</strong>来成功混合强化学习框架和深度学习，以达到超人的性能。<br/> DQN可能是一个旧模型(2015年在ML时间尺度上就像一个永恒)，但其核心概念是更复杂和最近的深度RL架构的构建块。</p><p id="45e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这篇文章澄清了你可能有的一些疑问，并让你对深度学习和强化学习如何结合起来构建超人代理有更多的直觉和理解。</p><p id="1558" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">瞧啊！👌</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="27e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ok">除特别注明外，所有图片均为作者所有</em></p><h1 id="49f1" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated"><strong class="ak">参考文献</strong></h1><p id="99f5" class="pw-post-body-paragraph ky kz it la b lb od ju ld le oe jx lg lh of lj lk ll og ln lo lp oh lr ls lt im bi translated"><strong class="la iu">论文:</strong> <br/> - <a class="ae lu" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">玩雅达利用深度强化学习</a>论文来自DeepMind <br/> - <a class="ae lu" href="https://arxiv.org/pdf/1509.06461.pdf" rel="noopener ugc nofollow" target="_blank">深度强化学习用双Q学习</a>论文来自DeepMind <br/> - <a class="ae lu" href="https://arxiv.org/pdf/1511.05952.pdf" rel="noopener ugc nofollow" target="_blank">优先化经验回放</a>论文来自DeepMind</p><p id="c9e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">博客&amp;帖子:</strong> <br/> - <a class="ae lu" href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26" rel="noopener ugc nofollow" target="_blank">用深度强化学习打败雅达利！</a>来自Adrien Ecoffet的great Deep RL系列<br/> - <a class="ae lu" href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" rel="noopener ugc nofollow" target="_blank">让我们制作一个DQN:双重学习和优先化经验回放</a>来自Jaromiru在Deep RL <br/>上的非常好的博客——丹尼尔·晴太在RL <br/>上的<a class="ae lu" href="https://danieltakeshi.github.io/archive.html" rel="noopener ugc nofollow" target="_blank">博客</a>帖子——罗伯特·兰格在DQN的<a class="ae lu" href="https://roberttlange.github.io/posts/2019/08/blog-post-5/" rel="noopener ugc nofollow" target="_blank">博客</a></p><p id="c192" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ok"> Au plus gentil des gars，我的朋友阿德里安。</em></p></div></div>    
</body>
</html>