<html>
<head>
<title>DINO-ViT — Beyond Self-Supervised Classifications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DINO-ViT —超越自我监督的分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dino-vit-beyond-self-supervised-classifications-3f5d43178216#2022-09-23">https://towardsdatascience.com/dino-vit-beyond-self-supervised-classifications-3f5d43178216#2022-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4af6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在没有监督的情况下提取精细的特征</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/25dd883f99d12774eb052eae071349d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsP3Mq83-WAvnNq6-fUUZw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。自我监督学习是实现真正人工智能的重要一步。从Unsplash检索的图像。</p></figure><p id="6005" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以前，我写过几篇文章简要讨论自我监督学习，特别是<a class="ae lu" rel="noopener" target="_blank" href="/contrastive-learning-in-3-minutes-89d9a7db5a28">对比学习</a>。然而，还没有包括的是一个并行的自我监督方法的分支，它使用了最近出现并表现出色的多个网络的交互作用。迄今为止，最先进的培训方法之一是一种主要的知识提取方法，名为DINO，应用于视觉变形金刚(DINO-ViT)。然而，这种架构最令人惊讶的元素不再是其强大的分类知识，而是其密集的功能，这些功能实际上能够执行更细粒度的任务，如部分分割，甚至跨多个对象的对应。</p><p id="d22d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将回顾DINO-ViT是如何被训练的，然后是一个简短的教程，介绍如何利用现有的库进行零件共分割和寻找对应关系。</p><h1 id="097e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">迪诺维特是什么？</h1><p id="687f" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">恐龙这个词来自于没有监督的自我升华。顾名思义，DINO-ViT利用传统知识提炼方法的变体，将其应用于强大的<a class="ae lu" rel="noopener" target="_blank" href="/vision-transformers-in-pytorch-43d13cb7ec7a">视觉转换器(ViT) </a>架构。这个想法多少受到了技术<a class="ae lu" href="https://arxiv.org/abs/2006.07733" rel="noopener ugc nofollow" target="_blank"> <em class="ms">引导你自己的潜能(BYOL) </em> </a>的启发，我们将在接下来的文章中更全面地介绍这个技术。</p><h2 id="6b00" class="mt lw it bd lx mu mv dn mb mw mx dp mf lh my mz mh ll na nb mj lp nc nd ml ne bi translated">知识提炼是如何工作的？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/c9b2503485701c2eeb8d9df321740546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0pkci1FSnZw7lGyM8Rz5w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。恐龙训练方法概述。从<a class="ae lu" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2104.14294</a>检索的图像。</p></figure><p id="b851" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单地说，知识提炼的目标是允许学生网络Ps向教师网络Pt学习。从计算机视觉的角度来看，目标可以转化为更新学生网络，以最小化给定图像x:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0de3041dd3906cf0551c6ab7585a06a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/1*qbs-x83oRxX0nQ6IdS3YvQ.gif"/></div></figure><h2 id="7507" class="mt lw it bd lx mu mv dn mb mw mx dp mf lh my mz mh ll na nb mj lp nc nd ml ne bi translated"><em class="nh">自我监督的知识蒸馏</em></h2><p id="2e0c" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">下面描述了蒸馏法是如何扩展到恐龙训练的:</p><p id="d7a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定特定的图像x，我们现在将该图像裁剪成两个全局视图和多个局部视图。所有作物(全局和本地)都被输入到学生网络中，但只有全局视图被输入到教师网络中。然后，我们按照知识提取方法最小化交叉熵。为了实现这一目标，训练学习在图像内创建局部和全局对应，从而鼓励学生网络学习适当的图像特征。</p><h2 id="5683" class="mt lw it bd lx mu mv dn mb mw mx dp mf lh my mz mh ll na nb mj lp nc nd ml ne bi translated"><strong class="ak">从学生那里得到老师</strong></h2><p id="b15e" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">与知识提炼的原始问题设置不同，其中教师网络的权重是先验给定的，DINO没有经过预训练的教师。因此，教师网络实际上是通过移动平均方法从以前的学生中检索的。执行额外的居中以避免两个模型塌陷。</p><h1 id="ddc3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类、细分等等</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/881cdcdcd82e21653423c974f0f4223c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMnOgvQTMY4V-WofAXDNUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。从https://arxiv.org/abs/2104.14294<a class="ae lu" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">取回的迪诺-维特图像注意图</a>。</p></figure><p id="64a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类一直是所有自我监督方法的标准基准，DINO-ViT无疑是领先的得分者之一。然而，DINO-ViT的迷人之处在于其通过自蒸馏获得的精细致密特性。图2说明了在没有任何监督的情况下，迪诺-维特的注意力是如何令人惊讶地集中在物体的前景上的。这意味着高分类精度实际上仅仅是能做更多事情的表示的结果。</p><h2 id="5fd1" class="mt lw it bd lx mu mv dn mb mw mx dp mf lh my mz mh ll na nb mj lp nc nd ml ne bi translated">下一步是什么？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/1312dd1d26f9b79a4c5c165e172ffc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I46h1NC5N_XmaVOZybsVeQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。使用DINO-ViT特征的零件共同分割结果。图片来自https://arxiv.org/abs/2112.05814<a class="ae lu" href="https://arxiv.org/abs/2112.05814" rel="noopener ugc nofollow" target="_blank"/>。</p></figure><p id="487e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在DINO-ViT之后是一篇名为<a class="ae lu" href="https://arxiv.org/abs/2112.05814" rel="noopener ugc nofollow" target="_blank"> <em class="ms">的论文，深度ViT特征作为密集的视觉描述符</em> </a>。Amir等人在这项工作中描述了DINO-ViT的密集分片特征，以及额外的聚类和简单的无监督方法，可以执行非常具有挑战性的任务，例如部分共分割和对应查找(如图3所示)。事实上，这些特征如此强大，以至于它可以在不同类别的图像中找到对应关系。</p><h2 id="4885" class="mt lw it bd lx mu mv dn mb mw mx dp mf lh my mz mh ll na nb mj lp nc nd ml ne bi translated"><strong class="ak">使用密集特征</strong></h2><p id="9bad" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Amir等人在他们的项目页面上为有兴趣的个人发布了现成的Google Colab笔记本。您也可以按照GitHub的说明安装软件包，并批量运行这些方法。</p><p id="b211" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该代码的链接可在此处找到:</p><div class="nk nl gp gr nm nn"><a href="https://github.com/ShirAmir/dino-vit-features" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">GitHub-shira mir/dino-vit-Features:论文“深度ViT特性作为…</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">论文《深度ViT特征作为密集视觉描述符》正式实施。我们证明了有效性…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">github.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ks nn"/></div></div></a></div><h1 id="2cc6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结束注释</h1><p id="3761" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">创造真正的人工智能的漫长旅程似乎是无穷无尽的，但自我监督学习无疑是朝着它迈出的一大步。迪诺-维特展示了对网络如何学习的见解，这些学习特征背后的巨大潜力确实将成为计算机视觉领域的重要垫脚石。</p><p id="2cf0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ms">感谢您坚持到现在</em>🙏<em class="ms">！</em> <em class="ms">我定期写关于计算机视觉/深度学习的不同领域，所以</em> <a class="ae lu" href="https://taying-cheng.medium.com/membership" rel="noopener"> <em class="ms">加入并订阅</em> </a> <em class="ms">如果你有兴趣了解更多！</em></p></div></div>    
</body>
</html>