<html>
<head>
<title>Uncertainty in Deep Learning: Experiments with Bayesian CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的不确定性:贝叶斯CNN实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954#2022-04-12">https://towardsdatascience.com/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954#2022-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ba8b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">KL-Weight如何影响模型？为胸部x光图像编写贝叶斯CNN。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f2e544c8300498dc9b6017dc3701b152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cq8KX7pyXGPiqEzw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@evan__bray?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃文·丹尼斯</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7e30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是深度学习中<strong class="ky ir">不确定性系列的<strong class="ky ir">第五部</strong>。</strong></p><ul class=""><li id="d8e3" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">第1部分—简介</a></li><li id="f9ab" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第2部分—随机不确定性和最大似然估计</a></li><li id="f108" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第3部分——认知不确定性和反向投影贝叶斯</a></li><li id="aae2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6">第4部分——实现完全概率贝叶斯CNN </a></li><li id="daf3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">第五部分——贝叶斯CNN实验</strong></li><li id="d88f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">第六部分——贝叶斯推理和变形金刚</li></ul><p id="95f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章是前几篇文章的后续。因此，在这篇文章中，并不是每一个术语或层都像在前面的部分中解释的那样进行解释。</p><h1 id="3641" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">介绍</h1><p id="d45a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第三部分</a>中，我们使用了<code class="fe nd ne nf ng b">DenseVariational</code>层，以便通过反向投影算法实现<strong class="ky ir">贝叶斯。这一层接受<code class="fe nd ne nf ng b">kl_weight</code>作为参数，我们将看到它是如何影响模型的。</strong></p><p id="bdbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文的下一节，我们将看到如何实现一个具有剩余连接的贝叶斯CNN模型。实现的模型将在真实数据集上训练。</p><p id="07f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该职位的组织结构如下:</p><ul class=""><li id="ab35" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">KL-重量摘要</strong></li><li id="0601" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">多元线性作为变分后验</strong></li><li id="5d1c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">不同KL-权重和方差的后验</strong></li><li id="37ab" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">加载真实数据集</strong></li><li id="1e78" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">作为正则化者的先验</strong></li><li id="0a6e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">提供自定义的先验&amp;后验</strong></li><li id="5319" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> DenseNet121作为特征提取器</strong></li><li id="4b2e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">贝叶斯ResNet </strong></li><li id="0320" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">检查结果和不确定度</strong></li><li id="8e2f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">结论</strong></li><li id="c9cc" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">接下来的步骤</strong></li></ul><h1 id="46b7" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">KL-重量概述</h1><p id="e329" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">回想一下，我们不能计算真实的后验概率，因此我们需要近似它。也就是说，我们将应用<strong class="ky ir">变分贝叶斯</strong>技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ce40aea934aca527e40e35da29b6259f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*eLXKbUDFmpOrefMDMdnzRQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。(摘自第三部分)</p></figure><p id="ee2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">凭直觉，我们希望这两个分布之间的KL-散度尽可能低，这表明我们的变分后验接近于原始分布。</p><p id="77c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于数学运算在第3部分有所解释，我将直接得出结论，最终损失如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/fcb0798a1b793dbcb610fbecc67e6cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JH2M8-ShTX2wjAmOer0T1g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终损失，贝叶斯反投影。(第三部分)</p></figure><p id="a591" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nd ne nf ng b">1/M</code>是KL-Weight，其中<code class="fe nd ne nf ng b">M</code>是训练样本的总数。我们将从一个玩具数据集开始。</p><p id="a0ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于KL重量部分，进口如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="19d3" class="nl mh iq bd mi nm nn dn mm no np dp mq lf nq nr ms lj ns nt mu ln nu nv mw nw bi translated"><code class="fe nd ne nf ng b">MultivariateNormalTriL()</code></h2><p id="1452" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">对于我们的任务，我将展示<code class="fe nd ne nf ng b">tfpl.MultivariateNormalTriL()</code>。这一层将允许我们把我们的变分后验概率指定为多元分布。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="2ff9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的先验分布只是一个标准的正态分布。然而，后验概率是一个具有完全协方差的多元正态分布。和<code class="fe nd ne nf ng b">tfpl.VariableLayer</code>一起，这将是一个可训练的分布。</p><p id="81c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nd ne nf ng b">params_size</code>将为<code class="fe nd ne nf ng b">MultivariateNormalTriL()</code>确定正确的参数数量。</p><p id="de01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看它如何影响模型参数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="b29b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设，我们在一个数据集中有<strong class="ky ir"> 3个要素和2个类</strong>。我们将尝试使用神经元来学习数据集。这基本上是逻辑回归。问题是，模特有14个可训练参数。让我们用一个图表来解释这个问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/ab1d3ca6f88ee5802653db64f40170a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIZZlxsoTcREZfLaL0GzIQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="1bd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">多元正态分布</strong>由<strong class="ky ir">均值</strong>向量和<strong class="ky ir">协方差矩阵</strong>定义。</p><p id="6000" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">协方差矩阵测量两个随机变量的依赖程度。换句话说，它显示了它们如何一起变化。在复杂模型中，由于协方差矩阵的原因，在每一层中为多元正态分布设置变分后验概率是不可行的。</p><h2 id="54a1" class="nl mh iq bd mi nm nn dn mm no np dp mq lf nq nr ms lj ns nt mu ln nu nv mw nw bi translated">KL-重量实验</h2><p id="ad2d" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">显然，KL-Weight参数将影响我们的变分后验概率。因此，我们需要创建一个自定义回调来监控我们的变化后验概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/fcb0798a1b793dbcb610fbecc67e6cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JH2M8-ShTX2wjAmOer0T1g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终损失，贝叶斯反投影。(第三部分)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="b48e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每个时期结束时，我们检索我们的变分后验标准差，并将其附加到一个列表中。</p><p id="b8df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Sklearn的帮助下，数据创建过程非常简单:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b9024e5b89d0fcebf3e60c4d167be1b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*jMuaX3EHdCcKrnwvpDWnMw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="c43b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于输出将是一个分布，我们可以直接使用负对数似然作为损失函数。回想一下，标签是热编码的，所以最后使用的是<code class="fe nd ne nf ng b">tfpl.OneHotCategorical</code>层。</p><p id="5f2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">后验函数和先验函数是相同的:</p><ul class=""><li id="f419" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">先前:</strong>正常</li><li id="4bc0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">变分后验:</strong>带协方差的多元正态分布</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="f812" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个循环中运行不同<code class="fe nd ne nf ng b">kl_weights</code>的实验:</p><ul class=""><li id="50ec" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><code class="fe nd ne nf ng b">1/x.shape[0]</code>在这里是正确的，因为我们除以数据集中的元素总数。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="3608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失值和准确度值看起来很接近。那用比如<code class="fe nd ne nf ng b"><strong class="ky ir">kl_weight</strong></code> <strong class="ky ir"> = 0可以吗？大多数情况下，你可能不想这么做。这是因为我们在这里做<strong class="ky ir">贝叶斯深度学习</strong>，避免得到固定的预测。</strong></p><p id="2b31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这听起来可能有点神秘，所以首先让我们检查不同<code class="fe nd ne nf ng b">kl_weight</code>项的变分后验方差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/81910f5f63541dfa1c42ebea545f78f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRc9wL63M3viU8jrwiQhBQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="2982" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于值<code class="fe nd ne nf ng b">0.0, 0.0001, 0.0005</code>，方差较低。请记住，为了得到预测，我们从变分后验样本。因此<strong class="ky ir">如果方差接近零或某个低值，采样值将趋于更接近</strong>。换句话说，如果方差变为零，我们可以收敛到一个MLE解！</p><p id="823e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这有一个严重的缺点，在贝叶斯深度学习中，我们的目标是测量模型对给定输入的信心。如果方差低于预期值，这可能会导致高估模型的可信度。</p><h1 id="304e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">真实数据集</h1><p id="4ec8" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在本帖中，我们将使用<strong class="ky ir"> Paul Mooney </strong>在Kaggle上发布的<strong class="ky ir">胸部x光图像(肺炎)【1】</strong>数据集。数据集包括两类，正常和肺炎。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="308a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nd ne nf ng b">label_mode</code>被选为分类层，因此我们可以使用<code class="fe nd ne nf ng b">OneHotCategorical</code>层作为模型的最后一层。数据加载过程很简单，所以我跳过这一部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/63f411b842e2ef05d101c79fdf805174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Ormc9c55NtHO1W8jXRbGHA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据集中的图像。</p></figure><p id="5271" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在进入模型创建过程之前，让我们也检查一下类分布</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/2d597311d1a51c227c442f8250fa45eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9TCWuzUMZYKmodXmpcxog.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="8117" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有绘制图像和类分布的相关代码都可以在笔记本中找到，我会在文章的最后分享它的链接。</p><p id="c93b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为先验，我会用<code class="fe nd ne nf ng b">MultivariateNormalDiag</code>。这种分布允许建立具有对角协方差矩阵多元正态分布。为<code class="fe nd ne nf ng b">Convolution2DReparameterization</code>设置自定义先验很容易，正如我们在前一部分看到的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="c065" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为先验没有可训练的参数，所以像这样构建并返回一个独立的分布对象就足够了。</p><p id="6dd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果那个<code class="fe nd ne nf ng b">MultivariateNormalDiag</code>是2D，它可以被想象成如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9a60e6c9e23f95c97d5903b5ffa0ec8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*Q0q3nlqky3S82ovhNizMNg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。(位置= 1.2，刻度= 3.0)</p></figure><p id="61e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们可以说它是一个球形分布。从贝叶斯的角度来看，<strong class="ky ir">先验充当了正则化子</strong>。所以如果你使用，比方说<strong class="ky ir">拉普拉斯</strong>先验，你实际上将使用<strong class="ky ir"> L1正则化</strong>。我不会在此详述，但如果您想深入了解，可以查看一下:</p><div class="od oe gp gr of og"><a href="https://stats.stackexchange.com/a/163450" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd ir gy z fp ol fr fs om fu fw ip bi translated">为什么L2正则化等价于高斯先验？</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">感谢您为交叉验证提供答案！请务必回答问题。提供详细信息并分享…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">stats.stackexchange.com</p></div></div><div class="op l"><div class="oq l or os ot op ou kp og"/></div></div></a></div><p id="2048" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，有许多分布可以用作先验分布:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/e965afbed3244384276fd805a1667741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bS_Z2bhdOUvccz_JoUmOA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="e18a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，TFP库使得使用这些发行版变得很容易。由于这些先验表示我们对模型参数的先验信念，它可以影响模型的收敛性。</p><p id="e31a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，马蹄形分布不会将大重量缩小太多。并且小的值将被推向零。</p></div><div class="ab cl ow ox hu oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="ij ik il im in"><h2 id="78ca" class="nl mh iq bd mi nm nn dn mm no np dp mq lf nq nr ms lj ns nt mu ln nu nv mw nw bi translated"><code class="fe nd ne nf ng b">Reparameterization Layers</code>的自定义后验</h2><p id="51bb" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">因为变化后验概率是可训练的，所以在<code class="fe nd ne nf ng b">Reparameterization</code>层中创建它有点笨拙。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="3adb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<code class="fe nd ne nf ng b">default_loc_scale_fn</code>,我们为可训练参数生成一个闭包。闭包接受与<code class="fe nd ne nf ng b">posterior</code>函数相同的参数，并返回<code class="fe nd ne nf ng b">loc, scale</code>，它可以被传递到一个分布中。在那之后，发行版将是可训练的。</p><p id="9b66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在引擎盖下，<code class="fe nd ne nf ng b">tf.nn.softplus</code>应用于<code class="fe nd ne nf ng b">scale</code>参数，所以我们不需要担心它，例如是否为负。</p><p id="101d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，当处理贝叶斯DL时，如果变分后验恰好是正态分布，则<strong class="ky ir">容易收敛。</strong></p><p id="49d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，<strong class="ky ir">我将使用正态分布</strong>，但如果你愿意，这就是你改变后验概率的方法。</p><h2 id="c65b" class="nl mh iq bd mi nm nn dn mm no np dp mq lf nq nr ms lj ns nt mu ln nu nv mw nw bi translated">逼近KL散度</h2><p id="3892" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">因为我们改变了先验，TFP将不能计算KL散度的解析值。我们需要近似它:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="540e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想提到的一点是:在实践中，用其他值进行缩放可能会更好。根据我的经验，这是可以的，但你也可以尝试其他合理的值。</p><h2 id="b5d6" class="nl mh iq bd mi nm nn dn mm no np dp mq lf nq nr ms lj ns nt mu ln nu nv mw nw bi translated">贝叶斯网</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="84ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些层需要很多参数，封装在这样的函数里更方便。对于后验分布，我们使用默认值，并通过<code class="fe nd ne nf ng b">is_singular = False</code>来确保它们是<strong class="ky ir">而不是</strong>点估计。</p><p id="e153" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nd ne nf ng b">tfpl.default_mean_field_normal_fn</code>是参数可训练的正态分布。它实际上是对后路的默认。</p><p id="a6d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩余部分很简单，看起来比平常长，因为<code class="fe nd ne nf ng b">reparameterization</code>层接受了太多的参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4e93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将遵循不同的方法。<strong class="ky ir">我们不是从头开始训练一个</strong> <strong class="ky ir">贝叶斯CNN，而是采用一个预训练的模型，仅用于特征提取</strong>，这意味着它们的层将被冻结，在训练期间不会更新。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="a674" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nd ne nf ng b">DenseNet121</code>将是该任务中的特征提取器。此外，不是采取整个架构，它将被分割使用功能API。</p><ul class=""><li id="a2c4" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们获取<code class="fe nd ne nf ng b">conv5_block9_concat</code>的输出，并将其提供给我们的贝叶斯残差层。</li><li id="70b5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">贝叶斯卷积层将分别有64个和128个滤波器，随后是<code class="fe nd ne nf ng b">swish</code>激活。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="cb94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们准备使用数据集来训练模型。</p><ul class=""><li id="4eb9" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">损失是负对数似然，因为产出是一个分布</li><li id="a2b1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用回调来提高训练，如果<code class="fe nd ne nf ng b">val_accuracy</code>没有连续提高7个纪元，早期停止将给出最佳权重。</li><li id="4da9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">如果<code class="fe nd ne nf ng b">val_accuracy</code>没有任何改善，学习率将每3个周期降低一次。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="cae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终，模型达到了85%的测试准确率。然而，我们更感兴趣的是预测的不确定性，而不是简单的准确性。</p><h1 id="b673" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结果</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4895" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们检查一下测试集中的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/fffdcd5fa9c11c0db604454e11261edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awPfNQnRa7R91zNWWTElDw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="e628" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于该预测，模型将很高的概率分配给正确的类别1。可以说模型对这个预测是有把握的，而且不确定性很低。</p><p id="16c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">条形图显示了预测的不确定性。因此，如果<strong class="ky ir">柱更高</strong>，我们<strong class="ky ir">得出结论，不确定性更高</strong>。这是因为，我们显示了95%的置信区间，条形的长度显示了百分位数2.5和97.5之间的差异。</p><p id="6329" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最好的情况下，模型将只分配1作为预测值，百分位数2.5和97.5之间的差值为零。在这种情况下，条形会更短。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/e455f325a4596095765dd0c0356121f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HncCzxilQK5tP_28Bbujg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="d7ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个具体的例子，真实类是正常的，但是模型似乎很难预测它。例如，在向前传递中，它为正常类分配了非常高的概率。但是在另一次向前传递中，类正常的概率很低。<strong class="ky ir">我们得出结论，模型对这个预测是不确定的，并且知道它不知道的东西！</strong></p><h1 id="5f1f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="955a" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在本帖中，我们:</p><ul class=""><li id="7b02" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">已经看到当选择<code class="fe nd ne nf ng b">MultivariateNormalTriL()</code>作为变分后验概率时，它如何影响模型的参数，</li><li id="9ef2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">已经看到了KL-Weight的影响，</li><li id="1070" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">已经学会如何为<code class="fe nd ne nf ng b">Reparameterization</code>层提供定制的先验和后验函数，</li><li id="0cac" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用了预先训练的模型作为特征提取器并将特征向量输入到贝叶斯ResNet模型中，</li><li id="e916" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">检查了测试集中两幅图像的不确定性。</li></ul><p id="7361" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以从这里获得<a class="ae kv" href="https://github.com/Frightera/Medium_Notebooks_English/tree/main/Bayesian%20CNN%20-%20Experiments" rel="noopener ugc nofollow" target="_blank">笔记本。</a></p><h1 id="4177" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">后续步骤</h1><p id="1fbc" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您可以尝试数据扩充技术，看看它是否有助于减少预测中的不确定性。</p><p id="60b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一篇文章中，我们将探讨关于<strong class="ky ir">变形金刚和贝叶斯推理。</strong></p><h1 id="17a3" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="5cda" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">[1]:科曼尼，丹尼尔；张、康；Goldbaum，Michael，“用于分类的标记光学相干断层扫描(OCT)和胸部X射线图像”，Mendeley Data，V2。2018，doi: 10.17632/rscbjbr9sj.2，【https://data.mendeley.com/datasets/rscbjbr9sj/2】T4(CC BY 4.0)</p></div></div>    
</body>
</html>