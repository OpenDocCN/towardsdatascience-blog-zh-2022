<html>
<head>
<title>How to Run a Stable Diffusion Server on Google Cloud Platform (GCP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在谷歌云平台(GCP)上运行稳定的扩散服务器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-run-a-stable-diffusion-server-on-google-cloud-platform-gcp-c879357808bf#2022-09-05">https://towardsdatascience.com/how-to-run-a-stable-diffusion-server-on-google-cloud-platform-gcp-c879357808bf#2022-09-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad18" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">构建和部署Flask应用程序的分步指南</h2></div><p id="4ad1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自从<a class="ae lb" href="http://Stability.ai" rel="noopener ugc nofollow" target="_blank"> Stability.ai </a>在短短几周前发布了stability Diffusion(他们的开源文本到图像模型)以来，ML社区已经为它打开的大门而疯狂。作为OpenAI的门控<a class="ae lb" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank"> DALL E 2 </a>的开源替代品，稳定扩散为每个人提供了一些东西:最终用户可以几乎免费地生成图像，开发者可以将模型嵌入到他们的服务中，ML工程师可以调查和修改代码，研究人员有充分的回旋余地来进一步推动最先进的技术。</p><p id="30b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管有大量关于如何利用稳定扩散的教程，但我自己找不到一个验证过的托管模型的方法。我的目标是从我的浏览器舒适地向我自己的服务发出HTTP请求。没有信用额度，没有登录的麻烦，没有人偷看我的照片。因此，我开始了为期一天的探索，在谷歌云上构建和部署一个稳定的扩散网络服务器。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/b8c0c3c4742bfafbe4748ece53d0f538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F37dOhZJNtYmTtaHyxGrHg.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">人工智能生成的图像。左:<a class="ae lb" href="https://stability.ai/blog/stable-diffusion-announcement" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>。右:<a class="ae lb" href="http://midjourney.com" rel="noopener ugc nofollow" target="_blank">中途</a>。通过<a class="ae lb" href="https://huggingface.co/succinctly/text2image-prompt-generator" rel="noopener ugc nofollow" target="_blank">生成的提示简洁AI </a>:“棉花糖做的火在雨中跳舞的机器人，超逼真，真实感，4k，精致，辛烷渲染”。</p></figure><p id="12bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章包括了我必须弄清楚的所有痛苦的小细节，希望它能节省你的时间。以下是一些高级步骤(我们将在下面更深入地探讨每一个步骤):</p><ol class=""><li id="0b3c" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">确保你有足够的GPU配额</li><li id="4471" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">创建一个连接了GPU的虚拟机</li><li id="e8f9" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">下载稳定扩散和测试推断</li><li id="7df5" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">捆绑稳定扩散到烧瓶应用程序</li><li id="0ebc" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">部署您的web服务器并使其可公开访问</li></ol><h1 id="ed2b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">1.确保你有足够的GPU配额</h1><p id="aa68" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">由于GPU仍然不便宜，谷歌对其GPU车队实行严格的管理，将有限的供应提供给最需要它的人，以及那些愿意付费的人。默认情况下，免费试用帐户没有GPU配额。要检查您的GPU配额:</p><p id="42bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">Navigation (hamburger menu) &gt; IAM &amp; Admin &gt; Quotas</code>和<code class="fe nd ne nf ng b">CTRL+F</code>表示“GPU(所有地区)”。如果您的限额为0或您当前的使用率为100%，您将需要请求额外的配额。否则，您可以跳到下面的步骤2。</p><p id="6d5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要增加您的配额，请选择“GPU(所有区域)”行，然后单击<code class="fe nd ne nf ng b">EDIT QUOTAS</code>按钮(控制台的右上角)。对于本教程，您将需要一个单一的GPU，所以增加您的配额1。请注意，您必须在您的请求中包含一个理由— <strong class="kh ir">确保您提供了一个CPU不能满足您需求的解释</strong>。我最初的请求被拒绝了，因为它只包含了一个空洞的便条。在我的第二次(也是成功的)尝试中，我明确表示我正在处理一个需要GPU的大型ML模型。请注意，如果您的请求由人工审核，可能需要2-3个工作日；如果你跟进并解释你的紧急情况，他们可能会更快回复。</p><h1 id="079e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">2.创建一个连接了GPU的虚拟机</h1><p id="1774" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">一旦有了GPU配额，现在就可以创建一个连接了GPU的虚拟机(VM)实例。</p><p id="65fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从导航(汉堡菜单):<code class="fe nd ne nf ng b">Compute Engine &gt; VM instances</code>。然后点击<code class="fe nd ne nf ng b">CREATE INSTANCE</code>(控制台左上角)。关于如何填写此表格的一般说明，您可以<a class="ae lb" href="https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus#create-new-gpu-vm" rel="noopener ugc nofollow" target="_blank">遵循此官方指南</a>；这里，我将重点介绍与运行稳定扩散特别相关的设置:</p><ul class=""><li id="e91b" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la nh ly lz ma bi translated"><strong class="kh ir">系列</strong>:选择<code class="fe nd ne nf ng b">N1</code></li><li id="acc9" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">机器类型:</strong>选择<code class="fe nd ne nf ng b">n1-standard-4</code>。这是最便宜的选择，有足够的内存(15GB)来加载稳定的扩散。不幸的是，下一个最便宜的选项(7.5GB)不够用，当加载模型并将其传输到GPU时，您将耗尽内存。</li><li id="4b56" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir"> GPU类型:</strong>展开<code class="fe nd ne nf ng b">CPU PLATFORM AND GPU</code>，点击<code class="fe nd ne nf ng b">ADD GPU</code>按钮。选择<code class="fe nd ne nf ng b">NVIDIA Tesla T4</code> —这是最便宜的GPU，它完成了这项工作(它有16GB的VRAM，符合稳定扩散的10GB要求)。如果好奇，可以看看<a class="ae lb" href="https://cloud.google.com/compute/docs/gpus#general_comparison_chart" rel="noopener ugc nofollow" target="_blank">对比图</a>和<a class="ae lb" href="https://cloud.google.com/compute/gpus-pricing#gpus" rel="noopener ugc nofollow" target="_blank">定价图</a>。请注意，您可以让GPU抢占以获得更好的价格(例如，谷歌将在需要它用于更高优先级的工作时收回它)，但我个人认为即使只是玩玩也令人沮丧。</li><li id="f833" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">图片:</strong>向下滚动到<code class="fe nd ne nf ng b">Boot disk</code>，点击<code class="fe nd ne nf ng b">SWITCH IMAGE</code>。对于操作系统，选择<code class="fe nd ne nf ng b">Deep Learning on Linux</code>；对于版本，选择<code class="fe nd ne nf ng b">Debian 10 based Deep Learning VM with CUDA 11.0 M95</code>。</li><li id="a53b" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">访问</strong>:假设你想要公开你的服务器:(a)在<code class="fe nd ne nf ng b">Identity and API access</code>下，选择<code class="fe nd ne nf ng b">Allow default access</code>；以及(b)在<code class="fe nd ne nf ng b">Firewall</code>下，选择<code class="fe nd ne nf ng b">Allow HTTP traffic</code>和<code class="fe nd ne nf ng b">Allow HTTPS traffic</code>。</li></ul><p id="d7ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，点击<code class="fe nd ne nf ng b">CREATE</code>按钮。请注意，这可能会相当昂贵(在撰写本文时，每月估计约为281美元)。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ni"><img src="../Images/37830eee4a3d9b53441d196f3e1c6bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CV6lyv1BgrDe6ENjdZ-bWQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在Google Cloud上安装了GPU的虚拟机的月成本明细(来自GCP控制台的截图)。</p></figure><h1 id="0fb1" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">3.下载稳定扩散和测试推断</h1><p id="4c48" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">一旦创建了VM实例，就可以通过SSH访问它。您可以从控制台选择“SSH-in-browser”选项，或者从终端运行以下命令:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="e6a2" class="nn mh iq ng b gy no np l nq nr">gcloud compute ssh --zone &lt;zone-name&gt; &lt;machine-name&gt; --project &lt;project-name&gt;</span></pre><p id="7f30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在你第一次登录的时候，你可以选择安装必要的GPU驱动程序。确保通过键入“y”来接受:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="a65c" class="nn mh iq ng b gy no np l nq nr">Would you like to install the Nvidia driver? [y/n]</span></pre><p id="f710" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，让我们验证一下，你可以在本地运行稳定扩散推断。首先，下载必要的工件:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="1aee" class="nn mh iq ng b gy no np l nq nr"># Clone the public Github repository.<br/>git clone <a class="ae lb" href="https://github.com/CompVis/stable-diffusion.git" rel="noopener ugc nofollow" target="_blank">https://github.com/CompVis/stable-diffusion.git</a></span><span id="3834" class="nn mh iq ng b gy ns np l nq nr"># Create a Python virtual environment.<br/>cd stable-diffusion<br/>conda env create -f environment.yaml<br/>conda activate ldm</span></pre><p id="e50c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用HuggingFace的<code class="fe nd ne nf ng b">diffusers</code>库来测试推论。创建一个名为<code class="fe nd ne nf ng b">inference.py</code>的新文件，内容如下:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="a0b5" class="nn mh iq ng b gy no np l nq nr">import torch<br/>from torch import autocast<br/>from diffusers import StableDiffusionPipeline</span><span id="6ab5" class="nn mh iq ng b gy ns np l nq nr">assert torch.cuda.is_available()</span><span id="d6e7" class="nn mh iq ng b gy ns np l nq nr">pipe = StableDiffusionPipeline.from_pretrained(<br/>    "CompVis/stable-diffusion-v1-4",<br/>    use_auth_token=True<br/>).to("cuda")<br/><br/>prompt = "a photo of an astronaut riding a horse on mars"<br/>with autocast("cuda"):<br/>    image = pipe(prompt)["sample"][0]  <br/>    <br/>image.save("astronaut_rides_horse.png")</span></pre><p id="a154" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，通过控制台登录HuggingFace，然后运行推理脚本:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="d76d" class="nn mh iq ng b gy no np l nq nr">huggingface-cli login<br/># Enter the access token from your HuggingFace account.</span><span id="a6b7" class="nn mh iq ng b gy ns np l nq nr">python inference.py</span></pre><p id="5767" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种调用可能会失败，并把你导向一个HuggingFace链接，在那里你被期望接受使用稳定扩散的条款和条件(他们只是想让你承认你不是邪恶的)。选中该框后，重新运行推理代码(大约需要15秒)，并确保可以在<code class="fe nd ne nf ng b">austronaut_rides_horse.png</code>下找到生成的图像。要将其下载到您的机器上进行查看，您可以使用<code class="fe nd ne nf ng b">gcloud compute scp</code>。</p><h1 id="25e5" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">4.捆绑稳定扩散到烧瓶应用程序</h1><p id="9b67" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">既然您已经验证了推理的正确性，我们将构建一个web服务器作为Flask应用程序。在每次查询时，服务器将读取<code class="fe nd ne nf ng b">prompt</code>参数，使用稳定扩散模型运行推理，并返回生成的图像。首先，安装Flask并为应用程序创建一个目录:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="2c4c" class="nn mh iq ng b gy no np l nq nr">pip install Flask<br/>cd ~; mkdir flask_app</span></pre><p id="aad1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将这个简单的Flask应用程序粘贴到一个名为<code class="fe nd ne nf ng b">app.py</code>的文件中:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="eb8d" class="nn mh iq ng b gy no np l nq nr">from flask import Flask, request, send_file<br/>import io<br/>import torch<br/>from torch import autocast<br/>from diffusers import StableDiffusionPipeline</span><span id="3c93" class="nn mh iq ng b gy ns np l nq nr">app = Flask(__name__)<br/>assert torch.cuda.is_available()</span><span id="152d" class="nn mh iq ng b gy ns np l nq nr">pipe = StableDiffusionPipeline.from_pretrained(<br/>        "CompVis/stable-diffusion-v1-4", <br/>        use_auth_token=True<br/>).to("cuda")</span><span id="9797" class="nn mh iq ng b gy ns np l nq nr">def run_inference(prompt):<br/>  with autocast("cuda"):<br/>      image = pipe(prompt)["sample"][0]  <br/>  img_data = io.BytesIO()<br/>  image.save(img_data, "PNG")<br/>  img_data.seek(0)<br/>  return img_data</span><span id="ca81" class="nn mh iq ng b gy ns np l nq nr"><a class="ae lb" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/')<br/>def myapp():<br/>    if "prompt" not in request.args:<br/>        return "Please specify a prompt parameter", 400</span><span id="ee98" class="nn mh iq ng b gy ns np l nq nr">    prompt = request.args["prompt"]<br/>    img_data = run_inference(prompt)<br/>    return send_file(img_data, mimetype='image/png')</span></pre><p id="6cf6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，这个应用程序非常简单，它只是返回原始图像。一个更实用的应用程序将返回一个HTML表单，其中有一个提示输入字段，可能还有其他旋钮(如所需的图像尺寸)。GradIO 和<a class="ae lb" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>是构建更复杂应用的优秀库。</p><p id="b858" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在验证Flask应用程序运行无误:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="5aff" class="nn mh iq ng b gy no np l nq nr">export FLASK_APP=app<br/>export FLASK_DEBUG=true<br/>flask run</span></pre><p id="7b48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这应该会在端口5000上启动本地主机上的服务器。您还不能从浏览器访问此服务器，因为默认情况下端口5000不可访问。</p><h1 id="ee49" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">5.部署并使您的服务器可公开访问</h1><p id="f716" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">虽然Flask的默认服务器很适合开发，但在生产中使用<a class="ae lb" href="https://gunicorn.org/" rel="noopener ugc nofollow" target="_blank"> gunicorn </a>部署Flask应用程序是标准做法。我不会在这里涵盖原因，但你可以阅读<a class="ae lb" href="https://blog.ironboundsoftware.com/2016/06/27/faster-flask-need-gunicorn/" rel="noopener ugc nofollow" target="_blank">这篇伟大的解释</a>为什么gunicorn是首选。要安装它，只需运行<code class="fe nd ne nf ng b">pip install gunicorn</code>。要启动web服务器，请运行以下命令:</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="0c36" class="nn mh iq ng b gy no np l nq nr">gunicorn -b :5000 --timeout=20 app:app</span></pre><p id="69ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nd ne nf ng b">-b</code>参数正在设置所需的端口。您可以将其更改为任何其他未使用的端口。<code class="fe nd ne nf ng b">--timeout</code>参数设置gunicorn重置其工人之前的秒数，假设出现了错误。由于在稳定扩散模型中运行正向传递平均需要15秒，因此将超时设置为至少20秒。</p><p id="b8e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您想让服务器在您退出VM实例后继续运行，那么您可以使用<code class="fe nd ne nf ng b">nohup</code> Linux实用程序(即“no hick-ups”):</p><pre class="ld le lf lg gt nj ng nk nl aw nm bi"><span id="f858" class="nn mh iq ng b gy no np l nq nr">nohup gunicorn -b :5000 --timeout=20 app:app &amp;</span></pre><p id="8227" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一个&amp;符号将进程发送到后台运行(因此您重新获得了对命令行的控制)。日志将被导出到一个名为<code class="fe nd ne nf ng b">nohup.out</code>的文件中，该文件通常位于您运行命令的目录中。</p><h2 id="5f4b" class="nn mh iq bd mi nt nu dn mm nv nw dp mq ko nx ny ms ks nz oa mu kw ob oc mw od bi translated">创建防火墙规则以使端口可访问</h2><p id="9114" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">最后一步是从浏览器向该服务器发出请求。为此，我们需要让您的端口可以访问。</p><p id="122e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从导航(汉堡菜单):<code class="fe nd ne nf ng b">VPC Network &gt; Firewall</code>。从顶部菜单中，点击<code class="fe nd ne nf ng b">CREATE FIREWALL RULE</code>。在表单中，设置以下内容:</p><ul class=""><li id="9dca" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la nh ly lz ma bi translated"><strong class="kh ir">名称</strong>:允许-稳定-扩散-访问(或您喜欢的名称)</li><li id="7d25" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">日志</strong>:开</li><li id="d467" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">交通方向</strong>:入口</li><li id="4eb6" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">匹配动作</strong>:允许</li><li id="64d2" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">目标</strong>:指定的目标标签</li><li id="4c94" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">目标标签</strong> : deeplearning-vm(当你选择“Linux上的深度学习”镜像时，这个标签会自动添加到你的vm中。您可以手动向VM添加另一个标记，并在这里引用它。)</li><li id="80fd" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nh ly lz ma bi translated"><strong class="kh ir">协议和端口:</strong>TCP-5000或您选择的端口。</li></ul><p id="2798" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">表格完成后，点击<code class="fe nd ne nf ng b">CREATE</code>。</p><h2 id="63d7" class="nn mh iq bd mi nt nu dn mm nv nw dp mq ko nx ny ms ks nz oa mu kw ob oc mw od bi translated">从浏览器向web服务器发送查询</h2><p id="52f5" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">最后，找到您的虚拟机的IP地址(从导航菜单，<code class="fe nd ne nf ng b">COMPUTE ENGINE &gt; VM INSTANCES</code>)并查看您的虚拟机的“外部IP”列。如果IP地址是12.34.56.789，那么您的网络服务器可以通过<a class="ae lb" href="http://12.34.56.789:5000" rel="noopener ugc nofollow" target="_blank">http://12 . 34 . 56 . 789:5000</a>访问。</p><p id="b184" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住，服务器需要一个名为<code class="fe nd ne nf ng b">prompt</code>的参数，我们可以将它作为HTTP参数发送。对于提示“机器人跳舞”，下面是网址的样子:</p><blockquote class="oe"><p id="feee" class="of og iq bd oh oi oj ok ol om on la dk translated"><a class="ae lb" href="http://12.34.56.789:5000?prompt=robots%20%dancing" rel="noopener ugc nofollow" target="_blank"> http://12.34.56.789:5000？prompt=robots%20dancing </a></p></blockquote><p id="302d" class="pw-post-body-paragraph kf kg iq kh b ki oo jr kk kl op ju kn ko oq kq kr ks or ku kv kw os ky kz la ij bi translated">确保浏览器不会自动默认为<code class="fe nd ne nf ng b">https</code>(而不是<code class="fe nd ne nf ng b">http</code>，因为我们没有设置SSL证书)。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ot"><img src="../Images/63b13685e860953eb1f9a66e78722674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_A0ihSspNApcbzu14Xp4Xw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">从浏览器访问您的稳定扩散网络服务。(图片由作者提供)</p></figure><h1 id="be17" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">免责声明:该服务器尚未投入生产</h1><p id="4001" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">这个webserver没有做好生产使用的准备有很多原因，但最大的瓶颈是它的单个GPU设备。考虑到运行推理需要10GB的VRAM(而我们的GPU只有15 GB的内存)，gunicorn无法提供一个以上的工作人员。换句话说，服务器一次只能处理一个查询(解决这个问题需要15秒)。</p><p id="54ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于计算强度较低的任务，标准解决方案是“无服务器的容器化微服务”平台，如<a class="ae lb" href="https://cloud.google.com/run" rel="noopener ugc nofollow" target="_blank">谷歌云运行</a>(GCR)；AWS和Azure也有类似的产品。开发人员将他们的网络应用捆绑在<em class="ou">容器</em>(独立的计算环境，包含运行应用所需的所有依赖关系，如<a class="ae lb" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>)中，并将它们交给云。GCR将这些容器部署在实际的机器上，并根据需求(每秒的请求数)扩展部署；如果有必要，GCR可以为您的服务分配数万个CPU，从而使其高度可用。您不需要担心自己关闭服务器，或者在服务器死机时重启它们。这种计费模式对用户来说也很方便，用户最终要为每次使用付费(而不是永久保持固定数量的机器)。</p><p id="2f60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，截至2022年9月，谷歌云运行不支持GPU。鉴于GPU的购买和运营成本仍然相当高，谷歌仍然保护GPU的使用并不令人惊讶。人们只能假设GCR的自动缩放算法无法防止设备长时间闲置；虽然闲置的CPU不是大损失，但闲置GPU是更大的机会成本。此外，他们可能希望防止人们盲目扩大规模，并在月底面临巨额账单的情况。</p><p id="f379" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">顺便提一下，<a class="ae lb" href="https://cloud.google.com/anthos/run/docs/configuring/compute-power-gpu" rel="noopener ugc nofollow" target="_blank"> Google Cloud Run for Anthos </a>开始提供GPU，但这是一项面向需要多个云和内部环境之间互操作性的高级用户/高端客户的服务。它绝对不适合那些想要自带稳定扩散web服务器的ML发烧友。</p><h1 id="e74e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">运行自己的服务器的替代方案</h1><p id="49da" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">虽然研究通过谷歌云提供稳定传播的最佳方式很有趣，但这不一定是生成人工智能图像的最有效方式。根据您的需要，以下工作流程可能更合适:</p><ol class=""><li id="dcef" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated"><strong class="kh ir">对于非技术用户</strong>:前往<a class="ae lb" href="https://beta.dreamstudio.ai/dream" rel="noopener ugc nofollow" target="_blank"> Dreamstudio </a>，Stability.ai的官方服务，在那里你可以获得一些免费积分。</li><li id="7043" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><strong class="kh ir">对于只想玩玩</strong>的ML爱好者:使用<a class="ae lb" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>。借助免费层，您可以随时访问GPU。10美元/月，你可以升级到<a class="ae lb" href="https://colab.research.google.com/signup" rel="noopener ugc nofollow" target="_blank"> Colab Pro </a>，它承诺“更快的GPU和更多的内存”。</li><li id="b189" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><strong class="kh ir">对于寻求将稳定扩散嵌入其服务的开发者</strong>:从<strong class="kh ir"> </strong> <a class="ae lb" href="http://replicate.com" rel="noopener ugc nofollow" target="_blank">调用API复制</a>，费用为0.0023美元/秒。他们保证80%的呼叫在15秒内结束，因此一幅图像的第80个成本百分位数约为0.0345美元。<a class="ae lb" href="http://replicate.com" rel="noopener ugc nofollow" target="_blank"> Replicate </a>类似于更广为人知的<a class="ae lb" href="http://huggingface.co" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>，但是更侧重于计算机视觉而不是自然语言处理。目前，HuggingFace还没有为稳定扩散提供他们的标准“加速推理”API，但它很可能正在工作中。</li></ol></div></div>    
</body>
</html>