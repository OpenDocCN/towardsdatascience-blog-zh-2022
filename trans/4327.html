<html>
<head>
<title>CLIP: The Most Influential AI Model From OpenAI — And How To Use It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CLIP:open AI最有影响力的人工智能模型——以及如何使用它</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1#2022-09-26">https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1#2022-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3031" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解该模型如何工作——包括编码示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/17d30bb510515e807be49ecb2093c6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KhDFiIJa_6k9enaE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/es/@maximalfocus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Maximalfocus </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="128a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近的AI突破，<strong class="lb iu"><em class="lv">【DALLE】</em>【1】</strong><strong class="lb iu"><em class="lv">稳定扩散</em>【2】</strong>有什么共同点？</p><p id="478e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们都使用了<strong class="lb iu"> <em class="lv"> CLIP的</em>【3】</strong>架构的组件。因此，如果你想掌握这些模型是如何工作的，理解<strong class="lb iu">夹子</strong>是一个先决条件。</p><p id="13ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，<a class="ae ky" href="https://twitter.com/haltakov/status/1351271379103002632" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">夹</strong>已用于索引</a>上的照片。</p><p id="af9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是<strong class="lb iu"> CLIP </strong>是做什么的，为什么它是AI社区的一个里程碑？</p><p id="de4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们开始吧！</p><h1 id="2edb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">剪辑—概述</h1><p id="612f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">剪辑</strong>代表<strong class="lb iu"><em class="lv">C</em></strong><em class="lv">onstastive</em><strong class="lb iu"><em class="lv">L</em></strong><em class="lv">语言-</em><strong class="lb iu"><em class="lv">I</em></strong><em class="lv">法师</em> <strong class="lb iu"> <em class="lv"> P </em> </strong> <em class="lv">再训练:</em></p><blockquote class="mt"><p id="7b75" class="mu mv it bd mw mx my mz na nb nc lu dk translated">CLIP是一个开源、多模态、零镜头的模型。给定图像和文本描述，该模型可以预测与该图像最相关的文本描述，而无需针对特定任务进行优化。</p></blockquote><p id="3b04" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们来分解一下这个描述:</p><ul class=""><li id="8a32" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><strong class="lb iu">开源:</strong>模型由<strong class="lb iu"> OpenAI </strong>创建并开源。我们稍后将看到如何使用它的编程教程。</li><li id="9ae4" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">多模态:</strong> <em class="lv">多模态</em>架构利用多个领域来学习特定的任务。<strong class="lb iu">剪辑</strong>结合了<em class="lv">自然语言处理</em>和<em class="lv">计算机视觉</em>。</li><li id="ab25" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">零距离:</strong> <em class="lv">零距离学习</em>是一种对看不见的标签进行归纳的方法，没有经过专门的分类训练。例如，所有的<a class="ae ky" href="https://www.image-net.org/update-mar-11-2021.php" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>模型都被训练识别1000个特定的类。<strong class="lb iu">夹</strong>不受此限制。</li><li id="86f0" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">const astic Language:</strong>通过这种技术，<strong class="lb iu"> CLIP </strong>被训练理解相似的表征应该靠近潜在空间，而不相似的表征应该远离潜在空间。这将在后面的例子中变得更加清楚。</li></ul><h2 id="47e5" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">关于CLIP的有趣事实:</h2><ul class=""><li id="3a2b" class="ni nj it lb b lc mo lf mp li oi lm oj lq ok lu nn no np nq bi translated"><strong class="lb iu"> CLIP </strong>使用数量惊人的<strong class="lb iu"> 4亿个图文对进行训练。</strong>作为比较，ImageNet数据集包含120万张图像。</li><li id="b520" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">最终调好的<strong class="lb iu">剪辑</strong>模型在256 <strong class="lb iu"> V100 GPU </strong>上训练了两周。对于一个关于<strong class="lb iu"> AWS Sagemaker </strong>的按需培训，这至少要花费20万美元！</li><li id="073c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">该模型使用32，768幅图像的小批量进行训练。</li></ul><h2 id="74d4" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">夹住动作</h2><p id="0d79" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们直观地演示一下<strong class="lb iu">夹子</strong>的作用。我们稍后将更详细地展示一个编码示例。</p><p id="f5f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们从Unsplash中选择一个自由图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38f571dbac88cf31063f740bb9aeaa50.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*r9SKmy1fQE6r2WrWQ-c03g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@anamnesis33" rel="noopener ugc nofollow" target="_blank">андрейкурган</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a213" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们为CLIP提供以下提示:</p><ul class=""><li id="5bb7" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">“一个戴着无沿帽的女孩”。</li><li id="3e14" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">一个戴着帽子的女孩。</li><li id="363a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">一个戴着无沿帽的男孩。</li><li id="4cdf" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">一个骑自行车的女孩。</li><li id="2fd5" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">‘一条狗’。</strong></li></ul><p id="f026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，第一个描述更好地描述了图像。</p><p id="45cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> CLIP </strong>通过分配一个标准化概率，自动找到哪个文本提示最恰当地描述了图像。我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3ab6ae90b9aaa9dce342ae2d02886549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*EqqumwYGzFmvOGnHhqajIQ.png"/></div></figure><p id="bd18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型<strong class="lb iu">成功</strong>定位最合适的图像描述。</p><p id="ab00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有<strong class="lb iu">，CLIP </strong>可以准确的识别出它从未见过的类和对象。</p><p id="7795" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有一个大的图像数据集，并且您想要将这些图像标记到特定的类/类别/描述中，<strong class="lb iu"> CLIP </strong>会自动为您完成这项工作！</p><p id="a67d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将展示<strong class="lb iu">夹子</strong>是如何工作的。</p><h1 id="0edd" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">剪辑架构</h1><p id="b632" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"> CLIP </strong>是一个深度学习模型，它使用了来自其他成功架构的新颖想法，并引入了自己的一些想法。</p><p id="989c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从第一部分开始，<em class="lv">对比预训练:</em></p><h2 id="a956" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">对比预训练</h2><p id="b55d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">图1 </strong>显示了<em class="lv">对比预培训</em>流程的概述。</p><p id="a59e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一批<code class="fe on oo op oq b">N</code>图像与它们各自的描述配对，例如<code class="fe on oo op oq b">&lt;image1, text1&gt;</code>、<code class="fe on oo op oq b">&lt;image2, text2&gt;</code>、<code class="fe on oo op oq b">&lt;imageN, textN&gt;</code>。</p><p id="a44f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对比预训练的目的是联合训练一个<em class="lv">图像</em>和一个<em class="lv">文本编码器</em>，产生图像嵌入[ <code class="fe on oo op oq b">I1</code>、<code class="fe on oo op oq b">I2</code>……<code class="fe on oo op oq b">IN</code>和文本嵌入[ <code class="fe on oo op oq b">T1</code>、<code class="fe on oo op oq b">T2</code>……<code class="fe on oo op oq b">TN</code>]，其方式是:</p><ul class=""><li id="c4fe" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">正确的<image-text>嵌入对<code class="fe on oo op oq b">&lt;I1,T1&gt;</code>、<code class="fe on oo op oq b">&lt;I2,T2&gt;</code>(其中<code class="fe on oo op oq b">i=j</code>)的余弦相似度被最大化。</image-text></li><li id="4a56" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">以对比的方式</strong>，相异对<code class="fe on oo op oq b">&lt;I1,T2&gt;</code>、<code class="fe on oo op oq b">&lt;I1,T3&gt;</code> … <code class="fe on oo op oq b">&lt;Ii,Tj&gt;</code>(其中<code class="fe on oo op oq b">i≠j</code>)的余弦相似度被最小化。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/e7485c64f338c3c380a47c798a58d371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ag6qUFmmXAr4E410Ll-eSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图1:<strong class="bd os">夹</strong> ( <a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)的</strong>对比预训练步骤</p></figure><p id="77f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们一步一步来看看会发生什么:</p><ol class=""><li id="4b67" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu ot no np nq bi translated">模型接收一批<code class="fe on oo op oq b">N</code> &lt;图文&gt;对。</li><li id="e848" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated"><em class="lv">文本编码器</em>是标准的<em class="lv">变压器</em>型号，带有<strong class="lb iu"> GPT2样式的修改【4】。</strong><em class="lv">图像编码器</em>可以是<em class="lv"> ResNet </em>或<strong class="lb iu">视觉转换器【5】</strong>。</li><li id="9cec" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">对于批次中的每幅图像，<em class="lv">图像编码器</em> <strong class="lb iu"> </strong>会计算一个图像矢量。第一幅图像对应于<code class="fe on oo op oq b">I1</code>矢量，第二幅图像对应于<code class="fe on oo op oq b">I2</code>，依此类推。每个向量的大小为<code class="fe on oo op oq b">de</code>，其中<code class="fe on oo op oq b">de</code>是潜在维度的大小。因此，该步骤的输出是<code class="fe on oo op oq b">N X de</code>矩阵。</li><li id="0d6d" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">类似地，文本描述被压缩成文本嵌入[ <code class="fe on oo op oq b">T1</code>，<code class="fe on oo op oq b">T2</code> … <code class="fe on oo op oq b">TN</code> ]，产生一个<code class="fe on oo op oq b">N X de</code>矩阵。</li><li id="b595" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">最后，我们将这些矩阵相乘，并计算每个图像和文本描述之间的成对余弦相似度。这产生了一个<code class="fe on oo op oq b">N X N</code>矩阵，如图<strong class="lb iu">图1 </strong>所示。</li><li id="0c21" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">目标是最大化对角线上的余弦相似性——这些是正确的<image-text>对。以对比的方式，非对角线元素的相似性应该最小化(例如，<code class="fe on oo op oq b">I1</code>图像由<code class="fe on oo op oq b">T1</code>描述，而不是由<code class="fe on oo op oq b">T2</code>、<code class="fe on oo op oq b">T2</code>、<code class="fe on oo op oq b">T3</code>等描述)。</image-text></li></ol><p id="2b1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一些额外的备注:</strong></p><ul class=""><li id="d504" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">该模型使用<strong class="lb iu">对称交叉熵损失</strong>作为其优化目标。这种类型的损失最小化了图像到文本的方向以及文本到图像的方向(记住，我们的对比损失矩阵保持了<code class="fe on oo op oq b">&lt;I1,T2&gt;</code>和<code class="fe on oo op oq b">&lt;I2,T1&gt;</code>余弦的相似性)。</li><li id="7d54" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">对比预培训并不是全新的。在之前的车型中引入，并由<strong class="lb iu">CLIP【6】</strong>进行适配。</li></ul><h2 id="5bae" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">零射击分类</h2><p id="2fa6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们现在已经预训练了我们的<em class="lv">图像</em>和<em class="lv">文本编码器</em>，我们已经为<em class="lv">零镜头分类</em>做好了准备。</p><p id="1b98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基线<br/> </strong>首先，我们来提供一些背景。前变形金刚时代是如何实现少镜头分类的？</p><p id="7d13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很简单<strong class="lb iu">【7】</strong>:</p><ul class=""><li id="860c" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">下载一个高性能预训练的<em class="lv"> CNN </em>比如<em class="lv"> ResNet </em>，用它进行特征提取，得到图像特征。</li><li id="a2be" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">然后，使用这些特征作为标准分类器的输入(例如<em class="lv">逻辑回归</em>)。分类器以<strong class="lb iu">监督方式训练，</strong>图像标签作为目标变量(<strong class="lb iu">图2 </strong>)。</li><li id="b03c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">如果你选择<strong class="lb iu"><em class="lv">K-shot</em></strong><em class="lv"/><strong class="lb iu"><em class="lv">学习，</em> </strong>你在分类阶段的训练集应该只包含每个类的<strong class="lb iu"> K </strong>实例。</li><li id="da2b" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">当<code class="fe on oo op oq b"><strong class="lb iu">K</strong>&lt;10</code>时，该任务被称为<strong class="lb iu">少镜头分类</strong>学习。相应地，对于<code class="fe on oo op oq b"><strong class="lb iu">K</strong>=1</code>，我们有<strong class="lb iu">一次性分类</strong>学习。如果我们使用所有可用的数据，这是一个完全监督的模型(老式的方法)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/5567ec402a71521c614913ba9455c95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfzIKlem9FNTP1f8Hhb5YQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图2: </strong>具有特征提取的图像分类(<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"> I </a>作者的图像)</p></figure><p id="6a14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意上面的关键字<strong class="lb iu">‘受监督的’</strong>——分类器应该事先知道类别标签。使用与分类器配对的图像提取器也被称为<strong class="lb iu">线性探头评估。</strong></p><p id="6691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> CLIP的竞争优势<br/>T13<strong class="lb iu">CLIP</strong>如何进行零次分类的过程如图<strong class="lb iu">图3: </strong></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/aec2e3e96ae77910d0ade07ef5ff76b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*oOYgcW9XgA4iUWQKOI6O1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图三:</strong>零拍分类使用<strong class="bd os">剪辑</strong> ( <a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="4c51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，这个过程很简单:</p><p id="132f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们提供一组文本描述，如<code class="fe on oo op oq b">a photo of a dog</code>或<code class="fe on oo op oq b">a cat eating an ice-cream</code>(我们认为最能描述一个或多个图像的任何内容)。这些文本描述被编码到文本嵌入中。</p><p id="b4b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们对图像做同样的事情——图像被编码成图像嵌入。</p><p id="c7d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<strong class="lb iu"> CLIP </strong>计算图像和文本嵌入之间的成对余弦相似度。具有最高相似度的文本提示被选为预测。</p><p id="357a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们可以输入多个图像。<strong class="lb iu"> CLIP </strong>巧妙的缓存了输入的文本嵌入，这样就不需要为其余的输入图像重新计算了。</p><p id="7f80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！我们现在已经总结了<strong class="lb iu">夹子</strong>是如何端到端工作的。</p><h1 id="366e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">寻找数据的问题</h1><p id="6f58" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"> CLIP </strong>使用30个公共数据集进行预训练。用大量数据拟合大型语言模型是很重要的。</p><p id="0131" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，很难找到具有成对图像-文本描述的健壮数据集。大多数公共数据集，比如<em class="lv"> CIFAR </em>，都是只有一个单词标签的图像——这些标签就是目标类。但是<strong class="lb iu">剪辑</strong>被创建来使用完整的文本描述。</p><p id="b510" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这种差异，作者没有排除这些数据集。相反，他们进行了一些特征工程:单个单词标签，如<code class="fe on oo op oq b">bird</code>或<code class="fe on oo op oq b">car</code>被转换成句子:<code class="fe on oo op oq b">a photo of a dog</code>或<code class="fe on oo op oq b">a photo of bird</code>。在<em class="lv">牛津-IIIT Pets </em>数据集<em class="lv">，</em>上，作者使用了提示:<code class="fe on oo op oq b">A photo of a {label}, a type of pet</code>。</p><p id="5a21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关预训练技术的更多信息，请查看原始论文[3]。</p><h1 id="1cc4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">人工智能中剪辑的影响</h1><p id="cf9e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最初，我们声称<strong class="lb iu">剪辑</strong>是人工智能社区的一个里程碑。</p><p id="c3db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看为什么:</p><h2 id="362d" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">1.作为零触发分级机的卓越性能</h2><p id="66b3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu"> CLIP </strong>是一个零镜头分类器，所以首先针对少量镜头学习模型测试<strong class="lb iu"> CLIP </strong>是有意义的。</p><p id="d2d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，作者针对由高质量预训练模型之上的线性分类器组成的模型测试了<strong class="lb iu">剪辑</strong>，例如<em class="lv"> ResNet </em>。</p><p id="2efb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如<strong class="lb iu">图4所示:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/0e3005de029e6f25eff14223406cc982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*EfilBU9_JgufVyWpBWR8IA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图4: </strong>根据少数镜头分类，CLIP相对于其他型号的性能(<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="e6df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> CLIP </strong>明显优于其他分类器。</p><p id="50df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，<strong class="lb iu"> CLIP </strong>能够匹配16-shot线性分类器<em class="lv"> BiT-M </em>的性能。换句话说，<em class="lv"> BiT-M的</em>分类器必须在每类至少16个样本的数据集上进行训练，以匹配<strong class="lb iu">剪辑的</strong>分数，而<strong class="lb iu">剪辑</strong>无需微调即可获得相同的分数。</p><p id="9c77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，作者将<strong class="lb iu"> CLIP </strong>评估为线性探针:他们仅使用<strong class="lb iu"> CLIP的</strong> <em class="lv">图像编码器</em>来获取图像特征，并将其输入线性分类器——就像其他模型一样。即使在这种设置下，<strong class="lb iu"> CLIP的</strong>少数镜头学习功能也是非常出色的。</p><h2 id="8829" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">2.对分布变化的无与伦比的鲁棒性</h2><p id="c3d9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">分配移位</strong>是一件大事，尤其是对于生产中的机器学习系统。</p><p id="12e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:你可能知道<em class="lv">分配移位</em>为<em class="lv">概念漂移</em>，虽然从技术上讲它们是不一样的。</strong></p><blockquote class="ox oy oz"><p id="4562" class="kz la lv lb b lc ld ju le lf lg jx lh pa lj lk ll pb ln lo lp pc lr ls lt lu im bi translated">分布偏移是当模型的训练数据随时间变化时发生的现象。因此，久而久之，模型的效率下降，预测变得不那么准确。</p></blockquote><p id="e242" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，分配转移并不是意料之外的事情——它将会发生。问题是，如何及早发现这种现象，以及需要采取什么措施来“重新校准”您的模型？这个不好解决，取决于很多因素。</p><p id="560f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，关于人工智能的新研究正朝着创造能适应分布变化的模型的方向发展。</p><p id="d2bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是作者测试<strong class="lb iu">剪辑的</strong>鲁棒性的原因。结果显示在<strong class="lb iu">图5: </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/226a10e96f2be52532dc4f1bdf0085d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*FwBGDyo_VXaH5p0VWvnqZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图5: </strong>根据分布偏移，CLIP对ResNet的性能(<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="10ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于<strong class="lb iu">夹子</strong>有两件事非常重要:</p><ol class=""><li id="2af7" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu ot no np nq bi translated"><strong class="lb iu"> CLIP </strong>在ImageNet上实现了与SOTA <em class="lv"> ResNet </em>模型相同的精度，尽管<strong class="lb iu"> CLIP </strong>是零镜头模型。</li><li id="f61e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">除了最初的<em class="lv"> ImageNet </em>之外，我们还有类似的数据集作为分布转移基准。似乎ResNet<em class="lv">正在与这些数据集作斗争。然而，<strong class="lb iu"> CLIP </strong>可以很好地处理未知图像——事实上，该模型在<em class="lv"> ImageNet </em>的所有变化中保持了相同的精确度！</em></li></ol><h2 id="ff35" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">3.计算效率</h2><p id="872e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在<em class="lv"> GPT-2 </em>之前，计算效率是理所当然的(某种程度上)。</p><p id="80b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如今，在一个模型需要用数百个8k美元的GPU训练数周的时代，计算效率问题得到了更认真的解决。</p><p id="a9cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CLIP是一个更加计算友好的架构。这一成功的部分原因是因为<strong class="lb iu"> CLIP </strong>使用了一个<em class="lv">视觉转换器</em>作为默认的<em class="lv">图像编码器</em>组件。结果如<strong class="lb iu">图6所示:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/bde1d94e1f3dc50017eab1d912744b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lWX00RxicR0mz3UfsgHqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图6: </strong>各种模型每幅图像的浮点运算次数(<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="fba4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，<strong class="lb iu"> CLIP </strong>与其他型号相比，能够更好地利用硬件资源。这也意味着在培训云服务(如<strong class="lb iu"> AWS Sagemaker </strong>)时可以节省额外的资金。此外，<strong class="lb iu">图6 </strong>显示<strong class="lb iu"> </strong>与其他型号相比，<strong class="lb iu">夹子</strong>在<strong class="lb iu">硬件操作</strong>与<strong class="lb iu">精度分数</strong>方面提供了更好的可扩展性。</p><p id="3f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有<strong class="lb iu">数据效率</strong>的问题。作者表明<strong class="lb iu">剪辑</strong>在零镜头设置中比类似模型更具数据效率。但是，它们没有解决<strong class="lb iu">片段在预训练阶段的</strong>数据效率问题。然而，在这方面可能没有太多事情可做，因为<strong class="lb iu"> CLIP </strong>使用两种类型的变压器——变压器本质上是数据密集型模型。</p><h2 id="9450" class="nw lx it bd ly nx ny dn mc nz oa dp mg li ob oc mi lm od oe mk lq of og mm oh bi translated">4.研究兴趣增加</h2><p id="6f7d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">CLIP的成功引发了对文本到图像模型的兴趣，并推广了<strong class="lb iu">对比预训练</strong>方法。</p><p id="6ad2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了<strong class="lb iu"> DALLE </strong>和<strong class="lb iu">稳定扩散，</strong>我们可以用<strong class="lb iu"> CLIP </strong>作为<em class="lv"> GANs </em>  <em class="lv">中的<a class="ae ky" href="https://twitter.com/sayantandas_/status/1351830997403254785" rel="noopener ugc nofollow" target="_blank"> <em class="lv">鉴别器</em>。</a></em></p><p id="8e8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，<strong class="lb iu"> CLIP </strong>的发布激发了类似的基于CLIP的出版物，这些出版物扩展了模型的功能，例如<strong class="lb iu"><em class="lv">dense CLIP</em>【8】</strong>和<strong class="lb iu"><em class="lv">CoCoOp</em>【9】<em class="lv">。</em>T25】</strong></p><p id="8154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，微软发布了<strong class="lb iu"><em class="lv">X-CLIP</em>【10】<em class="lv"/></strong>用于视频语言理解的<strong class="lb iu"> CLIP </strong>的最小扩展。</p><blockquote class="ox oy oz"><p id="0aae" class="kz la lv lb b lc ld ju le lf lg jx lh pa lj lk ll pb ln lo lp pc lr ls lt lu im bi translated"><strong class="lb iu">奖励信息:</strong>一款类似于猜字谜的app，名为<a class="ae ky" href="https://paint.wtf/" rel="noopener ugc nofollow" target="_blank"> paint.wtf </a>，使用<strong class="lb iu"> CLIP </strong>对你的画进行排名。试试吧——超级好玩！</p></blockquote><h1 id="5e26" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">如何使用剪辑编码示例</h1><p id="ea8d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">接下来，我们将展示如何使用<strong class="lb iu"> HugginFaces库来使用<strong class="lb iu">剪辑</strong>。</strong></p><p id="5260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们从Unsplash中选择3张图片。我们之前使用了第一个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38f571dbac88cf31063f740bb9aeaa50.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*r9SKmy1fQE6r2WrWQ-c03g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@anamnesis33" rel="noopener ugc nofollow" target="_blank">андрейкурган</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/602733b718de012a7df93d060256ff10.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*ds698oc72b482M0eEQc-2Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">理查德·布鲁约在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/27ec6269fd0c00d1acb958e5d1874c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*n_uTb8HZhtORFFne5vRVLg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奥斯卡·萨顿在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="fde9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用以下库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="549d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们加载<strong class="lb iu">剪辑</strong>模型的权重，tokenizer图像处理器:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="c30a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们用Python加载了上面的Unsplash图像:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="0d64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们为<strong class="lb iu">片段</strong>提供一些文本提示。</p><p id="849b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目标是让<strong class="lb iu"> CLIP </strong>将3个不清晰的图像分类成特定的文本描述。请注意，其中一个是误导性的，让我们看看是否可以混淆模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pg ph l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="66ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型成功地将所有3幅图像分类！</p><p id="927f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意两件事:</p><ol class=""><li id="e069" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu ot no np nq bi translated"><strong class="lb iu"> CLIP </strong>可以理解每个图像中的多个实体及其动作。</li><li id="4d8d" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated"><strong class="lb iu"> CLIP </strong>为每幅图像分配最具体的描述。例如，我们可以将第二幅图像描述为<code class="fe on oo op oq b">‘a dog’</code>和<code class="fe on oo op oq b">‘a dog at the beach’</code>。然而，该模型正确地判定<code class="fe on oo op oq b">‘a dog’</code>短语更好地描述了第二幅图像，因为没有海滩。</li></ol><p id="409c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请随意使用这个例子。完整的例子是<a class="ae ky" href="https://jovian.ai/nkafr/clip" rel="noopener ugc nofollow" target="_blank">这里的</a>。<br/>使用带有文字描述的图片，了解<strong class="lb iu">剪辑</strong>的工作原理。</p><h1 id="51bb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">局限性和未来工作</h1><p id="2e9a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然<strong class="lb iu">夹子</strong>是革命性的型号，但仍有改进的空间。作者指出了有进一步发展潜力的领域。</p><ul class=""><li id="3867" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><strong class="lb iu">准确度得分:CLIP </strong>是一个最先进的零射击分类器，直接挑战特定任务的训练模型。事实上<strong class="lb iu">剪辑</strong>与<em class="lv"> ImageNet </em>上完全监督的<em class="lv"> ResNet101 </em>的精确度相当惊人。然而，仍然有监督模型获得更高的分数。作者强调，<strong class="lb iu"> CLIP </strong>鉴于其惊人的可扩展性，可能会获得更高的分数，但这将需要天文数字的计算机资源。</li><li id="aee5" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">多义性:</strong>作者陈述<strong class="lb iu">片段</strong>患有多义性。有时，由于缺乏上下文，模型无法区分某些单词的含义。请记住，我们之前提到过，一些图像只标记了类别标签，而没有全文提示。作者提供了一个例子:在牛津-IIIT宠物数据集中，<code class="fe on oo op oq b">‘boxer’</code>这个词指的是一个狗品种，但是其他图片认为<code class="fe on oo op oq b">‘boxer’</code>是一个运动员。这里，罪魁祸首是数据的质量，而不是模型本身。</li><li id="af33" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">特定任务学习:</strong>虽然<strong class="lb iu"> CLIP </strong>可以区分复杂的图像模式，但该模型在一些琐碎的任务中会失败。例如，模型与手写数字识别任务进行斗争(<strong class="lb iu">图7 </strong>)。作者将这种错误分类归因于训练数据集中缺少手写数字。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/6698b8bb59d3b9fe2d1b40cc2f5b0396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*-bCQAe0IzEvgTtqaKRyqlQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd os">图7: </strong>数字分类(<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="3a7c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结束语</h1><p id="6b4c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">毫无疑问，CLIP是AI社区的一个重要模型。</p><p id="406f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，<strong class="lb iu"> CLIP </strong>为革新人工智能研究的新一代文本到图像模型铺平了道路。当然，不要忘记这个模型是开源的。</p><p id="1e84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后但同样重要的是，还有很大的改进空间。在整篇论文中，作者暗示许多<strong class="lb iu">剪辑的</strong>限制是由于低质量的训练数据。</p><h1 id="65bc" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">感谢您的阅读！</h1><p id="5a99" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我每个月写一篇关于AI的有影响力的论文的深度分析。<br/> <strong class="lb iu">保持连接！</strong></p><ul class=""><li id="678f" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">订阅我的<a class="ae ky" href="https://towardsdatascience.com/subscribe/@nikoskafritsas" rel="noopener" target="_blank">简讯</a>！</li><li id="30e4" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">在Linkedin上关注我！</li><li id="dd3c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" href="/@nikoskafritsas/membership" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">加入介质</strong> </a> <strong class="lb iu">！</strong>(附属链接)</li></ul><h1 id="98e9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><ol class=""><li id="3b2d" class="ni nj it lb b lc mo lf mp li oi lm oj lq ok lu ot no np nq bi translated">Aditya Ramesh等人<a class="ae ky" href="https://arxiv.org/pdf/2204.06125.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">分层文本条件图像生成与剪辑潜伏</em></a><em class="lv">(2022年4月)</em></li><li id="e323" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">罗宾·龙巴赫等人<a class="ae ky" href="https://arxiv.org/pdf/2112.10752.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">用潜在扩散模型合成高分辨率图像</em></a><em class="lv">(2022年4月)</em></li><li id="ecf9" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">亚历克·拉德福德等<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">从自然语言监督中学习可转移的视觉模型</em></a><em class="lv">(2021年2月)</em></li><li id="3d3e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">亚历克·拉德福德等人<a class="ae ky" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">语言模型是无监督的多任务学习器</em> </a> ( <em class="lv"> 2019 </em>)</li><li id="71fe" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">Dosovitskiy等人<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> <em class="lv">一幅图像抵得上16x16个字:变形金刚在尺度上的图像识别</em> </a> <em class="lv"> (2020) </em></li><li id="00c5" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">张宇豪等<a class="ae ky" href="https://arxiv.org/pdf/2010.00747.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a>【2020】从成对的图像和文本中对比学习医学视觉表征</li><li id="eefa" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">田，杨等<a class="ae ky" href="https://arxiv.org/pdf/2003.11539.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">反思少镜头图像分类:一个好的嵌入就够了？</em></a><em class="lv">【2020】</em></li><li id="a492" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">饶永明等<em class="lv"> </em> <a class="ae ky" href="https://arxiv.org/abs/2112.01518" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> DenseCLIP:带上下文感知提示的语言引导密集预测</em> </a></li><li id="5a94" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">周开阳等<em class="lv"> </em> <a class="ae ky" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">视觉语言模型的条件提示学习</em></a>(<em class="lv">2022年3月)</em></li><li id="d7b7" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu ot no np nq bi translated">倪博林等<a class="ae ky" href="https://arxiv.org/abs/2208.02816" rel="noopener ugc nofollow" target="_blank"> <em class="lv">扩展通用视频识别的语言-图像预处理模型</em></a><em class="lv"/>(2022年8月)</li></ol></div></div>    
</body>
</html>