<html>
<head>
<title>Audio Augmentations in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 中的音频增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/audio-augmentations-in-tensorflow-48483260b169#2022-02-25">https://towardsdatascience.com/audio-augmentations-in-tensorflow-48483260b169#2022-02-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cedc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直接加强和向前传球</h2></div><p id="2251" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于基于图像的任务，研究人员和从业人员通常都依赖于增强。这些是(人工)数据转换，如旋转、模糊或调整大小。与其他数据类型的修改相比，图像增强可以很快被理解。通常，一瞥向我们展示了一个特定的图像是如何转变的。虽然增强在图像领域很常见，但它们对其他数据类型也很有用，如音频。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/71dac1776bd913c03ba011cd7da5f2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Rno6E_NT6AO6hnEc"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">理查德·霍瓦特在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="96d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae lu" rel="noopener" target="_blank" href="/visualizing-audio-pipelines-with-streamlit-96525781b5d9">早先的一篇博文</a>中，我描述了一个简单的 GUI 来在浏览器中可视化这种增强。它还在运行，你可以在这里现场试用。在这篇文章中，我继续我早期的工作，并描述了如何在 TensorFlow 中将增强应用到数据集的两种方法。第一种方式直接修改数据；第二种方式是在网络的正向传递过程中实现的。</p><h1 id="2445" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">直接音频增强</h1><p id="f822" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在第一个场景中，我们从生成人工音频数据集开始。这个过程是直截了当的，并尽可能保持简单，所以你可以很容易地跟随。我们不加载预先存在的数据集，而是根据需要多次重复 librosa 库中的一个样本:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="a36f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可能已经注意到，在这个过程中，我们已经创建了一个 Dataset 对象。为了方便，我选择这样做；但是我们也可以使用纯 NumPy 数组。无论如何，音频数据现在与其采样率一起存储。如果您正在寻找其他方法来创建这样的数据集对象，您可以使用本<a class="ae lu" rel="noopener" target="_blank" href="/a-practical-guide-to-tfrecords-584536bc786c">实践指南</a>中给出的方法之一。</p><p id="2605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们的小数据集已经可以使用了，我们可以开始应用扩充了。对于这一步，我们使用了<a class="ae lu" href="https://github.com/iver56/audiomentations" rel="noopener ugc nofollow" target="_blank">一个<em class="mu">GUI 转换</em>库</a>(顺便说一下，这是为提到的 GUI 生成转换的库)。</p><p id="54ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们只导入三个修改，特别是 PitchShift、Shift 和 ApplyGaussianNoise。前两个移位音高(Pitchshift)和数据(Shift，可以认为是滚动数据；例如，狗的叫声将偏移+ 5 秒)。最后的变换使信号更加嘈杂，增加了神经网络以后的挑战。接下来，我们将所有三个扩展合并到一个管道中:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="7975" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在通过管道输入数据之前，我们必须编写一些额外的代码。我们需要这样做，因为我们正在使用一个 Dataset 对象，简单地说，它在应用占位符之前通过一个函数输入占位符(这仅在实际数据加载期间完成！).</p><p id="8638" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，这个额外的代码告诉 TensorFlow 将张量临时转换为 NumPy 数组，这被管道接受:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="2b93" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了这些辅助函数，我们现在可以扩充数据集了。此外，我们扩展了数据的维度，在末尾添加了一个人工轴。这将单个音频样本从(num_data_point，)变为(num_data_points，1)，表示我们有单声道音频。这是必要的，在这里我们通过网络来馈送数据:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="8f9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一节到此为止。应用增强后，任何后续操作，无论是网络还是其他，都将看到增强的数据。</p><h1 id="4e6d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">向前传球时的音频增强</h1><p id="06a0" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">与第一种技术相比，增加网络内的音频数据会增加前向传递的计算负荷。</p><p id="1196" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将使用 kapre 库，它提供了自定义的 TensorFlow 图层。这些层中有 MelSpectrogram 层，它接受原始(即未修改的)音频数据，并在 GPU 上计算 Mel 缩放的频谱图<em class="mu">。</em></p><p id="4458" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然与数据扩充没有直接关系，但这有两个好处:</p><p id="1ac6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们可以在例如超参数搜索期间优化用于频谱图生成的参数，使得重复的音频-频谱图生成变得不必要。第二，转换直接在 GPU 上进行，因此速度更快，无论是在原始转换速度还是在设备上的存储器布局方面。</p><p id="4817" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到这些好处，我们可以在计算出光谱图后再进行扩充。尽管在撰写本文时可用转换的数量有限，但我们可以在概念上欺骗这个过程。</p><p id="1f87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们从加载音频特定层开始，这些层是由 kapre 库提供的。如上所述，这些层获取原始音频数据并计算声谱图表示:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="3950" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之后，我们从<a class="ae lu" href="https://pypi.org/project/spec-augment/" rel="noopener ugc nofollow" target="_blank"> <em class="mu">规范增强</em>包</a>中添加一个增强层。这个软件包实现了研究人员 Park 等人[1]的 SpecAugment 论文，它屏蔽了部分谱图。屏蔽模糊了神经网络所需的信息，从而增加了挑战。反过来，这种修改迫使网络关注其他功能，扩展其能力以概括看不见的数据:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="2050" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以在上面添加更多的层。对于我们的例子，我添加了一个未经训练的残差网络，用任意 10 个类将数据分类成:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ms mt l"/></div></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="87dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这部分到此为止。我们现在有一个深度神经网络，在向前传递的过程中增强音频数据。</p><h1 id="af53" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="239b" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在这篇博文中，我们讨论了两种增加音频数据的方法:第一种方法直接修改音频数据，第二种方法作为神经网络正向传递的一部分。这两个方法应该给你如何在你的用例中继续的好主意。为了让你快速入门，你可以在<a class="ae lu" href="https://github.com/phrasenmaeher/tf-audio-augmentations" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到完整的笔记本。</p><p id="5687" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] Park 等，<a class="ae lu" href="https://arxiv.org/abs/1904.08779" rel="noopener ugc nofollow" target="_blank"> Specaugment:一种用于自动语音识别的简单数据增强方法</a>，2019，Proc。Interspeech 2019</p></div></div>    
</body>
</html>