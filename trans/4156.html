<html>
<head>
<title>Understanding LSTMs &amp; GRUs — The Intuitive Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解LSTMs &amp; GRUs —直观的方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-approach-to-understading-of-lstms-and-grus-c2191611a37d#2022-09-14">https://towardsdatascience.com/an-intuitive-approach-to-understading-of-lstms-and-grus-c2191611a37d#2022-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1db8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们将继续证明LSTMs &amp; GRU比你想象的要容易</h2></div><p id="78eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然当你听到自然语言处理或序列模型时，RNNs可能是你脑海中首先闪过的东西，但该领域的大多数成功并不归功于它们，而是(在很长一段时间内)归功于解决其消失梯度问题的改进版本。即，LSTM(长短期记忆)网络或不太常见的GRU(门控循环单元)网络。</p><p id="ae17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/unriddling-rnns-with-depth-and-in-both-directions-9ed336c4b392">之前的故事</a>中，我们展示了rnn如何工作，以及为什么它们会受到消失梯度的影响，这限制了它们处理序列数据中长相关性的能力。我们还暗示，这样的问题可以通过<strong class="kh ir">设计一个更复杂的循环层来解决。</strong>这正是LSTM/GRU网络所做的事情！它用一个更复杂的版本取代了标准的递归层，该版本可以用数学方法表示，以克服消失梯度问题；从而解决了rnn的长期依赖性问题，提高了rnn的整体性能。</p><h2 id="788c" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">但是什么是轮回层呢？</h2><p id="d066" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">正如我们在前面的故事中所描述的，这是一个</p><p id="0fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-接收前一层的激活并乘以一个权重矩阵(<strong class="kh ir"> W </strong> ₕ <strong class="kh ir"> <em class="ma"> ₓ </em> </strong>)，就像FFNNs中使用的密集层一样</p><p id="0fc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-获取其自身由于序列中的先前输入而产生的<strong class="kh ir">激活，然后将其乘以权重矩阵(<strong class="kh ir"> W </strong> ₕₕ).对于序列中的第一个字，激活向量通常被初始化为零，并且通常被称为“隐藏状态”。</strong></p><p id="7ae0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3-将偏差添加到前面两个结果中</p><p id="599f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4-对其应用激活函数并提供结果(hₜ)as输入到其他层，就像ffnns中使用的密集层一样</p><p id="eec9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这在数学上等同于写作</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/66d0128f024af056e2ecef682393091c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHLZ6-JjWTPpxxi5B7FLrg.png"/></div></div></figure><p id="c997" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">递归层使用的激活函数<strong class="kh ir"> <em class="ma"> f </em> </strong>一般是<strong class="kh ir"><em class="ma">【tanh(x)</em></strong>。它可能比ReLU(和Sigmoid)具有更好的收敛特性，如这里的<a class="ae lb" href="https://qr.ae/pvOLbF" rel="noopener ugc nofollow" target="_blank">和</a>。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/9e907dd1a4e4262e3a6b9b2412a7e4b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0amp8r2hV6nxwsyIXBIng.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae lb" href="https://unsplash.com/@tianshu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">天舒刘</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="05d6" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">“门”的概念</h2><p id="186d" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我们之前已经讨论过，LSTM或GRU网络改进RNNs的方式是用LSTM或GRU层代替标准的循环层；这两种方法都是经过精心设计的，用来处理渐变消失的问题。但是在我们揭示每一层的结构之前，我们需要理解“门”的概念，它在两种类型的层中都被用作构建块。</p><p id="71c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数学上，我们可以把门表示为</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/ac9e658a77c58df12f3695728911cd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBfnqlBfVSJnIVwHcYgD2g.png"/></div></div></figure><p id="7f11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着它只是一个循环层，但有两个根本的区别</p><p id="195a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-我们不需要跟踪它的状态；它将始终使用其所在的LSTM/GRU图层的先前隐藏状态。这就是为什么我们稍后会删除<em class="ma"> g </em>中的下标。</p><p id="d204" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-它使用乙状结肠激活。</p><p id="d015" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，它使用Sigmoid是什么使它的功能作为一个门；<em class="ma"> gₜ </em>的值将总是从0到1，因此当它按元素乘以另一个向量比如<em class="ma"> v </em>时，就好像<em class="ma"> v*gₜ </em>是<em class="ma"> v </em>经过门<em class="ma"> gₜ </em>的结果；例如，如果<em class="ma"> gₜ </em>全为1，则<em class="ma"> v </em>将完全通过，如果全为0，则<em class="ma"> gₜ </em>不会通过，并且如果需要，根据<em class="ma"> gₜ </em>的值，<em class="ma"> v </em>的一些元素可能通过，而其他一些元素不会通过，程度不同。</p><p id="1a94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后要注意的是，从现在开始，一个门将用<strong class="kh ir">γ</strong>表示，无论何时使用多个门，它们都是相互独立的；这意味着每一个都与其自身的权重和偏好相关联。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/b6a4237486c26358a82a9f1b29ba514a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHplu0s9ZRbGvROt4NjEtQ.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><a class="ae lb" href="https://unsplash.com/@tomcchen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">汤姆陈</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="1dc6" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">GRU层(门控循环单元)</h2><p id="1bf7" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">GRU网络用GRU层代替RNNs中的循环层。GRU层使用两个门，一个称为关联门<strong class="kh ir">(γᵣ)</strong>，另一个称为更新门<strong class="kh ir">(γᵤ)</strong>。</p><p id="77de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦计算出两个门，GRU层以下面的方式产生它的隐藏状态。</p><p id="267d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-通过递归层计算一个<strong class="kh ir">候选隐藏状态</strong> ( <strong class="kh ir"> ĥ </strong>)，该递归层在通过关联门<strong class="kh ir">(γᵣ)</strong>后使用该层的先前隐藏状态。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/0dd3cdbdee34730d0f96983be5e1a69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlDKx_x9T_86oMTX8SavcA.png"/></div></div></figure><p id="299a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-使用<strong class="kh ir">更新门(γᵤ)</strong>计算GRU层的隐藏状态，如下所示</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/657c0ea5d1974bb0a94bd708ba03ba37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zr0XeraUK3gUhd3Ua-VoMQ.png"/></div></div></figure><p id="2a7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了获得一些直觉，我们先假设没有关联门(设置为全1)。在这种情况下，每次迭代，更新门决定GRU层是否将作为标准递归层(基于输入学习新的隐藏状态)或者跳过学习新的隐藏状态并保留在前一次迭代中所学习的。你可以通过代入上式，确认前者成立是<strong class="kh ir">γᵤ</strong>全1，后者成立是<strong class="kh ir">γᵤ</strong>全0。</p><p id="ce32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着，如果我们有一个具有长期相关性的序列，例如“我今天早上看到的<strong class="kh ir">猫</strong><strong class="kh ir">非常饥饿】,那么网络有一种方法来保留它已经学习了“猫”,方法是从该点开始将更新门设置为低，这样当它必须在是/是之间做出决定时，它会记住它。当然，在现实中，更新门采用连续的值(它不必100%保留或100%更新它所学习的内容),你可以认为它只是简单地决定我们应该在多大程度上保留旧信息，以及我们应该在多大程度上用新信息替换旧信息。</strong></p><p id="db85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关联门有一个更简单的角色，它回答“在生成候选状态时，应该在多大程度上依赖于最后一个隐藏状态？我们应该强烈依赖它吗？或者，即使更新门决定它需要涉及当前令牌的新信息，那么先前的状态是不重要的，并且我们可以假装当前令牌是序列中的第一个(即，通过将<strong class="kh ir">γᵣ</strong>设置为0来重置)，这是真的吗？”</p><p id="5b76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，值得强调的是，虽然人们可以很容易地从方程中看出，门有助于GRU改善RNN的短期记忆，但网络必须自己找出答案。有了足够的训练数据和最小化损失的目标，神经网络将有希望能够学习如何设置权重，以便在给定输入序列的情况下，使用门，例如通过改善短期记忆问题来改善网络的性能。在我们进行修改之前，RNN没有办法通过解决这个问题来提高其性能。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/3e48ab7ef5d2e54da31eed79cd824714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zwlc-07lDfq404A929T7vw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">德里克·汤姆森在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="b130" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">LSTM层(长短期记忆)</h2><p id="cdc4" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">您可以将LSTM图层视为GRU图层的更复杂版本。主要区别在于</p><p id="cd79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-除了隐藏状态之外，它还有一个“单元格状态”</p><p id="f78f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-它放弃了相关性门，而是使用一个<strong class="kh ir">输出门</strong>用于类似的目的。</p><p id="bc26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3-除了<strong class="kh ir">更新门</strong>之外，它还有一个<strong class="kh ir">遗忘门</strong>。</p><p id="44d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦<strong class="kh ir">更新，</strong> <strong class="kh ir">忘记</strong>和<strong class="kh ir">输出</strong>门被计算，LSTM以如下方式进行</p><p id="11b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1-通过使用层的先前隐藏状态的递归层来计算候选单元状态</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/b2e90b277ae4d52c18c6d1f60fb46717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjyUGSFA7K4b1shpunUgZg.png"/></div></div></figure><p id="5708" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2-决定应该通过更新门保留多少候选状态，以及应该通过忘记门忘记多少先前的单元状态</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/34c22a79b2069fc3ee8b8282cb14e138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSSh1YjGRSq2RHdK7tfiJw.png"/></div></div></figure><p id="c315" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以被认为是对GRU的改进，在GRU，如果网络决定保留%的先前隐藏状态，那么这意味着将引入(100-A)%的候选隐藏状态。在这里，网络有单独的更新和遗忘门，所以它不会被迫做出这样的决定。如果它愿意，它可以更新0%的候选小区状态，并完全忘记之前的小区状态。</p><p id="4472" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3-决定有多少单元状态应该通过输出门激活该层</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/faaee8ef34061f865a78af122a056bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wxgUFDPWxokCDji-gZqKKQ.png"/></div></div></figure><p id="d572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这很像GRUs中的关联门。事实上，我们可以消除这个等式，但是将三个门、单元状态和进一步的层输入中的h <em class="ma"> ₜ_₁ </em>的每个实例替换为(γₒ*h<em class="ma">ₜ_₁).</em>不同之处在于，它不仅适用于候选隐藏状态计算，还适用于所有其他计算。</p><h2 id="cb64" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">让我们结束吧！</h2><p id="b87e" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">以下是针对LSTM和GRU图层中的每个令牌执行的公式:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ms"><img src="../Images/a494d8c61e8092782973a2bae5dbf5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZvm3Sf4iS7c6YxPmwE42w.png"/></div></div></figure><p id="2a22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然你可能会认为lstm总是做得更好，因为它们的复杂性和更多的可训练参数，有一些情况下gru往往比它们更好，但总的来说lstm是更占优势的选择。为了将大量的参数和计算包含在一个LSTM图层中，我绘制了下面图层的低层次表示。看一看它，以确保你理解流程。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mt"><img src="../Images/251becebedf47cc40395158f669a39c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNPpKAVrIb0Fr5lemAfyuw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者绘图。实际上，x和h只有一个来源</p></figure><p id="06e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们结束之前，有一件小事我想弄清楚。尽管我们上面提供了所有的直觉，无论是LSTM还是GRU，你总是可以<a class="ae lb" rel="noopener" target="_blank" href="/a-true-story-of-a-gradient-that-vanished-in-an-rnn-56437c1eea45">通过时间</a>进行反向投影，以显示他们解决/改善了消失梯度问题。你甚至可以假设，研究人员想出的方法包括通过时间进行反向投影，并思考如何修改方程以缓解梯度消失的问题。</p><p id="824c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的故事到此结束。希望它能帮助你更好地理解LSTMs和GRUs是如何工作的。下次见，再见。</p></div></div>    
</body>
</html>