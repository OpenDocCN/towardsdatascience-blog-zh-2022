<html>
<head>
<title>Papers Simplified: »Gradients without Backpropagation«</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文简化:无反向传播的梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/papers-simplified-gradients-without-backpropagation-96e8533943fc#2022-02-21">https://towardsdatascience.com/papers-simplified-gradients-without-backpropagation-96e8533943fc#2022-02-21</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="b6b1" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/machine-learning-research" rel="noopener">机器学习研究</a></h2><div class=""/><div class=""><h2 id="56b7" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">如何评估一个函数并一次计算其梯度的近似值</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/0b06c178b78f3343e004c110200b3a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bI6UTvavv7MruFQO"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk">Photo by <a class="ae li" href="https://unsplash.com/@cdd20?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">愚木混株 cdd20</a> on <a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></p></figure><p id="dd4b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在这篇文章中，我们将看看由atlm günebaydin，Barak A. Pearlmutter，Don Syme，Frank Wood和Philip Torr于2022年2月17日发表的一篇<a class="ae li" href="https://arxiv.org/abs/2202.08587" rel="noopener ugc nofollow" target="_blank">最近的论文</a>——<strong class="ll je">没有反向传播的梯度</strong>。<a class="ae li" href="https://arxiv.org/abs/2202.08587" rel="noopener ugc nofollow" target="_blank">你可以在这里找到</a>。</p><p id="c03e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这份文件仍在审查中，所以以后可能会有一些变化。尽管如此，非常简洁的核心思想应该保持不变。让我们读一下报纸的摘要:</p><blockquote class="mf mg mh"><p id="e7e1" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">使用反向传播来计算优化目标函数的梯度一直是机器学习的支柱。反向传播或反向模式微分是自动微分算法家族中的一个特例，它也包括正向模式。我们提出了一种仅基于方向导数计算梯度的方法，人们可以通过正演模式精确有效地计算方向导数。我们称这个公式为<strong class="ll je">正向梯度</strong>，这是一个无偏的梯度估计，可以在函数的单次正向运行中进行评估，<strong class="ll je">完全消除了梯度下降</strong>中的反向传播需求。我们在一系列问题中演示了前向梯度下降，显示出<strong class="ll je">在计算</strong>上的大量节省，并在某些情况下使<strong class="ll je">训练速度提高两倍</strong>。</p></blockquote><p id="f38e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">听起来不错，对吧？现在，我会给你一些直觉，告诉你这种方法实际上是如何工作的，以及为什么会工作。我还会在途中解释一些重要的数学概念，给那些对此感到有点不安全的人。让我们开始吧。</p><h1 id="7251" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">先决条件</h1><p id="5cf5" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">在开始我们的旅程之前，让我们回顾一些重要的事情，以便正确看待整个主题。让我们假设我们想要<strong class="ll je">优化某个目标函数<em class="mi"> f </em>，<em class="mi"> </em> </strong>只是想象最小化神经网络情况下的损失函数。</p><p id="ee73" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">f有很多，可能有几十亿个参数，因此这个任务很难完成。尽管如此，聪明人设法发明了许多算法来解决这个问题，其中有<em class="mi">一阶</em>方法，如<strong class="ll je">梯度下降</strong>，还有<strong class="ll je"> </strong> <em class="mi">高阶</em>优化算法，如<strong class="ll je">牛顿法</strong>。</p><p id="83f0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，由于作者关注的是前者，我们也将这样做。</p><h2 id="d5aa" class="nj mn iu bd mo nk nl dn ms nm nn dp mw ls no np my lw nq nr na ma ns nt nc ja bi translated">对渐变的需求</h2><p id="c155" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">你可能知道<em class="mi">通常</em>做梯度下降的方式。这里我指的是随机梯度下降(SGD)、RMSProp、Adam、Adamax等算法。这些工具在Tensorflow、PyTorch或scikit-learn等软件库中都很容易找到。</p><p id="eaf6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所有这些不同的优化算法都依赖于一个共同的成分:目标函数<em class="mi">f</em>t5】的<strong class="ll je">梯度，表示为<strong class="ll je"><em class="mi"/></strong>∇<em class="mi">f</em>(<em class="mi">θ</em>)。上述算法之间的区别在于它们如何处理梯度。例如，最简单的算法SGD从一个随机的<em class="mi"> θ </em>开始，然后<em class="mi"> </em>用简单的更新规则<em class="mi">θ</em>←<em class="mi">θ-α</em>∇<em class="mi">f</em>(<em class="mi">θ</em>)进行更新，直到收敛，即<em class="mi"> θ </em>或<em class="mi">f</em>(<em class="mi">θ</em>)<em class="mi"/><em class="mi"> </em>这里的<em class="mi"> α </em>是学习率<em class="mi">。</em></strong></p><p id="7f01" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">RMSProp，<em class="mi"> </em> Adam等人也采用了梯度，但是做了更精细的计算，这导致了更快的收敛<em class="mi"/>。考虑下面的动画，这是一个老歌，但歌蒂:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/f2d8a2871edea2f680735b2137f80956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*Y2KPVGrVX9MQkeI8Yjy59Q.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">亚历克·拉德福德的伟大动画。</p></figure><h2 id="8a8f" class="nj mn iu bd mo nk nl dn ms nm nn dp mw ls no np my lw nq nr na ma ns nt nc ja bi translated">反向传播</h2><p id="c36b" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">现在让我们继续研究神经网络。<em class="mi">通常</em>计算梯度∇ <em class="mi"> f </em> ( <em class="mi"> θ </em>)的方法是使用一种叫做<strong class="ll je">反向传播</strong>的技术，这实质上是微积分中的链式法则。别担心，我们不会谈论太久，也不会让你做冗长的计算。只看下面这个简单的例子函数:<em class="mi"> f </em> ( <em class="mi"> θ </em> ₁ <em class="mi">，θ </em> ₂ <em class="mi">，θ</em>₃<em class="mi">)=(θ</em>₁<em class="mi">+θ</em>₂<em class="mi">)</em><em class="mi">θ</em>₃.</p><p id="8644" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">使用反向传播计算梯度的工作方式如下:</p><ol class=""><li id="2672" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oa ob oc od bi translated"><strong class="ll je">前进传球:</strong>插<em class="mi"> θ </em>入<em class="mi">f，接<em class="mi"> f </em> ( <em class="mi"> θ </em> ) <em class="mi">。</em></em></li><li id="24b4" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oa ob oc od bi translated"><strong class="ll je">向后传递:</strong>从<em class="mi"> f </em> ( <em class="mi"> θ </em>)开始，计算向后的坡度。</li></ol><p id="2887" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们插入向前传球的数字<em class="mi"> θ = </em> (3，-1，2)，然后看看在我们的玩具网络中会发生什么:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/e3764fd5f65d51f392d6a6d2e132cdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/1*qiHVFP1yXZx92FOFYuwr6g.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">向前传球。我创作的动画。</p></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oj"><img src="../Images/a857e0d56134c57236e32e58d8f6c7fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/1*kLA6DXPVWisggd2FfQZDKg.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">向后传球。我创作的动画。右边的蓝色1是初始渐变值。</p></figure><p id="e2f1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">向前传递从左到右计算输出4，非常容易。然后向后传递从右到左计算所有的梯度。我们只对最后的梯度感兴趣，在我们的例子中，∇ <em class="mi"> f </em> ( <em class="mi"> θ </em> ) = (2，2，2)。由于直接计算得到∇<em class="mi">f</em>(<em class="mi">θ</em>)=(<em class="mi">θ</em>₃，<em class="mi"> θ </em> ₃，<em class="mi"> θ </em> ₁ <em class="mi"> + θ </em> ₂).，所以结果是正确的</p><blockquote class="mf mg mh"><p id="b641" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated"><strong class="ll je">注意:</strong>直接计算梯度是一种奢侈，我们在处理复杂公式中的数十亿个参数时没有这种奢侈。</p></blockquote><blockquote class="ok"><p id="264e" class="ol om iu bd on oo op oq or os ot me dk translated">到目前为止一切顺利，但这意味着我们必须通过网络两次。我们能否在一次正向传递中计算函数输出和梯度？</p></blockquote><p id="18a4" class="pw-post-body-paragraph lj lk iu ll b lm ou ke lo lp ov kh lr ls ow lu lv lw ox ly lz ma oy mc md me in bi translated">这篇论文的作者提出了一个简单的方法来做到这一点！但是我们仍然需要一个要素来理解他们的方法。</p><h2 id="ea14" class="nj mn iu bd mo nk nl dn ms nm nn dp mw ls no np my lw nq nr na ma ns nt nc ja bi translated">方向导数</h2><p id="698e" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">如果你以前从未和他们打过交道，这听起来可能很可怕，但让我用简单的方式解释给你听。在学校，你知道如何对一个变量求导，对吗？<strong class="ll je"> <em class="mi"> θ </em>为标量</strong> <em class="mi"> </em>时的定义如下:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oz"><img src="../Images/979c32d3818d5d6b091aed33fb30cb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ai__c2u7kSzdzu264UffWA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我创造的形象。</p></figure><p id="5294" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这个值<em class="mi">f’</em>(<em class="mi">θ</em>)就是图中点<em class="mi"> θ、</em>的切线的<strong class="ll je">斜率，即一个实数。</strong></p></div><div class="ab cl pa pb hy pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="in io ip iq ir"><p id="3e2c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，当处理神经网络时，我们的函数<em class="mi"> f </em>没有单个输入，而是潜在地有数十亿个输入，记得吗？因此，在这种情况下，我们必须为导数找到一个更一般的定义。当然，数学家已经帮我们找到了答案。<em class="mi"> f </em>相对于矢量<em class="mi"> v </em>在点<em class="mi"> θ </em>的方向导数定义为</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ph"><img src="../Images/7cf9dbc0271e616f299710bb5b496045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cG-vWVRCamH38dewahSZoQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我创造的形象。</p></figure><p id="4dce" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为什么我们需要一个<em class="mi"> v </em>？在更高维度中，我们有很多选择来定义图的切线。以抛物面为例:在这个形状表面的黑点中，我们可以任意定义多条切线。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pi"><img src="../Images/2cffdafe40fecc894aeaeaa0a0f01744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViGJCXNoM45XWtiALFMJJA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">原图来自<a class="ae li" href="https://de.wikipedia.org/wiki/Paraboloid" rel="noopener ugc nofollow" target="_blank">维基百科</a>。漂亮的切线是我加的。</p></figure><p id="0db1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果我们选择一些<em class="mi"> v = </em> ( <em class="mi"> v </em> ₁ <em class="mi">，v </em> ₂) <em class="mi">，</em>切线会指向同一个方向，只是多了一个<em class="mi">z</em>-坐标，即(<em class="mi"> v </em> ₁ <em class="mi">，v </em> ₂，<em class="mi"> s </em>)。这个斜坡<em class="mi"> s </em>正好是∇ <em class="mi"> ᵥf </em> ( <em class="mi"> θ </em>)顺便说一下。</p></div><div class="ab cl pa pb hy pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="in io ip iq ir"><p id="6afe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">无论如何，如果你对几何不感兴趣，只要记住∇ <em class="mi"> ᵥf </em> ( <em class="mi"> θ </em>)也是一个标量，就像<em class="mi">f’</em>(<em class="mi">θ</em>)一样。对于每个<em class="mi"> v，</em>都是不同的数字，表示<em class="mi"> f </em>在<em class="mi"> v </em>方向的斜率。现在可以证明以下几点:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/193d64c92c271b47ff280729039beaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*fPC5dh0n3rmYyF5w_fzE2w.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我创造的形象。点表示两个向量的点积。</p></figure><p id="ce99" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里，∇ <em class="mi"> f </em> ( <em class="mi"> θ </em>)是<em class="mi"> f </em>的通常梯度。很好记，对吧？从左到右，小下标<em class="mi"> v </em>刚好跳到旁边。快速检查一下:左边是一个标量，右边也是，因为它是两个向量的点积。</p><p id="ebe1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">太好了，现在我们有一切可以理解这篇论文了！</p><h1 id="ad73" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">向前的梯度</h1><p id="0c5c" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">因此，给定一个函数<em class="mi"> f </em>，作者定义了另一个函数<em class="mi"> gᵥ </em>，他们称之为<strong class="ll je">向前梯度</strong>。</p><blockquote class="mf mg mh"><p id="e38f" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">注意:作者在他们的论文中省略了下标v，但我发现把它放在那里很有用。它提醒我们向前的梯度依赖于一些v。</p></blockquote><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pk"><img src="../Images/880d6443f1f4ca09ac51512103e73bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcQwp-WTlbvYlqtYDvGf3w.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我创造的形象。</p></figure><p id="a4e5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以看到，<em class="mi"> gᵥ </em>取一个矢量<em class="mi"> θ </em>再次输出另一个矢量，某种缩放版的<em class="mi"> v </em> ( <strong class="ll je">记住:</strong><em class="mi"/>∇<em class="mi">ᵥf</em>(<em class="mi">θ</em>是标量)。<strong class="ll je">我们还需要<em class="mi"> θ </em>和<em class="mi"> v </em>具有相同的尺寸。</strong></p><p id="0c1b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但这看起来比之前更复杂，它甚至引入了另一个矢量<em class="mi"> v </em>！如何选择<em class="mi"> v </em>让<em class="mi"> gᵥ </em>更有用？答案很简单:将<em class="mi"> v </em>的每个分量独立采样为标准高斯<em class="mi"> N </em> (0，1)。现在，你可能想知道为什么这是有用的，因为这只是创建了许多随机向量。作者使用一些简单易懂的数学方法展示了以下内容:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pl"><img src="../Images/93a8e29d8e5923304c572028681174e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIK51dhkKdIrCGPI9sFBmQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">直接从报纸上。</p></figure><p id="f93b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这是对以下内容的巧妙描述:</p><blockquote class="ok"><p id="bd7f" class="ol om iu bd on oo pm pn po pp pq me dk translated">如果对大量v进行采样，计算相应的前向梯度，然后对这些前向梯度求平均值，最终会得到一个非常接近真实梯度的值。</p></blockquote><p id="6e54" class="pw-post-body-paragraph lj lk iu ll b lm ou ke lo lp ov kh lr ls ow lu lv lw ox ly lz ma oy mc md me in bi translated">让我们用一些简单的代码来验证这个声明。我们定义函数<em class="mi"> f </em> ( <em class="mi"> θ </em> ₁ <em class="mi">，θ</em>₂)=<em class="mi">θ</em>₁+<em class="mi">θ</em>₂，我们要计算它在点<em class="mi"> θ = </em> (2，1) <em class="mi">的梯度。</em></p><pre class="kt ku kv kw gu pr ps pt bn pu pv bi"><span id="8a83" class="pw mn iu ps b be px py l pz qa">import numpy as np<br/><br/>f = lambda theta: theta[:, 0]**2 + theta[:, 1]**2<br/>theta = np.array([[2, 1]])</span></pre><blockquote class="mf mg mh"><p id="e41b" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated"><strong class="ll je"> <em class="iu">注:</em> </strong> <em class="iu">我让函数一次取几个输入，即批量输入。</em></p></blockquote><p id="214e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">到目前为止，一切顺利。在我们继续之前，让我们自己计算一下梯度。我们有∇ <em class="mi"> f </em> ( <em class="mi"> θ </em> ₁ <em class="mi">，θ </em> ₂) = (2 <em class="mi"> θ </em> ₁，2 <em class="mi"> θ </em> ₂)，所以∇ <em class="mi"> f </em> (2 <em class="mi">，1 </em> ) = (4，2)。</p><p id="d052" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们也可以通过使用<em class="mi">规范基</em>计算方向导数来验证这一点，即所有向量除了一个1之外都是零。</p><pre class="kt ku kv kw gu pr ps pt bn pu pv bi"><span id="bfdd" class="pw mn iu ps b be px py l pz qa">h = 0.00001<br/>v = np.array([[1, 0], [0, 1]])<br/><br/>(f(theta + h*v) - f(theta)) / h # approximate derivative<br/><br/># Output:<br/># array([4.00001, 2.00001])</span></pre><p id="067b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，让我们计算许多随机向量<em class="mi"> v </em>并计算前向梯度<em class="mi"> gᵥ: </em></p><pre class="kt ku kv kw gu pr ps pt bn pu pv bi"><span id="fad0" class="pw mn iu ps b be px py l pz qa">np.random.seed(0) # set a seed to keep it reproducible<br/><br/>v = np.random.randn(1_000_000, 2) # a million vectors of size 2<br/>grad_v_f = ((f(theta + h*v) - f(theta)) / h) # ∇ᵥf(θ)<br/>g = grad_v_f.reshape(-1, 1) * v # scale v with grad_f to compute g</span></pre><blockquote class="mf mg mh"><p id="768a" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated"><strong class="ll je">重要提示:</strong>我calculate∇ᵥf<em class="iu">(</em>θ<em class="iu">)</em>的方式没那么好。它只是真实方向导数的近似，甚至在数值上是不稳定的。正如作者所说，<strong class="ll je">有一种方法可以在单次正向传递中使用正向模式自动微分</strong>来计算∇ ᵥf <em class="iu"> ( </em> θ <em class="iu"> ) </em>和f <em class="iu"> ( </em> θ <em class="iu"> ) </em>，但遗憾的是，他们没有为这种说法提供来源。</p><p id="2f6c" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated">例如，侯赛因·阿卜杜拉赫曼指出PyTorch使用双数。谢谢！</p></blockquote><p id="a12c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我不会说得太详细，但是<strong class="ll je">双数</strong>类似于复数<em class="mi"> a </em> + <em class="mi"> bi </em>，但是我们处理的不是具有属性<em class="mi"> i </em> = -1的虚数<em class="mi"> i </em>，而是具有属性<em class="mi"> ɛ </em> = 0的数字<em class="mi"> a </em> + <em class="mi"> bɛ </em>和<em class="mi"> ɛ </em>对于这些数字，我们有<em class="mi">f</em>(<em class="mi">a</em>+<em class="mi">bɛ</em>)<em class="mi">= f</em>(<em class="mi">a</em>)<em class="mi">+f’</em>(<em class="mi">a</em>)<em class="mi">bɛ</em>为<em class="mi">乖</em>函数<em class="mi">f</em>它很自然地扩展到多输入的函数，在这种情况下，我们接收(重命名变量后)<em class="mi">f</em>(<em class="mi">θ</em>+<em class="mi">ɛv</em>)<em class="mi">= f</em>(<em class="mi">θ</em>)<em class="mi">+ɛ</em>f(<em class="mi">θ</em>)<em class="mi">v【t100</em></p></div><div class="ab cl pa pb hy pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="in io ip iq ir"><p id="68cf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在<code class="fe qb qc qd ps b">g</code>中，现在存储了很多随机向量，但是根据定理，它们的平均值应该接近∇ <em class="mi"> f </em> (2 <em class="mi">，1 </em> ) = (4，2)。让我们看看。</p><pre class="kt ku kv kw gu pr ps pt bn pu pv bi"><span id="6b05" class="pw mn iu ps b be px py l pz qa">g.mean(axis=0)<br/><br/># Output:<br/># array([3.99433963, 2.00797197])</span></pre><p id="5107" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">看起来不错！但是我们对数学的期望并没有减少。😎我们甚至可以想象向前的梯度<em class="mi"> gᵥ: </em></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qe"><img src="../Images/82a7ab5bf8e4e6114673efe24248ac12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4qGsKxPDg1zvGUtHkq8Ww.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">我创造的形象。</p></figure><p id="f411" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以看到，围绕真实值(4，2)有相当大的差异，但正如作者进行的实验所证明的那样，该算法似乎工作得很好。但是在我们谈到它们之前，让我从纸上复制完整的算法，因为它很好，很短。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qf"><img src="../Images/51df09a0000654b089da1e1dd286e17e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7wLlt0HzIs6hxSxaFjGUg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">直接从报纸上。</p></figure><p id="fb25" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这基本上是一个普通的梯度下降，只是用另一种方法来计算梯度。而且，正如作者提到的，这仅仅是开始:创建Adam、RMSProp和任何其他使用梯度的算法的前向梯度版本是很简单的。所以，请在某个时候继续关注<strong class="ll je"> <em class="mi">法达姆</em> </strong>算法。</p><p id="3144" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">科学界仍然需要解决的唯一有趣的事情是，前向梯度引入的噪声是否会破坏Adam等更高级的算法。对于SGD来说，它工作得相当好，所以让我们来看看实验。</p><h1 id="ba7f" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">实验</h1><blockquote class="mf mg mh"><p id="e539" class="lj lk mi ll b lm ln ke lo lp lq kh lr mj lt lu lv mk lx ly lz ml mb mc md me in bi translated"><strong class="ll je">免责声明:</strong>我现在从报纸上复制所有图片。</p></blockquote><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qg"><img src="../Images/8da4076d94383d6aafc016ebf9acca9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1yTCWtIzQNRh_DUsLK9xQ.png"/></div></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qh"><img src="../Images/84e4f0a448e8e1d66527439ba7783742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMwZK3AdeFTKQptAN9wrUA.png"/></div></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qi"><img src="../Images/6ffaa1280c9bc99a194a82943339e84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMVM2x4XPOJyL_UT-I8NtA.png"/></div></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qi"><img src="../Images/c6e10122a18e4ab95be2637db98124f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZlUv_IRcMKD2PYpZ9RV3A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">直接从报纸上。</p></figure><p id="71b6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这看起来很有希望。特别是对于更复杂的神经网络，前向梯度比反向传播用更少的时间产生相似的性能。只是要小心，因为总是有可能作者没有向我们展示他们的算法不能很好工作的情况。</p><h1 id="e183" class="mm mn iu bd mo mp mq mr ms mt mu mv mw kj mx kk my km mz kn na kp nb kq nc nd bi translated">结论</h1><p id="5bcd" class="pw-post-body-paragraph lj lk iu ll b lm ne ke lo lp nf kh lr ls ng lu lv lw nh ly lz ma ni mc md me in bi translated">在这篇文章中，我和你一起回顾了最近的论文<a class="ae li" href="https://arxiv.org/abs/2202.08587" rel="noopener ugc nofollow" target="_blank"> <strong class="ll je">渐变无反向传播</strong> </a>。作者展示了如何仅使用单个正向传递而不是通过网络反向传播来进行梯度下降。这样既节省了时间，又不会增加内存消耗。</p><p id="b72a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">作者在PyTorch中实现了他们的方法，但是，我仍然在等待他们发表它。总的来说，我认为这是一篇写得很好的论文，有一个漂亮整洁的想法，易于理解甚至实施。</p></div><div class="ab cl pa pb hy pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="in io ip iq ir"><p id="1810" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="eef2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">作为最后一点，如果你</strong></p><ol class=""><li id="ecc1" class="nv nw iu ll b lm ln lp lq ls nx lw ny ma nz me oa ob oc od bi translated"><strong class="ll je">想支持我多写点机器学习和</strong></li><li id="cac4" class="nv nw iu ll b lm oe lp of ls og lw oh ma oi me oa ob oc od bi translated"><strong class="ll je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="9a04" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">为什么不通过这个环节</strong><a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"><strong class="ll je"/></a><strong class="ll je">？这将对我帮助很大！😊</strong></p><p id="0658" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="7fa7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="ok"><p id="6150" class="ol om iu bd on oo pm pn po pp pq me dk translated">如有任何问题，请在<a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我写信！</p></blockquote></div></div>    
</body>
</html>