<html>
<head>
<title>Deploy Any ML Model to Any Cloud Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将任何ML模型部署到任何云平台</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploy-any-ml-model-to-any-cloud-platform-f27a8311f6d4#2022-07-29">https://towardsdatascience.com/deploy-any-ml-model-to-any-cloud-platform-f27a8311f6d4#2022-07-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6507" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍Truss，一个用于模型打包和部署的开源库</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9055ba44871e73912c6d63579d2d7d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wMxOQOmgctXGCOFh"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Truss是用于ML模型服务的开源Python库|照片由<a class="ae kv" href="https://unsplash.com/@jcotten?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Joshua J. Cotten </a>在<a class="ae kv" href="https://unsplash.com/s/photos/truss?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d3cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型服务不仅仅是一个难题，它是一个不断需要新的解决方案的难题。</p><p id="1c49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为MLOps的一部分，模型服务是DevOps的挑战，即保持一个复杂、脆弱的工件(模型)在多个动态环境中工作。随着为培训模型构建和更新框架，以及生产环境为新的功能和约束而发展，数据科学家必须重新实现模型服务脚本并重建模型部署流程。</p><p id="9b76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在资源充足的大型组织中工作的数据科学家可以将他们的模型交给专业的MLOps团队进行服务和部署。但对于我们这些在初创公司和新公司工作的人来说，就像我在职业生涯的第一个十年所做的那样，我们必须自己处理ML部署的挑战。问题:服务和部署一个模型需要一套完全不同于培训的技能和技术。</p><p id="2fda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单介绍一下:我是Tuhin Srivastava，<a class="ae kv" href="http://baseten.co" rel="noopener ugc nofollow" target="_blank"> Baseten </a>的首席执行官，Truss最初就是在这里开发的。在努力找出数据科学家需要什么来实现MLOps的过程中，我们与数据科学领导者进行了交谈，并听到了类似这样的事情:</p><ul class=""><li id="526d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">“我们希望避免任何形式的自定义开发，自行托管模型。如果我们自己来做，我们可能需要在虚拟机或Kubernetes集群上部署我们自己的Docker，然后我们还必须处理围绕这些东西的所有开发工作。”<em class="mb"> </em> — Faaez Ul Haq，数据科学主管@ Pipe</li><li id="6bd6" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">“我们的团队主要由数据科学家和语言学家组成，我们不是DevOps专家。我们可以写Python，但我们不想整天写YAML配置。”<em class="mb"> </em> —丹尼尔·怀特纳克，数据科学家@ SIL</li></ul><p id="a6a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据科学家的工作环境是Jupyter notebook，这是一个为迭代实验而设计的灵活而宽松的系统。Jupyter笔记本是训练模型的一个很好的工具，但是作为一个非永久性的和面向开发的环境，它对于模型服务来说不是很好。模型服务需要像Docker这样的技术来带来一个稳定的、可预测的环境。</p><h1 id="deec" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">当今数据科学家如何处理服务模型</h1><p id="4384" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">为生产中的模型提供服务通常归结为几个步骤:</p><ol class=""><li id="ae3d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated">序列化模型</li><li id="75b3" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">将模型放在Flask之类的web服务器后面</li><li id="d091" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">将web服务器打包成Docker映像</li><li id="5f8a" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated">在容器上运行Docker映像</li></ol><p id="1476" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这些步骤中潜藏着额外的复杂性。该模型需要接受输入并以适当的格式生成输出，从Python优先的接口转换为web优先的接口。而且有些模型需要访问GPU硬件进行预测，或者安全访问秘密值，或者导入Python和系统包。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fbfe6a68c423acf09ca64a59e943e02b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GvsttCRnZo9CM1Dy"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">导航部署迷宫|照片由<a class="ae kv" href="https://unsplash.com/s/photos/lost-in-a-maze?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@rwlinder?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Robert Linder </a>拍摄</p></figure><p id="583a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是更大的问题是，即使基本步骤对于每个框架都是不同的，有时对于用同一框架构建的不同模型也是如此。因此，即使你知道如何服务TensorFlow模型，你也必须重新学习如何服务PyTorch模型，并在尝试拥抱脸模型时再次经历该过程。</p><p id="e417" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“嗯，没关系，”你可能会说，“我只是要使用一个建模框架。我会成为TensorFlow ML工程师。”问题是，我们没有不同的框架，因为数据科学家不擅长达成一致。因为不同的问题需要不同的方法。每个流行的建模框架擅长不同种类的底层算法和结构。但是模型服务技术不需要完全不同。</p><p id="c322" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像<a class="ae kv" href="https://github.com/replicate/cog" rel="noopener ugc nofollow" target="_blank"> Cog </a>、<a class="ae kv" href="https://github.com/bentoml/BentoML" rel="noopener ugc nofollow" target="_blank"> BentoML </a>和<a class="ae kv" href="https://github.com/mlflow/mlflow/" rel="noopener ugc nofollow" target="_blank"> MLflow </a>这样的开源包有助于简化模型部署过程。我们希望扩展这些想法，开发一个开源库，特别是针对初创企业的数据科学家。我们的两个关键信念:</p><ol class=""><li id="7031" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr ne ly lz ma bi translated"><strong class="ky ir">为Python用户构建:</strong>作为数据科学家，Python是我们的舒适区。我们想要一个可以完全用Python管理的模型部署库。</li><li id="44df" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr ne ly lz ma bi translated"><strong class="ky ir">与每个模型和平台一起工作:</strong>我们想要一个开源包，它可以处理模型部署，而不管模型框架和云平台。</li></ol><p id="edf6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这些想法的指导下，我们构建并开源了<a class="ae kv" href="https://github.com/basetenlabs/truss" rel="noopener ugc nofollow" target="_blank"> Truss </a>。</p><h1 id="e906" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">模型服务如何与Truss一起工作</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1e914c40f3c860aa743752cd30a6b351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*k0oGQwEBYbv0s0am"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">提供和部署模型|作者图像的步骤</p></figure><h2 id="417e" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">步骤1:标准化模型打包</h2><p id="abf6" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们在本地机器上的Jupyter笔记本中，这是数据科学家的家乡。使用拥抱脸变压器，我们将在这个例子中引入t5小模型作为管道。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4410" class="nf mi iq ns b gy nw nx l ny nz">from transformers import pipeline<br/>import truss<br/> <br/>pipe = pipeline(tokenizer="t5-small", model="t5-small")<br/>scaf = truss.mk_truss(pipe, target_directory="my_model")</span></pre><p id="5388" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拥抱脸是Truss开箱即用支持的许多流行框架之一，还包括LightGBM、PyTorch、scikit-learn、TensorFlow和XGBoost(更多即将推出)。因此，我们需要做的就是在模型上运行mk_truss，所有的东西都将被序列化和打包，以备使用。</p><h2 id="5e9f" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">第二步:扎实的地方发展</h2><p id="44ce" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">使用我们的Truss，我们可以在Jupyter环境中调用模型:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="dc9e" class="nf mi iq ns b gy nw nx l ny nz">print(scaf.server_predict({"inputs" : ["translate: hello world in german"]}))<br/># Expected result is {'predictions': [{'translation_text': 'Übersetzen: Hallo Welt in deutsch'}]}</span></pre><p id="305a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是Truss超越了代码内模型调用。有多种本地开发选项，包括在Docker容器中运行模型和发出API请求。</p><p id="612e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要启动docker容器:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="c2f4" class="nf mi iq ns b gy nw nx l ny nz">truss run-image my_model</span></pre><p id="826f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要提出请求:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="532e" class="nf mi iq ns b gy nw nx l ny nz">curl -X POST <a class="ae kv" href="http://127.0.0.1:8080/v1/models/model:predict" rel="noopener ugc nofollow" target="_blank">http://127.0.0.1:8080/v1/models/model:predict</a> -d "{'inputs': [{'translation_text': 'Übersetzen: Hallo Welt in deutsch'}]}"</span></pre><p id="c567" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本地开发不仅仅是测试。您可以更新您的模型，以便通过<a class="ae kv" href="https://truss.baseten.co/develop/processing" rel="noopener ugc nofollow" target="_blank">预处理和后处理功能</a>更好地与其他系统集成，创建<a class="ae kv" href="https://truss.baseten.co/develop/examples" rel="noopener ugc nofollow" target="_blank">样本输入</a>以记录测试用例，以及<a class="ae kv" href="https://truss.baseten.co/develop/configuration" rel="noopener ugc nofollow" target="_blank">配置您的Truss的每个方面</a>以满足您的需求。有了上面介绍的各种调用选项，您将能够通过一个紧密的开发循环快速地为生产准备好您的模型。</p><h2 id="b639" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">步骤3:无缝生产部署</h2><p id="310b" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">多亏了Docker，我们一直工作的开发环境与最终的生产环境非常匹配。根据您希望在哪里部署您的模型，请遵循针对平台的特定部署说明，如<a class="ae kv" href="https://truss.baseten.co/deploy/aws" rel="noopener ugc nofollow" target="_blank"> AWS ECS </a>、<a class="ae kv" href="https://truss.baseten.co/deploy/baseten" rel="noopener ugc nofollow" target="_blank"> Baseten </a>和<a class="ae kv" href="https://truss.baseten.co/deploy/gcp" rel="noopener ugc nofollow" target="_blank"> Google Cloud Run </a>。您的模型也可以部署在任何可以运行Docker映像的地方。</p><p id="7bac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据您的环境，调用部署的模型可能略有不同，但是应该是与步骤2中的请求相匹配的API请求，但是目标是生产域。</p><h2 id="3394" class="nf mi iq bd mj ng nh dn mn ni nj dp mr lf nk nl mt lj nm nn mv ln no np mx nq bi translated">步骤4:共享和迭代</h2><p id="d16e" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">您的序列化模型以及相关的代码和配置文件构成了整个桁架。因此，将您的模型打包成一个Truss使其具有可移植性，从而释放了两个关键用例:版本控制和共享。</p><p id="083a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一切都在文件中，所以您可以在Git中提交您的模型。通过这种方式，您可以实现模型版本化，测试您模型的不同迭代，并将您的模型推送到GitHub或类似的存储库中。</p><p id="b0a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尝试运行别人的模型最令人沮丧的部分是复制他们的环境。但是现在你的模型被保存为一个Truss，所有人需要做的就是从GitHub或另一个资源库下载它，安装<a class="ae kv" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>和<a class="ae kv" href="https://pypi.org/project/truss/" rel="noopener ugc nofollow" target="_blank"> Truss Python包</a>，并在本地服务模型。我们很高兴能够在开源建模中实现更多的协作和迭代。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/68ce055c3cc5609a6049925ceea686a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5eccoYqYulMVg_FR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">桁架标志|作者图片</p></figure><p id="3d22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">机器学习模型只会变得更加复杂和强大。反过来，在本地和生产中可靠地服务于这些模型变得比以往任何时候都更加重要。我们致力于对Truss的长期支持和开发，并让我们知道应该在我们的<a class="ae kv" href="https://github.com/basetenlabs/truss/blob/main/ROADMAP.md" rel="noopener ugc nofollow" target="_blank">路线图</a>中添加什么。</p><p id="4185" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Truss为跨模型框架和部署目标的模型服务提供了统一的方法。从由repo 主演的<a class="ae kv" href="http://github.com/basetenlabs/truss" rel="noopener ugc nofollow" target="_blank">开始，通过</a><a class="ae kv" href="https://baseten.gitbook.io/truss/e2e" rel="noopener ugc nofollow" target="_blank">端到端部署教程</a>学习您最喜欢的框架和平台。</p></div></div>    
</body>
</html>