<html>
<head>
<title>Scalable Reinforcement Learning Using Azure ML and Ray</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Azure ML和Ray的可扩展强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scalable-reinforcement-learning-using-azure-ml-and-ray-654d9f5c26a0#2022-07-13">https://towardsdatascience.com/scalable-reinforcement-learning-using-azure-ml-and-ray-654d9f5c26a0#2022-07-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="34c9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在Azure ML集群上使用RLLIB加速定制健身房环境中的深度RL训练。</h2></div><p id="dba8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">单机和单代理RL训练有许多挑战，最重要的是回报收敛所需的时间。代理在RL培训中花费的大部分时间都用于收集经验。简单的申请需要几个小时，复杂的申请需要几天。像Tensorflow这样的深度学习框架支持分布式训练；这同样适用于RL吗？答案是肯定的。</p><p id="11ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文通过一个实际例子关注单机训练的具体痛点，并演示scaled RL如何解决这个问题。本文假设读者对RL、深度RL和深度学习有基本的了解；对于详细的解释和演练，请浏览页面底部的整个视频。</p><p id="99f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可扩展强化学习技术通过将学习与行动分离，解决了高维状态空间环境中训练时间增加的问题。遵循分布式深度学习的实践，可以通过跨模型分布梯度计算或通过分布数据累积过程来实现并行性。已经为分布式学习提出了各种各样的算法。</p><p id="6832" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文主要关注使用分布式经验轨迹收集的加速学习。</p><p id="3415" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从以下内容开始:</p><ul class=""><li id="4661" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">理解问题陈述。</li><li id="38fc" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">建立一个简单的DQN代理，并为培训持续时间和奖励创建一个基线。</li><li id="06a0" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">创建一个分布式RL环境，以分布式方式运行实验，并将改进与上面的基线进行比较。</li></ul><p id="0ea2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于训练的代码工件的链接可以在<a class="ae ly" href="https://github.com/sriksmachi/supercabs" rel="noopener ugc nofollow" target="_blank">这里</a>(<a class="ae ly" href="https://github.com/sriksmachi/supercabs" rel="noopener ugc nofollow" target="_blank">https://github.com/sriksmachi/supercabs</a>)获得</p><h1 id="b4d0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated"><strong class="ak">自定义环境</strong></h1><p id="b047" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">网上的许多示例演示了如何在内置的体育馆环境中使用分布式RL，但是很少使用定制环境。在实践中，当选择RL来解决问题时，学习如何为分布式训练开发和注册定制的健身房环境是很重要的。这篇文章教如何创建一个自定义的健身房环境。出于本文和视频的目的，我们将使用下面定义的伪学习问题。</p><p id="a43b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">问题</strong>:我们处理的自定义环境叫做“Contoso Cabs”。Contoso Cabs是一家虚构的出租车公司，希望通过增加每个司机每月的总工作时间来增加利润。它目前在5个州运营，提供全天候服务。为了实现利润最大化，Contoso Cabs希望建立一个RL代理，帮助司机在出租车请求到达时做出正确的决策。由于利润与驾驶时间相关联，代理接受一次搭车以最大化累计折扣奖励，在这种情况下是总小时数。代理的目标是每集累积最大小时数(每集一个月)。下面的代码显示了本培训中使用的自定义健身房环境类。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h1 id="75d0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">DQN代理商</h1><p id="c156" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们首先使用以下架构构建一个简单的DQN代理。</p><ul class=""><li id="a108" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">输入:编码状态空间的大小是36(城市数量(5)+运行天数(7) +每天运行小时数(24))。</li><li id="1c6a" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">2个尺寸为36的FC隐藏层。</li><li id="643c" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">动作空间表示为(源，目的)的元组，动作空间的大小为5*5 = 25。为了简单起见，包含了具有相同源和目的对的元组，但是如果代理选择任何这样的动作，则增加了惩罚。实质上，我们向网络输入一批64个状态向量，每个状态向量的大小是36。输出代表每个动作的Q值。</li></ul><p id="ff03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当一个DQN特工被训练后，奖励稳定在1000集2500–3000之间，如下图所示。这意味着现在我们有了一个模型，可以用来确保司机的收入在2500-3000之间。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/78088e541b6bbabf5d5671217a827bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lb419jgP8wtzzhE5daxaIA.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">DQN培训结果。(图片由作者创作)</p></figure><p id="0e12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，挑战在于，对于一个简单的环境，代理需要大约35分钟才能收敛。这里考虑的状态空间仅包含36位(编码的)。在实时情况下，状态空间很容易高出10倍。</p><h1 id="dd2f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">天蓝色上的光线ML</h1><p id="4bd6" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在深入解决状态空间问题之前，让我们了解一下分布式RL算法、Ray、Azure ML和APEX的关键要素。</p><p id="1c96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分布式深度RL将动作(与env交互)与学习(从样本学习)解耦，这种解耦允许系统独立扩展。根据各种组件之间的交互如何发生以及RL方法学，已经发布了几种方法和算法，其中一些在我的文章<a class="ae ly" rel="noopener" target="_blank" href="/accelerate-training-in-rl-using-distributed-reinforcement-learning-architectures-b5a726b49826">这里</a>中有所描述。</p><p id="3544" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Ray的RLLIB是一个优秀的开源库，支持大多数提出的算法。使用现有的或定制的健身房环境可以很容易地配置和运行培训作业。在任何分布式集群上启用RLLIB都很简单。</p><p id="de64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Azure ML是Azure(微软的云)提供的服务，充当运行ML实验的工作空间。使用Azure ML，我们可以在交互模式或作业模式下运行ML实验，用于长期运行的实验。Azure ML提供了一个自托管的ML集群，可以使用CPU / GPU核心进行配置，以运行分布式ML实验。为了安装和运行分布式RL作业，我们需要在Azure ML集群上安装Ray。<a class="ae ly" href="https://github.com/microsoft/ray-on-aml" rel="noopener ugc nofollow" target="_blank"> Ray-on-aml </a>是一个开源库，用于将Azure ML集群转换为Ray集群。</p><p id="f7ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ly" href="https://arxiv.org/abs/1803.00933v1" rel="noopener ugc nofollow" target="_blank"> APEX </a>是一个大规模深度强化学习的分布式架构。它允许演员通过将表演与学习分离来有效地学习。APEX中的参与者可以扩展到1000个工作节点，每个参与者使用自己的环境副本。这些体验被收集起来并与公共重放缓冲器共享。除了经验，演员还分享使用时间差误差计算的优先级。学习者模型采样优先化的经历并重新计算优先级。网络参数被周期性地更新到演员的模型。</p><p id="a22f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了APEX的架构。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi no"><img src="../Images/070b3b6b5797636398fdc87a7e750906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*K3rMO0PbvYOAGO-ArzgiOg.jpeg"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">APEX建筑(图片由作者创建)</p></figure><p id="46be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与单个代理相比，上述架构允许学习者模型从丰富的经验集中学习。与均匀采样相比，优先化/重要性采样有助于更快的收敛。</p><p id="6400" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的代码显示了在转换为Ray集群的Azure ML集群上执行的作业。这是在作业模式下使用AML执行的，有关AML作业的详细信息，请参见此处编写的代码(<a class="ae ly" href="https://github.com/sriksmachi/supercabs/blob/master/run_experiment.py" rel="noopener ugc nofollow" target="_blank">https://github . com/sriksmachi/super cabs/blob/master/run _ experiment . py</a>)</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h1 id="6fb9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结果</h1><p id="d013" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">使用上面解释的方法，在射线簇上用10个演员训练“Contoso Cabs”环境3次训练迭代。这个实验只用了3分钟训练6K集，收敛到最大奖励。</p><p id="6d2c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练时间减少了90%，同时，我们通过分配工作量运行了6k集。下图显示了所有剧集中每集的最高奖励。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi np"><img src="../Images/11a6a8dbbf3b5502220d14e99b3f8ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPWdDI005vbuCa6jTa-4NQ.jpeg"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">APEX每集奖励(图片由作者创建)</p></figure><p id="53d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的视频详细解释了训练实验。如果你是RL新手，我建议你从头开始看视频。</p><figure class="mw mx my mz gt na"><div class="bz fp l di"><div class="nq nc l"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">代码演练:使用Ray的RLLIB和自定义GYM环境在Azure ML上分发深度RL。</p></figure><h1 id="5683" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">摘要</h1><p id="ce1a" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">总之，真实环境中RL代理的单机训练是一个耗时的过程。分布式深度RL训练有助于改进训练时间并更快地达到收敛。RLLIB (Ray)是一个用于生产级分布式RL工作负载的强大框架，只需几行代码，Azure ML集群就可以使用ray-on-ml转换为Ray集群。通过利用本文中提出的方法，可以大大减少培训时间。</p></div></div>    
</body>
</html>