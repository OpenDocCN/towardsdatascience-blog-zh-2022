<html>
<head>
<title>GIN: How to Design the Most Powerful Graph Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GIN:如何设计最强大的图形神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-design-the-most-powerful-graph-neural-network-3d18b07a6e66#2022-04-27">https://towardsdatascience.com/how-to-design-the-most-powerful-graph-neural-network-3d18b07a6e66#2022-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0c8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于图同构网络的图分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0267a930681f9193e080ef67dd4b4f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X13RjfsrTf9L1HhqBUt1sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ac09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图形神经网络不限于对节点进行分类。</p><p id="8677" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最流行的应用之一是<strong class="la iu">图分类</strong>。这是处理分子时的一项常见任务:它们被表示为图形，每个原子(节点)的特征可用于预测整个分子的行为。</p><p id="ea5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，GNNs只学习节点嵌入。如何将它们组合起来，以产生一个完整的嵌入的<strong class="la iu">图？在本文中，我们将:</strong></p><ul class=""><li id="a91f" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">看到一种新型的层，叫做“<strong class="la iu">全局池</strong>”，来组合节点嵌入；</li><li id="c3c7" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">介绍一种叫做<strong class="la iu">图同构网络</strong> (GIN)的新架构，由<a class="ae mi" href="https://arxiv.org/abs/1810.00826v3" rel="noopener ugc nofollow" target="_blank">徐等人</a>于2018年设计。</li></ul><p id="7372" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将详细说明与GCN或GraphSAGE相比，GIN在辨别能力方面的优势，以及它与Weisfeiler-Lehman测试的联系。除了它强大的聚合器，GIN还带来了关于GNNs的令人兴奋的收获。</p><p id="5e30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以用下面的<a class="ae mi" href="https://colab.research.google.com/drive/1b6SWugNKnxsI0L9auX1zwszlXf3rRZyS?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>运行代码。</p><h1 id="d22c" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🌐一.蛋白质数据集</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/13f23284578117005bf783d11d9d38cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sk7hX7GKcVqepVQCfOTduw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蛋白质的3D图(图片由作者提供)</p></figure><p id="d2fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae mi" href="https://chrsmrrs.github.io/datasets/docs/datasets/" rel="noopener ugc nofollow" target="_blank">蛋白质</a>是生物信息学中的一个流行数据集。它是代表蛋白质的1113个图的集合，其中节点是氨基酸。两个节点足够近的时候用一条边连接(&lt; 0.6纳米)。目标是将每种蛋白质归类为<strong class="la iu">酶</strong>或<strong class="la iu">而非</strong>。</p><p id="c1f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">酶是一种特殊类型的蛋白质，作为催化剂加速细胞中的化学反应。它们对消化(如脂肪酶)、呼吸(如氧化酶)和人体的其他重要功能至关重要。它们也用于商业应用，如抗生素的生产。</p><p id="9b98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该数据集也可在TUDataset上获得，并在PyTorch Geometric中实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="a3e7" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">Dataset: PROTEINS(1113)<br/></strong>----------------------<br/>Number of graphs: 1113<br/>Number of nodes: 23<br/>Number of features: 3<br/>Number of classes: 2</span></pre><p id="d271" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我不是生物化学家，所以我对这些蛋白质很好奇。让我们绘制一个图表，看看它看起来像什么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/fa5ebca3717e9f2e0b4f35def70fa521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gVrJo6SkZbVoFKZs76rr7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用matplotlib绘制蛋白质的3D图(图片由作者提供)</p></figure><p id="b201" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先前的3D结构是<strong class="la iu">随机生成的</strong>:获得正确的3D表示是一个非常困难的问题，这是<a class="ae mi" href="https://alphafold.ebi.ac.uk/" rel="noopener ugc nofollow" target="_blank"> AlphaFold </a>的全部要点。</p><p id="97a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图形不是表示分子的唯一方式。简化的分子输入行输入系统(<strong class="la iu">微笑</strong> ) 是另一种流行的方法，它使用行(串)符号。它是通过打印在稍微修改的分子图的深度优先树遍历中遇到的节点而获得的。</p><p id="445b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">研究人员在研究分子或化合物时经常使用这种表示法。对我们来说幸运的是，蛋白质数据集已经以图表的形式进行了编码。否则，我们可能不得不将微笑字符串转换成<code class="fe np nq nr nf b">networkx</code>图形。</p><p id="ac36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这并不意味着我们将直接向GNN提供蛋白质数据集。如果说<a class="ae mi" href="https://mlabonne.github.io/blog/graphsage/" rel="noopener ugc nofollow" target="_blank"> GraphSAGE </a>教会了我们什么的话，那就是<strong class="la iu">迷你批处理非常高效</strong>。现在，每当我们实施GNN时，它都是不可或缺的工具。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="e3c7" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">Training set</strong>   = 890 graphs (14 subgraphs)<br/><strong class="nf iu">Validation set</strong> = 111 graphs (2 subgraphs)<br/><strong class="nf iu">Test set</strong>       = 112 graphs (2 subgraphs)</span></pre><p id="efa3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">蛋白质并不是一个庞大的数据集，但是小批量将会加快训练的速度。我们可以使用GCN或GAT，但是我想介绍一种新的架构:图同构网络。</p><h1 id="cafa" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🍾二。图形同构网络</h1><p id="b0cd" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">杜松子酒是由研究人员设计的，他们试图最大限度地发挥GNN的代表性(或辨别力)。但是你如何定义“代表力”？</p><h1 id="d4a0" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">A.魏斯费勒-雷曼试验</h1><p id="712f" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">表征GNN的“能力”的一种方式是使用魏斯费勒-雷曼(WL)图同构测试。<a class="ae mi" href="https://en.wikipedia.org/wiki/Graph_isomorphism" rel="noopener ugc nofollow" target="_blank">同构的图</a>意味着它们具有<strong class="la iu">相同的结构</strong>:相同的连接但是节点有排列。WL测试能够判断两个图是否同构，但不能保证它们同构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/33b52dfea2da8af961e27fe4685b9100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8OQ8kcwzuYT0_h_ukENcNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个同构的图(图片作者)</p></figure><p id="8fe1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这可能看起来不多，但是要区分两个大图是非常困难的。其实这个问题不是已知的<strong class="la iu"> </strong>在多项式时间内可解，也不是NP完全的。它甚至可能介于两者之间，处于计算复杂性类别<a class="ae mi" href="https://en.wikipedia.org/wiki/Graph_isomorphism_problem" rel="noopener ugc nofollow" target="_blank">NP-中级</a>(如果它只存在的话)。</p><p id="f4b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好吧，但这和GNNs有什么关系？一些图形学习的研究人员注意到<strong class="la iu">这个测试和GNNs学习的方式奇怪地相似</strong>。在WL测试中，</p><ol class=""><li id="6731" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt ny ma mb mc bi translated">每个节点都以<strong class="la iu">相同的标号</strong>开始；</li><li id="ace8" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ny ma mb mc bi translated">来自相邻节点的标签被聚集并且<strong class="la iu">被散列</strong>以产生新的标签；</li><li id="97f6" class="lu lv it la b lb md le me lh mf ll mg lp mh lt ny ma mb mc bi translated">重复上述步骤，直到标签<strong class="la iu">停止变化</strong>。</li></ol><p id="fafe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对WL测试感兴趣，我会推荐<a class="ae mi" href="https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/" rel="noopener ugc nofollow" target="_blank">这篇大卫·比伯的博客文章</a>和<a class="ae mi" rel="noopener" target="_blank" href="/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49">这篇迈克尔·布朗斯坦的文章</a><a class="nz oa ep" href="https://medium.com/u/7b1129ddd572?source=post_page-----3d18b07a6e66--------------------------------" rel="noopener" target="_blank"/>。</p><p id="3156" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个测试不仅类似于GNNs中如何聚集特征向量，而且它区分图形的能力使它比许多架构更强大，包括GCNs和GraphSAGE。这启发了<a class="ae mi" href="https://arxiv.org/abs/1810.00826v3" rel="noopener ugc nofollow" target="_blank">徐等人</a>设计了一种新的聚合器，他们证明这种聚合器与测试一样好。</p><h1 id="4f06" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">B.一个聚合器来管理它们</h1><p id="062b" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">为了和WL测试一样好，这个新的聚合器在处理非同构图形时必须产生不同的节点嵌入。</p><p id="844d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将跳过这篇论文的数学部分，但是他们找到的解决方案是使用两个内射函数。哪些？我们不知道，我们可以用MLP学习它们！</p><ul class=""><li id="e1e6" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">对于GATs，我们使用神经网络来学习给定任务的<strong class="la iu">最佳加权因子</strong>；</li><li id="7572" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">有了GINs，由于<a class="ae mi" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>，我们现在学习两个内射函数的<strong class="la iu">逼近。</strong></li></ul><p id="0187" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是如何用GIN计算特定节点<em class="ob"> i </em>的隐藏向量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a62f1ac97f63e58e8e5c47a747240998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jK-37UeXyUo6l0lnMuCpjA.png"/></div></div></figure><p id="1d7b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个公式中，ɛ确定了目标节点相对于其邻居的<strong class="la iu">重要性(如果ɛ = 0，它具有相同的重要性)。它可以是可学习的参数，也可以是固定的标量。</strong></p><p id="3192" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们谈论MLP是为了强调存在不止一层的事实。根据作者的说法，一层对于一般的图形学习来说是不够的。</p><h1 id="e093" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">C.全球统筹</h1><p id="7167" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">全局池或图形级读出包括使用由GNN计算的节点嵌入产生一个<strong class="la iu">图形嵌入</strong>。</p><p id="4e71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">获得图嵌入的一个简单方法是使用嵌入<em class="ob"> hᵢ </em>的每个节点的<strong class="la iu">平均值</strong>、<strong class="la iu">总和</strong>、<strong class="la iu">、</strong>或<strong class="la iu">最大值</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/dec697dd076f86ccdc25ef6659f7c805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KNE3dcyTcT1lHyXmRYp0qA.png"/></div></div></figure><p id="97bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者提出了关于图形级读出的两个要点:</p><ul class=""><li id="0ac0" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">为了考虑所有的结构信息，有必要<strong class="la iu">保持来自先前层的嵌入</strong>；</li><li id="5a73" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">sum运算符令人惊讶地比均值和最大值更有表现力。</li></ul><p id="cec5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据这些观察，他们提出了以下全球统筹方法:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/215917a1497f763698ceac4ce9a8cdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8_D9BLSSWuE8DMVvWviOg.png"/></div></div></figure><p id="4808" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每一层，节点嵌入被<strong class="la iu">求和</strong>，并且结果被<strong class="la iu">连接</strong>。该解决方案将sum运算符的表达能力与串联中先前迭代的记忆结合起来。</p><h1 id="f58a" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">🧠三世。PyTorch几何形状的杜松子酒</h1><p id="0e1b" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">看到原始设计和它的实现之间的差异总是很有趣的。</p><p id="11a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PyTorch几何图形中有一个不同参数的<code class="fe np nq nr nf b">GINConv</code>层:</p><ul class=""><li id="0c37" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe np nq nr nf b">nn</code>:用于逼近我们两个内射函数的<strong class="la iu">MLP</strong>；</li><li id="290c" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe np nq nr nf b">eps</code>:ɛ的初始值，默认为<strong class="la iu">0</strong>；</li><li id="84cb" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe np nq nr nf b">train_eps</code>:判断ɛ是否可训练的真/假语句，默认为<strong class="la iu">假</strong>。</li></ul><p id="2b70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以看到，在这个实现中，ɛ被默认地完全删除了:它是一个我们可以调优的超参数，但可能不是一个必需的参数。</p><p id="78ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PyTorch几何里有一个<strong class="la iu">二轧棉层</strong>，叫<code class="fe np nq nr nf b">GINEConv</code>。它来自于GIN的<a class="ae mi" href="https://arxiv.org/pdf/1905.12265.pdf" rel="noopener ugc nofollow" target="_blank">本文的实现</a>，它将一个<em class="ob"> ReLU </em>函数应用于邻居的特征。我们不会在本教程中使用它，因为它的好处还不清楚。</p><p id="1926" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们仍然需要为<code class="fe np nq nr nf b">GINConv</code>层设计一个MLP。以下是我们将实施的设计，灵感来自原始文件:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/45f1890683796ad4df0d29446e6bb6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3dRGZWsMt7evfFOQKaqbHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLP用于轧棉层(图片由作者提供)</p></figure><p id="55b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这张纸叠了5层，但我们会用3层来代替。这是整个架构的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/f8e37569653d6e2c4228e9aec99e91a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oU83LfFGE4xYwPLliMOSiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的杜松子酒建筑(作者图片)</p></figure><p id="f026" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我找不到任何带有图嵌入<strong class="la iu">连接</strong>的GIN实现，所以这里是我的版本(它平均提高了1%的精度)。让我们将其与具有简单平均池(且没有串联)的GCN进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="ac8b" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">GCN </strong>test accuracy = <strong class="nf iu">59.38%</strong><br/><strong class="nf iu">GIN </strong>test accuracy<strong class="nf iu"> </strong>= <strong class="nf iu">73.70%</strong></span></pre><p id="5243" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，没有竞争！</p><p id="a0d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GIN架构的<strong class="la iu"> </strong>完全胜过GCN。这种差距(平均10%的准确度)是由几个原因造成的:</p><ul class=""><li id="e25a" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">GIN的聚合器是专门设计来<strong class="la iu">鉴别GCN的聚合器不能鉴别的图形</strong>；</li><li id="8483" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">来自每一层的图形隐藏向量被<strong class="la iu">连接，而不是仅考虑最后一层的</strong>；</li><li id="3a33" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">求和算子<strong class="la iu">优于均值算子</strong>(至少在理论上)。</li></ul><p id="103a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们想象一下我们用GCN和杜松子酒分类的蛋白质。</p><div class="kj kk kl km gt ab cb"><figure class="of kn og oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/1f0882452844433965f878175525088d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*jALpsWj0oIj-PP4ceInwwQ.png"/></div></figure><figure class="of kn og oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/82946aa62c8a2fe28b5a9aa0a759203c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*yS9xLSChYVQC79NHZ-Wu8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ol di om on translated">作者图片</p></figure></div><p id="defa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，这两个模型犯了<strong class="la iu">不同的错误</strong>。当不同的算法应用于同一问题时，这是机器学习中的一个常见结果。</p><p id="2fc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过创建一个<strong class="la iu">合奏</strong>来利用这种行为。有许多方法可以组合我们的图形嵌入。最简单的方法是取归一化输出向量的平均值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="ea39" class="nj mk it nf b gy nk nl l nm nn"><strong class="nf iu">GCN </strong>test accuracy     = <strong class="nf iu">59.38%</strong><br/><strong class="nf iu">GIN </strong>test accuracy<strong class="nf iu"> </strong>    = <strong class="nf iu">73.70%</strong><br/><strong class="nf iu">GCN+GIN </strong>test accuracy<strong class="nf iu"> </strong>= <strong class="nf iu">75.00%</strong></span></pre><p id="55db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，我们很幸运地看到<strong class="la iu">精度提高了</strong>。</p><p id="c209" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">显然，并不总是这样。更复杂的方法包括为分类建立一个完全不同的ML算法，比如随机森林。该分类器将图嵌入作为输入，并输出最终的分类。</p><h1 id="6980" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="1afc" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">图同构网络是理解GNNs的重要一步。</p><p id="ea7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它们不仅提高了几个基准测试的准确度，还提供了一个理论框架来解释为什么一个架构比另一个更好。在这篇文章中，</p><ul class=""><li id="3b25" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">我们看到了一个带有<strong class="la iu">图分类</strong>的新任务，使用全局池执行；</li><li id="e9d8" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我们介绍了<strong class="la iu"> WL测试</strong>及其与新GIN层的联系；</li><li id="9939" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我们实现了一个杜松子酒和一个GCN，并用它们的分类做了一个简单的合奏。</li></ul><p id="2173" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管GINs取得了很好的性能，特别是在社交图方面，但它们的理论优势在现实世界中并不总是很好。其他<a class="ae mi" href="https://arxiv.org/pdf/2003.00982.pdf" rel="noopener ugc nofollow" target="_blank">“可证明强大的”架构</a>也是如此，它们在实践中往往<strong class="la iu">表现不佳</strong>，比如3WLGNN。</p><p id="dbc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你喜欢这篇文章，请在推特上关注我，获取更多的图片内容。📣</p><h1 id="f346" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="2e62" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">[1]克里斯多夫·莫利斯和尼尔斯·m·克里格和弗兰卡·鲍斯和克里斯蒂安·克尔斯汀和佩特拉·穆策尔和马里恩·诺依曼。TUDataset:一个用于图形学习的基准数据集集合。在<em class="ob"> ICML 2020年关于图形表示学习和超越的研讨会</em>。</p><p id="226e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]徐、柯玉禄、胡、、莱斯科维克、朱雷、杰格尔卡、。图形神经网络有多强大？<em class="ob"> </em>在<em class="ob"> ICLR 2019 </em>。</p><h1 id="5884" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">相关文章</h1><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/introduction-to-graphsage-in-python-a9e7f9ecf9d7"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">Python中的GraphSAGE简介</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">将图形神经网络扩展到数十亿个连接</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/graph-attention-networks-in-python-975736ac5c0c"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">图形注意网络:自我注意的解释</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">使用PyTorch几何图形的带自我注意的GNNs指南</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pg l pc pd pe pa pf ks or"/></div></div></a></div></div></div>    
</body>
</html>