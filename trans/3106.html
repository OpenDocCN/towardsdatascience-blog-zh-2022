<html>
<head>
<title>The Deep Connection Between Boosting and LASSO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">助推和套索的深层联系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-deep-connection-between-boosting-and-lasso-b40c00fc71c6#2022-07-07">https://towardsdatascience.com/the-deep-connection-between-boosting-and-lasso-b40c00fc71c6#2022-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="058f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过彻底的视角转换理解强大的算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4701b1166bddc30dcd5c351d529b97fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9SJ--pW25cKm1pXD"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(来自<a class="ae kv" href="https://unsplash.com/photos/7hwF_T-1YPg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的<a class="ae kv" href="https://unsplash.com/@its_tgain" rel="noopener ugc nofollow" target="_blank">汤姆·盖诺尔</a>摄影)</p></figure><p id="82b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度推进既被普遍使用，又被普遍低估。</p><p id="6973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当讨论其有效性时，人们通常默认两种观点:</p><ol class=""><li id="d3d3" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">梯度提升从低方差和高偏差开始，并通过添加更多弱学习者来减少偏差</li><li id="91e8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">梯度推进近似于函数空间中的“梯度下降”,并试图最小化该空间中的损失</li></ol><p id="c093" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两种观点都很有趣，非常有价值！然而，它们并没有讲述整个故事；<strong class="ky ir">特别是，他们努力解释<em class="mg">为什么</em>梯度推进如此有效。</strong></p><p id="b101" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过将增强(特别是具有固定小权重的梯度增强)重新表述为近似套索(L1正则化)问题，我们可以对增强的作用有另一种理解，并将其与内核化方法等其他基本技术进行比较。我们为什么关心？找到不同技术的机制之间的共性可以帮助我们提出全新的算法！</p><p id="bfb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们通过扩展Rosset等人在<a class="ae kv" href="https://www.jmlr.org/papers/volume5/rosset04a/rosset04a.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>中讨论的一些观点来进行这种重新表述。以一种读者友好的方式。(注意:我将使用<em class="mg">树</em>作为原子模型的选择，但是许多相同的想法适用于各种基础学习者。)</p><h1 id="6557" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">概述:功能梯度下降</h1><p id="5a15" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">升压的一个常见类比是函数空间中的梯度下降。代替迭代更新n维参数向量，我们现在迭代更新参数<em class="mg">函数</em>(其属于潜在的无限维函数空间。)</p><p id="e1dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们快速回顾一下这个类比。现在，我们有一个以参数函数作为自变量的成本函数，而不是参数向量上的成本函数(函数上的函数称为“泛函”):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5cb95d10b9227e70c839f7b180eff882.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*gGCZld3DFFU06L8P8axyPA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:提升问题的成本函数(图片由作者提供)</p></figure><p id="da9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，花哨的“L”是我们传统的建模损失函数，它将真实标签和我们的模型(其中我们的模型由函数“f”表示)分离的输出作为参数。</p><p id="6bef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就像计算参数向量的梯度一样，也可以计算参数<em class="mg">函数</em>的梯度。结果也是一个函数(正如参数向量的梯度也是一个向量)，我们可以从参数函数中减去它来执行“下降”步骤。</p><p id="6b98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将省去实际梯度的求导；更多详情，请参见亚历克斯·格拉布和萨曼莎·霍瓦特的精彩课堂笔记。只需知道，得到的梯度是一个“尖峰”函数，当在训练点x{i}上评估时，它吐出当前预测的“残差”(如果损失函数“L”是平方损失，则为真残差，否则为伪残差)，当在其他地方评估时，吐出零。以下是梯度函数和结果更新的可视化效果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/25dea4d9c7e2487e72429ad59a5f80b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nM_j7c2zZb3184czDaI0Qw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:使用来自图1的成本函数的梯度进行更新(图片由作者提供)</p></figure><p id="a44b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将这个原始梯度添加到我们的当前函数将导致严重的过度拟合，本质上使结果成为一个存储我们训练数据上的目标的查找表。为了解决这种缺乏一般化的问题，我们可以通过在约束函数类(例如决策树)中找到一个与梯度“接近”的函数来“平滑”这种梯度，同时产生更一般化的结果。我们修改后的更新变成了我们熟悉的梯度增强更新:从我们的函数类中找到一个“接近”真实梯度的函数，并以较小的权重将其添加到我们当前的模型中。</p><p id="701e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">优雅的东西。然而，一个关键问题仍然存在:</p><p id="d8e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">如果弱学习者在拟合梯度方面做得很差(根据定义)，为什么推进会如此有效？</strong></p><p id="524f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，这就是高级功能梯度下降类比的失败之处(更详细的版本可能会更好。)这里有<em class="mg">和</em>其他解释提升能力的方法，但是LASSO公式既易于理解，又非常有助于将提升与其他方法如支持向量机进行比较。</p><p id="5f16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解这个公式，它有助于完全转移我们对增压问题的观点。让我们透过向量代数的镜头来看问题吧！</p><h1 id="ce12" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">提升树的向量空间</h1><p id="1ee3" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">当人们想到向量时，他们通常会想到欧几里得空间中的尖箭头。</p><p id="32ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，“向量”和“向量空间”的概念要抽象得多。</p><p id="c813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个空间成为向量空间的唯一要求是，增加和缩放向量会在该空间内产生另一个向量(您还需要存在一个“零”向量，当添加到另一个向量时，它什么也不做。)</p><p id="2d4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着许多，<em class="mg">许多</em>集合都是向量空间。所有多项式的集合是一个向量空间(使用规则验证这一点！)所有2x2矩阵的集合是一个向量空间。</p><p id="5ffc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">重要的是，所有提升的树的集合是一个向量空间。</strong></p><p id="ddf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们将“提升树”定义为具有固定深度D(其中D由我们选择)的决策树的有限加权和(即线性组合)，那么很明显，任何提升树的线性组合都是<em class="mg">自身</em>提升树。</p><p id="b8cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您可以通过一组有限的向量(轴向量)的线性组合来重新创建所有欧几里得空间一样，您也可以通过单棵树的线性组合来重新创建所有增强的树空间！如果我们只允许在观察到的数据点上或数据点之间进行树分裂，那么任何深度为D的单独的树都可以写成深度为D的有限多个“构建块”树的线性组合。</p><p id="f180" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来研究一种方法。请注意，如果我们只允许在观察到的数据点上或数据点之间进行分割，那么我们只能生成有限多个分区(即分割的组合)。然后，我们可以将一个给定分区的叶值的任意组合写成该分区上简单的“独热”树的线性组合。以下是使用两个特征维度X1和X2的深度为2的树的分解示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/07ada5694851609a6c7846bd9e54dbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vLXaHcAL_DCW_f-v_jnDNw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:分解为线性组合的深度为2的树(图片由作者提供)</p></figure><p id="e12f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们可以将任何深度为D的树写成“积木”树的线性组合，那么我们可以<em class="mg">也可以</em>将任何深度为D的提升树表示成线性组合，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/450b86fbe8260cf915e3c8cd12455b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTfNXW2xKJUZq8NyjZ-x5g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:作为个体树的线性组合的增强树(由作者提供的图像)</p></figure><p id="271b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">左边的公式看着眼熟吗？如果这看起来像线性回归，那么你就有所发现了！理论上，我们可以将寻找一个好的加性树集合的问题简化为对一组生成所有可能的提升树的空间的树的<strong class="ky ir">线性回归。但是，如果我们天真地试图建立一个回归模型，我们会遇到两个主要问题:</strong></p><ol class=""><li id="474a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们需要评估和使用大量的树作为特征。对于很多常见的数据集，在已知的宇宙中，唯一的树比夸克还多。</li><li id="c058" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><em class="mg">甚至</em>如果我们可以修复#1，我们将拥有比数据点多很多倍的<em class="mg"> </em>特征，并且将需要约束我们的优化问题或者添加复杂度惩罚以达到有用的优化。</li></ol><p id="4025" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于第二点，“拥有比数据点多很多倍的特征”正是LASSO能够解决的问题！</p><h1 id="32cf" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">增强近似于套索调谐</h1><p id="1c3f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">让我们考虑一下，使用有限的“积木”函数，LASSO回归是什么样子的。(注意，LASSO通常使用平方损失来定义，但我将把定义推广到使用任何凸损失c。)</p><p id="a6de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们评估所有数据点上的每个函数，以在我们的特征矩阵x中创建列。现在我们有了我们的特征，我们解决以下LASSO回归优化问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/02c761a50596d7596106ae186e59ea83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*ydNYvW0SJiKXA64GaTUN1w.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:特征函数F1，F2，…，FM的套索回归问题(图片由作者提供)</p></figure><p id="979f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个定义(用L1惩罚使损失最小化)可能为许多人所熟悉。不太熟悉但仍被普遍使用的是下面的套索的<strong class="ky ir"> L1约束公式:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/78f92df0bcece628c74d936038daa16f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*uaalwHzzgmcWOshZ3ehktg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:图5中问题的等价重写(作者的图片)</p></figure><p id="e470" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于图5中的每个λ值，在图6中存在唯一的T值，其恢复相同的解，反之亦然(其中增加的T映射到减小的λ，直到λ = 0，此时任何更大的T值恢复相同的解:非正则解。)</p><p id="65af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们快速展示为什么每个λ映射到一个唯一的T(另一个方向稍微复杂一点，使用拉格朗日对偶的论点可以在网上找到。)我将把这个证明自包含在下面的编号列表中，这样就可以很容易地跳过它:</p><ol class=""><li id="9d05" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">给定λ值，图5中优化问题的解对应于向量β，该向量β具有比对应于非正则化解(λ = 0)的范数小的某个有限L1范数N。</li><li id="97d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">现在，考虑图6中T = N的约束优化问题。图6中T = N的解不能具有小于N的L1范数，因为这将是约束集内的局部最优解(与我们的凸目标只有一个局部最优解的事实相矛盾)。)因此，图6中T = N的解具有正好为N的L1范数</li><li id="9c77" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">由于图6中T = N的解决方案在所有L1范数等于N的解决方案中最小化了图6的目标，所以它也必须在所有L1范数为N的解决方案中最小化图5中给定λ的目标</li><li id="578d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">因此，T = N的图6的解等于给定λ的图5的解，因为后一个解也具有N的L1范数</li><li id="4f3b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">图6中T &lt; N or T &gt; N的任何值都不能恢复相同的解，因为有一个不是N的范数(参见#2中的论证)。)因此，映射λ → T = N是唯一的。</li></ol><p id="8963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，解决图6中的L1约束优化问题，对于T的许多连续较大的值，解决了λ的许多连续较小的值(范围从λ = ∞到λ = 0)的套索优化问题。)</p><p id="31e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这对我们有什么帮助？事实证明，提升——更具体地说，<em class="mg">贪婪ε</em>提升(即，在我们的回归特征集合中执行最佳ε大小的系数更新)——对于t的连续较大值，非常接近L1约束优化问题的“解序列”</p><p id="1155" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，基于T和λ之间的对偶性，这意味着boosting解决了具有大范围λ值的多个套索问题，当我们找到导致良好验证性能的正则化水平时终止(这是实践中的“早期停止”)。换言之，提升在我们的非常高维的特征空间中有效地近似套索超参数调谐。</p><p id="29ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解为什么提升等价于一系列L1约束回归问题，考虑下面的增量优化问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/31638702588d114b83863bf5a456b617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*7R3PALpjI6cMcbuKLEBVBQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7: β_last是T (T_last)的某个值的图6的解(图片由作者提供)</p></figure><p id="5913" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，β_last是图6中的L1约束问题的解，具有T (T_last)的某个值。现在，我们想找到一个稍微宽松的L1约束的最优解，也就是说，T的值稍微大一点——这就是问题的“增量”本质。</p><p id="0b52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设图7中问题的解是“单调”更新，即解的每个坐标的绝对值大于或等于β_last的绝对值(换句话说，“单调性”意味着更新在每个坐标上增加了离原点的距离。)最佳更新实际上<em class="mg">是</em>单调更新在实践中是常见的，特别是对于小的ε值。(我们将在本节的后面回到<em class="mg">为什么</em>我们需要单调性。)</p><p id="240c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的几段中，我将使用图7中的优化约束来帮助我们理解这个“单调最佳更新”<em class="mg">实际上看起来像什么</em>。</p><p id="dddd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在—如果图7中的最佳更新是单调的，那么当您在图7中的现有约束之上添加新的单调性约束时，它仍然是最佳解决方案，因此我们可以在不改变最终解决方案的情况下添加该单调性约束。这两个组合约束等价于下面的约束集(“等价”在创建相同的可行β集的意义上):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f9f2a9f6e3e2bde7078fa362545cf990.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*APxJb_09tkrZ90-t7yzaFA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:β上新的等价约束集(作者图片)</p></figure><p id="394e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(您可以通过使用单调性的定义以及涉及算术和绝对值的不等式来展示这种等价性。这次我省略了证明，但是请随意检查我的证据！)</p><p id="aaf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们在图8中的约束对下恢复我们的单调最佳更新，那么如果我们丢弃第二个约束并且在第一行中仅使用L1邻域约束，我们也将恢复它。(注意，在该奇异L1邻域约束下可行的所有β在图7中的原始约束下也是可行的，因此我们永远不会“超越”原始约束。)</p><p id="0920" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们剩下的是一个约束，约束我们的更新在L1距离上接近前面的参数β_last。<strong class="ky ir">换句话说，如果图7中原始问题的最优解是单调更新，那么我们也可以通过以β_last为中心的局部优化问题来恢复它。</strong></p><p id="e696" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们快到了！让我们使用上面的新约束重写图7中的优化问题。此外，如果我们在β_last <strong class="ky ir"> </strong>周围的局部邻域足够“小”并且我们的成本函数C是平滑的，那么我们可以通过使用其梯度用线性近似来替换目标函数(我们在图7的花括号内最小化的“成本之和”函数)。对约束和目标函数进行这些替换产生了这个最终的优化问题，只要图7中的最佳更新是单调的，就恢复与图7相同的最佳更新:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/429bc565295e4a5275d2dfffc8eb5cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*x8KaSp6KTIoC-n9JxPXiOA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:“Obj”是图7中花括号内的“成本总和”目标函数(图片由作者提供)</p></figure><p id="022e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于目标是线性的，那么很容易看出这个约束优化问题的解决方案是仅改变目标在β_last的梯度具有最大绝对值的β的坐标，如果符号是负的则加上ε，如果符号是正的则减去ε(注意，根据定义这将是单调更新，因为我们假设<em class="mg">最佳</em>更新本身是单调的。)</p><p id="d1e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这听起来熟悉吗？在每一步，我们只更新β的最“有影响”的坐标；<strong class="ky ir">这相当于在最“有影响力”的特征函数(其系数就是β </strong>的所说坐标)上迈出了一小步。)</p><p id="ed16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">概括一下:图7中的优化问题是图6中优化问题的增量版本。如果我们针对某个T值求解图6，并且针对稍大的T值的解是单调更新，那么我们通过贪婪的ε提升来恢复它。只要每个增量问题通过单调更新得到解决，那么<strong class="ky ir">我们在一系列推进迭代下追踪出整个套索解序列。</strong>在这种情况下，boosting的每次迭代都解决了λ值稍小的套索问题。</p><p id="0627" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们提到了单调性很重要。这是因为如果违反了单调性，那么在我们的推进解序列和套索解序列之间就不再有完美的1:1对应关系。这是因为违反单调性意味着我们有效地“重新访问”并更新具有较小L1范数的解，而不是在该迭代上求解具有递增的较大L1范数(对应于递增的较小λ)的解。然而，在这种情况下，增强通常非常接近套索序列<em class="mg">甚至</em>。检查来自Efron等人的论文中的<a class="ae kv" href="https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-2/Least-angle-regression/10.1214/009053604000000067.full" rel="noopener ugc nofollow" target="_blank">下图。a1在糖尿病数据集上比较LASSO系数序列和boosting序列:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/fa687419831f2b41c5afcaf84045f3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*uQOSjHFRek3_XN78CeQmjw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10:在糖尿病数据集上，套索系数对增强(阶段式)系数。(图片由Efron等人提供。阿尔(2004年))</p></figure><p id="1419" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在该图中，LASSO和boosting(阶段式拟合)仅在原始特征上完成，而不是在功能特征上，但是紧密的对应关系仍然很能说明问题。当变量7(灰色线)违反单调性时，变量8之后会出现差异(见黑色箭头和红色线)，但这种差异很小，随着迭代次数的增加，似乎几乎可以解决。这里需要注意的是，这种“对违反单调性的鲁棒性”还没有在非常高的维度上测试过；然而，将助推与套索联系起来的基本原则依然存在。</p><h1 id="2434" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">放在一起:树推进</h1><p id="361b" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们讨论了如何发现最佳加性系综的问题通常可以写成在一组“构件”函数(对应于回归中的特征)上的线性回归问题。)参见图3中使用决策树的例子。</p><p id="0c3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本节中，我们将快速讨论使用我们的“构建块”树的epsilon greedy boosting更新是什么样子，以及它与经典树提升的比较。</p><p id="e1f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回想一下，我们通过使用目标函数相对于每个系数的梯度来确定更新哪个“构建块”系数(参见围绕图9的讨论)。)我们可以通过链式法则重写目标函数相对于β的每个坐标的梯度，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6be2406a864c8ee5ccf52a80dbfabbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*7-2aK8uRRGIlxXQipbReEw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:用链式法则求目标函数w.r.t. β_i的梯度(图片由作者提供)</p></figure><p id="3fd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回想一下，F_hat是通过在所有数据点上评估我们的加性系综(在任何非线性变换之前)而生成的输出向量。)图11右侧的一阶偏导数是当我们进行传统的梯度增强时，我们在每次迭代中“拟合”的残差/伪残差。二阶偏导数是引入对系数向量β的依赖性的元素。这是这个偏导数的完全展开:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b13949e000d4a4ff4fc26a5e05c0eb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*k1K55Q86tHlAc3V88dEZYA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12:相对于单个坐标扩展梯度(作者图片)</p></figure><p id="6c66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该图的推导源于这样一个事实，即原始向量F_hat只是在每个点上评估的函数的线性组合，因此关于单个系数的导数隔离了由该系数加权的分量函数。</p><p id="9360" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们的“构造块”函数是如图3所示的“独热树”，那么对于落入对应的独热树F_i的“活动叶”的所有x，图12中的向量元素等于1，否则为零。然后，图11中的产品可以重写如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6eafc2cf4cff7d798603966e8a83424a.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*hM0DJWmne5p3Fb4kokSyCA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图13:使用“独热树”重写图11(作者图片)</p></figure><p id="2d03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，<strong class="ky ir">找到梯度最大(绝对)的β的坐标，等价于找到残差/伪残差之和最大(绝对)的划分叶！</strong></p><p id="0860" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，传统的树增强并不像<em class="mg">那样工作。不是一次更新一片叶子(通过一个热树)，我们会找到一个最佳的<em class="mg">分区</em>并更新对应于该分区的所有</em>叶子。此外，在传统的树提升中，我们也不局限于epsilon大小的更新；相反，我们进行与<em class="mg">梯度</em>的大小成比例的更新，由ε加权以确保任何单独的更新不会太大。</p><p id="02d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，尽管两种范式之间存在差异，但“最优更新”背后的驱动力非常相似。如果我们将由传统树提升生成的集成映射回我们的独热树坐标系中的权重，那么我假设由具有相似系数L1范数的独热树提升生成的集成表现大致相似。对这两者进行比较的模拟将是这一讨论的一个非常有趣的后续。</p><h1 id="a7f2" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">与内核化方法的比较</h1><p id="50a6" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">boosting和kernelized方法都声称能够适应任何情况。在传统的ML世界中，两者都是“黑箱”。那么一个比另一个有什么优势呢？</p><p id="5b99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回想一下，一个内核化的方法是任何使用“内核技巧”来适应模型的方法。我不会在这里讨论这个概念背后的数学；只需知道，通过使用“内核技巧”，我们可以以很少的额外成本利用许多高度非线性的特性(通常在传统的线性模型中)。</p><p id="2b76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于内核化方法，我们通常在新的“内核”特征空间中拟合线性模型。下面是一个内核化线性回归的示例，优化目标扩展到包括隐式内核特性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/88d31a53ce1ceb6f433dd21591d385b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*X0V8lbSreh4q8yZo6JCjDQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图14:用隐式核特征扩展的核化线性回归目标(作者提供的图像)</p></figure><p id="544e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，这个问题看起来与图5中的升压问题非常相似！</p><p id="3c35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很快，我们可以看到内核化方法的一个主要缺点:<strong class="ky ir">我们对ɸ的隐式内核特性没有多少选择</strong>。</p><p id="76e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在boosting中，我们可以自由选择我们喜欢的任何函数集，只要我们可以在任何迭代中有效地评估“最佳方向”。对于树提升，找到这个“最佳方向”就像在每一步拟合一个与残差/伪残差“相关”的树一样容易。</p><p id="f27b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">内核化方法的另一个缺点是<strong class="ky ir">我们被锁定在某些先验中，也就是说，某些特征会自动受到比其他特征更重的惩罚</strong>。</p><p id="9dad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数内核化方法采用某种形式的正则化来处理过拟合和过参数化(图14是内核化脊回归的例子，即L2正则化。)并且众所周知，在采用L1/L2正则化和相关方法之前，您需要缩放您的特征；否则，“较小”的特征将比“较大”的特征受到更严重的惩罚。</p><p id="fe06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，内核化的方法使得<em class="mg">不可能</em>让你可靠地保持最终的“内核特性”不变！</p><p id="fc4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑多项式核。多项式核中的某些项具有相对较大的范围-想想(1 + xy)以及乘积项“xy”如何具有系数2，这使得最终1次要素的系数为√2，而最终2次要素的系数较小，为1。结果，当加入正则化时，我们被锁定在不成比例地惩罚某些项，这对于所谓的“通用”算法可能是不期望的。</p><p id="a586" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不言而喻，boosting不会遇到这个问题，因为我们可以完全控制我们的功能。</p><p id="046a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，<strong class="ky ir">广泛使用的内核化方法需要花费更多的时间来进行(正则化)超参数调整。</strong>这个<strong class="ky ir"> </strong>不可否认地不是这些方法的<em class="mg">固有的</em>问题，因为迭代解经常存在，在单个模型拟合中自动追踪每个超参数值的解。然而，在<em class="mg">练习</em>中，练习者通常会做的是，每次使用不同的超参数值对内核化模型进行多次改装。</p><p id="d56f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，我们已经探索了boosting和LASSO之间的联系，以及greedy epsilon boosting如何像LASSO的高效版本一样，在一次拟合中求解多个T/λ值，从而在多次boosting迭代中有效地描绘出整个“解的轨迹”。换句话说，通常实践的<strong class="ky ir">增强为我们提供了一种在<em class="mg">单一</em>拟合中的超参数调整形式。</strong>好看！</p><h1 id="efab" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结论</h1><p id="8b81" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">Boosting是一种极其强大的技术，但它远不像人们想象的那样是一个“黑箱”。虽然最终的结果可能无法解释，但驱动其部分力量的高层次思想甚至对于那些只上过线性模型课程的人来说也是可以理解的。</p><p id="78c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与神经网络形成对比，神经网络的基础理论仍在发展中，在数学上更加复杂。</p><p id="491a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总而言之，我希望这次深入探索能让你对你信赖的表格工具多一点点欣赏。优雅和力量的结合，没有什么比渐变更好的了！</p><h1 id="f7e4" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">参考</h1><p id="f733" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">[1] S. Rosset，J. Zhu，和T. Hastie，<a class="ae kv" href="https://www.jmlr.org/papers/volume5/rosset04a/rosset04a.pdf" rel="noopener ugc nofollow" target="_blank"> Boosting作为一个正则化的最大间隔分类器路径</a> (2004)，机器学习研究杂志5 941–973</p><p id="c603" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] A. Grubb和s .霍瓦特，<a class="ae kv" href="https://www.cs.cmu.edu/~16831-f14/notes/F11/16831_lecture15_shorvath.pdf" rel="noopener ugc nofollow" target="_blank">增强:函数空间中的梯度下降</a> (2012)，机器人学中的统计技术(16–831，F10)，第15讲(卡耐基梅隆大学)</p><p id="b66f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] B. Efron，T. Hastie，I. Johnstone和R. Tibshirani，<a class="ae kv" href="https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-2/Least-angle-regression/10.1214/009053604000000067.full" rel="noopener ugc nofollow" target="_blank">最小角度回归</a> (2004)，《统计年鉴》32(2):407–499</p></div></div>    
</body>
</html>