<html>
<head>
<title>A Visual Approach to Gradient Descent and other Optimization Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降和其他优化算法的可视化方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-visual-approach-to-gradient-descent-and-other-optimization-algorithms-c82f45b7fc87#2022-12-23">https://towardsdatascience.com/a-visual-approach-to-gradient-descent-and-other-optimization-algorithms-c82f45b7fc87#2022-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0c0a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可视化<strong class="ak">梯度下降</strong>、<strong class="ak">带动量梯度下降</strong>、<strong class="ak"> RMSprop </strong>和<strong class="ak"> Adam </strong>之间的异同</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/17cd16c0688bafee09b404a99b252e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wPV_xpk3ypjGHkDRFLNRqw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯汀·蒙克摄于 Pexels:<a class="ae ky" href="https://www.pexels.com/photo/photo-of-person-walking-on-unpaved-pathway-2599546/" rel="noopener ugc nofollow" target="_blank">https://www . Pexels . com/photo/photo-of-person-walking-on-unpaved-pathway-2599546/</a></p></figure><h1 id="9b07" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="33e1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你和我一样，方程不会自己说话。为了理解他们，<strong class="lt iu">我需要用一个具体的例子来看看他们是怎么做的。</strong>在这篇博文中，我将这一可视化原理应用于机器学习中常用的优化算法。</p><p id="41f4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如今，亚当算法是一个非常受欢迎的选择。Adam 算法将<strong class="lt iu">动量</strong>和<strong class="lt iu">学习速率的自调整</strong>添加到普通梯度下降算法中。但是动量和自调整到底是什么？</p><p id="bdcb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是这些概念所指内容的直观预览:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/210cee48861f221bcc9cf875ba4df8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*H8jafclns_xYV_gCtJzdVg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">几种优化算法的行为。来源:作者的计算</p></figure><h1 id="d2bb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">线性回归</h1><p id="d153" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了简单起见，我对二元线性回归模型使用了不同的优化算法:</p><p id="6736" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="mt"> y = a + bx </em></p><p id="d95b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">变量<em class="mt"> y </em>代表我们试图用另一个变量<em class="mt"> x </em>来预测/解释的一个量。未知参数有<strong class="lt iu">截距<em class="mt">a</em>T25】和<strong class="lt iu">斜率<em class="mt">b .</em>T29】</strong></strong></p><p id="b8d0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了使模型符合数据，我们最小化模型和数据之间的差异的均方，其可以简洁地表达如下:</p><p id="ae8c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="mt">损失(a，b)=1/m||y-a-bx|| </em></p><p id="bb7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">(假设我们有<em class="mt"> m </em>个观察值，并使用欧几里德范数)</p><p id="c9c7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过改变<em class="mt"> a </em>和<em class="mt"> b </em>的值，我们有望提高模型对数据的拟合度。使用二元回归模型，一件好事是我们可以将损失函数值绘制成未知参数<em class="mt"> a </em>和<em class="mt"> b </em>的函数。下面是损失函数的曲面图，黑点代表损失的最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/f6ad3ae3d9fb4469b4a41a2bcaf84552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWXWo-VdHZaGWq1MhNNdvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失函数 OLS。来源:作者的计算</p></figure><p id="3fdd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们也可以使用等高线图来可视化损失函数，其中线是水平集(使得<em class="mt">损失(a，b) = </em>恒定的点)。下面，白点代表损失函数的最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fb7e0146a5062c4d77969c5f39662e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCZL2KDLVUl5UdyatUMZsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等高线图损失函数 OLS。来源:作者的计算</p></figure><h1 id="d1e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降</h1><p id="3716" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">普通梯度下降算法包括在最陡下降的<strong class="lt iu">方向上采取大小为<strong class="lt iu"> η </strong>的步长，该步长由梯度的相反值给出。从数学上来说，更新规则如下:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/aa746fbd3560764eef5b5dfa9c64bd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*DQCbpYfBwGPW_kdzV0r3FA.png"/></div></figure><p id="b133" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在下一个图中，我展示了梯度下降算法隐含的一条轨迹。点代表迭代中<em class="mt"> a </em>和<em class="mt"> b </em>的值，而箭头是损失函数的梯度，告诉我们在下一次迭代中向哪里移动。</p><p id="2af5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个关键特征是梯度下降算法可能会在水平集之间产生一些振荡。在一个完美的世界中，我们会像<strong class="lt iu"> </strong>那样朝着最小值的方向平稳地移动。正如我们将看到的，增加动量是使轨迹向最小值平滑的一种方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/0f9ebb78f8c08abf76deebccd1090d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*IjX1yrDyLG_1XFiVBg6QBA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降。来源:作者的计算</p></figure><h1 id="59cf" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动量梯度下降</h1><p id="ccde" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">动量是指运动物体继续向同一方向运动的趋势</strong>。在实践中，我们可以通过考虑梯度的先前值来增加梯度下降的动量。这可以通过以下方式完成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/be21c239639fdecb9b8a06e88151b01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*nbofZ-Q9ojk8zHwqyyIHnw.png"/></div></figure><p id="25ba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> γ的值越高，</strong>当前更新中考虑的梯度的过去值越多。</p><p id="d79d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在接下来的图中，我展示了梯度下降算法暗示的轨迹<strong class="lt iu">(蓝色<strong class="lt iu"> ) </strong>和没有动量</strong>(白色)的<strong class="lt iu">。</strong></p><p id="8291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">动量减小了沿坡度系数值的波动。一旦动量平均效应开始发挥作用，大幅上下波动往往会相互抵消。结果，随着动量的增加，我们朝着真实值的方向移动得更快。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/b4519f01784d4ac5679bf5ff921ca691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rXu9JQmznGG2DBb03m6wIg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有动量(蓝色)或无动量(白色)的梯度下降。来源:作者的计算</p></figure><h1 id="dac5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">RMSprop</h1><p id="5888" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">动量是梯度下降的一个很好的转折。另一条改进路线是<strong class="lt iu">引入一个针对每个参数</strong>定制的学习率(在我们的例子中:一个斜率学习率，一个截距学习率)。</p><p id="917d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">但是如何选择这样一个系数特定的学习率呢？注意，前面的图显示梯度不一定指向最小值。至少在第一次迭代中没有。</p><p id="5f8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">直观地说，我们会给<strong class="lt iu">较小的权重</strong>给上下方向的移动，给<strong class="lt iu">较大的权重</strong>给左右方向的移动。RMSprop 更新规则嵌入了这个所需的属性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d22965b6ab1362f595635b3c4fa5f175.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*_OwhVVB06rJI4SyLVPSzfw.png"/></div></figure><p id="40a4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第一行仅仅定义了<strong class="lt iu"> g </strong>为损失函数的梯度。第二行表示我们计算梯度平方的移动平均值。在第三行中，我们在梯度给定的方向上迈出一步，但是通过过去梯度的移动平均值的平方根来重新调整。</p><p id="9e99" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们的例子中，因为斜率系数的梯度的平方往往很大，所以我们在那个方向采取小的步骤。截距系数则相反(小数值，大移动)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/a41161479f932e42b6ac0b5683ae4fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GosJuFIHEZEf0_aw9DbfWw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RMSprop(蓝色)和渐变下降(白色)。来源:作者的计算</p></figure><h1 id="bbb2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">亚当斯</h1><p id="9362" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">亚当优化算法具有<strong class="lt iu">动量</strong>，以及<strong class="lt iu">rms prop 的自适应学习速率</strong>。下面是<em class="mt">几乎是</em>亚当做的事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c735eb7e65c53199499759de5e279011.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*MwX8l8wDwrEYrarS_XxJbQ.png"/></div></figure><p id="3102" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更新规则与 RMSprop 非常相似。<strong class="lt iu">关键区别是动量:</strong>变化的方向由过去梯度的移动平均值给出。</p><p id="af71" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="mt">实际</em> Adam 更新规则使用<em class="mt"> m </em>和<em class="mt"> v </em>的“偏差修正”值。第一步，Adam 将<em class="mt"> m </em>和<em class="mt"> v </em>初始化为零。为了校正初始化偏差，作者建议使用重新加权版本的<em class="mt"> m </em>和<em class="mt"> v: </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/83d12bd2023330e2e41ec1e8e59137ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*vQ8NLOskpMPLTirpMZ8PDQ.png"/></div></figure><p id="640b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面，我们看到 Adam 诱导的轨迹有点类似于 RMSprop 给出的轨迹，但是开始更慢。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/5557c4474052b3fc5f1e4e3360f6c5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uexBym1zXf9jaxPlcVZtbw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚当(蓝色)和梯度下降(白色)。来源:作者的计算</p></figure><h1 id="d55d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">主要情节</h1><p id="368c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下一个图显示了由上述四种优化算法得出的轨迹。</p><p id="cfce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">主要结果如下:</p><ul class=""><li id="276e" class="my mz it lt b lu mn lx mo ma na me nb mi nc mm nd ne nf ng bi translated">有动量的梯度下降比没有动量的梯度下降波动小。</li><li id="cde1" class="my mz it lt b lu nh lx ni ma nj me nk mi nl mm nd ne nf ng bi translated">Adam 和 RMSprop 采用不同的路线，在斜率维度上移动较慢，在截距维度上移动较快。</li><li id="eb52" class="my mz it lt b lu nh lx ni ma nj me nk mi nl mm nd ne nf ng bi translated">正如预期的那样，Adam 显示了一些动量:当 RMSprop 开始向左转向最小值时，Adam 因为累积的动量而有更难的转弯时间。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/210cee48861f221bcc9cf875ba4df8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*H8jafclns_xYV_gCtJzdVg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">几种优化算法的行为。来源:作者的计算</p></figure><p id="109e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图是同一张图表，但是是 3d 的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/85b0746e545523a32c662e505cdcd337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rJsdS4DGSkmmw8X1swwjfQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">几种优化算法的行为。来源:作者的计算</p></figure><h1 id="174e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="2741" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇博文中，我的目的是让读者对机器学习中使用的关键优化算法有一个直观的理解。</p><p id="dc60" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在下面，你可以找到用来制作本文所用图表的代码。毫不犹豫地修改学习率和/或损失函数，看看这会如何影响不同的轨迹。</p><p id="a3ee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi">—</p><h1 id="cd21" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">代码(Python)</h1><p id="1c88" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">以下代码块加载依赖关系，定义损失函数并绘制损失函数(曲面和等高线图):</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="809f" class="nr la it nn b be ns nt l nu nv"># A. Dependencies <br/>%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>from matplotlib import cm<br/>from matplotlib.ticker import LinearLocator<br/><br/>plot_scale = 1.25<br/>plt.rcParams["figure.figsize"] = (plot_scale*16, plot_scale*9)<br/><br/>import numpy as np<br/>import pandas as pd<br/>import random<br/>import scipy.stats<br/>from itertools import product<br/>import os<br/>import time<br/>from math import sqrt<br/>import seaborn as sns; sns.set()<br/>from tqdm import tqdm as tqdm       <br/>import datetime<br/>from typing import Tuple<br/>class Vector: pass<br/>from scipy.stats import norm<br/>import torch<br/>from torch import nn<br/>from torch.utils.data import DataLoader<br/>import copy<br/>import matplotlib.ticker as mtick<br/>from torchcontrib.optim import SWA<br/>from numpy import linalg as LA<br/>import imageio as io #create gif<br/><br/># B. Create OLS problem<br/>b0 = -2.0 #intercept<br/>b1 = 2.0 #slope<br/>beta_true = (b0 , b1)<br/>nb_vals = 1000 #number draws<br/><br/>mu, sigma = 0, 0.001 # mean and standard deviation<br/>shocks = np.random.normal(mu, sigma, nb_vals)<br/><br/># covariate<br/>x0 = np.ones(nb_vals) #cst<br/>x1 = np.random.uniform(-5, 5, nb_vals)<br/>X = np.column_stack((x0, x1))<br/><br/># Data<br/>y = b0*x0 + b1*x1 + shocks<br/><br/>A = np.linalg.inv(np.matmul(np.transpose(X), X))<br/>B = np.matmul(np.transpose(X), y)<br/>np.matmul(A, B)<br/><br/>X_torch = torch.from_numpy(X).float()<br/>y_torch = torch.from_numpy(y).float()<br/><br/># Loss function and gradient (for plotting)<br/>def loss_function_OLS(beta_hat, X, y):<br/>    loss = (1/len(y))*np.sum(np.square(y - np.matmul(X, beta_hat)))<br/>    return loss<br/><br/>def grad_OLS(beta_hat, X, y):<br/>    mse = loss_function_OLS(beta_hat, X, y)<br/>    G = (2/len(y))*np.matmul(np.transpose(X), np.matmul(X, beta_hat) - y) <br/>    return G, mse<br/><br/># C. Plots for the loss function<br/>min_val=-10.0<br/>max_val=10.0<br/><br/>delta_grid=0.05<br/>x_grid = np.arange(min_val, max_val, delta_grid)<br/>y_grid = np.arange(min_val, max_val, delta_grid)<br/>X_grid, Y_grid = np.meshgrid(x_grid, y_grid)<br/><br/>Z = np.zeros((len(x_grid), len(y_grid)))<br/><br/>for (y_index, y_value) in enumerate(y_grid):<br/>    for (x_index, x_value) in enumerate(x_grid):<br/>        beta_local = np.array((x_value, y_value))<br/>        Z[y_index, x_index] = loss_function_OLS(beta_local, X, y)<br/><br/>fig, ax = plt.subplots(subplot_kw={"projection": "3d"})<br/><br/># Plot the surface.<br/>surf = ax.plot_surface(X_grid, Y_grid, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.2)<br/><br/>ax.zaxis.set_major_locator(LinearLocator(10))<br/>ax.zaxis.set_major_formatter('{x:.02f}')<br/><br/>ax.scatter([b0], [b1], [true_value], s=100, c='black', linewidth=0.5)<br/><br/>x_min = -10<br/>x_max = -x_min<br/>y_min = x_min<br/>y_max = -x_min<br/><br/>plt.xlim(x_min, x_max)<br/>plt.ylim(y_min, y_max)<br/><br/>plt.ylabel('Slope')<br/>plt.xlabel('Intercept')<br/><br/>fig.colorbar(surf, shrink=0.5, aspect=5)<br/><br/>filename = "IMGS/surface_loss.png"<br/>plt.savefig(filename)<br/>plt.show()<br/><br/># Plot contour<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/>plt.scatter([b0], [b1], s=100, c='white', linewidth=0.5)<br/>plt.ylabel('Slope')<br/>plt.xlabel('Intercept')<br/>plt.xlim(x_min, x_max)<br/>plt.ylim(y_min, y_max)<br/><br/>filename = "IMGS/countour_loss.png"<br/>plt.savefig(filename)<br/>plt.show()</span></pre><p id="3fa3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一段代码定义了函数，这样我们就可以使用 Pytorch 解决 OLS 问题。在这里，使用 Pytroch 有点过了，但好处是我们可以使用预编码的最小化算法(<em class="mt"> torch.optim </em>):</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="4318" class="nr la it nn b be ns nt l nu nv">def loss_OLS(model, y, X): <br/>    """<br/>    Loss function for OLS<br/>    """<br/>    R_squared = torch.square(y.unsqueeze(1) - model(X[:,1].unsqueeze(1)))<br/>    return torch.mean(R_squared)<br/><br/>def set_initial_values(model, w, b):<br/>    """<br/>    Function to set the weight and bias to certain values<br/>    """<br/>    with torch.no_grad():<br/>        for name, param in model.named_parameters():<br/>            if 'linear_relu_stack.0.weight' in name:<br/>                param.copy_(torch.tensor([w]))<br/>            elif 'linear_relu_stack.0.bias' in name:<br/>                param.copy_(torch.tensor([b]))<br/>    <br/>def create_optimizer(model, optimizer_name, lr, momentum):<br/>    """<br/>    Function to define an optimizer<br/>    """<br/>    if optimizer_name == "Adam":<br/>        optimizer = torch.optim.Adam(model.parameters(), lr) <br/>    elif optimizer_name == "SGD":<br/>        optimizer = torch.optim.SGD(model.parameters(), lr)<br/>    elif optimizer_name == "SGD-momentum":<br/>        optimizer = torch.optim.SGD(model.parameters(), lr, momentum)<br/>    elif optimizer_name == "Adadelta":<br/>        optimizer = torch.optim.Adadelta(model.parameters(), lr)<br/>    elif optimizer_name == "RMSprop":<br/>        optimizer = torch.optim.RMSprop(model.parameters(), lr)<br/>    else:<br/>        raise("optimizer unknown")<br/>    return optimizer<br/><br/>def train_model(optimizer_name, initial_guess, true_value, lr, momentum):<br/>    """<br/>    Function to train a model<br/>    """<br/>    # initialize a model<br/>    model = NeuralNetwork().to(device)<br/>    #print(model)<br/><br/>    set_initial_values(model, initial_guess[0], initial_guess[1])<br/><br/>    for name, param in model.named_parameters():<br/>        print(name, param)<br/><br/>    model.train()<br/><br/>    nb_epochs = 100<br/>    use_scheduler = False<br/>    freq_scheduler = 100<br/>    freq_gamma = 0.95<br/>    true_b = torch.tensor([true_value[0], true_value[1]])<br/><br/>    print(optimizer_name)<br/>    optimizer = create_optimizer(model, optimizer_name, lr, momentum)<br/><br/>    # A LOOP OVER EACH POINT OF THE CURRENT GRID<br/>    # store mean loss by epoch<br/>    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=freq_gamma)<br/>    loss_epochs = torch.zeros(nb_epochs)<br/>    list_perc_abs_error = [] #store abs value percentage error<br/>    list_perc_abs_error_i = [] #store index i<br/>    list_perc_abs_error_loss = [] #store loss<br/>    list_norm_gradient = [] #store norm of gradient<br/>    list_gradient = [] #store the gradient itself<br/>    list_beta = [] #store parameters<br/><br/>    calculate_variance_grad = False <br/><br/>    freq_loss = 1<br/>    freq_display = 10<br/><br/>    for i in tqdm(range(0, nb_epochs)):<br/><br/>        optimizer.zero_grad()<br/><br/>        # Calculate the loss<br/>        loss = loss_OLS(model, y_torch, X_torch)<br/>        loss_epochs[[i]] = float(loss.item())<br/><br/>        # Store the loss<br/>        with torch.no_grad():<br/>            # Extract weight and bias<br/>            b_current = np.array([k.item() for k in model.parameters()])<br/>            b_current_ordered = np.array((b_current[1], b_current[0])) #reorder (bias, weight)<br/>        list_beta.append(b_current_ordered)<br/>        perc_abs_error = np.sum(np.square(b_current_ordered - true_b.detach().numpy()))<br/>        list_perc_abs_error.append(np.median(perc_abs_error))<br/>        list_perc_abs_error_i.append(i)<br/>        list_perc_abs_error_loss.append(float(loss.item()))<br/><br/>        # Calculate the gradient<br/>        loss.backward()<br/><br/>        # Store the gradient<br/>        with torch.no_grad():<br/>            grad = np.zeros(2)<br/>            for (index_p, p) in enumerate(model.parameters()):<br/>                grad[index_p] = p.grad.detach().data<br/>            #reorder (bias, weight)<br/>            grad_ordered = np.array((grad[1], grad[0]))<br/>            list_gradient.append(grad_ordered)<br/><br/>        # Take a gradient steps<br/>        optimizer.step()<br/><br/>        if i % freq_display == 0: #Monitor the loss<br/>            loss, current = float(loss.item()), i<br/>            print(f"loss: {loss:&gt;7f}, percentage abs. error {list_perc_abs_error[-1]:&gt;7f}, [{current:&gt;5d}/{nb_epochs:&gt;5d}]")<br/>        if (i % freq_scheduler == 0) &amp; (i != 0) &amp; (use_scheduler == True):<br/>            scheduler.step()<br/>            print("i : {}. Decreasing learning rate: {}".format(i, scheduler.get_last_lr()))<br/><br/>    return model, list_beta, list_gradient <br/><br/>def create_gif(filenames, output_name):<br/>    """<br/>    Function to create a gif, using a list of images<br/>    """<br/>    with io.get_writer(output_name, mode='I') as writer:<br/>        for filename in filenames:<br/>            image = io.imread(filename)<br/>            writer.append_data(image)<br/><br/>    # Remove files, except the final one<br/>    for index_file, filename in enumerate(set(filenames)):<br/>        if index_file &lt; len(filenames) - 1:<br/>            os.remove(filename)<br/>        <br/># Define a neural network with a single node  <br/># Get cpu or gpu device for training.<br/>device = "cuda" if torch.cuda.is_available() else "cpu"<br/>print(f"Using {device} device")<br/><br/>nb_nodes = 1<br/># Define model<br/>class NeuralNetwork(nn.Module):<br/>    def __init__(self):<br/>        super(NeuralNetwork, self).__init__()<br/>        self.flatten = nn.Flatten()<br/>        self.linear_relu_stack = nn.Sequential(<br/>            nn.Linear(1, nb_nodes)<br/>        )<br/><br/>    def forward(self, x):<br/>        out = self.linear_relu_stack(x)<br/>        return out</span></pre><p id="1882" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用梯度下降的最小化；</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="fa11" class="nr la it nn b be ns nt l nu nv">lr = 0.10 #learning rate<br/>alpha = lr<br/>init = (9.0, 2.0) #initial guess<br/>true_value = [-2.0, 2.0] #true value for parameters<br/><br/># I. Solve<br/>optimizer_name = "SGD"<br/>momentum = 0.0<br/>model_SGD, list_beta_SGD, list_gradient_SGD = train_model(optimizer_name , init, true_value, lr, momentum)<br/><br/># II. Create gif<br/>filenames = []<br/>zoom=1 #to increase/decrease the length of vectors on the plot<br/>max_index_plot = 30 #when to stop plotting<br/><br/># Plot contour<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/><br/># Add points and arrows<br/>for (index, (bb, grad)) in enumerate(zip(list_beta_SGD, list_gradient_SGD)):<br/>    if index&gt;max_index_plot:<br/>        break<br/>    if index == 0:<br/>        label_1 = "SGD"<br/>    else:<br/>        label_1 = ""<br/>    # Point<br/>    plt.scatter([bb[0]], [bb[1]], s=10, c='white', linewidth=5.0, label=label_1)<br/>    # Arrows<br/>    plt.arrow(bb[0], bb[1], - zoom * alpha* grad[0], - zoom * alpha * grad[1], color='white')<br/>    # Add arrows for gradient:<br/>    # create file name and append it to a list<br/>    filename = "IMGS/path_SGD_{}.png".format(index)<br/>    filenames.append(filename)<br/>    plt.xlabel('cst')<br/>    plt.ylabel('slope')<br/>    plt.legend()<br/>    plt.savefig(filename)<br/><br/>filename = "IMGS/path_SGD.png"<br/>plt.savefig(filename)<br/>create_gif(filenames, "SGD.gif")<br/>plt.show()</span></pre><p id="80b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">利用动量梯度下降的最小化；</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="9934" class="nr la it nn b be ns nt l nu nv">optimizer_name = "SGD-momentum"<br/>momentum = 0.2<br/><br/># I. Solve<br/>model_momentum, list_beta_momentum, list_gradient_momentum = train_model(optimizer_name , init, true_value, lr, momentum)<br/><br/># II. Create gif<br/>filenames = []<br/>zoom=1 #to increase/decrease the length of vectors on the plot<br/>max_index_plot = 30 #when to stop plotting<br/><br/># Plot contour<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/><br/># Add points and arrows<br/>for (index, (bb, grad, bb_momentum, grad_momentum)) in enumerate(zip(list_beta_SGD, list_gradient_SGD, list_beta_momentum, list_gradient_momentum)):<br/>    if index&gt;max_index_plot:<br/>        break<br/>    if index == 0:<br/>        label_1 = "SGD"<br/>        label_2 = "SGD-momentum"<br/>    else:<br/>        label_1 = ""<br/>        label_2 = ""<br/>    # Point<br/>    plt.scatter([bb[0]], [bb[1]], s=10, c='white', linewidth=5.0, label=label_1)<br/>    plt.scatter([bb_momentum[0]], [bb_momentum[1]], s=10, c='blue', linewidth=5.0, alpha=0.5, label=label_2)<br/>    # Arrows<br/>    #plt.arrow(bb_momentum[0], bb_momentum[1], - zoom * alpha* grad[0], - zoom * alpha * grad[1], color='white')<br/>    plt.arrow(bb_momentum[0], bb_momentum[1], - zoom * alpha* grad_momentum[0], - zoom * alpha * grad_momentum[1], color="blue")<br/>    # create file name and append it to a list<br/>    filename = "IMGS/path_SGD_momentum_{}.png".format(index)<br/>    filenames.append(filename)<br/>    plt.xlabel('cst')<br/>    plt.ylabel('slope')<br/>    plt.legend()<br/>    plt.savefig(filename)<br/><br/>filename = "IMGS/path_SGD_momentum.png"<br/>plt.savefig(filename)<br/>create_gif(filenames, "SGD_momentum.gif")<br/>plt.show()</span></pre><p id="a537" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用 RMSprop 最小化:</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="dab3" class="nr la it nn b be ns nt l nu nv">optimizer_name = "RMSprop"<br/>momentum = 0.0<br/># I. Solve<br/>model_RMSprop, list_beta_RMSprop, list_gradient_RMSprop = train_model(optimizer_name , init, true_value, lr, momentum)<br/><br/># II. Create gif<br/>filenames = []<br/>zoom=1 #to increase/decrease the length of vectors on the plot<br/>max_index_plot = 30 #when to stop plotting<br/><br/># Plot contour<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/><br/># Add points and arrows<br/>for (index, (bb, grad, bb_RMSprop, grad_RMSprop)) in enumerate(zip(list_beta_SGD, list_gradient_SGD, list_beta_RMSprop, list_gradient_RMSprop)):<br/>    if index&gt;max_index_plot:<br/>        break<br/>    if index == 0:<br/>        label_1 = "SGD"<br/>        label_2 = "RMSprop"<br/>    else:<br/>        label_1 = ""<br/>        label_2 = ""<br/>    # Point<br/>    plt.scatter([bb[0]], [bb[1]], s=10, c='white', linewidth=5.0, label=label_1)<br/>    plt.scatter([bb_RMSprop[0]], [bb_RMSprop[1]], s=10, c='blue', linewidth=5.0, alpha=0.5, label=label_2)<br/>    # Arrows<br/>    plt.arrow(bb_RMSprop[0], bb_RMSprop[1], - zoom * alpha* grad_RMSprop[0], - zoom * alpha * grad_RMSprop[1], color="blue")<br/>    # create file name and append it to a list<br/>    filename = "IMGS/path_RMSprop_{}.png".format(index)<br/>    filenames.append(filename)<br/>    plt.xlabel('cst')<br/>    plt.ylabel('slope')<br/>    plt.legend()<br/>    plt.savefig(filename)<br/><br/>filename = "IMGS/path_RMSprop.png"<br/>plt.savefig(filename)<br/>create_gif(filenames, "RMSprop.gif")<br/>plt.show()</span></pre><p id="0549" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用 Adam 的最小化:</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="9e92" class="nr la it nn b be ns nt l nu nv">optimizer_name = "Adam"<br/>momentum = 0.0<br/><br/># I. Solve<br/>model_Adam, list_beta_Adam, list_gradient_Adam = train_model(optimizer_name , init, true_value, lr, momentum)<br/><br/># II. Create gif<br/>filenames = []<br/>zoom=1 #to increase/decrease the length of vectors on the plot<br/>max_index_plot = 30 #when to stop plotting<br/><br/># Plot contour<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/><br/># Add points and arrows<br/>for (index, (bb, grad, bb_Adam, grad_Adam)) in enumerate(zip(list_beta_SGD, list_gradient_SGD, list_beta_Adam, list_gradient_Adam)):<br/>    if index&gt;max_index_plot:<br/>        break<br/>    if index == 0:<br/>        label_1 = "SGD"<br/>        label_2 = "Adam"<br/>    else:<br/>        label_1 = ""<br/>        label_2 = ""<br/>    # Point<br/>    plt.scatter([bb[0]], [bb[1]], s=10, c='white', linewidth=5.0, label=label_1)<br/>    plt.scatter([bb_Adam[0]], [bb_Adam[1]], s=10, c='blue', linewidth=5.0, alpha=0.5, label=label_2)<br/>    # Arrows<br/>    plt.arrow(bb_Adam[0], bb_Adam[1], - zoom * alpha* grad_Adam[0], - zoom * alpha * grad_Adam[1], color="blue")<br/>    # create file name and append it to a list<br/>    filename = "IMGS/path_Adam_{}.png".format(index)<br/>    filenames.append(filename)<br/>    plt.xlabel('cst')<br/>    plt.ylabel('slope')<br/>    plt.legend()<br/>    plt.savefig(filename)<br/><br/>filename = "IMGS/path_Adam.png"<br/>plt.savefig(filename)<br/>create_gif(filenames, "Adam.gif")<br/>plt.show()</span></pre><p id="967a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用四条轨迹创造“主情节”:</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="d32f" class="nr la it nn b be ns nt l nu nv">max_iter = 100<br/>filenames = []<br/>cp = plt.contour(X_grid, Y_grid, np.sqrt(Z), colors='black', linestyles='dashed', linewidths=1, alpha=0.5)<br/>plt.clabel(cp, inline=1, fontsize=10)<br/>cp = plt.contourf(X_grid, Y_grid, np.sqrt(Z))<br/>colors = ["white", "blue", "green", "red"]<br/><br/># Add points:<br/>for (index, (bb_SGD, bb_momentum, bb_RMSprop, bb_Adam)) in enumerate(zip(list_beta_SGD, list_beta_momentum, list_beta_RMSprop, list_beta_Adam)):<br/>    if index % freq_plot == 0:<br/>        if index == 0:<br/>            label_1 = "SGD"<br/>            label_2 = "SGD-momentum"<br/>            label_3 = "RMSprop"<br/>            label_4 = "Adam"<br/>        else:<br/>            label_1, label_2, label_3, label_4 = "", "", "", ""<br/>        plt.scatter([bb_SGD[0]], [bb_SGD[1]], s=10, linewidth=5.0, label=label_1, color=colors[0])<br/>        plt.scatter([bb_momentum[0]], [bb_momentum[1]], s=10, linewidth=5.0, alpha=0.5, label=label_2, color=colors[1])<br/>        plt.scatter([bb_RMSprop[0]], [bb_RMSprop[1]], s=10, linewidth=5.0, alpha=0.5, label=label_3, color=colors[2])<br/>        plt.scatter([bb_Adam[0]], [bb_Adam[1]], s=10, linewidth=5.0, alpha=0.5, label=label_4, color=colors[3])<br/>    if index &gt; max_iter:<br/>        break<br/>    # create file name and append it to a list<br/>    filename = "IMGS/img_{}.png".format(index)<br/>    filenames.append(filename)<br/>    # Add arrows for gradient:<br/>    plt.xlabel('cst')<br/>    plt.ylabel('slope')<br/>    plt.legend()<br/><br/>    # save frame<br/>    plt.savefig(filename)<br/>    #plt.close()# build gif<br/>    <br/>create_gif(filenames, "compare_optim_algos.gif")</span></pre><p id="5f70" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">创建 3D“主图”:</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="6e5c" class="nr la it nn b be ns nt l nu nv">max_iter = 100<br/>fig, ax = plt.subplots(subplot_kw={"projection": "3d"})<br/><br/># Plot the surface.<br/>surf = ax.plot_surface(X_grid, Y_grid, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.1)<br/>ax.zaxis.set_major_locator(LinearLocator(10))<br/>ax.zaxis.set_major_formatter('{x:.02f}')<br/>ax.view_init(60, 35)<br/><br/>colors = ["black", "blue", "green", "red"]<br/>x_min = -10<br/>x_max = -x_min<br/>y_min = x_min<br/>y_max = -x_min<br/><br/># Add points:<br/>for (index, (bb_SGD, bb_momentum, bb_RMSprop, bb_Adam)) in enumerate(zip(list_beta_SGD, list_beta_momentum, list_beta_RMSprop, list_beta_Adam)):<br/>    if index == 0:<br/>        label_1 = "SGD"<br/>        label_2 = "SGD-momentum"<br/>        label_3 = "RMSprop"<br/>        label_4 = "Adam"<br/>    else:<br/>        label_1, label_2, label_3, label_4 = "", "", "", ""<br/>    ax.scatter([bb_SGD[0]], [bb_SGD[1]], s=100, linewidth=5.0, label=label_1, color=colors[0])<br/>    ax.scatter([bb_momentum[0]], [bb_momentum[1]], s=100, linewidth=5.0, alpha=0.5, label=label_2, color=colors[1])<br/>    ax.scatter([bb_RMSprop[0]], [bb_RMSprop[1]], s=100, linewidth=5.0, alpha=0.5, label=label_3, color=colors[2])<br/>    ax.scatter([bb_Adam[0]], [bb_Adam[1]], s=100, linewidth=5.0, alpha=0.5, label=label_4, color=colors[3])<br/>    if index &gt; max_iter:<br/>        break<br/>    # create file name and append it to a list<br/>    filename = "IMGS/img_{}.png".format(index)<br/>    filenames.append(filename)<br/>    # Add arrows for gradient:<br/>    plt.xlim(x_min, x_max)<br/>    plt.ylim(y_min, y_max)<br/>    plt.ylabel('Slope')<br/>    plt.xlabel('Intercept')<br/>    plt.legend()<br/>    # save frame<br/>    plt.savefig(filename)<br/><br/>filename = "IMGS/surface_loss.png"<br/>plt.savefig(filename)<br/>plt.show()<br/><br/>create_gif(filenames, "surface_compare_optim_algos.gif")</span></pre><p id="1ca6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi">—</p><h1 id="f466" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><ul class=""><li id="4b28" class="my mz it lt b lu lv lx ly ma nw me nx mi ny mm nd ne nf ng bi translated">鲁德，塞巴斯蒂安。"梯度下降优化算法概述."<em class="mt"> arXiv 预印本 arXiv:1609.04747 </em> (2016)</li><li id="95b4" class="my mz it lt b lu nh lx ni ma nj me nk mi nl mm nd ne nf ng bi translated">《论深度学习中初始化和动量的重要性》<em class="mt">机器学习国际会议</em>。PMLR，2013 年。</li></ul><p id="2bf8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">关于这个话题的非常好的系列视频:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div></figure></div></div>    
</body>
</html>