<html>
<head>
<title>Cloud ML Performance Checklist</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">云ML性能清单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cloud-ml-performance-checklist-caa51e798002#2022-08-21">https://towardsdatascience.com/cloud-ml-performance-checklist-caa51e798002#2022-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5883" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">优化基于云的培训的指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/aa5fa6e91700be3a2ad88bb93cf40783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p1jYhlP3nI61-iLw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杰克·威瑞克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4a2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在云中培训有很多好处。其中包括<strong class="ky ir">对多种培训实例类型的可访问性</strong>，将培训无限制地扩展到多个实例和多个并行实验的能力<strong class="ky ir">，以及丰富的<strong class="ky ir">生态系统</strong>功能和服务促进了ML工作负载。<strong class="ky ir">然而，如果管理不当，cloud ML会产生相当高的成本。</strong>虽然主要职责是制定和实施治理政策、监控云服务的使用和成本、与云服务提供商(CSP)协商定价策略等。，可能会落到您组织的云支持团队头上，<strong class="ky ir">算法开发人员也有责任尽自己的一份力量来提高效率，减少浪费</strong>。下面嵌入推文中的迷因说明了一切。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="ad09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文档描述了一个项目清单，我们发现该清单有助于指导算法开发人员提高训练效率，进而降低训练成本。</p><h1 id="f9ce" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">性能测定</h1><p id="f4d0" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们的主要性能指标是<strong class="ky ir"> <em class="mr">每美元的样本数</em> </strong>，即每花费一美元，训练循环遍历的样本数。<strong class="ky ir"> <em class="mr">样本每美元</em> </strong>的公式为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/3dbc30255a8c40f313e5fddc53218f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*S4EHa7BWzur1qIfarhuDOw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每美元公式示例(按作者)</p></figure><p id="9737" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="mr">每秒样本数=批量*每秒批次数</em>。培训实例费用可在您的CSP网站上找到。</p><p id="53b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然您的目标应该是最大化每美元 的<strong class="ky ir"> <em class="mr">样本，但有时您需要考虑其他因素。例如，您可能决定将训练分布在多个GPU上以加速训练，即使这可能会减少每美元</em> </strong>的<strong class="ky ir"> <em class="mr">个样本。另一个例子是，最大化每美元的样本会损害训练工作的收敛速度(例如，对批量大小的增加敏感的模型)。</em></strong></p><p id="7b9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">性能优化是一个迭代过程，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/1468d9cfdba1b93c25c1464d9554fa5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dhEF5df64QrgjiVLwxZqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">性能优化流程(按作者)</p></figure><p id="968d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个迭代过程应该伴随项目的整个生命周期。</p><h1 id="7ab3" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">性能分析工具</h1><p id="9a75" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">有许多分析性能的工具和方法。一些基本指标包括云服务提供商(CSP)报告的资源利用率指标和<a class="ae kv" href="https://www.tensorflow.org/guide/profiler" rel="noopener ugc nofollow" target="_blank"> TensorBoard profiler插件</a>。</p><p id="a429" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这张图片显示了一个由<a class="ae kv" href="https://aws.amazon.com/cloudwatch/" rel="noopener ugc nofollow" target="_blank">亚马逊云观察</a>监控工具报告的低GPU利用率的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/f3123bb0b94b3a204f1cc20331fdf22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxLda4OJ7fHqoS7Fm9VJHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">亚马逊CloudWatch中的GPU利用不足(作者)</p></figure><p id="f9ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.tensorflow.org/guide/profiler" rel="noopener ugc nofollow" target="_blank"> TensorBoard profiler插件</a>是一个强大的工具，支持许多流行的机器学习框架的分析(包括<a class="ae kv" href="https://www.tensorflow.org/guide/profiler" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>、<a class="ae kv" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae kv" href="https://jax.readthedocs.io/en/latest/profiling.html#tensorboard-profiling" rel="noopener ugc nofollow" target="_blank"> Jax </a>)。一开始，这个分析器看起来有点吓人，但是它非常容易设置，并且是识别性能瓶颈的一个非常有用的工具。更多详情参见<a class="ae kv" href="https://www.tensorflow.org/guide/profiler" rel="noopener ugc nofollow" target="_blank">分析器文档</a>。</p><p id="8d94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图来自TensorBoard profiler trace viewer，显示了长时间的GPU空闲时间，表明训练步骤中存在重大瓶颈。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/a91b4e6529e9418faac3a693641732ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BpKAIRStN3Fo-jDmxohRgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TensorBoard Profiler插件显示的性能瓶颈(作者)</p></figure><h1 id="4b50" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">性能分析清单</h1><p id="a5cc" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">本节包括衡量培训工作当前绩效的基本指南。</p><ol class=""><li id="57c7" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">您的培训计划是否包括定期报告每秒<em class="mr">培训样本的平均数量</em>？</li><li id="f526" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您目前衡量每美元样品的标准是多少？</li><li id="ad9d" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">在整个培训过程中,<em class="mr">每美元样品</em>测量值是否相对稳定？</li><li id="281c" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您的培训脚本是否支持启用概要分析的选项，例如通过TensorFlow中的<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard" rel="noopener ugc nofollow" target="_blank"> TensorBoard回调</a>？</li><li id="6176" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">CSP的云指标报告的当前平均GPU内存利用率是多少？一般来说(但不总是)，通过增加训练批量来最大化GPU内存的使用是一个好主意。这通常会增加每秒的样本数，因为它降低了固定操作(如内核加载和梯度共享)的成本。在某些情况下，批量大小的选择会对GPU内存对齐产生影响，这也会影响性能。</li><li id="7825" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">CSP的云指标报告的当前平均GPU利用率是多少？你的目标应该是尽可能地增加这一点。95%的利用率是一个很好的目标。任何低于80%的数据都意味着你的训练出现了瓶颈，应该会让你夜不能寐。</li><li id="ce00" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">其他系统指标，包括CPU和网络利用率，可能会对潜在的性能问题提供一些提示，但通常很难从中获得有用的信息。这些指标中是否有任何一项表明可能存在瓶颈？</li><li id="90f6" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">评估训练管道性能的一个有用方法是对缓存的输入运行训练循环。通过这样做，您可以有效地隔离出所有的输入流水线计算，并在这种情况下计算每美元 的<strong class="ky ir"> <em class="mr">个样本。这个数字表示当数据输入管道上没有瓶颈时可以达到的性能。如果有您正在努力解决的瓶颈，这个数字应该被视为您的目标性能。您的脚本支持在缓存的输入上计算每美元</em> </strong>的<strong class="ky ir"> <em class="mr">样本的选项吗？在Tensorflow中，缓存输入可以通过应用:<em class="mr"> dataset = dataset.take(1)轻松完成。缓存()。在数据集创建结束时重复()</em>(预取之前)。请注意，通过在数据集的其他阶段应用缓存，可以获得对CPU争用的潜在来源的更多见解，并且应该在分析输入管道上的瓶颈时使用。</em></strong></li><li id="d920" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">TensorBoard profiler是一个非常有用的性能分析工具。让它成为你发展周期的一个常规部分是很重要的。对其功能的全面概述超出了本文档的范围，但是强烈建议(并没有您想象的那么困难)学习如何使用它。您运行(并分析)分析器了吗？</li><li id="6866" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">查看探查器摘要报告。它是否报告了输入管道瓶颈？(请注意，如果在“内核加载”上花费了大量时间，这可能表明CPU争用率很高，并且可能表明存在CPU瓶颈。)</li><li id="7736" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">打开跟踪查看器。有没有什么麻烦的模式比如GPU长时间闲置或者频繁的CPU到GPU的拷贝？</li><li id="63ff" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">查看十大TensorFlow GPU操作。它们是你所期望的吗？这些有时可以暗示对模型的潜在优化。</li></ol><h1 id="b366" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">实例选择</h1><p id="a433" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">根据您选择的培训实例，每美元的样本量可能会有很大差异。选择错误的训练实例会对成本产生可怕的后果。一般来说，新一代的实例(比如Amazon EC2 G5和p4实例类型)包含了提高性能的硬件增强。另一个考虑因素是CPU与GPU计算能力的比率。较低的比率可能会增加对CPU资源的争用，并导致CPU瓶颈。</p><ol class=""><li id="0832" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">您目前的培训情况如何？你考虑过其他选择吗？如果你使用的是老一代GPU，比如Amazon EC2 p2或p3，你应该有一个很好的理由。</li><li id="af2f" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">替代(非GPU)加速器，如<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/dl1/" rel="noopener ugc nofollow" target="_blank">亚马逊EC2 dl1 </a>(基于<a class="ae kv" href="https://habana.ai/training/gaudi/" rel="noopener ugc nofollow" target="_blank">哈瓦那高迪</a>)和<a class="ae kv" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank">谷歌云TPU </a>(在GCP)通常(但不总是)提供比GPU更高的性价比。然而，它们有时需要对这些加速器进行一些调整。你有没有尝试过在一个专门的DNN培训加速器上运行你的模型？使用这些替代品有什么障碍？</li></ol><h1 id="b440" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">培训优化</h1><p id="1e6c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">下图显示了典型训练步骤的各个阶段。每一步都是性能瓶颈的潜在来源:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/eebbc17aa7fef6792b94c9cc66678481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHy23OovRmlEc9kyJhH6GQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">典型训练步骤的步骤(作者)</p></figure><h2 id="053a" class="nl lv iq bd lw nm nn dn ma no np dp me lf nq nr mg lj ns nt mi ln nu nv mk nw bi translated">存储到CPU</h2><p id="f08e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">训练步骤的第一阶段是将原始训练样本加载到CPU上。存储数据的方式需要特别注意，这样这个阶段才不会成为瓶颈。如果您的数据存储在远程(云)对象存储中，比如亚马逊S3，这一点尤其正确。实例类型网络入站容量、S3出站容量或CPU争用的限制可能会导致数据匮乏。训练速度可能会受到您选择的数据格式(例如顺序与列格式)、数据文件的大小、用于流式传输数据的方法(管道模式、FFM、直接下载等)的影响。)、数据压缩的使用等。</p><p id="f6c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分析数据流速度的一种方法是直接迭代数据集，测量每美元的<strong class="ky ir"> <em class="mr">样本</em> </strong>，而不运行训练步骤。理想情况下，结果将明显高于使用训练步骤跑步时的结果。</p><ol class=""><li id="507c" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">您的文件大小是否在100 MB左右？</li><li id="6d37" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">如果你在使用Amazon SageMaker，你会利用FFM或PipeMode来传输你的数据吗？</li><li id="9a6b" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">在没有训练的情况下迭代数据集时，<strong class="ky ir"> <em class="mr">每美元样本数</em> </strong>是多少？</li><li id="d6ab" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">在输入数据管道的开始应用数据缓存时，每美元的<strong class="ky ir"> <em class="mr">样本</em> </strong>是什么？如果它高于基准值，这可能表明有问题。</li><li id="e16b" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">使用数据压缩可以减少网络带宽需求，也可以降低存储成本。另一方面，数据解压缩可能会增加CPU争用。你是否研究过数据压缩对你训练速度的潜在影响？</li></ol><h2 id="35b3" class="nl lv iq bd lw nm nn dn ma no np dp me lf nq nr mg lj ns nt mi ln nu nv mk nw bi translated">CPU瓶颈</h2><p id="817c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">GPU利用率不足的最常见原因之一是CPU上的瓶颈。如果由于CPU争用而出现瓶颈，可以考虑以下选项:</p><ol class=""><li id="c421" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">您是否使用了基本的并行化技术，比如批处理<em class="mr">预取</em>和<em class="mr"> num_parallel_calls </em>？</li><li id="d74c" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">有没有可以转移到数据准备阶段的计算？</li><li id="0db4" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">有没有可以移到GPU上的计算层？请注意，这将增加整体GPU计算，但可能会导致步骤时间缩短。</li><li id="7438" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">将一些输入数据预处理卸载到辅助CPU会有帮助吗？请注意，需要考虑很多因素，例如总体成本、对网络数据流量的影响等。</li><li id="0a20" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您是否探索过优化CPU操作的选项(例如使用<a class="ae kv" href="https://www.tensorflow.org/xla#:~:text=XLA%20(Accelerated%20Linear%20Algebra)%20is,potentially%20no%20source%20code%20changes." rel="noopener ugc nofollow" target="_blank"> XLA </a>)？</li></ol><h2 id="8c8c" class="nl lv iq bd lw nm nn dn ma no np dp me lf nq nr mg lj ns nt mi ln nu nv mk nw bi translated">提高GPU利用率</h2><p id="3350" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">即使您的GPU得到了充分利用，您也可以采取一些措施来提高其性能:</p><ol class=""><li id="8dfc" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">使用<a class="ae kv" href="https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/" rel="noopener ugc nofollow" target="_blank">混合精密</a>浮子。TensorFlow和PyTorch的当前版本支持16位精度浮点。当使用<a class="ae kv" href="https://www.tensorflow.org/guide/mixed_precision" rel="noopener ugc nofollow" target="_blank"> TensorFlow混合精度</a>时，模型参数存储在全精度浮点表示中，而算术运算用低精度浮点执行。使用混合精度可以大幅降低GPU内存的使用(从而增加批量大小)并增加每美元的<em class="mr">样本数</em>。有两种类型的16位浮点表示:float16和bfloat16。Bfloat16是首选，因为它的动态范围与float32相似。Bfloat16由现代GPU(例如A100和A10)、云TPU和Habana Gaudi支持。如果您的实例支持bfloat16，那么您可以启用混合精度，而不会对您的模型收敛产生任何影响。对于float16，事情会变得更棘手，因为你可能需要使用渐变缩放技术来确保收敛。你的实例支持bfloat16吗？你在使用混合精度吗？</li><li id="5055" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><a class="ae kv" href="https://www.tensorflow.org/xla#:~:text=XLA%20(Accelerated%20Linear%20Algebra)%20is,potentially%20no%20source%20code%20changes." rel="noopener ugc nofollow" target="_blank"> XLA </a>是一个编译器，它通过融合单个操作来优化GPU计算。在许多情况下，启用XLA将导致每美元的<em class="mr">样本增加。你在用XLA吗？对性能有什么影响？</em></li><li id="d151" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">有时，您的计算图可能包含加速器不支持的操作。在这种情况下，操作可能会卸载到您的CPU。这会大大降低你的训练速度，应该避免。在TensorFlow中，可以使用<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/debugging/set_log_device_placement" rel="noopener ugc nofollow" target="_blank">TF . debugging . set _ log _ device _ placement</a>来识别这种情况。这种情况有时也可以在TensorBoard profiler trace viewer中发现，在GPU计算过程中，您会看到主机和设备之间频繁的内存复制。你确保所有的运算都在GPU上运行了吗？</li><li id="4500" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">DNN专用加速器，如云TPU和HPU (Habana Gaudi)可以增加每美元的<em class="mr">样本数</em>，但可能需要一些改造才能获得最佳性能。例如，TPU和HPU在具有动态形状(例如boolean_mask)的张量上表现不佳。您的模型是否针对您正在训练的加速器进行了优化？</li><li id="8af9" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您是否正在为您的任何模型层使用<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/py_function" rel="noopener ugc nofollow" target="_blank"> tf.py_function </a>？如果是这样，您可能需要考虑其他性能替代方案。</li><li id="9628" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">记忆格式会影响训练效果。例如，参见<a class="ae kv" href="https://pytorch.org/blog/tensor-memory-format-matters/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>关于将你的内存格式编程到底层加速器的重要性。你用的是什么内存格式？你的选择对你的培训资源来说是最优的吗？</li></ol><h2 id="87ce" class="nl lv iq bd lw nm nn dn ma no np dp me lf nq nr mg lj ns nt mi ln nu nv mk nw bi translated">多GPU训练</h2><p id="e1d6" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">多GPU训练是一种用于提高整体开发速度的常用技术。使用多个GPU的一种常见方法是执行数据分布式训练，在这种训练中，我们将全局批量大小划分到多个GPU上，每个GPU维护一个相同的模型副本。通过在每个训练步骤结束时共享梯度来保持同一性。理想情况下，您的培训绩效将呈线性增长，即每美元的<em class="mr">样本数</em>不会减少。然而，通常情况下，梯度共享，尤其是它所暗示的增加的网络通信流量，会降低每 <em class="mr">美元</em>的<em class="mr">样本。但是，有多种方法可以缓解这种情况:</em></p><ol class=""><li id="cbfa" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">你的多GPU训练的每美元的<em class="mr">样本是多少？与单GPU <em class="mr">每美元</em>采样相比如何？</em></li><li id="a574" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">一般来说，比起多个实例，您应该总是更喜欢具有多个GPU的单个实例。是这样吗？</li><li id="75d3" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">CSP通常包括专门的网络接口，可以提高节点间网络通信的性能(例如<a class="ae kv" href="https://aws.amazon.com/hpc/efa/" rel="noopener ugc nofollow" target="_blank">亚马逊EFA </a>和<a class="ae kv" href="https://cloud.google.com/blog/products/ai-machine-learning/how-to-optimize-google-cloud-for-deep-learning-training" rel="noopener ugc nofollow" target="_blank">谷歌FastSocket </a>)。你在多种情况下训练吗？您选择的实例支持加速的节点间网络通信吗？</li><li id="402d" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">你有没有探索替代的梯度分享策略？共享渐变的常见/默认策略是<a class="ae kv" rel="noopener" target="_blank" href="/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da">环全减</a>。你可能会发现你的模型受益于替代策略，比如鲱鱼。</li><li id="dc00" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">现代梯度共享策略将把梯度分块，并把它们并行分布到后退步骤。如果TensorBoard profiler跟踪查看器显示所有的<em class="mr">设备到设备</em>通信在该步骤结束时聚集在一起，这可能表明存在问题。</li><li id="07c7" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">梯度共享API将包括一个压缩梯度的选项，以减少网络流量。你考虑过这个选择吗？</li><li id="f5b9" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">由于多gpu旨在加速培训，并且您可能不太关心评估时间，因此您可能希望考虑将评估转移到单独的单个GPU实例。</li></ol><h1 id="5a14" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">项目优化</h1><p id="6a52" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">上一节重点介绍了对单个培训作业的优化。这里，我们建议在项目级别进行优化:</p><ol class=""><li id="060b" class="mw mx iq ky b kz la lc ld lf my lj mz ln na lr nb nc nd ne bi translated">你的项目包括自动提前停止失败实验的回调吗？</li><li id="9732" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您是否使用自动化超参数调整技术？</li><li id="8667" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">您的开发流程包括删除未使用的存储吗？</li><li id="8462" class="mw mx iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">你在用的工具(TensorBoard，comet.ai，clearML，Neptune等。)来管理你的实验？高效的实验管理可以增加订单并减少重复。</li></ol><h1 id="d106" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">摘要</h1><p id="e955" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">本页分享的列表绝非包罗万象。自然要适应自己项目的具体需求。请随时提出问题/评论/更正，特别是您认为应该包括的其他项目。</p></div></div>    
</body>
</html>