<html>
<head>
<title>Jax — Numpy on GPUs and TPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPU 和 TPU 上的 jax-Numpy</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/jax-numpy-on-gpus-and-tpus-9509237d9194#2022-03-08">https://towardsdatascience.com/jax-numpy-on-gpus-and-tpus-9509237d9194#2022-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0455" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解该库并从零开始实施 MLP</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3ea5f294319da1fe7e107590f29a269f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2iNLJ8jfr6Nk2Q4I"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卢卡斯·凯普纳在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="eb4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多 Python 库和框架，因为它们是我们天空中的星星。好吧，也许没有那么多，但在处理任何给定的任务时，肯定有很多选项可供选择。</p><p id="5608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Jax 就是这些库中的一个。在过去的几个月里，它作为开发机器学习解决方案的基础框架变得非常流行，尤其是在被 Deep Mind 的人大量使用之后。</p><p id="3a4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于任何数据科学家来说，比工具更重要的是手头任务的基础知识。然而，对可用工具有一个很好的了解可能会节省很多时间，并使我们成为更有生产力的研究人员。毕竟，如果我们的目标是实现良好的业务结果，我们需要能够快速安全地运行我们的假设。</p><p id="8d23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在这篇文章中，我将谈论 Jax，解释它是什么，为什么我认为应该熟悉它，它的优点，以及如何使用它来实现一个简单的多层感知器。</p><p id="6658" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章的最后，我希望你的工具箱里又多了一个在日常工作中可能有用的工具。</p><p id="4bd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的所有代码都可以在<a class="ae kv" href="https://www.kaggle.com/tiagotoledojr/jax-library-and-mlp-implementation" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和我的<a class="ae kv" href="https://github.com/TNanukem/paper_implementations/blob/main/jax-library-and-mlp-implementation.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p><h1 id="1376" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是 Jax</h1><p id="762c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Jax 是一个数字/数学库，非常类似于著名的 know Numpy。它是由谷歌开发的，目的只有一个:在处理典型的机器学习任务时，让 Numpy 更容易使用，更快。</p><p id="b5d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现这个结果，Jax 有一些内置的实现，允许大规模处理，如并行化和向量化，更快的执行，如实时编译，以及更简单的机器学习代数，如自动签名。</p><p id="5e34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了它，人们可以自然地加速他们的机器学习管道，而不必担心写太多代码。如果您使用多处理库来并行化您的实现，您知道这会很快变得难以承受。</p><p id="ac5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，更有趣的是，Jax 能够直接在 GPU 和 TPU 等加速器上自动编译您的代码，而不需要任何修改。这个过程是无缝的。</p><p id="a4be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着您可以使用类似于 Numpy 的语法编写一次代码，在您的 CPU 上测试它，然后将其发送到 GPU 集群，而无需担心任何事情。</p><p id="8da7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，有人可能会问一个非常合理的问题:Jax 与 TensorFlow 或 Pytorch 有何不同？让我们调查一下。</p><h1 id="dbfd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">Jax x 张量流 x Pytorch</h1><p id="a912" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">TensorFlow 和 Pytorch 已经在操场上待了很长时间了。他们正在充分利用深度学习库的实现来开发端到端的深度学习管道。</p><p id="2384" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Jax 对成为一个完整的深度学习库没有兴趣。它的目标是成为加速器的核心。因此，您不会看到 Jax 以同样的方式实现数据加载器或模型验证器，您也不应该期望 Numpy 这样做。</p><p id="26d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是在我们说话的时候，已经有深度学习库在 Jax 中实现了。他们的目标是使用库中这些优秀的功能来创建更快更干净的实现。众所周知，TensorFlow 和 Pytorch 都遭受了一些技术债务，这使得它们更难高效地开发一些东西。</p><p id="d0f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我相信基于 Jax 的框架将来会在我们的行业中变得更加突出，所以我认为当标准库(不可避免地)发生变化时，了解 Jax 的基础知识是一个很好的开端。</p><h1 id="137a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">关于 Jax 的一些细节</h1><p id="8f62" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Jax 的目标是尽可能接近 Numpy。有一个低级的库实现，但是这已经超出了本文的范围。现在，我们将认为我们将完全像“GPU 的 Numpy”一样使用它。</p><p id="b388" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们将安装 Jax:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="669e" class="mu lt iq mq b gy mv mw l mx my">pip install --upgrade "jax[cpu]"</span></pre><p id="be24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个命令只会为我们安装 CPU 支持来测试我们的代码。如果您想要安装 GPU 支持，请使用:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="d318" class="mu lt iq mq b gy mv mw l mx my">pip install --upgrade "jax[cuda]"</span></pre><p id="2502" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，您必须已经安装了 CUDA 和 CuDNN 才能工作。</p><p id="0bc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将导入 Numpy 接口和一些重要函数，如下所示:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="fa77" class="mu lt iq mq b gy mv mw l mx my">import jax.numpy as jnp</span><span id="726d" class="mu lt iq mq b gy mz mw l mx my">from jax import random<br/>from jax import grad, jit, vmap<br/>from jax.scipy.special import logsumexp</span></pre><p id="8cde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在必须了解 Jax 的一些不同方面。第一个是如何处理随机数生成器。</p><h2 id="b1ee" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">Jax 中的随机数</h2><p id="1389" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">通常，当我们处理 Numpy 时，我们设置一个随机状态整数(比如 42)，并用它来为我们的程序生成随机数。scikit-learn 等传统机器学习库使用这种范式。</p><p id="93f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们处理顺序执行时，这样做很好，但是，如果我们开始并行运行我们的函数，这就成了一个问题。让我告诉你怎么做:</p><ul class=""><li id="10c3" class="nl nm iq ky b kz la lc ld lf nn lj no ln np lr nq nr ns nt bi translated">我们来定义两个函数，bar 和 baz。每个都将返回一个随机数。</li><li id="9dd3" class="nl nm iq ky b kz nu lc nv lf nw lj nx ln ny lr nq nr ns nt bi translated">让我们定义一个函数 foo，它将对前面两个函数的结果进行计算</li></ul><p id="5da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在代码中:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="29f9" class="mu lt iq mq b gy mv mw l mx my">np.random.seed(0)<br/><br/>def bar(): return np.random.uniform()<br/>def baz(): return np.random.uniform()<br/><br/>def foo(): return bar() + 2 * baz()<br/><br/>print(foo())</span></pre><p id="1386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们设置种子等于零的随机状态。如果你运行这个代码二十次，你将会得到同样的结果，因为随机状态被设置了。</p><p id="378a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但如果我们在酒吧前给巴兹打电话呢？</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="dfa4" class="mu lt iq mq b gy mv mw l mx my">np.random.seed(0)<br/><br/>def bar(): return np.random.uniform()<br/>def baz(): return np.random.uniform()<br/><br/>def foo(): return 2 * baz() + bar()<br/><br/>print(foo())</span></pre><p id="6e4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，结果不一样。这是因为函数的执行顺序不再相同。来自随机状态的保证是相同的执行将产生相同的结果，在这种情况下，这不是真的。</p><p id="a48b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你可以看到当试图并行化我们所有的函数时，这是如何成为一个问题的。我们不能保证执行的顺序，因此，没有办法强制我们得到的结果的可再现性。</p><p id="139e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Jax 解决这个问题的方法是定义伪随机数生成器密钥，如下所示:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5ee4" class="mu lt iq mq b gy mv mw l mx my">random_state = 42<br/>key = random.PRNGKey(random_state)</span></pre><p id="e0fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Jax 中的每个随机函数都必须接收一个密钥，并且这个密钥对于每个函数必须是唯一的。这意味着，即使执行的顺序改变了，结果也是一样的，因为我们使用的键是一样的。</p><p id="ed2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们需要创建一个键列表，每个功能一个键？好吧，那真的很麻烦，所以 Jax 实现了一个叫做 split 的简便方法，它接收一个键并把它分割成所需数量的子键，然后我们可以把它传递给我们的函数:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="bb87" class="mu lt iq mq b gy mv mw l mx my"><em class="nz"># Here we split our original key into three subkeys</em><br/>random.split(key, num=3)</span></pre><p id="3cfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将返回一个包含 3 个键的列表。这样，我们可以确保在执行过程中结果的可重复性。</p><h2 id="08fd" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">自动微分</h2><p id="7d18" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Jax 的主要目标之一是自动区分原生 Python 和 NumPy 函数。当处理机器学习时，这是必需的，因为我们的大多数优化算法使用函数的梯度来最小化一些损失。</p><p id="b839" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 Jax 中做微分非常简单:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="7970" class="mu lt iq mq b gy mv mw l mx my">def func(x):<br/>    return x**2</span><span id="9b0f" class="mu lt iq mq b gy mz mw l mx my">d_func = grad(func)</span></pre><p id="1711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就这样。现在，函数 d_func 会返回函数的导数，当你给它传递一个值 x 时。</p><p id="a764" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好的一点是，你可以多次应用梯度来得到高阶导数。如果我们要创建一个返回 func 的二阶导数的函数，我们只需做:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="ef78" class="mu lt iq mq b gy mv mw l mx my">d2_func = grad(d_func)</span></pre><h2 id="be80" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">即时编译</h2><p id="e772" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Python 是一种解释型语言。还有其他语言，比如 C，叫做编译语言。在编译语言中，代码由编译器读取，并生成机器代码。然后，这个机器码在你调用程序的时候执行。</p><p id="6505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解释型语言为开发人员提供了一些优势，比如不需要设置变量的数据类型。然而，由于代码不是编译的，所以它通常比编译语言慢。</p><p id="5f36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些 Python 库如<a class="ae kv" href="https://numba.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Numba </a>实现了所谓的实时(JIT)编译。这样，解释器第一次运行一个方法时，它会把它编译成机器码，这样以后的执行会更快。</p><p id="24b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你有一个要运行一万次的方法(比如在一个训练循环中的梯度更新)，JIT 编译可以大大提高你代码的性能。用 Jax 来做非常简单:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="50f3" class="mu lt iq mq b gy mv mw l mx my">def funct(x):<br/>    return x * (2 + x)</span><span id="75a6" class="mu lt iq mq b gy mz mw l mx my">compiled_funct = jit(funct)</span></pre><p id="41d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，不是每个函数都可以编译。我建议您阅读 Jax 文档，以全面了解这种方法的局限性。</p><p id="65dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在稍后的帖子中看到由此带来的性能提升。</p><h2 id="09e1" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">…向量化…</h2><p id="650b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">矢量化是一个过程，在这个过程中，我们的操作(通常发生在一些单位(例如整数)上)被应用到向量中，这允许这些计算并行发生。</p><p id="544d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个过程可以在我们的管道中产生巨大的性能改进，Jax 库有一个名为 vmap 的内置函数，它接收一个函数并自动将其矢量化给我们。</p><p id="3879" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一节中，当我们实现 MLP 时，我们将看到如何在 for 循环中应用它的例子。</p><h1 id="8b45" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">在 Jax 中实现 MLP</h1><p id="59f7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，让我们在 Jax 中实现一个 MLP 来练习我们所学的关于库的知识。为了帮助我们，我们将从 TensorFlow 数据加载器加载 MNIST 数据集。该数据集可以免费使用:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="8c5b" class="mu lt iq mq b gy mv mw l mx my">import tensorflow as tf<br/>import tensorflow_datasets as tfds</span><span id="b837" class="mu lt iq mq b gy mz mw l mx my">data_dir = '/tmp/tfds'<br/><br/>mnist_data, info = tfds.load(name="mnist", batch_size=-1, data_dir=data_dir, with_info=True)<br/>mnist_data = tfds.as_numpy(mnist_data)<br/><br/>train_data, test_data = mnist_data['train'], mnist_data['test']<br/><br/>num_labels = info.features['label'].num_classes<br/>h, w, c = info.features['image'].shape<br/>num_pixels = h * w * c<br/><br/>train_images, train_labels = train_data['image'],train_data['label']</span><span id="446e" class="mu lt iq mq b gy mz mw l mx my">test_images, test_labels = test_data['image'], test_data['label']</span></pre><p id="c373" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们只是下载数据，并将其分为训练和测试，以便我们稍后训练我们的模型。</p><p id="b0cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们创建一个助手函数来一次性编码我们的目标:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c3ff" class="mu lt iq mq b gy mv mw l mx my">def one_hot(x, k, dtype=jnp.float32):<br/>    <em class="nz">"""</em><br/><em class="nz">    Create a one-hot encoding of x of size k.</em><br/><em class="nz">    </em><br/><em class="nz">    x: array</em><br/><em class="nz">        The array to be one hot encoded</em><br/><em class="nz">    k: interger</em><br/><em class="nz">        The number of classes</em><br/><em class="nz">    dtype: jnp.dtype, optional(default=float32)</em><br/><em class="nz">        The dtype to be used on the encoding</em><br/><em class="nz">    </em><br/><em class="nz">    """</em><br/>    return jnp.array(x[:, None] == jnp.arange(k), dtype)</span></pre><p id="10e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们对标签进行编码，并将图像转换为 jnp 张量:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5fdf" class="mu lt iq mq b gy mv mw l mx my">train_images = jnp.reshape(train_images, (len(train_images), num_pixels))<br/>train_labels = one_hot(train_labels, num_labels)</span><span id="8d2d" class="mu lt iq mq b gy mz mw l mx my">test_images = jnp.reshape(test_images, (len(test_images), num_pixels))<br/>test_labels = one_hot(test_labels, num_labels)</span></pre><p id="8fba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们将定义一个函数，该函数将生成供我们在训练循环中使用的批量数据:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="42e2" class="mu lt iq mq b gy mv mw l mx my">def get_train_batches(batch_size):<br/>    <em class="nz">"""</em><br/><em class="nz">    This function loads the MNIST and returns a batch of images given the batch size</em><br/><em class="nz">    </em><br/><em class="nz">    batch_size: integer</em><br/><em class="nz">        The batch size, i.e, the number of images to be retrieved at each step</em><br/><em class="nz">    </em><br/><em class="nz">    """</em><br/>    ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)<br/>    ds = ds.batch(batch_size).prefetch(1)<br/>    return tfds.as_numpy(ds)</span></pre><p id="995c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们的数据已经准备好了，让我们开始实现吧。</p><h2 id="c449" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">初始化参数</h2><p id="b1ae" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，我们将初始化 MLP 每一层的权重。对于这篇文章，我们将随机启动它们。为此，我们必须使用伪随机数生成器密钥来保证我们的所有执行都是可重复的。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="3f91" class="mu lt iq mq b gy mv mw l mx my">def random_layer_params(m, n, key, scale=1e-2):<br/>    <em class="nz">"""</em><br/><em class="nz">    This function returns two matrices, a W matrix with shape (n, m) and a b matrix with shape (n,)</em><br/><em class="nz">    </em><br/><em class="nz">    m: integer</em><br/><em class="nz">        The first dimension of the W matrix</em><br/><em class="nz">    n: integer</em><br/><em class="nz">        The second dimension of the b matrix</em><br/><em class="nz">    key: PRNGKey</em><br/><em class="nz">        A Jax PRNGKey</em><br/><em class="nz">    scale: float, optional(default=1e-2)</em><br/><em class="nz">        The scale of the random numbers on the matrices</em><br/><em class="nz">    """</em><br/>    <em class="nz"># Split our key into two new keys, one for each matrix</em><br/>    w_key, b_key = random.split(key, num=2)<br/>    return scale * random.normal(w_key, (m,n)), scale * random.normal(b_key, (n,))</span></pre><p id="f7cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此方法接收该层的神经元数量及其后一层的神经元数量。此外，我们传递密钥，这样我们就可以分裂它。</p><p id="ca76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们创建一个函数，它接收一个包含层大小(神经元数量)的列表，并使用随机层生成器以随机权重填充所有层:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="b50f" class="mu lt iq mq b gy mv mw l mx my">def init_network_params(layers_sizes, key):<br/>    <em class="nz">"""</em><br/><em class="nz">    Given a list of weights for a neural network, initializes the weights of the network</em><br/><em class="nz">    </em><br/><em class="nz">    layers_sizes: list of integers</em><br/><em class="nz">        The number of neurons on each layer of the network</em><br/><em class="nz">    key: PRNGKey</em><br/><em class="nz">        A Jax PRNGKey</em><br/><em class="nz">    """</em><br/>    <em class="nz"># Generate one subkey for layer in the network</em><br/>    keys = random.split(key, len(layers_sizes))<br/>    return [random_layer_params(m, n, k) for m, n, k <strong class="mq ir">in </strong>zip(layers_sizes[:-1], layers_sizes[1:], keys)]</span></pre><h2 id="1483" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">预测函数</h2><p id="913f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在我们将创建一个函数，给定一幅图像和权重，它将输出一个预测。为此，我们将首先定义一个 ReLU 函数:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c8fc" class="mu lt iq mq b gy mv mw l mx my">def relu(x):<br/>    return jnp.maximum(0, x)</span></pre><p id="3be3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，对于每一层，我们必须将权重应用于图像，对偏差求和，将 ReLU 应用于结果，并为下一层传播该激活，因此该方法看起来像这样:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c1be" class="mu lt iq mq b gy mv mw l mx my">def predict(params, x):<br/>    <em class="nz">"""</em><br/><em class="nz">    Function to generate a prediction given weights and the activation</em><br/><em class="nz">    </em><br/><em class="nz">    params: list of matrices</em><br/><em class="nz">        The weights for every layer of the network, including the bias</em><br/><em class="nz">    x: matrix</em><br/><em class="nz">        The activation, or the features, to be predicted</em><br/><em class="nz">    """</em><br/>    activations = x<br/>    <br/>    for w, b <strong class="mq ir">in</strong> params[:-1]:<br/>        output = jnp.dot(w.T, activations) + b<br/>        activations = relu(output)<br/>        <br/>    final_w, final_b = params[-1]<br/>    logits = jnp.dot(final_w.T, activations) + final_b<br/>    <br/>    return logits - logsumexp(logits)</span></pre><p id="f05c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，请注意，我们创建了这个函数，它一次只能对一个图像起作用。我们不能将一批 100 个图像传递给它，因为点积会因为形状不匹配而中断。</p><p id="445c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是 vmap 函数派上用场的地方。我们可以使用它来自动允许我们的预测函数处理批量数据。这是通过以下代码行完成的:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="1a74" class="mu lt iq mq b gy mv mw l mx my">batched_predict = vmap(predict, in_axes=(None, 0))</span></pre><p id="1903" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个参数是我们要对其应用矢量化的函数。第二个是一个 tuple，其中包含原始函数的每个输入参数的值，并说明批处理应该在哪个轴上传播。</p><p id="6735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，tuple (None，0)意味着我们不应该对第一个参数(权重)进行批处理，而应该对第二个参数(图像)的行(轴 0)进行批处理。</p><h2 id="255a" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">损失和准确性</h2><p id="3b49" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在让我们定义两个简单的函数，一个用来计算模型的准确性，另一个用来计算我们的损失:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="58c4" class="mu lt iq mq b gy mv mw l mx my">def accuracy(params, images, targets):<br/>    <em class="nz">"""</em><br/><em class="nz">    Calculates the accuracy of the neural network on a set of images</em><br/><br/><em class="nz">    params: list of matrices</em><br/><em class="nz">        The weights for every layer of the network, including the bias</em><br/><em class="nz">    images: list of matrices</em><br/><em class="nz">        The images to be used on the calculation</em><br/><em class="nz">    targets: list of labels</em><br/><em class="nz">        The true labels for each of the targets</em><br/><br/><em class="nz">    """</em><br/>    target_class = jnp.argmax(targets, axis=1)<br/>    <br/>    <em class="nz"># Predicts the probabilities for each class and get the maximum</em><br/>    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)<br/><br/>    return jnp.mean(predicted_class == target_class)<br/><br/>def loss(params, images, targets):<br/>    preds = batched_predict(params, images)<br/>    return -jnp.mean(preds * targets)</span></pre><h2 id="2d5d" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">更新功能</h2><p id="dd87" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们快到了。现在，我们必须创建更新函数。在我们的 MLP 上，更新将基于步长和损失的梯度来改变权重。正如我们所看到的，Jax 将通过 grad 函数帮助我们轻松做到这一点:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="ad1c" class="mu lt iq mq b gy mv mw l mx my">def update(params, x, y):<br/>    grads = grad(loss)(params, x, y)<br/>    return [(w - step_size * dw, b - step_size * db)<br/>          for (w, b), (dw, db) <strong class="mq ir">in</strong> zip(params, grads)]</span></pre><h2 id="2880" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">训练循环</h2><p id="fd90" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在我们准备运行我们的训练循环。让我们定义每层的神经元数量、梯度的步长、训练的时期数量和步长:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="34ae" class="mu lt iq mq b gy mv mw l mx my">layer_sizes = [784, 512, 512, 10]<br/><br/><em class="nz"># Training parameters</em><br/>step_size = 0.01<br/>num_epochs = 10<br/>batch_size = 128<br/><br/><em class="nz"># Number of labels</em><br/>n_targets = 10</span></pre><p id="caec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们初始化我们的权重:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="7fcd" class="mu lt iq mq b gy mv mw l mx my">params = init_network_params(layer_sizes, random.PRNGKey(0))</span></pre><p id="ceaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们循环这些时期，训练我们的网络:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="fa2f" class="mu lt iq mq b gy mv mw l mx my">for epoch <strong class="mq ir">in</strong> range(num_epochs):<br/>    for x, y <strong class="mq ir">in</strong> get_train_batches(batch_size):<br/>        x = jnp.reshape(x, (len(x), num_pixels))<br/>        y = one_hot(y, num_labels)<br/>        params = update(params, x, y)<br/><br/>    train_acc = accuracy(params, train_images, train_labels)<br/>    test_acc = accuracy(params, test_images, test_labels)<br/>    print("Epoch <strong class="mq ir">{}</strong> in <strong class="mq ir">{:0.2f}</strong> sec".format(epoch, epoch_time))<br/>    print("Training set accuracy <strong class="mq ir">{}</strong>".format(train_acc))<br/>    print("Test set accuracy <strong class="mq ir">{}\n</strong>".format(test_acc))</span></pre><p id="e867" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是了。循环结束后，我们将在 Jax 中成功地训练出一只 MLP！</p><h2 id="3df2" class="mu lt iq bd lu na nb dn ly nc nd dp mc lf ne nf me lj ng nh mg ln ni nj mi nk bi translated">绩效结果</h2><p id="da00" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在前面的代码中，我没有在任何地方使用 jit 函数。但是，正如我所说的，这可以大大改善我们的结果，特别是如果一些计算发生了很多，如我们的参数更新。</p><p id="62bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们要将 JIT 应用到我们的代码中，我们将创建一个 JIT 版本的更新函数，如下所示:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="e696" class="mu lt iq mq b gy mv mw l mx my">jit_update = jit(update)</span></pre><p id="81f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将内部 for 循环的最后一行改为:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="4527" class="mu lt iq mq b gy mv mw l mx my">params = jit_update(params, x, y)</span></pre><p id="805c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图显示了每种情况下每个历元经过的时间:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/89f85ae4a2139640f8326aa2c01f8eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*NjgVnrKJvmfh52njR-MQkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用和不使用 JIT 时每个时期的运行时间。由作者开发。</p></figure><p id="e845" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如我们所见，平均而言，我们的培训时间减少了 2 倍！</p><h1 id="1d9f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="a280" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Jax 是一个新手，但已经显示出巨大的潜力。了解我们的地区每天带给我们的新的可能性是很好的。</p><p id="fd1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将在未来发布一些帖子，介绍一些基于 Jax 的深度学习库，以便我们可以更好地概述这个正在创建的新生态系统。</p><p id="aad6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望这对你有用！</p><p id="6182" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章高度基于 Jax 文档。</p></div></div>    
</body>
</html>