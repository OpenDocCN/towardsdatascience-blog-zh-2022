<html>
<head>
<title>Fine-Tuning LayoutLM v2 For Invoice Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调用于发票识别的LayoutLM v2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e#2022-04-05">https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e#2022-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="526a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从注释到训练和推理</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e0b665aead87cadab64d5f9b2eea5fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRJ7LGADvS75tVKJohatIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:用于发票识别的LayoutLMV2</p></figure><h1 id="24b6" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="1a32" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">自从我写了上一篇关于利用layoutLM transformer模型进行发票识别的文章"<a class="ae mm" rel="noopener" target="_blank" href="/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4">微调Transformer模型</a>"之后，微软发布了一个新的layoutLM v2 transformer模型，与第一个layoutLM模型相比，在性能上有了显著的提高。在本教程中，我将一步一步地演示如何从数据注释到模型训练和推理来微调发票上的layoutLM V2。</p><p id="25dc" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">Google Colab上提供了训练和推理脚本。</p><p id="711b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">培训脚本:</p><div class="ms mt gp gr mu mv"><a href="https://colab.research.google.com/drive/18gi-OUCv8S_1ttD6AoHD9hCUO_0j35-4?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">谷歌联合实验室</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">编辑描述</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">colab.research.google.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><p id="518f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">推理脚本:</p><div class="ms mt gp gr mu mv"><a href="https://colab.research.google.com/drive/1G6fBdP6xHhuT1GdJA1Ntjomx_Um8XMhO?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">谷歌联合实验室</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">编辑描述</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">colab.research.google.com</p></div></div><div class="ne l"><div class="nk l ng nh ni ne nj ks mv"/></div></div></a></div><h1 id="05fa" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">布局V2模型</h1><p id="7c00" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">与第一个layoutLM版本不同，layoutLM v2在Transformer体系结构的第一个输入层中集成了可视功能、文本和位置嵌入，如下所示。这使得模型能够学习视觉和文本信息之间的跨模态交互，在单个多模态框架中文本、布局和图像之间的交互。下面是摘要中的一个片段:“<em class="nl">实验结果表明，LayoutLMv2大幅优于LayoutLM，并在多种下游视觉丰富的文档理解任务上取得了新的最新成果，包括FUNSD (0.7895 → 0.8420)、CORD (0.9493 → 0.9601)、SROIE (0.9524 → 0.9781)、Kleister-NDA (0.8340 → 0.8520)、RVL</em></p><p id="2d23" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">更多信息请参考<a class="ae mm" href="https://arxiv.org/pdf/2012.14740.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/353b52417f1d648df9bf62b93f77ab61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfsiqeTguo-qx-cXxpppJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布局LMV2架构(图片来自<a class="ae mm" href="https://arxiv.org/pdf/2012.14740.pdf" rel="noopener ugc nofollow" target="_blank">徐等人，2022 </a>)</p></figure><h1 id="36ba" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">注释</h1><p id="e86a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于本教程，我们已经使用<a class="ae mm" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank">ubai文本注释工具</a>注释了总共220张发票。<a class="ae mm" href="https://ubiai.tools/live-demo?from=OCR" rel="noopener ugc nofollow" target="_blank"> UBIAI OCR注释</a>允许以常规或手写形式直接在原生pdf、扫描文档或图像PNG和JPG上进行注释。我们最近增加了对20多种语言的支持，包括阿拉伯语和希伯来语等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/8b7cfb2d31918cd59d571d3560f866d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZFwJ7MFcYzqr3CxvldbYeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:UBIAI多语言OCR注释</p></figure><p id="2bc8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是关于如何使用该工具注释pdf和图像的精彩概述:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">UBIAI教程由<a class="ae mm" href="https://www.youtube.com/@karndeepsingh" rel="noopener ugc nofollow" target="_blank"> Karndeep Singh </a></p></figure><p id="1067" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">除了带标签的文本偏移量和边界框，我们还需要导出每个带注释的文档的图像。使用<a class="ae mm" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank"> UBIAI </a>可以很容易地做到这一点，因为它将所有注释和每个文档的图像一起导出到一个ZIP文件中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/df09f57a9ddbd3629dcae21b2cff8313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*U5-ib5Fntn_mgwgJUTu5zA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:注释数据JSON输出</p></figure><h1 id="9b2c" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">数据预处理:</h1><p id="714f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">从<a class="ae mm" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank"> UBIAI </a>导出ZIP文件后，我们将文件上传到google drive文件夹。我们将使用google colab进行模型训练和推理。</p><ul class=""><li id="62fd" class="nr ns it ls b lt mn lw mo lz nt md nu mh nv ml nw nx ny nz bi translated">第一步是打开一个google colab，连接你的google drive并安装transfromers和detectron2软件包:</li></ul><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="3a7a" class="of kz it ob b gy og oh l oi oj">from google.colab import drive</span><span id="eaba" class="of kz it ob b gy ok oh l oi oj">drive.mount('/content/drive')</span><span id="eb22" class="of kz it ob b gy ok oh l oi oj">!pip install -q git+https://github.com/huggingface/transformers.git</span><span id="b8d9" class="of kz it ob b gy ok oh l oi oj">!pip install -q datasets seqeval</span><span id="7231" class="of kz it ob b gy ok oh l oi oj">!python -m pip install -q 'git+https://github.com/facebookresearch/detectron2.git'</span></pre><ul class=""><li id="09c2" class="nr ns it ls b lt mn lw mo lz nt md nu mh nv ml nw nx ny nz bi translated">为了简化数据预处理和模型训练步骤，我们创建了preprocess.py和train.py文件，其中包含启动训练所需的所有代码。从github克隆文件:</li></ul><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="ace2" class="of kz it ob b gy og oh l oi oj">! rm -r layoutlmv2_fine_tuning</span><span id="073c" class="of kz it ob b gy ok oh l oi oj">! git clone -b main <a class="ae mm" href="https://github.com/walidamamou/layoutlmV2.git" rel="noopener ugc nofollow" target="_blank">https://github.com/walidamamou/layoutlmV2.git</a></span></pre><ul class=""><li id="1b4c" class="nr ns it ls b lt mn lw mo lz nt md nu mh nv ml nw nx ny nz bi translated">接下来，我们需要解压缩导出的数据集，并将所有文件放在一个文件夹中:</li></ul><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="11ac" class="of kz it ob b gy og oh l oi oj">IOB_DATA_PATH = "/content/drive/MyDrive/LayoutLM_data/Invoice_Project_XhDtcXH.zip"</span><span id="51e3" class="of kz it ob b gy ok oh l oi oj">! cd /content/</span><span id="c647" class="of kz it ob b gy ok oh l oi oj">! rm -r data</span><span id="20eb" class="of kz it ob b gy ok oh l oi oj">! mkdir data</span><span id="9996" class="of kz it ob b gy ok oh l oi oj">! cp "$IOB_DATA_PATH" data/dataset.zip</span><span id="4a11" class="of kz it ob b gy ok oh l oi oj">! cd data &amp;&amp; unzip -q dataset &amp;&amp; rm dataset.zip</span><span id="cb57" class="of kz it ob b gy ok oh l oi oj">! cd ..</span></pre><h1 id="878f" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">微调LayoutLM v2模型:</h1><p id="5e69" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们几乎准备好启动培训，我们只需要指定几个超参数来配置我们的模型和模型输出的路径。你当然可以改变这些变量来得到最好的结果。对于本教程，我们使用33%的测试规模，批量= 4，学习率= 5e-5和50个时期。</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="a994" class="of kz it ob b gy og oh l oi oj">#!/bin/bash</span><span id="0b26" class="of kz it ob b gy ok oh l oi oj">#preprocessing args</span><span id="56e2" class="of kz it ob b gy ok oh l oi oj">TEST_SIZE = 0.333</span><span id="3686" class="of kz it ob b gy ok oh l oi oj">DATA_OUTPUT_PATH = "/content/"</span><span id="beae" class="of kz it ob b gy ok oh l oi oj">#training args</span><span id="fe3c" class="of kz it ob b gy ok oh l oi oj">PROCESSED_DATA_PATH = DATA_OUTPUT_PATH</span><span id="d01d" class="of kz it ob b gy ok oh l oi oj">MODEL_OUTPUT_PATH = "/content/layoutlmv2-finetuned"</span><span id="942c" class="of kz it ob b gy ok oh l oi oj">TRAIN_BATCH_SIZE = 4</span><span id="0099" class="of kz it ob b gy ok oh l oi oj">VALID_BATCH_SIZE = 2</span><span id="940f" class="of kz it ob b gy ok oh l oi oj">LEARNING_RATE = 5e-5</span><span id="dab1" class="of kz it ob b gy ok oh l oi oj">EPOCHS = 50</span></pre><p id="1639" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们现在准备启动模型，只需运行:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="a9b7" class="of kz it ob b gy og oh l oi oj">! python3 layoutlmv2_fine_tuning/train.py --epochs $EPOCHS \</span><span id="e413" class="of kz it ob b gy ok oh l oi oj">--train_batch_size $TRAIN_BATCH_SIZE \</span><span id="4ac7" class="of kz it ob b gy ok oh l oi oj">--eval_batch_size $VALID_BATCH_SIZE \</span><span id="1e66" class="of kz it ob b gy ok oh l oi oj">--learning_rate $LEARNING_RATE \</span><span id="7a99" class="of kz it ob b gy ok oh l oi oj">--output_dir $MODEL_OUTPUT_PATH \</span><span id="1ae1" class="of kz it ob b gy ok oh l oi oj">--data_dir $PROCESSED_DATA_PATH</span></pre><p id="2186" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">训练完成后，精确度、召回率和F1分数将显示如下。我们获得了0.75的F1分数和0.96的准确度，这对于注释220张发票来说是不错的分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/4889da3cbdcfd894ced9b526a3d741e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsqFPKvGB9UiEKlEpjE1qA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:LayoutLMV2分数</p></figure><h1 id="6570" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">与layoutLM V2的推论:</h1><p id="68df" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们现在准备在一张新的看不见的发票上测试我们新训练的模型。对于这一步，我们将使用Google的Tesseract对文档进行OCR，并使用layoutLM V2从发票中提取实体。</p><p id="2f55" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们安装pytesseract库:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="997c" class="of kz it ob b gy og oh l oi oj">## install tesseract OCR Engine</span><span id="5e0b" class="of kz it ob b gy ok oh l oi oj">! sudo apt install tesseract-ocr</span><span id="93ce" class="of kz it ob b gy ok oh l oi oj">! sudo apt install libtesseract-dev</span><span id="27ea" class="of kz it ob b gy ok oh l oi oj">## install pytesseract , please click restart runtime button in the cell output and move forward in the notebook</span><span id="48ac" class="of kz it ob b gy ok oh l oi oj">! pip install pytesseract</span><span id="c9e8" class="of kz it ob b gy ok oh l oi oj">## install model requirements</span><span id="36d2" class="of kz it ob b gy ok oh l oi oj">!pip install -q git+https://github.com/huggingface/transformers.git</span><span id="f550" class="of kz it ob b gy ok oh l oi oj">!pip install -q torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><span id="d687" class="of kz it ob b gy ok oh l oi oj">!python -m pip install -q 'git+https://github.com/facebookresearch/detectron2.git'</span></pre><p id="b5d1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">接下来，我们将使用文件layoutlmv2Inference.py(先前克隆的),它将处理OCRd发票并应用模型来获得预测。</p><p id="22f3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，指定您的模型路径、图像路径、输出路径，并运行如下所示的推理脚本:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="498e" class="of kz it ob b gy og oh l oi oj">model_path = "/content/drive/MyDrive/LayoutLM v2 Model/layoutlmv2-finetuned.pth"</span><span id="1e8b" class="of kz it ob b gy ok oh l oi oj">imag_path = "/content/invoice_eng.PNG"</span><span id="6646" class="of kz it ob b gy ok oh l oi oj">output_path = "/content/Inference_output2"</span><span id="c7ba" class="of kz it ob b gy ok oh l oi oj">! python3 layoutlmv2Inference.py "$model_path" "$imag_path" "$output_path"</span></pre><p id="70b4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">一旦推断完成，您将在图像上找到覆盖的预测，以及一个包含所有标签、文本和推断中的偏移的JSON文件)output2文件夹。让我们看看模型预测:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ef44751f8f19311ef121ed4571f698b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4UwgRh1ZQBeZYAJG8N2SGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:LayoutLMV2预测</p></figure><p id="6fad" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是JSON文件的一个示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/63d330cfa01e1345f0e8511c75f29132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIvardHpzwpNAmucM8MGew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:JSON输出</p></figure><p id="511d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该模型能够预测大多数实体，如卖方，日期，发票号码和总额，但错误地预测“单价”为TTC_ID。这表明我们需要注释更多类型的发票，这样我们的模型就可以学习一般化。</p><h1 id="95c0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><p id="06d5" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">总之，我们已经展示了如何从注释开始到训练和推理来微调发票上的layoutLM V2的逐步教程。该模型可以在任何其他半结构化文档上进行微调，如驾照、合同、政府文档、财务文档等。</p><p id="6461" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果你想尝试UBIAI的OCR注释功能，只需免费注册即可开始注释。</p><p id="9fda" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在推特上关注我们<a class="ae mm" href="https://twitter.com/UBIAI5" rel="noopener ugc nofollow" target="_blank"> @UBIAI5 </a>或<a class="ae mm" href="https://walidamamou.medium.com/subscribe" rel="noopener">订阅这里</a>！</p><p id="f0b7" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">页（page的缩写）s:写完这篇文章后，一个关于培训layoutlmV3的新教程已经发布，如果你想了解更多，请点击这个<a class="ae mm" rel="noopener" target="_blank" href="/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf">链接</a>。</p></div></div>    
</body>
</html>