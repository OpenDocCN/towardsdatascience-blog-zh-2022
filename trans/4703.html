<html>
<head>
<title>Pair-Wise Hyperparameter Tuning with the Native XGBoost API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用原生XGBoost API进行成对超参数调优</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pair-wise-hyperparameter-tuning-with-the-native-xgboost-api-2f40a2e382fa#2022-10-19">https://towardsdatascience.com/pair-wise-hyperparameter-tuning-with-the-native-xgboost-api-2f40a2e382fa#2022-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2ae1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在解决偏差-方差权衡时搜索全局最小值</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d37e57ba6e131b233bb5ebd9f25dd43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mP3Yw8lshVRIoOm2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马库斯·斯皮斯克在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0801" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于Boosting Machine有过拟合的倾向，XGBoost非常关注解决<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ls">偏差-方差权衡</em> </strong> </a> <strong class="ky ir"> <em class="ls"> </em> </strong>并通过超参数调整方便用户应用各种正则化技术。</p><p id="2640" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章将带您浏览使用原生XGBoost API的超参数调优的代码实现，以解决偏差-方差权衡问题。该项目的全部代码发布在<a class="ae kv" href="https://github.com/deeporigami/Portfolio/blob/main/XGBoost/Version%201%20XGBoost%20Hyperparameter%20Tuning%20and%20Performance%20Landscape.ipynb" rel="noopener ugc nofollow" target="_blank">我的Github库</a>中。</p><p id="fa88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">读完这篇文章后，您可以复制或修改我在下面演示的过程，用本机XGBoost API为您自己的回归项目进行超参数调优。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="f456" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">介绍</h1><p id="074f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们的目标是对原生XGBoost API执行超参数调优，以提高其回归性能，同时解决偏差-方差权衡问题，特别是减轻Boosting机器的过度拟合倾向。</p><p id="5671" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了进行超参数调谐，本次分析采用<strong class="ky ir"> <em class="ls">网格搜索</em> </strong>的方法。换句话说，我们选择超参数的搜索网格，并计算搜索网格上所有超参数数据点的模型性能。然后，我们确定<strong class="ky ir"> <em class="ls">性能的全局局部最小值— </em> </strong>或产生最佳性能的超参数数据点(目标函数的最小值)—作为调整模型的最佳超参数值。</p><h2 id="4ea0" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">计算约束</h2><p id="9221" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">超参数调整在计算上可能非常昂贵，这取决于您如何设置搜索网格。因为它需要在搜索网格中设置的所有数据点上迭代性能计算。数据点越多，计算成本就越高。很简单。</p><p id="b353" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，我的笔记本电脑计算能力非常有限。但是，一个好消息是，Google Colab为其免费帐户的每个用户提供一个GPU。并且原生的XGBoost API具有GPU支持特性。总之，我可以通过使用Google Colab的GPU运行原生XGBoost API来加快调优过程。</p><p id="895e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管如此，当选择的搜索网格有大量的数据点要覆盖时，一个GPU仍然不够。因此，我仍然需要解决计算资源的限制。</p><p id="f855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理想情况下，对所有选定的超参数进行单个联合调整将准确捕捉它们之间所有可能的相互作用。然而，联合调谐具有巨大的搜索网格数据点，因此在计算上将是昂贵的。在这种情况下，我将执行多个<strong class="ky ir"> <em class="ls">成对超参数调整</em> </strong>，而不是<strong class="ky ir"> <em class="ls">单个联合同时调整所有超参数</em> </strong>。这不仅会减少用于调整的超参数数据点的数量，还允许我们为每个成对调整呈现性能前景的3D可视化。当然，这种方法有一个问题。我们将在以后讨论这个问题。</p><p id="65f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们开始编码。首先，我们需要下载数据进行分析。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="e1a7" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">分析</h1><h1 id="7fa5" class="ma mb iq bd mc md nj mf mg mh nk mj mk jw nl jx mm jz nm ka mo kc nn kd mq mr bi translated">A.数据</h1><p id="229e" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">对于回归分析，我们使用sklearn API的内置加州住房数据集。数据集的来源是StatLib的数据集存档中的<a class="ae kv" href="http://lib.stat.cmu.edu/datasets/" rel="noopener ugc nofollow" target="_blank"> houses.zip。数据集的许可证“License: BSD 3 clause”可以在这里的代码</a>的<a class="ae kv" href="https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/datasets/_california_housing.py" rel="noopener ugc nofollow" target="_blank">的第22行找到。关于数据集的解释，请阅读</a><a class="ae kv" href="https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/datasets/_california_housing.py" rel="noopener ugc nofollow" target="_blank">此链接</a>。</p><p id="5550" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ls">fetch _ California _ Housing</em></strong>函数</a>从<strong class="ky ir"><em class="ls">sk learn . datasets</em></strong>下载加州房屋数据集。</p><p id="eb60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将数据集存储到变量<strong class="ky ir"> <em class="ls"> housing_data </em> </strong>中。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="9b2e" class="mx mb iq np b gy nt nu l nv nw">from sklearn.datasets import fetch_california_housing<br/>housing_data = fetch_california_housing()</span></pre><p id="9580" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们需要将导入的数据集转换成熊猫数据帧。我们将特征和目标变量分别分离成df1和df2。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="2b4c" class="mx mb iq np b gy nt nu l nv nw">descr = housing_data['DESCR']<br/>feature_names = housing_data['feature_names']<br/>data = housing_data['data']<br/>target = housing_data['target']<br/>df1 = pd.DataFrame(data=data)<br/>df1.rename(columns={0: feature_names[0], 1: feature_names[1], 2: feature_names[2], 3: feature_names[3],<br/> 4: feature_names[4], 5: feature_names[5], 6: feature_names[6], 7: feature_names[7]}, inplace=True)<br/>df2 = pd.DataFrame(data=target)<br/>df2.rename(columns={0: 'Target'}, inplace=True)<br/>housing = pd.concat([df1, df2], axis=1)<br/>print(housing.columns)</span></pre><p id="33ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们看看数据帧功能的摘要信息。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="3436" class="mx mb iq np b gy nt nu l nv nw">df1.info()</span></pre><p id="3651" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是输出:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="cdd1" class="mx mb iq np b gy nt nu l nv nw">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 20640 entries, 0 to 20639<br/>Data columns (total 8 columns):<br/> #   Column      Non-Null Count  Dtype  <br/>---  ------      --------------  -----  <br/> 0   MedInc      20640 non-null  float64<br/> 1   HouseAge    20640 non-null  float64<br/> 2   AveRooms    20640 non-null  float64<br/> 3   AveBedrms   20640 non-null  float64<br/> 4   Population  20640 non-null  float64<br/> 5   AveOccup    20640 non-null  float64<br/> 6   Latitude    20640 non-null  float64<br/> 7   Longitude   20640 non-null  float64<br/>dtypes: float64(8)<br/>memory usage: 1.3 MB</span></pre></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="850f" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak"> <em class="nx"> B .库导入</em> </strong></h1><p id="4b62" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">接下来，让我们看一下我们将要使用的库。</p><ul class=""><li id="3fb7" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">首先，我们需要原生的XGBoost API来运行它和它的<strong class="ky ir"> <em class="ls"> plot_importance </em> </strong>来显示调优后所选模型的特性重要性。</li></ul><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="6454" class="mx mb iq np b gy nt nu l nv nw">import xgboost as xgb<br/>from xgboost import plot_importance</span></pre><ul class=""><li id="7de4" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">我们还使用来自<strong class="ky ir"><em class="ls">sk learn . metrics</em></strong>的<strong class="ky ir"><em class="ls">mean _ squared _ error</em></strong>进行度量。</li></ul><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="dfb2" class="mx mb iq np b gy nt nu l nv nw">from sklearn.metrics import mean_squared_error</span></pre><ul class=""><li id="59b8" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">对于训练测试数据集的拆分，从<strong class="ky ir"><em class="ls">sk learn . model _ selection</em></strong>中的<strong class="ky ir"><em class="ls">train _ test _ split</em></strong>可以方便我们拆分数据帧。</li></ul><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="b3d7" class="mx mb iq np b gy nt nu l nv nw">from sklearn.model_selection import train_test_split</span></pre><ul class=""><li id="95cc" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">为了可视化，我们使用Matplotlib中的以下项目来呈现3D图像。</li></ul><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="8d53" class="mx mb iq np b gy nt nu l nv nw">from matplotlib import pyplot<br/>import matplotlib.pyplot as plt<br/><br/>from mpl_toolkits.mplot3d import Axes3D</span></pre></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="d34c" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak"> <em class="nx"> C .超参数&amp;成对调谐</em> </strong></h1><h2 id="c8bd" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">1.选择用于调谐的超参数</h2><p id="7776" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">原生XGBoost API 内置了这么多超参数。由于给定的有限计算资源可用性，我们必须做一些挑选。</p><p id="387d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://machinelearningmastery.com/about" rel="noopener ugc nofollow" target="_blank"><em class="ls">Jason Brownlee</em></a><em class="ls"/>概述了正则化有<a class="ae kv" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"> 5类超参数</a>:迭代控制、收缩、树约束、随机采样、L1 &amp; L2正则化。根据他的框架，该分析选择了以下9个超参数来解决偏差-方差权衡问题。有关每个超参数的详细信息，请访问下面每个超参数中设置的超链接。</p><p id="3df5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a)迭代控制:以下两个超参数操作迭代控制。</p><ul class=""><li id="4338" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=cv()#xgboost.train" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">num _ boost _ round</strong>T37】</a></li><li id="b664" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=cv()#xgboost.train" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">【早_停_轮</strong> </a></li></ul><p id="b2fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其余的超参数需要设置在一个字典中，<strong class="ky ir"> <em class="ls"> pram_dict </em> </strong>。当我们在搜索网格上的数据点上迭代k倍交叉验证时，我们可以使用这个参数字典<strong class="ky ir"> <em class="ls"> </em> </strong>来操纵超参数的值。</p><p id="86eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">b)收缩率:</p><ul class=""><li id="b30f" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> eta </strong> </a></li></ul><p id="9c02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">c)采油树助推器约束:</p><ul class=""><li id="1019" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">最大_深度</strong> </a></li><li id="e585" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> min_child_weight </strong> </a></li><li id="bde3" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">伽玛</strong> </a></li></ul><p id="95d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d)随机子采样:</p><ul class=""><li id="6794" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">子样本</strong> </a></li><li id="df8e" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank">T17】col sample _ bytreeT19】</a></li></ul><p id="2b01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">e) L1和L2的正规化</p><ul class=""><li id="1b90" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">阿尔法</strong> </a></li><li id="94b3" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">λ</strong></a></li></ul><p id="7003" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f)不用于调谐的附加条目:</p><p id="923a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们需要在参数字典中指定以下4个额外的超参数。作为一项预防措施，这些超参数将是固定的，但不会被调整。</p><ul class=""><li id="446c" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><strong class="ky ir"> <em class="ls">目标</em> </strong>:选择回归的目标函数。</li><li id="6666" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> eval_metric </em> </strong>:选择回归的绩效评估指标</li><li id="b84e" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> gpu_id </em> </strong> : 0由Google Colab早先指定</li><li id="6986" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"><em class="ls">tree _ method</em></strong>:<strong class="ky ir"><em class="ls">gpu _ hist</em></strong>选择GPU设置</li></ul><p id="6011" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，我们已经讨论了3件事情:选择用于调整的超参数、参数字典的初始设置以及用于评估初始偏差-方差权衡的初始(预调整)性能曲线。我们发现了实质性的差异，或者说过度拟合。</p><h2 id="bfa0" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">3.成对超参数调谐</h2><p id="9112" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">理想地，对所有这些超参数的单个联合调谐将准确地捕捉参数字典中这些所选超参数之间的所有可能的相互作用。然而，联合调谐具有巨大的搜索网格数据点，因此在计算上将是昂贵的。</p><p id="621c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了节约分析的计算成本，我从8个参数对中选择了4个:即</p><ul class=""><li id="53ec" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">(最大深度，eta)</li><li id="022d" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">(子样本，列样本_字节树)</li><li id="5f37" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">(最小_孩子_体重，伽玛)，以及</li><li id="6494" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">(寄存器α，寄存器λ)</li></ul><p id="8105" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我对这4个超参数对中的每一个进行了成对调整。每次调整后，最佳调整结果用于替换超参数对的初始值。</p><p id="e19b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这种方式，每一次成对调优都会逐步提高调优模型的性能。这总体上减少了搜索网格数据点的数量，从而以网格搜索的公正性为代价节省了调整过程的计算成本。当然，这里的基本假设是，通过4次成对调节获得的结果不会与通过理想的单一联合调节所有8个超参数获得的结果有实质性的偏离。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="91dc" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak"> <em class="nx"> D .预调优回归:训练和测试数据集上的性能曲线</em> </strong></h1><p id="4068" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在调优之前，我们首先需要初始化参数字典的值，以便对训练数据集和测试数据集执行第一次回归。它是沿着训练迭代比较那些数据集上的初始性能曲线。这很重要，原因有二。</p><ul class=""><li id="0dd8" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">首先，我们想要比较训练性能和测试性能，以评估初始状态下的偏差-方差权衡，即欠拟合-过拟合权衡的问题。</li><li id="0b14" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">第二，在调整结束时，我们希望比较调整前的性能曲线(包括训练和测试)和调整后的性能曲线，以评估调整的正则化影响。我们希望评估调优是否有效地对模型执行正则化，以解决偏差-方差权衡问题。</li></ul><p id="e705" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是我们的参数字典的初始化。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="94f2" class="mx mb iq np b gy nt nu l nv nw">param_dict={'objective': 'reg:squarederror', # fixed. pick an objective function for Regression. <br/> 'max_depth': 6,<br/> 'subsample': 1,<br/> 'eta': 0.3,<br/> 'min_child_weight': 1,<br/> 'colsample_bytree': 1,<br/> 'gamma': 0,<br/> 'reg_alpha': 0.1,<br/> 'reg_lambda': 1,<br/> 'eval_metric': 'mae', # fixed. picked a evaluation metric for Regression.<br/> 'gpu_id': 0, <br/> 'tree_method': 'gpu_hist' # XGBoost's built-in GPU support to use Google Colab's GPU<br/>}</span></pre><p id="d49a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此处分配的值仅用于初始化目的。这些值(除了objective、eval_metric、gpu_id和tree_method)将在超参数调整期间发生变化。</p><p id="e178" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了pram_dict中指定的那些超参数，我们还需要在pram_dict之外初始化另外两个超参数num_boost_round和early_stopping_rounds。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="9ce2" class="mx mb iq np b gy nt nu l nv nw">num_boost_round = 1000<br/>early_stopping_rounds = 10</span></pre><p id="ba22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，利用初始化后的参数字典param_dict，我们可以使用内置的原生训练函数xgb.train()，定义一个自定义函数，称之为<strong class="ky ir"><em class="ls">bias _ variance _ performance()</em></strong>。其目的是通过比较训练数据集和测试数据集之间的初始模型性能曲线，评估沿着训练迭代的偏差-方差权衡。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="4725" class="mx mb iq np b gy nt nu l nv nw">def bias_variance_performance(params):<br/>   <br/>    evals_result = {}<br/>    model = xgb.train(<br/>        params,<br/>        dtrain=DM_train,<br/>        num_boost_round=num_boost_round,<br/>        evals=[(DM_train, "Train"), (DM_test, "Test")],<br/>        early_stopping_rounds=early_stopping_rounds,<br/>        evals_result=evals_result<br/>    )<br/>    train_error = evals_result["Train"]["mae"]<br/>    test_error = evals_result["Test"]["mae"]<br/>    <br/>   print("Best MAE before the Tuning: {:.4f} with {} rounds".format(model.best_score, model.best_iteration+1))</span><span id="b430" class="mx mb iq np b gy om nu l nv nw">return model, evals_result</span></pre><p id="feed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于内置训练函数xgb.train()的详细信息，请参考<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=train()#xgboost.train" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="cf34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是输出:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="0c38" class="mx mb iq np b gy nt nu l nv nw">Best MAE before the Tuning: 0.3103 with 121 rounds</span></pre><p id="a725" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出显示测试性能的最佳值，而不是训练性能的最佳值。在第121轮迭代中检测到了最佳的测试性能。换句话说，初始(预调整)模型(具有初始超参数设置)是在121轮迭代时确定的。</p><p id="6f96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们比较第121次迭代时训练数据集和测试数据集之间的初始(预调优)模型性能。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="f330" class="mx mb iq np b gy nt nu l nv nw">[120]	Train-mae:0.17322	Test-mae:0.31031</span></pre><p id="d8f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在121轮迭代中确定的初始(预调整)模型(具有初始超参数设置)的训练性能为0.17322，测试性能为0.3103。巨大的差距表明差异，或过度拟合。</p><p id="b99f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的图表有两条初始的(预调优的)性能曲线以及迭代循环:一条是蓝色的用于训练性能，另一条是红色的用于测试性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/4d1f0527bf2ea8799929e9d4012db916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GqjO2FDleQobXGk6H5KB4Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="feaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着迭代的进行，初始的(预调优的)训练性能继续提高，而初始的(预调优的)测试性能在某个点上停滞不前。这表明在初始(预调整)模型中有一个显著的<strong class="ky ir"> <em class="ls">方差</em> </strong>，或<strong class="ky ir"> <em class="ls">过拟合</em> </strong>。</p><p id="cf5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了解决偏差-方差的权衡，我们通过调整原生XGBoost API的超参数来对模型进行正则化。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="36eb" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak"> <em class="nx"> E .调法:</em> </strong></h1><p id="63bd" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在调优方法方面，我想强调几个重要的脚注。</p><ul class=""><li id="c66a" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">我为每个超参数对设置了<strong class="ky ir"> <em class="ls">搜索网格</em> </strong>，并使用<strong class="ky ir"> <em class="ls">网格搜索方法</em> </strong>进行成对参数调整。</li><li id="67d9" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">我迭代了<strong class="ky ir"> <em class="ls"> k重交叉验证</em> </strong>来计算预定搜索网格上所有数据点的训练数据集的性能度量。</li><li id="cdf2" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">可视化使用了<a class="ae kv" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank">三角插值技术</a>来填充搜索网格数据点之间的间隙，并渲染出一个平滑的表演场景表面。</li></ul><h2 id="ff7a" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated"><strong class="ak"> <em class="nx"> a) k倍交叉验证</em> </strong></h2><p id="9004" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">为了运行<a class="ae kv" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank"> k重交叉验证</a>，我们使用了内置的交叉验证函数，<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.cv" rel="noopener ugc nofollow" target="_blank"> xgb.cv() </a>。作为一项预防措施，k-fold交叉验证<a class="ae kv" href="https://stats.stackexchange.com/questions/436894/should-a-training-set-be-used-for-grid-search-with-cross-validation" rel="noopener ugc nofollow" target="_blank">只在训练数据集</a>(验证数据集从该数据集分配)上执行:不要将测试数据集传递到交叉验证函数:xgb.cv()。</p><p id="7276" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以对每个超参数对的预定搜索网格上的所有数据点重复k重交叉验证，以搜索回归性能度量的全局最小值，即平均绝对误差。</p><p id="c6a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">自定义k重交叉验证函数:(可选)</strong></p><p id="2e77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了方便调优迭代，我们可以通过使用原生XGBoost API的内置交叉验证函数xgb.cv()来定制一个k重交叉验证函数。对于我们的例子，我们为我们的分析设置了5个折叠。或者，可以探索其他值，例如10。</p><p id="756e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是我的自定义交叉验证函数cross_validation()的代码。这种定制是个人偏好的问题，因此是可选的步骤。关于要传递的超参数，<a class="ae kv" href="https://xgboost.readthedocs.io/en/stable/python/python_api.html?highlight=cv#xgboost.cv" rel="noopener ugc nofollow" target="_blank">请查看文档</a>。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="6609" class="mx mb iq np b gy nt nu l nv nw">def cross_validation(param_dict, DM_train, metrics, num_boost_round, early_stopping_rounds): <br/>    <br/>    cv_results = xgb.cv(<br/>        params=param_dict,<br/>        dtrain=DM_train, <br/>        num_boost_round=num_boost_round,<br/>        seed=42, # seed for randomization<br/>        nfold=5, # k of k-fold cross-validation<br/>        metrics=metrics,<br/>        early_stopping_rounds=early_stopping_rounds   <br/>    )<br/>    <br/>    return cv_results</span></pre><p id="a581" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">成对网格搜索:pair_wise_gridsearch_CV </strong></p><p id="efd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了定义成对网格搜索，我们可以定义一个函数，在超参数对的搜索网格的所有数据点上迭代k重交叉验证自定义函数cross_validation()。下一个函数，<strong class="ky ir"><em class="ls">pair _ wise _ grid search _ CV</em>，</strong>采用3个变量:</p><ul class=""><li id="65bc" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">param1_name和param1_name:要优化的超参数对的名称。</li><li id="9e3a" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">gridsearch_params:要调整的超参数对的搜索网格。</li></ul><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="7de0" class="mx mb iq np b gy nt nu l nv nw">def pair_wise_gridsearch_CV(param1_name, param2_name, gridsearch_params):</span><span id="fb6c" class="mx mb iq np b gy om nu l nv nw">min_mae = float("Inf")<br/>    best_params = None</span><span id="1de4" class="mx mb iq np b gy om nu l nv nw">x = []<br/>    y = []<br/>    z = []<br/>    for param1, param2 in gridsearch_params:<br/>        # Update our parameter dictionary for the tuning<br/>        param_dict[param1_name] = param1<br/>        param_dict[param2_name] = param2<br/>        print("CV with {}={}, {}={}".format(param1_name, param1,<br/>                             param2_name,<br/>                             param2))<br/>        # calculate cross_validation<br/>        cv_results = cross_validation(param_dict, DM_train, metrics={'mae'}, num_boost_round=1000, early_stopping_rounds=10)<br/>        mean_mae = cv_results['test-mae-mean'].min()<br/>        boost_rounds = cv_results['test-mae-mean'].argmin()<br/>        <br/>        x.append(param1)<br/>        y.append(param2)<br/>        z.append(mean_mae)<br/>        <br/>        print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))<br/>        <br/>        if mean_mae &lt; min_mae:<br/>            min_mae = mean_mae<br/>            best_params = (param1, param2)<br/>     <br/>return x, y, z, min_mae, best_params</span></pre><p id="ed1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">函数pair_wise_gridsearch_CV在gridsearch_params中定义的超参数对的搜索网格上的所有数据点上迭代自定义交叉验证函数cross_validation，以搜索性能前景的全局最小值。</p><p id="4991" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">gridsearch_params需要在每次成对超参数调整之前预先确定。这是后面要展示的。</p><h2 id="63a8" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated"><strong class="ak"> <em class="nx"> b)超参数对的性能景观的3D可视化</em> </strong></h2><p id="8ba7" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，我们可以定义一个可视化实用函数<strong class="ky ir"> <em class="ls"> Trisurf() </em> </strong>来呈现超参数的性能景观的3D可视化。我们将使用matplotlib的<a class="ae kv" href="https://matplotlib.org/3.1.0/tutorials/toolkits/mplot3d.html#tri-surface-plots" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ls">plot _ tris URF()</em></strong></a><strong class="ky ir"><em class="ls">。</em>T13】</strong></p><p id="e93e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">plot _ tri surf()<strong class="ky ir"><em class="ls"/></strong>使用三角插值技术<a class="ae kv" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank">填充搜索网格上的数据点之间的间隙，以渲染性能景观的平滑表面。</a></p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="fe5b" class="mx mb iq np b gy nt nu l nv nw">import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="50ac" class="mx mb iq np b gy om nu l nv nw">def Trisurf(xname, yname, x, y, z, min_mae, best_params):<br/>    # (xname, yname): the name of variables to be displayed for visualization<br/>    # (`x`,`y`): all the hyperparamters' datapoints on the search grid <br/>    # (`z`): the results of the evaluation metric the results of the evaluation metric over the datapoints<br/>    # (`min_mae`, `best_params`): the Native XGBoost API's `k-fold cross-validation` function's outputs the minimum mae result and the corresponding datapoint of the hyperparameters<br/>    fig = plt.figure(figsize=(12, 12))<br/>    ax = fig.add_subplot(111, projection="3d")<br/>    # Creating color map<br/>    my_cmap = plt.get_cmap('RdPu')<br/>    <br/>    # Creating dot plot of the minimum<br/>    ## min_mae*1.01 to float it for the visualization purpose.<br/>    ax.scatter(best_params[0], best_params[1], min_mae, color='red')<br/>    # Tri-Surface Plot<br/>    ax.plot_trisurf(x,y,z,cmap = my_cmap,<br/>                         linewidth = 0.2,<br/>                         antialiased = True,<br/>                         edgecolor = 'grey')<br/>    ax.set_xlabel(xname)<br/>    ax.set_ylabel(yname)<br/>    ax.set_zlabel('mae')<br/>    <br/>    # Write the Title<br/>    ax.set_title("Hyperparameters Performance Landscape ("+str(xname) + ' &amp; '+str(yname)+")") <br/>    <br/>    # Download the figure as jpeg file.<br/>    figname = str(xname) + '_'+str(yname)<br/>    fig.savefig(str(figname) + '.jpg')</span></pre><p id="a296" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于代码的更多细节，请访问<a class="ae kv" href="https://github.com/deeporigami/Portfolio/blob/main/XGBoost/Version%201%20XGBoost%20Hyperparameter%20Tuning%20and%20Performance%20Landscape.ipynb" rel="noopener ugc nofollow" target="_blank">我的Github repo </a>。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="e9e6" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">F.调整实现</h1><p id="05f0" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">到目前为止，我们已经准备好了调优所需的所有定制实用函数。</p><p id="2a44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们准备实施调优过程。</p><p id="4ab1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将一次对每一对进行成对的超参数调整。每次成对调整后，我们将使用最佳结果(产生损失函数的最低值)更新每个超参数对的值。我们继续下一对超参数来重复这个过程。通过这种方式，我们可以逐步提高模型的性能。</p><p id="5e86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将对4对超参数进行这一过程。</p><h2 id="83c0" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">a)第一对:<strong class="ak"> <em class="nx"> max_depth </em> </strong>和eta</h2><p id="f281" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，为了调整第一个超参数对max_depth和eta，我们为它们设置搜索网格如下。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="3d1f" class="mx mb iq np b gy nt nu l nv nw">pair_wise_gridsearch_params1 = [<br/>    (max_depth, eta)<br/>    for max_depth in range(5,15)<br/>    for eta in [i/10. for i in range(1, 10)]<br/>]</span></pre><p id="d964" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将这个搜索网格传递到定制函数pair_wise_gridsearch_CV()中，以便在超参数对上调优模型。这是代码。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="3abb" class="mx mb iq np b gy nt nu l nv nw">x1, y1, z1, min_mae1, best_params1 = pair_wise_gridsearch_CV('max_depth', 'eta', pair_wise_gridsearch_params1)</span></pre><p id="c8f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是等式左侧输出变量的描述。</p><ul class=""><li id="6f2d" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> x1 </em> </strong>和<strong class="ky ir"> <em class="ls"> y1 </em> </strong>存储搜索网格中超参数对的所有数据点；</li><li id="ea19" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> z1 </em> </strong>存储每个搜索网格数据点的性能值(<strong class="ky ir"> <em class="ls"> x1，y1</em></strong>)；</li><li id="7fc4" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> min_mae1 </em> </strong>存储最佳性能值，损失函数的最小值。</li><li id="27b4" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><strong class="ky ir"> <em class="ls"> best_params1 </em> </strong>存储产生最佳性能的超参数对的搜索网格数据点。</li></ul><p id="067e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls"> min_mae1 </em> </strong>和<strong class="ky ir"> <em class="ls"> best_params1 </em> </strong>是成对调优的结果。</p><p id="3fe2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有这些变量都将被传递到我们刚才定义的自定义可视化实用程序函数中，以呈现性能场景的3D可视化。</p><p id="1944" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="4b0d" class="mx mb iq np b gy nt nu l nv nw">Best params: 7, 0.1, MAE: 0.2988302932498683</span></pre><p id="05cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出表明，在max_depth = 7和eta = 0.1的搜索网格中，在0.2988处产生了最佳性能。</p><p id="4c92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，出于可视化的目的，我们将把所有调整输出——x1、y1、z1、min_mae1和best _ params 1——连同超参数对的名称一起传递到自定义可视化函数中。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="a153" class="mx mb iq np b gy nt nu l nv nw">Trisurf("max_depth", 'eta',x1,y1,z1, min_mae1, best_params1)</span></pre><p id="1321" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是搜索网格中超参数对的所有数据点的性能前景的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ad71e324b276543b39e5bb701d6c80cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*aDlQwhofqmWb-DzS8SrsTw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="b230" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一对的表演景观max_depth和<strong class="ky ir"> <em class="ls"> eta </em> </strong>展示了表演景观形状的合理一致性，而没有多个凹陷和凸起。这似乎表明，在这两个超参数的最佳值附近，模型性能具有合理的可预测性。</p><p id="9c38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们用成对调整的最佳结果来更新超参数对的值，即max_depth = 7，eta = 0.1。由于这些值存储在变量best_params1中，我们可以更新关于max_depth和eta的超参数对的参数字典，如下所示。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="82e8" class="mx mb iq np b gy nt nu l nv nw">param_dict["max_depth"] = best_params1[0]<br/>param_dict["eta"]= best_params1[1]</span></pre><p id="cbd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这逐渐提高了模型的性能。</p><p id="e082" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以对其他3个超参数对重复相同的过程来调整模型。作为预防措施，自定义函数的输出将具有不同的数字后缀，以区分不同的超参数对。然而，该过程将经历与我们刚刚经历的完全相同的步骤。</p><p id="a82e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于接下来的3个超参数对，我将跳过过程的细节，只显示相关的过程。</p><h2 id="9b4f" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated"><strong class="ak"> b)第二对:</strong> <code class="fe op oq or np b">subsample </code>和<code class="fe op oq or np b">colsample_bytree</code></h2><p id="3741" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">接下来，我们有两个与子采样相关的超参数，subsample和colsample_bytree <strong class="ky ir"> <em class="ls">。</em> </strong>因为两者都处理子采样，所以它们可能会展示彼此之间的一些交互。出于这个原因，我们将这一对放在一起调谐，以观察相互作用。</p><p id="f494" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们按如下方式设置搜索网格。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="1b57" class="mx mb iq np b gy nt nu l nv nw">pair_wise_gridsearch_params2 = [<br/>    (subsample, colsample_bytree)<br/>    for subsample in [i/10. for i in range(1, 10)]<br/>    for colsample_bytree in [i/10. for i in range(1, 10)]<br/>]</span></pre><p id="363f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将这个搜索网格传递到自定义函数pair_wise_gridsearch_CV()中，以便在第二个超参数对上调整模型。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="7293" class="mx mb iq np b gy nt nu l nv nw">x2, y2, z2, min_mae2, best_params2= pair_wise_gridsearch_CV('subsample', 'colsample_bytree', pair_wise_gridsearch_params2)</span></pre><p id="e02f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="65cb" class="mx mb iq np b gy nt nu l nv nw">Best params: 0.9, 0.8, MAE: 0.2960672089507568</span></pre><p id="edc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出表明，0.296处的最佳性能是在subsample =0.9和colsample_bytree =0.8的搜索网格中产生的。正如预期的那样，性能从最后一次调优开始逐步提高，最后一次调优的性能为0.2988。</p><p id="7a98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是超参数对的搜索网格的所有数据点上的性能前景的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f3d01c0c134d8a3d5db6c553b8df475e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*ASGF0DxXwuORHxGMLOoLzA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="80b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二对subsample和colsample_bytree的性能状况显示了性能状况的合理一致性，没有多次下降和起伏。这似乎表明，在这两个超参数的最佳值附近，模型性能具有合理的可预测性。</p><p id="03cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们用成对优化的最佳结果来更新超参数对的值，即subsample = 0.9和colsample_bytree = 0.8。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="7bf6" class="mx mb iq np b gy nt nu l nv nw">param_dict["subsample"] = best_params2[0]<br/>param_dict["colsample_bytree"]= best_params2[1]</span></pre><p id="d955" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这再次逐步提高了模型的性能。</p><h2 id="5cdd" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">c)第三对:<code class="fe op oq or np b">min_child_weight</code> &amp; <code class="fe op oq or np b">gamma</code></h2><p id="1e7e" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">接下来，第三个超参数对min_child_weight和gamma都与树的划分相关联。所以，让我们一起调一下，看看它们之间的相互作用。</p><p id="83fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们按如下方式设置搜索网格。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="719f" class="mx mb iq np b gy nt nu l nv nw">pair_wise_gridsearch_params3 = [<br/>    (min_child_weight, gamma)<br/>    for min_child_weight in range(0, 10)<br/>    for gamma in range(0, 10)<br/>]</span></pre><p id="742f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将这个搜索网格传递到定制函数pair_wise_gridsearch_CV()中，以便在超参数对的搜索网格上的所有数据点上调整模型。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="5d5d" class="mx mb iq np b gy nt nu l nv nw">x3, y3, z3, min_mae3, best_params3 = pair_wise_gridsearch_CV('min_child_weight', 'gamma', pair_wise_gridsearch_params3)</span></pre><p id="bcd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="4c86" class="mx mb iq np b gy nt nu l nv nw">Best params: 3, 0, MAE: 0.29524631108655486</span></pre><p id="0392" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出显示，0.2952处的最佳性能是在min_child_weight = 3且gamma =0的搜索网格处生成的。正如预期的那样，性能从最后一次调优开始逐步提高，最后一次调优的性能为0.296。</p><p id="e669" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是超参数对的不同值的性能情况的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/5ba78326cd9b502eef2aeb5e495687f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*VZu7CEdH7Zjb4ZYWjEXGYQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="9953" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个与分区相关的超参数min_child_weight和gamma的性能状况显示了性能状况的合理一致性，没有多次下降和上升。这似乎表明，在这两个超参数的最佳值附近，模型性能具有合理的可预测性。</p><p id="a23d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们用成对调整的最佳结果来更新超参数对的值，即min_child_weight = 3，gamma =0。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="f1c8" class="mx mb iq np b gy nt nu l nv nw">param_dict["min_child_weight"] = best_params3[0]<br/>param_dict["gamma"]= best_params3[1]</span></pre><h2 id="96e1" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">d)第四对:<code class="fe op oq or np b">reg_alpha</code>和<code class="fe op oq or np b">reg_lambda</code></h2><p id="24e4" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">最后，我们将对以下两个正则化超参数reg_alpha和reg_lambda进行最后的成对超参数调整。</p><p id="02dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们按如下方式设置搜索网格。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="bd64" class="mx mb iq np b gy nt nu l nv nw">pair_wise_gridsearch_params4 = [<br/>    (reg_alpha, reg_lambda)<br/>    for reg_alpha in [0, 1e-2, 0.1, 1, 2, 3, 4, 8, 10, 12, 14]<br/>    for reg_lambda in [0, 1e-2, 0.1, 1, 2, 3, 4, 8, 10, 12, 14]<br/>]</span></pre><p id="d277" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们将这个搜索网格传递到定制函数pair_wise_gridsearch_CV()中，以便在超参数对上调优模型。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="8c61" class="mx mb iq np b gy nt nu l nv nw">x4, y4, z4, min_mae4, best_params4 = pair_wise_gridsearch_CV('reg_alpha', 'reg_lambda', pair_wise_gridsearch_params4)</span></pre><p id="f636" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="ce08" class="mx mb iq np b gy nt nu l nv nw">Best params: 0.1000, 8.0000, MAE: 0.2923583947526392</span></pre><p id="91d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出表明，在reg_alpha = 0.1和reg_lambda = 8.0的搜索网格中，在0.292358处产生了最佳性能。正如预期的那样，性能从最后一次调优开始逐步提高，最后一次调优的性能为0.2952。</p><p id="1ad5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是超参数对的不同值的性能情况的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0d83c6700ead74fb84b082fd60ccfe8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*Tx3rIfLJq99a2VvmOD0_ew.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="437d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与其他3个早期的性能环境不同，reg_alpha和reg_lambda的成对性能环境呈现了多个局部最小值，如图所示。它展示了一个崎岖不平的性能景观，有各种各样的下降和颠簸。有证据表明，在调整期间，模型的性能对这两个正则化超参数reg_alpha和reg_lambda的值的微小变化非常敏感。</p><p id="3401" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，他们价值观的微小变化会极大地影响他们的表现结果。当我们将新类型的数据集传递到部署域中的优化模型时，这很可能会转化为模型的不稳定性。</p><p id="d87f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这需要仔细研究后才能下定论。</p><p id="b70f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们用reg_alpha和reg_lambda <strong class="ky ir"> <em class="ls"> </em> </strong>的当前最佳值更新参数字典，并在测试数据集上计算调优模型的性能。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="571c" class="mx mb iq np b gy nt nu l nv nw">param_dict["reg_alpha"] = best_params4[0]<br/>param_dict["reg_lambda"]= best_params4[1]</span></pre></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="0694" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">G.第一次成对调整后的测试结果</h1><p id="699e" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">因为我们已经逐段更新了参数字典param_dict，所以它的当前值已经在所有8个超参数上更新了。让我们打印出来，以确认优化后超参数的更新值。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="86c4" class="mx mb iq np b gy nt nu l nv nw">print("Parameters after the Pair Wise Tuning:", param_dict)</span></pre><p id="e5bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是输出。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="3e16" class="mx mb iq np b gy nt nu l nv nw">Parameters after the Pair Wise Tuning: {'objective': 'reg:squarederror', 'max_depth': 7, 'subsample': 0.9, 'eta': 0.1, 'min_child_weight': 3, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0.1, 'reg_lambda': 8, 'eval_metric': 'mae', 'gpu_id': 0, 'tree_method': 'gpu_hist'}</span></pre><p id="2b6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以在测试数据集上拟合带有这些超参数的更新值的优化模型，以评估测试性能的改进。让我们将更新后的参数字典param_dict传递到bias_variance_performance()的自定义函数中。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="386c" class="mx mb iq np b gy nt nu l nv nw">pw_model_1, pw_evals_result_1=bias_variance_performance(param_dict)</span></pre><p id="a386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="ac05" class="mx mb iq np b gy nt nu l nv nw">Best MAE after the Pair-Wise Tuning: 0.2903 with 315 rounds</span></pre><p id="26d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第315轮看结果吧。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="7e85" class="mx mb iq np b gy nt nu l nv nw">[314]	Train-mae:0.15790	Test-mae:0.29034</span></pre><p id="479f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在315轮迭代时确定调整后的模型(具有调整后的超参数设置)。它在训练数据集上的性能为0.1579，而在测试数据集上的性能为0.29034。</p><p id="8845" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与预调优的模型性能(在训练数据集上为0.17322，而在测试数据集上为0.3103)相比，有所提高。</p><p id="9354" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是训练数据集和测试数据集上的性能曲线的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/15dff0d1c55c26d3577a5f7cee711b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ooRkeDD_it6o0jxzPL28ew.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="4196" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果在<strong class="ky ir"> <em class="ls">偏差-方差权衡</em> </strong>中产生了轻微的改进。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="0c73" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">H.<strong class="ak"> <em class="nx">多个局部最小值和模型不稳定性:</em> </strong>关于<code class="fe op oq or np b">reg_alpha</code>和<code class="fe op oq or np b">reg_lambda</code>的高度敏感的模型性能</h1><p id="8c34" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">重复地，这两个正则化参数reg_alpha和reg_lambda的调整过程产生了具有各种下降和上升以及多个局部最小值的粗糙性能状况。有证据表明，在调整过程中，性能对这两个正则化超参数值的微小变化非常敏感。它们值的微小变化会显著影响它们性能的结果，即目标函数(损失函数)的值。</p><p id="63a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们至少可以更仔细地观察当前的性能状况，并评估模型对超参数对值的微小变化的敏感性。</p><ul class=""><li id="79e1" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">首先，我们可以根据性能指标对前10个调优结果进行排名，并确定前10个数据点。</li><li id="8a01" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">第二，根据对前10个调优结果的观察，我们可以对性能状况进行切片，以便更仔细地了解其一致性或性能稳定性。</li></ul><h2 id="10d5" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">a)检查前10个最低性能值，<code class="fe op oq or np b">mae</code></h2><p id="4d1c" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">下一个排序的数据帧按照mae评估度量值的升序排列前10个性能数据点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3c6efcee53940d73d8f8cb552ccefa68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*_rOgQizjKjpPsLr4vTNOgg.jpeg"/></div></figure><p id="eb68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，当我们看到reg_alpha时，前10个调整结果数据点沿着reg_alpha分布在(0.1，4.0)范围内。</p><p id="ff90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> reg_lambda </a>，它的前10个排名值分散在一个很大的范围内(0.00到12)。</p><p id="a398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更好地了解3D性能，我们可以从以下几个方面进行分析:</p><ul class=""><li id="30a6" class="ny nz iq ky b kz la lc ld lf oa lj ob ln oc lr od oe of og bi translated">reg_alpha = 0.1</li><li id="db1f" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated">reg_alpha = 3.0</li><li id="fdc2" class="ny nz iq ky b kz oh lc oi lf oj lj ok ln ol lr od oe of og bi translated"><a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster" rel="noopener ugc nofollow" target="_blank"> reg_lambda </a> = 8.0</li></ul><p id="37fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有这些值都出现在列表中的前2个性能数据点中。这是他们的分段性能曲线:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/48b428115d6183294526881c4f48f213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ielpqG3vC-FzlpZ_ztIdSA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/06eefc9d0412b568ca2a8d880f8ea1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEyoq8OZ8XUpvk6pGbIjDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/e07983bb5a4c54d77dcceb1c8f657ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUrUptgKVkPHCSWfnGYNLA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure><p id="722e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在(reg_alpha = 3.0)处的性能状况的第二部分确定了在(reg_lambda =0.01)和(reg_lambda =1.0)之间形成的严重深渊。</p><p id="da01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与搜索网格的任何其他部分相比，沿(reg_lambda =0.01)、(reg_lambda =0.1)和(reg_lambda = 1)设置的更精细的局部搜索网格捕捉到了深渊形状的强烈凹凸纹。换句话说，如果我们在搜索网格中错过了(reg_lambda =0.1)的单个数据点，我们将无法捕捉到(reg_lambda =0.01)和(reg_lambda =1.0)之间的局部最小值的存在。</p><p id="aafa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">搜索网格设置中更精细的粒度可能会揭示更粗糙的性能环境。也就是说，通过更精细的搜索网格设置，我们可能会发现更多的凹陷和凸起。它可能会发现许多其他隐藏的局部最小值的存在，这些最小值在当前的性能环境中是不可见的。甚至它可以在当前搜索网格无法捕捉到的地方发现最佳性能数据点。</p><h2 id="aff8" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">b)第二轮成对调谐<code class="fe op oq or np b">reg_alpha</code>和<code class="fe op oq or np b">reg_lambda</code></h2><p id="1a0c" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，让我们定义一个新的更细粒度的搜索网格。这是新的搜索网格设置。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="6032" class="mx mb iq np b gy nt nu l nv nw">pair_wise_gridsearch_params4 = [<br/>    (reg_alpha, reg_lambda)<br/>    for reg_alpha in [0, 1e-2, 2e-2, 3e-2, 0.1, 0.5, 1, 1.5, 1.8, 1.9, 2, 2.1, 2.2, 2.5, 3, 3.5, 4]<br/>    for reg_lambda in [0, 1e-2, 2e-2, 3e-2, 0.1, 0.5, 1, 1.5, 1.8, 1.9, 2, 2.1, 2.2, 2.5, 3, 4, 5, 6, <br/>                       7, 7.5, 8, 8.5, 9, 9.5, 10, 10.5, 11, 11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5]<br/>]</span></pre><p id="d408" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用上面的新搜索网格设置，在reg_alpha和reg_lambda对上再次运行成对调整。</p><p id="b12d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是第二次调优的结果:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="c5c6" class="mx mb iq np b gy nt nu l nv nw">Best params: 0.5000, 13.5000, MAE: 0.29122008437950175</span></pre><p id="d4ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出表明，在reg_alpha = 0.5和reg_lambda =13.5的搜索网格中，在0.29122处产生了最佳性能。正如所推测的，更精细的搜索网格粒度在与第一个结果完全不同的位置识别出了更好的最佳性能数据点，其reg_alpha = 0.1和reg_lambda = 8.0，这产生了0.292358的性能。</p><p id="3f60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是使用新的搜索网格设置的性能前景的可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/300caba1932d136fa3fd519637c6baeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*t-K6xmXF-ZkUJJmwza44fA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Matplotlib的plot_trisurf进行3D可视化:由Michio Suginoo制作</p></figure><p id="ddc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如推测的那样，第二次调优捕捉到了一个更崎岖的性能景观，有更多的下降和颠簸。正如所推测的，更精细的搜索网格粒度确定了更好的最佳性能数据点，reg_alpha = 0.5和reg_lambda =13.5，位于与第一个结果完全不同的位置，第一个结果是reg_alpha = 0.1和reg_lambda =8.0，这产生了0.292358的性能。第二次调整也在0.29122处产生了更好的最佳性能。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="1fa0" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak"> <em class="nx">一、型号选择</em> </strong></h1><p id="f7b0" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，我们能说第二次调整的模型比第一次调整的模型更好吗？</p><p id="b9bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们仍然不知道。</p><p id="b59a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，我们是基于训练数据集来比较结果的，训练数据集包括验证数据集。我们需要用测试数据集来测试这些调整后的模型，以评估偏差-方差权衡，即欠拟合-过拟合权衡的问题。</p><h2 id="a44d" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">a)偏差-方差权衡，过度拟合的风险</h2><p id="3bd9" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，让我们在测试数据集上运行新调整的模型。我们可以将参数字典param_dict的最新更新传递给自定义函数bias_variance_performance()。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="8660" class="mx mb iq np b gy nt nu l nv nw">pw_model_2, pw_evals_result_2=bias_variance_performance(param_dict)</span></pre><p id="a275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是结果。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="203d" class="mx mb iq np b gy nt nu l nv nw">Best MAE after the Pair-Wise Tuning: 0.2922 with 319 rounds</span></pre><p id="4aeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面说最好的测试成绩，0.2922，出现在第319轮。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="36bd" class="mx mb iq np b gy nt nu l nv nw">[318]	Train-mae:0.17208	Test-mae:0.29225</span></pre><p id="9694" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们比较第二次调整后模型与调整前模型和第一次调整后模型的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/61a8edbdcbf1925b068bcd0bed839920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1cjX1MJwnDOrB2smFU_nA.jpeg"/></div></div></figure><p id="2527" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的比较表显示，就测试性能而言，第一次调优在三次调优中产生了最佳性能。这意味着第二次调整的更好的训练性能是在训练数据集(包括验证数据集)上过度拟合模型的结果。这是一个典型的偏差-方差权衡。</p><p id="72c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于模型开发期间调优的总体目标是追求部署领域中的模型稳定性，因此我们需要在选择调优模型时解决偏差-方差权衡问题。</p><p id="60d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们最好选择第一次调整的结果，而不是第二次调整的结果。</p><h2 id="17c9" class="mx mb iq bd mc my mz dn mg na nb dp mk lf nc nd mm lj ne nf mo ln ng nh mq ni bi translated">b)绘制特征重要性</h2><p id="d274" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在，我们想知道每个特性在定义优化模型中的贡献有多大。我们可以使用第一次调整的模型来绘制<strong class="ky ir"> <em class="ls">特征重要性</em> </strong>。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="3bcb" class="mx mb iq np b gy nt nu l nv nw">from xgboost import plot_importance<br/>plot_importance(pw_model_1)<br/>pyplot.show()</span></pre><p id="8d3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/36bc571ae04b697b44ade7bfcaac6e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ttAS3uG0IwjpdCexBctCw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作人:杉尾道夫</p></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="e02d" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结论</h1><p id="48fa" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">这篇文章向您介绍了本机XGBoost API的超参数调优的代码实现步骤。</p><p id="9c17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原生XGBoost API使用户能够调整超参数，以应用各种正则化技术来解决偏差-方差权衡问题。这个分析迭代了内置的交叉验证函数来调整超参数。</p><p id="986e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您可以为自己的项目复制或修改流程。</p><p id="e66b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将在下面的帖子中单独分享我对这个特殊案例结果的进一步观察:<a class="ae kv" rel="noopener" target="_blank" href="/risk-implications-of-excessive-multiple-local-minima-during-hyperparameter-tuning-fe6f517e57e8">超参数调优期间过多多个局部最小值的风险含义</a>。</p><p id="cb39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另外，如果你对XGBoost的快速概述感兴趣，欢迎阅读我的另一篇文章:<a class="ae kv" rel="noopener" target="_blank" href="/xgboost-its-genealogy-its-architectural-features-and-its-innovation-bf32b15b45d2"> XGBoost:它的系谱、它的架构特性和它的创新</a></p><p id="48bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读这篇长文。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="9fd0" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">确认</h1><p id="abe6" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我要感谢TDS的编辑团队，特别是<a class="ae kv" href="https://www.linkedin.com/in/ACoAAArEpWwBHvFKeyTA3JKk6gApzplqwM2hjKY" rel="noopener ugc nofollow" target="_blank">凯瑟琳·普雷里</a>，感谢他们在编辑阶段提供的宝贵意见。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="a637" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">参考</h1><p id="efab" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">Brownlee，J. <a class="ae kv" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="ls">对机器学习的梯度推进算法的温和介绍</em> </a> <em class="ls">。</em> (2016)。检索自机器学习掌握:<a class="ae kv" href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://Machine Learning Mastery . com/gentle-introduction-gradient-boosting-algorithm-Machine-Learning/</a></p><p id="086f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据砖。<a class="ae kv" href="https://github.com/databricks/xgboost-linux64/blob/master/doc/model.md" rel="noopener ugc nofollow" target="_blank">T15】xgboost-Linux 64</a>T18】。 (2017)。<em class="ls">。</em>从github检索:<a class="ae kv" href="https://github.com/databricks/xgboost-linux64/blob/master/doc/model.md" rel="noopener ugc nofollow" target="_blank">https://github . com/databricks/xgboost-Linux 64/blob/master/doc/model . MD</a></p><p id="bd8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Hunter，j .，Dale，d .，Firing，e .，Droettboom，m .，Matplotlib开发团队。<a class="ae kv" href="https://matplotlib.org/3.1.0/tutorials/toolkits/mplot3d.html#mpl_toolkits.mplot3d.Axes3D.plot_trisurf" rel="noopener ugc nofollow" target="_blank"><em class="ls">mplot3d工具包</em> </a> <em class="ls">。</em> (2019)。从matplotlib.org检索:<a class="ae kv" href="https://matplotlib.org/3.1.0/tutorials/toolkits/mplot3d.html#mpl_toolkits.mplot3d.Axes3D.plot_trisurf" rel="noopener ugc nofollow" target="_blank">https://matplotlib . org/3 . 1 . 0/tutorials/toolkits/mplot3d . html # mpl _ toolkits . mplot3d . axes3d . plot _ tris URF</a></p><p id="bf5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">jeeteshgavande30、singghakshay和anikakapoor。<a class="ae kv" href="https://www.geeksforgeeks.org/tri-surface-plot-in-python-using-matplotlib/" rel="noopener ugc nofollow" target="_blank"><em class="ls">Python中的三面绘图使用Matplotlib </em> </a> <em class="ls">。</em> (2021)。检索自geeks forgeeks:<a class="ae kv" href="https://www.geeksforgeeks.org/tri-surface-plot-in-python-using-matplotlib/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/tri-surface-plot-in-python-using-matplotlib/</a></p><p id="0a65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">莱昂d . m .<a class="ae kv" href="https://github.com: https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/datasets/_california_housing.py#L53" rel="noopener ugc nofollow" target="_blank"><em class="ls">_ California _ housing . py</em></a><em class="ls">。</em> (2022) <em class="ls">。</em>检索自<a class="ae kv" href="https://github.com:" rel="noopener ugc nofollow" target="_blank">https://github . com:</a><a class="ae kv" href="https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/datasets/_california_housing.py#L53" rel="noopener ugc nofollow" target="_blank">https://github . com/scikit-learn/scikit-learn/blob/36958 fb24/sk learn/datasets/_ California _ housing . py # L53</a></p><p id="c54e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">佩斯，R. K .和罗纳德·b .(1997年)。稀疏空间自回归，统计和概率字母，33291–297。StatLib数据集存档:<a class="ae kv" href="http://lib.stat.cmu.edu/datasets/" rel="noopener ugc nofollow" target="_blank">http://lib.stat.cmu.edu/datasets/</a></p><p id="9397" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sci kit-学习开发者。<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html" rel="noopener ugc nofollow" target="_blank"><em class="ls">sk learn . datasets . fetch _ California _ housing</em></a><em class="ls">。</em>(未注明)检索自<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.htm" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . datasets . fetch _ California _ housing . htm</a>l</p><p id="aa37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">维基百科。<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank"> <em class="ls">偏差-方差权衡</em> </a> <em class="ls">。</em> (2022) <em class="ls">。</em>从维基百科检索:<a class="ae kv" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#:~:text=In%20statistics%20and%20machine%20learning,bias%20in%20the%20estimated%20parameters." rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Bias % E2 % 80% 93 variance _ trade off #:~:text = In % 20 statistics % 20 and % 20 machine % 20 learning，Bias % 20 In % 20估计% 20参数。</a></p><p id="56ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">维基百科。<a class="ae kv" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank"> <em class="ls">【交叉验证(统计)</em> </a> <em class="ls">。</em> (2022) <em class="ls">。</em>从维基百科检索:<a class="ae kv" href="https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Cross-validation _ % 28 statistics % 29 # k-fold _ Cross-validation</a></p><p id="fe15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">维基百科。<a class="ae kv" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank"> <em class="ls">曲面三角剖分</em> </a> <em class="ls">。</em> (2022) <em class="ls">。</em>从维基百科检索:<a class="ae kv" href="https://en.wikipedia.org/wiki/Surface_triangulation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Surface_triangulation</a></p><p id="55f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">xgboost开发人员。<a class="ae kv" href="https://xgboost.readthedocs.io/: https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="ls"> XGBoost文档</em> </a> <em class="ls">。</em>(未注明)。检索自<a class="ae kv" href="https://xgboost.readthedocs.io/:" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/:</a>T38】https://xgboost.readthedocs.io/en/stable/</p></div></div>    
</body>
</html>