<html>
<head>
<title>Quantifying the Uncertainty for Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量化语音识别的不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantifying-the-uncertainty-for-speech-recognition-1932dc198c49#2022-06-03">https://towardsdatascience.com/quantifying-the-uncertainty-for-speech-recognition-1932dc198c49#2022-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5412" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">何时以及如何信任您的语音识别模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/919027c835f8217d4c93bc69155723dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*gnhjx5FQ2RXR1Ryf.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.attendi.nl/en/assistant/" rel="noopener ugc nofollow" target="_blank">Attendi语音服务的标志</a>。<em class="kv">经作者许可展示</em></p></figure><h1 id="1bdd" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated">目录</h1><ul class=""><li id="703a" class="lo lp it lq b lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">什么是不确定性？</li><li id="43be" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">语音识别的不确定性估计方法</li><li id="58d3" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">我们如何从不确定性评估中获益？</li><li id="2e93" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">摘要</li><li id="3d05" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">关于我</li><li id="5926" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">参考</li></ul></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="b065" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi nh translated"><span class="l ni nj nk bm nl nm nn no np di">在过去的几年里，自动语音识别(ASR)已经转向更复杂和更大的神经网络架构。更高的复杂性有利于模型的性能，但另一方面，它会变得更加难以信任的结果。一个复杂的神经网络模型就像一个黑匣子，我们只能希望它对看不见的数据同样有效。</span></p><p id="6800" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在<a class="ae ku" href="https://www.attendi.nl/" rel="noopener ugc nofollow" target="_blank"> Attendi </a>，我们为荷兰的(健康)保健专业人士提供量身定制的演讲服务。用户录制的音频可能有明显的噪声背景、行话或口音，而模型没有见过(或在这种情况下，听到了<a class="ae ku" href="https://emojipedia.org/slightly-smiling-face/" rel="noopener ugc nofollow" target="_blank">🙂</a>)之前。在这种情况下，知道我们可以在多大程度上相信预测是有益的。</p><p id="4024" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">通过测量预测的不确定性，我们有可能:</p><ul class=""><li id="1278" class="lo lp it lq b lr mu lt mw lv nq lx nr lz ns mb mc md me mf bi translated">找出最有可能出错的话语并突出显示出来，以便用户检查</li><li id="5b04" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">根据最“不确定”的样本微调模型</li><li id="65b3" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">监控模型的性能</li></ul><p id="4216" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">但是我们如何测量不确定性呢？幸运的是，有一些不确定性估计的相关方法可以应用于ASR系统。在本文中，我们将研究如何定义不确定性，以及哪些方法可以应用于语音识别模型。最后，我们将了解在哪些实际设置中，我们可以将不确定性估计应用于ASR系统。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="aa1a" class="kw kx it bd ky kz nt lb lc ld nu lf lg jz nv ka li kc nw kd lk kf nx kg lm ln bi translated">什么是不确定性？</h1><p id="afa6" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">不确定性衡量模型做出预测的可信度[1]。根据不确定性的来源，我们可以将其分为两种类型:<em class="ob">任意的</em>和<em class="ob">认知的。</em></p><p id="5c06" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated"><em class="ob">随机</em>型是由于固有的随机过程而出现的统计不确定性。例如，掷硬币的结果具有任意的不确定性。掷硬币的结果要么是正面，要么是反面，<br/>两者都有50%的几率发生。任意型是不确定性的不可约部分。不可能改变正面或反面的概率，因为这个事件是由一个固有的随机过程描述的。</p><p id="1fb0" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated"><em class="ob">认知</em>不确定性是由于不适当的模型架构、训练程序、数据的分布变化或先前未知数据的出现(例如用于狗/猫<br/>图像分类的鸟的图像)[1]。这是不确定性的可减少部分，这意味着我们可以通过改进模型及其训练参数，或者通过改进训练数据的可变性来增加预测的可靠性。</p><p id="f022" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在更实际的设置中，我们最感兴趣的是<em class="ob">预测</em>不确定性，即在网络单次前向传递后可以估计的不确定性。这种类型的不确定性非常有用，因为它很容易在现有的ASR管道中实现。我们也更喜欢关注认知类型的方法，因为这是不确定性中唯一可约的部分。</p><h1 id="66a7" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated"><em class="kv">语音识别的不确定性估计方法</em></h1><p id="345a" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">理想情况下，我们想知道对于给定的输入数据<strong class="lq iu"> X </strong>，模型的输出是否是不确定的。在ASR领域中，通常定义一种方法，在每个模型的预测之后输出分数S<em class="ob">(</em><strong class="lq iu"><em class="ob">)X</em></strong><em class="ob">)</em>。对于一个取值范围<em class="ob"> s </em> ₀，和<em class="ob"> s </em> ₁，我们可以确定模型的输出是否不确定。</p><p id="0c78" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">指示不确定性的简单阈值函数Sel( <strong class="lq iu"> X </strong>)可以定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b323875cb3ce36fdf6331ad323ce1883.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*lc9Ori6BaasQgEhKVB5ZHw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 1。给定得分函数S( <strong class="bd od"> X </strong>，我们可以定义哪些数据使得模型的输出不确定。</p></figure><p id="1918" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">其中<em class="ob"> s </em> ₀和<em class="ob">s</em>₁<em class="ob">t30】分别是下限和上限。<em class="ob"> s </em> ₀和<em class="ob"> s </em> ₁ <em class="ob"> </em>的值根据具体情况确定，因为它们取决于许多因素，例如模型架构或不确定性估计方法。</em></p><p id="0abb" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">通过这个简单的阈值函数，我们可以识别ASR模型需要处理的数据。然而，主要的问题是评分函数S( <strong class="lq iu"> X </strong>)无法访问基本事实标签，因为我们对未标记的“实时”数据进行预测。因此，这些评分函数只能使用原始模型的输出来计算不确定性，或者必须对真实标签分布做出假设。</p><p id="da80" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在下面的小节中，我们将看看这些方法的选择，并了解它们如何生成分数S( <strong class="lq iu"> X </strong>)。</p><h2 id="16d1" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">最大软最大概率</h2><p id="e0f7" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">一种简单的方法是使用模型的softmax分布来估计不确定性[2]。为了计算得分S( <strong class="lq iu"> X </strong>)，我们可以取模型的softmax输出的最大值<em class="ob"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f404b0c86133966566ba03f07d750277.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*HmPOCIEqcILs03gbAUUv2A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 2。从softmax输出中取最大值</p></figure><p id="4ee2" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">但是我们如何将这种方法推广到ASR呢？上面的等式只考虑了非顺序输出([ <em class="ob"> batch_size，no. classes] </em>)，当我们想要预测整个话语(<em class="ob">【batch _ size，no. predicted tokens，no . classes】)</em>时，情况并非如此。我们需要定义另一个函数来聚合整个话语的得分:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9d8832e2fbbbc00282e66a0522fcd647.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*n92_sutbjgiIfH0BMd6S0A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 3。在[预测令牌数]维度上平均MSP分数</p></figure><p id="f83b" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">其中，softmax( <strong class="lq iu"> Y </strong> ₜ)是令牌<em class="ob"> t. </em>的softmax分发输出</p><p id="b0c4" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在上面的等式中，我们计算整个话语的平均不确定性得分。也可以使用最大值、最小值或其他聚合函数来代替平均值。然而，尽管这种方法为整个话语提供了单个分数，但是这些分数不如每个标记的分数那样具有信息量。即，在聚合之后，我们不能再标记不正确的单词/标记。我们能做的最好的事情就是找到一个不正确的发音。简单来说，我们失去了细节的颗粒度。</p><h2 id="f6ac" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">欧丁神</h2><p id="36a0" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">MSP方法有一些缺点。神经网络模型倾向于对域外(OOD)数据做出过于自信的预测[3]。通常，它们没有很好地校准，这意味着softmax输出与进行预测的置信度没有很好的关联[3]。</p><p id="1027" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">ODIN试图通过向输入数据添加温度标度和小扰动来解决这个问题[4]。这些方程非常类似于MSP方法。</p><p id="e271" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">首先，温度缩放用于校准softmax输出，使其更好地符合预测的置信度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7e30577fdd4c2a278197c6ca80eddb8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*X8gVBe_2hz2IwxmVp-psuw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 4。ODIN通过添加温度比例参数<em class="kv"> T </em>来修改softmax方程</p></figure><p id="add1" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">其中神经网络<strong class="lq iu"> f </strong> =(f₁，…，fₙ)被训练来分类n个类。为了获得最终得分S( <strong class="lq iu"> x </strong>)，我们可以从得分Si中取最大值(如等式2所示)。</p><p id="c84c" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在论文[4]中，作者指出温度比例必须足够大，并指出他们的模型在T=1000时性能最佳。</p><p id="edc0" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">第二步，我们加入输入扰动。这有助于扩大分布内和分布外样本之间的softmax得分差距。要添加输入扰动，我们需要执行以下操作:</p><ol class=""><li id="956d" class="lo lp it lq b lr mu lt mw lv nq lx nr lz ns mb ot md me mf bi translated">计算<em class="ob">最大值</em> softmax分数(等式。4和Eq。2)</li><li id="98a5" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb ot md me mf bi translated">将交叉熵损失w.r.t .反向传播到输入端<strong class="lq iu"> x </strong></li><li id="9939" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb ot md me mf bi translated">计算梯度w.r.t输入<strong class="lq iu"> x </strong></li><li id="0a2c" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb ot md me mf bi translated">取一个<a class="ae ku" href="https://en.wikipedia.org/wiki/Sign_function" rel="noopener ugc nofollow" target="_blank">符号</a>(梯度)，用ε缩放，并从输入<strong class="lq iu"> x </strong>中减去它</li></ol><p id="ff4d" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">这个过程可以用下面的等式来解释:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fb4b58166889a8597f539fb831725b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*A_Ps1juJQ98BN7EDQAX0Iw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 5。输入扰动有助于区分分布内和分布外样本的分数</p></figure><p id="2def" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">要将其应用于顺序ASR任务，我们可以遵循与MSP相同的方法。将它聚集在一个[预测令牌数]上，会将其减少到一个数字分数。</p><h2 id="19ed" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">GradNorm</h2><p id="214b" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">MSP和ODIN cherrypicks中的<em class="ob"> max </em>函数从softmax分布中选取一个值，并忽略其他输出值。最重要的是，这些方法不考虑输入信号如何在网络中传播。所有这些方法只是盲目地从最终的softmax层中查看单个元素。</p><p id="1f79" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">GradNorm [5]从不同的角度处理这个问题，并从整体上看待这个模型。为了计算不确定性得分，GradNorm研究梯度如何在网络中传播以及最终的输出分布。</p><p id="9d65" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">梯度范数的主要概念如下。我们假设如果输入数据<strong class="lq iu"> X </strong>不确定，那么最终softmax层的输出将(或多或少)均匀分布。如果模型对其预测有信心，那么我们将在softmax层中看到某个类的峰值。</p><p id="6ee1" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">为了计算不确定性，我们首先计算softmax分布和均匀分布之间的<a class="ae ku" href="https://machinelearningmastery.com/divergence-between-probability-distributions/" rel="noopener ugc nofollow" target="_blank"> KL发散损失</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/fa04af446f165f3ce16213208d393835.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*2mMlwXQotDqL6FVO0tmkDA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 6。我们假设地面真值分布<strong class="bd od"> u </strong>是均匀的。基于此，我们计算softmax(f( <strong class="bd od"> x </strong>)和<strong class="bd od"> u </strong>之间的KL发散损失。</p></figure><p id="9272" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">为了计算最终得分，我们反向传播KL散度损失，然后计算pₜₕ层的梯度幅度(p-范数)。在论文中，他们发现最好的分数是从最后一层开始计算的，所以<em class="ob"> p=1: </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/4a9dccb0a005c59bca474e423352fc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*r7_Tx09GfxtMwIM1czf6VA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Eq 7。为了计算最终得分，反向传播KL发散损失并计算pth层的p范数。p=1给出了最好的结果，也是计算效率最高的。</p></figure><h1 id="0f23" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated"><em class="kv">我们如何从不确定性评估中获益？</em></h1><p id="1b51" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">了解预测的不确定性在实际应用中非常有用。在这一部分，我们将看看一些最有用的应用。</p><h2 id="275c" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">用户校正输入</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oy oz di pa bf pb"><div class="gh gi ox"><img src="../Images/9c83b923965c0658daf07777e14deac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hutkhLJ7T_p3Gw8u.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.attendi.nl/en/speech-service/" rel="noopener ugc nofollow" target="_blank">来自Attendi应用</a>的用户修正输入。经作者许可展示。</p></figure><p id="ba9f" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">不确定性分数允许我们找到可能不正确的话语。我们可以为用户突出显示这些话语，这样他/她就可以快速找到并纠正它们。</p><h2 id="0c86" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">主动学习</h2><p id="4ee4" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">主动学习假设我们有一定的配额来给新数据添加基础事实标签。在ASR的环境中，我们可以雇佣注释者转录一定数量的音频样本来帮助微调模型。但是我们应该优先考虑手动转录哪个音频呢？</p><p id="f1bd" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">在这种情况下，我们可以计算不确定性分数，并选择具有某个期望S( <strong class="lq iu"> X </strong>)的音频。我们可以采取最‘不确定’的样本，或者选择特定范围的S( <strong class="lq iu"> X </strong>)。</p><h2 id="cee3" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">监视</h2><p id="986e" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">部署的ASR模型可以暴露于不同的音频信号。他们可能有不同的环境噪音、行话或口音，这些都会影响模型的性能。我们希望，通过监控音频的不确定性分数，我们可以检测到模型的性能何时下降并采取行动。<em class="ob">然而，对我们来说，这仍然是一项正在进行的工作。我们首先需要确定模型的性能(在这种情况下是WER)是否与不确定性相关。</em></p><h1 id="8eba" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated">摘要</h1><p id="c0df" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">在本文中，我们定义了什么是不确定性，以及如何使用它作为分数S( <strong class="lq iu"> X </strong>)来衡量模型对其预测的信心。</p><p id="80e7" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">有不同的方法来计算不确定性。最大Softmax概率(MSP)和ODIN使用softmax输出计算不确定性得分。然而，这些方法的缺点是，它们只是盲目地查看模型的输出。GradNorm采用了一种不同的方法，既关注输出分布，也关注梯度如何在整个模型中反向传播。</p><p id="5306" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">最后，我们看了不确定性分数估计的潜在用例。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h2 id="cd38" class="oe kx it bd ky of og dn lc oh oi dp lg lv oj ok li lx ol om lk lz on oo lm op bi translated">信用</h2><p id="550c" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated"><em class="ob">我要感谢来自Attendi的Jan-Willem van Leussen、Omar Elbaghdadi和Berend Jutte为我的文章提供了有用的反馈和更正。</em></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="6d61" class="kw kx it bd ky kz nt lb lc ld nu lf lg jz nv ka li kc nw kd lk kf nx kg lm ln bi translated">关于我</h1><p id="7b1d" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">我是阿姆斯特丹大学的人工智能硕士学生。在我的业余时间，你可以发现我摆弄数据或者调试我的深度学习模型(我发誓这很有效！).我也喜欢徒步旅行:)</p><p id="4a68" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">如果你想了解我的最新文章和其他有用的内容，以下是我的其他社交媒体资料:</p><ul class=""><li id="b905" class="lo lp it lq b lr mu lt mw lv nq lx nr lz ns mb mc md me mf bi translated"><a class="ae ku" href="https://www.linkedin.com/in/kacperkubara/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="c917" class="lo lp it lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae ku" href="https://github.com/KacperKubara" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ul><h1 id="71ea" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated">参考</h1><p id="291a" class="pw-post-body-paragraph ms mt it lq b lr ls ju mv lt lu jx mx lv ny mz na lx nz nc nd lz oa nf ng mb im bi translated">[1] <a class="ae ku" href="https://arxiv.org/pdf/2107.03342.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络中不确定性的调查</a></p><p id="890d" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">[2] <a class="ae ku" href="https://arxiv.org/abs/1610.02136" rel="noopener ugc nofollow" target="_blank">用于检测神经网络中错误分类和非分布样本的基线</a></p><p id="5919" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">[3] <a class="ae ku" href="https://arxiv.org/pdf/1706.04599.pdf" rel="noopener ugc nofollow" target="_blank">关于现代神经网络的校准</a></p><p id="7707" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">[4] <a class="ae ku" href="https://arxiv.org/abs/1706.02690" rel="noopener ugc nofollow" target="_blank">增强神经网络中非分布图像检测的可靠性</a></p><p id="9730" class="pw-post-body-paragraph ms mt it lq b lr mu ju mv lt mw jx mx lv my mz na lx nb nc nd lz ne nf ng mb im bi translated">[5] <a class="ae ku" href="https://arxiv.org/abs/2110.00218" rel="noopener ugc nofollow" target="_blank">关于梯度对检测野外分布变化的重要性</a></p></div></div>    
</body>
</html>