<html>
<head>
<title>Machine Learning 103: Loss Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习103:损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-103-loss-functions-37b84f3e9b73#2022-02-15">https://towardsdatascience.com/machine-learning-103-loss-functions-37b84f3e9b73#2022-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c590" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么机器学习是一个优化问题</h2></div><p id="37a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在之前的两篇文章中，我介绍了机器学习中使用的两个最基本的模型——<a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-101-linear-regression-72ba6c04fbf1">线性回归</a>和<a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-102-logistic-regression-9e6dc2807772">逻辑回归</a>。在这两种情况下，我们感兴趣的是搜索产生被观察目标<strong class="kh ir"> d </strong>的最佳模型预测<strong class="kh ir">d’</strong>的一组模型参数<strong class="kh ir"> m </strong>，并且在这两种情况下，这是通过最小化一些损失函数<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)来完成的，该损失函数测量<strong class="kh ir">d’</strong>和<strong class="kh ir"> d </strong>之间的误差。</p><p id="8774" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很大一部分机器学习——从简单的线性回归到深度学习模型，本质上都涉及某种损失函数的最小化——然而，许多数据科学或机器学习书籍/教程/材料往往更强调模型本身，而不是损失函数！在这篇文章中，我们将继续前两篇文章的内容，并在以后的文章中探索更高级的模型之前，重点关注损失函数！</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/8b80b43b11b7bcdf81f888b31ead45c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_iXbiOk8LOqGJbfTBZ9skw.jpeg"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">美国肯塔基州观测到的美丽日食。照片由<a class="ae lb" href="https://unsplash.com/@sarahleejs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">李宗盛</a>在<a class="ae lb" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄。</p></figure><h1 id="1295" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">简而言之，训练机器学习模型</h1><p id="2f1d" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">很大一部分训练机器学习模型可以简化如下:</p><ol class=""><li id="49e7" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">我们有一组观察目标<strong class="kh ir"> d </strong>和一组相应的解释变量<strong class="kh ir"> G </strong>。我们希望使用相应的新值<strong class="kh ir"> G </strong>来预测新值<strong class="kh ir"> d </strong>。<strong class="kh ir"> d </strong> — <strong class="kh ir"> G </strong>对的例子包括信用分析中的<a class="ae lb" href="https://en.wikipedia.org/wiki/Credit_analysis#Classic_credit_analysis" rel="noopener ugc nofollow" target="_blank">贷款违约状态—信用数据</a>，物理实验中的<a class="ae lb" href="https://en.wikipedia.org/wiki/Pendulum_(mechanics)#Small-angle_approximation" rel="noopener ugc nofollow" target="_blank">摆的周期—摆长的平方根</a>，或者猫狗图像分类中的<a class="ae lb" href="https://en.wikipedia.org/wiki/Computer_vision" rel="noopener ugc nofollow" target="_blank">图像标签— RGB图像</a>🐈/🐕。</li><li id="702a" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">我们想要创建一个机器学习模型<em class="lc"> f </em> ( <strong class="kh ir"> G </strong>，<strong class="kh ir"> m </strong>)，该模型使用一组模型参数<strong class="kh ir"> m </strong>在给定一些解释变量<strong class="kh ir">G</strong>:<strong class="kh ir">d’</strong>=<em class="lc">f</em>(<strong class="kh ir">G</strong>，<strong class="kh ir"> m </strong>)的情况下对<strong class="kh ir"> d </strong>进行预测。</li><li id="b417" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">训练模型<em class="lc"> f </em> ( <strong class="kh ir"> G </strong>，<strong class="kh ir"> m </strong>)本质上是寻找能够给出最佳预测的<strong class="kh ir"> m </strong>。使用某种损失函数<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)来衡量模型预测的质量；<strong class="kh ir"> d </strong>，<strong class="kh ir">d’</strong>)，为了方便起见我们写成<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)。损失函数本质上是测量<strong class="kh ir">d’</strong>和<strong class="kh ir"> d </strong>之间的误差，一般来说<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)的值越小，模型的预测越好。</li><li id="5494" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">当我们最终找到导致最小<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)值的<strong class="kh ir"> m </strong>时，我们就结束了我们的搜索——对于给定的一组<strong class="kh ir"> d </strong>和<strong class="kh ir"> G </strong>，我们已经最小化(或优化)<em class="lc"> L </em> ( <strong class="kh ir"> m </strong>)！</li></ol><p id="42e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，正如“最佳”是一个非常主观的词，损失函数也是如此！在某些情况下，一些损失函数会比其他函数更好，使用好的损失函数可以显著提高模型性能！不幸的是，对于某些任务，找到一个好的损失函数可能是一个不小的问题。</p><p id="f78e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在许多情况下，我们通常从众所周知的损失函数开始，然后再转向更复杂的损失函数。通过使用不同的损失函数重新创建模型，我们通常最终会找到产生最佳性能模型的东西！</p><p id="ae13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的两节中，我们将讨论回归和分类问题中使用的一些损失函数。</p><h1 id="8aca" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">线性回归的损失函数</strong></h1><p id="a32d" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">对于这里的回归损失函数，我们将预测值和观测值之间的误差定义为:<em class="lc"> eᵤ </em> ( <strong class="kh ir"> m </strong>)是向量的第<em class="lc"> u </em>个元素:</p><p id="1d94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">e</strong>(<strong class="kh ir">m</strong>)<em class="lc">=</em><strong class="kh ir">d</strong><em class="lc"/>-<em class="lc">f</em>(<strong class="kh ir">G</strong>，<strong class="kh ir"> m </strong>)，</p><p id="36cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而<em class="lc"> mᵥ </em>是矢量<strong class="kh ir"> m </strong>的第<em class="lc"> v </em>个元素。</p><p id="0fb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步显示不同回归损失函数的效果，我们对以下问题执行线性回归:通过由等式产生的19个点找到最佳拟合曲线:</p><p id="0a0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lc">y</em>(<em class="lc">x</em>)= 3<em class="lc">x</em>-<em class="lc">x</em>+2、</p><p id="9f69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在原始数据集的右侧添加了两个异常点。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/bbba03ea03a37af63bebaa298ba2d584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*DJfmpunAg7Rolou5QBDRLg.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><em class="nf">从y</em>(<em class="nf">x</em>)= 3<em class="nf">x</em>-<em class="nf">x</em>+2中抽取的一些数据点，有两个离群点。作者创作的人物。</p></figure><ul class=""><li id="dbb2" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><strong class="kh ir">均方误差(L2范数)</strong><br/><em class="lc">l</em>(<strong class="kh ir">m</strong>)=σ<em class="lc">ᵤ</em>(<em class="lc">eᵤ</em>(<strong class="kh ir">m</strong>))。<br/>这可能是回归问题最广泛使用的损失函数，假设数据中的噪声来自高斯分布。由于误差的平方，该损失函数受到异常值的强烈影响，如下图所示。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ca236a1f272f52d910042285d8156748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*ugD4_kV93MQAHirsnNvNsA.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">使用L2范数误差(均方差)训练的模型的最佳拟合曲线。L2范数误差受离群数据点的影响。作者创作的人物。</p></figure><ul class=""><li id="8769" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><strong class="kh ir">平均绝对误差(L1范数)</strong><br/><em class="lc">l</em>(<strong class="kh ir">m</strong>)=σ<em class="lc">ᵤ</em>|<em class="lc">eᵤ</em>(<strong class="kh ir">m</strong>)|。<br/>该损失函数不像均方误差那样广泛使用，因为它假设数据中的噪声来自指数分布。由于使用了误差的绝对值，该损失函数不会受到异常值的强烈影响，如下图所示——最佳拟合曲线完全忽略了两个异常点！</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ad3ec6518c9d2ba7ef0da2964348d847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*tdsUj-1Tpz9DJkCY5gz65Q.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">使用L1范数误差(平均绝对误差)训练的模型的最佳拟合曲线。L1范数误差不受离群数据点的强烈影响。作者创作的人物。</p></figure><ul class=""><li id="589c" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><strong class="kh ir"> Lasso正则化均方误差(L2范数带L1正则化)</strong><br/><em class="lc">l</em>(<strong class="kh ir">m</strong>)=σ<em class="lc">ᵤ</em>(<em class="lc">eᵤ</em>(<strong class="kh ir">m</strong>)+λσ<em class="lc">ᵥ</em>|<em class="lc">mᵥ</em>|，其中λ为待指定的超参数。如果λ = 0，这个损失函数就变成了均方误差。<br/>如果数据包含许多异常值，则使用lasso正则化均方误差，但如果假设数据中的噪声来自高斯分布。实质上，套索正则化迫使模型参数尽可能接近零。lasso误差损失函数受异常数据点的影响不如正则化项导致的均方误差大，但比误差项平方导致的平均绝对误差大，如下图所示。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/19bca9eaaf417d63b5a42d8a8f12be72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*F2qfN9FaKCjxYxXxnU2O2w.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">使用套索误差训练的模型的最佳拟合曲线。lasso误差受异常数据点的影响不如均方差大，但比平均绝对误差大。作者创作的人物。</p></figure><ul class=""><li id="f2d4" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><strong class="kh ir">岭正则化均方误差(L2范数加L2正则化)</strong><br/><em class="lc">l</em>(<strong class="kh ir">m</strong>)=σ<em class="lc">ᵤ</em>(<em class="lc">eᵤ</em>(<strong class="kh ir">m</strong>)+λσ<em class="lc">ᵥmᵥ</em>，其中λ为待指定的超参数。如果λ = 0，这个损失函数就变成了均方误差。<br/>岭正则化均方误差用于数据中存在多重共线性。理想情况下，解释变量中的任何两个特征都不应该相互关联。然而，这有时是无法避免的，可以使用岭正则化来确保模型参数具有相同的大小。下图显示了岭正则化线性回归模型的最佳拟合曲线。</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/49c25dc98d12bbd6250f80984355a68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*OIv41HmL5fGnUtQuxpWw0w.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">使用岭误差损失函数训练的模型的最佳拟合曲线。如果数据中存在多重共线性，则岭误差会变得有意义，但这里的情况并非如此。作者创作的人物。</p></figure><h1 id="f86a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">逻辑回归的损失函数</h1><p id="be27" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/machine-learning-102-logistic-regression-9e6dc2807772">机器学习102:逻辑回归</a>中，我们展示了对于分类问题，我们建模概率而不是实际类别。因此，分类问题中使用的损失函数直接处理来自模型的概率输出:<em class="lc">p</em>(<strong class="kh ir">m</strong>)=<em class="lc">f</em>(<strong class="kh ir">G</strong>，<strong class="kh ir"> m </strong>)。</p><p id="2fb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步显示不同损失函数的影响，我们对用scikit-learn打包的<a class="ae lb" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank">乳腺癌数据集</a>进行了逻辑回归。原始数据集有357个良性(1级)和212个恶性(0级)数据点。虽然原始数据集已经不平衡，但我们通过将恶性数据点的数量减少到50个来进一步扭曲数据，导致1类和0类之间的比例为87:12。</p><ul class=""><li id="4c80" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><strong class="kh ir">二元交叉熵</strong><br/><em class="lc">L</em>(<strong class="kh ir">m</strong>)=-(<em class="lc">y</em>log(<em class="lc">p</em>(<strong class="kh ir">m</strong>)+(1-<em class="lc">y</em>)log(1-<em class="lc">p</em>(<strong class="kh ir">m</strong>))。<br/>这是最广泛使用的二元分类损失函数。对于多类分类，存在一个等价的分类。然而，由于二进制交叉熵平等地对待类0和类1，因此它通常不适合不平衡的数据集，这可以从下面的分类报告中看出-训练的模型无法检测类0的任何实例。</li></ul><pre class="le lf lg lh gt ni nj nk nl aw nm bi"><span id="92d8" class="nn lu iq nj b gy no np l nq nr">              precision    recall  f1-score   support</span><span id="a802" class="nn lu iq nj b gy ns np l nq nr">           0       0.00      0.00      0.00        15 <br/>           1       0.85      1.00      0.92        87</span><span id="bf3e" class="nn lu iq nj b gy ns np l nq nr">    accuracy                           0.85       102<br/>   macro avg       0.43      0.50      0.46       102<br/>weighted avg       0.73      0.85      0.79       102</span></pre><ul class=""><li id="7e0f" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la ng mw mx my bi translated"><a class="ae lb" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">焦十字熵</strong></a><br/><em class="lc">l</em>(<strong class="kh ir">m</strong>)=-(<em class="lc">y</em><em class="lc">α</em>(1-<em class="lc">p</em>(<strong class="kh ir">m</strong>)<em class="lc">ᵞ</em>log(<em class="lc">p</em>(<strong class="kh ir">m</strong>)+(1-<em class="lc">y</em>如果<em class="lc"> α </em> = 1且<em class="lc"> γ </em> = 0，则该焦交叉熵损失转化为二元交叉熵损失。默认情况下，<em class="lc"> α </em> = 0.25，<em class="lc"> γ </em> = 2。<br/>焦点交叉熵损失是对二元交叉熵损失的扩展，它考虑了具有比例因子的两个类别的比例:<em class="lc"> α </em>和(1 - <em class="lc"> α </em>，并迫使模型关注具有调制因子的错误分类预测:(1-<em class="lc">p</em>(<strong class="kh ir">m</strong>)<em class="lc">ᵞ</em>和<em class="lc">p</em>(【T89】)对于不平衡数据集，焦点交叉熵损失往往优于二进制交叉熵损失，如下面的分类报告所示-训练模型现在更能够检测类0的实例。</li></ul><pre class="le lf lg lh gt ni nj nk nl aw nm bi"><span id="47c7" class="nn lu iq nj b gy no np l nq nr">              precision    recall  f1-score   support</span><span id="5357" class="nn lu iq nj b gy ns np l nq nr">           0       1.00      0.87      0.93        15<br/>           1       0.98      1.00      0.99        87</span><span id="3d37" class="nn lu iq nj b gy ns np l nq nr">    accuracy                           0.98       102<br/>   macro avg       0.99      0.93      0.96       102<br/>weighted avg       0.98      0.98      0.98       102</span></pre><h1 id="d9b5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">复杂模型的损失函数</h1><p id="ce54" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在前两节中，我们介绍了回归和分类模型中一些常用的损失函数。上面的列表并不详尽，还有许多其他更复杂的损失函数！此外，某些机器学习算法使用特别定制的损失——例如<a class="ae lb" href="https://en.wikipedia.org/wiki/Hinge_loss" rel="noopener ugc nofollow" target="_blank">铰链损失</a>被广泛用于<a class="ae lb" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机</a>，而<a class="ae lb" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">基尼杂质</a>被广泛用于<a class="ae lb" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">基于决策树的模型</a>。</p><p id="3667" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，深度学习模型等高度复杂模型的损失函数往往具有多个局部最小值。在这种情况下，在模型训练期间，优化过程很可能陷入局部最小值，而不是收敛到损失函数的全局最小值。因此，对于复杂模型，通常使用不同的随机生成的初始模型参数同时训练几个模型，以增加模型收敛的机会，并使用随机优化器，如<a class="ae lb" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>，以随机将优化器踢出局部最小值。</p><h1 id="8b93" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><p id="85e9" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在本文中，我们仔细研究了损失函数在训练机器学习模型中所起的作用，以及使用不同的损失函数如何导致同一组数据的非常不同的模型性能。使用适当的损失函数将确定回归模型是否过度拟合离群数据点，或者分类模型是否过度拟合多数类。数据科学家和机器学习工程师应该注意使用适合他们正在处理的问题的损失函数！</p><h1 id="0e85" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="e06a" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">[1] W. M. Menke (2012)，<em class="lc">地球物理数据分析:离散逆理论MATLAB版</em>，Elsevier。</p><p id="153d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] C. M. Bishop (2006)，<em class="lc">模式识别与机器学习</em>，Springer</p><p id="8386" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]宗-林逸，普里亚·戈亚尔，罗斯·吉尔希克，明凯·何和皮奥特·多勒(2018)，<a class="ae lb" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lc">密集物体探测的焦损失</em> </a>，<a class="ae lb" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"> arXiv:1708.02002 </a>。</p></div></div>    
</body>
</html>