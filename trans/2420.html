<html>
<head>
<title>Super Resolution — A Basic study</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超分辨率——基础研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/super-resolution-a-basic-study-e01af1449e13#2022-05-26">https://towardsdatascience.com/super-resolution-a-basic-study-e01af1449e13#2022-05-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8515" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对超分辨率、其根源以及通常用于模型训练的不同类型的损失函数的研究</h2></div><h2 id="eb43" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">什么是超分辨率？</h2><p id="3113" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">超分辨率是从对应的低分辨率图像重建照片级高分辨率图像的任务。在计算机视觉界，这一直是一项具有挑战性的任务。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/b9d03019425eecafffdd5dbcf3aec334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IV2G9AK3A2-e1VV7yNst7A.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">[来源:图片由作者提供]</p></figure><p id="9284" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">这项任务的主要挑战是使它尽可能逼真。</p><blockquote class="mp mq mr"><p id="665f" class="lb lc ms ld b le mk jr lg lh ml ju lj mt mm ll lm mu mn lo lp mv mo lr ls lt ij bi translated">照片写实主义是一种绘画风格，其中图像看起来如此真实，以至于很难与照片区分开来<strong class="ld ir"/>。(来源:[ <a class="ae mw" href="https://www.macmillandictionary.com/buzzword/entries/photorealism.html#:~:text=photorealism%20also%20photo%2Drealism,hard%20to%20distinguish%20from%20photographs" rel="noopener ugc nofollow" target="_blank"> 2 </a>)</p></blockquote><p id="bcc8" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">让我首先从一些可以使用的基本方法开始。</p><h1 id="ccf0" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">最近邻插值(非照片真实感)</h1><p id="6b61" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">目标是拍摄尺寸为2x2(随机选择的尺寸)的图像，并创建尺寸为4x4的更高分辨率版本(假设尺寸加倍，可以是任何相对尺寸)。一个<strong class="ld ir">天真的</strong> <strong class="ld ir">方法</strong>是获取偶数像素，并根据需要在其附近创建尽可能多的副本(如下图所示，再创建3个副本以使尺寸加倍，其效果是简单地使每个像素变大)。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ni"><img src="../Images/fe810aecc43beb694aa399c7e75cc8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z5L-pgxRyV5N0K_O_vy4PQ.png"/></div></div></figure><p id="187c" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">当应用于图像时，它给图像一个方格的外观，如下所示。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nj"><img src="../Images/7bd1a87d98c4cb042cef772b9651966b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2sAuMJthrKrI5WSo1H8BA.png"/></div></div></figure><p id="a481" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">这种简单方法的一个可能的应用是调整条形码的大小。</p><h2 id="cdf1" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">双线性插值</h2><p id="c7ac" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在这种方法中，对于每个新像素，它取其周围最近的2x2旧像素的加权和，根据其与旧像素的距离进行加权。由于颜色融合在一起，这会导致边缘稍微模糊。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nk"><img src="../Images/5a3968c951cbfbafdb069ce5c1fb9e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9f1hnDv9SGnsxaCB_YVQ3g.png"/></div></div></figure><p id="d2d9" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">超分辨率技术的目标是以保留边缘锐度并且图像看起来不像素化的方式进行插值。这为深度学习的进入铺平了道路，从简单的基于CNN的模型开始，后来使用更复杂的GAN架构。</p><h1 id="bb91" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">评估指标</h1><p id="dcdc" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">有两个常用指标用于评估超分辨率后的图像质量:</p><ol class=""><li id="f2cb" class="nl nm iq ld b le mk lh ml ko nn ks no kw np lt nq nr ns nt bi translated">峰值信噪比(PSNR):</li><li id="991f" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">结构相似性指数度量(SSIM)</li></ol><p id="bd0a" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated"><strong class="ld ir"> PSNR </strong></p><p id="5858" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">PSNR是信号的最大可能功率与影响其表示保真度的干扰噪声功率之间的比率。PSNR通常用于控制数字信号传输的质量。在图像的情况下，每个像素可以被认为是具有8位RGB值的信号的组成部分。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b7be9458fd8176ddaaff2571a05ffb2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*-ahhvU_Ml_doSmJBhQR-xQ.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">PSNR表情【来源:<a class="ae mw" href="http://amroamroamro.github.io/mexopencv/opencv/image_similarity_demo.html" rel="noopener ugc nofollow" target="_blank">http://amroamroamro . github . io/mexopencv/opencv/image _ similarity _ demo . html</a></p></figure><p id="5610" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">这里，MAX_I是像素的最大有效值，MSE是高分辨率图像和超分辨率图像之间的均方误差。使用对数标度是因为像素值具有非常宽的动态范围。</p><p id="7cd2" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">这是对整个图像的逐像素比较</p><p id="b858" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated"><strong class="ld ir"> SSIM </strong></p><p id="10cf" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">SSIM是一种试图复制人类视觉系统(HVS颜色模型)工作方式的方法。它是基于三个因素设计的——相关性、亮度失真和对比度失真。</p><p id="db89" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">该指数是在图像的各个窗口上计算的，而不是直接的逐像素比较。(更详细的解释可以在维基百科上的图片来源处找到)</p><p id="4138" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">如果<em class="ms"> x，y </em>是图像中尺寸为<em class="ms"> N </em> x <em class="ms"> N的窗口:</em></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oa"><img src="../Images/5dac5703c06ca4047288c6fed4642137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UcDVWPtAdwM06QojCq8-w.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">https://en.wikipedia.org/wiki/Structural_similarity SSIM表情【来源:<a class="ae mw" href="https://en.wikipedia.org/wiki/Structural_similarity" rel="noopener ugc nofollow" target="_blank"/></p></figure><h1 id="51e3" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">SRCNN模型</h1><p id="9ace" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">现在被认为是一种经典的方法，这是第一个在没有大量像素化的情况下成功创建清晰边缘的模型之一。</p><p id="6637" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">它利用CNN作为使用卷积神经网络(CNN)从低分辨率(LR)输入到高分辨率(HR)输出的映射函数。LR图像首先使用双三次插值进行超分辨率处理，然后通过网络输出另一幅超分辨率(SR)图像。使用通过将SR图像与原始HR图像进行比较而产生的损失来训练该模型。(当在上下文中涉及网络输入时，该论文将双三次超分辨率图像称为LR)</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ob"><img src="../Images/1289621114eec865da70cd10405e2664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pja5YK19abaevUfxjhIbXQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">超分辨率对比【来源:<a class="ae mw" href="https://arxiv.org/pdf/1501.00092.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1501.00092.pdf</a></p></figure><h2 id="61c8" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">网络</h2><p id="6695" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">SRCNN的神经网络被公式化为三步过程:</p><ol class=""><li id="a6b6" class="nl nm iq ld b le mk lh ml ko nn ks no kw np lt nq nr ns nt bi translated">小块提取和表示:将来自LR图像的小块转换成由一组特征图组成的高维向量表示</li><li id="3a6f" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">非线性映射:非线性映射将矢量表示转换成由一组特征映射组成的另一个高维矢量表示</li><li id="1be7" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">重建:聚合分片向量以生成最终的SR图像。</li></ol><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oc"><img src="../Images/7e85c128a39435eacf9b741d5e85aa85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8F4R05Wy3LDN8Nq80gC1mw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">SRCNN网络架构【来源:<a class="ae mw" href="https://arxiv.org/pdf/1501.00092.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1501.00092.pdf</a></p></figure><h2 id="d66b" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">损失函数</h2><p id="2db4" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">使用的损失函数是一组HR图像和LR图像之间的均方误差(MSE)</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi od"><img src="../Images/68648a2bf499eb02ced6d95fdefd823a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UeD6Fzj8_U8d_axPt8w3Uw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">SRCNN损失函数</p></figure><h2 id="29f7" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果</h2><p id="1efe" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">与当时的现有模型相比，SRCNN模型实现了最高的PSNR和SSIM值。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oe"><img src="../Images/ff910a70b5e11c229596d80a101dfba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9dJN2pAnf3e3QpNmj_b54g.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">SRCNN的调查结果【来源:<a class="ae mw" href="https://arxiv.org/pdf/1501.00092.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1501.00092.pdf</a></p></figure><p id="def4" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">这篇论文的一个更重要的结果是，它将这个问题带到了前沿，更多的人开始探索各种架构和损失函数，以提高SR图像的质量。</p><h1 id="d4d0" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">损失函数</h1><p id="bdc5" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在过去几年中，已经设计了多种损失函数来训练超分辨率模型</p><p id="4cfd" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">基于像素的损耗:</p><ol class=""><li id="8561" class="nl nm iq ld b le mk lh ml ko nn ks no kw np lt nq nr ns nt bi translated">像素损失(MSE)</li><li id="fde9" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">平均绝对误差</li><li id="ef6a" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">总变化损失(电视损失)</li></ol><p id="1bcf" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">基于网络的损耗:</p><ol class=""><li id="6c17" class="nl nm iq ld b le mk lh ml ko nn ks no kw np lt nq nr ns nt bi translated">知觉丧失</li><li id="56ed" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">纹理损失</li><li id="41a4" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">对抗性损失</li></ol><h1 id="ae31" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">基于像素的损失</h1><h2 id="cc2a" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">像素损失(MSE)</h2><p id="951a" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">这与SRCNN中使用的均方误差(L2损失)相同，SR图像中的每个像素与HR图像中的相对像素进行比较。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi od"><img src="../Images/68648a2bf499eb02ced6d95fdefd823a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UeD6Fzj8_U8d_axPt8w3Uw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">像素损失</p></figure><h2 id="69bc" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">平均绝对误差</h2><p id="af9e" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">这是超分辨率图像和高分辨率图像中像素之间的L1损耗。它是两组对应像素之间的绝对差值之和。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi of"><img src="../Images/d267b0d733d4cd20cdf4a8ee1466aa20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WUxgI3Px7ai4LtzSeFMAYg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">MAE损失</p></figure><p id="b378" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">因为与MSE相比，它受离群值的影响较小，所以从人类观察者的角度来看，它更有可能给出更高质量的图像。</p><h2 id="29db" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">电视损耗</h2><p id="f9ec" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">总变差损失用于抑制生成图像中的噪声。它通过获取相邻像素之间的绝对差值来测量图像中的噪声。</p><p id="9cd4" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">对于具有高度<em class="ms"> H </em>、宽度<em class="ms"> W </em>和通道<em class="ms"> C、</em>的SR图像</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi og"><img src="../Images/7c0429096072c3d47f58c1aa2123a0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AqXYh-gtvVa0NG6-Jklsw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">电视损耗</p></figure><p id="2ec8" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">注意，与其他损失函数不同，这仅取决于SR图像，而不与其HR对应物进行比较。</p><h1 id="2851" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">网络损耗</h1><h2 id="47e8" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">知觉丧失</h2><p id="df87" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">该损失函数基于与原始HR图像的感知质量相比所生成的SR图像的感知质量来评估损失。一种计算方法是通过使用来自预训练图像分类网络的高级特征。</p><p id="8488" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">对于给定的预训练网络<em class="ms"> N </em>(可以是VGG或雷斯网)，以及该网络中的层<em class="ms"> l </em>，</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oh"><img src="../Images/ca76fef440b2b4ab769fa023dc358cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyCgoVLkVfQPbRtJAtAhrQ.png"/></div></div></figure><p id="96eb" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">不是在图像的像素之间取MSE，而是在两个图像的高级特征之间取MSE。它也被称为内容损失。</p><h2 id="3b5d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">纹理损失</h2><p id="df31" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">这个灵感来源于<em class="ms">风格转移</em> ( <a class="ae mw" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> Gatys et。艾尔。</em> </a>)接近。该损失函数用于使SR图像具有与HR图像相同的风格。风格是指图像中呈现的纹理、颜色和对比度。也被称为<em class="ms">风格损失</em>或<em class="ms">克矩阵损失。</em>不是直接从特征图中获取损失，而是从特征图中计算gram矩阵并用于计算损失。</p><p id="d7ac" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">向量集的格拉姆矩阵是由这些向量的所有可能的内积组成的矩阵。在这种情况下，向量是从预训练网络<em class="ms"> N </em>的层<em class="ms"> l </em>获得的特征通道。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oi"><img src="../Images/080ae05c98c24b14fc6d7d1fd32f94d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouBVhNL7mVc-GfDWzNtnVw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图像的Gram矩阵计算(I和j表示来自同一层的两个特征图)</p></figure><p id="7ca2" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">该矩阵表示各特征通道之间的相关性，本文将其定义为图像的纹理。一旦我们有了克矩阵<em class="ms"> G，</em>纹理损失被定义为</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi oj"><img src="../Images/798770f077879b25008f21fb7e5f7d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABx0MtmFqfF45bUU_xn0OA.png"/></div></div></figure><p id="8d76" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">该损失函数中的MSE取自gram矩阵之间，而不是直接取自特征图。这种度量经常被视为在生成的图像中产生更好的色彩。</p><h2 id="f965" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">对抗性损失</h2><p id="e48f" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">近年来，生成对抗网络被大量用于图像的超分辨率。在这方面，有两种相互竞争的模式:</p><ol class=""><li id="7f3a" class="nl nm iq ld b le mk lh ml ko nn ks no kw np lt nq nr ns nt bi translated">生成器—生成逼真的图像</li><li id="ada3" class="nl nm iq ld b le nu lh nv ko nw ks nx kw ny lt nq nr ns nt bi translated">鉴别器—将图像分类为真实图像或由生成器生成的图像</li></ol><p id="49de" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">通过反复训练这两个网络，我们得到了一个<em class="ms">生成器</em>，它可以从原始LR图像生成非常逼真的图像。</p><h1 id="d362" class="mx kg iq bd kh my mz na kk nb nc nd kn jw ne jx kr jz nf ka kv kc ng kd kz nh bi translated">参考</h1><p id="f1be" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated"><em class="ms">“所有图片，除非特别注明，均为作者所有。”</em></p><p id="447f" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">董，李，何，唐，谢:学习用于图像超分辨率的深度卷积网络。ECCV (2014年)</p><p id="5c31" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">L. A .的Gatys、A. S .的Ecker和m .的Beth ge(2016年)。使用卷积神经网络的图像风格转换。在<em class="ms">IEEE计算机视觉和模式识别会议论文集</em>(第2414–2423页)。</p><p id="3726" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated"><a class="ae mw" href="https://theailearner.com/2018/11/15/image-interpolation-using-opencv-python/" rel="noopener ugc nofollow" target="_blank">https://theailearner . com/2018/11/15/image-interpolation-using-opencv-python/</a></p><p id="3890" class="pw-post-body-paragraph lb lc iq ld b le mk jr lg lh ml ju lj ko mm ll lm ks mn lo lp kw mo lr ls lt ij bi translated">cambridgeincolour.com/tutorials/image-interpolation.htm</p><div class="ok ol gp gr om on"><a href="https://en.wikipedia.org/wiki/Structural_similarity" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">结构相似性-维基百科</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">结构相似性指数测度(SSIM)是一种预测数字电视感知质量的方法</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="ok ol gp gr om on"><a href="http://amroamroamro.github.io/mexopencv/opencv/image_similarity_demo.html" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ir gy z fp os fr fs ot fu fw ip bi translated">相似性度量(PSNR和SSIM)</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">有损压缩下的图像相似性。来源:我们想检查我们的压缩操作是多么难以察觉…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">amroamroamro.github.io</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb me on"/></div></div></a></div></div></div>    
</body>
</html>