<html>
<head>
<title>Labeling And Visualizing Images For Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标记和可视化用于对象检测的图像</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/labeling-and-visualizing-images-for-object-detection-55b48f0da326#2022-05-10">https://towardsdatascience.com/labeling-and-visualizing-images-for-object-detection-55b48f0da326#2022-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d888" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">经典的计算机视觉深度学习示例项目从包含图像和标签的数据集开始。然而，当需要计算机视觉来解决业务问题时，数据通常是未标记的，标记数据本身就是一个挑战。本文介绍了大规模标注影像以及相关的挑战</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b492d1f4ecd3f909b465f1266dc33be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dVQcijWpo54S4IJ7.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自带注释边框的谷歌地图|塞犍陀·维维克</p></figure><p id="50c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度学习计算机视觉项目的经典例子始于包含图像和标签的数据集。这取决于你所拥有的标签类型和你想要完成的任务——图像分类、物体检测或图像分割；你会从一组合适的深度学习模型中进行选择。有许多资源可以使用，包括Kaggle数据集和笔记本、GitHub repos，以及内置的示例数据集，如深度学习包TensorFlow和PyTorch中提供的MNIST数据集。这类示例项目的重点主要是选择模型架构和调整超参数。有时，对于独特的数据集，采用迁移学习可能是有意义的，其中你应用预先训练的模型(<a class="ae lu" href="https://chaoscontrol.net/transfer-learning-tutorial-using-pytorch-on-animals-10-dataset/" rel="noopener ugc nofollow" target="_blank">参见我的博客，例如PyTorch on Animals-10 Dataset </a>中的深度迁移学习教程)。</p><p id="148b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在你遇到的大多数计算机视觉问题中，无论是在试图检测大型机械上的<a class="ae lu" href="http://www.nscorp.com/content/nscorp/en/bizns/developing-_intelligent-wayside-track-detectors.html" rel="noopener ugc nofollow" target="_blank">缺陷的公司，还是有特定需求的客户，标记数据都是一个挑战。举个例子，假设一家保险公司想要从他们的无人机拍摄的卫星图像中检测房屋。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/8f6633d1c6fcb86622981a98b34b2a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5piId7enor_BG_SD.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自谷歌地图|塞犍陀·维维克</p></figure><p id="832b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，无人机拍摄的图像或谷歌地图上的卫星图像没有预先标记。你跟踪的这么多现有的深度学习资源，现在都缺少一个关键的步骤，而这个步骤是你启动项目所必须的。</p><blockquote class="lw"><p id="f031" class="lx ly it bd lz ma mb mc md me mf lt dk translated">首先，你如何给图像加标签？</p></blockquote><p id="b2a6" class="pw-post-body-paragraph ky kz it la b lb mg ju ld le mh jx lg lh mi lj lk ll mj ln lo lp mk lr ls lt im bi translated">原来有多个图像标注服务商。然而，缺乏关于如何给图像加标签的教程。这很令人惊讶。我发现了一个很好的免费在线解决方案——<a class="ae lu" href="https://www.makesense.ai/" rel="noopener ugc nofollow" target="_blank">make sense . ai</a>。如下图所示，很容易上手。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d2ac346f8e1f15efd00f48035faa8c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Pz2BxyOk64WFbghc.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Makesense.ai</p></figure><p id="adaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上传图像后，请确保根据您是要检测图像中的多个对象还是要分类(识别)图像来选择正确的标记任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/f62c2c1bc161f64ea0afa91a5ede4a08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*LG6ew-pfaTbwj5HJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Makesense.ai</p></figure><p id="0344" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，创建一个标签列表。这里我只是检测房屋，所以我将只创建一类标签。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/c032b37c7c3a0786ceba58680f8b8bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ELmT5OUeNFZYWroE.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Makesense.ai</p></figure><p id="4b00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我通过在各个图像周围画一个边界框来注释图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/b8a7033cdefa46c030fb5faa43e46000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Dt5GnG9pPZ5U4ulP.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Makesense.ai</p></figure><p id="2b4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我以特定的格式下载注释。在这种情况下，我选择YOLO，这是一个受欢迎的家庭的对象检测模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/5bd7fd340d64a6c495b762ffd261ba80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ptIykbiLJMaSrxxI.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Makesense.ai</p></figure><p id="2d06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我打开zip文件夹时，注释在一个. txt文件中，其内容如下。每一排都是不同的房子。这5列表示对象类别、中心X和Y坐标以及注释的宽度和高度。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="c416" class="mu mv it mq b gy mw mx l my mz">#YOLO annotations</span><span id="d54a" class="mu mv it mq b gy na mx l my mz">0 0.100204 0.266547 0.122651 0.425760<br/>0 0.245263 0.257603 0.122651 0.436494<br/>0 0.373811 0.248658 0.110858 0.450805<br/>0 0.502359 0.251342 0.117934 0.420394<br/>0 0.633265 0.277281 0.113217 0.411449<br/>0 0.761224 0.281753 0.119113 0.441860<br/>0 0.229931 0.764758 0.103782 0.449016<br/>0 0.367324 0.751342 0.104961 0.404293<br/>0 0.499410 0.739714 0.123831 0.459750<br/>0 0.766531 0.722719 0.129727 0.486583<br/>0 0.633855 0.753131 0.119113 0.432916<br/>0 0.909820 0.735242 0.126189 0.490161</span></pre><p id="b279" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们如何知道我们的注释是正确的？我们可以在Python中加载我们的注释和图像，并创建一个自定义函数来可视化我们的注释，如下所示:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="1d68" class="mu mv it mq b gy mw mx l my mz">from PIL import Image, ImageDraw<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import os</span><span id="dd9b" class="mu mv it mq b gy na mx l my mz">#code adapted from <a class="ae lu" href="https://blog.paperspace.com/train-yolov5-custom-data/" rel="noopener ugc nofollow" target="_blank">https://blog.paperspace.com/train-yolov5-custom-data/</a></span><span id="1919" class="mu mv it mq b gy na mx l my mz">def plot_bounding_box(image, annotation_list):<br/>    annotations = np.array(annotation_list)<br/>    w, h = image.size<br/>    <br/>    plotted_image = ImageDraw.Draw(image)</span><span id="431e" class="mu mv it mq b gy na mx l my mz">transformed_annotations = np.copy(annotations)<br/>    <br/>    try: <br/>        transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w<br/>        transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h <br/>    <br/>        transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)<br/>        transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)<br/>        transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]<br/>        transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]<br/>    except:<br/>        transformed_annotations[[1,3]] = annotations[[1,3]] * w<br/>        transformed_annotations[[2,4]] = annotations[[2,4]] * h <br/>    <br/>        transformed_annotations[1] = transformed_annotations[1] - (transformed_annotations[3] / 2)<br/>        transformed_annotations[2] = transformed_annotations[2] - (transformed_annotations[4] / 2)<br/>        transformed_annotations[3] = transformed_annotations[1] + transformed_annotations[3]<br/>        transformed_annotations[4] = transformed_annotations[2] + transformed_annotations[4]  <br/>        <br/>        print(transformed_annotations)<br/>        <br/>        <br/>    for ann in transformed_annotations:<br/>        try:<br/>            obj_cls, x0, y0, x1, y1 = ann<br/>            plotted_image.rectangle(((x0,y0), (x1,y1)), width = 10, outline="#0000ff")<br/>        <br/>        except: <br/>            obj_cls= transformed_annotations[0]<br/>            x0=transformed_annotations[1]<br/>            y0=transformed_annotations[2]<br/>            x1=transformed_annotations[3]<br/>            y1=transformed_annotations[4]<br/>            plotted_image.rectangle(((x0,y0), (x1,y1)), width = 10, outline="#0000ff")<br/>        <br/>    <br/>    plt.imshow(np.array(image))<br/>    plt.show()</span><span id="3518" class="mu mv it mq b gy na mx l my mz">#get an annotation file<br/>annotation_file = './houses.txt'</span><span id="9847" class="mu mv it mq b gy na mx l my mz">#Get the corresponding image file<br/>image_file = annotation_file.replace("txt", "png")<br/>assert os.path.exists(image_file)</span><span id="0ca0" class="mu mv it mq b gy na mx l my mz">#Load the image<br/>image = Image.open(image_file)</span><span id="f745" class="mu mv it mq b gy na mx l my mz">#Plot the Bounding Box<br/>plot_bounding_box(image, np.loadtxt(annotation_file))</span></pre><p id="fba4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注释看起来一点不错！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/d9f78a3a7408ce6ef17337a52915d53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*P3eoDT6OLVNSHwRt.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自带注释边框的谷歌地图|塞犍陀·维维克</p></figure><h1 id="6f0a" class="nc mv it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">结论和挑战</h1><p id="ebe5" class="pw-post-body-paragraph ky kz it la b lb nt ju ld le nu jx lg lh nv lj lk ll nw ln lo lp nx lr ls lt im bi translated">虽然makesense.ai是一个很棒的免费平台，但它不具备可扩展性。为了训练一个YOLO风格的对象检测模型，你至少需要数百个(如果不是数千个)对象来获得合理的精度。Makesense.ai不保存任何注释或合并工作流。你需要一次单独完成所有这些。如果一个人连续坐几个小时来标记成千上万的图像，将会花费大量的时间。还有其他服务，比如提供定制报价和标签服务的ango.ai，每张图片只需几分钱。在我看来，这是一个有待改进和挖掘潜力的领域。随着人工智能变得越来越受欢迎和易于使用，对大规模轻松、准确和廉价的定制标签的需求将变得更加重要。</p><p id="de00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这向您展示了端到端计算机视觉项目的冰山一角；模特训练只是拼图的一部分。请关注更多讨论以业务为中心的端到端深度学习的其他关键方面的博客！</p><p id="bd93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以从GitHub上的这篇文章中找到代码:</p><p id="85f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">https://github.com/skandavivek/Visualizing-YOLO-annotations<a class="ae lu" href="https://github.com/skandavivek/Visualizing-YOLO-annotations" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="ae64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="of">如果你还不是中会员，想支持我这样的作家，可以通过我的推荐链接随意报名:</em><a class="ae lu" href="https://skanda-vivek.medium.com/membership" rel="noopener"><em class="of">https://skanda-vivek.medium.com/membership</em></a></p><p id="a1b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://medium.com/@skanda.vivek" rel="noopener"> <em class="of">关注我</em> </a> <em class="of">如果你喜欢这篇文章——我经常在数据科学、物理和社会的界面上写作。</em></p><p id="91e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="of">每周数据透视</em> <a class="ae lu" href="https://skandavivek.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="of">订阅此处</em> </a> <em class="of">！</em></p></div></div>    
</body>
</html>