<html>
<head>
<title>Geometric Priors II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几何先验II</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/geometric-priors-ii-6cd359dfbcf4#2022-06-07">https://towardsdatascience.com/geometric-priors-ii-6cd359dfbcf4#2022-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="bff5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/geometric-deep-learning" rel="noopener" target="_blank">几何深度学习</a></h2><div class=""/><div class=""><h2 id="571d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">GDL蓝图</h2></div><p id="58cc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">一系列博文，总结了AMMI计划的</em> <a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">几何深度学习(GDL)课程</em></a><em class="lk">；</em> <a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">非洲机器智能硕士</em> </a> <em class="lk">，授课老师</em> <a class="ae ll" href="https://www.cs.ox.ac.uk/people/michael.bronstein/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">迈克尔·布朗斯坦</em> </a> <em class="lk">，</em> <a class="ae ll" href="https://cims.nyu.edu/~bruna/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">琼·布鲁纳</em> </a> <em class="lk">，</em> <a class="ae ll" href="https://tacocohen.wordpress.com/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">塔科·科恩</em> </a> <em class="lk">，以及</em><a class="ae ll" href="https://petar-v.com/" rel="noopener ugc nofollow" target="_blank">T35】佩塔尔·韦利奇科维奇</a></p><p id="f143" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">维数灾难是高维学习中最具挑战性的问题之一。在之前的一篇帖子<em class="lk"> ( </em> <a class="ae ll" rel="noopener" target="_blank" href="/geometric-priors-i-cc9dc748f08"> <em class="lk">几何先验I </em> </a> <em class="lk"> ) </em>中，我们讨论了机器学习和几何深度学习中的各种概念，包括对称、不变和等变网络。在这篇文章中，我们将这些数学概念与维数灾难联系起来。然后，我们引入第二个几何先验；<em class="lk">尺度分离</em>，代表了战胜维数灾难的有效工具。最后，我们将看到如何将这两个几何先验(对称性和尺度分离)结合起来，构建<em class="lk">几何深度学习(GDL)蓝图</em>，它可以作为不同几何域的通用架构。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/796b8cfb53b1645cb6ec367b95f9a4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dB4dJMxg6ksC6w7yGe0hqQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">尺度分离先验(图像分类)。图片来自GDL原型书，第3.4节。</p></figure><p id="e481" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">本帖与</em><a class="ae ll" href="https://medium.com/@mmohamedkhair" rel="noopener"><em class="lk">M . Elfatih Salah</em></a><em class="lk">合著。另见上一篇</em> s <em class="lk">关于</em> <a class="ae ll" rel="noopener" target="_blank" href="/geometric-deep-learning-da09e7c17aa3"> <em class="lk"> Erlangen程序的ML </em> </a>，<a class="ae ll" rel="noopener" target="_blank" href="/high-dimensional-learning-ea6131785802"> <em class="lk">高维学习</em> </a>，<a class="ae ll" rel="noopener" target="_blank" href="/geometric-priors-i-cc9dc748f08"> <em class="lk">几何先验I </em> </a> <em class="lk">。这些博文是根据四位导师的</em> <a class="ae ll" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank"> <em class="lk"> GDL原书</em> </a> <em class="lk">和</em> <a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> <em class="lk"> GDL课程</em></a><em class="lk">at</em><a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"><em class="lk">AMMI</em></a><em class="lk">改编的。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="b80a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> R </span>调用以前帖子中的学习设置，在统计学习中，我们有一个目标函数<em class="lk"> f </em> *，一个从输入空间𝒳到输出空间<em class="lk"> ℝ </em>的映射；<em class="lk"> f </em> * <em class="lk"> : </em> 𝒳 <em class="lk"> → ℝ.</em>我们也把输入空间𝒳写成一组信号𝒳(ω，𝒞)={𝓍∶ω→𝒞}，其中ω是几何域，𝒞是定义在ω上的向量空间。在<a class="ae ll" rel="noopener" target="_blank" href="/geometric-priors-i-cc9dc748f08"> <em class="lk">几何先验I </em> </a>中，我们已经看到了几何域的各种例子，以及如何使用域ω来捕捉与该域中信号的性质密切相关的各种变换。(本帖与<a class="ae ll" rel="noopener" target="_blank" href="/geometric-priors-i-cc9dc748f08"> <em class="lk">几何先验I </em> </a> <em class="lk"> ) </em>。</p><p id="fcc6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这样，从前面介绍的内容来看，目标函数<em class="lk"> f </em> *定义为<em class="lk">f</em>*<em class="lk">:</em>𝒳(ω，𝒞) <em class="lk"> → ℝ </em>。我们的目标是在假设类𝓕中学习这个函数，我们可以按照我们想要的方式参数化它，例如使用神经网络。我们现在的承诺是目标<em class="lk"> f </em> *是𝔊-invariant ( <em class="lk"> f </em> *保持不变，如果考虑变换群𝔊的任何元素和任何信号‘输入’的话)。数学上，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/faa977b2adc9bf1b3dc30c0fe8561bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjlYX0wP7cXey1QKi7JvrA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">𝔊-Invariant函数。GDL课程，第四讲。</p></figure><blockquote class="mt"><p id="b3c2" class="mu mv iq bd mw mx my mz na nb nc lj dk translated">因此，自然的问题是:我们如何在假设课上利用这个承诺？这足以打破维度的诅咒吗？</p></blockquote><p id="9635" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">为了回答第一个问题，让我们定义一个<em class="lk">组平滑算子</em>，它是一个采用任何假设<em class="lk"> f </em>并用组𝔊.中所有变换的平均值替换它的算子在一个数学设置中，如果𝔊是一个离散的有限群并且<em class="lk"> f </em>是我们的假设，我们将𝑓的𝔊-smoothing算子定义为，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/a6d1164764c57e56be2d22bc549590c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qO-U9LWaRdLxBIH8OgtYCQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">函数𝑓.的𝔊-Smoothing算子来自GDL课程的方程式，第四讲。</p></figure><p id="1e1d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们仔细观察，可以发现函数<em class="lk"> f </em>的这个平滑算子有一个有趣的性质；它也是𝔊-invariant(如果𝑓已经是𝔊-invariant，那么应用平滑运算符不会改变它)。因此，给定假设类𝓕，我们可以通过应用平滑算子使其成为𝔊-invariant，并且更正式地如下:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/4cb0bb7ddf1a213cde29a139c5d9b730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLPJhPG-ZPRKWwlDROph6Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">假设类𝓕.的𝔊-Smoothing算子来自GDL课程的方程式，第四讲。</p></figure><p id="8b3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">(同样，如果我们扩展不变性群𝔊，不变性类将变得更小，这似乎是直观的)。</p><h2 id="81d4" class="ni nj iq bd nk nl nm dn nn no np dp nq kx nr ns nt lb nu nv nw lf nx ny nz iw bi translated">不变性下的学习</h2><p id="8976" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">通过</span>将平滑算子应用于我们的假设类，我们可以证明近似误差不受该平滑操作的影响，并且我们可以这样正式表述:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/8c91a5474d8cb7da53a923f69cd362bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o2Od8bQ-WNAvQVW83czZpQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">近似误差不受平滑操作的影响。GDL课程，第四讲。</p></figure><p id="ec80" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了证明，我们可以注意到平滑算子是在<em class="lk"> L </em> (𝒳).)中的正交投影算子所以预测中产生的<em class="lk"> L </em>误差可以分解为该算子图像中的误差加上正交补中的误差，如下所示，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/bbfd16a485bbe74d6be84ff51286a6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1rlE6mskv9Y8i7lvvUSTg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">近似误差不受平滑操作的影响(证明)。GDL课程，第四讲。</p></figure><p id="6915" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，用<em class="lk"> f </em> *替换𝔤，知道<em class="lk"> f </em> *不受平滑操作影响，我们得到，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/218c8e7e72761cb99b58cd415f68a282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZRrMu_ULo9wDQIts5jmZQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">近似误差不受平滑操作的影响(证明)。GDL课程，第四讲。</p></figure><blockquote class="mt"><p id="364c" class="mu mv iq bd mw mx my mz na nb nc lj dk translated">另一方面，由于平滑后假设类更小，统计误差减少，但问题是:<em class="og">多少？</em></p></blockquote><p id="cd6b" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">看一看假设类之一，我们假设它是Lipschitz类，我们知道它具有以下属性:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/4111dd3ea815c8a3c481020ae1cefdde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7VmWqprfxbgPUx9zhvw3-A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">李普希茨函数类。GDL课程，第四讲。</p></figure><p id="9cc6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，其平滑版本具有以下属性:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/b183185a8e43a56722e297dc1fd9279b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7eyUQhmhojZS5x5_6mTvSQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">应用𝔊-Smoothing算子时的李普希茨函数类。GDL课程，第四讲。</p></figure><p id="e088" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">平滑版本使得Lipschitz假设更强，因为距离更弱，并且主要结果:</p><blockquote class="mt"><p id="aa0a" class="mu mv iq bd mw mx oh oi oj ok ol lj dk translated">定理[ <a class="ae ll" href="https://arxiv.org/pdf/2106.07148.pdf" rel="noopener ugc nofollow" target="_blank"> Bietti，Venturi，B.'21 </a> ]:利用一个𝔊-invariant核岭回归，学习一个李普希兹，𝔊-invariant函数<em class="og"> f </em> *的泛化误差满足𝔼𝓡<em class="og">(f̂)≲</em>θ(|𝔊|n)⁻/ᵈ，其中n是样本数，d是维数。</p></blockquote><p id="43b3" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">该定理导致样本复杂度的量化增益，该增益与组的大小成比例，因为泛化误差由样本的数量乘以组的大小来限定，都是<em class="lk"> -1/d. </em>的幂</p><p id="7edf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，组大小|𝔊|在维度上可以是指数的，例如，如果我们考虑局部转换(可能的局部转换的数量在域的大小上是指数的)。然而，正如我们所看到的，速率仍然受到维数的诅咒，这表明<em class="lk">引入群不变性不太可能打破维数的诅咒。</em></p><h2 id="7d2b" class="ni nj iq bd nk nl nm dn nn no np dp nq kx nr ns nt lb nu nv nw lf nx ny nz iw bi translated">到目前为止的结论:</h2><p id="8bf7" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">到目前为止，我们已经看到使用<em class="lk">全局对称性</em>或<em class="lk">不变性先验</em>减少了统计误差并保持近似误差不变，但是仍然不足以打破维数灾难。所以问题来了:<em class="lk">打破维度诅咒所必须的对称性之外的假设有哪些，我们遗漏了什么？</em></span></p><p id="a85f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">还有，我们一直在讨论理论问题，没有讨论如何高效地构建不变类。我们所描述的平滑操作符并不是我们想要在计算机中直接实现的东西(因为计算所有转换组的平均值是非常不切实际的)。从基本原则出发，我们如何高效地做到这一点？</p><h2 id="4ae5" class="ni nj iq bd nk nl nm dn nn no np dp nq kx nr ns nt lb nu nv nw lf nx ny nz iw bi translated">深度学习“归纳偏差”:组合性</h2><p id="47a0" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">深度学习研究领域有影响力的人</span>可能会在不同的演讲中提到，比如<a class="ae ll" href="http://yann.lecun.com/" rel="noopener ugc nofollow" target="_blank"> Yann LeCun </a>和Y <a class="ae ll" href="https://yoshuabengio.org/profile/" rel="noopener ugc nofollow" target="_blank"> oshua Bengio </a>，深度学习之所以有效，是因为<a class="ae ll" href="https://en.wikipedia.org/wiki/Principle_of_compositionality" rel="noopener ugc nofollow" target="_blank"> <em class="lk">组合性</em> </a> <em class="lk">原理。</em>此外，有来自大脑的证据表明，大脑也是分层组织的，这种想法并不是深度学习研究人员开发的，但它已经存在很长时间了。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi om"><img src="../Images/43baa8e5550770417a830a45945f6027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLp9L4POgNqTfwTdF0Wb9g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">深度学习为什么有效？Y <a class="ae ll" href="https://yoshuabengio.org/profile/" rel="noopener ugc nofollow" target="_blank">奥舒亚·本吉奥</a>(上)和<a class="ae ll" href="http://yann.lecun.com/" rel="noopener ugc nofollow" target="_blank">扬·勒村</a>(下)。GDL课程，第四讲。</p></figure><blockquote class="mt"><p id="f3a8" class="mu mv iq bd mw mx my mz na nb nc lj dk translated">既然我们似乎都同意组合性对深度学习的成功至关重要，我们应该问我们如何才能形式化这种直觉？</p></blockquote><h2 id="9b21" class="ni nj iq bd nk nl on dn nn no oo dp nq kx op ns nt lb oq nv nw lf or ny nz iw bi translated">多尺度结构</h2><p id="6390" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">形式化这种直觉的一个</span>方法是观察物理学和计算科学的不同领域。在不同的领域，我们可以看到<em class="lk">多尺度结构</em>是一个基本原则和许多学科的基础。</p><p id="4f2c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在计算生物学中，为了知道生物学是如何工作的，我们必须理解不同尺度下的过程。有些事情我们可以用分子动力学或者功能图来理解。<a class="ae ll" href="https://en.wikipedia.org/wiki/Turbulence" rel="noopener ugc nofollow" target="_blank"> <em class="lk">比如湍流</em> </a>，展现了数学物理中不同尺度之间的交流。<a class="ae ll" href="https://en.wikipedia.org/wiki/Percolation#:~:text=Percolation%20(from%20Latin%20percolare%2C%20%22,is%20described%20by%20Darcy's%20law." rel="noopener ugc nofollow" target="_blank"> <em class="lk">逾渗</em> </a>另一种模型，描述流体通过多孔材料的运动。因此，我们必须考虑不同的尺度。你可能也有自己的例子，在不同的尺度下，系统有不同的行为。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/72b65dd4d3dc50d085245e6bf027ee2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XmA3VYjul62H0AKbcr2VuA.gif"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">逾渗模型。来自<a class="ae ll" href="https://en.wikipedia.org/wiki/Percolation#:~:text=Percolation%20(from%20Latin%20percolare,%20%22,is%20described%20by%20Darcy's%20law." rel="noopener ugc nofollow" target="_blank">维基百科</a>的动画。</p></figure><h2 id="5026" class="ni nj iq bd nk nl nm dn nn no np dp nq kx nr ns nt lb nu nv nw lf nx ny nz iw bi translated">多分辨率分析基础</h2><p id="16f9" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>我们的上下文中，我们希望<em class="lk">将</em>多尺度结构的概念与几何域(网格、组、图形和流形)联系起来。另一个非常相关的概念是<em class="lk">多分辨率分析(MRA)。</em>(为了简单起见，我们将假设一个网格域，即ω是一个2D网格，但大多数东西可以更一般地描述，然后应用于所有的几何域)。</p><p id="6267" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">多分辨率分析是一种方案，该方案试图根据存在于<em class="lk">较小域</em> ( <em class="lk">较粗域</em> <em class="lk">域</em>)中的信号来分解存在于该域中的信号。例如，在网格域的情况下，我们想要更少的像素。在多分辨率分析中，我们试图理解如何从一个分辨率(特定数量的像素)到下一个分辨率(<em class="lk">减去</em>数量的像素)，以允许我们保留信息。一般来说，我们可以将一幅图像分解成不同分辨率(分辨率更低)的同一幅图像，加上我们需要返回到第一幅图像的细节(见下图)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ot"><img src="../Images/78193949af37f3ea4f9af8c0350181c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbmbLzuEKktcVZEgg6QE_g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图像分解。GDL课程，第四讲。</p></figure><blockquote class="mt"><p id="3e55" class="mu mv iq bd mw mx my mz na nb nc lj dk translated">在这里，我们必须声明，通过降低网格的分辨率，我们正在攻击维度的诅咒。这是由于在这种情况下像素数量的指数依赖性。我们可以保证，在某一分辨率下，由于样本数量与新分辨率成正比，因此将避免维数灾难。因此，我们的问题将是如何将多尺度结构的想法与学习中的先验结合起来？</p></blockquote><h1 id="bb74" class="ou nj iq bd nk ov ow ox nn oy oz pa nq kf pb kg nt ki pc kj nw kl pd km nz pe bi translated"><strong class="ak">秤分离之前</strong></h1><p id="c2fe" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> L </span>让我们讨论一些如何在学习中结合多尺度结构和先验知识的例子。想象一下，目标函数<em class="lk"> f </em> *可以在图像的粗糙版本上定义。在下图所示的分类问题中，我们只想知道图像是“<em class="lk">海滩</em>还是“<em class="lk">山</em>”而不是它的分辨率。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pf"><img src="../Images/56ac637a209e5d622959653f3bfdfb9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9y_pFHFMB_3LlCvNGBxuA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">两幅图像的原始版本(左)和粗糙版本(右)(分类问题)。GDL课程，第四讲。</p></figure><p id="2c71" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这个问题中，我们可以将图像的分辨率降低到能够给出正确分类的某个水平。如前所述，这将减少网格的大小(像素数)。因此，通过<em class="lk">粗化，我们可以避免维数灾难。然而，这通常是不正确的，因为我们需要非常小心粗化的程度。如下图所示，太粗化会丢失太多信息，我们无法分辨图像中到底有什么。</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pg"><img src="../Images/5ebe44d6b83b49dc082a76697e336879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKxDKJcv099UBQACx7vmsg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">过度粗化会丢失图像信息的示例。GDL课程，第四讲。</p></figure><p id="16bc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看多尺度结构也能有所帮助的另一个例子。假设目标函数<em class="lk"> f* </em>可以通过局部项的和来近似，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ph"><img src="../Images/27f1d5816c0fe20386ae8f9e88870e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-iHxcvLp3AePZp2-AHqGug.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">用局部项的和来逼近函数<em class="og"> f* </em>。GDL课程，第四讲。</p></figure><p id="9ca8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">局部术语<em class="lk"> x(u) </em>是指提取以像素<em class="lk"> u </em>为中心的<em class="lk">补丁图像</em>。比如在识别一个纹理(下图所示的分类问题)时，如果我们在一堵砖墙或者一堆草里。我们当地的一个术语<em class="lk">就属于这一类。因此，如果我们想要一个好的分类，我们可以只取局部描述符的平均值，因为在粗尺度上没有很多可变性(因为纹理是一个具有空间同质性的结构)。</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pi"><img src="../Images/33094513604bfb438dd5708c5f42146b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D04A5mw9A8Pd73Zn-WWaxQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">本地术语的示例:黄色块显示的补丁图像。GDL课程，第四讲。</p></figure><p id="a1e3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以看到，维数灾难也被避免了，因为这个问题中的相关维数减少到了<em class="lk">面片维数</em>。</p><p id="e0ac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还必须声明，这是一个强有力的假设，通常是不充分的，如下图所示；所有的本地术语都可能给出错误的分类。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pj"><img src="../Images/a293d50efad583062cfb7b5c25cf5dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KOpfghQ9vIB3soX0LIwoA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">当本地术语可能给出错误分类时的例子。GDL课程，第四讲。</p></figure><p id="1b44" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重要的是要说明这样一个事实，我们可能有原始尺度和粗略尺度的信息，但不知何故它们以一种不太极端的方式相互作用。因此，在某种意义上，我们必须将这两种尺度结合起来。</p><p id="971a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们考虑一个通用的组合模型，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pk"><img src="../Images/3233918a6b7f052d0a4d82e6873bce6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjTrbiYOnsq4Pp-RBuJrPQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">具有两个算子(非线性局部粗化和非线性假设)的一般成分模型。GDL课程，第四讲。</p></figure><p id="ab62" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">函数<em class="lk"> f </em> *由两个运算符组成；<em class="lk">非线性局部粗化</em>结合<em class="lk">非线性假设</em>在粗尺度上提取一些信息(比单独平均小块强得多)，两者都是可以学习的。你可能会问<em class="lk">为什么这个组合</em>和<em class="lk">什么时候可以论证这个组合模型更有效率</em>。当我们考虑一些具体的例子时，比如动态编程和分治算法，我们是从更小的问题的角度来看待这个问题的。在这些情况下，我们可以认为这种组合模型更有效。然而，这仍然是一个<em class="lk">未解决的问题</em>，并且从理论的角度来看，这个组成模型还没有被完全理解。</p><p id="409d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，我们结合尺度分离和群不变量这两个先验，从第一性原理给出一个强有力的模型。然后，我们介绍完整的<em class="lk">几何深度学习(GDL)蓝图。</em></p><h2 id="e9ad" class="ni nj iq bd nk nl nm dn nn no np dp nq kx nr ns nt lb nu nv nw lf nx ny nz iw bi translated">不变性与尺度分离的结合</h2><p id="0576" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> S </span>由于我们有一个函数类是<em class="lk">对群体行为不变的</em>(𝔊-invariant函数)，我们想把它和上面描述的<em class="lk">多尺度结构</em>(尺度分离之前)合并成一个<em class="lk">架构</em>。因为我们讨论过的这些几何先验给了我们模型中应该有的必要条件，而不是一个特定架构的所有要素。</p><p id="158f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们从一个对群作用不变的线性算符(<em class="lk">线性𝔊-invariant函数</em>)开始，使用前面介绍的𝔊-smoothing算符，我们可以写成，</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/90cf282a818268268f6e845e91cf69ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlAVoO0OlBLfatLtoU_YmQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">带有𝔊-Smoothing算子的线性𝔊-Invariant函数。GDL课程，第四讲。</p></figure><p id="3cec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中<em class="lk"> x̅ </em>是群轨道上的平均值(我们将<em class="lk"> x </em>的群平均值称为<em class="lk"> A x </em>)。(第一个不等式是因为<em class="lk"> f </em>是𝔊-invariant，我们在应用𝔊-smoothing算子，第二个不等式是因为<em class="lk"> f) </em>的线性。</p><p id="4852" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，如果我们仅仅依靠这个假设，我们会丢失很多信息。因为如果我们有一个平移组，例如，一个图像的轨道将由该图像所有可能的平移组成，然后我们只需将所有这些图像平均为一个。</p><p id="b922" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作为补充，我们可以使用<em class="lk">线性𝔊-equivariant函数</em>，我们在<a class="ae ll" rel="noopener" target="_blank" href="/geometric-priors-i-cc9dc748f08"> <em class="lk">几何先验I </em> </a>中讨论过。(如果𝔊同时作用于𝒳和𝒴，那么映射𝐵 : 𝒳 → 𝒴就是𝔊-equivariant。𝑥) = 𝔤.𝐵(𝑥)).为了实现一个更强大的模型，我们可以用一个<em class="lk">元素式非线性函数</em> 𝜎合成这个等变函数𝐵；𝜎 : 𝒳 → 𝒳，𝑤𝑖𝑡ℎ𝜎𝑥(<em class="lk">u</em>)= 𝜎(𝑥(<em class="lk">u</em>)，以给出一个<em class="lk">非线性𝔊-equivariant函数u，即U := </em> 𝜎 ⚬ 𝐵(由于非线性是逐元素应用的，<em class="lk"> U </em>将保持𝔊-equivariant).</p><p id="9873" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，我们可以通过组合组平均值<em class="lk"> A </em>和非线性𝔊-equivariant函数<em class="lk">u</em>来提供强大的𝔊-invariant函数</p><blockquote class="mt"><p id="38f8" class="mu mv iq bd mw mx oh oi oj ok ol lj dk translated">一个迫切的问题是，这种组合是否会产生一个富裕的阶层？换句话说，我们能很好地逼近任何一个目标函数，𝔊-invariant，并且有足够的b和𝜎选择吗？</p></blockquote><p id="a2f1" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">参考<a class="ae ll" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank"> <em class="lk">通用逼近定理</em> </a>，这种组平均与非线性等变映射的组合产生了通用逼近器。然而，为了更稳定的表示，我们需要使用一个<em class="lk">局部</em>等变图。另外，如果我们有一个具有长程相互作用的函数，我们不能用单层的局部𝔊-equivariant映射来近似它；相反，我们必须组合几个局部等变函数，使区域变粗。<em class="lk">(为证明，</em> <a class="ae ll" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank"> <em class="lk"> GDL原典</em> </a> <em class="lk">，第3.5节)。</em></p><p id="9915" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，为了构建一个丰富的𝔊-invariant函数类，我们需要一个局部等变映射、一个全局不变映射和一个粗化操作符。</p><h1 id="8f8e" class="ou nj iq bd nk ov ow ox nn oy oz pa nq kf pl kg nt ki pm kj nw kl pn km nz pe bi translated">几何深度学习(GDL)蓝图</h1><p id="e9bb" class="pw-post-body-paragraph ko kp iq kq b kr oa ka kt ku ob kd kw kx oc kz la lb od ld le lf oe lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> L </span>很快，我们可以在所谓的<em class="lk"> GDL蓝图</em>中，为学习目标函数<em class="lk"> f </em> *所需的模型勾勒出上面讨论的所有成分。GDL蓝图是一个建设性的方法，我们可以用它来定义一个<em class="lk">通用架构</em>，它可以应用于各种几何领域(网格、组、图形和流形)。</p><p id="53e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">设ω是一个定义域，𝔊是ω上的对称群，ω′是ω的一个更粗的定义域，即ω′⊆ω。几何深度学习(GDL)蓝图的构建模块是:</p><ul class=""><li id="8c50" class="po pp iq kq b kr ks ku kv kx pq lb pr lf ps lj pt pu pv pw bi translated"><em class="lk">线性𝔊-equivariant层</em>𝐵∶𝒞𝒳(ω)→𝒞′𝒳(ω′)，满足𝐵(𝔤.<em class="lk"> x </em> ) = 𝔤.𝐵(𝑥)适用于所有𝔤 ∈ 𝔊和𝑥∈𝒳(ω、𝒞).</li><li id="a94e" class="po pp iq kq b kr px ku py kx pz lb qa lf qb lj pt pu pv pw bi translated"><em class="lk">非线性</em>𝜎∶𝒞→𝒞′按元素应用as(𝝈(<em class="lk">x</em>)(<em class="lk">u</em>)= 𝜎(<em class="lk">𝑥</em>(<em class="lk">u</em>))。</li><li id="bca3" class="po pp iq kq b kr px ku py kx pz lb qa lf qb lj pt pu pv pw bi translated"><em class="lk">局部汇集(粗化)</em>𝑃∶𝒳(ω，𝒞)→𝒳(ω′，𝒞)，使得ω′⊆ω。</li><li id="7205" class="po pp iq kq b kr px ku py kx pz lb qa lf qb lj pt pu pv pw bi translated"><em class="lk"> 𝔊-invariant层(全球统筹)</em>𝐴∶𝒳(ω，𝒞) → 𝒴，满足𝐴(𝔤.𝑥) = 𝐴(𝑥)适用于所有𝔤 ∈ 𝔊和𝑥∈𝒳(ω、𝒞).</li></ul><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qc"><img src="../Images/f149651b21135db49a7dda18731aca5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFc_xA6CUAO8kd4ooigXMw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">几何深度学习(GDL)蓝图(图形输入)。图片来自GDL原型书，第3.5节。</p></figure><p id="da30" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">把这些块拼在一起，我们就能创造出一个<em class="lk">丰富的</em> <em class="lk"> 𝔊-invariant函数f:</em>𝒳(ω，𝒞) → 𝒴这种采取形式的</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qd"><img src="../Images/6a8ce3244fa0c2043644ca7e0bc22f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIzZUgk0J7cOSJg7npK1kg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">丰富的<em class="og"> 𝔊-Invariant函数类。</em> GDL原型书，第3.5节。</p></figure><p id="4b8a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中选择每个块，使得其输出尺寸与下一个块的输入尺寸相匹配。另外，每个块可能具有不同的对称群𝔊.</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="aab4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated">我们在这篇文章中已经看到，仅仅对称可能不足以打破维度的诅咒。然后我们提出了第二个几何先验，称为尺度分离。基于多尺度结构的假设，我们讨论了尺度分离如何抵消维数灾难。我们以GDL蓝图作为几何领域的<em class="lk"/><em class="lk">框架</em>来结束，它是由两个几何先验构建的。我们将在随后的文章中详细讨论这个GDL蓝图，以及它如何应用于各种几何领域。我们将在这个框架中看到我们最喜欢的架构，如CNN、transformers和GNNs。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="7267" class="ou nj iq bd nk ov qe ox nn oy qf pa nq kf qg kg nt ki qh kj nw kl qi km nz pe bi translated">参考</h1><ul class=""><li id="b462" class="po pp iq kq b kr oa ku ob kx qj lb qk lf ql lj pt pu pv pw bi translated"><a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> GDL球场</a>，(<a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"> AMMI </a>，2021年夏季)。</li><li id="d534" class="po pp iq kq b kr px ku py kx pz lb qa lf qb lj pt pu pv pw bi translated">j .布鲁纳第四讲[ <a class="ae ll" href="https://youtu.be/ERL17gbbSwo" rel="noopener ugc nofollow" target="_blank">视频</a> | <a class="ae ll" href="https://www.dropbox.com/s/trmwssg0iqqgefs/AIMS%20Lecture%204%20-%20Geometric%20Priors%20II.pdf?dl=0" rel="noopener ugc nofollow" target="_blank">幻灯片</a>。</li><li id="2c1e" class="po pp iq kq b kr px ku py kx pz lb qa lf qb lj pt pu pv pw bi translated">米（meter的缩写））m .布朗斯坦、j .布鲁纳、t .科恩和p .韦利奇科维奇，<a class="ae ll" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank">几何深度学习:网格、组、图形、测地线和量规</a> (2021)。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="9c51" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们感谢汉尼斯·斯特尔克对草案提出的有益意见。</p></div></div>    
</body>
</html>