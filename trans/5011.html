<html>
<head>
<title>What do countries talk about at the UN General Debate? Topic modelings using LDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">各国在联合国一般性辩论上谈些什么？使用LDA的主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-do-countries-talk-about-at-the-un-general-debate-topic-modelings-using-lda-19873cf00fe0#2022-11-08">https://towardsdatascience.com/what-do-countries-talk-about-at-the-un-general-debate-topic-modelings-using-lda-19873cf00fe0#2022-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="04eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">LDA背后的直觉及其局限性，以及使用Gensim的python实现</h2></div><p id="2b18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由<a class="ae le" href="https://www.linkedin.com/in/lan-chu-b319a289/" rel="noopener ugc nofollow" target="_blank">兰楚</a>和<a class="ae le" href="https://www.linkedin.com/in/rsokolewicz/" rel="noopener ugc nofollow" target="_blank">罗伯特·扬·索科列维奇</a>拍摄。在这里找到这篇文章<a class="ae le" href="https://github.com/lanchuhuong/LDA-Topic-modelling" rel="noopener ugc nofollow" target="_blank">的代码</a>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/b47975978588515634718c17962debbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ucyWW0z4zvpEla7cyrEn5w.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@siora18" rel="noopener ugc nofollow" target="_blank"> Siora摄影</a>在<a class="ae le" href="https://unsplash.com/photos/VtNSaArmb-o" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="e543" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1.什么是潜在狄利克雷分配？</h2><p id="a04c" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在分析大量文本时，潜在狄利克雷分配(LDA)是一种流行的模型。它是一个生成概率模型，使用户能够从一组文档中发现隐藏的(“潜在的”)主题。LDA将每个文档建模为通过从统计分布中重复采样单词和主题的过程而生成的。通过应用巧妙的算法，LDA能够恢复在这个生成过程中使用的最可能的分布(Blei，2003)。这些分布告诉我们存在哪些主题，以及它们是如何在每个文档中分布的。</p><p id="bbbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们首先考虑一个简单的例子来说明LDA的一些关键特性。假设我们有以下5个文档的集合</p><ul class=""><li id="4033" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi">🍕🍕🍕🍕🍕🦞🦞🦞🐍🐍🐋🐋🐢🐌🍅</li><li id="4ef4" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi">🐌🐌🐌🐌🐍🐍🐍🐋🐋🐋🦜🦜🐬🐢🐊</li><li id="c6db" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi">🐋🐋🐋🐋🐋🐋🐢🐢🐢🐌🐌🐌🐍🐊🍕</li><li id="1591" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi">🍭🍭🍭🍭🍭🍭🍕🍕🍕🍕🍅🍅🦞🐍🐋</li><li id="6d41" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi">🐋🐋🐋🐋🐋🐋🐋🐌🐌🐌🐌🐌🐍🐍🐢</li></ul><p id="10bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且希望了解存在哪种主题以及它们如何在文档之间分布。快速观察显示，我们有很多与食物和动物相关的表情符号</p><ul class=""><li id="ce79" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated"><strong class="kk iu">食物:</strong> {🍕,🍅,🍭,🦞}</li><li id="dad6" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">动物: {🦞🐍🐋🐬🐌🦜}</li></ul><p id="77da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且这些主题在每个文件中出现的比例不同。文档4主要是关于食物的，文档5主要是关于动物的，前三个文档是这些主题的混合。这就是我们在谈论<em class="nh">主题分布</em>时所指的，主题的比例在每个文档中的分布是不同的。此外，我们看到表情符号🐋和🍭比其他表情符号出现得更频繁。这就是我们在讲每个题目的<em class="nh">单词分布</em>时所指的。</p><p id="9e66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些主题和每个主题中的单词的分布正是LDA返回给我们的。在上面的例子中，我们将每个主题分别标记为<em class="nh">食物</em>和<em class="nh">动物</em>，不幸的是，LDA没有为我们做这件事。它只是返回给我们每个主题的单词分布，作为用户，我们必须从中推断出主题的实际含义。</p><p id="8622" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么LDA是如何实现每个话题的词分布的呢？如前所述，它假设每个文档都是通过从各种分布中抽取主题和单词的随机过程产生的，并使用一种聪明的算法来寻找最有可能产生数据的参数。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="76d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">得到💬任何数据科学或编程问题的GPT式答案。为成千上万的人生成摘要和学习笔记📚只需一次点击即可获得学习资源。👉</p><div class="np nq gp gr nr ns"><a href="https://aigents.co/learn" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">面向数据科学家和开发人员的免费学习资源。精选的博客、教程、书籍和…</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">机器学习和人工智能工程师的培训课程、黑客马拉松、活动和工作</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">aigents.co</p></div></div><div class="ob l"><div class="oc l od oe of ob og lp ns"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="d358" class="oh lw it bd lx oi oj ok ma ol om on md jz oo ka mg kc op kd mj kf oq kg mm or bi translated"><strong class="ak"> 2。LDA背后的直觉</strong></h1><p id="2497" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">LDA是一个概率模型，它利用了<em class="nh">狄利克雷分布和多项分布</em>。在我们继续讨论LDA如何使用这些分布的细节之前，让我们稍微休息一下，回顾一下这些分布的含义。<em class="nh">狄利克雷和多项式分布是贝塔和二项式分布的推广</em>。Beta和二项式分布可以理解为涉及投掷硬币(返回离散值)的随机过程，而Dirichlet和多项式分布处理随机过程，例如投掷骰子。所以，让我们后退一步，考虑稍微简单一点的分布:贝塔分布和二项式分布。</p><p id="0461" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2.1贝塔和二项式分布</strong></p><p id="bbcb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们将用一个抛硬币的例子来说明LDA是如何工作的。想象一下，我们有一个完全用两个词写成的文档:🍕和🍅，并且该文档是通过重复投掷硬币而生成的。每次硬币正面朝上，我们就写🍕，每次硬币落下时，我们写下🍅。如果我们事先知道硬币的偏向是什么，换句话说，它产生的可能性有多大🍕，我们可以使用二项式分布来模拟生成文档的过程。生产的可能性🍕🍕🍕🍕🍕🍕🍕🍅🍅🍅(以任何顺序)例如由P = 120P(🍕)⁷P(🍅)，其中120是排列7个披萨和3个番茄的组合数。</p><p id="fe29" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们如何知道概率P(🍕)和P(🍅)?给定上面的文档，我们可以估计P(🍕)= 7/10和P(🍅)= 3/10，但是我们在分配这些概率时有多大把握呢？将硬币翻转100次，甚至1000次，将会进一步缩小这些概率。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/65b44cfaf97aadd010ea7c7aa922508d.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*OJOagjJJ1UAI9WstLh7ifA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">掷硬币。作者图片</p></figure><p id="7db3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的实验中，每个实验都会给我们相同的概率P(🍕)=7/10 = 0.7.然而，随后的每一次实验都会加强我们的信念，即P(🍕)=7/10.在看到更多证据后，贝塔分布给了我们一种量化这种信念强化的方法。贝塔分布采用两个参数𝛼和𝛽，并产生概率的概率分布。参数𝛼和𝛽可以被视为“伪计数”,代表了我们对硬币有多少先验知识。𝛼和𝛽值越低，分布越广，代表不确定性和缺乏先验知识。另一方面，𝛼和𝛽的较大值产生在某个值(例如，第三个实验中的0.7)附近急剧达到峰值的分布。这意味着我们可以支持我们的说法，P(🍕)=0.7.我们在下图中展示了与上述三个实验相对应的beta分布。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/4cffd7c1889cfb264638b2b7087efe04.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*HVKcSFqP4AuK3muQ"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">具有不同α和β值的β分布。大量的实验意味着较窄的分布。作者图片。</p></figure><p id="608e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在这样想，我们有一个先验的信念，着陆头的概率是0.7。这是我们的假设。你继续另一个实验——把硬币抛100次，得到74个正面和26个反面，先验概率等于0.7是正确的概率是多少？<em class="nh"/>发现你可以描述一个<a class="ae le" href="https://en.wikipedia.org/wiki/Event_(probability_theory)" rel="noopener ugc nofollow" target="_blank">事件</a>的<a class="ae le" href="https://en.wikipedia.org/wiki/Probability" rel="noopener ugc nofollow" target="_blank">概率</a>，基于可能与事件相关的先验知识，通过<a class="ae le" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>，它显示了如何给定一些可能性，以及一个假设和证据，我们可以获得后验概率:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/937bcd2e7cf5acd5a0d849105158ece4.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*ArcanoEspUmKp1o2"/></div></figure><p id="1d15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有4个组件:</p><ul class=""><li id="7e5a" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated"><strong class="kk iu">先验概率</strong>:假设或先验概率。它定义了我们对一个事件的先验信念。这个想法是我们<em class="nh">假设</em>某种先验分布，这是我们已知的最合理的分布。先验概率是每个硬币的P(头),通过使用beta分布生成，在我们的例子中是0.7</li><li id="a84f" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="kk iu">后验概率</strong>:给定证据的先验概率<strong class="kk iu">的概率。这是一个概率的概率。给定100次74头26尾的翻转，先验概率等于0.7是正确的概率是多少？换句话说，有了证据/观测数据，先验信念正确的概率是多少？</strong></li><li id="abc6" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="kk iu">似然性</strong>:似然性可以描述为假设我们的假设为真，观察到数据/证据的概率。比如说，我把一枚硬币抛100次，得到70个正面和30个反面。假设我们之前认为P(头)是0.7，那么在100次翻转中观察到70个头和30个尾的可能性有多大？我们将使用二项式分布来量化可能性。二项式分布使用贝塔分布的先验概率(0.7)和实验次数作为输入，并从二项式分布中抽取正面/反面的数量。</li><li id="1056" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated"><strong class="kk iu">证据的概率</strong>:证据是观察到的数据/实验结果。在不知道假设是什么的情况下，观察到数据的可能性有多大？量化这一项的一种方法是为每个可能的假设计算P(H)*P(E|H)并求和。因为证据项在分母中，所以我们看到证据和后验概率之间的反比关系。换句话说，证据的概率高，后验概率小，反之亦然。证据的高概率反映了替代假设和当前假设一样与数据兼容，所以我们不能更新我们先前的信念。</li></ul><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7812a6703a14c19b4e13e15579df6eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*jfOWYHeGK-cIyS0e"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">先验概率等于0.7的10次试验的二项式分布的概率密度曲线。可以看出，10次翻转中有7次头部落地的概率最高。作者图片。</p></figure><p id="7157" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 2.2文档和主题建模</strong></p><p id="ffed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以这样想:我想使用一些数学框架生成一个新文档。这有什么意义吗？如果是的话，我该怎么做？LDA背后的思想是，每个文档都是由主题混合生成的，每个主题都是单词的分布(Blei，2003)。您可以做的是随机选取一个主题，从该主题中抽取一个单词，然后将该单词放入文档中。你重复这个过程，挑选下一个单词的主题，下一个单词的样本，把它放在文档中…等等。该过程重复进行，直到完成。</p><p id="ad52" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遵循贝叶斯定理，LDA学习如何以下列形式表示主题和文档:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d33dbf4134a861a9bd898b62d0c77cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/0*3ywRBBkdY126o5xS"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Blei等人(2003年)。潜在狄利克雷分配。</p></figure><p id="1032" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有两件事值得一提:</p><ul class=""><li id="8568" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">首先，一个文档由多个主题组成，每个主题z取自一个多项式分布z~Mult(θ) (Blei et al .，2003)</li></ul><p id="b3dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们称Theta (θ)为给定文档的主题分布，即每个主题出现在文档中的概率。为了确定θ的值，我们从狄利克雷分布中抽取一个主题分布样本。如果你还记得我们从beta分布中学到的东西，每枚硬币都有不同的正面着地概率。同样，每个文档都有不同的主题分布，这就是为什么我们要从狄利克雷分布中得出每个文档的主题分布θ。狄利克雷分布使用α/α—适当的知识/假设作为输入参数生成话题分布θ，即θ∞Dir(α)。Dirichlet中的α值是我们关于该文档的主题混合的先验信息。接下来，我们使用由狄利克雷分布生成的θ作为多项式分布z∞Mult(θ)的参数来生成文档中下一个单词的主题。</p><ul class=""><li id="86aa" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">第二，每个主题z由词的混合物组成，其中每个词从多项式分布w~Mult(ϕ中抽取)(Blei等人，2003)</li></ul><p id="0f14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们称之为<strong class="kk iu">φ</strong>(ϕ)每个主题的词分布，即词汇表中每个词出现在给定主题z中的概率。为了确定ϕ的值，我们使用β作为输入参数β——关于主题中词频的先验信息，从狄利克雷分布φz∞dir(β)中采样给定主题的词分布。例如，我可以使用给定主题中每个单词被分配的次数作为β值。接下来，我们使用从Dir(β)生成的<strong class="kk iu"> phi </strong> (ϕ)作为多项式分布的参数来采样文档中的下一个单词——假设我们已经知道下一个单词的主题。</p><p id="ef02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个文档的整个LDA生成过程如下:</p><blockquote class="ow"><p id="3cf3" class="ox oy it bd oz pa pb pc pd pe pf ld dk translated">p(w,z,θ,ϕ | α，β ) = p(θ| α)* p(z| θ) * p(ϕ| β) *p(w| ϕ)</p></blockquote><p id="7372" class="pw-post-body-paragraph ki kj it kk b kl pg ju kn ko ph jx kq kr pi kt ku kv pj kx ky kz pk lb lc ld im bi translated">总的来说，第一步是获得文档的主题混合物— θ，它是由带有参数α的狄利克雷分布生成的。这给了我们第一项。下一个单词的题目<em class="nh"> z </em>是从一个参数为θ的多项式分布中抽取出来的，这个分布给了我们第二项。接下来，从具有参数β的狄利克雷分布中得出每个单词在文档ϕ中出现的概率，这给出了第三项p(ϕ|β).一旦我们知道了下一个词z的主题，我们就使用以ϕ为参数的多项式分布来确定这个词，这就给出了最后一个词。</p><h2 id="914a" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">3.用Gensim实现Python</h2><h2 id="d2f3" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">3.1数据集</h2><p id="d45a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">本文中的LDA python实现使用了一个数据集，该数据集由联合国一般性辩论的文本组成。它包含了从1970年到2020年每个国家的代表在联合国一般性辩论中的所有发言。数据集是开放数据，可在<a class="ae le" href="https://dataverse.harvard.edu/file.xhtml?fileId=4590189&amp;version=6.0" rel="noopener ugc nofollow" target="_blank">在线</a>获得。你可以通过阅读<a class="ae le" href="https://journals.sagepub.com/doi/full/10.1177/2053168017712821" rel="noopener ugc nofollow" target="_blank">这篇论文</a>来获得对其内容的额外洞察。</p><h2 id="1a35" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">3.2数据预处理</h2><p id="ab50" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">作为预处理的一部分，我们将使用以下配置:</p><ul class=""><li id="f86c" class="mt mu it kk b kl km ko kp kr mv kv mw kz mx ld my mz na nb bi translated">小写字母盘</li><li id="81fd" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">令牌化(使用NLTK令牌化将文档分割成令牌)。</li><li id="2457" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">对令牌进行符号化(来自NLTK的WordNetLemmatizer()</li><li id="d122" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">删除停用词</li></ul><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pl pm l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">文本预处理功能。</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pn"><img src="../Images/7bc7ee8b2fa49efb548e54b46a937f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jpoYaGTdy4lRKMVaR39NQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">数据集中某些属性的示例</p></figure><div class="lg lh li lj gt ab cb"><figure class="po lk pp pq pr ps pt paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/c35b5e5e0cb11233c5f0fef87121419f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*qjePmX6QqL0XUMfu"/></div></figure><figure class="po lk pp pq pr ps pt paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><img src="../Images/69820cc6b7d49b2b33e72a2871e90d7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*2c_bITHQl2wtC4ce"/></div><p class="lr ls gj gh gi lt lu bd b be z dk pu di pv pw translated">2020年美国和阿富汗使用TF-IDF演讲中最重要的10个词的例子。单词的大小代表了重要性的大小。</p></figure></div><p id="5091" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.3使用Gensim训练LDA模型</strong></p><p id="68d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Gensim是一个免费的开源Python库，用于处理原始的非结构化文本，并将文档表示为语义向量。Gensim使用各种算法，如Word2Vec、FastText、潜在语义索引LSI、潜在狄利克雷分配LDA等。它将通过检查训练文档语料库中的统计共现模式来发现文档的语义结构。</p><p id="ed84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于训练参数，首先我们来讨论一下房间里的大象:文档里有多少个主题？这个问题没有简单的答案，取决于你的数据和你对数据的了解，以及你实际需要多少题目。我随机使用了10个主题，因为我希望能够解释和标记这些主题。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="pl pm l"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">LDA模型训练参数</p></figure><p id="2361" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nh"> Chunksize </em>参数控制在训练算法中一次处理多少个文档。只要文档块适合内存，增加<em class="nh">块大小</em>将加速训练。<em class="nh">通道</em>控制我们在整个语料库上训练模型的频率，这被称为<em class="nh">时期</em>。<em class="nh"> Alpha </em>和<em class="nh"> eta </em>是在第2.1和2.2节中解释的参数。这里的形状参数<em class="nh">η</em>对应于<em class="nh">β。</em></p><p id="fb69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的LDA模型的结果中，让我们看看主题0，作为一个例子。它说0.015是单词“nation”在文档中产生/出现的概率。这意味着如果你无限次抽取样本，那么单词“nation”将被抽取0.015%的时间。</p><pre class="lg lh li lj gt px py pz qa aw qb bi"><span id="66c2" class="lv lw it py b gy qc qd l qe qf">topic #0 (0.022): 0.015*"nation" + 0.013*"united" + 0.010*"country" + 0.010*"ha" + 0.008*"people" + 0.008*"international" + 0.007*"world" + 0.007*"peace" + 0.007*"development" + 0.006*"problem" </span><span id="7d2a" class="lv lw it py b gy qg qd l qe qf">topic #1 (0.151): 0.014*"country" + 0.014*"nation" + 0.012*"ha" + 0.011*"united" + 0.010*"people" + 0.010*"world" + 0.009*"africa" + 0.008*"international" + 0.008*"organization" + 0.007*"peace" </span><span id="0451" class="lv lw it py b gy qg qd l qe qf">topic #2 (0.028): 0.012*"security" + 0.011*"nation" + 0.009*"united" + 0.008*"country" + 0.008*"world" + 0.008*"international" + 0.007*"government" + 0.006*"state" + 0.005*"year" + 0.005*"assembly" </span><span id="ee7e" class="lv lw it py b gy qg qd l qe qf">topic #3 (0.010): 0.012*"austria" + 0.009*"united" + 0.008*"nation" + 0.008*"italy" + 0.007*"year" + 0.006*"international" + 0.006*"ha" + 0.005*"austrian" + 0.005*"two" + 0.005*"solution" </span><span id="04e1" class="lv lw it py b gy qg qd l qe qf">topic #4 (0.006): 0.000*"united" + 0.000*"nation" + 0.000*"ha" + 0.000*"international" + 0.000*"people" + 0.000*"country" + 0.000*"world" + 0.000*"state" + 0.000*"peace" + 0.000*"organization" </span><span id="c2a6" class="lv lw it py b gy qg qd l qe qf">topic #5 (0.037): 0.037*"people" + 0.015*"state" + 0.012*"united" + 0.010*"imperialist" + 0.010*"struggle" + 0.009*"aggression" + 0.009*"ha" + 0.008*"american" + 0.008*"imperialism" + 0.008*"country" </span><span id="881c" class="lv lw it py b gy qg qd l qe qf">topic #6 (0.336): 0.017*"nation" + 0.016*"united" + 0.012*"ha" + 0.010*"international" + 0.009*"state" + 0.009*"world" + 0.008*"country" + 0.006*"organization" + 0.006*"peace" + 0.006*"development" </span><span id="1760" class="lv lw it py b gy qg qd l qe qf">topic #7 (0.010): 0.020*"israel" + 0.012*"security" + 0.012*"resolution" + 0.012*"state" + 0.011*"united" + 0.010*"territory" + 0.010*"peace" + 0.010*"council" + 0.007*"arab" + 0.007*"egypt" </span><span id="7b5d" class="lv lw it py b gy qg qd l qe qf">topic #8 (0.048): 0.016*"united" + 0.014*"state" + 0.011*"people" + 0.011*"nation" + 0.011*"country" + 0.009*"peace" + 0.008*"ha" + 0.008*"international" + 0.007*"republic" + 0.007*"arab" </span><span id="01be" class="lv lw it py b gy qg qd l qe qf">topic #9 (0.006): 0.000*"united" + 0.000*"nation" + 0.000*"country" + 0.000*"people" + 0.000*"ha" + 0.000*"international" + 0.000*"state" + 0.000*"peace" + 0.000*"problem" + 0.000*"organization"</span></pre><p id="4d70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> 3.4题目质量</strong></p><p id="40d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">生成的主题通常用于讲述一个故事。高质量的主题是那些容易被人类理解的。评估主题质量的一个流行指标是连贯性，连贯性值越大，通常对应的主题越容易理解。这意味着我们可以使用主题一致性分数来确定主题的最佳数量。两个流行的一致性度量是UMass和word2vec一致性分数。UMass计算一个主题中的两个单词在同一文档中出现的频率，相对于它们单独出现的频率。具有诸如<em class="nh"> United、Nations和States </em>的主题将具有较低的一致性分数，因为即使United经常与Nations和州在同一文档中共同出现，<em class="nh"> Nations </em>和<em class="nh"> States </em>则不会。另一方面，基于word2vec的一致性分数采用不同的方法。对于主题中的每一对单词，word2vec都会对每个单词进行矢量化，并计算余弦相似度。余弦相似性得分告诉我们两个单词是否在语义上彼此相似。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/fbadb63aacaa95e389585393accc8412.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*ENQdF_w1qd73gIhWsNHgEg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">对应于LDA模型的相干值具有相应的主题数量。看起来选择5个主题可能会产生最好的一致性分数/质量，即使在这种情况下不同数量的主题的分数差异是微不足道的。作者图片。</p></figure><h2 id="3fe0" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">4.限制</h2><ul class=""><li id="a896" class="mt mu it kk b kl mo ko mp kr qi kv qj kz qk ld my mz na nb bi translated">顺序无关紧要:LDA将文档作为“单词包”来处理。单词包意味着您查看文档中单词的频率，而不考虑单词出现的顺序。显然，在这个过程中会丢失一些信息，但是我们的主题建模的目标是能够从大量的文档中查看“大画面”。另一种思考方式是:我有10万个单词的词汇表，在100万个文档中使用，然后我使用LDA查看500个主题。</li><li id="d52f" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">主题的数量是一个需要由用户设置的超参数。在实践中，用户使用不同数量的主题运行几次LDA，并比较每个模型的一致性分数。更高的一致性分数通常意味着主题更容易被人类理解。</li><li id="820b" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">LDA并不总是能很好地处理诸如推文和评论之类的小文档。</li><li id="f376" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">LDA生成的主题并不总是可解释的。事实上，研究表明，具有最低困惑或对数似然的模型通常具有较少的可解释的潜在空间(Chang等人，2009)。</li><li id="ab0d" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">常用词往往主导每个话题。实际上，这意味着从每个文档中删除停用词。这些停用词可能特定于每个文档集合，因此需要手工构建。</li><li id="b12b" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">为先验分布选择正确的结构。实际上，像Gensim这样的包默认选择对称的Dirichlet。这是一种常见的选择，在大多数情况下不会导致更差的主题提取(Wallach等人2009年，Syed等人2018年)。</li><li id="c8cb" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld my mz na nb bi translated">LDA不考虑主题之间的相关性。例如,“烹饪”和“饮食”更有可能在同一文档中共存，而“烹饪”和“合法”则不会(Blei等人，2005年)。在LDA中，由于选择使用狄利克雷分布作为先验，主题被假设为彼此独立。克服这个问题的一种方法是使用相关主题模型而不是LDA，LDA使用逻辑正态分布而不是狄利克雷分布来绘制主题分布。</li></ul><p id="842c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nh">感谢您的阅读。如果你觉得我的帖子有用，并且正在考虑成为中级会员，你可以考虑通过这个</em> <a class="ae le" href="https://medium.com/@huonglanchu0712/membership" rel="noopener"> <em class="nh">推荐会员链接</em> </a> <em class="nh">:)来支持我，我将收取你的一部分会员费，不需要你额外付费。如果你决定这样做，非常感谢！</em></p><h2 id="1837" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="dbdb" class="mt mu it kk b kl mo ko mp kr qi kv qj kz qk ld ql mz na nb bi translated">布雷，大卫m，安德鲁Y. Ng，迈克尔乔丹。"潜在的狄利克雷分配."<em class="nh">机器学习研究杂志</em> 3。一月(2003):993–1022。</li><li id="d3bd" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">艾达的小册子。LDA和Gibs采样概述。克里斯塔夫斯。</li><li id="913f" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">Chang，Jonathan，Sean Gerrish，，Jordan Boyd-Graber和David Blei。"阅读茶叶:人类如何解读主题模型."<em class="nh">神经信息处理系统进展</em> 22 (2009)。</li><li id="e516" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">拉弗蒂、约翰和大卫·布雷。"相关主题模型."神经信息处理系统进展18 (2005)。</li><li id="1388" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">沃勒克、汉娜、大卫·米米诺和安德鲁·麦卡勒姆。"反思LDA:为什么先验很重要."神经信息处理系统进展22 (2009)。</li><li id="8535" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">托马斯·霍夫曼。"概率潜在语义分析."arXiv预印本arXiv:1301.6705 (2013)。</li><li id="86c0" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">赛义德、沙欣和马可·斯普雷特。"为潜在的狄利克雷分配选择先验."2018年IEEE第12届语义计算国际会议(ICSC)，第194–202页。IEEE，2018。</li><li id="eda2" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated"><a class="ae le" href="https://dataverse.harvard.edu/file.xhtml?fileId=4590189&amp;version=6.0" rel="noopener ugc nofollow" target="_blank">https://dataverse.harvard.edu/file.xhtml?fileId=4590189&amp;版本=6.0 </a></li><li id="d552" class="mt mu it kk b kl nc ko nd kr ne kv nf kz ng ld ql mz na nb bi translated">贝塔分布</li></ol><div class="np nq gp gr nr ns"><a href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">测试版-维基百科</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">在概率论和统计学中，贝塔分布是一族连续的概率分布，定义为…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">en.wikipedia.org</p></div></div><div class="ob l"><div class="qm l od oe of ob og lp ns"/></div></div></a></div></div></div>    
</body>
</html>