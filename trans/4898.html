<html>
<head>
<title>How DeepMind discovered new ways of multiplying matrices using AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind如何利用人工智能发现矩阵乘法的新方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-deepmind-discovered-new-ways-of-multiplying-matrices-using-ai-a04557e9f861#2022-11-01">https://towardsdatascience.com/how-deepmind-discovered-new-ways-of-multiplying-matrices-using-ai-a04557e9f861#2022-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/43d5cddba12b03188090088b74bacace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FSeEOyJ_s5dny0Wq"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由<a class="ae kf" href="https://unsplash.com/@gmalhotra?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Gayatri Malhotra </a>在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ee91" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上个月，DeepMind <a class="ae kf" href="https://www.nature.com/articles/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank">发表了一篇论文</a>，其中他们展示了AlphaTensor，这是一种人工智能算法，能够找到更快的方法来完成最常见的代数运算之一:矩阵乘法。当然，你可能会问自己一个合理的问题:这和我有什么关系？在这篇文章中，我将解释论文的内容，如果你坚持到最后，你会找到一些例子，说明为什么这是一个如此大的突破，以及它如何<strong class="ki iu"> <em class="le">将</em> </strong>影响你未来的日常生活。</p><h2 id="bf23" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">为什么矩阵乘法很重要？</h2><p id="6b7f" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">也许你还记得我在<a class="ae kf" rel="noopener" target="_blank" href="/accelerating-neural-networks-on-hardware-baa3c14cd5ba">的上一篇文章</a>中提到，矩阵乘法是最常见的人工智能算法之一:神经网络的训练和部署的重要组成部分。请记住，这不是这个代数运算的唯一用途。它有数以百计的应用(机器人的坐标变换，复杂优化问题的解决)，但为了这篇文章，我们将只关注人工智能。</p><p id="cc1a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这实际上是一个非常简单的操作:每个矩阵都是一个二维数字数组，在乘法过程中，左边矩阵(维数为[M，N])的每个行元素都与右边矩阵(维数为[N，P])的每个列元素相乘。所得矩阵的维数为[M，P]。然后将每个乘法结果累加在一起。让我们记住一个[2，2]*[2，2]矩阵乘法的小例子:</p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">记住矩阵乘法的工作原理。作者制作的动画。</p></figure><p id="8004" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过以这种简单的方式计算结果，很容易确定需要多少次乘法来计算输出矩阵的所有元素。给定一个[M，N]*[N，P]矩阵乘法，我们需要做M*N*P次乘法，以便计算输出矩阵。所以一个[2，2]*[2，2]矩阵乘法需要2*2*2=8个单独的乘法。</p><p id="01c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么我们关心我们需要做多少次乘法？好吧，让我们来看看下图:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mj"><img src="../Images/4094ae8767a519acef6740b1a4560c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOSTYoB3CkJuUjD78gbh4Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片由作者提供。原始数据来自<a class="ae kf" href="https://www.rle.mit.edu/eems/wp-content/uploads/2020/09/2020_sscs_dnn.pdf" rel="noopener ugc nofollow" target="_blank">《如何评估深度神经网络处理器》，2020 </a>。</p></figure><p id="4f27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看那个！与相同的位表示加法相比，与某个位表示(8位、32位浮点等)的乘法使用更多的能量！当然，我们在这里谈论的是皮焦(0.0000000000001焦耳)，这是非常小的…但乘以一些最先进的神经网络所需的数十亿次运算，然后<em class="le">然后</em>想象这些是24/7运行的，你开始得到不可忽略的能耗。</p><p id="7d9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，重要的问题是:我们能否用更少的乘法运算来执行相同的代数运算，以节省能量？</p><h2 id="ee27" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">什么是张量分解？</h2><p id="c450" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">嗯，令人惊讶的是，答案竟然是肯定的！方法是使用一种叫做<em class="le">张量分解</em>的技术，这是DeepMind在论文中用来表示矩阵乘法运算的技术。</p><p id="e3d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，主要思想是以这样一种方式对矩阵的元素进行分组，即我们需要更少的乘法来实现相同的结果，并且加法的数量略有增加(但是因为这些使用更少的能量，所以没什么大不了的)。听起来违反直觉，对吗？如何用较少的乘法运算得到<em class="le">相同的</em>数学<em class="le"> </em>结果？</p><p id="047f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，事实上很简单:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/43def6e124c81706ff15112a3ed151b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*MVSFVCu06ixpq2n5NKP7gQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">两个M=N=P=2的矩阵相乘的Strassen算法。图片由作者提供。</p></figure><p id="0d84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，我们通过使用较少的乘法来计算结果！我们首先组合输入矩阵的元素，并将它们相乘以生成中间结果。最后，在它们之间进行组合，以获得输出矩阵的实际元素(可以使用简单的代数来证明，这种中间结果组合等价于上一个动画中显示的形式)。这就是Strassen算法，它能够使用7次<strong class="ki iu">单独乘法</strong>来计算[2，2]*[2，2]矩阵乘法的结果(我们在上一节中看到的2个矩阵相乘的简单方法需要8次<strong class="ki iu">单独乘法</strong>)！</p><p id="8cc2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种矩阵乘法运算的分解可以用3个矩阵来表示:U、V和W。如下面的动画所示，U和V用于组合输入矩阵的元素，W用于组合中间结果。这被称为秩7分解，因为U、V和W的秩<em class="le">等于7。</em></p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="ml mi l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">斯特拉森算法动画，但显示矩阵U，V和w。</p></figure><h2 id="ea69" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">发现算法如何工作</h2><p id="f30c" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">所以让我们来看看实际的论文提出了什么，好吗？</p><p id="f042" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，DeepMind基于其之前非常成功的AlphaZero训练了AlphaTensor，该算法可以为不同的矩阵乘法大小生成这种重组算法。从概念上讲，他们将问题设计为一个<a class="ae kf" href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/" rel="noopener ugc nofollow" target="_blank">强化学习</a>问题，其中一个代理试图通过试图最小化结果矩阵的<em class="le">秩</em>(列)来找到矩阵U、V和W。因为秩与乘法次数直接相关，这有效地最小化了所需的元素乘法。这篇论文的技巧和绝对的<strong class="ki iu"> <em class="le">精彩之处</em> </strong>在于他们能够将问题建模为一个游戏，因此一个强化学习算法能够尝试做最少的乘法来得到结果。</p><p id="e436" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在一组不同大小的矩阵乘法上，AlphaTensor能够找到使用与当前已知解决方案相等甚至更少乘法的算法！他们对Strassen算法特别感兴趣，该算法使用49次乘法将2个4x4矩阵相乘，这是自1969年发现以来该矩阵大小的最佳解决方案。AlphaTensor找到了一个更好的算法，只使用了47次乘法！</p><p id="80c7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，他们在Nvidia V100 GPU和TPU上比较了使用Strassen算法和AlphaTensor算法的大矩阵乘法(M=N=P=8192和更大)的加速。他们表明，与Strassen算法相比，alpha张量发现算法能够实现1.5%到4 %的加速。</p><h2 id="8567" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">那么…这跟我有什么关系？</h2><p id="0a2b" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">如果你认为这是一些与你的生活无关的深奥的代数，那么…你错了！人工智能算法训练和预测(例如，神经网络)所需的计算都可以(通常)用矩阵乘法来表示。那么，如果我们现在使用更少的乘法来获得相同的数学结果，会发生什么呢？</p><ul class=""><li id="c6eb" class="mm mn it ki b kj kk kn ko kr mo kv mp kz mq ld mr ms mt mu bi translated">训练将更快，消耗更少的能量，这将允许我们使用相同的硬件来训练更复杂的网络。</li><li id="fd0f" class="mm mn it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated">神经网络将适合更小的设备，为这些算法开辟了一个全新的应用领域。</li><li id="3226" class="mm mn it ki b kj mv kn mw kr mx kv my kz mz ld mr ms mt mu bi translated">如果硬件也被设计成考虑这些计算矩阵乘法的新方法，则应该实现显著的能量节省。</li></ul><h2 id="1765" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">动手！</h2><p id="9570" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">你知道这篇论文最精彩的部分是什么吗？你可以在网上免费获得它！如果您对自己分析数据感兴趣，可以看看AlphaTensor 的<a class="ae kf" href="https://github.com/deepmind/alphatensor" rel="noopener ugc nofollow" target="_blank"> GitHub资源库。</a></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="64fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">欢迎在<a class="ae kf" href="https://twitter.com/PecciaF" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae kf" href="https://www.linkedin.com/in/fpecc/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，让我知道你对这篇文章的看法，或者<a class="ae kf" href="https://www.buymeacoffee.com/pecciaf" rel="noopener ugc nofollow" target="_blank">请我喝杯咖啡</a>，如果你真的喜欢它！</p><p id="b97e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h2 id="9070" class="lf lg it bd lh li lj dn lk ll lm dp ln kr lo lp lq kv lr ls lt kz lu lv lw lx bi translated">参考</h2><p id="c575" class="pw-post-body-paragraph kg kh it ki b kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">原始纸张:</p><ul class=""><li id="88c8" class="mm mn it ki b kj kk kn ko kr mo kv mp kz mq ld mr ms mt mu bi translated">Fawzi，a .，Balog，m .，Huang，A. <em class="le">等</em> <a class="ae kf" href="https://doi.org/10.1038/s41586-022-05172-4" rel="noopener ugc nofollow" target="_blank">用强化学习发现更快的矩阵乘法算法</a>。<em class="le">性质</em> <strong class="ki iu"> 610 </strong>，47–53(2022)。</li></ul><p id="676a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习的有用指南:</p><ul class=""><li id="412c" class="mm mn it ki b kj kk kn ko kr mo kv mp kz mq ld mr ms mt mu bi translated"><a class="ae kf" href="https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/" rel="noopener ugc nofollow" target="_blank">什么是强化学习？完整指南— deepsense.ai </a></li></ul><p id="b6d1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原始AlphaZero文件:</p><ul class=""><li id="4eb8" class="mm mn it ki b kj kk kn ko kr mo kv mp kz mq ld mr ms mt mu bi translated">Silver，d .等<a class="ae kf" href="https://www.science.org/doi/10.1126/science.aar6404" rel="noopener ugc nofollow" target="_blank">一种通用的强化学习算法，精通国际象棋、shogi，并通过自玩</a>。科学362，1140–1144(2018)</li></ul></div></div>    
</body>
</html>