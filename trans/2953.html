<html>
<head>
<title>Perplexity of language models revisited</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语言模型的困惑再探</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792#2022-06-28">https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792#2022-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4831" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从基础信息论到实用计算</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4fba08a38db2cf69b23d6883ee3d3afd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSoP9RE3xGhr7Uj2nZCfLg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://medium.com/@pirminlemberger/perplexity-of-language-models-revisited-6b9b4cf46792" rel="noopener">作者</a></p></figure><h2 id="d151" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">又一个关于语言模型困惑的帖子？</h2><p id="2f28" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir">语言模型</strong> (LM)目前处于NLP研究的前沿。基于Transformer架构[1]的预训练模型，如GPT-3 [2]、伯特[3]及其众多变体XLNET[4]、罗伯塔[5]…通常用作解决各种下游任务的基础，从机器翻译到文档摘要或开放域问题回答。他们的零射击能力似乎很有前途，该领域最大胆的人将它们视为更普遍的认知技能的第一瞥，而不是迄今为止监督学习的狭义概括能力[6]。</p><p id="985e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">从一个更平凡的角度来看，LM是一个简单的概率分布模型，<em class="mq"> p </em> ( <em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)在一系列标记(<em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)上，这些标记组成了给定语言中的可理解文本，希望是你正在阅读的语言。</p><p id="3dae" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在过去的几年中，NLP社区已经设计了一些度量和基准来评估这种LM的质量。一种选择是测量<strong class="lu ir">下游</strong>任务的性能，比如分类精度，任务范围内的性能，这就是GLUE基准测试所做的[7]。对于语言生成的更微妙和难以量化的方面，如生成文本的连贯性或可接受性，人们也可以求助于主观人类评价[8]。另一方面，我们发现<strong class="lu ir">内在的</strong>，独立于用例，像交叉熵(CE)、每字符位数(BPC)或基于信息理论概念的困惑(PP)这样的度量。在这篇短文中，我们将关注<strong class="lu ir">困惑</strong>。</p><p id="3983" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">简而言之，当一个语言模型生成一个新的标记时，它的困惑度量了一个LM的不确定性程度，这个不确定性是在很长的序列上平均的。因此，PP越低，LM越好。显然，PP将依赖于模型所使用的特定的<strong class="lu ir">标记化</strong>，因此比较两个LM只有在两个模型使用相同的标记化的情况下才有意义。</p><p id="0aec" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">有很多论文、博客文章和评论试图解释这一指标的直觉和信息理论来源。但是我敢说，除了少数例外[9，10]，我发现这种过多的资源相当令人困惑，至少对于像我这样的数学头脑来说是如此。因此，本<strong class="lu ir">教学笔记</strong>的目标是建立困惑的定义，并以简化的方式解释它，从基本信息和理论概念开始，摒弃任何类型的术语。只是很好的旧数学。关于证明，例如参见[11]。</p><p id="4e05" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">免责声明</strong>:这个笔记不会帮助你成为一个Kaggle专家。</p><h2 id="f6f2" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">单个随机变量的熵和困惑</h2><p id="b698" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">先说简单的事情。让我们回顾一下如何在有限的𝒳.集合中测量单个随机变量(r.v.) <em class="mq"> X </em>的随机性您可能会将<em class="mq"> X </em>视为文本信息的来源，将值<em class="mq"> x </em>视为由此来源生成的标记或单词，将𝒳视为某种标记化过程产生的词汇表。在继续之前，让我们修正一些不言自明的符号:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/75db9a30b03adc05442bc6312c07b7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyADif4rxzJehXh7hCYn_A.png"/></div></div></figure><p id="b7d9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">源<em class="mq"> X </em>的<strong class="lu ir">熵</strong>定义为(对数的底数为2，因此<em class="mq"> H </em> [ <em class="mq"> X </em> ]以位为单位测量):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/d8f66e2107569d0f0acebafe53c0ac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4uYOBPNqtbtYcgcfQgLjXg.png"/></div></div></figure><p id="2742" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">正如经典的<strong class="lu ir">信息论</strong>【11】告诉我们的那样，这既是对r.v. <em class="mq"> X </em>随机性程度的一个很好的度量，也是对信源<em class="mq"> X </em>产生信息的速率的一个度量。事实上，如果<em class="mq">l</em>(<em class="mq">x</em>):= |<em class="mq">c</em>(<em class="mq">x</em>)|表示前缀码<em class="mq"> C </em>(粗略地说，这是一种可以被动态解码的码)的编码<em class="mq"> C </em> ( <em class="mq"> x </em>)在𝒳中的长度比<strong class="lu ir"> <em class="mq">香农的长</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/7c5d10728f798b8d54362ee61b4d9bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbAfPrkESuyeNsjsIHZktQ.png"/></div></div></figure><p id="e3ad" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">此外，对于最佳代码<em class="mq"> C </em> *，长度验证达到一位[11]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/df2433dec8178020f62267b092a5412c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbRv8Sc7Buzhs-RvxvG0EA.png"/></div></div></figure><p id="dc4e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这证实了我们的直觉，即频繁出现的令牌应该分配较短的代码。熵是一个深刻而多面的概念，因此我们不会在这篇短文中穷尽它的全部含义，但是这些事实仍然应该说服最怀疑定义(1)的相关性的读者。当<em class="mq"> X </em>为常数时，熵<em class="mq"> H </em> [ <em class="mq"> X </em>为零，当<em class="mq"> X </em>在𝒳:上均匀分布时，熵取最大值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/c21294c29e448fa0ebfe2c3449ccfd8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z_tTFWeoKdPkHnglQf5NOg.png"/></div></div></figure><p id="46d1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">因此,( 2)中的上限促使将单个随机变量的<strong class="lu ir">困惑度</strong>定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/c73d5c4f50804a25beeee7d95020ab25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL1e4BVaW8Wo-5GoNhvemg.png"/></div></div></figure><p id="059a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">因为对于一辆统一的房车来说，它只是减少了|𝒳|可供选择的箱子数量。对于一个非均匀的r.v. <em class="mq"> X </em>我们可以把PP[ <em class="mq"> X </em> ]解释为我们面对的一个有效的不确定性，我们应该猜测它的值<em class="mq"> x </em>。我们还需要两个r.v. <em class="mq"> X </em>和<em class="mq"> Y </em>的<strong class="lu ir">关节</strong>和<strong class="lu ir">条件</strong> <strong class="lu ir">熵</strong>的定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/b0c98c06a4ec9d16a3653048591462a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7PCU88AyBxKU2aTJswUDA.png"/></div></div></figure><p id="716d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">上面的第一个定义很容易暗示熵是两个独立随机变量的可加量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/34ac7e707b6b402e8ebc61aa88e082af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ks1a8G6CQ6fOw-UtyLosKQ.png"/></div></div></figure><p id="508f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">第二个将条件熵定义为条件分布的熵，在条件<em class="mq"> y </em>上平均。让我们假设对于一个源，我们有一个<strong class="lu ir">未知分布</strong> <em class="mq"> P </em>和一个<strong class="lu ir">模型</strong> <em class="mq"> Q </em>来近似它。然后我们将源<em class="mq"> P </em>相对于模型<em class="mq"> Q </em>的<strong class="lu ir">交叉熵</strong> CE[ <em class="mq"> P </em>，<em class="mq"> Q </em> ]定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/54ff21fcf81b35a27c0563a2fc0ae72d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoqYxUqjTPkfI6wtKtK4Nw.png"/></div></div></figure><p id="7817" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">KL是众所周知的<strong class="lu ir"> Kullback-Leibler散度</strong>，它是概率分布之间的近似性的几种可能定义之一。CE[P，Q]和KL[<em class="mq">P</em>∨<em class="mq">Q</em>]在码长方面都有很好的解释。CE是当令牌<em class="mq"> x </em>由源<em class="mq"> P </em>产生，但是它们的编码被选择为对于<em class="mq"> Q </em>最优时，编码长度<em class="mq"> l </em> ( <em class="mq"> x </em>的期望值。情商。(8)由此可见，KL[<em class="mq">P</em>∩<em class="mq">Q</em>]就是这么说的我们在使用错误编码时必须付出的代价。</p><h2 id="587d" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">平稳遍历过程的熵和困惑</h2><p id="59cc" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">一个<strong class="lu ir">随机过程</strong> (SP)是一个r.v .的索引集。为了我们的目的，这个索引将是一个整数，你可以把它解释为一个记号在一个随机记号序列中的位置:(<em class="mq"> X </em> ₁，<em class="mq"> X </em> ₂，…)。最简单的SP是一组从同一分布<em class="mq"> P </em>中提取的<strong class="lu ir"> i.i.d. </strong> r.v。假设我们有一个样本<em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，……从这样一个SP中抽取，我们可以将它的<strong class="lu ir">经验熵</strong>定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/bc51cd86ff58360622c4ca55bf582bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bcik-GUKjxirAMySvNmYw.png"/></div></div></figure><p id="0b1a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">弱的<strong class="lu ir">大数定律</strong>随即意味着相应的估计量趋向于<em class="mq"> P </em>的熵<em class="mq"> H </em> [ <em class="mq"> X </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/aca5b240b39a09d3434f97f212571111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IY0-WCIYzgk1YdrnUoFVNw.png"/></div></div></figure><p id="9c6f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">也许用更直观的术语来说，这意味着对于足够大的样本，我们有一个近似值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/65e7051edea9bb84e57c2b6f3d710ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5f9C0jJ_xf9HKYeB9qlpg.png"/></div></div></figure><p id="9b89" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">或者等同地</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/9848431d0a337012527058545cd93e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5N0YpelBjrSph81hWRqKsg.png"/></div></div></figure><p id="76f4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">从这个基本观察开始，通过定义所谓的典型<strong class="lu ir">序列</strong>的集合为那些经验熵不太远离真实熵的序列，信息论的基本结果可以被证明[11](其中SNCT在上面)，但是我们在这里不会被这些问题困扰。</p><p id="8e8e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在NLP中，我们感兴趣的是<strong class="lu ir">的随机来源。r.v. ( <em class="mq"> X </em> ₁，<em class="mq">x</em>₂……)的序列，因为有意义的文本中出现的单词肯定不是独立的。我们将把𝜒称为这样一个物种。那么，对于一个长句子来说，概率<em class="mq"> p </em> ( <em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)的近似值(6)是什么呢？实际上，我们必须在这里对SP 𝜒:=( <em class="mq"> X </em> ₁，<em class="mq">x</em>₂……)做一个简化的假设，假设它是<strong class="lu ir">静止的</strong>，我们的意思是</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/f2f6bece6595e7bd8d2f5d3ce90af118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAAnZj4mAYRu4sLfxwiW7w.png"/></div></div></figure><p id="f815" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">对于令牌的所有序列(<em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)以及所有<strong class="lu ir">时移</strong> <em class="mq"> t </em>。严格地说，对于文本文档来说，这当然不是正确的，因为单词在文本的开头和结尾的分布是不同的。但这是我们前进时必须做出的一个近似值。对于这样的平稳随机过程，我们可以考虑以至少两种方式定义<strong class="lu ir">熵率</strong>(即每个令牌的熵)。这里有一个将熵率定义为非常长的序列的每个令牌的平均熵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/25c85286bad157b16ff86d47b564f106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*inlzFpR2FNtyBW_nLvw_Zg.png"/></div></div></figure><p id="9b6e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这是另一个定义，它是基于前一个记号的最后一个记号的<strong class="lu ir">平均熵，同样对于非常长的序列:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/6985c9683b3a57ce0e7511d01ccaaba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bZi-scaljLrM16mCD40LzQ.png"/></div></div></figure><p id="d1d9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">将我们的注意力限制在定态粒子上的全部意义在于，可以证明[11]这两个极限是一致的，从而为我们提供了定态粒子𝜒.的熵率<em class="mq">h</em>[𝜒]的一个很好的定义</p><p id="f128" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后，我们可以类似于(3)将静态SP的困惑度定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/0c5401e1c73dc20cfd19b1c0b0ff9726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmH27f7JDVwdV-M8NaNWig.png"/></div></div></figure><p id="5661" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这种解释很简单，也是我们从一开始就试图抓住的。它是稳定的SP 𝜒.的每个令牌的<strong class="lu ir">不确定性有没有一种近似法可以把方程(7)推广到稳态速度？可惜，一般的<em class="mq">没有</em>！我们必须对SP 𝜒.做一个额外的技术假设也就是说，我们必须假设SP 𝜒是<strong class="lu ir">遍历的</strong>。对遍历性的详细解释会把我们引入歧途，但感兴趣的读者可以参见[11]中的第16章。非常粗略地说，遍历性条件确保了任何单个r.v. <em class="mq"> X </em> ₁在过程𝜒的分布<em class="mq"> P </em>上的期望𝔼[ <em class="mq"> X </em> ₁可以用从𝜒 ( <strong class="lu ir"> <em class="mq">比尔科夫遍历定理</em> </strong>)中抽取的单个很长序列(<em class="mq"> x </em> ₁，<em class="mq">x</em>₂……)的时间平均值来代替:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/331d289f208c82a0a8ff775d1c920b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lyAzXE_-01R4oCR3sYmuw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对于遍历过程，斯朗大数定律成立。</p></figure><p id="4996" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">因此，如果我们假设我们的源𝜒确实是稳定的和遍历的(这在实践中对于文本可能仅是近似正确的)，那么(7)的以下概括成立(<strong class="lu ir"> <em class="mq">香农，麦克米兰，布雷曼定理</em> </strong> (SMB) [11]):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/a99bced12088e43a523a19de1c832a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsXIX7Tr9sjM91SgB5ubYg.png"/></div></div></figure><p id="53ad" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">因此我们看到，要计算一个遍历过程𝜒的熵率<em class="mq">h</em>【𝜒】(或困惑PP[𝜒】)，我们只需要<strong class="lu ir">画一个很长的序列</strong>，计算它的负对数概率，我们就完成了！不需要执行大量的求和。所以让我们欢呼吧！(11)背后的直觉是，在某种程度上，一个无限长的序列实际上包含了它们全部。</p><h2 id="2b47" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">界定未知过程的困惑</h2><p id="7ac2" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">如果我们知道相应的概率分布<em class="mq"> p </em> ( <em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)，所有这些对于计算像英语这样的语言的熵(或困惑)将是完美的。但不幸的是我们没有，因此我们必须求助于语言模型<em class="mq"> q </em> ( <em class="mq"> x </em> ₁，<em class="mq">x</em>₂……)作为近似。幸运的是，我们将能够为<em class="mq"> p </em>构建熵率的上限。这个上限将会是模型<em class="mq"> Q </em>(语言模型)相对于源<em class="mq"> P </em>(实际语言)的<strong class="lu ir">交叉熵</strong>。它的定义与SP的熵率(8，9)和两个普通分布的交叉熵(4)直接相似:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/628982708cfc86baf3df81eabad17f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ul_EpZSqiZkyhPulWUUGoA.png"/></div></div></figure><p id="6d41" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">因此，当面对由源<em class="mq"> P </em>产生的令牌时，它是模型<em class="mq"> Q </em>的每个令牌的不确定性。第二个等式是一个类似于为熵率建立等式(8)和(9)的定理。语言的未知熵的承诺界限是简单的[9]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/4dd3b8d3647e82762b7ce89e12dd7a18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oKcb8DAW3QuR1_btKMvFA.png"/></div></div></figure><p id="17a5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后，对于被视为未知源SP <em class="mq"> P </em>的语言，模型 <em class="mq"> Q </em>的<strong class="lu ir">困惑度被定义为:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/8c677184515fd144a1271af67258b75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTtflVP7ncCdLNeGG77zvw.png"/></div></div></figure><p id="5e44" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">换句话说:当由语言<em class="mq"> P </em>生成时，模型<em class="mq"> Q </em>不确定接下来会出现哪个标记，就好像它必须在PP[ <em class="mq"> P </em>，<em class="mq"> Q </em>选项中进行猜测一样。为了计算PP[ <em class="mq"> P </em>，<em class="mq"> Q </em> ]或CE[ <em class="mq"> P </em>，<em class="mq"> Q </em>，我们可以使用SMB定理[9]的扩展:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/c2aae6d76c37f44b57a4969ab06db559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x9yWt2R0GCsh9xm4FjU46g.png"/></div></div></figure><h2 id="759c" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">最后:语言模型的困惑</h2><p id="76d8" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">为了具体起见，假设我们有一个语言模型，它的概率<em class="mq"> q </em> ( <em class="mq"> x </em> ₁，<em class="mq"> x </em> ₂，…)由一个类似LSTM的RNN定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/367beb574735ac459439c979b6b1c276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2VnABvR56ab8qK9N-vLkOw.png"/></div></div></figure><p id="c3a5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">SMB结果(13)然后告诉我们，我们可以通过采样任何足够长的令牌序列并计算其对数概率来估计CE[ <em class="mq"> P </em>，<em class="mq"> Q </em>。将RNN分布(14)的显式表达式代入(13)以获得(12)中CE[ <em class="mq"> P </em>，<em class="mq"> Q </em> ]的近似值，我们最终获得语言模型 <em class="mq"> Q </em>相对于语言源<em class="mq"> P </em>的困惑度的显式<strong class="lu ir">公式:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/e8b9b19869321db7f7d1500a55be0345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSqDq3LCd34k87dxQyNQ7A.png"/></div></div></figure><p id="9f01" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">作为<strong class="lu ir"> a数值</strong>的一个例子，GPT-2在维基百科数据集上实现了每个字符1比特(=令牌)，因此具有<strong class="lu ir">字符困惑度</strong> 2 =2。英语单词的平均长度等于5，这就相当于单词困惑度等于2⁵=32.</p><h2 id="c745" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">困惑的实用计算</h2><p id="87ae" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们在实践中可以使用(15)来计算困惑度的序列的长度<em class="mq"> n </em>受到由LM定义的序列的最大长度的限制。例如，GPT-2的最大长度等于1024个令牌。为了获得困惑的可靠近似值，最好的办法似乎是使用<strong class="lu ir">滑动窗口</strong>，正如这里的<a class="ae kv" href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif" rel="noopener ugc nofollow" target="_blank">所示</a>【10】。为了提高性能，也可以使用大于1的步幅。拥抱脸文档[10]有更多的细节。</p><p id="37ad" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">瞧啊！</p><h2 id="2399" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">参考</h2><p id="181f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">[1] Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，ukasz Kaiser，Illia Polosukhin，<em class="mq">注意力是你所需要的一切</em>，神经信息处理系统的进展30 (NIPS 2017)。</p><p id="cabe" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[2] Tom Brown等人<em class="mq">语言模型是很少出手的学习者</em>，神经信息处理系统进展33 (NeurIPS 2020)。</p><p id="68f8" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[3] Jacob Devlin，Ming-Wei Chang，Kenton Lee，Kristina Toutanova，<em class="mq"> BERT:用于语言理解的深度双向转换器的预训练</em>，计算语言学协会北美分会2019年会议录:人类语言技术，第1卷(长和短论文)。</p><p id="fee0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[4]杨，戴子航，，Jaime Carbonell，Russ R. Salakhutdinov，Quoc V. Le，XLNet: <em class="mq">用于语言理解的广义自回归预训练</em>，神经信息处理系统进展32 (NeurIPS 2019)。</p><p id="30b9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[5]刘，迈勒奥特，纳曼戈亚尔，杜，曼达尔乔希，陈，奥梅尔列维，，卢克泽特勒莫耶，韦塞林斯托扬诺夫，<em class="mq">罗伯塔:一种稳健优化的伯特预训练方法，</em> <a class="ae kv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">，</a> (2019)。</p><p id="18e2" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[6] Takeshi，Shane Gu，Machel Reid，Yutaka Matsuo，Yusuke Iwasawa，<em class="mq">大型语言模型是零射击推理器</em>，<a class="ae kv" href="https://paperswithcode.com/paper/large-language-models-are-zero-shot-reasoners" rel="noopener ugc nofollow" target="_blank">论文代码</a>(2022年5月)</p><p id="467d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[7]王敬实，阿曼普里特·辛格，朱利安·迈克尔，菲利克斯·希尔，奥梅尔·利维，塞缪尔·r·鲍曼，<em class="mq"> GLUE:一个多任务的自然语言理解基准和分析平台</em>，<a class="ae kv" href="https://arxiv.org/abs/1804.07461" rel="noopener ugc nofollow" target="_blank"> arXiv:1804.07461 </a>。</p><p id="06f8" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[8]欧阳龙等<em class="mq">用人类反馈训练语言模型遵循指令</em>，<a class="ae kv" href="https://arxiv.org/abs/2203.02155" rel="noopener ugc nofollow" target="_blank"/>(2022年3月)</p><p id="6545" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[9] Peter F. Brown，Vincent J. Della Pietra，Robert L. Mercer，Stephen A. Della Pietra，Jennifer C. Lai，<em class="mq">英语熵的一个上界估计，</em>计算语言学，第18卷，第1期，1992年3月。</p><p id="548f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">【10】抱脸文档，<a class="ae kv" href="https://huggingface.co/docs/transformers/perplexity#perplexity-of-fixedlength-models" rel="noopener ugc nofollow" target="_blank"> <em class="mq">定长模特的困惑</em> </a>。</p><p id="944c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">[11]托马斯·m·盖，乔伊·a·托马斯，<em class="mq">信息论要素，第2版</em>，威利2006。</p></div></div>    
</body>
</html>