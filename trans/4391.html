<html>
<head>
<title>Bayesian Regression Using PyMC3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyMC3的贝叶斯回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-regression-using-pymc3-e279ccc81d20#2022-09-28">https://towardsdatascience.com/bayesian-regression-using-pymc3-e279ccc81d20#2022-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20c3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用PyMC3包在Python中实现贝叶斯回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8da1b56def8c4a58f04620d379dce2af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hX3sHnixH-ZZQeGa"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Joachim Schnürle 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="9340" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">背景</h1><p id="467c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://www.pymc.io/welcome.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> PyMC3 </strong> </a>(现在简称PyMC)是一个贝叶斯建模包，使我们作为数据科学家能够轻松地执行<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯推理</strong> </a>。</p><p id="031b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在引擎盖下，PyMC3使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">【MCMC】</strong></a><strong class="lt iu"/>的方法计算后验分布。现在，这种方法非常复杂，需要另外一篇文章来全面介绍。因此，我在这里链接了一个很好地解释了这个话题的帖子<a class="ae ky" rel="noopener" target="_blank" href="/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50">。</a></p><p id="788a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能想知道为什么封装使用MCMC？嗯这是为了绕过<a class="ae ky" href="https://www.reddit.com/r/askmath/comments/cghi5c/what_is_an_intractable_integral_and_why_this/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">计算<a class="ae ky" href="https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">归一化常数</strong> </a>中的<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayes'_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯定理</strong> </a></strong></a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/18feaecf9728b45456c78d0768346a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*RKYfKnmcfWUgTlwlhoonAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="2806" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<em class="mt"> P(H | D) </em>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">后验</strong> </a>，<em class="mt"> P(H) </em> <strong class="lt iu"> </strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">先验</strong> </a>，<em class="mt"> P(D | H) </em>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">似然</strong> </a>，<em class="mt"> P(D) </em>是归一化常数，定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/b98f8d5ecdb0845861836aa96d433dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*WFzP9sQh9c6bbX5CjODjVw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="4992" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于许多问题来说，这个积分要么没有封闭形式的解，要么无法计算。因此，像MCMC这样的方法被开发出来解决这个问题，并允许我们使用贝叶斯方法。</p><p id="3c6f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你不熟悉贝叶斯定理，我推荐你看看我以前关于这个主题的文章:</p><div class="mv mw gp gr mx my"><a href="https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">条件概率和贝叶斯定理浅释</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">条件概率和贝叶斯定理的简单直观的解释。</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">pub.towardsai.net</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ks my"/></div></div></a></div><p id="b2a3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">还有另一种叫做<a class="ae ky" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">共轭先验</strong> </a>的方法也解决了这个棘手的问题，但是它的可塑性不如MCMC。如果你想了解更多关于共轭先验的知识，请查看我以前的文章:</p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/bayesian-conjugate-priors-simply-explained-747218be0f70"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">贝叶斯共轭先验简单解释</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">执行贝叶斯统计的一种计算有效的方法</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm ks my"/></div></div></a></div><p id="5198" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我们将介绍如何使用PyMC3包实现<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯线性回归</strong> </a>，并快速浏览一下它与普通的<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">线性回归</strong> </a>有何不同。如果你想知道如何从零开始实现贝叶斯线性回归，那么看看我最近的帖子:</p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/bayesian-regression-from-scratch-a1fe19ff64c"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">贝叶斯回归从零开始</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">使用Python从基本原理导出贝叶斯线性回归</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="no l nj nk nl nh nm ks my"/></div></div></a></div><h1 id="edbe" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">概述:贝叶斯与频率主义回归</h1><p id="4d88" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从本质上来说，<a class="ae ky" href="https://en.wikipedia.org/wiki/Frequentist_inference" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Frequentist </strong> </a>和Bayesian回归方法之间的关键区别在于它们如何处理参数。在频率统计中，线性回归模型的参数是固定的，而在贝叶斯统计中，它们是随机变量。</p><p id="df08" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">频率主义者使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>的方法来推导线性回归模型的值。MLE的结果是每个参数的单一固定值。</p><p id="fb9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，在贝叶斯世界中，你的参数有一个值的分布，它们可能有一定的概率。然后使用更多的数据更新这个分布，这样我们就可以更加确定参数的取值。这个过程被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯更新</strong> </a>，你可以在这里了解更多信息:</p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/bayesian-updating-simply-explained-c2ed3e563588"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">贝叶斯更新简单解释</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">用贝叶斯定理更新信念的直观解释</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="np l nj nk nl nh nm ks my"/></div></div></a></div><p id="6441" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是对贝叶斯和频率主义回归之间的主要区别的一瞥。如果你想要更深入的观点，有很多资源可以比我解释得更好！</p><h1 id="dd58" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用PyMC3</h1><h2 id="041e" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">包装</h2><p id="0ece" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，我们装入我们的包:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="67a9" class="nq la it od b gy oh oi l oj ok"># Import pyMC3 and also arviz for visualisation<br/>import pymc3 as pm<br/>import arviz as az</span><span id="cbbe" class="nq la it od b gy ol oi l oj ok"># Import the other core data science packages<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets<br/>from scipy.stats import norm<br/>import statsmodels.formula.api as smf</span></pre><p id="3fea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可能需要安装PyMC3和<a class="ae ky" href="https://www.arviz.org/en/latest/" rel="noopener ugc nofollow" target="_blank">T5 ArviZ</a><strong class="lt iu">。为此，只需遵循他们网站上的安装说明。</strong></p><h2 id="9543" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">数据</h2><p id="d0d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们使用sklearn的<em class="mt"> make_regression </em>函数生成一些数据:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="5877" class="nq la it od b gy oh oi l oj ok"># Generate data<br/>x, y = datasets.make_regression(n_samples=10_000,<br/>                                n_features=1,<br/>                                noise=10,<br/>                                bias=5)</span><span id="7e6a" class="nq la it od b gy ol oi l oj ok"># Create the dataframe<br/>data = pd.DataFrame(list(zip(x.flatten(), y)),columns =['x', 'y'])</span><span id="481e" class="nq la it od b gy ol oi l oj ok"># Plot the data<br/>fig, ax = plt.subplots(figsize=(9,5))<br/>ax.scatter(data['x'], data['y'])<br/>ax.ticklabel_format(style='plain')<br/>plt.xlabel('x',fontsize=18)<br/>plt.ylabel('y',fontsize=18)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c0853b6d2ccf9c42ec2e851053e6a799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*2btFjRfAfmTR_5V7Qac4TQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><h2 id="61e7" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">频率回归线</h2><p id="d53f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">【普通最小二乘法(OLS) </strong> </a>的方法绘制频率线性回归线:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="0b05" class="nq la it od b gy oh oi l oj ok"># OLS line<br/>formula = 'y ~ x'<br/>results = smf.ols(formula, data=data).fit()<br/>results.params</span><span id="32d9" class="nq la it od b gy ol oi l oj ok"># Get our equation of the OLS line<br/>inter = results.params['Intercept']<br/>slope = results.params['x']<br/>x_vals = np.arange(min(x), max(x), 0.1)<br/>ols_line = inter + slope * x_vals</span><span id="7e20" class="nq la it od b gy ol oi l oj ok"># Plot the target against our feature with the OLS regression<br/>fig, ax = plt.subplots(figsize=(9,5))<br/>ax.scatter(data['x'], data['y'])<br/>ax.plot(x_vals, ols_line,label='OLS Fit', color='red')<br/>ax.ticklabel_format(style='plain')<br/>plt.xlabel('x',fontsize=18)<br/>plt.ylabel('y',fontsize=18)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/462c2f2b3f4f940dc6dc6ef61a914082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*SUQLvkKM-Mi5-k1xf2WvWw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><h2 id="f7c9" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">PyMC3: 100个样本</h2><p id="0786" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要使用PyMC3，我们必须启动一个模型，选择我们的先验，并告诉模型我们的后验应该是什么分布:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="f2ac" class="nq la it od b gy oh oi l oj ok"># Start our model<br/>with pm.Model() as model_100:</span><span id="5922" class="nq la it od b gy ol oi l oj ok">    # Define the priors on each parameter:<br/>    grad = pm.Uniform("grad",<br/>                      lower=results.params['x']*0.5,<br/>                      upper=results.params['x']*1.5)<br/>    <br/>    inter = pm.Uniform("inter",<br/>                       lower=results.params['Intercept']*0.5,<br/>                       upper=results.params['Intercept']*1.5)<br/>    <br/>    sigma = pm.Uniform("sigma",<br/>                       lower=results.resid.std()*0.5,\<br/>                       upper=results.resid.std()*1.5)<br/>    <br/>    # Linear regression line<br/>    mean = inter + grad*data['x']<br/>    <br/>    # Describe the distribution of our conditional output<br/>    y = pm.Normal('y', mu = mean, sd = sigma, observed = data['y'])</span><span id="47b5" class="nq la it od b gy ol oi l oj ok">    # Run the sampling using pymc3 for 100 samples<br/>    trace_100 = pm.sample(100,return_inferencedata=True)</span></pre><p id="0920" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这段代码将运行MCMC采样器来计算每个参数的后验概率。同样，如果您有兴趣了解MCMC是如何工作的，请参考背景部分的链接。</p><p id="4301" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们现在可以绘制每个参数的后验分布:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="e9e0" class="nq la it od b gy oh oi l oj ok">with model_100:<br/>    az.plot_posterior(trace_100,<br/>                      var_names=['grad', 'inter', 'sigma'],<br/>                      textsize=18,<br/>                      point_estimate='mean',<br/>                      rope_color='black')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/916a4d962982cb18879a989cbb8a497a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNXwQgAAX3DJ5OsQzpCcrw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><p id="de2a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些后验分布的平均值与OLS估计值相同，但是这不是参数可以采用的唯一值。如你所见，有很多值。这是贝叶斯线性回归背后的要点。</p><p id="08d6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">HDI代表<a class="ae ky" href="https://stats.stackexchange.com/questions/148439/what-is-a-highest-density-region-hdr" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">高密度区间</strong> </a>，描述了我们在参数估计中的确定性。如果你想了解更多关于人类发展指数的信息，请查看我在<a class="ae ky" href="https://en.wikipedia.org/wiki/Credible_interval" rel="noopener ugc nofollow" target="_blank">T21【贝叶斯可信区间T23】上的博文:</a></p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/bayesian-credible-intervals-simply-explained-24989c9259a3"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">贝叶斯可信区间简单解释</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">贝叶斯可信区间的简明描述及其在Python中的实现</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="oo l nj nk nl nh nm ks my"/></div></div></a></div><p id="11dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个模拟只使用了我们数据中的100个样本。与贝叶斯方法一样，我们随着更多的数据变得更加确定。</p><h2 id="9cbf" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">PyMC3: 10，000个样本</h2><p id="1198" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们再次运行该过程，但现在使用10，000个样本:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="16c4" class="nq la it od b gy oh oi l oj ok"># Start our model<br/>with pm.Model() as model_10_100:</span><span id="0af0" class="nq la it od b gy ol oi l oj ok">    # Define the priors on each parameter:<br/>    grad = pm.Uniform("grad",<br/>                      lower=results.params['x']*0.5,<br/>                      upper=results.params['x']*1.5)<br/>    <br/>    inter = pm.Uniform("inter",<br/>                       lower=results.params['Intercept']*0.5,<br/>                       upper=results.params['Intercept']*1.5)<br/>    <br/>    sigma = pm.Uniform("sigma",<br/>                       lower=results.resid.std()*0.5,<br/>                       upper=results.resid.std()*1.5)<br/>    <br/>    # Linear regression line<br/>    mean = inter + grad*data['x']<br/>    <br/>    # Describe the distribution of our conditional output<br/>    y = pm.Normal('y', mu = mean, sd = sigma, observed = data['y'])</span><span id="5035" class="nq la it od b gy ol oi l oj ok">    # Run the sampling using pymc3 for 10,000 samples<br/>    trace_10_000 = pm.sample(10_000,return_inferencedata=True)</span></pre><p id="56ad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们再次查看参数的后验分布:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="ecdf" class="nq la it od b gy oh oi l oj ok">with model_10_100:<br/>    az.plot_posterior(trace_10_000,<br/>                      var_names=['grad', 'inter', 'sigma'],<br/>                      textsize=18,<br/>                      point_estimate='mean',<br/>                      rope_color='black')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/9aaaed10469bf07d0825bbd97240a5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hlfk_t8mHK2TJHA4PnaDTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><p id="f0dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">平均预测没有改变，但是随着我们更加确定参数的分布，总体上分布变得更加平滑和紧密。</p><h2 id="3a48" class="nq la it bd lb nr ns dn lf nt nu dp lj ma nv nw ll me nx ny ln mi nz oa lp ob bi translated">密码</h2><p id="587c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要查看我在这篇文章中使用的全部代码，请点击这里查看GitHub上的笔记本:</p><div class="mv mw gp gr mx my"><a href="https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/pymc3_tutorial.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">Medium-Articles/pym C3 _ tutorial . ipynb at main egorhowell/Medium-Articles</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">我在我的媒体博客/文章中使用的代码。通过创建一个关于…的帐户，为egorhowell/Medium-Articles的开发做出贡献</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="op l nj nk nl nh nm ks my"/></div></div></a></div><h1 id="b0b8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="1b50" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们浏览了贝叶斯统计的主要原则，并解释了它如何采取不同于频率统计的线性回归方法。然后我们看了一个基本的例子，如何使用PyMC3包进行贝叶斯回归。</p><h1 id="0ec4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系！</h1><ul class=""><li id="ca40" class="oq or it lt b lu lv lx ly ma os me ot mi ou mm ov ow ox oy bi translated">要在媒体上阅读无限的故事，请务必在此注册！T3<em class="mt">T5】💜</em></li><li id="9786" class="oq or it lt b lu oz lx pa ma pb me pc mi pd mm ov ow ox oy bi translated"><a class="ae ky" href="/subscribe/@egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="mt">当我在这里发布注册邮件通知时，可以获得更新！</em> </a> <em class="mt"> </em>😀</li><li id="b8dc" class="oq or it lt b lu oz lx pa ma pb me pc mi pd mm ov ow ox oy bi translated"><a class="ae ky" href="https://www.linkedin.com/in/egor-howell-092a721b3/" rel="noopener ugc nofollow" target="_blank"> <em class="mt">领英</em> </a> <em class="mt"> </em>👔</li><li id="a7a0" class="oq or it lt b lu oz lx pa ma pb me pc mi pd mm ov ow ox oy bi translated"><a class="ae ky" href="https://twitter.com/EgorHowell" rel="noopener ugc nofollow" target="_blank"> <em class="mt">推特</em> </a> <em class="mt"> </em> 🖊</li><li id="fb18" class="oq or it lt b lu oz lx pa ma pb me pc mi pd mm ov ow ox oy bi translated"><a class="ae ky" href="https://github.com/egorhowell" rel="noopener ugc nofollow" target="_blank"><em class="mt">github</em></a><em class="mt"/>🖥</li><li id="87b0" class="oq or it lt b lu oz lx pa ma pb me pc mi pd mm ov ow ox oy bi translated"><a class="ae ky" href="https://www.kaggle.com/egorphysics" rel="noopener ugc nofollow" target="_blank"><em class="mt"/></a><em class="mt"/>🏅</li></ul><blockquote class="pe pf pg"><p id="72f1" class="lr ls mt lt b lu mn ju lw lx mo jx lz ph mp mc md pi mq mg mh pj mr mk ml mm im bi translated">(所有表情符号由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>设计——开源表情符号和图标项目。许可证:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></blockquote></div></div>    
</body>
</html>