<html>
<head>
<title>A True Story of a Gradient that Vanished in an RNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个在RNN消失的渐变的真实故事</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-true-story-of-a-gradient-that-vanished-in-an-rnn-56437c1eea45#2022-09-20">https://towardsdatascience.com/a-true-story-of-a-gradient-that-vanished-in-an-rnn-56437c1eea45#2022-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4669" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">你需要的只是一个简单明了的解释</h2></div><p id="fb9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">递归神经网络试图推广前馈神经网络来处理序列数据。它们的存在彻底改变了深度学习，因为它们为从语言建模到翻译的众多迷人应用铺平了道路。然而，阻碍RNNs在许多这些应用中流行的一个问题是它们不能处理当输入序列很长时自然出现的长期依赖性。</p><p id="78f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在<a class="ae lb" rel="noopener" target="_blank" href="/unriddling-rnns-with-depth-and-in-both-directions-9ed336c4b392">最后一个故事</a>中所描述的，rnn无法处理长期依赖性可以归因于它们的消失梯度问题，这是时间反向传播的副作用——用于训练这种网络的算法。</p><p id="46b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个故事中，我们将开始一个快速的旅程来了解时间的反向传播是如何工作的，以找出:</p><ol class=""><li id="7ada" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">为什么渐变消失了？</li><li id="69f3" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">为什么会引起短时记忆？</li><li id="f18a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">我们能做些什么来阻止它们消失？</li></ol><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/9cc2fb6779bc99c0257f7468c8bec51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7OluJYTa64Wx3RfTt3nJ4g.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@aronvisuals?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aron视觉效果</a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="5e0e" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">为什么渐变消失了？</h2><p id="6403" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">在RNN中，任何示例(序列)造成的损失是序列中每个令牌造成的损失之和。这假定了常规设置，其中RNN为输入中的每个令牌生成一个输出。因此我们可以写</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ne"><img src="../Images/2aee4006a97b2133d5a1580e0b15b10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmPGmB_Yepry-u-ES90IFA.png"/></div></div></figure><p id="3af3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们试着找到<strong class="kh ir">∂l/∂w</strong>ₕ<strong class="kh ir">T9】ₓ</strong>因为我们将需要它来进行梯度下降以更新<strong class="kh ir">w</strong>ₕ<strong class="kh ir"><em class="nf">ₓ</em></strong><em class="nf">。</em></p><p id="4b08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，<strong class="kh ir">w</strong>ₕ<strong class="kh ir">t23】ₓT25】是输入权重矩阵(用于循环层连接的输入)，而<strong class="kh ir"> W </strong> ₕₕ是隐藏权重矩阵(用于从循环层到其先前输出的连接)。同时，<strong class="kh ir"> <em class="nf"> W </em> ₒ </strong> ₕ是输出权重矩阵(用于从递归层到输出的连接)。</strong></p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ne"><img src="../Images/61114cf5d775f70713eabacc5036166c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*al0cmMdMN-bAZsIAyvZNNw.png"/></div></div></figure><p id="c2d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在很明显，一旦我们找到<strong class="kh ir">∂l</strong>ₜ<strong class="kh ir">/∂w</strong>ₕ<strong class="kh ir"><em class="nf">ₓ</em></strong>，我们就找到了我们的目标<strong class="kh ir">∂l/∂w</strong>ₕ<strong class="kh ir">ₓ</strong>。我们利用链式法则把它写成</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ne"><img src="../Images/a37a161d9138ec9cb8e9ea23d76901a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJPu8D2wSun-cOn9Uv8vmA.png"/></div></div></figure><p id="b4b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">蓝色的两项很容易计算。对于第一项，损失是输出的直接函数，对于第二项，我们知道yₜ=<em class="nf">f(</em><strong class="kh ir"><em class="nf">【wₒ】</em></strong><em class="nf">【ₕhₜ+bₕ】</em>，所以在这两种情况下，只需要求导。</p><p id="f55d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，对于第三个学期，我们需要记住</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/d3b64a235d9b6445fb73cc570f0f78b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PhbusDMMY3e1pePdAKh4GA.png"/></div></div></figure><p id="6815" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nh"><img src="../Images/9f352275ab802ec15a25c976edc9e2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RtyxrP9dsTmFyJzXALM4g.png"/></div></div></figure><p id="388e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以通过使用链式法则，我们可以写出</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/43d29639fca6ae920a6ba191f3abbcc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pu6WG9CctUTSmef6w8yoVQ.png"/></div></div></figure><p id="99be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">蓝色的项很容易计算，因为每个h<strong class="kh ir"><em class="nf">【ᵢ】</em></strong>都是ₓ.<strong class="kh ir"><em class="nf">w</em></strong><em class="nf">中的直接函数</em> </p><blockquote class="ni nj nk"><p id="afe4" class="kf kg nf kh b ki kj jr kk kl km ju kn nl kp kq kr nm kt ku kv nn kx ky kz la ij bi translated">请注意，如果我们想要使用<a class="ae lb" href="https://mathworld.wolfram.com/TotalDerivative.html" rel="noopener ugc nofollow" target="_blank">完美符号</a>，那么我们可能应该声明类似于<strong class="kh ir"> h=f(…) </strong>的内容，然后在右侧使用<strong class="kh ir"> f </strong>而不是<strong class="kh ir"> h </strong>，但是我会这样保留它，因为除了其他原因之外，否则它看起来会更麻烦。</p></blockquote><p id="c33b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同时，对于另一个术语，我们似乎需要另一轮链式法则，因为</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi no"><img src="../Images/bfda5e274b758ad5ab05b4b41ee90b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCC9gn0sKnIlEt0dJX7LBA.png"/></div></div></figure><p id="a779" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们必须写</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi no"><img src="../Images/3e76f89dc2eecc433df26a257fc4a029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMfKytZ6El5CiTJxvEBEgA.png"/></div></div></figure><p id="1eae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中后插<strong class="kh ir">∂h</strong>t32】ₜt34】/∂wₕ<strong class="kh ir">t37】ₓt39】产量</strong></p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi np"><img src="../Images/56f7a070b6eaefd60eba06e868eaf8a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEFzFZu3HYQP6okzn29ELQ.png"/></div></div></figure><h2 id="c9f7" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">为什么会引起短时记忆？</h2><p id="468f" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">现在，为了更仔细地观察消失梯度问题，让我们考虑一个4个单词的句子。这意味着<em class="nf"> t=4 </em>并且上面的表达式计算为</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/42783a9f85334dc621aa62069df73e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQhxp3foUH9bvlIz7U_mbQ.png"/></div></div></figure><p id="715d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这我们可以认为是第四个词在<strong class="kh ir"><em class="nf">w</em></strong><em class="nf">ₕ</em><strong class="kh ir"><em class="nf">ₓ</em></strong><em class="nf">中更新权重的贡献。</em> <strong class="kh ir"> <em class="nf"> </em> </strong>注意<strong class="kh ir">考虑第一个词的唯一方法</strong>是通过第一个词，这个词(正如我们将要展示的)预计非常小。因此，就好像它不存在，尽管它与第四项有任何关联。</p><p id="eecb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果是参数将被偏向于仅捕获训练期间的短期依赖性。产生一个神经网络，它不能确定是否选择“今天早上我看到的猫非常饿”。</p><p id="bf57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了支持第一项预计会很小的事实，我们需要观察</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/32e85744010d5106269dfd7d8bf6b45c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8nZ-xT4lo84KNta9bkdaQ.png"/></div></div></figure><p id="a12a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于网络的初始化方式(通常来自正态分布)，权重自然很小，这同样适用于激活函数的导数，因为它通常是导数≤ 1的双曲正切函数。用类似ReLU的东西替换Tanh可能只能解决部分问题。</p><h2 id="e738" class="mg mh iq bd mi mj mk dn ml mm mn dp mo ko mp mq mr ks ms mt mu kw mv mw mx my bi translated">我们能做些什么来阻止它们消失？</h2><p id="ca7d" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">问题的根源显然是</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/f87fbdf7119951c5f15b5547c86564f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXQaHfaR6G0QCEsuEeunLQ.png"/></div></div></figure><p id="8068" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们能否重新设计循环层，使其不再必要？假设，我们宁愿让隐藏状态h被定义为</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ng"><img src="../Images/6eb745ebddad3159d9f91e941ef920c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgKvL26bT6O_W5qSg_uIGQ.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">*是基于元素的产品</p></figure><p id="7fcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> G₁、</strong>g₂和<strong class="kh ir"> ĥ </strong>中的每一个都是使用<strong class="kh ir"> h </strong>作为先前隐藏状态的不同重现层的结果(它们都是它的函数)。</p><p id="2e02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，我们有</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nq"><img src="../Images/334af13cbe30ff1e601172aa06953ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_oNNnzuoDt-X7wI1hjgJQ.png"/></div></div></figure><p id="9d3b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不能再声称它总是≤ 1，即使这四项都成立。这个事实正是像LSTM和GRU这样的建筑用来克服梯度消失问题的。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nr"><img src="../Images/5e0b77be327c995bebe7fa60f5683f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEhmBxMUSPXL8BxmzrxcMg.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">杰奎琳·弗洛克在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f748" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的故事到此结束。我们已经探索了rnn如何以及为什么会遭受短期记忆的困扰，以及可以做些什么来缓解这个问题。希望你觉得这个故事很容易读懂。下次见，再见。</p></div></div>    
</body>
</html>