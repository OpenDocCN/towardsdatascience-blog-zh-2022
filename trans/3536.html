<html>
<head>
<title>Why Using Learning Rate Schedulers in NNs May Be a Waste of Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么在神经网络中使用学习率调度器可能是浪费时间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-using-learning-rate-schedulers-in-nns-may-be-a-waste-of-time-8fa20339002b#2022-08-05">https://towardsdatascience.com/why-using-learning-rate-schedulers-in-nns-may-be-a-waste-of-time-8fa20339002b#2022-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="532c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">提示:批量大小是关键，它可能不是你想的那样！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b229fd06b49a90f050b3c953411deed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H-NPJXpUJHNptEHo"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安德里克·朗菲尔德在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><blockquote class="kw kx ky"><p id="2b3b" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ir">TL；DR: </strong>不是用一个因子来降低学习速率，而是用同一个因子来增加批量大小，以实现更快的收敛，如果不是更好的话，也能获得更好的训练结果。</p></blockquote></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="02d7" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">近年来，神经网络的不断发展使得其在现实世界中的应用越来越多。尽管神经网络和其他类似的深度学习方法很受欢迎，但它们仍然存在严重的缺陷，影响了它们在未知数据集上的可用性或性能。</p><p id="f725" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">众所周知，神经网络消耗大量的计算资源和时间。随着对神经网络研究的不断深入，越来越多的复杂结构被提出并付诸实践。然而，即使一些较大的网络与较小的网络相比可能能够获得不错的性能，但其训练和推理时间的总量使它们不适用于许多现实世界的场景。减少神经网络训练时间的一个标准解决方案是增加批量大小，或者增加网络在其参数的一次更新中看到的数据量。从逻辑上讲，如果网络在每次更新时看到更多的数据，那么与每次出现新样本就更新其参数的网络相比，参数更新的总数将会减少。我们可以通过减少参数更新的次数来有效地减少网络训练时间。</p><h2 id="107a" class="mg mh iq bd mi mj mk dn ml mm mn dp mo md mp mq mr me ms mt mu mf mv mw mx my bi translated">批量大小和泛化差距</h2><p id="b246" class="pw-post-body-paragraph kz la iq lc b ld mz jr lf lg na ju li md nb ll lm me nc lp lq mf nd lt lu lv ij bi translated">然而，最近的研究表明，更大的批量会导致更大的泛化差距(参见Nitish Shirish Keskar等人的“关于深度学习的大批量训练:泛化差距和尖锐极小值”)。术语“泛化差距”与“过度拟合”同义，描述的是训练数据和测试数据之间存在性能“差距”的现象，后者比前者更差。为了提高在看不见的数据集上的性能，模型需要能够将它们在训练中所学的知识归纳并应用到新的、看不见的样本上。</p><p id="6f14" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">为了解释为什么增加批量大小会导致显著的泛化差距，想象一个以2递增的数字串，从“1，3，5，…”，一直到“…，37，39”。现在，假设你被告知在不看的情况下从列表中随机选择15个数字。在你睁开眼睛后，你有多大可能发现原始列表中的增量模式(假设事先没有告诉你模式)？经过一番重新整理，答案估计很有可能。这是因为您获得了列表中20个值中的15个；偶然地，一些组合可以泄露模式。如果你只能从列表中随机选择三个值会怎么样？你还能辨认出模式吗？答案并不确定，因为列表中20个值中的3个可能很少或没有给出关于每个数字之间的原始关系的信息(增量为2)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/d174107a86790d39ae0e446a43489e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0TpRNIL-3LDl_VurtSuHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个关于大批量和小批量之间差异的思想实验。图片来自作者。</p></figure><p id="e53c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">批量大小对模型性能的影响与上述思维实验的方式相同:批量越大，模型越有可能在更少的时期内发现特征和目标之间的关系。这一发现可能有利于训练，因为该模型可以更快地达到全局最小值，但这是以较差的泛化能力为代价的。当模型在每个更新步骤中看到的数据较少时，它可能无法找到适合训练数据的最佳参数，但它更有可能为同一类型的所有数据集归纳出一个通用规则(就像世界上不是每个数字列表都有递增2的模式一样)。批量较大的模型更容易过度拟合，从而比批量较小的模型产生更大的泛化差距，而批量较小的模型可以在看不见的样本上表现良好。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h2 id="2b72" class="mg mh iq bd mi mj mk dn ml mm mn dp mo md mp mq mr me ms mt mu mf mv mw mx my bi translated">批量大小和学习率与噪音训练的关系</h2><p id="3ab0" class="pw-post-body-paragraph kz la iq lc b ld mz jr lf lg na ju li md nb ll lm me nc lp lq mf nd lt lu lv ij bi translated">Samuel L. Smith和Quoc V. Le在他们的研究“关于一般化和随机梯度下降的贝叶斯观点”中，通过提出SGD优化器的“噪声标度”计算进一步证明了这一点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="c6d6" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">其中N是训练样本的数量，B是批量大小，ε是学习速率。由于B &lt;&lt; N，在大多数情况下，我们可以将噪声比例简化为</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="a323" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">“噪声标度”值描述了由于批量更新而导致的SGD训练中随机波动的幅度。直观地说，噪声越小，训练结果越好，但请注意，在训练中保持合理的噪声量有助于模型的泛化，从而使训练规范化。将批量大小设置为相对较大的值将在整个训练过程中产生最小的噪声，从而导致较差的泛化能力，因此较大批量大小周围的直觉会导致较大的泛化差距。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nh ng l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">增加批量大小和降低学习速率都会降低噪声水平。</p></figure><p id="e6a1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">衰减的学习率通常被用作训练的一部分来降低噪声。通过在训练开始时具有较大的学习率值，网络将能够在噪声情况下搜索较大的参数空间。一旦模型找到了要采取的最佳方向/步骤，额外的噪声可能会开始损害训练(即，验证损失开始增加)，然后学习速率会慢慢降低。当模型接近全局最小值时，这个动作将减少噪声，减慢到更稳定的训练阶段。这样做可以确保模型的泛化和良好的训练结果，但代价是复杂网络需要长达数天甚至数周的训练时间。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h2 id="dd4a" class="mg mh iq bd mi mj mk dn ml mm mn dp mo md mp mq mr me ms mt mu mf mv mw mx my bi translated">增加批量大小，而不是降低学习速度</h2><p id="9922" class="pw-post-body-paragraph kz la iq lc b ld mz jr lf lg na ju li md nb ll lm me nc lp lq mf nd lt lu lv ij bi translated">注意，不仅衰减ε/学习率的值可以降低噪声水平，而且增加B/批量大小的值也可以达到相同的效果。利用这一事实，Samuel L. Smith、Pieter-Jan Kindermans、Chris Ying和Quoc V. Le在他们的研究中表明，学习速度和批量大小成反比。作者提出，不使用学习率调度器，而是在训练期间增加批量大小，这可以根据经验获得与衰减学习率相同的结果。在作者的实验中，他们在以下条件下训练了三种类型的模型:</p><ol class=""><li id="0d24" class="ni nj iq lc b ld le lg lh md nk me nl mf nm lv nn no np nq bi translated">学习率下降:学习率反复下降5倍</li><li id="104a" class="ni nj iq lc b ld nr lg ns md nt me nu mf nv lv nn no np nq bi translated">混合:在第一步增加批量，同时保持学习速率不变，然后训练类型恢复到#1。这种训练方法模拟了较大批量不可行的潜在硬件限制</li><li id="1766" class="ni nj iq lc b ld nr lg ns md nt me nu mf nv lv nn no np nq bi translated">增加批量:批量以与#1相同的速率增加5倍。</li></ol><p id="c1f5" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">下图显示了他们的培训结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/75e66d7bd2243fdbafcfa40d8c0fecde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WjaWeqe9AyykBPHXz4XPDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三种模型的实验结果。来自塞缪尔·史密斯等人。</p></figure><p id="d01c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">在左边，我们观察到，随着批量的增加，模型的训练结果等价于学习率下降的训练。然而，与传统的衰减学习率模型相比，增加批量大小的模型实现了67%的参数更新。更具体地说，作者指出，批量大小应增加到大约N/10；然后，在剩余的训练持续时间内，该模型将回复到传统的衰减学习率训练风格。</p><p id="818b" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">本质上，我们可以用“批量调度器”代替学习率调度器，并大大减少训练时间。下面是作者在Keras中的方法的简单演示。使用森林覆盖类型数据集(可在Scikit-learn中找到)。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="5d5a" class="mg mh iq ny b gy oc od l oe of">from sklearn.datasets import fetch_covtype</span><span id="b1e6" class="mg mh iq ny b gy og od l oe of">dataset = fetch_covtype()<br/>X = pd.DataFrame(dataset.data)<br/>y = pd.DataFrame(dataset.target)<br/>y = pd.get_dummies(y[0])</span><span id="99e3" class="mg mh iq ny b gy og od l oe of">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)</span></pre><p id="2796" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">用于演示的模型是一个相当深的表格数据网络，有七个隐藏层和一个类似瓶颈的设计。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="783c" class="mg mh iq ny b gy oc od l oe of">def create_testing_model():<br/>    <br/>    inp = L.Input(X.shape[1])<br/>    x = L.Dense(512, activation="relu")(inp)<br/>    x = L.Dropout(0.25)(x)<br/>    x = L.Dense(256, activation="relu")(x)<br/>    x = L.Dense(256, activation="relu")(x)<br/>    x = L.Dropout(0.25)(x)<br/>    x = L.Dense(128, activation="relu")(x)<br/>    x = L.Dense(256, activation="relu")(x)<br/>    x = L.Dense(256, activation="relu")(x)<br/>    x = L.Dropout(0.25)(x)<br/>    x = L.Dense(512, activation="relu")(x)<br/>    x = L.Dropout(0.25)(x)<br/>    out = L.Dense(7, activation="softmax")(x)<br/>    normal_model = M.Model(inputs=inp, outputs=out)<br/>    normal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=6e-4), loss=tf.keras.losses.CategoricalCrossentropy())<br/>    <br/>    return normal_model</span></pre><p id="b8ad" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">通过模仿Keras <code class="fe oh oi oj ny b">ReduceLROnPlateau</code>回调，当验证损失在三个时期内没有改善时，批量大小从256开始增加5倍。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="b70e" class="mg mh iq ny b gy oc od l oe of"># timing purposes<br/>batch_start = time.time()<br/># total epoch number<br/>epoch=60<br/># ensure the batch size is 256 on the first epoch<br/>batch_size=256/reduce_fac<br/># for recording training history<br/>batch_history_total = pd.DataFrame([], columns=["loss", "val_loss"])<br/># init model<br/>batch_model = create_testing_model()</span><span id="5200" class="mg mh iq ny b gy og od l oe of"># make sure batch size does not exceed sample size/10 and epoch doens't go below 0<br/>while epoch &gt; 0 and batch_size &lt;= len(X_train)/10:<br/>    <br/>    print(F"CURRENT BATCH SIZE: {batch_size*reduce_fac}")<br/>    # early stop if val loss stops improving for 3 epochs<br/>    early_stop = C.EarlyStopping(monitor="val_loss", patience=3, verbose=2)</span><span id="1055" class="mg mh iq ny b gy og od l oe of">batch_history = batch_model.fit(X_train, y_train, epochs=epoch, batch_size=int(batch_size*reduce_fac), <br/>                                    validation_data=(X_test, y_test), callbacks=[early_stop], verbose=2)<br/>    cur_history = pd.DataFrame(batch_history.history)<br/>    # concat the current training history to the total dataframe<br/>    batch_history_total = pd.concat([batch_history_total, cur_history], axis=0).reset_index(drop=True)<br/>    # adjust batch size<br/>    batch_size = batch_size*(reduce_fac)<br/>    # decrease total epoch by the number of epochs trained<br/>    epoch_trained = len(cur_history)<br/>    epoch -= epoch_trained</span><span id="14d3" class="mg mh iq ny b gy og od l oe of">if epoch &gt; 0: <br/>    # reverting to reduce lr training<br/>    print("reverting to reduce lr training")<br/>    reduce_lr = C.ReduceLROnPlateau(monitor="val_loss", factor=(1/reduce_fac), patience=3, verbose=2)<br/>    batch_history = batch_model.fit(X_train, y_train, epochs=epoch, batch_size=int(batch_size), validation_data=(X_test, y_test), callbacks=[reduce_lr], verbose=2)<br/>    batch_history_total = pd.concat([batch_history_total, pd.DataFrame(batch_history.history)], axis=0).reset_index(drop=True)</span><span id="f03d" class="mg mh iq ny b gy og od l oe of">batch_end = time.time()</span></pre><p id="ea2c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">为了进行比较，用256的恒定批量训练相同的模型，同时降低验证损失没有改善的每三个时期的学习率。两个模型都训练了60个历元，仅用<code class="fe oh oi oj ny b">ReduceLROnPlateau</code>训练的模型令人惊讶地慢了两倍(在GPU上)，验证性能稍差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/a2eb84cc0cf14f57149d052976f0518e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phPkW-SNQ6rTufkPWEttHQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">减少批量方法和衰减lr方法的训练结果比较。图片来自作者。</p></figure><p id="7927" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li md lk ll lm me lo lp lq mf ls lt lu lv ij bi translated">下次，在训练任何神经网络时，无论是图像分类、表格数据还是音频任务，都要给“批量调度程序”一个尝试。结果可能会为您节省一些时间(和细微的改进)！</p></div></div>    
</body>
</html>