<html>
<head>
<title>Ace your Machine Learning Interview — Part 6</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第六部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-6-5f0d84e435a1#2022-11-10">https://towardsdatascience.com/ace-your-machine-learning-interview-part-6-5f0d84e435a1#2022-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bbdd03d0194b24ee3f8a3459b239f8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1cnqyQM52TXKurZN"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迪伦·吉利斯在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="e553" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用 Python 深入决策树</h2></div><p id="41b2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">今天我继续我的“Ace your Machine Learning Interview”系列的第六篇文章，讨论决策树！</p><p id="e47a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您对本系列之前的文章感兴趣，我在这里留下了链接:</p><ol class=""><li id="880d" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-1-e6a5897e6844"> <em class="ma"> Ace your Machine Learning 面试—第一部分</em> </a> <em class="ma">:深入线性、套索和岭回归及其假设</em></li><li id="6112" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-2-c58526b5faba"><em class="ma">Ace your Machine Learning 访谈—第二部分</em> </a> <em class="ma">:使用 Python 深入研究分类问题的逻辑回归</em></li><li id="1145" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-3-af432f922aa7"><em class="ma">Ace your Machine Learning 面试—第三部分</em> </a> <em class="ma">:使用 Python 研究朴素贝叶斯分类器</em></li><li id="6b59" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-4-e30b695ce63"><em class="ma">Ace your Machine Learning 访谈—第四部分</em> </a> <em class="ma">:深入研究使用 Python 的支持向量机</em></li><li id="ad71" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-5-3de48703cd65"><em class="ma">Ace your Machine Learning 访谈—第五部分</em> </a> <em class="ma">:深入研究使用 Python 的内核支持向量机</em></li></ol><h2 id="d926" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated"><strong class="ak">简介</strong></h2><p id="2952" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">我们现在已经到了引入决策树的时候了。让我们先说这个算法受到那些<strong class="kx jh">高度重视模型的可解释性</strong>的人的喜爱。也就是能够解释<strong class="kx jh">机器学习模型为什么会给出某个输出结果</strong>。想象一下你去银行申请抵押贷款。你的抵押贷款被拒绝了。然后你问为什么抵押贷款被拒绝给你。如果银行家回答你，“嗯……我不知道，电脑显示抵押贷款被拒绝了，所以我什么也做不了”，那就不太好了。这就是使用像神经网络这样的机器学习算法会发生的事情。相反，使用决策树，银行家将能够给你确切的原因。你的抵押贷款被拒绝有三个主要原因:(1)你的工资(2)你的年龄(3)你的历史。</p><p id="bf71" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我想马上告诉你，这个算法虽然用起来很简单，但并不是很'<em class="ma">强大'</em>，它很容易<strong class="kx jh">导致数据过拟合</strong>。但是我们将在后面看到<strong class="kx jh">如何用一种组装方法</strong>来改进它。</p><h2 id="13d0" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">决策图表</h2><p id="5dbe" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">当我想到决策树时，我总是想象有一个数据集，为了简化可视化，只有两个特征，并像切馅饼一样切这个数据集。</p><p id="b2c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">只有您可以制作的切片必须平行于作为轴的特征。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/70e2b0eb18b61ae086eb1b4ba1294010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K73FmEmM7id2CA8TKnzZtw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">用决策树剪切(图片由作者提供)</p></figure><p id="c21b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上图中，我们有一个由几个点组成的二维数据集。这些点中的每一个都与一个类别相关联:三角形、圆形或十字形。决策树一次进行一次切割，目的是将每个特定类别的点隔离在一个空间中。这样，在我们必须对一个新点进行分类的推断时刻，我们只需要看到它将被放置在哪个空间中，我们就可以立即对它进行分类。</p><h2 id="d01e" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">更详细</h2><p id="cdc2" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">但是这些削减意味着什么呢？让我们考虑特征 1 的第一个切割，绿色切割。假设特征 1 是虹膜数据集的<em class="ma">花瓣长度</em>特征。然后在轴的 f1 点把空间分成两部分的绿切简单的问“<strong class="kx jh"> <em class="ma">你的花瓣长度值是大于还是小于 f1？</em> </strong>“之后，我们将询问相同类型的问题，但针对特征轴 2 上的另一个值。同样的事情也发生在特征 1 上，蓝色切割等等…</p><p id="0c89" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最终，我们将会在每个点自己的子空间中对其进行正确的分类。</p><p id="d38f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是模型的可解释性的来源！当我们对一个新的数据项进行分类时，我们所做的就是回答一系列这样的问题。当一个点被归类为某个标签 A 时，我们将完全能够说出原因！</p><p id="7a41" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了便于可视化，这些问题以树的形式表示，为了对数据进行分类，我们必须从上到下遍历树。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/0d4f636e443965dfdb52e9f989f0837b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukDkUG0SrrHWXW-lTjhbYQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">决策树问题(图片由作者提供)</p></figure><p id="1bf7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">这些类型的树不一定是二进制的，它们也可以是 d 进制的</strong>，但是库的实现经常使用二进制的树，例如 sklearn。</p><h2 id="3f11" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">如何找到最佳切工？</h2><p id="ea58" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">现在出现的问题是<em class="ma">“但是我应该在哪个特征上进行第一次切割，我应该在那个特征上进行切割的值是多少？”</em></p><blockquote class="nk nl nm"><p id="a38f" class="kv kw ma kx b ky kz kh la lb lc kk ld nn lf lg lh no lj lk ll np ln lo lp lq ij bi translated">从树根开始，分割产生最大<strong class="kx jh">信息增益(IG) </strong>的特征数据</p></blockquote><p id="210d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基本上，我们从树中的一个节点开始，首先从根节点开始。每次我们进行分割(剪切)时，数据集中的一些数据将进入该节点的左侧子节点，一些数据将进入右侧子节点。如果这些子节点中的一个现在包含来自我们的数据集的数据，这些数据都具有相同的标签，我们将说该节点是纯的。否则，我们将继续使用其他拆分来拆分节点中的剩余数据集点。一个纯节点被称为树中的一片叶子。尽管一个节点不是纯的，它也可以变成一片叶子。在这种情况下，<strong class="kx jh">与这个节点相关联的标签是它包含在其中的多数类的值。</strong></p><p id="a894" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，如果一个叶子包含 10 个标签为 A 类的数据项，那么这个叶子的值将是 A。但是，如果一个叶子包含 8 个标签为 A 的数据和 2 个标签为 B 的数据，那么这个叶子的值仍然是 A。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/aadef6854279d0a6ae9de06f8604f78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8iExrk1Kx_DOi_RIiwb3Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">决策树分类</p></figure><p id="842a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以现在在推理的时候，当我们被输入一个标签未知的数据项时，我们只需要遍历树，看看我们在哪个叶子上结束。我们将相应地对新数据项进行分类。</p><h2 id="6722" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">什么是纯洁？</h2><p id="3ade" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">我们直观地理解，我们做这些切割或分割是为了增加每个节点的纯度，也就是说，在每个节点上有一个单一类别的数据。<strong class="kx jh">但是纯度或者更确切地说杂质是如何被正式描述的呢</strong>？通常，有两种方法，我们可以使用熵或者基尼系数。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ce70ec1df530d3ad654268e3f6c21acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*GKGy3BlJvZPOSJFgyPVyGA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数学中的杂质(作者图片)</p></figure><p id="9ad8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在每次拆分时，我们都希望减少每个节点的杂质。然而，我们也可以说，随着每一次分裂，我们希望增加信息增益。也就是说，<strong class="kx jh">在每次分割之后，我将拥有越来越多的关于节点的信息，并且我将越来越确信我对它进行了正确的分类</strong>。所以我们可以说，我们想要创建一个<strong class="kx jh">决策树，目标是最大化信息增益(IG) </strong>。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/7be42befc4c53c227a43af4f513726b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXAMQxvLhO1ZQGEVRw1GhA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">DT 中的信息增益(图片由作者提供)</p></figure><h2 id="c3f2" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">过度拟合</h2><p id="093e" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated"><strong class="kx jh">决策树</strong>没有被广泛使用，因为它们<strong class="kx jh">容易过拟合</strong>。事实上，<strong class="kx jh">考虑到您可以对任何类型的数据集进行分类</strong>，您只需进行连续的分割，直到您为输入数据的每一个点创建了一个节点叶。显然这样的 DT 会有<strong class="kx jh">很差的泛化能力。那该怎么办呢？<strong class="kx jh">我们可以设置正则化参数</strong>。例如，我们可以说树的深度不应该超过 5 分，这样我们可以避免过度拟合。最常见的参数如下。</strong></p><ul class=""><li id="7b4a" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq ns lx ly lz bi translated"><em class="ma">树的最大深度</em></li><li id="bb6b" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq ns lx ly lz bi translated"><em class="ma">最小样本分割</em>:一个节点在被分割之前必须拥有的最小样本数</li><li id="b42d" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq ns lx ly lz bi translated"><em class="ma">最小样本叶</em>:一个叶节点必须拥有的最小样本数</li></ul><p id="79ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一种广泛使用的方法是<strong class="kx jh"> <em class="ma">后剪枝</em> </strong>。一旦树被创建，我们去剪一些叶子或子树，所以我们将有一个模型，将有更大的泛化能力。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/0985e230272a84dfa217639db8634e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrRRFIiep_csgiuMNSDPxA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">DT 中的后期剪枝(图片由作者提供)</p></figure><h2 id="7c2f" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">我们来编码吧！</h2><p id="ffd3" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">像往常一样，我们将使用虹膜数据集。数据集由 sklearn 在开放许可下提供，可以在这里找到<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#" rel="noopener ugc nofollow" target="_blank"/>。数据集如下。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nu nv l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">虹膜数据集</p></figure><p id="82d3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">出于可视化的目的，我们将仅使用 Iris 数据集的两个特征。因此，让我们加载并标准化我们的数据。虽然 DT 不需要<strong class="kx jh">特征缩放(酷！)</strong>。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nu nv l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输入数据</p></figure><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="fd66" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，用 2 行简单的代码，我们将创建和适应 oud DT 模型。我们将绘制决策边界，以查看模型是否有效，是否能够对我们的数据进行分类。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="5abe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个非常酷的功能是让你绘制整个决策树，这样模型就不是一个黑盒，但你可以知道它背后发生了什么。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9bd32dc4b232450f1e794c753c464562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*XlR0JxOVEsoxeqS0O2sE0A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">决策树(图片由作者提供)</p></figure><h1 id="2222" class="nx mh jg bd mi ny nz oa ml ob oc od mo km oe kn mr kp of kq mu ks og kt mx oh bi translated">最后的想法</h1><p id="5f48" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">我们来总结一下决策树的特点。</p><p id="3a0d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">优点:</strong></p><ol class=""><li id="17b8" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">清晰的可视化:算法易于理解、解释和可视化。DT 的输出很容易被人理解。</li><li id="b535" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">决策树看起来像 if-else 语句，容易理解</li><li id="2dff" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">用于回归和分类</li><li id="2131" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">可以处理分类变量和连续变量</li><li id="ac1e" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">自动处理缺失值</li><li id="a32e" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">对异常值稳健</li><li id="37c6" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">训练很快</li><li id="e610" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">不需要特征缩放。</li></ol><p id="2369" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">缺点:</strong></p><ol class=""><li id="2531" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">它通常会导致过度拟合。为了拟合数据(即使是有噪声的数据)，它会不断生成新的节点。</li><li id="18a1" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">它不稳定。添加新的数据点会导致整个树的重新生成。</li><li id="a7ea" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">不适合大型数据集。树可能长得太复杂，导致过度拟合。</li></ol><p id="4363" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi">😁</p><h1 id="89df" class="nx mh jg bd mi ny nz oa ml ob oc od mo km oe kn mr kp of kq mu ks og kt mx oh bi translated">结束了</h1><p id="39cd" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated"><em class="ma">马赛洛·波利蒂</em></p><p id="5dca" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>