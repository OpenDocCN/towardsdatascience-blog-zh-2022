<html>
<head>
<title>A Fast Look at Spark Structured Streaming + Kafka</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速浏览Spark结构化流+卡夫卡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325#2022-11-05">https://towardsdatascience.com/a-fast-look-at-spark-structured-streaming-kafka-f0ff64107325#2022-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0aa7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何使用这款强大的duo执行流处理任务的基础知识</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/dc0d116a183395417beacc83617b3450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UFDmADsH4N8WFgMA"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@nikhita?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Nikhita Singhal </a>拍摄的照片</p></figure><h1 id="d9f7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">简介</strong></h1><p id="4b75" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最近，我开始大量研究Apache Kafka和Apache Spark，这是数据工程领域的两项领先技术。</p><p id="937e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在过去的几个月里，我用它们做了几个项目；“<a class="ae kv" href="https://medium.com/p/c5f3996afe8f?source=post_stats_page-------------------------------------" rel="noopener"> <strong class="lq ir">用Kafka、Debezium、BentoML </strong> </a>进行机器学习串流”就是一个例子。我的重点是学习如何使用这些现代著名工具创建强大的数据管道，并了解它们的优缺点。</p><p id="6bfe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在过去的几个月里，我已经讲述了如何使用这两种工具创建ETL管道，但是从来没有一起使用过，这就是我今天要填补的空白。</p><p id="b3e8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们的目标是学习使用Spark+Kafka构建流应用程序背后的一般思想，并使用真实数据快速浏览其主要概念。</p><h1 id="f61f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">卡夫卡与《一言以蔽之的火花》</strong></h1><p id="c7d6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个想法很简单——Apache Kafka是一个消息流工具，生产者在队列的一端编写消息(称为<em class="mp">主题</em>)，供另一端的消费者阅读。</p><p id="d504" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但它是一个非常复杂的工具，旨在成为一个弹性的分布式消息服务，具有各种交付保证(一次、一次、任意)、消息存储和消息复制，同时还允许灵活性、可伸缩性和高吞吐量。它有更广泛的用例，如微服务通信、实时事件系统和流式ETL管道。</p><p id="4162" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Apache Spark是一个分布式的基于内存的数据转换引擎。</p><p id="302e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它也是一个非常复杂的工具，能够连接各种数据库、文件系统和云基础设施。它适合在分布式环境中运行，以在机器之间并行处理，通过使用其惰性评估理念和查询优化来实现高性能转换。</p><p id="889f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最酷的是，到最后，代码只是您通常的SQL查询或(几乎)您的Python+pandas脚本，所有的巫术都抽象在一个友好的高级API下。</p><p id="cbdf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">将这两种技术结合起来，我们就可以完美地构建一个流ETL管道。</p><h1 id="aeea" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">申请</strong></h1><p id="cf24" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将使用来自巴西米纳斯吉拉斯州首府贝洛奥里藏特市交通传感器的数据。这是一个庞大的数据集，包含城市中几个地方的交通流量测量值。每个传感器定期检测在该位置行驶的车辆类型(汽车、摩托车、公共汽车/卡车)、速度和长度(以及其他我们不会使用的信息)。</p><p id="02af" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该数据集精确地代表了流式系统的经典应用之一——一组从现场连续发送读数的传感器。</p><p id="2ccd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这种情况下，Apache Kafka可以用作传感器和使用其数据的应用程序之间的抽象层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/957b2c43ff01842ca116e7a5fb4c734b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBWFPXJiL8nNeTGSkPOw_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Kafka用作源和服务之间的抽象层。图片作者。</p></figure><p id="1565" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有了这种基础设施，就有可能建立各种(所谓的)<a class="ae kv" href="https://www.kai-waehner.de/blog/2022/09/29/real-time-logistics-shipping-transportation-with-apache-kafka/" rel="noopener ugc nofollow" target="_blank">实时事件驱动系统</a>，就像一个程序，当车辆数量突然增加而平均速度下降时，它可以检测并警告交通堵塞。</p><p id="55de" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这就是Apache Spark发挥作用的地方。</p><p id="cb19" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它有一个名为<em class="mp"> Spark结构化流</em>的本地流处理模块，可以连接到Kafka并处理其消息。</p><h2 id="e405" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">设置环境</strong></h2><p id="e324" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">你所需要的是docker和docker-compose。</p><p id="b208" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将使用基于以下存储库的docker-compose文件配置:<a class="ae kv" href="https://hub.docker.com/r/bitnami/spark" rel="noopener ugc nofollow" target="_blank"> link spark </a>，<a class="ae kv" href="https://hub.docker.com/r/bitnami/kafka" rel="noopener ugc nofollow" target="_blank"> link kafka </a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="94f2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">。/src </strong>卷是我们放置脚本的地方。</p><p id="bddc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要启动环境，只需运行</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="6bf3" class="mr kx iq ng b gy nk nl l nm nn">docker-compose up</span></pre><p id="eefa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所有代码都可以在这个<a class="ae kv" href="https://github.com/jaumpedro214/traffic-flow-spark-kafka" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到。</p><h1 id="28b7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">实施</strong></h1><p id="8ae3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">开始研究Spark时，我最喜欢的一件事是它的编写代码和我通常使用的python+pandas脚本之间的相似性。迁移是非常容易的。</p><p id="0e91" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">遵循同样的逻辑，spark的流模块与常见的Spark代码非常相似，这使得从批处理应用程序迁移到流应用程序变得很容易。</p><p id="067b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">也就是说，在接下来的几节中，我们将重点学习Spark结构化流的特性，即它有哪些新功能。</p><h2 id="1c40" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">我们的第一份工作</strong></h2><p id="85ad" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们慢慢开始，建立一个玩具的例子</p><p id="ee8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先要做的是创建一个Kafka主题，我们的spark作业将从这个主题中消费消息。</p><p id="11bd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是通过<a class="ae kv" href="https://stackoverflow.com/questions/30172605/how-do-i-get-into-a-docker-containers-shell" rel="noopener ugc nofollow" target="_blank">访问Kafka容器终端</a>并执行:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="c54c" class="mr kx iq ng b gy nk nl l nm nn">kafka-topics.sh --create --bootstrap-server localhost:9092 --topic test_topic</span></pre><p id="d555" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了模拟一个制作人在这个主题上写消息，让我们使用<em class="mp">卡夫卡-控制台-制作人。</em>同样在集装箱内:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="2b60" class="mr kx iq ng b gy nk nl l nm nn">kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test_topic --property "parse.key=true" --property "key.separator=:"</span></pre><p id="b4ae" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从现在开始，在终端中键入的每一行都将作为消息发送到测试主题。字符“:”用于分隔消息的键和值(键:值)。</p><p id="59f0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们创造一个火花工作来消费这个话题。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="fbfe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">代码需要放在<strong class="lq ir"> /src/streaming </strong>文件夹中(没什么特别的，只是我选择的文件夹)。</p><p id="2ec4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">需要注意的关键是，我们使用属性<em class="mp"> readStream </em>和<em class="mp"> writeStream，</em>来代替普通的读写。这是Spark将我们的工作视为流媒体应用的主要原因。</p><p id="ac1f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要连接Kafka，需要指定服务器和主题。选项<em class="mp">starting offsets = "</em>earliest "告诉Spark从头开始阅读主题。此外，因为Kafka以二进制形式存储它的消息，它们需要被解码成字符串。</p><p id="7df4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">将进一步探讨其他选择。</p><p id="ac06" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们访问Spark容器并运行作业。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="7f24" class="mr kx iq ng b gy nk nl l nm nn">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /src/streaming/read_test_stream.py</span></pre><p id="33bd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">经过几秒钟的配置，它就开始消费话题了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/bec9e57d85d86e3e39d6f4eceda7d676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wweQ774b6UvKCtF7_zw_bg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自卡夫卡的火花消费信息。图片作者。</p></figure><p id="0c8d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Spark Streaming在<em class="mp">微批处理</em>模式下工作，这就是为什么当它使用消息时我们会看到“批处理”信息。</p><blockquote class="np nq nr"><p id="6dd7" class="lo lp mp lq b lr mk jr lt lu ml ju lw ns mm lz ma nt mn md me nu mo mh mi mj ij bi translated">微批处理在某种程度上介于完全“真实”的流和普通的批处理之间，前者所有消息在到达时被单独处理，后者数据保持静态并按需使用。Spark将等待一段时间，尝试累积消息来一起处理它们，从而减少开销并增加延迟。这可以根据你的需要进行调整。</p></blockquote><p id="99f8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我打字不是很快，所以Spark会先处理消息，然后才能在当前批处理中包含新消息。</p><p id="7849" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是我们的第一个流媒体工作！</p><p id="b086" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我希望您有这样的感觉:编写一个流处理作业并不难，但是有一些问题。</p><h2 id="f0be" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">向Kafka流写入数据</strong></h2><p id="b7cf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在是时候开始处理传感器数据了。</p><p id="5af5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">可以从2022年8月开始下载<em class="mp"> zip </em>文件，解压到<strong class="lq ir"> /data </strong>卷。数据最初是在JSON中，需要大约23Gb的空间。首先要做的是将其转换为parquet，以优化磁盘空间和读取时间。</p><p id="9550" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">GitHub资源库中详细介绍了完成这项工作的spark作业，您需要做的就是执行它们:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="fdb4" class="mr kx iq ng b gy nk nl l nm nn">spark-submit /src/transform_json_to_parquet.py</span><span id="6256" class="mr kx iq ng b gy nv nl l nm nn">spark-submit /src/join_parquet_files.py</span></pre><p id="806c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据您的机器，执行可能需要一些时间。但这是值得的，最终的parquet文件大小约为1Gb(比原来小20倍以上),读取速度也快得多。</p><p id="c1af" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还需要创建Kafka主题来接收我们的消息:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="17d9" class="mr kx iq ng b gy nk nl l nm nn">kafka-topics.sh --create --replication-factor 1 --bootstrap-server localhost:9092 --topic traffic_sensor</span></pre><p id="32b7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者，如果您想显示到达的消息，可以设置一个控制台消费者。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="eb6b" class="mr kx iq ng b gy nk nl l nm nn">kafka-console-consumer.sh --topic traffic_sensor --bootstrap-server localhost:9092</span></pre><p id="127e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">以Kafka为主题写数据很容易，但是有一些细节。</p><p id="280b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在结构化流中，默认行为是不尝试推断数据模式(列及其类型)，所以我们需要传递一个。</p><p id="9207" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Kafka消息只是键值二进制字符串对，所以我们需要用这种格式表示我们的数据。这可以通过将所有行转换为JSON字符串、以二进制编码并将结果存储在“value”列中来轻松实现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/a94137d24b7e07227e282baa1fb00528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6Jeo8X4bpuuw-sEOwNkNQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将列转换为JSON字符串。图片作者。</p></figure><p id="dc37" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">消息键在Kafka中非常重要，但是在我们的测试中没有用，所以所有的消息都有相同的键。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="c143" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如前所述，这个数据集非常大，所以我将插入的消息数量限制为500，000。</p><p id="5f5e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们传递Kafka服务器和主题以及一个“<em class="mp"> checkpointLocation </em>”，spark将在其中存储执行进度，这对从错误中恢复很有用。</p><p id="164c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">执行作业:</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="93d1" class="mr kx iq ng b gy nk nl l nm nn">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /src/streaming/insert_traffic_topic.py</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/c770d8bc07fdfa72ec7ddbe2105a4cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5wVsoCale5SG7CZQXoqtIQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">将数据插入卡夫卡。图片作者。</p></figure><p id="ff7a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在左边，Spark作业读取文件，在右边，一个<em class="mp">Kafka-控制台-消费者</em>显示到达的消息。</p><p id="a799" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们的流量主题已经填充完毕，几乎可以处理了。</p><p id="a9e9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">重要的是要记住，我们使用spark作业来填充我们的主题只是出于学习目的。在真实场景中，传感器本身会直接向卡夫卡发送读数。</p><p id="2865" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了模拟这种动态行为，下面的脚本每2.5秒向主题中写入一行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="1f68" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">输出模式——按类型统计车辆数量</strong></h2><p id="e6cd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">接下来，让我们创建一个按类型统计车辆数量的作业。</p><p id="2d06" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“分类”栏包含检测到的车辆类型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="43bf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当我们阅读主题时，我们需要将JSON二进制字符串转换回列格式。</p><p id="96bb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦完成，就可以照常构建查询了。有趣的是，查询心脏正好是<em class="mp">选择</em>()。<em class="mp">分组依据</em>()。<em class="mp">计数</em>()序列，其余都是相对于流式逻辑。</p><p id="61ea" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以是时候解决<em class="mp">输出模式</em>()选项了。</p><p id="c923" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">流应用程序的输出模式指定了当新数据到达时，我们希望如何(重新)计算和写入结果。</p><p id="4070" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它可以假设三个不同的值:</p><ul class=""><li id="dbdb" class="ny nz iq lq b lr mk lu ml lx oa mb ob mf oc mj od oe of og bi translated"><strong class="lq ir">追加</strong>:仅向输出添加新记录。</li><li id="cb2e" class="ny nz iq lq b lr oh lu oi lx oj mb ok mf ol mj od oe of og bi translated"><strong class="lq ir">完成</strong>:重新计算每个新记录的完整结果。</li><li id="7017" class="ny nz iq lq b lr oh lu oi lx oj mb ok mf ol mj od oe of og bi translated"><strong class="lq ir">更新</strong>:更新变更记录。</li></ul><p id="0775" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些模式可能有意义，也可能没有意义，这取决于编写的应用程序。例如，如果执行任何分组或排序，“完整”模式可能没有意义。</p><p id="8096" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们在“完成”模式下执行作业，并查看结果。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="e46b" class="mr kx iq ng b gy nk nl l nm nn">spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 /src/streaming/group_by_vehicle_type.py</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/e51838bbd594c4134ee333761fb8d4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PhvegRuJQ_gHd5LA_XQfgg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卡车、汽车、未定义的汽车、公共汽车、摩托车。图片作者。</p></figure><p id="7b28" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">随着新记录被插入到流中(见右边的终端)，作业重新计算完整的结果。这在行排序很重要的情况下很有用，比如排名或竞争。</p><p id="33c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，如果组的数量太大或者单个的改变不影响整体结果，这种方法可能不是最佳的。</p><p id="805a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，另一种选择是使用“更新”输出模式，它只为发生变化的组生成新消息。见下文:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/52dc899c983ac2db44b19c53caa1729e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9bEfRCttIRqZMLmdu97LUA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出模式为“更新”的查询。图片作者。</p></figure><p id="3d63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">“append”模式不适用于分组查询，因此我将无法使用相同的作业进行展示。但我认为这是最简单的模式，它总是向输出中添加一条新记录。</p><p id="0f46" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果考虑将结果保存到表中，这些输出模式会更容易理解。在完整输出模式中，对于处理的每个新消息，表都将被重写；在更新模式中，只重写发生了某些更新的行，append总是在末尾添加一个新行。</p><h2 id="3deb" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">翻转时间窗口—使用时间间隔聚合</strong></h2><p id="981c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在流系统中，消息有两个不同的相关时间戳:事件时间(创建消息的时间，在我们的示例中是传感器的读取时间)和处理时间(处理代理读取消息的时间，在我们的示例中是消息到达Spark的时间)。</p><p id="0c11" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">流处理工具的一个重要特性是处理事件时间的能力。滚动窗口是不重叠的固定时间间隔，用于使用事件时间列进行聚合。更简单地说，他们将时间线分割成大小相等的片段，这样每个事件都属于一个时间间隔。</p><p id="360b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">例如，每5分钟计算一次，在过去5分钟内检测到多少辆车。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/346d0c3fbadb89eaf2ce88bd222d928f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0PL9CfUaZEeEFFDsPIwmg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">5分钟翻滚窗口。图片作者。</p></figure><p id="6734" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">下面的代码说明了这一点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/fb416f237b3ba01b0327e717adab64b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmkhE5grg117UXZH2gM4aQ.png"/></div></div></figure><p id="037c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这种处理在许多情况下非常有用。回到之前提出的交通堵塞检测器，一种可能的方法是测量10分钟窗口内车辆的平均速度，并查看它是否低于某个阈值。</p><blockquote class="np nq nr"><p id="7bca" class="lo lp mp lq b lr mk jr lt lu ml ju lw ns mm lz ma nt mn md me nu mo mh mi mj ij bi translated">排气时间处理是一个复杂的话题。在处理它的时候，任何事情都可能发生，比如消息丢失、到达太晚或者顺序混乱。Spark有几种机制来尝试缓解这些问题，比如<a class="ae kv" href="https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html" rel="noopener ugc nofollow" target="_blank">水印</a>，我们不会重点讨论。</p></blockquote><p id="c270" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">时间窗口也可以与<em class="mp"> groupBy() </em>中的其他列结合使用。以下示例按类型统计了5分钟窗口内的车辆数量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/7334514bc815eeb30ebba1e87788f333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OqZr1t_CZW7Xn3VUk0Iiw.png"/></div></div></figure><h2 id="3827" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated"><strong class="ak">滑动时间窗——时间间隔的灵活性</strong></h2><p id="862a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">滑动时间窗口是滚动窗口的灵活化。它们允许定义创建每个间隔的频率，而不是创建不重叠的间隔。</p><p id="7d8f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">例如，每隔5分钟，计算在过去30分钟内检测到多少车辆。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/da0e355dd416ce27f98dbc73db91284c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRdvw0zLcjmokObRQEvWfA.png"/></div></div></figure><p id="c2bf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，事件可以属于多个时间间隔，并根据需要进行多次计数。</p><p id="d54a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要定义滑动窗口，只需将更新间隔传递给<em class="mp">窗口</em>()函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="d3a5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们看看输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/48f04b82180068b504d83fec682e9433.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*Bw5rwN4mDvP8Nbq2ORkeLA.png"/></div></div></figure><p id="339a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如我们所见，每5分钟就有30分钟的窗口被创建。</p><p id="0ae3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这种灵活性对于定义更具体的业务规则和更复杂的触发器非常有用。例如，我们的交通堵塞检测器可以在过去10分钟内每5秒钟发送一次响应，并在平均车速低于20公里/小时时发出警报</p><h1 id="f72b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="83b2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是对Spark结构化流的主要概念以及它们如何应用于Kafka的快速浏览。</p><p id="7d8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Apache Kafka和Apache Spark都是可靠和健壮的工具，许多公司使用它们来处理难以置信的大量数据，这使它们成为流处理任务中最强的一对。</p><p id="3241" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们已经学习了如何使用Spark jobs填充、消费和处理Kafka主题。这不是一个困难的任务，正如在帖子中提到的，流处理API几乎等同于通常的批处理API，只是做了一些小的调整。</p><p id="4871" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还讨论了不同的输出模式，特定于流应用程序的内容，以及每种模式的使用方法。最后但同样重要的是，我们探索了带时间窗的聚合，这是流处理的主要功能之一。</p><p id="1790" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">同样，这只是快速浏览，如果你想更深入地探索，我会在下面留下一些参考资料。</p><p id="4bff" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">希望我有所帮助，感谢您的阅读！:)</p><h1 id="0646" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><blockquote class="np nq nr"><p id="dc45" class="lo lp mp lq b lr mk jr lt lu ml ju lw ns mm lz ma nt mn md me nu mo mh mi mj ij bi translated"><em class="iq">所有的代码都在这个</em> <a class="ae kv" href="https://github.com/jaumpedro214/traffic-flow-spark-kafka" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> GitHub资源库</em> </a> <em class="iq">中。<br/>使用的数据—</em><a class="ae kv" href="https://dados.gov.br/dataset/contagens-volumetricas-de-radares" rel="noopener ugc nofollow" target="_blank">Contagens volumétricas de Radares</a>，<a class="ae kv" href="https://opendefinition.org/licenses/cc-by/" rel="noopener ugc nofollow" target="_blank">公开数据</a>，巴西政府。</p></blockquote><p id="582c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[1] <a class="ae kv" href="https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html" rel="noopener ugc nofollow" target="_blank">功能深度挖掘:Apache Spark结构化流中的水印</a> — Max Fisher在Databricks博客上的文章<br/> [2] Chambers，b .，&amp; Zaharia，M. (2018)。Spark:权威指南:简化大数据处理。奥莱利媒体公司。<br/>【3】<a class="ae kv" href="https://www.kai-waehner.de/blog/2022/09/29/real-time-logistics-shipping-transportation-with-apache-kafka/" rel="noopener ugc nofollow" target="_blank">与阿帕奇·卡夫卡</a> <a class="ae kv" href="https://docs.ksqldb.io/en/latest/tutorials/etl/" rel="noopener ugc nofollow" target="_blank"> </a>实时物流、海运、运输——凯·沃纳<br/>【4】<a class="ae kv" href="https://www.confluent.io/blog/how-kafka-is-used-by-netflix/" rel="noopener ugc nofollow" target="_blank">以阿帕奇·卡夫卡为特色的网飞工作室和金融世界</a>——汇流博客<br/>【5】星火流媒体&amp;卡夫卡——<a class="ae kv" href="https://sparkbyexamples.com/" rel="noopener ugc nofollow" target="_blank">https://sparkbyexamples.com/</a></p></div></div>    
</body>
</html>