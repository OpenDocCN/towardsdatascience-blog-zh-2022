<html>
<head>
<title>Understanding min_child_weight in Gradient Boosting Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解梯度推进决策树中的最小子权重</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-min-child-weight-in-gradient-boosting-decision-trees-293f2381306#2022-05-18">https://towardsdatascience.com/understanding-min-child-weight-in-gradient-boosting-decision-trees-293f2381306#2022-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a18" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">你真的知道这个超参数是怎么工作的吗？</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/160c1f1c3c8acc3d458dd366b1aa2521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QerUz0DCAX5HktMscvVvtA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@zuzi_ruttkay?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Zuzana Ruttkay </a>在<a class="ae ky" href="https://unsplash.com/s/photos/understanding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="c1a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">建模工作中最具创造性和神秘的一个方面是超参数设置。这也是许多人在没有真正试图理解细节的情况下，随意遵循指南的地方。如果你恰好是这些人中的一员，那就让我们改变这一切吧！</p><p id="8021" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将深入探讨名为<code class="fe lv lw lx ly b">min_child_weight</code>的超参数。在我倾向于使用的GBDT包LightGBM中，这个参数有几个其他的别名，我个人认为<code class="fe lv lw lx ly b">min_sum_hessian</code>是一个更好的名字，这也是我们实际上将要使用的名字。</p><p id="5e8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将:</p><ul class=""><li id="01b4" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">看看可用的文档，</li><li id="4754" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">定义如何为三个不同的模型目标计算hessian，</li><li id="95b0" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">总结经验法则，帮助用户掌握min_sum_hessian参数的大小，</li><li id="6e8a" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">最后，收集一些更棘手的、不太广为人知的方面。</li></ul><p id="a126" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在示例中使用的完整脚本可以在我的<a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_min_sum_hessian.R" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。这个例子是在R和LightGBM中，但是自然也适用于Python / XGBoost。</p><h1 id="6380" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">文档怎么说？</h1><h2 id="80ed" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">LightGBM</h2><p id="3aeb" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">让我们看看<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>对这个超参数有什么看法。与LightGBM一样，同一个参数使用了一堆不同的别名，这些别名都代表同一个东西:<code class="fe lv lw lx ly b">min_sum_hessian_in_leaf</code>、<code class="fe lv lw lx ly b">min_sum_hessian_per_leaf</code>、<code class="fe lv lw lx ly b">min_sum_hessian</code>、<code class="fe lv lw lx ly b">min_hessian</code>和<code class="fe lv lw lx ly b">min_child_weight</code>。</p><p id="bd6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LightGBM文档对此超参数的定义是:“<em class="nw">一片叶子中的最小和hessian”</em>。我想我们都同意，这并不完全有用...</p><h2 id="6046" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">XGBoost</h2><p id="c969" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">XGBoost文档更有帮助(根据我的经验，这种情况很常见)。这是定义:</p><p id="f5bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nw">“一个孩子所需的最小实例体重总和(黑森)。如果树划分步骤导致实例权重之和小于</em> <code class="fe lv lw lx ly b"><em class="nw">min_child_weight</em></code> <em class="nw">的叶节点，那么构建过程将放弃进一步的划分。在线性回归任务中，这只是对应于每个节点中需要的最小实例数。</em> <code class="fe lv lw lx ly b"><em class="nw">min_child_weight</em></code> <em class="nw">越大，算法就越保守。”</em></p><p id="51fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为这是一个很好的总结。在树发展的每一点上，数据中的每一个观察值都有一个所谓的hessian值。为了考虑分裂，两个孩子的观察值必须具有至少与超参数值一样高的hessian值之和。</p><p id="de85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我想我们现在唯一需要弄清楚的就是所谓的黑森是什么。</p><h1 id="d472" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">观测值的海森值</h1><h2 id="73b5" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">理论</h2><p id="6782" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">来自XGBoost 的这个<a class="ae ky" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">文档给出了一个很好的数学背景的高层次总结，即使它跳过了一些概念，比如学习率。</a></p><p id="7106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论上，hessian应该是基于实际值和预测值的损失函数相对于预测值的二阶导数。一阶导数叫做梯度。</p><p id="a05e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设您想要最小化平方差，那么您正在使用默认回归模型:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="2227" class="nf mo it ly b gy ob oc l od oe">loss: (actual_value - predicted_value) ^ 2<br/>gradient: - 2 * (actual_value - predicted_value)<br/>hessian: 2</span></pre><p id="ef57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这种理论方法在实践中并不准确…</p><h2 id="f14f" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">实践</h2><p id="91ef" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">我认为对黑森的一个更有用的定义是:</p><blockquote class="of"><p id="583c" class="og oh it bd oi oj ok ol om on oo lu dk translated">特定目标的hessian是实际值和预测值的函数，正如您正在使用的梯度增强包的源代码中所定义的那样。</p></blockquote><p id="5133" class="pw-post-body-paragraph kz la it lb b lc op ju le lf oq jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">查看软件包的源代码总是一个好主意，而不是假设可能会发生什么。对于LightGBM，目标函数存储在GitHub 的这个<a class="ae ky" href="https://github.com/microsoft/LightGBM/tree/master/src/objective" rel="noopener ugc nofollow" target="_blank">文件夹中。假设我们正在寻找回归目标，这些都在这个</a><a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank">脚本</a>中。</p><p id="63b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(LightGBM命名我们需要牢记:<code class="fe lv lw lx ly b">label</code>是实际值，<code class="fe lv lw lx ly b">score</code>是预测值。如果目标使用链接函数(例如，具有对数链接的泊松)，两者都是在应用链接函数之后。)</p><h2 id="1a00" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">重量的快速说明</h2><p id="62d1" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">以下<strong class="lb iu">仅适用于未加权的例子。</strong>如果您传递一个权重向量，单个hessian值将与它相乘，并且它将反馈到<code class="fe lv lw lx ly b">min_sum_hessian</code>参数调节的方式中。</p><h2 id="a081" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">L2回归目标</h2><p id="cbea" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">事不宜迟，这就是我们在<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank">代码</a>中寻找的部分(在撰写本文时大约在第440行):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou ov l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">L2回归梯度和海森计算</p></figure><p id="54b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，他们简化了我们上面计算的公式，并将梯度和hessian除以2。</p><blockquote class="ow ox oy"><p id="a7d8" class="kz la nw lb b lc ld ju le lf lg jx lh oz lj lk ll pa ln lo lp pb lr ls lt lu im bi translated">L2回归目标中观测值的海森常数是1。经验法则非常简单:min_sum_hessian实际上意味着这个目标的观察次数。如果将min_sum_hessian设置为X，树将只在两边至少有X个观察值的地方进行切割。</p></blockquote><h2 id="cc26" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">泊松目标</h2><p id="073e" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">现在，让我们看看一个更困难的目标。</p><p id="4f4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设观察值遵循泊松分布的可能性(来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Poisson_distribution" rel="noopener ugc nofollow" target="_blank">泊松分布</a>函数):</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="274c" class="nf mo it ly b gy ob oc l od oe">likelihood = predicted_value ^ actual_value * <br/>exp(-predicted_value) / factorial(actual_value)</span></pre><p id="b148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数可能性:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="4b38" class="nf mo it ly b gy ob oc l od oe">log-likelihood = actual_value * log(predicted_value) - predicted_value - log(factorial(actual_value))</span></pre><p id="3197" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们实际上并没有对预测值进行建模，该模型假设了最终预测值和原始分数之间的对数关系。(是的，这里有几个不同的日志，必须小心！)我们可以用下面的表达式重写上面的公式:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="9a84" class="nf mo it ly b gy ob oc l od oe">raw_score = log(predicted_value)</span></pre><p id="8a63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">收件人:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="03fb" class="nf mo it ly b gy ob oc l od oe">log-likelihood = actual_value * raw_score - exp(raw_score) - log(factorial(actual_value))</span></pre><p id="29eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">损失函数就是对数似然乘以-1:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="d3ec" class="nf mo it ly b gy ob oc l od oe">loss = - actual_value * raw_score + exp(raw_score) + log(factorial(actual_value))</span></pre><p id="009c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由<code class="fe lv lw lx ly b">raw_score</code>取损失函数的一阶和二阶导数来分别计算梯度和hessian，这就是我们得到的结果:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="552a" class="nf mo it ly b gy ob oc l od oe">gradient = exp(raw_score) - actual_value</span><span id="7cd3" class="nf mo it ly b gy pc oc l od oe">hessian = exp(raw_score)</span></pre><p id="ffc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们将其与<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank">源代码</a>进行比较(在撰写本文时大约在第440行):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou ov l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">泊松回归梯度和海森计算</p></figure><p id="1cb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常接近我们得出的结果，但是公式中的<code class="fe lv lw lx ly b">max_delta_step</code>是什么？原来XGBoost和LightGBM都在他们的泊松海森计算中加入了一个隐藏的额外因素。<code class="fe lv lw lx ly b">max_delta_step</code>默认为0.7。这增加了hessian，因为我们要用它来划分，以得出新的预测，这将导致一个更保守的树构建，步长更小。本质上与使用较低的<code class="fe lv lw lx ly b">learning_rate</code>是一样的。</p><blockquote class="ow ox oy"><p id="ac17" class="kz la nw lb b lc ld ju le lf lg jx lh oz lj lk ll pa ln lo lp pb lr ls lt lu im bi translated">泊松回归目标中观测值的hessian值是当前预测值乘以exp(0.7) ≈ 2。如果将min_sum_hessian设置为X，树将只在两边至少有X / 2个预测总值的地方进行切割。</p></blockquote><p id="e965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，<code class="fe lv lw lx ly b">min_sum_hessian</code>在不同的回归目标之间具有完全不同的含义！</p><h2 id="fa5f" class="nf mo it bd mp ng nh dn mt ni nj dp mx li nk nl mz lm nm nn nb lq no np nd nq bi translated">二元目标</h2><p id="f832" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">让我们看看另一个共同的目标，二元分类，这是迄今为止最复杂的三个，我们将在这里考虑。(如果你感兴趣，我写了另一篇文章来介绍这个目标的更多细节。我们现在只关注黑森人。)</p><p id="313e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要从损失函数开始。对于任何给定的y观察值，其中y可以是0或1，模型将预测(0，1)范围内的概率p(y)。我们希望最小化的损失函数:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="d9b4" class="nf mo it ly b gy ob oc l od oe">loss = - y * log(p(y)) - (1-y) * log(1-p(y))</span></pre><p id="2a0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是所谓的日志丢失功能。对于为1的观察值，它是<code class="fe lv lw lx ly b">log(p(y))</code>，对于为0的观察值，它是<code class="fe lv lw lx ly b">log(1-p(y)))</code>。</p><p id="6a75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，就像泊松分布一样，虽然最终产品是p(y)值，但该模型不会与p(y)本身一起工作，而是与一个称为sigmoid的链接函数一起工作。模型实际适合的<code class="fe lv lw lx ly b">raw_score</code>是:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="1013" class="nf mo it ly b gy ob oc l od oe">raw_score = log(p(y) / (1-p(y))) / sigma</span></pre><p id="d615" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中sigma是一个常量参数，默认值为0.7。</p><p id="a937" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上面的公式，</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="9122" class="nf mo it ly b gy ob oc l od oe">p(y) = exp(raw_score * sigma) / (1 + exp(raw_score * sigma))</span></pre><p id="4619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要在原始损失函数中插入这个表达式，以获得梯度增强算法将使用的格式。为了简单起见，让我们集中于取值为1的观察值，即损失函数的前半部分。</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="7030" class="nf mo it ly b gy ob oc l od oe">loss_positive = - y * log(p(y)) = - log(exp(raw_score * sigma) / (1 + exp(raw_score * sigma)))</span></pre><p id="6259" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度是上述公式相对于<code class="fe lv lw lx ly b">raw_score</code>的导数。在2-3步的算术魔术中，我们得到梯度:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="e3d2" class="nf mo it ly b gy ob oc l od oe">gradient_positive = - sigma / (1 + exp(raw_score * sigma))</span></pre><p id="d8c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">取二阶导数，我们得到了hessian:</p><pre class="kj kk kl km gt nx ly ny nz aw oa bi"><span id="de4d" class="nf mo it ly b gy ob oc l od oe">hessian_positive = sigma ^ 2 * exp(raw_score * sigma) / ((1 + exp(raw_score * sigma)) ^ 2)</span></pre><p id="c944" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/d517ba12f2e7862ac533908304dddbd770655d2b/src/objective/binary_objective.hpp" rel="noopener ugc nofollow" target="_blank">源代码</a>！相关部分(在撰写本文时大约在第105行):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou ov l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二元分类梯度和Hessian计算</p></figure><p id="f57f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与L2和泊松目标不同，这有点难以分辨，但是公式与我们计算的公式相同(对于阳性情况)。查看我的<a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_binary.R" rel="noopener ugc nofollow" target="_blank">二进制分类脚本</a>的结尾，进行一些检查。</p><p id="096e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不知道你怎么想，但我很难理解在这种情况下黑森的强度应该是多少，所以我做了一个图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/7a756152238e2368c0622fc973ff2d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmbQW_Tvf0dtfTpEAy_P-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在二元对数损失计算中作为概率函数的Hessian</p></figure><p id="cb36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于损失函数的对称性质，对于取值为0的观测值，我们不必重复它。</p><blockquote class="ow ox oy"><p id="eb7a" class="kz la nw lb b lc ld ju le lf lg jx lh oz lj lk ll pa ln lo lp pb lr ls lt lu im bi translated">二元分类目标中观察值的hessian是当前预测概率的函数。在p = 0.5时，取最高值0.1225。其含义非常有趣:相同的min_sum_hessian参数可能会限制更接近0或1的概率预测，但不会限制更接近0.5的概率预测。换句话说，在预测值接近0.5的情况下，即模型对分类不确定的情况下，模型在进行分割时是最宽松的。</p></blockquote><h1 id="e245" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">你知道吗…？</h1><p id="c3fb" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">为了证明我的观点<code class="fe lv lw lx ly b">min_sum_hessian</code>不容易理解，这里有一些我认为经常被忽略的评论。</p><p id="99a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lv lw lx ly b">min_sum_hessian</code>是…</p><ol class=""><li id="b5e7" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu pe mf mg mh bi translated">…与一片叶子中的数据数量不同，并且取决于目标，它甚至可能与观察的数量不成比例。</li><li id="9db5" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu pe mf mg mh bi translated">…并不是在过程的每一步都以相同的方式进行调整-随着您添加更多的树，相同数据点的hessia可能会发生变化，这取决于您的目标，这意味着同一组观察值的hessia总和可能会发生变化，无论它们是在<code class="fe lv lw lx ly b">min_sum_hessian</code>之上还是之下。</li><li id="1fa1" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu pe mf mg mh bi translated">…如果在计算中加入重量矢量，情况会更复杂。需要记住的事实是，单个的hessian值会乘以权重。</li><li id="ef48" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu pe mf mg mh bi translated">…仍然是关于树枝和树叶大小的最佳测量方法，没有其他超参数可以让您简单地限制观察次数。看似让您限制观察次数的超参数实际上是基于hessian值的估计。</li></ol><div class="pf pg gp gr ph pi"><a href="https://matepocs.medium.com/membership" rel="noopener follow" target="_blank"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd iu gy z fp pn fr fs po fu fw is bi translated">加入我的推荐链接-伴侣概念</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">matepocs.medium.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw ks pi"/></div></div></a></div></div></div>    
</body>
</html>