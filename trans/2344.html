<html>
<head>
<title>What Are Transposed Convolutions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是转置卷积？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-transposed-convolutions-2d43ac1a0771#2022-05-23">https://towardsdatascience.com/what-are-transposed-convolutions-2d43ac1a0771#2022-05-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="421e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解“反向”卷积的工作原理</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bf1e294f9271ac4d3a965b9fe30a188c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ztzwOvIusdz5Rm_z"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@filip42?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Philippe D. </a>拍摄的照片</p></figure><h1 id="7d1a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="1338" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">转置卷积就像卷积家族的“<em class="mn">丑小鸭</em>”。它们古怪而怪异，但转置卷积并不像看上去那样简单。</p><p id="faba" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">你会经常在<em class="mn">自动编码器</em>的<em class="mn">解码器</em>部分，或者<em class="mn"> GANs </em>的<em class="mn">生成器</em>部分找到<strong class="lt iu">层转置卷积</strong>。</p><p id="0b24" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">他们<strong class="lt iu">的目的</strong>很明确:为了<strong class="lt iu">增加他们输入</strong>的尺寸、高度和宽度。</p><p id="1362" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这是与<em class="mn">常规卷积</em>的相反的<em class="mn">:缩小它们输入的大小、高度和宽度。</em></p><blockquote class="mt mu mv"><p id="81b1" class="lr ls mn lt b lu mo ju lw lx mp jx lz mw mq mc md mx mr mg mh my ms mk ml mm im bi translated">"所以，转置卷积是卷积的逆运算？"</p></blockquote><h1 id="b5f4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">转置卷积</h1><p id="fe24" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">上面的问题没有简单的答案，因为<strong class="lt iu">卷积不能“<em class="mn">还原</em>”</strong>，也就是说，在给定结果输出的情况下，不能检索原始值。即使是最简单的过滤器，即<em class="mn">身份</em>过滤器，在图像上进行卷积时，也会导致<strong class="lt iu">一些信息永久丢失</strong>(原始图像最外面的行和列中的数字，如下例所示):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/d2b5fdf0d03da3577f075cf34062ddcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEvLW8SoeUqTkNZgZNFCNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:图片来自我的书《用PyTorch一步一步进行深度学习》，第5章，“卷积”</p></figure><p id="2a68" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">此外，已知卷积通常产生尺寸<strong class="lt iu">减小</strong>(高度和重量)的输出。虽然我们对丢失的信息无能为力，但是我们可以恢复缩小的大小！这就是一个<strong class="lt iu">“转置卷积”</strong>的作用。</p><blockquote class="mt mu mv"><p id="b7a7" class="lr ls mn lt b lu mo ju lw lx mp jx lz mw mq mc md mx mr mg mh my ms mk ml mm im bi translated">“为什么叫换位？”</p></blockquote><p id="b764" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我也不喜欢这个名字，但是我估计叫它“<em class="mn">反卷积</em>也不够准确，所以就是这样。退一步说，转置卷积的<strong class="lt iu">算法并不那么直观</strong>，所以让我们用PyTorch做几个例子来理解它是如何工作的。</p><p id="b1b6" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这两个例子中，我们将使用一个<strong class="lt iu">单通道、2x2图像作为输入</strong>，以及一个同样大小的<strong class="lt iu">内核/过滤器</strong>:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="1ffa" class="nf la it nb b gy ng nh l ni nj">input_image = torch.ones((1, 1, 2, 2)) # N, C, H, W<br/>input_image<br/><strong class="nb iu">Output:<br/>tensor([[[[1., 1.],<br/>          [1., 1.]]]])</strong></span><span id="0c57" class="nf la it nb b gy nk nh l ni nj">kernel_size = 2<br/>kernel = torch.ones((1, 1, kernel_size, kernel_size))<br/>kernel<br/><strong class="nb iu">Output:<br/>tensor([[[[1., 1.],<br/>          [1., 1.]]]])</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fff8b75fab2833171495837562bf8088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*ZTfzTrboGKgmuZ4FxiKQIQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单通道、2x2、图像作为输入，以及相同大小的内核/滤波器。图片作者。</p></figure><h2 id="8fde" class="nf la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">使用1的步幅(转置)</h2><p id="d0ee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，让我们尝试一个2D转置卷积(<code class="fe nx ny nz nb b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.conv_transpose2d.html" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">F.conv_transposed2d</strong></a></code>，在PyTorch的函数式API中)，使用一个的<strong class="lt iu">步幅(转置)，和一个<strong class="lt iu">零</strong>的填充(转置):</strong></p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="7e98" class="nf la it nb b gy ng nh l ni nj">import torch.nn.functional as F</span><span id="2dfb" class="nf la it nb b gy nk nh l ni nj">stride_transp = 1<br/>padding_transp = 0</span><span id="67ff" class="nf la it nb b gy nk nh l ni nj">F.conv_transpose2d(input_image,<br/>                   weight=kernel,<br/>                   stride=stride_transp,<br/>                   padding=padding_transp)<br/><strong class="nb iu">Output:<br/>tensor([[[[1., 2., 1.],<br/>          [2., 4., 2.],<br/>          [1., 2., 1.]]]])</strong></span></pre><p id="4c9b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我不知道你，但我第一次看到那个输出，我就被它难倒了！让我们深入了解一下！</p><p id="840e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这就是转置卷积在幕后实际做的事情:</p><ul class=""><li id="9764" class="oa ob it lt b lu mo lx mp ma oc me od mi oe mm of og oh oi bi translated">首先，它<strong class="lt iu">用零</strong>填充图像，尽管它<strong class="lt iu">没有使用作为参数的填充(转置)</strong>而是使用<strong class="lt iu">隐式填充代替</strong>(是啊，说到令人困惑！).</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3766163ab54315aee99f85ab91d5820e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ZGhiNjoBgVv3v5oPM8-s8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隐式填充。图片作者。</p></figure><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a47c" class="nf la it nb b gy ng nh l ni nj">padding_implicit = kernel_size - padding_transp - 1<br/>padded_image = F.pad(input_image, pad=[padding_implicit]*4)<br/>padded_image<br/><strong class="nb iu">Output: <br/>tensor([[[[0., 0., 0., 0.],<br/>          [0., 1., 1., 0.],<br/>          [0., 1., 1., 0.],<br/>          [0., 0., 0., 0.]]]])</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/c2ad8930e71023c045de51aacebc8615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUSYlN1SlN3xjGBlteYntw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用隐式填充进行填充。图片作者。</p></figure><ul class=""><li id="5041" class="oa ob it lt b lu mo lx mp ma oc me od mi oe mm of og oh oi bi translated">然后，<strong class="lt iu">使用常规的2D卷积(<code class="fe nx ny nz nb b"><strong class="lt iu">F.conv2d</strong></code>，使用一个</strong>的<strong class="lt iu">固定步幅(<strong class="lt iu">不是用作参数的步幅(转置))，在填充图像</strong>上卷积内核/滤波器！</strong>)</li></ul><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="7935" class="nf la it nb b gy ng nh l ni nj">F.conv2d(padded_image, weight=kernel, stride=1)<br/><strong class="nb iu">Output: <br/>tensor([[[[1., 2., 1.],<br/>          [2., 4., 2.],<br/>          [1., 2., 1.]]]])</strong></span></pre><p id="8487" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们走吧！这与转置卷积函数产生的输出相同！下图说明了这两个步骤:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/6facb218dc09062a9f4519e59fa593f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-FOlqOOuu3Ltuy_YO6hkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用一步(转置)的转置卷积。图片作者。</p></figure><blockquote class="mt mu mv"><p id="737e" class="lr ls mn lt b lu mo ju lw lx mp jx lz mw mq mc md mx mr mg mh my ms mk ml mm im bi translated">“好的，但是<strong class="lt iu">步幅(转置)</strong>呢？根本没用！”</p></blockquote><p id="5212" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">接得好！事实上，它并没有被使用，这就引出了第二个例子。</p><h2 id="f287" class="nf la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">使用2的步幅(转置)</h2><p id="fcb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将保持相同的输入图像和内核/过滤器，相同的零的<strong class="lt iu">填充(转置)，但是我们现在将使用两个</strong>的<strong class="lt iu">步距(转置)。</strong></p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="db79" class="nf la it nb b gy ng nh l ni nj">stride_transp = 2<br/>padding_transp = 0</span><span id="96b5" class="nf la it nb b gy nk nh l ni nj">F.conv_transpose2d(input_image,<br/>                   weight=kernel,<br/>                   stride=stride_transp,<br/>                   padding=padding_transp)<br/><strong class="nb iu">Output:<br/>tensor([[[[1., 1., 1., 1.],<br/>          [1., 1., 1., 1.],<br/>          [1., 1., 1., 1.],<br/>          [1., 1., 1., 1.]]]])</strong></span></pre><p id="5094" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这个输出也令人困惑，对吗？大于一个的<strong class="lt iu">步幅(转置)实际上在开始时引入了<strong class="lt iu"> <em class="mn">又一步</em> </strong>:</strong></p><ul class=""><li id="2f1c" class="oa ob it lt b lu mo lx mp ma oc me od mi oe mm of og oh oi bi translated">首先，<strong class="lt iu">在输入图像的现有1</strong>之间插入0序列作为列和行。要插入的<strong class="lt iu">列/行数</strong>由<strong class="lt iu">步距(转置)减去一个</strong>给出。在我们的例子中，它将插入一列和一行，产生如下图像:</li></ul><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e741" class="nf la it nb b gy ng nh l ni nj">zero_added_image = torch.tensor([[[[1, 0, 1],<br/>                                   [0, 0, 0],<br/>                                   [1, 0, 1]]]]).float()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/4185609176bddcf0527952b1f7eb406c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UFa-v2l-PABp4Cpld86Igg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">添加零的列和行。图片作者。</p></figure><ul class=""><li id="1ac4" class="oa ob it lt b lu mo lx mp ma oc me od mi oe mm of og oh oi bi translated">接下来，它使用一个的固定步幅执行<strong class="lt iu">隐式填充</strong>和<strong class="lt iu">常规卷积，就像第一个例子一样:</strong></li></ul><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="749d" class="nf la it nb b gy ng nh l ni nj">padding_implicit = kernel_size - padding_transp - 1</span><span id="4f31" class="nf la it nb b gy nk nh l ni nj">F.conv2d(zero_added_image,<br/>         weight=kernel,<br/>         stride=1,<br/>         padding=padding_implicit)<br/><strong class="nb iu">Output:<br/>tensor([[[[1., 1., 1., 1.],<br/>          [1., 1., 1., 1.],<br/>          [1., 1., 1., 1.],<br/>          [1., 1., 1., 1.]]]])</strong></span></pre><p id="731c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">又来了！这与转置卷积函数产生的输出相同！下图说明了整个事情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/4909736b4089df25cbb03fe25d4c23d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBN9Cbt2shuRz6hgutBHXg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用步长为2的转置卷积。图片作者。</p></figure><p id="9e9d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">上面的两个例子足以让你理解转置卷积的要点，以及如何使用它们来增加输出的大小(高度和重量)。</p><h2 id="963e" class="nf la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">输出大小</h2><blockquote class="mt mu mv"><p id="7967" class="lr ls mn lt b lu mo ju lw lx mp jx lz mw mq mc md mx mr mg mh my ms mk ml mm im bi translated">"有没有一种简单的方法来计算产量的大小？"</p></blockquote><p id="5603" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">当然，给定图像和内核大小、步幅(转置)和填充(转置)，结果输出的大小遵循以下公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3969161ad2db822050418ffaae5b1f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*XaBcyDRlLjn-tyK9_mqqmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转置卷积的输出大小</p></figure><h1 id="7c06" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法</h1><p id="737f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">转置卷积非常有用，尤其是在生成模型中，并且很容易简单地假设它们是常规卷积的“相反”操作(因为它们增加了输入的高度和宽度)，而无需过多考虑。不过，它们的内部工作方式有点复杂，所以我希望我能帮助你更好地理解它们。</p><p id="a0cf" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">如果你想了解更多PyTorch，计算机视觉，NLP的知识，给我自己的系列丛书，<a class="ae ky" href="https://pytorchstepbystep.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mn">深度学习用PyTorch循序渐进</em> </a>，一试:-)</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="6825" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">如果您有任何想法、意见或问题，请在下面留下评论或通过我的</em> <a class="ae ky" href="https://bio.link/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="mn">个人资料链接</em> </a> <em class="mn">页面联系。</em></p><p id="a43b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><em class="mn">如果你喜欢我的文章，请考虑使用我的推荐页面</em> <a class="ae ky" href="https://dvgodoy.medium.com/membership" rel="noopener"> <em class="mn">注册一个中级会员</em> </a> <em class="mn">来直接支持我的工作。对于每一个新用户，我从中获得一小笔佣金:-) </em></p></div></div>    
</body>
</html>