<html>
<head>
<title>Smart Distributed Training on Amazon SageMaker with SMD: Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带SMD的Amazon SageMaker上的智能分布式培训:第3部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-3-db707db8a202#2022-09-20">https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-3-db707db8a202#2022-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f8ef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用SageMaker分布式模型并行优化模型分布</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/831832cb53342a0d572e56dd04071d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rnH5Asy2ciue5p53"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马丁·杰恩伯格在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4b2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是关于优化分布式培训的三篇文章的最后一篇。在<a class="ae kv" href="https://chaimrand.medium.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee" rel="noopener">第一部分</a>中，我们提供了分布式训练算法的简要概述。我们注意到所有算法的共同点是它们依赖于多个GPU之间的高速通信。我们推测，考虑到底层实例拓扑的分布式算法，特别是GPU对之间通信链路的差异，会比不考虑的算法性能更好。在<a class="ae kv" href="https://chaimrand.medium.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-2-c833e7139b5f" rel="noopener">第二部分</a>中，我们关注数据分发，并展示了Amazon SageMaker的<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank">分布式数据并行</a> (SDP)库的优势。</p><p id="19da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第三部分中，我们将把注意力转向模型分布。我们将展示一种方式，通过这种方式，<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker的分布式模型并行</a>库允许您以区分节点内和节点间GPU到GPU通信的方式来配置您的模型分布算法。</p><h1 id="864d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">模型分布与SageMaker分布式模型并行</h1><p id="c1e0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与数据分布的SDP类似，<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker分布式模型并行</a> (SMP)库旨在简化和加速模型分布式训练。该库包含了在<a class="ae kv" href="https://chaimrand.medium.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee" rel="noopener">第一部分</a>中讨论的每一种模型分发技术的API(尽管您应该查看文档以获得支持矩阵的详细信息)。它允许组合一些方法，并包括一些自动化配置的控件。</p><p id="8151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本节中，我们将重点介绍对<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html" rel="noopener ugc nofollow" target="_blank">分片数据并行性</a>的支持。回想一下，由ZeRO算法规定的参数分片算法(在<a class="ae kv" href="https://chaimrand.medium.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee" rel="noopener">第一部分</a>中描述)并不区分节点内GPU链接和节点间GPU链接。考虑到这种算法相对较高的通信量(详见<a class="ae kv" href="https://arxiv.org/pdf/1910.02054.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)以及这种算法通常应用于相对较大的模型的事实，节点间带宽可能成为潜在的瓶颈。SMP库通过支持一种称为<a class="ae kv" href="https://www.amazon.science/blog/near-linear-scaling-of-gigantic-model-training-on-aws" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> MiCS </strong> </a>的技术(由于它使<strong class="ky ir">mi</strong>c通信<strong class="ky ir"> s </strong> cale】或<strong class="ky ir">零2D </strong>的事实)使您能够解决这个问题。MiCS引入了分区组的概念。整个GPU集合被划分为分区组(大小相等)，每个分区组包含模型的一个副本。每个模型副本的参数在模型的分区组中的GPU之间进行分片(使用零算法)。在一个分区组中，参数根据零算法进行通信。模型副本之间的对齐通过不同分区组中相应GPU之间的梯度共享来维护。这导致了如下图所示的分层通信策略，图中显示了两个节点，每个节点有两个GPU:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/75cd8db0c388a95667787d7f59efab3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ISHQ-4jcuQ6WSvd6njJCDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">零2D中分层通信模式的一个例子。在每个节点内应用基于零的模型分片算法，并且在单独节点的相应GPU之间共享梯度。(作者)</p></figure><h2 id="6cd4" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">例子</h2><p id="6002" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这里我们展示了一个将<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html" rel="noopener ugc nofollow" target="_blank">分片数据并行</a>集成到PyTorch (1.12)培训工作中的例子。我们选择的模型是一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank">视觉转换器</a> (ViT)模型，大约有6.3亿个参数。该模型是使用<a class="ae kv" href="https://pypi.org/project/transformers/" rel="noopener ugc nofollow" target="_blank"> transformers </a> python包构建的。</p><p id="6360" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码块演示了如何使用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html" rel="noopener ugc nofollow" target="_blank">分片数据并行</a> API在八个<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/g4/" rel="noopener ugc nofollow" target="_blank"> g4dn.12xlarge </a>实例(每个实例有4个GPU)上实例化一个模型分布式训练作业。请注意，这种情况下需要模型并行性，因为我们选择的ViT模型太大，不适合放在单个GPU中。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="01ac" class="mq lt iq nd b gy nh ni l nj nk">from sagemaker.pytorch import PyTorch</span><span id="912d" class="mq lt iq nd b gy nl ni l nj nk">distribution = {<br/>        "mpi": {<br/>            "enabled": True,<br/>            "processes_per_host": 4<br/>        },<br/>        "smdistributed": {<br/>            "modelparallel": {<br/>                "enabled": True,<br/>                "parameters": {<br/>                    "ddp": True,<br/>                    <strong class="nd ir">"sharded_data_parallel_degree": 32,</strong><br/>           <em class="nm">         </em>"delayed_parameter_initialization": True<br/>           <em class="nm">    </em>},<br/>            }<br/>        },<br/>    }</span><span id="f2ae" class="mq lt iq nd b gy nl ni l nj nk">pytorch = PyTorch(entry_point='train.py',<br/>                  role=&lt;role&gt;,<br/>                  instance_type='ml.g4dn.12xlarge',<br/>                  instance_count=8,<br/>                  framework_version='1.12',<br/>                  py_version='py38',<br/>                  distribution=distribution)</span><span id="6f1c" class="mq lt iq nd b gy nl ni l nj nk">pytorch.fit()</span></pre><p id="9142" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的脚本配置为<em class="nm">分片_数据_并行_度</em>设置为32。这将运行经典的基于零的参数分片算法。要使用MiCS(零2D)，请将该参数重新配置为所需的分区组大小。</p><p id="e4f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码块包含相关培训脚本的代码。</p><pre class="kg kh ki kj gt nc nd ne nf aw ng bi"><span id="20f1" class="mq lt iq nd b gy nh ni l nj nk">import torch<br/>from torch.utils.data import Dataset<br/>import time<br/>import argparse<br/>from transformers import ViTForImageClassification, ViTConfig<br/></span><span id="2127" class="mq lt iq nd b gy nl ni l nj nk">class FakeDataset(Dataset):<br/>  def __len__(self):<br/>    return 1000000<br/>  def __getitem__(self, index):<br/>    rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>    label = torch.tensor(data=[index % 1000], dtype=torch.int64)<br/>    return rand_image, label</span><span id="d13b" class="mq lt iq nd b gy nl ni l nj nk">def build_model():<br/>  model_args = {<br/>    "image_size": 256,<br/>    "patch_size": 16,<br/>    "hidden_size": 1024,<br/>    "num_hidden_layers": 50,<br/>    "num_attention_heads": 16,<br/>    "intermediate_size": 4*1024,<br/>    "num_labels": 1000<br/>  }<br/>  model = ViTForImageClassification(ViTConfig(**model_args))<br/>  return model<br/></span><span id="db85" class="mq lt iq nd b gy nl ni l nj nk">if __name__ == '__main__':<br/>  parser = argparse.ArgumentParser()<br/>  parser.add_argument('--model_dir', default='/tmp', type=str)<br/>  args, _ = parser.parse_known_args()</span><span id="608b" class="mq lt iq nd b gy nl ni l nj nk">  <em class="nm"># init<br/>  </em>import smdistributed.modelparallel.torch as smp<br/>  smp.init()<br/>  from deepspeed.runtime.zero import stage3<br/>  stage3.assert_ints_same_as_other_ranks = lambda x: None<br/>  torch.cuda.set_device(smp.local_rank())</span><span id="84b2" class="mq lt iq nd b gy nl ni l nj nk">  dataset = FakeDataset()<br/>  data_loader = torch.utils.data.DataLoader(dataset,<br/>                                batch_size=4, num_workers=12)</span><span id="9eae" class="mq lt iq nd b gy nl ni l nj nk">  model = build_model()<br/>  model = smp.DistributedModel(model)</span><span id="f91a" class="mq lt iq nd b gy nl ni l nj nk">  <em class="nm"># add checkpoint activation<br/>  </em>for m in model.get_module().vit.encoder.layer.children():<br/>    smp.set_activation_checkpointing(m)</span><span id="7540" class="mq lt iq nd b gy nl ni l nj nk">  optimizer = torch.optim.Adam(model.parameters())<br/>  optimizer = smp.DistributedOptimizer(optimizer)<br/>  loss_function = torch.nn.CrossEntropyLoss()</span><span id="60d1" class="mq lt iq nd b gy nl ni l nj nk">  model.train()<br/>  t0 = time.perf_counter()<br/>  summ = 0<br/>  count = 0</span><span id="7788" class="mq lt iq nd b gy nl ni l nj nk">  @smp.step<br/>  def train_step(model, inputs, targets):<br/>    outputs = model(inputs)<br/>    loss = loss_function(outputs['logits'], targets)<br/>    model.backward(loss)<br/>    return loss</span><span id="7acb" class="mq lt iq nd b gy nl ni l nj nk">  for idx, (inputs, targets) in enumerate(data_loader, start=1):<br/>    inputs = inputs.to(torch.cuda.current_device())<br/>    targets = targets.to(torch.cuda.current_device())<br/>    optimizer.zero_grad()<br/>    loss_mb = train_step(model, inputs, targets)<br/>    loss = loss_mb.reduce_mean()<br/>    optimizer.step()<br/>    if torch.distributed.get_rank() == 0:<br/>      batch_time = time.perf_counter() - t0<br/>      print(f'step: {idx}: step time is {batch_time}')<br/>      if idx &gt; 1:  <em class="nm"># skip first step<br/>        </em>summ += batch_time<br/>        count += 1<br/>  if torch.distributed.get_rank() == 0:<br/>    print(f'average step time: {summ/count}')</span></pre><p id="54f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关SMP API的更多详细信息，请参见<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-sharded-data-parallelism.html" rel="noopener ugc nofollow" target="_blank"> SageMaker文档和示例</a>。</p><h2 id="f7ed" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">结果</h2><p id="30b7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在下表中，我们将标准零基参数分片算法(<em class="nm">sharded _ data _ parallel _ degree = 32</em>)的步进时间结果与分区组设置为4(<em class="nm">sharded _ data _ parallel _ degree = 4</em>)的零2D (MiCS)的结果进行了比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/36a4eb4409cfc08b78a32216ddb73ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*IjWqckhXhgJsVNJKqBD9VQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">平均步骤时间(越短越好)——(作者)</p></figure><p id="c0fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以再次看到，考虑到环境细节的算法<strong class="ky ir">比基本算法</strong>大约高出14%。我们再次警告不要对您自己的模型下任何结论，因为相对性能可能会因模型细节和环境设置而异。</p><h1 id="5bc3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="bdae" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">多实例训练可能非常昂贵，任何提高训练速度的机会都会对你的总成本产生有意义的影响。在这篇文章中，我们展示了根据底层训练环境的拓扑结构定制的分布式训练算法如何提高性能。然而，这种算法对你的帮助程度，以及采用它们是否是你的正确选择，将在很大程度上取决于你的项目和培训环境的细节。在我们分享的例子中，我们试图演示如何编程您的代码，以便您可以轻松地在不同的选择之间切换。有选择是好事。</p></div></div>    
</body>
</html>