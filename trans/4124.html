<html>
<head>
<title>Serving ML Models with Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark服务ML模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serving-ml-models-with-apache-spark-3adc278f7a78#2022-09-13">https://towardsdatascience.com/serving-ml-models-with-apache-spark-3adc278f7a78#2022-09-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3796" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何使用PySpark为模型提供服务的端到端指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f15283052ba96f15285a05925ed47e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_6Oq56glS1mfEIVVMrTiw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://pixabay.com/illustrations/technology-developer-touch-finger-3389917/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kw">来源</strong> </a></p></figure><p id="a6db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">处理大型数据集伴随着由技术和编程语言设置的限制的困难。一个有效的步骤是了解分布式处理技术及其支持库。对于希望利用数据处理、<a class="ae kv" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLlib </a>和<a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>的模型服务功能的机器学习工程师和数据科学家来说，这篇文章非常重要。</p><h1 id="234c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">什么是阿帕奇火花？</h1><p id="b1be" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae kv" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是一个借助其广泛的软件包提供基于集群的分布式计算环境的系统，包括:</p><ul class=""><li id="f080" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">SQL查询，</li><li id="f77f" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">流式数据处理，以及</li><li id="d19c" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">机器学习。</li></ul><p id="78db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Apache Spark支持<a class="ae kv" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>、<a class="ae kv" href="https://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala </a>、<a class="ae kv" href="https://www.java.com/" rel="noopener ugc nofollow" target="_blank"> Java </a>和<a class="ae kv" href="https://cran.r-project.org/" rel="noopener ugc nofollow" target="_blank"> R </a>编程语言。</p><p id="0993" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Apache Spark服务于内存计算环境。根据Kashif Munir撰写的《绿色企业的云计算技术》一书，该平台支持一个正在运行的作业在内存中执行100倍的速度，在磁盘上执行10倍的性能。</p><h2 id="2b51" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">PySpark是什么？</h2><p id="3693" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">最初，Apache Spark是用Scala语言实现的。由于大多数机器学习库和高级数据处理包都是用Python编写的，因此与Spark集成的需求是显而易见的。为了满足这种需求，为Spark开发了一个Python API。它被命名为PySpark。它是在名为<a class="ae kv" href="https://www.py4j.org/" rel="noopener ugc nofollow" target="_blank"> Py4J </a>的Python解释器的帮助下建立的，该解释器同步到<a class="ae kv" href="https://www.javatpoint.com/jvm-java-virtual-machine" rel="noopener ugc nofollow" target="_blank"> Java虚拟机(JVM) </a>的连接。</p><h2 id="fe60" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">Spark是如何工作的？</h2><p id="e08d" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Apache Spark支持作业的横向处理。它通过允许使用内存中的属性和增强的SQL熟练度来推进这项任务。Spark的功能包括但不限于:</p><ul class=""><li id="42be" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">操作许多分布式脚本，</li><li id="38e8" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">启用数据处理，</li><li id="2051" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">生成数据工作流，以及</li><li id="9c6a" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">使用MLlib函数执行分析方法</li></ul><h1 id="0eb6" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">火花部件</h1><p id="d0b3" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark项目由各种紧密结合的部分组成。在中心，Spark包含一个计算执行机制，可以规划、并行化和筛选众多应用程序。同时使用所有的火花部件不是强制性的。根据现有的情况和要求，它们中的一些可以与火花芯一起使用。然而，Spark核心的使用是必须的，因为它是Spark架构的核心。</p><h2 id="656b" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">火花核心</h2><p id="3c0c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark Core是通用实现组件的中心，它支持平台中的其他功能。</p><h2 id="7541" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#:~:text=Spark%20Streaming%20is%20an%20extension,processing%20of%20live%20data%20streams.&amp;text=Spark%20Streaming%20provides%20a%20high,a%20continuous%20stream%20of%20data." rel="noopener ugc nofollow" target="_blank">火花流</a></h2><p id="1b55" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark Streaming允许Spark处理从各种系统消耗的在线流数据，包括<a class="ae kv" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> HDFS </a>、<a class="ae kv" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank"> S3 </a>、<a class="ae kv" href="https://kafka.apache.org/" rel="noopener ugc nofollow" target="_blank">卡夫卡</a>、<a class="ae kv" href="https://flume.apache.org/" rel="noopener ugc nofollow" target="_blank">水槽</a>等。，并输出到不同的数据库系统。</p><h2 id="1e47" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated"><a class="ae kv" href="https://spark.apache.org/sql/" rel="noopener ugc nofollow" target="_blank">火花开关</a> L</h2><p id="85d2" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark SQL是利用其查询功能提取结构化数据的主要模块。使用Spark SQL可以读取各种数据格式。其中包括<a class="ae kv" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank">拼花</a>、<a class="ae kv" href="https://www.json.org/json-en.html" rel="noopener ugc nofollow" target="_blank"> JSON </a>、<a class="ae kv" href="http://avro.apache.org/docs/current/spec.html" rel="noopener ugc nofollow" target="_blank"> Avro </a>等等。此外，它允许用户定义的函数生成和HiveQL的使用。</p><h2 id="30ef" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> GraphX </a></h2><p id="2ba6" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">GraphX可以表示为具有并行分布式执行的图形数据库系统的火花。抽象地说，它是由顶点和边组成的。GraphX在其系统内部关联图形计算。</p><h2 id="6897" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated"><a class="ae kv" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLlib(机器学习)</a></h2><p id="0386" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">MLlib是Spark的核心机器学习库。它使分布式方法能够记录和处理数据。它由各种算法组成，包括回归、决策树、k-means聚类等。</p><h1 id="6ba5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">火花建筑</h1><p id="888f" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark架构由驱动程序和工作节点组成。这些节点在集群管理器的帮助下连接在一起。</p><h2 id="7260" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">驱动节点</h2><p id="e54b" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">驱动程序节点是负责执行“main()”方法的主节点。其主要目的是成功创建所需的Spark会话。</p><h2 id="74be" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">集群管理器</h2><p id="4341" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">集群管理器充当在请求的作业之间分配资源的结构。您可以选择<a class="ae kv" href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" rel="noopener ugc nofollow" target="_blank"> Hadoop Yarn </a>、<a class="ae kv" href="https://spark.apache.org/docs/latest/running-on-mesos.html" rel="noopener ugc nofollow" target="_blank"> Mesos、</a>或<a class="ae kv" href="https://spark.apache.org/docs/latest/running-on-kubernetes.html" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>作为集群管理器。</p><h2 id="14a0" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">工作节点</h2><p id="bf99" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">worker节点包含处理相关代码块的任务。在它内部，执行器在执行完调度的作业后维护内存中的数据。执行器架构中的最小单元被称为任务。</p><h1 id="e640" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">为什么使用Apache Spark</h1><p id="227a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">选择Spark有无数的理由。最重要的是它的易用性、快速性和支持。</p><h2 id="2c0f" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">易用性</h2><p id="382c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark的能力通过众多API开放。它们都是为大规模信息的快速有效交流而设计的。凭借其易于理解的结构，用户可以在短时间内使用Spark快速产生结果。</p><h2 id="afc9" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">迅速</h2><p id="7b8e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark旨在实现快速性能。它在内存和本地存储中都有效。Spark的执行速度与Hadoop的<a class="ae kv" href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" rel="noopener ugc nofollow" target="_blank"> MapReduce </a>相比有着高达百倍的显著差异。</p><h2 id="699d" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">支持</h2><p id="3b91" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark支持多种编程语言，包括Python、R、Java和Scala。它集成了对Hadoop环境中各种内存应用的支持。此外，Apache Spark开发人员社区是巨大的、动态的和全球性的。Hadoop的商业供应商也为Spark应用提供广泛的服务。</p><h1 id="0efd" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">火花装置</h1><p id="5a81" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark可以根据平台以不同的方式安装。在本节中，让我们介绍两种不同的安装选项:</p><ul class=""><li id="d695" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">在Google Colab上设置它，然后</li><li id="c302" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">在您的本地机器上安装它。</li></ul><h2 id="2de0" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">在Google Colab上设置Spark</h2><p id="2b92" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Google Colab是一个用户在浏览器中有效实现Python脚本的环境。要在Google Colab上执行Spark with Python，您必须安装适当的Spark、Hadoop和Java版本。在Google Colab上安装Spark可以如下图所示:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b929" class="ne lu iq ns b gy nw nx l ny nz">!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null<br/> !wget -q <a class="ae kv" href="https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz</a><br/> !tar xf spark-2.4.8-bin-hadoop2.7.tgz</span></pre><p id="8804" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在Google Colab上成功安装相应版本后，就可以为Spark和Java设置环境变量了。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="d1cd" class="ne lu iq ns b gy nw nx l ny nz">import os<br/> os.environ[“JAVA_HOME”] = “/usr/lib/jvm/java-8-openjdk-amd64”<br/> os.environ[“SPARK_HOME”] = “/content/spark-2.4.8-bin-hadoop2.7”</span></pre><p id="fbbb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“<em class="nq"> findspark </em>”有助于找到以前安装的PySpark版本。然后，在`<em class="nq"> findspark.init() </em>的帮助下，使PySpark可以作为库导入。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="5a05" class="ne lu iq ns b gy nw nx l ny nz">import findspark</span><span id="2e39" class="ne lu iq ns b gy oa nx l ny nz">findspark.init()</span></pre><h2 id="1e07" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">在本地机器上安装Apache Spark</h2><p id="1774" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Apache Spark可以在任何安装了Python、Scala或Java的环境中运行。本文将关注Python语言。紧凑快速地安装所需的Python包和Jupyter Notebook的最简单方法是使用<a class="ae kv" href="https://www.anaconda.com/products/individual#Downloads" rel="noopener ugc nofollow" target="_blank"> Anaconda </a>。</p><p id="a76e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在Anaconda提示符下使用下面的命令安装Spark:</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ec24" class="ne lu iq ns b gy nw nx l ny nz">conda install pyspark</span></pre><h1 id="b344" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Apache Spark基础知识</h1><p id="6b6a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark支持弹性分布式数据集(RDD)结构。使用此结构可以读取外部数据源。使用RDD将方法传递给Spark是可能的。这些函数可应用于现有数据集或新数据集。</p><p id="f1c2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在接下来的章节中，您将了解更多关于RDD结构、Spark转换和操作的内容。</p><h2 id="3465" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">弹性分布式数据集</h2><p id="eddd" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">弹性分布式数据集是Spark中面向客户端的基本编程接口，用于内存计算。它是数据组件的组合。</p><h2 id="4e18" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">RDD创作</h2><p id="f490" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在创建RDD之前，您需要创建一个Spark会话。这是在“<em class="nq"> SparkContext </em>的帮助下完成的。“<em class="nq"> SparkConf </em>”用于设置其他火花配置。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="0ab6" class="ne lu iq ns b gy nw nx l ny nz">from pyspark import SparkContext, SparkConf</span></pre><p id="ae3a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下一步是定义我们想要的火花配置。在这种情况下，我们将使用本地集群，因为我们是在本地机器上工作。您还可以选择在<a class="ae kv" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">集群模式</a>下设置Spark。</p><p id="1a51" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">指定`<em class="nq">local[*]【T13]`意味着Spark将使用本地机器中的所有内核。这通常是独立模式下的默认设置。</em></p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="24fe" class="ne lu iq ns b gy nw nx l ny nz">spark_configurations = (SparkConf().setMaster(“local[*]”).\</span><span id="8ab8" class="ne lu iq ns b gy oa nx l ny nz">setAppName(“firstSparkSession”).\</span><span id="92d3" class="ne lu iq ns b gy oa nx l ny nz">set(“spark.executor.memory”, “2g”))</span></pre><p id="d4f6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有了这个配置，我们就可以创建Spark会话了。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4fc8" class="ne lu iq ns b gy nw nx l ny nz">spark_context = SparkContext(conf = spark_configurations)</span></pre><p id="ecae" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有一些用于查看spark配置的内置函数。</p><p id="6ff1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Spark版本可以使用`<em class="nq">版本</em>'属性进行检索。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="f279" class="ne lu iq ns b gy nw nx l ny nz">spark_context.version</span></pre><p id="5287" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Python版本可以使用`<em class="nq"> pythonVer </em>`属性来显示。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="7382" class="ne lu iq ns b gy nw nx l ny nz">spark_context.pythonVer</span></pre><p id="9207" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">要查看分配给独立模式的内核数量，您可以向spark会话变量添加一个“<em class="nq">主</em>”参数。在下面的例子中，Spark会话的名称叫做`<em class="nq"> spark_context </em>。'</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4256" class="ne lu iq ns b gy nw nx l ny nz">spark_context = SparkContext(conf = spark_configurations)</span><span id="816f" class="ne lu iq ns b gy oa nx l ny nz">spark_context.master</span></pre><p id="173e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">每个Spark会话都有一个唯一的名称。`<em class="nq"> setAppName </em>属性可用于设置会话的名称。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="bc96" class="ne lu iq ns b gy nw nx l ny nz">spark_configurations = (SparkConf().setAppName(“firstSparkSession”))</span><span id="4902" class="ne lu iq ns b gy oa nx l ny nz">spark_context = SparkContext(conf = spark_configurations)</span></pre><p id="1763" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">分配应用程序名称后，可以使用`<em class="nq"> appName </em>'属性查看它。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="0e8b" class="ne lu iq ns b gy nw nx l ny nz">spark_context.appName</span></pre><p id="4734" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Spark为每个会话创建一个惟一的应用程序id。可以使用`<em class="nq"> applicationId </em>'属性来检索id。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="3fd8" class="ne lu iq ns b gy nw nx l ny nz">spark_context.applicationId</span></pre><p id="acc3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Spark通过并行性在每个Spark会话中分配任务。您可以手动设置或使用默认选项。</p><p id="ded4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">可以使用`<em class="nq">默认并行度</em>'属性查看默认设置。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="24bf" class="ne lu iq ns b gy nw nx l ny nz">spark_context.defaultParallelism</span></pre><p id="2214" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">您可以在Spark上下文的配置阶段设置默认并行度。这是通过使用`<em class="nq">spark . default . parallelism</em>参数来完成的。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="493b" class="ne lu iq ns b gy nw nx l ny nz">spark_context.setConf(”spark.default.parallelism”, “50”)</span></pre><p id="5791" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，Spark允许为作业分配不同数量的分区。可以通过在“spark.default.partitions”配置参数中添加数字来设置所需的分区数量。在下面的例子中，“50”是确定分区的数量。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="2dcb" class="ne lu iq ns b gy nw nx l ny nz">spark_context.setConf(“spark.default.partitions”, “50”)</span></pre><p id="531e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">若要打印RDD的最小分区数的默认设置，请使用“defaultMinPartitions”属性。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="f2fb" class="ne lu iq ns b gy nw nx l ny nz">spark_context.defaultMinPartitions</span></pre><h2 id="60f4" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">RDD行动</h2><p id="1ef5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在Spark中，RDD操作由`<em class="nq">变换</em>和`<em class="nq">动作</em>组成。`<em class="nq">转换</em>`是可以通过使用旧的RDD创建一个不存在的操作。</p><h2 id="6b8e" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">火花变换</h2><p id="48bc" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在激活的Spark会话中，有几种Spark转换可用。在这一节，我们来介绍最常见的。</p><p id="4523" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">地图</strong></p><p id="25e7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“<em class="nq"> Map </em>”方法返回一个新的分布式数据集，该数据集是通过函数传递每个元素而得到的。在下面的示例中,“<em class="nq"> collect </em>”操作负责检索现有RDD中的所有项目。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="3de9" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])<br/>mapped_list = items.map(lambda x: x+2).collect()</span><span id="b1d5" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing mapped items for map operation of RDD: “, (mapped_list))</span></pre><p id="8891" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">平面地图</strong></p><p id="6e45" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">flatMap方法通过对RDD的每个项目执行计算，然后执行拼合操作来进行操作。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="505b" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([2,4,13])<br/> items.flatMap(lambda x: range(1, x)).collect()</span></pre><p id="39db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">地图分区</strong></p><p id="667a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在`<em class="nq"> mapPartitions </em>的帮助下，一个方法可以应用到指定RDD的每个分区。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="534a" class="ne lu iq ns b gy nw nx l ny nz">partitioned = spark_context.parallelize ([4,13,13,28,36,47,56], 2)</span><span id="2302" class="ne lu iq ns b gy oa nx l ny nz">def mapPartitionFunc(ind): yield sum(ind)<br/>partitioned.mapPartitions(mapPartitionFunc).collect()</span></pre><p id="c296" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> MapPartitionsByIndex </strong></p><p id="042b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“mapPartitionsWithIndex”方法通过不丢失核心分区的索引，使函数能够在RDD的每个分区上执行。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1250" class="ne lu iq ns b gy nw nx l ny nz">partitioned = spark_context.parallelize ([4,13,13,28,36,47,56], 4)</span><span id="51b8" class="ne lu iq ns b gy oa nx l ny nz">def mapPartitionByIndexFunc(indSlicer, ind): yield indSlicer<br/>partitioned.mapPartitionsWithIndex(mapPartitionByIndexFunc).sum()</span></pre><p id="2764" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">过滤器</strong></p><p id="5211" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">filter方法在选择在特定条件下返回“<em class="nq"> true </em>”的项目后，返回一个新的数据集。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="85f6" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])</span><span id="7365" class="ne lu iq ns b gy oa nx l ny nz">filtered_list = items.filter(lambda x: x % 2 == 0).collect()</span><span id="70d6" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing filtered list items for filter operation of RDD: “, (filtered_list))</span></pre><p id="24f6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">样品</strong></p><p id="ce3b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">采样可用于数据处理的任何阶段。对于RDD数据集，可以通过在“样本”函数中指定一个百分比值来进行采样。当请求相同的子集时，可以将种子id添加到方法中。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="950f" class="ne lu iq ns b gy nw nx l ny nz">sampling_items = spark_context.parallelize(range(20), 4)<br/>sampling_items.sample(True, 0.3, 1234).collect()</span></pre><p id="d300" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">加入</strong></p><p id="4d88" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">可以使用“连接”方法将RDD数据集连接到一对匹配的键上。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="f927" class="ne lu iq ns b gy nw nx l ny nz">list1 = spark_context.parallelize([(“k”, 98), (“m”, 65)])<br/>list2 = spark_context.parallelize([(“k”, 120), (“k”, 43)])<br/>sorted(list1.join(list2).collect())</span></pre><p id="c0ee" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">工会</strong></p><p id="a82d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">`<em class="nq">联合</em>'操作有助于联合指定的rdd。它一个接一个地添加。此操作不会搜索它们之间的匹配密钥。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="43ef" class="ne lu iq ns b gy nw nx l ny nz">union_items = spark_context.parallelize(range(5), 2)<br/>union_items.union(union_items).collect()</span></pre><p id="c53a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">路口</strong></p><p id="8629" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“<em class="nq">交集</em>”方法负责在RDD数据集中查找元素的交集。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b872" class="ne lu iq ns b gy nw nx l ny nz">group1 = spark_context.parallelize([2, 10, 17, 3, 14, 5])<br/>group2 = spark_context.parallelize([2, 8, 5, 34, 42, 14])</span><span id="19e2" class="ne lu iq ns b gy oa nx l ny nz">group1.intersection(group2).collect()</span></pre><p id="ef9e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">截然不同的</strong></p><p id="b0e6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">`<em class="nq"> distinct </em>'函数用于从RDD中获取一组唯一的元素。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="54b7" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4, 13, 13, 28, 36, 47, 56])<br/> unique_element_list = items.distinct().collect()</span><span id="6ea0" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing distinct items for distinct operation of RDD: “, (unique_element_list))</span></pre><p id="4a48" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> GroupByKey </strong></p><p id="0903" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用“<em class="nq"> groupByKey </em>”函数需要将每个键的元素分组在一行中。在此操作之后，RDD的输出将具有哈希分区。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ddad" class="ne lu iq ns b gy nw nx l ny nz">groupedKeys = spark_context.parallelize([(“first_num”, 300),</span><span id="8805" class="ne lu iq ns b gy oa nx l ny nz">(“second_num”, 500), (“third_num”, 900)])</span><span id="ae62" class="ne lu iq ns b gy oa nx l ny nz">print(sorted(groupedKeys.groupByKey().mapValues(len).collect()))<br/> print(sorted(groupedKeys.groupByKey().mapValues(list).collect()))</span></pre><p id="8530" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">减速键</strong></p><p id="1daa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“<em class="nq"> reduceByKey </em>”方法对RDD元素的值执行合并操作。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="cf68" class="ne lu iq ns b gy nw nx l ny nz">from operator import sub<br/> reducedKeys = spark_context.parallelize([(“first_num”, 300),(“second_num”, 500),(“third_num”, 900),(“second_num”, 500)])</span><span id="c10d" class="ne lu iq ns b gy oa nx l ny nz">print(sorted(reducedKeys.reduceByKey(sub).collect()))</span></pre><p id="66c1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> AggregateByKey </strong></p><p id="d31d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在公共键的结构中需要两个独立的rdd来执行聚合操作。首先，实现了每个项目的聚合。在此步骤之后，操作应用于输出。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8b82" class="ne lu iq ns b gy nw nx l ny nz">item_group1 = spark_context.parallelize([(‘first’,5),(‘first’,3),(‘second’,3)])</span><span id="36d7" class="ne lu iq ns b gy oa nx l ny nz">item_group2 = spark_context.parallelize(range(20))<br/> <br/> firstGroup = (lambda x,y: (x[0]+y,x[1]+1))<br/> aggregatedGroup = (lambda x,y:(x[0]+y[0],x[1]+y[1]))<br/> <br/> print(item_group2.aggregate((0,0),firstGroup,aggregatedGroup))<br/> print(item_group1.aggregateByKey((0,0),firstGroup,aggregatedGroup))</span></pre><p id="96ee" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">排序键</strong></p><p id="b391" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">`<em class="nq"> sortByKey </em>`方法负责以升序方式对元素对进行排序。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="bed9" class="ne lu iq ns b gy nw nx l ny nz">item_list = [(‘first’, 7), (‘second’, 9),</span><span id="0235" class="ne lu iq ns b gy oa nx l ny nz">(‘third’, 11), (‘fourth’, 34), (‘fifth’, 58)]</span><span id="70de" class="ne lu iq ns b gy oa nx l ny nz">spark_context.parallelize(item_list).sortByKey().first()</span></pre><h2 id="aed9" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">火花动作</h2><p id="fe24" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">现在让我们来看看一些Spark操作。</p><p id="65b1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">收藏</strong></p><p id="2bde" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">`<em class="nq"> collect </em>'函数将数据集的所有元素作为数组返回。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ce74" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])<br/> number_list = items.collect()</span><span id="e14c" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing elements for collect: %s” % (number_list))</span></pre><p id="12da" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">第一部</strong></p><p id="6a4e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“first”方法用于从RDD中获取第一项。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1361" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])<br/> first_element = items.first()</span><span id="07af" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing first element with first operation of RDD: %s” % (first_element))</span></pre><p id="92e5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">取</strong></p><p id="d7e3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“take(n)”方法返回数据集的前n个元素。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="5192" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])<br/> take_element = items.take(3)</span><span id="15de" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing specified number of elements with take operation of RDD: %s” % (take_element))</span></pre><p id="a0d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">取样品</strong></p><p id="4aba" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">`<em class="nq"> takeSample </em>`方法返回指定长度的RDD。在该方法中，第一个参数是“替换为的<em class="nq">”。它指示是否需要用旧结果替换新结果。如果是，则设置为`<em class="nq">真</em>，否则设置为`<em class="nq">假</em>`。</em></p><p id="1e1c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第二个参数是要采样的数字。第三个参数是“种子”数。当设置为任意数字时，它是该特定样本的标识符ID。每当您在当前Spark会话中使用相同的ID运行这个采样函数时，它都会返回相同的样本子集。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="60a8" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([5,13,13,28,36,47,56])<br/>items.takeSample(True, 5, 1)</span></pre><p id="f8c6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">已订购</strong></p><p id="31af" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“已订购”功能以升序方式从RDD中取出确定数量的商品。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1a01" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([44,131,836,147,56]).takeOrdered(6)</span><span id="4e1a" class="ne lu iq ns b gy oa nx l ny nz">print (items)</span></pre><p id="1f63" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">计数</strong></p><p id="d8eb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“计数”函数返回元素的数量，而不考虑在RDD中找到的重复记录或非重复记录。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="203f" class="ne lu iq ns b gy nw nx l ny nz">element_count = items.count()</span><span id="97e7" class="ne lu iq ns b gy oa nx l ny nz">print (“Printing number of instances for count operation of RDD: %i” % (element_count))</span></pre><p id="53e0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">计数键</strong></p><p id="9673" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“<em class="nq"> countByKey </em>”功能与“<em class="nq"> count </em>”功能的不同之处在于通过相应的键对项目进行计数。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="ac55" class="ne lu iq ns b gy nw nx l ny nz">countKey = spark_context.parallelize([(“first_num”, 300), (“second_num”, 500), (“third_num”, 900), (“second_num”, 500), ])<br/> sorted(countKey.countByKey().items())</span></pre><p id="a803" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">保存文本文件</strong></p><p id="450b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">借助`<em class="nq"> saveasTextFile </em>`功能，可以将RDD数据集保存为文本格式。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="474b" class="ne lu iq ns b gy nw nx l ny nz">items = spark_context.parallelize ([4,13,13,28,36,47,56])</span><span id="b8fc" class="ne lu iq ns b gy oa nx l ny nz">saved_list = items.saveAsTextFile(“items.txt”)</span></pre><h2 id="2789" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence" rel="noopener ugc nofollow" target="_blank"> RDD的坚持</a></h2><p id="04e1" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark的主要优势是能够跨分区将数据集保存在内存中。持久性是通过缓存实现的。分区在内存中进行处理，以便将RDD存储在缓存中。之后，它们可以在该数据集的不同操作中重用。这使得未来的行动要快得多。</p><p id="e66e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">RDD可以第一次在活动中注册。之后，它将被分区保存在内存中。但是，这些RDD分区有丢失的风险。Spark可以重新计算最初所做的更改。</p><p id="4d87" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通过缓存rdd，用户可以继续将数据集保存在硬盘上，并在其他作业中重用它。有些rdd可以多次使用。在这些rdd上使用“持久”操作可能是有利的。</p><p id="92ea" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用“持久”方法的存储级别，当选择“内存和磁盘”时，RDD分区将仅缓存到内存和磁盘一次。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6ef8" class="ne lu iq ns b gy nw nx l ny nz">item_list = spark_context.parallelize([(‘first’,5), (‘first’,3), (‘second’,3)])</span><span id="6df1" class="ne lu iq ns b gy oa nx l ny nz">item_list.persist(pyspark.StorageLevel.MEMORY_AND_DISK )</span><span id="5690" class="ne lu iq ns b gy oa nx l ny nz">item_list.getStorageLevel()</span><span id="985e" class="ne lu iq ns b gy oa nx l ny nz">print(item_list.getStorageLevel())</span></pre><p id="53db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">另一方面，当选择“内存和磁盘2”时，RDD分区将在内存和磁盘上有两个副本。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b7ba" class="ne lu iq ns b gy nw nx l ny nz">item_list = spark_context.parallelize([(‘first’,5), (‘first’,3), (‘second’,3)])</span><span id="bd18" class="ne lu iq ns b gy oa nx l ny nz">item_list.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2 )</span><span id="525a" class="ne lu iq ns b gy oa nx l ny nz">item_list.getStorageLevel()</span><span id="429e" class="ne lu iq ns b gy oa nx l ny nz">print(item_list.getStorageLevel())</span></pre><h1 id="5389" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">使用PySpark创建Spark会话</h1><p id="521e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">运行PySpark函数需要Spark会话。首先，必须导入所需的库。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="a308" class="ne lu iq ns b gy nw nx l ny nz">from pyspark.sql import SparkSession<br/>from pyspark.context import SparkContext</span></pre><p id="9cd7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">加载相关库后，只需添加一个`<em class="nq"> appName </em>'和一个`<em class="nq"> getOrCreate </em>'函数，就可以启动spark会话。如果存在任何额外的配置要求，例如执行器的内存大小，则可以在Spark会话构建块中包含一个“config”参数。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="a9c4" class="ne lu iq ns b gy nw nx l ny nz">spark = SparkSession.builder.\<br/> appName(“FirstSparkApplication”).\<br/> config (“spark.executor.memory”, “8g”).\<br/> getOrCreate()</span></pre><h2 id="f07d" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">通过读取不同的数据格式创建Spark数据帧</h2><p id="a380" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">创建会话后，可以读取数据并将其作为数据帧加载到支持数据结构中。数据帧可以描述为基于列的表格格式的集合。</p><p id="a081" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下面，使用相应的文件名函数读取不同格式的文件。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b254" class="ne lu iq ns b gy nw nx l ny nz">json_df = spark.read.json(“dataset.json”)<br/> text_df = spark.read.text(“dataset.txt”)<br/> csv_df = spark.read.csv(“dataset.csv”)<br/> parquet_df = spark.read.parquet(“dataset.parquet”)</span></pre><h1 id="4b99" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Spark中的机器学习</h1><p id="6748" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Spark包含一个名为MLlib的独立库，支持几种机器学习算法。MLlib增强的核心字段有:</p><ul class=""><li id="d6fb" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">机器学习计算，</li><li id="c90b" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">特色化，</li><li id="fbc9" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">生成管道结构，</li><li id="4b11" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">坚持</li></ul><p id="649a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们讨论使用Spark实现机器学习模型的步骤。</p><h1 id="67e5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据准备</h1><p id="4c5c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在整篇文章中，将使用一个非常著名的数据集“<a class="ae kv" href="https://github.com/drewszurko/kaggle-titanic/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">泰坦尼克号</a>”。公共数据集可以从其<a class="ae kv" href="https://github.com/drewszurko/kaggle-titanic" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a>下载。另外，你可以通过链接查看它的许可证<a class="ae kv" href="https://github.com/drewszurko/kaggle-titanic/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2495" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">作为第一步，我们将在Spark会话的帮助下读取数据集。</p><p id="fd72" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">数据集有其格式，其基本要求是使用“<em class="nq"> csv </em>”函数。它被指定为`<em class="nq"> spark.read.format() </em>中的参数。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6fc8" class="ne lu iq ns b gy nw nx l ny nz">training_dataset = spark.read.format(“csv”).\</span><span id="eaae" class="ne lu iq ns b gy oa nx l ny nz">option(“inferSchema”, True). option(“header”, “true”).\</span><span id="2f7b" class="ne lu iq ns b gy oa nx l ny nz">load(‘dataset/titanic_train.csv’)</span><span id="e8f5" class="ne lu iq ns b gy oa nx l ny nz">test_dataset = spark.read.format(“csv”).\</span><span id="1eae" class="ne lu iq ns b gy oa nx l ny nz">option(“inferSchema”, True).option(“header”, “true”).\</span><span id="d9df" class="ne lu iq ns b gy oa nx l ny nz">load(‘dataset/titanic_test.csv’)</span></pre><p id="3a80" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">作为最初的分析步骤，让我们显示列名。这可以通过使用PySpark以三种不同的方式完成。</p><p id="d4dd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第一种方法是使用`<em class="nq"> show </em>'方法，同时将您想要显示的行数作为参数进行传递。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="87f5" class="ne lu iq ns b gy nw nx l ny nz">training_dataset.show(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7d62c3fc1bcd3f87f08b9520b770a573.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*T2Ox2fYktN84kb7-WtIbUA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图1 .</strong>‘show(5)’功能的输出。归作者所有。</p></figure><p id="3fef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第二种方法是使用`<em class="nq"> show() </em>`方法，不传递任何参数。该操作将输出20行。其默认格式包含截断的列内容。对于“Name”列，可以观察到截断的列，因为它超过了默认长度。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b993" class="ne lu iq ns b gy nw nx l ny nz">training_dataset.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1e869f3d3f69dc1b7e635cd0df9ec44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*dxEbz5EWd_iRwCBgWrR8wQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图2 .</strong>‘show()’函数的输出。归作者所有。</p></figure><p id="cc8f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通过在“<em class="nq"> show </em>方法中设置“<em class="nq"> truncate = False </em>”可以查看完整的列内容。此外，可以通过在“<em class="nq"> show() </em>”函数中添加“<em class="nq"> vertical = True </em>”来更改默认的水平显示。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="6d73" class="ne lu iq ns b gy nw nx l ny nz">The full column content can be viewed by setting `truncate = False` in the `show` method. Also, the default horizontal display can be changed by adding `vertical = True` in the `show()` function.</span><span id="3d32" class="ne lu iq ns b gy oa nx l ny nz">training_dataset.show(2, truncate=False, vertical=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/91f78d70718f53a37094b403e0c59918.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*IRXJrRsCItDrat9aGKWebQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图3 .</strong>` show(truncate = False，vertical=True)`函数的输出。归作者所有</p></figure><h2 id="c32d" class="ne lu iq bd lv nf ng dn lz nh ni dp md lg nj nk mf lk nl nm mh lo nn no mj np bi translated">用Spark进行数据预处理</h2><p id="230c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">查看列名及其类型后，检查数据集是否包含任何“null”或“nan”值至关重要。我们必须在建模步骤之前填写它们。</p><p id="fe55" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们在下面显示空列和非空列。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="3307" class="ne lu iq ns b gy nw nx l ny nz">from pyspark.sql.functions import *<br/> print (“NaN values\n”)<br/> training_dataset.select([count(when(isnan(item), item)).alias(item) for item in training_dataset.columns]).show(5)<br/> <br/> print (“Null values\n”)<br/> training_dataset.select([count(when(col(item).isNull(), item)).alias(item) for item in training_dataset.columns]).show(5)<br/> <br/> print (“Not Null values\n”)<br/> training_dataset.select([count(when(col(item).isNotNull(), item)).alias(item) for item in training_dataset.columns]).show(5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5dd5bd1571e224e790113a7c990f2acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*QaE5peZmj3fZIW2h8E-Exg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图4。</strong>填充和未填充值的输出。归作者所有</p></figure><p id="626d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一些列名可以使用“<em class="nq"> withColumnRenamed </em>”函数重命名。使用这种方法，只需用点分隔符逐个添加多个列，就可以对它们进行重命名。该函数的第一个参数是原始值，第二个参数是新的列名。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="e569" class="ne lu iq ns b gy nw nx l ny nz">print(“Renaming Column Name”)</span><span id="bf34" class="ne lu iq ns b gy oa nx l ny nz">training_dataset = training_dataset.\<br/> withColumnRenamed(“Pclass”,”PassengerClasses”).\<br/> withColumnRenamed(“Sex”,”Gender”)<br/> training_dataset</span></pre><p id="7551" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“group by”SQL操作可以通过“count”操作应用于单个列。此外，可以在函数中为多个分组操作添加多个值。</p><p id="2a07" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">也可以在“count()”函数的末尾添加“<em class="nq"> sort() </em>”函数。通过观察输出中各等级的计数，我们可以看到三等舱男女乘客的“幸存”计数最高。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="dd0a" class="ne lu iq ns b gy nw nx l ny nz">print(“Counting the number of Passenger per Classes”)</span><span id="563f" class="ne lu iq ns b gy oa nx l ny nz">training_dataset.groupBy(“PassengerClasses”).\</span><span id="2b78" class="ne lu iq ns b gy oa nx l ny nz">count().\</span><span id="3cc2" class="ne lu iq ns b gy oa nx l ny nz">sort(“PassengerClasses”).show()<br/> <br/> <br/> print(“Counting the number of Survivals by Classes”)<br/> training_dataset.groupBy(“PassengerClasses”,<br/> “Gender”,<br/> “Survived”).count().sort(“PassengerClasses”,<br/> “Gender”,<br/> “Survived”).show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f105364062c342b7ce1dc2b3e8b21499.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*3II3Yy-dPTkFonwjoFQXqQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图五。</strong>‘group by’操作的输出。归作者所有。</p></figure><h1 id="1d3a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">PySpark的特征工程</h1><p id="823b" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在特征工程的帮助下，可以从数据集中的现有变量中提取更有见地的信息。</p><p id="dc99" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在titanic数据集中有一个“T32姓名”列，其中也包括这个人的头衔。该信息可能对模型有益。所以我们把它生成为一个新变量。可以使用`<em class="nq"> withColumn </em>'操作创建一个新的标题列。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="1dc1" class="ne lu iq ns b gy nw nx l ny nz">training_dataset = training_dataset.withColumn(“Title”, regexp_extract(col(“Name”),”([A-Za-z]+)\.”, 1))</span><span id="8f0d" class="ne lu iq ns b gy oa nx l ny nz">training_dataset.select(“Name”,”Title”).show(10)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/00601f0af12bd25c5a046af75050c9f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*9Um6fUBHlrWAJs1mBej3jQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图6。</strong>‘with column’标题提取操作的输出。归作者所有。</p></figure><p id="5c59" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一个名为`<em class="nq">标题</em>'的新列被生成。按数量列出每个标题可以告诉我们，有些标题只出现过一次。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="b19b" class="ne lu iq ns b gy nw nx l ny nz">training_dataset.groupBy(“Title”).count().show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f140804a9641df0d9234b738e4b557c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*7OIDFdnmoYZflwp4Eg-PMw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图七。</strong>‘group by’操作的输出。归作者所有。</p></figure><p id="35c7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有一些不同格式的重复标题。有些是可以替换的。为此，可以使用“<em class="nq">替换</em>”功能。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4ca3" class="ne lu iq ns b gy nw nx l ny nz">feature_df = training_dataset.\<br/> replace([“Mme”, <br/> “Mlle”,”Ms”,<br/> “Major”,”Dr”, “Capt”,”Col”,”Rev”,<br/> “Lady”,”Dona”, “the Countess”,”Countess”, “Don”, “Sir”, “Jonkheer”,”Master”],<br/> [“Mrs”, <br/> “Miss”, “Miss”,<br/> “Ranked”,”Ranked”,”Ranked”,”Ranked”,”Ranked”,<br/> “Royalty”,”Royalty”,”Royalty”,”Royalty”,”Royalty”, “Royalty”, “Royalty”,”Royalty”])<br/> <br/> feature_df.groupBy(“Title”).count().sort(desc(“count”)).show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/57c22bda14bcd90f29c60d3a7fb75383.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*SJLUcTfNVnO8CNybNGdZHw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图8。“标题”列的“分组”操作的输出。归作者所有。</strong></p></figure><p id="c4a2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在替换操作之后，标题的分布似乎比以前更准确。</p><h1 id="1d97" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">用PySpark MLlib构建机器学习模型</h1><p id="f867" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在模型实现阶段之前，应该检查变量的类型。由于预测算法需要数字格式的变量，字符串格式的列可能会导致错误。</p><p id="537a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">PySpark的`<em class="nq"> dtypes </em>`函数可以用来打印变量的类型。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="55f8" class="ne lu iq ns b gy nw nx l ny nz">feature_df.dtypes</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/145b2e0515848adbb7be6b9da524a828.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*9RYjWE9pLJGGBj8d0jUxyQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图9。</strong>数据帧的“数据类型”操作的输出。归作者所有。</p></figure><p id="0717" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在打印变量类型后，可以观察到“性别”、“已装载”和“标题”列具有字符串格式。这些列需要转换成数字形式。</p><p id="fecb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">名为“StringIndexer”的专用PySpark函数将变量拟合并转换为数值类型。下面就来实现吧。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="0863" class="ne lu iq ns b gy nw nx l ny nz">from pyspark.ml.feature import StringIndexer</span><span id="e9eb" class="ne lu iq ns b gy oa nx l ny nz">parchIndexer = StringIndexer(inputCol=”Parch”, outputCol=”Parch_Ind”).fit(df)</span><span id="42c7" class="ne lu iq ns b gy oa nx l ny nz">sibspIndexer = StringIndexer(inputCol=”SibSp”, outputCol=”SibSp_Ind”).fit(df)</span><span id="c4f2" class="ne lu iq ns b gy oa nx l ny nz">passangerIndexer = StringIndexer(inputCol=”PassengerClasses”, outputCol=”PassengerClasses_Ind”).fit(df)</span><span id="66ff" class="ne lu iq ns b gy oa nx l ny nz">survivedIndexer = StringIndexer(inputCol=”Survived”, outputCol=”Survived_Ind”).fit(df)</span></pre><p id="3da7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在索引和删除旧的字符串格式的操作之后，数据帧就有了所有的数字变量。因为所有的列都是非字符串格式，所以我们可以使用数据帧中的列生成一个特征向量。“VectorAssembler”可用于转换“<em class="nq">特征</em>”向量列。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8e93" class="ne lu iq ns b gy nw nx l ny nz">from pyspark.ml.feature import VectorAssembler</span><span id="48ff" class="ne lu iq ns b gy oa nx l ny nz">assembler = VectorAssembler(<br/> inputCols = [“PassengerClasses”,”SibSp”,”Parch”],<br/> outputCol = “features”)</span><span id="319b" class="ne lu iq ns b gy oa nx l ny nz">The next step after creating the feature vector is to split the data into train and test sets. You can use the `randomSplit` function to achieve this.</span><span id="4421" class="ne lu iq ns b gy oa nx l ny nz">(train, test) = df.randomSplit([0.8, 0.2], seed = 345)</span></pre><p id="1f61" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在应用预测算法之前，需要实现分类器和流水线生成阶段。</p><p id="6b78" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们一起来定义这些步骤。首先，让我们从MLlib库内置的PySpark函数中选择一个分类器。添加导入语句后，可以通过分配`<em class="nq"> labelCol </em>和`<em class="nq">features coll</em>列来创建分类器。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="58bd" class="ne lu iq ns b gy nw nx l ny nz">from pyspark.ml.classification import DecisionTreeClassifier</span><span id="3d57" class="ne lu iq ns b gy oa nx l ny nz">classifier = DecisionTreeClassifier(labelCol=”Survived”, featuresCol=”features”)</span><span id="4463" class="ne lu iq ns b gy oa nx l ny nz">In this step, a pipeline is created by adding parameters to `stages` accordingly.</span><span id="3048" class="ne lu iq ns b gy oa nx l ny nz">from pyspark.ml import Pipeline</span><span id="1ca3" class="ne lu iq ns b gy oa nx l ny nz">pipeline = Pipeline(stages=[assembler, model_identifier])</span><span id="1f37" class="ne lu iq ns b gy oa nx l ny nz">When the pipeline is established, parameters of the classifier can be optimized with the help of `ParamGridBuilder`.</span><span id="c3e9" class="ne lu iq ns b gy oa nx l ny nz">Appropriate parameters will be produced after the grid search.</span><span id="bf5e" class="ne lu iq ns b gy oa nx l ny nz">from pyspark.ml.tuning import ParamGridBuilder<br/> <br/> paramGrid = ParamGridBuilder() \<br/> .addGrid(model_identifier.maxDepth, [10,20]) \<br/> .addGrid(model_identifier.maxBins, [50, 100]) \<br/> .build()</span></pre><p id="2aaf" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为此，相应的`<em class="nq">标签</em>、`<em class="nq">特征</em>和`<em class="nq">公制</em>列被填充。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="d6e3" class="ne lu iq ns b gy nw nx l ny nz">tvs = TrainValidationSplit(<br/> estimator=pipeline,<br/> estimatorParamMaps=paramGrid,<br/> evaluator=MulticlassClassificationEvaluator(labelCol=”Survived”,</span><span id="c64d" class="ne lu iq ns b gy oa nx l ny nz">predictionCol=”prediction”, metricName=”weightedPrecision”),<br/> trainRatio=0.8)</span></pre><p id="c708" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在“<em class="nq">trainivalidationsplit</em>”阶段完成后，我们准备好装配模型。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="bed0" class="ne lu iq ns b gy nw nx l ny nz">model = tvs.fit(train)</span></pre><h1 id="540d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">模型评估</h1><p id="a001" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">作为一种模型评估方法，可以应用度量“T28”精度。`<em class="nq">精度</em>的数学公式如下。</p><blockquote class="oh oi oj"><p id="5a9c" class="kx ky nq kz b la lb jr lc ld le ju lf ok lh li lj ol ll lm ln om lp lq lr ls ij bi translated">(true positive+true negative)/<br/>(true positive+true negative+false positive+false negative)</p></blockquote><p id="0318" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用下面的代码行，我们可以通过每个参数获得准确性度量。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="8d39" class="ne lu iq ns b gy nw nx l ny nz">list(zip(model.validationMetrics, model.getEstimatorParamMaps()))</span></pre><h1 id="de76" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">服务于Apache Spark机器学习模型</h1><p id="0fe9" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">使用PySpark生成的机器学习模型可以使用<a class="ae kv" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank"> MLflow </a>来服务。在下面的部分中，将解释MLflow包的安装。此外，模型服务方法将在概念描述的末尾添加一些示例脚本。</p><h1 id="c4e0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">为Spark模型服务安装MLflow</h1><p id="5902" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">MLflow可以用作PySpark模型的模型服务库。在Spark会话中使用MLflow需要安装库。对于PySpark，可以使用以下命令安装该包。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="4ef6" class="ne lu iq ns b gy nw nx l ny nz">pip install mlflow</span></pre><p id="a247" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">安装MLflow后导入`<em class="nq"> spark </em>`。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="bdf6" class="ne lu iq ns b gy nw nx l ny nz">from mlflow import spark</span></pre><h1 id="1698" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">使用MLflow服务火花模型</h1><p id="873c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">导入MLflow后执行`<em class="nq"> start_run() </em>`函数，在Spark会话中激活MLflow。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="bd56" class="ne lu iq ns b gy nw nx l ny nz">import mlflow</span><span id="6233" class="ne lu iq ns b gy oa nx l ny nz">from mlflow import spark<br/>with mlflow.start_run():</span><span id="37c2" class="ne lu iq ns b gy oa nx l ny nz">mlflow.spark.log_model(model, “sparkML-model”)</span></pre><p id="1314" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">执行`<em class="nq"> log_model </em>操作后，将创建MLflow、model ` <em class="nq">工件</em>`、`<em class="nq">度量</em>`、`<em class="nq">参数</em>`、以及`<em class="nq">标记</em>`文件夹。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/14c805082dd7578a950d3552bf08c719.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*4ouXBz4b7N7iJL1auYcaog.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图10。</strong>基于桌面的mlflow文件夹结构。归作者所有。</p></figure><p id="0d3e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在“artifacts”文件夹中，您会找到spark ML-model文件夹。在“sparkML模型”中，有“元数据”和“阶段”文件夹。“阶段”记录模型的生命周期。可以有单个阶段或多个阶段。另一方面，“元数据”代表描述和包含模型信息的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/17f860a12be425e55d00cac0d040a920.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*CGgQr3UayveijNBr0lTFhw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图11。</strong>元数据和阶段文件夹结构。归作者所有。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3a0d0e48247df761cb5055bc7d343431.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*Hi_xjsm0PVkwhf3zUXhBaw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图十二。</strong>spark ml文件夹结构。归作者所有。</p></figure><p id="1d2b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">stages文件夹包含“最佳模型”信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2944a98884802d39b662a6084d980dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Ldfn90Cjx6s9c7TUA8_1ZQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图十三。</strong>阶段文件夹路径。归作者所有。</p></figure><p id="939c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在下面的中，您可以从保存在sparkML-model文件下的“MLmodel”文件的示例格式中找到一个片段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/b54168542ddf7d30938f1161982a7b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*9YYrAmf1I6XGISbfvdG69A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图14。</strong>ml model输出。归作者所有。</p></figure><p id="7f39" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">可以使用“<em class="nq"> mlflow.pyfunc </em>”模块生成推论。首先，模型和数据集路径是分开定义的。其次，使用模型路径定义火花UDF。第三，读取数据集并将其注册到数据帧中。最后一步，在先前定义的Spark UDF的帮助下，通过选择所需的列，生成一个新列。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="fb41" class="ne lu iq ns b gy nw nx l ny nz">import mlflow.pyfunc<br/> from pyspark.sql import SQLContext<br/> <br/> train.toPandas().to_csv(‘dataset.csv’)<br/> <br/> model_path = ‘/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/mlruns/1/51ef199ab3b945e8a31b47cdfbf60912/artifacts/sparkML-model’</span><span id="ac1d" class="ne lu iq ns b gy oa nx l ny nz">titanic_path = ‘/Users/ersoyp/Documents/LAYER/ServingModelsWithApacheSpark/Scripts/dataset.csv’</span><span id="9e25" class="ne lu iq ns b gy oa nx l ny nz">titanic_udf = mlflow.pyfunc.spark_udf(spark, model_path)<br/> <br/> df = spark.read.format(“csv”).option(“inferSchema”, True).option(“header”, “true”).option(‘delimiter’, ‘;’).load(titanic_path)<br/> <br/> columns = [‘PassengerClasses’, ‘SibSp’, ‘Parch’]<br/> <br/> df.withColumn(‘Inferences’, titanic_udf(*columns)).show(False)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/600dbb49d1cd5a94142e63fdfd33c6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*S_rpH9L21BxJqvbf7a3EsQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kw">图十五。</strong>ml flow输出。归作者所有。</p></figure><h1 id="2e62" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">元数据存储和图层实验跟踪</h1><p id="64e5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了最终确定一个理想的模型，您需要反复改变各种模型参数或数据准备方法，以达到最佳解决方案。在每次迭代中，保持对参数和过程的跟踪是至关重要的。此外，您可以在每个阶段尝试不同的设置。不遵循这些指令可能会导致重复这些指令，从而浪费时间和计算资源。</p><p id="5c9c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">实验跟踪</a>可以定义为跟踪所有这些数据的技术。</p><p id="278b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一个<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">元数据存储库</a>——一个可以跟踪和保留上述所有信息的工具——对于成功的实验跟踪是必要的。创建机器学习模型时产生的数据保存在元数据存储中。通过存储实验元数据，ML实验的可比性和可重复性成为可能。</p><p id="9f7c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">与现有的<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">元数据存储</a>相比，层还支持无缝的本地和远程开发交互。例如，在使用本地资源进行一些快速试验后，您可以利用利用<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">层</a>基础设施的更有效的计算资源来训练模型。无论是利用本地模式还是远程模式、图层版本、元数据和日志。</p><p id="7c9a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用layer.log()、@dataset和@model decorators，您可以随着项目的进展跟踪您的发现、数据集和模型。您的实体由<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">层</a>自动版本化，以便您可以准确地重复实验并查看其进展。</p><p id="88ae" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有了数据集、ML模型、元数据和文档的通用存储库，您可以缩小协作差距并简化新员工入职流程。记录参数、图表、指标等。快速识别ML实体；制作项目文件；毫不费力地转移控制权。</p><p id="1ddc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">不稳定的生产管道是不好的。作为对生产管道的健全性检查，<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">层</a>自动对数据集和模型进行版本化。</p><p id="15a7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">复杂的任务受到缺乏计算能力的限制。您可以使用Layer的云资源，在强大的预定义GPU和CPU上训练您的模型。Layer SDK处理基础设施的所有难题和复杂性。如果你不需要电，你可以使用你自己的资源。</p><p id="dab5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Layer提供的一些Python装饰器产生了一个健壮的模型和数据注册中心。它提供了一个中心位置来管理与创建机器学习模型相关的所有数据。</p><p id="6a61" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了帮助您快速入门，图层社区网站提供了各种示例项目和数据集。GitHub库也有更多的例子。您可以通过访问layer.ai网站开始构建实验追踪和元数据存储包。</p><p id="1483" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">不稳定的生产管道以不希望的方式运行。作为对生产管道的健全性检查，<a class="ae kv" href="https://layer.ai/" rel="noopener ugc nofollow" target="_blank">层</a>自动对数据集和模型进行版本化。</p><p id="e3c1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">复杂的任务受到缺乏计算能力的限制。您可以使用Layer的云资源，在强大的预定义GPU和CPU上训练您的模型。Layer SDK处理基础设施的所有难题和复杂性。如果你不需要电，你可以使用你自己的资源。</p><p id="abcf" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Layer提供的一些Python装饰器产生了一个健壮的模型和数据注册中心。它提供了一个中心位置来管理与创建机器学习模型相关的所有数据。</p><p id="1a27" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了帮助您快速入门，图层社区网站提供了各种示例项目和数据集。GitHub库也有更多的例子。您可以通过访问layer.ai网站开始构建实验追踪和元数据存储包。</p><h1 id="622a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">最后的想法</h1><p id="5a8a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在整篇文章中，在最初描述概念和用示例脚本实现解决方案的结构中出现了广泛的主题。主题包括介绍Spark，然后在Spark中构建机器学习模型。主题包括但不限于:</p><ul class=""><li id="95a3" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">火花的概念</li><li id="a83d" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">Spark组件及其独特的架构</li><li id="d133" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">Spark和Python的装置</li><li id="3164" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">RDD及其操作要点</li><li id="cefe" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">通过它们不同的功能激发转变和作用</li><li id="8080" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">火花会话生成和数据帧</li><li id="6791" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">使用Spark进行探索性数据分析</li><li id="82a5" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">PySpark中的机器学习</li><li id="8e37" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">使用PySpark的数据准备、预处理和特征工程</li><li id="00d6" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">PySpark中的模型构建阶段</li><li id="8f5f" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">使用Apache Spark的模型服务</li><li id="2742" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">使用Layer SDK进行元数据存储和实验跟踪</li></ul><h1 id="5713" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">资源</h1><ul class=""><li id="1b50" class="mq mr iq kz b la ml ld mm lg ot lk ou lo ov ls mv mw mx my bi translated"><a class="ae kv" href="https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html" rel="noopener ugc nofollow" target="_blank"> Spark胜Daytona Gray Sort 100TB基准</a></li><li id="4df4" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae kv" href="https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">在Colab运行py spark</a></li><li id="2018" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae kv" href="https://blog.layer.ai/" rel="noopener ugc nofollow" target="_blank">层艾博客</a></li><li id="3f85" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae kv" href="https://github.com/pinarersoy/PySpark_SparkSQL_MLib/blob/master/RDD%20Basics%20and%20PySpark%20ML%20Model%20Serving.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本，适用于RDD基础和ML型号，配有PySpark </a></li><li id="dea1" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated"><a class="ae kv" href="https://colab.research.google.com/drive/1w-RosBbdeKrc-QYTgI-5LsUDFr-RR1Ds?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本，适用于RDD基础和ML模型，配有Colab </a></li></ul></div></div>    
</body>
</html>