<html>
<head>
<title>Are your training and test sets comparable?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的训练集和测试集有可比性吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-your-training-and-test-sets-comparable-fc29b0e0b167#2022-05-02">https://towardsdatascience.com/are-your-training-and-test-sets-comparable-fc29b0e0b167#2022-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="373b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何创建可比较的训练和测试数据集的简短指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/fedd3f85d240d86d5d4540279a0b1382.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*s7lYFs838AQF91wk.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="4157" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">数据科学家通常将数据集分为训练集和测试集。他们的模型是在前者上训练的，然后在后者上检查其性能。但是，如果这些集合被错误地采样，模型性能可能会受到偏差的影响。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="709e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">训练集和测试集应该相似吗？</h1><p id="d5c7" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">从我职业生涯的开始，每个人都习惯于随机统一地将数据集分成训练集和测试集。这是一种常见的、被普遍接受的方法，我大体上同意这种方法。但是，如果训练测试在统计上不同于训练集，则训练的模型可能会受到偏差的影响，并且可能无法在看不见的数据上正常工作。</p><p id="a1e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">把完整的数据集想象成橘子和苹果的混合体。如果我们在橙子上训练我们的模型，并在苹果上测试它，它可能不会正常工作。相反，如果我们创建一个混合了橙子和苹果的训练数据集，在这样的数据集上训练的模型将在由橙子和苹果组成的测试集上正常工作。</p><p id="37e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，在训练我们的模型之前，我们必须确保训练和测试数据集在统计上是相似的。</p><h1 id="90fa" class="lx ly it bd lz ma mu mc md me mv mg mh jz mw ka mj kc mx kd ml kf my kg mn mo bi translated">如何计算统计相似度</h1><p id="64d1" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">在本文中，我将展示如何从单变量方法开始计算训练和测试数据集之间的统计相似性。</p><p id="5790" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个想法是计算每个特征的累积分布函数。然后，我们将训练数据集中特征的这种函数与测试数据集中相同特征的相同函数进行比较。</p><p id="ffb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们从sklearn的糖尿病数据集开始看一个例子。让我们导入并拆分它:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4ee8" class="ne ly it na b gy nf ng l nh ni">from sklearn.datasets import load_diabetes </span><span id="3554" class="ne ly it na b gy nj ng l nh ni">from sklearn.model_selection import train_test_split </span><span id="2cdd" class="ne ly it na b gy nj ng l nh ni">dataset = load_diabetes(as_frame=True) </span><span id="894f" class="ne ly it na b gy nj ng l nh ni">X,y = dataset['data'],dataset['target'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)</span></pre><p id="4579" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们使用seaborn为训练和测试数据集绘制“bp”特征的累积分布函数:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="6fbe" class="ne ly it na b gy nf ng l nh ni">feature_name = 'bp' </span><span id="8b4a" class="ne ly it na b gy nj ng l nh ni">df = pd.DataFrame({ feature_name:np.concatenate((X_train.loc[:,feature_name],X_test.loc[:,feature_name])), 'set':['training']*X_train.shape[0] + ['test']*X_test.shape[0] })</span><span id="80d9" class="ne ly it na b gy nj ng l nh ni">sns.ecdfplot(data=df,x=feature_name,hue='set')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0781afc991fdb8e3872b1e6e350b7672.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*YSv8frvwIqQCC7c_.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="071d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所看到的，两条曲线是相似的，但在中间有一个明显的区别。这意味着，在两个数据集之间，由于采样过程，要素的分布被扭曲了。</p><p id="cffc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了用单个数字量化两个分布的差异，我们可以使用Kolmogorov-Smirnov距离。给定<em class="nk"> C(x) </em>特征<em class="nk"> x </em>在训练数据集中的累积分布和<em class="nk"> G(x) </em>相同特征在测试数据集中的累积分布，距离被定义为这些曲线之间的最大距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/4c220529cba984fd09749ab7fec1fafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*klb7d0ZW8PDxZooEDkqLbQ.png"/></div></figure><p id="62d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">距离越小，训练数据集和测试数据集之间的特征分布就越相似。</p><p id="88fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">计算这个度量的一个简单方法是使用scipy和K-S检验。</p><p id="3828" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，距离是:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="70d4" class="ne ly it na b gy nf ng l nh ni">ks_2samp(X_train.loc[:,feature_name],X_test.loc[:,feature_name]).statistic </span><span id="5a5d" class="ne ly it na b gy nj ng l nh ni"># 0.11972417623102555</span></pre><p id="244d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以将两个数据集之间的距离计算为它们的要素之间的最大距离。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="ccff" class="ne ly it na b gy nf ng l nh ni">distances = list(map(lambda i : ks_2samp(X_train.iloc[:,i],X_test.iloc[:,i]).statistic,range(X_train.shape[1])))</span></pre><p id="cc95" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些是每个要素的距离:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="5455" class="ne ly it na b gy nf ng l nh ni">for i in range(X_train.shape[1]):<br/>    print(X_train.columns[i],distances[i])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0f65d79e036cdec035541a29e2eaba4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/0*Eia7JC7j8fatfqnC.png"/></div></figure><p id="0012" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，偏向性较低的特征是‘性别’，而最差的是‘BP’。数据集之间的距离是最差距离，因此为0.1197。</p><p id="e356" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们使用R平方分数在这样的数据集上训练并测试线性模型，我们得到:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8e1b" class="ne ly it na b gy nf ng l nh ni">lr = LinearRegression() <br/>lr.fit(X_train,y_train) <br/>r2_score(y_test,lr.predict(X_test)) <br/># 0.4033025232246107</span></pre><h1 id="1ee6" class="lx ly it bd lz ma mu mc md me mv mg mh jz mw ka mj kc mx kd ml kf my kg mn mo bi translated">如何正确选择训练集和测试集</h1><p id="8a03" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">想法是生成几对训练/测试数据集，并选择最小化距离的对。创建不同分割的最简单方法是使用随机状态。</p><p id="a54b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，我们可以遍历随机状态的几个值，计算距离，然后选择使其最小化的随机状态。</p><p id="a9b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们尝试100个可能的样本:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="2693" class="ne ly it na b gy nf ng l nh ni">n_features = X.shape[1] </span><span id="ec7f" class="ne ly it na b gy nj ng l nh ni">n_tries = 100 </span><span id="5edf" class="ne ly it na b gy nj ng l nh ni">result = [] </span><span id="eeb4" class="ne ly it na b gy nj ng l nh ni">for random_state in range(n_tries): <br/>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state) </span><span id="326e" class="ne ly it na b gy nj ng l nh ni">    distances = list(map(lambda i : ks_2samp(X_train.iloc[:,i],X_test.iloc[:,i]).statistic,range(n_features))) </span><span id="cddb" class="ne ly it na b gy nj ng l nh ni">    result.append((random_state,max(distances))) result.sort(key = lambda x : x[1])</span></pre><p id="e406" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">根据随机状态得出的距离，按升序排序为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/75800524b5698302b4edeaf8fa5e948e.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/0*VpwAUhPs4zYGKFN2.png"/></div></figure><p id="3fb7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以，给我们最低距离的随机状态是9。</p><p id="9d36" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看用这种随机状态构建的训练/测试数据集的“bp”特性。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="dae3" class="ne ly it na b gy nf ng l nh ni">idx = 0 </span><span id="dabf" class="ne ly it na b gy nj ng l nh ni">random_state = result[idx][0] </span><span id="12fa" class="ne ly it na b gy nj ng l nh ni">feature = 'bp' </span><span id="c1cc" class="ne ly it na b gy nj ng l nh ni">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state) </span><span id="7447" class="ne ly it na b gy nj ng l nh ni">df = pd.DataFrame({ feature:np.concatenate((X_train.loc[:,feature],X_test.loc[:,feature])), 'set':['training']*X_train.shape[0] + ['test']*X_test.shape[0] }) </span><span id="8a17" class="ne ly it na b gy nj ng l nh ni">sns.ecdfplot(data=df,x=feature,hue='set')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/9c957a446516046880cedd66a59ace36.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*vSvT54BeDZ15_wVV.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="4e32" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在曲线比以前重叠得多。</p><p id="3131" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，距离是:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="227b" class="ne ly it na b gy nf ng l nh ni">ks_2samp(X_train.loc[:,feature],X_test.loc[:,feature]).statistic </span><span id="274c" class="ne ly it na b gy nj ng l nh ni"># 0.07849721390855781</span></pre><p id="bddd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，它比原来的低。</p><p id="f652" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们现在在这些新的训练和测试数据集上训练模型，结果会显著改善:</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="9d04" class="ne ly it na b gy nf ng l nh ni">lr = LinearRegression() </span><span id="58d9" class="ne ly it na b gy nj ng l nh ni">lr.fit(X_train,y_train) </span><span id="9bb0" class="ne ly it na b gy nj ng l nh ni">r2_score(y_test,lr.predict(X_test)) </span><span id="191c" class="ne ly it na b gy nj ng l nh ni"># 0.5900352656383732</span></pre><p id="374f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们从40%上升到59%。一个很大的进步。</p><h1 id="b77f" class="lx ly it bd lz ma mu mc md me mv mg mh jz mw ka mj kc mx kd ml kf my kg mn mo bi translated">结论</h1><p id="d8b2" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">当我们必须训练我们的模型时，检查训练/测试分割是否被正确执行是至关重要的。本文中建议的过程基于Kolmogorov-Smirnov距离，并逐个考虑特征(避免相关性)。通过这个简单的过程，您可以创建统计上相似的数据集，从而提高模型的训练能力。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><p id="b34b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="nk">原载于2022年5月2日</em><a class="ae no" href="https://www.yourdatateacher.com/2022/05/02/are-your-training-and-test-sets-comparable/" rel="noopener ugc nofollow" target="_blank"><em class="nk">【https://www.yourdatateacher.com】</em></a><em class="nk">。</em></p></div></div>    
</body>
</html>