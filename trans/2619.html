<html>
<head>
<title>Understanding AutoEncoders with an Example: A Step-by-Step Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过示例了解自动编码器:一步一步的教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-autoencoders-with-an-example-a-step-by-step-tutorial-a79d2ea2945e#2022-06-07">https://towardsdatascience.com/understanding-autoencoders-with-an-example-a-step-by-step-tutorial-a79d2ea2945e#2022-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="19a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第二部分:可变自动编码器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e5a1e77dc6213b91b3d809256130ee6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6iIo1p66v7sYbqQu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@tine999?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tine ivani</a>拍摄的照片</p></figure><h1 id="a15d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e64b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自动编码器很酷，<strong class="lt iu">变型自动编码器更酷！</strong></p><p id="384d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是“<em class="ms">通过示例理解自动编码器</em>系列的第二篇(也是最后一篇)文章。在<a class="ae ky" href="https://medium.com/p/693c3a4e9836" rel="noopener"> <strong class="lt iu">第一篇文章</strong> </a>中，我们生成了一个<em class="ms">合成数据集</em>并构建了一个普通的自动编码器来重建圆的图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/a6af7786c9fb36525fd5dcfc9b87c11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Y9Cvo1gQIyidudEKSZvaA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个重建的圆圈，从第一个帖子开始。图片作者。</p></figure><blockquote class="mu mv mw"><p id="cfb5" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">我们将再次使用相同的合成数据集，因此如果需要的话，请查看“<a class="ae ky" href="https://medium.com/p/693c3a4e9836#d28b" rel="noopener">一个类似MNIST的圆形数据集</a>”部分进行复习。</p></blockquote><p id="6209" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我们将关注一种不同的自动编码器:<em class="ms">可变自动编码器(VAEs)。</em>我们还将了解什么是著名的<em class="ms">重新参数化技巧</em>，以及<em class="ms"> Kullback-Leibler发散/损失的作用。</em></p><p id="a919" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们邀请您在使用Google Colab运行随附笔记本的同时阅读这一系列文章，随附笔记本可从我的GitHub的“<a class="ae ky" href="https://github.com/dvgodoy/AccompanyingNotebooks" rel="noopener ugc nofollow" target="_blank">随附笔记本</a>”存储库中获得:</p><div class="na nb gp gr nc nd"><a href="https://colab.research.google.com/github/dvgodoy/AccompanyingNotebooks/blob/main/Understanding%20AutoEncoders.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">谷歌联合实验室——了解自动编码器</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">随附笔记本</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">colab.research.google.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr ks nd"/></div></div></a></div><p id="759f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，我建立了一个<em class="ms">目录</em>来帮助你浏览两篇文章的主题，如果你把它作为一个<strong class="lt iu">迷你课程</strong>并且一次一个主题地浏览内容的话。</p><h1 id="8e1f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><h2 id="66d2" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">第一部分:普通自动编码器</h2><ul class=""><li id="b41b" class="oe of it lt b lu lv lx ly ma og me oh mi oi mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#d28b" rel="noopener">一个类似MNIST的圆形数据集</a></li><li id="012b" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#9828" rel="noopener">编码器</a></li><li id="6ba1" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#dc2a" rel="noopener">潜在空间</a></li><li id="c66e" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#bcda" rel="noopener">解码器</a></li><li id="8513" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#1fef" rel="noopener">损失函数</a></li><li id="ae6b" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/693c3a4e9836#4600" rel="noopener">自动编码器(AE) </a></li><li id="4edb" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated">奖励:<a class="ae ky" href="https://medium.com/p/693c3a4e9836#1e0c" rel="noopener">自动编码器作为异常检测器</a></li></ul><h2 id="0a66" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">第二部分:变化的自动编码器(本文)</h2><ul class=""><li id="7854" class="oe of it lt b lu lv lx ly ma og me oh mi oi mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#0e6a" rel="noopener">变分自动编码器(VAE) </a></li><li id="1407" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#19b6" rel="noopener">重新参数化绝招</a></li><li id="7d7b" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#3443" rel="noopener">库尔贝克-莱布勒发散/损失</a></li><li id="3032" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#13ed" rel="noopener">损失的规模</a></li><li id="f7c8" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated"><a class="ae ky" href="https://medium.com/p/a79d2ea2945e#a1e5" rel="noopener">卷积变分自动编码器(CVAE) </a></li></ul><h1 id="0e6a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">变分自动编码器(VAE)</h1><p id="7fc3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在传统的自动编码器中，<em class="ms">潜在空间不是连续的</em>，也就是说，<strong class="lt iu">不仅在“边缘”之外的<em class="ms">处，而且在从训练集中的图像映射的点</em>之间的<em class="ms">处，都有<em class="ms">空的</em>潜在空间</em></strong>。这并不是说从“<em class="ms">空的</em>”空间中的点重建的图像只产生噪音/垃圾——在第一篇文章中我们甚至把它们弄出了一些圆圈——但是这并不能保证一个更复杂的问题。</p><blockquote class="mu mv mw"><p id="65eb" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">"那么我们可以让潜在空间<strong class="lt iu">连续</strong>吗？"</p></blockquote><p id="95e3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你打赌！让我们以这样的方式修改普通的自动编码器，<strong class="lt iu">不是在潜在空间中产生<em class="ms">点</em>，而是在潜在空间</strong>中产生<em class="ms">分布</em>。我给你介绍<strong class="lt iu">变型自动编码器(VAE) </strong>！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/baea6066996a125588203f5542a59f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TR5aWLPsWHr3MzG-p2At0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用规则采样的变分自动编码器。图片作者。</p></figure><p id="9926" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在VAE中，编码器不再输出潜在空间，但是<strong class="lt iu">产生<em class="ms">表示</em>和<em class="ms">标准偏差</em> </strong>，每个维度一个(与您为矢量<strong class="lt iu"> <em class="ms"> z </em> </strong>选择的大小一样多)。然后，每对平均值和标准偏差用于定义<strong class="lt iu">对应的分布</strong>(我们使用正态分布)，我们<strong class="lt iu">从该分布中进行采样，以获得向量<em class="ms"> z </em> </strong>的值。</p><blockquote class="ot"><p id="a78f" class="ou ov it bd ow ox oy oz pa pb pc mm dk translated">嘭！潜伏空间现在是连续的！</p></blockquote><p id="91ce" class="pw-post-body-paragraph lr ls it lt b lu pd ju lw lx pe jx lz ma pf mc md me pg mg mh mi ph mk ml mm im bi translated">此外，还记得我们在第一部分的<a class="ae ky" href="https://medium.com/p/693c3a4e9836#9828" rel="noopener">“编码器”部分将编码器的输出层(<code class="fe pi pj pk pl b"><strong class="lt iu">lin_latent</strong></code>)与模型的其余部分分开吗？现在你可以明白为什么了——在VAE中，我们<strong class="lt iu">使用基本模型的输出来馈送<em class="ms">两个</em>输出层:</strong> <code class="fe pi pj pk pl b"><strong class="lt iu">lin_mu</strong></code> <strong class="lt iu">和</strong> <code class="fe pi pj pk pl b"><strong class="lt iu">lin_var</strong></code>。</a></p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="ec79" class="ns la it pl b gy pq pr l ps pt">EncoderVar(<br/>  (base_model): Sequential(<br/>    (0): Flatten(start_dim=1, end_dim=-1)<br/>    (1): Linear(in_features=784, out_features=2048, bias=True)<br/>    (2): LeakyReLU(negative_slope=0.01)<br/>    (3): Linear(in_features=2048, out_features=2048, bias=True)<br/>    (4): LeakyReLU(negative_slope=0.01)<br/>  )<br/>  (lin_mu): Linear(in_features=2048, out_features=1, bias=True)<br/>  (lin_var): Linear(in_features=2048, out_features=1, bias=True)<br/>)</span></pre><p id="44d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">看起来不错吧？但是，有一个小问题… <strong class="lt iu">不能通过计算图</strong>中的<em class="ms">随机节点</em>反向传播！</p><p id="dc2d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">上图中的“随机节点”是我们的向量<strong class="lt iu"> <em class="ms"> z </em> </strong>，因为它在表示<strong class="lt iu">采样</strong>的<strong class="lt iu">红色箭头</strong>的接收端。</p><blockquote class="mu mv mw"><p id="ed1d" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“这是什么意思？”</p></blockquote><p id="4fd5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这意味着，每当分布的<strong class="lt iu">采样</strong>被引入到<strong class="lt iu">正向通道</strong>时，在该点之前使用的任何参数/权重对于<strong class="lt iu">反向推进</strong>程序而言都变得<strong class="lt iu">不可到达</strong>，因此在我们的情况下不可能训练编码器。</p><p id="5852" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我知道这看起来很糟糕，但幸运的是，使用一个小技巧就可以很容易地解决这个问题…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/7879760e52a531d8ccf3c45978ddd158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/0*ZCVjDdVM15bK-0vy.jpg"/></div></figure><h2 id="19b6" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">重新参数化技巧</h2><p id="9b2b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所以，问题是“<em class="ms">随机节点</em>”，对吗？如果我们从计算图中去掉随机性会怎么样？如果随机性来自输入而不是<strong class="lt iu">呢？它甚至不一定是用户提供的输入——从字面上看，它可能是一些<strong class="lt iu">内部随机输入</strong>！</strong></p><p id="1ccd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">事情是这样的:</p><ul class=""><li id="d0c6" class="oe of it lt b lu mn lx mo ma pv me pw mi px mm oj ok ol om bi translated">编码器继续像以前一样产生成对的平均值和标准偏差(两个与<strong class="lt iu"> <em class="ms"> z </em> </strong>大小相同的向量)；</li><li id="d180" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated">在<strong class="lt iu">编码器的正向传递</strong>中，我们<strong class="lt iu">从一个标准<strong class="lt iu">正态(<em class="ms"> mu </em> =0，<em class="ms"> std </em> =1)分布</strong>中抽取样本</strong>来构建一个<strong class="lt iu">随机向量(<em class="ms">ε</em>)</strong>与<strong class="lt iu"> <em class="ms"> z </em> </strong> — <strong class="lt iu">大小相同，即内部随机输入</strong>；</li></ul><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="79cb" class="ns la it pl b gy pq pr l ps pt"><strong class="pl iu">def forward(self, x):</strong><br/>    base_out = self.base_model(x)<br/>        <br/>    self.mu = self.lin_mu(base_out)<br/>    self.log_var = self.lin_var(base_out)<br/>    std = torch.exp(self.log_var/2)<br/>                <br/>    <strong class="pl iu">eps = torch.randn_like(self.mu)<br/>    z = self.mu + eps * std</strong></span><span id="857c" class="ns la it pl b gy py pr l ps pt">return z</span></pre><ul class=""><li id="7a37" class="oe of it lt b lu mn lx mo ma pv me pw mi px mm oj ok ol om bi translated">我们将随机(<code class="fe pi pj pk pl b"><strong class="lt iu">eps</strong></code>)向量和标准差(<code class="fe pi pj pk pl b"><strong class="lt iu">std</strong></code>)向量相乘，并将均值(<code class="fe pi pj pk pl b"><strong class="lt iu">mu</strong></code>)向量加到结果上——<strong class="lt iu">voilà</strong>，<strong class="lt iu">我们得到了全新的向量<em class="ms"> z </em> </strong>！</li></ul><p id="99cb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能想知道编码器实际产生的是什么，因为<strong class="lt iu">标准偏差(</strong> <code class="fe pi pj pk pl b"><strong class="lt iu">std</strong></code> <strong class="lt iu">)不是编码器</strong>的直接输出…</p><p id="014b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">编码器的输出实际上是<strong class="lt iu">对数方差</strong> ( <code class="fe pi pj pk pl b"><strong class="lt iu">log_var</strong></code>)，但是我们可以使用下面的表达式计算标准偏差(<code class="fe pi pj pk pl b"><strong class="lt iu">std</strong></code>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/0892d51f68de4bc2db947755cdafe65a.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*y7cFDfzTbC79Gxtk8wGcnQ.png"/></div></figure><blockquote class="mu mv mw"><p id="92a2" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“好，我明白了，但是<strong class="lt iu">为什么</strong>不直接求标准差呢？”</p></blockquote><p id="9d2d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">嗯，我们<em class="ms">不可能</em>对<strong class="lt iu">标准差</strong>有<strong class="lt iu">负值</strong>，对吧？但是线性输出层(<code class="fe pi pj pk pl b"><strong class="lt iu">lin_var</strong></code>)会输出满地的值，负的，正的。理论上，我们<em class="ms">可以</em>使用一个ReLU激活函数来只保持正值，当然，但这更多的是一种变通方法而不是解决方案。</p><p id="d769" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们将编码器 <strong class="lt iu">的<strong class="lt iu">输出视为方差</strong>的对数(而不是方差本身)，我们将有效地<strong class="lt iu">去除方差</strong>的所有负值(那些负值的取幂将总是产生(0，1)范围内的值)，更好的是，我们正在平滑地进行<em class="ms"/>，而不是作为截止<em class="ms">。</em>此外，由于每次对数在公式中出现都很典型，<em class="ms">它提高了数值的稳定性</em> :-)</strong></p><p id="3fed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">弄清楚这一点后，现在让我们看看这个图是什么样子的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/eb0a897c8f5c10e0454ce0487386eb0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OY_oETw8WLn2Vk0m2--SDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用重新参数化技巧的变分自动编码器！图片作者。</p></figure><p id="0514" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"><em class="ms">随机节点</em>不再是</strong>，因为红色箭头(采样)指向一个输入(ε)。计算图又是完整的——因此允许我们模型中的每个参数，从解码器到编码器，在反向投影过程中更新。简单说就是<em class="ms">再参数化绝招</em>！</p></div><div class="ab cl qb qc hx qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="im in io ip iq"><p id="2596" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">在PyTorch中，常规采样和使用重新参数化技巧采样的区别在于用于从</em> <a class="ae ky" href="https://pytorch.org/docs/stable/distributions.html#normal" rel="noopener ugc nofollow" target="_blank"> <em class="ms">正态分布</em></a><em class="ms">:</em><code class="fe pi pj pk pl b"><a class="ae ky" href="https://pytorch.org/docs/stable/_modules/torch/distributions/normal.html#Normal.sample" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">sample</strong></a><strong class="lt iu"> </strong></code><em class="ms">vs</em><code class="fe pi pj pk pl b"><a class="ae ky" href="https://pytorch.org/docs/stable/_modules/torch/distributions/normal.html#Normal.rsample" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">rsample</strong></a></code><em class="ms">中采样的方法。</em></p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="f181" class="ns la it pl b gy pq pr l ps pt"><strong class="pl iu"><em class="ms">def sample(self, sample_shape=torch.Size()):</em></strong><em class="ms"><br/>    shape = self._extended_shape(sample_shape)<br/>    with torch.no_grad():<br/>        return torch.normal(self.loc.expand(shape),<br/>                            self.scale.expand(shape))</em></span><span id="b65c" class="ns la it pl b gy py pr l ps pt"><strong class="pl iu"><em class="ms">def rsample(self, sample_shape=torch.Size()):</em></strong><em class="ms"><br/>    shape = self._extended_shape(sample_shape)<br/>    eps = _standard_normal(shape, <br/>                           dtype=self.loc.dtype,<br/>                           device=self.loc.device)<br/>    return self.loc + eps * self.scale</em></span></pre><p id="4085" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms"/><code class="fe pi pj pk pl b"><strong class="lt iu">rsample</strong></code><em class="ms">方法执行的操作与上面我们自己的正向传递完全相同。我们本可以使用</em> <code class="fe pi pj pk pl b"><strong class="lt iu">rsample</strong></code> <em class="ms">来代替自己编码，但是，出于教育目的，我认为最好在正向传递本身中显式地显示这个计算。</em></p></div><div class="ab cl qb qc hx qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="im in io ip iq"><blockquote class="mu mv mw"><p id="026b" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“就这些？真的吗？”</p></blockquote><p id="6fab" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">真的——不一定要复杂——这个想法相当简单优雅。</p><blockquote class="mu mv mw"><p id="487e" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">"那么，我们能对代码做这些改变并训练我们的VAE吗？"</p></blockquote><p id="7d32" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">别急，我们还需要讨论另一个损失，Kullback-Leibler散度/损失(简称KL损失)。</p><h2 id="3443" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">库尔贝克-莱布勒发散/损失</h2><p id="4660" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">KL <strong class="lt iu"> </strong>散度/损失是两个分布之间的<strong class="lt iu">相异度</strong>、<strong class="lt iu"> q(z) </strong>、<strong class="lt iu"> </strong>我们试图实现的分布，以及<strong class="lt iu"> p(z) </strong>，我们实际拥有的分布。我们在这里不讨论细节和公式，但是我们仍然需要知道，p(z)越接近q(z) 的<strong class="lt iu">，越低的<strong class="lt iu">的</strong>和<strong class="lt iu">的发散/损失</strong>。</strong></p><blockquote class="mu mv mw"><p id="4715" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">"好吧，但这如何适用于我们的VAE？"</p></blockquote><p id="987c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">分布<strong class="lt iu"> p(z) </strong>是我们使用编码器产生的一对<strong class="lt iu">平均值和标准差得到的<strong class="lt iu">正态分布</strong>，这是我们实际拥有的分布。</strong></p><p id="11d2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一个分布，<strong class="lt iu"> q(z) </strong>，是<strong class="lt iu">标准正态(<em class="ms"> mu </em> =0，<em class="ms"> std </em> =1)分布</strong>，我们试图达到的分布。</p><blockquote class="mu mv mw"><p id="1bd0" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">编码器产生的与标准正态(mu=0，std=1) 分布越相似的<strong class="lt iu"> KL发散/损耗</strong>将越<strong class="lt iu">低</strong>。</p><p id="fbff" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">因此，<strong class="lt iu">最小化KL损失</strong>意味着我们的<strong class="lt iu">编码器</strong>将为我们的<strong class="lt iu">潜在空间</strong>(向量<strong class="lt iu"> z </strong>)的每一个维度产生尽可能接近标准正态(mu=0，std=1) 的<strong class="lt iu">分布。</strong></p></blockquote><p id="b386" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于两种分布<strong class="lt iu"> p(z) </strong>和<strong class="lt iu"> q(z) </strong>都是<strong class="lt iu">正态</strong>，并且后者是标准正态分布，KL发散/损失由下面的<em class="ms">相对简单的</em>表达式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/804e3d73ccb92265bc6cfbb038abadfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*Lb21w3qJLrMgYREhBoIX3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些正态分布和标准正态之间的KL偏差/损失</p></figure><p id="fbd8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">或者，如果你喜欢用代码来写:</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="e6a6" class="ns la it pl b gy pq pr l ps pt"><strong class="pl iu">def kl_div(mu, std):</strong><br/>    kl_div = -0.5 * (1 + np.log(std**2) - mu**2 - std**2)<br/>    return kl_div</span></pre><p id="1508" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图说明了两种分布的KL散度/损失如何变化，橙色的<strong class="lt iu"> p(z) </strong>(根据相应的行和列具有不同的属性)，蓝色的<strong class="lt iu"> q(z) </strong>(标准正态分布):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a1c2679697700d8f8b4afa30c639d708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4FKZqACPNJF8_fu0pxtqtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">N(mu，std)和N(0，1)之间的KL散度。图片作者。</p></figure><p id="9d90" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是同样的数据，这一次是热图，为了更简洁的可视化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qj"><img src="../Images/138e7966249b19210f2557adaf51db4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckMCSKvLELCtYgJL4y-asQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">N(mu，std)和N(0，1)之间的KL散度。图片作者。</p></figure><blockquote class="mu mv mw"><p id="3193" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“我们为什么关心这个，我的意思是，为什么潜在空间的维度作为标准正态分布很重要？”</p></blockquote><p id="c196" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因为这将使我们以后的生活更容易——我们将能够使用一个众所周知和熟悉的分布——标准正态分布,从潜在空间中进行<strong class="lt iu">采样。此外，训练集中的图像将被映射到潜在空间的每个维度的标准法线的典型区间(-3.0到3.0)。这意味着<strong class="lt iu">如果我们的样本也在这个区间内</strong>，我们很有可能得到好的重建。</strong></p><p id="b58c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，我们终于可以对编码器的代码进行修改了，<em class="ms">variable-style</em>！我们需要修改<code class="fe pi pj pk pl b"><strong class="lt iu">forward()</strong></code> <strong class="lt iu">方法</strong>(如“重新参数化技巧”一节所述)并添加一个<code class="fe pi pj pk pl b"><strong class="lt iu">kl_loss()</strong></code> <strong class="lt iu">方法</strong>来使用我们的编码器产生的平均值和对数方差计算KL偏差/损失:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变型编码器！</p></figure><p id="e3c9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">解码器和自动编码器模型与第一部分中的标准自动编码器保持一致:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">可变自动编码器(VAE)</p></figure><blockquote class="mu mv mw"><p id="7bc6" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">"所以，我们把KL损失加到MSE(或BCE)损失上，我们就可以训练它了？"</p></blockquote><p id="549c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">是的，但是我们仍然需要考虑…</p><h2 id="13ed" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">损失的规模</h2><p id="1579" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们很容易忽略这个问题——我们太习惯于使用默认的"<em class="ms">均值</em>"作为PyTorch损失函数(如MSE或BCE)中的约简参数，以至于我们可能不会再考虑它。因此，如果我们在处理KL损失时简单地默认相同的程序，这不应该是令人惊讶的，对吗？</p><p id="1e53" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">嗯，是的，但这可能不会工作训练VAE。这就是为什么你会看到，在大多数关于VAE的教程中，<strong class="lt iu">减少损失使用的是“<em class="ms">总和</em>”</strong>而不是“<em class="ms">表示</em>”，尽管大多数时候，对其背后的推理只字未提。所以让我们仔细看看！</p><p id="7b76" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将从获取一批圆形图像开始，然后对它们进行编码，并使用我们的(未训练的)模型来重建它们。</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="e812" class="ns la it pl b gy pq pr l ps pt">x, y = next(iter(circles_dl))<br/>zs = encoder_var(x)<br/>reconstructed = decoder_var(zs)</span></pre><p id="415c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们将使用<code class="fe pi pj pk pl b"><strong class="lt iu">reduction='none'</strong></code>获得MSE损失的原始值:</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="ec97" class="ns la it pl b gy pq pr l ps pt">loss_fn_raw = nn.MSELoss(reduction=’none’)<br/>raw_mse = loss_fn_raw(reconstructed, x)<br/>raw_mse.shape<br/><strong class="pl iu">Output:<br/>torch.Size([32, 1, 28, 28])</strong></span></pre><p id="1d45" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不出所料，一批图像(32)中每个通道(1)的每个像素(28x28)都有一个平方误差，总共25，088个值。如果我们在损失函数中使用标准缩减(“<em class="ms">表示</em>或“<em class="ms">总和</em>”)，他们将简单地计算所有25，088个值的相应缩减，例如:</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="9c0d" class="ns la it pl b gy pq pr l ps pt">raw_mse.sum(), nn.MSELoss(reduction=’sum’)(reconstructed, x)<br/><strong class="pl iu">Output:<br/>(tensor(23960.8066, grad_fn=&lt;SumBackward0&gt;),<br/> tensor(23960.8066, grad_fn=&lt;MseLossBackward&gt;))</strong></span></pre><p id="b9ce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">绝配！但是我们不一定要像那样减少它，我们可以更有选择性一点，然后<strong class="lt iu">计算每幅图像</strong>像素的总和，然后<strong class="lt iu">才计算整批图像的平均值</strong>:</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="50e0" class="ns la it pl b gy pq pr l ps pt">sum_over_pixels = raw_mse.sum(dim=[1, 2, 3])<br/>sum_over_pixels.mean()<br/><strong class="pl iu">Output:<br/>tensor(748.7753, grad_fn=&lt;MeanBackward0&gt;)</strong></span></pre><p id="2c9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">看到了吗？那是完全不同的结果！我们也可以<em class="ms">翻转</em>减少量，首先计算像素的平均值，然后计算整批的总和。下表总结了四种可能的策略:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/79d23cfba547e96724d14678e971513c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*87LIzJvGeJKGP03kv_xHJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平方误差损失的缩减函数。图片作者。</p></figure><p id="32c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们对KL损失做同样的处理，使用在变分编码器中实现的<code class="fe pi pj pk pl b"><strong class="lt iu">kl_loss()</strong></code> <strong class="lt iu">方法</strong>获取其原始值:</p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="4d12" class="ns la it pl b gy pq pr l ps pt">raw_kl = encoder_var.kl_loss()<br/>raw_kl.shape<br/><strong class="pl iu">Output:<br/>torch.Size([32, 1])</strong></span></pre><p id="f4cf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于KL散度/损失是为潜在空间的每个维度(向量<strong class="lt iu"> <em class="ms"> z </em> </strong>)计算的，并且我们<strong class="lt iu"> </strong>选择<strong class="lt iu"> </strong>只有一个维度<strong class="lt iu"/>，所以批中的每个图像只有一个值。我们选择的缺点是，对向量<strong class="lt iu"><em class="ms"/></strong>(dim = 1)取“<em class="ms"> sum </em>”或“<em class="ms"> mean </em>”将总是产生相同的结果，如下表所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/5881b96994f2a0f5c1b5303cb3c90430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*0FxaXfoqK8LwsIVTDpGCVw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KL损失的缩减函数。图片作者。</p></figure><p id="352a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将对MSE和KL损失使用<strong class="lt iu">组合# 4</strong>，但是我鼓励你尝试其他组合，并了解它们如何影响模型训练。出于这个原因，<strong class="lt iu">我们将在训练循环中手动地、明确地减少两次每个损失:除了第一个维度，在每个维度上减少一次，在批次上再减少一次</strong>。</p><blockquote class="mu mv mw"><p id="d233" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“如果这些组合都没有产生好的结果呢？怎么办？”</p></blockquote><p id="dd89" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">问得好！事实证明，通常在重建损失中包含一个<strong class="lt iu">乘数。这个因素是<strong class="lt iu"> </strong> a <strong class="lt iu">超参数</strong>您可以在训练VAE时进行调整。</strong></p><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="95ab" class="ns la it pl b gy pq pr l ps pt">loss = loss_fn(yhat, x).sum(dim=[1, 2, 3]).sum(dim=0)<br/>kl_loss = model_vae.enc.kl_loss().sum(dim=1).sum(dim=0)<br/>total_loss = reconstruction_loss_factor * loss + kl_loss</span></pre><blockquote class="mu mv mw"><p id="639c" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">如果<strong class="lt iu">因子太高</strong>，则<strong class="lt iu">重建损失将支配训练</strong>，这就好像我们有一个<strong class="lt iu">普通自动编码器</strong>。</p><p id="9cdf" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">如果<strong class="lt iu">因子太低</strong>，则<strong class="lt iu"> KL损失将主导训练</strong>，并且<strong class="lt iu">重建图像不会有任何好处</strong>。</p></blockquote><h2 id="56f9" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">模特培训(VAE)</h2><p id="666c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们需要修改训练循环以考虑损失、重建和KL以及前者的倍增因子。修改属于第二步(第21至29行，计算损失)，并且仅属于第二步。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练变分自动编码器</p></figure><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="6e35" class="ns la it pl b gy pq pr l ps pt">Epoch 001 | Loss &gt;&gt; 4622.8542/4566.6690/56.1851<br/>Epoch 002 | Loss &gt;&gt; 625.7960/554.5768/71.2191<br/>...<br/>Epoch 029 | Loss &gt;&gt; 269.6513/198.1815/71.4698<br/>Epoch 030 | Loss &gt;&gt; 257.9317/182.5780/75.3537</span></pre><h2 id="c5a5" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">潜在空间分布(VAE)</h2><p id="b061" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们绘制一个潜在空间的直方图(<strong class="lt iu"> <em class="ms"> z </em> </strong>)(左图)，我们会看到，虽然它还不完全是正态分布，但它有点接近它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/1f5be111babd7d051e3e029a5bba946f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Msa0hYj1Ezy9qfgpiMBSlQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左图:潜在空间的分布；右图:潜在空间与半径。图片作者。</p></figure><p id="3a57" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，如果我们根据每个圆的相应半径绘制潜在空间，我们将得到右边的图。在半径的0.2标记处仍然有一个“<em class="ms">弯头</em>，但是垂直线已经消失了(来自在<a class="ae ky" href="https://medium.com/p/693c3a4e9836" rel="noopener">第一篇文章</a>中训练的普通自动编码器)，这意味着我们在重建图像的任何地方都不会看到圆形的“<em class="ms">混合</em>”。我们去看看吧！</p><h2 id="7d81" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">重建(VAE)</h2><p id="bb50" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们正在重建五幅图像，对应于第一篇文章中使用的潜在空间(-3.0，-0.5，0.0，0.9，3.0)中相同的五个点。</p><blockquote class="mu mv mw"><p id="acc6" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">别忘了我们的<strong class="lt iu">潜伏空间现在是连续的</strong>！</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/639e9c2d6742e582db8deb4f6f98612d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A49R95I_6r7O6khDXJQ8CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜在空间中所选点的重建图像(VAE)。图片作者。</p></figure><p id="d864" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在使用标准正态分布的极值([-3.0]和[3.0])的潜在空间中的点生成的图像上，仍然有一些噪声<strong class="lt iu"><em class="ms">；仍然有小半径([-0.5])的圆的一点点“混合”</em></strong><strong class="lt iu">；但它看起来肯定比普通的自动编码器好。</strong></p><p id="e96f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，是时候去<em class="ms">卷积</em>了！</p><h1 id="a1e5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">卷积变分自动编码器(CVAE)</h1><p id="4f9d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">直到最近，卷积神经网络(CNN)还是计算机视觉任务事实上的标准。虽然现在变压器被冠以这个头衔，但CNN仍然是有用、简单和快速的架构，所以我们将使用它们来构建卷积变分自动编码器(CVAE)。</p><p id="0795" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我假设您熟悉常规CNN，以及它们如何用于简单的图像分类任务，因为我们的编码器看起来非常像典型的CNN:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卷积变分编码器的基本模型</p></figure><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="8713" class="ns la it pl b gy pq pr l ps pt">EncoderVar(<br/>  (base_model): Sequential(<br/>    (0): Conv2d(1, 32, kernel_size=(3, 3), <br/>                stride=(1, 1), padding=(1, 1))<br/>    (1): LeakyReLU(negative_slope=0.01)<br/>    (2): Conv2d(32, 64, kernel_size=(3, 3), <br/>                stride=(2, 2), padding=(1, 1))<br/>    (3): LeakyReLU(negative_slope=0.01)<br/>    (4): Conv2d(64, 64, kernel_size=(3, 3), <br/>                stride=(2, 2), padding=(1, 1))<br/>    (5): LeakyReLU(negative_slope=0.01)<br/>    (6): Conv2d(64, 64, kernel_size=(3, 3),<br/>                stride=(1, 1), padding=(1, 1))<br/>    (7): LeakyReLU(negative_slope=0.01)<br/>    (8): Flatten(start_dim=1, end_dim=-1)<br/>  )<br/>  (lin_mu): Linear(in_features=3136, out_features=1, bias=True)<br/>  (lin_var): Linear(in_features=3136, out_features=1, bias=True)<br/>)</span></pre><p id="ab60" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">看到了吗？编码器的基本模型是CNN，变化部分由两个线性输出层给出，一个用于均值，另一个用于对数方差，就像我们以前的VAE一样。</p><p id="373a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而<strong class="lt iu">解码器是完全不同的</strong>，因为它是使用<strong class="lt iu">转置卷积</strong>构建的。要快速了解这些“逆卷积”，请查看我的博文:</p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/what-are-transposed-convolutions-2d43ac1a0771"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">什么是转置卷积？</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">了解“反向”卷积的工作原理</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="qp l no np nq nm nr ks nd"/></div></div></a></div><p id="7b26" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">转置卷积用于将图像从7x7像素一直放大到原始大小28x28:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">卷积解码器</p></figure><blockquote class="mu mv mw"><p id="bd38" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">“等等，现在有<strong class="lt iu">乙状结肠</strong>层了？”</p></blockquote><p id="510d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接得好！<strong class="lt iu">是的，有一个sigmoid层，但我们会继续使用MSE作为损失函数</strong>。</p><blockquote class="mu mv mw"><p id="df19" class="lr ls ms lt b lu mn ju lw lx mo jx lz mx mp mc md my mq mg mh mz mr mk ml mm im bi translated">我鼓励您尝试移除sigmoid层或将损失函数更改为BCE，或许还可以调整重建损失因子，然后训练模型，看看效果如何。</p></blockquote><h2 id="3c7a" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">模特培训(CVAE)</h2><p id="50d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">尽管我们对编码器和解码器都使用了更复杂的架构，但这仍然是一个变化的自动编码器，训练循环本身与以前完全相同:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk ql l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练卷积变分自动编码器(CVAE)</p></figure><pre class="kj kk kl km gt pm pl pn po aw pp bi"><span id="1bcd" class="ns la it pl b gy pq pr l ps pt">Epoch 001 | Loss &gt;&gt; 2188.3729/2118.5622/69.8107<br/>Epoch 002 | Loss &gt;&gt; 615.1194/611.0034/4.1160<br/>...<br/>Epoch 029 | Loss &gt;&gt; 172.8306/85.8559/86.9747<br/>Epoch 030 | Loss &gt;&gt; 169.7600/81.1885/88.5715</span></pre><h2 id="fd96" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">潜在空间分布(CVAE)</h2><p id="f208" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们绘制潜在空间的直方图(<strong class="lt iu"><em class="ms">【z】</em></strong>)(左图)，我们会看到，这一次，<strong class="lt iu">分布非常接近标准正态分布</strong>！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/b02d0ac39e29aa6e7803429c5ce625e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X0EBBKayoQgrY35aAv4sHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左图:潜在空间的分布；右图:潜在空间与半径。图片作者。</p></figure><p id="98b9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更好的是，如果我们将潜在空间与每个圆的相应半径对应起来，就不再有<strong class="lt iu">的“肘”</strong>——在潜在空间和半径这两者之间有一个大致的线性关系。</p><p id="1fc8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">重建的图像呢？</p><h2 id="7dbc" class="ns la it bd lb nt nu dn lf nv nw dp lj ma nx ny ll me nz oa ln mi ob oc lp od bi translated">重建(CVAE)</h2><p id="4ec0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果我们从标准正态分布中抽取一维潜在空间的样本，我们会在中心得到平均大小的圆，在分布的最左侧得到小圆，在分布的最右侧得到大圆。<strong class="lt iu">那个</strong>有多<em class="ms">牛逼</em>？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/edeec64229238e7a0fa6fc2ef41355a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXJe7kP8-GyzII6QcMwMgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">潜在空间中所选点的重建图像(CVAE)。图片作者。</p></figure><p id="61dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">自动编码器学会了在潜在空间中表示半径！</strong></p><p id="c884" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们在这里的工作已经完成了:-)</p><h1 id="477c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法</h1><p id="a0cf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">感谢您坚持到这篇长文的结尾:-)，但是，尽管它<em class="ms">是一篇长文，但它只是一个介绍，并且它只涵盖了您需要了解的开始试验各种自动编码器的基本工具和技术。</em></p><p id="f689" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你想了解更多关于自动编码器和生成模型的知识，我推荐大卫·福斯特的<em class="ms">生成深度学习</em>，作者是奥赖利。</p><p id="2fd7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你想了解更多PyTorch，计算机视觉，NLP的知识，给我自己的系列丛书，<a class="ae ky" href="https://pytorchstepbystep.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ms">深度学习用PyTorch循序渐进</em> </a>，一试:-)</p></div><div class="ab cl qb qc hx qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="im in io ip iq"><p id="6825" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">如果您有任何想法、意见或问题，请在下方留下评论或通过我的</em> <a class="ae ky" href="https://bio.link/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="ms">简历链接</em> </a> <em class="ms">页面联系。</em></p><p id="a43b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms">如果你喜欢我的帖子，请考虑使用我的推荐页面</em> <a class="ae ky" href="https://dvgodoy.medium.com/membership" rel="noopener"> <em class="ms">注册一个中级会员</em> </a> <em class="ms">来直接支持我的工作。对于每一个新用户，我从Medium中获得一小笔佣金:-) </em></p></div></div>    
</body>
</html>