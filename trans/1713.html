<html>
<head>
<title>Getting Started with Recurrent Neural Network (RNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络(RNNs)入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-recurrent-neural-network-rnns-ad1791206412#2022-04-22">https://towardsdatascience.com/getting-started-with-recurrent-neural-network-rnns-ad1791206412#2022-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4280" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用RNNs进行情感分析</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/85808b219062154d9165108fa5fa9178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dH9Cef71t0-mlNcV"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/photos/jL-gB8uM7NU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的<a class="ae kv" href="https://unsplash.com/@nishaan_ahmed" rel="noopener ugc nofollow" target="_blank"> Nishaan Ahmed </a>拍摄</p></figure><p id="9ca4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将讨论一组称为递归神经网络(RNNs)的独立网络，用于解决序列或时间序列问题。</p><p id="01ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧！</p><h1 id="bbdd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是递归神经网络？</h1><p id="f3f0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个<strong class="ky ir">循环神经网络</strong>是一种特殊类别的神经网络，允许信息双向流动。RNN具有短期记忆，这使它能够在产生输出时考虑先前的输入。短期记忆允许网络保留过去的信息，从而揭示彼此远离的数据点之间的关系。rnn非常适合处理时间序列和序列数据，如音频和文本。</p><h1 id="8b43" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">RNN结构</h1><p id="0feb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">下图显示了RNN的结构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/d020c4d162776165fb00123e97b24c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2b3fj1y2nohOnu1t"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg" rel="noopener ugc nofollow" target="_blank">维基媒体的RNN图像开源结构</a></p></figure><p id="043b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">展开的网络产生于为每个时间步长<em class="mq"> t </em>创建RNN的副本。ht表示网络在时间<em class="mq"> t，</em>的输出，而Xt是网络在时间<em class="mq"> t </em>的输入。</p><h1 id="43b6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">rnn的类型</h1><p id="ad6d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在让我们简单地提一下各种类型的rnn。</p><p id="a419" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">rnn主要有四种类型:</p><ul class=""><li id="e74f" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">单输入单输出的一对一</strong>。</li><li id="bce0" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">单输入多输出的一对多</strong>，例如图像字幕。</li><li id="3a67" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">多对一</strong>多输入单输出，例如情感分析。</li><li id="020e" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">多对多</strong>多输入多输出，例如机器翻译。</li></ul><h1 id="8538" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">递归神经网络是如何工作的？</h1><p id="2f15" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如你在上面展开的RNN中看到的，RNNs通过应用<strong class="ky ir">时间</strong> (BPPTT)反向传播来工作。以这种方式，当前和先前输入的权重被更新。通过将误差从最后一个时间步长传播到第一个时间步长来更新权重。因此，计算每个时间步长的误差。</p><p id="42fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">rnn不能处理长期依赖。在很长的时间步长中会出现两个主要问题:消失梯度和爆炸梯度。</p><h1 id="5039" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">RNN挑战——消失的梯度问题</h1><p id="a169" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">当用于计算权重的梯度开始<em class="mq">消失时，消失梯度出现，</em>意味着它们变成接近零的小数字。结果就是网络不学习。与此相反的是爆炸梯度问题。</p><p id="4068" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">爆炸梯度问题通过<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/clip_by_norm" rel="noopener ugc nofollow" target="_blank">剪切梯度</a>来解决。消失梯度问题由某种类型的rnn解决，这种rnn可以处理长期依赖性，例如，<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTMs) </a>。</p><h1 id="9b5b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">LSTMs</h1><p id="e2c4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">递归神经网络有几种变体。这里有几个例子:</p><ul class=""><li id="562c" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">长短期记忆(LSTM) </strong>通过引入<em class="mq">输入、输出、</em>和<em class="mq">忘记</em>门解决了消失梯度问题。这些网关控制着网络中保留的信息。</li><li id="a33c" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">门控循环单元(GRU) </strong>通过引入重置和更新门来处理消失梯度问题，重置和更新门决定在整个网络中保留哪些信息。</li></ul><p id="c49b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们主要关注LSTMs。LSTMs可以处理长期依赖关系，更适合处理长序列问题，如情感分析。例如，在一个长的电影评论中，在决定评论的情绪时，记住评论的开始是很重要的。想想评论，“这部电影相当慢，有些角色相当无聊，但总的来说，这是一部非常有趣的电影。”如果你只考虑这篇评论的第一部分，你会给它贴上负面评论的标签。然而，在评论快结束时，观众说这部电影很有趣。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/184894cf396680af0041302567fd487c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*NFo9WMVPdHQAEcrImi0VQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://commons.wikimedia.org/wiki/File:LSTM_cell.svg" rel="noopener ugc nofollow" target="_blank">维基媒体开源LSTM图片</a></p></figure><h1 id="92a5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为情感分析实现LSTMs</h1><p id="4120" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">接下来，让我们在TensorFlow中实现用于情感分析的LSTM。我们将使用Kaggle上著名的50K电影评论的IMDB数据集。该数据集在斯坦福大学网站上公开发布。</p><p id="cb28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用<a class="ae kv" href="http://layer.ai" rel="noopener ugc nofollow" target="_blank">层</a>获取数据并运行项目。因此，第一步是验证图层帐户并初始化项目。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="f956" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ni nj nk nl b">get_dataset</code>函数可用于获取数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/7eaeadfffc3947fdfc3f52b348ca1e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IcmVM_TePVp67RIN"/></div></div></figure><p id="187f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，让我们定义一个从评论中删除常用词的函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="e9f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们使用上面的函数来清理评论。我们可以保存清理后的数据集，这样就不会再次重复这个过程。</p><p id="dec9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据集保存到层是通过用<a class="ae kv" href="https://docs.app.layer.ai/docs/sdk-library/dataset-decorator" rel="noopener ugc nofollow" target="_blank"> @dataset </a>装饰器装饰函数来完成的。<code class="fe ni nj nk nl b">pip_requirements</code>装饰器用于传递运行该函数所需的包。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="e7b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行上述函数将保存数据并输出一个可用于查看的链接。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/bda80f7898ece636cbade85dde19345f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VV7nRfnNZLnL15bx"/></div></div></figure><h1 id="2516" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">文本预处理</h1><p id="67f0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">由于数据是文本形式的，我们需要将其转换成数字表示。此外，我们必须做到以下几点:</p><ul class=""><li id="2487" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">删除特殊字符和标点符号。</li><li id="90a1" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">将评论转换成小写。</li></ul><p id="b2c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用来自<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>的Tokenizer类来执行上述所有操作。这是通过创建该类的一个实例并在训练集上安装它的<code class="fe ni nj nk nl b">fit_on_texts</code>方法来实现的。在这样做的时候，我们定义了一个词汇表之外的标记，在以后将文本转换为序列时，这个标记将用于替换词汇表之外的单词。默认情况下，分词器会保留最常用的单词，但是，这可以使用<code class="fe ni nj nk nl b">num_words</code>参数来覆盖。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="3418" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此时，每个单词都被映射为一个整数表示。这被称为<em class="mq">单词索引</em>，可以通过调用分词器上的<code class="fe ni nj nk nl b"><em class="mq">word_index</em></code> <em class="mq"> </em>来查看。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/97aa64bf9be4b5bbc17f208b450b65b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GnFi0S8RVSaJT-SA"/></div></div></figure><p id="4afc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们可以保存这个标记器，这样我们就不必在这个数据集上再次训练标记器，那就太好了。让我们创建一个函数来保存这个记号赋予器。我们使用<a class="ae kv" href="https://docs.app.layer.ai/docs/sdk-library/model-decorator" rel="noopener ugc nofollow" target="_blank"> @model decorator </a>，因为该函数返回一个模型。<a class="ae kv" href="https://docs.app.layer.ai/docs/sdk-library/fabric-decorator" rel="noopener ugc nofollow" target="_blank"> @fabric decorator </a>决定了函数执行的环境类型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d912" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行上述函数将训练记号赋予器并保存它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="fd53" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">创建文本序列</h1><p id="5acb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">既然每个单词都有一个整数表示，我们需要为每个评论创建一个序列表示。这是由<code class="fe ni nj nk nl b">texts_to_sequences</code>函数完成的。我们获取刚刚保存的记号赋予器，并将其用于该操作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="47eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图显示了一个评论及其数字表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/ae9ee77d1b8d8663744d3e4851e7e744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p85Boz8BvSlaJ3yC"/></div></div></figure><h1 id="a527" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">填充文本序列</h1><p id="53f8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">处理这些评论的挑战之一是它们的长度不同。但是，我们稍后要构建的LSTM神经网络希望数据长度相同。我们通过定义每个评论的最大长度并截断它们来解决这个问题。之后，我们用零填充最大长度以下的评论。填充可以在评论的开始(前)或结束(后)进行。<code class="fe ni nj nk nl b">pad_sequences</code>是从<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences?authuser=1" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>中得到的预处理方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="5367" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">填充后的数据如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/c09144a88ee5822cda5cde67cd392e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wyeXdsIVT7EJwow6"/></div></div></figure><p id="ed0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在创建LSTM模型之前，让我们将数据捆绑到一个<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据集</a>中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="81e7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">定义LSTM模式</h1><p id="d9a1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">准备好数据后，我们可以创建一个简单的LSTM网络。为此，我们将使用<a class="ae kv" href="https://keras.io/api/models/sequential/" rel="noopener ugc nofollow" target="_blank"> Keras顺序API </a>。该网络将由以下主要构件组成:</p><ul class=""><li id="47fb" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">一个<strong class="ky ir"> </strong> <a class="ae kv" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">嵌入层</strong> </a> <strong class="ky ir">。</strong>单词嵌入是单词在密集向量空间中的表示。在该空间中，语义相似的单词一起出现。例如，这有助于情感分类，因为负面词汇可以捆绑在一起。</li></ul><p id="274c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Keras嵌入层希望我们传递词汇表的大小、密集嵌入的大小和输入序列的长度。该层还在迁移学习中加载预训练的单词嵌入权重。</p><ul class=""><li id="1e12" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><a class="ae kv" href="https://keras.io/api/layers/recurrent_layers/bidirectional/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">双向lstm</strong></a>允许数据从两边通过，即从左到右和从右到左。两个方向的输出连接在一起，但可以选择求和、求平均值或相乘。双向LSTMs帮助网络学习过去和未来单词之间的关系。</li></ul><p id="2712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当如下定义两个LSTM时，第一个必须返回将传递给下一个LSTM的序列。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d321" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们将上述操作捆绑到一个单一的训练函数中。该函数将返回LSTM模型。为了保存这个模型、它的度量和参数，我们用@model decorator包装它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="bf2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行上述函数训练并保存模型。它还记录函数中定义的所有项目，如训练和验证准确性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/8bd7d019de58c4cfaef082ff866ad983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weSLhhNXzXDfCpW2pZZE6A.png"/></div></div></figure><p id="657d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过训练的模型现在可以对新数据进行预测了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="6d9a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后的想法</h1><p id="1841" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们已经讨论了递归神经网络及其变体。特别是，我们涵盖了:</p><ul class=""><li id="519d" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">什么是递归神经网络？。</li><li id="b073" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">一种递归神经网络的结构。</li><li id="655a" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">与注册护士合作的挑战。</li><li id="a021" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">如何在TensorFlow和<a class="ae kv" href="http://layer.ai" rel="noopener ugc nofollow" target="_blank">层</a>实现LSTM？</li></ul><p id="4824" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.linkedin.com/in/mwitiderrick/" rel="noopener ugc nofollow" target="_blank">在LinkedIn </a>上关注我，了解更多技术资源。</p><h2 id="10a7" class="no lt iq bd lu np nq dn ly nr ns dp mc lf nt nu me lj nv nw mg ln nx ny mi nz bi translated">资源</h2><p id="13cf" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><a class="ae kv" href="https://colab.research.google.com/github/layerai/examples/blob/main/sentiment-analysis/sentiment_analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="7755" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mq">图片经许可使用。</em></p><p id="9d93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mq">数据集引用</em></p><pre class="kg kh ki kj gt oh nl oi oj aw ok bi"><span id="9d33" class="no lt iq nl b gy ol om l on oo">@InProceedings{maas-EtAl:2011:ACL-HLT2011,<br/>  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},<br/>  title     = {Learning Word Vectors for Sentiment Analysis},<br/>  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},<br/>  month     = {June},<br/>  year      = {2011},<br/>  address   = {Portland, Oregon, USA},<br/>  publisher = {Association for Computational Linguistics},<br/>  pages     = {142--150},<br/>  url       = {http://www.aclweb.org/anthology/P11-1015}<br/>}</span></pre></div></div>    
</body>
</html>