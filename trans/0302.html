<html>
<head>
<title>Artificial Neural Networks as universal function approximators</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">作为通用函数逼近器的人工神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/artificial-neural-networks-as-universal-function-approximators-a6ac6547a35f#2022-02-12">https://towardsdatascience.com/artificial-neural-networks-as-universal-function-approximators-a6ac6547a35f#2022-02-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4bd8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">人工神经网络目前非常流行，这是理所当然的。</h2></div><p id="df18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大型科技公司到处都在使用它们。例如，当你使用谷歌翻译时，或者当推荐出现在你的网飞 feed 上时，复杂的人工神经网络正在幕后使用。在<a class="ae le" href="https://en.wikipedia.org/wiki/AlphaGo" rel="noopener ugc nofollow" target="_blank">阿尔法围棋</a>在<a class="ae le" href="https://www.washingtonpost.com/news/innovations/wp/2016/03/15/what-alphagos-sly-move-says-about-machine-creativity/" rel="noopener ugc nofollow" target="_blank">围棋对抗李塞多尔</a>的比赛中取得成功的背后，安被用来确定下一步的最佳走法。</p><p id="8188" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我将讨论神经网络流行背后的原因</p><p id="6fdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">剧透警告:</strong>这与 ANN 是<strong class="kk iu">通用函数逼近器有关。</strong></p><p id="983e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我包含了 Julia 代码来说明实际中事情是如何工作的。我选择的工具是<a class="ae le" href="https://julialang.org/" rel="noopener ugc nofollow" target="_blank"> Julia </a>，因为它真的很快，而且是一种越来越流行的编程语言(如果你已经使用 Python，学习起来并不难)。对于机器学习任务来说，<a class="ae le" href="https://github.com/FluxML/Flux.jl" rel="noopener ugc nofollow" target="_blank"> Flux.jl </a>是一个非常好的选择，所以我们也来使用它。你可以在这里下载代码<a class="ae le" href="https://github.com/JulienPascal/ANN_Flux" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="d227" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">一.一些理论</h1><h1 id="89d8" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">体系结构</h1><p id="c406" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">如果你在这里结束，你可能已经有一些关于什么是人工神经网络的知识。所以我会很简短。简而言之，神经网络由几个相互连接的层组成。每一层都由节点组成。相邻层之间的节点相互交换信息。节点之间相互通信的方式由与每个节点相关联的参数值捕获。</p><p id="d253" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请参见下图:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/58955657379557fc56bdafa762252512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6TxBPlsUcrvSwxs4DiklQ.png"/></div></div></figure><p id="a050" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">来源:<a class="ae le" href="https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Artificial_neural_network.svg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Artificial _ neural _ network #/media/File:Artificial _ neural _ network . SVG</a></p><p id="ca66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">人工神经网络在高层次上模仿大脑的行为。大脑由神经元组成，神经元通过突触相互连接。我们的大脑非常擅长识别模式，所以人们可能希望人工神经网络可以成为一个很好的模式识别机器。</p><p id="6ac3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际情况就是这样。更好的是，我们有一些定理告诉我们，人工神经网络非常非常好。</p><h1 id="559a" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">通用逼近定理</h1><p id="fa97" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">让我描述两篇重要的论文。下面，我复制了他们摘要的一些精选部分:</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="1f67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">霍尼克·斯廷奇科姆和怀特(1989 年)</p><p id="5b25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“本文严格证明了使用任意压缩函数<strong class="kk iu">的具有少至一个隐藏层</strong>的<strong class="kk iu">标准多层前馈网络能够以任意期望的精度</strong>逼近从一个有限维空间到另一个有限维空间的任何 Borel 可测函数，只要有足够多的隐藏单元可用。从这个意义上说，<strong class="kk iu">多层前馈网络是一类通用逼近器。</strong>”</p><p id="ed59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf" rel="noopener ugc nofollow" target="_blank">巴伦(1993) </a></p><p id="23b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">表明具有一层 s 形非线性的前馈网络实现 O(1/n)阶的积分平方误差，其中 n 是节点的数量。</strong> […]对于本文研究的函数类，网络参数化的近似率和简约性在高维设置中具有惊人的优势<strong class="kk iu">。</strong></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="3b15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Hornik Stinchcombe 和 White (1989) 的论文告诉我们，一个非常大的函数类可以由具有上述结构的人工神经网络来近似。我们旨在逼近的基础函数只需要是“Borel 可测的”(从一个有限维空间到另一个空间)，它包含了几乎所有你在经济学中使用的有用函数(从一个有限维空间到另一个有限维空间的连续函数是 Borel 可测函数)。</p><p id="044d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Barron (1993) 的论文告诉我们，当处理多维数据时，人工神经网络是特别好的近似器。换句话说，ANN 可以帮助减轻维度诅咒。理解维数灾难的一种方法是，逼近一个函数所需的点数随着维数呈指数增长，而不是线性增长。我们想解释复杂的现象，有许多方面和相互作用，但传统的近似方法通常在这种情况下表现不佳。</p><p id="bdfe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，这些结果告诉我们，人工神经网络是非常好的函数逼近器，即使当维数很高时。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="bb62" class="lf lg it bd lh li mv lk ll lm mw lo lp jz mx ka lr kc my kd lt kf mz kg lv lw bi translated">二。应用</h1><p id="a432" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">现在我们来看两个应用。为了热身，我们将从一个平滑的函数开始。然后我们将转向一个更复杂的函数。</p><h1 id="b9df" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">二。a .简单功能</h1><p id="bf8a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">让我们加载一些有用的包，并定义我们想要近似的函数</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="6a1f" class="nf lg it nb b gy ng nh l ni nj"># Dependencies<br/>using Flux<br/>using Plots<br/>using LinearAlgebra<br/>using ProgressMeter<br/>using Statistics<br/>using LaTeXStrings<br/>using Surrogates<br/>gr()</span><span id="19aa" class="nf lg it nb b gy nk nh l ni nj"># Define function that we would like to learn with our neural network<br/>f(x) = x[1].^2 + x[2].^2</span><span id="6668" class="nf lg it nb b gy nk nh l ni nj">f (generic function with 1 method)</span></pre><p id="db2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">函数是无限维的对象。但是我们需要有限数量的值来训练我们的神经网络。为此，让我们从一个区间中创建一个样本点(我使用<a class="ae le" href="https://en.wikipedia.org/wiki/Sobol_sequence" rel="noopener ugc nofollow" target="_blank"> Sobol 采样</a>)，然后评估这些点的真实函数值。</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="136b" class="nf lg it nb b gy ng nh l ni nj">n_samples = 100<br/>lower_bound = [-1.0, -1.0]<br/>upper_bound = [1.0, 1.0]</span><span id="1eaa" class="nf lg it nb b gy nk nh l ni nj">xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())<br/>rawInputs = xys<br/>rawOutputs = [[f(xy)] for xy in xys] # Compute outputs for each input<br/>trainingData = zip(rawInputs, rawOutputs);</span></pre><p id="0c76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是决定我们的人工神经网络的架构的有趣部分。我选择两个隐藏层。第一层的节点数由输入的维度(2d 向量)以及最终节点的维度(标量)决定。我们仍然需要选择中间节点的数量。对于第一个隐藏层，我选择 784 个节点，第二个隐藏层选择 50 个节点。平心而论，这些选择有点随意(我是受了这里<a class="ae le" href="https://fluxml.ai/Flux.jl/stable/training/training/" rel="noopener ugc nofollow" target="_blank"> Flux.jl 教程</a>的影响)。随意尝试不同的价值观。</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="abf6" class="nf lg it nb b gy ng nh l ni nj"># Define the neural network layers (this defines a function called model(x))<br/># Specify our model<br/>dim_input = 2<br/>dim_ouptut = 1<br/>Q1 = 784;<br/>Q2 = 50;</span><span id="ee80" class="nf lg it nb b gy nk nh l ni nj"># Two inputs, one output<br/>model = Chain(Dense(2,Q1,relu),<br/>            Dense(Q1,Q2,relu),<br/>            Dense(Q2,1,identity));</span></pre><p id="7183" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来我们定义一个<strong class="kk iu">损失函数</strong>，它测量近似的精确度。损失越小越好。我们使用<a class="ae le" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a>损失函数。游戏的名字是寻找最小化损失函数的参数值。最小化损失函数的一种方法是使用<strong class="kk iu">梯度下降算法</strong>。</p><p id="2729" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是梯度下降的直观解释。想象一下，你在一座山顶上，那里有很多雾，让你看不到远处。你真的想下去。你该怎么办？</p><p id="aaac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个策略是看看你站在哪里，评估你所在位置附近<strong class="kk iu">最陡下降的方向(你看不到很远)。然后朝那个方向迈出一步。然后重复这个过程。如果山是“行为良好的”(它没有局部最小值)，你将设法下山，即使你只是在每一步使用局部信息。(在这篇博文的最底部可以看到一个非常简单的问题的梯度下降图)。</strong></p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="60a5" class="nf lg it nb b gy ng nh l ni nj"># Define loss function and weights<br/>loss(x, y) = Flux.Losses.mse(model(collect(x)), y)</span><span id="4ba6" class="nf lg it nb b gy nk nh l ni nj">lr = 0.001 # learning rate</span><span id="0715" class="nf lg it nb b gy nk nh l ni nj"># V1. Gradient descent<br/>opt = Descent(lr)</span><span id="06db" class="nf lg it nb b gy nk nh l ni nj"># V2. ADAM<br/>#decay = 0.9<br/>#momentum =0.999<br/>#opt = ADAM(lr, (decay, momentum))</span><span id="63d3" class="nf lg it nb b gy nk nh l ni nj">epochs = 1000 # Define the number of epochs<br/>trainingLosses = zeros(epochs);# Initialize a vector to keep track of the training progress</span></pre><p id="139c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是最有收获的一步:<strong class="kk iu">训练部分</strong>。下面的代码块执行梯度下降。函数<code class="fe nl nm nn nb b">Flux.train!</code>一次性使用样本中的所有观察值。因为一次迭代不足以达到最小值，所以我们重复这个过程几次<code class="fe nl nm nn nb b">epochs</code>。在每个时期之后，我们计算均方误差来看模型做得有多好。</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="f66d" class="nf lg it nb b gy ng nh l ni nj">ps = Flux.params(model) #initialize weigths<br/>p = Progress(epochs; desc = "Training in progress"); # Creates a progress bar<br/>showProgress = true</span><span id="84e7" class="nf lg it nb b gy nk nh l ni nj"># Training loop<br/>@time for ii in 1:epochs</span><span id="787d" class="nf lg it nb b gy nk nh l ni nj">    Flux.train!(loss, ps, trainingData, opt)</span><span id="9267" class="nf lg it nb b gy nk nh l ni nj">    # Update progress indicator<br/>    if showProgress<br/>        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])<br/>        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)<br/>    end</span><span id="59fd" class="nf lg it nb b gy nk nh l ni nj">end</span><span id="1100" class="nf lg it nb b gy nk nh l ni nj"> 24.753884 seconds (41.00 M allocations: 37.606 GiB, 6.56% gc time, 0.48% compilation time)</span></pre><p id="df07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一个图显示了原始函数和人工神经网络返回值(点)的曲面图。效果相当不错。右上图显示了训练进行时损失函数的值。梯度下降似乎工作得很好，因为损失函数以良好的单调方式降低。底部图显示了经过训练的人工神经网络的表面图。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi no"><img src="../Images/602843e0bdb6f26c931be49e81c7b087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPE6402uDatTX_Y-mCeKyQ.png"/></div></div></figure><h1 id="c1e4" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">更具挑战性的职能</h1><p id="a7fd" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">好了，我们的人工神经网络使用一个简单的函数，这是令人放心的。现在让我们转向一个更具挑战性的函数。例如，我们可以尝试逼近<a class="ae le" href="https://en.wikipedia.org/wiki/Ackley_function" rel="noopener ugc nofollow" target="_blank">阿克利函数</a>，这是一个有点疯狂的函数，通常用于测试最小化算法(它在原点有一个全局最小值)。</p><p id="7c36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使是更复杂的函数，我们的人工神经网络也能很好地逼近真实函数，如下图所示。</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="87c9" class="nf lg it nb b gy ng nh l ni nj">function ackley(x; e = exp(1), a = 10.0, b = -0.2, c=2.0*π)<br/>    #a, b, c = 10.0, -0.2, 2.0*π<br/>    len_recip = inv(length(x))<br/>    sum_sqrs = zero(eltype(x))<br/>    sum_cos = sum_sqrs<br/>    for i in x<br/>        sum_cos += cos(c*i)<br/>        sum_sqrs += i^2<br/>    end<br/>    return -a * exp(b * sqrt(len_recip*sum_sqrs)) - exp(len_recip*sum_cos) + a + e<br/>end</span><span id="c09d" class="nf lg it nb b gy nk nh l ni nj">n_samples = 1000<br/>lower_bound = [-2.0, -2.0]<br/>upper_bound = [2.0, 2.0]<br/>xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())<br/>rawInputs = xys</span><span id="b432" class="nf lg it nb b gy nk nh l ni nj">rawOutputs = [[ackley(xy)] for xy in xys] # Compute outputs for each input<br/>trainingData = zip(rawInputs, rawOutputs);</span><span id="32e8" class="nf lg it nb b gy nk nh l ni nj"># Define the neural network layers (this defines a function called model(x))<br/># Specify our model<br/>Q1 = 784;<br/>Q2 = 50;<br/>Q3 = 10;</span><span id="5b2b" class="nf lg it nb b gy nk nh l ni nj"># Two inputs, one output<br/>model = Chain(Dense(2,Q1,relu),<br/>            Dense(Q1,Q2,relu),<br/>            Dense(Q2,1,identity));</span><span id="8cc4" class="nf lg it nb b gy nk nh l ni nj"># Define loss function and weights<br/>loss(x, y) = Flux.Losses.mse(model(collect(x)), y)<br/>ps = Flux.params(model)</span><span id="cdae" class="nf lg it nb b gy nk nh l ni nj"># Train the neural network<br/>epochs = 1000<br/>showProgress = true<br/>lr = 0.001 # learning rate</span><span id="16ba" class="nf lg it nb b gy nk nh l ni nj"># Gradient descent<br/>opt = Descent(lr)</span><span id="dd14" class="nf lg it nb b gy nk nh l ni nj">trainingLosses = zeros(epochs) # Initialize vectors to keep track of training<br/>p = Progress(epochs; desc = "Training in progress") # Creates a progress bar</span><span id="055f" class="nf lg it nb b gy nk nh l ni nj">@time for ii in 1:epochs</span><span id="4f54" class="nf lg it nb b gy nk nh l ni nj">    Flux.train!(loss, ps, trainingData, opt)</span><span id="a017" class="nf lg it nb b gy nk nh l ni nj">    # Update progress indicator<br/>    if showProgress<br/>        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])<br/>        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)<br/>    end</span><span id="4954" class="nf lg it nb b gy nk nh l ni nj">end</span><span id="b951" class="nf lg it nb b gy nk nh l ni nj">242.064635 seconds (407.63 M allocations: 375.931 GiB, 6.81% gc time, 0.04% compilation time)</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi no"><img src="../Images/bb7df0f3eddb50f3d619bfd067942263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oYwIVp7Pvh24JYKCj0B59w.png"/></div></div></figure><h1 id="575c" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">结论</h1><p id="91a1" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">人工神经网络是<strong class="kk iu">通用函数逼近器。这篇博文解释了这意味着什么，并展示了如何开始使用人工神经网络来逼近相对简单的函数。</strong></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><p id="6b51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇博文最初发布在我的网站上:</p><p id="4025" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://julienpascal.github.io/post/ann_1/" rel="noopener ugc nofollow" target="_blank">https://julienpascal.github.io/post/ann_1/</a></p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="14ac" class="lf lg it bd lh li mv lk ll lm mw lo lp jz mx ka lr kc my kd lt kf mz kg lv lw bi translated">额外:视觉梯度下降</h1><p id="c30d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">下面是梯度下降的图解。我们想找到函数<code class="fe nl nm nn nb b">J(x)=x^2</code>的最小值，我们从点<code class="fe nl nm nn nb b">-20</code>开始。</p><p id="bc5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该算法迭代进行:</p><ol class=""><li id="748c" class="np nq it kk b kl km ko kp kr nr kv ns kz nt ld nu nv nw nx bi translated">在当前值下计算<strong class="kk iu">梯度</strong>。这给了我们函数<code class="fe nl nm nn nb b">J</code>最大变化的方向。</li><li id="afe6" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">因为我们在寻找一个最小值，而不是最大值，所以向最大变化的相反方向迈出一步</li><li id="45df" class="np nq it kk b kl ny ko nz kr oa kv ob kz oc ld nu nv nw nx bi translated">重复步骤 1–2</li></ol><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="8f76" class="nf lg it nb b gy ng nh l ni nj">using GradDescent<br/>#Code from here: https://jacobcvt12.github.io/GradDescent.jl/stable/<br/>#I made only slight modifications to the original code</span><span id="6258" class="nf lg it nb b gy nk nh l ni nj"># objective function and gradient of objective function<br/>J(x) = x^2<br/>dJ(x) = 2*x</span><span id="295a" class="nf lg it nb b gy nk nh l ni nj"># number of epochs<br/>epochs = 150</span><span id="abd2" class="nf lg it nb b gy nk nh l ni nj"># instantiation of Adagrad optimizer with learning rate of 2<br/>opt = Adagrad(η=2.0)</span><span id="f8f0" class="nf lg it nb b gy nk nh l ni nj"># initial value for x (usually initialized with a random value)<br/>x = -20.0 #initial position on the function<br/>values_x = zeros(epochs) #initialization<br/>value_y = zeros(epochs) #initialization<br/>iter_x = zeros(epochs) #initialization</span><span id="48d2" class="nf lg it nb b gy nk nh l ni nj">for i in 1:epochs<br/>    # Save values for plotting<br/>    values_x[i] = x<br/>    value_y[i] = J(x)<br/>    iter_x[i] = i</span><span id="5380" class="nf lg it nb b gy nk nh l ni nj">    # calculate the gradient wrt to the current x<br/>    g = dJ(x)</span><span id="314b" class="nf lg it nb b gy nk nh l ni nj">    # change to the current x<br/>    δ = update(opt, g)<br/>    x -= δ<br/>end</span></pre><p id="fc16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你在下图中看到的，我们从左手边开始，然后向右做了一些很大的移动。久而久之，这些点从黄色变成了深色。大约 150 次迭代后，我们非常接近 0 处的真实最小值。</p><pre class="md me mf mg gt na nb nc nd aw ne bi"><span id="33e8" class="nf lg it nb b gy ng nh l ni nj">plot(values_x, value_y, label="J(x)")<br/>scatter!(values_x, value_y, marker_z = iter_x, color = cgrad(:thermal, rev = true), label="Position", colorbar_title="Iteration")<br/>xlabel!(L"x")<br/>ylabel!(L"J(x)")</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8a2d8696b09549b82bf2a1d901012566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*HPu5_POEI75dtptwN7SSGw.png"/></div></figure></div></div>    
</body>
</html>