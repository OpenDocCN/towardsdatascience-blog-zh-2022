<html>
<head>
<title>Automatic Evaluation of Synthesized Speech</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">合成语音的自动评估</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatic-evaluation-of-synthesized-speech-354f7a2a20d7#2022-11-28">https://towardsdatascience.com/automatic-evaluation-of-synthesized-speech-354f7a2a20d7#2022-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c54" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解深度学习和预训练模型如何用于评估自动语音的指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8a5dc002893dc3b7aeda09d0261558da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9r_HRn87W1jD8kCC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Vika Strawberrika 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1060" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语音合成，或文本到语音(TTS)，是一项有几十年历史的技术，由于深度学习提供的巨大改进，它在最近几年强势回归。</p><p id="6f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着时间的推移，合成语音听起来越来越自然，越来越难与人声区分开来。</p><p id="a5ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是总的趋势，但是不同的系统达到不同的质量水平，并且在科学努力中测量进展是重要的。不幸的是，直到最近，衡量 TTS 质量的唯一方法是请一组人类法官进行听力测试。他们会听合成的声音，并给声音的自然度打个分，通常在 1 到 5 分之间。</p><p id="eb78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程听起来很清楚，但是它有很多缺点。首先，该过程缓慢且昂贵，因此在开发过程中不能经常执行。这与许多其他领域不同，例如机器翻译或语音识别，在这些领域，像<a class="ae ky" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank"> BLEU </a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Word_error_rate" rel="noopener ugc nofollow" target="_blank"> WER </a>这样的自动化指标伴随研究人员数十年来快速衡量他们的进步。二是主观性强，不可复制。两套不同的评委会给出不同的分数，所以进入不同科学出版物的结果是无法相互比较的。第三，这个过程很嘈杂，因为我们人类不善于考虑单一因素，而忽略更广泛的背景。举例来说，有语法错误的好听声音可能会得到低分。</p><p id="cf62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，希望有一种自动度量来克服这些问题。文献中的一个答案是利用深度学习模型自动计算 MOS。</p><h1 id="b629" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">平均意见得分</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/23870dc18b34cbed195584c7ac4010fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nQWp1Tzv2564JXxy"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@soundtrap?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">声阱</a>拍摄的照片</p></figure><p id="1238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Mean_opinion_score" rel="noopener ugc nofollow" target="_blank">平均意见得分</a> (MOS)是一种历史悠久的电话通话质量测量方法。它仅仅是 N 个人类裁判给出的分数的平均值。在 TTS 中，通常要求评委在 1 到 5 的范围内评估声音的自然度，其中 1 表示“绝对机器人化”，5 表示“绝对自然”。</p><p id="0018" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，这是一种昂贵、缓慢且有噪声的测量方法。人类有不同的判断和不同的期望，这些都反映在他们的个人得分上。由于这些原因，测量许多人的综合得分是很重要的。</p><p id="fc47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，系统的 MOS 易受时间变化的影响，因为技术的改进改变了对其质量的期望。此外，它还受法官的人口统计数据的影响。年龄、对技术的接触和文化背景都是影响他们将分配的分数的因素。例如，说第二语言的人通常会比母语为 TTS 语言的人给出更高的分数，因为他们会发现更多的发音错误。</p><h1 id="7bd4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">自动 MOS</h1><p id="d6a0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一种使评估过程更加自动化的解决方案是训练机器学习模型来再现人类得分。作为参考，我们将遵循 Erica Cooper 及其同事的“MOS 预测网络的泛化能力”。这不是第一篇关于这个话题的论文，但我发现它真的很有用。</p><p id="7d7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇论文是关于 MOS 预测网络的训练技术以及它们如何推广到未知领域的有趣研究。在这篇文章中，我将强调最突出的几点。我将报告我认为最有趣的想法，并尝试为不熟悉该主题的读者解释较难的部分。我推荐感兴趣的读者去看看这篇论文。这篇文章可以看作是对这个主题的介绍和阅读指南。</p><h1 id="0e4a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据集</h1><p id="2b13" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">与任何机器学习模型一样，自动 MOS 预测需要数据集来训练模型，并在训练训练阶段期间和之后对其进行评估。</p><p id="1db9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集基于自 2008 年以来多年进行的大量听力测试。主数据集被称为<strong class="lb iu"> BVCC </strong>，它是由作者创建的，然后他们还使用了三个额外的域外数据集进行测试。</p><h2 id="965c" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">术语</h2><p id="a5e2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在继续之前，澄清一些将在下文中使用的术语是有用的。</p><p id="fb9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<strong class="lb iu">听众</strong>是人类评价中的法官。他/她提供网络旨在模拟的评级。</p><p id="67ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">扬声器</strong>是用于训练 TTS 模型的配音演员。通常，TTS 模型是根据一组预定义的声音进行训练的，并且只能产生这些声音。</p><p id="462b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<strong class="lb iu">系统</strong>是一个带有推理算法的 TTS 模型。</p><p id="7f59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<strong class="lb iu">文本</strong>仅仅是用来合成声音的文本。这将在下文中考虑，因为对于 TTS 模型来说，在训练期间生成看不见的单词是具有挑战性的。</p><h2 id="f668" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">BVCC</h2><p id="7c49" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">BVCC 之所以被这样称呼，是因为它主要是由 TTS 的<a class="ae ky" href="https://www.synsig.org/index.php/Blizzard_Challenge_2021" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">B</strong>lizzard Challenge</a>和<a class="ae ky" href="https://github.com/nii-yamagishilab/VCC2020-database" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">V</strong>office<strong class="lb iu">C</strong>on version<strong class="lb iu">C</strong>hallenge</a>以及用开源工具<a class="ae ky" href="https://github.com/espnet/espnet" rel="noopener ugc nofollow" target="_blank"> ESPnet </a>生成的公开可用的句子构建而成。由于数据来自多个语境，作者进行了一次新的听力测试，对样本进行统一评分。</p><p id="314d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们总计超过 7000 个样本，由 8 位评委、27 位扬声器和 187 个测试系统进行评分。</p><h2 id="631a" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">域外数据</h2><p id="7861" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这种情况下，域外数据是未进行额外听力测试的注释数据(合成语音+人类评级)。它们在域外，因为它们的上下文不同于训练数据的上下文。如前所述，评级会因人而异，并随着时间的推移而变化，因此我们可以预期评级分布与培训数据的分布不同。</p><p id="9cc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，还有一个日语数据集和一个汉语语音数据集，这两个数据集在语言领域之外。</p><h2 id="47ca" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">培训/开发/测试设备</h2><p id="6729" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在划分数据集时，一个目标是每个子集匹配全局数据集的 MOS 分布；此外，它们的目标是匹配每个话语的评分标准偏差的分布。后一点的原因是，一些系统经常收到比其他系统更多样化的评级，这应该反映在所有子集中，以便进行正确的评估。</p><p id="0b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，开发和测试子集应该有看不见的演讲者、系统、听众和文本。如果一个实体不在训练集中，它对开发集是不可见的。如果一个实体既不存在于训练集也不存在于开发集中，那么它对测试集是不可见的。</p><p id="5b40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，他们首先将发言者、系统、听众或文本分配给开发或测试集，不进行替换，然后用剩余的材料将其余部分填充到所需的数量，从而生成多个候选拆分。然后使用总数据分布和每个子集之间的<a class="ae ky" href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance" rel="noopener ugc nofollow" target="_blank">推土机距离</a> (EMD)的总和来评估候选分割。它是针对单个样本的分数以及标准差计算的。</p><p id="3229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们用不同的随机种子计算了 1000 个分裂，然后选择了 EMD 总和最低的一个，因为这意味着它是全局分布和每个子集之间距离最低的分裂。</p><p id="3493" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">域外数据集被分成大小相等的训练集、开发集和测试集。然后，考虑两种不同的情况:</p><ul class=""><li id="84f3" class="nf ng it lb b lc ld lf lg li nh lm ni lq nj lu nk nl nm nn bi translated"><strong class="lb iu"> Zero-shot: </strong>在域外测试集上对系统进行评估，之前没有对其训练数据进行任何类型的训练。</li><li id="0f7a" class="nf ng it lb b lc no lf np li nq lm nr lq ns lu nk nl nm nn bi translated"><strong class="lb iu">微调:</strong>系统首先使用域外数据集的训练集和开发集进行训练，然后在其测试集上进行评估。</li></ul><h2 id="80ec" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">预训练模型的作用</h2><p id="e3cd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">从到目前为止的讨论中，我们理解为自动 MOS 的任务获取训练数据是困难和昂贵的。这项研究的作者发现，仅仅根据 BVCC 的数据从零开始训练一个 MOSNet [2]并不太好。</p><p id="ee12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更有效的策略在于使用在大量音频数据上预先训练的模型，并在相同的 BVCC 数据上对其进行微调。Wav2vec2 [3]似乎符合目标。它最初是在大型音频数据上以自我监督的学习目标进行训练的，并且已经在其他具有音频输入的任务上成功地进行了微调。说话者、情绪和音素分类都是受益于使用预先训练的 wav2vec2 模型的任务。此外，它在<a class="ae ky" href="https://github.com/facebookresearch/fairseq" rel="noopener ugc nofollow" target="_blank"> fairseq </a>工具包和<a class="ae ky" href="https://huggingface.co/docs/transformers/model_doc/wav2vec2" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚</a>中免费提供代码和预训练模型。</p><p id="8e03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者还用 XLSR [4]进行了实验，这是另一个大型预训练模型，它是针对多语言音频数据进行训练的。据推测，多语言训练对语音识别是有用的，但从目前的研究结果来看，情况并非如此。XLSR 显然比从零开始训练要好，但只在针对日本人的零射实验中胜过 wav2vec2。但 wav2vec2 在实验条件上更好，对日本小数据进行微调，对中国人同时进行零炮和微调。</p><h1 id="f8cd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">挑战</h1><p id="32bb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这篇论文的进一步分析清楚地表明，这种自动 MOS 网络很难预测看不见的说话者、听者、系统和文本的正确结果，但方式不同。</p><p id="6c0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看不见的<em class="nt">系统</em>绝对是最困难的挑战，所以如果可能的话，在你想要评估的系统上微调你的 MOS 预测器总是一个好主意。看不见的<em class="nt">文本</em>和<em class="nt">监听器</em>只对 6 个测试系统中的 2 个系统特别困难，而看不见的<em class="nt">扬声器</em>只对没有微调的预训练模型有问题。</p><h1 id="b44e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="5785" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">用于 TTS 模型评估的自动 MOS 预测是一项具有挑战性的任务。训练数据的获取必须遵循严格的准则，并考虑许多变量，如各种测试系统、扬声器、听众和文本。</p><p id="1037" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，很明显，系统的零射击评估特别困难。在快速开发环境中，最好的评估方法是为新系统收集一些训练数据进行评估。然后，根据这些新数据微调 MOS 预测器，最后对系统进行大规模评估。</p><p id="d3d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与语音识别的单词错误率等指标相比，这种方法仍然很慢，但预训练模型的存在使其比从头开始训练好得多。</p><p id="1a1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">研究更好的系统零射击评估方法将会改变这一领域的游戏规则。</p><h1 id="7e76" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">更多来自我</h1><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/tips-for-reading-and-writing-an-ml-research-paper-a505863055cf"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">阅读和撰写 ML 研究论文的技巧</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">从几十次同行评审中获得的经验教训</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/parse-dont-validate-f559372cca45"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">解析，不验证| Python 模式</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">验证数据就像看门人一样，将数据解析成有意义的数据类型，为原始数据增加了有价值的信息</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="om l oi oj ok og ol ks nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/data-processing-automation-with-inotifywait-663aba0c560a"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">使用 inotifywait 实现数据处理自动化</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">如何在拥有生产就绪的 MLOps 平台之前实现自动化</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="on l oi oj ok og ol ks nx"/></div></div></a></div><h1 id="c7f1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考文献(带链接)</h1><p id="dc5a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] Cooper，Erica，等.“MOS 预测网络的泛化能力”<em class="nt">ICA ssp 2022–2022 IEEE 声学、语音和信号处理国际会议(ICASSP) </em>。IEEE，2022。https://arxiv.org/abs/2110.02635v3<a class="ae ky" href="https://arxiv.org/abs/2110.02635v3" rel="noopener ugc nofollow" target="_blank"/></p><p id="e8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] Lo，Chen-Chou，等.“MOSNet:基于深度学习的语音转换客观评估”<em class="nt"> Proc。散客 2019</em>(2019):1541–1545。<a class="ae ky" href="https://arxiv.org/abs/1904.08352" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1904.08352</a></p><p id="f714" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]巴耶夫斯基、阿列克谢等，《wav2vec 2.0:语音表征的自我监督学习框架》<em class="nt">神经信息处理系统进展</em>33(2020):12449–12460。<a class="ae ky" href="https://arxiv.org/abs/2006.11477" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.11477</a></p><p id="0938" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Conneau，Alexis 等，“用于语音识别的无监督跨语言表征学习”<em class="nt"> arXiv 预印本 arXiv:2006.13979 </em> (2020)。<a class="ae ky" href="https://arxiv.org/abs/2006.13979" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.13979</a></p><h1 id="3466" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">感谢您的阅读</h1><p id="0579" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这不是一篇简单易懂的文章，所以非常感谢你能坚持到现在。敬请关注我的更多内容！</p></div></div>    
</body>
</html>