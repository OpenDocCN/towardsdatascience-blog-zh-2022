<html>
<head>
<title>Semi-Supervised Approach for Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器的半监督方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semi-supervised-approach-for-transformers-38722f1b5ce3#2022-05-03">https://towardsdatascience.com/semi-supervised-approach-for-transformers-38722f1b5ce3#2022-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="41f4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种训练基于变压器的模型的实用方法，通过半监督方法实现鲁棒性和高精度。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1a4073baa50162c3c4fc4384f9b7528f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWeMU99YSCE4BPEPotLg_A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">阿瑟尼·托古列夫在<a class="ae kv" href="https://unsplash.com/s/photos/transformers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a4e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变形金刚模型已经成为NLP任务的首选模型。在本文中，我们将按照自上而下的方法，使用Transformers架构来完成训练高度健壮的NLP模型的端到端过程。我们总是尝试使用半监督方法来训练NLP模型，无论是分类还是生成。</p><p id="e198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将主要讨论微调BERT和文本分类。让我们先了解一下总体情况，然后再谈细节。</p><p id="8d20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于文本数据增强，请阅读这篇<a class="ae kv" href="https://medium.com/analytics-vidhya/data-augmentation-for-text-with-code-6da46aad443d" rel="noopener">文章</a>。</p><h2 id="c347" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">了解半监督变压器架构</strong></h2><p id="09a8" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">训练半监督模型需要2个步骤:</p><ul class=""><li id="0c83" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">先训练无监督的部分。那就是训练语言模型。对于变压器，可以微调伯特、GPT等。对于这部分训练，我们只需要未标注的数据或句子。这一步调整模型的权重，以便它能够理解我们想要训练它的数据的上下文。上下文可以是时尚、新闻、体育等。例如，当我们试图训练一个“时尚”模特或“新闻”模特时，“睡觉”这个词可以有不同的上下文。</li><li id="053d" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">其次是培训被监督的部分。也就是用标注的数据训练模型。如果第一部分具有大量的数据，则受监督部分即使具有少量的数据也会具有高精度，但反之则不成立。</li></ul><p id="afe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图是半监督训练的GIF。首先，我们微调BERT，然后我们训练一个文本分类器或命名实体识别模型。它给出了在微调语言模型和然后使用微调模型进行分类之间的联系的直观想法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/02d71f75f854cc3c3f5b43aea93783a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*O2GSObfbVdNEpLXxbx2cFQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">端到端流程为<strong class="bd nf">微调BERT </strong>，训练一个<strong class="bd nf">文本分类</strong>模型，训练一个<strong class="bd nf"> NER </strong>模型(图片由作者提供)</p></figure><p id="103a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们检查一下上图的所有组件:</p><h2 id="f635" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">嵌入层:</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/1253c02d537b6c2918111fe5646e5c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bof_cbEin8qfRYNQ30luMQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">嵌入层(图片由作者提供)</p></figure><p id="0ec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入层的主要工作是将文本输入数据转换成模型可理解的格式。它有三个主要组成部分:</p><ul class=""><li id="5427" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">单词嵌入:</strong>这是为了得到每个单词的矩阵表示。对于BERT来说，它的词汇量为30，522，768个值的数组代表一个单词。因此，如果输入是4个单词，单词嵌入的输出将是4 x 768。</li><li id="2b62" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">位置嵌入:</strong>用于获取单词的位置。在transformers中，单词的位置作为输入传递，因为句子的上下文高度依赖于单词在句子中的位置。</li><li id="d0ca" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">线性层:</strong>该层是一个线性卷积层，用于合并各层，最终提供一个768特征的矩阵。</li></ul><h2 id="0bd3" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">编码器层数:</strong></h2><p id="9da6" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在这个编码层中总共有11个BERT层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/e67a9157e152aae9f4dda6e4f75741ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTOqOXZwOz8I3y5reXexTw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BERT编码器层(图片由作者提供)</p></figure><p id="1aea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每层有两个主要组件:</p><ul class=""><li id="2935" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">自我关注层:</strong>这是变形金刚模型的核心，有助于确定每个单词相对于句子中所有其他单词的重要性，以及在句子的整体上下文中的重要性。下图显示了用作键(K)、查询(Q)和值(V)的嵌入。k、Q和V经过几层卷积、点积和softmax处理，以获得所需的输出。自我关注本身是一个需要更多解释的话题。如果你没有意识到自我关注，我建议你看一下youtube上的视频<a class="ae kv" href="https://youtu.be/-9vVhYEXeyQ?t=146" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"/></a><strong class="ky ir"/>以获得更好的理解。<br/>该层接受768个特征的输入，进行多头注意操作，并输出具有768个特征的矩阵。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/7401340e3bf90516bb79793857b42d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4SBoRFqjleSiDRekvPO9hQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多头关注(图片来自<a class="ae kv" href="https://wiki.math.uwaterloo.ca/statwiki/index.php?title=STAT946F20/BERT:_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding" rel="noopener ugc nofollow" target="_blank"><strong class="bd nf">wiki . math . waterloo</strong></a>)</p></figure><ul class=""><li id="d9a4" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">回旋层:</strong>这些层之后是自我关注层。它包括一对夫妇的密集层，层正常化，辍学，和GELU激活功能。这些图层的输入和输出要素数量为768。</li></ul><h2 id="2e58" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">语境向量:</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/f140a8b92460482be717014f3aa4f32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHsKDlSBTLmhEe-Xp1v8GQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上下文向量(图片由作者提供)</p></figure><p id="609c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不是根据定义，而是上述2层(嵌入和编码层)的输出，给出了具有768个特征的矩阵，该矩阵是句子的上下文，因此我们将其称为上下文向量。这是模型的<strong class="ky ir">最重要的部分，因为在无监督部分完成的所有训练都是为了训练权重以正确获得上下文向量。</strong></p><p id="e7e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在诸如RNN和LSTM的模型中，模型的输入是不能捕捉句子上下文的单词嵌入。因此，我们在单词嵌入层的顶部训练一个神经网络来捕捉句子的上下文。现在，这个输出捕获了768个特征的上下文。该输出可以作为输入传递给分类或生成模型，使得整个模型高度稳健和准确。</p><h2 id="8075" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated"><strong class="ak">语言模型:</strong></h2><p id="601d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这是训练中无人监督的部分。额外的卷积层被添加到上下文向量的输出中。该模型的最终输出是词汇量为30，522的矩阵。这个模型需要从现有的词汇中预测一个单词。我们输入数百万未标记的句子，并允许模型调整权重以获得适当的上下文向量。</p><p id="dbf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有两种方法可以训练语言模型:</p><p id="efa8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">蒙面语言模型:</strong></p><p id="bb5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们屏蔽了输入句子的10–30 %,并要求模型预测被屏蔽的单词。正如你在下图中看到的，输入句子是“女式碎花印花[面具]上衣”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/91a58a1b31e828c05a7ff09ea27637fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAxYiYWJnOnP3D_LE3znsA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MLM的输入(图片由作者提供)</p></figure><p id="3138" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积层被添加到编码器层以预测丢失的单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/445878bb067bbe16450ae36f89e8f35b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fiXZkUem_SRrBOlZkWRSVA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">屏蔽语言模型底层(图片由作者提供)</p></figure><p id="0c92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型的输出是具有vocab大小(在这种情况下是30，522)的矩阵，以预测丢失的单词。基于模型的预测，计算损失并调整模型的权重。</p><p id="dbc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在使用掩蔽语言模型技术为时尚领域训练了一个具有2000万个句子的BERT语言模型之后，您可以在下面的GIF中清楚地观察到差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/6d354b662eb2bcf8872be17b626adeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*vbgB2hoeJH7sSMqhGBQZUw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左图:<strong class="bd nf">伯特接受时尚</strong>数据训练，右图:<strong class="bd nf">原始伯特</strong>(图片由作者提供)</p></figure><p id="2ad4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输入</strong>:女装【面具】上衣<br/> <strong class="ky ir"> BERT(时尚)</strong>:女装碎花印花裁剪上衣<br/> <strong class="ky ir"> BERT(原创)</strong>:女装上衣</p><p id="51e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">休闲语言模式:</strong></p><p id="2e86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于CLM，该模型试图从现有词汇中预测下一个单词，并将其与真实输出进行比较。通常，CLM用于生成模型，即GPT架构。在本文中，我们不涉及基于GPT的架构，但您可以看看下面的GIF，它显示了特定时尚的GPT2模型与具有原始权重的GPT2模型之间的差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/96e9b7c544041d4fe6937a37baefdb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*as-AIk35I9rmsp6SzbVqmw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左:<strong class="bd nf">根据时尚</strong>数据训练的GPT2，右:<strong class="bd nf">原始GPT2 </strong>(图片由作者提供)</p></figure><p id="5057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦伯特/GPT与定制数据集进行了微调，第二部分是在其上训练分类和生成模型。</p><p id="63d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然在训练时整个模型的权重得到更新，但是为了我们的理解，我们可以假设上下文向量现在是模型的输入。</p><h2 id="ab35" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">文本分类</h2><p id="0739" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">现在是一个简单的分类问题。在下图中，密集图层已被添加到上下文矢量的输出中，并最终将特征减少到类的数量。上下文层输出越好，模型就越健壮和准确。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/f87c0d7b7b5fe6507b93c99ade998911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fH8GhnZ45uwy_7vAmw3OrA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用上下文向量的文本分类(图片由作者提供)</p></figure><p id="3f1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们处理变形金刚时，我们还可以获得单词级的置信度得分，这些得分有助于对所述类进行预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/07b727ca77470153dd89d5a6e6a9c7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drJPihcMHbINmhwqwAjmnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分析文本分类输出，绿色表示肯定，红色表示否定(图片由作者提供)</p></figure><p id="be24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于句子<strong class="ky ir">panta loons green shirt with jacket，wear it with black jeans，</strong>我们试图预测一个时尚类别。上面的句子有很多种类，包括衬衫、夹克和牛仔裤。但是正如你在第一个例子中看到的，它正确地预测了衬衫，单词shirt具有最高的可信度。在其他的例子中，我们已经将基本事实改变为不同的标签，然后尝试观察行为。随着标签的改变，句子的单词级置信度改变，并且它自动指向对新标签的预测有积极贡献的单词。例如，在第二个例子中，当我们将真实标签改变为牛仔裤时，单词牛仔裤的置信度变为正，而诸如衬衫和夹克的其他单词变为负。这就是自我关注模型的美妙之处。</p><h2 id="4a1f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">命名实体识别</h2><p id="b5f9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">与文本分类相同，我们假设上下文向量作为输入，并为NER任务编写层。NER模型预测每个输入单词的标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/cfd5fef8d65b824da29b7f2f96da62a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAvCSw6r7m-VTDurU5lzRg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用上下文向量的命名实体识别(图片由作者提供)</p></figure><p id="767e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为数据是特定于时尚的，所以标签是细粒度的。NER也有助于短语发现，进而导致趋势发现。NER有很多方法，但本文的范围是理解半监督方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/655077bc3866c535349e93ce0fce74e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*f4yL7BvZ-pX3SU-cYhi6uA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">时尚NER(图片由作者提供)</p></figure><p id="4a78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们已经完成了所有组件，我建议您再次查看变形金刚端到端培训GIF。</p><p id="4e3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训代码可在<a class="ae kv" href="https://huggingface.co/docs/transformers/notebooks" rel="noopener ugc nofollow" target="_blank">抱脸</a>获取。</p><ul class=""><li id="ab5c" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">微调语言型号:<a class="ae kv" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li><li id="0b8f" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">训练一个文本分类模型:<a class="ae kv" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li><li id="489b" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">训练一个NER模型:<a class="ae kv" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a></li></ul><p id="0ede" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我试图给出一个半监督训练的想法，我们可以用变形金刚来训练高度精确的模型。让我知道你对它的想法。</p></div></div>    
</body>
</html>