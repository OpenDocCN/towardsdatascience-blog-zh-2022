<html>
<head>
<title>MetaFormer: De facto need for Vision?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">元形式:视觉的事实需要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/metaformer-de-facto-need-for-vision-93476cc9fec5#2022-02-20">https://towardsdatascience.com/metaformer-de-facto-need-for-vision-93476cc9fec5#2022-02-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ef8d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">《视觉变形金刚》成功的关键是什么？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4a73972f4aadf082aaab88527c088056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z7h_iWVj0MFy_Ehz"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@aditya1702?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aditya Vyas </a>拍摄</p></figure><h1 id="bc08" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">视觉变压器</h1><p id="d4fe" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最近，自从<a class="ae kv" href="https://scholar.google.de/citations?user=FXNJRDoAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank">阿列克谢·多索维茨基</a>在他的<a class="ae kv" href="https://arxiv.org/abs/2010.11929v2" rel="noopener ugc nofollow" target="_blank">论文</a>中提出视觉变形金刚(ViT)后，《变形金刚》开始流行起来。ViT 已被证明能够在图像分类任务中胜过卷积神经网络(CNN ),同时需要较少的计算资源。</p><p id="ec71" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在过去的十年中，CNN 一直在计算机视觉领域独占鳌头，一个年轻的 ViT 第一次踏入这个领域，出人意料地击败了 CNN。这就产生了一个大问题:</p><blockquote class="mp mq mr"><p id="4864" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">ViT 成功的真正原因是什么？</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/6e322d68a074162c90dee68cf02038d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*fqrCidC419ZTmSd-.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2010.11929v2" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="85b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">上图显示了作为 ViT 核心的变压器编码器的结构。几年后，许多后续工作都专注于通过用多层感知(MLPs)等其他结构完全取代<em class="ms">多头注意力</em>来提高性能，并且他们在图像分类基准上取得了有竞争力的性能。这一轨迹后来吸引了许多研究人员去寻找新的注意力模块或<em class="ms">令牌混合器</em>。</p><h1 id="0f57" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">元格式器</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/f4e45396184e65e65ef7a0f371d60736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djedJeSMHNiRDE8FDq4Y-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸张</a></p></figure><p id="56d2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者，新加坡国立大学的于等人认为，变形金刚在计算机视觉领域的成功主要依赖于它的总体架构，而不是令牌混合器的设计。为了验证这一点，他们已经提出使用一种令人尴尬的简单非参数操作，<em class="ms">平均池</em>作为<em class="ms">令牌混合器</em>，并且仍然在各种计算机视觉任务上实现了最先进的性能，如图像分类、对象检测和实例分割。</p><p id="dd3f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在论文中，他们将 Transformer 的一般架构称为<em class="ms">元形成器</em>，并研究了<em class="ms">池形成器</em>的效能，其中<em class="ms">平均池</em>操作被用作<em class="ms">令牌混合器</em>。令人惊讶的是，<em class="ms"> PoolFormer </em>优于采用注意力和空间 MLP 等其他复杂模块的 Transformer 架构。</p><h1 id="10f6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">前池</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/0afb24e59a0f054fb8c3fc9b7bc0edd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9T4QC4bIK7OjsidHwSjRRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><p id="5778" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ms"> PoolFormer </em>的整体架构如上图所示。该模型有 4 个主要阶段，它们具有相同的设计，一个<em class="ms">补丁嵌入</em>层，随后是<em class="ms"> PoolFormer 块</em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/a8370573f9e34306d7ab3354dc625261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FUJ_nz0KOUGhSoHfCOG0cQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5128" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">每个阶段的细节可以如上图所示。<em class="ms">补丁嵌入</em>通过使用一个 3x3 的 Conv 层实现。<em class="ms">定额</em>图层可以批量归一化、图层归一化、分组归一化；在本文中，作者应用了组归一化，因为它在他们的实验中显示了更好的结果。<em class="ms">信道 MLP </em>模块采用两个 1x1 的 Conv 层，中间是一个路斯激活层。官方代码可以在:<a class="ae kv" href="https://github.com/sail-sg/poolformer" rel="noopener ugc nofollow" target="_blank">https://github.com/sail-sg/poolformer</a>找到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/9e4899c7231aeff618bec19864ba76b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjQgCHRes6PzgiwQmdgH5w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="d8c3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结果</h1><p id="0cb3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">PoolFormer 已经在计算机视觉任务的各种基准上与其他最先进的模型进行了比较，包括图像分类、对象检测和实例分割。结果总结如下:</p><h2 id="f9d2" class="nb kx iq bd ky nc nd dn lc ne nf dp lg lx ng nh li mb ni nj lk mf nk nl lm nm bi translated">图像分类</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/03a36c869cd8afc46686a7b1356852e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvg0MbBBuZtDIO8tOAXjjQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/dd1dbb4a2f6b064e5131b32efe0a8119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0Hmc5e39tfJURrZEOt7nQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><h2 id="bc91" class="nb kx iq bd ky nc nd dn lc ne nf dp lg lx ng nh li mb ni nj lk mf nk nl lm nm bi translated">对象检测和实例分割</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/368cb50321cb80e6cfb49aa436299a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjoESqmZ2wx3neV3uE7oQg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/31e9cef860b36e4ddbdad09d2d77a766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2pQpo_uRuu1NUO0HkcOORg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/8eb63171f8570df0e5cc7615f33824b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CGi63pJUhnud73kn-3diSA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><h1 id="b498" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">PoolFormer 块的 Pytorch 类代码</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/e891e9f85a9433f7982e06fb1b2f4a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFiZcQVDtHjvHBDW_gvL_w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/c2e4b4b9deb0df71de342373d994be30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vy1Ctw4xvQpPI9UwM4X2Dg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸张</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/faa4ef7f08e0f9b64b59aae8edb1cb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoWn-JQ7kRg3WwfWTZHjCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://arxiv.org/abs/2111.11418" rel="noopener ugc nofollow" target="_blank">纸</a></p></figure><h1 id="af1f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="37d2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这篇文章中，我简要回顾了 MetaFormer，这是一种基于变形金刚的通用架构，它是变形金刚及其变体在计算机视觉中取得成功的真正原因，作者说。作者基于一个假设提出了元形式，即变形金刚或类似 MLP 的模型的能力是通过它们的一般结构而不是<em class="ms">令牌混合器</em>获得的。为了证明这个假设，一个令人尴尬的简单的非参数操作符，<em class="ms">池</em>，被用作<em class="ms">令牌混合器</em>，并且仍然优于其他高级模块，例如 attention。MetaFormer 的能力也已经在图像分类、对象检测和实例分割任务的基准上得到验证。</p><p id="f12e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">欢迎读者访问我的脸书粉丝页面，分享关于机器学习的事情:<a class="ae kv" href="https://www.facebook.com/diveintomachinelearning" rel="noopener ugc nofollow" target="_blank">深入机器学习</a>。我的其他著名帖子也可以在这里找到:</p><ul class=""><li id="7680" class="nv nw iq lq b lr mk lu ml lx nx mb ny mf nz mj oa ob oc od bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/yolov4-5d-an-enhancement-of-yolov4-for-autonomous-driving-2827a566be4a">约洛夫 4–5D 评论</a></li><li id="86e9" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/darkeras-execute-yolov3-yolov4-object-detection-on-keras-with-darknet-pre-trained-weights-5e8428b959e2">黑暗时代</a></li><li id="40f3" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/efpn-extended-feature-pyramid-network-for-small-object-detection-980af794a093"> EFPN:扩展特征金字塔网络</a></li><li id="5878" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-compilation-with-python-and-opencv-b76b1cd500e0">数据扩充</a></li><li id="da55" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-distillation-for-object-detection-92a89fe5d996">数据提炼</a></li><li id="9cdc" class="nv nw iq lq b lr oe lu of lx og mb oh mf oi mj oa ob oc od bi translated">而其他人在<a class="ae kv" href="https://tranlevision.medium.com/" rel="noopener">我的页面</a>。</li></ul><p id="2212" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">感谢您抽出时间！</p></div></div>    
</body>
</html>