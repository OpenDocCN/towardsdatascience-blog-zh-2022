<html>
<head>
<title>Multivariate Time Series Clustering Using Growing Neural Gas and Spectral Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于生长神经气体和谱聚类的多元时间序列聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multivariate-time-series-clustering-using-growing-neural-gas-and-spectral-clustering-adc83680639f#2022-03-08">https://towardsdatascience.com/multivariate-time-series-clustering-using-growing-neural-gas-and-spectral-clustering-adc83680639f#2022-03-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3865" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种基于图论的时间序列聚类方法</h2></div><p id="d062" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">聚类是一种无监督的方法，有很多应用领域。它也可以应用于时间序列，尽管它不像时间序列的预测和异常检测那样流行。重点是分析整个系统，该系统有许多不同的 KPI 随时间流动，并将系统的行为表示为状态，并将相似的行为聚集在一起。换句话说，获取特定时间段的系统快照，并检测变化模式。</p><p id="9bff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我提到了一种新颖的实现方法，它将生长神经气体(GNG)和谱聚类结合起来用于这个目的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/40117fdd58c2baabc90c87a23e8dc345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*85QU2Hb6j-uQJOFe"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">艾伯·布朗在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="32d5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">场景定义和数据集检查</h2><p id="ab26" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">让我们假设一个系统由几个设备组成，每个设备由 100 个不同的 KPI 表示，这些 KPI 随时间流动，换句话说，多变量时间序列用于确定系统的总体概况。目标是检测系统的不同行为，并沿着定义的时间段对它们进行聚类。类似地，我们也可以洞察我们遇到的异常情况。为了保持较小的数据量和简单快速的计算，我使用了一个简化的数据集。为了澄清这一点，请看一下图 1 中数据集的简化版本。您甚至可以通过忽略“设备”列并研究随时间推移会创建多个 KPI 的单个设备，将此数据集转换为更简化的数据集。</p><p id="4801" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了不失去对研究的关注，我简单地假设数据已经过预处理，这意味着丢失的值已经得到处理，并且数据已经过规范化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mq"><img src="../Images/0dcb1a12904dc6c05fc7b1703cf2d873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93a51yn9SFTm6LJOEmYnRg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图片由作者提供—图 1</p></figure><h2 id="358e" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">1-应用 GNG</h2><p id="52ce" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">GNG 主要用于学习图形格式数据的拓扑结构。GNG 算法是由 Fritzke 于 1995 年首先提出的。<a class="ae lr" href="https://proceedings.neurips.cc/paper/1994/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf" rel="noopener ugc nofollow" target="_blank">在原论文</a>中，基本说明了它的工作原理。这是一种简单的无监督神经网络方法，类似于<a class="ae lr" rel="noopener" target="_blank" href="/self-organizing-maps-1b7d2a84e065">自组织映射(SOMs) </a>。然而，GNG 和 SOM 都不包括常见的神经网络结构和步骤，如层、前馈和反向传播。类似地，GNG 和 SOM 中的<em class="mr">权重</em>概念基本上对应于节点在空间中的位置，并由具有特征大小的向量来表示。</p><p id="336c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它们都是迭代方法，并且在其迭代期间具有相似的最佳匹配单元/最近单元推理。然而，它们在实现上也有基本的区别。与 SOM 不同的是，在 GNG 中，您不需要在流程开始时初始化节点，这些节点是通过迭代动态确定的，这就是“增长”的来源。GNGs 试图将数据包含到拓扑中，在拓扑中，模型在每次迭代中表现最差。通过这种方式，它们不会均匀增长，也不会覆盖整个数据。gng 通常也用于从图像生成<a class="ae lr" href="http://neupy.com/2018/03/26/making_art_with_growing_neural_gas.html" rel="noopener ugc nofollow" target="_blank">有趣的动画。我利用</a><a class="ae lr" href="http://neupy.com/apidocs/neupy.algorithms.competitive.growing_neural_gas.html" rel="noopener ugc nofollow" target="_blank"> NeuPy </a>包来实现 python 中的 GNG，它提供了一个简单的抽象。</p><p id="e4c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以在下面找到一个简单的介绍性代码片段，它为我们的数据组成了一个简单的 GNG 模型。直接影响模型的性能和准确性的一个重要参数是节点的数量，换句话说，就是数据表示的分辨率。要找到节点计数的最佳数量并不那么简单，在我们的例子中，您可能只需要根据唯一的设备数量和工作时间窗口实现一些简单的推理。注意，在 Neupy 包中为 GNG 模型实现的<em class="mr">图</em>属性提供了对数据拓扑结构的洞察。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="d22e" class="ls lt iq mt b gy mx my l mz na">from neupy import algorithms<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.cluster import KMeans<br/>from scipy import stats</span><span id="6f57" class="ls lt iq mt b gy nb my l mz na">gng = algorithms.GrowingNeuralGas(<br/> n_inputs=self.n,<br/> n_start_nodes=5,<br/> shuffle_data=True,<br/> verbose=True,<br/> step=0.1,<br/> neighbour_step=0.01,<br/> max_edge_age=50,<br/> max_nodes=100,<br/> n_iter_before_neuron_added=100,<br/> after_split_error_decay_rate=0.5,<br/> error_decay_rate=0.995,<br/> min_distance_for_update=0.05<br/> )<br/>epoch = 100<br/>gng.train(df_scaled_data, epochs=epoch) # only including features, features are named as '0', '1', '2' ...<br/>g = gng.graph<br/>node_list = g.nodes</span></pre><p id="aefc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在构建了 GNG 模型之后，我们还有一个问题需要补充。也就是找出哪个样本匹配到哪个节点。我们可以简单地利用具有<em class="mr"> n_neighbors=1 </em>参数的 KNN 算法来将样本与最近的节点进行匹配。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="dfe9" class="ls lt iq mt b gy mx my l mz na">number_of_features = 92 # we have 92 different KPIs in our data<br/>node_koor_node = []<br/>for node in gng.graph.nodes:<br/> row = node.weight[0].tolist()<br/> row.append(node_list.index(node))<br/> node_koor_node.append(row)</span><span id="c067" class="ls lt iq mt b gy nb my l mz na">df_train = pd.DataFrame(node_koor_node)<br/>df_test = pd.DataFrame(df_scaled_data)</span><span id="586e" class="ls lt iq mt b gy nb my l mz na">X_train = df_train.iloc[:, 0:number_of_features] <br/>y_train = df_train.iloc[:, number_of_features]<br/>X_test = df_test.iloc[:, 0:number_of_features]</span><span id="d379" class="ls lt iq mt b gy nb my l mz na">knn = KNeighborsClassifier(n_neighbors=1)<br/>knn.fit(X_train, y_train)<br/>knn_result = knn.predict(X_test)</span><span id="8f31" class="ls lt iq mt b gy nb my l mz na">scaled_sample_node_matching = []<br/>for i, data in enumerate(df_scaled_data):<br/> row = data.tolist()<br/> node_idx = knn_result[i]<br/> row.append(node_idx)<br/> scaled_sample_node_matching.append(row)</span><span id="adc6" class="ls lt iq mt b gy nb my l mz na">df_scaled_sample_node_matching = pd.DataFrame(scaled_sample_node_matching)<br/>df_scaled_sample_node_matching = df_scaled_sample_node_matching.rename(columns={number_of_features: “node”})</span><span id="e64a" class="ls lt iq mt b gy nb my l mz na">df_node_sample_match = pd.merge(df_static_cols, df_scaled_sample_node_matching, left_index=True,<br/>                   right_index=True) # df_static_cols includes static columns and datetime</span></pre><h2 id="48a2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">2-包括时间行为</h2><p id="aaa0" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">好的，到目前为止还不错，但是我们忽略了一些重要的东西。这是关于时间序列的。GNG 没有考虑数据中的时间行为，换句话说，样本之间的时间依赖性。我们必须在数据中包含时间行为。出于这个原因，我们简单地忽略了所有的边在前面的步骤中建立的 GNG，而保持节点和他们在空间中的位置。在我们的例子中，通过考虑相同器件的连续时间周期之间的转换来重新计算边缘。为了澄清这一点，请看下图。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/dc1536c2ad76f2ae402119aab1c149a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v2tRy1NyFUhnbP4VAtfN-w.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片—图 2</p></figure><p id="100f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一步中，我们确定了分布在数据空间中的节点及其包含的所有样本。换句话说，我们知道哪些设备在什么时间位于哪个节点上。通过利用这些信息，我们还能够组成邻接矩阵，向我们显示节点之间的转换。邻接矩阵是图结构的一个非常重要的表示。在我们的例子中，我们不构成仅由 0 和 1 组成的二元邻接矩阵。我们计算节点之间转移的概率(换句话说，接近程度)，将该值保存在邻接矩阵的每个单元中。在这个意义上，我们构造了一个加权邻接矩阵。它也被称为亲和矩阵。</p><p id="c3b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们还构建了用新边更新的图的度矩阵。度矩阵在除对角线以外的每个单元中包括零，并且基本上指定每个节点的度，从对应的节点出去多少条边。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="b49a" class="ls lt iq mt b gy mx my l mz na">device_list = df_node_sample_match['DEVICE'].unique()<br/>n = len(gng.graph.nodes)<br/>mat_transition = np.zeros((n, n))<br/>previous_node = -1</span><span id="2f8b" class="ls lt iq mt b gy nb my l mz na">for device in device_list:<br/>    df_device = df_node_sample_match.loc[df_node_sample_match['DEVICE'] == device]<br/>    df_device = df_device.sort_values('DATETIME')</span><span id="0585" class="ls lt iq mt b gy nb my l mz na">    for i, row in df_device.iterrows():<br/>        current_node = row['node']<br/>        if (previous_node != -1):<br/>            mat_transition[previous_node, current_node] = mat_transition[previous_node, current_node] + 1<br/>        else:<br/>            print('It is the starting node, no transaction!')<br/>        previous_node = current_node<br/>    previous_node = -1</span><span id="90a7" class="ls lt iq mt b gy nb my l mz na">df_adjacency = pd.DataFrame(mat_transition)</span><span id="3d7a" class="ls lt iq mt b gy nb my l mz na">df_adjacency['sum_of_rows'] = df_adjacency.sum(axis=1)<br/>list_degree = df_adjacency['sum_of_rows'].tolist()<br/>degree_matrix = np.diag(list_degree)</span><span id="29d3" class="ls lt iq mt b gy nb my l mz na"># calculating transition probability<br/>df_affinity = df_adjacency.loc[:, :].div(df_adjacency["sum_of_rows"], axis=0)<br/>df_affinity = df_affinity.drop(["sum_of_rows"], axis=1)<br/>df_affinity = df_affinity.fillna(0)</span></pre><h2 id="5185" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">3-应用谱聚类</h2><p id="8e12" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">谱聚类是一种降维技术，主要是将高维数据集聚类成更小的相似聚类。它的推理来自于图论，目的是根据拓扑中的连通性来检测接近的节点，并分别对它们进行聚类。它也适用于不同的数据形式。谱聚类的主要步骤如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/c1e413c85e9f744e80047162b3799d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvREA9TkxFzADwof0XzPrA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Spectral_clustering" rel="noopener ugc nofollow" target="_blank">谱聚类的主要步骤，来源</a></p></figure><p id="9ae2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了更好地理解谱聚类，请看一下 K-means 和谱聚类在图 3 中下面的圆形数据上的行为。谱聚类利用拉普拉斯矩阵(图拉普拉斯)的谱(特征向量),该谱是通过使用亲和度矩阵和度矩阵计算的。注意，我们已经在上一步中计算了两个矩阵。所以，我们准备好看拉普拉斯矩阵了。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/4793174817992090ce213b4a8bb050f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbL7PmvB0JEnrtRNn9RrlQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">使用两种不同方法对圆形数据进行聚类，图片由作者提供—图 3</p></figure><p id="6f30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而拉普拉斯矩阵可以用等式<em class="mr"> L = D - A </em>来计算；其中<em class="mr"> D </em>是度矩阵，而<em class="mr"> A </em>是邻接矩阵(在我们的情况下是亲和矩阵)，归一化拉普拉斯矩阵可以被定义为以下等式:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nf"><img src="../Images/69fba733856b417283f885e99f148515.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*dfnNGpHOYuDXeg24ewdc9Q.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">归一化拉普拉斯方程，<a class="ae lr" href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="dc07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的问题，我们<a class="ae lr" href="https://math.stackexchange.com/questions/1113467/why-laplacian-matrix-need-normalization-and-how-come-the-sqrt-of-degree-matrix/1113572" rel="noopener ugc nofollow" target="_blank">更喜欢</a>使用归一化拉普拉斯算子而不是拉普拉斯矩阵。可以借助下面的代码片段进行计算。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="f5ca" class="ls lt iq mt b gy mx my l mz na">I = np.identity(df_affinity.shape[0])<br/>sqrt = np.sqrt(degree_matrix)<br/>D_inv_sqrt = np.linalg.inv(sqrt)<br/>normalised_laplace = I — np.dot(D_inv_sqrt, df_affinity).dot(D_inv_sqrt)<br/>df_normalised_laplace = pd.DataFrame(normalised_laplace)</span></pre><p id="8050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为第二步，我们确定具有对应的最小<em class="mr"> k </em>特征值的特征向量，其中<em class="mr"> k </em>是指聚类的数量。调整最佳聚类数并不简单。你可以找到一个解释的方法来确定它<a class="ae lr" rel="noopener" target="_blank" href="/spectral-clustering-aba2640c0d5b">这里</a>。归一化拉普拉斯矩阵的第二特征值给出了对图的连通性的洞察。在我们的例子中，我们选择它为 10。为了找到图拉普拉斯的特征向量和特征值，我们利用了<a class="ae lr" rel="noopener" target="_blank" href="/simple-svd-algorithms-13291ad2eef2">奇异值分解(svd) </a>技术。另一种可能是使用 NumPy 包的<a class="ae lr" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html" rel="noopener ugc nofollow" target="_blank"> <em class="mr"> eigh </em> </a>方法。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="290c" class="ls lt iq mt b gy mx my l mz na">number_of_nodes = df_affinity.shape[0]<br/>cluster_number = 10<br/>u, s, vN = np.linalg.svd(normalised_laplace, full_matrices=False)<br/>vN = np.transpose(vN)<br/>eigen_vecs = vN[:, number_of_nodes — cluster_number:number_of_nodes] # 10 eigenvectors with smallest 10 eigenvalues</span></pre><p id="828a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为最后一步，组成的特征向量子集可用于对数据进行聚类，只需输入<em class="mr"> k-means </em>。聚类之后，我们将节点与相应的聚类进行匹配。</p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="13f1" class="ls lt iq mt b gy mx my l mz na"># Clustering<br/>kmeans = KMeans(n_clusters=cluster_number, max_iter=1000, random_state=0)<br/>kmeans.fit(eigen_vecs)<br/>labels = kmeans.labels_<br/>cluster_centers = kmeans.cluster_centers_</span><span id="b072" class="ls lt iq mt b gy nb my l mz na"># node-cluster matching<br/>clusters = dict()<br/>for i, n in enumerate(node_list):<br/> if labels[i] not in clusters:<br/>    clusters[labels[i]] = list()<br/> clusters[labels[i]].append(i)</span></pre><h2 id="809d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">4-确定异常群集和根本原因</h2><p id="e1bc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">集群是最终目的地吗？不太可能。一个可能的问题是定义异常集群。另一种说法是找出设备和相应的时间在我们的情况下以异常的方式表现。此外，这种异常的原因也是一个问题。很明显，小星团往往是异常的。以这种方式，例如，我们可以得出结论，表示小于整个数据的 10%的聚类是异常聚类。我们预计少数几个聚类将覆盖大部分数据。对我们来说这是一个开放的研究领域，仍然在研究。更先进的技术比基于规则的方法更受欢迎。此外，领域专家还可以解释聚类结果，并对异常情况发表意见。</p><p id="f45d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在定义异常集群之后，能够检测这些异常的根本原因将是研究的一个资产。此时，异常群集和正常群集之间的每个 KPI 分布的差异出现在表中。为了测量和排序两种分布之间的差异，您可以利用<em class="mr"> Kolmogorov-Smirnov </em>测试。你也可以在这里找到不同的选项<a class="ae lr" href="https://medium.com/geekculture/techniques-to-measure-probability-distribution-similarity-9145678d68a6" rel="noopener">。对于异常聚类中样本的每个 KPI(特征),我们只需应用<em class="mr"> Kolmogorov-Smirnov </em>检验来找到与正常聚类的相应 KPI 的相似性。特征越不相似，就越有可能是异常的原因。</a></p><pre class="lc ld le lf gt ms mt mu mv aw mw bi"><span id="7a6d" class="ls lt iq mt b gy mx my l mz na"># We somehow determine the anomaly clusters<br/>anomaly_clusters_list = [3, 4, 5, 7, 8, 9]<br/>normal_cluster_list = [0, 1, 2, 6]</span><span id="1dec" class="ls lt iq mt b gy nb my l mz na">node_cluster_matching = []<br/>for i in range(len(node_list)):<br/> for j in range(cluster_number):<br/>   if i in clusters[j]:<br/>     node_cluster_matching.append([i, j])</span><span id="4fdb" class="ls lt iq mt b gy nb my l mz na">df_node_cluster_matching = pd.DataFrame(node_cluster_matching)<br/>df_node_cluster_matching.columns = [‘node’, ‘cluster’]<br/>df_sample_node_cluster = pd.merge(df_node_sample_match, df_node_cluster_matching, on=[‘node’], how=’inner’)</span><span id="11c2" class="ls lt iq mt b gy nb my l mz na">list_ = np.arange(0, number_of_features, 1).tolist()<br/>list_ = [str(i) for i in list_]<br/>list_.append(‘cluster’)</span><span id="1f49" class="ls lt iq mt b gy nb my l mz na">df_anomaly_clusters_data = df_sample_node_cluster.loc[<br/> df_sample_node_cluster[‘cluster’].isin(anomaly_clusters_list)]<br/>df_anomaly_clusters_data = df_anomaly_clusters_data.loc[:, df_anomaly_clusters_data.columns.isin(list_)]<br/>df_anomaly_clusters_data = df_anomaly_clusters_data.reset_index(drop=True)</span><span id="3738" class="ls lt iq mt b gy nb my l mz na">df_normal_clusters_data = df_sample_node_cluster.loc[<br/> df_sample_node_cluster[‘cluster’].isin(normal_cluster_list)]<br/>df_normal_clusters_data = df_normal_clusters_data.loc[:, df_normal_clusters_data.columns.isin(list_)]<br/>df_normal_clusters_data = df_normal_clusters_data.reset_index(drop=True)</span><span id="89af" class="ls lt iq mt b gy nb my l mz na">dict_of_cluster_feature_pvaluelist = dict()<br/>for c in anomaly_clusters_list:<br/> filtered_anomaly_data = df_anomaly_clusters_data.loc[df_anomaly_clusters_data[‘cluster’] == c]<br/> pvalue_feature_list = []<br/> for i in range(number_of_features):<br/> anomaly_clusters_data_feature = filtered_anomaly_data[str(i)].to_numpy().tolist()<br/> normal_clusters_data_feature = df_normal_clusters_data[str(i)].to_numpy().tolist()<br/> result = stats.ks_2samp(anomaly_clusters_data_feature, normal_clusters_data_feature)<br/> print(‘Result of cluster ‘ + str(c) + ‘ feature ‘ + str(i))<br/> print(result)<br/> pvalue = result.pvalue<br/> if pvalue &lt;= 0.05:<br/> pvalue_feature_list.append([i, pvalue])<br/> pvalue_feature_list.sort(key=lambda x: x[1])<br/> pvalue_feature_list = pvalue_feature_list[0: 3] # lets say we pick top 3 probable features for root cause <br/> dict_of_cluster_feature_pvaluelist[‘c’ + str(c)] = pvalue_feature_list</span></pre><p id="ced7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前就这些。感谢 Sezin Gürkan、Ersin Aksoy 和 Amir Yavariabdi，我非常感谢他们对这项研究做出的宝贵贡献。</p><h2 id="b4ff" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">有用的链接</h2><div class="ng nh gp gr ni nj"><a href="https://medium.com/starschema-blog/growing-neural-gas-models-theory-and-practice-b63e5bbe058d" rel="noopener follow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">生长神经气体:使用 GNGs 量化糖尿病视网膜病变中的硬性视网膜渗出</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">不断发展的神经气体算法可以帮助我们量化眼底病变和评估糖尿病的严重程度…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">medium.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/spectral-clustering-aba2640c0d5b"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">谱聚类</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">基础与应用</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="ny l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://math.stackexchange.com/questions/1113467/why-laplacian-matrix-need-normalization-and-how-come-the-sqrt-of-degree-matrix/1113572" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">为什么拉普拉斯矩阵需要归一化，度矩阵的 sqrt 是怎么来的？</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">拉普拉斯矩阵为什么需要归一化，度矩阵的平方幂是怎么来的？对称归一化…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">math.stackexchange.com</p></div></div><div class="ns l"><div class="nz l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://www.mygreatlearning.com/blog/introduction-to-spectral-clustering/#WhatisSpectralclustering" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">谱聚类简介</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">近年来，谱聚类已经成为最流行的现代聚类算法之一。很容易…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">www.mygreatlearning.com</p></div></div><div class="ns l"><div class="oa l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://juanitorduz.github.io/spectral_clustering/" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">谱聚类入门 Juan Camilo Orduz 博士</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">12 分钟阅读这篇文章，我想探索谱聚类背后的思想。我不打算发展这个理论…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">juanitorduz.github.io</p></div></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://math.stackexchange.com/questions/573888/how-do-i-compute-the-eigenvectors-for-spectral-clustering-from-a-singular-value" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">如何从奇异值分解计算谱聚类的特征向量？</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">感谢为数学栈交换贡献一个答案！请务必回答问题。提供详细信息…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">math.stackexchange.com</p></div></div><div class="ns l"><div class="ob l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://stats.stackexchange.com/questions/314046/why-does-andrew-ng-prefer-to-use-svd-and-not-eig-of-covariance-matrix-to-do-pca" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">为什么吴恩达更喜欢用奇异值分解而不是协方差矩阵的 EIG 来做主成分分析？</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">开始组$ @阿米巴对 PCA 问题有很好的回答，包括这个关于奇异值分解和 PCA 关系的问题。回答给…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">stats.stackexchange.com</p></div></div><div class="ns l"><div class="oc l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">如何用 Python -机器学习掌握从头计算 SVD</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">矩阵分解，也称为矩阵分解，涉及到描述一个给定的矩阵使用其组成…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ns l"><div class="od l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/pca-and-svd-explained-with-numpy-5d13b0d2a4d8"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">用 numpy 解释 PCA 和 SVD</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">主成分分析和奇异值分解到底有什么关系，如何用 numpy 实现？</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="oe l nu nv nw ns nx ll nj"/></div></div></a></div><div class="ng nh gp gr ni nj"><a href="https://stackoverflow.com/questions/24913232/using-numpy-np-linalg-svd-for-singular-value-decomposition" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd ir gy z fp no fr fs np fu fw ip bi translated">使用 Numpy (np.linalg.svd)进行奇异值分解</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">TL；DR: numpy 的 SVD 计算 X = PDQ，所以 Q 已经转置了。奇异值分解将矩阵 X 有效地分解成…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">stackoverflow.com</p></div></div><div class="ns l"><div class="of l nu nv nw ns nx ll nj"/></div></div></a></div></div></div>    
</body>
</html>