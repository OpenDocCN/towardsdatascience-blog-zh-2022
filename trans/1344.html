<html>
<head>
<title>A Succinct Guide To Bidirectional Associative Memory (BAM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双向联想记忆简明指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-succinct-guide-to-bidirectional-associative-memory-bam-d2f1ac9b868#2022-04-04">https://towardsdatascience.com/a-succinct-guide-to-bidirectional-associative-memory-bam-d2f1ac9b868#2022-04-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/1e253175b2c68b4ff92f8435bd789c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVi4yfMwkhO6Y2m_iJydjw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Arthur V. Ratz的照片</p></figure><div class=""/><div class=""><h2 id="841f" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">读者需要了解的关于双向联想记忆(BAM)的一切，包括Python 3.8和NumPy中的示例</h2></div><h1 id="aa4a" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated"><strong class="ak">简介</strong></h1><p id="f081" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi mi translated"><span class="l mj mk ml bm mm mn mo mp mq di"> B </span>双向联想记忆(BAM)是一种特殊类型的递归神经网络(RNN)，最初由Bart Kosko在20世纪80年代初提出，试图克服自联想Hopfield网络和ann的几个已知缺点，后者从连续训练中学习数据的关联。</p><p id="807b" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">BAM是异联想存储器，提供存储相关数据的能力，而不管其类型和结构，无需连续学习。在从BAM的存储器中调用数据关联之前，数据关联被同时存储，仅存储一次。通常，BAM比现有的ann跑得快，在回忆关联时提供明显更好的性能。此外，与现有的人工神经网络相比，使用BAM完美地解决了已知的按位异或问题，这使得在其存储器中存储编码为二进制(双极性)形式的数据成为可能。</p><p id="e9d4" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">与单向Hopfield网络不同，BAM能够双向调用分配给其输入或输出的数据关联。此外，它允许检索正确的关联，即使是不完整或失真的输入数据。</p><p id="152f" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">当作为基于人工智能的决策过程的一部分部署时，BAM模型是相当有效的，它基于许多相关数据的各种关联来推断特定数据分析问题的解决方案。</p><p id="c671" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">一般来说，BAM模型可用于多种应用，例如:</p><ul class=""><li id="bb7e" class="mw mx jf lo b lp mr ls ms lv my lz mz md na mh nb nc nd ne bi translated"><strong class="lo jg">分类和聚类数据</strong></li><li id="c692" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated"><strong class="lo jg">不完整数据扩充</strong></li><li id="62f5" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated"><strong class="lo jg">恢复受损或损坏的数据</strong></li></ul><p id="15f2" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">每当人工神经网络获取的各种知识不足以处理引入人工智能进行分析的数据时，BAM模型就非常有用。</p><p id="65d4" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">例如，用人工神经网络预测不完整文本中遗漏的单词，基本上需要将单词与句子的关联存储在人工神经网络的存储器中。然而，这将导致不正确的预测，因为相同的缺失单词可能出现在不止一个不完整的句子中。在这种情况下，使用BAM提供了存储和调用数据实体的所有可能关联的能力，例如<em class="nk">单词到句子</em>、<em class="nk">句子到单词</em>、<em class="nk">单词到单词</em>和<em class="nk">句子到句子</em>，反之亦然<em class="nk">。这反过来显著提高了预测的质量。为了扩充或聚集这些数据，还推荐多方向地关联各种数据实体。</em></p><p id="f0ba" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">最后，每个ANN的层的内存容量都绑定在最大浮点类型的大小:<em class="nk"> float64 </em>。例如，shape (100x100)的单个ANN层的容量只有1024字节。显然，人工神经网络的层数以及每层中神经元的数量都必须增加，以具有存储联想和多方向回忆的能力。这反过来又会对基于人工神经网络的内存延迟产生负面影响，因为它的学习和预测工作量与人工神经网络的规模成比例地增长。</p><p id="030b" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">尽管如此，BAM的学习和预测的高性能提供了根据需要分配大量存储空间的能力，超过了传统ANN的存储容量限制。同样，可以将多个bam聚合到内存中，具有分层结构。</p><h1 id="1b0b" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">存储在内存中的关联…</h1><p id="6019" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">BAM学习各种数据的关联，转换成双极模式。双极模式是二进制向量的特例，其元素分别是<strong class="lo jg"> 1的</strong>和<strong class="lo jg"> -1的</strong>的值。多个模式(列)沿着矩阵的y轴排列成2D模式映射。x模式映射是嵌入，其每一列对应于存储在BAM的存储器中的特定输入和输出数据项:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5b1026c8efb6fd744ef0800faee5acf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*TuS0xMXlK7AogvNj9iSaAg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者将数据编码成双极2D图形形状图(s×n)|图像</p></figure><p id="a91f" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">BAM模型同时从模式映射中学习相关数据，分别分配给其相应的输入和输出:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/02beba994487217ec61a64a807fc2655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3LRtAeBcs796SxqphgYmKg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者分配给BAM模型|图像的输入(X)和输出(Y)模式映射</p></figure><p id="f043" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">在上图中，BAM模型学习数据的关联，分别编码到形状(𝙨 x 𝙣)和(𝙨 x 𝙢)的模式映射𝙓和𝒀中。在学习时，来自输入和输出模式的值被分配给BAM模型的相应输入𝙭ᵢ<strong class="lo jg">∑</strong>𝙓和输出𝙮ⱼ<strong class="lo jg">∑</strong>𝒀。反过来，输入和输出模式映射(列)必须具有相同数量的模式(𝙨)。由于BAM是一个异联想记忆模型，它有不同数量的输入(𝙣)和输出(𝙢)，因此，相应地，在𝙓和𝒀这些模式映射中也有不同数量的行。</p><h1 id="8992" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">BAM的拓扑和结构</h1><p id="dbdf" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">通常，BAM是一个原始的神经网络(NN ),仅由输入和输出层组成，通过突触权重相互连接:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e1074ce86a74db69765bd6978ea143a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*sCXGKFXRzjRAXfZnayC69g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">BAM模型的神经网络拓扑(4 x 3) |图片由作者提供</p></figure><p id="e6e1" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">在上图中，输入层的神经元(𝒏)不执行输出计算，而是将输入𝑿前馈到BAM输出层的存储单元(𝒎)。它的大小基本上取决于BAM的输入和输出的数量(例如，输入和输出模式映射的维度)。上面显示的形状为(4 x 3)的NN，在其输入和输出层分别由4个神经元和3个存储单元组成。</p><p id="5713" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">与传统的人工神经网络不同，BAM的输出层由执行BAM输出计算的存储单元组成。每个存储单元基于由突触权重𝑾转发给该单元的多个输入𝑿来计算其输出𝒀，每个输出都具有特定的强度。</p><p id="88a0" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">存储单元的输出对应于模式的特定值，被调用:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/2bc490a8f69672a4ade11ff84549693c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQmpcGv1IIAL0J3-uDSGyQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">BAM内存单元的输出(Y )|作者提供的图像</p></figure><p id="6b3d" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">BAM的输出计算为所有BAM输入𝑾ᵀ𝙓的加权和，作为双极性阈值函数的参数应用。每个输出𝒀的<strong class="lo jg"> <em class="nk">正</em> </strong> (+)或<strong class="lo jg"> <em class="nk">负</em> </strong> (-)值(例如<strong class="lo jg"> 1 </strong>或<strong class="lo jg"> -1) </strong>总是与该单元的加权输入之和的大小成比例。</p><p id="b797" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">连接输入神经元和记忆细胞的所有突触权重存储由BAM基于下面讨论的Hebbian监督学习算法学习的基本记忆。</p><h1 id="7094" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">学习算法</h1><p id="6b97" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">从代数角度来看，BAMs存储和回忆关联的能力完全依赖于矩阵的双向属性，T. Kohonen和J. Anderson在20世纪50年代中期对此进行了研究:</p><blockquote class="nt nu nv"><p id="a77b" class="lm ln nk lo b lp mr kg lr ls ms kj lu nw mt lx ly nx mu mb mc ny mv mf mg mh ij bi translated"><em class="jf">两个矩阵𝑿和𝒀的内点积，给出一个相关矩阵𝑾据说是双向稳定的，是𝑿和𝒀回忆和反馈到𝑾的结果。</em></p></blockquote><p id="c79b" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">通常，这意味着矩阵𝑿和𝒀的相应行和列，以及它们的标量矢量积，同样对𝑾 <em class="nk">、</em>的值有贡献，作为回忆:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/3c10fae37dcb8ba5411952465e3e8dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F77IWp_tjpuJraj6AU6TsA.png"/></div></div></figure><p id="5fff" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">因为，矩阵𝑿和𝒀的每一个向量都可以通过下面讨论的内点积𝑾𝑿ₖ或𝑾 𝒀ₖ很容易地回忆起来。</p><p id="8802" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">在这种情况下，𝑾是一个矩阵，它推断𝑿ₖ或𝒀ₖ向量的相关性。整个学习过程非常类似于<strong class="lo jg"> <em class="nk">保持</em> </strong> (𝙬ᵢⱼ <strong class="lo jg"> ≠ 0 </strong>)或<strong class="lo jg"> <em class="nk">丢弃</em></strong>(𝙬ᵢⱼ<strong class="lo jg">= 0</strong>)<em class="nk"/>特定权重，将BAM的输入神经元和记忆细胞相互连接。</p><p id="1a25" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">突触权重𝙬ᵢⱼ <strong class="lo jg"> ∈ </strong> 𝑾的值可以大于、小于或等于<strong class="lo jg"> 0 </strong>。BAM只保存那些突触权重，其值要么是正的，要么是负的。在初始化模型时，等于<strong class="lo jg"> 0 </strong>的突触权重被简单地丢弃。</p><p id="085d" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">Donald Hebb提出了双向矩阵性质，作为著名的监督学习规则，它是Hopfield网络和其他联想记忆模型的基础。</p><p id="ba5b" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">根据Hebbian算法，在初始化突触权重的矩阵𝑾ₙₓₘ时，所有相关模式被存储在BAM的存储空间中，仅一次(例如，存储空间)。通过分别取输入𝑿ₚₓₙ和输出𝒀ₚₓₘ模式图(即，矩阵)的内点积来获得矩阵𝑾ₙₓₘ:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/2946f74b670a5fdf81c877653cac5891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mki-ipaTcuswF5VXWMG9Bg.png"/></div></div></figure><p id="77ee" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">或者，权重矩阵𝑾ₙₓₘ可以从𝑿ₚₓₙ、𝒀ₚₓₘ矩阵获得，作为相应模式𝙭ₖ和𝙮ₖ的外部克罗内克乘积的和。</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/919975df083a18a852452466186c0bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZxyonXI5Wvg4v5JbgQ2kg.png"/></div></div></figure><p id="c0c0" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">与𝑿和𝒀的内点积不同，(𝙭ₖ,𝙮ₖ)<strong class="lo jg">∑</strong>𝑿,𝒀向量的外积产生矩阵𝙬ₖ<strong class="lo jg">∑</strong>𝑾(即基本记忆)。最后，通过逐点获得它们的和，所有的矩阵𝙬ₖ被组合成单个矩阵𝑾。</p><p id="ff68" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">BAM的突触权重初始化的整个过程如下图所示:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/bc9ba953082b952228d18d10b478451e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2mPivPJ7Hw3t-_YWbWlVQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基于Hebbian算法学习BAM模型|图片作者</p></figure><p id="f7a0" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">然而，相关矩阵𝑾为作为正交向量的模式提供了最稳定和一致的记忆存储。𝑿,𝒀正交向量的乘积给出了对称相关矩阵𝑾，其中关联的基本记忆分别从模式图𝑿和𝒀学习。</p><p id="ff9d" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">因此，强烈建议在训练模型之前执行𝑿和𝒀模式图的正交化。由于𝑿和𝒀是两极的，𝒀的正交化可以很容易地做为:𝒀 = -𝑿.通过取负-𝑿，𝒀模式将具有与𝑿相同的值，其符号<strong class="lo jg"> (+/-) </strong>变为相反。</p><p id="3887" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">BAM的学习算法很简单，可以用Python 3.8和NumPy中的一行代码轻松实现:</p><figure class="nm nn no np gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="a14d" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">从记忆中回忆模式</h1><p id="34d5" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">因为输入和输出模式映射已经存储在BAM的存储器中，所以通过在多次迭代中计算BAM的完整输出来调用特定模式的关联，直到检索到输入模式𝑿的正确关联。</p><p id="a7e5" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">为了计算完整的内存输出，在每次迭代中，𝞭-iterations，𝒌=𝟏..𝞭，𝙓模式的BAM的输入𝙭ᵢ乘以相应的突触权重𝙬ᵢⱼ <strong class="lo jg"> ∈ </strong> 𝙒.在代数上，这是通过取权重矩阵𝙒-transpose和输入模式𝙓.的内积来完成的然后，将BAM输入的加权和𝙒 ᵀ𝙓应用于双极阈值函数𝒀=𝙁<strong class="lo jg"><em class="nk">(</em></strong>【𝙒ᵀ𝙓<strong class="lo jg"><em class="nk">)</em></strong>，以计算每个存储单元的输出，该输出对应于被调用的输出模式𝙮ⱼ <strong class="lo jg"> ∈ </strong> 𝒀的特定值<strong class="lo jg"> 1 </strong>或<strong class="lo jg"> -1 </strong>。这种计算类似于通过传统人工神经网络的每一层前馈输入。</p><p id="d459" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">双极阈值函数𝙁与经典阈值函数完全相同，唯一的区别是其正(+)或<strong class="lo jg"> 0 </strong>输入的值为<strong class="lo jg"> 1、</strong>和负(-)，除非另有说明。在这种情况下，𝙁的实际输入也包括零<strong class="lo jg"> : </strong></p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/de1bbcac3a5a10177ee2cff484770d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnvQ6XyNiFpyssa_FCmtYA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">双极性阈值函数Y=F(X)</p></figure><p id="c082" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">双极阈值函数的曲线图如下所示:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6b4ddfc4225b535a816e67640ccbf324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*rgjAoUtNe6-bgJGviLxwaQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">双极阈值激活功能|图片作者</p></figure><p id="1dbd" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">下面列出了一个简单的代码片段，演示了BAM的内存单元激活:</p><figure class="nm nn no np gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="df1a" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">与传统的人工神经网络不同，输出层中的存储单元还为下一次迭代𝙠+𝟏.计算新的输入𝙓ₖ₊₁然后，它检查当前𝙓和新输入𝙓ₖ₊₁是否不相等(𝙓≠𝙓ₖ₊₁).如果没有，它将新的输入分配给BAM，在下一次迭代中继续计算输出𝒀。否则，如果是𝙓=𝙓ₖ₊₁，则意味着输出𝒀是输入𝙓的正确关联，并且算法已经收敛，从过程返回模式𝒀。此外，BAM通过基于所讨论的相同算法双向计算存储器输出，提供了调用输入𝙓和输出𝒀的关联的能力。</p><p id="2c7b" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">图形调用过程如下图所示:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6227538faaefd7efa73200498b89fe79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*RPc3IKov8jj6mTn2PJj1Ng.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">内存单元输出(Y)计算|作者图片</p></figure><p id="5e45" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated"><strong class="lo jg">从BAM存储器中调出关联的算法如下:</strong></p><p id="5193" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">设𝑾ₙₓₘ—为相关权重矩阵(即BAM存储器的存储空间)，𝑿ₙ—为长度为𝒏的输入模式，𝒀ₘ—为长度为𝒎:的输出模式</p><p id="cb1a" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">回想一下先前存储在BAM内存中的输入模式𝑿的关联𝒀:</p><p id="2689" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated"><strong class="lo jg">对于每个𝞭-iterations、<em class="nk">𝟏</em>≤</strong>𝞭<strong class="lo jg">≤</strong><strong class="lo jg">𝒕，执行以下操作:</strong></p><ol class=""><li id="2487" class="mw mx jf lo b lp mr ls ms lv my lz mz md na mh of nc nd ne bi translated">用输入向量𝑿₁←𝑿.初始化BAM的输入</li><li id="17ab" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh of nc nd ne bi translated">计算BAM输出向量𝒀ₖ，作为权重矩阵𝑾 <strong class="lo jg"> ᵀ </strong>转置和输入向量𝑿ₖ的内点积，用于𝒌-th迭代:</li></ol><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/43c0897829478e417921aaa74bd63cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mvbMZVuxCNpaWYkHGggDxQ.png"/></div></div></figure><p id="63a3" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">3.获得下一次𝒌+𝟏)-th迭代的新输入向量𝑿ₖ₊₁，例如:</p><figure class="nm nn no np gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/094ff5bd0337cfce60e7d3fa2c77d30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oadIEKegFHOxIyqyn_ifQ.png"/></div></div></figure><p id="67c9" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">4.检查新的和现有的矢量𝑿ₖ₊₁≠𝑿ₖ是否<strong class="lo jg">不</strong>相同:</p><p id="cc85" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">如果<strong class="lo jg">不是</strong>，返回步骤1，计算𝒌+𝟏)-th迭代的输出𝒀ₖ₊₁，否则进行下一个步骤5。</p><p id="ca65" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">5.从过程中返回输出向量𝒀←𝒀ₖ，作为输入𝑿向量的正确关联。</p><p id="c581" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">6.继续步骤2–4，直到收敛。</p><p id="6f39" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">下面列出了Python 3.8和NumPy中实现模式预测算法的一段代码:</p><figure class="nm nn no np gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="d919" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">内存评估(测试)</h1><p id="b32e" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">下面列出的代码演示了BAM评估(测试)过程。它建立特定形状<em class="nk">(模式×神经元×记忆_细胞)</em>的BAM模型，并生成1D数组，由<strong class="lo jg"> 1 </strong>和<strong class="lo jg"> -1 </strong>值组成。然后，它将数组重新整形为输入的二维图案图𝑿.为了获得𝒀的模式图，它执行𝑿的正交化，例如:𝒀=−𝑿.接下来，计算相关矩阵，将𝑿和𝒀模式的关联存储到BAM的内存中。</p><p id="b961" class="pw-post-body-paragraph lm ln jf lo b lp mr kg lr ls ms kj lu lv mt lx ly lz mu mb mc md mv mf mg mh ij bi translated">最后，它对每个输入模式𝙭ᵢ <strong class="lo jg"> ∈ </strong> 𝙓执行模型评估，它从BAM的存储器中双向调用一个关联。当从存储器中调出输入模式x⃗的关联y⃗ₚ时，它会检查输出y⃗ₚ目标y⃗模式是否相同，显示每个输入模式𝙭ᵢ <strong class="lo jg"> ∈ </strong> 𝙓:的结果</p><figure class="nm nn no np gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="d0db" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">预测不完整的模式</h1><p id="24f7" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">BAM模型的优点之一是能够通过<strong class="lo jg"> &gt; 30 </strong>不完整或损坏的输入来预测正确的关联。下面的代码片段演示了不完整的模式预测。它从模式图𝑿中随机选择一个输入模式，并扭曲它，用任意的<strong class="lo jg"> 1 </strong>或<strong class="lo jg"> -1 </strong>替换它的多个值。它应用poison(…)函数(在下面的代码中实现)来扭曲随机选择的模式x⃗，正在回忆它的关联。最后，它预测一个关联模式，如果目标和预测的关联y⃗和y⃗ₚ是相同的，则执行一致性检查。如果是这样的话，x⃗被召回的正确联想模式是:</p><figure class="nm nn no np gt is"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="9353" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">结论</h1><p id="b810" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">尽管BAM模型无需持续学习就能灵活地存储和调用关联，但对于某些应用程序来说，它的效率可能会降低，因为它无法调用某些类型数据的正确关联。根据最新的研究，预测各种不完整或损坏的数据的关联基本上需要由大量存储单元组成的巨大尺寸的bam。当回忆少量模式的正确关联时，也存在几个问题。显然，BAM模型的现有变体必须经过改进才能用作大量应用的持久存储器。尽管如此，以下问题的解决方法仍在开发中。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><h1 id="a363" class="ku kv jf bd kw kx on kz la lb oo ld le kl op km lg ko oq kp li kr or ks lk ll bi translated">源代码:</h1><ul class=""><li id="52f8" class="mw mx jf lo b lp lq ls lt lv os lz ot md ou mh nb nc nd ne bi translated"><a class="ae ov" href="https://colab.research.google.com/drive/1M9oTh4cruwLJBvKs3ijMpY7ptYP_BGXJ" rel="noopener ugc nofollow" target="_blank"><em class="nk">《Python 3.8和NumPy中的双向联想记忆(BAM)》，Jupyter Notebook @ Google Colab</em></a></li><li id="a745" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated"><a class="ae ov" href="https://github.com/arthurratz/bam_associations_intro" rel="noopener ugc nofollow" target="_blank"> <em class="nk">双向联想记忆，Python 3.8 + NumPy样本，Visual Studio 2022 Project @ GitHub资源库</em> </a></li></ul><h1 id="32b2" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">放弃</h1><p id="beb6" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">所有的图片都是由这个故事的作者使用Draw.io应用程序设计的，<a class="ae ov" href="https://www.diagrams.net/" rel="noopener ugc nofollow" target="_blank">https://www.diagrams.net/</a></p><h1 id="c7cd" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">参考</h1><ol class=""><li id="24f8" class="mw mx jf lo b lp lq ls lt lv os lz ot md ou mh of nc nd ne bi translated"><a class="ae ov" href="https://en.wikipedia.org/wiki/Bidirectional_associative_memory" rel="noopener ugc nofollow" target="_blank"> <em class="nk">“双向联想记忆”——来自维基百科，免费百科。</em>T13】</a></li><li id="f051" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh of nc nd ne bi translated"><a class="ae ov" href="https://opg.optica.org/DirectPDFAccess/52A68963-3B21-4588-868A1CEBDFCC7199_30894/ao-26-23-4947.pdf?da=1&amp;id=30894&amp;seq=0&amp;mobile=no" rel="noopener ugc nofollow" target="_blank"> <em class="nk">自适应双向联想记忆，Bart Kosko，IEEE系统汇刊，人与控制论，第18卷第1期，1988年1月/2月。</em> </a></li><li id="380c" class="mw mx jf lo b lp nf ls ng lv nh lz ni md nj mh of nc nd ne bi translated"><a class="ae ov" href="http://www.cs.uccs.edu/~jkalita/work/cs587/2014/05PatternAssoc.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nk">模式协会或联想网络，Jugal Kalita科罗拉多大学科罗拉多泉分校。</em> </a></li></ol></div></div>    
</body>
</html>