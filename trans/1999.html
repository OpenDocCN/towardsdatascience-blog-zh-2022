<html>
<head>
<title>How to Build an AI Fashion Designer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何打造一个AI服装设计师</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-an-ai-fashion-designer-575b5e67915e#2022-05-06">https://towardsdatascience.com/how-to-build-an-ai-fashion-designer-575b5e67915e#2022-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee9b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于StyleGAN和GANSpace的服装语义编辑</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/ddbcbc36959f6f55a9cdb4d3fb61346e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*f_gxdq-LPjVbkoivCV_5WA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">ClothingGAN演示[图片由作者提供]</p></figure><h1 id="5ba0" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">概观</h1><p id="210e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这是我的老项目<a class="ae mi" href="https://github.com/mfrashad/ClothingGAN" rel="noopener ugc nofollow" target="_blank">clothing an</a>的一篇报道。该项目使用StyleGAN用AI生成服装设计，并使用袖子、尺码、连衣裙、夹克等属性对其进行语义编辑。您也可以如上图所示进行风格转换，首先生成2个不同的服装设计(输出1)，使用不同的种子编号。然后，它将生成第三个设计(输出2)，该设计混合了前两个设计。然后，您可以调整希望它从两个原始设计中继承多少风格或结构。</p><blockquote class="mj"><p id="0d0f" class="mk ml it bd mm mn mo mp mq mr ms mh dk translated">你可以试试这里的<a class="ae mi" href="https://huggingface.co/spaces/mfrashad/ClothingGAN" rel="noopener ugc nofollow" target="_blank">演示</a>，这里是<a class="ae mi" href="https://github.com/mfrashad/ClothingGAN" rel="noopener ugc nofollow" target="_blank">源代码</a>(随意开始回购)</p></blockquote><h1 id="42c9" class="ku kv it bd kw kx ky kz la lb lc ld le jz mt ka lg kc mu kd li kf mv kg lk ll bi translated">概述</h1><ul class=""><li id="6e91" class="mw mx it lo b lp lq ls lt lv my lz mz md na mh nb nc nd ne bi translated">灵感</li><li id="10b5" class="mw mx it lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated">我是如何建造的</li><li id="77f7" class="mw mx it lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated">训练风格GAN模型</li><li id="f131" class="mw mx it lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated">用GANSpace进行语义编辑</li><li id="3371" class="mw mx it lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated">用Gradio构建UI</li><li id="2d1f" class="mw mx it lo b lp nf ls ng lv nh lz ni md nj mh nb nc nd ne bi translated">部署到巨大的空间</li></ul><h1 id="33fb" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">灵感</h1><p id="7f7e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">GAN或生成对抗网络是一种生成模型，它能够通过学习大型图像数据集的概率分布来生成图像。我总是觉得GANs很吸引人，因为它使我能够创作出高质量的艺术或设计，即使没有绘画方面的技术或艺术技巧。最近在GAN上看到很多人脸编辑演示，但在其他数据集上很少看到语义操纵。因此，我创建了ClothingGAN应用程序，你可以与人工智能合作设计衣服，而不需要很高的技术专业知识。</p><h1 id="b98b" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">我是如何建造的</h1><p id="0dee" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">第一步，要有一个可以生成服装的生成模型。我没有设法找到一个公共模型，可以产生一个体面的质量图像，因此我决定用样式训练我自己的甘服装模型。然后我使用GANSpace，一种基于潜在空间的语义编辑方法，来提供编辑功能。它在GAN潜在空间中找到重要的方向，这些方向可能代表某些视觉属性，然后我手动标记这些属性。最后，我使用Gradio库构建了演示界面，并将其部署到HuggingFace空间。</p><h1 id="4e5f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">训练风格GAN模型</h1><p id="5f90" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我使用了StyleGAN2-ADA[2]模型，因为在项目进行的时候，最新的StyleGAN模型是StyleGAN2-ADA模型。但是，您可能希望使用当前最新版本StyleGAN3。虽然我不确定StyleGAN3与我的方法或我正在使用的其他库有多兼容。</p><p id="77af" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">为了训练模型，我使用了由<a class="ae mi" href="https://dgyoo.github.io/" rel="noopener ugc nofollow" target="_blank">dong eun Yoo</a>在<a class="ae mi" href="https://github.com/fxia22/PixelDTGAN" rel="noopener ugc nofollow" target="_blank">PixelDTGAN【1】</a>论文中创建的服装数据集。该数据集具有84，748幅图像，包括9，732幅具有干净背景的上部服装图像，这些图像与剩余的75，016幅时装模特图像相关联。我只使用了背景干净的服装图片。因此，用于训练StyleGAN模型的总图像大约是分辨率为512×512的9k图像。这是作者网站上分享的数据集的<a class="ae mi" href="https://dgyoo.github.io/" rel="noopener ugc nofollow" target="_blank">链接。PixelDTGAN论文受麻省理工学院许可。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/30a452592d46d84ef3ebc1aeea6b2ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWh8fiNIDswrb3625E2iQA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">LookBook数据集一览[图片由作者提供，数据集由<a class="ae mi" href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_31" rel="noopener ugc nofollow" target="_blank"> PixelDTGAN[1] </a>提供]</p></figure><p id="6171" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">我不会讨论如何训练模型的具体步骤，因为我之前已经就这个主题写了一篇文章。只需对所选数据集执行相同的步骤。</p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/how-to-train-stylegan2-ada-with-custom-dataset-dc268ff70544"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">如何使用自定义数据集训练StyleGAN2-ADA</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">了解如何训练人工智能生成您想要的图像</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ko nx"/></div></div></a></div><p id="e7ff" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">这是训练后的结果。</p><div class="kj kk kl km gt ab cb"><figure class="om kn on oo op oq or paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><img src="../Images/f7595d1e055d42096c00cc58823972d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*n3unHunwEo55-Igq.jpg"/></div></figure><figure class="om kn os oo op oq or paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><img src="../Images/3793467fbb18853a6109fe8923fa740f.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/0*suhM0_J42Rr2af2v.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk ot di ou ov translated">由训练模型生成的设计的样本和插值[图片由作者提供]</p></figure></div><h1 id="ac9c" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">用GANSpace进行语义编辑</h1><p id="bf24" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">语义图像编辑是在给定的源图像中修改语义属性(例如样式或结构)的任务。例如，修改一个人的头发颜色，同时保留这个人的身份。图像编辑的应用范围很广，从照片增强、用于艺术和设计目的的风格处理到数据扩充。语义图像编辑通常有两个目标:允许同时对多个属性进行连续操作，并在保持图像真实性的同时尽可能多地保留源图像的身份。</p><p id="d43d" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">现有的使用GANs的语义图像编辑方法可以主要分为图像空间编辑或潜在空间编辑。图像空间编辑学习直接将源图像转换成目标域中的另一图像的网络。这些方法通常只允许二进制属性改变，而不允许连续改变。这些方法的例子有pix2pix、StarGAN和DRIT++等。</p><p id="e03c" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">相反，潜在空间编辑通过操纵跨越GAN模型的潜在空间的输入向量来间接操纵图像。这些方法主要集中于在潜在空间中寻找代表生成图像的语义属性的路径。在这些路径中导航输入向量允许连续编辑属性。</p><p id="6d50" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">无监督、自我监督和监督的潜在空间编辑方法都已被提出。gan space<a class="ae mi" href="https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html" rel="noopener ugc nofollow" target="_blank">【3】</a>在潜在或特征空间中使用主成分分析(PCA)以无监督的方式寻找重要方向。使用闭型因子分解也可以类似地找到重要方向(来自SeFa论文)。自监督方法也能够在没有标签的情况下找到这些方向，因为它们生成自己的标签，但是通常受限于几何属性，例如旋转或缩放。另一方面，像InterfaceGAN这样的监督方法需要标签信息或方法的属性分类器。</p><p id="2682" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated"><a class="ae mi" href="https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html" rel="noopener ugc nofollow" target="_blank"> GANSpace[3] </a>讨论了预训练的GAN模型在对生成的图像进行造型时的使用。GAN模型学习将噪声分布z映射到图像分布的函数。因此，给定不同的噪声输入z，产生的输出将会不同。然而，深度学习模型通常是一个黑箱，它并不明确知道噪声输入和生成的输出之间的关系，因此无法明确控制输出。然而，GAN模型可以被调整为在给定类标签的情况下生成特定的类输出，如条件GAN中所研究的。然而，在训练期间，需要数据集的标签信息来调节GAN模型，这对于某些情况可能是不可行的。</p><p id="5028" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">另一方面，GANSpace的论文[3]提出，可以在z潜在空间中找到某些重要的方向，z潜在空间表示生成的输出中的已知语义概念，例如输出的风格。为了找到这个方向，对于几个样本观察中间层中的激活，并且从中间网络激活空间中的值计算PCA方向v。然后，方向v将被转移以在z潜在空间中找到对应方向u。整个过程如下图所示，取自GANSpace的论文。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/8f576f8210780aeb0eaf67660bdc6a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*q2wm5ulZ-2koaQjxLi5h1w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在GAN潜在空间中识别PCA方向的2D图解[来源:<a class="ae mi" href="https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html" rel="noopener ugc nofollow" target="_blank"> GANSpace论文[3] </a></p></figure><p id="9caf" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">可以在不同层中计算重要方向u，并且每层中的方向可以表示不同的语义概念。在早期层中找到的方向通常表示高级特征，如布料结构，而在最后几层中找到的方向通常表示低级特征，如光照或颜色。通过在这些已知的方向上操纵噪声输入z，我们可以将生成的输出操纵到期望的特征。下图显示了在不同GAN模型中应用GANSpace方法时的操作结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c1cba4f690e22b8f74decc1bbc03eb0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*WR4-aGqRXteT7OmmekxAXQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">GANSpace的结果是不同的模型。[来源:<a class="ae mi" href="https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html" rel="noopener ugc nofollow" target="_blank">甘斯佩斯论文[3] </a></p></figure><h2 id="dafc" class="oy kv it bd kw oz pa dn la pb pc dp le lv pd pe lg lz pf pg li md ph pi lk pj bi translated">在训练好的模型中寻找方向</h2><p id="c740" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这里将显示的代码是在Google Colab上测试的，你可以跟随<a class="ae mi" href="https://colab.research.google.com/drive/1JRJ9GfnLKybQFbs7iE3PEz4DZrAN4BY8?usp=sharing" rel="noopener ugc nofollow" target="_blank">我的笔记本</a>或者在你自己的环境中，但是如果你跟随Colab环境之外的环境，确保你的环境有预先安装在Colab中的依赖项。</p><blockquote class="mj"><p id="c239" class="mk ml it bd mm mn mo mp mq mr ms mh dk translated">如果你想跟随，这是教程笔记本</p></blockquote><p id="dc68" class="pw-post-body-paragraph lm ln it lo b lp pk ju lr ls pl jx lu lv pm lx ly lz pn mb mc md po mf mg mh im bi translated">首先，我们需要安装GANSpace所需的依赖项。</p><pre class="kj kk kl km gt pp pq pr ps aw pt bi"><span id="df88" class="oy kv it pq b gy pu pv l pw px">!pip install ninja gradio fbpca boto3 requests==2.23.0 urllib3==1.25.11`</span></pre><p id="24e0" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">运行完代码后重启运行时，然后克隆GANSpace repo。</p><pre class="kj kk kl km gt pp pq pr ps aw pt bi"><span id="7da5" class="oy kv it pq b gy pu pv l pw px">!git clone https://github.com/mfrashad/ClothingGAN.git</span><span id="7d28" class="oy kv it pq b gy py pv l pw px">%cd ClothingGAN/</span></pre><p id="ef45" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">运行以下代码进行进一步设置。确保您位于GANSpace文件夹中。</p><pre class="kj kk kl km gt pp pq pr ps aw pt bi"><span id="6985" class="oy kv it pq b gy pu pv l pw px">!git submodule update --init --recursive</span><span id="3f17" class="oy kv it pq b gy py pv l pw px">!python -c "import nltk; nltk.download('wordnet')"</span></pre><p id="1c82" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">接下来，我们必须修改GANSpace代码来添加我们的定制模型。对于StyleGAN2，我们需要模型文件的PyTorch版本。因为我们的StyleGAN模型文件在Tensorflow中。pkl格式，我们需要用rosinality做的转换器把它改成pytorch格式。pt文件。只要按照这个<a class="ae mi" href="https://colab.research.google.com/github/dvschultz/stylegan2-ada-pytorch/blob/main/SG2_ADA_PT_to_Rosinality.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>里的步骤就可以了。(该项目是在正式的StyleGAN2 PyTorch版本实现之前完成的，如果您的模型文件已经存在，您可以跳过这一部分。pt或Pytorch格式)。</p><p id="bbc0" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">接下来，回到GANspace文件夹，修改<code class="fe pz qa qb pq b">models/wrappers.py</code>来添加我们的模型文件。首先，转到StyleGAN2类，在<code class="fe pz qa qb pq b">config</code>变量中添加我们的模型名称和输出分辨率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/1faef0ba0f789fa646d90fbf9d0e2437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*nbmxB4XPxJYGpL6bpFfELg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">我在models/wrappers . py[Image by Author]的第117行的配置变量中添加了分辨率为512x512的“lookbook”模型</p></figure><p id="38e2" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">接下来，再向下滚动一点，在<code class="fe pz qa qb pq b">checkpoints</code>变量中添加到模型的链接。要生成到我们模型的链接，只需将模型文件上传到Google drive，并使用<a class="ae mi" href="https://sites.google.com/site/gdocs2direct/" rel="noopener ugc nofollow" target="_blank">这个站点</a>生成到它的直接链接。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qd"><img src="../Images/c614bfa0227299783b46d8185f774e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQnFntVc--J1ofAA-kirLg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">我在models/wrappers.py文件的第149行添加了一个新的生成器模型“lookbook”</p></figure><p id="c693" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">将模型添加到文件中之后。运行<code class="fe pz qa qb pq b">visualize.py</code> c脚本进行PCA，并在向计算出的主成分方向移动输入时可视化视觉变化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qe"><img src="../Images/8a70b543656b21883eff3d9e0258fdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SBx_lmnOXZzfwzWi902nrA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">用于PCA和可视化变化的命令[图片由作者提供]</p></figure><p id="edb0" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated"><code class="fe pz qa qb pq b">--use_w</code>选项意味着我们将操纵中间潜在代码<code class="fe pz qa qb pq b">w</code>，而不是StyleGAN中的原始潜在代码<code class="fe pz qa qb pq b">z</code>。<code class="fe pz qa qb pq b">num_components</code>是指定你想保留多少个方向或主成分。最大组件将是512或输入<code class="fe pz qa qb pq b">z</code>或<code class="fe pz qa qb pq b">w</code>尺寸。<code class="fe pz qa qb pq b">--video</code>选项是在主成分方向上移动时生成视觉变化的视频，而不仅仅是生成图像。该脚本可能需要大约30分钟才能完成。</p><p id="b734" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">一旦完成，它将在out文件夹中生成可视化的更改。对我来说，它在<code class="fe pz qa qb pq b">out/StyleGAN2-lookbook</code>文件夹下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qf"><img src="../Images/9263ac9ab2593eb53bf441cb4fdefd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*a8hzK_kjVImUebREd5ephQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">生成的可视化输出[图片由作者提供]</p></figure><p id="803e" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">我们将看一下<code class="fe pz qa qb pq b">style/ipca/summ/components_W.jpg</code>，因为它可视化了前14个主要组件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qg"><img src="../Images/48c1f9fbde7d996fa2b66787771b8edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ecZBE66AD6jkYJF7J5nQSQ.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">前14个主成分的可视化[图片由作者提供]</p></figure><p id="4ac4" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">从上图中，我们可以开始选择要放入演示中的主要组件，并给它们贴上标签。例如，在我看来，C0可以标记为袖长，C1为夹克，C2和C3为外套，C4和C5为服装的亮度，C6为较短的服装。</p><p id="149a" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">您还可以在附加文件<code class="fe pz qa qb pq b">sampX_real_W.jpg</code>中看到不同样本的可视化，以确保由主成分引起的变化在不同样本之间保持一致。还有9个额外的样本是由<code class="fe pz qa qb pq b">visualize.py</code>脚本生成的。</p><p id="d0e7" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">这是另一个例子的可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qg"><img src="../Images/01e88e48b9705d02f937d54ad2e9b836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqe6q1ZgBu6bgkJkytRC3g.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">不同样本的主要成分的可视化[图片由作者提供]</p></figure><p id="e1f4" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">你可以看到，即使是不同的样品(袖长为C0，夹克为C1等)，变化也大致一致。</p><p id="5738" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">此外，您还可以在<code class="fe pz qa qb pq b">comp</code>或<code class="fe pz qa qb pq b">inst</code>文件夹中看到每个组件的可视化视频。主成分本身以<code class="fe pz qa qb pq b">.npz</code>的格式保存在<code class="fe pz qa qb pq b">cache/components/</code>文件夹中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/2e5b2bc0c1c1944f1e1ad772053870c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*s7nmvv5W1l8ly7JJr5pQzg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">计算出的主成分文件的位置[图片由作者提供]</p></figure><p id="4c62" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">一旦我们有了组件，我们就可以开始构建演示UI了。</p><h1 id="b20c" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">用Gradio构建UI</h1><p id="3b3e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">Gradio 是一个python库，它使得用几行代码构建ML项目的UI/demo变得极其容易。下面是Gradio有多简单的一个例子:</p><div class="kj kk kl km gt ab cb"><figure class="om kn qi oo op oq or paragraph-image"><img src="../Images/dc76082cc2a806304cb6289d768706cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*4QThHxuvaQRR8Vt6IZA4Lw.png"/></figure><figure class="om kn qj oo op oq or paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><img src="../Images/1bfef46acb10ce615db9466228e2bfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*U887smFdbUArMPblXTBFNw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk qk di ql ov translated">示例代码和使用Gradio生成的应用程序[图片由作者提供]</p></figure></div><p id="c1b1" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">当你想把你的ML应用程序演示成一个单一的函数时，Gradio是合适的。</p><p id="5c05" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">首先，我们需要将发电机模型和主要组件加载到内存中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qm qn l"/></div></figure><p id="437b" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">然后，我们将定义一个效用函数来操作指定方向的<code class="fe pz qa qb pq b">w</code>输入，并使用生成器生成图像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qm qn l"/></div></figure><p id="b57c" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">最后，我们可以定义主函数<code class="fe pz qa qb pq b">generate_image</code>并使用Gradio库为该函数构建UI。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qm qn l"/></div></figure><p id="f0c4" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">这就是结果！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qo"><img src="../Images/91aeb2831bac915f6850cd759ff2622c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wi7HOIroo1-r_y2rsi8_0Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">带有Gradio的演示用户界面[图片由作者提供]</p></figure><p id="18f2" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">Gradio还将提供一个链接，您可以在那里分享您的朋友或任何人都可以尝试演示。然而，演示的主机并不是永久的，Colab服务器被限制在12或24小时后才会自行终止。对于永久托管，您可以简单地在云中或您自己的服务器上运行代码。但幸运的是，拥抱脸创造了<a class="ae mi" href="https://huggingface.co/spaces" rel="noopener ugc nofollow" target="_blank"> Spaces </a>，一个你可以简单上传你的ML应用并永久免费托管的平台(如果你想要GPU需要付费)。此外，它与Gradio和Streamlit完美集成，开箱即用。</p><h1 id="00e1" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">部署到巨大的空间</h1><p id="08e0" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">首先，前往<a class="ae mi" href="https://huggingface.co/spaces" rel="noopener ugc nofollow" target="_blank">空间</a>并登录/注册您的帐户。然后单击“创建新空间”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qp"><img src="../Images/d9ea9c38fafbe77b5b47144d21a452c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V73IKscL3MuUCtsX-HZvjA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">拥抱脸空间主页[图片由作者提供]</p></figure><p id="4176" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">然后，选择您想要的名称和许可证，并选择Gradio作为space SDK。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qq"><img src="../Images/1ea56fa2509205d3ba80bae259f1ad8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Suv8c3LvB9DnvHUqSQUZrw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">创造新空间[作者图片]</p></figure><p id="2fb7" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">接下来，克隆拥抱脸回购。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qr"><img src="../Images/a79f25447828fcaae655bb92ac5b4791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NuvxTT8hxc0ZRnqy1IPtCQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">[图片由作者提供]</p></figure><p id="66fc" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">就我个人而言，在推送回购时，我遇到了一个身份验证问题，我不得不使用令牌作为身份验证方法，方法是将远程URL设置为:</p><pre class="kj kk kl km gt pp pq pr ps aw pt bi"><span id="196a" class="oy kv it pq b gy pu pv l pw px">https://HF_TOKEN@huggingface.co/spaces/mfrashad/Test</span></pre><p id="9b70" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">或者你也可以在URL中使用你的拥抱脸账户的用户名和密码进行认证。</p><pre class="kj kk kl km gt pp pq pr ps aw pt bi"><span id="316a" class="oy kv it pq b gy pu pv l pw px">https://HF_USERNAME:PASSWORD@huggingface.co/spaces/mfrashad/Test</span></pre><p id="efc9" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">一旦克隆完成，我们就可以开始创建演示所需的文件。在Spaces repo中需要3个重要的文件:<code class="fe pz qa qb pq b">requirements.txt</code>指定要与<code class="fe pz qa qb pq b">pip install</code>一起安装的所有python依赖项，<code class="fe pz qa qb pq b">packages.txt</code>指定要与<code class="fe pz qa qb pq b">apt install</code>一起安装的依赖项，以及包含Gradio演示代码的主python文件<code class="fe pz qa qb pq b">app.py</code>。</p><p id="4fe6" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">此外，您需要使用git-lfs将任何二进制文件上传到repo，例如图像。</p><p id="79c8" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">所以我所做的只是将我们在Colab中的所有代码复制到Spaces repo中。删除图像、二进制文件和演示不需要的文件。把我们笔记本里所有的python代码放到一个python文件<code class="fe pz qa qb pq b">app.py</code>里。然后创建<code class="fe pz qa qb pq b">requirements.txt</code>和<code class="fe pz qa qb pq b">packages.txt</code>。一旦完成，简单地<code class="fe pz qa qb pq b">git push</code>和瞧！该演示将在拥抱脸空间提供给任何人尝试(假设你没有任何错误)。</p><blockquote class="qs qt qu"><p id="0b16" class="lm ln qv lo b lp nk ju lr ls nl jx lu qw nm lx ly qx nn mb mc qy no mf mg mh im bi translated">关于代码的完整内容，你可以查看<a class="ae mi" href="https://huggingface.co/spaces/mfrashad/ClothingGAN/tree/main" rel="noopener ugc nofollow" target="_blank">clothingan Space repo</a>中的文件。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qz"><img src="../Images/47693f89bce24ac9065de3a903a5d18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_i6wWD8ihnTfmDXarZB_-w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">您的共享空间演示将显示在您的个人资料和共享空间主页上[图片由作者提供]</p></figure><p id="6688" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">恭喜你！你设法一直读到这一点，并希望设法做好一切。对于更多的挑战，您可以尝试训练自己的StyleGAN模型，并应用语义编辑。例如，我也将相同的方法应用于角色和时装模型的生成。</p><p id="1577" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated"><a class="ae mi" href="https://huggingface.co/spaces/mfrashad/CharacterGAN" rel="noopener ugc nofollow" target="_blank">你可以在这里试试CharacterGAN的试玩</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/398db2c244aaf9f8e10eb6409bb58ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/1*7FktVCoC0a77UgDqvTToQA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">字符生成和语义编辑[图片由作者提供]</p></figure></div><div class="ab cl rb rc hx rd" role="separator"><span class="re bw bk rf rg rh"/><span class="re bw bk rf rg rh"/><span class="re bw bk rf rg"/></div><div class="im in io ip iq"><p id="8d86" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">如果你喜欢阅读这样的教程，并希望支持我成为一名作家，可以考虑注册成为一名媒体会员。每月5美元，你可以无限制地阅读媒体上的故事。如果你注册使用我的链接，我会赚一小笔佣金。</p><div class="nu nv gp gr nw nx"><a href="https://medium.com/@mfrashad/membership" rel="noopener follow" target="_blank"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">通过我的推荐链接加入Medium—Fathy Rashad</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">medium.com</p></div></div><div class="og l"><div class="ri l oi oj ok og ol ko nx"/></div></div></a></div><p id="ce74" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">另外，看看我的其他故事。</p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/how-i-built-an-ai-text-to-art-generator-a0c0f6d6f59f"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">我如何建立一个人工智能文本到艺术的生成器</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">一篇关于我如何建设Text2Art.com的详细、循序渐进的文章</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="rj l oi oj ok og ol ko nx"/></div></div></a></div><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/animating-yourself-as-a-disney-character-with-ai-78af337d4081"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">用人工智能让你自己成为一个迪斯尼角色</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">先睹为快数字艺术的未来</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="rk l oi oj ok og ol ko nx"/></div></div></a></div><h1 id="196f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">参考</h1><p id="73df" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">[1] Yoo，d .，Kim，n .，Park，s .，Paek，A. S .，&amp; Kweon，I. S. (2016年10月)。像素级域转移。在<em class="qv">欧洲计算机视觉会议</em>(第517–532页)。斯普林格，查姆。</p><p id="97b9" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">[2]t . Karras，m . Aittala，j . hells ten，Laine，s .，Lehtinen，j .，&amp; Aila，T. (2020年)。用有限数据训练生成性对抗网络</p><p id="2487" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">[3]哈尔科宁、赫茨曼、莱蒂宁和巴黎(2020年)。Ganspace:发现可解释的gan控制。<em class="qv">神经信息处理系统的进展</em>，<em class="qv"> 33 </em>，9841–9850。</p></div></div>    
</body>
</html>