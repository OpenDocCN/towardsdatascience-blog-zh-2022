<html>
<head>
<title>Simulate Any Functions with a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用神经网络模拟任何功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simulate-any-functions-with-neural-network-8e52f2083e3d#2022-04-06">https://towardsdatascience.com/simulate-any-functions-with-neural-network-8e52f2083e3d#2022-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="90d1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用PyTorch建立一个神经网络模型来模拟任何函数</h2></div><p id="cc3c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们知道线性回归模型可以帮助生成一条线来模拟“线性数据”。一个形式为<strong class="kk iu"> f(x) = ax + b </strong>的函数。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/725e319f2746749ff51cc5f55eb4af01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9q9zNd1b4BU_OBa3jsA95w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">线性回归，作者图片</p></figure><p id="734b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，其他函数呢，比如非线性函数？甚至是我们一无所知的不规则形状。我们还能模拟一个模型来拟合它吗？像头图中的那种甚至是螺旋形的？</p><p id="74f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，我受到了这个视频的启发:<a class="ae lu" href="https://www.youtube.com/watch?v=0QczhVg5HaI&amp;ab_channel=EmergentGarden" rel="noopener ugc nofollow" target="_blank">为什么神经网络可以学习几乎任何东西</a>。我意识到，有了真实的数据在手，我们可以…</p><blockquote class="lv"><p id="aef2" class="lw lx it bd ly lz ma mb mc md me ld dk translated">用神经网络模型模拟任何函数！</p></blockquote><p id="12cb" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">紧急花园的视频确实是一个被低估的视频，强烈推荐你也去看，视频中的链接质量也很高。</p><p id="1828" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将使用PyTorch实现一个<strong class="kk iu">任意函数模拟器</strong>。出于两个目的:</p><ol class=""><li id="7df0" class="mk ml it kk b kl km ko kp kr mm kv mn kz mo ld mp mq mr ms bi translated">见证神经网络模型的威力。看试衣过程真的很好玩。</li><li id="8df5" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated">经过训练的模型也可以用于检测异常数据。常规数据通常处于稳定的模式，比如说，在工作日相对较高，在周末下降。经过训练的神经网络模型可用于检测异常数据。</li></ol><h1 id="2210" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">生成培训数据</h1><p id="c7c1" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">以下代码将初始化<strong class="kk iu"> x </strong>数字，并使用以下函数生成用于模拟的<strong class="kk iu"> y </strong>数字。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5c899e777aa0c9bca07768385572b677.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/0*-IVoinKp0OVbkZRG"/></div></figure><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="b409" class="ob mz it nx b gy oc od l oe of">import numpy as np<br/>import matplotlib.pyplot as plt<br/>X = np.array([*range(-20,20)],<em class="og">dtype</em>=np.float32)<br/>X = X*0.1<br/>y = [x**3+ x**2 -3*x -1 for x in X]<br/>plt.plot(X,y,'ro')</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oh"><img src="../Images/461f8142d233d3b0a689566015eb9c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNedW4lWFnvuv08deEkiIg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">一个“未知形状的点”，作者图片</p></figure><p id="5297" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们既有输入<strong class="kk iu"> x </strong>又有输出<strong class="kk iu"> y </strong>，接下来让我们忘记上面的函数，因为我要构建一个神经网络来模拟“未知”函数。</p><h1 id="aa53" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">PyTorch模型的转换数据</h1><p id="2078" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">在将数据输入NN模型之前，我需要将list和NumPy数组数据转换为PyTorch张量。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="b042" class="ob mz it nx b gy oc od l oe of">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="329c" class="ob mz it nx b gy oi od l oe of">X_t = torch.tensor(X,<em class="og">dtype</em>=torch.float32)<br/>y_t = torch.tensor(y,<em class="og">dtype</em>=torch.float32)<br/>X_t = X_t.view(X_t.shape[0],1)<br/>y_t = y_t.view(y_t.shape[0],1)</span></pre><p id="db07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，<strong class="kk iu"> X_t </strong>和<strong class="kk iu"> y_t </strong>都是2d张量数组。</p><h1 id="d587" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">定义具有一个隐藏层的神经网络模型</h1><p id="cf56" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">在这里，我将定义一个简单的神经网络模型，只有一个隐藏层，隐藏层神经数字设置为<strong class="kk iu"> 16 </strong>更容易可视化。在代码中，我将把数字增加到<strong class="kk iu"> 128 </strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oj"><img src="../Images/1eeccac7a1d76142e503b6e7dbe945b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X62lDupegqQVfSJnroxeYw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">具有一个隐藏层的NN。Andrew Zhu使用此<a class="ae lu" href="http://alexlenail.me/NN-SVG/LeNet.html" rel="noopener ugc nofollow" target="_blank">工具</a>生成的图像</p></figure><p id="1d77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代码如下:</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="f055" class="ob mz it nx b gy oc od l oe of"><em class="og">class</em> func_simulator(nn.Module):<br/>    <em class="og">def</em> __init__(<em class="og">self</em>):<br/>        super(func_simulator,<em class="og">self</em>).__init__()<br/>        <em class="og">self</em>.l1 = nn.Linear(1,128)<br/>        <em class="og">self</em>.l2 = nn.Linear(128,1)</span><span id="ef3f" class="ob mz it nx b gy oi od l oe of"><em class="og">    def</em> forward(<em class="og">self</em>,<em class="og">x</em>):<br/>        out = <strong class="nx iu">F.relu</strong>(<em class="og">self</em>.l1(<em class="og">x</em>))<br/>        out = <em class="og">self</em>.l2(out)<br/>        return out</span></pre><p id="33df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ReLU激活是这里的关键，不要改成乙状结肠。</p><h1 id="2c82" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">训练并验证结果</h1><p id="a6ea" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">现在，让我们设置学习率，历元数，损失，梯度函数开始训练。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="d1c7" class="ob mz it nx b gy oc od l oe of">learning_rate,num_epochs    = 0.0001,100000<br/>model                       = func_simulator()<br/>loss                        = nn.MSELoss()<br/>gradient = torch.optim.SGD(model.parameters(),<em class="og">lr</em>=learning_rate)</span><span id="4c51" class="ob mz it nx b gy oi od l oe of"># start training<br/>for epoch in range(num_epochs):<br/>    y_pred = model.forward(X_t)<br/>    l = loss(y_pred,y_t)<br/>    l.backward()<br/>    gradient.step()<br/>    gradient.zero_grad()<br/>    if(epoch+1)%1000==0:<br/>        print(<em class="og">f</em>'epoch:{epoch+1},loss={l.item()<em class="og">:.3f</em>}')</span></pre><p id="9ee5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看结果:</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="988d" class="ob mz it nx b gy oc od l oe of">predicted = model(X_t).detach().numpy()<br/>plt.plot(X_t,y_t,'ro')<br/>plt.plot(X_t,predicted,'b')<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ok"><img src="../Images/ff6d49bf718bdce7d7db6acc3b5bea9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSwK4TGQgtv0ckwunKXd_g.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">带有一个隐藏层的模拟线条，图像由Andrew Zhu提供</p></figure><p id="81be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">红点是训练点，蓝线是来自已训练模型的点。模拟在一些区域不是很好，特别是开始和结束部分，我添加了另一个隐藏层，看看这两层是否能带来更好的模拟。</p><h1 id="ad02" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">用两个隐藏层微调模型</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ol"><img src="../Images/d54fa3851ab75a304f97b657e194374f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qwdz6nxQa8vBegFJ_cjNsA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">具有两个隐藏层的NN。Andrew Zhu使用此<a class="ae lu" href="http://alexlenail.me/NN-SVG/LeNet.html" rel="noopener ugc nofollow" target="_blank">工具</a>生成的图像</p></figure><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="537a" class="ob mz it nx b gy oc od l oe of"><em class="og">class</em> func_simulator(nn.Module):<br/>    <em class="og">def</em> __init__(<em class="og">self</em>):<br/>        super(func_simulator,<em class="og">self</em>).__init__()<br/>        <em class="og">self</em>.l1 = nn.Linear(1,128)<br/>        <strong class="nx iu"><em class="og">self</em>.l2 = nn.Linear(128,10)</strong><br/>        <strong class="nx iu"><em class="og">self</em>.l3 = nn.Linear(10,1)</strong></span><span id="c724" class="ob mz it nx b gy oi od l oe of">    <em class="og">def</em> forward(<em class="og">self</em>,<em class="og">x</em>):<br/>        out = F.relu(<em class="og">self</em>.l1(<em class="og">x</em>))<br/>        <strong class="nx iu">out = F.relu(<em class="og">self</em>.l2(out))</strong><br/>        out = <em class="og">self</em>.l3(out)<br/>        return out</span></pre><p id="de99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我强调了与单层版本不同的代码，2层模型可以生成更好的模拟。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi om"><img src="../Images/7cfe87528e80c232099add66e25b46ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38hLIlqVBO8nHOfBL0960A.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">红点是实数，蓝线是NN模拟的线。模拟线与两个隐藏层，图像由安德鲁朱</p></figure><h1 id="5ec3" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">包裹</h1><p id="1eb3" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">为什么神经网络模型可以模拟任何函数？这种强大的功能是从神经网络模型的本质继承而来的。本质上，神经网络是一个多层参数系统。提供给模型的训练数据越多，需要的参数就越多(层也越多)。一层中每个神经捕捉一个微小特征，几个组合的神经确定一个主要特征，等等。</p><p id="eec5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的示例中，我们可以看到，与只有一个隐藏层的模型相比，两层神经网络生成了更好的拟合线。训练数据越多，训练模型需要的神经元和层数就越多。但最终，如果存在一台无限的计算机，有足够的数据，我们可以模拟几乎任何功能。</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="738d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我意识到，只要有足够的训练数据，计算机和神经网络就可以模拟任何函数。我对图像对象检测的理解提高了，并导致了一个更令人震惊的猜想:</p><blockquote class="lv"><p id="f99c" class="lw lx it bd ly lz ma mb mc md me ld dk translated">如果存在一台超级强大的计算机，它能模拟我们的现实世界，甚至整个宇宙吗？</p></blockquote><p id="b4a5" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">也许我们都生活在一个模拟的世界里。</p><h1 id="ab40" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">参考</h1><ol class=""><li id="5663" class="mk ml it kk b kl nq ko nr kr ou kv ov kz ow ld mp mq mr ms bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/can-neural-networks-really-learn-any-function-65e106617fc6">神经网络能解决任何问题吗？</a></li><li id="bf21" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated"><a class="ae lu" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">神经网络可以计算任何函数的直观证明</a></li><li id="d4a5" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated"><a class="ae lu" href="https://www.youtube.com/watch?v=0QczhVg5HaI&amp;ab_channel=EmergentGarden" rel="noopener ugc nofollow" target="_blank">为什么神经网络几乎可以学习任何东西</a></li><li id="21dd" class="mk ml it kk b kl mt ko mu kr mv kv mw kz mx ld mp mq mr ms bi translated"><a class="ae lu" href="https://www.deeplearningbook.org/contents/mlp.html" rel="noopener ugc nofollow" target="_blank">深度前馈网络</a></li></ol><h1 id="bdbe" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">附录—代码</h1><p id="eca4" class="pw-post-body-paragraph ki kj it kk b kl nq ju kn ko nr jx kq kr ns kt ku kv nt kx ky kz nu lb lc ld im bi translated">这里是本文中使用的完整代码，您可以在安装了PyTorch的机器上复制并运行它，不需要GPU。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ox oy l"/></div></figure></div></div>    
</body>
</html>