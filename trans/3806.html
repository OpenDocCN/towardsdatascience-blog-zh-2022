<html>
<head>
<title>AI Accelerators and Machine Learning Algorithms: Co-Design and Evolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能加速器和机器学习算法:协同设计和进化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179#2022-08-24">https://towardsdatascience.com/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179#2022-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="83b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">人工智能加速器机器学习中的高效算法和方法—英伟达GPU、英特尔Habana Gaudi和AWS Trainium和推理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/22fed35d2d57d2587bca7e9cb963650e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAgQSIEprO1looCxAdf52w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="4f19" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">很有可能你正在一台现代电脑上阅读这篇博文，这台电脑要么放在你的膝盖上，要么放在你的手掌上。这种现代计算机具有中央处理器(CPU)和其他专用芯片，用于专门的任务，如图形、音频处理、网络和传感器融合等。这些专用处理器可以比通用CPU更快、更有效地执行专门的任务。</p><p id="fab4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从计算的早期开始，我们就将CPU与专用处理器配对。70年代早期的8位和16位CPU在执行浮点计算时速度很慢，因为它们依赖软件来模拟浮点指令。由于计算机辅助设计(CAD)、工程模拟等应用需要快速浮点运算，CPU与一个数学协处理器配对，它可以将所有浮点计算卸载到这个专用协处理器。数学协处理器是专用处理器的一个例子，它被设计用来比CPU更快更有效地完成特定任务(浮点运算)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lr"><img src="../Images/67bfb257484ad63ddeaa1a3c41f6b4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gn077OnurZwvEZBLg0-ZMg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="b75d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你一直在关注人工智能或半导体行业的最新进展，你可能听说过专用机器学习处理器(人工智能加速器)。虽然最受欢迎的加速器是NVIDIA GPUs，但还有其他加速器，包括英特尔Habana Gaudi、Graphcore Bow、谷歌TPUs、AWS Trainium和Inferentia。为什么今天有这么多AI加速器选项？它们和CPU有什么不同？算法如何改变以支持这些硬件？硬件如何发展以支持最新的算法？让我们找出答案。在这篇博文中，我将介绍:</p><ul class=""><li id="f66c" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">为什么我们需要专门的AI加速器？</li><li id="f02f" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">对机器学习硬件进行分类——CPU、GPU、AI加速器、FPGAs和ASICs</li><li id="a4f4" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">“硬件感知”算法和“算法感知”硬件</li><li id="b7d7" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">人工智能加速器和高效机器学习算法的共同进化</li><li id="fba2" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">人工智能加速器和有效的推理算法</li><li id="aa7c" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">人工智能加速器和用于训练的高效算法</li><li id="9e2d" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">人工智能加速器的未来</li></ul><h1 id="24c1" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">为什么我们需要专门的AI加速器？</h1><p id="ea0e" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">构建机器学习专用处理器的三个最重要的原因是<strong class="kx ir"> (1)能效(2) </strong> <strong class="kx ir">更快的性能和(3)模型大小和复杂性</strong>。提高模型准确性的最新趋势是引入具有更多参数的更大模型，并在更大的数据集上训练它们。我们在包括计算机视觉、自然语言和推荐系统在内的所有应用中都看到了这种趋势。</p><p id="2bb4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像GPT 3这样的语言模型有1750亿个参数，这在几年前还被认为是极端的。从那时起，我们已经看到了像GLaM的模型有1.2万亿个参数，NVIDIA的MT-NLG有5300亿个参数。如果历史有任何指示，模型大小将继续变大，当前的处理器将无法在严格的训练时间和推理延迟要求下提供训练或运行这些模型的推理所需的处理能力。</p><p id="107c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，我们需要专门的人工智能加速器并使开发定制芯片的经济性发挥作用的一个最重要的原因是对能源效率的需求。</p><h2 id="a7de" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">为什么我们需要节能处理器？</strong></h2><p id="dcb4" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">机器学习模型越大，需要执行的内存访问操作就越多。像矩阵-矩阵和矩阵-向量计算这样的计算操作是比存储器访问更节能的操作。如果你看看从存储器读取访问所消耗的能量与加法和乘法运算所消耗的能量相比，存储器访问操作比计算操作多消耗几个数量级的能量。由于大型网络不适合片上存储，因此需要更多消耗能源的DRAM访问。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/e90e6e3cac827d2068c9ec89a8d542c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDQfg0CDm9jEH9WkBBt7rQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae np" href="https://arxiv.org/pdf/1506.02626v3.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02626v3.pdf</a></p></figure><p id="8129" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当像深度神经网络这样的应用程序在通用处理器上运行时，即使通过扩展来实现很小的性能改善，也会带来巨大的能耗和设备成本。</p><blockquote class="nr ns nt"><p id="dcae" class="kv kw nu kx b ky kz jr la lb lc ju ld nv lf lg lh nw lj lk ll nx ln lo lp lq ij bi translated">G <!-- -->通用处理器(如CPU)以能效换取多功能性，专用处理器(人工智能加速器)以能效换取多功能性。</p></blockquote><p id="cfb6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一方面，人工智能加速器可以设计为具有最大限度地减少内存访问的功能，提供更大的片上缓存，并包括专用硬件功能来加速矩阵-矩阵计算。由于人工智能加速器是专门建造的设备，它“知道”它运行的算法，其专用功能将比通用处理器更有效地运行它。</p><h1 id="4b2e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">对机器学习硬件进行分类——CPU、GPU、AI加速器、FPGAs和ASICs</h1><p id="3f4d" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">现在让我们来看看不同类型的处理器，以及它们在通用处理器和专用处理器中的位置。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/f453f1ff349fdc4ac56033cd9b2f77c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePAKUjsHi4aAHeLSpUuXcg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="cfa0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在谱的左端是CPU，这些是真正通用的，因为你可以在它们上面运行任意代码。您可以使用它们来执行专用芯片执行的任何任务，包括图形、音频处理、机器学习等。如前所述，您将放弃性能和能效。</p><p id="8089" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一端是专用集成电路(ASICs)。这些也被称为固定功能芯片，因为它们的存在只是为了做一件或几件事情，通常是不可编程的，并且不公开开发人员关注的API。这方面的一个例子是耳机中的噪声消除处理器。它需要高能效以延长耳机的电池寿命，并且功能足够强大以达到目标延迟，这样当您使用它们观看您最喜欢的电视节目时，就不会遇到延迟或令人沮丧的不同步问题。</p><p id="89f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">谱的左端是关于通用和可编程性，谱的右端是关于专用和效率。</p><h2 id="6c1c" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated">像GPU、FPGAs和AI加速器这样的处理器属于这个范围的什么位置？</h2><p id="cdd5" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated"><strong class="kx ir">回答:</strong>介于这两个极端之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/371ebed613ebbd38aa93fa93d016de80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T-4pDj4RmqnI3xOTeKmUWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="aeaa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更接近ASIC一端的是现场可编程门阵列(FPGAs)。顾名思义，它是可编程的，但你需要硬件设计技能和硬件描述语言(HDL)的知识，如verilog或VHDL。换句话说，它太低级了，离芯片太近，软件开发人员没有技能和工具来编程。</p><p id="138e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在CPU一端的右边是GPU。与CPU不同，GPU是专用处理器，擅长并行工作负载，如图形着色器计算和矩阵乘法。CPU更适合延迟敏感型应用，而GPU更适合需要高吞吐量的应用。GPU在某种意义上类似于CPU，它们是可编程的，并使用NVIDIA CUDA和OpenCL等语言。与CPU相比，这些并行处理器可以运行更少的任意功能，但真正擅长利用并行性的代码。</p><p id="d4d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人工智能加速器，如英特尔Habana Gaudi、AWS Trainium和AWS推理，位于GPU的右侧。Habana Gaudi提供可编程性，但不如GPU通用，所以我们可以让它更接近GPU。AWS推理是不可编程的，但它提供了一系列支持的操作，如果您的机器学习模型不支持这些操作，那么AWS推理会为这些操作实现CPU后退模式。因此，我们可以将AWS推理进一步放在光谱的右边。</p><h1 id="c21e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">“硬件感知”算法和“算法感知”硬件</h1><p id="6248" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">既然我们已经有了一个思考不同类型处理器的心智模型，让我们来讨论这些处理器如何与运行在其上的软件进行交互。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/67c80a153a3e1bb702a095967dffb2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TrBVlQ4UoRRTFj4tzbXzw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="654a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通用计算模型有两个组件(1)软件和算法，以及(2)运行软件的硬件处理器。在大多数情况下，这两者是相当独立的，因为当你写软件时，你很少考虑它在哪里运行。设计硬件功能时，它们旨在支持最广泛的软件。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/91582ab24a7dd9ffb806d23e9d6dd16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cT_F6n4FEicfNc_D3rG5Cw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="b1ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该模型开始为深度学习等需要更高性能和能效的应用而发展，弥合了算法设计和硬件设计之间的差距。现代机器学习计算模型还包括两个组件(1)机器学习算法和软件框架(2)与AI加速器配对的通用处理器。</p><p id="a648" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与通用计算模型不同，研究人员开发了“硬件感知”的机器学习算法，即他们编写的代码利用了硬件特定的功能，如多精度类型(INT8、FP16、BF16、FP32)，并针对特定的芯片功能(混合精度、结构化稀疏)。作为用户，你可以通过流行的机器学习软件框架来利用这些特性。类似地，硬件设计师构建“算法感知”的人工智能加速器，即他们设计专用的硅功能，可以加速机器学习矩阵计算(例如张量核)。</p><p id="f452" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种“硬件-算法意识”是可能的，因为人工智能加速器硬件和机器学习算法正在共同进化。硬件设计师正在向人工智能加速器添加由机器学习算法利用的功能，机器学习研究人员正在创建新的算法和方法，可以利用人工智能加速器上的特定功能。</p><blockquote class="nr ns nt"><p id="ab72" class="kv kw nu kx b ky kz jr la lb lc ju ld nv lf lg lh nw lj lk ll nx ln lo lp lq ij bi translated">硬件和软件作为一个整体更加紧密地工作，从而带来更高的性能和<strong class="kx ir">能效</strong>。</p></blockquote><h1 id="30d1" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">人工智能加速器和高效机器学习算法的共同进化</h1><p id="22a2" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">人工智能加速器分为两类——<strong class="kx ir">(1)用于训练的人工智能加速器(2)用于推理的人工智能加速器</strong>。由于训练和推理的目标是不同的，并且AI加速器是用于专门工作负载的专用处理器，因此为每种类型的工作负载设计单独的处理器是有意义的。</p><p id="38f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练加速器的目标是减少训练时间，并且训练加速器被设计成具有训练算法可以利用的硬件特征。这通常意味着训练加速器是更高性能的处理器，更多的内存支持更高的吞吐量(每秒处理的数据)。由于训练加速器是吞吐量集中的设备，如果您最大化吞吐量或利用率，它的能量成本/吞吐量将会更低。训练加速器还支持混合精度训练，使用专用硬件功能，在低精度下执行计算速度更快，在高精度下累积结果，与通用处理器相比，更加节能。我们将在人工智能训练加速器一节中更详细地讨论混合精度训练。</p><p id="cdba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一方面，推理加速器的目标是减少大量独立数据批次的预测延迟。因此，推理加速器需要是能量有效的，并且具有低能量成本/预测。您可以使用训练加速器进行推理(训练的正向传递本质上是一个推理任务)，但是训练加速器上的能量成本/推理会高得多，因为它在小批量推理请求中没有得到充分利用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/17375496e703d7079e1326ad5e8a2f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BcM6yfx5Xt04Xv0PuiouTw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="56b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个训练加速器就像一辆公共交通巴士，只有当巴士一直爆满，燃料成本/乘客成本较低时，它才是节能的。如果你使用公共汽车每次只运送一个人，那么每位乘客的燃料成本就会更高。另一方面，推理加速器就像一辆跑车，比公共汽车更快，比一个乘客的公共汽车更节能(因为每名乘客的燃料成本更低)。但是，如果你想用一辆跑车运送50个人，效率会很低，速度也会很慢(更不用说违法了)。</p><p id="c007" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下一节中，我们将分别仔细研究训练和推理工作流。对于每一个，我们将看看为其任务提供最佳性能和能效的人工智能加速器和软件功能。</p><h1 id="d487" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">人工智能加速器和有效的推理算法</h1><p id="fb97" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">我们知道，机器学习推理包括将训练好的模型应用于新数据，以获得模型预测结果。在这一节中，我们将讨论在人工智能加速器上运行的高效算法，以使推理更加高效和节能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/9b04195ff96ee4474fc4601243040d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCXmbioRmyHlGJXk9aiKcw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="0977" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">能让机器学习推理更高效的最重要的软硬件绝招就是<strong class="kx ir">量化</strong>。为了充分理解量子化，我们必须理解计算机硬件中的数字表示。数字计算机中的浮点数是连续实数值的离散表示。机器学习算法通常存储和处理基于IEEE 754标准的单精度(FP32)数字。IEEE 754指定了额外的浮点类型，如AI加速器通常支持的半精度(FP16)和双精度(FP64)。现代AI加速器还支持其他非IEEE 754标准表示，如bfloat16(由Google Brain推出，由NVIDIA Ampere GPUs、AWS Inferentia、AWS Trainium、Intel Habana Gaudi、Google TPUs支持)和TF32(在NVIDIA Ampere架构和AWS Trainium加速器中)。推理加速器通常也支持整数精度，如INT8和INT4。</p><h2 id="7fe3" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">推理量化的好处</strong></h2><p id="ceac" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">对于推理任务，模型权重和激活可以被量化，即从FP32(典型的训练精度)转换为较低精度的表示，例如FP16、bfloat16或INT8。较低精度的型号意味着更好的性能和能效。当您从FP32位转到FP16操作时，数据大小减少了2倍，能耗减少了大约<a class="ae np" href="https://arxiv.org/pdf/1506.02626v3.pdf" rel="noopener ugc nofollow" target="_blank">4倍</a>，FP16操作的芯片面积减少了大约4倍。</p><p id="63f0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从仅推理硬件设计的角度来看，构建更小的仅推理加速器是有意义的，这些加速器仅支持较低的精度，因此更节能。从FP32迁移到INT8结果将导致更低的能源利用率，因为数据大小减少了4倍。</p><p id="0791" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种巨大的性能效率提升通常是以模型预测准确性为代价的。移动到较低精度的表示是一种压缩，这意味着量化时会丢失一些数据。与FP16和INT8相比，FP32具有较大的动态范围，如下所示。因此，推理量化的目标是在量化到较低精度时只保留“信号”,并去掉“噪声”,有几种方法可以做到这一点。</p><h2 id="9f6b" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">用NVIDIA GPUs量化</strong></h2><p id="0cc3" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">基于NVIDIA Ampere和NVIDIA Turing架构的新一代NVIDIA GPUs支持一系列仅用于推断的精度类型。虽然FP16 precision type早在2016年就在NVIDIA Pascal架构中首次推出，但最新的NVIDIA Ampere和基于图灵的GPU是真正“硬件-算法协同进化”的例子。我已经在我的博客文章中讨论了GPU、GPU架构及其特性的完整历史:</p><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">为AWS上的深度学习选择合适的GPU</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">如何选择正确的亚马逊EC2 GPU实例进行深度学习训练和推理——从最佳性能到最佳性能</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov kp oh"/></div></div></a></div><p id="83b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一节中，我将重点介绍GPU上量化的硬件和软件支持。</p><p id="bbeb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们以NVIDIA Ampere GPU架构为例。在AWS上，您可以通过启动p4d Amazon EC2实例来访问基于Ampere的NVIDIA A100，通过从G5实例类型启动GPU来访问NVIDIA A10G。这两款基于NVIDIA Ampere的GPU都支持FP64、FP32、FP16、INT8、BF16、TF32精度类型，并且还包括专用于混合精度算法的芯片，NVIDIA称之为张量内核。为了便于推断，我们只关心的精度类型是FP16和INT8，在下一节讨论训练时，我们将再次讨论其他精度类型。</p><p id="0f14" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于大多数深度学习框架在NVIDIA GPUs上以FP32训练模型，因此为了更有效地运行推理，NVIDIA提供了TensorRT，这是一种可以将模型权重和激活量化为FP16或INT8的编译器。为了执行量化，TensorRT将确定一个比例因子，以将FP32张量的动态范围映射到FP16或INT8的动态范围。这对INT8来说尤其具有挑战性，因为它的动态范围远远小于FP32的动态范围。INT8只能表示256个不同的值，而FP32可以表示大约4.2倍的10⁹值！</p><p id="a28f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ML研究和硬件社区提出了两种方法来利用量化而不损失准确性:</p><ol class=""><li id="c013" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq ow ly lz ma bi translated">训练后量化(PTQ):从FP32中的训练模型开始，导出比例因子以映射FP32 -&gt; INT8。TensorRT通过测量每一层的激活分布并找到最小化参考分布和量化分布之间的信息损失(KL散度)的比例因子来做到这一点。</li><li id="70df" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq ow ly lz ma bi translated">量化感知训练(QAT):在训练过程中计算比例因子，允许模型适应并最小化信息损失。</li></ol><p id="c85f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们花点时间来欣赏一下硬件和算法之间的这种动态关系。我们可以看到，硬件已经发展到提供更高效的芯片功能，如降低精度，现在算法必须发展到利用芯片功能！</p><p id="e452" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有关量化如何在GPU上与NVIDIA TensorRT一起工作的代码示例，请阅读我的博客文章:</p><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">深度学习推理的人工智能加速器完全指南——GPU、AWS推理和亚马逊…</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">了解CPU、GPU、AWS推理和亚马逊弹性推理，以及如何选择正确的人工智能加速器…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="ox l os ot ou oq ov kp oh"/></div></div></a></div><h2 id="cc31" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">用AWS推理进行量化</strong></h2><p id="bc8a" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">虽然NVIDIA GPUs最初是为了加速图形而创建的，并正在发展成为强大的人工智能加速器，但AWS Inferentia的创建只有一个目的，即加速机器学习推理。每个AWS推理芯片有4个神经内核，每个神经内核是一个脉动阵列矩阵乘法引擎，具有两级存储器层次和一个非常大的片上缓存。它支持FP16、BF16和INT8数据类型，不支持更高精度的格式，因为您不需要它来进行推断——它毕竟是一个专用处理器。就像英伟达针对GPU的TensorRT编译器，AWS Neuron SDK以及支持量化和优化高效推理的编译器。</p><p id="badc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在撰写本文时，尽管AWS推理硬件支持INT8，但AWS神经元编译器仅支持FP16和bfloat16作为量化目标。在编译过程中，用FP32训练的模型会自动转换成bfloat16。如果您在使用AWS Neuron编译器之前手动将权重从FP32量化到FP16，那么它将保留FP16精度进行推理。</p><p id="f3df" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与GPU相比，AWS推理加速器不可编程，因此在我们之前讨论的硬件频谱中，它属于GPU的右侧，更接近ASICs。如果模型具有AWS推理完全支持的操作，那么对于特定模型和批量大小的组合，它可能比GPU更节能。但是，如果模型有不支持的操作，AWS Neuron编译器会自动将这些操作放到主机CPU上。不幸的是，这会导致CPU和加速器之间的额外数据移动，从而降低其性能和效率。</p><h1 id="58c2" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">训练人工智能加速器和高效算法</h1><p id="7290" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">机器学习训练包括使用训练数据优化模型的参数，以提高模型的预测准确性。在这一节中，我们将讨论在人工智能加速器上运行的高效算法，以使推理更加高效和节能。</p><p id="3e6d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们重新审视一下precision，但这次是为了培训工作流。正如我们之前在训练期间讨论的，权重和激活存储在FP32表示中，FP32表示是先于深度学习的IEEE 754浮点标准。ML的研究人员选择这个作为默认的浮点表示，因为在训练期间，FP16不能容纳足够的信息，而FP64太大了，我们实际上不需要那么大的数量和精度。他们需要的是介于两者之间的东西，但当时硬件中没有这种东西。</p><p id="a0f1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">换句话说，现有的硬件不知道算法需求，或者说不知道我们前面讨论过的“算法感知”。</p><p id="5ed9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果ML研究人员有能力选择任何对机器学习非常有用的浮点表示，那么他们会选择一种看起来不同于FP32的表示，或者使用精度的组合来提高性能和效率。这正是人工智能加速器在混合精度训练中采取的方向，它需要硬件和算法的共同设计才能工作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/9b04195ff96ee4474fc4601243040d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCXmbioRmyHlGJXk9aiKcw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><h2 id="d2c2" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">混合精度训练以提高性能和效率</strong></h2><p id="1a9d" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">矩阵乘法运算是神经网络训练和推理的面包和黄油，人工智能加速器的大部分时间都花在乘以各层输入数据和权重的大型矩阵上。混合精度训练背后的想法是，训练期间的矩阵乘法发生在较低精度的表示(FP16、bfloat16、TF32)中，因此它们更快、更节能，结果在FP32中累积，以最大限度地减少信息损失，使训练更有效、更快。</p><h2 id="e348" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">使用NVIDIA GPUs进行混合精度训练</strong></h2><p id="cf5a" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">2017年，英伟达宣布了Volta架构，该架构在其芯片中采用了专门的机器学习功能，称为英伟达张量核心。张量核心支持使用FP16数学和FP32累加的混合精度训练。新一代NVIDIA GPU架构将支持从FP16扩展到其他精度降低的格式(bfloat16、TF32)。在芯片级，Tensor Core通过FP32累加执行精度降低的融合乘加(FMA)。</p><p id="427b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每一代NVIDIA架构的改进都是硬件算法协同设计和协同进化的一个很好的例子。</p><ul class=""><li id="e7be" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">NVIDIA Volta architecture (2017)推出了第一代张量内核，仅支持FP16数学和FP32混合精度累加</li><li id="ed26" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">NVIDIA Turing (2018)扩展了张量核以支持INT8和INT4简化精度(这主要有利于推理，而不是训练)</li><li id="a224" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">NVIDIA Ampere (2020)扩展了张量内核以支持bfloat16、TF32，这意味着它可以执行FP16、bfloat16或TF32数学运算，并在FP32中累加以实现混合精度</li></ul><p id="60e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">混合精度训练的一个挑战是，从软件的角度来看，它不“只是工作”。用户必须在训练过程中执行额外的步骤，例如在可能的情况下将权重转换为FP16数据类型，同时保留FP32权重和损失比例的副本。虽然NVIDIA支持深度学习框架在最少代码更改的情况下完成这些步骤，但它对最终用户的概念要求很高，不像FP32中的培训那样简单。</p><p id="5b1c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过NVIDIA Ampere，硬件已经发展到支持新的TF32格式，显著解决了这一用户体验缺陷。TF32令人兴奋的是，它具有FP32的范围和FP16的精度，这意味着深度学习框架可以支持它开箱即用，而不需要执行像转换和簿记这样的额外步骤。虽然TF32在没有开发人员开销的情况下提供了比FP32更好的性能，但NVIDIA仍然建议使用基于FP16或bfloat16的混合精度训练来获得最快的训练性能。</p><h2 id="156f" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">用其他AI加速器进行混合精度训练</strong></h2><h2 id="80d1" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak">英特尔哈瓦那高迪</strong></h2><p id="4ce0" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">Habana Gaudi加速器<a class="ae np" href="https://docs.habana.ai/en/latest/PyTorch/PyTorch_User_Guide/PyTorch_Gaudi_Integration_Architecture.html?highlight=mixed%20precision#mixed-precision-support" rel="noopener ugc nofollow" target="_blank">支持混合精度</a>的方式与NVIDIA支持它的方式类似——借助一个附加工具，你可以与深度学习框架一起使用，该框架可以为你执行造型和记账。在AWS上，您可以通过启动亚马逊EC2 DL1实例来访问英特尔Habana Gaudi AI加速器，这将为您提供8个Gaudi加速器。</p><h2 id="9cff" class="nd mh iq bd mi ne nf dn mm ng nh dp mq le ni nj ms li nk nl mu lm nm nn mw no bi translated"><strong class="ak"> AWS Trainium </strong></h2><p id="a3f9" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">AWS在re:invent 2021上宣布了AWS Trainium，这是一个由AWS的Annapurna实验室建造的人工智能加速器。在撰写本文时，AWS Trainium尚未全面上市。在这次发布会上，AWS分享了AWS Trainium在发布时将支持FP16、TF32、bfloat16、INT8以及一种令人兴奋的新格式cFP8，它代表可以定制的8位浮点。</p><h1 id="59e6" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">人工智能加速器的未来</h1><p id="6088" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">对于机器学习算法研究和机器学习硬件设计来说，我们都生活在一个激动人心的时代。人工智能加速器将继续发展，以提供更高的性能，更好的能效，并变得像通用处理器一样无缝使用。</p><p id="57f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现代人工智能加速器已经包括预期的硬件功能，例如支持INT1和INT4精度类型，这种类型目前不用于训练或推理任务，但有一天可能会实现新的机器学习算法。人工智能加速器的网络是另一个正在经历变革的重要领域。随着模型规模的不断增长，我们将需要更大的计算集群，其中许多人工智能加速器相互连接，以支持更大的工作负载。英伟达通过NVLink和NVSwitch提供高带宽GPU间互连，英特尔Habana Gaudi集成了片上和基于以太网的RoCE RDMA。随着越来越多的应用程序集成基于人工智能的解决方案，人工智能加速器将继续存在，并将成为现代计算环境中的一个主要部分。</p><p id="1ff0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我期望看到更多进步的领域是用户/开发者体验。今天的异构计算模型涉及到使用多个CPU、人工智能加速器及其网络和存储设置，这对大多数数据科学家和开发人员来说都是非常具有挑战性的。虽然像Amazon SageMaker这样的云中托管服务消除了管理基础设施的所有需求，使扩展机器学习变得容易，但开源框架仍然希望用户了解底层硬件、精度类型、编译器选项和网络原语等。</p><p id="3de7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在未来，开发人员将登录远程IDE并使用开源机器学习框架运行代码，而不会再考虑它在哪里以及如何运行。他们只会指定成本与速度的权衡——更快的结果花费更多，更慢的结果节省成本——这将是他们唯一的认知负担。我是一个乐观的人，我认为我们很接近了。</p><h1 id="f7e1" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">你好。谢谢朋友，看完！</h1><p id="d16e" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如果你觉得这篇文章很有趣，可以考虑给它一点掌声，然后在medium上跟我学。也请查看我在<a class="ae np" href="https://medium.com/@shashankprasanna" rel="noopener"> medium </a>上的其他博客帖子，或者在twitter ( <a class="ae np" href="https://twitter.com/shshnkp" rel="noopener ugc nofollow" target="_blank"> @shshnkp </a>)、<a class="ae np" href="https://www.linkedin.com/in/shashankprasanna/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，或者在下面留下评论。想让我写一个特定的机器学习主题吗？我很想收到你的来信！</p></div></div>    
</body>
</html>