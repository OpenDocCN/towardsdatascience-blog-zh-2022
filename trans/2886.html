<html>
<head>
<title>Three Performance Evaluation Metrics of Clustering When Ground Truth Labels Are Not Available</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">地面实况标签不可用时聚类的三个性能评估指标</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/three-performance-evaluation-metrics-of-clustering-when-ground-truth-labels-are-not-available-ee08cb3ff4fb#2022-06-23">https://towardsdatascience.com/three-performance-evaluation-metrics-of-clustering-when-ground-truth-labels-are-not-available-ee08cb3ff4fb#2022-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="da3f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">无监督学习</h2><div class=""/><div class=""><h2 id="da1b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如果基础事实标签不可用，应该使用哪个度量来评估聚类结果？在这篇文章中，我将介绍其中的三种。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/cf8ef9d4902770bf0ebad9594e080f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zCZ8k6ts7VZnwz_f"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">西蒙·穆格在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="04ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型评估始终是机器学习管道中的一个重要步骤，因为它告诉我们模型在描述数据方面有多好。</p><p id="3a8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当谈到模型评估时，我们更多地是指监督学习模型，其中数据的真实标签是可用的。监督学习中的性能度量可以基于来自模型的真实标签和预测标签之间的差异/一致来开发。</p><p id="63bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然不太直接，但对无监督学习模型的性能评估也很重要。在这篇文章中，我将谈论如何评估聚类模型的性能，这是无监督学习中的一项主要任务，如果地面真相标签不可用的话。</p><p id="878b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我将要谈到的每种方法都需要完成聚类步骤，这意味着每个数据点都有一个聚类标签以及用于进行聚类的那些特征。</p><h2 id="a323" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">聚类评估的主要思想</h2><p id="a1cd" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">聚类评估的思路很简单。它比较<strong class="lk jd">类内</strong>(自身类)距离和<strong class="lk jd">类间</strong>(相邻类)距离，以决定类的分离程度。</p><p id="bcec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个好的聚类应该具有较小的类内距离和较大的类间距离，如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/4403320e1eb1141614e04db43a4a6d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*0cYUfWLFW6SEyLSmpVh3DQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">良好聚类结果中的类间距离和类内距离的示例。用三种颜色突出显示的分类标签。(图片由作者提供)</p></figure><p id="1d5d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如果聚类不好，则簇间距离和簇内距离没有那么明显，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/af1b6adde2546bd16cf3d8be2d2cdd67.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*qA_lPZ24pwh8YiQRaQWy1w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">不良聚类结果中的类间距离和类内距离的示例。用三种颜色突出显示的分类标签。(图片由作者提供)</p></figure><p id="4c45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要注意的是，当我们谈论聚类是好是坏时，聚类实际上是指数据点和聚类标签之间的分配。对于同一个数据集，用一种方法聚类可能很好，但用另一种方法聚类可能很差。上述所有距离定义都必须基于已经聚类(标记)的数据集。</p><p id="b31f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好的，在理解了聚类评估的主要思想之后，你会发现下面三个指标非常简单。</p><h2 id="69bf" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">轮廓系数</h2><p id="b8b5" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">作为最常用的聚类评估指标之一，轮廓系数将类内/类间距离比较总结为-1到1之间的分数。</p><p id="60a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接近1的值指示超好的聚类结果，其中聚类间距离远大于聚类内距离；而接近-1的值意味着完全错误的聚类分配，其中聚类间距离甚至不能与聚类内距离相比。</p><p id="7935" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来我们来看看-1比1之间的分数是如何构造的。</p><p id="5c03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们需要知道如何计算特定点的平均类内/类间距离。</p><p id="08c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">至于<strong class="lk jd">类内</strong>距离，对于类C内的任何数据点I，<strong class="lk jd"> a </strong>被定义为I和C内所有其他数据点之间的平均距离</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/101c417b26cdc2a73504b14e73e2dc9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*mapFkMc7uIS0TelD2fM7sw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">组内距离定义。(图片由作者提供)</p></figure><p id="5d7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中| <strong class="lk jd"> C_I </strong> |是属于簇<strong class="lk jd"> i </strong>的点数，<strong class="lk jd"> d(i，j) </strong>是簇<strong class="lk jd"> C_I </strong>中数据点<strong class="lk jd"> i </strong>和<strong class="lk jd"> j </strong>之间的距离。</p><p id="e117" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，对于任何给定点I，小的<strong class="lk jd">分数</strong>a(I)表示点I的<strong class="lk jd">好的</strong>聚类分配，因为它接近相同聚类内的点。相反，<strong class="lk jd">大的</strong>分数<strong class="lk jd"> a(i) </strong>表示点I的<strong class="lk jd">差的</strong>聚类，因为它远离它自己的聚类中的点。</p><p id="7ecc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于<strong class="lk jd">类间</strong>距离，对于类C内的任何数据点I，<strong class="lk jd"> b </strong>被定义为I到任何其他类中所有点的<em class="ne">最小</em>平均距离，I不是该类的成员。换句话说，<strong class="lk jd"> b </strong>是<strong class="lk jd"> i </strong>到其最近邻簇所有点之间的平均距离。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/097641561af1d05cc3dd4a1963bfc9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*MXgplz3B9B8lxZfZ6UicNw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">聚类间距离定义。(图片由作者提供)</p></figure><p id="1569" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在获得数据集中每个点的聚类内和聚类间平均距离之后，轮廓分数被如此定义，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/21d8575ba2968e99d7c859e9b7edfc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*LqIe6XaNCV-83STPu9gcQA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">轮廓分数定义。(图片由作者提供)</p></figure><p id="aed6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于C_I = 1的罕见情况(聚类C中只有一个数据点I)，轮廓得分被定义为0。</p><p id="e180" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以从上面的公式中看到，分数完全由-1和1限定，并且较大的分数表示较好的聚类分离。剪影乐谱的一个最重要的优点是它易于理解和界定。</p><p id="03c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">轮廓分数的最大缺点是计算量大。在相对较大的数据集上超长的运行时间使得它在现实应用中用处不大。</p><p id="10ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了用简单的解释换取更快的计算，人们通常求助于以下两个指标，卡林斯基-哈拉巴斯指数和T2指数。</p><h2 id="2f81" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">卡林斯基-哈拉巴斯指数</h2><p id="f461" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Calinski-Harabasz指数(也称为方差比标准)被定义为所有聚类的类间距离平方和与类内距离平方和的比值。距离的平方和由自由度修正。</p><p id="734e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，基于从聚类中的数据点到其自己的聚类质心的距离来估计聚类内，并且基于聚类质心到全局质心的距离来估计聚类间。</p><p id="8469" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据集D上K个聚类的卡林斯基-哈拉巴斯指数(CH)被定义为，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nh"><img src="../Images/357f99475964d01548b97311b4775c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gxXRXvJR3jrqxQh92j56Gw.png"/></div></div></figure><p id="472d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中，<em class="ne"> d_i </em>是数据点I的特征向量，<em class="ne"> n_k </em>是第<em class="ne">k</em>簇的大小，<em class="ne"> c_k </em>是第<em class="ne">k</em>簇的质心的特征向量，<em class="ne"> c </em>是整个数据集的全局质心的特征向量，<em class="ne"> N </em>是数据点的总数。</p><p id="1699" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，分子是从单个聚类质心到全局质心的距离平方的加权和(按聚类大小<em class="ne"> n_k </em>)除以自由度。分母是从每个单独的数据点到它自己的聚类质心的距离的平方和除以自由度。自由度用于将两个部分调整到相同的比例。</p><p id="1f18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以从上面的等式中看到，<strong class="lk jd"> CH </strong>越高，聚类彼此分离得越好，并且CH没有像剪影分数那样的上限。</p><p id="170f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们考虑一个基于CH指数的理想聚类结果。可能有几个“球状”集群，其中集群质心彼此远离，而集群成员靠近各自的质心(如下所示)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/c0d63c1dbab0673dd3c15f9509a0b509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcdM7E98pZ3yvrawriTYCg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">获得Calinski-Harabasz指数奖励的良好聚类结果示例(图片由作者提供)</p></figure><p id="87dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，如果聚类不具有这样的形状，基于质心的距离将不能提供足够的信息来判断聚类算法的质量。因此，CH指数是<strong class="lk jd">而不是</strong>推荐用于基于密度的方法，如<a class="ae lh" rel="noopener" target="_blank" href="/understanding-mean-shift-clustering-and-implementation-with-python-6d5809a2ac40">均值漂移</a>聚类、<a class="ae lh" rel="noopener" target="_blank" href="/understanding-dbscan-and-implementation-with-python-5de75a786f9f"> DBSCAN </a>、<a class="ae lh" rel="noopener" target="_blank" href="/understanding-optics-and-implementation-with-python-143572abdfb6">光学</a>等..</p><h2 id="cd34" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">戴维斯-波尔丁指数</h2><p id="ef54" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Davies-Bouldin指数类似于CH指数，但是组间/组内距离比的计算与CH指数相反。在Davies-Bouldin指数的计算中，有一个概念，即相似性得分，它衡量两个集群彼此相似的程度，其定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5f00e41c8afbf2c2e93e063c1b41c37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*nMfGG_BJj78aMltIEdWYmg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">戴维斯-波尔丁指数中的相似性计算。(图片由作者提供)</p></figure><p id="2c75" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中R_ij是相似性得分，S_i和S_j分别是从点到聚类I和j内的质心的平均距离；并且M_ij是簇I和簇j的质心之间的距离</p><p id="af59" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从等式中我们可以看出，较小的相似性分数表示较好的聚类分离，因为小分子意味着聚类内距离小，而大分母意味着聚类间距离大。</p><p id="5eaf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Davies-Bouldin指数被定义为所有聚类与其最近邻聚类的平均相似性得分，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/9b6b24eb7670b80a10f0d4c5593487bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*xOeIqLdJuttyl3xjuEM3pg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/28c0bb321b62adadb5470363ea17d361.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*rmQnwjKB64XkKP9kEsNpyg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">戴维斯-波尔丁指数计算(图片由作者提供)</p></figure><p id="0380" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="ne"> D_i </em>是第<em class="ne">个</em>聚类在所有其他聚类中的最差(最大)相似性得分，最终DB索引是N个聚类的平均<em class="ne">D _ I</em>a。</p><p id="b6d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到，DB指数越小，聚类分离越好。它具有与ch指数相似的缺点，CH指数在处理没有特定形状假设的聚类方法(如基于密度的聚类)时表现不佳。但是CH和DB索引都比轮廓分数计算快得多。</p><h2 id="6388" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">用Python实现</h2><p id="3bab" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">多亏了scikit-learn包，这三个指标在Python中非常容易计算。</p><p id="f196" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们使用kmeans作为示例聚类算法。以下是计算剪影得分、Calinski-Harabasz指数和Davies-Bouldin指数的示例代码。</p><pre class="ks kt ku kv gt nm nn no np aw nq bi"><span id="8432" class="me mf it nn b gy nr ns l nt nu"><strong class="nn jd">from</strong> <!-- -->sklearn <strong class="nn jd">import</strong> <!-- -->datasets<br/><strong class="nn jd">from</strong> <!-- -->sklearn.cluster <strong class="nn jd">import</strong> <!-- -->KMeans<br/><strong class="nn jd">from</strong> <!-- -->sklearn <strong class="nn jd">import</strong> <!-- -->metrics</span><span id="4c3c" class="me mf it nn b gy nv ns l nt nu">X, y <strong class="nn jd">=</strong> <!-- -->datasets.load_iris(return_X_y<strong class="nn jd">=</strong>True)<br/>kmeans <strong class="nn jd">=</strong> <!-- -->KMeans(n_clusters<strong class="nn jd">=</strong>3, random_state<strong class="nn jd">=</strong>1).fit(X)<br/>labels <strong class="nn jd">=</strong> <!-- -->kmeans.labels_</span><span id="4cfc" class="me mf it nn b gy nv ns l nt nu">Sil = metrics<!-- -->.silhouette_score(X, labels)<br/>CH = metrics.calinski_harabasz_score(X, labels)<br/>DB = metrics<!-- -->.davies_bouldin_score(X, labels)</span></pre><p id="080d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！希望文章对你有帮助。</p><p id="5824" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你喜欢阅读这些文章，请<a class="ae lh" href="https://jianan-lin.medium.com/subscribe" rel="noopener">订阅我的账号</a>！</p><h2 id="3f55" class="me mf it bd mg mh mi dn mj mk ml dp mm lr mn mo mp lv mq mr ms lz mt mu mv iz bi translated">参考资料:</h2><div class="nw nx gp gr ny nz"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">sk learn . metrics . silhouette _ score</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">计算所有样本的平均轮廓系数。轮廓系数是使用平均值计算的…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">scikit-learn.org</p></div></div><div class="oi l"><div class="oj l ok ol om oi on lb nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">sk learn . metrics . Davies _ bouldin _ score</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">计算戴维斯-波尔丁分数。分数被定义为每个聚类的平均相似性度量</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">scikit-learn.org</p></div></div><div class="oi l"><div class="oo l ok ol om oi on lb nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">sk learn . metrics . calinski _ harabasz _ score</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">编辑描述</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">scikit-learn.org</p></div></div><div class="oi l"><div class="op l ok ol om oi on lb nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://en.wikipedia.org/wiki/Silhouette_%28clustering%29" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">剪影(聚类)-维基百科</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">剪影指的是一种解释和验证数据簇一致性的方法。技术…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd jd gy z fp oe fr fs of fu fw jc bi translated">戴维斯-波尔丁索引-维基百科</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">戴维·l·戴维斯和唐纳德·w·波尔丁于1979年提出的戴维斯-波尔丁指数(DBI)是一种评估…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/393251d224ddbcfc451923d315ada9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3P58loG4tXY5L16r"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Artem Kniaz 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>