<html>
<head>
<title>Double Debiased Machine Learning (part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双重去偏机器学习(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/double-debiased-machine-learning-part-1-of-2-eb767a59975b#2022-05-31">https://towardsdatascience.com/double-debiased-machine-learning-part-1-of-2-eb767a59975b#2022-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="15e3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/causal-data-science" rel="noopener" target="_blank">因果数据科学</a></h2><div class=""/><div class=""><h2 id="7bf9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">因果推理、机器学习和正则化偏差</em></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/2685f853f5e5e75097679a2145144b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNKxOJK6VduYUxAqmYCT5A.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="560a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在因果推断中，我们通常通过对其他变量的分析来估计因果影响。我们通常将这些变量称为<strong class="lh ja">控制变量</strong>或<strong class="lh ja">混杂变量</strong>。在随机对照试验或AB测试中，条件反射可以通过减少随机化后出现的组间不平衡来增加分析的能力。然而，条件作用在观察性研究中更为重要，在观察性研究中，如果没有随机化，可能<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">对恢复因果关系</a>至关重要。</p><p id="b1f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我们有许多控制变量时，我们可能希望<strong class="lh ja">选择最相关的变量</strong>，可能捕捉非线性和相互作用。机器学习算法非常适合这项任务。然而，在这些情况下，我们引入了一个偏差，称为<strong class="lh ja">正则化或预测试，或特征选择偏差</strong>。在这篇和下一篇博文中，我试图解释偏见的来源和一个非常强大的解决方案，称为<strong class="lh ja">双去偏机器学习</strong>，这可能是过去十年中机器学习和因果推理交叉点上最相关的进步之一。</p><h1 id="b9d1" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">预测试</h1><p id="c0e8" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">既然这是一个复杂的话题，那就从一个简单的例子开始吧。</p><p id="9639" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设我们是一家公司，我们对广告支出对收入的影响感兴趣:广告值得吗？还有很多其他因素可能会影响销售，因此，我们正在考虑在分析中控制过去的销售，以提高我们的分析能力。</p><p id="146b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设数据生成过程可以用下面的<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c"> <strong class="lh ja">有向无环图(DAG) </strong> </a>来表示。如果你对DAGs不熟悉，我在这里写了一个简短的<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">介绍</a>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/9a76a036f877d7c734d22e44d1f57644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T39foqcyQXVeVnPTiSezZw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据生成过程的DAG，按作者排序的图像</p></figure><p id="b167" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我从<code class="fe na nb nc nd b"><a class="ae mb" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py" rel="noopener ugc nofollow" target="_blank">src.dgp</a></code>导入数据生成过程<code class="fe na nb nc nd b">dgp_pretest()</code>，从<code class="fe na nb nc nd b"><a class="ae mb" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py" rel="noopener ugc nofollow" target="_blank">src.utils</a></code>导入一些绘图函数和库。</p><pre class="kq kr ks kt gt ne nd nf ng aw nh bi"><span id="b8df" class="ni md iq nd b gy nj nk l nl nm">from src.utils import *<br/>from src.dgp import dgp_pretest<br/><br/>df = dgp_pretest().generate_data()<br/>df.head()</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nn"><img src="../Images/e8bf892bf177a7c87228a973953e7259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukYfn7ymwgSZ3Ztc8fQJYw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="a2f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们有关于<em class="no"> 1000个</em>不同市场的数据，其中我们观察当前<code class="fe na nb nc nd b">sales</code>，在<code class="fe na nb nc nd b">advertisement</code>和<code class="fe na nb nc nd b">past sales</code>花费的金额。</p><p id="1a00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们想了解<code class="fe na nb nc nd b">ads</code>支出是否能有效增加<code class="fe na nb nc nd b">sales</code>。一种可能性是使用以下回归模型对前者进行回归，该回归模型也称为<strong class="lh ja">短模型</strong>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi np"><img src="../Images/e39e414c81ddf18c1772db48d835dee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_aMSbPUw9dfWmZsyGoQvQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">短回归，作者图片</p></figure><p id="abc6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们是不是也应该把<code class="fe na nb nc nd b">past sales</code> it纳入回归？那么回归模型将如下，也称为<strong class="lh ja">长模型</strong>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nq"><img src="../Images/ec386513c055ec959d822f03972187aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8EDW5NEY8AoLzqjdLhnTA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">长回归，作者图片</p></figure><p id="e38d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由于我们不确定是否以<code class="fe na nb nc nd b">past sales</code>为条件进行分析，我们可以<strong class="lh ja">让数据决定</strong>:我们可以运行第二次回归，如果<code class="fe na nb nc nd b">past sales</code>、<em class="no"> β </em>的影响在统计上是显著的，我们就可以使用长模型，否则，我们运行短模型。</p><pre class="kq kr ks kt gt ne nd nf ng aw nh bi"><span id="f35f" class="ni md iq nd b gy nj nk l nl nm">smf.ols('sales ~ ads + past_sales', df).fit().summary().tables[1]</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nr"><img src="../Images/fb4f7b3efdce0c6d98806048f6276738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iqdzCcIB6xbBWE1w9VyzyA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">回归摘要，按作者分类的图像</p></figure><p id="c13c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">看来<code class="fe na nb nc nd b">past sales</code>对电流<code class="fe na nb nc nd b">sales</code>的影响是积极而显著的。因此，我们对我们的规范感到满意，我们的结论是<code class="fe na nb nc nd b">ads</code>对<code class="fe na nb nc nd b">sales</code>的影响是积极且显著的，95%的置信区间为【0.912，1.029】。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="a3b8" class="mc md iq bd me mf nz mh mi mj oa ml mm kf ob kg mo ki oc kj mq kl od km ms mt bi translated">偏见</h1><p id="ea7f" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">这个过程有一个<strong class="lh ja">问题</strong>:我们没有考虑到这样一个事实，即我们已经运行了一个测试来决定是否将<code class="fe na nb nc nd b">past_sales</code>包括在回归中。我们决定包括<code class="fe na nb nc nd b">past_sales</code>的事实是因为它的系数<em class="no">是重要的</em>对关于<code class="fe na nb nc nd b">ads</code>对<code class="fe na nb nc nd b">sales</code>、<em class="no"> α </em>的影响的推断有影响。</p><p id="c58b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">理解问题的最好方法是通过<strong class="lh ja">模拟</strong>。由于我们可以访问数据生成过程<code class="fe na nb nc nd b">dgp_pretest()</code>(与现实生活中不同)，我们可以测试如果我们多次重复这个过程会发生什么:</p><ol class=""><li id="1560" class="oe of iq lh b li lj ll lm lo og ls oh lw oi ma oj ok ol om bi translated">我们从数据生成过程中抽取一个新样本。</li><li id="2485" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated">我们在<code class="fe na nb nc nd b">ads</code>和<code class="fe na nb nc nd b">past_sales</code>上回归<code class="fe na nb nc nd b">sales</code>。</li><li id="0a13" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated">如果<code class="fe na nb nc nd b">past_sales</code>的系数在95%的水平上显著，我们从(2)中保留<em class="no"> α̂-long </em>。</li><li id="9e68" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated">否则，我们只对<code class="fe na nb nc nd b">ads</code>回归<code class="fe na nb nc nd b">sales</code>，并保持系数<em class="no"> α̂-short.</em></li></ol><p id="6e47" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我写了一个<code class="fe na nb nc nd b">pre_test</code>函数来实现上面的程序。我还保存了两个回归的系数，long和short，选择的系数称为<strong class="lh ja">预测试系数</strong>。</p><p id="5eea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">提醒</strong>:我们正在预测<code class="fe na nb nc nd b">past_sales</code>对<code class="fe na nb nc nd b">sales</code>的影响，但是<code class="fe na nb nc nd b">ads</code>对<code class="fe na nb nc nd b">sales</code>的影响系数。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="25fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在可以绘制估计系数的分布图(通过模拟)。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="os ot l"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/a57e72de117849578988048d339b1d3a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*cKF7AT3ZfOh5pThvCeSGXQ.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">α̂在模拟中的分布，图片由作者提供</p></figure><p id="1cdd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的图中，我已经描述了不同回归规格的模拟的估计系数。</p><p id="597b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我们从第一张图中看到的，如果我们总是运行<strong class="lh ja">长回归</strong>，我们的估计量<em class="no"> α̂-long </em>将是无偏的，并且是正态分布的。然而，如果我们总是运行<strong class="lh ja">短期回归</strong>(第二个图)，我们的估计量<em class="no"> α̂-short </em>会有<strong class="lh ja">偏差</strong>。</p><p id="e4f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">预测试</strong>程序生成一个估计量<em class="no"> α̂-pretest </em>，它是两者的混合:大多数时候我们选择正确的规格，长回归，但有时预测试无法拒绝<code class="fe na nb nc nd b">past sales</code>对<code class="fe na nb nc nd b">sales</code>、<em class="no"> H₀:β=0 </em>没有影响的零假设，我们选择不正确的规格，运行短回归。</p><p id="a73e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">重要的是，预测试程序<strong class="lh ja">不会产生有偏估计值</strong>。正如我们在最后一幅图中看到的，估计系数非常接近真实值1。原因是，在大多数情况下，我们选择短<em class="no">回归的次数足够小，不会引入偏差，但也没有小到可以进行有效的推断。</em></p><p id="7b65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">的确，<strong class="lh ja">预测试扭曲了推论</strong>:估计量<em class="no"> α̂-pretest </em>的分布不再是正态分布，而是双峰分布。<strong class="lh ja">的后果</strong>是我们对于<em class="no"> α </em>的置信区间将会有错误的覆盖范围(包含真实的效应，其概率与声称的不同)。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="8328" class="mc md iq bd me mf nz mh mi mj oa ml mm kf ob kg mo ki oc kj mq kl od km ms mt bi translated">什么时候预测试是个问题？</h1><p id="0428" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">预测试的问题出现是因为运行短回归产生的偏差:<a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699"> <strong class="lh ja">省略变量偏差(OVB) </strong> </a>。如果你不熟悉OVB，我在这里写了一篇<a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699">简短介绍</a>。然而，一般来说，我们可以通过忽略<em class="no"> X </em>对<em class="no"> D </em>回归<em class="no"> Y </em>引入的省略变量偏差表示为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ov"><img src="../Images/c77588842a5f2229ed01687236a59c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHQGRO_dyPjWnjb2boa1Lw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="4a9a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<em class="no"> β </em>是<em class="no"> X </em>(我们例子中的<code class="fe na nb nc nd b">past sales</code>)对<em class="no"> Y </em> ( <code class="fe na nb nc nd b">sales</code>)的影响，<em class="no"> δ </em>是<em class="no"> D </em> ( <code class="fe na nb nc nd b">ads</code>)对<em class="no"> X </em>的影响。</p><p id="50a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">预测试是一个<strong class="lh ja">问题</strong>如果</p><ol class=""><li id="580c" class="oe of iq lh b li lj ll lm lo og ls oh lw oi ma oj ok ol om bi translated">我们运行短回归而不是长回归<em class="no">和</em></li><li id="d536" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated">偏见的影响是明显的</li></ol><p id="7387" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有什么可以帮助提高(1)，即正确拒绝<code class="fe na nb nc nd b">past sales</code>、<em class="no"> H₀:β=0 </em>零效应零假设的概率？答案很简单:更大的样本量。如果我们有更多的观察，我们可以更精确地估计<em class="no"> β </em>，并且我们不太可能犯<a class="ae mb" href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" rel="noopener ugc nofollow" target="_blank">类型2错误</a>并运行短回归而不是长回归。</p><p id="e446" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们模拟不同样本量下的估计系数<em class="no"> α̂ </em>。记住，直到现在使用的样本大小是<em class="no"> N=1000 </em>。</p><pre class="kq kr ks kt gt ne nd nf ng aw nh bi"><span id="8f50" class="ni md iq nd b gy nj nk l nl nm">Ns = [100,300,1000,3000]<br/>alphas = {f'N = {n:.0f}':  pre_testing(N=n)['Pre-test'] for n in Ns}<br/>plot_alphas(alphas, true_alpha=1)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/b58f8ff8e4f629f75c8de837ba024159.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6eoh-LsNaPd8FC7X5btJgA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">α̂在模拟中的分布，图片由作者提供</p></figure><p id="bdf8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从图中我们可以看出，随着样本量的增加(从左到右)，偏差减小，估计量<em class="no"> α̂-pretest </em>的分布收敛于正态分布。</p><p id="5bf3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果<em class="no"> β </em>的值不同，会发生什么？这可能会影响上一段中的第(2)点，但如何影响呢？</p><ul class=""><li id="9888" class="oe of iq lh b li lj ll lm lo og ls oh lw oi ma ow ok ol om bi translated">如果<em class="no"> β </em>是<strong class="lh ja">非常小的</strong>，将很难检测到它，我们将经常以运行<em class="no">短</em>回归而告终，从而引入偏差。然而，如果<em class="no"> β </em>非常小，这也意味着偏差的<strong class="lh ja">幅度很小，因此不会对我们对<em class="no"> α </em>的估计产生太大影响</strong></li><li id="664a" class="oe of iq lh b li on ll oo lo op ls oq lw or ma ow ok ol om bi translated">如果<em class="no"> β </em>非常大<strong class="lh ja"/>，这将很容易被检测到，我们将经常结束运行<em class="no">长</em>回归，避免偏差(尽管偏差会非常大)。</li></ul><p id="288e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们模拟不同的<em class="no"> β </em>值下的估计系数<em class="no"> α̂ </em>。直到现在使用的真实值是<em class="no"> β=0.3 </em>。</p><pre class="kq kr ks kt gt ne nd nf ng aw nh bi"><span id="36fb" class="ni md iq nd b gy nj nk l nl nm">betas = 0.3 * np.array([0.1,0.3,1,3])<br/>alphas = {f'beta = {b:.2f}':  pre_testing(b=b)['Pre-test'] for b in betas}<br/>plot_alphas(alphas, true_alpha=1)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/bf8a93f04a12c76b54c2b877e13f18e3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*1wIlvnp9HqbqKEu6J7hMtg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">α̂在模拟中的分布，图片由作者提供</p></figure><p id="63db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从图中我们可以看出，随着<em class="no"> β </em>值的增加，偏置先出现，然后消失。当<em class="no"> β </em>较小时(左图)，我们往往选择短回归，但偏差较小，平均估计值非常接近真实值。对于<em class="no"> β </em>的中间值，偏差是明显的，对推断有明显的影响。最后，对于大值的<em class="no"> β </em>(右图)，我们总是运行长回归，偏差消失。</p><p id="0895" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是<strong class="lh ja">一个系数什么时候大或者小</strong>？相对于什么来说是大还是小？答案很简单:相对于<strong class="lh ja">样本量</strong>，或者更准确地说，相对于样本量平方根的倒数<em class="no"> 1/√n </em>。原因深植于<a class="ae mb" href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="noopener ugc nofollow" target="_blank">中心极限定理</a>，这里就不赘述了。</p><p id="9ca7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个想法展示起来容易解释起来难，所以让我们重复上面同样的模拟，但是现在我们将同时增加系数和样本量。</p><pre class="kq kr ks kt gt ne nd nf ng aw nh bi"><span id="77b8" class="ni md iq nd b gy nj nk l nl nm">betas = 0.3 * 30 / np.sqrt(Ns)<br/>alphas = {f'N = {n:.0f}':  pre_testing(b=b, N=n)['Pre-test'] for n,b in zip(Ns,betas)}<br/>plot_alphas(alphas, true_alpha=1)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/3bab87c671ea902b954fed73fce0bec3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Ys3NAHCbTfYVs89L0aoHVg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">α̂在模拟中的分布，图片由作者提供</p></figure><p id="4fdc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我们所见，既然<em class="no"> β </em>与<em class="no"> 1/√n </em>成正比，那么无论样本大小如何，失真都不会消失。所以推论永远是错的。</p><p id="669e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然取决于样本大小的系数可能听起来<strong class="lh ja">不直观</strong>，但它很好地抓住了<strong class="lh ja">量级</strong>的概念，在这个世界上，我们依靠渐近结果进行推断，首先是<a class="ae mb" href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="noopener ugc nofollow" target="_blank">中心极限定理</a>。事实上，中心极限定理依赖于无限大的样本量。然而，对于无限量的数据，没有一个系数是小的，并且任何非零效应都可以确定地检测到。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="1898" class="mc md iq bd me mf nz mh mi mj oa ml mm kf ob kg mo ki oc kj mq kl od km ms mt bi translated">预测试和机器学习</h1><p id="5139" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">到目前为止，我们讨论了只有两个变量的线性回归。答应我们的<strong class="lh ja">机器学习</strong>在哪里？</p><p id="2bfe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，我们不是只有一个控制变量(或混杂因素)，而是很多。此外，对于这些控制变量进入模型所采用的函数形式，我们可能要灵活一些。一般来说，我们将假设以下模型:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ox"><img src="../Images/c406f82243ae181f30149fac3473e3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BjJyDIsA0aoiCcKyqQvDSA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="5edd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在利益效应仍然是<em class="no"> α </em>的情况下，<em class="no"> X </em>可能是高维的，我们不会对<em class="no"> X </em>影响<em class="no"> D </em>或<em class="no"> Y </em>的函数形式采取立场。</p><p id="d1c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这种设置中，很自然地使用机器学习算法来估计<em class="no"> g₀ </em>和<em class="no"> m₀ </em>。然而，机器学习算法通常会引入与预测试相当的<strong class="lh ja">正则化偏差</strong>。</p><p id="1bb0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可能，“最简单”的思考方式是<a class="ae mb" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">套索</a>。Lasso在<em class="no"> X </em>中是线性的，带有一个惩罚项，有效地执行我们上面讨论的变量选择。因此，如果我们在<em class="no"> Y </em>上使用<em class="no"> X </em>和<em class="no"> D </em>的套索，我们将引入正则化偏差，推论将被扭曲。更复杂的算法也是如此。</p><p id="1e9c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，您可能仍然想知道“为什么模型在治疗变量<em class="no"> D </em>中是线性的？”。在线性模型中进行推断要容易得多，不仅是因为计算原因，也是因为解释原因。此外，如果处理<em class="no"> D </em>是二元的，则线性函数形式不失一般性。一个更强的假设是<em class="no"> D </em>和<em class="no"> g(X) </em>的加性可分性。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="7363" class="mc md iq bd me mf nz mh mi mj oa ml mm kf ob kg mo ki oc kj mq kl od km ms mt bi translated">结论</h1><p id="2e80" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在这篇文章中，我试图解释正则化偏差是如何出现的，以及为什么它会成为因果推理中的一个问题。这个问题本质上与具有许多控制变量的设置有关，或者当控制混杂因素时，我们希望有一个无模型(即非参数)的设置。这些正是机器学习算法可以发挥作用的设置。</p><p id="5685" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章的第二部分中，我介绍了一个简单却非常强大的解决方案:双去偏机器学习。</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/double-debiased-machine-learning-part-2-bf990720a0b2"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ja gy z fp pg fr fs ph fu fw iz bi translated">双重去偏机器学习(下)</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">如何使用后双重选择消除正则化偏差</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp kz pb"/></div></div></a></div><h2 id="0dc4" class="ni md iq bd me pq pr dn mi ps pt dp mm lo pu pv mo ls pw px mq lw py pz ms iw bi translated">参考</h2><p id="df0d" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">[1] A. Belloni，D. Chen，V. Chernozhukov，C. Hansen，<a class="ae mb" href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" rel="noopener ugc nofollow" target="_blank">应用于征用权的最优工具的稀疏模型和方法</a> (2012)，<em class="no">计量经济学</em>。</p><p id="df6f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] A. Belloni，V. Chernozhukov，C. Hansen，<a class="ae mb" href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" rel="noopener ugc nofollow" target="_blank">高维对照中选择后对治疗效果的推断</a> (2014)，<em class="no">《经济研究综述》</em>。</p><p id="7146" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3] V. Chernozhukov，D. Chetverikov，M. Demirer，E. Duflo，C. Hansen，W. Newey，J. Robins，<a class="ae mb" href="https://academic.oup.com/ectj/article/21/1/C1/5056401" rel="noopener ugc nofollow" target="_blank">用于治疗和结构参数的双/去偏置机器学习</a> (2018)，<em class="no">《计量经济学杂志》</em>。</p><h2 id="5093" class="ni md iq bd me pq pr dn mi ps pt dp mm lo pu pv mo ls pw px mq lw py pz ms iw bi translated">相关文章</h2><ul class=""><li id="6024" class="oe of iq lh b li mu ll mv lo qa ls qb lw qc ma ow ok ol om bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699">理解省略变量偏差</a></li><li id="7ebf" class="oe of iq lh b li on ll oo lo op ls oq lw or ma ow ok ol om bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/59f801eb3299">理解弗里希-沃-洛弗尔定理</a></li><li id="6e9d" class="oe of iq lh b li on ll oo lo op ls oq lw or ma ow ok ol om bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">Dag和控制变量</a></li></ul><h2 id="3221" class="ni md iq bd me pq pr dn mi ps pt dp mm lo pu pv mo ls pw px mq lw py pz ms iw bi translated">密码</h2><p id="9fc4" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">你可以在这里找到Jupyter的原始笔记本:</p><div class="oy oz gp gr pa pb"><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/pretest.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ja gy z fp pg fr fs ph fu fw iz bi translated">Blog-post/pretest . ipynb at main matter courthoud/Blog-post</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">我博客文章的代码和笔记本。通过在…上创建帐户，为matteocourthoud/Blog-Posts的发展做出贡献</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">github.com</p></div></div><div class="pk l"><div class="qd l pm pn po pk pp kz pb"/></div></div></a></div></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h2 id="683d" class="ni md iq bd me pq pr dn mi ps pt dp mm lo pu pv mo ls pw px mq lw py pz ms iw bi translated">感谢您的阅读！</h2><p id="1594" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated"><em class="no">真的很感谢！</em>🤗<em class="no">如果你喜欢这个帖子并且想看更多，可以考虑</em> <a class="ae mb" href="https://medium.com/@matteo.courthoud" rel="noopener"> <strong class="lh ja"> <em class="no">关注我</em> </strong> </a> <em class="no">。我每周发布一次与因果推断和数据分析相关的主题。我尽量让我的帖子简单而精确，总是提供代码、例子和模拟。</em></p><p id="ba22" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="no">还有，一个小小的</em> <strong class="lh ja"> <em class="no">免责声明</em> </strong> <em class="no">:我写作是为了学习所以错误是家常便饭，尽管我尽力了。当你发现他们的时候，请告诉我。也很欣赏新话题的建议！</em></p></div></div>    
</body>
</html>