<html>
<head>
<title>Rainbow DQN — The Best Reinforcement Learning Has to Offer?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">彩虹 DQN——最好的强化学习能提供什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86#2022-12-08">https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86#2022-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6379" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如果深度 Q 学习中最成功的技术被组合到一个算法中，会发生什么？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/03d8944e02a2d38e6976f151d348c407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9K2oKzLpXwse8-QB-y8Yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">彩虹 DQN 性能与其他 DQN 技术相比。这些结果好到不真实吗？【来源:<a class="ae ky" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank"> DeepMind 论文</a></p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="4f16" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">自经典的<a class="ae ky" href="https://medium.com/towards-data-science/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff" rel="noopener">表格 Q 学习</a>时代以来，强化学习已经走过了漫长的道路。<strong class="li iu">深度 Q 学习网络(DQN) </strong>推动了该领域的一场革命，实现了对状态的强大概括。</p><p id="0e1e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过用强大的神经网络(理论上能够将任何输入状态转换为每个动作的输出值)取代 Q 表(存储所有状态-动作对的值)，我们可以<strong class="li iu">处理大规模状态空间的问题</strong>，这标志着该领域的突破。</p><p id="4e58" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，使用神经网络也带来了无数的问题。作为回应，大量的技术被用来提高 DQN 算法的性能。常见的技术包括<a class="ae ky" rel="noopener" target="_blank" href="/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172">重放缓冲区、目标网络和熵奖励</a>。通常，结合这些技术可以确保显著的性能改进。</p><blockquote class="mc"><p id="c415" class="md me it bd mf mg mh mi mj mk ml mb dk translated">如果一种技术可以如此显著地提高性能，为什么不使用所有的技术呢？</p></blockquote><p id="376b" class="pw-post-body-paragraph lg lh it li b lj mm ju ll lm mn jx lo lp mo lr ls lt mp lv lw lx mq lz ma mb im bi translated">这正是 DeepMind 团队(他们 2017 年的论文有 10 位作者，所以我就称之为“团队”)肯定想知道的。如果你将所有这些单独的技术结合成一个单一的算法，你会得到一个梦之队还是科学怪人？</p><h1 id="5115" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">彩虹 DQN 中使用的技术</h1><p id="d506" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">我们将暂时搁置性能问题，首先深入研究在彩虹 DQN 中部署和组合的六种技术。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/5a3cb82f7d96c7f3833b709033ecd9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*281tooj8Wb-EZgLZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@mango_quan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Harry Quan </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="426e" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">一.双 Q 学习</h2><p id="f680" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">Q-learning 以最大化状态-动作对的方式选择动作。然而，这带来了一个根本性的缺陷。因为 Q 值仅仅是近似值，所以该算法倾向于 Q 值被夸大的动作。这意味着我们系统地高估了 Q 值！自然，当试图找到最佳决策政策时，这种偏见没有帮助。更糟糕的是，Q-learning 是一种自举方法，即使在多次迭代后，高估效应可能仍然存在。</p><p id="c0da" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">双 Q-learning 通过部署一个网络来选择动作，另一个网络来更新值，从而克服了这一缺陷。因此，即使我们使用一个膨胀的 Q 值来选择动作，我们也用一个(可能)不膨胀的 Q 值来更新。每次，我们随机选择一个网络进行操作选择，另一个网络进行更新。因此，我们将选择机制与评估机制分离开来。网络 A 的更新过程如下所示(显然，网络 B 的部署正好相反):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/32a223282f882704fdeded5428bc9316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*HjrDUnyN8-3neAwgC_mBKA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度 Q 学习的更新机制。用一个网络选择最佳已知动作，用另一个网络确定其 Q 值。</p></figure><p id="66a0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从经验上看，双 Q 架构成功地减少了偏差，并导致更好的决策政策。</p><h2 id="0270" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">二。优先体验重放</h2><p id="fb41" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">收集观察数据通常成本很高。q-learning——作为一种非策略方法——允许将观察结果存储在重放缓冲区中，随后我们可以从该缓冲区中<strong class="li iu">采样经验来更新我们的策略</strong>。此外，它打破了状态之间的相关性——由于后续状态通常表现出很强的相关性，神经网络往往会局部过度拟合。</p><p id="bb25" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，并不是每一次过去的经历都同样值得重温。为了最大化学习，我们可能希望对过去产生<strong class="li iu">高(绝对)值函数误差</strong>的元组进行采样，我们希望再次评估这些经验并减少误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/eb0f40ed9dcc7fafc1f53eb727e20f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*G4tLSxY40K0a7dKtemUi6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">优先级采样方案，其中 p_i 表示体验的优先级(例如，其等级或绝对 TD 误差+一些噪声),指数α确定优先化的强度。</p></figure><p id="2159" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简单地选择误差最大的经验会引入抽样偏差。为了补救这一点，使用一种形式的<strong class="li iu">加权重要性采样</strong>来执行权重更新:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c7519e0136de82a39d1cfed7e9491d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*qTcBMST_pvpr2F85-8Filg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加权重要性抽样方案，其中 N 是批量，β是比例参数。</p></figure><p id="08fe" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">优先体验重放被证明始终优于统一重放抽样。</p><h2 id="f50b" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">三。决斗网络</h2><p id="92d2" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">深度 Q 网络通常返回 Q 值<em class="oe"> Q(s，a) </em>，作为对应于给定状态-动作对<em class="oe"> (s，a) </em>的值函数的近似。在最初的贝尔曼方程中，这非常有意义，因为计算最优价值函数会产生最优策略。在 Q-learning 中，状态值和动作值的<strong class="li iu">串联可能是有问题的</strong>，因为可能很难区分一个值是主要源于状态、动作还是两者。</p><p id="de21" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Q 值可以分解如下:<em class="oe"> Q(s，a)=V(s)+A(s，a) </em>。假设我们在最佳已知策略下操作(即，该策略根据其估计返回最佳行动)，则<strong class="li iu">预期优势为 0 </strong>，Q 值等于状态值。</p><p id="658e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">特别是，对于一些州来说，动作选择并不那么重要，例如，考虑一辆无人驾驶汽车处于无障碍状态，优势值可以忽略不计。或者，再举一个例子，一个<strong class="li iu">状态可能是差的，不管我们在其中采取什么动作</strong>。对于这样的状态，我们不希望通过探索所有的动作来浪费计算的努力。</p><p id="f0f0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">决斗架构允许更<strong class="li iu">快速地确定好的动作</strong>，因为差的动作可以更容易地被丢弃，可比较的动作可以更快地被识别。通过将优势价值从状态价值中分离出来，我们获得了关于行动价值的更细粒度的视角。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/d1ce702cab72a45c21aa0eeedc072ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hemJjT2e_Q44Jq8aYkzJHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统深度 Q 网络(上)和决斗深度 Q 网络(下)架构之间的比较。后者有单独的流来输出状态值和优势值，相加后返回 Q 值。[图改编自<a class="ae ky" href="https://arxiv.org/pdf/1511.06581v3.pdf" rel="noopener ugc nofollow" target="_blank">王等，2016 </a> ]</p></figure><p id="ae52" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">该网络具有由<em class="oe"> θ </em>参数化的层的联合集合、由<em class="oe"> α </em>参数化的流以确定优势函数，以及由<em class="oe"> β </em>参数化的流以确定价值函数。<strong class="li iu">两个流相加，返回 Q 值</strong>。</p><p id="5f19" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了确保价值函数和优势函数确实依赖于<strong class="li iu">单独参数化的流</strong>，从所选动作的优势中减去所有动作的平均优势:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/75eff20fd1dc1b381c19a334f4642ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*BcELxs86ozNckhIrhQC8Jw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在决斗网络中，一个流(由β参数化)返回状态相关值函数，另一个流(由α参数化)返回优势函数。</p></figure><p id="1ef6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">实验结果表明，所提出的体系结构可以快速识别好的动作，因为识别动作值已经成为学习问题的显式部分。</p><h2 id="a38c" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">四。多步学习</h2><p id="9d10" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">一项行动的影响并不总是直接可见的。许多个人行为甚至可能不会立即产生回报。例如，赛车比赛中的一个糟糕的转弯可能会导致稍后失去杆位。典型的 Q-learning 更新——或者更一般地说，TD(0)更新——只考虑直接跟随行动的回报，引导决策的下游值。</p><p id="f10f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">另一方面，蒙特卡罗/ TD(1)学习在更新 Q 值之前首先观察所有回报，但这通常导致高方差和慢收敛。多步学习—也称为<em class="oe">n</em>—Q 学习— <strong class="li iu">在偏差和方差</strong>之间取得平衡，在执行更新之前观察多个时间步。例如，如果<em class="oe"> n=3 </em>，更新将是三个观察到的奖励和剩余时间范围的自举价值函数的组合。</p><p id="e590" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在一般形式下，多步奖励可计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/cfb8c9c0fb4d1b07159f7a00fb0bcc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*tpch-uTGR_9hzjqMw7rnTQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在多步学习中，我们跟踪 n 个时间步的回报，并使用一个 Q 值来估计之后的效果。这种方法经常在时间差异和蒙特卡罗学习之间找到平衡。</p></figure><p id="7335" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相应的更新方案定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/33daee222a01c85f44387deeaf1e208c.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*e3s_F2iSVdgYNekhkdLsMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多步 Q 学习的权重更新方案。</p></figure><p id="c9e7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于许多问题设置，多步学习比 TD(0)和 TD(1)学习产生了实质性的改进，<strong class="li iu">减轻了偏差和方差影响，并承认了决策的延迟影响</strong>。主要的挑战是针对手头的问题适当地调整<em class="oe"> n </em>。</p><h2 id="de15" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">动词 （verb 的缩写）分布式强化学习</h2><p id="2ee7" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">大多数强化学习算法使用预期的返回值。相反，我们可以尝试学习收益的<strong class="li iu">分布</strong>(Q 值代表期望值)。虽然传统上主要用于获得洞察力(例如，回报的风险)，但这种学习方式似乎实际上也能提高绩效。</p><p id="22db" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">学习<em class="oe">分布</em>而不是<em class="oe">期望值</em>的主要用例是在随机环境中的应用。即使奖励是嘈杂的，并不直接有助于收敛期望值，我们仍然可以利用它们来更好地把握<strong class="li iu">潜在的奖励产生分布</strong>。</p><p id="0ba4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">鉴于典型的 Q 值由<em class="oe"> Q(s，a)=r(s，a)+γQ(s’，a’)</em>定义，分布变量定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4f7760a54e378a2cdeeb1c52d2141f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*cANWsvZsvzbw9qMNPeVMkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奖励分配的定义，取代 Q 值。在分布 RL 中，我们的目标是学习整个分布，其期望值等于 Q 值。</p></figure><p id="4aaf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，<em class="oe"> Z </em>表示收益的分布，而不是预期(Q-)值。另外，<em class="oe"> R </em>、<em class="oe">S’</em>和<em class="oe">A’</em>都是随机变量。</p><p id="72b2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们的目标不是减少 TD 误差，而是<strong class="li iu">最小化目标分布的采样</strong> <a class="ae ky" href="https://medium.com/towards-data-science/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c" rel="noopener"> <strong class="li iu"> Kullback-Leibner 散度</strong> </a>。我们最大限度减少的损失如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/654989696eb1dd8f46daf318e60d8415.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*Hhadx1Bg5LLioL65a0QkHg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分布 RL 的损失函数。我们构建了一个样本奖励分布，我们用它来更新预测的奖励分布。</p></figure><p id="09bf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，𝒯是收缩映射，<em class="oe">φ</em>是分布投影，<em class="oe"> bar θ </em>是表示目标网络的冻结参数副本。恐怕你真的需要整篇论文才能理解它。</p><p id="0a89" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">也许更直观的是:网络输出一个离散化的概率质量函数，其中<strong class="li iu">每个输出(‘atom’)代表一个小回报区间</strong>的概率。随着每次更新，我们试图使预测的奖励分布更接近观察到的。</p><p id="db81" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">分布式 RL 产生了很好的结果，尤其是在奖励稀少且嘈杂的问题环境中。</p><h2 id="67e0" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">不及物动词嘈杂的勘探层</h2><p id="48e1" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">传统的 q 学习使用ε-贪婪学习，随机选择具有概率ϵ(通常为 0.05 或 0.10)的动作，而不是具有预期最大值的动作。这种机制允许通过确保探索来避免局部最优。熵奖励是另一种奖励偏离行为的常用技术。</p><p id="11ce" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在嘈杂的环境中，我们可能需要更多的探索。我们可以通过向我们的神经网络添加噪声探索层来实现这一点。我们用由确定性流和噪声流组成的<strong class="li iu">层替换标准线性层——以规范形式定义为<em class="oe"> y=Wx </em>，其中<em class="oe"> x </em>是输入，<em class="oe"> W </em>是权重集，而<em class="oe"> y </em>是输出:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1d58a1ff7df83c53abb42caff58830f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*CddPAo6qPV8vHq3XRW0aTw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络中的噪声线性层。输出 y 取决于确定性流和噪声流。对应于两个流的权重随时间更新。</p></figure><p id="968d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">正如您所想象的，图层变换产生的噪声会影响最终的 Q 值，网络产生的最高 Q 值可能会有所不同，从而推动探索行为。随着时间的推移，算法可以<strong class="li iu">学会忽略噪声流</strong>，因为<em class="oe">b _ noise</em>和<em class="oe">W _ noise</em>是可学习的参数。然而，信号和噪声之间的比率可以根据状态而变化，从而鼓励在必要时进行探索，并在适当时收敛到确定性动作。</p><p id="eb0f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">噪声神经网络似乎是比例如ε贪婪或熵奖励更健壮的探索机制。与这里提到的其他技术一样，经验表明它可以提高许多基准问题的性能。</p><h1 id="36a7" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">那么…有用吗？</h1><p id="d829" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">DeepMind 团队将他们的彩虹算法(结合了上述六种技术)与 DQN 算法进行了测试，每次只使用一种技术。下图代表了 57 款 Atari 游戏的平均性能，100%表示人类水平的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/03d8944e02a2d38e6976f151d348c407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9K2oKzLpXwse8-QB-y8Yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">经验表现，平均超过 57 雅达利游戏。彩虹 DQN 在学习速度和最终表现方面都远远超过了单个 DQN 技术[来源:<a class="ae ky" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank"> DeepMind 论文</a></p></figure><p id="9c2e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如图所示，<strong class="li iu"> Rainbow DQN 明显优于所有使用单一组件 DQN 的基准测试</strong>。同样有趣的是，它获得良好的性能比其他的要快得多。在大约 15m 帧后，它可以媲美类似人类的性能；有四个基准根本没有达到那个点。在 44/200 米帧之后，彩虹 DQN 已经处于持续超越所有竞争对手的水平，无论运行时间如何。在 2 亿帧之后，它是这场拍卖中无可争议的赢家。</p><p id="5889" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">DeepMind 团队还通过省略彩虹中的个别技术，检查了所有组件是否都是必要的。<strong class="li iu">有些技术比其他技术贡献更大</strong>，但是——稍微简化一下论文的结论——每种技术都在性能方面给算法增加了一些东西。见下面的对比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/03d7e6e00d52fcf202d7cf2b24425286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*gvCBzhK-99nE_czV4f0UXA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虽然不是每种技术都对彩虹 DQN 的性能有同等的贡献，但实验结果表明，每种技术都增加了价值【来源:<a class="ae ky" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank"> DeepMind 论文</a></p></figure><p id="7045" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总的来说，实验结果看起来绝对令人震惊，那么我们为什么不一直使用彩虹 DQN 呢？</p><h1 id="a8fd" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">弊端？</h1><p id="bcf9" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">首先，由于部署了如此多的精细技术，<strong class="li iu">彩虹 DQN 速度很慢</strong>。每种技术都需要一系列额外的计算，这些自然会累积起来。对于每一帧，该算法可以学习很多，但是处理一帧的时间要长得多。因此，与其他算法进行基于时间的比较不太有利。</p><p id="3f78" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">彩虹 DQN 的第二个缺点是大量的调谐。多步骤过程中要采取的步骤数量、噪声层中的噪声量、目标网络的更新频率——一切都必须进行适当的调整。知道网格搜索爆炸的速度有多快，<strong class="li iu">在这样一个高度参数化的算法</strong>中进行调整并不容易，尤其是考虑到算法的低速度。</p><p id="fea4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，部分由于前面的几点，彩虹 DQN 可能不稳定并且<strong class="li iu">难以调试</strong>。一项技术中的一个编码错误就可能毁掉整个算法——祝你好运找到错误。行为不符合预期？它可以是任何东西，甚至是各种技术之间的某种特殊的相互作用。</p><p id="e413" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">考虑到算法的困难，彩虹 DQN 目前<strong class="li iu">还没有包含在流行的 RL 库中</strong>，因此不能轻易的使用。从某种意义上说，彩虹确实是 RL 技术的梦之队，但似乎胜利并不是一切。</p><h1 id="6150" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">结束语</h1><p id="bacc" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated">正如我们所见，<strong class="li iu"> Rainbow DQN 公司报告了其基准测试的令人印象深刻的结果</strong>。然而，应该注意的是，原始论文仅将其与其他 DQN 技术进行比较。特别是，整个基于政策的<a class="ae ky" href="https://medium.com/towards-data-science/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a" rel="noopener">类</a>方法——包括流行的算法如<a class="ae ky" href="https://medium.com/towards-data-science/trust-region-policy-optimization-trpo-explained-4b56bd206fc2" rel="noopener"> TRPO </a>和<a class="ae ky" rel="noopener" target="_blank" href="/proximal-policy-optimization-ppo-explained-abed1952457b">PPO</a>——都被排除在比较之外。</p><p id="1bfa" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在 OpenAI 的一项研究中(虽然不是针对相同的问题)，性能与 PPO 进行了比较，PPO 是当前连续控制中的 go-to 算法。通过标准实现，彩虹 DQN 轻而易举地击败了 PPO。然而，当部署联合训练机制时——本质上是转移学习，同时保持算法本身相同——<strong class="li iu">联合 PPO 以微弱优势击败联合彩虹 DQN </strong>。鉴于 PPO 比彩虹 DQN 更简单快捷，它更受欢迎也就不足为奇了。</p><p id="6106" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最终，性能非常重要，但它不是 RL 算法的唯一相关方面。我们应该能够使用它，修改它，调试它，最重要的是，理解它。彩虹 DQN 证明了该领域的进步可以相互加强，该算法产生了<strong class="li iu">最先进的结果</strong>，但最终没有说服人类用户全心全意地接受它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/a2916c636ea42540a2d850b70c4bf06a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*797cabaYgh5pOwHC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@peeeola?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Paola Franco </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="0d1d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">想更多地了解 DQN 吗？查看以下文章:</p><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/deep-q-learning-for-the-cliff-walking-problem-b54835409046"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">悬崖行走问题的深度 Q 学习</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">一个完整的 Python 实现，用 TensorFlow 2.0 导航悬崖。</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">如何对经验重放、批量学习和目标网络进行建模</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">使用 TensorFlow 2.0，快速学习稳定和成功的深度 Q 学习的三个基本技巧</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pg l pc pd pe pa pf ks or"/></div></div></a></div><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">TensorFlow 2.0 中深度 Q 学习的最小工作示例</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">一个多臂土匪的例子来训练一个 Q 网络。使用 TensorFlow，更新过程只需要几行代码</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="ph l pc pd pe pa pf ks or"/></div></div></a></div><h1 id="af5d" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">参考</h1><h2 id="8bf6" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">彩虹 DQN</h2><p id="4b71" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated"><strong class="li iu">彩虹 DQN 论文:</strong> Hessel，m .，Modayil，j .，Van Hasselt，h .，Schaul，t .，Ostrovski，g .，Dabney，w .，… &amp; Silver，D. (2018，4 月)。<a class="ae ky" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank">彩虹:结合深度强化学习的改进。</a>在<em class="oe">第三十二届 AAAI 人工智能大会</em>。</p><p id="cc68" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">比较彩虹 DQN 和 PPO: </strong>尼科尔，a .，普法乌，v .，黑塞，c .，克里莫夫，o .，&amp;舒尔曼，J. (2018)。<a class="ae ky" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/retro-contest/gotta_learn_fast_report.pdf" rel="noopener ugc nofollow" target="_blank">快速学习:学习力泛化的新基准。</a> <em class="oe"> arXiv 预印本 arXiv:1804.03720 </em>。</p><h2 id="d57d" class="np ms it bd mt nq nr dn mx ns nt dp nb lp nu nv nd lt nw nx nf lx ny nz nh oa bi translated">DQN 技术</h2><p id="5225" class="pw-post-body-paragraph lg lh it li b lj nj ju ll lm nk jx lo lp nl lr ls lt nm lv lw lx nn lz ma mb im bi translated"><strong class="li iu">一、双 Q 学习:</strong> Hasselt，H. (2010)。<a class="ae ky" href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf" rel="noopener ugc nofollow" target="_blank">双 Q-学习。</a> <em class="oe">神经信息处理系统的进展</em>、<em class="oe"> 23 </em>。</p><p id="336d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">二。优先经验回放:</strong> Schaul，t .、Quan，j .、Antonoglou，I .、&amp; Silver，D. (2015)。<a class="ae ky" href="https://arxiv.org/pdf/1511.05952v4.pdf" rel="noopener ugc nofollow" target="_blank">优先体验回放。</a>T24】arXiv 预印本 arXiv:1511.05952 。</p><p id="dea7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">三。决斗网络:</strong>王，z .，绍尔，t .，赫塞尔，m .，哈瑟尔特，h .，兰托特，m .，&amp;弗雷塔斯，N. (2016，6 月)。<a class="ae ky" href="https://arxiv.org/pdf/1511.06581v3.pdf" rel="noopener ugc nofollow" target="_blank">深度强化学习的决斗网络架构</a>。在<em class="oe">机器学习国际会议</em>(第 1995–2003 页)。PMLR。</p><p id="d59f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">四。多步学习:</strong>萨顿，R. S .，&amp;巴尔托，A. G. (2018)。<a class="ae ky" href="http://www.incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="oe">强化学习:入门</em></a><em class="oe">(7.1 节，n 步 TD 预测)</em>。麻省理工出版社。</p><p id="e59b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">五、分布式学习:</strong>贝勒马尔，M. G .，达布尼，w .，&amp;穆诺斯，R. (2017，7 月)。<a class="ae ky" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank">强化学习的分布式视角。</a>在<em class="oe">机器学习国际会议上</em>(第 449–458 页)。PMLR。</p><p id="307c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">六。嘈杂网络: Fortunato，m .，Azar，M. G .，Piot，b .，Menick，j .，Osband，I .，Graves，a .，… &amp; Legg，S. (2017)。<a class="ae ky" href="https://arxiv.org/pdf/1706.10295.pdf?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">探索嘈杂的网络。</a> <em class="oe"> arXiv 预印本 arXiv:1706.10295 </em>。</p></div></div>    
</body>
</html>