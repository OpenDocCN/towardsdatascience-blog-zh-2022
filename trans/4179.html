<html>
<head>
<title>Classification of Neural Network Hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络超参数的分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-of-neural-network-hyperparameters-c7991b6937c3#2022-09-16">https://towardsdatascience.com/classification-of-neural-network-hyperparameters-c7991b6937c3#2022-09-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b211" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">受网络结构、学习和优化以及正则化效应的影响</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b6c08d37fb551bdf1479454d01152f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVu30hz3wEBLcDLZT0eAIw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰-马克·史密斯在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ea04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用DL算法时的一个主要挑战是设置和控制超参数值。这在技术上叫做<strong class="lb iu"> <em class="lv">超参数调优</em> </strong>或者<strong class="lb iu"> <em class="lv">超参数优化</em> </strong>。</p><p id="b1ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数控制DL算法的许多方面。</p><ul class=""><li id="3800" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">他们可以决定运行算法的时间和计算成本。</li><li id="5e6a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">他们可以定义神经网络模型的结构</li><li id="490f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">它们会影响模型的预测准确性和泛化能力。</li></ul><p id="32a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，超参数控制神经网络模型的行为和结构。因此，了解每个超参数在正确分类的神经网络中的作用非常重要(<a class="ae ky" href="#a105" rel="noopener ugc nofollow">见图表</a>)。</p><h1 id="70da" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">关于超参数的重要事实</h1><p id="b507" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在介绍和分类神经网络超参数之前，我想列出以下关于超参数的重要事实。要结合实例详细了解参数和超参数的区别，请阅读我的<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener"><strong class="lb iu"><em class="lv">参数Vs超参数:有什么区别？</em></strong></a><strong class="lb iu"><em class="lv"/></strong>条。</p><ul class=""><li id="f505" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">不要混淆<strong class="lb iu"> <em class="lv">超参数</em> </strong>和<strong class="lb iu"> <em class="lv">参数</em> </strong>。两者都是存在于ML和DL算法中的变量。但是，它们之间有明显的区别。</li><li id="205e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">参数是变量，其值从数据中学习并在训练期间更新。</li><li id="1fbd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">在神经网络中，权重和偏差是参数。它们在反向传播期间被优化(更新)以最小化成本函数。</li><li id="457b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">一旦找到参数的最佳值，我们就停止训练过程。</li><li id="1d4d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">超参数是变量，其值由ML工程师或任何其他人在训练模型之前设置。这些值不是从数据中自动学习的。因此，我们需要手动调整它们来构建更好的模型。</li><li id="935c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">通过改变超参数的值，我们可以建立不同类型的模型。</li><li id="6af7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">在ML和DL中，寻找超参数的最佳值是一项具有挑战性的任务。</li><li id="4880" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">超参数的最佳值还取决于数据集的大小和性质以及我们想要解决的问题。</li></ul><h1 id="a105" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">神经网络超参数分类标准</h1><p id="4cca" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">可以通过考虑以下标准对神经网络中的超参数进行分类。</p><ul class=""><li id="6ce3" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">网络结构</li><li id="502d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://rukshanpramoditha.medium.com/overview-of-a-neural-networks-learning-process-61690a502fa" rel="noopener">学习和优化</a></li><li id="679a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><a class="ae ky" href="https://rukshanpramoditha.medium.com/list/regularization-techniques-for-neural-networks-c4ad21cce618" rel="noopener">正规化效果</a></li></ul><p id="fb94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于上述标准，神经网络超参数可以分类如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/fd0769945d66f95679a2c38219cae62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9coeJ4OSPqAjiKLcQqiew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者图表，用draw.io制作)</p></figure><h1 id="e0a5" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">定义神经网络结构的超参数</h1><p id="8b5b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">根据该标准分类的超参数直接影响神经网络的结构。</p><h2 id="d4cc" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">隐藏层数</h2><p id="b24e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这也被称为网络的<em class="lv">深度</em>。深度学习中的术语<strong class="lb iu">【深度】</strong>是指一个神经网络的隐含层数(<em class="lv">深度</em>)。</p><p id="fe60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在设计<a class="ae ky" rel="noopener" target="_blank" href="/creating-a-multilayer-perceptron-mlp-classifier-model-to-identify-handwritten-digits-9bac1b16fe10"> MLP </a>、<a class="ae ky" rel="noopener" target="_blank" href="/coding-a-convolutional-neural-network-cnn-using-keras-sequential-api-ec5211126875"> CNN </a>、<a class="ae ky" rel="noopener" target="_blank" href="/generate-mnist-digits-using-shallow-and-deep-autoencoders-in-keras-fb011dd3fec3"> AE </a>等神经网络时，隐含层数决定了网络的<em class="lv">学习能力</em>。为了学习数据中所有重要的非线性模式，神经网络中应该有足够数量的隐藏层。</p><p id="8917" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当数据集的大小和复杂性增加时，神经网络需要更多的学习能力。因此，大型复杂数据集需要太多的隐藏层。</p><p id="8075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常少量的隐藏层会生成较小的网络，可能会使训练数据不足。这种类型的网络不会学习训练数据中的复杂模式，也不会对看不见的数据进行很好的预测。</p><p id="4959" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过多的隐藏层会生成一个较大的网络，可能会使训练数据过拟合。这种类型的网络试图记忆训练数据，而不是学习数据中的模式。因此，这种类型的网络不能很好地概括新的看不见的数据。</p><p id="ebd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://rukshanpramoditha.medium.com/list/addressing-overfitting-868959382d1d" rel="noopener">过拟合</a>没有欠拟合有害，因为过拟合可以通过适当的调整方法减少或消除。</p><h2 id="c783" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">每层中的节点数(神经元/单元)</h2><p id="774f" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这也就是所谓的<strong class="lb iu"> <em class="lv">宽度</em> </strong>的网络。</p><p id="d1fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隐藏层中的节点通常称为<strong class="lb iu"> <em class="lv">隐藏单元</em> </strong>。</p><p id="2b87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">隐藏单元的数量是影响网络学习能力的另一个因素。</p><p id="3134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太多的隐藏单元会创建非常大的网络，这可能会使训练数据过拟合，而非常少的隐藏单元会创建较小的网络，这可能会使训练数据过拟合。</p><p id="d624" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MLP输入层中的结点数量取决于输入数据中的<a class="ae ky" rel="noopener" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b">维度</a>(要素数量)。</p><p id="5ab8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MLP输出图层中的结点数量取决于我们想要解决的问题类型。</p><ul class=""><li id="92f9" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">二元分类:</strong>使用输出层的一个节点。</li><li id="2dbf" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">多标签分类:</strong>如果有<strong class="lb iu"> <em class="lv"> n </em> </strong>个相互包含的类，则在输出层使用<strong class="lb iu"> <em class="lv"> n </em> </strong>个节点。</li><li id="feef" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">多类分类:</strong>如果有<strong class="lb iu"> <em class="lv"> n </em> </strong>个互斥类，<strong class="lb iu"> <em class="lv"> n </em> </strong>个节点用于输出层。</li><li id="ea99" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">回归:</strong>使用输出层的一个节点。</li></ul><blockquote class="nu"><p id="1e04" class="nv nw it bd nx ny nz oa ob oc od lu dk translated">新手总是问一个神经网络层应该包含多少个隐层或者多少个节点。</p></blockquote><p id="2075" class="pw-post-body-paragraph kz la it lb b lc oe ju le lf of jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">要回答这个问题，你可以利用上述事实以及以下两点。</p><ul class=""><li id="9b16" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">当隐藏层和隐藏单元的数量增加时，网络变得非常大，参数的数量显著增加。训练这样的大型网络，需要大量的计算资源。因此，大型神经网络在计算资源方面非常昂贵。</li><li id="2a3f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">我们可以通过添加或删除隐藏层和隐藏单元来试验不同的网络结构，然后通过<a class="ae ky" rel="noopener" target="_blank" href="/replicate-a-logistic-regression-model-as-an-artificial-neural-network-in-keras-cd6f49cf4b2c#0346">绘制训练误差和测试(验证)误差相对于训练期间的时期数</a>来查看模型的性能。</li></ul><h2 id="edf0" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">激活功能的类型</h2><p id="30e6" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这是定义网络结构的最后一个超参数。</p><p id="5445" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在神经网络层中使用激活函数。输入层不需要任何激活功能。我们必须在隐藏层中使用激活函数来给网络引入非线性。输出层中使用的激活类型由我们想要解决的问题类型决定。</p><ul class=""><li id="dd4f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">回归:</strong>同一个节点的身份激活函数</li><li id="acb9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">二元分类:</strong>具有一个节点的Sigmoid激活函数</li><li id="cab9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">多类分类:</strong>每类一个节点的Softmax激活函数</li><li id="2e06" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">多标签分类:</strong>每类一个节点的Sigmoid激活函数</li></ul><p id="a82a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要通过图形表示详细了解不同类型的激活函数，请阅读我的<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c"><strong class="lb iu"><em class="lv">如何为神经网络选择正确的激活函数</em></strong></a><strong class="lb iu"><em class="lv"/></strong>一文。</p><p id="2191" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要阅读激活功能的使用指南，请点击此处的<a class="ae ky" rel="noopener" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c#e2bb"/>。</p><p id="1433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解激活函数的好处，请阅读我的<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/3-amazing-benefits-of-activation-functions-in-neural-networks-22b17b91a46e" rel="noopener"><strong class="lb iu"><em class="lv">神经网络中激活函数的3个惊人好处</em></strong><strong class="lb iu"><em class="lv"/></strong>文章。</a></p><p id="d4a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解如果在神经网络的隐藏层中不使用任何激活函数会发生什么，请阅读本文。</p><h1 id="3331" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">决定如何训练网络的超参数</h1><p id="86da" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">根据该标准分类的超参数直接控制网络的训练过程。</p><h2 id="dacd" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">优化器的类型</h2><p id="84a4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">优化器也叫<strong class="lb iu"> <em class="lv">优化算法</em> </strong>。优化器的任务是通过更新网络参数来最小化损失函数。</p><p id="d386" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度下降是最流行的优化算法之一。它有三种变体。</p><ul class=""><li id="6f95" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">批量梯度下降</li><li id="7dd5" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">随机梯度下降</li><li id="ccce" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">小批量梯度下降</li></ul><p id="3964" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些变量<a class="ae ky" href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a#968f" rel="noopener">的不同之处在于我们用来计算损失函数梯度的批量</a>(稍后将详细介绍)。</p><p id="46dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为解决梯度下降算法的缺点而开发的其他类型的优化器有:</p><ul class=""><li id="cef6" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">动量梯度下降</li><li id="bb06" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li><li id="a157" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">阿达格拉德</li><li id="032f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">阿达德尔塔</li><li id="d84b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">阿达马克斯</li><li id="efbb" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">那达慕</li><li id="efd7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">Ftrl</li><li id="50fd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">RMSProp (Keras默认值)</li></ul><h2 id="1e76" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">学习率- <strong class="ak"> α </strong></h2><p id="a05d" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这个超参数可以在任何优化算法中找到。</p><p id="c0d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在优化过程中，优化器采取微小的步骤来降低误差曲线。学习率指的是步长的大小。它决定了优化器下降误差曲线的快慢。步长的方向由梯度(导数)决定。</p><p id="3b50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是神经网络训练中最重要的超参数之一。</p><p id="42a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较大的学习速率值可以用来更快地训练网络。过大的值会导致损失函数在最小值附近振荡，永远不下降。那样的话，模型就永远训练不出来了！</p><p id="e552" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习率的值太小会导致模型训练甚至几个月。在这种情况下，收敛发生得非常慢。该网络将需要许多纪元(稍后将详细介绍)才能以非常小的学习速率收敛。</p><p id="c56d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们应该避免使用太大和太小的学习率值。最好从一个小的学习率开始，例如0.001(大多数优化器中的默认值)，然后如果网络需要太多时间收敛，则系统地增加学习率。</p><h2 id="4790" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">损失函数的类型</h2><p id="82af" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">应该有一种方法在训练期间测量神经网络的性能。损失函数用于计算预测值和实际值之间的损失分数(误差)。我们的目标是通过使用优化器来最小化损失函数。这就是我们在训练中取得的成绩。</p><p id="21dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练中使用的损失函数的类型取决于我们所面临的问题的类型。</p><ul class=""><li id="3355" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">均方误差(MSE) </strong> —这是用来衡量回归问题的表现。</li><li id="88c9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">平均绝对误差(MAE) </strong> —这是用来衡量回归问题的表现。</li><li id="c5bf" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">平均绝对百分比误差</strong> —用于衡量回归问题的表现。</li><li id="a75a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu"> Huber Loss </strong> —用于衡量回归问题的表现。</li><li id="e0d7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">二元交叉熵(对数损失)</strong> —这用于衡量二元(两类)分类问题的性能。</li><li id="0296" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">多类交叉熵/分类交叉熵</strong> —用于衡量多类(两类以上)分类问题的性能。</li><li id="b84c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><strong class="lb iu">稀疏分类交叉熵</strong> —在多类分类问题中，自动将标量值标签转换为<strong class="lb iu"> <em class="lv">一个热点向量</em> </strong>。点击了解更多关于此<a class="ae ky" href="https://rukshanpramoditha.medium.com/one-hot-encode-scalar-value-labels-for-deep-learning-models-4d4053f185c5" rel="noopener">的信息。</a></li></ul><h2 id="5d50" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">模型评估指标的类型</h2><p id="087b" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">就像我们在训练期间使用损失函数来测量神经网络的性能一样，我们在测试期间使用评估度量来测量模型的性能。</p><p id="8945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于分类任务，将使用“准确性”、“精确度”、“召回率”、“auc”指标。对于回归任务，将使用“均方误差”、“平均绝对误差”。</p><h2 id="e1c8" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">批量</h2><p id="2491" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">批量</em> </strong>是在<code class="fe oj ok ol om b">model.fit()</code>方法中发现的另一个重要的超参数。</p><blockquote class="nu"><p id="0cbb" class="nv nw it bd nx ny on oo op oq or lu dk translated">批次大小指的是批次中训练实例的数量—来源:<a class="ae ky" href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a" rel="noopener">关于批次大小、时期和神经网络中的训练步骤，您需要了解的全部信息</a></p></blockquote><p id="750d" class="pw-post-body-paragraph kz la it lb b lc oe ju le lf of jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">换句话说，它是每次渐变更新(迭代)使用的实例数。</p><p id="2e6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">批量大小的典型值为16、32 (Keras默认值)、64、128和256、512和1024。</p><p id="af20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较大的批量通常需要每个时段大量的计算资源，但是需要较少的时段来收敛。</p><p id="f6d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较小的批量不需要每个时期大量的计算资源，但是需要许多时期来收敛。</p><h2 id="45c3" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">纪元</h2><p id="b9d4" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">时期</em> </strong>是在<code class="fe oj ok ol om b">model.fit()</code>方法中发现的另一个重要的超参数。</p><blockquote class="nu"><p id="f686" class="nv nw it bd nx ny on oo op oq or lu dk translated">时期指的是模型看到整个数据集的次数——来源:<a class="ae ky" href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a" rel="noopener">您需要知道的关于批量大小、时期和神经网络中的训练步骤的所有信息</a></p></blockquote><p id="be08" class="pw-post-body-paragraph kz la it lb b lc oe ju le lf of jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">当，</p><ul class=""><li id="7028" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">网络是用很小的学习率训练出来的。</li><li id="f6fb" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">批量太小。</li></ul><p id="0799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时，网络会倾向于用大量的历元来过度拟合训练数据。也就是说，在收敛之后，验证误差在某个点开始增加，而训练误差进一步减小。当这种情况发生时，该模型在训练数据上表现良好，但在新的未知数据上表现不佳。此时，我们应该停止训练过程。这就叫<a class="ae ky" href="https://rukshanpramoditha.medium.com/using-early-stopping-to-reduce-overfitting-in-neural-networks-7f58180caf5b" rel="noopener"> <strong class="lb iu"> <em class="lv">早停</em> </strong> </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f52c09ba4193e9f756e58d7185b18971.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*lil9hOnHckXG-CmRzmZtIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ot">提前停止进程</strong>(图片由作者提供，用matplotlib和draw.io制作)</p></figure><h2 id="91e7" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">每个时期的训练步骤(迭代)</h2><blockquote class="nu"><p id="1268" class="nv nw it bd nx ny on oo op oq or lu dk translated">一个训练步骤(迭代)是一个梯度更新—来源:<a class="ae ky" href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a" rel="noopener">关于神经网络中的批量大小、时期和训练步骤，您需要知道的一切</a></p></blockquote><p id="71f5" class="pw-post-body-paragraph kz la it lb b lc oe ju le lf of jx lh li og lk ll lm oh lo lp lq oi ls lt lu im bi translated">我们不需要为此超参数设置一个值，因为算法会按如下方式自动计算它。</p><pre class="kj kk kl km gt ou om ov ow aw ox bi"><span id="c6ec" class="ni ml it om b gy oy oz l pa pb"><strong class="om iu">Steps per epoch = (Size of the entire dataset / batch size) + 1</strong></span></pre><p id="427d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们加1来补偿任何小数部分。比如我们得到18.75，我们忽略0.75，18加1。总数是19。</p><p id="d777" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Keras <code class="fe oj ok ol om b">model.fit()</code>方法中，这个超参数由<strong class="lb iu"> steps_per_epoch </strong>参数指定。它的默认值是<code class="fe oj ok ol om b">None</code>，这意味着算法自动使用使用上述等式计算的值。</p><p id="17f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，如果我们为这个参数指定一个值，将会覆盖默认值。</p><blockquote class="pc pd pe"><p id="3667" class="kz la lv lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>要通过例子了解更多关于批量大小、时期和训练步骤之间的联系，请阅读我的<strong class="lb iu"><em class="it"/></strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a" rel="noopener"><strong class="lb iu"><em class="it">所有你需要知道的关于神经网络中批量大小、时期和训练步骤的文章</em></strong></a><strong class="lb iu"><em class="it"/></strong>。</p></blockquote><h1 id="cefb" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">应用正则化效果的超参数</h1><p id="e85e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">根据该标准分类的超参数直接控制神经网络中的过拟合。</p><p id="b6d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不打算像以前在我的其他文章中所做的那样，在这里详细讨论每个超参数。链接到以前发表的文章将被包括在内。</p><h2 id="e2c6" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">L1和L2正则化中的λ-λ</h2><p id="d20a" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">λ是控制L1和L2正则化级别的正则化参数(因子)。λ的特殊值为:</p><ul class=""><li id="64a6" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><code class="fe oj ok ol om b">lambda=0</code>:不适用正规化</li><li id="873b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe oj ok ol om b">lambda=1</code>:应用完全正则化</li><li id="7979" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe oj ok ol om b">lambda=0.01</code> : Keras默认</li></ul><p id="f31a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">λ可以取0到1之间的任何值(包括0和1)。</p><p id="d0ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要详细了解这个超参数以及李和的正则化技术，请阅读我的文章<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/how-to-apply-l1-and-l2-regularization-techniques-to-keras-models-da6249d8a469" rel="noopener"><strong class="lb iu"><em class="lv">【如何将和的正则化技术应用于Keras模型</em></strong></a><strong class="lb iu"><em class="lv"/></strong>。</p><h2 id="dde4" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">辍学调整中的辍学率</h2><p id="99e9" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">该超参数定义了丢失正则化中的丢失概率(要从网络中移除的节点比例)。两个特殊值是:</p><ul class=""><li id="85e1" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><code class="fe oj ok ol om b">rate=0</code>:不应用辍学调整</li><li id="3dba" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><code class="fe oj ok ol om b">rate=1</code>:从网络中删除所有节点(不实用)</li></ul><p id="d57a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">辍学率可以取0到1之间的任何值。</p><p id="d18b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解关于这个超参数以及辍学正则化的更多信息，请阅读我的<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/how-dropout-regularization-mitigates-overfitting-in-neural-networks-9dcc3e7102ff" rel="noopener"><strong class="lb iu"><em class="lv">辍学正则化如何减轻神经网络中的过度拟合</em></strong></a><strong class="lb iu"><em class="lv"/></strong>文章。</p><h1 id="ce82" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">摘要</h1><p id="eadc" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">术语<strong class="lb iu"> <em class="lv">超参数</em> </strong>在ML和DL中是一个非常重要的概念。对于DL中的给定任务，神经网络架构的类型也是超参数。例如，我们可以使用MLP或CNN架构来分类手写数字。这里，在MLP和CNN之间选择是一种设置超参数的类型！</p><p id="e07c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于给定的神经网络架构，存在上述超参数。注意，正则化超参数是可选的。</p><p id="5981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Keras中，一些超参数可以通过函数中的相关参数作为层或字符串标识符添加。</p><p id="3b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，可以通过以下方式之一将ReLU激活功能添加到层中。</p><pre class="kj kk kl km gt ou om ov ow aw ox bi"><span id="b925" class="ni ml it om b gy oy oz l pa pb"># As a string identifier<br/>model.add(Dense(100, activation='relu'))</span></pre></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><pre class="ou om ov ow aw ox bi"><span id="513b" class="ni ml it om b gy pp pq pr ps pt oz l pa pb"># As a layer<br/>from tensorflow.keras.layers import ReLU<br/>model.add(Dense(100))<br/>model.add(ReLU())</span></pre><p id="606d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个选项给出了相同的结果。当我们想要在激活功能之前添加任何其他层(例如<a class="ae ky" href="https://rukshanpramoditha.medium.com/batch-normalization-explained-in-plain-english-3436014f9e52" rel="noopener">批量归一化层</a>)时，第二个选项是理想的选择。</p><p id="355a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个例子是，优化器可以被定义为字符串标识符或函数本身。</p><pre class="kj kk kl km gt ou om ov ow aw ox bi"><span id="5560" class="ni ml it om b gy oy oz l pa pb"># As a string identifier<br/>model.compile(<!-- -->optimizer="rmsprop",...<!-- -->)</span></pre><p id="6ffb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们将RMSProp优化器定义为字符串标识符时，默认的学习率0.001将应用于该算法。没有办法改变。但是，我们经常需要在训练过程中改变学习率的值来获得最佳结果。为此，我们可以使用以下选项。</p><pre class="kj kk kl km gt ou om ov ow aw ox bi"><span id="53db" class="ni ml it om b gy oy oz l pa pb"># As a function itself<br/>from tensorflow.keras.optimizers import RMSprop<br/>model.compile(<!-- -->optimizer=<!-- -->RMSprop(<!-- -->learning_rate=0.005<!-- -->)<!-- -->,...<!-- -->)</span></pre><p id="4634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，优化器被定义为RMSprop()函数。期望的学习速率可以被定义为该函数中的一个自变量。</p><p id="fe8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些都是新手经常混淆的东西。希望你今天拿到了这些分！</p></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><p id="7a3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="5a65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有任何问题或反馈，请告诉我。</p><h2 id="97eb" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">阅读下一篇(推荐)</h2><ul class=""><li id="d712" class="lw lx it lb b lc nc lf nd li pu lm pv lq pw lu mb mc md me bi translated"><strong class="lb iu">参数Vs超参数:有什么区别？</strong></li></ul><div class="px py gp gr pz qa"><a href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener follow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd iu gy z fp qf fr fs qg fu fw is bi translated">参数Vs超参数:区别是什么？</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">用4个不同的例子进行讨论</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qj l"><div class="qk l ql qm qn qj qo ks qa"/></div></div></a></div><ul class=""><li id="438d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">所有你需要知道的关于批量大小、时期和神经网络中的训练步骤</strong></li></ul><div class="px py gp gr pz qa"><a href="https://rukshanpramoditha.medium.com/all-you-need-to-know-about-batch-size-epochs-and-training-steps-in-a-neural-network-f592e12cdb0a" rel="noopener follow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd iu gy z fp qf fr fs qg fu fw is bi translated">你所需要知道的关于批量大小、时期和神经网络中的训练步骤</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">并用简单的英语举例说明它们之间的联系</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qj l"><div class="qp l ql qm qn qj qo ks qa"/></div></div></a></div><ul class=""><li id="de7a" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">如何为神经网络选择正确的激活函数</strong></li></ul><div class="px py gp gr pz qa"><a rel="noopener follow" target="_blank" href="/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd iu gy z fp qf fr fs qg fu fw is bi translated">如何为神经网络选择合适的激活函数</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">用视觉表征分析不同类型的激活功能——神经网络和深度学习…</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">towardsdatascience.com</p></div></div><div class="qj l"><div class="qq l ql qm qn qj qo ks qa"/></div></div></a></div><ul class=""><li id="e0ba" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><a class="ae ky" href="https://rukshanpramoditha.medium.com/list/regularization-techniques-for-neural-networks-c4ad21cce618" rel="noopener"><strong class="lb iu"/></a><strong class="lb iu">(文章收藏)</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/regularization-techniques-for-neural-networks-c4ad21cce618"><div class="gh gi qr"><img src="../Images/db134bf93dcc9c099e20a81d47151051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*cckwsYAN75tJNxZiGw71gg.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure><ul class=""><li id="fdfd" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><strong class="lb iu">全集我的</strong> <a class="ae ky" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener"> <strong class="lb iu">神经网络与深度学习教程</strong></a><strong class="lb iu"/></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75"><div class="gh gi qs"><img src="../Images/518f1f6e5364c7b78c8a8752b2920c8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*sGA5ssW5E3DsykOmRTS8yg.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><h2 id="58dd" class="ni ml it bd mm nj nk dn mq nl nm dp mu li nn no mw lm np nq my lq nr ns na nt bi translated">支持我当作家</h2><p id="7412" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">注册成为会员</em> </strong> </a> <em class="lv">以获得无限制的媒体访问权限。它只需要每月5美元，我会收到你的会员费的一部分。</em></p><div class="px py gp gr pz qa"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="qb ab fo"><div class="qc ab qd cl cj qe"><h2 class="bd iu gy z fp qf fr fs qg fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="qh l"><h3 class="bd b gy z fp qf fr fs qg fu fw dk translated">阅读Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qi l"><p class="bd b dl z fp qf fr fs qg fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qj l"><div class="qt l ql qm qn qj qo ks qa"/></div></div></a></div><p id="854a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p><p id="fbc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="qu qv ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----c7991b6937c3--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lb iu">2022–09–16</strong></p></div></div>    
</body>
</html>