<html>
<head>
<title>A Brief Introduction to Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-brief-introduction-to-recurrent-neural-networks-638f64a61ff4#2022-12-26">https://towardsdatascience.com/a-brief-introduction-to-recurrent-neural-networks-638f64a61ff4#2022-12-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2c42" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">RNN、LSTM 和 GRU 及其实施情况介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/dcec9f24d6ef6f9eb203a9dd0492c61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0q2ZLsUUw31eEImeVf3PQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">RNN、LSTM 和 GRU 的牢房。</p></figure><p id="8ac0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您想要对序列或时间序列数据(例如，文本、音频等)进行预测。)传统的神经网络是一个糟糕的选择。但是为什么呢？</p><p id="3a3b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在时间序列数据中，当前的观测值依赖于以前的观测值，因此观测值不是相互独立的。然而，传统的神经网络认为每个观察都是独立的，因为网络不能保留过去或历史信息。基本上，他们对过去发生的事情没有记忆。</p><p id="0210" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这导致了递归神经网络(RNNs)的兴起，它通过包括数据点之间的依赖性，将记忆的概念引入神经网络。这样，可以训练 rnn 基于上下文记住概念，即学习重复的模式。</p><p id="f104" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是 RNN 人是如何获得这种记忆的呢？</p><p id="b944" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">RNNs 通过细胞中的反馈回路实现记忆。这是 RNN 和传统神经网络的主要区别。与信息仅在层间传递的前馈神经网络相比，反馈回路允许信息在层内传递。</p><p id="8960" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，rnn 必须定义哪些信息足够相关，可以保存在存储器中。为此，不同类型的 RNN 进化而来:</p><ul class=""><li id="8f04" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">传统递归神经网络(RNN)</li><li id="f844" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">长短期记忆递归神经网络(LSTM)</li><li id="53a6" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">门控递归单元递归神经网络(GRU)</li></ul><p id="5b47" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我将向你介绍 RNN、LSTM 和格鲁。我将向你展示他们的相同点和不同点以及一些优点和缺点。除了理论基础，我还向您展示了如何使用<code class="fe mf mg mh mi b">tensorflow</code>在 Python 中实现每种方法。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="5cda" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">递归神经网络(RNN)</h1><p id="c2f9" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">通过反馈回路，一个 RNN 单元的输出也被该单元用作输入。因此，每个单元有两个输入:过去和现在。利用过去的信息会产生短期记忆。</p><p id="e4c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了更好地理解，我们展开 RNN 单元的反馈回路。展开单元的长度等于输入序列的时间步长数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/984a7f6a080e033cc79a2e79afedc208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iP_ahgzkiMNu2hPYhkjlXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开的递归神经网络。</p></figure><p id="c4d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以看到过去的观察是如何通过展开的网络作为隐藏状态传递的。在每个单元中，当前时间步长的输入<strong class="kx ir"> <em class="no"> x </em> </strong>(当前值)、先前时间步长的隐藏状态<strong class="kx ir"> <em class="no"> h </em> </strong>(过去值)和偏差被组合，然后被激活函数限制以确定当前时间步长的隐藏状态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5dfdd5541dc8ae5270d46b4cf9f6b737.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*0gFfVWuKCjYaYE5_oFy9EQ@2x.png"/></div></figure><p id="b8b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，小而粗的字母代表向量，大写的粗字母代表矩阵。</p><p id="b4d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过时间反向传播(BPTT)算法更新 RNN 的权重<strong class="kx ir"> <em class="no"> W </em> </strong>。</p><p id="e9f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">rnn 可用于一对一、一对多、多对一和多对多预测。</p><h2 id="cddc" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">RNNs 的优势</h2><p id="ec3b" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">由于它们的短期记忆，rnn 可以处理顺序数据并识别历史数据中的模式。此外，rnn 能够处理不同长度的输入。</p><h2 id="5d76" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">RNNs 的缺点</h2><p id="d878" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">RNN 遭受消失梯度下降。在这种情况下，用于在反向传播期间更新权重的梯度变得非常小。将权重与接近零的梯度相乘可以防止网络学习新的权重。这种学习的停止导致 RNN 人忘记了在更长的序列中看到的东西。消失梯度下降的问题增加了网络的层数。</p><p id="c81d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于 RNN 只保留了最近的信息，这个模型在考虑很久以前的观测数据时存在问题。因此，RNN 倾向于在长序列上丢失信息，因为它只存储最新的信息。因此，RNN 只有短期记忆，而没有长期记忆。</p><p id="ebdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，由于 RNN 及时使用反向传播来更新权重，所以网络还遭受爆炸梯度，并且如果使用 ReLu 激活函数，还遭受死 ReLu 单元。前者可能会导致收敛问题，而后者可能会停止学习。</p><h2 id="875f" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">RNNs 在 tensorflow 中的实现</h2><p id="92bd" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">我们可以使用<code class="fe mf mg mh mi b">tensorflow</code>在 Python 中轻松实现 RNN。为此，我们使用<code class="fe mf mg mh mi b">Sequential</code>模型，它允许我们堆叠 RNN 的层，即<code class="fe mf mg mh mi b">SimpleRNN</code>层类和<code class="fe mf mg mh mi b">Dense</code>层类。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="1a59" class="og mr iq mi b be oh oi l oj ok">from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import SimpleRNN, Dense<br/>from tensorflow.keras.optimizers import Adam</span></pre><p id="06e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">只要我们想使用缺省参数，就没有必要导入优化器。然而，如果我们想要定制优化器的任何参数，我们也需要导入优化器。</p><p id="a667" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了构建网络，我们定义了一个<code class="fe mf mg mh mi b">Sequential</code>模型，然后使用<code class="fe mf mg mh mi b">add()</code>方法添加 RNN 层。要添加一个 RNN 层，我们使用<code class="fe mf mg mh mi b">SimpleRNN</code>类并传递参数，比如单元数量、退出率或激活函数。对于我们的第一层，我们也可以传递输入序列的形状。</p><p id="c865" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们堆叠 RNN 层，我们需要设置前一层的<code class="fe mf mg mh mi b">return_sequence</code>参数为<code class="fe mf mg mh mi b">True</code>。这可确保图层的输出对于下一个 RNN 图层具有正确的格式。</p><p id="1769" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了生成一个输出，我们使用一个<code class="fe mf mg mh mi b">Dense</code>层作为我们的最后一层，传递输出的数量。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="81ec" class="og mr iq mi b be oh oi l oj ok"># define parameters<br/>n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]<br/><br/># define model<br/>rnn_model = Sequential()<br/>rnn_model.add(SimpleRNN(130, dropout=0.2, return_sequences=True, input_shape=(n_timesteps, n_features)))<br/>rnn_model.add(SimpleRNN(110, dropout=0.2, activation="tanh", return_sequences=True))<br/>rnn_model.add(SimpleRNN(130, dropout=0.2, activation="tanh", return_sequences=True))<br/>rnn_model.add(SimpleRNN(100, dropout=0.2, activation="sigmoid", return_sequences=True))<br/>rnn_model.add(SimpleRNN(40, dropout=0.3, activation="tanh"))<br/>rnn_model.add(Dense(n_outputs))<br/></span></pre><p id="0cb7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们定义了我们的 RNN 之后，我们可以使用<code class="fe mf mg mh mi b">compile()</code>方法编译这个模型。这里，我们传递损失函数和我们想要使用的优化器。<code class="fe mf mg mh mi b">tensorflow</code>提供了一些内置的<a class="ae ol" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" rel="noopener ugc nofollow" target="_blank">损失函数</a>和<a class="ae ol" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers" rel="noopener ugc nofollow" target="_blank">优化器</a>。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="425d" class="og mr iq mi b be oh oi l oj ok">rnn_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))</span></pre><p id="bf44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们训练 RNN 之前，我们可以使用<code class="fe mf mg mh mi b">summary()</code>方法来看看模型和参数的数量。这可以让我们对模型的复杂性有一个大概的了解。</p><p id="8ab3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们使用<code class="fe mf mg mh mi b"><a class="ae ol" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit" rel="noopener ugc nofollow" target="_blank">fit()</a></code>方法训练模型。这里，我们需要传递训练数据和不同的参数来定制训练，包括时期数、批量大小、验证分割和提前停止。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="276c" class="og mr iq mi b be oh oi l oj ok">stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)<br/>rnn_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[stop_early])</span></pre><p id="d291" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了对我们的测试数据集或任何未知数据进行预测，我们可以使用<code class="fe mf mg mh mi b">predict()</code>方法。<code class="fe mf mg mh mi b">verbose</code>参数只是说明我们是否想要获得任何关于预测过程状态的信息。在这种情况下，我不想打印出任何状态。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="7f49" class="og mr iq mi b be oh oi l oj ok">y_pred = rnn_model.predict(X_test, verbose=0)</span></pre><h2 id="3427" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">张量流中 RNNs 的超参数调谐</h2><p id="d98c" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">正如我们所见，RNN 的实现非常简单。然而，找到正确的超参数，如每层的单元数、辍学率或激活函数，要困难得多。</p><p id="418d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是我们可以使用<code class="fe mf mg mh mi b">keras-tuner</code>库，而不是手动改变超参数。该库有四个调谐器，<code class="fe mf mg mh mi b">RandomSearch</code>、<code class="fe mf mg mh mi b">Hyperband</code>、<code class="fe mf mg mh mi b">BayesianOptimization</code>和<code class="fe mf mg mh mi b">Sklearn</code>，用于从给定的搜索空间中识别正确的超参数组合。</p><p id="16cc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要运行调谐器，我们首先需要导入<code class="fe mf mg mh mi b">tensorflow</code>和 Keras 调谐器。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="41da" class="og mr iq mi b be oh oi l oj ok">import tensorflow as tf<br/>import keras_tuner as kt</span></pre><p id="8a81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们建立超调模型，其中我们定义了超参数搜索空间。我们可以使用一个函数来构建超级模型，在这个函数中，我们以与上述相同的方式来构建模型。唯一的区别是，我们为每个想要调优的超参数添加了搜索空间。在下面的示例中，我想要调整每个 RNN 图层的单位数、激活函数和辍学率。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="d4c9" class="og mr iq mi b be oh oi l oj ok">def build_RNN_model(hp):<br/>    <br/>    # define parameters<br/>    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]<br/><br/>    # define model<br/>    model = Sequential()<br/><br/>    model.add(SimpleRNN(hp.Int('input_unit',min_value=50,max_value=150,step=20), return_sequences=True, dropout=hp.Float('in_dropout',min_value=0,max_value=.5,step=0.1), input_shape=(n_timesteps, n_features)))<br/>    model.add(SimpleRNN(hp.Int('layer 1',min_value=50,max_value=150,step=20), activation=hp.Choice("l1_activation", values=["tanh", "relu", "sigmoid"]), dropout=hp.Float('l1_dropout',min_value=0,max_value=.5,step=0.1), return_sequences=True))<br/>    model.add(SimpleRNN(hp.Int('layer 2',min_value=50,max_value=150,step=20), activation=hp.Choice("l2_activation", values=["tanh", "relu", "sigmoid"]), dropout=hp.Float('l2_dropout',min_value=0,max_value=.5,step=0.1), return_sequences=True))<br/>    model.add(SimpleRNN(hp.Int('layer 3',min_value=20,max_value=150,step=20), activation=hp.Choice("l3_activation", values=["tanh", "relu", "sigmoid"]), dropout=hp.Float('l3_dropout',min_value=0,max_value=.5,step=0.1), return_sequences=True))<br/>    model.add(SimpleRNN(hp.Int('layer 4',min_value=20,max_value=150,step=20), activation=hp.Choice("l4_activation", values=["tanh", "relu", "sigmoid"]), dropout=hp.Float('l4_dropout',min_value=0,max_value=.5,step=0.1)))<br/><br/>    # output layer<br/>    model.add(Dense(n_outputs))<br/><br/>    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=1e-3))<br/><br/>    return model</span></pre><p id="6a1e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了定义每个变量的搜索空间，我们可以使用不同的方法，如<code class="fe mf mg mh mi b">hp.Int</code>、<code class="fe mf mg mh mi b">hp.Float</code>和<code class="fe mf mg mh mi b">hp.Choice</code>。前两者的用法非常相似。我们给它们一个名字，一个最小值，一个最大值和一个步长。该名称用于识别超参数，而最小值和最大值定义了我们的值范围。步长参数定义了我们用于调整的范围内的值。<code class="fe mf mg mh mi b">hp.Choice</code>可用于调整分类超参数，如激活函数。在这里，我们只需要传递一个我们想要测试的选项列表。</p><p id="f605" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们构建了超模型之后，我们需要实例化调谐器并执行超调谐。尽管我们可以选择不同的算法进行调优，但它们的实例化非常相似。我们通常需要指定要优化的目标和要训练的最大历元数。这里，建议将时期设置为比我们预期的时期数稍高的数字，然后使用提前停止。</p><p id="2c33" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，如果我们想要使用<code class="fe mf mg mh mi b">Hyperband</code>调谐器和验证损耗作为目标，我们可以将调谐器构建为</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="f8e2" class="og mr iq mi b be oh oi l oj ok">tuner = kt.Hyperband(build_RNN_model,<br/>                     objective="val_loss",<br/>                     max_epochs=100,<br/>                     factor=3,<br/>                     hyperband_iterations=5,<br/>                     directory='kt_dir',<br/>                     project_name='rnn',<br/>                     overwrite=True)</span></pre><p id="a137" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我还传递了存储结果的目录，以及调谐器迭代整个 Hyperband 算法的频率。</p><p id="7d8f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们实例化了调谐器之后，我们可以使用<code class="fe mf mg mh mi b">search()</code>方法来执行超参数调谐。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="3a2f" class="og mr iq mi b be oh oi l oj ok">stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)<br/>tuner.search(X_train, y_train, validation_split=0.2, callbacks=[stop_early])</span></pre><p id="cfdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了提取最佳超参数，我们可以使用<code class="fe mf mg mh mi b">get_best_hyperparameters()</code>方法和<code class="fe mf mg mh mi b">get()</code>方法以及我们调整的每个超参数的名称。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="8ded" class="og mr iq mi b be oh oi l oj ok">best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]<br/>print(f"input: {best_hps.get('input_unit')}")<br/>print(f"input dropout: {best_hps.get('in_dropout')}")</span></pre></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="5912" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">长短期记忆(LSTM)</h1><p id="bab3" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">LSTMs 是一种特殊类型的 rnn，其解决了简单 rnn 的主要问题，即消失梯度的问题，即丢失更远过去的信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/d948d53b0d9b835028ebb7a37f394a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEN1Ziu4VvRAaH9zagN3EQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开的长短期记忆细胞。</p></figure><p id="9e7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">LSTMs 的关键是<strong class="kx ir">单元状态</strong>，它从单元的输入传递到输出。因此，单元状态允许信息沿着整个链流动，仅通过三个门进行较小的线性动作。因此，细胞状态代表了 LSTM 的长期记忆。这三个门被称为遗忘门、输入门和输出门。这些门起着过滤器的作用，控制信息的流动，并决定哪些信息被保留或忽略。</p><p id="53a4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">遗忘门</strong>决定要保留多少长期记忆。为此，使用 sigmoid 函数来说明单元状态的重要性。输出在 0 和 1 之间变化，并表示保留了多少信息，即 0 表示不保留信息，1 表示保留单元状态的所有信息。输出由当前输入<strong class="kx ir"> <em class="no"> x </em> </strong>、前一时间步的隐藏状态<strong class="kx ir"> <em class="no"> h </em> </strong>和 a 偏置<strong class="kx ir"> <em class="no"> b </em> </strong>共同决定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5c25e058c39b6f0e5339829c6dbd9834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*N4lZYpRk_L3gB1JkHRp0mw@2x.png"/></div></figure><p id="4040" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">输入门</strong>决定将哪些信息添加到单元状态，从而添加到长期记忆中。这里，sigmoid 图层决定更新哪些值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6f0b081f43561a47f35eab6c413a13e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Sa9nypGQBuIdZysUjupbWw@2x.png"/></div></figure><p id="dd80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">输出门</strong>决定单元状态的哪些部分建立输出。因此，输出门负责短期记忆。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/f8fbf880912cfdb45e6e18eef9c69cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*-VPhIMgeD14UR5CKXcYg6Q@2x.png"/></div></figure><p id="5a3b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可以看出，所有三个门都由相同的函数表示。只有权重和偏差不同。单元状态通过遗忘门和输入门更新。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/b793671bb94d2ca4d915fe8a761938a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*xvPCan06sqdUZpG_c6PuZg@2x.png"/></div></figure><p id="d432" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述等式中的第一项决定了保留多少长期记忆，而第二项向细胞状态添加新信息。</p><p id="1425" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，当前时间步长的隐藏状态由输出门和一个双曲正切函数决定，该函数将单元状态限制在-1 和 1 之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/535b2ff5e62936babca85a4df77c7a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*cVpDyafKJWkFMPYCWpFxGg@2x.png"/></div></figure><h2 id="d21b" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">LSTMs 的优势</h2><p id="a659" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">LSTM 的优势类似于 RNNs，主要好处是它们可以捕获序列的长期和短期模式。因此，它们是最常用的 rnn。</p><h2 id="41f4" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">LSTMs 的缺点</h2><p id="329a" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">由于其更复杂的结构，LSTMs 在计算上更昂贵，导致更长的训练时间。</p><p id="c2a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为 LSTM 也使用时间反向传播算法来更新权重，所以 LSTM 遭受反向传播的缺点(例如，死 ReLu 元素、爆炸梯度)。</p><h2 id="f391" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">张量流中 LSTMs 的实现</h2><p id="0457" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated"><code class="fe mf mg mh mi b">tensorflow</code>中 LSTMs 的实现非常类似于一个简单的 RNN。唯一的区别是我们导入了<code class="fe mf mg mh mi b">LSTM</code>类，而不是<code class="fe mf mg mh mi b">SimpleRNN</code>类。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="cea9" class="og mr iq mi b be oh oi l oj ok">from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import LSTM, Dense<br/>from tensorflow.keras.optimizers import Adam</span></pre><p id="c698" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以用简单的 RNN 的方法来组合 LSTM 网络。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="b3d1" class="og mr iq mi b be oh oi l oj ok"># define parameters<br/>n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]<br/><br/># define model<br/>lstm_model = Sequential()<br/>lstm_model.add(LSTM(130, return_sequences=True, dropout=0.2, input_shape=(n_timesteps, n_features)))<br/>lstm_model.add(LSTM(70, activation="relu", dropout=0.1, return_sequences=True))<br/>lstm_model.add(LSTM(100, activation="tanh", dropout=0))<br/><br/># output layer<br/>lstm_model.add(Dense(n_outputs, activation="tanh"))<br/><br/>lstm_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))<br/><br/>stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)<br/>lstm_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[stop_early])</span></pre><p id="5be7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">超参数调谐也与简单 RNN 相同。因此，我们只需要对我上面展示的代码片段做一些小的改动。</p><h1 id="81cb" class="mq mr iq bd ms mt os mv mw mx ot mz na jw ou jx nc jz ov ka ne kc ow kd ng nh bi translated">门控循环单元(GRU)</h1><p id="5346" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">类似于 LSTMs，GRU 解决了简单 rnn 的消失梯度问题。然而，与 LSTMs 的区别在于 gru 使用较少的门，并且没有单独的内部存储器，即单元状态。因此，GRU 仅仅依靠隐藏状态作为记忆，导致更简单的架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/130382c50b788bc30e7eb26c7154c843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ldMy6GqBy8D25uNKQl2gA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开门控循环单元(GRU)。</p></figure><p id="f73a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">重置门</strong>负责短期记忆，因为它决定保留和忽略多少过去的信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/624e0ffb01719207efe82717f480ad67.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*O2Ww0OYtvzu73_aVzY3sHQ@2x.png"/></div></figure><p id="7c83" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">向量<strong class="kx ir"> <em class="no"> r </em> </strong>中的值由 sigmoid 函数限制在 0 和 1 之间，并取决于前一时间步的隐藏状态<strong class="kx ir"> <em class="no"> h </em> </strong>和当前输入<strong class="kx ir"> <em class="no"> x </em> </strong>。使用权重矩阵<strong class="kx ir"> <em class="no"> W </em> </strong>对二者进行加权。此外，还增加了一个偏差<strong class="kx ir"> <em class="no"> b </em> </strong>。</p><p id="120f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">相比之下，<strong class="kx ir">更新门</strong>负责长期记忆，与 LSTM 的遗忘门相当。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2551d56d2cfb0653a98232407cbeb7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*EqdGgbMbN025YuUPHBJhCQ@2x.png"/></div></figure><p id="ec79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们所看到的，复位和更新门之间的唯一区别是权重<strong class="kx ir"> <em class="no"> W </em> </strong>。</p><p id="1138" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当前时间步长的隐藏状态是基于两步过程确定的。首先，确定候选隐藏状态。候选状态是当前输入和前一时间步的隐藏状态以及激活函数的组合。在这个例子中，使用了一个双曲正切函数。前一隐藏状态对候选隐藏状态的影响由复位门控制</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4702730a571ff4ddb1ba79ad56b6d81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*a6nQGmaRuh3UxBsa9Z0SZQ@2x.png"/></div></figure><p id="e6e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第二步，将候选隐藏状态与前一时间步的隐藏状态相结合，生成当前隐藏状态。如何组合先前隐藏状态和候选隐藏状态由更新门决定。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/5a9e66c39fe6e7b67aa5eb38937e1d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*anOgDAO4gKeNVlf5G9HAaw@2x.png"/></div></figure><p id="8993" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果更新门给出值 0，则完全忽略先前的隐藏状态，并且当前隐藏状态等于候选隐藏状态。如果更新门给出的值为 1，反之亦然。</p><h2 id="702d" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">GRUs 的优势</h2><p id="51f7" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">由于与 LSTMs 相比更简单的架构(即，两个而不是三个门，一个状态而不是两个)，gru 在计算上更有效并且训练更快，因为它们需要更少的存储器。</p><p id="23ca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，GRUs haven 被证明对于较小的序列更有效。</p><h2 id="09df" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">GRUs 的缺点</h2><p id="e624" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">由于 GRUs 没有单独的隐藏细胞状态，它们可能无法考虑远至 LSTM 的观测结果。</p><p id="8ba9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">类似于 RNN 和 LSTM，GRU 也可能遭受及时反向传播以更新权重的缺点，即，死 ReLu 元素、爆炸梯度。</p><h2 id="d590" class="nq mr iq bd ms nr ns dn mw nt nu dp na le nv nw nc li nx ny ne lm nz oa ng ob bi translated">张量流中 GRUs 的实现</h2><p id="9575" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">至于 LSTM，GRU 的实现非常类似于简单的 RNN。我们只需要导入<code class="fe mf mg mh mi b">GRU</code>类，其余的保持不变。</p><pre class="kg kh ki kj gt oc mi od bn oe of bi"><span id="90e6" class="og mr iq mi b be oh oi l oj ok">from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import GRU, Dense<br/>from tensorflow.keras.optimizers import Adam<br/><br/># define parameters<br/>n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]<br/><br/># define model<br/>gru_model = Sequential()<br/>gru_model.add(GRU(90,return_sequences=True, dropout=0.2, input_shape=(n_timesteps, n_features)))<br/>gru_model.add(GRU(150, activation="tanh", dropout=0.2, return_sequences=True))<br/>gru_model.add(GRU(60, activation="relu", dropout=0.5))<br/>gru_model.add(Dense(n_outputs))<br/><br/>gru_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))<br/><br/>stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)<br/>gru_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[stop_early])</span></pre><p id="2a37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这同样适用于超参数调谐。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="cb29" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">结论</h1><p id="8b3f" class="pw-post-body-paragraph kv kw iq kx b ky ni jr la lb nj ju ld le nk lg lh li nl lk ll lm nm lo lp lq ij bi translated">递归神经网络将记忆引入神经网络。这样，序列和时间序列数据中观测值的相关性就包含在我们的预测中。</p><p id="57ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我向您展示了三种类型的递归神经网络，即简单的 RNN、LSTM 和格鲁。我向您展示了它们的工作原理、优缺点以及如何使用<code class="fe mf mg mh mi b">tensorflow</code>在 Python 中实现它们。</p><p id="8dcd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请让我知道你对这篇文章的看法！</p><p id="744c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">*除非另有说明，所有图片均为作者所有。</p></div></div>    
</body>
</html>