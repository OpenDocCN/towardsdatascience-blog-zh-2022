<html>
<head>
<title>Choosing Neural Networks over N-Gram Models for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为自然语言处理选择神经网络而不是N-Gram模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/choosing-neural-networks-over-n-gram-models-for-natural-language-processing-156ea3a57fc#2022-07-06">https://towardsdatascience.com/choosing-neural-networks-over-n-gram-models-for-natural-language-processing-156ea3a57fc#2022-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="84de" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">今天，我们将通过一个利用LSTMs分析金融科技数据的例子，来看看在N元模型上使用递归神经网络、门控递归单元和LSTMS的优势</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6c7393cb90ba873a55d871d93f4fd0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YO6Ys4hpfs5O0D0L"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔希·里默尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2bdd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">为什么是神经网络？传统的学习模式需要大量的空间和内存！！！</strong></h1><p id="b67a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">传统的学习模型将文本从一种语言转换成另一种语言。为了准确翻译，您将使用一个<em class="mn"> N-Gram </em>语言模型来计算一个句子的概率。一个限制是捕捉相距较远的单词之间的差异。这需要一个大的语料库，意味着更多的空间和内存。与<em class="mn"> N- </em> Grams相比，递归神经网络(RNNs)和门控递归单元(GRUs)对于机器翻译更有效，因为它们可以使用过去记忆的概念和结合来预测单词是什么，或者文本语料库的情感。虽然神经网络可能更适合您的分析，但传统的学习模型仍然有效，至少会为您的工作提供一个起点。</p><h1 id="4e20" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">普通递归神经网络(RNN) </strong></h1><p id="e72c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">rnn有能力捕捉传统语言模型没有捕捉到的依赖关系。RNN将信息从句子的开头一直传播到结尾。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/1d216dbff9bbe4e4ce41a0b2f65cb3cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AX2mvFvL3NWhU3tE13idMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本的RNN结构(图片来自作者)</p></figure><p id="bbf8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">如上所示，与旧信息相比，最新信息在RNN的给定步骤中具有更高的影响。通过训练RNN更新的参数有<strong class="lt iu"> <em class="mn"> Wx </em>、<em class="mn"> Wh </em>、</strong>和<strong class="lt iu"> <em class="mn"> W </em> </strong>。</p><h2 id="748f" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">优势</h2><ul class=""><li id="bc48" class="ng nh it lt b lu lv lx ly ma ni me nj mi nk mm nl nm nn no bi translated">在序列之间传播信息</li><li id="ad59" class="ng nh it lt b lu np lx nq ma nr me ns mi nt mm nl nm nn no bi translated">计算共享相同的参数</li></ul><h2 id="cabb" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">数学</h2><p id="ba06" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">RNN的第一个计算是隐藏状态，它通过激活函数做矩阵乘法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ca6df8d9ce8cd4ab666f16b070e48acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*auVhQLcfY85sXZhoU2YkIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隐藏状态计算(图片来自作者)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/6f6973963971b6c49b5b8be21132c11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHsDBilgTBSzzVV8cAtXaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RNN中的计算顺序(图片来自作者)</p></figure><p id="ff17" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">在计算出下一个时间步的隐藏状态后，状态乘以另一个参数矩阵<strong class="lt iu"> <em class="mn"> Wyh </em> </strong>并通过一个激活函数，得到该时间步的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b6e40316a10ffa4308646563369fb8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vWPBFQnOWlzUayN-O68Z5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测计算(图片来自作者)</p></figure><h2 id="0856" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">价值函数</h2><p id="4172" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">RNNs可以利用交叉熵损失进行训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2f76fcec12610f5f55068b48163b41e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*ygj9NhAcW6PvyZpS_TOYHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉熵损失(图片来自作者)</p></figure><p id="cd1d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">RNN的交叉熵损失的计算略有不同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ae4b8a6d0ced4a06a405af82f3a62b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*k1EO_8z1W5qtmt_rB_-3Cw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RNN交叉熵损失(图片来自作者)</p></figure><ul class=""><li id="e66e" class="ng nh it lt b lu mp lx mq ma nz me oa mi ob mm nl nm nn no bi translated">K →类别数</li><li id="0193" class="ng nh it lt b lu np lx nq ma nr me ns mi nt mm nl nm nn no bi translated">T →时间步长T</li><li id="8476" class="ng nh it lt b lu np lx nq ma nr me ns mi nt mm nl nm nn no bi translated">查看实际值和预测值之间的差异。</li></ul><h1 id="4fa2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">门控循环单位</h1><p id="a83b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">gru不同于rnn，因为它们现在将学习随着时间的推移保存相关信息(可能是词性)</strong>。如果与正在解决的问题相关，长期记忆就不会丢失。这很重要，因为在一个普通的RNN中，长期信息将开始随着时间消失，<strong class="lt iu">导致消失梯度</strong>。基本上，RNN架构增加了一个额外的输入。GRU的架构中有一个<em class="mn">相关性</em>和<em class="mn">更新门</em>。更新门确定应该保留或更新多少以前的信息。gru更加数学化，因此，使用它们的一个缺点是更大的内存和计算成本。</p><h2 id="4048" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">数学</h2><p id="cbf8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如上所述，gru比普通的rnn更加数学密集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a544938f4654eeb01e906c0148c32544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4c9ETyEgffOk3tXwk-JHig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">门控循环单元(图片来自作者)</p></figure><p id="4cd1" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">两个sigmoid激活单元是关联门和更新门。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/d99d0848fe531d5d76213383e3c0e9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tsqPGnd6urEzQCsn8eowjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相关性和更新方程式(图片来自作者)</p></figure><p id="0e25" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">这些门的权重参数将随时间变化，以确定应该保留、更新和从该单元传递什么信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/5d0f48ade17e90b2cca9dd5e3c77962b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EDB-mBt7LwY6spxXD1aag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隐藏状态候选人(图片来自作者)</p></figure><p id="41d3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">隐藏状态候选从关联门和隐藏状态中获取信息进行计算，并最终通过<em class="mn"> tanh </em>激活函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ba2bcdc9cc96cee3a40639cfb2ec8862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJ9yD5G1Q_Zr3NVgV3ou3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隐藏状态(图片来自作者)</p></figure><p id="0fb7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">基于由更新门传递的信息来更新GRU的隐藏状态。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d3046db5402e4f9dd20ece5d7452b5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7AicHvRKGX9FsdW-8rNdKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测(图片来自作者)</p></figure><p id="d102" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">通过最后激活函数的最后一次传递是由GRU做出的预测。</p><h1 id="c55e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">双向RNNs </strong></h1><p id="59ed" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">想象未来流向现在，这基本上是双向RNN所做的。它将有两个方向的信息流，其中每个方向都独立于另一个方向。双向rnn是非循环图，这意味着一个方向的计算独立于另一个方向的计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/7768083ad43d57e4eb7a63953c91d340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4pqK3JoNHdq9D3TFgMTng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM细胞(图片来自作者)</p></figure><p id="28f6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">双向RNN的一个例子是长短期模型(LSTM)。LSTM单元最大的变化是，它现在把状态分成两个向量。在两个新的状态向量中，一个数组表示长期记忆，另一个表示短期记忆。如上面的单元格所示，有许多输入和变量计算与LSTM单元格相关联。<strong class="lt iu"><em class="mn"/></strong>是细胞的短期信息<strong class="lt iu"> <em class="mn"> c(t) </em> </strong>是细胞的长期信息。</p><p id="5241" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">对于长期信息，它将首先通过一个遗忘门来丢弃一些记忆信息，然后通过加法门添加新信息。在添加新的记忆并通过<em class="mn"> tanh </em>激活功能后，短时记忆、<strong class="lt iu">、<em class="mn">、【t】、</em></strong>、<em class="mn">、</em>被创建。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/4b1fb86553385f066162c73cff07e8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-VSGYqJxyGq49BexCkAfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM方程(图片来自作者)</p></figure><p id="3e26" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">上面显示的是LSTM细胞在训练过程中进行的不同计算。主要的要点是，LSTM将更新不同的权重矩阵<strong class="lt iu"><em class="mn"/></strong>以识别给定问题和数据的相关长期和短期记忆。</p><h1 id="bc35" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">例子</h1><p id="bb98" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我想使用Kaggle 的<a class="ae ky" href="https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">金融情绪分析数据集提供一个快速编码示例。我训练了一个LSTM网络来识别金融问题的情绪是积极的还是消极的。今天的代码将帮助您重新创建和训练模型，但请注意，由于模型计算的复杂性和组合性，训练LSTM模型可能需要很长时间。虽然这不同于</a></p><p id="2a28" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">首先，让我们导入库和数据。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="0ecc" class="mu la it oi b gy om on l oo op">from keras.layers import Input,GlobalMaxPooling1D, Dense, Dropout, LSTM, SpatialDropout1D<br/>from keras.models import Model, Sequential<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.optimizers import Adam<br/>from keras.losses import BinaryCrossentropy<br/>from keras.utils import pad_sequences<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>import re</span><span id="f4dc" class="mu la it oi b gy oq on l oo op"><br/>df = pd.read_csv('finance_data.csv')<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/6a59888ff9b5ab800a5dedd2f5c13d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f0inHOQlb0Eao5pKzy16aA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集示例(图片来自作者)</p></figure><p id="573f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">如您所见，数据集是讨论金融部门不同领域的条目的集合。在“情绪”栏中，我们有“积极”、“消极”和“中性”情绪，我们将它们映射为0或1。此外，我们希望预处理我们的句子，并将它们转换成NumPy数组。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="2ee7" class="mu la it oi b gy om on l oo op">df['Sentence'] = df['Sentence'].apply(lambda x: x.lower())<br/>df['Sentence'] = df['Sentence'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))</span><span id="0de8" class="mu la it oi b gy oq on l oo op">mapping = {'negative' :0,<br/>            'neutral' : 1,<br/>            'positive': 1}<br/>df['Sentiment'] = df['Sentiment'].map(mapping)</span><span id="3f5b" class="mu la it oi b gy oq on l oo op">X = df['Sentence'].to_numpy() #turn the sentences into numpy arrays<br/>y = df['Sentiment'].values #Targert sentimen values</span></pre><p id="1532" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">接下来将使用80/20分割创建训练集和测试集。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="c5fa" class="mu la it oi b gy om on l oo op">X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size=0.2)</span></pre><p id="1154" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">接下来，我们要标记我们的句子。标记化是将每个句子分解成单个单词的操作，其中每个单词都是一个标记。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="66a9" class="mu la it oi b gy om on l oo op">vocab_size = 10000<br/>tokenizer = Tokenizer(num_words=vocab_size, oov_token="&lt;OOV&gt;")<br/>tokenizer.fit_on_texts(X_train) #Fit the tokenizer on the training data</span><span id="6fa7" class="mu la it oi b gy oq on l oo op">#We will then fit onto each of the sequences<br/>X_train = tokenizer.texts_to_sequences(X_train)</span><span id="2871" class="mu la it oi b gy oq on l oo op">print(X_trai[0])</span><span id="6a81" class="mu la it oi b gy oq on l oo op">[2, 100, 3, 2, 138, 12, 326, 9, 259, 29]</span></pre><p id="61c3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">在我们转换了我们的序列后，我们将填充它们，使它们都是相同的形状。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="b928" class="mu la it oi b gy om on l oo op">seq_length = 100<br/>X_train = pad_sequences(X_train, maxlen=seq_length, padding='post', truncating='post')</span><span id="675e" class="mu la it oi b gy oq on l oo op">#Now do the same for the test data <br/>X_test = tokenizer.texts_to_sequences(X_test)<br/>X_test = pad_sequences(X_test, maxlen=seq_length, padding='post', truncating='post')</span></pre><p id="7419" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">让我们创建我们的分类模型。该模型将首先使用嵌入层，该嵌入层将是序列向量的嵌入大小(在这种情况下是FinTech概要)。为简单起见，我使用了1个LSTM层，但两个或更多层也可以(这里建议使用进一步的超参数调整以及<em class="mn"> k- </em>折叠交叉验证)。最后，sigmoid激活函数将用于预测输出为0或1。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="1893" class="mu la it oi b gy om on l oo op">embed_dim = 100<br/>batch_size = 32</span><span id="cece" class="mu la it oi b gy oq on l oo op">model = Sequential()<br/>model.add(Embedding(vocab_size, embed_dim, input_length=sequence_length))<br/>model.add(LSTM(units = 100))<br/>model.add(Dense(1,activation='sigmoid'))<br/>model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])<br/>print(model.summary())</span></pre><p id="f86c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">我们的最后一步是训练和执行模型。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="8b09" class="mu la it oi b gy om on l oo op">history = model.fit(X_train, y_train, epochs=100, <br/>                    validation_data=(test_padded, y_test))</span></pre><p id="8e85" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">训练这个模型需要很长时间，但是训练之后，你会有一个模型，可以预测任何FinTeh相关的摘要/评论/语料库的情绪。</p><h1 id="65f9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="5881" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">虽然N-gram模型是预测句子中下一个单词的伟大工具，但神经网络是一个更强大的工具，因为它们可以保留长期信息，在进行NLP分析时不应被忽略。神经网络不仅可以用于n-gram预测，而且今天的例子还显示了用Python创建LSTM网络并在您的数据上实现它是多么简单！</p><p id="24cb" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">如果你喜欢今天的阅读，请关注我，让我知道你是否还有其他想让我探讨的话题(这对我的帮助超乎你的想象)！另外，在<a class="ae ky" href="https://www.linkedin.com/in/benjamin-mccloskey-169975a8/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">LinkedIn</strong></a><strong class="lt iu">上加我，或者随时联系！感谢阅读！</strong></p><h2 id="6884" class="mu la it bd lb mv mw dn lf mx my dp lj ma mz na ll me nb nc ln mi nd ne lp nf bi translated">来源</h2><p id="cbd9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">-盖伦，奥雷连恩。<em class="mn">使用Scikit-Learn、Keras和TensorFlow进行动手机器学习:构建智能系统的概念、工具和技术</em>。第二版。，奥莱利，2019。</p><div class="os ot gp gr ou ov"><a href="https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">金融情绪分析</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">带有情感标签的金融句子</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">www.kaggle.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div><p id="5ca5" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma mr mc md me ms mg mh mi mt mk ml mm im bi translated">-该数据集属于首席运营官公共领域，允许公众使用。</p></div></div>    
</body>
</html>