<html>
<head>
<title>A Quick Way to Check the Linearity of Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检查数据线性的快速方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-way-to-check-the-linearity-of-data-9e9cca23f26b#2022-02-13">https://towardsdatascience.com/a-quick-way-to-check-the-linearity-of-data-9e9cca23f26b#2022-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9f81" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何使用 PCA 来检查数据是线性的还是非线性的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/892cb28772b040126d7100b0b7f90103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkKxbumVo7M5NDo8zbAVvQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9bbd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated">当处理新数据时，首先想到的问题是数据的分布。该分布可以是线性的或非线性的。假设我们想将一个 100 维的特征向量压缩到更少的维度。我们可以使用 PCA 或自动编码器来压缩特征。但是，正如我们所知，主成分分析并不适合非线性数据。另一方面，自动编码器可以模拟非线性数据。因此，在数据压缩之前，了解数据是否是线性的是至关重要的。</p><p id="8b7b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个简短的教程中，我们将看到如何判断数据是否是线性的。利用特征值，我们可以判断数据看起来是线性的还是非线性的。</p><ol class=""><li id="26b3" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated"><strong class="kx ir"> <em class="mj">对于线性数据，前几个特征值会明显变大。其余的值将几乎为零。</em> </strong></li><li id="3c88" class="ma mb iq kx b ky mk lb ml le mm li mn lm mo lq mf mg mh mi bi translated"><strong class="kx ir"> <em class="mj">对于非线性数据，很多主成分会有非零的特征值。特征向量给出了数据最大扩散的方向。现在，如果数据是非线性的，并且不是在单个方向上传播，那么所有的特征向量将具有非零特征值。这是因为没有一个数据传播的大方向。</em>T9】</strong></li></ol><p id="9be2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了证明这一点，我们将创建不同的数据分布并计算它们的 PCA。下面是 python 中的一小段代码，用于演示。</p><p id="4e7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">步骤 1:导入 python 库</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="d7f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第二步:生成线性数据。</strong>我们将随机生成 6 维线性数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="92b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">步骤 3:现在，生成随机非线性数据。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="28d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第四步:</strong> <strong class="kx ir">现在，我们将生成一个 6 维单位超球。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="ef72" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第五步:可视化三种数据分布</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/fe9f50bf460bbfd8ec152c911c5d4c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TK1dOJaqRmS3W6dFKrI9jg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原始数据散点图</p></figure><p id="82ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第六步:计算 PCA，观察特征值，显示主成分。</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/ed3f2d152d89a682363463844dde5cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTMZj_0caMH1b7nr4uoN4w.png"/></div></div></figure><p id="1e5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">现在，观察每种情况下的特征值:</strong></p><ul class=""><li id="a30e" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mt mg mh mi bi translated"><em class="mj">对于线性数据，第一个特征值为 0.51，其余为零。</em></li><li id="ab28" class="ma mb iq kx b ky mk lb ml le mm li mn lm mo lq mt mg mh mi bi translated"><em class="mj">对于随机非线性数据，前 3 个特征值具有有效的非零值，然后是零个特征值。</em></li><li id="9696" class="ma mb iq kx b ky mk lb ml le mm li mn lm mo lq mt mg mh mi bi translated"><em class="mj">对于一个单位超球来说，每个方向上的扩散几乎相等。因此，所有特征值都是非零的，并且具有几乎相等的量值。</em></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/fe9f50bf460bbfd8ec152c911c5d4c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TK1dOJaqRmS3W6dFKrI9jg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原始数据散点图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/f8d66403cd8eda660bcce4d33b0ce99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zB2DFgrLT85qatTlV9wJnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原始数据的主成分分析散点图</p></figure><p id="3225" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">从该图中我们可以看出线性和非线性数据的 PCA 之间的差异。在线性数据的情况下，图中显示的是一条直线。对于 PC1，PC0 表现出很大的差异。在非线性数据的情况下(中间的和右边的)，在水平和垂直方向上都有扩展。</strong></p><p id="d5f6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完整的代码可以在 GitHub <a class="ae mv" href="https://github.com/AdityaDutt/PCALinearityCheck" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">这里</strong> </a>获得。</p><p id="c0c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经演示了如何通过观察特征值和解释 PCA 图来判断数据是线性还是非线性。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><blockquote class="nd ne nf"><p id="a9c2" class="kv kw mj kx b ky kz jr la lb lc ju ld ng lf lg lh nh lj lk ll ni ln lo lp lq ij bi translated">我希望这篇文章对你有用！</p><p id="17cd" class="kv kw mj kx b ky kz jr la lb lc ju ld ng lf lg lh nh lj lk ll ni ln lo lp lq ij bi translated">我写这篇文章是因为许多学生发现很难解释 PCA 的输出。要理解算法或模型的工作原理，最好的方法是从观察简单数据集的输出开始。这将有助于获得更多关于算法的直觉。</p></blockquote></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="e1f1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">【https://www.linkedin.com】最初发表于<a class="ae mv" href="https://www.linkedin.com/pulse/quick-way-check-linearity-data-aditya-dutt/?trackingId=RLORiw4ll4pJjydgYkPpoQ%3D%3D" rel="noopener ugc nofollow" target="_blank"><em class="mj"/></a><em class="mj">。</em></p></div></div>    
</body>
</html>