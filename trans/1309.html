<html>
<head>
<title>K-Medoid Clustering (PAM)Algorithm in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的K-Medoid聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-medoid-clustering-pam-algorithm-in-python-with-solved-example-c0dcb35b3f46#2022-04-02">https://towardsdatascience.com/k-medoid-clustering-pam-algorithm-in-python-with-solved-example-c0dcb35b3f46#2022-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c14" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一步一步的教程-有一个解决的例子</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/ce88be8b3e670d731908fa8deef8d4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*AUwCwG1QZlI8PSdTMY0fGw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源——由作者使用Jupyter笔记本准备。</p></figure><h1 id="d2a7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">1.介绍</h1><h2 id="b92b" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">1.1K-均值聚类与挑战</h2><p id="5c15" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">大规模数据的聚类是实现基于分段的算法的关键。细分可以包括识别客户群以促进有针对性的营销，识别处方者群以允许医疗保健参与者向他们传达正确的信息，以及识别数据中的模式或异常值。<em class="mu"> K </em> -Means是跨不同问题领域采用的最流行的聚类算法，主要是由于它的计算效率和算法的易于理解性。<em class="mu"> K- </em>意味着依赖于从数据中识别聚类中心。它使用欧几里德距离度量在向这些聚类中心分配点之间交替，并重新计算聚类中心，直到达到收敛标准。然而，<em class="mu">K</em>-均值聚类有一系列的缺点:</p><ol class=""><li id="d52d" class="mv mw it md b me mx mh my lr mz lu na lx nb mt nc nd ne nf bi translated"><em class="mu">K</em>-表示聚类不使用除欧几里德距离之外的任何其他距离度量。在使用稀疏数据的推荐系统中，经常使用不同的距离度量，如余弦距离。</li><li id="8ecd" class="mv mw it md b me ng mh nh lr ni lu nj lx nk mt nc nd ne nf bi translated">在聚类中，最终中心不一定是来自数据的数据点，而是对不同应用领域(如计算机视觉中的图像)的可解释性的瓶颈。</li></ol><h1 id="3a33" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">2.水母周围的分区(PAM)</h1><p id="bb03" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">PAM代表“围绕Medoids的分区”PAM将PAM的每一步从确定性计算问题转化为统计估计问题，并将样本大小的复杂度从<em class="mu"> n </em>降低到<em class="mu"> O(n log n) </em>。<em class="mu"> Medoids是选择作为聚类中心的数据点。K- </em>表示聚类的目的是最小化类内距离(通常称为总平方误差)。相反，K-Medoid最小化了聚类中的点和被认为是该聚类的中心的点之间的不相似性。</p><blockquote class="nl nm nn"><p id="7757" class="mb mc mu md b me mx ju mg mh my jx mj no np ml mm nq nr mo mp ns nt mr ms mt im bi translated">当且仅当数据集中的任意点与聚类中所有其他数据点的不相似性最小时，该点才可被视为medoid。</p></blockquote><p id="0977" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">PAM的基本概念包括:</p><ol class=""><li id="77d5" class="mv mw it md b me mx mh my lr mz lu na lx nb mt nc nd ne nf bi translated">从<em class="mu">大小n </em> (n为记录数)的数据点中找出一组<em class="mu"> k </em>的Medoids ( <em class="mu"> k为聚类数，M为medoids </em>的集合)。</li><li id="ae6e" class="mv mw it md b me ng mh nh lr ni lu nj lx nk mt nc nd ne nf bi translated">使用任何距离度量(比如d。)，可能是欧几里德，曼哈顿等。)，尝试定位使数据点到最近的Medoid的总距离最小的med oid。</li><li id="8bb2" class="mv mw it md b me ng mh nh lr ni lu nj lx nk mt nc nd ne nf bi translated">最后，在所有可能的<em class="mu"> k(n-k) </em>对中交换降低损失函数L的中值和非中值对。损失函数定义为:</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/62a4c08c6ffb7cf2758f8027d4af7296.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*UbHVRTN1tYo64bV1h_i_wA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图一。说明了PAM算法使用的损失函数。</p></figure><p id="e1bf" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">更新质心:在K-Means的情况下，我们正在计算聚类中所有点的平均值。但是对于PAM算法，质心的更新是不同的。如果在一个聚类中有m个点，则用所有其他(m-1)个点交换先前的质心，并将该点最终确定为具有最小损失的新质心。最小损失由图1所示的成本函数计算得出。下面列出了一个相同的工作示例(GeeksforGeeks，2019)。</p><h2 id="5954" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">2.1算法—数据集</h2><p id="76f1" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">让我们考虑一组数据点和特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f80f9bb8bd72f3573104a0faadc50175.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*vSOsppjt5Ed_qUV8AX8adg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图二。显示了7个数据点的一组假设特征。作者使用Jupyter和Markdown制作的图像。</p></figure><h2 id="e979" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">2.2初始化步骤</h2><ol class=""><li id="47b8" class="mv mw it md b me mf mh mi lr nw lu nx lx ny mt nc nd ne nf bi translated">让我们随机选择两个medoids，所以选择𝐾= 2，让M1 =(4，5)，M2 =(9，10)是这两个medoids。注释聚类算法从数据中选择数据点作为中间点</li><li id="d4db" class="mv mw it md b me ng mh nh lr ni lu nj lx nk mt nc nd ne nf bi translated">计算所有点到M1和M2的距离。注:相异度的计算方法是将medoids和数据点之间的绝对差值进行<strong class="md iu">求和。例如D=|Mx —特征X| + |My —特征Y| </strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oa ob di oc bf od"><div class="gh gi nz"><img src="../Images/600e3bbabf5e31f2aa0ce7fb2fb69e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJ6AsEa3VpukrslfCTMi-w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3。说明了相异度计算。作者使用Jupyter和Markdown制作的图像。</p></figure><h2 id="2c69" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">2.3分配步骤</h2><p id="7695" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">将数据点分配给具有较低相异度(成本)的Medoids。数据点0和2指向M2，数据点3、4和5指向M1。<strong class="md iu">总成本=成本/分配给M1的积分差异+成本/分配给M2的积分差异。</strong></p><p id="7e99" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">总成本=(4+3+4)+(4+7)= 22；M1的成本=点3与M1的差异(4) +点4与M1的差异(3)+点5与M1的差异(4)。对M2重复同样的计算。</p><h2 id="142b" class="lm kv it bd kw ln lo dn la lp lq dp le lr ls lt lg lu lv lw li lx ly lz lk ma bi translated">2.4质心重新编译步骤</h2><p id="0187" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">随机选择一个非medoid点并重新计算成本。现在，让我们选择数据点5 M1 as (2，3)作为中间点，并重新计算成本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e5a5d9514bbbb42fb7da80d32ec5cffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*8BX6YW62N-uvO5He_z8Sww.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图4。示出了使用更新的质心的相异度计算，即，使用数据点5 (2，3)作为M1。作者使用Jupyter和Markdown制作的图像。</p></figure><p id="f52e" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">现在只有4分和6分去了M1，其余的分都去了M2。总成本=(7+4)+(4+7+6)=28，交换成本=新成本-以前的成本= 28–22 = 6。</p><p id="ac60" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">我们将取消交换，因为交换成本大于0。这个过程一直持续到满足收敛标准。</p><h1 id="2c6d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">3.使用Python实现</h1><p id="432a" class="pw-post-body-paragraph mb mc it md b me mf ju mg mh mi jx mj lr mk ml mm lu mn mo mp lx mq mr ms mt im bi translated">我们将使用来自scikit-learn-extra 的<a class="ae of" href="https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html" rel="noopener ugc nofollow" target="_blank"> K-Medoid算法来模拟上面的例子。很少代码和概念，灵感来自软件包文档(scikit-learn-extra，2019)</a></p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="b0c7" class="lm kv it oh b gy ol om l on oo"># — — — — — — -Importing Packages — — — — — — — — — — — -</span><span id="be86" class="lm kv it oh b gy op om l on oo">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="47da" class="lm kv it oh b gy op om l on oo">from sklearn_extra.cluster import KMedoids</span><span id="1471" class="lm kv it oh b gy op om l on oo"># — — — — — — -Assigning Initial Centers — — — — — — — — — — — -<br/>centers = [[4, 5], [9, 10]]</span><span id="050e" class="lm kv it oh b gy op om l on oo"># — — — — — — -Assigning Data: Dummy Data used in example above — — — — — — — — — — — — — — — — — — <br/>df=np.array([[7,8], [9,10], [11,5], [4,9], [7,5], [2,3], [4,5]])</span><span id="91af" class="lm kv it oh b gy op om l on oo"># — — — — — — -Fit KMedoids clustering — — — — — — — — — — — -<br/>KMobj = KMedoids(n_clusters=2).fit(df)</span><span id="ea4e" class="lm kv it oh b gy op om l on oo"># — — — — — — -Assigning Cluster Labels — — — — — — — — — — — -<br/>labels = KMobj.labels_</span></pre><p id="a7ca" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated">可视化集群。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="3396" class="lm kv it oh b gy ol om l on oo"># — — — — — — -Extracting Unique Labels — — — — — — — — — — — -</span><span id="55d3" class="lm kv it oh b gy op om l on oo">unq_lab = set(labels)</span><span id="20bb" class="lm kv it oh b gy op om l on oo"># — — — — — — -Setting Up Color Codes — — — — — — — — — — — -<br/>colors_plot = [<br/> plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unq_lab))<br/>]</span><span id="3f66" class="lm kv it oh b gy op om l on oo">for k, col in zip(unq_lab, colors_plot):</span><span id="dc8a" class="lm kv it oh b gy op om l on oo">class_member_mask = labels == k<br/> <br/> # — — — — — — -Setting datapoint Feature X and Feature Y — — — — — — — — — — — -</span><span id="2a79" class="lm kv it oh b gy op om l on oo">xy = df[class_member_mask]<br/> <br/> # — — — — — — -Plotting Feature X and Feature Y for each cluster labels — — — — — — — — — — — -<br/> <br/> plt.plot(<br/> xy[:, 0],<br/> xy[:, 1],<br/> “o”,<br/> markerfacecolor=tuple(col),<br/> markeredgecolor=”white”,<br/> markersize=10,<br/> );</span><span id="f852" class="lm kv it oh b gy op om l on oo"># — — — — — — -Annotate Centroids — — — — — — — — — — — -</span><span id="3015" class="lm kv it oh b gy op om l on oo">plt.plot(<br/> KMobj.cluster_centers_[:, 0],<br/> KMobj.cluster_centers_[:, 1],<br/> “o”,<br/> markerfacecolor=”orange”,<br/> markeredgecolor=”k”,<br/> markersize=10,<br/>);</span><span id="da7b" class="lm kv it oh b gy op om l on oo"># — — — — — — -Add title to the plot — — — — — — — — — — — -</span><span id="00d0" class="lm kv it oh b gy op om l on oo">plt.title(“KMedoids clustering on Dummy Data- Medoids are represented in Orange.”, fontsize=14);</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/ce88be8b3e670d731908fa8deef8d4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*AUwCwG1QZlI8PSdTMY0fGw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图5。使用K-Medoids演示了最终的簇形心。图片由作者使用Jupyter笔记本制作。</p></figure><h1 id="a1fe" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">参考</strong></h1><ol class=""><li id="8589" class="mv mw it md b me mf mh mi lr nw lu nx lx ny mt nc nd ne nf bi translated">scikit-learn-extra。(2019).<em class="mu"> KMedoids演示— scikit-learn-extra 0.2.0文档</em>。sci kit-Learn-extra . readthedocs . io .<a class="ae of" href="https://scikit-learn-extra.readthedocs.io/en/stable/auto_examples/plot_kmedoids.html#sphx-glr-auto-examples-plot-kmedoids-py" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn-extra . readthedocs . io/en/stable/auto _ examples/plot _ kmedoids . html # sphx-glr-auto-examples-plot-kmedoids-py</a></li><li id="17fc" class="mv mw it md b me ng mh nh lr ni lu nj lx nk mt nc nd ne nf bi translated">极客之福。(2019年5月17日)。<em class="mu"> ML | K-Medoids聚类与求解实例</em>。极客之福。<a class="ae of" href="https://www.geeksforgeeks.org/ml-k-medoids-clustering-with-example/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/ml-k-med OIDs-clustering-with-example/</a></li></ol></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="c574" class="pw-post-body-paragraph mb mc it md b me mx ju mg mh my jx mj lr np ml mm lu nr mo mp lx nt mr ms mt im bi translated"><em class="mu">关于作者:高级分析专家和管理顾问，通过对组织数据的业务、技术和数学的组合，帮助公司找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；可以和我在</em> <a class="ae of" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"> <em class="mu">上联系</em> </a> <em class="mu">和</em> <a class="ae of" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"> <em class="mu">推特</em></a><em class="mu">；</em></p></div></div>    
</body>
</html>