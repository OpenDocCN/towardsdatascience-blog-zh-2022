<html>
<head>
<title>Why Packaging your ML Code is Not as Painful as You Might Expect</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么打包你的ML代码没有你想象的那么痛苦</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-packaging-your-ml-code-is-not-as-painful-as-you-might-expect-d3583439ec1d#2022-09-20">https://towardsdatascience.com/why-packaging-your-ml-code-is-not-as-painful-as-you-might-expect-d3583439ec1d#2022-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="daf5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何将您的机器学习代码及其依赖项转移到您的生产环境中。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4d58dc1677f95665673f981c2aef2374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MuI2KFTkyqtYQ0P7UmzOHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包装您的ML模型—用稳定扩散创建的图像</p></figure><p id="2c8e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Jane在一家成功的初创公司担任机器学习(ML)工程师。他们即将发布其产品的第一个版本，该版本严重依赖于她正在研究的ML算法的性能。经过几次迭代之后，她所训练的模型在一个坚持的测试集上表现得相当好，她已经准备好进行下一步了。</p><p id="7963" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，她使用<a class="ae lu" href="https://gradio.app/" rel="noopener ugc nofollow" target="_blank"> Gradio </a>快速开发出一个原型，并将其放在拥抱脸<a class="ae lu" href="https://huggingface.co/spaces" rel="noopener ugc nofollow" target="_blank">空间</a>上。现在，她有了一个运行中的ML驱动的应用程序，它有一个简单的UI，可以与信任的朋友和同事分享。Jane利用这一点获得反馈，并验证模型在真实场景中的性能。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/keep-your-ml-models-out-of-your-application-servers-9fe58f9c91a5"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">让您的ML模型远离您的应用服务器</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">如何将一个有前途的ML模型变成一个有用的ML驱动的产品</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="b6b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">她现在准备开始开发过程的下一阶段；构建一个合适的推理服务器。她确信模型即服务(MaaS)方法是前进的方向，因此在她的任务列表中添加了一些新项目:</p><ul class=""><li id="42c9" class="mn mo it la b lb lc le lf lh mp ll mq lp mr lt ms mt mu mv bi translated">打包模型</li><li id="e2f6" class="mn mo it la b lb mw le mx lh my ll mz lp na lt ms mt mu mv bi translated">构建一个REST API</li><li id="0143" class="mn mo it la b lb mw le mx lh my ll mz lp na lt ms mt mu mv bi translated">优化服务的性能</li><li id="168f" class="mn mo it la b lb mw le mx lh my ll mz lp na lt ms mt mu mv bi translated">解决缩放问题</li><li id="a576" class="mn mo it la b lb mw le mx lh my ll mz lp na lt ms mt mu mv bi translated">简化版本推出</li></ul><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/the-unnerving-sweet-spot-for-ml-powered-products-c34b54e17179"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">ML动力产品令人不安的最佳点</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">维护一个推理服务器是痛苦的，但也是必要的。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="nb l mj mk ml mh mm ks ly"/></div></div></a></div><p id="ffa4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">今天是星期一。她拿起单子上的第一项:包装模型。在这个故事中，我们研究了她的两种选择，以及为什么走得最多的路——尽管是一条崎岖的路——是甜蜜点路线。</p><blockquote class="nc nd ne"><p id="dce7" class="ky kz nf la b lb lc ju ld le lf jx lg ng li lj lk nh lm ln lo ni lq lr ls lt im bi translated"><a class="ae lu" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=model-packaging" rel="noopener ugc nofollow" target="_blank"> Learning Rate </a>是一份时事通讯，面向那些对AI和MLOps世界感到好奇的人。你会在每个月的第一个星期六收到我关于最新人工智能新闻和文章的更新和想法。订阅<a class="ae lu" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=model-packaging" rel="noopener ugc nofollow" target="_blank">这里</a>！</p></blockquote><h1 id="7d76" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">模型打包和依赖管理</h1><p id="4aa7" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">ML模型是一个数学函数，它接受一个明确定义的输入并产生一个输出。但这也是Jane必须打包的一段代码，然后加载到一个与她工作的环境完全不同的环境中。</p><p id="ad91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型预测依赖于她编写的代码、它的依赖关系和模型权重。所有这些工件都需要出现在推理服务器上。不幸的是，这不是一件容易的事情。您可能认为，对于Jane来说，启动一个VM，在其中使用ssh，安装所有的依赖项，复制模型的代码和权重，运行一个flask应用程序，并将其称为推理服务器就足够了。它不是；那么，有什么问题呢？</p><p id="bc67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">依赖性是很多令人头疼的问题的根源。很难在不同的环境中保持它们的一致性，甚至其中一个环境中的微小升级都会产生难以追踪的问题。</p><p id="e9f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您的模型代码可能依赖于数十或数百个其他库(例如，<code class="fe og oh oi oj b">pytorch</code>、<code class="fe og oh oi oj b">torchvision</code>、<code class="fe og oh oi oj b">torchmetrics</code>、<code class="fe og oh oi oj b">numpy</code>、<code class="fe og oh oi oj b">scipy</code>等)。).您的应用服务器也有自己的依赖项(例如<code class="fe og oh oi oj b">flaks</code>、<code class="fe og oh oi oj b">gunicorn</code>等)。).众所周知，跟踪一切并维护独特的环境非常困难。Jane必须同步她的开发、测试、试运行和生产环境。这是一场噩梦。</p><p id="481e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Jane以前亲身经历过这些困难，所以她知道有两种选择:以不可知的形式保存模型以约束模型的依赖关系，或者将整个推理应用程序包装在一个容器中。</p><h2 id="b773" class="ok nk it bd nl ol om dn np on oo dp nt lh op oq nv ll or os nx lp ot ou nz ov bi translated">约束模型依赖关系</h2><p id="cf51" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">Jane的选择之一是约束她的模型的依赖关系。今天，完成这项工作的主要方法是使用一个名为ONNX(开放式神经网络交换)的库。</p><p id="1fdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ONNX的目标是成为ML模型的互操作性标准。使用ONNX，Jane可以用任何语言定义网络，并在任何地方一致地运行它，不管ML框架、硬件等等。</p><p id="1e72" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这看起来很理想，但简以前也曾上当。由于ML和数值库迭代速度很快，所以翻译层经常会出现难以追踪的bug。ONNX，不管它的意图是好是坏，仍然会导致比它试图解决的问题更多的问题。</p><p id="c07d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，正如我们所看到的，Jane必须考虑与她的模型无关的部分。数据处理部分或者服务器的代码呢？</p><h2 id="310d" class="ok nk it bd nl ol om dn np on oo dp nt lh op oq nv ll or os nx lp ot ou nz ov bi translated">将推理代码打包到容器中</h2><p id="60e3" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">当我们谈论集装箱时，十有八九是指码头集装箱。虽然Docker有一点学习曲线，但入门并不困难，是Jane的最佳选择。</p><p id="e7a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Jane必须编写DockerHub文件，构建映像，并将其推送到映像注册中心，比如DockerHub。在稍后阶段，这可以通过CI/CD管道实现自动化，因此在这方面投入时间会让她走得很远。Jane现在可以将这个映像放入任何运行容器运行时的计算机中，并毫无困难地执行它。</p><p id="f305" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">容器和Docker彻底改变了我们打包应用程序的方式。它们无处不在，即使你不想深入细节，一些服务也可以帮助你将你的ML代码打包成标准的、生产就绪的容器。<a class="ae lu" href="https://github.com/replicate/cog" rel="noopener ugc nofollow" target="_blank"> Cog </a>、<a class="ae lu" href="https://www.bentoml.com/" rel="noopener ugc nofollow" target="_blank"> BentoML </a>和<a class="ae lu" href="https://truss.baseten.co/" rel="noopener ugc nofollow" target="_blank"> Truss </a>是提供更快速运送模型的项目。</p><p id="9565" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，我的观点是，既然您无论如何都要编写一个YAML文件，那么最好学习Docker和Kubernetes的语法。您也可以将这些知识转移到其他领域，帮助您对部署过程中的许多步骤有一个总体的了解。</p><h1 id="3ccd" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">结论</h1><p id="f8c4" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">开发您的ML模型只是构建ML驱动的应用程序的一步。事实上，它往往是前进的许多小步中最小的一步。</p><p id="0b80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您准备打包它并构建推理服务器时，您实际上有两个选择:约束它的依赖关系或将其打包到一个容器中。</p><p id="f98f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个故事中，我们看到，虽然ONNX等工具的承诺是雄心勃勃的，但将模型代码及其依赖项打包到一个容器中仍然是一条路要走。接下来，我们将看到如何使用Docker、KServe和Kubernetes来实现这一点。</p><h1 id="be24" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">关于作者</h1><p id="19c1" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我叫<a class="ae lu" href="https://www.dimpo.me/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=model-packaging" rel="noopener ugc nofollow" target="_blank"> Dimitris Poulopoulos </a>，我是一名为<a class="ae lu" href="https://www.arrikto.com/" rel="noopener ugc nofollow" target="_blank"> Arrikto </a>工作的机器学习工程师。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲央行、经合组织和宜家等主要客户设计和实施过人工智能和软件解决方案。</p><p id="62b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的帖子，请关注我的<a class="ae lu" href="https://towardsdatascience.com/medium.com/@dpoulopoulos/follow" rel="noopener" target="_blank"> Medium </a>、<a class="ae lu" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或Twitter上的<a class="ae lu" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"> @james2pl </a>。</p><p id="dcde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。</p></div></div>    
</body>
</html>