<html>
<head>
<title>Dealing with the EU Artificial Intelligence Act</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应对欧盟人工智能法案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dealing-with-the-eu-artificial-intelligence-act-40e7059c8210#2022-07-22">https://towardsdatascience.com/dealing-with-the-eu-artificial-intelligence-act-40e7059c8210#2022-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fff7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">风险类别、要求、修订和最佳实践</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9b543ee7da46cdb757538d65c9e0ee6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7bqQFDSy8K59SIoCVGpAA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片提供:<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> unsplash </a></p></figure><p id="65ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在撰写本文时，关于欧盟人工智能(AI)法案的讨论很少，除了附件之外，很少有具体的来源简要介绍该法案的主要观点。</p><p id="26e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">《人工智能法案》尚未得到认可，它是一个革命性的里程碑，将人工智能带入了主流技术。受到政府当局的监控必然会使人工智能更加强大、可靠和标准化。尽管建立符合法案标准的基础可能具有挑战性，但下一步除了大规模可伸缩性之外别无选择。</p><p id="b794" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文摘录了AI法案的主要亮点，并向读者简要介绍了AI的<strong class="ky ir">风险类别、高风险AI的禁令和要求，以及避免不遵守拟议法律的后果的最佳实践</strong>。这本书非常适合决策者或团队领导，他们可以在各自的组织中塑造和重定向人工智能基础设施和人工智能实践的过程。</p><h1 id="c589" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是欧盟人工智能法案</h1><p id="ea8d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">《欧盟人工智能法案》是由一个主要监管机构提出的首创性提案。《大赦国际法》是在2021年4月提出的，尚未成熟成为通过的法案。</p><p id="4398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管这是第一个主要命题，但当中国和巴西分别于2021年11月和9月通过其人工智能法规时，它失去了作为第一个具体人工智能法律的地位。</p><p id="20d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">立法者对人工智能的兴趣证明了人工智能不仅在慢慢超越日常技术，也证明了政府开始将其作为主流和可观察的技术使用的兴趣。</p><p id="fc23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人工智能法案通过将人工智能分为三个明确的类别来建立其基础:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/b1a869353b46a8dc6e1c5809016d77bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_M9j3BnK7CsHXUmWcSx15w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AI法案规定的AI风险类别|图片由<a class="ae kv" href="http://censius.ai" rel="noopener ugc nofollow" target="_blank"> censius.ai </a>提供</p></figure><h2 id="363d" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">不可接受的风险</strong></h2><p id="4fba" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对最终用户的福祉或隐私构成严重威胁的应用程序，如某些政府使用的社交评分应用程序，将被禁止。</p><h2 id="6c5b" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">高风险</strong></h2><p id="4e68" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">根据法案中的定义，高风险应用范围很广。我们将在本文后面讨论更多的细节。简而言之，高风险应用程序是那些干预平民活动的应用程序，如简历扫描工具。这些申请受到该法规定的严格法律和禁令的约束。</p><h2 id="0e1f" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">低风险或最小风险</strong></h2><p id="24a3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">低风险或最小风险应用程序是那些不会对最终用户的隐私、安全或健康造成任何威胁的应用程序。比如推荐风景滤镜的娱乐类app。建议当局鼓励和促进行为准则，以促进对低风险人工智能系统自愿适用这些要求。</p><p id="3cfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将深入探讨构成人工智能法案症结的高风险应用程序。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="964a" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">为什么是现在？</h1><p id="742b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">人工智能在学术领域已经存在了几十年，在主流企业或公司应用程序中也存在了大约五年。那为什么政府当局现在想到要管制它呢？</p><p id="1aa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过多年的概念验证项目和跨行业测试，人工智能已经表明，它有足够的能力和利润成为日常应用中的一项深入吸收的技术。因此，我们现在在主要应用中看到人工智能，有时尽管没有意识到它的存在也在使用它。例如，当我们浏览视频或电子市场时，人工智能就在发挥作用。当我们在谷歌文档或任何流行的文档平台上简单地写一句话时，人工智能甚至会提示我们。它推荐书，推荐要买的东西，甚至推荐吃什么食物。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/95af867334ebdd195f9a0a10cd22b283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOsvVnz68g09zz6YbtdHWA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为什么现在提出AI法案？|图片由<a class="ae kv" href="http://censius.ai" rel="noopener ugc nofollow" target="_blank"> censius.ai </a></p></figure><p id="358b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着人工智能逐渐开始帮助平民的决策活动，政府当局感到有必要规范人工智能的影响，以避免建筑商的错误判断或故意恶意造成的任何重大伤害。</p><p id="e7d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">近年来发现的人工智能的力量可以显著提高政府的能力，这也不足为奇。这是为什么人工智能需要被密切监控和监管的另一个重要原因，以便政府和执法机构也可以利用它，而不会扰乱基本人权的安宁。因此，人工智能法案成为理解和实现如何在保护最终用户权利的同时构建各种易受风险影响的人工智能应用的桥梁。</p><p id="4bab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AI法案的直接摘录，列出了为什么现在需要AI法案:</p><ul class=""><li id="efd3" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">确保投放到联盟市场和使用的人工智能系统是安全的，并尊重关于基本权利和联盟价值观的现有法律；</li><li id="c1f5" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">确保法律确定性，以促进人工智能领域的投资和创新；</li><li id="0d01" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">加强对适用于人工智能系统的基本权利和安全要求的现有法律的治理和有效执行；</li><li id="5092" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">促进合法、安全和值得信赖的人工智能应用的单一市场的发展，防止市场分裂。</li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="0bda" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">为什么按照AI法案开始建造是必要的？</h1><p id="b2ad" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">是的，人工智能法案仍然是一个命题，它肯定需要一段时间才能作为一个独立的法案通过。拖延既有政治原因，也有技术原因。</p><p id="8261" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">政治倾向于不同立法者之间的争论。然而，立法者意识到了这一延迟，并希望在11月举行最终投票以通过该法律，并弥补失去的时间，即使这意味着损害一项完全证明的法律。</p><p id="f47b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">延迟背后也有技术原因，因为人工智能法案相当雄心勃勃，旨在提供一个解决方案来监管各种各样的人工智能应用。目前版本的拟议法律有一些漏洞，一些机构和组织正在发送他们的建议，以便对最终版本进行明智的修改。</p><blockquote class="oc od oe"><p id="3fc9" class="kw kx of ky b kz la jr lb lc ld ju le og lg lh li oh lk ll lm oi lo lp lq lr ij bi translated">“……目标是建立一个有利于创新、经得起未来考验并能抵御干扰的法律框架”。</p></blockquote><p id="fb27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，尽管欧盟人工智能法案似乎还有几个月的时间，但中国和巴西等其他监管机构已经通过了有关人工智能的法律。因此，要参与全球市场，关键是要开始理解和执行管理机构建议的最低义务。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="b7aa" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">你在构建高风险的AI吗？</h1><p id="23c2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">《人工智能法案》非常明确地阐述了高风险应用程序的定义。然而，即使这个定义也不是完全包罗万象的，因为人工智能是一个非常活跃的领域，每隔一天都会带来新的创新和技术。以下是法案中定义高风险应用的一些直接摘录。</p><p id="83e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果满足以下条件，AI被视为高风险:</p><p id="aaf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(a)人工智能系统旨在用作产品的安全部件，或者本身就是一种产品，由附录II中所列的欧盟协调立法涵盖</p><ul class=""><li id="4c8e" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">简单来说，这意味着任何影响系统安全组件的人工智能产品，或者本身就是一个完整的产品，都将被视为高风险</li><li id="9c05" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir"> <em class="of">产品或系统的安全组件</em> </strong>是指产品或系统的一个组件，该组件实现该产品或系统的安全功能，或其故障或失灵危及人身或财产的健康和安全</li><li id="406c" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">作为快速参考，可以在<a class="ae kv" href="https://artificialintelligenceact.eu/annexes/" rel="noopener ugc nofollow" target="_blank">附件文件</a>的第2页找到欧盟协调立法清单</li></ul><p id="7711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">“(b)其安全组件为人工智能系统的产品，或人工智能系统本身作为一种产品，需要进行第三方符合性评估，以便根据附录II中所列的欧盟协调立法将该产品投放市场或投入使用。”</strong></p><ul class=""><li id="7614" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">"<strong class="ky ir"> <em class="of">投放市场</em> </strong>是指第一个在欧盟市场上提供的人工智能系统；"</li><li id="7a91" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir"><em class="of"/></strong>投入使用是指将人工智能系统直接提供给用户首次使用或在欧盟市场上供自己使用以达到其预期目的</li><li id="6135" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir"> <em class="of">符合性评估</em> </strong>是指验证高风险人工智能系统的要求是否得到满足的过程；</li></ul><p id="d568" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了上面提到的高风险人工智能系统，在<a class="ae kv" href="https://artificialintelligenceact.eu/annexes/" rel="noopener ugc nofollow" target="_blank">附件三</a>(第4页)中提到的区域运行的人工智能系统也应被视为高风险。附件三中提到的领域概述如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/d3c6af56634f9db6704d8101049bc425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOcbHPNdAEvdXFF3fSy-Xg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">附件三中提到的地区|图片由<a class="ae kv" href="http://censius.ai" rel="noopener ugc nofollow" target="_blank"> censius.ai </a>提供</p></figure><ul class=""><li id="bb08" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">自然人的生物识别和分类</li><li id="0daf" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">关键基础设施的安全组件</li><li id="a3e2" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">教育和职业培训，包括准入、分配或评估</li><li id="c10a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">就业、工人管理和自营职业</li><li id="9719" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">获得和享受基本的私人服务和公共服务及福利</li><li id="bb7a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">法律的实施</li><li id="c147" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">移民、庇护和边境控制管理</li><li id="9225" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">司法和民主进程</li></ul><p id="159f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<strong class="ky ir"> </strong>人工智能系统对健康和安全造成危害的风险，或对基本权利造成不利影响的风险，即就其严重性和发生概率而言，等于或大于高风险人工智能系统造成的危害或不利影响的风险，则可通过增加高风险人工智能系统来更新上述标准。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="c1ce" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">高风险人工智能系统的要求</h1><p id="4663" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于高风险的人工智能应用，有特定的要求，如果遵循这些要求，可以确保供应商和最终用户的最大安全。通过确保遵循这些要求，开发人员也可以确保规模，因为基础隐含地变得强大和可靠。</p><p id="dd2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是对高风险系统的全部要求的一个总结或几个要点，如提议的AI法案所述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/a9f8699c1e85deca55d090bd5ec5f2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqGLJBGc1XvA8UquuuOfXg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">高风险AI系统需求|图片由<a class="ae kv" href="http://censius.ai" rel="noopener ugc nofollow" target="_blank"> censius.ai </a></p></figure><h2 id="c457" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">风险管理系统</h2><ul class=""><li id="6b54" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">应建立、实施、记录和维护与高风险人工智能系统相关的风险管理系统(RMS)。</li><li id="62ce" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">RMS应包含一个连续的迭代过程，要求定期系统更新。</li><li id="def5" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">应<strong class="ky ir">识别&amp;分析</strong>已知和可预见的风险，<strong class="ky ir">估计&amp;评估</strong>常规使用或误用可能出现的风险，基于<strong class="ky ir">上市后监控</strong>系统收集的数据分析评估其他可能出现的风险，<strong class="ky ir">采取适当的风险管理</strong>措施</li><li id="e969" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">识别风险后，需要<strong class="ky ir">消除&amp;控制措施和信息发布</strong>措施，并且必须是风险管理系统计划的一部分</li><li id="47ce" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">"<strong class="ky ir">测试</strong>应针对初步<strong class="ky ir">定义的度量和概率阈值</strong>进行，这些度量和概率阈值适用于高风险人工智能系统的预期目的。"</li><li id="faf5" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%209-,Risk%20management%20system,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="e7e8" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">数据和数据治理</h2><ul class=""><li id="7ed6" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">保持高质量的培训、测试和验证数据，使其没有<strong class="ky ir">抽样误差、偏差或数据缺口</strong></li><li id="33db" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">高风险系统的提供商可以处理特殊类别的个人数据，这些数据对于<strong class="ky ir">确保与高风险人工智能系统</strong>相关的偏差监控、检测和纠正是绝对必要的</li><li id="3b03" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">应适用适当的<strong class="ky ir">数据治理和管理实践</strong></li><li id="d2d9" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">点击阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2010-,Data%20and%20data%20governance,-1." rel="noopener ugc nofollow" target="_blank"/></li></ul><h2 id="3367" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">技术资料</h2><ul class=""><li id="7a84" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">在系统投放市场或投入使用之前，必须编制详细的技术文件<strong class="ky ir">，并保持更新。</strong></li><li id="c668" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">技术文件必须证明高风险人工智能系统符合高风险人工智能系统的要求</li><li id="7cb8" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2011-,Technical%20documentation,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="7bfc" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">记录保存</h2><ul class=""><li id="9518" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">系统必须自动记录<strong class="ky ir">事件(‘日志’)</strong></li><li id="72b0" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">测井能力应符合公认的标准或通用规范。</li><li id="157b" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">记录功能应使<strong class="ky ir">能够监控</strong>高风险人工智能系统的运行，以防止可能导致人工智能系统出现风险的情况发生</li><li id="4dde" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2012-,Record%2Dkeeping,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="2230" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">透明度和向用户提供信息</h2><ul class=""><li id="24c4" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">系统应<strong class="ky ir">使用户能够解释系统的输出</strong>并正确使用</li><li id="88a6" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">该系统应附有以适当的数字格式或其他方式使用的说明，包括简明、完整、正确和清晰的信息，这些信息对用户来说是相关的、可访问的和可理解的。</li><li id="c7b2" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">这些信息应该包括系统的预期<strong class="ky ir">目的、精确度、健壮性、网络安全</strong>以及任何其他可能需要的规格。</li><li id="daba" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2013-,Transparency%20and%20provision%20of%20information%20to%20users,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="c91a" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">人为监督</h2><ul class=""><li id="d32b" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">要求在人工智能系统使用期间，该系统能够受到自然人的有效监督</li><li id="198c" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">当高风险人工智能系统按照其预期目的使用或在可合理预见的误用条件下使用时，监督者应致力于防止或最大限度地降低健康、安全或基本权利的风险</li><li id="ab65" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">监督者应充分了解高风险人工智能系统的能力和局限性，并能够适时<strong class="ky ir">监控其运行，以便尽早发现和解决异常、功能障碍和意外性能的迹象</strong></li><li id="13a3" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">系统应该有一种方法或一个按钮，当监管人员认为功能可能有风险时，可以停止功能</li><li id="52b7" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">对于特定系统，最终决定不应仅取决于该系统，而必须首先由至少两个自然人核实</li><li id="5c11" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2014-,Human%20oversight,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul><h2 id="311b" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">准确性、稳健性和网络安全</h2><ul class=""><li id="21f7" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">系统需要<strong class="ky ir">保持准确性或标准度量的一致性</strong>,使其永远不会低于合适的阈值</li><li id="330c" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">高风险人工智能系统的稳健性可以通过技术冗余解决方案来实现，这可能包括备份或故障安全计划，如关于不一致或故障的<strong class="ky ir">根本原因分析</strong></li><li id="0dc5" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">有必要<strong class="ky ir">警惕异常情况</strong>，例如当未经授权的第三方试图访问系统时</li><li id="0f4a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">阅读直接摘录<a class="ae kv" href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN#:~:text=Article%2015-,Accuracy%2C%20robustness%20and%20cybersecurity,-1." rel="noopener ugc nofollow" target="_blank">此处</a></li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="bd32" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">高风险人工智能的禁止事项</h1><p id="082a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">除了高风险系统必须遵循的要求，法案还列出了高风险系统严禁从事的几点:</p><ul class=""><li id="e665" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">在一个人的意识之外使用潜意识技术，从物质上扭曲一种导致或可能导致身体或心理伤害的行为</li><li id="73a6" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">利用特定人群因其年龄、身体或精神残疾而存在的任何弱点，从实质上扭曲该人群中造成或可能造成身体或心理伤害的人的行为</li><li id="114c" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">公共当局或代表公共当局使用人工智能系统对自然人在一定时期内的可信度进行评估或分类</li><li id="fa5a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">在公共场所使用“实时”远程生物识别系统进行执法，除非此类使用对于一系列列出的目标是绝对必要的</li></ul><p id="b931" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述例子可能包括伤害少数群体的有偏见的应用程序，推动有影响力的媒体扭曲一个人的行为的营销应用程序，甚至是战略性地放置内容以影响大众行为的社交媒体算法，这些行为会影响重大决策，如下一位政治候选人。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="bcf7" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">预算造价</h1><p id="637d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据创新中心的一份报告声称，欧盟人工智能法案将在未来五年内花费€<strong class="ky ir">310亿美元</strong>，并减少人工智能投资近20%。Meeri Haataja和Joanna Bryson等独立研究人员发表了他们自己的工作，表明成本可能会低得多，因为该法案主要覆盖了一小部分被认为是高风险的人工智能应用。</p><p id="cf19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直接从法案中提取的其他估计成本:</p><p id="28aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开发或使用对公民的安全或基本权利构成高风险的人工智能应用程序的企业或公共机构必须遵守特定的要求和义务。</p><ul class=""><li id="6a1d" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated">“<strong class="ky ir">符合这些要求</strong>将意味着到2025年，供应一个平均约170000欧元€的高风险人工智能系统的成本总计约为6000至7000欧元€。”</li><li id="6572" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">“对于人工智能用户来说，根据使用情况，在适当的情况下，还会有每年花费在确保<strong class="ky ir">人工监督</strong>上的时间成本。据估计，这些费用每年约为5000欧元至8000欧元。”</li><li id="df89" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">“<strong class="ky ir">对于高风险人工智能的供应商来说，验证成本</strong>可能会达到另外3000至7500欧元的€。”</li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="a541" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">修正和建议</h1><p id="aa82" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如前所述，AI法案是一个命题，各大组织和机构目前都在参与对最早可以通过的最终版本进行完善修正或建议。来自不同实体的一些重要建议分享如下:</p><ul class=""><li id="973c" class="no np iq ky b kz la lc ld lf nq lj nr ln ns lr nt nu nv nw bi translated"><strong class="ky ir">欧盟轮值主席国斯洛文尼亚</strong>:改善人工智能操纵的禁令</li><li id="8999" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">未来生命研究所(Future of Life Institute):该法案应该确保人工智能提供商考虑他们的应用对个人和整个社会的影响</li><li id="bd15" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir">剑桥</strong>:增加监管的灵活性，允许对高风险系统清单进行修改</li><li id="8899" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><strong class="ky ir">现在就进入欧洲</strong>:该提案不足以保护生物识别应用(如情感识别)的基本权利</li><li id="9a80" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">未来社会:人工智能法案应该确保政府对新的技术趋势保持敏感。</li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="c332" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">与AI法案保持一致的最佳实践</h1><p id="0573" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">以下是一些最佳实践，以确保尽早始终如一地满足法案规定的要求:</p><h2 id="2b03" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">测井</strong></h2><p id="e838" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">记录元数据、事件发生、设备、网络和每一个微小的细节对于确保可以跟踪数据传承以及可以重现或跟踪任何结果都是至关重要的。这也确保了解决方案的高度清晰，任何操作人员都可以轻松管理系统，而不会有太多的复杂性。</p><h2 id="5444" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">测试</strong></h2><p id="7fe6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">测试不仅仅局限于模型和数据验证。测试涵盖整个ML堆栈，其中每个端点都需要进行交叉检查，以确保解决方案基础架构和模型输出的完全安全性以及最大功能和健康。</p><h2 id="41ef" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">监控</strong></h2><p id="7cc8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">监控是构建可信的人工智能应用的最重要的实践之一。应持续监控模型和数据的重要指标，如性能、质量、漂移和偏差，以便尽早标记和处理任何潜在问题。</p><h2 id="c178" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">可解释性</strong></h2><p id="374e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果问题没有得到及时解决，那么监控和检测问题只会带来部分好处。强烈建议人工智能解释，因为它揭示了标记问题背后的根本原因，并使人工智能团队能够缩小雷达范围并修复有针对性的干扰。</p><h2 id="2bc5" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">资源规划</strong></h2><p id="ab44" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果不提前规划资源，遵守该法案规定的所有限制和义务可能会非常昂贵。在提交资源计划之前，评估资源成本、构建与购买成本、流程成本和其他几项成本非常重要。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h1 id="293b" class="ls lt iq bd lu lv nj lx ly lz nk mb mc jw nl jx me jz nm ka mg kc nn kd mi mj bi translated">最后一个音符</h1><p id="7411" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管人工智能法案尚未通过，但立法者和各种组织正在积极努力建立一套经得起未来考验的法律，以最终规范和监管人工智能应用。同样的过程以前已经应用于各种其他技术，今天我们将它们作为主流技术使用，最不关心后果，因为有法律保护我们的基本权利和安全。也是时候对人工智能进行监管，以确保越来越多的最终用户免受潜在伤害。</p><p id="f4b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开始确保你的人工智能应用程序符合法案中的规定是至关重要的。这样，一旦法律规定成为强制性的，人们就可以避免被切断主流市场的风险。《人工智能法》可能最终会像《GDPR》一样成为一项全球标准。</p><p id="9a28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确保您的应用程序遵守法规的最快方法是遵循MLOps实践，这从本质上使整个ML堆栈更具弹性、可再现性和可信赖性。请关注此处，了解即将发布的关于MLOps实践和工具的详细内容。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="0935" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">顶级参考</h2><ul class=""><li id="24e2" class="no np iq ky b kz mk lc ml lf ok lj ol ln om lr nt nu nv nw bi translated">幕正文:<a class="ae kv" href="https://artificialintelligenceact.eu/the-act/" rel="noopener ugc nofollow" target="_blank">https://artificialintelligenceact.eu/the-act/</a></li><li id="38a6" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated">附件:【https://artificialintelligenceact.eu/annexes/ T2】</li><li id="4e3f" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><a class="ae kv" href="https://www.technologyreview.com/2022/05/13/1052223/guide-ai-act-europe/" rel="noopener ugc nofollow" target="_blank">https://www . technology review . com/2022/05/13/1052223/guide-ai-act-Europe/</a></li><li id="ec4a" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><a class="ae kv" href="https://artificialintelligenceact.substack.com/archive?sort=new" rel="noopener ugc nofollow" target="_blank">https://artificialintelligenceact.substack.com/archive?sort=new </a></li><li id="fc9c" class="no np iq ky b kz nx lc ny lf nz lj oa ln ob lr nt nu nv nw bi translated"><a class="ae kv" href="https://www.iccl.ie/news/flaws-in-ex-post-enforcement-in-the-ai-act/" rel="noopener ugc nofollow" target="_blank">https://www . iccl . ie/news/flaws-in-ex-post-enforcement-in-the-ai-act/</a></li></ul></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><h2 id="5b06" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated"><strong class="ak">取得联系！</strong></h2><p id="f176" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果你想了解更多关于MLOps和可靠AI框架的知识，请参考我们的<a class="ae kv" href="https://censius.ai/blogs" rel="noopener ugc nofollow" target="_blank">资源(博客、电子书、白皮书</a>)。有关可行的入门步骤，<a class="ae kv" href="https://censius.ai/get-started" rel="noopener ugc nofollow" target="_blank">请联系我们</a>以获得在您的组织中实施MLOps的指导计划。欢迎<a class="ae kv" href="http://censius.ai/get-started" rel="noopener ugc nofollow" target="_blank">向我们索取一个关于增强人工智能解决方案可信度的监控和可解释性的演示</a>。</p></div></div>    
</body>
</html>