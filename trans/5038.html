<html>
<head>
<title>Generating Word Embeddings from Text Data using Skip-Gram Algorithm and Deep Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中使用Skip-Gram算法和深度学习从文本数据生成单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6#2022-11-09">https://towardsdatascience.com/generating-word-embeddings-from-text-data-using-skip-gram-algorithm-and-deep-learning-in-python-a8873b225ab6#2022-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="da20" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用人工神经网络和Gensim介绍自然语言处理中的嵌入</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/473aa8a6dc09693f480d82767ebd80ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hE30pfaWvYr159Ol"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莱昂纳多·大久保俊郎在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="fa7e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.介绍</h1><p id="cbcb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">NLP中最大的挑战是设计一种方法来表示单词的意思。这是一个关键的组成部分，因为不同的人以不同的方式传达相同的信息。例如，“我喜欢这里的食物”和“食物质量似乎不错”。我爱死了！”在一天结束的时候，两个文本都反映了一个由一系列单词或短语代表的想法或思想。在分布语义学中，一个词的含义是由最频繁出现在附近的词给出的。当一个<strong class="lt iu">词(w) </strong>出现在一个句子中时，其<strong class="lt iu"> <em class="mn">上下文</em> </strong>是出现在附近(在固定的窗口大小内)的词的集合或序列。例如，</p><ol class=""><li id="5230" class="mo mp it lt b lu mq lx mr ma ms me mt mi mu mm mv mw mx my bi translated">“我们可以使用传统的<em class="mn">机器</em> <strong class="lt iu">学习</strong> <em class="mn">算法</em>来创建驱动程序模型。”</li><li id="2b46" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm mv mw mx my bi translated">“建<em class="mn">深</em> <strong class="lt iu">学</strong> <em class="mn">框架</em>是当务之急。”</li><li id="fa0c" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm mv mw mx my bi translated">“我们能否使用传统的方法，将<em class="mn">学习</em> <strong class="lt iu">转向</strong> <em class="mn">提高绩效？”</em></li></ol><p id="3851" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">在NLP中，我们可以使用<strong class="lt iu">学习</strong>的上下文词(例如，(机器，算法)，(深度，框架)，(转移，朝向))来构建<strong class="lt iu">学习的向量表示。稍后我们会谈到单词向量。</strong></p><h1 id="1230" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.什么是单词嵌入？</h1><p id="2518" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在自然语言处理(NLP)中使用单词嵌入来描述如何为文本分析表示单词。通常，这种表示采用实值向量的形式，该向量对单词的含义进行编码，期望在向量空间中彼此更接近的单词将具有相似的含义。在称为单词嵌入的过程中，每个单词被表示为预定向量空间中的实值向量。这种方法被称为深度学习，因为每个单词都被分配给一个向量，向量值就像神经网络一样被学习(Jason Browniee，2017)。</p><h2 id="ce26" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">2.1嵌入的示例</h2><p id="a22d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对每个单词采用密集分布的表示对于产生嵌入的方法是必要的。通常具有几十、几百甚至几千维的实值向量代表每个单词。相比之下，稀疏单词表示，如一键编码，需要数千或数百万维。</p><p id="c425" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">这是一个简单的例子，展示了两个不同的单词在三维空间中的表现。请注意，向量符号和假设值表示单词“Angel”和“Eugene”</p><p id="c9d1" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated"><strong class="lt iu">天使= 3i + 4j + 5k，尤金= 2i + 4j + 4k </strong></p><p id="c9ed" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">嵌入的妙处在于它允许我们计算单词的相似度。这里的相似代表语境，不一定代表意义。比如说，如果我们用一个余弦相似度，那么<strong class="lt iu">相似度(安吉尔，尤金)=(3 x2+4 x4+5 x4)/(sqrt((3+4+5))x sqrt(2+4+4))= 42/42.42 = 0.99或者99% </strong>相似度。您可能想知道为什么这种相似性在NLP中很重要。这纯粹是为了让模型将单词上下文化，以便出现在相似上下文中的单词具有相似的含义。为了验证这一点，我们来看看下面的两句话。</p><ol class=""><li id="319c" class="mo mp it lt b lu mq lx mr ma ms me mt mi mu mm mv mw mx my bi translated">"安吉尔和尤金正在做一个项目提案，并计划明天推出."</li><li id="6bb3" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm mv mw mx my bi translated">“Eugene对模型验证的意见似乎是正确的。安吉尔，你能实现这些改变吗？”</li></ol><p id="5076" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">虽然我们在这里看的是两句话，看起来安吉尔和尤金是队友；因此，相似性仍然很高。同样，这不是单词是否有相似的意思，而是它们是否出现在相似的上下文中或共现。</p><h1 id="c519" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.基本嵌入模型</h1><h2 id="e325" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">3.1简单概率模型</h2><p id="8ead" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">简单的概率模型可以使用链式规则来计算单词在文本中出现的概率。它可以使用组合关系来识别共现单词并计算它们的概率。下面的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/85312ef363cbd617917234797f2af54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uB1y535X6prwASMq1qA0IQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">公式1: </strong>简单概率模型。作者使用Latex &amp; Jupyter笔记本创建的图像。</p></figure><p id="1097" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">p(天使玩的好)= p(天使)x p(玩|天使)x p(玩的好|天使)。计算这些概率需要计算单词和术语的出现次数。尽管越来越复杂和精确，像这样的直接概率模型还没有达到很好的性能。</p><h2 id="f18c" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">3.2 Word2vec</h2><p id="357b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">学习单词向量的框架叫做Word2vec。Word2vec是一个神经网络，它使用单词“矢量化”来解析文本。它接受文本语料库作为输入，并产生代表语料库中的单词的一组特征向量。将相似的词向量在向量空间中分组在一起是Word2vec的目标和好处。Word2Vec中有两种截然不同的架构:<strong class="lt iu">跳格</strong>和<strong class="lt iu">连续字包</strong> (CBOW)。</p><p id="c562" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">假设我们有大量的文本。在固定的词汇表中，每个单词由一个向量表示。我们使用以下逐步方法计算嵌入:</p><ul class=""><li id="8833" class="mo mp it lt b lu mq lx mr ma ms me mt mi mu mm nv mw mx my bi translated">浏览文本的每个位置<strong class="lt iu"> t </strong>，每个位置都有一个<strong class="lt iu"> <em class="mn">居中的单词</em> </strong> <strong class="lt iu"> c </strong></li><li id="ece6" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated"><strong class="lt iu"> <em class="mn">语境(或“外界”)</em> </strong>中的词表示为<strong class="lt iu"> o </strong></li><li id="8904" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">使用<strong class="lt iu"> c </strong>和<strong class="lt iu"> o </strong>的词向量的相似度，计算给定<strong class="lt iu"> c </strong>的<strong class="lt iu"> o </strong>的<strong class="lt iu">可能性</strong>。(反之亦然)</li><li id="b1f3" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">继续修改单词向量到<strong class="lt iu">增加这种可能性</strong></li></ul><h2 id="e529" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">跳过克模型</h2><p id="f44f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在跳格模型中，我们试图对给定特定单词的上下文单词(例如，单词窗口大小)进行建模。</p><p id="3a20" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">例如，如果我们有文本“Angel踢足球踢得好”，考虑足球作为中心词(假设)，剩余的词成为上下文。窗口大小表示我们想要考虑多少上下文单词。窗口大小示例和使用两个不同示例计算概率的过程。</p><div class="kj kk kl km gt ab cb"><figure class="nw kn nx ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/85c0fdc2b63dcfa157be4a6ec0b3e94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*FmRgQu9bqicRkW4GcYgaJg.png"/></div></figure><figure class="nw kn oc ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/618c5e6297cada8e21ed612d731ffa41.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*KwkD5gPz6sXgyUcXmr06GA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk od di oe of translated"><strong class="bd nu">图1: </strong>跳格模型示例。图表用两个不同的例子展示了中心词和上下文词的概念。这里t表示中心单词的位置，而t-1和t+1表示上下文单词的位置。作者使用PowerPoint创建的图像。</p></figure></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/0a76e5a5c732cdf1cba0d1f7de6d1d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1ZzvAwR3q5nmmAnsLPWRg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图二。</strong>说明了Skip Gram模型的似然函数。在真实的单词中，训练一种算法来最大化这种可能性，这将导致使用嵌入更准确地表示单词。图片由作者使用Latex &amp; Jupyter笔记本创建。</p></figure><h2 id="c589" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">使用Gensim生成基于跳格图的嵌入的3.2.1.1示例</h2><p id="26d3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Gensim是一个开源框架，使用现代统计机器学习进行无监督的主题建模、文档索引、嵌入创建和其他NLP功能。我们将使用Gensim中的一些标准文本来创建单词嵌入。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="364b" class="nh la it oi b gy om on l oo op">#----------------------------------------Install and import necessary libraries-----------------------------------</span><span id="c4ca" class="nh la it oi b gy oq on l oo op"># !pip install contractions</span><span id="afa2" class="nh la it oi b gy oq on l oo op">import re, string, unicodedata                          # Import Regex, string and unicodedata.<br/>import contractions                                     # Import contractions library.<br/>from bs4 import BeautifulSoup                           # Import BeautifulSoup.</span><span id="a6b3" class="nh la it oi b gy oq on l oo op">import numpy as np                                      # Import numpy.<br/>import pandas as pd                                     # Import pandas.<br/>import nltk                                             # Import Natural Language Tool-Kit.</span><span id="c027" class="nh la it oi b gy oq on l oo op"># nltk.download('stopwords')                              # Download Stopwords.<br/># nltk.download('punkt')<br/># nltk.download('wordnet')</span><span id="bfdf" class="nh la it oi b gy oq on l oo op">from nltk.corpus import stopwords                       # Import stopwords.<br/>from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.<br/>from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer. For removing stem words<br/>import unicodedata                                      # Removing accented characters<br/>from nltk.stem import LancasterStemmer</span><span id="0222" class="nh la it oi b gy oq on l oo op">from IPython.display import display</span></pre><p id="ec4b" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">这些包对于数据清理和预处理非常有用。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="3af5" class="nh la it oi b gy om on l oo op">import gensim</span><span id="8191" class="nh la it oi b gy oq on l oo op">from gensim.models import Word2Vec</span><span id="0776" class="nh la it oi b gy oq on l oo op">from gensim.test.utils import common_texts</span><span id="f877" class="nh la it oi b gy oq on l oo op">common_texts</span><span id="f7a3" class="nh la it oi b gy oq on l oo op">[['human', 'interface', 'computer'],<br/> ['survey', 'user', 'computer', 'system', 'response', 'time'],<br/> ['eps', 'user', 'interface', 'system'],<br/> ['system', 'human', 'system', 'eps'],<br/> ['user', 'response', 'time'],<br/> ['trees'],<br/> ['graph', 'trees'],<br/> ['graph', 'minors', 'trees'],<br/> ['graph', 'minors', 'survey']]</span></pre><p id="ffc1" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">在Word2Vec()中使用sg=1创建一个跳格模型。向量大小表示由模型产生的每个单词的嵌入数量。窗口表示窗口大小，即用于生成嵌入的上下文单词的数量。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="7893" class="nh la it oi b gy om on l oo op">model = Word2Vec(sentences=common_texts, vector_size=3, window=5, min_count=1, workers=4, sg=1)<br/>model.save(“word2vec.model”)</span><span id="be40" class="nh la it oi b gy oq on l oo op">print("Embeddings of a Word computer with 3 as Embedding Dimension")</span><span id="ab65" class="nh la it oi b gy oq on l oo op">print(model.wv['computer'])</span><span id="e89a" class="nh la it oi b gy oq on l oo op">print("Embeddings of a Word Tree with 3 as Embedding Dimension")</span><span id="2c2c" class="nh la it oi b gy oq on l oo op">print(model.wv['trees'])</span><span id="59af" class="nh la it oi b gy oq on l oo op">print("Embeddings of a Word Graph with 3 as Embedding Dimension")</span><span id="a728" class="nh la it oi b gy oq on l oo op">print(model.wv['graph'])</span><span id="48f5" class="nh la it oi b gy oq on l oo op">Embeddings of a Word computer with 3 as Embedding Dimension<br/>[ 0.19234174 -0.2507943  -0.13124168]<br/>Embeddings of a Word Tree with 3 as Embedding Dimension<br/>[ 0.21529572  0.2990996  -0.16718094]<br/>Embeddings of a Word Graph with 3 as Embedding Dimension<br/>[ 0.3003091  -0.31009832 -0.23722696]</span></pre><p id="6e33" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">我们现在将这些嵌入转换成一个列表，并使用matplotlib()创建一个可视化。如果您检查嵌入的值，它们可能没有什么意义，但是当用图形表示时，它们应该看起来彼此更接近。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="8127" class="nh la it oi b gy om on l oo op">inter_array=model.wv['human']<br/>comp_array=model.wv['user']<br/>tree_array=model.wv['interface']</span><span id="be63" class="nh la it oi b gy oq on l oo op">from mpl_toolkits import mplot3d<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="69ea" class="nh la it oi b gy oq on l oo op">%matplotlib inline<br/> <br/>c=[10,20,30]</span><span id="67e3" class="nh la it oi b gy oq on l oo op">labels=['human', 'user', 'interface']</span><span id="c0e5" class="nh la it oi b gy oq on l oo op">fig = plt.figure(figsize=(10, 8))<br/> <br/>ax = plt.axes(projection ='3d')<br/> <br/># defining all 3 axes<br/>z = [comp_array[2], inter_array[2], tree_array[2]]<br/>x = [comp_array[0], inter_array[0], tree_array[0]]<br/>y = [comp_array[1], inter_array[1], tree_array[1]]<br/> <br/># plotting<br/>ax.scatter(x, y, z, color=['red','green','blue'])</span><span id="b8a4" class="nh la it oi b gy oq on l oo op">ax.text(comp_array[0],comp_array[1],comp_array[2], "human")<br/>ax.text(inter_array[0],inter_array[1],inter_array[2], "user")<br/>ax.text(tree_array[0],tree_array[1],tree_array[2], "interface")</span><span id="2b21" class="nh la it oi b gy oq on l oo op">ax.legend(loc="lower right")</span><span id="46fc" class="nh la it oi b gy oq on l oo op">ax.set_title('3D Representation of words - computer, graph, and tree')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/08c9440a9e212c04a093046a482cf3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*iupjkCO7TlJOd52r0AJeWA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图3。</strong>使用从Gensim生成的嵌入，单词“界面”、“用户”和“人”的三维表示。作者使用Jupyter笔记本创建的图像。</p></figure><h2 id="669f" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">3.2.2连续词袋(CBOW)</h2><p id="c3b9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在连续词袋(CBOW)模型中，我们试图在给定其周围词(例如，词窗口大小)的情况下对中心词进行建模。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/49220f107f7f60a1fbb2af7bb12ba2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQ-yF0lCk8O2uhV4bmV1wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图4:</strong>CBOW模型的例子。与Skip Gram类似，CBOW的可能性函数如上所示。作者使用Jupyter笔记本和Latex创建的图像。</p></figure><h1 id="0f69" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">4.从头开始实现Skip Gram模型</h1><p id="8445" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用随机生成的关于梅西和罗纳尔多的数据，并通过训练神经网络模型来尝试和开发单词嵌入(Bujokas，2020)。我们将把嵌入的大小保持为两个，允许我们用Python创建一个图，并直观地检查给定上下文中哪些单词是相似的。你可以在这里找到作者<a class="ae ky" href="https://github.com/angeleastbengal/Data-Collection" rel="noopener ugc nofollow" target="_blank">生成的样本数据。</a></p><h2 id="9a3b" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.1基本NLP库</h2><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="8b0d" class="nh la it oi b gy om on l oo op">import itertools<br/>import pandas as pd<br/>import numpy as np<br/>import re<br/>import os<br/>from tqdm import tqdm</span><span id="60a1" class="nh la it oi b gy oq on l oo op">#----------------------Drawing the embeddings in Python<br/>import matplotlib.pyplot as plt</span><span id="ddf7" class="nh la it oi b gy oq on l oo op">#-----------------------Deep learning: ANN to create and train embeddings<br/>from tensorflow.keras.models import Model</span><span id="b45c" class="nh la it oi b gy oq on l oo op">from tensorflow.keras.layers import Input, Dense</span></pre><h2 id="bedf" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.2用户自定义函数:从数据到字典的单词/记号</h2><p id="c8f1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">创建字典的用户定义函数，其中键代表唯一的单词，键值是索引或索引。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="821f" class="nh la it oi b gy om on l oo op">def create_unique_word_to_dict(sample):<br/>    """<br/>    User defined function that creates a dictionary where the keys represents unique words<br/>    and key values are indices or indexes<br/>    """<br/>    #-------Extracting all the unique words from our sample text and sorting them alphabetically<br/>    #-------set allows us to remove any duplicated words and extract only the unique ones<br/>    words_list = list(set(sample))<br/>    <br/>    #-------------Sort applied here<br/>    <br/>    words_list.sort()</span><span id="03da" class="nh la it oi b gy oq on l oo op">#------Creating a dictionary for each unique words in the sample text<br/>    unique_word_dict_containing_text = {}<br/>    <br/>    for i, word in enumerate(words_list): #------For each word in the document<br/>        <br/>        unique_word_dict_containing_text.update({<br/>            word: i<br/>        <br/>        })</span><span id="ea1c" class="nh la it oi b gy oq on l oo op">return unique_word_dict_containing_text</span></pre><h2 id="67a6" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.3用户自定义功能:数据清理或文本预处理</h2><p id="5d97" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是一个自定义的文本预处理函数。人们可以增强这里使用的停用词列表，以说明更频繁使用的不需要的词。我们使用该功能执行以下操作:</p><ul class=""><li id="09a9" class="mo mp it lt b lu mq lx mr ma ms me mt mi mu mm nv mw mx my bi translated">删除标点符号</li><li id="9cee" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">删除号码</li><li id="9edf" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">删除空白</li><li id="a7c6" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">删除停用词</li><li id="bbc6" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm nv mw mx my bi translated">返回清除的文本</li></ul><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="42a7" class="nh la it oi b gy om on l oo op">def text_preprocessing_custom(sample_text):<br/>    <br/>    #---------------Custome Text Preprocessing created as UDF-----------------------------<br/>      <br/>    #----------------Regular expression for Punctuations------------------<br/>    <br/>    punctuations = r''';:'"\,&lt;&gt;./?*_“~'''<br/>    <br/>    #---------------Stop Words (Custom List)--------------------<br/>    <br/>    stop_words=['i', 'dont', 'we', 'had','and', 'are', 'of', 'a', 'is', 'the', 'in', 'be',\<br/>                'will', 'this', 'with', 'if', 'be', 'as', 'to', 'is', 'don\'t']<br/>    <br/>    """<br/>    A method to preproces text<br/>    """<br/>    for x in sample_text.lower():<br/>        <br/>        if x in punctuations: <br/>            <br/>            sample_text = sample_text.replace(x, "")</span><span id="bc5a" class="nh la it oi b gy oq on l oo op">#-------------Removing words that have numbers in them using regular expression---------<br/>    sample_text = re.sub(r'\w*\d\w*', '', sample_text)</span><span id="94eb" class="nh la it oi b gy oq on l oo op">#-------------Removing whitespaces---------------------<br/>    sample_text = re.sub(r'\s+', ' ', sample_text).strip()</span><span id="d7a7" class="nh la it oi b gy oq on l oo op"># --------Convert to lower case----------------------<br/>    sample_text = sample_text.lower()</span><span id="9c84" class="nh la it oi b gy oq on l oo op">#--------------Converting all our text to a list------------------<br/>    sample_text = sample_text.split(' ')</span><span id="a08e" class="nh la it oi b gy oq on l oo op">#--------------Deleting empty strings---------------------<br/>    sample_text = [x for x in sample_text if x!='']</span><span id="60a3" class="nh la it oi b gy oq on l oo op"># Stopword removal<br/>    <br/>    sample_text = [x for x in sample_text if x not in stop_words]</span><span id="956b" class="nh la it oi b gy oq on l oo op">return sample_text</span></pre><h2 id="9417" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.4数据预处理和[中心，上下文]单词生成</h2><p id="4b12" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">牢记Skip Gram模型的概念，我们需要创建一个框架，允许我们使用大小为2的窗口从文本中识别中心词和上下文词的所有组合。请注意，给定文本中的每个单词(经过预处理后)在一个实例中可以是中心单词，在另一个实例中可以是上下文单词。理想情况下，我们允许文本中的每个单词作为中心单词，然后相应地找到相关的上下文单词。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="a49f" class="nh la it oi b gy om on l oo op">from scipy import sparse</span><span id="86c4" class="nh la it oi b gy oq on l oo op">sample = pd.read_csv('Text Football.csv') #-----Reading text data</span><span id="7736" class="nh la it oi b gy oq on l oo op">sample = [x for x in sample['Discussion']]</span><span id="ae8c" class="nh la it oi b gy oq on l oo op">print("--"*50)</span><span id="a94a" class="nh la it oi b gy oq on l oo op">print("Text in the File")</span><span id="2c50" class="nh la it oi b gy oq on l oo op">print("--"*50)</span><span id="95aa" class="nh la it oi b gy oq on l oo op">print(sample)<br/>#----------------------------------------Defining the window for context----------------------------------------<br/>window_size = 2<br/>#----------------------------------------Creating empty lists to store texts---------------<br/>word_lists = []<br/>all_text = []</span><span id="a858" class="nh la it oi b gy oq on l oo op">#----------------Combines preprocessed texts from the Sample Data</span><span id="c511" class="nh la it oi b gy oq on l oo op">for text in sample:<br/>#----------------------------------------Cleaning the text<br/>    text = text_preprocessing_custom(text)<br/>    <br/>    all_text += text</span><span id="ed57" class="nh la it oi b gy oq on l oo op">#-------------------------Iterating across each word in a text<br/>    for i, word in enumerate(text):</span><span id="f7c0" class="nh la it oi b gy oq on l oo op">for ws in range(window_size):</span><span id="fc5c" class="nh la it oi b gy oq on l oo op">if i + 1 + ws &lt; len(text):<br/>                    <br/>                    word_lists.append([word] + [text[(i + 1 + ws)]])</span><span id="9566" class="nh la it oi b gy oq on l oo op">if i - ws - 1 &gt;= 0:<br/>                    word_lists.append([word] + [text[(i - ws - 1)]])<br/>    <br/>unique_word_dict = create_unique_word_to_dict(all_text)</span><span id="0309" class="nh la it oi b gy oq on l oo op">print("--"*50)<br/>print("Text to Sequence in post cleaning")<br/>print("--"*50)<br/>print(unique_word_dict)<br/>print("--"*50)<br/>print("Text to WordList [main word, context word]")<br/>print("--"*50)<br/>print(word_lists)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/732b428a7c5a547d24192daaf120b33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*knZ9Fz0uMf8BVdizbg8h1A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图5: </strong>如何生成中心(主要)和上下文单词的示例。作者使用Jupyter笔记本创建的图像。</p></figure><h2 id="27c3" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.5为神经网络生成输入和输出数据</h2><p id="32c2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们现在将使用主要单词(中心单词)作为神经网络的输入，使用上下文单词作为输出。输入和输出层之间的深层致密层将学习嵌入。想法是在单个隐藏层内训练一个2神经元神经网络，其中在训练模型时学习的权重将反映从数据中学习的嵌入。为了让我们的数据与Tensorflow架构兼容，我们对上面生成的主单词和上下文单词列表执行了一次热编码(OHE)。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="7ef8" class="nh la it oi b gy om on l oo op">def create_OHE_data(word_dict, word_lists):</span><span id="b27e" class="nh la it oi b gy oq on l oo op">#----------------------------------------Defining the number of unique words----------------------------------------<br/>    n_unique_words = len(word_dict)</span><span id="8c49" class="nh la it oi b gy oq on l oo op">#----------------------------------------Getting all the unique words----------------------------------------<br/>    words = list(word_dict.keys())</span><span id="8606" class="nh la it oi b gy oq on l oo op">#----------------------------------------Creating the X and Y matrices using one hot encoding---------------------------------<br/>    X = []<br/>    Y = []</span><span id="9a30" class="nh la it oi b gy oq on l oo op">for i, word_list in tqdm(enumerate(word_lists)):</span><span id="1f89" class="nh la it oi b gy oq on l oo op">#----------------------------------------Getting Indicies----------------------------------------</span><span id="69ee" class="nh la it oi b gy oq on l oo op">index_main_word = word_dict.get(word_list[0])<br/>        index_context_word = word_dict.get(word_list[1])</span><span id="5198" class="nh la it oi b gy oq on l oo op">#----------------------------------------Creating the placeholders   <br/>        Xrow = np.zeros(n_unique_words)<br/>        Yrow = np.zeros(n_unique_words)</span><span id="a2e7" class="nh la it oi b gy oq on l oo op">#----------------------------------------One hot encoding the main word, Input Matrix<br/>        Xrow[index_main_word] = 1</span><span id="3ce3" class="nh la it oi b gy oq on l oo op">#----------------------------------------One hot encoding the Y matrix words, Output Matrix <br/>        Yrow[index_context_word] = 1</span><span id="82bd" class="nh la it oi b gy oq on l oo op">#----------------------------------------Appending to the main matrices<br/>        X.append(Xrow)<br/>        Y.append(Yrow)</span><span id="576c" class="nh la it oi b gy oq on l oo op">#------------------------Converting the matrices into a sparse format because the vast majority of the data are 0s<br/>    X_Matrix = sparse.csr_matrix(X)<br/>    Y_Matrix = sparse.csr_matrix(Y)</span><span id="6551" class="nh la it oi b gy oq on l oo op">print("--"*50)<br/>    print("Input Data [Showing the First Record]")<br/>    print("--"*50)</span><span id="01c5" class="nh la it oi b gy oq on l oo op">print(X_Matrix.todense()[0])<br/>    print(X_Matrix.todense().shape)</span><span id="20a6" class="nh la it oi b gy oq on l oo op">print("--"*50)<br/>    print("Output Data [Showing the first record]")<br/>    print("--"*50)</span><span id="1410" class="nh la it oi b gy oq on l oo op">print(Y_Matrix.todense()[0])<br/>    print(Y_Matrix.todense().shape)<br/>    <br/>    return X_Matrix, Y_Matrix</span><span id="b041" class="nh la it oi b gy oq on l oo op">X, Y = create_OHE_data(unique_word_dict, word_lists)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/e63bbc24c20b4b0e2090fafab67366d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxVoxy1UxBsG45qCDbco6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图6:</strong>OHE生成输入输出数据的例子。作者使用Jupyter笔记本创建的图像。</p></figure><h2 id="5cda" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.6训练神经网络以生成嵌入</h2><p id="0227" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下面展示了一个示例架构的外观。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/26cea71e18513ad2de4b46f2665315e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kj-b76GT8P4YxxVPHvmsLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图7: </strong>示出了用于训练嵌入的双神经元神经网络。作者使用PowerPoint开发的图像。</p></figure><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="04b3" class="nh la it oi b gy om on l oo op">from tensorflow.keras.models import Sequential</span><span id="203e" class="nh la it oi b gy oq on l oo op">def embedding_model():<br/>    <br/>    model=Sequential()<br/>    <br/>    model.add(Dense(2, input_shape=(X.shape[1],), activation='relu'))<br/>    <br/>    model.add(Dense(units=Y.shape[1], activation='softmax'))<br/>    <br/>    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')<br/>    <br/>    return model</span><span id="ef10" class="nh la it oi b gy oq on l oo op">model_embbedding=embedding_model()</span><span id="3004" class="nh la it oi b gy oq on l oo op">model_embbedding.summary()</span><span id="5c2d" class="nh la it oi b gy oq on l oo op">#----------------------------------------Optimizing the network weights----------------------------------------<br/>model_embbedding.fit(<br/>    x=X.todense(), <br/>    y=Y.todense(), <br/>    batch_size=64,<br/>    epochs=1000,<br/>    verbose=0<br/>    )</span><span id="0ee7" class="nh la it oi b gy oq on l oo op">#----------------------------------------Obtaining the weights from the neural network----------------------------------------<br/>#----------------------------------------These weights are equivalent to word embeddings--------------------------------------</span><span id="b921" class="nh la it oi b gy oq on l oo op">weights = model_embbedding.get_weights()[0] #---embeddings</span><span id="76fa" class="nh la it oi b gy oq on l oo op">#----------------------------------------Creating Dictionary to Store Embeddings----------------------------------------<br/>embedding_dict = {}<br/>for word in words_list: <br/>    embedding_dict.update({<br/>        word: weights[unique_word_dict.get(word)]<br/>        })<br/>embedding_dict</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/127731d691464d48ca9102871d4b25e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*P4b0-fJbHT4J3lb0frcPlw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图8: </strong>从模型中生成的嵌入示例。作者使用Jupyter笔记本创建的图像。</p></figure><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="0001" class="nh la it oi b gy om on l oo op"># !pip install ann_visualizer</span><span id="d1a4" class="nh la it oi b gy oq on l oo op">from ann_visualizer.visualize import ann_viz</span><span id="bc6b" class="nh la it oi b gy oq on l oo op">ann_viz(model_embbedding, view=True, filename='embed.pdf', title='ANN Architecture for embedding')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/c9ee70f6f9fe3a56c8b1a53c12daef08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ZA8QbG8pjnlVtfhErf2bg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图九。</strong>说明了用于生成嵌入的人工神经网络架构。作者使用Jupyter + ann_visualizer()开发的图片</p></figure><div class="ox oy gp gr oz pa"><a rel="noopener follow" target="_blank" href="/how-to-visualize-neural-network-architectures-in-python-567cd2aa6d62"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd iu gy z fp pf fr fs pg fu fw is bi translated">如何用Python可视化神经网络架构</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">使用Jupyter或Google Colab创建神经网络图示的快速指南</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ks pa"/></div></div></a></div><h2 id="f21d" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.7直观地表示学习到的嵌入</h2><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="4030" class="nh la it oi b gy om on l oo op">#----------------------------------------Ploting the embeddings----------------------------------------<br/>plt.figure(figsize=(15, 10))</span><span id="bf26" class="nh la it oi b gy oq on l oo op">import seaborn as sns</span><span id="5165" class="nh la it oi b gy oq on l oo op">sns.set(color_codes=True)</span><span id="29fb" class="nh la it oi b gy oq on l oo op">plt.title("Embeddings")</span><span id="11fd" class="nh la it oi b gy oq on l oo op">for word in list(unique_word_dict.keys()):<br/>    <br/>    coord = embedding_dict.get(word) #------------------------Extracting Embeddings<br/>    <br/>    plt.scatter(coord[0], coord[1]) #-----------------------Plotting Embeddings<br/>    <br/>    plt.annotate(word, (coord[0], coord[1]))  #----------------Annotate tokens</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/719c2f70a496150e125f2dbe644b9d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0k06q1X-SVktpViR6psWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nu">图9: </strong>从模型中生成的嵌入的可视化表示。作者使用Jupyter笔记本创建的图像。</p></figure><h2 id="5fac" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">4.8观察</h2><p id="090a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在足球界，梅西和罗纳尔多经常被全球球迷拿来比较。因此，它们更有可能在数据中同时出现。像“最伟大”、“球员”、“得分”和“得分外”这样的术语经常被用来评论球员的表现。使用随机生成的12个文本，我们在上图中捕捉到了这些变化和共现。注意少数停用词没有被删除；因此他们出现在情节中。这可以使用我们的text_preprocessing_custom()函数来增强。正如前面所反映的，当我们研究数值时，图8中的嵌入可能没有多大意义。尽管如此，当绘制时，我们推断在相似上下文中使用的共现单词看起来彼此更接近。</p><h1 id="083c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.结论</h1><p id="e126" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Tensorflow的递归神经网络提供了一个嵌入层，可以在我们训练分类器模型时自动从文本中生成嵌入。这个过程包括使用文本矢量化将单词或句子映射到向量或一组数字。我们的独热向量——稀疏，因为它的大多数条目都是零——被嵌入层转换成密集的嵌入向量(密集，因为维度小得多，并且所有元素都是自然数)。它只需要一个完全连接的层来构成这个嵌入层(张量流，n.d .)。然而，当我们训练分类模型时，训练嵌入是耗时的。在这种情况下，如果我们有大量的数据，生成嵌入的灵活性允许我们减少创建分类模型所涉及的计算时间。嵌入也是比传统方法更好的单词表示，如单词袋和词频-逆文档频率。其他预先训练的模型，如伯特，生物伯特，手套，Edu伯特等。，为用户提供使用预训练模型生成嵌入的灵活性。</p><h1 id="3dd3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">6.参考</h1><ol class=""><li id="fca0" class="mo mp it lt b lu lv lx ly ma pq me pr mi ps mm mv mw mx my bi translated">布约卡斯，E. (2020年5月30日)。创建单词嵌入:使用深度学习在Python中编码Word2Vec算法。从中观网站检索:<a class="ae ky" rel="noopener" target="_blank" href="/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8">https://towards data science . com/creating-word-embeddings-coding-the-word 2 vec-algorithm-in-python-using-deep-learning-b 337d 0 ba 17 a 8</a></li><li id="7617" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm mv mw mx my bi translated">杰森·布朗尼。(2017年10月10日)。什么是文本的单词嵌入？从机器学习大师网站检索:<a class="ae ky" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">https://machinelementmastery . com/what-are-word-embedding/</a></li><li id="2669" class="mo mp it lt b lu mz lx na ma nb me nc mi nd mm mv mw mx my bi translated">张量流。(未注明)。基于RNN的文本分类。从TensorFlow网站检索:<a class="ae ky" href="https://www.tensorflow.org/text/tutorials/text_classification_rnn" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/text/tutorials/text _ classification _ rnn</a></li></ol></div><div class="ab cl pt pu hx pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="im in io ip iq"><p id="839d" class="pw-post-body-paragraph lr ls it lt b lu mq ju lw lx mr jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated"><em class="mn">关于作者:高级分析专家和管理顾问，帮助公司通过对组织数据的商业、技术和数学的组合找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；可以和我在</em> <a class="ae ky" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"> <em class="mn">上联系</em> </a> <em class="mn">和</em> <a class="ae ky" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"> <em class="mn">推特</em></a><em class="mn">；</em></p></div></div>    
</body>
</html>