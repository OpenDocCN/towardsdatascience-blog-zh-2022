<html>
<head>
<title>Using AI to Generate Art</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用人工智能产生艺术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-ai-to-generate-art-22e030497f73#2022-07-26">https://towardsdatascience.com/using-ai-to-generate-art-22e030497f73#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ce46" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">应用机器学习</h2><div class=""/><div class=""><h2 id="c500" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个支持语音的艺术生成工具</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/8d424cceadd046b00b0912e189b13084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*Yzesp8TtwwuwFnzb75MjxA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="ee19" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi lz translated">我已经阅读并应用了一些最新的人工智能研究，所以你不必这么做。在本文中，我将介绍最先进的文本到图像的生成模型，并应用它们来制作一个支持语音的艺术生成工具。</p><h2 id="d483" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">介绍</h2><p id="740b" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">在过去的几年里，文本到图像生成领域出现了一些开创性的论文。这个领域正变得越来越受欢迎，新的方式来赚钱这些艺术品在线。</p><p id="143d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">艺术生成模型在很大程度上建立在我以前的一些文章上，特别是生成方法，如<a class="ae nf" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">变分自动编码器(VAEs) </a>和<a class="ae nf" rel="noopener" target="_blank" href="/how-gans-learn-a-simple-introduction-6d21081773bd">生成对抗网络(GANs) </a>。</p><p id="09f9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">作为一个小小的剧透，这里有一些这些惊人的模型可以制作的图像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8cc97bd921622c0c9c8a57523cc4bd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*M3aJAYuoRmhZr64-j37uhg.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">绿色景观中的酷炫建筑(图片由作者提供)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ed07c67682ff403291d212f3f8d8fe8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*q2lmNvvjE8JKm3mIYwTcbQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">手绘草图(图片由作者提供)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/371b5dc1f46d93f72ade40453521d925.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*kGxYBdpuxRctQyviXAxFrQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">日落、船和湖(图片由作者提供)</p></figure><p id="dfd9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">如你所见，结果是惊人的。但是在我使用这个应用程序之前，先讲一点理论。我将首先回顾VAEs。然后我会谈到这些在VQ-维斯和VQ-甘斯是如何被改编的。最后，我将谈谈OpenAI的回形针，以及它如何改变了文本到艺术的游戏。</p><h2 id="4f87" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">VAEs概述</h2><p id="a292" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">可变自动编码器是这些模型工作方式的一个组成部分。自动编码器通过训练两个神经网络来学习。第一个是编码器，第二个是解码器。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/b752bf0364d832deec1416a8aa405a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-0SflikuGsemm-UqrAzh3g.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="3251" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">编码器的任务是将输入图像(训练数据)的维度缩小到更低维度的潜在向量。解码器的工作是从潜在向量中尽可能好地重建图像。</p><p id="32d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">标准自动编码器和VAEs之间的区别在于，VAEs不是寻找潜在向量，而是通过例如寻找平均值和标准偏差来参数化潜在空间(最常见的是各向同性高斯)。通过将这个潜在空间拟合为高斯型，我们就能够非常便宜地对这个高斯型进行采样，并将采样提供给解码器。然后，解码器可以用于生成看起来像训练数据的新图像。</p><p id="1297" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="nm">要点:自动编码器学习潜在向量和图像空间之间的映射。VAEs学习潜在空间和图像空间之间的映射。</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nn"><img src="../Images/1b600c9e7fe1dd2ea2036e59277bd5b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVY9ypw_JhIoFVA8akC9Gw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">ELBO(图片由作者提供)</p></figure><p id="adeb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VAEs试图最大化ELBO(证据下限)。如果你想知道这是从哪里来的，看看这篇文章。第一项是重建误差，第二项是后验q(z|x)和先验p(z)之间的KL散度。</p><ul class=""><li id="3a25" class="no np it lf b lg lh lj lk lm nq lq nr lu ns ly nt nu nv nw bi translated">q(z|x)是后验概率，在给定输入数据<em class="nm"> x </em>的情况下，看到潜在向量的概率</li><li id="76dc" class="no np it lf b lg nx lj ny lm nz lq oa lu ob ly nt nu nv nw bi translated">p(z)是先验分布，我之前说过通常是高斯分布</li></ul><p id="86b5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最小化第二项本质上是一种正则化模型的方法，试图总是使空间尽可能接近先验(例如高斯)。</p><h2 id="e6de" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">VQ值(向量量化值)</h2><blockquote class="oc od oe"><p id="62a6" class="ld le nm lf b lg lh kd li lj lk kg ll of ln lo lp og lr ls lt oh lv lw lx ly im bi translated">基于:<a class="ae nf" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank">神经离散表示学习</a>，作者:亚伦·范·登·奥尔德、奥里奥尔·维尼亚尔斯、科拉伊·卡武克库奥卢。2017年11月。[1]</p></blockquote><p id="9a94" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">今天，许多最先进的人工智能模型，如DALL-E和VG-GANs，都是基于VQ-瓦斯。正确理解这篇论文是最基本的。VQ-VAE是由Deep Mind开发的一个模型，它提高了VAE生成图像的能力。</p><p id="f18f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VAEs和VQ-VAEs有两个主要区别。VQ-维斯学习它们的先验，并且编码器的输出是离散的(而不是学习潜在分布)。</p><p id="013b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VQ-维斯仍然在学习潜在空间和图像空间之间的映射，然而，他们将潜在空间离散化。</p><p id="9a5e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">图像的离散输出背后的推理是，语言本质上是离散的，可以用一系列离散的符号来表达。图像可以通过语言来描述，因此解码器应该能够从离散空间重建图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oi"><img src="../Images/d2ff2163258d32d9fcb7630b0c93a816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5V-6IyzmLUq48g1Nv8qA2Q.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">图片来自<a class="ae nf" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank">神经离散表示学习</a>论文【1】</p></figure><p id="e35b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VQ-维斯首先使用编码器将图像编码成一系列矢量。然后，使用“码本”将编码矢量映射到嵌入空间。码本是可学习的，它是图中所示的作为嵌入空间的一系列向量。人们可以把码本想象成字典，把离散化的潜在空间想象成我们的语言。一旦使用码本映射了模型的输出，我们基本上就有了输入图像的描述。由解码器获取该编码描述，并重建原始图像。</p><p id="e843" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，来自我们的编码图像的每个向量被映射到来自码本的嵌入向量。实现的方式是通过选择码本中最接近每个编码矢量的矢量(距离是L2范数)。</p><p id="cd5b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一旦这些向量被映射到它们的离散化潜在空间，它们就被馈送到解码器，解码器有望精确地重建输入图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oj"><img src="../Images/e117cb8e05faeb71258643c57d637149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W20vAjjizbuaXGlPD5UrXQ.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">VQ-维斯损失[1]</p></figure><p id="517d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以上是VQ-维斯的损失项。将其与VAE·爱尔博损失进行比较，可以看到我们有类似的重建误差。我们不再有KL(因为统一的先验而消失)，取而代之的是第二个和第三个项。在第二项中，我们有一个停止梯度，它冻结编码矢量(z)并帮助学习码本矢量(e)。第三项是相反的，它冻结码本向量，并帮助学习编码向量。</p><p id="abd2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">解码器仅使用第一项进行优化，编码器使用第一项和最后一项，嵌入(学习码本向量)仅使用中间项。</p><p id="ad65" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在整个训练过程中，先验保持一致，但是，在训练之后，它们使用PixelCNN拟合新的先验。</p><h2 id="8b25" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">VQ-甘斯(矢量量化甘斯)</h2><blockquote class="oc od oe"><p id="3f57" class="ld le nm lf b lg lh kd li lj lk kg ll of ln lo lp og lr ls lt oh lv lw lx ly im bi translated">基于:Patrick Esser、Robin Rombach、bjrn Ommer的《驯服高分辨率图像合成的变形金刚》。2020年12月[2]</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ok"><img src="../Images/177d44055d5f27fa3539ca45cb4cf379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-f3weB-rdKtk9wq2ByHVQ.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">高分辨率合成图像由<a class="ae nf" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成变压器</a>【2】</p></figure><p id="aa98" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VQ-甘斯是VQ-维斯的改进。这篇论文是开创性的，因为它首次能够通过使用变压器生成连贯的高分辨率图像。他们能够通过应用基本假设来做到这一点，即图像中的低级结构可以通过图像中的局部连通性来描述，而这种方法不能用于图像的高级结构。</p><p id="d1f5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些模型结合使用了GANs和VAEs。作为对GANs的快速提醒，这些模型分两部分学习。第一个是生成图像的生成器，第二个是鉴别器，一个分类器，试图识别你给它的图像是真是假。这些模型可以一起学习，本质上是试图欺骗对方。一旦它们被训练好，你就用生成器来生成新的图像。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ol"><img src="../Images/9a4ffd4cb1037bac4c6e081e0cb7ccd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3K-0YplDtmS6dHfbcGvJ7A.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">图像由<a class="ae nf" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank">驯服高分辨率图像合成变压器</a> [2]</p></figure><p id="aa03" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上图展示了VQ甘斯的建筑。您可能已经意识到，底层模块是集成到该架构中的VQ-瓦斯，因为它充当GAN的发生器。</p><p id="4d27" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">VQ-甘斯本质上是对VQ-VAE的改进。两个主要区别是，他们没有使用PixelCNN作为模型来拟合先验，而是拟合了一个变压器(他们使用了OpenAI的GPT-2)。第二个区别是他们改进了损失函数，这次使用了对抗性损失(使用GAN架构)。</p><p id="2576" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你可能会注意到上图中鉴频器的输出是一个矩阵，而不是二进制输出。这是因为他们使用了补丁GAN。这将获取图像的重叠碎片，并像普通鉴别器一样对每个碎片进行操作。</p><p id="85c4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">图像中的变换器像顺序语言模型一样被训练，其中变换器试图预测编码码本向量的序列。然后，变换器用于学习码本嵌入向量。</p><h2 id="0be2" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">对比图像语言预训练</h2><blockquote class="oc od oe"><p id="3840" class="ld le nm lf b lg lh kd li lj lk kg ll of ln lo lp og lr ls lt oh lv lw lx ly im bi translated">基于:<a class="ae nf" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型</a>亚历克·拉德福德等人2021年2月。[3]</p></blockquote><p id="bba8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">到目前为止，我们已经看到了生成图像的方法，但是我们如何将它与文本联系起来呢？OpenAI的剪辑是一个模型，可以确定哪个标题与哪个图像最匹配。它不能生成图像，但它是图像生成和文本之间的连接。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oj"><img src="../Images/8bd186f77641a4f0e8acb7abd05f8ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4C73xJtbpWQFxnenW3xWuw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">图像由<a class="ae nf" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型</a> [3]</p></figure><p id="4194" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以上是CLIP模型的架构。图像中显示的模型有三个部分。</p><ol class=""><li id="9b95" class="no np it lf b lg lh lj lk lm nq lq nr lu ns ly om nu nv nw bi translated"><em class="nm">对比预培训</em></li></ol><p id="78e9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">模型的输入是图像和描述文本对。文本和图像输入分别通过各自的编码器。一旦这样做了，你就可以构建上面矩阵所示的对比空间。训练的目标是将蓝色方块的余弦相似度推至1，将所有其他方块的余弦相似度推至0。通过这种方式，我们教导文本编码器和图像编码器将它们的输入编码到相似的空间。</p><p id="c199" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">2.<em class="nm">从标签文本创建数据集分类器</em></p><p id="51f3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对比预训练不足以确定文本和图像是否相同。我们需要进行某种分类。例如，假设我们有一只狗和一只猫的两张图片。这些图像可能彼此非常相似。如果我们拍摄了狗的图像，但是文本显示为猫，我们希望CLIP知道这个图像与所涉及的文本无关。</p><p id="afa5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了将该分类器功能应用于预训练的模型，他们首先需要建立分类器数据库。他们首先从标记的图像数据中构建样本句子。网上有很多带标签的图像数据集，如CIFAR-10、ImageNet等。这些图像被分配了类别，而不是句子。为了解决这个问题，他们将类添加到句子中。</p><p id="4586" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">例如，如果你有一张狗的图片，你可以给文本编码器一个句子，比如“一张狗的照片”。我们这样做是为了构造数据集中每个类的嵌入向量。</p><p id="1831" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">3.<em class="nm">用于零炮预测</em></p><p id="b3c5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一旦我们有了学习过的嵌入向量，我们可以给图像编码器相应的图像对到文本，我们可以比较编码器的输出和我们的嵌入。余弦相似度最大的将是该类。</p><p id="de10" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">文本和图像编码器都可以通过这种方式进行改进，从而得到更好的结果。</p><h2 id="3fd2" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">VQ-甘+剪辑</h2><p id="8097" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">因此，我们有VQ-甘斯，它可以基于离散空间生成图像，我们有CLIP，它可以评估图像与文本的匹配程度，反之亦然。</p><p id="6e44" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当文本输入到VQ-甘-剪辑，VQ-甘首先用随机噪声初始化，并产生一个图像。然后，该图像和编码的码本向量可以被馈送到剪辑图像编码器，并根据它们与文本输入的相似性进行评估。然后可以相应地更新VQ-甘的权重以反映文本输入。这可以递归地进行N次迭代，直到生成器的输出是文本输入的满意表示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9d3e1ad0d6f8cc75572ef84c9a900ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/1*wEtL4YGtC5HzQ-DEfqxLZA.gif"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">算法的收敛性(作者GIF)</p></figure><p id="5310" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">还有更先进的模型，如OpenAI的DALL-E和最近的DALL-E2。(DALL-E这个名字来源于Dalí和WALL-E的混合，非常聪明)。这些模型产生了文本输入的极其复杂的表示。然而，它们不是公开的，所以我不能乱来，也不能在这里展示他们的作品。但是你可以通过<a class="ae nf" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">这个链接</a>查看其中的一些。</p><h2 id="1e36" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">我自己的艺术生成应用</h2><p id="ebf1" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">现在我已经看完了所有的理论和最新的模型，我想自己应用它们。之前我做了自己的<a class="ae nf" rel="noopener" target="_blank" href="/using-ai-to-make-my-own-smart-assistant-app-5ad015449447">智能助手App </a>。在这里，我想修改它，让它能够使用人工智能生成艺术。你可以在<a class="ae nf" href="https://github.com/diegounzueta/Medium-Articles-/tree/master/AssemblyAI%20Smart%20Assistant" rel="noopener ugc nofollow" target="_blank">我的Github </a>上查看这个应用的一些代码。</p><p id="380c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个想法是能够通过我的声音访问这些模型。我想正常使用智能助手，但如果我给模型一个“触发词”，它会使用句子的其余部分作为提示，生成一幅艺术作品。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oo"><img src="../Images/07c49d0d5352c96d91e8e625c3c0b496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOZOjH-iyDPAzI73Y9Tlwg.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">应用架构(图片由作者提供)</p></figure><p id="225e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了将音频转录为文本，我使用了AssemblyAI的Websocket api来实时地将我的讲话转录为文本。为了执行模型，我使用了<a class="ae nf" href="https://cloud.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌云</a>的VertexAI。</p><p id="a963" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了运行模型并生成艺术作品，我需要将模型作为作业在云上执行。我不能在我的本地机器上运行这些模型，因为它们真的需要GPU。为了运行这些模型并生成图像，我首先通过设置一个“setup.py文件”来安装所需的库。然后，我在可以访问虚拟GPU的虚拟机上执行作业。</p><p id="5aa5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最后要做的是配置智能助手，使其能够生成艺术。我通过简单地添加一个if语句来合并“触发词”，如果句子以“生成”开头，那么我将句子的其余部分传递给艺术生成器模型，作为Vertex AI的一项工作。工作完成后，我将图像保存到一个桶中，然后我可以访问并加载到应用程序中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi op"><img src="../Images/c48b9cab4f0135facf022259db3b7796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jMNJKzkI4a3DoSyyAQS5Q.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="6d80" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在这里，我要求机器人“生成一幅城市天际线的画”。触发词是“生成”，然后应用程序获取句子的其余部分，并将其传递给作业请求。我发现这个触发词是可靠的，而且效果很好。</p><p id="e0b1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">就这样，我现在有了一个智能助理应用程序，可以根据请求为我生成艺术作品。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oq"><img src="../Images/bb24e0f1df6d3fe3c6f5c2d23cb7efc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgSmq-CQcgnW-OBEpj4mRw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><h2 id="889d" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">结论</h2><p id="849c" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">在这篇文章中，我通过最新的人工智能研究论文的技术方面来从文本中生成图像。我回顾了基本的生成器方法架构，比如VAEs(可变自动编码器)和GANs(生成对抗网络)。然后我会谈到如何在矢量量化的研究论文中采用这些方法来提高它们生成图像的能力。总之，这些方法设法离散化状态空间，当与CLIP(一种比较图像与文本相似性的方法)结合使用时，这些模型能够生成令人惊叹的艺术作品。</p><p id="1fe7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我特别兴奋地看到这一研究领域将如何在未来传授给数字艺术行业，如视频游戏或动画电影，因为我相信这项技术有可能彻底改变这些行业！</p><h2 id="837f" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">支持我</h2><p id="ae46" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">希望这对你有所帮助，如果你喜欢，可以<a class="ae nf" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lf jd">跟我来！</strong>T3】</a></p><p id="5703" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">您也可以使用我的推荐链接成为<a class="ae nf" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lf jd">中级会员</strong> </a> <strong class="lf jd"> </strong>，并访问我的所有文章及更多:<a class="ae nf" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="1276" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">你可能喜欢的其他文章</h2><p id="b1f5" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated"><a class="ae nf" href="https://medium.com/p/4f50d1e3d4d2" rel="noopener">支持向量机</a></p><p id="cc67" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae nf" href="https://medium.com/p/d1d5bad79e72" rel="noopener">精度vs召回</a></p><h2 id="abe1" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">额外收获:如何向Vertex AI启动作业</h2><p id="ec56" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">作为一点额外的补充，我想包括一些关于如何部署这些模型的信息，因为我遇到了很多困难，而且这并不像看起来那么简单。</p><p id="9134" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这些模型运行在云中预先构建的映像上。在GCP上有很多，我用的是“Europe-docker . pkg . dev/vertex-ai/training/py torch-GPU . 1–10:latest”。</p><p id="8587" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然而，为了让模型运行起来，我还需要一些镜像中没有的包。为了解决这个问题，我随自定义作业一起传递了一组包。</p><p id="51cc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了传递这些必须编译成tar.gz文件的软件包，我使用了“setup tools”Python库。</p><p id="e1d4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，我必须将Python作业包装成一个模块。这是通过添加__init__来完成的。py文件添加到task.py文件的位置，并稍微修改python脚本。</p><p id="d7c2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是我使用的自定义工作函数:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="6ef4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">作业不能每次都是相同的python脚本。这是因为每次我请求不同的画时，标题都会改变。每当我启动一个作业时，我都需要将该信息传递给那个特定的作业。</p><p id="57dc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我这样做的方式是通过旗帜。在执行Python文件时，可以通过命令行将标志作为选项传递。有了absl-py库，我能够阅读这些，并把它们传递给艺术生成模型。</p><p id="2dd4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">下面是我发送给VertexAI的任务的task.py文件结构。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="5d2d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">希望这有所帮助:)</p><h2 id="d0cc" class="mi mj it bd mk ml mm dn mn mo mp dp mq lm mr ms mt lq mu mv mw lu mx my mz iz bi translated">参考</h2><p id="d4a5" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">[1] A. van den Oord，O. Vinyals和K. Kavukcuoglu，“神经离散表征学习”(VQ-VAEs)，<em class="nm">康奈尔大学</em>，2017年。可用:【https://arxiv.org/abs/1711.00937】T2。</p><p id="ff06" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2] P. Esser、R. Rombach和B. Ommer，“驯服高分辨率图像合成的变形金刚”(VQ-甘斯)，<em class="nm">康乃尔大学</em>，2020年。可用:<a class="ae nf" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2012.09841</a></p><p id="cf65" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3] A .拉德福德等，“从自然语言监督中学习可转移视觉模型”(CLIP)，<em class="nm">康乃尔大学</em>，2022。可用:<a class="ae nf" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.00020</a></p></div></div>    
</body>
</html>