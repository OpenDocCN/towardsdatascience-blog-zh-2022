<html>
<head>
<title>Extract State-of-the-Art Insights from every Piece of Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从每一篇文本中提取最先进的见解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-state-of-the-art-text-embeddings-with-hardware-accessible-by-everyone-46bc7d084703#2022-07-01">https://towardsdatascience.com/generating-state-of-the-art-text-embeddings-with-hardware-accessible-by-everyone-46bc7d084703#2022-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ea68" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">不需要OpenAI GPT-3和类似的工具来为你的自然语言处理任务获得最先进的语义文本洞察。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/294d1aeb43cb7ec6a9d8df79fe730c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n-LimL396XEM5oFdk4VeRw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="e553" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这篇文章中，我将展示大型语言模型，如<strong class="ky ir"> GPT-3，并不能通过其密集的文本嵌入</strong>为许多NLP(自然语言处理)任务生成最佳的语义文本洞察，以及<strong class="ky ir">如何让每个人都可以用广泛可用的硬件生成最先进的嵌入</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/24eff4faf4a265e42a65e20d98e64f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*QxvH99_YHxtZyeil1-SQrQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:强力胶模型对比—全面评测<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="b874" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">密集文本嵌入是编码单词含义的文本的矢量表示。这样，意义相近的词在矢量表示中是相近的。这些嵌入对于复杂的任务非常有用，例如比较文本内容的相似性、语义搜索、释义挖掘等。在过去的几年中，语言模型的规模呈指数增长，由于资源的限制，许多从业者几乎不可能用最新的模型进行实验。我可能有一个解决方案给你😉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/d793d5a0cfb7f4852f040dae751dde8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwcrYS4zzdAHUCSATNvJdQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:蒸馏伯特，伯特的蒸馏版本:更小、更快、更便宜、更轻——<a class="ae kv" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.01108.pdf</a></p></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="06ff" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">问题陈述和前进方向</h1><p id="fa9a" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">我们大多数人面临的限制是，我们没有权限或财力部署大型GPU/TPU集群来运行大型语言模型，如T5、MPNet或GPT-3。为了克服这些限制并获得最先进的文本嵌入，我在以下两个步骤上花了很多时间，我想与你们分享:</p><ol class=""><li id="b3e9" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">基于所有相关语言模型的性能和模型参数的数量对其进行比较和排序，以找到最有效的预训练模型。</li><li id="d4c9" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">应用并比较不同的微调技术如何提高预训练模型的性能。</li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="6800" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">动机</h1><p id="d4f5" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">当然，我有自己的理由生成高质量的文本嵌入。我的目标是使用文本嵌入作为我自己的模型的输入，以帮助决定我自己的金融资产选择。由于我收到了很多公开我的模型的请求，我发起了一个名为<a class="ae kv" href="https://www.pinklion.xyz/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> PinkLion </strong> </a>的项目。</p><p id="951e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我设计了PinkLion，通过提供对潜在预测模型的访问，实现动态投资组合优化和资产分析。这些模型可以访问成千上万只股票、基金/ETF和加密货币的日常资产数据。</p><blockquote class="nv nw nx"><p id="1096" class="kw kx ny ky b kz la jr lb lc ld ju le nz lg lh li oa lk ll lm ob lo lp lq lr ij bi translated">请随意尝试，并分享反馈🙏。(还处于粗略状态)<a class="ae kv" href="http://www.pinklion.xyz/" rel="noopener ugc nofollow" target="_blank"> www.pinklion.xyz </a></p></blockquote><div class="oc od gp gr oe of"><a href="https://www.pinklion.xyz/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">粉色狮子</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">不再猜测，投资时使用科学。股票，基金和密码在一个地方。用机器学习改善你的投资组合。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">www.pinklion.xyz</p></div></div><div class="oo l"><div class="op l oq or os oo ot kp of"/></div></div></a></div></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="ecf3" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">1.根据性能和规模对语言模型进行比较和排序</h1><p id="ccdf" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">作为第一步，我们必须对现有的语言模型、它们的性能以及它们的规模有一个很好的了解。因此，目标是基于各种基准测试任务比较不同的语言模型，并确定它们相对于其参数数量的表现如何。</p><h2 id="62dd" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">模型性能的定义—胶水和强力胶</h2><p id="e6ea" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">尽管阅读了大量研究论文，但令人惊讶的是，很少发现跨多个基准任务(如SentEval [1]、GLUE [2]、SQuAD[3]、RACE [4]或SWAG [5])的完全透明的模型评估。在进一步研究这个话题后，我得出结论，认为<a class="ae kv" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ny">【通用语言理解评测】【2】</em></strong></a><strong class="ky ir"><em class="ny"/></strong>似乎是最广泛使用的评测语言模型性能的基准。</p><blockquote class="pg"><p id="c9cd" class="ph pi iq bd pj pk pl pm pn po pp lr dk translated">GLUE是一个工具和数据集的集合，用于评估一个模型在一组九个自然语言任务中的性能。这九项任务集中在文本相似性和释义、分类和推理领域。</p></blockquote><p id="1c7e" class="pw-post-body-paragraph kw kx iq ky b kz pq jr lb lc pr ju le lf ps lh li lj pt ll lm ln pu lp lq lr ij bi translated">有了这个结论，我开始收集所有相关语言模型的GLUE评估结果。尽管GLUE被广泛使用，但是仍然很难获得所有相关的分数，尤其是对于较小的模型版本。本文中的所有后续数字都是按照以下顺序从GLUE排行榜、模型的研究论文或相应模型的Github页面获得的。</p><p id="33ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每个对如何详细组成<a class="ae kv" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"> GLUE [2] </a>基准感兴趣的人来说，这里是组成基准的任务名称、缩写、描述和度量。</p><blockquote class="nv nw nx"><p id="36d8" class="kw kx ny ky b kz la jr lb lc ld ju le nz lg lh li oa lk ll lm ob lo lp lq lr ij bi translated">如果您对详细的粘合任务不感兴趣，请跳到下一节“<strong class="ky ir">模型比较和排名”。</strong></p></blockquote><ul class=""><li id="b80c" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr pv nn no np bi translated"><strong class="ky ir">语言可接受性语料库(CoLA) </strong>:判断一个句子的语法是否正确。— <em class="ny">指标:马修相关性</em></li><li id="dcfa" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">斯坦福情感树库(SST-2) </strong> : <strong class="ky ir"> </strong>判断句子是正面情感还是负面情感。——<em class="ny">公制:</em> <strong class="ky ir"> <em class="ny"> </em> </strong> <em class="ny">准确度</em></li><li id="ee8f" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">微软研究院释义语料库(MRPC) </strong>:判断两个句子是否是彼此的释义。— <em class="ny">指标:准确性/F1 </em></li><li id="a092" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">语义文本相似度基准(STS-B) </strong>:确定两个句子的相似度，分值从1到5。— <em class="ny">指标:皮尔森相关性/斯皮尔曼相关性</em></li><li id="e76c" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">问答式自然语言推理(QNLI) </strong>:判断问题的答案是否在第二句话中。(这个数据集是从班数据集构建的。)— <em class="ny">度量:准确度</em></li><li id="a916" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated">Quora问题对(QQP) :判断两个问题是否语义等价。— <em class="ny">指标:准确性/F1 </em></li><li id="a1f9" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">多体裁自然语言推理(MNLI) </strong>:确定一个句子是否包含、反驳或与一个给定的假设无关。(该数据集有两个版本，一个版本的验证和测试集来自同一个发行版，另一个版本称为不匹配，其中验证和测试使用域外。— <em class="ny">指标:准确度</em></li><li id="a79d" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir">识别文本蕴涵(RTE) </strong>:确定一个句子是否包含给定的假设。— <em class="ny">指标:准确度</em></li><li id="c8d5" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir"> Winograd自然语言推理(WNLI) </strong>:判断含有匿名代词的句子和替换了该代词的句子是否包含。(该数据集是从Winograd模式挑战数据集构建的。)— <em class="ny">度量:准确度</em></li></ul><p id="cdda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过取所有任务的平均值来计算最终的粘合分数。像MRPC和STS-B这样有两个跟踪指标的任务首先被平均，然后被认为是该任务的单一得分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/0345908321049bfb0265be32b499da06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIyzdw3IdTwQ08GmtAjJ2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:胶水任务描述—表格可在此处<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">找到</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><h2 id="72cd" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">模型比较和排序</h2><p id="20fa" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在定义了我们如何度量模型的性能之后，这里是收集到的不同语言模型家族的见解。下表显示了每个模型的参数数量、模型的内存存储量(这些数字已从Tensorflow Hub或相应模型的Github页面获得)以及上文所述的粘合分数。最后，<strong class="ky ir">最后一列<em class="ny">模型性能/大小比率</em>显示了基于模型的粘合分数及其参数数量计算的比率。</strong></p><blockquote class="pg"><p id="8e52" class="ph pi iq bd pj pk pl pm pn po pp lr dk translated">模型性能/尺寸比=粘合分数/模型参数数量</p></blockquote><p id="6607" class="pw-post-body-paragraph kw kx iq ky b kz pq jr lb lc pr ju le lf ps lh li lj pt ll lm ln pu lp lq lr ij bi translated">换句话说，如果一个模型在GLUE benchmark上用更少的参数表现得更好，那么这个比率就更高。因此，<em class="ny">型号性能/尺寸比</em>栏中的数字越高越好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/0d6952d4476509de21c7adfcd07fc542.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*8T2guEybYMM7VFFl4UsheA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:语言模型对比—全面评测<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="7e4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从整体来看，很明显，随着模型尺寸的增加，性能/尺寸比迅速降低。因此，模型中每个增加的参数都有很强的收益递减。简单看一下Google<a class="ae kv" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">【11】</a>已经公布的T5车型家族，这一点就变得非常明显。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/78795ff74ee9e5303fdd84827f95d3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*9gTRGbjAl2z3wlmKMivkpQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:T5家族对比—全面评测<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="941f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最小的T5型号T5-small拥有6000万个参数，粘合分数为77.4，性能/尺寸比为1.29。相比之下，最大的T5型号T5–11B的胶合分数为90.3，比T5-small高12.9分。</p><p id="c4f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了实现12.9点或16.6%的分数增加，模型大小必须从6000万个参数增加到110亿个参数，即增加了18300%🤯。</p><p id="20d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你可能想知道为什么GPT-3没有出现在上表中。原因是GPT-3不是在普通胶水基准上正式评估的，而是在<a class="ae kv" href="https://super.gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">强力胶【6】</a>基准上评估的。</p><blockquote class="pg"><p id="f987" class="ph pi iq bd pj pk pl pm pn po pp lr dk translated">SuperGLUE是在image GLUE之后创建的一个新基准，它具有一组新的更难的语言理解任务。</p></blockquote><p id="749a" class="pw-post-body-paragraph kw kx iq ky b kz pq jr lb lc pr ju le lf ps lh li lj pt ll lm ln pu lp lq lr ij bi translated">幸运的是，强力胶排行榜列出了普通胶水基准测试中的GPT-3和多个型号(伯特-大号、罗伯塔-大号、T5–11B)。比较伯特-基地和GPT-3，GPT-3只有更高的强力胶分数2.8分(69.0 → 71.8)。尽管GPT 3号的参数是它的1605倍。同样，RoBERTa-large和T5–11B的参数比GPT-3少，但得分更高，分别为84.6和89.3。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/24eff4faf4a265e42a65e20d98e64f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*QxvH99_YHxtZyeil1-SQrQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:强力胶对比—全面评测<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="1943" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">了解到真正的大型模型，如GPT-3在强力胶上的表现优于其他模型(罗伯塔-大型和T5–11B)，我们可以假设GPT-3在普通胶水基准上也优于这些模型。</p><p id="a1e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们正常的GLUE比较，我们可以看到实现最佳<em class="ny">性能/大小比</em>的语言模型家族是ALBERT (A Lite BERT) <strong class="ky ir"> </strong>模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/07207f37525fdb10bc88ab3594600221.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*m_UwXhHOuzDSfF8noiKjGw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:阿尔伯特家族对比—全面评估<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="6add" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ALBERT-base模型实现了<strong class="ky ir"><em class="ny"/></strong>的<strong class="ky ir"> 6.986 </strong>，是所有测试语言模型中最高的比率。高比率可以解释为非常好的胶合分数83.9，同时是所有测试模型中最小的，只有1200万个参数。因此，ALBERT-base提供了每个部署参数的最佳性能。由于参数数量很少，艾伯特-基和艾伯特-大可以很容易地在K80或P100 GPU上训练。</p><h2 id="69d9" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">模型比较的结论</h2><p id="caa7" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">当用有限的资源生成密集文本嵌入时，最好考虑的模型族是ALBERT族。在所有评估的语言模型中，ALBERT-base和ALBERT-large具有最好的<em class="ny">性能/大小比</em>。两者都有1200万和1800万个参数，胶合分数分别为83.8和85.7。这使得这两种型号成为最小的型号之一，同时提供了与T5型号相当的性能，而T5型号则大得多。此外，GPT-3在SuperGLUE基准测试中表现不佳，如果下游任务与文本生成无关，则不应该选择它作为模型。完整评价可以在<a class="ae kv" href="https://docs.google.com/spreadsheets/d/1Oe-v2zhYyFW3P1K-mLZME0pwxWMQ9qxP0fhZCRZ_XVA/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">这里</strong> </a>找到。</p><p id="6031" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的章节中，我将展示如何使用不同的微调技术来进一步提高<strong class="ky ir"> ALBERT-base </strong>的性能。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="296d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">2.通过应用和比较不同的微调技术来提高预训练模型的性能</h1><p id="f400" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在选择了ALBERT-base作为我们的语言模型之后，我们现在想要微调一般训练的模型，使它更适合我们自己的应用程序。有许多方法可以微调模型以生成文本嵌入。然而，不同的微调方法会导致非常不同的定性结果。因为比较不同的微调技术需要付出大量的努力，所以我想分享我自己的结果，以便将来其他人更容易理解。</p><p id="91ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一节中，我们将比较三种不同的微调过的ALBERTs，它们都是用不同的微调技术改变的。这里是不同方法和结果的初步概述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi py"><img src="../Images/320ff1274698686e7ebda89dba25e77d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*uAh6-0MWDfarm5lC2078lg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:比较ALBERT-base微调架构—全面评估<a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/3-Evaluate_All_ALBERT_Classifiers.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><ol class=""><li id="1aef" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated"><strong class="ky ir">冻结艾伯特:</strong>冻结艾伯特由常规预训练艾伯特基础模型组成，该模型被冻结，因此在微调时不会更新其参数。此外，它还有一个可训练的分类头，包括一个dense和一个softmax分类层。该模型将作为我们的<strong class="ky ir">基线基准</strong>，因为它将代表ALBERT-base在没有重大微调的情况下的表现。</li><li id="60de" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">经过训练的艾伯特:经过训练的艾伯特和冰冻的艾伯特有着完全一样的架构，然而，所有的组件都是完全可以训练和微调的。这意味着ALBERT-base组件和分类头(密集层+ Softmax层)可以针对分类微调任务完全调整它们的权重。</li><li id="0ba7" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated"><strong class="ky ir">暹罗阿尔伯特:</strong>暹罗阿尔伯特的灵感来自通用<a class="ae kv" href="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf" rel="noopener ugc nofollow" target="_blank">暹罗框架【7】</a>，在我们的例子中，它包括三个相同的基于阿尔伯特的网络，这些网络使用共享权重。除了三个子网络之外，还为分类微调任务添加了由softmax层表示的分类头。</li></ol><h2 id="54a8" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">资料组</h2><p id="ef1e" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">为了了解不同的微调方法在现实世界中的比较，我选择了<a class="ae kv" href="https://www.kaggle.com/datasets/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank"> HuffPost新闻分类数据集【8】</a>作为文本语料库。该数据集包含2012年至2018年间从《赫芬顿邮报》收集的<strong class="ky ir"> 200853 </strong>条新闻记录，标题和描述分布在41个独特的新闻类别中。因此，这个数据集非常适合于查看我们可以在多大程度上针对分类任务对模型进行微调。这是数据集的一个示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/1816c7e350ce1c803829139dbd832af8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3k01dByE7juU49JdbOSpJA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:赫芬顿邮报新闻分类数据样本——由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><blockquote class="pg"><p id="61a5" class="ph pi iq bd pj pk qa qb qc qd qe lr dk translated">我们微调任务的目标是根据给定的新闻标题预测新闻类别。</p></blockquote><h2 id="7957" class="ou ml iq bd mm ov qf dn mq ox qg dp mu lf qh pa mw lj qi pc my ln qj pe na pf bi translated">数据准备</h2><p id="097a" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">由于我们希望仅使用新闻标题作为输入来预测新闻类别，因此我们首先希望通过计算和绘制每个新闻标题的字符和字数分布来获得数据集的更好概述。</p><p id="7c3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于新闻标题的字数，最小字数为0，最大字数为44，平均一个新闻标题有9个字。然而，从分布情况来看，我们可以得出，由于许多标题有大量的单词，所以平均值被夸大了。</p><p id="3735" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新闻标题的字符计数的分布显示了类似的情况，其中大量标题具有大量字符。对于字符计数，0是最小计数，320是最大计数，平均一个标题有57个字符。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qk"><img src="../Images/8b632d3d3b39cac0ad2ad61540440012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhxchXp4hT270qmGP6fg-g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:赫芬顿邮报新闻标题单词和字符的概率分布——由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><p id="db1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了提高输入数据的质量，我们删除了每个标题少于10个单词的所有新闻记录。应用此条件<strong class="ky ir">将记录数量从200853减少到189339。</strong></p><p id="6419" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">删除标题少于10个单词的所有记录，并应用80/20训练和验证数据集拆分后，类别分布如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/190bda0ff9edea12e6c20a2e68f29cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*J4g82_YpvW_4KpW-fIYRqw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:赫芬顿邮报训练和验证拆分新闻类别——由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><p id="f11d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在很明显，我们有一个高级别不平衡，与<em class="ny">政治</em>是占主导地位的阶级。这种偏差将在训练过程中通过给每个类分配不同的权重来纠正(稍后会有更多的介绍)。</p><h2 id="eedb" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">比较微调技术</h2><p id="71e9" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在对我们的3个模型(<em class="ny">冰冻艾伯特</em>、<em class="ny">训练艾伯特</em>、<em class="ny">暹罗艾伯特</em>)进行了训练之后，我们现在能够更详细地查看模型的性能，该训练数据集具有160682条对新闻类别进行分类的记录。</p><p id="812f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下所有指标均来自验证数据集，该数据集显示，从整体来看，<strong class="ky ir"> <em class="ny">暹罗阿尔伯特</em>的性能最好</strong>，<strong class="ky ir"> </strong>，其次是<em class="ny">受过训练的阿尔伯特</em>，然后是<em class="ny">冷冻阿尔伯特</em>。</p><blockquote class="pg"><p id="4bb4" class="ph pi iq bd pj pk pl pm pn po pp lr dk translated"><strong class="ak"> <em class="qm">暹罗阿尔伯特</em>几乎在所有指标上表现最佳</strong></p></blockquote><p id="15fc" class="pw-post-body-paragraph kw kx iq ky b kz pq jr lb lc pr ju le lf ps lh li lj pt ll lm ln pu lp lq lr ij bi translated">首先比较<em class="ny">冻结的艾伯特</em>和<em class="ny">训练的艾伯特</em>，前者使用基于艾伯特的网络的未改变的权重，后者由于赫芬顿邮报的训练过程而具有微调的权重，我们已经可以看到显著的性能提升。经过<em class="ny">训练的艾伯特</em>的准确率为58%，F1得分为60%，AUC-micro值为0.996，而<em class="ny">冷冻艾伯特</em>的准确率为32%，F1得分为35%，AUC-micro值为0.905。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qn"><img src="../Images/f523e5186feb323bbe60f71e8ba84562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0lQPaQpnDImg_sOOxy-kg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:比较ALBERT-base微调指标—全面评估<a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/3-Evaluate_All_ALBERT_Classifiers.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><p id="a9fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步比较<em class="ny">siame ALBERT</em>我们可以看到，siame版本在几乎所有指标上都优于其他两款车型。<em class="ny">暹罗阿尔伯特</em>在验证数据集上的准确率为63%，几乎是<em class="ny">冷冻阿尔伯特</em>准确率的两倍，也高于<em class="ny">训练过的阿尔伯特</em>的准确率。综上所述，Siamese ALBERT的准确率为63%，F1分数为64%，AUC-micro值为0.946，略低于<em class="ny">训练过的ALBERT </em>方法的同等分数。</p><p id="0b7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们最初的目标是创建最好的文本嵌入，这里是由Siamese ALBERT生成的嵌入。正如我们所看到的，我们确实为不同的新闻标题类别实现了不同的聚类。暹罗嵌入的一个有趣的特征是，它不仅为不同的类别创建聚类，而且具有类似内容的新闻标题类别，如T <em class="ny">世界邮报</em>、<em class="ny">世界邮报</em>和<em class="ny">世界新闻</em>在嵌入空间中彼此靠近。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/a44c11606687b234c74244c16b2f420d.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*ptG2aQVU0c__LCtfYwLx1A.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:暹罗阿尔伯特新闻标题嵌入——由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创建</p></figure><p id="58bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不同评估步骤和嵌入代的完整代码可在我的<strong class="ky ir"/><a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/3-Evaluate_All_ALBERT_Classifiers.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">Github</strong></a>上获得。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="7243" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">暹罗阿尔伯特的实现</h1><p id="6b70" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">上面介绍的<em class="ny"> Siamese ALBERT </em>是在硬件受限的情况下产生高质量文本嵌入的最佳方法。因此，我也想分享一下<em class="ny">暹罗阿尔伯特</em>是如何工作的，以及如何实现这个网络并重现上面的结果。</p><p id="d5dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们先来看看隐藏在单个<em class="ny">连体阿尔伯特</em>盒子后面的架构的高级表示。Siamese ALBERT由3个基于ALBERT的子网组成，这些子网相互分担重量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qp"><img src="../Images/e19ec9ee3fbcf8b75cf708a42730062d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1zbyGyIxSDHKIxhu5c0Bw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:暹罗伟业网络建设— <a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/2-Train_Siamese_ALBERT_Classifier_TripletSemiHardLoss_.ipynb" rel="noopener ugc nofollow" target="_blank">代号</a> —由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="d391" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">暹罗系统的输入是文本标题和新闻类别的三元组。该三元组由来自相同类别的两个标题(例如，类别世界新闻)和来自不同类别的一个标题(例如，类别政治)组成。文本标题通过三个子网络，产生三个输出，这些输出被传递到<strong class="ky ir">三元半硬损失函数</strong>，该函数试图使两个相似类别的输入更接近，同时试图在嵌入空间内使具有不同类别的第三个输入的输入远离。尽管看起来三元连体结构有3个网络，但它只有一个网络为每个子网提供相同的权重。因此，子网彼此共享它们的权重。</p><h2 id="0b52" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">三重损失</h2><p id="2ebb" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在2015年【9】的<a class="ae kv" href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener ugc nofollow" target="_blank"> Face Net研究论文中引入了三重损失，以找到一种可扩展的方法来处理人脸识别等任务，这些任务的特点是类别数量多而每类样本数量少。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/35e22f878fc4f4d9c9048d64548890b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*YsLol31Kbp1WTEo4NmM4Sw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:三重损失嵌入图—由<a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener">作者</a>创作</p></figure><p id="6712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，三联体损失-连体结构可以用于非常不同的目的，例如文本和图像相似性任务。这可以归因于这样的事实，即它允许直接学习从其输入到紧致欧几里得空间的映射，其中距离直接对应于相似性的度量。换句话说，<strong class="ky ir">三重损失最小化锚和正之间的距离，最大化锚和负之间的距离</strong>。锚和阳性具有相同的类别，而阴性是从不同的类别中取样的。<em class="ny">三重态损失</em>函数由欧拉距离函数表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/53111e6e26e6b3570535e218dec52c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*b7F9JBZ2k8wHkbBj.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:三重损失图解—<a class="ae kv" href="https://www.tensorflow.org/addons/tutorials/losses_triplet" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/addons/tutorials/losses_triplet</a></p></figure><h2 id="5c99" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">暹罗阿尔伯特实施</h2><p id="a47b" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">但是在我们迷失在理论之前，让我们从实现开始。一如既往，第一步是加载数据集，在我们的例子中，它是《赫芬顿邮报》的数据集，随后，清理和准备数据，以便可以提供给我们的网络。这些步骤已经在前面的章节中简要描述过，用于数据准备的代码结合完整的实现可以在<a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/1-Train_Siamese_ALBERT_TripletSemiHardLoss.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">这里</strong> </a>找到。</p><p id="dc26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在清理和准备好我们的数据之后，我们加载<strong class="ky ir"> ALBERT tokenizer </strong>并设置一个数据生成器函数，以实现一个灵活的实现，将我们的数据批量提供给siamese模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="cd43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将从Tensorflow Hub加载预训练的ALBERT-base模型。在这个例子中，暹罗艾伯特网络采用三个输入(单词_ id、掩码、类型_ id)。这些不是之前三重损耗输入的<em class="ny">锚、正</em>和<em class="ny">负</em>,而是艾伯特模型需要的正常输入。在3个并行输入之后，加载的艾伯特基网络被合并到<em class="ny">暹罗艾伯特</em>中作为单一层，我们将从其中提取<em class="ny">汇集输出</em>权重<em class="ny"> </em>并将它们馈送到64维密集层中。密集层之后是L2归一化层，而最后的<strong class="ky ir"> L2层对于训练成功至关重要，否则网络会有不稳定的训练迭代</strong>和问题收敛。</p><p id="eac2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你现在可能想知道:为什么没有3个独立的子网络或不同的输入流来为网络提供一个锚定的、积极的和消极的类示例？</p><p id="c7e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，我分享的实现不需要配置由3个子网络组成的网络架构。在定义我们网络的损耗函数时，我们使用了<a class="ae kv" href="https://www.tensorflow.org/addons/tutorials/losses_triplet" rel="noopener ugc nofollow" target="_blank"> Tensorflow Addons三重半硬损耗【10】</a>函数实现，它在后台处理这个问题。损失函数确保在我们摄入的批次中找到三元组。这个过程被称为在线学习，并确保只训练(在我们的半硬损失函数的情况下)半硬三元组。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qs qt l"/></div></figure><p id="e113" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们定义了一个回调函数来保存我们的训练运行。最后，我们用model.fit()启动训练运行。这些都是训练一个合理的连体网络所需的步骤。完整的代码可以在 这里<a class="ae kv" href="https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/text%20embeddings/1-Train_Siamese_ALBERT_TripletSemiHardLoss.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">访问。</strong></a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qs qt l"/></div></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="0e0a" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">结论</h1><p id="af3f" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">当面临硬件限制时，ALBERT-base和ALBERT-large是生成高质量文本嵌入的优秀语言模型。这两种模式都拥有目前所有可用的预训练语言网络中最好的性能/规模比之一。GPT-3不是关于许多语言任务的最先进的模型，因此除了语言生成应用之外，不应该考虑使用它。</p><p id="b90a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用微调技术来进一步提高模型在各种指标上的性能，与传统的微调方法相比，siamese架构在性能上有所提高，传统的微调方法只是在预训练网络之后添加微调头。</p><p id="7a19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望我的发现可以帮助人们在未来建立自己的项目和产品。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><blockquote class="nv nw nx"><p id="c3ed" class="kw kx ny ky b kz la jr lb lc ld ju le nz lg lh li oa lk ll lm ob lo lp lq lr ij bi translated">感谢您的阅读！</p></blockquote><h2 id="475c" class="ou ml iq bd mm ov ow dn mq ox oy dp mu lf oz pa mw lj pb pc my ln pd pe na pf bi translated">-简·施密茨</h2><ul class=""><li id="abaf" class="nh ni iq ky b kz nc lc nd lf qu lj qv ln qw lr pv nn no np bi translated"><strong class="ky ir"> <em class="ny">如果你喜欢这个，</em> </strong> <a class="ae kv" href="https://medium.com/@janschmitz_80340/membership" rel="noopener"> <strong class="ky ir"> <em class="ny">订阅</em> </strong> </a> <strong class="ky ir"> <em class="ny">到我的媒介获取更多内容！</em>T13】</strong></li><li id="9f99" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir"> <em class="ny">你也可以</em> </strong> <a class="ae kv" href="https://medium.com/subscribe/@janschmitz_80340" rel="noopener"> <strong class="ky ir"> <em class="ny">关注我上媒</em> </strong> </a></li><li id="cb18" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr pv nn no np bi translated"><strong class="ky ir"> <em class="ny">或</em></strong><a class="ae kv" href="https://www.pinklion.xyz/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ny">PinkLion</em></strong></a><strong class="ky ir"><em class="ny">并给我一些反馈</em> </strong></li></ul><h1 id="7102" class="mk ml iq bd mm mn qx mp mq mr qy mt mu jw qz jx mw jz ra ka my kc rb kd na nb bi translated">参考资料:</h1><p id="2256" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">[1] SentEval:句子嵌入评估工具包<a class="ae kv" href="https://github.com/facebookresearch/SentEval" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/SentEval</a></p><p id="b718" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]胶水:通用语言理解评测<a class="ae kv" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">https://gluebenchmark.com/</a></p><p id="7a5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]小队:史丹福问答数据集【https://rajpurkar.github.io/SQuAD-explorer/ T42】</p><p id="8503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] RACE:来自考试的大规模阅读理解数据集<a class="ae kv" href="https://arxiv.org/pdf/1704.04683.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1704.04683.pdf</a></p><p id="3aee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] SWAG:基于常识推理的大规模对抗性数据集<a class="ae kv" href="https://arxiv.org/pdf/1808.05326.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1808.05326.pdf</a></p><p id="19bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]强力胶:通用语言理解系统的一个更棘手的基准<a class="ae kv" href="https://arxiv.org/pdf/1905.00537.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1905.00537.pdf</a>T50】https://super.gluebenchmark.com/</p><p id="3fd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]使用“暹罗”时间延迟神经网络的签名验证<a class="ae kv" href="https://proceedings.neurips.cc/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings . neur IPS . cc/paper/1993/file/288 cc 0 ff 022877 BD 3d f 94 BC 9360 B9 c5d-paper . pdf</a></p><p id="6e28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]米斯拉，里沙卜。(2018).新闻类别数据集。10.13140/rg . 2 . 2 . 20331.18729 .<a class="ae kv" href="https://www.kaggle.com/datasets/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/RMI SRA/news-category-dataset</a>—Misra，Rishabh &amp; Grover，Jigyasa。(2021).为ML雕刻数据:机器学习的第一步。</p><p id="dddc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9] FaceNet:人脸识别和聚类的统一嵌入【https://arxiv.org/pdf/1503.03832.pdf T2】</p><p id="c9be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[10]tensor flow Addons loss:TripletSemiHardLoss【https://www.tensorflow.org/addons/tutorials/losses_triplet T4】</p><p id="d215" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[11]用统一的文本到文本转换器探索迁移学习的极限<a class="ae kv" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.10683.pdf</a></p></div></div>    
</body>
</html>