<html>
<head>
<title>Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-fbce2a22c6e0#2022-04-08">https://towardsdatascience.com/principal-component-analysis-fbce2a22c6e0#2022-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9347" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PCA的概念性解释及其背后的数学原理的逐步演示。Python和R中结果的可视化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9db33c847abf77f2e5dd620c9451619f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOwk9b_E2f_GHks83wPEUw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">丹尼尔·罗伯特在<a class="ae ky" href="https://pixabay.com/sv/photos/prisma-ljus-spektrum-optik-6174502/" rel="noopener ugc nofollow" target="_blank">的照片</a></p></figure><p id="eb54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本主成分分析指南中，我将给出PCA的概念性解释，并提供一步一步的演练，以找到用于计算主成分和主成分得分的特征向量。我还将展示我们如何使用Python和R中的<em class="lv"> sklearn </em>在iris数据集上执行小PCA后找到并可视化结果。在R中使用<em class="lv"> factoextra </em>和<em class="lv"> gglot2 </em>包可视化PCA结果非常容易。</p><ol class=""><li id="fd10" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">什么是主成分分析</li><li id="071b" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">PCA背后的数学:如何计算主成分</li><li id="241e" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">解释PCA的结果</li><li id="4f52" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">Python中的主成分分析</li><li id="2f04" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">使用ggplot2和factoextra可视化R中的PCA结果</li></ol><p id="647c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该指南以笔记本的形式提供，其中包括更多python代码，用于本<a class="ae ky" href="https://github.com/TalkingDataScience/Principal_Component_Analysis" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中的所有计算和绘图。repo还包含一个笔记本，其中包含r中的PCA和可视化。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="7caa" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">1.什么是主成分分析</h1><p id="8f3a" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">主成分分析可以作为一种降维技术，我们可以形成新的变量，这些变量是原始变量的线性组合。如果您正在进行回归建模，并且数据中存在多重共线性，这也是一种有用的技术。多重共线性可能导致参数的高标准误差，从而导致不可靠的估计值。主成分分析将形成不相关的新变量。</p><h2 id="1039" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">1.1 PCA的几何表示</h2><p id="ac3e" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在进入主成分分析的数学之前，查看PCA的几何表示可能有助于获得概念上的解释。希望它能帮助我们将数学计算与主要成分的不同元素联系起来。考虑下面的图表，它显示了一些我们想要找出主要成分的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0d7920d4458715325b6ea8b1477a0098.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*hMYMOR7aZorXAavpsfUD9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9b1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们观察数据点，很明显X1和X2都不是获取数据最大方差的轴。获得最大方差的轴是X1*，这是我们在主成分分析中想要识别的备选轴。这个新的轴将成为我们的第一个主要组成部分。原始数据点在X1*上的投影将是数据点相对于这个新X1*的坐标。</p><p id="298f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的图中，我们让X1*是二维平面中的任意轴，与轴X1成θ度角。点相对于新轴的坐标是点相对于原始轴的坐标的线性组合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5e06dc652931ebf2819f984fccc0e27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*TchF0SPA8tOzkSoOMGHWDg.png"/></div></figure><p id="8329" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由新轴X1*捕获的总方差的百分比将根据角度θ的度数而变化。这意味着有且只有一个备选轴会产生一个新变量，该变量保留数据中的最大方差。这个新轴不能解释数据的所有方差，所以我们需要确定第二个新轴，X2*。与X1*正交的第二个轴将说明X1*未捕捉到的最大方差。我们可以认为PCA是将轴旋转到一个位置，使新的轴能够捕获数据中的最大方差。</p><h2 id="7349" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">1.2需要牢记的重要概念</h2><p id="f85e" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">从PCA的几何表示中，我们可以得出一些重要的观察结果:</p><ul class=""><li id="b749" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu oc mc md me bi translated">当我们旋转轴时，二维空间中数据点的方向不会改变。这意味着观察值可以相对于原始轴或新轴来表示，而不会丢失任何信息。</li><li id="e3f6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">新轴𝑋1*和𝑋2*是我们的<strong class="lb iu">主成分</strong>，新变量的值是我们的主成分<strong class="lb iu">分数</strong>，我们的转换数据。</li><li id="6db7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">新变量X1*和𝑋2*是我们原始特征𝑋1和𝑋2的线性组合，它们保持均值校正(或标准化，取决于我们使用的数据中心化方法)。</li><li id="7912" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">新变量的总方差与原始变量相同。</li><li id="5ff9" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">新变量的相关性为零。换句话说，主成分之间不会有多重共线性。</li></ul><p id="b62a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的几何表示只有两个变量，因为很难说明更高的维度。然而，该方法可以很容易地扩展到任何数量的变量<em class="lv"> p </em>。具有<em class="lv"> p </em>个变量的数据集可以在<em class="lv"> p </em>维空间中表示，具有<em class="lv"> p </em>原始轴和<em class="lv"> p </em>新轴，使得:</p><ul class=""><li id="707f" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu oc mc md me bi translated">每个新变量都是原始变量的线性组合。</li><li id="d7d6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">选择第一个新变量来获取数据集中的最大方差。</li><li id="7654" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">第二个将说明第一个没有捕捉到的最大方差。</li><li id="b647" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">第p个<em class="lv">新变量将解释P1个新变量未捕捉到的数据中的最大方差。</em></li><li id="a9dc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu oc mc md me bi translated">新变量不相关。</li></ul><p id="e469" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一节中，我们将找到新的轴，并手动计算主成分。我们将从变量的协方差矩阵计算特征向量，这给出了用于形成上述方程的权重w_𝑖𝑗(这是我们的主分量):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/50cce5a37c5bdac46ffedd753c66adf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*p_c4vzjgHWwj1ttNwA-Ktw.png"/></div></figure><p id="7cf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中X1，X2，…，Xp是p个主成分，w_ <em class="lv"> ij </em>是第<em class="lv"> i </em>个主成分的第<em class="lv"> j </em>个变量的权重(特征向量)。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="1378" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">2.主成分分析的数学:如何计算主成分</h1><p id="368c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在第一部分中，我们将研究PCA背后的数学，我们将创建一个小的虚拟数据集，小到足以用手轻松地进行计算。数据集将包含两个要素X1和X2以及七个观测值。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="671a" class="no ms it of b gy oj ok l ol om"># Create a small dataset</span><span id="5965" class="no ms it of b gy on ok l ol om">data = pd.DataFrame({'X1':[110, 112, 112, 114, 116, 116, 118], <br/>                     'X2': [179, 180, 181, 182, 182, 184, 186]})</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/2c45cceb855f997d0ac259ea41e50e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*2O9egq3R1Ma3N6qPHge2lA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="b540" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.1将数据居中</h2><p id="9cc8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在进行主成分分析之前，我们可以对数据进行均值校正或标准化，并且要记住，主成分分析的结果可能会受到我们使用的方法的影响。使用均值校正数据将允许单个变量的相对方差影响用于计算主成分的权重。方差相对于其他变量较高的变量将获得较高的权重，方差相对于其他变量较低的变量将获得较低的权重。如果我们把数据标准化，每个变量的方差都是一样的(均值0，std/var 1)。</p><p id="9d9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方法的选择可能取决于数据和分析的目的等因素。例如，如果我们要查看消费者价格指数的食品价格，不同食品项目的方差可能会有很大差异。在下面的主成分中，由于较高的方差，鳄梨比其他食物项目具有更高的影响。如果我们没有理由相信对于一个家庭来说，鳄梨比其他食物更重要，我们应该将数据标准化，以避免这种人为增加的鳄梨重量。</p><p id="10a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">PC1 = 0.13 *牛肉+0.85 *牛油果+0.04 *大米+0.36 *牛奶</em></p><p id="6fad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能还有其他情况，我们有理由相信特征的方差确实表明了某些特征的重要性，在这些情况下，我们可以使用均值校正数据，并允许对某些因素产生更高的影响。</p><p id="0a24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我们的计算，我们将使用平均校正数据。均值校正数据仅仅意味着我们从变量的每个观察值中减去变量的均值:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="0159" class="no ms it of b gy oj ok l ol om"><em class="lv"># Center the data with mean-correction </em></span><span id="3c44" class="no ms it of b gy on ok l ol om">data_centered = data.apply(lambda x: x-x.mean())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8837413021b1a14fb836ac04cb6d088d.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*ksdlJgdYpcZyXHhnAIht2A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="794e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以绘制原始数据和居中数据的数据点，我们可以看到，将数据居中并不影响数据点的方向，只是轴的比例发生了变化。我们将数据集中在原点周围，这将有助于我们在旋转轴寻找主分量时的计算。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7a8258880565220b8dafac2c835c0720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*xt5OQBIQHv4-NbOzqckjag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="5762" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.2计算协方差矩阵</h2><p id="ec13" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我们将使用的方法是协方差矩阵的特征分解，因此我们首先需要计算的是协方差矩阵。协方差矩阵由每对变量的协方差组成，每个变量的方差在主对角线上。</p><p id="676d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据居中不会影响要素的方差或协方差。非居中和居中数据的协方差矩阵是相同的。我们可以检查情况是否如此:</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="a30b" class="no ms it of b gy oj ok l ol om"><em class="lv"># Print values of the non-centered data and its covariance matrix</em><br/>non_centered = np.stack((data['X1'], data['X2']), axis=0)<br/>cov_mat = np.cov(non_centered)<br/>print(non_centered)<br/>print(cov_mat)</span><span id="a0d1" class="no ms it of b gy on ok l ol om"><em class="lv"># Print values of the centered data and its covariance matrix</em><br/>centered = np.stack((data_centered['X1'], data_centered['X2']), axis=0)<br/>cov_mat_centered = np.cov(centered)<br/>print(centered)<br/>print(cov_mat_centered)</span><span id="2f11" class="no ms it of b gy on ok l ol om"><em class="lv">Output:</em></span><span id="4c24" class="no ms it of b gy on ok l ol om">[[110 112 112 114 116 116 118]<br/> [179 180 181 182 182 184 186]]<br/>[[8.         6.33333333]<br/> [6.33333333 5.66666667]]</span><span id="c4d0" class="no ms it of b gy on ok l ol om">[[-4. -2. -2.  0.  2.  2.  4.]<br/> [-3. -2. -1.  0.  0.  2.  4.]]<br/>[[8.         6.33333333]<br/> [6.33333333 5.66666667]]</span></pre><h2 id="31d3" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.2.1每个特性的差异</h2><p id="adcb" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">矩阵的对角线包含每个特征X1和X2的方差。差异的计算公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/a37df74338411a5ec7e69cb1b64fa079.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*LFBH0TJ3udEp8ggVZxWUbg.png"/></div></figure><p id="c416" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于居中数据的平均值为0，我们可以简单地将变量的平方值相加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/ef029c96e33fe34869a5d01260b6cb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxxdOX0ss5nxzKj64_9UmQ.png"/></div></div></figure><h2 id="6275" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">特征的协方差</h2><p id="67bc" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">两个变量X1和X2之间的协方差通过以下公式计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e832d9e49a9ecc9f79f5258ed467a96c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*kNLe2BnPijFzcOp-3XzqjA.png"/></div></figure><p id="9a95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以找到X1和X2的协方差。同样，由于居中数据的平均值为0，我们可以将X1和X2中的值相乘并求和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/1225af37fc07c503ea1c6a3937dc54e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kLDgHF0kQ4q5j7OlofIOsA.png"/></div></div></figure><p id="79af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算X1和X2的方差和协方差显示了Python中的<code class="fe ov ow ox of b">np.cov()</code>函数打印的相同结果。我们可以看到，X1的方差大于X2，协方差介于X1和X2的方差之间。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="a66d" class="no ms it of b gy oj ok l ol om">[[8.         6.33333333]  <br/> [6.33333333 5.66666667]]</span></pre><h2 id="93ae" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.3计算协方差矩阵的特征值</h2><p id="d52c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">现在我们有了协方差矩阵，我们可以继续计算特征值。在下面的行列式公式中，我们将协方差矩阵表示为<em class="lv"> A </em>，将单位矩阵表示为<em class="lv"> I </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f64d75122fa868ced889e29dcb819d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*2vbXKYh-EPM254hHbCPgDA.png"/></div></figure><p id="732d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从协方差矩阵中减去λ÷I矩阵得到以下矩阵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4a8a533f09a6b70fccfb43432c385923.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*XO9VDugfzVaIecxSkOpkTA.png"/></div></figure><p id="4755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以继续计算它的行列式</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/3f2ff6259b4d938815a02836933c6be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*vIGP1Xvhi23EAq4dZL1dEw.png"/></div></figure><p id="eb79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简化后，我们得到这个二次方程</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/f649bd26b498c1fbecd9c058fbc6a712.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*KZlEgKQKt7AluNi4fj5tJw.png"/></div></figure><p id="3c55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">求解0，我们得到𝜆1=0.39和λ2=13.27，这些是我们协方差矩阵的<strong class="lb iu">特征值</strong>。</p><h2 id="2d1c" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.4计算特征向量</h2><p id="d997" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">一旦我们有了特征值，我们就可以计算这些特征值对应的特征向量。一个特征向量是一个非零向量，它满足方程𝐴𝑣=𝜆v.计算协方差矩阵的特征向量，我们使用这个方程，把协方差矩阵代入𝐴，把其中一个特征值代入λ，并把它们与列向量𝑣=(𝑥,𝑦).相乘我们从特征值λ=13.27开始:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/ab1616a75380756c716bfa05c34af7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*7L_KfREDrMyRN_phFWPEug.png"/></div></figure><p id="295f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到下面的方程组:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/16fdef3d9fd488e7743a8e86da231e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*sV1w5M1ncH_JadT9S4hqfA.png"/></div></figure><p id="7e22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将第一个等式中的8.0x项和第二个等式中的5.7y项移到等式的右侧，求解y，得到y = 0.83x:</p><p id="65be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">𝑦 = 0.83𝑥 <br/> 0.83𝑥 = y</p><p id="d464" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果将x设为1，y将为0.83，从而得到特征向量𝑣_2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/283aa964c54b7dbbe6298ba8fd08ab90.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*qmz0hK1mlsh47L4SM6yyuQ.png"/></div></figure><p id="cb69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是协方差矩阵的<strong class="lb iu">特征向量</strong>。我们使用以下公式将向量归一化为单位长度，这意味着它的长度为1:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f7a1ee2497560f3221e80a27ddb7e019.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*XL07210NwmnEgE6XEcftHg.png"/></div></figure><p id="3d37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归一化后，特征向量v_2是这样的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/1185701fea73b433d6c851d2bfa2358c.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*eGlU7JcRMnRpN8t_Quy-xQ.png"/></div></figure><p id="f525" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们对另一个特征值𝜆1=0.39做所有的计算，我们得到另一个特征向量v1:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/4608f072be1169b07db65bf47cf40865.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*qwdS74GU7VkPgOljcCWW-w.png"/></div></figure><h2 id="ff07" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">2.5计算主成分和主成分得分</h2><p id="a21b" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我们将最大特征值的特征向量先λ1&gt;λ2重新排序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/59a77d2e79573182880dab172c729cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*gRZfOKMPXSd31pcZLwQ4TA.png"/></div></figure><p id="120e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些特征向量现在可以用来转换数据，计算主成分。特征向量包含表示形成主分量的等式中的权重w_ <em class="lv"> ij，</em>的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/50cce5a37c5bdac46ffedd753c66adf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*p_c4vzjgHWwj1ttNwA-Ktw.png"/></div></figure><p id="3811" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将这两个特征向量放在一个矩阵E中，这样我们可以使用矩阵乘法将所有主成分的特征向量与整个原始数据集相乘。第一列表示具有最高特征值的第一特征向量，第二列是具有第二高特征值的第二特征向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/d6c261836fe01f1ba8c27b8e523cdfea.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*ydjHQmBBFxLYdigVxsUUIw.png"/></div></figure><p id="c800" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个矩阵现在可以用来转换我们的中心数据，存储在矩阵D，通过乘以它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/fe3f0acae4dc51bc0b6f877cf099efc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:270/format:webp/1*NDR0Nn2-82KTsriY1FIwyQ.png"/></div></figure><p id="f632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们将原始中心数据乘以特征向量时，我们得到新的矩阵DE，它包含转换后的数据、我们的<strong class="lb iu">主成分、</strong>和我们的<strong class="lb iu">主成分得分</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/c8d78418140f6a97e12bd6c243eacafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*3ZxoivT6Xr3f1arxBfGbSA.png"/></div></figure><p id="4bdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新矩阵<em class="lv"> DE </em>包含我们的主成分，其中每一列都是一个主成分。左栏是我们的第一个主成分，PC1，捕捉数据中的最大方差，右栏是第二个主成分，PC2。</p><p id="cbc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些变换后的数据点的值就是主成分得分<strong class="lb iu">。</strong>这些分数<strong class="lb iu"> </strong>表示主成分空间中的居中数据，因为它们已经被投影到旋转轴，从而最大化沿着第一主成分的轴的方差。从分值可以看出，PC1的方差比PC2大得多。</p><p id="fc8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个新变量是不相关的。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="58c4" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">3解释主成分分析的结果</h1><h2 id="2f3b" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">3.1视觉比较:绘制PCA前后的数据点</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/a079e8509297deb4dbcff134adc04516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*36680OCfjGRmBqNopwGwCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1863" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们通过观察数据点如何落在新旧轴上来比较原始数据和主成分的方差，我们可以看到第一主成分PC1包含比原始特征X1多得多的方差。</p><h2 id="e449" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">3.2数值比较:PCA前后的方差和协方差</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/38f0314b1130621d0b367e8ca07744e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*sl97TXTQ4arv_jlbMErZMA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4855" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们观察我们的变换数据的协方差矩阵，我们可以看到主成分1和主成分2的方差分别是13.27和0.39，并且它们之间没有相关性。</p><p id="eeed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里需要注意的是，主成分的<em class="lv">方差是我们之前根据原始数据的协方差矩阵计算的特征值</em>。完成主成分分析后，您将查看特征值，以帮助确定提取多少主成分用于数据的进一步分析/建模。特征值用于生成碎石图。</p><p id="d6dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">variance=𝜆1/(𝜆1+𝜆2)=13.27/(13.27+0.39)=0.97的百分比</p><p id="0182" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转换后，第一个主成分捕获了数据中所有方差的97%。原始数据中的特征X1仅捕获了58%的变化。</p><p id="fa5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过进行这种主成分变换，我们获得了两大好处。我们已经消除了两个要素之间的多重共线性问题，并且最大化了较少要素的方差，以便减少模型/分析中包含的要素数量。</p><h2 id="bb85" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">3.3加载和评分</h2><p id="bad1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">负载是原始特征和新的主成分之间的相关性。载荷给出了原始变量对形成主分量有多大影响的指示。</p><p id="f858" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，分数是原始数据乘以特征向量后得到的转换数据。可以绘制分数图，以便对结果进行进一步分析。它们可能给出具有相似性的数据点的集群的指示。更常见的是，它们将被用作进一步多变量分析和建模的输入数据，例如分类或回归。</p></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="b35c" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">4.Python中虹膜数据集的主成分分析</h1><p id="c1c3" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在对数据执行PCA之后，您可以使用下面的代码访问结果。</p><p id="dec1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">分数</strong> <br/> <code class="fe ov ow ox of b">pca.transform(data)</code>将数据转换成PCA分数</p><p id="f4fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">载荷</strong> <br/> <code class="fe ov ow ox of b">pca.components_</code>将打印载荷。如果你希望每个主成分的载荷以列的方式显示，你可以请求转置<code class="fe ov ow ox of b">pca.components.T</code></p><p id="6a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">各主成分的方差(特征值)</strong> <br/> <code class="fe ov ow ox of b">pca.explained_variance_</code></p><p id="c331" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解释方差比例</strong> <br/> <code class="fe ov ow ox of b">pca.explained_variance_ratio_</code></p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="6aa0" class="no ms it of b gy oj ok l ol om">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.decomposition import PCAimport plotly.express as px<br/>import plotly.io as pio<br/>import plotly.graph_objects as go</span></pre><h2 id="8fc5" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">4.1导入和缩放数据</h2><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="0194" class="no ms it of b gy oj ok l ol om"><em class="lv"># Import iris data set and drop Id column</em><br/>iris = pd.read_csv('Iris.csv')<br/>iris = iris.drop('Id', axis=1)</span><span id="b2c0" class="no ms it of b gy on ok l ol om"><em class="lv"># Store label in y</em><br/>y = iris.pop('Species')</span><span id="cfc5" class="no ms it of b gy on ok l ol om"><em class="lv"># Store column names</em><br/>col_names = iris.columns</span><span id="211c" class="no ms it of b gy on ok l ol om"><em class="lv"># Center data</em><br/>iris_centered = scale(iris)</span></pre><h2 id="f028" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">4.2执行PCA并将分数存储在数据帧中</h2><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="3f3a" class="no ms it of b gy oj ok l ol om"><em class="lv"># Perform PCA on centered iris data</em><br/>pca = PCA()<br/>pca.fit(iris_centered)<br/>iris_scores = pca.transform(iris_centered) # pca.transform gives the scores</span><span id="8973" class="no ms it of b gy on ok l ol om"><em class="lv"># Convert scores to a data frame</em><br/>iris_scores_df = pd.DataFrame(iris_scores)<br/>iris_scores_df = iris_scores_df.set_axis(['PC1', 'PC2', 'PC3', 'PC4'], axis=1)</span><span id="8001" class="no ms it of b gy on ok l ol om"><em class="lv"># Add labels to score data frame for plotting</em><br/>iris_scores_df = pd.concat([iris_scores_df, y], axis=1)<br/>iris_scores_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/a44cd9d2403d22d00e04531ab413b41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*y0-R82HIth9slZKLQ9TvZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="725e" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">4.3每个主成分(特征值)的方差和累积方差</h2><p id="ec2c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">如前所述，我们首先要看的是每个主成分保留了多少数据方差，更有趣的是，总方差的比例。我们使用<code class="fe ov ow ox of b">pca.explained_variance_ratio.</code>访问方差的比例，我们可以将这些值与累积比例一起存储在数据框中，以便绘图。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="d147" class="no ms it of b gy oj ok l ol om"><em class="lv"># Proportion of variance each principal component</em><br/>prop_variance = pca.explained_variance_ratio_<br/>prop_variance_df = pd.DataFrame(prop_variance, columns=['Proportion of Explained Variance'])</span><span id="2cd6" class="no ms it of b gy on ok l ol om"><em class="lv"># Add cumulative proportion of variance</em><br/>cumulative_prop = np.cumsum(np.round(prop_variance, decimals=4))<br/>cumulative_prop_df = pd.DataFrame(cumulative_prop, columns=['Cumulative Proportion of Variance'])</span><span id="e5cc" class="no ms it of b gy on ok l ol om"><em class="lv"># Combine into one dataframe</em><br/>pc_df = pd.DataFrame(['PC1', 'PC2', 'PC3', 'PC4'], columns=['Principal Component'])<br/>variance_df = pd.concat([pc_df, prop_variance_df, cumulative_prop_df], axis=1)<br/>variance_df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/cf000f418fbec30e1e1e5d16244037ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*UMOnh0nz5OQ3Sx2q0z3zhA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="560f" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">4.4碎石图</h2><p id="da64" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">选择主成分数量的一个常用方法是绘制方差百分比，并寻找一个拐点。这个图被称为碎石图。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="1cea" class="no ms it of b gy oj ok l ol om"><em class="lv"># Plot proportion of explained variance</em><br/>fig = px.line(variance_df, x = 'Principal Component', y='Proportion of Explained Variance',<br/>              text='Proportion of Explained Variance', width=600)</span><span id="768f" class="no ms it of b gy on ok l ol om">fig.update_traces(texttemplate='%{text:.3f}')<br/>fig.update_layout(title_text='Proportion of Explained Variance')<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/e56306d54752a7de5d6ce4e75ce52291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xaqA8VDu6waTCBAKxQXkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="81e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以用柱状图表示方差的比例，并将累积方差加在一条线上。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="826b" class="no ms it of b gy oj ok l ol om"><em class="lv"># Plot proportion of variance in bars and cumulative proportion in line</em></span><span id="35a5" class="no ms it of b gy on ok l ol om">fig = go.Figure()</span><span id="882f" class="no ms it of b gy on ok l ol om"><em class="lv"># Plot explained variance</em><br/>fig.add_trace(<br/>    go.Bar(<br/>        x=variance_df['Principal Component'],<br/>        y=variance_df['Proportion of Explained Variance'],<br/>        marker=dict(color='Blue')<br/>    )<br/>    )</span><span id="f00e" class="no ms it of b gy on ok l ol om"><em class="lv"># Plot cumulative variance</em><br/>fig.add_trace(<br/>    go.Scatter(<br/>        x=variance_df['Principal Component'],<br/>        y=variance_df['Cumulative Proportion of Variance']<br/>    )<br/>    )</span><span id="8df5" class="no ms it of b gy on ok l ol om">fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/79c4d3aede652c40e8f8d4ff696ceb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*PM6rfbIaJ-v2eMMcfKrKBQ.png"/></div></figure><h2 id="a736" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">4.5分数图</h2><p id="3078" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我们还可以绘制分数图来进一步解释结果。iris数据集是一个很好的例子，因为它显示了不同物种的不同聚类。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="1d56" class="no ms it of b gy oj ok l ol om"># 3d scatter plot with scores of the first 3 principal components<br/>fig = px.scatter_3d(iris_scores_df, x='PC1', y='PC2', z='PC3', color='Species', symbol='Species', opacity=0.6)</span><span id="15a3" class="no ms it of b gy on ok l ol om">fig.update_layout(template='seaborn')</span><span id="f70a" class="no ms it of b gy on ok l ol om">fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/361b89bf4cdeb495bb08d639d506b2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0uiHp9nAkEVoKUvsURWNhA.png"/></div></div></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><h1 id="d715" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">5.在R中可视化PCA结果</h1><p id="a011" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">r有一个名为<em class="lv"> factoextra </em>的有用软件包，可以帮助提取和可视化多变量分析的结果，包括主成分分析。我将包括几个图表，以简洁的方式显示分数和加载。r允许以一种非常简单紧凑的方式运行PCA并提取结果。让我们在绘图之前快速地看看结果是如何被访问的。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="90b5" class="no ms it of b gy oj ok l ol om"><em class="lv"># Perform PCA and store PCA object in iris_pca</em><br/>iris_pca &lt;- prcomp(iris, scale=TRUE)</span></pre><p id="60fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行PCA后，您可以使用<code class="fe ov ow ox of b">str(PCA_object)</code>来查看PCA对象包含的内容。简单地打印对象将显示主成分的标准偏差和特征向量。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="41ea" class="no ms it of b gy oj ok l ol om"><em class="lv"># Printing the pca object which will show the eigenvectors</em><br/>iris_pca</span><span id="9680" class="no ms it of b gy on ok l ol om"><em class="lv">Output:</em></span><span id="ebf4" class="no ms it of b gy on ok l ol om">Standard deviations (1, .., p=4):<br/>[1] 1.7061120 0.9598025 0.3838662 0.1435538<br/><br/>Rotation (n x k) = (4 x 4):<br/>                     PC1         PC2        PC3        PC4<br/>SepalLengthCm  0.5223716 -0.37231836  0.7210168  0.2619956<br/>SepalWidthCm  -0.2633549 -0.92555649 -0.2420329 -0.1241348<br/>PetalLengthCm  0.5812540 -0.02109478 -0.1408923 -0.8011543<br/>PetalWidthCm   0.5656110 -0.06541577 -0.6338014  0.5235463</span></pre><p id="673a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印摘要将显示每个主成分的标准差、方差比例和累积方差比例。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="f414" class="no ms it of b gy oj ok l ol om"><em class="lv"># Print summary to show st.dev, proportion of variance and the cumulative proportion of variance of each principal component</em><br/>summary(iris_pca)</span><span id="294b" class="no ms it of b gy on ok l ol om"><em class="lv">Output:</em></span><span id="630e" class="no ms it of b gy on ok l ol om">Importance of components:<br/>                          PC1    PC2     PC3     PC4<br/>Standard deviation     1.7061 0.9598 0.38387 0.14355<br/>Proportion of Variance 0.7277 0.2303 0.03684 0.00515<br/>Cumulative Proportion  0.7277 0.9580 0.99485 1.00000</span></pre><p id="7d98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分数作为x变量存储在对象中。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="4cc0" class="no ms it of b gy oj ok l ol om"><em class="lv"># Get the scores using dollar sign (X_pca$x) and store scores in a dataframe for plotting in ggplot2</em><br/>scores_df &lt;- as.data.frame(iris_pca$x)</span><span id="48c9" class="no ms it of b gy on ok l ol om"><em class="lv"># Add column with species to the scores_df</em><br/>scores_label &lt;- cbind(scores_df, data$Species)</span></pre><h2 id="54ae" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">5.1碎石图</h2><p id="077a" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">factoextra包允许我们通过简单地将PCA对象传递给<code class="fe ov ow ox of b">fviz_eig()</code>来绘制scree图。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="0c77" class="no ms it of b gy oj ok l ol om"><em class="lv"># Scree plot showing the proportion of variance of the principal components</em><br/>fviz_eig(iris_pca)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/4e604cb270ae7357d251da5bcec37572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AedZqlH3t-nBjpx_JwCtg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="5ed0" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">5.2情节得分</h2><p id="d6cb" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在python图中，我们看到了分数的3D图。在这里，我们将看看2D的几个地块。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="59b1" class="no ms it of b gy oj ok l ol om"><em class="lv"># Graph of individuals, similar observations are grouped closer together.</em><br/>fviz_pca_ind(iris_pca,<br/>            col.ind = 'cos2',<br/>            repel = FALSE)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/7e0f1d04df277af88b662038ad5bb414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NngwgbC9mGfzfVhUL-iAig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="07a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以包括用ggplot2绘制的分数图，因为这允许通过颜色和围绕聚类的多元t-分布的95%置信区间来绘制结果。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="b98f" class="no ms it of b gy oj ok l ol om"><em class="lv"># Plot scores with colored labels and confidence interval</em><br/>ggplot(scores_label, aes(x = PC1, y = PC2, col = data$Species)) +<br/>            geom_point() +<br/>            stat_ellipse() <em class="lv"># add a confidence interval</em></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/e61106ef0c709df72881108842841e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3ZHIaDYPS5tq7ZBf5xokA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="10dc" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">5.3地块荷载</h2><p id="4111" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">如果我们想知道每个变量对前两个主成分的形成有多大影响，我们可以画出载荷。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="7011" class="no ms it of b gy oj ok l ol om"><em class="lv"># Graph of loadings</em><br/>fviz_pca_var(iris_pca, <br/>            col.var = 'contrib',<br/>            repel = TRUE # Do not overlap text<br/>            )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/ee32953f6fc9b33421ebc04370e09015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BiSns0kAWrrCxU0pdWeYOw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="bddc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图显示，对于第一主成分，花瓣具有正相关性。长度，花瓣。宽度和萼片。第二个主成分对所有的特征都有负的系数，尽管对花瓣的相关性。长度和花瓣。宽度非常小。</p><h2 id="dfe7" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">5.4显示分数和负荷的双标图</h2><p id="0276" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我们也可以使用双标图在同一个图中显示得分和负载。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="c785" class="no ms it of b gy oj ok l ol om"><em class="lv"># Biplot of individuals and variables</em><br/>fviz_pca_biplot(iris_pca, <br/>                col.var = 'blue', # color of variables<br/>                col.ind = 'darkgray', # color of individuals <br/>                )</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/631df747b2d684d42fef13d524c2bad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBYhxiSdXNbVe5rft1IVYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mk ml hx mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="im in io ip iq"><p id="c355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇指南能帮助你更好地理解主成分分析及其工作原理。Python和R中的示例旨在让您了解可以使用什么类型的图来解释PCA的结果。在PCA的实际应用中，您可能会发现自己处理的数据要大得多，可能不如iris数据集直观。毕竟，PCA是一种常用的降维技术。但是具有小的、良好的数据的例子是理解方法元素的好方法。我希望这篇文章能帮助您理解如何使用协方差矩阵、特征值和特征向量来寻找我们的主成分和主成分得分，以及它们如何表示原始数据。</p></div></div>    
</body>
</html>