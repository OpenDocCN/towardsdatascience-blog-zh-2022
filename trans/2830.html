<html>
<head>
<title>What is Regularization: Bias-Variance Tradeoff</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是正则化:偏差-方差权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-bias-variance-tradeoff-and-regularization-94846f945131#2022-06-20">https://towardsdatascience.com/machine-learning-bias-variance-tradeoff-and-regularization-94846f945131#2022-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a5d1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用看不见的数据改进预测的良好实践</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d89f7744894389575a158b85952c1150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*um2g1EsizFJ0AOPv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马库斯·温克勒在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="77e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们谈论使用机器学习模型的<strong class="lb iu">预测</strong>时，理解预测误差(即<strong class="lb iu">偏差</strong>和<strong class="lb iu">方差</strong>)很重要。任何机器学习模型的目标都是找到一个最小化对<strong class="lb iu">看不见的</strong>数据的预测误差的模型。在模型最小化偏差和方差之间的预测误差的能力上有一个<strong class="lb iu">权衡</strong>。理解这些概念将有助于我们解决<strong class="lb iu">过拟合</strong>和<strong class="lb iu">欠拟合</strong>的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lv"><img src="../Images/eb211b7a5dd3b28f3d769de502f4cf9f.png" data-original-src="https://miro.medium.com/v2/format:webp/0*d72qGLfATXY8TJio.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1(作者图片)</p></figure><h2 id="7b42" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">什么是偏见？</h2><p id="98c0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">当我们开发一个模型时，我们可以用训练好的模型对目标进行单独的预测。例如，贝叶斯线性回归可以使用遵循多元高斯分布的所有可能的回归权重进行重复预测。因此，对于给定的模型，我们可能有一组不同的预测值。</p><p id="6b2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图1中，我们假设红色目标的中心是真实的输出值。使用训练好的模型，我们可以在蓝色圆圈内生成预测，因此蓝色圆圈的中心代表平均预测值。</p><blockquote class="mu mv mw"><p id="137a" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated"><strong class="lb iu">偏差</strong>是我们模型的<strong class="lb iu">平均预测值</strong>和我们试图预测的<strong class="lb iu">真实值</strong>之间的差异。</p></blockquote><p id="fcac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">偏差告诉我们训练模型预测真实目标的<strong class="lb iu">能力</strong>。偏差越低，给定的训练模型就越有能力。</p><h2 id="f52a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">什么是方差？</h2><p id="5b60" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在图1中，假设蓝色圆圈的半径代表预测值的方差。</p><blockquote class="mu mv mw"><p id="eaff" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated"><strong class="lb iu">方差</strong>是对训练模型给定输入的预测值的<strong class="lb iu">可变性</strong>(又名<strong class="lb iu">分布</strong>)的度量。</p></blockquote><p id="c220" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">方差越低，经过训练的模型做出的预测就越精确。</p><h2 id="bbc6" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">什么是不可约误差？</h2><p id="8443" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">除了偏差和方差之外，第三种误差称为不可约误差。</p><blockquote class="mu mv mw"><p id="a2a4" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated"><strong class="lb iu">不可约误差</strong>代表数据中的<strong class="lb iu">噪声</strong>无法被训练好的模型解释。不管训练出来的模型有多好，它总是存在的。</p></blockquote></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h2 id="5e3a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">预测误差是方差、偏差和不可约误差的总和</h2><p id="fd82" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">让我们从数学上证明<strong class="lb iu"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lv"><img src="../Images/5d2b574a139fad78c747324f6df38919.png" data-original-src="https://miro.medium.com/v2/format:webp/1*pf1IsArpVWgiCDxvHXrQAw.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lv"><img src="../Images/a159f489bc869d32f8bfb49c7d018bf8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NanroBHdvmyentO0X15nQA.png"/></div></figure><h2 id="b600" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">什么是不合身？</h2><p id="aaee" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">当模型无法捕捉和概括数据的潜在趋势时，就会出现欠拟合。直觉上，它与数据拟合得不够好，因此，它会在<strong class="lb iu">训练</strong>和<strong class="lb iu">测试</strong>数据上产生很高的误差(即<strong class="lb iu">高偏差</strong>)。</p><p id="b9b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不适合的原因可能是</p><ul class=""><li id="2768" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">缺乏开发模型的数据(例如，少量的训练数据，关键解释变量不可用)。</li><li id="a399" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">基础模型不能捕捉数据中的模式(例如，使用具有非线性数据的线性模型)</li></ul><h2 id="d4e3" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">什么是过度拟合？</h2><p id="480a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">另一方面，当模型太好地拟合训练并且它<strong class="lb iu">开始对训练数据的噪声</strong>建模时，发生过拟合。它在训练数据中具有低误差(即<strong class="lb iu">低偏差</strong>，但在测试数据中具有高误差(即<strong class="lb iu">高方差</strong>)</p><p id="025f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">过度拟合的原因可能是</p><ul class=""><li id="1043" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">由于模型过于复杂，包括<strong class="lb iu">太多变量</strong>或包括<strong class="lb iu">有问题的变量</strong>，如高阶多项式变量、无关变量、高度相关的变量或同时受响应变量影响的变量。</li><li id="cacf" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">模型的过度训练</strong>。例如，在没有显式停止标准的情况下训练决策树容易过度拟合。</li></ul><h2 id="5427" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">偏差-方差权衡</h2><p id="b07e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">理想情况下，我们希望减少模型的偏差和方差。然而，这是非常困难的，有时是不可能实现的。当您试图减少定型数据的预测误差时，测试数据的预测误差可能会增加。偏差和方差经常朝着相反的方向移动。</p><p id="b43e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当使用训练数据开发模型时，很容易拟合复杂的模型来进行偏差较小的预测，但由于测试数据的预测方差较高，可能不会产生最佳结果。</p><blockquote class="mu mv mw"><p id="e6c8" class="kz la mx lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated">或者，我们可以探索偏差和方差之间的权衡，在这种情况下，<strong class="lb iu">接受一些偏差以减少方差可能会更好</strong>。因此，我们对未知数据的预测误差会更小。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/ef7bf43299be8d88b942d39011be0090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6iqpYUskXGWctfWEfppEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2(作者图片)</p></figure></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="30c0" class="nx lx it bd ly ny nz oa mb ob oc od me jz oe ka mh kc of kd mk kf og kg mn oh bi translated">什么是正规化？</h1><p id="8a25" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">正则化是一种实现偏差和方差的折衷的概念，有助于减少预测误差。先说一些常用的正则化技术。</p><h2 id="28bd" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">类型1:修改成本函数</strong></h2><p id="7003" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在线性回归模型中，我们可以改变<a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> <strong class="lb iu">代价函数</strong> </a>来构建实现正则化的不同模型。</p><p id="bcac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在一个<strong class="lb iu">岭回归</strong>模型中，我们将成本函数修改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/8d7cc4638e0ec483a33bdccd111ed799.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fpZJugDT8WycsXI2.png"/></div></div></figure><p id="2ed9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个<strong class="lb iu">拉索回归</strong>模型中，我们将成本函数修改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/45d5c5d02303c5561adbc4b414704035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6m-a1gF9ovCdkhSY.png"/></div></div></figure><p id="4bc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">弹性网回归</strong>模型中，我们将成本函数修改为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/08750f06f2bafe7a94631dc1ff256c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZNnjTcnqdtjgrVUE.png"/></div></div></figure><p id="6ede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归的这三个变量与正则化相关联，正则化会损害模型的灵活性和复杂性，以防止过度拟合的风险。</p><p id="fdd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里λ是一个超参数，它决定了模型的灵活性将受到多大的惩罚。λ的值越高，对每个特征的权重施加的约束就越多，因此，它可以防止权重变得太大，从而避免过度拟合。然而，如果λ的值太高，可能会产生欠拟合的问题，因为关键解释变量的权重变得太小，它不能准确地解释响应变量。</p><p id="82dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用<strong class="lb iu"> K倍交叉验证</strong>找到λ的最佳值(我们将在后面介绍)。</p><h2 id="4551" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">类型2: K倍交叉验证</h2><p id="d44e" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">K-Fold交叉验证是另一种用于训练机器学习模型的流行工具。这种方法的思想很简单，我们从观测数据中创建多组训练数据，然后训练模型并基于验证数据评估模型，这类似于在看不见的数据上评估模型。它包括以下步骤，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/38eb3e6b6390d75d7b34f78c8e81035b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BonLxYQWRrG03NUylUmNqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3(作者图片)</p></figure><p id="5bee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤1:我们将观察到的数据拆分成训练数据和测试数据(例如，1:9拆分，有时需要分层)。然后，我们将训练数据随机放入训练文件夹和验证文件夹。例如，在5重交叉验证中，我们将有5组训练数据和验证数据。</p><p id="7e4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤2:在每组中，我们使用来自训练文件夹的数据训练模型，并在验证文件夹中评估模型性能。使用验证折叠评估模型模拟了ML模型的实际应用(即样本外预测或对未知数据的预测)。然后我们得到K个性能结果。</p><p id="bb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤3:对于给定的训练模型，我们通过平均K个性能结果来计算总体性能分数。</p><p id="ca82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤4:挑选具有最佳总体性能得分的训练模型，并将该模型应用于测试数据以计算性能得分。</p><p id="63fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试数据中的性能分数应该接近整体性能分数。如果情况更糟，那么我们需要更深入地研究数据，调查差异。</p><h2 id="ffeb" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">类型3:修改最大似然算法</h2><p id="bb0d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">另一种避免过度拟合的方法是简单地改变机器学习算法。例如，一棵过度生长的决策树容易过度拟合。为了解决这个问题，我们可以使用一个具有预定义最大树深度的随机森林。在训练神经网络的情况下，我们可以应用dropout方法来确保模型可以用所选节点的随机子集来概括模式。</p><h2 id="f9a3" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">最终注释</h2><p id="c284" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了训练一个预测误差最小的机器学习模型，我们需要确保我们探索了偏差和方差之间的权衡。正则化是我们在开发模型时需要考虑的一个重要步骤。</p><p id="b4b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对<strong class="lb iu">线性回归</strong>和<strong class="lb iu">因果推断</strong>感兴趣，这里有一些相关的帖子可以浏览。</p><ul class=""><li id="df88" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/causal-inference-econometric-models-vs-a-b-testing-190781fe82c5"> <strong class="lb iu">因果推断:计量经济模型vs. A/B检验</strong> </a></li><li id="fd08" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> <strong class="lb iu">线性回归与逻辑回归:OLS、最大似然估计、梯度下降</strong> </a></li><li id="e67b" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e"><strong class="lb iu">OLS线性回归:无偏、一致、蓝色、最佳(有效)估计量</strong> </a></li><li id="245c" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc"> <strong class="lb iu">线性回归因果推断:省略变量和无关变量</strong> </a></li><li id="46ed" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/causal-inference-with-linear-regression-endogeneity-9d9492663bac"> <strong class="lb iu">用线性回归进行因果推断:内生性</strong> </a></li><li id="916f" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-regression-with-ols-heteroskedasticity-and-autocorrelation-c12f1f65c13"> <strong class="lb iu">与OLS的线性回归:异方差和自相关</strong> </a></li></ul><h1 id="c99f" class="nx lx it bd ly ny ok oa mb ob ol od me jz om ka mh kc on kd mk kf oo kg mn oh bi translated">感谢您的阅读！！！</h1><p id="b2d5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">如果你喜欢这篇文章，并且想<strong class="lb iu">请我喝杯咖啡，请<a class="ae ky" href="https://ko-fi.com/aaronzhu" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</strong></p><p id="abaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以注册一个<a class="ae ky" href="https://aaron-zhu.medium.com/membership" rel="noopener"> <strong class="lb iu">会员</strong> </a>来解锁对我的文章的完全访问，并且可以无限制地访问介质上的所有内容。如果你想在我发表新文章时收到电子邮件通知，请订阅。</p></div></div>    
</body>
</html>