<html>
<head>
<title>Transformers: The bigger, the better?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚:越大越好？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-the-bigger-the-better-19f39f222ee3#2022-04-20">https://towardsdatascience.com/transformers-the-bigger-the-better-19f39f222ee3#2022-04-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="864e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌再次给我们带来惊喜，它创造了变压器参数数量的新纪录</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/617c568dccb3070c76b57e2fb6ce197c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95alF3kSgmlsqe2Uw_6o8A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">(图片由</em> <a class="ae kz" href="https://torres.ai" rel="noopener ugc nofollow" target="_blank"> <em class="ky">作者</em> </a> <em class="ky"> ) </em></p></figure><p id="688e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">(<a class="ae kz" href="https://torres.ai/nuevo-record-de-google-en-el-tamano-de-un-modelo-de-lenguaje-con-transformers/" rel="noopener ugc nofollow" target="_blank">西班牙语版</a>)</p><p id="4204" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用<a class="ae kz" href="https://torres-ai.medium.com/how-to-take-advantage-of-the-new-disruptive-ai-technology-called-transformers-9e57a26506cb" rel="noopener">变压器</a>的大型语言模型目前构成了人工智能领域最活跃的领域之一，由于所需的计算资源，只有少数技术公司负担得起。几天前，<a class="ae kz" href="https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf" rel="noopener ugc nofollow" target="_blank"> Google用一个语言模型</a>中参数数量的新纪录让我们大吃一惊。</p><h1 id="8cf7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">迄今为止最大的型号</strong></h1><p id="cc6c" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">自2018年<a class="ae kz" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>以来，其他几个大型语言模型(这些模型都是<a class="ae kz" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> Transformer架构</a>的变体)已经被开发出来，这继续推动了艺术的发展。这些模型的改进主要来自于根据参数数量来缩放模型的大小(图1)。谷歌的这个最新模型名为Pathways语言模型(PaLM ),优于目前所有现有的模型。具体来说，这个模型包括5400亿个参数，比迄今为止最大的模型，即所谓的微软/英伟达<a class="ae kz" href="https://arxiv.org/abs/2201.11990" rel="noopener ugc nofollow" target="_blank">威震天-图灵NLG </a>多100亿个参数。两者的参数都是著名的<a class="ae kz" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>的三倍多，后者“只有”1750亿个参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/459bf0343d1dd172144380edcb62e27f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FS3cRnfZNUlcQDnfknSX4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">图1:最先进的主要大型语言模型大小随时间变化的趋势(图片由作者提供)。</em></p></figure><h1 id="1ef6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">需要大量的计算资源</h1><p id="e45a" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">正如我们在之前的几篇文章中讨论的那样，<a class="ae kz" rel="noopener" target="_blank" href="/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d">人工智能是一个超级计算问题</a>。因此，基于转换器的大型语言模型需要大量的计算资源来训练。谷歌报告称，这种模式需要25.6亿次失败。具体来说，他们已经使用了<em class="mu">“6144个TPU v4芯片1200小时，3072个TPU v4芯片336小时”</em>，也就是在一个巨型超级计算系统中已经需要超过两个月的执行时间。</p><p id="e83d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在图2中，我们包括了我们在图1中展示的模型在模型训练所需的FLOPs方面的计算要求。在这个图中，我们可以很快观察到与模型规模成比例的计算需求的增长趋势。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/9d97962e7226f57703661d89273ea528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMOQnnMP33j2lb-PkeCdiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ky">图2:主大型语言模型的FLOPs(指数轴)训练计算(图片由作者提供)。</em></p></figure><p id="d0c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">众所周知，能耗是最重要的运营变量之一，因为组成超级计算数据中心的不同元素需要大量能源。幸运的是，作者报告说，他们主要使用清洁能源来训练模型:<em class="mu">“俄克拉荷马州的数据中心主要由风力和其他可再生能源提供动力，在PaLM-540B被训练的时间段内，以89%的无碳能源运行”</em>。好消息！</p><h1 id="fa10" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">仍有改进的空间</strong></h1><p id="5753" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">根据<a class="ae kz" href="https://goo.gle/palm-paper" rel="noopener ugc nofollow" target="_blank">文章</a>，所提出的模型显示了迄今为止相对于现有模型的卓越改进:推理、自然语言理解/生成或代码生成。在谷歌人工智能博客的<a class="ae kz" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank">这篇文章中，</a>你可以找到PaLM性能优于以前型号的令人信服的例子。</p><p id="c556" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">但值得注意的是，作者认为该模型仍然可以通过增加其规模来改善其结果，因为他们已经验证了该模型的性能作为规模的函数(在本文中，他们对表1中所示的同一模型的三个不同规模进行了实验)遵循与以前的模型类似的线性对数行为，这表明规模带来的性能增益尚未达到稳定状态。有尝试改进的空间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/f0718bc5108b043393e297f41362a2c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9-HNcAWDVNLRW4ov1w7YQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:为了更好地理解缩放行为，<a class="ae kz" href="https://goo.gle/palm-paper" rel="noopener ugc nofollow" target="_blank">文章</a>显示了三种不同参数缩放的结果(图片来源于<a class="ae kz" href="https://goo.gle/palm-paper" rel="noopener ugc nofollow" target="_blank">文章</a>)。</p></figure><p id="8df8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">此外，考虑到DeepMind几天前刚刚发表的一篇文章<a class="ae kz" href="https://arxiv.org/pdf/2203.15556.pdf" rel="noopener ugc nofollow" target="_blank"/>表明这些模型的性能可以通过比参数更积极地扩展数据来大幅提高，如果对大量数据进行训练，该模型可能会更加强大。</p><h1 id="6846" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">结论</strong></h1><p id="e3f0" class="pw-post-body-paragraph la lb it lc b ld mo ju lf lg mp jx li lj mq ll lm ln mr lp lq lr ms lt lu lv im bi translated">谷歌提出的这个模型是一系列趋势的最新一个，证明了通过使用超级计算机(由数千个加速器组成)扩展它们，它们的性能可以在各种自然语言处理、推理和代码任务中得到改善。此外，随着这一研究领域的快速交叉授粉，未来似乎很快就会到来。这种迅速引发了另一场急需解决的关键辩论，这场辩论与我们正在创造的伟大语言模型相关的<a class="ae kz" href="https://arxiv.org/pdf/2112.04359.pdf" rel="noopener ugc nofollow" target="_blank">道德和社会风险有关。但是我们把这个留给另一篇文章。</a></p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><div class="kj kk kl km gt ne"><a href="https://torres-ai.medium.com/how-to-take-advantage-of-the-new-disruptive-ai-technology-called-transformers-9e57a26506cb" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">如何利用新的颠覆性人工智能技术“变形金刚”</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">变压器神经网络正在动摇人工智能</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">torres-ai.medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div><div class="nt nu gp gr nv ne"><a href="https://torres-ai.medium.com/transformers-the-new-gem-of-deep-learning-d0ae04bc4a75" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">变形金刚:“深度学习”的新宝石</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">近年来，变形金刚在自然语言处理方面经历了快速的进步</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">torres-ai.medium.com</p></div></div><div class="nn l"><div class="nw l np nq nr nn ns ks ne"/></div></div></a></div><div class="nt nu gp gr nv ne"><a href="https://torres-ai.medium.com/transfer-learning-the-democratization-of-transformers-1d2493b14883" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">迁移学习:变形金刚的民主化</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">拥抱脸库让变形金刚人人买得起</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">torres-ai.medium.com</p></div></div><div class="nn l"><div class="nx l np nq nr nn ns ks ne"/></div></div></a></div></div></div>    
</body>
</html>