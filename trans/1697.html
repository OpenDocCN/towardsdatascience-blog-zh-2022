<html>
<head>
<title>Boost Performance of Text Classification tasks with Easy Data Augmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过简单的数据扩充提高文本分类任务的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boost-performance-of-text-classification-tasks-with-easy-data-augmentation-1420d45b914a#2022-04-21">https://towardsdatascience.com/boost-performance-of-text-classification-tasks-with-easy-data-augmentation-1420d45b914a#2022-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0e99" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自然语言处理任务的文本数据扩充</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b824ac104a8b3f2ac9451cc907ca52b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3QWQ1CwzqRQKBUkmXCvSg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/sik-life-2171488/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3106982" rel="noopener ugc nofollow" target="_blank">菲利克斯·利希滕费尔德</a>从<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3106982" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>拍摄</p></figure><p id="5a4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对小样本数据的训练增加了过度拟合的机会。数据扩充是一种创建现有数据的人工相似样本的技术。数据扩充技术通常用于模型需要大量数据，但我们对数据的访问有限的任务。它甚至可以帮助模型在小样本数据上很好地概括。</p><p id="63b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据增强非常成功，经常用于卷积神经网络(CNN)模型，因为它通过进行小的改变，如剪切、翻转、旋转、模糊、缩放等，来创建图像数据的人工样本。但是当涉及到NLP任务时，文本数据的数据扩充就不那么容易了。</p><p id="245b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论Jason Wei和的一篇论文，该论文讨论了如何对文本数据执行数据增强以提高文本分类任务的性能。</p><h1 id="9513" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">开始使用:</h1><p id="e5dc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Jason Wei &amp;邹凯的论文 <code class="fe ms mt mu mv b"><strong class="lb iu">“Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks”</strong></code>探讨了4种简单但强大的文本增强技术，它们是增强文本数据的良好基准:</p><ul class=""><li id="bdbd" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">同义词替换</li><li id="1af7" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">随机插入</li><li id="5f24" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">随机交换</li><li id="21fc" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">随机删除</li></ul><p id="0f5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文还在5个基准文本分类任务上执行了上述增强技术，从而提高了卷积和递归神经网络的性能。</p><p id="a121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将讨论上述每种文本增强技术是如何在幕后工作的，以及它对文本分类任务的改进。</p><h2 id="3b05" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">1)同义词替换(SR):</h2><p id="0c72" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">同义词替换技术从句子中随机选取n个单词，不包括停用词，并用随机选取的同义词替换这些单词。这种技术执行原地单词替换。</p><p id="850c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于下面提到的例句，我们随机选取n=2个单词(sad，played)并用它们的同义词替换它们。</p><blockquote class="nw nx ny"><p id="0835" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated"><em class="it">句子:</em>大概是我有史以来<strong class="lb iu">最喜欢的</strong>电影，一个无私的故事，<strong class="lb iu">牺牲</strong>，献身崇高事业。</p><p id="8a24" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated"><em class="it">更新句子:</em>大概是我有史以来<strong class="lb iu">最亲爱的</strong>电影，一个无私、<strong class="lb iu">出家</strong>献身崇高事业的故事。</p></blockquote><h2 id="1e41" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">2)随机插入(RI):</h2><p id="a660" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随机插入是一种类似的同义词替换技术，但在这种情况下，随机选取的n个单词的同义词被插入到随机位置，而不删除原始单词。</p><p id="7c84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于下面提到的示例句子，我们随机选取n=1个单词(喜剧)，并在随机位置插入其同义词。</p><blockquote class="nw nx ny"><p id="ef84" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated">大概是我一直以来最喜欢的电影，一个关于无私、牺牲和献身于崇高事业的故事。</p><p id="bac0" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated"><em class="it">更新句子:</em>大概是我有史以来最喜欢的电影，一个无私、牺牲的故事，和<strong class="lb iu">编年史</strong>献身崇高事业的故事。</p></blockquote><h2 id="a270" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">3)随机互换(RS):</h2><p id="c769" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随机交换技术从句子中随机选择任意两个单词并交换它们的位置。这种技术可以对n对单词执行n次。</p><p id="95fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于下面提到的示例句子，我们随机选取n=1对单词(the，roads)并在随机位置插入其同义词。</p><blockquote class="nw nx ny"><p id="21ad" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated">大概是我有史以来最喜欢的电影，一个关于无私、牺牲和献身于崇高事业的故事。</p><p id="c924" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated"><em class="it">更新句子:</em>可能是我有史以来最喜欢的电影，一部无私、牺牲和献身于<strong class="lb iu">事业的<strong class="lb iu">高尚的</strong>故事。</strong></p></blockquote><h2 id="ae90" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">4)随机删除(RD):</h2><p id="3cc2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">随机删除技术以概率‘p’随机删除句子中的每个单词。</p><blockquote class="nw nx ny"><p id="c03d" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated">大概是我有史以来最喜欢的电影，一部关于无私、牺牲和献身于崇高事业的故事。</p><p id="87c9" class="kz la nz lb b lc ld ju le lf lg jx lh oa lj lk ll ob ln lo lp oc lr ls lt lu im bi translated"><em class="it">更新句子:</em>可能是我有史以来最喜欢的电影，一部关于无私、牺牲和对崇高事业的奉献的电影。</p></blockquote><p id="1326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文讨论了上述四种文本增强技术。作者没有提到他们为什么选择这些增强规则，以及他们还尝试了哪些没有成功的规则。</p><h1 id="2a38" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基准测试:</h1><p id="21e6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，他们使用增强技术来测试5个文本分类任务的性能变化:</p><ul class=""><li id="e77c" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">SST-2:斯坦福情感树库</li><li id="24e5" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">CR:客户评论</li><li id="d3ac" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">主题:主观性/客观性数据集</li><li id="afad" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">TREC:问题类型数据集</li><li id="4898" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">PC:赞成-反对数据集</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a8bfc3550d3e417af2461b6c926c5daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*MnV4bZkqrpVloXraIzHiYA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供，<a class="ae ky" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">参考文献</a>)，在不同的训练集规模上，具有和不具有EDA的模型的五个文本分类任务的平均性能(%)</p></figure><p id="9de3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上述基准测试数据中，我们可以得出结论，完整数据集的平均性能提高了0.8%，拥有500个随机样本的数据集的平均性能提高了3.0%。</p><h1 id="ad9f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论:</h1><p id="05b2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我们讨论了4种文本数据扩充技术，它们提高了在小样本数据集上执行的文本分类任务的性能。我们观察到，与大数据相比，较小样本量(约500)的性能提高了约3%。这很好地描述了数据扩充技术对于小数据集非常有效。</p><h1 id="ebe1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><p id="0d0b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[1]Jason Wei和的论文《提高文本分类任务性能的简单数据扩充技术:<a class="ae ky" href="https://arxiv.org/abs/1901.11196" rel="noopener ugc nofollow" target="_blank">的论文</a></p><blockquote class="oe"><p id="7348" class="of og it bd oh oi oj ok ol om on lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>