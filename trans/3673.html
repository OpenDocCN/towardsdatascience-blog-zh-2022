<html>
<head>
<title>Let’s BLOOM with BigScience’s New AI Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们与BigScience的新AI模型一起绽放</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lets-bloom-with-bigscience-s-new-ai-model-803b1a0d677#2022-08-16">https://towardsdatascience.com/lets-bloom-with-bigscience-s-new-ai-model-803b1a0d677#2022-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d063" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用bnb-Int8为拥抱脸部署大型语言模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cd2e128fbfb380a7063b401c203165c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tjTSaaa1-GF7_7Rm"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@saffu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨夫</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="cce4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">这是怎么回事？</h1><p id="2db7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本教程中，我们将在亚马逊SageMaker 端点中部署BigScience的<a class="ae kv" href="https://bigscience.huggingface.co/blog/bloom" rel="noopener ugc nofollow" target="_blank"> BLOOM模型</a>，这是最令人印象深刻的大型语言模型(LLM)之一。为此，我们将利用<a class="ae kv" href="https://github.com/TimDettmers/bitsandbytes" rel="noopener ugc nofollow" target="_blank"> bitsandbytes </a> (bnb) Int8集成，用于来自<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> Hugging Face (HF) Hub </a>的模型。有了这些Int8权重，我们可以运行以前不适合我们的GPU的大型模型。</p><p id="3b67" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本教程的代码可以在这个<a class="ae kv" href="https://github.com/marshmellow77/sm-bnb-transformers" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到。请注意，用于HF集成的bnb-Int8目前处于公开测试阶段，其目的是收集不同型号和设置可能出现的错误。在生产环境中使用它之前，请考虑这一点。你可以在这里找到更多关于测试计划的信息。</p><blockquote class="mp mq mr"><p id="bf0c" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">声明:本教程的目的是逐步完成设置bnb-Int8 + HF的步骤。因此，我们将只部署3B版本的BLOOM，无需bnb-Int8集成即可轻松托管。部署较大模型的步骤是相同的，但是由于下载它们需要时间，所以需要的时间要长得多。</p></blockquote><h1 id="c510" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么这很重要？</h1><p id="26dc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">自从OpenAI在2020年发布GPT-3以来，LLMs已经席卷了全世界。几周之内，大量令人印象深刻的演示被制作出来，这些演示可以在棒极了的GPT-3网站上找到。很快就清楚了，这些LLM是自然语言处理(NLP)中最重要的发现之一。这是因为这些模型具有令人印象深刻的零射击性能，即它们无需任何进一步的模型训练即可使用。他们也非常多才多艺(从撰写法律文本到代码)和多语言。</p><p id="8341" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，越来越多的LLM由不同的组织创建，试图提高LLM的性能和/或使其开源和更加透明(GPT-3是一个专有模型)。BigScience的布鲁姆模型是“诞生”的最新也可能是最重要的模型之一。从他们的<a class="ae kv" href="https://bigscience.huggingface.co/blog/bloom" rel="noopener ugc nofollow" target="_blank">网站</a>:</p><blockquote class="mp mq mr"><p id="4510" class="lo lp ms lq b lr mk jr lt lu ml ju lw mt mm lz ma mu mn md me mv mo mh mi mj ij bi translated">大型语言模型(LLM)对人工智能研究产生了重大影响。这些强大的通用模型可以根据用户的指令承担各种新的语言任务。然而，学术界、非营利组织和小公司的研究实验室发现很难创建、研究甚至使用LLM，因为只有少数拥有必要资源和专有权的工业实验室可以完全访问它们。今天，我们发布了<a class="ae kv" href="https://huggingface.co/bigscience/bloom" rel="noopener ugc nofollow" target="_blank">布鲁姆</a>，这是第一个在完全透明的情况下训练的多语言LLM，以改变这一现状——这是有史以来人工智能研究人员在单个研究项目中最大规模合作的结果。</p></blockquote><p id="1fd5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，即使这些新的LLM现在是开源的，也不意味着我们可以下载它们并在我们的笔记本电脑上使用它们。这些模型需要大量的磁盘空间、RAM和GPU来运行。这就是为什么像<a class="ae kv" href="https://github.com/TimDettmers/bitsandbytes" rel="noopener ugc nofollow" target="_blank"> bitsandbytes </a>这样的计划是重要的——它们使用漂亮的技术来降低LLM的硬件要求，并使它们以适当的延迟和合理的成本运行成为可能:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/4c76e6ec123e9037dbbd0261f666decd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7WJJ0iO3QgRpHlgX"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">LLM的不同硬件设置(<a class="ae kv" href="https://docs.google.com/document/d/1JxSo4lQgMDBdnd19VBEoaG-mMfQupQ3XvOrgmRAVtpU/edit" rel="noopener ugc nofollow" target="_blank">源</a>)</p></figure><h1 id="21aa" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">如何缩小LLM</h1><p id="b08b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有几种方法可以减少大型人工智能模型的大小和内存占用——其中一些最重要的方法是<a class="ae kv" href="https://neptune.ai/blog/knowledge-distillation" rel="noopener ugc nofollow" target="_blank">知识提炼</a>、<a class="ae kv" href="https://wandb.ai/authors/pruning/reports/Plunging-into-Model-Pruning-in-Deep-Learning--VmlldzoxMzcyMDg" rel="noopener ugc nofollow" target="_blank">权重修剪</a>和量化。请注意，这些并不相互排斥，可以相互组合。关于如何使用这些技术的一个很好的例子，参见《变形金刚的自然语言处理》一书的第8章。</p><p id="103a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">也就是说，bnb团队专注于最后一项技术，量化。他们引入了一种叫做<em class="ms">分块量化</em>的特殊技术，你可以在这里阅读更多关于<a class="ae kv" href="https://arxiv.org/pdf/2110.02861.pdf" rel="noopener ugc nofollow" target="_blank">的内容</a>(这篇文章<em class="ms">稍微超出了本教程的范围</em>😉).他们的成果令人印象深刻:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/d78cc3972d838fbd5904ba7490dc4ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h_YscgJTT0wZeYMk7oB1Gw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">bnb-Int8 ( <a class="ae kv" href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4" rel="noopener ugc nofollow" target="_blank">来源</a>)节省的内存空间</p></figure><h1 id="a86f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">部署布鲁姆模型</h1><p id="018d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">因此，让我们开始将<a class="ae kv" href="https://huggingface.co/bigscience/bloom-3b" rel="noopener ugc nofollow" target="_blank">布鲁姆-3B </a>模型部署到亚马逊SageMaker (SM)端点。我们的游戏计划如下:</p><ol class=""><li id="36c4" class="my mz iq lq b lr mk lu ml lx na mb nb mf nc mj nd ne nf ng bi translated">从HF模型中心下载模型</li><li id="461f" class="my mz iq lq b lr nh lu ni lx nj mb nk mf nl mj nd ne nf ng bi translated">编写自定义推理脚本</li><li id="3698" class="my mz iq lq b lr nh lu ni lx nj mb nk mf nl mj nd ne nf ng bi translated">把所有东西打包在一个<em class="ms">model.tar.gz</em>文件中并上传到S3</li><li id="1b40" class="my mz iq lq b lr nh lu ni lx nj mb nk mf nl mj nd ne nf ng bi translated">将模型部署到端点</li><li id="1d6c" class="my mz iq lq b lr nh lu ni lx nj mb nk mf nl mj nd ne nf ng bi translated">测试模型</li></ol><h2 id="4af3" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">下载模型</h2><p id="be68" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以使用Git LFS下载所有模型文件，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3896" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这将下载文件到我们的本地机器。</p><h2 id="de1f" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">推理脚本</h2><p id="d11f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">AWS &amp; Hugging Face开发了<a class="ae kv" href="https://github.com/aws/sagemaker-huggingface-inference-toolkit" rel="noopener ugc nofollow" target="_blank">sage maker humping Face推理工具包</a>，使得在SM上部署HF模型进行推理变得容易。我们需要做的就是<a class="ae kv" href="https://github.com/aws/sagemaker-huggingface-inference-toolkit#-user-defined-codemodules" rel="noopener ugc nofollow" target="_blank">编写一个推理脚本</a>(和<em class="ms"> requirements.txt </em>文件)来定义我们想要如何加载模型和生成预测:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="e6cc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">注意，我们使用参数<em class="ms"> load_in_8bit=True </em>来使用bnb-Int8积分。</p><p id="ff23" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还需要一个requirements.txt文件，确保所需的模块将安装在推理容器映像中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="8b7e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后一行确保安装最新版本的<em class="ms">变形金刚</em>库。</p><h2 id="3913" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">上传到S3</h2><p id="8c76" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，我们将所有内容打包到一个model.tar.gz文件中，并将该文件上传到S3:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h2 id="e7c1" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">部署模型</h2><p id="0a7b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，我们创建一个指向S3位置的模型表示，并使用一个命令部署该模型:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="c1d5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这需要几分钟时间。</p><h2 id="8279" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">测试</h2><p id="0f1a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在模型成功部署后，我们可以测试它。我们以这样一种方式编写我们的推理脚本，我们可以传递任何和所有用于文本生成的参数，在这里查看这些参数的详细列表<a class="ae kv" href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#generation" rel="noopener ugc nofollow" target="_blank">。让我们使用采样和0.5的温度:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/12d5516c7230b783f97761b02ab17055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXdfODgYEO6bYG1vtzRovQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="de6d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它似乎工作，但我们可能要调整参数多一点。不确定bnb-Int8是否真的在我的Android手机上效果最好！😂</p><h1 id="40b4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="83bc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本教程中，我们利用bnb-Int8集成将BigScience的布鲁姆-3B模型部署到SageMaker端点。从这里开始，下一步可能是尝试和部署更大的模型——如果您尝试了，请告诉我您的体验。如果您在bnb-Int8集成中遇到任何意外行为，请记住它仍处于测试阶段——请让团队知道任何错误。</p></div></div>    
</body>
</html>