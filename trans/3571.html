<html>
<head>
<title>How Back-Propagation Works — A Python Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播的工作原理——Python实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-back-propagation-works-a-python-implementation-21004d3b47c6#2022-08-09">https://towardsdatascience.com/how-back-propagation-works-a-python-implementation-21004d3b47c6#2022-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="20df" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文主要关注Python中反向传播的实现。我们已经在下面链接的前一篇文章中讨论了反向传播的数学基础。在这篇文章的最后，你会明白如何从头开始构建神经网络。</p><div class="ko kp gp gr kq kr"><a rel="noopener follow" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48"><div class="ks ab fo"><div class="kt ab ku cl cj kv"><h2 class="bd iu gy z fp kw fr fs kx fu fw is bi translated">反向传播在神经网络中是如何工作的？</h2><div class="ky l"><h3 class="bd b gy z fp kw fr fs kx fu fw dk translated">用一个例子演示背景如何在神经网络中工作。</h3></div><div class="kz l"><p class="bd b dl z fp kw fr fs kx fu fw dk translated">towardsdatascience.com</p></div></div><div class="la l"><div class="lb l lc ld le la lf lg kr"/></div></div></a></div><h1 id="2fff" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">数据、神经网络结构和初始参数</h1><p id="ad7e" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">我们将使用具有<code class="fe mk ml mm mn b">395</code>个数据点和<code class="fe mk ml mm mn b">3</code>特征(<code class="fe mk ml mm mn b">x1</code>、<code class="fe mk ml mm mn b">x2</code>和<code class="fe mk ml mm mn b">x3</code>)的数据集来训练<code class="fe mk ml mm mn b">3–4–1</code>网络。目标(<code class="fe mk ml mm mn b">y</code>)由两个值组成(<code class="fe mk ml mm mn b">0</code>表示失败，<code class="fe mk ml mm mn b">1</code>表示成功)。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mo"><img src="../Images/57ce300e20aa2e5358d9fa31ac0c7f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqC2ZS7MkunRMLkX39vzgQ.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图1:数据和神经网络架构。我们将实现一个3–4–1 NN，数据集为395个示例，具有3个特征和1个目标值y，图中显示的所有参数值都是初始值(来源:<a class="ae nd" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">https://towardsdatascience . com/how-does-back-propagation-work-in-neural-networks-with-worked-example-BC 59 DFB 97 f 48</a>)。</p></figure><p id="a5f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所有层的初始参数如下所示:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ne"><img src="../Images/6079fdfdb5c3e474d111a28ef9a1c4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qyuVuE3_YL4f2Tji3FbWA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图2:这些是参数的初始值(权重和偏差)。该图还显示了输入层上的单个数据点(来源:<a class="ae nd" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">https://towardsdatascience . com/how-does-back-propagation-work-in-neural-networks-with-worked-example-BC 59 DFB 97 f 48</a>)。</p></figure><p id="8ed3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将使用NumPy来执行大多数操作，充分利用它为操作的向量化和数组广播进行了优化这一事实。让我们使用NumPy进行一些简单的操作，您会发现它的用处。</p><h1 id="27ee" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">使用Numpy执行数学运算</h1><p id="c59b" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">假设我们想要同时执行由隐藏层的所有4个神经元进行的计算。通过矩阵乘法和加法，NumPy让事情变得更简单。<code class="fe mk ml mm mn b">z=w·x+b</code>其中<code class="fe mk ml mm mn b">w·x</code>是使用<code class="fe mk ml mm mn b">np.dot(w, x)</code>函数在Numpy ( <code class="fe mk ml mm mn b">np</code>)中实现的矩阵乘法。</p><p id="bda7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mk ml mm mn b">import numpy as np<br/>w = np.array([[ 0.179, 0.044, 0.01 ],<br/>[-0.186, -0.028, -0.035],<br/>[-0.008, -0.063, -0.004],<br/>[-0.048, -0.131, 0.088]])<br/>print("w shape: “, w.shape)<br/>x = np.array([7, 8, 10]).reshape(-1, 1)<br/>print(“x shape: “, x.shape)<br/>b = np.array([0, 0, 0, 0]).reshape(-1, 1)<br/>print(“b shape: “, b.shape)<br/>z = np.dot(w, x) + b<br/>print(z)</code></p><p id="2f65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:<br/> </strong> <em class="nf"> w1形态:(4，3) <br/> x形态:(3，1) <br/> b形态:(4，1)<br/>[[1.705]<br/>[-1.876]<br/>[-0.6]<br/>[-0.504]]</em></p><p id="b6cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这给了我们所有隐藏层的<code class="fe mk ml mm mn b">z</code>-值。接下来，通过矢量化，NumPy还允许我们为上面生成的所有值应用激活。</p><p id="2a05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mk ml mm mn b">sigmoid = lambda z: 1/(1+np.exp(-z))<br/>print(sigmoid(z1))</code></p><p id="ff6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:<br/></strong><em class="nf">[[0.84618664]<br/>【0.132849】<br/>【0.35434369】<br/>【0.37660112】]</em></p><p id="cdfb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">矢量化属性允许我们将所有z值传递到Sigmoid激活函数中，并获得隐藏层中所有4个神经元的所有输出。然后，这些值可以作为最后一层的输入进行传递。</p><h2 id="0972" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated">记号</h2><p id="0b4d" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">既然我们现在知道如何通过NumPy中的矩阵乘法和加法对给定层中的所有神经元执行操作，我们将采用以下新的符号。</p><ul class=""><li id="9682" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">x⁰</code>将表示整个输入数据—一个维度矩阵(特征数量、训练示例数量)，</li><li id="7376" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">wˡ</code> —连接层<code class="fe mk ml mm mn b">l-1</code>到<code class="fe mk ml mm mn b">l</code>的权重矩阵，</li><li id="9691" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">bˡ </code> —应用于层l中神经元的偏置向量，</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d03150a3bfda2e9e2c4f5030782959a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*26kekwGdstqXxglSm0fBFg.png"/></div><p class="mz na gj gh gi nb nc bd b be z dk translated">l层权重和偏差的维度(来源:<a class="ae nd" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">https://towards data science . com/how-neural-networks-actually-work-python-implementation-simplified-a 1167 B4 f 54 Fe</a>)</p></figure><ul class=""><li id="b1f9" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">zˡ</code> —加权输入加偏差。也就是代码中的<code class="fe mk ml mm mn b">wˡ·x + bˡ</code>或<code class="fe mk ml mm mn b"> np.dot(wˡ, x)+bˡ</code>，</li><li id="5405" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">g</code> —激活功能。我们将对隐藏层和输出层使用Sigmoid函数，</li></ul><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oh"><img src="../Images/4c0fd8303e220a179d1112d6245e02e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxhfbZKwn66FpKw_9EsRTg.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">乙状结肠激活功能。给定z的任意值，函数<code class="fe mk ml mm mn b">g</code>输出0到1范围内的实数。</p></figure><ul class=""><li id="cb9c" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">fˡ</code>—<code class="fe mk ml mm mn b">l</code>层输出的向量，即<code class="fe mk ml mm mn b">g(wˡ·x+bˡ)</code> = <code class="fe mk ml mm mn b">g(zˡ)</code>。这成为下一层<code class="fe mk ml mm mn b">(l+1)</code>的输入，</li><li id="9c57" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">yhat </code> —模型的最终输出，</li><li id="dca4" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">y</code>或<code class="fe mk ml mm mn b">t</code> —真实值，在我们的例子中是<code class="fe mk ml mm mn b">0</code>和<code class="fe mk ml mm mn b">1</code>。</li><li id="44d3" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">m</code>——#训练的例子。</li></ul><h1 id="476e" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">我们需要实现反向传播的关键方程</h1><p id="b4b8" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">为了在我们的浅层NN的代码中实现反向传播，我们需要在以下3个部分中简要讨论的<strong class="js iu"> 10个方程</strong>:<code class="fe mk ml mm mn b">A</code>、<code class="fe mk ml mm mn b">B</code>和<code class="fe mk ml mm mn b">C</code>(关键方程编号为<code class="fe mk ml mm mn b">1</code>到<code class="fe mk ml mm mn b">10</code>)。</p><h2 id="0a57" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated"><strong class="ak"> A .更新方程式，以及成本函数:</strong></h2><p id="88be" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">参数(权重和偏差)将使用以下等式在每一层进行更新:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oi"><img src="../Images/5b2cec2300acab5c00b1d782e0b3ce16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YIa-7ZW19pSSiRZC_DXoLQ.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">更新方程式</p></figure><p id="6d6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mk ml mm mn b">ϵ</code>是学习率——一个超参数，<code class="fe mk ml mm mn b">t</code>是学习步骤，<code class="fe mk ml mm mn b">E</code>是成本/误差函数。我们将使用<a class="ae nd" rel="noopener" target="_blank" href="/cross-entropy-loss-function-f38c4ec8643e">二元交叉熵损失函数</a>作为函数<code class="fe mk ml mm mn b">E</code>。它被定义为:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oj"><img src="../Images/e0a841969cf7d027fc7f66631724fb98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4njQhmdIzUs7Ub8bdsj8g.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">等式4:二元交叉熵损失函数。</p></figure><blockquote class="ok ol om"><p id="be6a" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">在代码中:</p><p id="b6c5" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated"><code class="fe mk ml mm mn b">cost = -np.sum(np.multiply(y, np.log(yhat)) + np.multiply(1-y, np.log(1-yhat)))/m</code></p><p id="c799" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">(针对m训练实例)。</p></blockquote><p id="79ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<code class="fe mk ml mm mn b">t</code>是真值(<code class="fe mk ml mm mn b">0</code>或<code class="fe mk ml mm mn b">1</code>)，而<code class="fe mk ml mm mn b">yhat</code>是模型的输出。由于我们在输出端使用Sigmoid函数，<code class="fe mk ml mm mn b">yhat</code>将在<code class="fe mk ml mm mn b">0</code>和<code class="fe mk ml mm mn b">1</code>的范围内。我们的参数(权重和偏差)在<code class="fe mk ml mm mn b">θ</code>中。</p><p id="a759" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mk ml mm mn b">E</code>相对于(wrt) <code class="fe mk ml mm mn b">yhat</code>的偏导数可以表示为(此处勾选<a class="ae nd" rel="noopener" target="_blank" href="/derivative-of-sigmoid-and-cross-entropy-functions-5169525e6705"/>):</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oq"><img src="../Images/3229f119b044e18ad895ced4068c5481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dq4k4iiVyb7yR3yf-ryv3Q.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">二元交叉熵损失函数的导数。</p></figure><p id="e303" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">而Sigmoid函数的导数由下式给出(参见<a class="ae nd" rel="noopener" target="_blank" href="/derivative-of-sigmoid-and-cross-entropy-functions-5169525e6705">本文</a>):</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi or"><img src="../Images/b069807218d5f878d0e98c1fa931e275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzh8E-1YgsC1Wn-T3O5nTQ.png"/></div></div></figure><p id="ef83" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意关于成本函数<code class="fe mk ml mm mn b">E</code>的以下内容。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi os"><img src="../Images/0e91750e6912bfe86bc540241010d043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DW0DQ4DQjRAJVUKCaf_rpw.png"/></div></div></figure><p id="6ae5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也就是说:</p><ul class=""><li id="00a6" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">E</code>是<code class="fe mk ml mm mn b">yhat</code>和<code class="fe mk ml mm mn b">t</code>的函数(已知)，</li><li id="3ab1" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">yhat=g(z)</code>表示<code class="fe mk ml mm mn b">yhat</code>是<code class="fe mk ml mm mn b">z</code>的函数，</li><li id="0019" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated"><code class="fe mk ml mm mn b">z =wx+b</code>，暗示，<code class="fe mk ml mm mn b">z</code>是<code class="fe mk ml mm mn b">w</code>和<code class="fe mk ml mm mn b">b</code>(我们的参数)的函数</li><li id="91da" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn nx ny nz oa bi translated">实际上，<code class="fe mk ml mm mn b">E</code>是权重(<code class="fe mk ml mm mn b">w</code>)和偏差(<code class="fe mk ml mm mn b">b</code>)的函数，可以写成<code class="fe mk ml mm mn b">E(yhat, t)</code>或<code class="fe mk ml mm mn b">E(g(z), t)</code>或<code class="fe mk ml mm mn b">E(g(wx+b), t)</code>。</li></ul><p id="01c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">含义:</strong>要得到<code class="fe mk ml mm mn b">E</code> wrt对<code class="fe mk ml mm mn b">w</code>和<code class="fe mk ml mm mn b">b</code>的导数(需要更新参数)，我们需要得到</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ot"><img src="../Images/e4c04cd1a0a50e5e77144d94a264a8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNuiB_-3_CD-RmLyWHICIA.png"/></div></div></figure><p id="5738" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此(根据链式微分法则)，</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ou"><img src="../Images/dcd8a12ca6d4da3f94cd8cc3d1c1d5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTVO2xovsGIViBfzuil2Vw.png"/></div></div></figure><h2 id="3637" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated">B.更新输出隐藏层(w和b)中的参数所需的关键方程</h2><p id="42d9" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">如果我们需要更新<code class="fe mk ml mm mn b">w²</code>和<code class="fe mk ml mm mn b">b²</code>，我们需要计算:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2af72447127e48210bad83ade7c3e75f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*9JyEyNV-Jzw9vdyTK-TdYg.png"/></div></figure><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ow"><img src="../Images/a46167e51c9b6a7cd683ac8e8159b185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWiZiNcjHV4daoIkwM175A.png"/></div></div></figure><p id="c7f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ox"><img src="../Images/39358ec5c01664e880cd2e05fc5f8b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhleXhEpJ59M_Lu_Bn7ZDQ.png"/></div></div></figure><p id="fdc4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oy"><img src="../Images/9b7bf6d2c0ea16b3746c63a17ca81589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fA1u6YrTGSJ-phFeFvCo6A.png"/></div></div></figure><blockquote class="ok ol om"><p id="be05" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">在代码中:</p><p id="3cc8" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">方程式1: <code class="fe mk ml mm mn b">dz2 = yhat-y</code> <br/>方程式2: <code class="fe mk ml mm mn b">dw2 = np.dot(dz2, f1.T)/m</code> <br/>方程式3: <code class="fe mk ml mm mn b">db2 = 1/m * np.sum(dz2, axis=1, keepdims=True)</code></p><p id="4d8c" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">(A.T是矩阵A的转置)</p></blockquote><p id="78f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在有了更新<code class="fe mk ml mm mn b">w²</code>和<code class="fe mk ml mm mn b">b²</code>所需的等式，如下所示，</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oz"><img src="../Images/83312fd39f1a15a54264f5de15fc4440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wm2m6ACzs6XLhz0Am2f-MQ.png"/></div></div></figure><blockquote class="ok ol om"><p id="f256" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">在代码中:</p><p id="8c73" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">方程式4:<code class="fe mk ml mm mn b">w2 = w2-learning_rate*dw2</code>T35】方程式5: <code class="fe mk ml mm mn b">b2 = b2-learning_rate*db2</code></p></blockquote><h2 id="72be" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated">C.更新隐藏输入层(w和b)中的参数所需的重要方程</h2><p id="8edc" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">我们需要计算下面的导数。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pa"><img src="../Images/52c063bcce14fc0fdb7dc80083ae6b0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Szaqemhvn_EKJrMXtVrA0Q.png"/></div></div></figure><p id="dd37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你求解这些导数，你将得到以下结果(如果有些东西没有意义，请参考<a class="ae nd" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">主要参考资料</a>)</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pb"><img src="../Images/118180a44bb4f55cb1a9fd4064b32de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIRoWceZPM4dLeSzN4-Qiw.png"/></div></div></figure><blockquote class="ok ol om"><p id="f4fe" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">在代码中:</p><p id="d879" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">方程式6: <code class="fe mk ml mm mn b">dz1 = np.dot(w2.T, dZ2) * (1-f1)*f1</code> <br/>方程式7: <code class="fe mk ml mm mn b">dw1 = np.dot(dz1, X.T)/m</code> <br/>方程式8: <code class="fe mk ml mm mn b">db1 = 1/m * np.sum(dz1, axis=1)</code></p></blockquote><p id="6ef3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们可以用以下等式更新隐藏输入端的参数:</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pc"><img src="../Images/959e960c622db0bc31093b8f9d37d9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sf8b7rfQ-IBDDiJZnqKxow.png"/></div></div></figure><blockquote class="ok ol om"><p id="7782" class="jq jr nf js b jt ju jv jw jx jy jz ka on kc kd ke oo kg kh ki op kk kl km kn im bi translated">代码:<br/>公式9:<code class="fe mk ml mm mn b">w1 = w1-self.learning_rate*dw1</code>T41】公式10: <code class="fe mk ml mm mn b">b1 = b1-self.learning_rate*db1</code></p></blockquote><p id="955a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">至此，我们有了<code class="fe mk ml mm mn b">10</code>关键方程。让我们把它们放在桌子上。因为我们将用m &gt; 1个训练例子来训练我们的模型，我们将使用右边的等式。</p><figure class="mp mq mr ms gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi pd"><img src="../Images/4a9bbf2417b791bae494ef718227967b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z6F9hd-OqnPIrXxHDuy4Dg.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">在代码中实现反向传播所需的10个等式。*注意，当实现m个示例的back-prop时(右)，变量可能是具有m个元素的向量。例如，<code class="fe mk ml mm mn b">yhat</code>和<code class="fe mk ml mm mn b">t</code>将分别包含m个示例的模型的所有预测和所有示例的真值。</p></figure><h1 id="4205" class="lh li it bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">把一切都放进Python代码</h1><p id="34de" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">为了从头开始构建一个迭代的神经网络，我们需要实现前向传播和后向传播。我们已经在后面的文章中看到了如何实现前向传播。我只粘贴下面的代码；然后，我们添加反向传播组件。该代码在我们的<code class="fe mk ml mm mn b">3–4–1</code> NN中通过一个训练示例实现了向前传递。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="56ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="mp mq mr ms gt pg mn ph pi aw pj bi"><span id="2e3b" class="ng li it mn b gy pk pl l pm pn">Input shape (3, 1)<br/>w1 shape:  (4, 3)<br/>w2 shape:  (1, 4)<br/>w1 shape:  (4, 3)<br/>b1 shape (4, 1)<br/>w2 shape:  (1, 4)<br/>b2 shape (1, 1)<br/>f1 shape (4, 1)<br/>z2.shape (1, 1)<br/>yhat shape (1, 1)<br/>[[0.521]]</span></pre><p id="0c4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">单个数据点<code class="fe mk ml mm mn b">x=[7, 8, 10]</code>的单次正向传递输出<code class="fe mk ml mm mn b">~0.521 </code>(如预期的那样，见图1)。</p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><p id="8337" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在前向传播之后，我们需要<strong class="js iu">计算近似的成本</strong>，<strong class="js iu">执行反向传播</strong>，最后，<strong class="js iu">更新参数</strong>。为了实现这些步骤，让我们向上面代码中的<code class="fe mk ml mm mn b">ForwardPass</code>类添加三个函数，并将该类重命名为<code class="fe mk ml mm mn b">OurNeuralNet</code>。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="3a0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们根据需要增加了三个功能:<code class="fe mk ml mm mn b">compute_cost(args)</code>、<code class="fe mk ml mm mn b">backward_propagation(args)</code>和<code class="fe mk ml mm mn b">update_parameters(args)</code>。注意，我们的类现在需要我们定义学习率。</p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h2 id="f7e8" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated">训练模型</h2><p id="fd4d" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">类<code class="fe mk ml mm mn b">OurNeuralNet</code>现在有了实现NN操作所需的所有函数——参数初始化、向前传递、损失计算、反向传播和更新参数。这一次，让我们调用上面的类在更大的数据集上训练模型，而不是单个例子。数据在以下链接:【https://kipronokoech.github.io/assets/datasets/marks.csv】T21。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="ee9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我们将调用<code class="fe mk ml mm mn b">train_model(args)</code>函数用部分数据训练我们的模型。在<code class="fe mk ml mm mn b">395</code>个例子中，我们将使用第一个<code class="fe mk ml mm mn b">300</code>来训练模型。我们正在为<code class="fe mk ml mm mn b">4500</code>迭代训练模型，学习率为<code class="fe mk ml mm mn b">0.2</code>(实际上是任意选择)。在每次迭代中，我们还计算预测<code class="fe mk ml mm mn b">yhat</code>的模型精度。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="e77c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><pre class="mp mq mr ms gt pg mn ph pi aw pj bi"><span id="1308" class="ng li it mn b gy pk pl l pm pn">iteration 0 cost:--&gt; 0.691 accuracy--&gt; 0.67<br/>iteration 1000 cost:--&gt; 0.461 accuracy--&gt; 0.78<br/>iteration 2000 cost:--&gt; 0.203 accuracy--&gt; 0.917<br/>iteration 3000 cost:--&gt; 0.162 accuracy--&gt; 0.933<br/>iteration 4000 cost:--&gt; 0.169 accuracy--&gt; 0.947<br/>iteration 4500 cost:--&gt; 0.108 accuracy--&gt; 0.99<br/>{'w1': array([[ 0.074, -0.349,  1.217],<br/>       [-0.05 , -0.167,  1.144],<br/>       [-0.265,  0.091,  1.068],<br/>       [-0.265,  0.079,  1.077]]), 'b1': array([[-7.196],<br/>       [-7.103],<br/>       [-6.843],<br/>       [-6.818]]), 'w2': array([[2.81 , 2.743, 2.586, 2.573]]), 'b2': array([[-7.392]])}</span></pre><p id="e63d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">哇！如您所见，随着模型的训练，预测的成本在下降，训练的准确度在上升，达到了<code class="fe mk ml mm mn b">99%</code>的高点。我们的模式是学习！</p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h2 id="4f23" class="ng li it bd lj nh ni dn ln nj nk dp lr kb nl nm lv kf nn no lz kj np nq md nr bi translated">测试模型</h2><p id="9504" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated"><strong class="js iu">理解</strong>:我们已经使用<code class="fe mk ml mm mn b">300/395</code>数据例子进行模型训练。这被称为<em class="nf">训练集</em>。为了确保我们的模型能够在<em class="nf">看不见的数据</em>(通常称为<em class="nf">测试集</em>)上进行归纳，我们将使用没有用于模型训练的<code class="fe mk ml mm mn b">95</code>数据点进行测试(参见下面的代码片段)。</p><p id="61ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意:</strong>为了进行测试，我们需要使用我们在前面部分中训练模型后找到的参数对特征数据进行单次正向传播。</p><figure class="mp mq mr ms gt mt"><div class="bz fp l di"><div class="pe pf l"/></div></figure><p id="344d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><pre class="mp mq mr ms gt pg mn ph pi aw pj bi"><span id="8d94" class="ng li it mn b gy pk pl l pm pn">Testing on 95 data points</span><span id="8adb" class="ng li it mn b gy pv pl l pm pn">0.9157894736842105</span></pre><p id="63e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">厉害！该模型在测试集上达到了<code class="fe mk ml mm mn b">91.6%</code>精度。这意味着该模型可以很好地概括看不见的(测试)数据。</p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><p id="ab1a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这标志着本文的结束。如果您发现这里的一些概念具有挑战性，请浏览以下参考资料:</p><ol class=""><li id="040d" class="ns nt it js b jt ju jx jy kb nu kf nv kj nw kn pw ny nz oa bi translated"><a class="ae nd" rel="noopener" target="_blank" href="/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b">神经网络的基础知识</a></li><li id="5c80" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn pw ny nz oa bi translated"><a class="ae nd" rel="noopener" target="_blank" href="/feed-forward-neural-network-with-example-neural-network-series-part-2-eeca7a081ef5">神经网络如何工作——使用工作示例</a></li><li id="6dc0" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn pw ny nz oa bi translated"><a class="ae nd" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">神经网络的实际工作方式——Python实现(简化)</a></li><li id="71e6" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn pw ny nz oa bi translated"><a class="ae nd" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45">神经网络实际上是如何工作的——Python实现第2部分(简化)</a></li><li id="ba3f" class="ns nt it js b jt ob jx oc kb od kf oe kj of kn pw ny nz oa bi translated">神经网络中的反向传播是如何工作的？【数学基础】(T29)</li></ol></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h1 id="21eb" class="lh li it bd lj lk px lm ln lo py lq lr ls pz lu lv lw qa ly lz ma qb mc md me bi translated">使用的代码</h1><ul class=""><li id="254f" class="ns nt it js b jt mf jx mg kb qc kf qd kj qe kn nx ny nz oa bi translated">本文中使用的代码可以在我的git repo中找到:<a class="ae nd" href="https://github.com/kipronokoech/Neural-Networks-from-Scratch" rel="noopener ugc nofollow" target="_blank">https://github . com/kipronokech/Neural-Networks-from-Scratch</a>。</li></ul></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><p id="eaec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请<a class="ae nd" href="https://medium.com/@kiprono_65591/membership" rel="noopener">以每月5美元的价格注册成为medium会员</a>，这样就可以在Medium上阅读我和其他作者的所有文章。</p><p id="1c0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你也可以<a class="ae nd" href="https://medium.com/subscribe/@kiprono_65591" rel="noopener">订阅，以便在我发表文章时将我的文章发送到你的邮箱</a>。</p><p id="5a60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读，下次再见！！！</p></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><h1 id="56d8" class="lh li it bd lj lk px lm ln lo py lq lr ls pz lu lv lw qa ly lz ma qb mc md me bi translated">结论</h1><p id="51e6" class="pw-post-body-paragraph jq jr it js b jt mf jv jw jx mg jz ka kb mh kd ke kf mi kh ki kj mj kl km kn im bi translated">在本文中，我们从头开始训练一个浅层神经网络(一个只有一个隐藏层的神经网络)。我们已经了解了如何初始化参数、执行特征数据的正向传递、实现反向传播以及更新参数。在接下来的文章中，我们将讨论如何选择学习速率、适当的参数初始化以及改变隐藏神经元的数量对模型性能的影响。</p></div></div>    
</body>
</html>