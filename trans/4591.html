<html>
<head>
<title>Applied Reinforcement Learning II: Implementation of Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用强化学习II:Q学习的实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applied-reinforcement-learning-ii-implementation-of-q-learning-464ec017e777#2022-10-12">https://towardsdatascience.com/applied-reinforcement-learning-ii-implementation-of-q-learning-464ec017e777#2022-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0e4e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Q学习算法的实现及其在OpenAI Gym的Taxi-v3环境中的应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/87e596a1d2af5c9e8e9bfc1157f13954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T5DLrc5gg6neoBxA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">理查德·贝尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第一篇文章介绍了任何强化学习系统的基本概念和组件，并解释了Q学习算法背后的理论。在本文中，我们的目标是在Python3中实现该算法，并将其应用到真实的训练环境中。第一篇文章中提到的所有概念(<strong class="lb iu">应用强化学习I: Q-Learning </strong>)都将在这篇文章中应用，假设读者知道并理解它们，所以如果你不熟悉这些概念，或者没有读过第一篇文章，你可以在下面访问它:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/applied-reinforcement-learning-i-q-learning-d6086c1f437"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">应用强化学习I:Q-学习</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">逐步理解Q学习算法，以及任何基于RL的系统的主要组件</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="123e" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">环境— Taxi-v3</h1><p id="835a" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">为了使这篇文章更有启发性，我们选择了一个简单的基本环境，它不会给训练增加太多的复杂性，因此Q学习算法的学习可以得到充分的理解。环境是OpenAI Gym的<em class="nr">Taxi-v3</em><strong class="lb iu">【1】</strong>，它由一个网格世界组成，其中的代理是一名出租车司机，他必须搭载一名客户并让他在目的地下车。</p><h2 id="5eac" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">行动</h2><p id="fc6b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">至于动作空间，以下离散动作可供智能体与环境交互:<em class="nr">前进</em>，<em class="nr">后退</em>，<em class="nr">向右</em>，<em class="nr">向左</em>，<em class="nr">搭载乘客</em>和<em class="nr">让他下车</em>。这样总共有6个可能的动作，为了便于编程，这些动作依次用0到5的数字进行编码。动作与数字的对应关系如图<em class="nr">图1 </em>所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/711b8cb4d942fbd4da573026b590381e.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*I9tRzbURpnmkl7lmvitNnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd of">图一</strong>。操作-数字映射。作者图片</p></figure><h2 id="6234" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">州</h2><p id="5af3" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">离散状态空间要大得多，因为每个状态都表示为一个元组，其中包含代理/出租车司机在网格上的位置、要搭载的乘客的位置及其目的地。由于地图是二维网格，代理人的位置完全可以用其在x轴上的位置和在y轴上的位置来表示，所以表示代理人状态的元组是:<strong class="lb iu"> ( <em class="nr"> pos_agent_x </em>，<em class="nr"> pos_agent_y </em>，<em class="nr"> passenger_location </em>，<em class="nr"> destination </em> ) </strong>。与动作的情况一样，表示状态的元组被编码成0到499之间的整数(500个可能的状态)。<em class="nr">图3 </em>显示了代理的位置/目的地与表示该位置/目的地的整数之间的对应关系，该整数用于描述状态的元组中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/592b092f9e162db79566952147e93d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*oorPWO4YBEFHVycrNOfxAA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd of">图二</strong>。乘客位置/目的地—号码映射。作者图片</p></figure><p id="af20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了直观地理解可能状态的元组表示，代理的两个状态示例如图3 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/028dbb66eeeeb6d8dbc1bdede1081745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmCY4Z99beNEpbRPhVkMXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd of">图3 </strong>。状态的可视化示例。作者图片</p></figure><h2 id="b5a8" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">奖励</h2><p id="26c8" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">至于代理执行的每一步所获得的奖励，将会是:</p><ul class=""><li id="593a" class="oi oj it lb b lc ld lf lg li ok lm ol lq om lu on oo op oq bi translated">+20表示成功运送乘客(终点站状态)</li><li id="4552" class="oi oj it lb b lc or lf os li ot lm ou lq ov lu on oo op oq bi translated">-10用于非法执行<em class="nr">拾取</em>或<em class="nr">放下</em>动作</li><li id="c865" class="oi oj it lb b lc or lf os li ot lm ou lq ov lu on oo op oq bi translated">-1，除非触发了其他奖励</li></ul></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="86d7" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated"><strong class="ak">Q-学习实现</strong></h1><p id="841b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">现在，环境是众所周知的，Q学习算法的实现可以继续进行。与第一篇文章一样，从巴尔托和萨顿的书<strong class="lb iu">【2】</strong>中提取的伪代码将被用作支持算法实现的参考。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/05b708844e4979270c1ccf851f383f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*6rhkj1RlyskejsYcuKdHBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q-学习伪代码。摘自萨顿和巴尔托:《强化学习:<br/>导论】<strong class="bd of">【2】</strong></p></figure><h2 id="3480" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">1)初始化Q表</h2><p id="07e1" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Q表被初始化为一个<strong class="lb iu"> m </strong> x <strong class="lb iu"> n </strong>矩阵，其所有值都被设置为零。其中<strong class="lb iu"> m </strong>是状态空间的大小，<strong class="lb iu"> n </strong>是动作空间的大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="b423" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为动作和状态都被编码成整数，所以可以使用这些整数作为索引来构建Q表。</p><h2 id="d17f" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">2)定义ε-贪婪策略</h2><p id="dfb1" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">ε-贪婪策略为给定状态选择具有最高Q值的动作<strong class="lb iu">或随机动作</strong>，这取决于所选择的ε参数。例如，epsilon为0.15表示15%的时间会随机选择一个动作，而epsilon为1表示该动作总是随机选择的(100%的时间)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="0376" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该策略非常有趣的一点是，ε值可以随着训练而变化，允许在高ε值(探索阶段)开始时采取更多随机行动，最终使用非常低的ε值(开发阶段)采取具有更高Q值的行动。然而，对于这种环境，训练是以恒定的ε值进行的。</p><h2 id="0615" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">3)定义一集的执行</h2><p id="b329" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">对于每一集，代理将根据需要执行尽可能多的时间步长，以达到最终状态。在每个时间步中，代理将<strong class="lb iu"> 1) </strong>选择一个遵循ε-贪婪策略的动作，并执行该动作。在执行之后，代理将<strong class="lb iu"> 2) </strong>观察达到的新状态和获得的奖励，这些信息将被用于<strong class="lb iu"> 3) </strong>更新其Q表的Q值。</p><p id="5b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对每个时间步长重复这个过程，直到获得最佳Q值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="d69a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以看到，代码非常简单:前面定义的<strong class="lb iu">get _ epsilon _ greedy _ action()</strong>函数用于选择动作，代理通过环境的<strong class="lb iu"> step() </strong>方法执行选择的动作，最后通过应用贝尔曼最优性方程的适配来更新Q值，如前一篇文章中所描述和解释的。</p><h2 id="6457" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">4)培训代理人</h2><p id="49b9" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">此时，只需要定义算法的超参数，分别是:<strong class="lb iu">学习率<em class="nr">α</em>T13】、<strong class="lb iu">折扣因子<em class="nr">γ</em>T17】和<strong class="lb iu">ε</strong>。除此之外，还需要指定代理必须完成的集数，以便认为培训已经完成。</strong></strong></p><p id="6688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义完所有这些变量后，训练的执行将包括为每个训练集运行上面定义的<strong class="lb iu"> execute_episode() </strong>函数。每一集(以及每一集内的每个时间步长)将更新Q表，直到达到最佳值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><p id="39ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，培训执行将每集获得的奖励记录在<em class="nr"> rewards_history </em>变量中，以便能够在培训后显示评估结果。</p><h2 id="6945" class="ns mv it bd mw nt nu dn na nv nw dp ne li nx ny ng lm nz oa ni lq ob oc nk od bi translated">5)评估代理</h2><p id="8f28" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">为了评估代理，将使用从每个训练集获得的奖励、执行一集的被训练代理的可视化以及被训练代理的几次执行的度量。</p><p id="b548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在培训过程中获得的奖励是一个非常重要的指标，因为它应该显示奖励向最佳值的收敛。在这种情况下，由于除了终端状态，每个时间步长的回报都是负的，所以算法应该使回报尽可能接近0，甚至超过0。下面显示了在具有不同超参数的两次训练期间获得的每集奖励的曲线图。可以看出，第一个图显示了奖励如何快速达到最大值，在0处画了一条渐近线，这意味着代理已经学会了在每个状态下获得最佳可能的奖励(最优Q值)。另一方面，第二张图显示，在从0到-20000的范围内，奖励既不变得更好也不变得更差，这意味着代理人没有学习任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/5b3995458311969354aac14242864d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*AheDQlXuMVgU53oeFpZaAA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奖励-具有不同超参数的两次训练的情节图。作者图片</p></figure><p id="f8e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，第二次训练效果如此差的原因是由于ε的值过高，因为ε为0.99将导致大多数操作被随机选择，完全忽略了开发阶段。</p><p id="b8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于代理的可视化，将使用渲染模式，这允许代理在与环境交互时被可视化。为了查看代理已经学习了什么，执行了epsilon为0的一集，即:代理总是采取具有最高Q值的动作，从而遵循最优策略。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练有素的Q学习代理执行一集。作者GIF</p></figure><p id="615b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图所示，执行上面的代码将显示训练有素的代理接载乘客并把他带到目的地，明确显示代理已经学会如何正确执行他的任务。</p><p id="e878" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，还将在几个不同的情节中评估受训代理的行为，从而发现代理从乘客的不同起点、不同地点和目的地成功完成任务的能力。就像在可视化中一样，代理将总是选择具有最高Q值的动作，从而显示它已经学习了什么，而不是随机行为。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/9cd190bf7ea27c726b169f5916ed8a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*tjoPq2unMttG2v-l-l_5JA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">execute _ eptions _ on _ trained _ agent()方法的执行日志。作者图片</p></figure><p id="d6d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如执行日志所示，完成一集所需的平均时间步数为13.8，每集获得的平均奖励为7.2。这两个度量都表明代理已经很好地学会了执行任务，因为它需要很少的时间步来完成任务，并且在所有执行中实现了大于0的奖励。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="43ae" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结论</h1><p id="5a92" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Q-Learning已经显示出能够非常容易地学习任务，因为当用适当的超参数执行时，它只需要50集就可以达到最佳Q值。尽管结果非常好，但必须强调超参数在这种类型的训练中的重要性(两个每集奖励图显示了epsilon在这种类型的训练中的决定性)，因为足够的值会使代理快速收敛到最优Q值，而错误的值会导致代理永远无法学习。还必须考虑到的是，<em class="nr"> Taxi-v3 </em>环境是一个离散且简单的环境，对于Q-Learning算法来说，它总是显示出非常好的结果，因此代理已经快速且有效地学习了任务的事实并不意味着如果将它应用于其他类型的环境，这种学习也会同样有效。</p><p id="e0e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，对于Q学习的应用来说，动作空间和状态空间都是离散的是至关重要的，因为Q表的构造对于连续的状态或动作是不可行的(Q表将具有无限的行或列)。因此，尽管从这种训练中获得了很好的结果，Q-Learning并不总是适用于其他类型的环境，例如连续问题，这将在本系列的后续文章中介绍。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="82ff" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">密码</h1><p id="de0d" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">为了便于理解和学习，Q-Learning算法的完整实现，以及前面提到的对经过训练的代理的可视化和评估，可以在我的GitHub存储库中找到，作为一个Jupyter笔记本。</p><div class="lv lw gp gr lx ly"><a href="https://github.com/JavierMtz5/ArtificialIntelligence" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">GitHub-Javier mtz 5/人工智能</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">github.com</p></div></div><div class="mh l"><div class="pc l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="2ad6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考</h1><p id="a788" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><strong class="lb iu">【1】</strong>open ai健身房Taxi-v3<br/><a class="ae ky" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py" rel="noopener ugc nofollow" target="_blank">https://github . com/open ai/Gym/blob/master/Gym/envs/toy _ text/Taxi . py</a></p><p id="fdbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【2】</strong>理查德·萨顿；强化学习:介绍。麻省理工学院出版社，2018</p></div></div>    
</body>
</html>