<html>
<head>
<title>Flux.jl on MNIST — Variations of a theme</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MNIST——主题变奏曲</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/flux-jl-on-mnist-variations-of-a-theme-c3cd7a949f8c#2022-06-23">https://towardsdatascience.com/flux-jl-on-mnist-variations-of-a-theme-c3cd7a949f8c#2022-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d1da35fba9bf360fa5f21982b0fcb407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_6Y2uTmnRbinHgzfiXVDg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">詹姆斯·奥尔在<a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><div class=""><h2 id="e924" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">Flux.jl是一个ML-stack，提供轻量级组件来创建模型并训练它们。使用MNIST数据集，我们将看到通过将这些组件组合在一起，构建不同的数据集分类方法是多么容易。</h2></div><h1 id="f86c" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">概观</h1><h2 id="1398" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">Flux.jl</h2><p id="5d1e" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated"><a class="ae jd" href="https://fluxml.ai/" rel="noopener ugc nofollow" target="_blank"> Flux.jl </a>是一个100%用Julia写的包。它旨在建立模型，这些模型通常使用基于自动微分的迭代方法进行训练。这类模型中最常见的可能是神经网络(NN ),它使用梯度下降算法(GD)的一种变体来训练。</p><p id="1a28" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">与其他一些包含现成的“机器”来构建和训练模型的ML库相比，Flux提供了一套轻量级的乐高积木，您可以根据自己的特定需求将它们组装在一起。</p><p id="dfeb" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在本文中，我将展示这种类似lego的方法如何应用于构建几个NN模型(准确地说是<a class="ae jd" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank"> <em class="na">多层感知器</em> </a>)来对图像进行分类，并实现GD算法的不同变体来训练它们。</p><h2 id="baac" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">MNIST数据集</h2><p id="7ec6" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">为此，我们将使用众所周知的<a class="ae jd" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST数据集</a>，它由70，000张手写(和标记)数字图像组成。这些图像中的60.000个用作训练数据，剩余的10.000个实例将用于测试模型。每个图像由28×28灰度像素组成。</p><p id="9826" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">使用<code class="fe nb nc nd ne b">MLDatasets</code>包可以轻松加载整个数据集:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="2dde" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">第一个表达式(第3行)自动获取带有相应标签的60.000个训练图像，第4行的变体使用<code class="fe nb nc nd ne b">split = :test</code>获取其余的用于测试。所以<code class="fe nb nc nd ne b">train_x_raw</code>是一个28x28x60000元素的数组，<code class="fe nb nc nd ne b">test_x_raw</code>经过这次运算后是一个28x28x10000元素的数组。标签存储在<code class="fe nb nc nd ne b">train_y_raw</code>和<code class="fe nb nc nd ne b">test_y_raw</code>中，它们分别是大小为60.000和10.000的数组，包含0到9之间的整数。</p><p id="6af7" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">函数<code class="fe nb nc nd ne b">convert2image</code>允许我们显示这样的图像。我们通过调用<code class="fe nb nc nd ne b">convert2image(MNIST, train_x_raw[:,:,10])</code>得到例如第10个训练图像:</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3fd0c314e09d637c1f07dbb2611b3b89.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*XIn1WpD5zVqB3OgAGvZ77A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">手写数字4的灰度图像[图片由作者提供]</p></figure><p id="ce88" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">相应的标签(4)在<code class="fe nb nc nd ne b">train_y_raw[10]</code>。</p><p id="07e3" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">这张图片(在<code class="fe nb nc nd ne b">train_x_raw[:,:,10]</code>中)的数据只是一个28x28元素的矩阵，包含0到1之间的数字，每个数字代表一个灰色阴影。</p><h1 id="e81d" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">模型</h1><p id="24b6" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">如上所述，我们的目标是创建能够对这些图像进行分类的不同模型。也就是说，我们的模型的输入将是一个手写数字的图像(以28x28矩阵的形式)，输出是一个0到9之间的数字，告诉我们图像包含哪个数字。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/dc909b26e602e30d15d0c83c19d50842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkNb50HIj_vahOiE6W477g.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基本处理流程[图片由作者提供]</p></figure><h2 id="3515" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">数据</h2><p id="59bc" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">不幸的是，神经网络既不直接处理矩阵，也不以我们想要的方式输出数字。所以我们必须对我们的数据做一些调整。</p><p id="6c62" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">神经网络的<strong class="me jh">输入</strong>必须是包含所有值的(列)向量。因此，我们必须转换我们的28x28矩阵，并将每张图片的28列堆叠在一起。结果是一个784元素的向量(28 x 28 = 784)。通量函数<code class="fe nb nc nd ne b">flatten()</code>就是这样做的:</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="2514" class="ln kw jg ne b gy nr ns l nt nu">train_x = Flux.flatten(train_x_raw)<br/>test_x  = Flux.flatten(test_x_raw)</span></pre><p id="ab06" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">这分别产生一个784x60000和一个784x10000元素的数组。</p><p id="5a22" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">用于这种分类问题的NN的<strong class="me jh">输出</strong>通常是所谓的独热向量。这是一个位向量(在我们的例子中有10个元素，因为我们有10个不同的类),其中正好有一位是1，所有剩余的位都是0。值为1的元素表示相应的类别，即如果第一位为1，则表示“数字0”，如果第四位为1，则表示“数字3”，依此类推。</p><p id="e90c" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">通量函数<code class="fe nb nc nd ne b">onehotbatch()</code>对整个标签阵列进行这种转换:</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="80b0" class="ln kw jg ne b gy nr ns l nt nu">train_y = Flux.onehotbatch(train_y_raw, 0:9)<br/>test_y  = Flux.onehotbatch(test_y_raw, 0:9)</span></pre><p id="7103" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">自变量<code class="fe nb nc nd ne b">0:9</code>告知必须由结果独热向量表示的数字的(可能)范围。这些语句的结果分别是10x60000和10x10000位数组。</p><p id="ff71" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">因此，我们为图像预测调整的处理管道看起来像这样:</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/c6c0abbcd6bdfd1a8387ac950d86ffc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBrHq9vR5g2IDHrZyUCheQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">改编的处理流程[图片由作者提供]</p></figure><p id="0f22" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated"><em class="na">注意</em>:在现实世界的应用中，神经网络很少会产生“真正的”单热点向量。相反，将产生包含概率的向量，并且如果NN工作良好，这些值中的一个将接近1，而所有其他值将接近0。</p><h2 id="fd53" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">多层感知器网络</h2><p id="4611" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">我们想要使用的模型是所谓的<em class="na">多层感知器网络</em> (MLP)。这些是“经典”神经网络，由几层神经元组成，其中一层的<em class="na">每个</em>神经元连接到下一层的<em class="na">所有</em>神经元(也称为<em class="na">全连接</em>或<em class="na">密集</em>网络):</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/ee10cfa2ffb14c3247b92a1ae8eb77e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*siEq5hZRKZ4BdPSPsroCMA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一个三层感知器[图片由作者提供]</p></figure><p id="47be" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">不同的MLP在</p><ul class=""><li id="d575" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu oc od oe of bi translated">层数，</li><li id="5076" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated">每层中神经元的数量</li><li id="afdd" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated">所谓的激活函数，其在传递到下一层之前应用于神经元的结果；这种激活功能可能因层而异。</li></ul><h2 id="8176" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">MNIST分类模型</h2><p id="8379" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在我们的例子中，第一层中输入值的数量已经由输入数据固定(到784)，最后一层必须有10个神经元(产生10个输出值)，因为我们有10个类。但是我们可以自由选择模型的其他特征。</p><p id="fd3f" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">为了简单起见，我选择了三个可以在互联网上其他地方找到的模型:一个来自格兰特·桑德森在他的YouTube频道“3Blue1Brown”上关于神经网络的精彩视频(“T10”在这里你可以找到第一个及其精彩的可视化)，第二个(改编自)来自我们主题的这个<a class="ae jd" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">深度处理</a>，第三个来自<a class="ae jd" href="https://fluxml.ai/tutorials/2021/01/26/mlp.html" rel="noopener ugc nofollow" target="_blank"> Flux文档</a>。</p><p id="4000" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">具体来说，我们有以下型号(我根据它们的主要特征命名)</p><ul class=""><li id="c313" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu oc od oe of bi translated"><strong class="me jh"> 4LS </strong>:一个<strong class="me jh"> 4层</strong>模型，使用16个内层节点和<strong class="me jh">s形</strong>激活函数</li><li id="1d0d" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><strong class="me jh"> 3LS </strong>:一个<strong class="me jh">三层</strong>模型，使用60个内层节点和<strong class="me jh"> sigmoid </strong>激活函数</li><li id="11f1" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><strong class="me jh"> 2LR </strong>:使用内层32个<strong class="me jh"> </strong>节点和<strong class="me jh"> relu </strong>激活功能的<strong class="me jh"> 2层</strong>模型</li></ul><p id="71b7" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated"><em class="na">旁注</em>:为了更接近Flux中的以下模型定义，我没有将输入和输出作为一个单独的层(在别处是这样做的)。</p><p id="e5f6" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">使用通量的方法，我们可以将这三个模型定义如下(在注释中，您可以看到每个模型的参数数量):</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="8ae9" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">第三种模式(<code class="fe nb nc nd ne b">model2LR</code>)仅在第一层使用<code class="fe nb nc nd ne b">relu</code>-激活功能。在第二层上，没有指定这样的功能。这意味着使用默认流量(身份)。因此，该层可能会产生[-∞，∞]范围内的值，这不是我们想要的。因此，<code class="fe nb nc nd ne b">softmax</code>-函数应用于结果，将它们标准化到从0到1的范围，并确保它们的总和为1。</p><p id="5526" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">使用<code class="fe nb nc nd ne b">sigmoid</code>-功能激活的前两个型号是相当“传统”的NNs，而现在<code class="fe nb nc nd ne b">sigmoid</code>已经大部分被<code class="fe nb nc nd ne b">relu</code>取代。</p><h1 id="dc43" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">梯度下降训练</h1><p id="fe19" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">训练这样的模型意味着找到最佳的参数值，以便该模型能够以高精度进行预测。</p><h2 id="ab1e" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">价值函数</h2><p id="2f78" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">为了测量一组特定的参数相对于这个目标的表现有多好，我们需要一个所谓的成本函数。该成本函数<em class="na"> C </em>将利用特定参数集做出的<em class="na">预测</em> <em class="na"> ŷ </em>与训练数据中的<em class="na">实际类别y </em>进行比较，并在此基础上计算该“距离”(= <em class="na"> C(ŷ，y) </em>的指标。即成本函数的结果越小，选择的参数越好。所以我们的目标是找到一组使成本函数最小的参数。</p><p id="1f63" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在这种情况下使用的典型成本函数是:</p><ul class=""><li id="c27a" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu oc od oe of bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差</a> ( <em class="na">均方误差</em></li><li id="9b24" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a> ( <em class="na"> ce </em>)</li></ul><p id="e816" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated"><em class="na"> mse </em>通常与具有<code class="fe nb nc nd ne b">sigmoid</code>激活功能的型号一起使用，而<em class="na"> ce </em>通常与<code class="fe nb nc nd ne b">relu</code>激活功能一起使用。这些配对背后的基本原理是，在这些星座中，可以保证成本函数是<a class="ae jd" href="https://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank">凸的</a>，即它恰好有<em class="na">个最小值</em>。因此，用于寻找(全局)最小值的算法将不会被可能的其他(局部)最小值分散注意力。</p><p id="2627" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">话虽如此，我不会隐瞒“<em class="na">损失函数中局部最小值的存在在实践中似乎不是一个主要问题</em>”(这是“<em class="na">某种程度上的理论谜团</em>”)，正如LeCun等人在他们关于“<a class="ae jd" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="na">基于梯度的学习应用于文档识别</em> </a>”的论文中所写的那样。IEEE的，1998年11月)。因此，也使用其他配对。</p><h2 id="4186" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">梯度下降</h2><p id="d855" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">用于训练模型的<a class="ae jd" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降算法</a>的基本思想(即，为最小化成本函数<em class="na"> C </em>的模型找到一组参数)如下:</p><ol class=""><li id="204e" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu ol od oe of bi translated">从任意选择的一组参数开始<em class="na"> W₀ </em></li><li id="fe72" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu ol od oe of bi translated">在位置<em class="na">w₀</em>(∇<em class="na">c</em>(<em class="na">w₀</em>):<br/><em class="na">w₁</em>=<em class="na">w₀</em>–α∇<em class="na">c</em>(<em class="na">w₀)向<em class="na"> C </em>的<a class="ae jd" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank">梯度</a>的反方向移动一小步(α)，计算出一组新的“改良”参数<em class="na"> W₁ </em></em></li></ol><p id="e859" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">由于位置<em class="na"> W₀ </em>处的<em class="na"> C </em>(矢量)的梯度指向<em class="na"> C </em>最陡的方向，我们必须向相反方向移动以达到最小值(因此公式中为负)。</p><p id="bab3" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">重复第2步，直到<em class="na"> C </em>向最小值收敛(或者直到我们有一组参数<em class="na"> Wi </em>为我们想要的应用提供足够的精度):</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/7932400611f215efc124ac68d2ae6804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eJBH6ZG2Zu8BkHUTyoZwg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">梯度下降算法的迭代步骤</p></figure><p id="c663" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">步长α称为<em class="na">学习率。</em>启发式选择。如果太大，算法可能会“超调”。如果太小，算法收敛非常慢。在实践中，用0.1、0.01和0.001这样的值进行实验并根据结果进行调整是一个好主意。</p><h2 id="98d8" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">Flux.jl中的实现</h2><p id="d214" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">那么，如何使用Flux.jl在Julia中实现这些概念呢？模型(<code class="fe nb nc nd ne b">model4LS</code>、<code class="fe nb nc nd ne b">model3LS</code>和<code class="fe nb nc nd ne b">model2LR</code>)已经定义，训练数据已经可以使用(在<code class="fe nb nc nd ne b">train_x</code>和<code class="fe nb nc nd ne b">train_y</code>)。</p><p id="2ae4" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">剩下的是成本函数的定义(Flux称之为<em class="na">损失函数</em>)和梯度下降算法的实现。Flux中每个GD实现的核心是<code class="fe nb nc nd ne b">Flux.train!()</code>函数。它在GD的每次迭代中被调用，并计算模型参数的(稍微)改进版本(如上所述)。必须将以下参数传递给<code class="fe nb nc nd ne b">train!()</code>:</p><ul class=""><li id="5c8e" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu oc od oe of bi translated"><em class="na">成本函数</em> <code class="fe nb nc nd ne b">loss</code></li><li id="a3a9" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><em class="na">模型参数</em> <code class="fe nb nc nd ne b">params</code> <em class="na"> </em> ( <em class="na"> Wi </em>来自最后一次迭代步骤或第一次运行时的初始参数)</li><li id="680f" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><em class="na">训练数据</em> <code class="fe nb nc nd ne b">data</code>(在<code class="fe nb nc nd ne b">train_x</code>和<code class="fe nb nc nd ne b">train_y</code>中)</li><li id="eb0e" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated">一个所谓的<em class="na">优化器</em><code class="fe nb nc nd ne b">opt</code>；这是用于产生模型参数的新(改进)版本的公式，如上所示(梯度下降算法的<em class="na">迭代步骤)。上面的公式是最基本的变体。近年来，已经开发出了更复杂的变体。与基本变体的主要区别在于，它们在每个迭代步骤上动态地调整学习速率α。<br/>除了我们在<code class="fe nb nc nd ne b">Descent(α)</code>中得到的基本变体，我们将应用一个最先进的版本:<a class="ae jd" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" rel="noopener ugc nofollow" target="_blank"> ADAM </a> (=自适应矩估计)，它在<code class="fe nb nc nd ne b">ADAM(α)</code>中可用。</em></li></ul><p id="7986" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">我们可以使用<code class="fe nb nc nd ne b">Flux.params()</code>功能访问每个模型的参数:</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="f595" class="ln kw jg ne b gy nr ns l nt nu">params4LS = Flux.params(model4LS)<br/>params3LS = Flux.params(model3LS)<br/>params2LR = Flux.params(model2LR)</span></pre><p id="25bd" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">成本函数<em class="na"> mse </em>和<em class="na"> ce </em>也在Flux中预定义(如<code class="fe nb nc nd ne b">Flux.Losses.mse()</code>和<code class="fe nb nc nd ne b">Flux.Losses.crossentropy()</code>)。由于<code class="fe nb nc nd ne b">train!()</code>期望一个可以直接传递训练数据(<code class="fe nb nc nd ne b">train_x</code>和<code class="fe nb nc nd ne b">train_y</code>)的成本函数，而两个预定义的损失函数期望基于<code class="fe nb nc nd ne b">train_x</code>的模型预测作为它们的第一个参数，我们必须定义一些“转换”:</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="b898" class="ln kw jg ne b gy nr ns l nt nu">loss4LSmse(x,y) = Flux.Losses.mse(model4LS(x),y)<br/>loss4LSce(x,y)  = Flux.Losses.crossentropy(model4LS(x),y)<br/>loss3LSmse(x,y) = Flux.Losses.mse(model3LS(x),y)<br/>loss3LSce(x,y)  = Flux.Losses.crossentropy(model3LS(x),y)<br/>loss2LRce(x,y)  = Flux.Losses.crossentropy(model2LR(x),y)</span></pre><p id="6c25" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">由于<em class="na"> mse </em>成本函数不能很好地与使用<code class="fe nb nc nd ne b">relu</code>的神经网络一起工作，我们只为2LR模型定义了基于<em class="na"> ce </em>的损失函数。</p><p id="75bc" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">现在，我们可以通过调用这些函数来查看模型的初始(随机)参数损失有多大。例如，<code class="fe nb nc nd ne b">loss4LSmse(x_train, y_train)</code>在我的环境中提供0.2304102的值。但是由于初始参数是随机选择的，如果你在电脑上尝试，你会得到另一个值。因此，这个值只是用于(相对)比较，以便查看在GC的一些迭代之后它会发生多大的变化。</p><h1 id="2a80" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">实现梯度下降的变体</h1><h2 id="349f" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">批量梯度下降</h2><p id="9d03" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">然后我们准备使用Julia和Flux.jl实现GD的第一个版本:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="341c" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">我们梯度下降的第一个版本<code class="fe nb nc nd ne b">train_batch()</code>将训练数据<code class="fe nb nc nd ne b">X</code>和<code class="fe nb nc nd ne b">y</code>、损失函数<code class="fe nb nc nd ne b">loss</code>、优化器<code class="fe nb nc nd ne b">opt</code>、模型参数<code class="fe nb nc nd ne b">params</code>和我们想要训练的迭代次数<code class="fe nb nc nd ne b">epochs</code>作为参数。GD的这个实现本质上是在每次迭代中调用<code class="fe nb nc nd ne b">train!()</code>的for循环。</p><p id="b985" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">由于<code class="fe nb nc nd ne b">train!()</code>期望训练数据被包装在数组的元组中，我们在第2行准备了这个结构。就是这样！</p><p id="809d" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">例如，为了使用<em class="na"> mse </em>成本函数和标准GD公式训练我们的4LS模型，在100次迭代中学习率为0.1，我们必须写出:</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="a79e" class="ln kw jg ne b gy nr ns l nt nu">train_batch(train_x, train_y, loss4LSmse, <br/>            Descent(0.1), params4LS, 100)</span></pre><p id="3915" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">执行此语句后，我们应该看到损失值有所改善。在我的环境中，我通过调用<code class="fe nb nc nd ne b">loss4LSmse(x_train, y_train)</code>得到一个值0.12874180。所以我们可以在这100次迭代中减少一半的损失。</p><p id="5bd0" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">GD的这种变体被称为<em class="na">批次</em>梯度下降，因为在每次迭代中，损失函数的梯度在<em class="na">完整的</em>训练数据集上进行评估。即所有60.000幅图像都用于计算该值(在每次迭代中！).</p><p id="406a" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">显然，这在计算上相当昂贵！因此，开发了GD的其他变体。</p><h2 id="a4e5" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">随机梯度下降</h2><p id="5b84" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated"><em class="na">随机</em>梯度下降算法在每次迭代中仅使用训练数据的单个(随机选择的)实例(即一个单个图像)来评估损失函数的梯度。这显然成本更低。</p><p id="bfef" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">考虑到仅仅<em class="na">一个</em>实例(而不是60.000)当然不会产生损失函数的实际梯度。即合成向量不会指向该函数最陡的方向。但实际上，产生的矢量并没有那么差。</p><p id="18a2" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">使用这种方法，我们不会得到到达最小值的最直接的路径，而是到达(或接近)那个点的曲折路径。即使我们没有直接达到最小值，结果通常也足够接近，并且我们以相当低的成本达到最小值。所以这通常是首选的变体。</p><p id="3e92" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">实现如下所示:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="87be" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">小批量梯度下降</h2><p id="0209" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">GD的第三个变体是我们看到的前两个变体之间的一个很好的妥协。它既不作用于整个数据集，也不作用于每次迭代的单个实例，而是选择例如100个实例的随机样本，并在此基础上计算梯度。这个样品被称为<em class="na">小批量</em>。</p><p id="5144" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">这个小批量的大小可以自由选择。因此，为此我们需要一个额外的参数<code class="fe nb nc nd ne b">batchsize</code>:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="a379" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">我们在这里实现了一个简单的策略来提取小批量:在第4行中选择整个数据集内的一个随机位置，然后我们从该位置(第5+6行)开始提取一个大小为<code class="fe nb nc nd ne b">batchsize</code>的小批量。</p><p id="63e8" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">更高级的策略是随机选择小批量的所有实例。这可以通过使用焊剂<code class="fe nb nc nd ne b"><a class="ae jd" href="https://fluxml.ai/Flux.jl/stable/data/mlutils/#DataLoader" rel="noopener ugc nofollow" target="_blank">DataLoader</a></code>来实现。</p><p id="7a0c" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">GD的第三个变体当然是最通用的实现，我们可以用<code class="fe nb nc nd ne b">train_minibatch()</code>来表示前两个变体，如下所示(因此我们实际上只需要小批量GD的实现):</p><pre class="nf ng nh ni gt nn ne no np aw nq bi"><span id="235d" class="ln kw jg ne b gy nr ns l nt nu">train_batch(X, y, loss, opt, params, epochs) = <br/>        train_minibatch(X, y, loss, opt, params, epochs, size(X)[2])</span><span id="b0b4" class="ln kw jg ne b gy on ns l nt nu">train_stochastic(X, y, loss, opt, params, epochs) = <br/>        train_minibatch(X, y, loss, opt, params, epochs, 1)</span></pre><h1 id="879c" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">结论</h1><p id="81fc" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">我们已经看到Flux.jl如何用于预处理数据和定义模型。此外，使用构建模块，如</p><ul class=""><li id="65d7" class="nx ny jg me b mf mv mi mw ls nz lv oa ly ob mu oc od oe of bi translated">损失函数</li><li id="58eb" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated">优化者</li><li id="bd73" class="nx ny jg me b mf og mi oh ls oi lv oj ly ok mu oc od oe of bi translated"><code class="fe nb nc nd ne b">train!()</code>-功能</li></ul><p id="a20f" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">我们可以很容易地实现GD算法的不同变体。</p><p id="a75d" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在后续文章中，我将结合不同的损失函数、优化器和其他参数来评估这里提出的模型，以了解需要花费多少努力来训练它们，以及从MNIST数据集中识别手写数字可以达到什么精度。所以，敬请期待！</p></div></div>    
</body>
</html>