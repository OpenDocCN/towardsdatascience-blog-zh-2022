<html>
<head>
<title>CUDA by Numba Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CUDA by Numba示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cuda-by-numba-examples-215c0d285088#2022-09-22">https://towardsdatascience.com/cuda-by-numba-examples-215c0d285088#2022-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d183" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">阅读本系列文章，从头开始学习使用Python进行CUDA编程</h2></div><h1 id="e2ef" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">第2部分，共4部分:穿针引线</h1><h1 id="a8b8" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="7e9c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的第一期<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-1-4-e0d06651612f">中，我们讨论了如何使用GPU运行令人尴尬的并行算法。令人尴尬的并行任务是那些任务彼此完全独立的任务，例如对两个数组求和或应用任何元素级函数。</a></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/d45f89f0afe5439a88047f44d5d6aebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giYSelu6J1NR8T3dbFLPhA.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2.0。用“穿针赛博朋克”运行稳定扩散。学分:在CreativeML Open RAIL-M许可下拥有作品。</p></figure><h1 id="9342" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">在本教程中</h1><p id="f946" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">许多任务虽然不是令人尴尬的并行，但仍然可以从并行化中受益。在本期的<em class="mk"> CUDA by Numba Examples </em>中，我们将介绍一些允许线程在计算中协作的常用技术。</p><p id="b0dc" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae lt" href="https://colab.research.google.com/drive/1GkGLDexnYUnl2ilmwNxAlWAH6Eo5ZK2f?usp=sharing" rel="noopener ugc nofollow" target="_blank">点击这里在Google colab中抓取代码。</a></p><p id="791f" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">本教程后面还有两个部分:<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-7652412af1ee">第三部分</a>和<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-c583474124b0">第四部分</a>。</p><h1 id="a167" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">入门指南</h1><p id="6d00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">导入和加载库，确保你有一个GPU。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="fa2a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">线程协作</h1><h2 id="c16e" class="ms kg iq bd kh mt mu dn kl mv mw dp kp lg mx my kr lk mz na kt lo nb nc kv nd bi translated">简单并行归约算法</h2><p id="0d33" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将从一个非常简单的问题开始这一部分:对一个数组的所有元素求和。串行地，这个算法非常简单。不借助NumPy，我们可以这样实现它:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="e691" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我知道，这看起来不太像蟒蛇。但是它强调了<code class="fe ne nf ng nh b">s</code>正在跟踪数组中的所有元素。如果<code class="fe ne nf ng nh b">s</code>依赖于数组的每一个元素，我们如何将这个算法并行化？首先，我们需要重写算法以允许一些并行化。如果有我们不能并行化的部分，我们应该允许线程相互通信。</p><p id="df52" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">然而，到目前为止，我们还没有学会如何让线程相互通信…事实上，我们以前说过，不同块中的线程是不通信的。我们可以考虑只启动一个块，但是请记住，在大多数GPU中，块只能有1024个线程！</p><p id="325c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们如何克服这一点？那么，如果我们把数组分成1024个(或者适当数量的<code class="fe ne nf ng nh b">threads_per_block</code>)的块，然后分别对每个块求和，会怎么样呢？最后，我们可以把每个数据块的结果相加。图2.1显示了一个非常简单的两块分割的例子。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5ff99ef9c9d2023018aca0c72bd9c52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*cafxl-v4A1f5Af-m7MqBXw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2.1。对数组元素求和的“分治”方法。图片作者。</p></figure><p id="4dcb" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们如何在GPU上做到这一点？首先，我们需要将数组分成几个块。每个块都对应于一个块，有固定数量的线程。在每个块中，每个线程可以对多个数组元素求和(grid-stride循环)。然后，我们必须在整个块中计算每个线程的值。该位需要线程进行通信。我们将在下一个例子中讨论如何做到这一点。</p><p id="579d" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">由于我们是在块上并行化，内核的输出应该作为一个块来确定大小。为了完成缩减，我们将它复制到CPU并在那里完成工作。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="aeaf" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><strong class="kz ir"> <em class="mk">警告</em> </strong> <em class="mk">:共享数组必须</em></p><ul class=""><li id="ca71" class="nj nk iq kz b la ml ld mm lg nl lk nm lo nn ls no np nq nr bi translated"><em class="mk">要“小”。确切的大小取决于GPU的计算能力，通常在48 KB和163 KB之间。参见本表</em>  <em class="mk">中</em> <a class="ae lt" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability" rel="noopener ugc nofollow" target="_blank"> <em class="mk">项“每个线程块的最大共享内存量”。</em></a></li><li id="a301" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated"><em class="mk">在编译时有一个已知的大小(这就是为什么我们调整共享数组</em> <code class="fe ne nf ng nh b"><em class="mk">threads_per_block</em></code> <em class="mk">而不是</em> <code class="fe ne nf ng nh b"><em class="mk">blockDim.x</em></code> <em class="mk">)。诚然，我们总是可以定义一个</em> <a class="ae lt" href="https://en.wikipedia.org/wiki/Factory_(object-oriented_programming)" rel="noopener ugc nofollow" target="_blank"> <em class="mk">工厂函数</em> </a> <em class="mk">来共享任意大小的数组...但是要注意这些内核的编译时间。</em></li><li id="1137" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated"><em class="mk">有</em> <code class="fe ne nf ng nh b"><em class="mk">dtype</em></code> <em class="mk">由Numba类型指定，而不是Numpy类型(不要问我为什么！).</em></li></ul><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="0469" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我在Google Colab上运行了这个，我们得到了10倍的加速。相当不错！</p><h1 id="212f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">一种更好的并行归约算法</h1><p id="6747" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你现在可能想知道为什么我们把一切都命名为“幼稚”。这意味着有一些非幼稚的方式来做同样的功能。事实上，有很多技巧可以加速这类代码(参见CUDA  演示中的<a class="ae lt" href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mk">优化并行化缩减作为基准)。</em></a></p><p id="a002" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在我们展示更好的方法之前，让我们回忆一下内核的最后一点:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="2539" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们几乎并行化了所有的事情，但是在内核的最后，我们让一个线程负责对共享数组<code class="fe ne nf ng nh b">s_block</code>的所有<code class="fe ne nf ng nh b">threads_per_block</code>元素求和。为什么我们不把这个求和也并行化呢？</p><p id="2c14" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">听起来不错，怎么样？图2.2显示了如何在<code class="fe ne nf ng nh b">threads_per_block</code>尺寸为16的情况下实现这一点。我们从8个线程开始工作，第一个线程将对<code class="fe ne nf ng nh b">s_block[0]</code>和<code class="fe ne nf ng nh b">s_block[8]</code>中的值求和。第二个是<code class="fe ne nf ng nh b">s_block[1]</code>和<code class="fe ne nf ng nh b">s_block[9]</code>中的值，直到最后一个线程将值<code class="fe ne nf ng nh b">s_block[7]</code>和<code class="fe ne nf ng nh b">s_block[15]</code>相加。</p><p id="07f2" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在下一步中，只有前4个线程需要工作。第一个线程将对<code class="fe ne nf ng nh b">s_block[0]</code>和<code class="fe ne nf ng nh b">s_block[4]</code>求和；第二，<code class="fe ne nf ng nh b">s_block[1]</code>和<code class="fe ne nf ng nh b">s_block[5]</code>；第三，<code class="fe ne nf ng nh b">s_block[2]</code>和<code class="fe ne nf ng nh b">s_block[6]</code>；第四个也是最后一个，<code class="fe ne nf ng nh b">s_block[3]</code>和<code class="fe ne nf ng nh b">s_block[7]</code>。</p><p id="fead" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">第三步，我们现在只需要2个线程来处理<code class="fe ne nf ng nh b">s_block</code>的前4个元素。第四步也是最后一步将使用一个线程对2个元素求和。</p><p id="18b9" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">由于工作在线程之间进行了划分，因此它被并行化了。当然，这不是每个线程的平均划分，但这是一种改进。在计算上，这个算法是O(log2( <code class="fe ne nf ng nh b">threads_per_block</code>))，而第一个算法是O( <code class="fe ne nf ng nh b">threads_per_block</code>)。在我们的例子中，简单算法有1024次运算，而改进算法只有10次！</p><p id="6cc7" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">还有最后一个细节。在每一步，我们都需要确保所有线程都已经写入共享数组。所以我们要调用<code class="fe ne nf ng nh b">cuda.syncthreads()</code>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nx"><img src="../Images/12fb596c236af411acb1e3ac080275ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9aAAWeiXg_stKq_90pfxw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2.2。通过顺序寻址进行归约。鸣谢:马克·哈里斯，<a class="ae lt" href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ny">优化CUDA中的并行还原</em> </a>。</p></figure><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="f17c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在我的机器上，这比简单的方法快25%。</p><p id="a4ba" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><strong class="kz ir"> <em class="mk">警告</em> </strong> <em class="mk">:您可能想将</em> <code class="fe ne nf ng nh b"><em class="mk">syncthreads</em></code> <em class="mk">移动到</em> <code class="fe ne nf ng nh b"><em class="mk">if</em></code> <em class="mk">块内，因为在每一步之后，超出当前线程数一半的内核将不会被使用。然而，这样做将使名为</em> <code class="fe ne nf ng nh b"><em class="mk">syncthreads</em></code> <em class="mk">的CUDA线程停止并等待所有其他线程，而所有其他线程将继续运行。因此，停止的线程将永远等待永不停止的线程进行同步。要点是:如果同步线程，确保在所有线程</em>  <em class="mk">中调用</em> <code class="fe ne nf ng nh b"><strong class="kz ir"><em class="mk">cuda.syncthreads()</em></strong></code> <strong class="kz ir"> <em class="mk">。</em></strong></p><pre class="lv lw lx ly gt nz nh oa ob aw oc bi"><span id="9552" class="ms kg iq nh b gy od oe l of og">i = cuda.blockDim.x // 2<br/>while (i &gt; 0):<br/>    if (tid &lt; i):<br/>        s_block[tid] += s_block[tid + i]<br/>        cuda.syncthreads() # don't put it here<br/>    cuda.syncthreads()  # instead of here<br/>    i //= 2</span></pre><h1 id="ec5a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">数字减少</h1><p id="fa3a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因为上面的归约算法很重要，Numba提供了一个方便的<code class="fe ne nf ng nh b">cuda.reduce</code>装饰器，将二进制函数转换成归约。上述长而复杂的算法可以替换为:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="a567" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">就个人而言，我发现手写的归约通常要快得多(至少快2倍)，但是Numba递归非常容易使用。也就是说，我鼓励阅读Numba源代码中的<a class="ae lt" href="https://github.com/numba/numba/blob/main/numba/cuda/kernels/reduction.py" rel="noopener ugc nofollow" target="_blank"> reduction代码。</a></p><p id="12a5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">还需要注意的是，默认情况下，缩减拷贝到主机，这将强制同步。为了避免这种情况，您可以使用设备数组作为输出来调用缩减:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="92fe" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">2D还原示例</h1><p id="f113" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">并行归约技术很棒，但如何将其扩展到更高维度并不明显。虽然我们总是可以用一个未展开的数组(<code class="fe ne nf ng nh b">array2d.ravel()</code>)来调用Numba归约，但是理解我们如何手动归约多维数组是很重要的。</p><p id="69f0" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在这个例子中，我们将结合我们对2D核的了解和对1D约简的了解来计算2D约简。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="f3ff" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">设备功能</h1><p id="77f8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">到目前为止，我们只讨论了内核，这是启动线程的特殊GPU函数。内核通常依赖于较小的函数，这些函数在GPU中定义，并且只能访问GPU数组。这些被称为<em class="mk">设备功能</em>。与内核不同，它们可以返回值。</p><p id="01c0" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在本部分教程的最后，我们将展示一个跨不同内核使用设备函数的例子。该示例还将强调在使用共享数组时同步线程的重要性。</p><p id="3f9e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><strong class="kz ir"><em class="mk">INFO</em></strong><em class="mk">:CUDA新版本中，内核可以启动其他内核。这叫做动态并行，Numba CUDA还不支持。</em></p><h1 id="1651" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">2D共享阵列示例</h1><p id="80df" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本例中，我们将在一个固定大小的数组中创建一个波纹图案。我们首先需要声明我们将使用的线程数量，因为这是共享数组所要求的。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e340930a8395be410e7feea7f4d4ea30.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*CsFgSmIw3Eo45p0qxbag-w.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2.3。左:来自同步(正确)内核的结果。右图:来自不同步(不正确)内核的结果。学分:自己的工作。受Sanders和Kandrot的CUDA示例中图5.5和5.6的启发。</p></figure><h1 id="31cc" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="8fac" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本教程中，你学习了如何开发需要一个<em class="mk">归约</em>模式来处理1D和2D数组的内核。在此过程中，我们学习了如何利用共享阵列和设备功能。</p></div></div>    
</body>
</html>