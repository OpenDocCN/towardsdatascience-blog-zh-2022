<html>
<head>
<title>Introduction to Word2Vec (Skip-gram)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec 简介(跳过程序)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-word2vec-skip-gram-cb3e9533bcf1#2022-12-16">https://towardsdatascience.com/introduction-to-word2vec-skip-gram-cb3e9533bcf1#2022-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="94cd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">NLP 基础</h2><div class=""/><div class=""><h2 id="3827" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">Python 中的单词嵌入简介</h2></div><p id="fd3b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当处理文本数据时，我们需要将文本转换成数字。用数字数据表示文本有不同的方式。单词包(又名 BOW)是一种用数字表示文本的流行而简单的方法。然而，单词包中没有单词相似性的概念，因为每个单词都是独立表示的。因此，像<em class="ln">、【太棒了】、<em class="ln">、【太棒了】、</em>这样的单词的嵌入彼此之间的相似性，就像它们与单词<em class="ln">、【书】、</em>的嵌入一样。</em></p><p id="4a76" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">单词嵌入是用数字表示文本的另一种好方法。使用这种方法，每个单词都由一个嵌入的密集向量(即一个数字数组)来表示。该方法保留了单词之间的关系，并且能够捕获单词相似性。出现在相似上下文中的单词在向量空间中具有更近的向量。因此，单词<em class="ln">‘伟大’</em>很可能比<em class="ln">‘书’</em>有更多与<em class="ln">‘牛逼’</em>相似的嵌入。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/ab85e54ec85a1f15ad802b50fd45f253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pf7L2gtrw9Kz6znS"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">由<a class="ae me" href="https://unsplash.com/@sebastiansvenson?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Sebastian Svenson </a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9dff" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在本文中，我们将对单词嵌入进行概述，尤其是一种称为 Word2Vec 的嵌入算法，并深入了解该算法如何在 Python 中的一个玩具示例上运行。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="38d0" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">📜Word2Vec (Skipgram)概述</h1><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b9b0d1afdce32b552be5a35c0395029b.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*5gFnzhhOBLvKzjPg_Qn1lg.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">图片作者|预处理一个示例文档的对比:“Hello world！”有两种方法。假设单词包方法的词汇量为 5，单词嵌入的嵌入量为 3。</p></figure><p id="0b4e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当使用单词包方法时，我们通过<em class="ln"> n </em>将文本转换成一个<em class="ln"> m </em>的文档术语矩阵，其中<em class="ln"> m </em>是文档/文本记录的数量，而<em class="ln"> n </em>是所有文档中唯一单词的数量。这通常会产生一个很大的稀疏矩阵。如果你想详细了解这种方法，请查看本教程。</p><p id="b5dd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在单词嵌入中，每个单词由一个向量表示，通常大小为<a class="ae me" href="https://datascience.stackexchange.com/a/51549" rel="noopener ugc nofollow" target="_blank"> 100 到 300 </a>。Word2Vec 是一种创建嵌入的流行方法。Word2Vec 背后的基本直觉是这样的:<em class="ln">我们可以通过观察一个单词的上下文/邻居</em>来获得关于它的有用信息。在 Word2Vec 中，我们可以使用两种架构或学习算法来获得单词的矢量表示(只是另一个用于嵌入的单词):C <em class="ln">连续单词包</em>(又名<em class="ln"> CBOW) </em>和 S <em class="ln"> kip-gram </em>。<br/> ◼️ <strong class="kt jd"> CBOW: </strong>预测<em class="ln">焦点词</em>给定周边<em class="ln">上下文词</em>t17】◼️<strong class="kt jd">跳跃式:</strong>预测<em class="ln">上下文词</em>给定<em class="ln">焦点词</em>(本文的重点)</p><p id="ce6f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在现阶段，这可能没有太大意义。我们将很快看到一个例子，这将变得更加清楚。</p><p id="6b39" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">当使用 Skip-gram 算法训练嵌入时，我们在高层次上经历以下三个步骤:<br/> ◼️ <strong class="kt jd">获取文本:</strong>我们从未标记的文本语料库开始——所以这是一个无监督的学习问题。<br/> ◼️ <strong class="kt jd">变换数据:</strong>然后，我们对数据进行预处理，并将预处理后的数据重新排列成作为特征的焦点词和作为虚拟监督学习问题的目标的上下文词。所以，它变成了一个多分类问题，其中 P(上下文词|焦点词)。下面是一个在单个文档中可能出现的情况的示例:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nf"><img src="../Images/6b69d43bf9a7ef6c100ac115fa2516c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPtdcdOntP6Orn9yyuMLIw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片|我们首先将文本预处理成标记。然后，对于作为焦点单词的每个标记，我们找到窗口大小为 2 的上下文单词。这意味着我们将焦点单词前后的两个标记视为上下文单词。我们可以看到，在这样一个小的示例文本中，并不是所有的标记前后都有 2 个标记。在这些情况下，我们使用可用的令牌。在这个例子中，我们不严格地、可互换地使用术语单词和令牌。</p></figure><p id="5e8b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">同一个特性有多个目标可能会让人难以想象。以下是考虑如何准备数据的另一种方式:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/9927c885e5bf7a224ca3140b8f400fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*fwKZrw1rEqhfC1PNKE5tgA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="4d91" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本质上，我们准备特征和目标对。<br/> ◼️ <strong class="kt jd">建立一个简单的神经网络:</strong>然后，我们使用新构建的数据集为监督学习问题训练一个具有单个隐藏层的简单神经网络。我们训练神经网络的主要原因是从隐藏层获得训练的权重，该隐藏层成为单词嵌入。出现在相似上下文中的单词的嵌入倾向于彼此相似。</p><p id="2534" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">概述完之后，是时候用 Python 实现它来巩固我们所学的内容了。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="54cb" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">🔨Python 中的 Word2Vec</h1><p id="9090" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">由于这篇文章的重点是开发算法如何工作的更好的直觉，我们将专注于自己构建它，而不是使用预先训练的 Word2Vec 嵌入来加深我们的理解。</p><blockquote class="nm nn no"><p id="d653" class="kr ks ln kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated"><em class="it">🔗</em>免责声明:在开发这篇文章的代码时，我大量使用了以下资源库:<br/> ◼️ <a class="ae me" href="https://github.com/Eligijus112/word-embedding-creation" rel="noopener ugc nofollow" target="_blank">单词嵌入创建</a>作者<a class="ae me" href="https://github.com/Eligijus112" rel="noopener ugc nofollow" target="_blank">埃利吉尤斯 112 </a>(他的媒体页面:<a class="ae me" href="https://eligijus-bujokas.medium.com/" rel="noopener">埃利吉尤斯布约卡斯</a>)<br/>◼️<a class="ae me" href="https://github.com/DerekChia/word2vec_numpy" rel="noopener ugc nofollow" target="_blank">word 2 vec _ numpy</a>作者<a class="ae me" href="https://github.com/DerekChia" rel="noopener ugc nofollow" target="_blank"> DerekChia </a></p><p id="396e" class="kr ks ln kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated">我要感谢这些了不起的作者让他们的有用的工作为他人所用。如果您想加深对 word2vec 的理解，他们的存储库是很好的额外学习资源。</p></blockquote><h2 id="01fc" class="ns mn it bd mo nt nu dn ms nv nw dp mw la nx ny my le nz oa na li ob oc nc iz bi translated">🔨带 Gensim 的 Word2vec</h2><p id="034c" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">经过他的允许，我们将使用来自<a class="ae me" href="https://github.com/Eligijus112" rel="noopener ugc nofollow" target="_blank"> Eligijus112 </a>知识库的<a class="ae me" href="https://github.com/Eligijus112/word-embedding-creation/blob/master/input/sample.csv" rel="noopener ugc nofollow" target="_blank">这个样本玩具数据集</a>。让我们导入库和数据集。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="86e4" class="ns mn it oe b gy oi oj l ok ol">import numpy as np<br/>import pandas as pd<br/>from nltk.tokenize.regexp import RegexpTokenizer<br/>from nltk.corpus import stopwords<br/>from gensim.models import Word2Vec, KeyedVectors<br/>from scipy.spatial.distance import cosine</span><span id="7429" class="ns mn it oe b gy om oj l ok ol">import tensorflow as tf<br/>from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import Input, Dense</span><span id="1df9" class="ns mn it oe b gy om oj l ok ol">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style='darkgrid', context='talk')</span><span id="9cfd" class="ns mn it oe b gy om oj l ok ol">text = ["The prince is the future king.",<br/>        "Daughter is the princess.",<br/>        "Son is the prince.",<br/>        "Only a man can be a king.",<br/>        "Only a woman can be a queen.",<br/>        "The princess will be a queen.",<br/>        "Queen and king rule the realm.", <br/>        "The prince is a strong man.",<br/>        "The princess is a beautiful woman.",<br/>        "The royal family is the king and queen and their children.",<br/>        "Prince is only a boy now.",<br/>        "A boy will be a man."]</span></pre><p id="cb18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在将非常轻松地对文本进行预处理。让我们创建一个函数，它将文本小写，将文档标记为字母标记，并删除停用词。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi on"><img src="../Images/dffe65538a643764c617ca33f0ae80e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*Eo5UB7XJShzLtqGznM7CEw.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="c970" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">预处理的级别可以随实现的不同而不同。在一些实现中，可以选择做很少的预处理，保持文本几乎原样。另一方面，你也可以选择做一个比这个例子更彻底的预处理。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="beff" class="ns mn it oe b gy oi oj l ok ol">def preprocess_text(document):<br/>    tokeniser = RegexpTokenizer(r"[A-Za-z]{2,}")<br/>    tokens = tokeniser.tokenize(document.lower())<br/>    key_tokens = [token for token in tokens <br/>                  if token not in stopwords.words('english')]<br/>    return key_tokens<br/>    <br/>corpus = []<br/>for document in text:<br/>    corpus.append(preprocess_text(document))<br/>corpus</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/322e49b4965518c6472276c591ec2117.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*jluefdrNUw26SqrCLB9y0Q.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="f284" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在每个文档都由令牌组成。我们将在自定义语料库上使用 Gensim 构建 Word2Vec:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="7550" class="ns mn it oe b gy oi oj l ok ol">dimension = 2<br/>window = 2</span><span id="e017" class="ns mn it oe b gy om oj l ok ol">word2vec0 = Word2Vec(corpus, min_count=1, vector_size=dimension, <br/>                     window=window, sg=1)<br/>word2vec0.wv.get_vector('king')</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/00df88705505701ca8c3e981f15f65cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*KB3Dw9bkurlp5j-08KfKyA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="7620" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们为上下文选择大小为 2 的<code class="fe oq or os oe b">window</code>。这意味着我们将在焦点标记之前和之后查看 2 个标记。<code class="fe oq or os oe b">dimension</code>也被设置为 2。这是指向量的大小。我们选择 2 是因为我们可以很容易地在二维图表中将其可视化，并且我们正在使用一个非常小的文本语料库。这两个超参数可以用不同的值来调整，以提高用例中单词嵌入的有用性。在准备 Word2Vec 时，我们通过指定<code class="fe oq or os oe b">sg=1</code>来确保使用 Skip-gram 算法。一旦嵌入准备就绪，我们就可以看到令牌的嵌入<code class="fe oq or os oe b">'king'</code>。</p><p id="b921" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们看看嵌入有多直观。我们会挑选一个样本词:<code class="fe oq or os oe b">'king'</code>，看看向量空间中与它最相似的词是否有意义。让我们找出与<code class="fe oq or os oe b">'king'</code>最相似的 3 个词:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="06fe" class="ns mn it oe b gy oi oj l ok ol">n=3<br/>word2vec0.wv.most_similar(positive=['king'], topn=n)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ad0f4b5d1b2217cbed27534d5074ce96.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*do_ilbsXAGij1vj182ny7A.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="b752" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这个元组列表显示了与<code class="fe oq or os oe b">'king'</code>最相似的单词及其余弦相似度。鉴于我们使用的数据非常少，这个结果还不错。</p><p id="b572" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们为词汇表准备一个嵌入的数据框架，这是唯一标记的集合:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="3e9b" class="ns mn it oe b gy oi oj l ok ol">embedding0 = pd.DataFrame(columns=['d0', 'd1'])<br/>for token in word2vec0.wv.index_to_key:<br/>    embedding0.loc[token] = word2vec0.wv.get_vector(token)<br/>embedding0</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/6f9a4607eb0369274e73311e8128bfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*WpAlp0Ek7JvdmqFP-C55Pg.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="2a10" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们将可视化二维向量空间中的记号:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="9eff" class="ns mn it oe b gy oi oj l ok ol">sns.lmplot(data=embedding0, x='d0', y='d1', fit_reg=False, aspect=2)<br/>for token, vector in embedding0.iterrows():<br/>    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), <br/>                   size=14)<br/>plt.tight_layout()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/8eef33fffa4307ec433029bf6aee089e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*DWNZz_-ebIlgd0njRXf3IQ.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><blockquote class="nm nn no"><p id="3159" class="kr ks ln kt b ku kv kd kw kx ky kg kz np lb lc ld nq lf lg lh nr lj lk ll lm im bi translated">🔗如果你想了解更多关于 Gensim 中 Word2Vec 的知识，这里有一个由 Gensim 的创建者 Radim Rehurek 编写的教程。</p></blockquote><p id="b8ca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">好吧，这是个不错的热身。在下一节中，我们将创建一个嵌入我们自己的 Word2Vec。</p><h2 id="5159" class="ns mn it bd mo nt nu dn ms nv nw dp mw la nx ny my le nz oa na li ob oc nc iz bi translated">🔨手动 Word2Vec —方法 1</h2><p id="eab8" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">我们将从语料库中查找词汇开始。我们将为词汇表中的每个标记赋值:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="1c09" class="ns mn it oe b gy oi oj l ok ol">vocabulary = sorted([*set([token for document in corpus for token in <br/>                           document])])<br/>n_vocabulary = len(vocabulary)<br/>token_index ={token: i for i, token in enumerate(vocabulary)}<br/>token_index</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/41fdcf5c1bcc9c38b5fda0f7d1dba37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*9LsMIYcM0T3euW5GBK2xJw.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="a7e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们将制作标记对作为神经网络的准备。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/705e09a54a35b1862a44ae27be02776d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*_2xvtbSwFvQ1rnSUbkhi7Q.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="9e07" class="ns mn it oe b gy oi oj l ok ol">token_pairs = []</span><span id="40ac" class="ns mn it oe b gy om oj l ok ol">for document in corpus:<br/>    for i, token in enumerate(document):<br/>        for j in range(i-window, i+window+1):<br/>            if (j&gt;=0) and (j!=i) and (j&lt;len(document)):<br/>                token_pairs.append([token] + [document[j]])</span><span id="050f" class="ns mn it oe b gy om oj l ok ol">n_token_pairs = len(token_pairs)<br/>print(f"{n_token_pairs} token pairs")</span><span id="c8fb" class="ns mn it oe b gy om oj l ok ol">token_pairs[:5]</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/df01e093eb09191f366c4db474f31366.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*1lbD-_H8po2PzN05mbnKzA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="6789" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">令牌对已经准备好了，但是它们仍然是文本形式。现在我们需要对它们进行一次热编码，以便它们适用于神经网络。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1e69bcc87ca81921ba3c8ab688815d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*2nV75ONd930DVsqq1I5Umw.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="038a" class="ns mn it oe b gy oi oj l ok ol">X = np.zeros((n_token_pairs, n_vocabulary))<br/>Y = np.zeros((n_token_pairs, n_vocabulary))</span><span id="3698" class="ns mn it oe b gy om oj l ok ol">for i, (focus_token, context_token) in enumerate(token_pairs):    <br/>    X[i, token_index[focus_token]] = 1<br/>    Y[i, token_index[context_token]] = 1<br/>print(X[:5])</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/2fa359cfe8218707e230e47f2ece302f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*GU7o8OI9_nM7d12qPGED8A.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="1cf8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在输入数据已经准备好了，我们可以建立一个只有一个隐藏层的神经网络:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="603a" class="ns mn it oe b gy oi oj l ok ol">tf.random.set_seed(42)<br/>word2vec1 = Sequential([<br/>    Dense(units=dimension, input_shape=(n_vocabulary,), <br/>          use_bias=False, name='embedding'),<br/>    Dense(units=n_vocabulary, activation='softmax', name='output')<br/>])<br/>word2vec1.compile(loss='categorical_crossentropy', optimizer='adam')<br/>word2vec1.fit(x=X, y=Y, epochs=100)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi pb"><img src="../Images/2f62cf3624244846d8c2eea513f4b72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*jpgp-ySaRGTA3hO2JO9oSQ.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="dcd5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们指定隐藏层没有偏见条款。因为我们希望隐藏层有线性激活，我们不需要指定。图层中的单元数反映了矢量的大小:<code class="fe oq or os oe b">dimension</code>。</p><p id="7ff1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们从隐藏层中提取权重，我们的嵌入。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="e59f" class="ns mn it oe b gy oi oj l ok ol">embedding1 = pd.DataFrame(columns=['d0', 'd1'])<br/>for token in token_index.keys():<br/>    ind = token_index[token]<br/>    embedding1.loc[token] = word2vec1.get_weights()[0][ind]<br/>embedding1</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/707dc7cf605c8f1f4e55d60aa0f29657.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*CgJX1-YsDMJ5jjyW9Aej9Q.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="6fc1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用我们的新嵌入，让我们看看与<code class="fe oq or os oe b">'king'</code>最相似的 3 个单词:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="708c" class="ns mn it oe b gy oi oj l ok ol">vector1 = embedding1.loc['king']<br/>similarities = {}</span><span id="5941" class="ns mn it oe b gy om oj l ok ol">for token, vector in embedding1.iterrows():<br/>    theta_sum = np.dot(vector1, vector)<br/>    theta_den = np.linalg.norm(vector1) * np.linalg.norm(vector)<br/>    similarities[token] = theta_sum / theta_den</span><span id="9557" class="ns mn it oe b gy om oj l ok ol">similar_tokens = sorted(similarities.items(), key=lambda x: x[1], <br/>                        reverse=True)<br/>similar_tokens[1:n+1]</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/7484b9db5bdec0effe02c8d710d92928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*Vx-Nx2N6bOkj8iVN-Lj7bA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="05d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">太好了，这说得通。我们可以保存嵌入并使用 Gensim 加载它们。一旦加载到 Gensim，我们可以检查我们的相似性计算。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="7f96" class="ns mn it oe b gy oi oj l ok ol">with open('embedding1.txt' ,'w') as text_file:<br/>    text_file.write(f'{n_vocabulary} {dimension}\n')<br/>    for token, vector in embedding1.iterrows():<br/>        text_file.write(f"{token} {' '.join(map(str, vector))}\n")<br/>text_file.close()</span><span id="0b95" class="ns mn it oe b gy om oj l ok ol">embedding1_loaded = KeyedVectors.load_word2vec_format('embedding1.txt', binary=False)<br/>embedding1_loaded.most_similar(positive=['king'], topn=n)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/93c40a1ee6d62031bf9d43aa27d0b1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*83VWn7zT23G0olBf_OZH5A.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="aaa6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Gensim 计算的相似性与我们的手动计算相匹配。</p><p id="31d1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在将可视化向量空间中的嵌入:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="48fe" class="ns mn it oe b gy oi oj l ok ol">sns.lmplot(data=embedding1, x='d0', y='d1', fit_reg=False, aspect=2)<br/>for token, vector in embedding1.iterrows():<br/>    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), <br/>                   size=14)<br/>plt.tight_layout()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/babc17d1f2bbabd1c4cd6c8df9abb4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*6-Hm5cZcOtYA6S2cuNy2wA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="3010" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在下一节中，我们将利用面向对象的编程方法手动创建单词嵌入。</p><h2 id="4e2c" class="ns mn it bd mo nt nu dn ms nv nw dp mw la nx ny my le nz oa na li ob oc nc iz bi translated">🔨手动 Word2Vec —方法 2</h2><p id="e6ab" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">我们将从创建一个名为<code class="fe oq or os oe b">Data</code>的类开始，它集中了与数据相关的任务:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pg ph l"/></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/892e1e556bb655872721b6e7a6b846a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*FTRqO3Pvuti3BP7l6ZWw6w.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="f74e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到,<code class="fe oq or os oe b">corpus</code>属性看起来和前面几节中的一样。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="bf27" class="ns mn it oe b gy oi oj l ok ol">len([token for document in data.corpus for token in document])</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6f2cc037010847ce9a239b080ccd7916.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/format:webp/1*CYBFN0o2FV-TH82PBKdBNQ.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="c482" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的玩具语料库中有 32 个代币。</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="6046" class="ns mn it oe b gy oi oj l ok ol">len(data.focus_context_data)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6f2cc037010847ce9a239b080ccd7916.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/format:webp/1*CYBFN0o2FV-TH82PBKdBNQ.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="a5ad" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与之前不同的是，<code class="fe oq or os oe b">data.focus_context_data</code>没有被格式化为标记对。相反，这 32 个标记中的每一个都与它们所有的上下文标记映射在一起。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/e7055efd4635080b0d7d06f71b8778de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*MRK6Cw4PlnrH3zhTAwl5dA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="7fa2" class="ns mn it oe b gy oi oj l ok ol">np.sum([len(context_tokens) for _, context_tokens in <br/>        data.focus_context_data])</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/37d89efdf918feac8aa78154792d57d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:58/format:webp/1*9keY7DAgZmmGG3eeAXhGow.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="a2af" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">像以前一样，我们总共还有 56 个上下文标记。现在，让我们将关于 Word2Vec 的代码集中在一个对象中:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="pg ph l"/></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/c57b8f0004dd6d1f2edb607a31bb7754.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*jU6ldpkQqI9du18GVAcpww.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片|仅部分输出</p></figure><p id="9361" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们刚刚训练了我们的自定义 Word2Vec 对象。让我们检查一个样本向量:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="fe29" class="ns mn it oe b gy oi oj l ok ol">word2vec2.extract_vector('king')</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/9fba72032faf6db82fb6c2a6f4225018.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*3YR0Nm0_0A6Os5Y5_QBEwA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="cfdc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在来看看与<code class="fe oq or os oe b">'king'</code>最相似的三个词:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="71f9" class="ns mn it oe b gy oi oj l ok ol">word2vec2.find_similar_words("king")</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi po"><img src="../Images/bc430eecd89770f92b63160379f7af81.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*e3NZnPqY6jcAohJVMBaklg.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="6107" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这很好。是时候将嵌入转换成数据帧了:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="ae76" class="ns mn it oe b gy oi oj l ok ol">embedding2 = pd.DataFrame(word2vec2.w1, columns=['d0', 'd1'])<br/>embedding2.index = embedding2.index.map(word2vec2.data.index_token)<br/>embedding2</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/f6e21e921805c601f10c620874d846fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*zE6JGtEkny4cjDk7Lk9QEA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="4f21" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在可以很容易地看到新的嵌入:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="12f1" class="ns mn it oe b gy oi oj l ok ol">sns.lmplot(data=embedding2, x='d0', y='d1', fit_reg=False, aspect=2)<br/>for token, vector in embedding2.iterrows():<br/>    plt.gca().text(vector['d0']+.02, vector['d1']+.03, str(token), <br/>                   size=14)<br/>plt.tight_layout()</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/c0424482830a752d486e287da9952831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*KSVGLGsn1VrwTnT30Du0bg.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="6bde" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们之前所做的那样，我们可以再次保存嵌入并使用 Gensim 加载它，然后进行检查:</p><pre class="lp lq lr ls gt od oe of og aw oh bi"><span id="40ab" class="ns mn it oe b gy oi oj l ok ol">with open('embedding2.txt' ,'w') as text_file:<br/>    text_file.write(f'{n_vocabulary} {dimension}\n')<br/>    for token, vector in embedding2.iterrows():<br/>        text_file.write(f"{token} {' '.join(map(str, vector))}\n")<br/>text_file.close()</span><span id="4243" class="ns mn it oe b gy om oj l ok ol">embedding2_loaded = KeyedVectors.load_word2vec_format('embedding2.txt', binary=False)<br/>embedding2_loaded.most_similar(positive=['king'], topn=n)</span></pre><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/c2b3647a16f7bec3a40dbc655731aa70.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*kdWpuf3Z4AfI7rmY8gc4Rw.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">作者图片</p></figure><p id="73ec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在计算余弦相似度寻找相似词时，我们这次用了<code class="fe oq or os oe b">scipy</code>。除了浮点精度误差，这种方法与 Gensim 的结果相匹配。</p><p id="4c72" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这就是这篇文章的全部内容！希望您已经对什么是单词嵌入以及 Word2Vec 如何使用 Skip-gram 算法生成单词嵌入有了基本的了解。到目前为止，我们关注的是用于 NLP 的 Word2Vec，但是这种技术对于推荐系统也是有帮助的。<a class="ae me" href="http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/" rel="noopener ugc nofollow" target="_blank">这里</a>有一篇关于这方面的深刻文章。如果你想了解更多关于 Word2Vec 的知识，这里有一些有用的资源:<br/> ◼️ <a class="ae me" href="https://www.youtube.com/watch?v=ERibwqs9p38" rel="noopener ugc nofollow" target="_blank">第二讲|单词向量表示法:word 2 vec—YouTube</a><br/>◼️<a class="ae me" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">Google code archive—Google code project hosting 的长期存储</a> <br/> ◼️ <a class="ae me" href="https://arxiv.org/pdf/1411.2738.pdf" rel="noopener ugc nofollow" target="_blank"> word2vec 参数学习讲解</a></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ps"><img src="../Images/cb9c3f3f0a980ea5c0a492ae544dbbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9eFT2t8FyzwI5Se3"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">由<a class="ae me" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Milad Fakurian </a>在<a class="ae me" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="e12f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ln">您想要访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae me" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="ln">我的推荐链接</em></a><em class="ln">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="6e0e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">谢谢你看我的帖子。如果你感兴趣，这里有我的一些帖子的链接:</p><p id="92d6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">◼️️ <a class="ae me" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道、ColumnTransformer 和 FeatureUnion 解释</a>T2】◼️️<a class="ae me" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6">feature union、ColumnTransformer &amp;管道用于预处理文本数据</a>t5】◼️<a class="ae me" rel="noopener" target="_blank" href="/enrich-your-jupyter-notebook-with-these-tips-55c8ead25255">用这些提示丰富您的 Jupyter 笔记本</a> <br/> ◼️ <a class="ae me" rel="noopener" target="_blank" href="/organise-your-jupyter-notebook-with-these-tips-d164d5dcd51f">用这些提示组织您的 Jupyter 笔记本</a> <br/> ◼️ <a class="ae me" rel="noopener" target="_blank" href="/explaining-scikit-learn-models-with-shap-61daff21b12a">解释 Scikit-用 SHAP 学习模型</a> <br/> ◼️️ <a class="ae me" rel="noopener" target="_blank" href="/feature-selection-in-scikit-learn-dc005dcf38b7">在 scikit 中选择特性</a> <br/> ◼️️ <a class="ae me" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较</a></p><p id="4883" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>