<html>
<head>
<title>COLING 2022 Highlights</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">科林2022亮点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/coling-2022-highlights-921fd53b712c#2022-10-25">https://towardsdatascience.com/coling-2022-highlights-921fd53b712c#2022-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="acba" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">更强大的评估指标，不理解任何东西的语言模型，以及对语法错误纠正的更好评估</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8df8774255f3df5cbe1cb90f04ba235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rQMQXUMQItGAQbzvNW_Rw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由来自Pixabay的<a class="ae ky" href="https://pixabay.com/users/hyungname-9864966/" rel="noopener ugc nofollow" target="_blank"> hyungname </a>拍摄。</p></figure><p id="83c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://coling2022.org/" rel="noopener ugc nofollow" target="_blank"> COLING 2022 </a>于10月中旬在韩国庆州举行。</p><p id="4303" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这次自然语言处理(NLP)会议收到了来自世界各地的2253份提交材料，其中只有632份(28.1%)被1935名评审人员和44名项目委员会的高级区域主席接受出版。</p><p id="205b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这些亮点，我选择了6篇文章，保留了我的注意力。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ff19" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated"><a class="ae ky" href="https://aclanthology.org/2022.coling-1.300.pdf" rel="noopener ugc nofollow" target="_blank">层或表示空间:是什么让基于BERT的评估度量变得健壮？</a></h2><p id="c4e5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">作者Doan Nam Long Vu(达姆施塔特技术大学)、Nafise Sadat Moosavi(谢菲尔德大学)和Steffen Eger(比勒费尔德大学)</em></p><p id="33de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近自然语言生成的度量依赖于预先训练的语言模型，例如BERTScore、BLEURT和COMET。这些度量标准与标准基准上的人工评估高度相关。然而，对于那些在他们的训练数据中没有被很好地代表的风格和领域，这些度量的表现如何还不清楚。</p><p id="c3f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，这些指标是否稳健？</p><p id="93a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者发现，BERTScore对字符级扰动并不稳健。例如，从句子中插入/删除一些字符将显著降低与人类评价的相关性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/3519d08f8b7f57ebbbd0da6e5e768c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tO8cWu3WfaRC1jzQ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2来自Doan Nam Long Vu(达姆施塔特技术大学)、Nafise Sadat Moosavi(谢菲尔德大学)和Steffen Eger(比勒菲尔德大学)的论文。</p></figure><p id="9f7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者表明，使用具有字符嵌入的模型，如ByT5，而不是标准的BERT模型，可以使BERTScore更健壮，特别是如果我们使用来自第一层的嵌入。</p><p id="e5f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我看来，这是一项杰出的工作，可能应用于广泛的自然语言生成任务。</p><p id="14a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从这篇论文中得出的结论是，基于单词嵌入的度量标准(如原始的BERTScore)可能不擅长评估涉及用户生成文本的任务，即可能包含大量语法错误的文本，如来自在线讨论平台的文本。在我看来，用ByT5对BERTScore的这种改编可以提高对用户生成文本的评估。</p><p id="91f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="na">注:这篇论文获得了大会优秀论文奖。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="e2bd" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated"><a class="ae ky" href="https://aclanthology.org/2022.coling-1.246.pdf" rel="noopener ugc nofollow" target="_blank">语法纠错:我们到了吗？</a></h2><p id="08eb" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">Muhammad Reza qo rib(新加坡国立大学)和Hwee Tou Ng(新加坡国立大学)</em></p><p id="89a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文首先表明，最近的语法错误纠正方法(GEC)在标准基准上似乎优于人类。</p><p id="5b97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，作者发现评估的GEC系统实际上未能纠正标准GEC基准的大量句子，这在人类中没有观察到。</p><p id="b5f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GEC系统在纠正不自然的短语、长句和复杂的句子结构方面似乎更容易失败。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/2c65091e1241510288c44df2ba8078d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8fm5D8AAjqNA2xix"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Muhammad Reza Qorib(新加坡国立大学)和Hwee Tou Ng(新加坡国立大学)论文中的表4和表5</p></figure><p id="ebaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者得出结论，GEC系统离人类的表现还很远，但目前的基准对GEC系统来说有点太容易了。他们建议建立新的基准，关注GEC体系仍然难以纠正的语法错误。</p><p id="93ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我特别喜欢这项工作，因为它指出了GEC系统的一些实际限制。虽然最近的工作赞扬了GEC系统的超人性能，但这篇论文有助于<strong class="lb iu">降低期望值，并激励未来的研究工作</strong>进一步改进GEC系统，以便它们最终能够实现与人类相当的性能。</p><p id="106c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以回答一下论文的标题:N <strong class="lb iu"> o，GEC系统还没有出现。</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="b08d" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">机器阅读，快与慢:模型何时“理解”语言？</h2><p id="aec8" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">作者:Sagnik Ray Choudhury(密歇根大学、哥本哈根大学)、Anna Rogers(哥本哈根大学)和Isabelle Augenstein(哥本哈根大学)</em></p><p id="a41c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是另一项表明大型语言模型什么都不懂的工作。</p><p id="fdea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们评估了5种语言模型的两种语言学技能:比较和共指消解。</p><p id="8c0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们的结果清楚地表明，所有的模型都依赖于特定的词汇模式，而不是人类用来出色完成这些任务的信息。</p><p id="7b9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们表明，通过将模型暴露于分布外的反事实扰动。模型不知道如何处理它们，表现明显不佳。因此，有人认为这些模型只是记忆词汇模式，而不是“理解”</p><p id="9696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现这篇论文特别有趣，因为它选择了一种方法来证明计算机和人类处理文本的方式不同。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ddb2" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated"><a class="ae ky" href="https://aclanthology.org/2022.coling-1.445.pdf" rel="noopener ugc nofollow" target="_blank">关于资源丰富的机器翻译预训练和随机初始化的互补性</a></h2><p id="e10f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">由常通赞(中国石油大学)、(JD探索学院)、(JD探索学院)、(悉尼大学)、(中国石油大学)和陶大成(悉尼大学JD探索学院)</em></p><p id="3551" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">资源丰富的机器翻译是尚未明显受益于预训练语言模型(LM)的任务之一。</p><p id="a664" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这项工作，作者提出了一项研究，以更好地理解预训练语言模型在资源丰富的情况下何时以及如何对初始化机器翻译系统有用。</p><p id="2b2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们首先表明，虽然它对翻译准确性几乎没有影响，但使用预训练的LM进行初始化会导致更平坦的损失场景和更平滑的词汇概率分布。</p><p id="d9d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据这些观察，他们假设使用预训练的LM进行初始化可以为域外数据集带来更好的翻译准确性，而随机初始化将更好地翻译域内数据集。他们用实验证实了这些假设。</p><p id="7568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，他们提出了预训练LM和随机初始化的<strong class="lb iu">协调，以在同一训练中获得它们的最佳效果。</strong></p><p id="d1b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不确定这项工作最终是否会推动预训练LM在资源丰富的机器翻译中的集成。尽管如此，我认为值得一提的是，研究人员仍在积极致力于此。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="1c36" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated"><a class="ae ky" href="https://aclanthology.org/2022.coling-1.466.pdf" rel="noopener ugc nofollow" target="_blank">缓解神经机器翻译的注意力不均衡</a></h2><p id="94b3" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">作者:孙(人工智能实验室)、(南京大学、实验室)、辛(南京大学)、(南京大学)</em></p><p id="6577" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先前的工作表明，机器翻译中的所有注意力并不同等重要。</p><p id="8c73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据这一观察，这项工作提出了一种新颖的“头部面具”，以迫使模型更好地平衡注意力头部之间的训练。</p><p id="4ae7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">描述了两种非常简单的头部屏蔽方法:随机屏蔽和屏蔽重要头部。</p><p id="5d0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们观察到，使用这两种方法，不同语言对的BLEU都有(轻微的)改善。随机遮罩似乎表现更好，尽管它看起来并不显著。然而，重要头部屏蔽在训练期间更好地成功平衡头部之间的重要性。</p><p id="5e6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我特别喜欢<strong class="lb iu">方法</strong>的简单性。这种头部屏蔽<strong class="lb iu">可以在现有的机器翻译框架</strong>中轻松实现。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="dec4" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated"><a class="ae ky" href="https://aclanthology.org/2022.coling-1.555.pdf" rel="noopener ugc nofollow" target="_blank">作为无监督机器翻译的释义生成</a></h2><p id="2eb7" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated"><em class="na">由孙晓菲(浙江大学，香农。艾)、田(加州大学)、孟玉贤(香农。艾)、彭南云(美国加州大学)、(浙江大学)、李继伟(浙江大学香农。AI)，还有春帆(北京大学)</em></p><p id="7754" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可用于训练的监督释义生成的大多数训练数据集是英语的，限于几个领域，并且很小。</p><p id="c05f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了减轻这些限制，无监督的释义生成已在最近的工作中提出。这些方法只需要大量感兴趣语言的文本。</p><p id="9edc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，作者提出了一种新的方法，灵感来自无监督机器翻译(UMT)。</p><p id="71ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UMT需要训练源语言和目标语言的两个语料库。两种语言的单词嵌入首先被联合学习并用于初始化机器翻译系统。然后，使用自动编码和反向翻译损耗的组合来改进该翻译系统的模型。</p><p id="11c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于释义生成，我们没有源语言和目标语言。他们建议在领域(或主题)层面进行工作。来自UMT的源和目标语言成为源和目标域。为了获得这些域，他们在一些单语数据集上执行LDA和k-means聚类，其中每个聚类(潜在地)是不同的域。然后，他们为多个领域对训练UMT模型。最后，在由先前训练的多个UMT模型生成的2500万个句子对上训练单个MT模型。</p><p id="8b67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们通过大量的实验来评估他们的方法，以证明对以前工作的改进。这个评价很有说服力:</p><ul class=""><li id="beac" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">他们使用了3种不同的自动度量标准:iBLEU、BLEU和ROUGE</li><li id="b702" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">他们从以前的工作中复制了几个基线系统</li><li id="2a21" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">他们在4个不同的基准上进行实验</li><li id="db47" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">他们进行了人体评估</li></ul><p id="7082" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个在无监督机器翻译技术上做了很多工作的人，我期待它在某个时候被应用于释义，但找不到如何应用。在本文中，<strong class="lb iu">单语数据的主题/领域聚类似乎是其工作的主要原因</strong>。</p><p id="72b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这些改进令人信服，但我不明白为什么它能如此有效。在这篇论文中，对于集群的需求并没有很好的动机，但是它似乎是这项工作的一个关键部分。此外，他们没有详细讨论为什么他们的方法比以前的工作更好。这项工作解决了以前的无监督释义生成方法的局限性？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="c3d4" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结论</h2><p id="db2f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我在这里只选择了发表的632篇论文中的几篇。我鼓励你们仔细看看<a class="ae ky" href="https://aclanthology.org/events/coling-2022/" rel="noopener ugc nofollow" target="_blank">的全部会议记录</a>和研讨会。</p><p id="30b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对机器翻译的最新进展感兴趣，你也可以看看我的AMTA 2022亮点:</p><div class="nr ns gp gr nt nu"><a rel="noopener follow" target="_blank" href="/amta-2022-highlights-4802372b53ed"><div class="nv ab fo"><div class="nw ab nx cl cj ny"><h2 class="bd iu gy z fp nz fr fs oa fu fw is bi translated">AMTA 2022亮点</h2><div class="ob l"><h3 class="bd b gy z fp nz fr fs oa fu fw dk translated">面向用户和研究人员的机器翻译技术现状</h3></div><div class="oc l"><p class="bd b dl z fp nz fr fs oa fu fw dk translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oe l of og oh od oi ks nu"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="a0c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想支持这项工作，<a class="ae ky" href="https://medium.com/@bnjmn_marie" rel="noopener">请在Medium </a>上跟随我。</p></div></div>    
</body>
</html>