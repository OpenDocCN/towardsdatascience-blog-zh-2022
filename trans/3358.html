<html>
<head>
<title>The Word2vec Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2vec分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-word2vec-classifier-5656b04143da#2022-07-26">https://towardsdatascience.com/the-word2vec-classifier-5656b04143da#2022-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ee6d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/word-embeddings-primer" rel="noopener" target="_blank">单词嵌入入门</a></h2><div class=""/><div class=""><h2 id="77ef" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何训练单词嵌入</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b4fb1537722033cb40e35f93131c8ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGdqC-744QsaipTEHfOJyg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@mukulwadhwa" rel="noopener ugc nofollow" target="_blank">穆库尔·瓦德瓦</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="056c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文是4ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的入门:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec后面有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<strong class="lk jd">word 2 vec分类器</strong> | <br/> 5。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<a class="ae lh" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="bc12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上一篇文章<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener"> <strong class="lk jd">统计学习理论</strong> </a>回顾了逻辑回归的概念和数学，以及如何使用浅层神经网络确定回归系数。在本文中，我们将扩展这些知识，学习Word2vec用来导出单词嵌入的分类算法的概念和数学。</p><h1 id="392b" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">基于预测的嵌入</h1><p id="5471" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在本系列第二篇文章讨论的向量空间模型中，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词到向量</strong> </a>，向量是从全局语料库统计创建的。向量可以被修改、重新加权，甚至降维，但它们的来源是一种基于计数的方法。</p><p id="8ec2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">适用于基于计数的方法的相同思想也包含在我们将在这里探讨的用于创建单词嵌入的神经网络方法中。</p><p id="db1c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当使用机器学习来创建单词向量时，该算法使用关于单词的邻近关系的信息来训练其权重。使用Word2vec，上下文信息是基于教导单词向量来预测单词所在的上下文。这就是为什么神经网络学习的单词向量，如Word2vec中的那些，据说使用了基于<em class="ni">预测的</em>方法(Almeida and Xexéo，2019；Voita，2020)。</p><p id="2b3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，术语<em class="ni">嵌入</em>用于单词向量，因为从神经网络训练的角度来看，单词向量的值被“嵌入”在较低维度的空间中。</p><h1 id="60e6" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">Word2vec的模型:CBOW和Skip-gram</h1><p id="1ad1" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">Mikolov等人的第一篇介绍Word2vec的论文(Mikolov等人，2013a)描述了两种对数线性模型，可用于使用Word2vec导出单词嵌入:<em class="ni">连续单词包</em> (CBOW)和<em class="ni">连续跳格</em>，他们使用下图对其进行了对比:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/04817fc45ba57d57e791b1c5b5a42646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YTjJs-zOfwfEmXPsLoM41A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nk"> Word2vec模型架构</strong> (Mikolov等人，2013a)</p></figure><p id="9320" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在其基本形式中，CBOW和Skip-gram使用多项式逻辑回归执行分类任务，这在本系列的前一篇文章<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener"> <strong class="lk jd">统计学习理论</strong> </a>中有所描述。对于分类，不是二元响应(用0或1表示的肯定或否定结果)，而是有多个结果(或类别)，其中每个单词是一个类别。多项式逻辑回归使用softmax函数作为其激活函数，而不是为逻辑回归引入的sigmoid函数。</p><p id="3ae7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与CBOW相比，Skip-gram在每个语料库中执行更多的训练示例，并且对于小语料库和罕见单词产生更好的单词嵌入，但是Skip-gram在计算上更加密集。</p><p id="3346" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mikolov等人的第二篇论文介绍了Word2vec (Mikolov等人，2013b)，详细介绍了采用Skip-gram模型时减少计算要求的两种方法:分层Softmax和负采样。</p><p id="6ae2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ni">分层Softmax </em>使用语料库词汇上霍夫曼编码的二叉树，在进行多项逻辑回归时，将Softmax的训练计算从<em class="ni"> O </em> ( <em class="ni"> v </em>)减少到<em class="ni"> O </em> (log₂ <em class="ni"> v </em>)。</p><p id="f60f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ni">另一方面，负采样</em>将算法从多项式逻辑回归简化为更简单的逻辑回归，其中一个单词是匹配的(阳性)，而其他单词的子集(样本)不是匹配的。这种方法是噪声对比估计(NCE)的简化，它使用了前一篇文章中描述的二进制分类数学(包括sigmoid激活函数)。</p><p id="6ef1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下一节中，我们将详细研究使用负抽样的跳格模型。</p><h1 id="7edd" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">负采样跳跃图</h1><p id="774d" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">让我们重温一下本系列第二篇文章中的共现矩阵示例，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词成向量</strong> </a>。它包括出现在语料库中每个文档中的成对单词的计数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/f5ff7b443f7fa3b3947b6e6106b272fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RJkNwBm8LdWWAKXwEIThQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nk">同现矩阵示例</strong> <br/>(图片由作者提供，灵感来自Potts和MacCartney，2019)</p></figure><p id="7fd6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不包括文档级别的计数，现在让我们考虑当文档被读取时，每个单词之前的<em class="ni"> l </em>个单词和之后的<em class="ni"> l </em>个单词的窗口；也就是说，我们将在彼此的<em class="ni"> l </em>单词距离内创建一个单词的连续单词对计数。我们将被评估的单词定义为目标单词<em class="ni"> t </em>和围绕它的上下文单词<em class="ni"> c </em>。</p><p id="02d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种计数方法的结果是一个<em class="ni"> v </em> × <em class="ni"> v </em>矩阵(其中<em class="ni"> v </em>是词汇表中的字数)。然而，当我们创建词对时，我们将通过机器学习技术来处理它们，将词向量长度限制在更少的维度，通常为50-1000，最典型的是300(Jurafsky和Martin，2019；曼宁，2021)。</p><p id="f751" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种机器学习技术是Word2vec的基础。该技术最大化了成对单词之间的点积，它将点积用作单词相似度的近似值。回想一下本系列的第二篇文章，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词成向量</strong> </a>，那个余弦相似度，也就是归一化的点积，是最典型的比较单词向量的方式。</p><p id="66a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在每个训练步骤，我们将训练一个分类器，使用逻辑回归的概念，通过降维近似。对于每个目标单词，我们将最大化与其周围上下文单词的点积，同时最小化所有其他非上下文单词的点积。这种方法是Word2vec中Skip-gram算法的核心，它形成单词嵌入来预测每个目标单词周围的2个<em class="ni"> l </em>上下文单词。</p><p id="2b85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Skip-gram负采样(SGNS)中的<em class="ni">负采样</em>通过允许算法仅处理上下文窗口中不存在的单词子集来简化训练。它还允许将训练算法的复杂性从多项式逻辑回归降低到二元逻辑回归。</p><p id="a9b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为题外话，回想一下Word2vec也可以以相反的方式训练。也就是说，Word2vec可以通过让上下文单词预测最可能的目标单词来进行训练。这个版本的Word2vec被称为连续单词包(CBOW)。</p><p id="94a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在接下来的部分中，我们将开发Word2vec SGNS的数学，但是首先让我们回顾一些Word2vec SGNS的高级概念:</p><ul class=""><li id="d0e0" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated">研究人员根据他们对最适合其应用的理解，在Word2vec之外对单词进行预处理(参见本系列第二篇文章中的“实验设计考虑”部分，<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lk jd">单词到矢量</strong> </a>)。Word2vec并不是简单地将原始文本作为数据。Word2vec作者甚至提供了一种额外的预处理技术来查找文本中的短语，以便这些短语可以作为训练中的一个单元(Mikolov等人，2013b)。</li><li id="692f" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">目标单词的“窗口”由目标单词之后的<em class="ni">和</em>之前的单词组成，不像一些早期的<em class="ni">n</em>gram语言模型通过仅在目标单词之前查找来建立单词向量。</li><li id="f14d" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">Word2vec最大化由窗口大小定义的文本中彼此靠近的单词的向量的点积，同时最小化没有出现在窗口中的随机单词的点积。点积被用作余弦相似性的代理，余弦相似性在概念上也接近皮尔逊相关系数。</li><li id="1ca2" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">Word2vec使用机器学习逻辑回归技术来训练分类器(对数线性),该分类器区分正面和负面(真和假)示例。训练的回归权重被用作单词嵌入。</li><li id="82f7" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">Word2vec在训练时使用了多种重新加权技术，包括下面这些，将在本系列的下一篇文章中详细讨论，<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener"><strong class="lk jd">word 2 vec超参数</strong> </a>:</li></ul><p id="a07c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi oa translated"><span class="l ob ok bm di ol"> <img alt="P" class="lb om on oo op oq fc n ih dh bf" src="../Images/1843c22374ddd6d5198ef2d56a300e70.png" width="115" height="79" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fit:230/1*T-aWNhvwmEcelKNH1YvHgQ@2x.png"/> <span class="l ob oc od bm oe of og oh oi di oj"> P </span> </span>修剪生僻字</p><p id="caf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi oa translated"><span class="l ob ok bm di ol"> <img alt="S" class="lb om on oo op oq fc n ih dh bf" src="../Images/1843c22374ddd6d5198ef2d56a300e70.png" width="115" height="79" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fit:230/1*T-aWNhvwmEcelKNH1YvHgQ@2x.png"/> <span class="l ob oc od bm oe of og oh oi di oj">年代</span> </span>子采样常用词</p><p id="93c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi oa translated"><span class="l ob ok bm di ol"> <img alt="D" class="lb om on oo op oq fc n ih dh bf" src="../Images/1843c22374ddd6d5198ef2d56a300e70.png" width="115" height="79" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fit:230/1*T-aWNhvwmEcelKNH1YvHgQ@2x.png"/> <span class="l ob oc od bm oe of og oh oi di oj"> D </span> </span>根据与目标单词的距离，动态加权上下文窗口中的单词</p><p id="2a1a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi oa translated"><span class="l ob ok bm di ol"> <img alt="S" class="lb om on oo op oq fc n ih dh bf" src="../Images/1843c22374ddd6d5198ef2d56a300e70.png" width="115" height="79" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fit:230/1*T-aWNhvwmEcelKNH1YvHgQ@2x.png"/> <span class="l ob oc od bm oe of og oh oi di oj"> S </span> </span>选择否定样词</p><ul class=""><li id="bb6d" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated">需要优化培训的一组参数，并且需要完成充分的培训。</li></ul><p id="56a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，Word2vec算法的某些方面没有公布或没有得到很好的定义(Goldberg和Levy，2014)。研究人员不得不通过阅读代码并从Mikolov等人的论文(Mikolov等人，2013aMikolov等人，2013年b)。</p><p id="ddb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2vec具有微妙性和复杂性，它的一些成功即使在今天也没有得到很好的理解(Goldberg和Levy，2014；Jurafsky和Martin，2019年)，但这里有一个从Goldberg和Levy那里获得的有趣信息，为了与本文中使用的术语保持一致，对其进行了重新措辞:</p><blockquote class="or os ot"><p id="c89b" class="li lj ni lk b ll lm kd ln lo lp kg lq ou ls lt lu ov lw lx ly ow ma mb mc md im bi translated">SGNS没有建模<em class="it"> P </em> ( <em class="it"> c|t </em> ) <em class="it">，</em>而是一个与目标词<em class="it"> t </em>及其每个上下文<em class="it"> c </em>的联合概率相关的量。联合学习<em class="it"> t </em>和<em class="it"> c </em>的表示，使模型非凸。如果固定目标单词并仅在上下文表示上训练，或者固定上下文表示并仅学习目标表示，则该模型简化为逻辑回归，并且是凸的。<em class="it">(戈德堡和利维，2014) </em></p></blockquote><p id="40a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于统计师来说，Word2vec值得强调的几点包括:</p><ul class=""><li id="c745" class="nm nn it lk b ll lm lo lp lr no lv np lz nq md nr ns nt nu bi translated">Word2vec并不是从独立同分布(iid)的随机变量的原始数据开始的。我们处于机器学习领域，寻找最有效的方法，并应用统计学原理来帮助获得最成功的结果。</li><li id="c4db" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">Word2vec做了一个简化的假设，即语境词是独立的，这是不成立的(Jurafsky和Martin，2019)。</li><li id="06c1" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">Word2vec SGNS算法使用概率分类器，该分类器使用逻辑回归机器学习技术(如softmax或sigmoid函数进行归一化)的各个方面来分配概率，但结果并不完全是概率性的。</li><li id="e5fa" class="nm nn it lk b ll nv lo nw lr nx lv ny lz nz md nr ns nt nu bi translated">最后，尺寸受到设计的限制，使得Word2vec的结果只是一个近似值。</li></ul><p id="dd8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在高层次上，上面概述的几点是Word2vec如此重要的原因。Manning是GloVe (Pennington等人，2014年)的合著者，GloVe是一种在Word2vec一年后推出的性能稍好的单词嵌入算法，他在<em class="ni">自然语言处理与深度学习</em>中对Word2vec这样说:</p><blockquote class="or os ot"><p id="8355" class="li lj ni lk b ll lm kd ln lo lp kg lq ou ls lt lu ov lw lx ly ow ma mb mc md im bi translated">“从某种意义上说，对将NLP世界转向神经网络方向产生最大影响的是托马斯·米科洛夫在2013年提出的一种算法，称为Word2vec算法。这是一种非常简单、非常可扩展的学习单词矢量表示的方式，它真正打开了闸门。”<em class="it">(曼宁，2019) </em></p></blockquote><p id="5cff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们回顾一下SGNS的数学。</p><h1 id="5a4e" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">SGNS分类器</h1><p id="2411" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">当在他们关于Word2vec的第二篇论文中定义负采样与Skip-gram一起使用时，Mikolov等人从他们在第一篇论文中使用的softmax转向引用sigmoid函数的使用(Mikolov等人，2013bMikolov等人，2013a)。</p><p id="7c3e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在统计学中，sigmoid(或逻辑函数)在进行逻辑回归(即二元分类)时创建概率分布；然而，对于名义多项式模型，softmax函数定义了概率分布。</p><p id="2578" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">SGNS执行二元分类任务。给定两个词，来自上下文窗口内的目标词<em class="ni"> t </em>和潜在上下文词<em class="ni"> c </em>，SGNS应该返回上下文词确实是来自语料库的上下文词的概率。</p><p id="ca8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了更好地说明这种二元分类是如何工作的，让我们来看一个句子中的短语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f51721e36481151e655b46e192808610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*v99pxvmGX4MKEEfXwwCr1Q@2x.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供，短语灵感来自Chia，2018)</p></figure><p id="2210" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用窗口<em class="ni"> l </em> =2，当单词‘处理’是目标单词<em class="ni"> t </em>时，有2=4个上下文单词:<em class="ni"> c </em> ₁、<em class="ni"> c </em> ₂、<em class="ni"> c </em> ₃、<em class="ni"> c </em> ₄.</p><p id="564e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个目标词<em class="ni"> t </em>和上下文词<em class="ni"> c </em>，让我们创建一个二元分类器，预测<em class="ni"> c </em>为<em class="ni"> t </em>的真实(<em class="ni"> y </em> =1其中<em class="ni"> y </em> ∈{0，1})上下文词的概率(Jurafsky和Martin，2019；戈德堡和利维，2014):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/f340ccb19070bcc3f982ae4af45596a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*WUeYe3yCzJ9vroe53keqqg@2x.png"/></div></figure><p id="e293" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，<em class="ni"> c </em>是<em class="ni"> t </em>的假上下文词的概率，即不真实情况(<em class="ni"> y </em> =0)，是1减去真实情况:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/bbf2718a747a602a7b883b36e2b0e28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*psRQhFApH3CixDEKcRpqrg@2x.png"/></div></figure><p id="a6c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2vec算法使用向量<strong class="lk jd"> t </strong>和<strong class="lk jd"> c </strong>的点积，通过近似公式计算这些概率:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/804d75c3029e46c2a8e2742a49de9080.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*9Voe0m9hjjewLIZz_3ghBA@2x.png"/></div></figure><p id="1702" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能还记得本系列的第二篇文章，余弦相似性的等式(通常用于测量单词向量相似性)是归一化的点积:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/5bdd68cd050bb578355d447392c09685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R-cycRwUARJsARljE5rO4w@2x.png"/></div></div></figure><p id="c0fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<strong class="lk jd"> a </strong>和<strong class="lk jd"> b </strong>是两个大小为<em class="ni"> n </em>的向量。</p><p id="c6bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为<strong class="lk jd"> t c </strong> <em class="ni"> </em>的点积不产生从0到1的实际概率值，所以在SGNS模型中使用了逻辑或<em class="ni"> sigmoid </em>函数<em class="ni"> σ </em> ( <em class="ni"> z </em>):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/861535aeba7d763eb8904a16dfbcf0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*lDGo6FvuwoAfq3n_sAkOdw@2x.png"/></div></figure><p id="a694" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用sigmoid转换为概率来源于本系列上一篇文章讨论的逻辑回归，<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener"> <strong class="lk jd">统计学习理论</strong> </a>。在逻辑回归中，<em class="ni">z =</em><strong class="lk jd">X<em class="ni">β</em></strong>≡<strong class="lk jd">X<em class="ni">β</em></strong>为模型提供了线性回归中的最优直线，但在本例中<em class="ni"> z = </em> <strong class="lk jd"> t c </strong> <em class="ni">。</em>所以，根据点积的<em class="ni">相似度</em>公式，<em class="ni"> c </em>为<em class="ni"> t、</em>的真实上下文词的近似概率为(Jurafsky and Martin，2019；戈德堡和利维，2014):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c1b1dbef5a4de6d14b645cecd3271b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*8V19K1WDZ-rxVtgiCTpvZg@2x.png"/></div></figure><p id="11ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ni"> c </em>为假上下文词，即<em class="ni"> y </em> =0，对于<em class="ni"> t </em>的近似概率为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c23016f01e062e2a62e5f16ef4d704cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*dJG82_MsZjfIIsfw6Nz_3g@2x.png"/></div></figure><p id="d108" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，我们来回顾一下Word2vec的神经网络模型对SGNS使用的学习过程。</p><h1 id="2caa" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">SGNS损失函数</h1><p id="8054" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">SGNS通过抽取否定情况的样本(即，是错误上下文单词的单词)来简化处理。因此，如果我们取<em class="ni">一个</em>目标/上下文单词对(<em class="ni"> t，c </em>)并将其与<em class="ni"> k </em>负样本单词<em class="ni"> s </em> ₁ <em class="ni"> …sₖ </em>相结合，则学习目标是最大化<em class="ni"> t </em>和<em class="ni"> c </em>匹配且负样本不匹配的概率。</p><p id="2fc6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了做到这一点，SGNS做出了不正确但数学上简化的假设，即所有上下文词和负样本都是独立的，这允许它们的概率被相乘(Jurafsky和Martin，2019)。因此，我们需要找到这个似然函数的最大值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/aab72e1aca1aae8502d67a6ad71e2eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*SkyMJYFZnQCjJX2hKWi8Cw@2x.png"/></div></figure><p id="7df4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了进一步简化数学，我们可以像将最大似然法应用于逻辑回归时一样使用对数。自然对数不会影响结果，因为函数是单调递增的。如前一节所示，Word2vec使用一个近似公式计算<em class="ni">相似度</em>，使用单词向量的点积。因此，学习目标<em class="ni"> L </em>是(Jurafsky和Martin，2019):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/4a0c1d4112cbdddb62e1f0a3f9c513f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*3OxH7su-at__lZ0RuXOa_g@2x.png"/></div></figure><p id="9c5c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，θ被认为是目标和上下文词向量的串联，即它们的权重矩阵(Kim，2019)。</p><p id="1ad8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在机器学习中，<em class="ni">损失函数</em>(也称为目标函数或成本函数)<em class="ni"> J </em> ( <em class="ni"> θ </em>)，是将<em class="ni">最小化</em>的函数，因此是上述对数似然方程的负数(Kim，2019；Voita，2020):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9b37ae35a7da392b08fe03fad00e7749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*4lSyKSAGDw7rkDjzHbZvUg@2x.png"/></div></figure><h1 id="674d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">SGNS训练和随机梯度下降</h1><p id="040f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">为了在数学上找到损失函数中的最小值，我们取其导数，然后将其设置为零。在机器学习中，人们估计每一步向最小值的方向，并向它迈出一小步(Voita，2020):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/1ecc4bfdadc4503d61dfcf3b23dda19a.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*pisrZSu2LzvvX3c4Cxo26g@2x.png"/></div></figure><p id="7d66" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这里，𝛼就是那一小步，也就是<em class="ni">学习速率</em>。朝着最小值前进被称为<em class="ni">随机梯度下降</em>。</p><p id="03f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在SGNS中，相对于<strong class="lk jd"><em class="ni"/></strong>、<strong class="lk jd"> <em class="ni"> c </em> </strong>、以及负样本、<strong class="lk jd"> <em class="ni"> s </em> </strong> <em class="ni"> ⱼ </em>，使用上面所示的<em class="ni"> J </em> ( <em class="ni"> θ </em>)的偏导数来确定朝向最小值的方向。因此，对于每个第<em class="ni"> m </em>个训练实例，单词向量值更新如下(Jurafsky和Martin，2021):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/b7fbcad1055924e3455d555dcab2df8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZEtH9qz0SH16dhBWhN9jA@2x.png"/></div></div></figure><p id="d1ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="ni"> σ </em> ( <em class="ni"> z </em>)是上面定义的sigmoid函数。</p><p id="40cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们通过在目标单词向量矩阵<strong class="lk jd"> T </strong>和上下文单词向量矩阵<strong class="lk jd"> C </strong>(权重矩阵)中为每个单词建立随机向量来进行学习过程。有时初始值采用1和1之间的均匀分布。然而，2017年科奇米和Ondřej证明，使用以零为中心的低方差随机正态分布(例如，𝒩(𝜇=0，𝜎 =0.01)，会导致更快的优化。</p><p id="120f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，对于负采样，我们可以通过选择取多少负样本来调整计算量。然而，Skip-gram的训练时间与窗口大小<em class="ni"> l </em>成比例。</p><h1 id="0c78" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">SGNS权重矩阵</h1><p id="4ea5" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">训练完成后，每个单词有两个嵌入，<strong class="lk jd"> <em class="ni"> t </em> </strong> <em class="ni"> ᵢ </em>和<strong class="lk jd"> <em class="ni"> c </em> </strong> <em class="ni"> ᵢ </em>。通常在实践中，<strong class="lk jd"> C </strong> <em class="ni"> </em>矩阵被忽略，而<strong class="lk jd"> T </strong>将每个单词嵌入定义为向量<strong class="lk jd"> <em class="ni"> t </em> </strong> <em class="ni"> ᵢ </em>。然而，也可以将两个嵌入相加或平均，或者连接两个嵌入，以创建两倍长的向量(Jurafsky和Martin，2019；曼宁，2019)。</p><p id="34ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下图显示了训练过程中的一个更新，其中目标单词“processing”与上下文单词“language”配对，单词“aardvark”被随机选为负样本。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/f27e6190f6f26ca147daaf454141c7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BW4RpcTdpEcP7WCH3r1o8A@2x.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/00f26b65e3bfd1ec1c9c89f9961c93c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EI5e7SECibTqbspGLklbWA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nk">用一个否定样本</strong>更新目标/上下文单词对<br/>的权重矩阵T和C(图片由作者提供，灵感来自Jurafsky和Martin，2019)</p></figure><p id="e969" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">重要的是要理解，在训练期间，<em class="ni">矩阵、<strong class="lk jd">矩阵、</strong>矩阵和<strong class="lk jd">矩阵和</strong>矩阵中的单词嵌入都会更新。更具体地说，在本例中，<strong class="lk jd"> T </strong>中的“处理”被更新为<strong class="lk jd"> C </strong>中“语言”的目标词，而<em class="ni">不是</em>中“土豚”的目标词，并且<strong class="lk jd"> C </strong>中的“语言”被更新为<strong class="lk jd"> T </strong>中“处理”的上下文词，而<strong class="lk jd"> C </strong>中的“土豚”被更新为</em>同样，训练使用点积作为<em class="ni">相似性的度量。</em></p><p id="7e62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这也是进行二元分类的前馈神经网络的框架:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/32ed48f0218216e5c895ae5f6dd4f32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUSahyxKhTNon808yjSkgA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd nk">二元分类器的神经网络框架</strong> <br/>(图片由作者提供，灵感来自Raschka，2020)</p></figure><p id="9e18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这个简单的浅层神经网络中，向量<strong class="lk jd"> w </strong>中的权重代表预测系数，它们可以用于对一组新的输入<strong class="lk jd"> x </strong>进行预测。然而，在Word2vec中,“权重”是单词嵌入本身，有两组。<strong class="lk jd"> T </strong>中的行是单词嵌入，每个都被优化以预测它们在<strong class="lk jd"> C </strong>中的上下文单词，并且<strong class="lk jd"> C </strong>中的列是单词嵌入，每个都被优化为用于<strong class="lk jd"> T </strong>中的每个目标单词的那些上下文单词。</p><p id="d7a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然<strong class="lk jd"> T </strong>和<strong class="lk jd"> C </strong>都包含词汇表的所有单词，但是<strong class="lk jd"> T </strong>和<strong class="lk jd"> C </strong>针对不同的条件进行了优化，因此包含不同的信息。然而，正如Jurafsky和Martin (2019年)和Manning (2019年)指出的，通常会忽略<strong class="lk jd"> C </strong>矩阵。</p><p id="194a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个有趣的问题是，<em class="ni">当</em> <strong class="lk jd"> <em class="ni"> C </em> </strong> <em class="ni">矩阵被忽略时会丢失什么？</em></p><p id="efc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2vec SGNS不是典型的神经网络。Word2vec SGNS算法训练速度很快，并且具有使其成为研究对象的品质。例如，Levy和Goldberg (2014b)证明了SGNS分解移位的PMI矩阵，Aggarwal (2018)补充说，SGNS执行一种逻辑矩阵分解，即一种自动编码。</p><h1 id="c8e8" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">摘要</h1><p id="cd35" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在本文中，我们了解到Word2vec使用两个基本模型，一个是对上下文单词集进行平均来预测目标单词(CBOW)，另一个是目标单词预测上下文单词(Skip-gram)。这些模型使用执行多项式回归的浅层神经网络。我们还看到了如何使用负采样减少Skip-gram的计算需求，负采样将算法转换为二进制分类器。</p><p id="f237" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们从概念上和数学上研究了负采样跳过图(SGNS)是如何工作的。</p><p id="eafa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在下一篇文章中，<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener"><strong class="lk jd">Word2vec超参数</strong>，</a>我们将了解word 2 vec用来优化训练其单词嵌入的一组附加技术。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="d56f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章是4ᵗʰ系列文章<strong class="lk jd">中关于单词嵌入的初级读本:<br/> </strong> 1。<a class="ae lh" href="https://medium.com/@jongim/a-primer-on-word-embeddings-95e3326a833a" rel="noopener">word 2 vec背后有什么</a> | 2。<a class="ae lh" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae lh" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。Word2vec分类器| <br/> 5。<a class="ae lh" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<a class="ae lh" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p><p id="6f36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">关于这个主题的更多信息:</strong>与其他单词嵌入技术相比，我推荐学习Word2vec的一个资源是斯坦福大学的这个在线计算机科学课程:Manning，C. (2021)，<a class="ae lh" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> CS224N自然语言处理与深度学习</em> </a>。</p><h1 id="8c4d" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参考</h1><p id="25d5" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">阿加瓦尔，C. (2018)。<em class="ni">神经网络和深度学习:一本教科书</em>。瑞士查姆:施普林格国际出版公司。</p><p id="eab3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">阿尔梅达，f .和Xexéo，G. (2019)。词汇嵌入:一个综述。可从<a class="ae lh" href="https://arxiv.org/abs/1901.09069" rel="noopener ugc nofollow" target="_blank"> arXiv:1901.09069v1 </a>获得。</p><p id="00f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Goldberg和o . Levy(2014年)。word2vec解释:推导Mikolov等人的负采样单词嵌入法。可从<a class="ae lh" href="https://arxiv.org/abs/1402.3722v1" rel="noopener ugc nofollow" target="_blank"> arXiv:1402.3722v1 </a>获得。</p><p id="66b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Chia，D. (2018)。使用NumPy和Google Sheets的Word2Vec实现指南。<em class="ni">走向数据科学。</em></p><p id="f0cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">jurafsky d .和Martin j .(2019年)。<em class="ni">语音和语言处理:自然语言处理、计算语言学和语音识别导论</em>。徒弟堂，<a class="ae lh" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">第三版，2019稿</a>。</p><p id="d55f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">jurafsky d .和Martin j .(2021)。<em class="ni">语音和语言处理:自然语言处理、计算语言学和语音识别的介绍</em>。徒弟堂，<a class="ae lh" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">第三版，2021稿</a>。</p><p id="3654" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">金，E. (2019)。<a class="ae lh" href="https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling" rel="noopener ugc nofollow" target="_blank">揭秘Skip-Gram语言建模中的神经网络</a>。<em class="ni"> Github </em>。</p><p id="3c05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">t .科奇米和b . ondřej(2017年)。深度学习任务中单词嵌入初始化的探索。P <em class="ni">第十四届国际自然语言处理会议论文集(ICON-2017)</em>第56–64页。P <a class="ae lh" href="https://www.aclweb.org/anthology/W17-7508" rel="noopener ugc nofollow" target="_blank"> DF。</a></p><p id="6cec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Levy和y . Goldberg(2014年b)。神经单词嵌入作为隐式矩阵分解。第27届国际神经信息处理系统会议录，2:2177–2185。<a class="ae lh" href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="9225" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Manning，c .(2019)<a class="ae lh" href="https://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"><em class="ni">cs 224n自然语言处理与深度学习</em> </a>。在线计算机科学课程。加州斯坦福:斯坦福大学。</p><p id="90c7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Manning，c .(2021)<a class="ae lh" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ" rel="noopener ugc nofollow" target="_blank"><em class="ni">cs 224n自然语言处理与深度学习</em> </a>。在线计算机科学课程。加州斯坦福:斯坦福大学。</p><p id="04a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mikolov、Corrado、G . Chen、k .和j . Dean(2013年a)。向量空间中单词表示的有效估计。可从<a class="ae lh" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> arXiv:1301:3781v3 </a>获得。</p><p id="25f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Mikolov、Corrado、G . Chen、k . Sutskever和j . Dean(2013年b)。词和短语的分布式表示及其组合性。可在<a class="ae lh" href="https://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank"> arXiv:1310.4546v1 </a>获得。</p><p id="1058" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Pennington、r . Socher和c . Manning(2014年)。GloVe:单词表示的全局向量。<em class="ni">2014年自然语言处理经验方法会议论文集</em>，第1532–1543页。<a class="ae lh" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwj_r9_nsartAhUMC-wKHXu4BqYQFjABegQIAxAC&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Fglove.pdf&amp;usg=AOvVaw3XPTcwWcbYOXnahjvpeDTu" rel="noopener ugc nofollow" target="_blank"> PDF </a>。</p><p id="754f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Potts c .和MacCartney b .(2019)。<a class="ae lh" href="https://web.stanford.edu/class/cs224u/2019/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="ni"> CS224U自然语言理解</em> </a>，在线计算机科学课程。加州斯坦福:斯坦福大学。</p><p id="c25a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">沃伊塔湖(2020)。<a class="ae lh" href="https://lena-voita.github.io/nlp_course/word_embeddings.html" rel="noopener ugc nofollow" target="_blank">自然语言处理课程:单词嵌入</a>。<em class="ni"> Github </em>。</p><p id="abcf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">*除非另有说明，数字和图像均由作者提供。</p></div></div>    
</body>
</html>