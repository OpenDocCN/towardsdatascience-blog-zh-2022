<html>
<head>
<title>Stable Diffusion, DreamFusion, Make-A-Video, Imagen Video, and What’s Next</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稳定扩散，梦想融合，制作视频，图像视频，下一步是什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generative-ai-878909fb7868#2022-10-19">https://towardsdatascience.com/generative-ai-878909fb7868#2022-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2aee" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">生成式人工智能概述</h2><div class=""/><div class=""><h2 id="c7d4" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用于文本到图像、文本到3D和文本到视频的生成式人工智能</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c169d13d6e231f469788b9f20eef9b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7yqvoM0SUxLWEGWsoxfyiA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">星夜(作者利用<a class="ae le" href="https://www.mage.space" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>)</p></figure><p id="e309" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">新一代人工智能还处于萌芽状态，但正在呈指数级增长。自从OpenAI首次亮相GPT-3和DALL E以来，它一直在人工智能领域抢尽风头</p><p id="17bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2022年是文本到内容的时代(又名AIGC)。2022年4月，OpenAI发布了DALL E 2，在<a class="ae le" href="https://arxiv.org/abs/2204.06125" rel="noopener ugc nofollow" target="_blank">的论文</a>中描述了关于剪辑和扩散模型。这是第一次从自然语言的文本描述中创造出逼真的图像和艺术。</p><p id="97e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">四个月后，初创公司StabilityAI <a class="ae le" href="https://stability.ai/blog/stable-diffusion-public-release" rel="noopener ugc nofollow" target="_blank">宣布</a>发布stability Diffusion，这是一款开源的文本到图像生成器，可以在几秒钟内创作出令人惊叹的艺术作品。它可以在消费级GPU上运行，速度和质量都有所突破。它如此炙手可热，以至于在2022年10月17日的种子轮中成为了<a class="ae le" href="https://www.bloomberg.com/news/articles/2022-10-17/digital-media-firm-stability-ai-raises-funds-at-1-billion-value" rel="noopener ugc nofollow" target="_blank">独角兽。</a></p><p id="d8cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2022年9月29日，谷歌宣布<a class="ae le" href="https://arxiv.org/abs/2209.14988" rel="noopener ugc nofollow" target="_blank"> DreamFusion </a>使用2D扩散技术进行文本到3D的生成。同一天，Meta宣布<a class="ae le" href="https://arxiv.org/abs/2209.14792" rel="noopener ugc nofollow" target="_blank">制作一个视频</a>用于没有文本视频数据的文本到视频生成。</p><p id="59ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不到一周，谷歌似乎回应了Meta的视频制作，推出了文本转视频的图片视频。</p><p id="cc08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在过去半年的这段激动人心的旅程中，<a class="ae le" href="https://www.midjourney.com/app/" rel="noopener ugc nofollow" target="_blank"> Midjourney </a>和<a class="ae le" href="https://github.com/THUDM/CogVideo" rel="noopener ugc nofollow" target="_blank"> CogVideo </a>同样重要。Midjourney是一个独立的研究实验室，提供Midjourney Bot来从文本生成图像。CogVideo是第一个开源的大规模预训练文本转视频模型，拥有94亿个参数。</p><p id="f295" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，我将描述他们是如何工作的稳定扩散，文本到3D，文本到视频。此外，让我们体验一下无需编码的文本到图像的美妙体验，看看接下来会发生什么。</p><h2 id="55ec" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">稳定扩散和不稳定扩散</h2><p id="daec" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated"><a class="ae le" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>引入了条件潜在扩散模型(LDM ),以实现图像修复和类别条件图像合成的最新技术水平，以及在各种任务上极具竞争力的性能，包括文本到图像合成、无条件图像生成和超分辨率，同时与基于像素的LDM相比显著降低了计算要求。该方法可以在不降低质量的前提下，显著提高去噪扩散模型的训练和采样效率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/13e34cc2a39bd3a7ff3e9ac5a0647cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqOEYiCcIiXErCWZL3P1mg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">条件潜在扩散模型通过串联或更一般的交叉注意机制来解释(来源:<a class="ae le" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">潜在扩散模型</a></p></figure><p id="50cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在释放稳定扩散的同时，StabilityAI开发了一个基于AI的安全分类器，默认启用。它理解几代人的概念和其他因素，以消除用户不想要的输出。但是对于强大的图像生成模型，可以很容易地调整它的参数。</p><p id="7d1f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于稳定的扩散，<a class="ae le" href="https://www.mage.space" rel="noopener ugc nofollow" target="_blank">法师</a>出现在浏览器中生成<a class="ae le" href="https://en.wikipedia.org/wiki/Not_safe_for_work" rel="noopener ugc nofollow" target="_blank"> NSFW </a>内容。这是直接和免费使用，没有NSFW滤波。</p><p id="0450" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不要混淆。<strong class="lh ja">不稳定扩散</strong>是一个使用稳定扩散支持AI生成NSFW内容的社区。毫无疑问，这些模特可以在<a class="ae le" href="https://www.patreon.com" rel="noopener ugc nofollow" target="_blank">帕特伦</a>和<a class="ae le" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>上找到。</p><h2 id="cb1d" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">文本转3D的DreamFusion</h2><p id="e81b" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">谷歌和UCB共同推出了使用2D扩散的文本到3D生成的DreamFusion。</p><p id="73f7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://arxiv.org/abs/2209.14988" rel="noopener ugc nofollow" target="_blank"> DreamFusion </a>通过一种新颖的SDS(分数提取采样)方法和一种新颖的NeRF(神经辐射场)类渲染引擎，将可扩展的高质量2D图像扩散模型转移到3D域。DreamFusion不需要3D或多视图训练数据，仅使用预训练的2D扩散模型(仅在2D图像上训练)来执行3D合成。[3]</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/4e400a81e367b12a95108c5f59078b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqaW1qnLu-6PGIaI70-eJw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">DreamFusion演示了从自然语言字幕生成3D对象(来源:<a class="ae le" href="https://arxiv.org/abs/2209.14988" rel="noopener ugc nofollow" target="_blank"> DreamFusion </a>)</p></figure><p id="2d51" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上所示，场景由针对每个字幕随机初始化并从头开始训练的NeRF来表示。NeRF使用MLP参数化体积密度和反照率(颜色)。DreamFusion从随机相机渲染NeRF，使用从密度梯度计算的法线以随机照明方向对场景进行着色。阴影揭示了从单个视点看不清楚的几何细节。为了计算参数更新，DreamFusion扩散渲染并使用(冻结的)条件Imagen模型重建渲染，以预测注入的噪声。</p><p id="d8fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管DreamFusion产生了引人注目的结果，但它仍处于文本到3D的早期阶段。当应用于图像采样时，SDS不是完美的损失函数。因此，在NeRF环境中，它经常产生过度饱和和过度平滑的结果，并且缺乏多样性。此外，DreamFusion使用64 × 64 <a class="ae le" href="https://arxiv.org/abs/2205.11487" rel="noopener ugc nofollow" target="_blank"> Imagen模型</a>来平衡质量和速度。</p><h2 id="02c4" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">为文本到视频制作视频</h2><p id="0e44" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">梅塔(又名脸书)在人工智能进化中从未落后。在DreamFusion宣布的同一天，Meta推出了文本到视频生成的Make-A-Video。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nj"><img src="../Images/99ddea06040f032e9cc1d4964598f6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbleDLawXJGBNYR2rjne_A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">元制作视频高层架构(来源:<a class="ae le" href="https://arxiv.org/abs/2209.14792" rel="noopener ugc nofollow" target="_blank">制作视频</a>)</p></figure><p id="4aed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据上面的高层架构，<a class="ae le" href="https://arxiv.org/abs/2209.14792" rel="noopener ugc nofollow" target="_blank">制作视频</a>主要有三层:1)。在文本-图像对上训练的基础T2I(文本-图像)模型；2).时空卷积和注意力层将网络的构建模块扩展到时间维度；和3)。时空网络由时空层和T2V生成所需的另一个关键元素组成，即用于高帧率生成的帧插值网络。[4]</p><p id="edae" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，制作视频是建立在T2I模型上的，具有新颖实用的时空模块。它加速了T2V模型的训练，而不需要从头开始学习视觉和多模态表示。它不需要成对的文本-视频数据。并且生成的视频继承了广阔性(审美的多样性，奇幻的描绘等。)</p><h2 id="1cf6" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">谷歌的图像视频</h2><p id="3a96" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">Google的Imagen Video是一个基于视频扩散模型级联的文本条件视频生成系统。</p><p id="3f1d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">给定文本提示，<a class="ae le" href="https://arxiv.org/abs/2210.02303" rel="noopener ugc nofollow" target="_blank"> Imagen Video </a>使用基本视频生成模型和一系列交错的空间和时间视频超分辨率模型生成高清视频。</p><p id="afe7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它包括七个子模型来执行文本条件视频生成、空间超分辨率和时间超分辨率。整个级联以每秒24帧的速度生成128帧(~ 5.3秒)的高清1280×768(宽×高)视频，约为1.26亿像素。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/134292b6076bacefa906ff50cc1fc687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4L9KoXmBwaKI3UqKeWUnrg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Imagen视频示例为"<em class="nl">一束秋叶落在平静的湖面上，形成文字' Imagen视频'。平稳。</em>“生成的视频分辨率为1280×768，时长5.3秒，每秒24帧(来源:<a class="ae le" href="https://arxiv.org/abs/2210.02303" rel="noopener ugc nofollow" target="_blank">成像视频</a>)</p></figure><h2 id="f65c" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">稳定扩散无代码AI</h2><p id="33ec" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">如上所述，我们可以看到扩散模型是文本到图像、文本到3D和文本到视频的基础。我们用稳定扩散来体验一下。</p><p id="fe29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用建议文本:<em class="nm">亚马逊雨林中的高科技太阳能乌托邦</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/08c81264aa7265e6ba52241b9c14ff01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O32xaF1x2q18lH_DtCl9DQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(作者使用<a class="ae le" href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>)</p></figure><p id="a6f8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正文:<em class="nm">动漫女机器人头上长出花朵</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/0a4778bc2281d10a6b09c915063be801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-rLWkgo6Cr9nuv0sl8xGA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(作者使用<a class="ae le" href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>)</p></figure><p id="01f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正文:<em class="nm">从天空中看到的瑞尼尔山近景</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/f870e4c161af04295ab843143bb61f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AY6v0Kfcpy-r_VJfNGCqOQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://beta.dreamstudio.ai/dream" rel="noopener ugc nofollow" target="_blank"> DreamStudio </a>生成(左)与作者拍摄的照片(右)</p></figure><p id="7e52" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可能等不及了。下面是许多不用任何代码就可以尝试的方法。</p><ol class=""><li id="8a8a" class="nq nr iq lh b li lj ll lm lo ns ls nt lw nu ma nv nw nx ny bi translated">StabilityAI的<a class="ae le" href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>主持在拥抱的脸上</li><li id="51f9" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">稳定扩散<a class="ae le" href="https://stablediffusionweb.com/" rel="noopener ugc nofollow" target="_blank">在线</a></li><li id="60f8" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">StabilityAI的<a class="ae le" href="https://beta.dreamstudio.ai/dream" rel="noopener ugc nofollow" target="_blank">梦工厂</a></li><li id="b4c3" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><a class="ae le" href="https://www.mage.space/" rel="noopener ugc nofollow" target="_blank">法师</a>启用了NSFW</li><li id="4ed5" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><a class="ae le" href="https://playgroundai.com/" rel="noopener ugc nofollow" target="_blank">游乐场</a>艾</li><li id="e0bb" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">在<a class="ae le" href="https://www.canva.com/apps/text-to-image-(beta)" rel="noopener ugc nofollow" target="_blank"> Canva </a>上的文本到图像(Beta)</li><li id="3d30" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">Wombo艺术</li></ol><h2 id="25b3" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">下一步是什么？</h2><p id="07b3" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">生殖人工智能令人惊叹，发展迅速。当我们还沉浸在文本到图像的真实图像和艺术中时，我们现在正在进入下一个前沿领域:文本到视频和文本到3D。</p><p id="b1d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是它是新生的，充满了关于相关性、高保真度、广阔性和效率的挑战。</p><ol class=""><li id="65a5" class="nq nr iq lh b li lj ll lm lo ns ls nt lw nu ma nv nw nx ny bi translated"><strong class="lh ja">相关性</strong>:我们注意到在相同的输入文本下，模型产生不同的(甚至显著不同的)结果，以及一些不相关的结果。在生成艺术的时候，如何用自然语言描述输入的内容似乎成了一门艺术。</li><li id="d813" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><strong class="lh ja">高保真</strong>:来自DALL-E 2和稳定扩散的许多逼真图像给我们留下了深刻的印象，但它们都仍然有很大的高保真空间。</li><li id="5c70" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><strong class="lh ja">广阔</strong>:广阔是关于审美的多样性，荒诞的描绘，等等。它可以为各种输入提供丰富的结果。</li><li id="b950" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><strong class="lh ja">效率</strong>:生成一张图像需要几秒几分钟。3D和视频需要更长时间。例如，DreamFusion使用更小的64 × 64 Imagen模型，通过牺牲质量来加快速度。</li></ol><p id="a928" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">好消息是，它开启了许多令人兴奋的<strong class="lh ja">机会</strong>:人工智能工程、基础模型和生成性人工智能应用。</p><ol class=""><li id="19ed" class="nq nr iq lh b li lj ll lm lo ns ls nt lw nu ma nv nw nx ny bi translated"><strong class="lh ja">人工智能工程</strong>:人工智能工程对于实现MLOps自动化、提高数据质量、增强ML可观察性和自生成应用内容至关重要。</li><li id="4b16" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><strong class="lh ja">基础模型</strong>:独立训练和操作很多大型模型，成本很高，变得不现实。最终，它将统一或集成到几个基础模型中。这些大规模模型运行在云中，为上面的不同领域和应用服务。</li><li id="6a67" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated"><strong class="lh ja">生成式人工智能应用</strong>:有了人工智能工程和基础模型，这是一个巨大的应用机会，包括元宇宙和NFT空间的数字内容。例如，初创公司<a class="ae le" href="https://www.rosebud.ai" rel="noopener ugc nofollow" target="_blank"> Rosebud </a>专注于多样化的照片生成。</li></ol><p id="2385" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">到2025年，生成式人工智能模型将产生<a class="ae le" rel="noopener" target="_blank" href="/newsql-lakehouse-htap-and-the-future-of-data-69d427c533e0"> 10%的数据</a>。按照目前人工智能进化的速度，我们可以预计未来几年将会发生显著的变化。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><h2 id="c272" class="mk ml iq bd mm mn mo dn mp mq mr dp ms lo mt mu mv ls mw mx my lw mz na nb iw bi translated">参考</h2><ol class=""><li id="199f" class="nq nr iq lh b li nc ll nd lo ol ls om lw on ma nv nw nx ny bi translated">具有剪辑潜在性的分层文本条件图像生成:<a class="ae le" href="https://arxiv.org/abs/2204.06125" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2204.06125</a></li><li id="0194" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">用潜在扩散模型合成高分辨率图像:<a class="ae le" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2112.10752</a></li><li id="7b40" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">DreamFusion:使用2D扩散的文本到3D:<a class="ae le" href="https://arxiv.org/abs/2209.14988" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2209.14988</a></li><li id="3104" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">制作视频:没有文本-视频数据的文本-视频生成:<a class="ae le" href="https://arxiv.org/abs/2209.14792" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2209.14792</a></li><li id="29d3" class="nq nr iq lh b li nz ll oa lo ob ls oc lw od ma nv nw nx ny bi translated">Imagen视频:采用扩散模型的高清视频生成:<a class="ae le" href="https://arxiv.org/abs/2210.02303" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2210.02303</a></li></ol></div></div>    
</body>
</html>