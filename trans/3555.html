<html>
<head>
<title>On Probability versus Likelihood</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率与可能性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-probability-versus-likelihood-83386b81ad83#2022-08-08">https://towardsdatascience.com/on-probability-versus-likelihood-83386b81ad83#2022-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="236f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对两个经常互换使用但在数学上截然不同的术语的讨论</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1c31b56906e4e2ac4b0146132cc4ad49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XEooKfBX_2wmC50x"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奥地利国家图书馆在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6f10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从机器学习和数据科学的角度来看，概率和可能性用于量化不确定性，或者说某个观察值属于某个类别的可能性有多大。他们在查看混淆矩阵时突然出现；事实上，像朴素贝叶斯分类这样的算法基本上是概率模型。现实是，数据科学家无法逃避这些概念。</p><p id="cfd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在日常语言中，我们倾向于互换使用概率和可能性这两个术语。事实上，听到诸如“今天下雨的可能性有多大？”这样的话并不少见。或者“这件事或那件事发生的可能性有多大？”我实话实说。最初对我来说，这并不是一个明显的区别，但是概率和可能性是<em class="lv">不同的</em> <strong class="lb iu"> <em class="lv"> </em> </strong>(尽管是相关的)概念。作为数据科学家，这种可互换性经常会渗透到我们的工作中，因此明确这两个术语之间的区别非常重要。因此，在这篇文章中，我想我应该尝试一下说明这两种观点之间的区别。</p><h2 id="d093" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">可能性</h2><p id="9de7" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">简而言之，概率和可能性之间的根本区别在于什么被允许<em class="lv">变化——让我来解释一下。</em></p><p id="7b58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你需要知道的是<em class="lv">可能性</em>——尽管具体来说，我实际上是在谈论<em class="lv">可能性函数</em>——实际上是从一个统计模型中得出的，并被认为是从该模型中生成数据的参数的函数。迷茫？不要担心——让我们更具体一点，考虑掷十次公平硬币。</p><p id="66dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道这是一个由比率参数<em class="lv"> p </em>控制的二项式过程，比率参数定义了在总共<em class="lv"> N次</em>投掷中，掷出<em class="lv"> K次</em>正面(或反面——这无关紧要)的预期概率。这里，观察头数<em class="lv"> K </em>是一个随机变量，它响应于不同的<em class="lv"> p </em>值而变化。对于任何固定的参数值(例如，<em class="lv"> p </em> = 0.7)，观察到<em class="lv"> K </em>头部的<em class="lv">概率</em>由概率质量函数(PMF)给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/22be6be84e52472b49085a61246a1547.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*fGt9UT2lgnUcBsBbppEUIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二项式PMF。小写的希腊字母theta简单的指模型参数(图片由作者提供)。</p></figure><p id="d98d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a4125c31654d69748a5aa764b7a94e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*4cmsLMgycIch_rxHskOTkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二项式系数(图片作者提供)。</p></figure><p id="6a17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，因为我说硬币是公平的，而且它被抛了十次，我们可以让<em class="lv"> p </em> = 0.5，<em class="lv"> N </em> = 10。如果我们将这些值代入上面的等式——并让<em class="lv"> K </em>变化——我们会得到类似于图1左侧面板的结果。基于此，我们可以看到,<em class="lv"> K </em> = 5是最有可能的结果，这应该是有道理的:如果硬币是公平的，我们扔10次，我们应该预期——从长远来看——我们通常会得到5个正面和5个反面。但是你也应该注意到，获得4或6个头也不是不常见的。</p><p id="ec9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，PMF告诉我们的是，给定一组固定的模型参数，随机过程的特定实现<em class="lv">有多可能。在这种情况下，我们假设模型参数<em class="lv">是固定的</em>，并且数据可以自由地<em class="lv">变化</em>。这里的要点是，如果模型参数是已知的，那么我们要问的是关于可能观察到的数据类型的问题。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/e4010f32da0cd74011891c8a7890fd1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsBSGj05onq-bM-srMh1RQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd mx">图1: A) </strong>二项式模型的概率质量函数；<strong class="bd mx"> B) </strong>二项似然函数。点表示不同参数值的可能性(图片由作者提供)。</p></figure><h2 id="5a4d" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">可能性</h2><p id="c490" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">好吧，那么如果我告诉你我已经抛了十次硬币，得到了7个正面呢？我的问题是我扔的硬币是否公平。</p><p id="7fbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，在这种情况下，<em class="lv"> K </em>不再是随机的——我们有一个<em class="lv">观察到的</em>二项式过程的实现，这意味着它现在是一个固定值。给定该信息，二项式模型的似然函数可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/2728b598607fc1c38313133f76a72dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*oDqZlbOaZ8fBJ6BSe6Dupw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二项式模型的似然函数。</p></figure><p id="7f5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我使用“帽子”符号来表明<em class="lv"> K </em>是从十次投掷中观察到的头数。现在，这看起来就像我在说，可能性函数和PMF是一样的——嗯，我有点像是在说。但是，不同之处在于，现在<em class="lv">数据是固定的</em>，而模型参数<em class="lv"> p </em>可以自由变化。</p><p id="d6c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能性函数给我们的是一个<em class="lv">测量值</em>,表示已知<em class="lv"> K </em>等于某个观察值时<em class="lv"> p </em>的任何特定值出现的可能性。就像上面一样，如果我们将<em class="lv"> K </em> = 7代入上面的等式——让<em class="lv"> p </em>取所有可能的值——我们会得到类似于上图右侧面板的结果。注意，这里的似然函数是不对称的；相反，它的峰值超过了p = 0.7。具体来说，该分布的<em class="lv">模式</em>(即峰值本身)与所谓的<em class="lv">最大似然估计</em> (MLE)相一致。那是什么？嗯，这是最有可能给定观测数据的<em class="lv"> p </em>的值——这个名字有点泄露了它，真的。我将在另一篇文章中更多地讨论MLE，但是现在，你需要知道的是MLE的似然函数值大约是0.27。</p><p id="d7a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，在继续之前，需要注意的是:我知道可能性函数看起来像分布函数，但它是<strong class="lb iu">而不是</strong>合适的概率密度函数(即，它通常不会积分为1)。更重要的是，可能性函数是<strong class="lb iu">而不是</strong><em class="lv">p</em>等于特定值的概率(为此你需要计算后验分布。我会在另一篇文章中谈到这一点)。</p><p id="e220" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，继续。</p><h2 id="e22f" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">统计论据</h2><p id="1652" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">还记得我说过似然函数被解释为一种度量吗？确切地说，它可以用来量化统计证据。我还说过，关于最大似然估计，你需要知道的就是它的似然值大约是0.27。但是这告诉我们什么呢？说可能性大约是0.27是什么意思？它当然没有告诉我们我使用的硬币是否公平——那么从这里开始呢？</p><p id="e79f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我需要介绍一些东西:</p><ul class=""><li id="d53f" class="mz na it lb b lc ld lf lg li nb lm nc lq nd lu ne nf ng nh bi translated"><strong class="lb iu">似然原理</strong>指出，对于给定的统计模型，所有与模型参数相关的证据都存在于采样(观察)数据本身中。</li><li id="dfaf" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu">似然定律</strong>指出，证据支持一个参数值超过另一个参数值的程度取决于它们各自似然性的比率。</li></ul><p id="a011" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">把这些想法放在一起意味着孤立地看待一个单一的可能性告诉我们的很少。是的，它提供了一定程度的证据，但相对于什么呢？上面的定义告诉我们，与其他可能性相比，可能性更有用。这就是为什么我们通常更喜欢相对度量，如<em class="lv">似然比</em>。</p><p id="6cc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住这一点，回想一下我问你的问题是，根据你掌握的数据，我扔的硬币是否公平。为了正确解决这个问题，我们还需要观察数据的可能性，如果<em class="lv"> p </em> = 0.5，大约是0.12。注意，这两点都在图1中的似然函数上标出。给定这两条信息，我们现在可以计算似然比:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c83f492d3d5ec1d3a56d592ffb5ef46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*Ura6NVSGaMpOe5curhURLQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在给定观察数据的情况下，似然比比较不同参数值的相对证据(图片由作者提供)。</p></figure><p id="9455" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这给了我们什么？大约0.44。你需要一些背景知识。</p><p id="dfa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，似然比量化了数据支持断言<em class="lv"> p </em> = 0.5(公平硬币)的程度。如果这个比例是1，无论哪种方式都没有证据。如果比率大于1，那么证据有利于分子(这里，<em class="lv"> p </em> = 0.5)。但是，如果比值小于1，那么证据支持分母(这里，<em class="lv"> p </em> = 0.7)。看来我们有证据反对公平硬币假说。事实上，如果我们取倒数(即1 / .44 ),我们会看到<em class="lv"> p </em> = 0.7比<em class="lv"> p </em> = 0.5的可能性大约高2.3倍(请记住，这是仅与一个选项进行的比较，除了<em class="lv"> p </em> = 0.7，还有其他可能性)。</p><p id="1df7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实中，这是一个微不足道的结果，而似然原理和似然定律意味着使似然函数最大化的参数值就是数据最支持的值。接下来的问题是，这是否足以令人信服地证明我抛的硬币是不公平的。那是改天的话题。</p><h2 id="7842" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">包扎</h2><p id="2050" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我希望这篇文章明确了概率和可能性的区别。概括一下:<em class="lv">概率</em>通常是当我们有一个带有固定参数集的模型，并且我们对可能生成的数据类型感兴趣时，我们会考虑的东西。相反，<em class="lv">可能性</em>在我们已经观察到数据并且我们想要检查某些模型参数的可能性时发挥作用。特别是，我们可以使用似然比来量化支持一个值胜过另一个值的证据。</p><p id="d8e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读，欢迎在下面给我留言。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="bcf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，并且想保持更新，那么请考虑在Medium上关注我。这将确保你不会错过新的内容。如果你更喜欢的话，你也可以在LinkedIn和Twitter上关注我😉</p></div></div>    
</body>
</html>