<html>
<head>
<title>Advanced Topic Modeling with BERTopic</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 BERTopic 进行高级主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-topic-modeling-with-bertopic-85fb8a90369e#2022-12-19">https://towardsdatascience.com/advanced-topic-modeling-with-bertopic-85fb8a90369e#2022-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="69ff" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们如何组织世界上最难组织的数据？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3f3a063fce43abec4f7255fb3e4974a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*44NUGu6xKmBB2fqh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">阿什·埃德蒙兹在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。原文章发表在<a class="ae ky" href="https://www.pinecone.io/learn/bertopic/" rel="noopener ugc nofollow" target="_blank"> pinecone.io </a></p></figure><p id="5560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">组织大量的非结构化文本数据是复杂的。它不是为机器理解而设计的，让人类处理如此大量的数据非常昂贵，而且非常慢。</p><p id="03b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，隧道的尽头有光明。越来越多的这种非结构化文本变得可以被机器访问和理解。我们现在可以根据<em class="lv">的含义</em> 搜索文本，识别文本情感，提取实体，等等。</p><p id="93b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变形金刚是这一切的幕后推手。这些变压器(不幸的是)不是迈克尔·贝的汽车人和霸天虎，也(幸运的是)不是嗡嗡作响的电箱。我们的 NLP 变形金刚介于两者之间，它们还不是有意识的汽车人，但它们能够理解语言，这种理解方式直到几年前还只存在于科幻小说中。</p><p id="db49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有类似人类语言理解能力的机器对于组织大量非结构化文本数据非常有帮助。在机器学习中，我们将这项任务称为<em class="lv">主题建模</em>，即将数据自动聚类到特定主题中。</p><p id="ac8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERTopic 利用这些(尚未感知的)transformer 模型的高级语言能力，并使用其他一些 ML magic，如 UMAP 和 HDBSCAN(稍后将详细介绍)来产生当今语言主题建模中最先进的技术之一。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文章<a class="ae ky" href="https://youtu.be/fb7LENb9eag" rel="noopener ugc nofollow" target="_blank">视频版看这里</a>！</p></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="ff30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将深入 BERTopic [1]背后的细节，但在此之前，让我们看看如何使用它，并先看一眼它的组件。</p><p id="d2e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要一个数据集。我们可以通过以下方式从 HuggingFace datasets 下载数据集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="cb63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集包含使用 Reddit API 从<em class="lv"> /r/python </em>子编辑中提取的数据。用于这个(和所有其他例子)的代码可以在找到。</p><p id="8f61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Reddit 线程内容可以在<code class="fe mg mh mi mj b">selftext</code>特性中找到。有些是空的或短的，所以我们用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="ea3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<code class="fe mg mh mi mj b">BERTopic</code>库执行主题建模。<em class="lv">“基本”</em>方法只需要几行代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="35f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<code class="fe mg mh mi mj b">model.fit_transform</code>我们返回两个列表:</p><ul class=""><li id="c3ae" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated"><code class="fe mg mh mi mj b">topics</code>包含输入到其建模的<em class="lv">主题</em>(或集群)的一对一映射。</li><li id="b4ad" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><code class="fe mg mh mi mj b">probs</code>包含一个输入属于他们指定主题的概率列表。</li></ul><p id="ae21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以使用<code class="fe mg mh mi mj b">get_topic_info</code>查看主题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="a2ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顶部的<code class="fe mg mh mi mj b">-1</code>主题通常被认为是不相关的，它通常包含停用词，如<em class="lv">、</em>、<em class="lv">、</em>、<em class="lv">、</em>。然而，我们通过<code class="fe mg mh mi mj b">vectorizer_model</code>参数删除了停用词，因此它向我们显示了主题的<em class="lv">【最一般】</em>，如<em class="lv">【Python】</em><em class="lv">【code】</em><em class="lv">【data】</em>。</p><p id="af28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该库有几个内置的可视化方法，如<code class="fe mg mh mi mj b">visualize_topics</code>、<code class="fe mg mh mi mj b">visualize_hierarchy</code>和<code class="fe mg mh mi mj b">visualize_barchart</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/08d8ed86335f9cdfb23afd600dae9695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p2bjhagOLCVRVcpH.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERTopic 的<code class="fe mg mh mi mj b">visualize_hierarchy</code>可视化让我们可以查看主题的“层次”。</p></figure><p id="4b60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些代表了 BERTopic 库的表层，它有很好的文档，所以我们在这里不再赘述。相反，让我们试着理解一下<em class="lv">BERTopic</em>是如何工作的。</p><h1 id="1f83" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">概观</h1><p id="93d9" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">BERTopic [2]中使用的<em class="lv">有四个</em>关键部件，它们是:</p><ul class=""><li id="cfc1" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated">变压器嵌入模型</li><li id="570e" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">UMAP 降维</li><li id="2cf5" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">HDBSCAN 聚类</li><li id="d4e1" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">使用 c-TF-IDF 的聚类标记</li></ul><p id="e0ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经在这几行 BERTopic 代码中完成了所有这些工作；一切都被抽象化了。但是，我们可以通过了解每个组件的要点来优化流程。本节将在没有 BERTopic 的情况下完成每个组件<em class="lv">，并在最后返回 BERTopic 之前了解它们是如何工作的。</em></p><h2 id="9d2f" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">变压器嵌入</h2><p id="7a2d" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">BERTopic 支持几个库，用于将我们的文本编码成密集的矢量嵌入。如果我们构建低质量的嵌入，我们在其他步骤中所做的任何事情都无法帮助我们，所以从一个支持的库中选择一个合适的嵌入模型<em class="lv">非常重要</em>，这包括:</p><ul class=""><li id="d0e5" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated">句子变形金刚</li><li id="c72b" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">天资</li><li id="4bc5" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">空间</li><li id="34bf" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">根西姆</li><li id="15e5" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">使用(从 TF Hub)</li></ul><p id="85bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，<em class="lv">语句转换器</em>库提供了最广泛的高性能<a class="ae ky" href="https://www.pinecone.io/learn/sentence-embeddings/" rel="noopener ugc nofollow" target="_blank">语句嵌入模型</a>库。在 HuggingFace Hub 上搜索<em class="lv">“句子变形金刚”</em>就能找到。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/e0f350000516104eb7453b984794400c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UDQEVgli0F8ony0W.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在 HuggingFace Hub 上搜索<em class="oi">“句子变形金刚”</em>，可以找到<em class="oi">官方</em>句子变形金刚模型。</p></figure><p id="074f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个搜索的第一个结果是<code class="fe mg mh mi mj b">sentence-transformers/all-MiniLM-L6-v2</code>，这是一个流行的高性能模型，它创建了<em class="lv"> 384 </em>维句子嵌入。</p><p id="c301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了初始化模型并编码我们的 Reddit 主题数据，我们首先<code class="fe mg mh mi mj b">pip install sentence-transformers</code>然后编写:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="4ed1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们对文本进行了成批的<code class="fe mg mh mi mj b">16</code>编码。每个批次都被添加到<code class="fe mg mh mi mj b">embeds</code>数组中。一旦我们拥有了<code class="fe mg mh mi mj b">embeds</code>中所有的<a class="ae ky" href="https://www.pinecone.io/learn/sentence-embeddings/" rel="noopener ugc nofollow" target="_blank">句子嵌入</a>，我们就准备好进入下一步了。</p><h2 id="9d67" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">降维</h2><p id="e28c" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在构建我们的嵌入之后，BERTopic 将它们压缩到一个低维空间中。这意味着我们的 384 维向量转化为二维/三维向量。</p><p id="8b0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以这样做，因为 384 维是很多的 T21，我们不太可能真的需要那么多维来表示我们的文本。相反，我们试图将这些信息压缩成二维或三维。</p><p id="a930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们这样做是为了更有效地执行下面的 HDBSCAN 集群步骤。用 384 个维度执行聚类步骤会非常慢[5]。</p><p id="6d11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个好处是我们可以可视化我们的数据；这在评估我们的数据是否可以聚集时非常有用。可视化也有助于调整降维参数。</p><p id="90e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了帮助我们理解降维，我们将从世界的 3D 表示开始。你可以在这里找到这部分的代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/46e4aa5f3ac70f14d36703cc6efe7454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dz97Jnh3LeIS-b9uHIdug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<code class="fe mg mh mi mj b"><a class="ae ky" href="https://huggingface.co/datasets/jamescalam/world-cities-geo" rel="noopener ugc nofollow" target="_blank">jamescalam/world-cities-geo</a></code>数据集的点的 3D 散点图。<a class="ae ky" href="https://www.pinecone.io/learn/bertopic/#:~:text=3D%20scatter%20plot%20of%20points" rel="noopener ugc nofollow" target="_blank">互动视觉可以在这里找到</a>。</p></figure><p id="899b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以对这些数据应用许多降维技术；两个最受欢迎的选择是 PCA 和 t-SNE。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/9c4bf75bae58f1d5d901e3a945baf1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cup4IC8kfa6XWXnrVpZpqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的 2D 世界减少了使用五氯苯甲醚。</p></figure><p id="323e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA 通过保持<em class="lv">更大的距离</em>(使用均方误差)来工作。结果是数据的<em class="lv">全局结构</em>通常被保留[6]。我们可以看到上面的行为，因为每个大陆都与其相邻的大陆组合在一起。当我们在数据集中有容易区分的聚类时，这可能是好的，但对于更细微的数据，这种方法表现不佳，因为<em class="lv">局部结构</em>很重要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/facab3de065f8862ebe650c37a804a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-1eZ5UB4J9VMyyqGmy-OA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">2D 地球减少使用 t-SNE。</p></figure><p id="7ee9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t-SNE 则相反；它保留了<em class="lv">局部结构</em>而不是<em class="lv">全局结构</em>。这种局部的焦点来自于 t-SNE 建立一个图表，连接所有最近的点。这些局部结构可以间接地暗示整体结构，但它们没有被强烈地捕捉到。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="5595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> PCA 侧重于保持</em> <strong class="lb iu"> <em class="lv">相异</em> </strong> <em class="lv">而 t-SNE 侧重于保持</em> <strong class="lb iu"> <em class="lv">相似</em> </strong> <em class="lv">。</em></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="6686" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，我们可以使用一种不太为人所知的技术来捕捉这两者的最佳之处，这种技术叫做<strong class="lb iu">U</strong>n form<strong class="lb iu">M</strong>anifold<strong class="lb iu">A</strong>approximation 和<strong class="lb iu">P</strong>production(UMAP)。</p><p id="e427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用使用<code class="fe mg mh mi mj b">pip install umap-learn</code>安装的 UMAP 库在 Python 中应用 UMAP。要使用默认的 UMAP 参数映射到 3D 或 2D 空间，我们只需编写:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="c51c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UMAP 算法可以使用几个参数进行微调。尽管如此，仅用<code class="fe mg mh mi mj b">n_neighbors</code>参数就可以实现最简单和最有效的调优。</p><p id="d6fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个数据点，UMAP 搜索其他点并识别第<strong class="lb iu"> k </strong>个最近的邻居[3]。它是由<code class="fe mg mh mi mj b">n_neighbors</code>参数控制的<strong class="lb iu"> k </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/889149b0c4cd857034657a58ce1e1400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_BjBVZ6otwZCWT4Mpxumig.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd om"> k </strong>和<code class="fe mg mh mi mj b">n_neighbors</code>在这里是同义词。当我们增加<code class="fe mg mh mi mj b">n_neighbors</code>时，UMAP 构建的图表可以考虑更远的点，更好地代表全球结构。</p></figure><p id="4480" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们有许多点(高密度区域)的地方，我们的点和它的第<strong class="lb iu"> k </strong>个最近邻点之间的距离通常较小。在点数较少的低密度区域，距离会大得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/e258899b0e9bffac9c4e370d20b94210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xTaaIosJlYnIgs0o.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用不同区域中第<strong class="bd om"> k </strong>个最近邻居之间的距离间接测量密度。</p></figure><p id="ad12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UMAP 将试图保持到第 k 个最近点的距离，这就是 UMAP 在转移到一个更低维度时试图保持的距离。</p><p id="0c39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过增加<code class="fe mg mh mi mj b">n_neighbors</code>，我们可以保留更多的全局结构，而更低的<code class="fe mg mh mi mj b">n_neighbors</code>可以更好地保留局部结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/ba85b7385c033adab6eb939d1498b7ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QsidsPGPErMkTuUz.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更高的<code class="fe mg mh mi mj b">n_neighbors</code> ( <strong class="bd om"> k </strong>)意味着我们保留了更大的距离，从而保留了更多的全局结构。</p></figure><p id="2b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 PCA 或 t-SNE 等其他降维技术相比，找到一个好的<code class="fe mg mh mi mj b">n_neighbours</code>值可以让我们相对较好地保留<em class="lv">局部和全局结构。</em></p><p id="04f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将其应用到我们的 3D 地球，我们可以看到邻国仍然是邻居。同时大陆摆放正确(南北倒置)，岛屿与大陆分离。我们甚至还有“西欧”的伊比利亚半岛。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/69eeba8d78222aa4489eb085e654a07f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8lrFuW4PFpYpHdYWrmPm9w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">UMAP 还原的地球。</p></figure><p id="06e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UMAP 保留了 PCA 没有保留的可识别特征和比 SNE 霸王龙更好的全局结构。这是 UMAP 利益所在的一个很好的例子。</p><p id="bb3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们已经标记了数据，通过将标签传递给<code class="fe mg mh mi mj b">target</code>参数，UMAP 也可以用作监督降维方法。使用这种有监督的方法有可能产生更有意义的结构。</p><p id="9659" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住所有这些，让我们将 UMAP 应用于我们的 Reddit 主题数据。使用<code class="fe mg mh mi mj b">3</code> - <code class="fe mg mh mi mj b">5</code>中的<code class="fe mg mh mi mj b">n_neighbors</code>似乎效果最好。我们可以添加<code class="fe mg mh mi mj b">min_dist=0.05</code>来允许 UMAP 将点放得更近(默认值是<code class="fe mg mh mi mj b">1.0</code>)；这帮助我们将三个<em class="lv">相似的</em>主题从<em class="lv"> r/Python </em>、<em class="lv"> r/LanguageTechnology </em>和<em class="lv"> r/pytorch </em>中分离出来。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/644fa5920e97753a9a6b9392947bdb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NH_kT5TcsBVg7gLIEDHTwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Reddit 主题使用 UMAP 将数据简化到 3D 空间。</p></figure><p id="a1d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着我们的数据减少到一个低维空间，主题很容易视觉识别，我们在一个很好的位置上移动到集群。</p><h2 id="ebf6" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">HDBSCAN 聚类</h2><p id="5a2f" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">我们已经使用现有的<em class="lv">子</em>特征为我们的聚类着色，可视化了 UMAP 缩减的数据。它看起来很漂亮，但是我们通常不执行主题建模来标记已经标记的数据。如果我们假设我们没有现有的标签，我们的 UMAP 视觉将如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/b9d0661388c8835e7efab7b75157d290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3K7wJZQiekkxNXKq2KVMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">UMAP 减少了城市数据，我们可以区分许多集群/大洲，但没有标签着色就困难得多。</p></figure><p id="2551" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看如何使用 HDBSCAN 对(现在的)低维向量进行聚类。</p><p id="bfab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">聚类方法可以分为平面或分层的<em class="lv">和</em>质心或基于密度的技术【5】。每一种都有自己的优点和缺点。</p><p id="5a9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">扁平化或层次化仅仅关注聚类方法中是否存在层次。例如，我们可以(理想地)将我们的图层次结构视为从大陆到国家再到城市。这些方法允许我们查看一个给定的层次结构，并尝试沿着树识别一个逻辑“切割”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/74a2aa3a7c5f9c97923e7f7cffdc7f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4EYSpYnDByw32vxy.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分层技术从一个大的集群开始，将这个集群分成越来越小的部分，并试图在层次结构中找到理想的集群数量。</p></figure><p id="7e8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种分裂是基于质心或基于密度的聚类。即基于接近质心的聚类或基于点密度的聚类。基于质心的聚类非常适合于<em class="lv">【球形】</em>聚类，而基于密度的聚类可以处理更不规则的形状<em class="lv">和</em>识别异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/c9e3a31b79e0093dbdb2476f6b6e82f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9Jl6ioUoCHAvo8-s.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于质心的聚类(左)与基于密度的聚类(右)。</p></figure><p id="773f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">HDBSCAN 是一种基于密度的分层方法。这意味着我们可以受益于分层数据更容易的调整和可视化，处理不规则的聚类形状，<em class="lv">和</em>识别异常值。</p><p id="da3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们第一次将 HDBSCAN 聚类应用于我们的数据时，我们将返回由下面的<em class="lv">浓缩树图</em>中的红色<em class="lv">圆圈</em>标识的<em class="lv">多个</em>微小聚类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="aad2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">随着算法通过增加λ值进行扫描，浓缩树图显示了离群点的减少和聚类的分裂。</em></p><p id="c2d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> HDBSCAN 根据不同 lambda 值下的大小和持久性选择最终聚类。树中最粗、最持久的“分支”被认为是最稳定的，因此是聚类的最佳候选。</em></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="2bf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些聚类不是很有用，因为<em class="lv">默认</em>创建一个聚类所需的最小点数仅仅是<code class="fe mg mh mi mj b">5</code>。给定我们的～3K 点数据集，我们的目标是产生～4 个子旋涡星团，这是很小的。幸运的是，我们可以使用<code class="fe mg mh mi mj b">min_cluster_size</code>参数增加这个阈值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="9d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好些了，但还不够。我们可以尝试将<code class="fe mg mh mi mj b">min_cluster_size</code>减少到<code class="fe mg mh mi mj b">60</code>，以将三个集群拉到绿色区块下方。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="921b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，这仍然拉进绿色块，甚至允许<em class="lv">太小的</em>簇(如左边)。另一个选择是保留<code class="fe mg mh mi mj b">min_cluster_size=80</code>，但增加<code class="fe mg mh mi mj b">min_samples=40</code>，以允许更多稀疏的核心点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="1201" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有四个集群，我们可以使用<code class="fe mg mh mi mj b">clusterer.labels_</code>中的数据来可视化它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/4b3eed6d072478f685a26076bfe9d474.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m80WtNLMYDmwrIF9Z1fNoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">HDBSCAN 对 reddit 主题进行聚类，准确识别不同的子 Reddit 聚类。稀疏的蓝点是异常值，并且不被识别为属于任何聚类。</p></figure><p id="13cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些异常值用蓝色标记，其中一些有意义(固定的每日讨论线程),其他没有意义。然而，总的来说，这些集群是非常准确的。有了这些，我们就可以试着识别这些集群的意义。</p><h2 id="6edd" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">基于 c-TF-IDF 的主题抽取</h2><p id="1dd1" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">BERTopic 的最后一步是为每个集群提取主题。为此，BERTopic 使用了 TF-IDF 的修改版本，称为 c-TF-IDF。</p><p id="f26b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF 是一种流行的技术，用于识别给定一个术语或一组术语的最相关的<em class="lv">【文档】</em>。c-TF-IDF 通过在一个集群中找到所有“文档”中最相关的<em class="lv">术语</em>来扭转这一局面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/37836a929e5d78cfd971e5487e3b3008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z9NpyHX5uhUHCLWr.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">c-TF-IDF 从每个类(集群)中寻找最相关的术语来创建主题。</p></figure><p id="78ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的 Reddit 主题数据集中，我们已经能够识别非常独特的集群。然而，我们仍然需要确定这些集群谈论什么。我们首先预处理<code class="fe mg mh mi mj b">selftext</code>来创建令牌。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="fa03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">c-TF-IDF 的一部分要求计算类别<em class="lv"> c </em>中术语<em class="lv"> t </em>的频率。为此，我们需要查看哪些令牌属于每个类。我们首先将集群/类标签添加到<code class="fe mg mh mi mj b">data</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="1e5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在创建特定于类的令牌列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="328f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以计算每个类的频率(TF)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="7b08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这可能需要一些时间；我们的 TF 过程将可读性放在任何效率概念之上。一旦完成，我们就可以计算<strong class="lb iu">I</strong>v 反转<strong class="lb iu"> D </strong>文档<strong class="lb iu"> F </strong>频率(IDF ),它告诉我们一个术语有多常见。罕见术语比常见术语具有更高的相关性(并将输出更高的 IDF 分数)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="197e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe mg mh mi mj b">tf_idf</code>，我们有一个<code class="fe mg mh mi mj b">vocab</code>大小的每个职业的 TF-IDF 分数列表。我们可以使用 Numpy 的<code class="fe mg mh mi mj b">argpartition</code>函数来检索包含每个类的最大 TF-IDF 分数的索引位置。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="789a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们将这些索引位置映射回<code class="fe mg mh mi mj b">vocab</code>中的原始单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><h1 id="5275" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">回到 BERTopic</h1><p id="dfba" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">我们已经讨论了相当多的内容，但是我们能把我们所学的应用到 BERTopic 库吗？</p><p id="9c64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，我们只需要几行代码。和以前一样，我们初始化我们的自定义嵌入、UMAP 和 HDBSCAN 组件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="77b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会注意到，我们已经将<code class="fe mg mh mi mj b">prediction_data=True</code>作为一个新参数添加到了<code class="fe mg mh mi mj b">HDBSCAN</code>中。我们需要这样做，以避免在将我们的自定义 HDBSCAN 步骤与 BERTopic 集成时出现<strong class="lb iu"> AttributeError </strong>。添加<code class="fe mg mh mi mj b">gen_min_span_tree</code>为 HDBSCAN 增加了另一个步骤，可以改善生成的集群。</p><p id="c206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还必须初始化一个<code class="fe mg mh mi mj b">vectorizer_model</code>来处理 c-TF-IDF 步骤中的停用词移除。我们将使用 NLTK 中的停用词列表，但是添加了一些看起来会影响结果的标记。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="f481" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备将所有这些传递给一个<code class="fe mg mh mi mj b">BERTopic</code>实例，并处理我们的 Reddit 主题数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf lx l"/></div></figure><p id="f5b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用<code class="fe mg mh mi mj b">model.visualize_barchart()</code>来想象新的主题</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/b0d860bb1dec0227f4e19f7701b21eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YnRy057ejr_wIPP0.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的最终主题是使用 BERTopic 库和经过调整的 UMAP 和 HDBSCAN 参数生成的。</p></figure><p id="1c11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到主题与<em class="lv">r/投资</em>、<em class="lv"> r/pytorch </em>、<em class="lv"> r/LanguageTechnology </em>和<em class="lv"> r/Python </em>完美契合。</p><p id="bcb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Transformers、UMAP、HDBSCAN 和 c-TF-IDF 显然是功能强大的组件，在处理非结构化文本数据时有着巨大的应用。BERTopic 已经抽象了这个堆栈的大部分复杂性，允许我们只用几行代码就可以应用这些技术。</p><p id="7511" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然 BERTopic 可能很简单，但您已经看到它可以深入到各个组件中。通过对这些组件的高度理解，我们可以极大地提高我们的主题建模性能。</p><p id="0aee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经在这里介绍了一些要点，但是在本文中，我们实际上只是触及了主题建模的皮毛。BERTopic 和每个组件的内容比我们希望在一篇文章中涵盖的要多得多。</p><p id="3e49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，去应用你在这里学到的东西吧，记住，尽管这里展示了 BERTopic 令人难以置信的性能，它还可以做更多的事情。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="bd2e" class="mz na it bd nb nc op ne nf ng oq ni nj jz or ka nl kc os kd nn kf ot kg np nq bi translated">资源</h1><p id="3b4e" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">🔗<a class="ae ky" href="https://github.com/pinecone-io/examples/tree/master/learn/algos-and-libraries/bertopic" rel="noopener ugc nofollow" target="_blank">所有笔记本脚本</a></p><p id="86d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] M. Grootendorst，<a class="ae ky" href="https://github.com/MaartenGr/BERTopic" rel="noopener ugc nofollow" target="_blank"> BERTopic Repo </a>，GitHub</p><p id="e5cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] M. Grootendorst，<a class="ae ky" href="https://arxiv.org/abs/2203.05794" rel="noopener ugc nofollow" target="_blank"> BERTopic:基于类的 TF-IDF 过程的神经主题建模</a> (2022)</p><p id="41ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] L .麦金尼斯，j .希利，j .梅尔维尔，<a class="ae ky" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank"> UMAP:一致流形逼近与降维投影</a> (2018)</p><p id="0ed4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] L .麦金尼斯，<a class="ae ky" href="https://www.youtube.com/watch?v=nq6iPZVUxZU" rel="noopener ugc nofollow" target="_blank">谈 UMAP 降维</a> (2018)，SciPy 2018</p><p id="f9ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] J. Healy，<a class="ae ky" href="https://www.youtube.com/watch?v=dGsxd67IFiU" rel="noopener ugc nofollow" target="_blank"> HDBSCAN，基于快速密度的聚类，如何和为什么</a> (2018)，PyData NYC 2018</p><p id="837f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] L .麦金尼斯，<a class="ae ky" href="https://www.youtube.com/watch?v=9iol3Lk6kyU" rel="noopener ugc nofollow" target="_blank">《一个骗子的降维指南》</a> (2018)，PyData NYC 2018</p><p id="7e80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=6BPl81wGGP8" rel="noopener ugc nofollow" target="_blank"> UMAP 解说</a>，AI 与 Letitia 的茶歇，YouTube</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="036c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">*除另有说明外，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>