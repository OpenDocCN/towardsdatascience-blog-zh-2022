<html>
<head>
<title>A Visual Guide to Learning Rate Schedulers in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中学习速率调度程序的可视化指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#2022-12-06">https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863#2022-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6bea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python 深度学习的 LR 衰减和退火策略</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c55c421e63bbf0f467e1ae33cc1a002c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qe6nYlH8zsmUdScyHMhRCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="a3b0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">深度学习中的学习率是多少？</h1><p id="5465" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">神经网络有许多影响模型性能的超参数。<strong class="ls iu">基本超参数</strong>之一是学习率(LR)，它决定了训练步骤之间模型权重的变化程度。在最简单的情况下，LR 值是 0 和 1 之间的固定值。</p><p id="5a9d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，选择正确的 LR 值可能具有挑战性。一方面，<strong class="ls iu">大的学习率</strong>可以帮助算法快速收敛。但它也可能导致算法在最小值附近反弹，而不会达到最小值，或者如果最小值太大，甚至会跳过它。另一方面，一个<strong class="ls iu">小的学习率</strong>可以更好地收敛到最小值。但是，如果太小，优化器可能需要太长时间才能收敛或者停滞不前。</p><h1 id="57a4" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">什么是学习率计划程序？</h1><p id="5afa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">帮助算法<strong class="ls iu">快速收敛到最优</strong>的一个解决方案是使用学习率调度器。学习率调度器在训练过程中根据预定义的时间表调整学习率。</p><blockquote class="mr"><p id="0f32" class="ms mt it bd mu mv mw mx my mz na ml dk translated">帮助算法快速收敛到最优的一个解决方案是使用学习率调度器。</p></blockquote><p id="6358" class="pw-post-body-paragraph lq lr it ls b lt nb ju lv lw nc jx ly lz nd mb mc md ne mf mg mh nf mj mk ml im bi translated">通常，学习率在训练开始时被设置为较高的值，以允许更快的收敛。随着训练的进行，学习率降低，以使收敛到最优，从而导致更好的性能。在训练过程中降低学习率也被称为<strong class="ls iu">退火或衰退</strong>。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="c6c4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">不同学习速率调度程序的数量可能会令人不知所措。因此，本文旨在向您概述 PyTorch 中不同的预定义学习率调度程序如何在训练期间调整学习率:</p><ul class=""><li id="5796" class="nn no it ls b lt mm lw mn lz np md nq mh nr ml ns nt nu nv bi translated"><a class="ae nw" href="#eec7" rel="noopener ugc nofollow"> StepLR </a></li><li id="f5a6" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#e836" rel="noopener ugc nofollow">多步</a></li><li id="64cf" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#deba" rel="noopener ugc nofollow">常数 R </a></li><li id="c409" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#a9e8" rel="noopener ugc nofollow"> LinearLR </a></li><li id="8b57" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#b465" rel="noopener ugc nofollow">指数运算</a></li><li id="d757" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#1209" rel="noopener ugc nofollow">多项式</a></li><li id="ed00" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#fad1" rel="noopener ugc nofollow"> CosineAnnealingLR </a></li><li id="3958" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#6de9" rel="noopener ugc nofollow">CosineAnnealingWarmRestartsLR</a></li><li id="3d8e" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#1dbe" rel="noopener ugc nofollow"> CyclicLR </a></li><li id="4ad0" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#badd" rel="noopener ugc nofollow"> OneCycleLR </a></li><li id="cbce" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#5407" rel="noopener ugc nofollow">reducelronplateaaulr</a></li><li id="774e" class="nn no it ls b lt nx lw ny lz nz md oa mh ob ml ns nt nu nv bi translated"><a class="ae nw" href="#811c" rel="noopener ugc nofollow">带有 Lambda 功能的定制学习率调度器</a></li></ul><h1 id="6936" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">学习率调度程序</h1><p id="e434" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于本文，我们使用 PyTorch 版本 1.13.0。您可以在<a class="ae nw" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank"> PyTorch 文档</a>中阅读更多关于学习率调度器的详细信息。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="7731" class="oh kz it od b be oi oj l ok ol">import torch</span></pre><p id="fce8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">您可以在本文末尾的附录中找到用于可视化 PyTorch 学习率调度程序的 Python 代码。</p><h2 id="eec7" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">StepLR</h2><p id="a730" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在每经过预定数量的训练步骤后，<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank">StepLR</a></code>将学习率降低一个倍数。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="b16b" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import StepLR<br/><br/>scheduler = StepLR(optimizer, <br/>                   step_size = 4, # Period of learning rate decay<br/>                   gamma = 0.5) # Multiplicative factor of learning rate decay</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/d85f1d7a71f407b4d264e6543566f9a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wFMvRnA4NTLdt-1GTcz4Eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">StepLR </code>(图片由作者提供)</p></figure><h2 id="e836" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">多步骤</h2><p id="3527" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR" rel="noopener ugc nofollow" target="_blank">MultiStepLR</a></code>——类似于<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank">StepLR</a></code>——也以倍增因子降低学习率，但在每个预定义的里程碑之后。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="e8cf" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import MultiStepLR<br/><br/>scheduler = MultiStepLR(optimizer, <br/>                        milestones=[8, 24, 28], # List of epoch indices<br/>                        gamma =0.5) # Multiplicative factor of learning rate decay</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/dc301c30d6d88d336a231028186559b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6dKtKQWqVbsSx5q3kHFxMg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">MultiStepLR </code>(图片由作者提供)</p></figure><h2 id="deba" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">ConstantLR</h2><p id="4348" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR" rel="noopener ugc nofollow" target="_blank">ConstantLR</a></code>以倍增系数降低学习率，直到训练步数达到预定义的里程碑。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="2c0e" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import ConstantLR<br/><br/>scheduler = ConstantLR(optimizer, <br/>                       factor = 0.5, # The number we multiply learning rate until the milestone.<br/>                       total_iters = 8) # The number of steps that the scheduler decays the learning rate</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/fa4f9e5aa3f29b116e7c0a4795763f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-Au1Km8D2HXA2ya2VJV4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">ConstantLR </code>(图片由作者提供)</p></figure><p id="bc76" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">正如你可能已经注意到的，如果你的起始因子小于 1，这个学习率计划程序会在训练过程中增加学习率，而不是减少。</p><h2 id="a9e8" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">线性 LR</h2><p id="63ac" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">与<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.html#torch.optim.lr_scheduler.ConstantLR" rel="noopener ugc nofollow" target="_blank">ConstantLR</a></code>类似，<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR" rel="noopener ugc nofollow" target="_blank">LinearLR</a></code>也在训练开始时以倍增系数降低学习率。但是它在限定数量的训练步骤上线性地增加学习速率，直到它达到它最初设置的学习速率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="1235" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import LinearLR<br/><br/>scheduler = LinearLR(optimizer, <br/>                     start_factor = 0.5, # The number we multiply learning rate in the first epoch<br/>                     total_iters = 8) # The number of iterations that multiplicative factor reaches to 1</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/d205242d03b23315272397a615e21074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YY8QeD9-cIkCVLC1DNbFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">LinearLR </code>(图片由作者提供)</p></figure><p id="3700" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你的起始因子小于 1，这个学习率计划程序还会在训练过程中提高学习率，而不是降低它。</p><h2 id="b465" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">指数 LR</h2><p id="eecd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在每个训练步骤中，<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR" rel="noopener ugc nofollow" target="_blank">ExponentialLR</a></code>以倍增系数降低学习率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="d6b7" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import ExponentialLR<br/><br/>scheduler = ExponentialLR(optimizer, <br/>                          gamma = 0.5) # Multiplicative factor of learning rate decay.</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/6757a08cf5331063f74eb7b4f03ae118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IF-wtsedzcMSsLMQIsdCHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">ExponentialLR </code>(图片由作者提供)</p></figure><h2 id="1209" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">多项式 lLR</h2><p id="42f2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.PolynomialLR.html#torch.optim.lr_scheduler.PolynomialLR" rel="noopener ugc nofollow" target="_blank">PolynomialLR</a></code>通过使用一个多项式函数来定义步数，从而降低学习率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="d9ff" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import PolynomialLR<br/><br/>scheduler = PolynomialLR(optimizer, <br/>                         total_iters = 8, # The number of steps that the scheduler decays the learning rate.<br/>                         power = 1) # The power of the polynomial.</span></pre><p id="9156" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">下面你可以看到<code class="fe oy oz pa od b">power = 1</code>的学习率下降。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/3057599129a40ce6188de1a0cc79b644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVSgHtT9ab1sjj56ryD0cw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">PolynomialLR with Power = 1 </code>(图片由作者提供)</p></figure><p id="3e98" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">power = 2</code>得到的学习率衰减将如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/62477fd74168441ad3d5c80128ea692e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcFgfcihkVk61x6vz-D6lA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">PolynomialLR with Power = 2 </code>(图片由作者提供)</p></figure><h2 id="fad1" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">CosineAnnealingLR</h2><p id="02fa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" rel="noopener ugc nofollow" target="_blank">CosineAnnealingLR</a></code>通过余弦函数降低学习率。</p><p id="4c08" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">虽然您可以从技术上安排学习率调整遵循多个周期，但想法是在最大迭代次数的半个周期内衰减学习率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="d031" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import CosineAnnealingLR<br/><br/>scheduler = CosineAnnealingLR(optimizer,<br/>                              T_max = 32, # Maximum number of iterations.<br/>                             eta_min = 1e-4) # Minimum learning rate.</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/f74dffaeb01e0ffce1ec896f759b09fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bk4xhtvg_Su42GmiVtvigg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">CosineAnnealingLR </code>(图片由作者提供)</p></figure><p id="4c53" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">两位 Kaggle 竞赛大师 Philipp Singer 和 Yauhen Babakhin 推荐使用余弦衰减作为深度迁移学习的学习速率调度器[2]。</p><h2 id="6de9" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">CosineAnnealingWarmRestartsLR</h2><p id="a65f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts" rel="noopener ugc nofollow" target="_blank">CosineAnnealingWarmRestarts</a></code>类似于余弦退火程序。但是，它允许您使用初始 LR 在每个时期重新启动 LR 计划。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="9f62" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts<br/>scheduler = CosineAnnealingWarmRestarts(optimizer, <br/>                                        T_0 = 8,# Number of iterations for the first restart<br/>                                        T_mult = 1, # A factor increases TiTi​ after a restart<br/>                                        eta_min = 1e-4) # Minimum learning rate</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/8a66c1730ff32450fae34c53021d9c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssuzguqjYJ6QmEagG1Rb8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">CosineAnnealingWarmRestarts </code>(图片由作者提供)</p></figure><p id="f177" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这被称为热重启，于 2017 年推出[1]。增加 LR 会导致模型发散。然而，这种有意的发散使得模型能够避开局部极小值并找到更好的全局极小值。</p><h2 id="1dbe" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">CyclicLR</h2><p id="12c0" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR" rel="noopener ugc nofollow" target="_blank">CyclicLR</a></code>根据循环学习率策略调整学习率，该策略基于我们在上一节刚刚讨论过的热重启概念。PyTorch 中有三个内置策略。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="7f50" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import CyclicLR<br/><br/>scheduler = CyclicLR(optimizer, <br/>                     base_lr = 0.0001, # Initial learning rate which is the lower boundary in the cycle for each parameter group<br/>                     max_lr = 1e-3, # Upper learning rate boundaries in the cycle for each parameter group<br/>                     step_size_up = 4, # Number of training iterations in the increasing half of a cycle<br/>                     mode = "triangular")<br/></span></pre><p id="83ff" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">mode = "triangular"</code>,得到的学习率衰减将遵循基本的三角周期，没有幅度缩放，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/a3c5b4d51aca147cb6c912d9c7a15e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GwAGVpP01AymI1uZnGLbDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">CyclicLR with mode = 'triangular' </code>(图片由作者提供)</p></figure><p id="4bab" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">mode = "triangular2"</code>，最终的学习率衰减将遵循一个基本的三角周期，在每个周期将初始振幅减半，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/a4895ead40643d443b7a50e046611c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZrwUMXATsUFAXdnLyoCIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">CyclicLR with mode = 'triangular2' </code>(图片由作者提供)</p></figure><p id="4476" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">mode = "exp_range"</code>得到的学习率衰减将如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/8406c2ac19cb9fd601f77ac8a4ca647d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZrhoPV7JnyUoMS7EbgWy-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">CyclicLR with mode = 'exp_range' </code>(图片由作者提供)</p></figure><h2 id="badd" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">OneCycleLR</h2><p id="df95" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR" rel="noopener ugc nofollow" target="_blank">OneCycleLR</a></code>根据 1cycle 学习率策略降低学习率，该策略在 2017 年的一篇论文中介绍过[3]。</p><p id="5679" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">与许多其他学习率调度器相比，学习率不仅在训练过程中降低。相反，学习率从初始学习率增加到某个最大学习率，然后再次降低。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="ff31" class="oh kz it od b be oi oj l ok ol">from torch.optim.lr_scheduler import OneCycleLR<br/><br/><br/>scheduler = OneCycleLR(optimizer, <br/>                       max_lr = 1e-3, # Upper learning rate boundaries in the cycle for each parameter group<br/>                       steps_per_epoch = 8, # The number of steps per epoch to train for.<br/>                       epochs = 4, # The number of epochs to train for.<br/>                       anneal_strategy = 'cos') # Specifies the annealing strategy</span></pre><p id="db15" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">anneal_strategy = "cos"</code>得到的学习率衰减将如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/55b6812cf9637b579254146fa265bf21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PKXhfCWYqTnU3rtkkWdZIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">OneCycleLR with </code> anneal_strategy = 'cos '(图片由作者提供)</p></figure><p id="87f2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">使用<code class="fe oy oz pa od b">anneal_strategy = "linear"</code>得到的学习率衰减将如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/8eba3486c33efcba896750832defdd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wFFQ4n33Njzyo1vq8ATeBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 学习率调度器<code class="fe oy oz pa od b">OneCycleLR with </code> anneal_strategy = 'linear '(图片由作者提供)</p></figure><h2 id="ec74" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">reducelronplateaaulr</h2><p id="197b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">当指标停止改善时,<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" rel="noopener ugc nofollow" target="_blank">ReduceLROnPlateau</a></code>会降低学习率。正如您所猜测的，这很难可视化，因为学习率降低的时间取决于您的模型、数据和超参数。</p><h2 id="811c" class="om kz it bd la on oo dn le op oq dp li lz or os lk md ot ou lm mh ov ow lo ox bi translated">带有 Lambda 函数的定制学习率调度程序</h2><p id="267e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果内置的学习率调度程序不能满足你的需要，你可以定义一个带有 lambda 函数的调度程序。lambda 函数是一个基于纪元值返回乘法因子的函数。</p><p id="472d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR" rel="noopener ugc nofollow" target="_blank">LambdaLR</a></code>通过将来自λ函数的乘法因子应用于<strong class="ls iu">初始</strong> LR 来调整学习率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="ca10" class="oh kz it od b be oi oj l ok ol">lr_epoch[t] = lr_initial * lambda(epoch)</span></pre><p id="c6bb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">另一方面，<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR" rel="noopener ugc nofollow" target="_blank">MultiplicativeLR</a></code>通过将来自 lambda 函数的乘法因子应用于前一时期的 LR 来调整学习率。</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="9405" class="oh kz it od b be oi oj l ok ol">lr_epoch[t] = lr_epoch[t-1] * lambda(epoch)</span></pre><p id="8c22" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些学习率调度器也有点难以可视化，因为它们高度依赖于定义的 lambda 函数。</p><h1 id="646a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论和可视备忘单</h1><p id="7f7a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在你已经看到了各种不同的内置 PyTorch 学习速率调度器，你可能很好奇你应该为你的深度学习项目选择哪种学习速率调度器。</p><p id="97f7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">不幸的是，答案并不那么容易——在生活中经常如此。有一段时间，<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" rel="noopener ugc nofollow" target="_blank">ReduceLROnPlateau</a></code>是一个流行的学习率调度器。如今，像<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR" rel="noopener ugc nofollow" target="_blank">CosineAnnealingLR</a></code>和<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR" rel="noopener ugc nofollow" target="_blank">OneCycleLR</a></code>这样的方法或者像<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts" rel="noopener ugc nofollow" target="_blank">CosineAnnealingWarmRestarts</a></code>和<code class="fe oy oz pa od b"><a class="ae nw" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR" rel="noopener ugc nofollow" target="_blank">CyclicLR</a></code>这样的热重启方法越来越受欢迎。</p><p id="bd40" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">尽管如此，您可能需要运行一些实验来确定哪种学习速率调度程序最适合您的问题。但是，我们可以说的是，使用任何学习调度程序最有可能导致更好的模型性能。</p><p id="4a2e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">下面是 PyTorch 中所讨论的学习率调度器的可视化总结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/27fa93d1e773508b278f51afab2f491b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38YtYpsGTrunrauXfusVbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch 中学习率调度器的备忘单(图片由作者提供)</p></figure></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><h1 id="544b" class="ky kz it bd la lb pd ld le lf pe lh li jz pf ka lk kc pg kd lm kf ph kg lo lp bi translated">喜欢这个故事吗？</h1><p id="ae97" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="pi">如果你想把我的新故事直接发到你的收件箱，</em> <a class="ae nw" href="https://medium.com/subscribe/@iamleonie" rel="noopener"> <em class="pi">订阅</em> </a> <em class="pi">！</em></p><p id="e677" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="pi">成为媒介会员，阅读更多其他作家和我的故事。报名时可以用我的</em> <a class="ae nw" href="https://medium.com/@iamleonie/membership" rel="noopener"> <em class="pi">推荐链接</em> </a> <em class="pi">支持我。我将收取佣金，不需要你额外付费。</em></p><div class="pj pk gp gr pl pm"><a href="https://medium.com/@iamleonie/membership" rel="noopener follow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">通过我的推荐链接加入 Medium—Leonie Monigatti</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">阅读 Leonie Monigatti(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">medium.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ks pm"/></div></div></a></div><p id="c115" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="pi">在</em><a class="ae nw" href="https://twitter.com/helloiamleonie" rel="noopener ugc nofollow" target="_blank"><em class="pi">Twitter</em></a><em class="pi"/><a class="ae nw" href="https://www.linkedin.com/in/804250ab/" rel="noopener ugc nofollow" target="_blank"><em class="pi">LinkedIn</em></a><em class="pi"/><a class="ae nw" href="https://www.kaggle.com/iamleonie" rel="noopener ugc nofollow" target="_blank"><em class="pi">Kaggle</em></a><em class="pi">！</em></p><h1 id="dc75" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">参考</h1><p id="69aa" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">[1] Loshchilov，I .，&amp; Hutter，F. (2016)。Sgdr:带有热重启的随机梯度下降。arXiv 预印本 arXiv:1608.03983 。</p><p id="73a2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">[2] Singer，P. &amp; Babakhin，Y. (2022) <a class="ae nw" href="https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH" rel="noopener ugc nofollow" target="_blank">深度迁移学习的实用技巧。2022 年巴黎卡格勒日。</a></p><p id="f476" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">[3]纽约州史密斯和托平(2019 年)。超收敛:使用大学习率非常快速地训练神经网络。在<em class="pi">面向多领域运算应用的人工智能和机器学习</em>(第 11006 卷，第 369–386 页)。SPIE。</p><h1 id="c5b0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">附录</h1><p id="725a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下面是我用来可视化学习率调度程序的代码:</p><pre class="kj kk kl km gt oc od oe bn of og bi"><span id="c8df" class="oh kz it od b be oi oj l ok ol">import torch<br/>from torch.optim.lr_scheduler import StepLR # Import your choice of scheduler here<br/><br/>import matplotlib.pyplot as plt<br/>from matplotlib.ticker import MultipleLocator<br/><br/>LEARNING_RATE = 1e-3<br/>EPOCHS = 4<br/>STEPS_IN_EPOCH = 8<br/><br/># Set model and optimizer<br/>model = torch.nn.Linear(2, 1)<br/>optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)<br/><br/># Define your scheduler here as described above<br/># ...<br/><br/># Get learning rates as each training step<br/>learning_rates = []<br/><br/>for i in range(EPOCHS*STEPS_IN_EPOCH):<br/>    optimizer.step()<br/>    learning_rates.append(optimizer.param_groups[0]["lr"])<br/>    scheduler.step()<br/><br/># Visualize learinig rate scheduler<br/>fig, ax = plt.subplots(1,1, figsize=(10,5))<br/>ax.plot(range(EPOCHS*STEPS_IN_EPOCH), <br/>        learning_rates,<br/>        marker='o', <br/>        color='black')<br/>ax.set_xlim([0, EPOCHS*STEPS_IN_EPOCH])<br/>ax.set_ylim([0, LEARNING_RATE + 0.0001])<br/>ax.set_xlabel('Steps')<br/>ax.set_ylabel('Learning Rate')<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)<br/>ax.xaxis.set_major_locator(MultipleLocator(STEPS_IN_EPOCH))<br/>ax.xaxis.set_minor_locator(MultipleLocator(1))<br/>plt.show()</span></pre></div></div>    
</body>
</html>