<html>
<head>
<title>Linear Regression with Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拥抱脸线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-with-hugging-face-3883fe729324#2022-06-07">https://towardsdatascience.com/linear-regression-with-hugging-face-3883fe729324#2022-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fb6f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是对用变压器实现线性回归分析的可行性的探索</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6ada591d58d6c4f4f952aaf9bce28f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LO3fhCZB0KHy7EUJYWRitQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯科特·韦伯摄影:<a class="ae kv" href="https://www.pexels.com/photo/shapes-and-pattern-2346594/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/shapes-and-pattern-2346594/</a></p></figure><h1 id="cf74" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="b008" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">众所周知，研究人员使用拥抱脸变形器在文本分类和文本聚类问题上花费了大量的努力。您是否想过可以在线性回归项目中利用预先训练好的转换器？拥抱脸API居然支持！我将带您看两个例子，并在最后分享一些见解。</p><h1 id="1e8c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据</h1><p id="c00f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">首先，我找到了一个包含数百篇带分数的批改过的作文的数据集。这些文章是根据每月的选择撰写的，旨在训练巴西高中生参加大学入学考试。数据集有695篇文章，我省略了缺少值的行或<code class="fe mk ml mm mn b">total_score = 0</code>处的记录。经过预处理，100篇文章被删除。</p><ul class=""><li id="6160" class="mo mp iq lq b lr mq lu mr lx ms mb mt mf mu mj mv mw mx my bi translated">下面是数据的链接:<a class="ae kv" href="https://www.kaggle.com/datasets/danlessa/corrected-essays-with-scores-from-uol" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/danlessa/corrected-essays-with-scores-from-uol</a>。</li><li id="ec13" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">下面是许可证的数据:<a class="ae kv" href="https://opendatacommons.org/licenses/odbl/1-0/" rel="noopener ugc nofollow" target="_blank">https://opendatacommons.org/licenses/odbl/1-0/</a>。</li><li id="65cb" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">任务是用他们的短文(长文档)来预测总分。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/dffef4e464d574e8f45849778f85694e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ad17NAEaVGN2RL6iBFuafg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">巴西论文数据</p></figure><p id="1b92" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">第二个数据集是一个女装电子商务数据集，围绕着顾客写的评论。它的九个支持特性提供了一个很好的环境来从多维度解析文本。原始数据集有23，486条客户评论，没有缺失值。</p><ul class=""><li id="2767" class="mo mp iq lq b lr mq lu mr lx ms mb mt mf mu mj mv mw mx my bi translated">下面是数据的链接:<a class="ae kv" href="https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/nica potato/women-ecommerce-clothing-reviews</a>。</li><li id="91ca" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">下面是许可证的数据:<a class="ae kv" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a>。</li><li id="c0e3" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">任务是使用他们的审核意见(简短文档)来预测评级。我听起来更像是一个多类分类问题。但是用这个数据拟合一个线性回归模型仍然是可以接受的，因为我们的目标只是探索拥抱脸API的用法。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/335fd4efc17e4fad61371d4f6ae6588c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ATJgh8K5aurYuHyC-SMPTA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">客户评论数据</p></figure><h1 id="9e9d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">密码</h1><p id="d4cb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">仅供参考，整个项目是在谷歌Colab进行的。在我们开始实验之前，让我们检查一下<code class="fe mk ml mm mn b">GPU</code>是否可用，因为没有<code class="fe mk ml mm mn b">GPU</code>整个过程会非常慢。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/cbefda18031b875aa84daee0e0011ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*it83j2WvCndC9MLuDrnSrQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d566" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">然后，让我们导入我们需要的所有库:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="573b" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">步骤1:准备数据</h2><p id="d219" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这一步中，我们需要分割数据，对文本进行编码，并将数据转换成torch数据，以便进行下游建模。这与我为分类问题准备数据的过程是一样的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="20ec" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated"><strong class="lq ir"> <em class="ny">注意:由于我们训练的是一个线性回归模型，在__getitem__函数中，我们需要将</em> </strong> <code class="fe mk ml mm mn b"><strong class="lq ir"><em class="ny">label</em></strong></code> <strong class="lq ir"> <em class="ny">转换为浮点型。请参考第26行。</em> </strong></p><h2 id="be56" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">步骤2:加载模型</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="e586" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">这是最重要的一步。根据拥抱脸的库，当我们从拥抱脸API加载预训练模型时，将AutoModelForSequenceClassification的<code class="fe mk ml mm mn b">num_labels</code>设置为1将触发线性回归，并自动使用<code class="fe mk ml mm mn b">MSELoss()</code>作为损失函数。你可以在这里找到原剧本<a class="ae kv" href="https://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/models/bert/modeling_bert.py#L1564-L1575" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="d466" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">步骤3:为回归创建计算指标</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="8d19" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated"><code class="fe mk ml mm mn b">logits</code>实际上是我们的预测。但是，在我们的语境下看起来很怪异。想了解更多关于<code class="fe mk ml mm mn b">logits</code>的信息，这里有一个来自哥大的很好的学习资源:<a class="ae kv" href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf" rel="noopener ugc nofollow" target="_blank"> Logit/Probit </a>。尽管默认的损失函数是<code class="fe mk ml mm mn b">MSE</code>，我在这里再次添加损失函数，以验证损失函数是否正确实现。</p><h2 id="3f4a" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">第4步:构建培训师</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="fdae" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">巴西作文分数预测</h2><p id="3f1c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们需要为每个项目从拥抱脸中选择一个合适的预训练模型。由于巴西的论文都是用葡萄牙语写的，我选择了<code class="fe mk ml mm mn b">“neuralmind/bert-large-portuguese-cased” </code>作为基本模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/857d0ddbee61a88f3f8a3140d72f8442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NszdxcHQONSA4rTnZ_JQLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="617b" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">因为文章是长文档，所以平均长度比大多数预训练模型的默认最大长度长；我只是将<code class="fe mk ml mm mn b">max_length </code>设置为512来捕捉最多的信息。<code class="fe mk ml mm mn b">test_size</code>设定为20%。</p><p id="61c4" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">经过几次实验后，我发现这个模型很难捕捉到分数和论文之间的任何线性关系，因为<code class="fe mk ml mm mn b">R2 </code>分数等于负数。因此，我决定对目标变量进行<code class="fe mk ml mm mn b">log10</code>转换。由于模型很难收敛，我运行了100个时期的<code class="fe mk ml mm mn b">Trainer</code>，在第44个时期找到了最好的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="6bae" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">即使最好的<code class="fe mk ml mm mn b">R2</code>只有0.4643(这意味着只有大约46%的总分的<code class="fe mk ml mm mn b">log10</code>可以用短文嵌入来解释。)，我们可以看到一个明显的学习趋势(<code class="fe mk ml mm mn b">R2 = -2.4783</code>表示短文的语义嵌入根本不能解释总分的<code class="fe mk ml mm mn b">log10</code>。).</p><h2 id="4579" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">客户评审项目</h2><p id="5edd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">因为这个项目的所有东西都是用英语写的，所以我简单地选择了<code class="fe mk ml mm mn b">‘bert-base-uncased’</code>作为基础模型。我将前3000条评论作为测试集。从经验上来说，将更大百分比的数据子集化为验证数据可以提高测试数据上的微调模型的性能。因此，我将这个任务的<code class="fe mk ml mm mn b">test_size</code>设置为66%。因为客户评论是短文本数据，所以对于这个项目，我将<code class="fe mk ml mm mn b">max_length</code>设置为126。</p><p id="3490" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">正如我在开始时提到的，这个问题听起来更像一个多类分类问题，而不是一个线性回归问题，我添加了一个<code class="fe mk ml mm mn b">accuracy</code>度量来代替<code class="fe mk ml mm mn b">SMAPE</code>。下面是计算方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="9ee7" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">我训练了五个纪元的模型。最好的<code class="fe mk ml mm mn b">accuracy (0.6694)</code>在第三时段获得，而最好的<code class="fe mk ml mm mn b">R2 (0.6422)</code>和<code class="fe mk ml mm mn b">RMSE (0.6589)</code>在第二时段获得。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="369e" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">现在，我们开始最后一步:让我们在测试数据集上测试微调后的模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="690e" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">输出如下所示。我要说的是，与模型在验证数据集上的表现相比，结果还算不错。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4065" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">完整代码脚本:<a class="ae kv" href="https://github.com/jinhangjiang/Medium_Demo/blob/main/Transformers_Linear_Regression/Transformers_Linear_Regression.ipynb" rel="noopener ugc nofollow" target="_blank">Transformers _ Linear _ regression . ipynb</a></p><h1 id="de34" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">外卖</h1><ol class=""><li id="5727" class="mo mp iq lq b lr ls lu lv lx oa mb ob mf oc mj od mw mx my bi translated">拥抱脸API通过设置<code class="fe mk ml mm mn b">num_labels = 1</code>通过ForSequenceClassification接口支持线性回归。<code class="fe mk ml mm mn b">problem_type </code>将自动设置为<code class="fe mk ml mm mn b">‘regression’</code>。</li><li id="03e4" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj od mw mx my bi translated">由于线性回归是通过分类函数实现的，因此预测有点混乱。根据这个<a class="ae kv" href="https://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/pipelines/text_classification.py#L140-L162" rel="noopener ugc nofollow" target="_blank">文件</a>从抱紧脸(第140行到第162行)，他们会在<code class="fe mk ml mm mn b">problem_type == “multi_label_classification” or num_labels == 1</code>时对输出应用<code class="fe mk ml mm mn b">sigmoid</code>函数。此时我最好的解释是当<code class="fe mk ml mm mn b">problem_type == ‘regression’ </code>，后处理功能被触发来设置<code class="fe mk ml mm mn b">function_to_apply = ClassificationFunction.NONE</code>，所以没有额外的功能应用于输出。</li><li id="a097" class="mo mp iq lq b lr mz lu na lx nb mb nc mf nd mj od mw mx my bi translated">当连续值很大时，模型很难学习目标(参考我在第一个例子中发现的，而我输入的目标值像700，1000等。模型给我的预测是15.6，16.8，…)。在这种情况下，对数转换可能会有所帮助。</li></ol><h1 id="f2c7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">相关阅读</h1><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/sbert-vs-data2vec-on-text-classification-e3c35b19c949"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">SBERT与Data2vec在文本分类上的比较</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">使用两个流行的预训练拥抱脸模型进行文本分类的代码演示</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="or l os ot ou oq ov kp oh"/></div></div></a></div><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/morethansentiments-a-python-library-for-text-quantification-e57ff9d51cd5"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">用于文本量化的Python库</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">帮助研究人员计算样本、冗余、特异性、相对流行率的函数集合…</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="ow l os ot ou oq ov kp oh"/></div></div></a></div><div class="oe of gp gr og oh"><a rel="noopener follow" target="_blank" href="/customer-churn-accuracy-a-4-6-increase-with-feature-engineering-29bcb1b1ee8f"><div class="oi ab fo"><div class="oj ab ok cl cj ol"><h2 class="bd ir gy z fp om fr fs on fu fw ip bi translated">客户流失准确度:通过TextBlob &amp; SBERT提高了4.6%</h2><div class="oo l"><h3 class="bd b gy z fp om fr fs on fu fw dk translated">关于分类问题中的特征工程如何将其预测精度限制提高4.6%的演练。</h3></div><div class="op l"><p class="bd b dl z fp om fr fs on fu fw dk translated">towardsdatascience.com</p></div></div><div class="oq l"><div class="ox l os ot ou oq ov kp oh"/></div></div></a></div><h1 id="fe10" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="b46b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">拥抱脸。(2021).modeling _ Bert . py .<a class="ae kv" href="https://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/models/bert/modeling_bert.py#L1564-L1575" rel="noopener ugc nofollow" target="_blank">https://github . com/hugging face/transformers/blob/7 ae6f 070044 b 0171 a 71 f 3269613 BF 02 FD 9 FCA 6 f 2/src/transformers/models/Bert/modeling _ Bert . py # l 1564-l 1575</a></p><p id="7dc5" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">拥抱脸。(2021).text _ class ification . py .<a class="ae kv" href="https://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/pipelines/text_classification.py#L140-L162" rel="noopener ugc nofollow" target="_blank">https://github . com/hugging face/transformers/blob/7 ae6f 070044 b 0171 a 71 f 3269613 BF 02 FD 9 FCA 6 f 2/src/transformers/pipelines/text _ class ification . py # L140-L162</a></p><p id="7103" class="pw-post-body-paragraph lo lp iq lq b lr mq jr lt lu mr ju lw lx nf lz ma mb ng md me mf nh mh mi mj ij bi translated">拉爪哇研发(2022)。使用BERT和变压器的文本输入回归。<a class="ae kv" href="https://lajavaness.medium.com/regression-with-text-input-using-bert-and-transformers-71c155034b13" rel="noopener">https://lajavaness . medium . com/regression-with-text-input-using-Bert-and-transformers-71c 155034 B13</a></p></div></div>    
</body>
</html>