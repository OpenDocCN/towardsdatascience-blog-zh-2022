<html>
<head>
<title>Experiences with SOTA Semi-Supervised Learning NLP Algorithms on different public datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不同公共数据集上SOTA半监督学习NLP算法的经验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/experiences-with-sota-semi-supervised-learning-nlp-algorithms-on-different-public-datasets-a368132ed6f#2022-05-31">https://towardsdatascience.com/experiences-with-sota-semi-supervised-learning-nlp-algorithms-on-different-public-datasets-a368132ed6f#2022-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="527e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在NLP中，我们如何处理少量的训练数据？半监督学习——拯救我们！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/462b760dba024d09d206019eabd3b148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vmkYxdm504B8h_MwZWuKQQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:此处见<a class="ae kv" rel="noopener" target="_blank" href="/explained-deep-learning-in-tensorflow-chapter-0-acae8112a98">。</a></p></figure><p id="64c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">半监督学习是机器学习领域中一个活跃的研究领域。它通常用于通过利用大量未标记的数据(即输入或特征可用但基础事实或实际输出值未知的观察值)来提高监督学习问题的概化能力(即基于提供的输入和基础事实或每个观察值的实际输出值来训练模型)。</p><h2 id="16ab" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">实验设置</h2><p id="0ea4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们有以下两个不同的数据集(从分类的角度来看，以复杂性递增的顺序排列):</p><p id="c226" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB评论</a>数据:由斯坦福主持，这是一个电影评论的情感(二元-正面和负面)分类数据集。更多详情请参考<a class="ae kv" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="5cbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">20Newsgroup 数据集:这是一个多类分类数据集，其中每个观察都是一篇新闻文章，并标有一个新闻主题(政治、体育、宗教等)。这些数据也可以通过开源库<a class="ae kv" href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>获得。关于这个数据集的更多细节可以在<a class="ae kv" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">这里</a>阅读。</p><p id="0572" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经进行了实验，以观察这些算法在低、中和高数据量下对未标记数据的表现。我们已经了解了sklearn的SelfTrainingClassifier，它作为一个包装器函数来支持任何基于sklearn的算法的自我训练。我们已经看到了简单逻辑回归模型和ANN(人工神经网络)模型的自我训练前后结果的比较。</p><p id="fbe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文的第<a class="ae kv" href="https://medium.com/@naveen.rathani/improve-your-models-performance-with-unlabeled-data-d6e78a57fadb" rel="noopener">部分-1 </a>中，对该领域中使用的一些初始方法进行了彻底的讨论——随后是更复杂的第<a class="ae kv" href="https://medium.com/@naveen.rathani/improving-model-performance-using-semi-supervised-wrapper-methods-31d8712ca20" rel="noopener">部分-2 </a>部分，您可以从中了解如何在实践中应用这些方法。</p><p id="a07b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一部分，我们将介绍一些最新的半监督学习(SSL)方法，以及它们在我们的实验中的表现。我们先来看看一个先进的NLP模型——比如LSTM/CNN(是的，你没看错！！CNN可以是一个很好的NLP模型。下一节将详细介绍)或像BERT这样的先进(SOTA)模型在半监督学习(SSL)环境中执行。然后，我们继续讨论这一领域的最新方法——UDA和mixt，它们采用了一些新颖的思想来解决SSL问题。最后，我们对这些算法进行了详细的比较，并提出了进一步的研究方向。</p><p id="7280" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧！</p><h1 id="90e8" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">LSTM</h1><p id="0bc4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们已经看到LSTMs从一开始就扰乱了NLP空间。因此，我们认为使用LSTM将是下一个合乎逻辑的步骤。<a class="ae kv" rel="noopener" target="_blank" href="/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740">这篇</a>文章很好地提供了LSTMs和rnn的简史和用法。</p><p id="2dfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于LSTM是一个序列模型，tf-idf嵌入帮助不大。因此，我们决定使用<a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>嵌入，它可以作为顺序输入提供给模型。我们使用了一个相当简单的架构来创建LSTM模型。它由三个LSTM层组成，每个层后面都有一个下降层。最后，我们有一个密集层，后面是输出层。</p><p id="3506" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下代码显示了该架构的外观:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="d008" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，本系列中的所有代码都可以在这个<a class="ae kv" href="https://github.com/abhinivesh-s/Semi-Supervised-Text" rel="noopener ugc nofollow" target="_blank"> github </a>页面上找到。</p><p id="0032" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">官方<code class="fe nd ne nf ng b"><a class="ae kv" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.semi_supervised" rel="noopener ugc nofollow" target="_blank">sklearn.semi_supervised</a></code>。SelfTrainingClassifier不适用于模型中包含功能模块的TensorFlow组件。在这种情况下，我们将使用scikeras.wrappers中的KerasClassifier，它的工作方式与SelfTrainingClassifier完全相同，并且支持TensorFlow模型的功能模块。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="ae4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在这一点上没有包括LSTM的结果，因为即使是相对较浅的架构，运行它也需要相当长的时间，甚至性能也无法与CNN模型相提并论——这是我们将在下一节中看到的。</p><h1 id="bdff" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">美国有线新闻网；卷积神经网络</h1><p id="cd06" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">现在，像CNN这样主要用于计算机视觉的模型在我们的NLP实验中做什么呢？人们已经尝试使用这种方法，并取得了巨大的成功。参见<a class="ae kv" href="https://aclanthology.org/S15-2079.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>论文，了解关于此类工作的更多细节。</p><p id="5493" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们的问题，我们认为尝试CNN是值得的，原因有二:</p><ol class=""><li id="0b62" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">与CNN相比，LSTMs执行起来相对较慢。</li><li id="c494" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">在许多使用案例中，CNN的性能几乎与LSTMs一样好，甚至更好。</li></ol><p id="a236" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用相同的手套嵌入和训练我们的CNN。参见<a class="ae kv" rel="noopener" target="_blank" href="/convolutional-neural-network-in-natural-language-processing-96d67f91275c">这篇</a>综合文章，了解如何对文本数据应用CNN的更多细节。</p><p id="8c33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一段代码可以帮助你理解我们的CNN架构:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="ed79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用了我们在LSTM实验中使用的来自scikeras.wrappers的相同的KerasClassifier。</p><p id="90eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经看到CNN的表现至少和LSTM一样好，有时甚至更好！运行时间也大大减少了。</p><p id="68d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/cc0633ecf4fefba1962d6832c675a35a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3NOGL5eohT3wxIRT4ZiCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN关于IMDB数据</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/23d9c1a1ef5408117ec339ba0949c5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WF8WYVgQB2tdV2MPZqoZow.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CNN新闻组数据</p></figure><h1 id="9093" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">伯特</h1><p id="feaa" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果没有试用BERT，这些天的任何NLP模型构建练习都是不完整的。BERT是一个预训练的语言模型，由谷歌在2018年推出，从那时起，它扰乱了NLP空间。它使用一组具有编码器-解码器架构的转换器，并使用一种称为<strong class="ky ir">掩蔽语言模型(MLM) </strong>的新技术在大量数据上进行训练。如果这些听起来不熟悉，你可以通过<a class="ae kv" href="https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c" rel="noopener">这篇</a>文章来更深入地理解BERT背后的思想和机制。</p><p id="b7f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在BERT之后，出现了许多预训练模型的变体，它们具有不同的架构，并使用不同的技术进行训练。总而言之，像BERT这样的预训练模型携带了大量关于它们被训练的语料库的信息和知识。这些模型唯一的缺点是它们需要大量的GPU形式的计算能力。</p><p id="2f10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们不需要为BERT单独使用任何单词嵌入，因为它产生的BERT嵌入在上下文嵌入的意义上优于GloVe、单词到vec和其他嵌入..<em class="nx">即</em>根据出现的上下文，同一个单词可以有不同的嵌入。例如:</p><ol class=""><li id="8e3a" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">苹果是我最喜欢的水果。</li><li id="c192" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">我买了一部<strong class="ky ir"> <em class="nx">苹果</em> </strong> iPhone。</li></ol><p id="5637" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面两个句子中的单词“apple”在不同的上下文中会有不同的BERT嵌入。</p><p id="6858" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于BERT是一个已经预先训练好的模型，我们不再从头开始训练它。但是我们会微调它。微调BERT是一种受监督的方法，包括传递一些带标签的训练示例，之后，它将准备好为我们的测试数据给出标签。</p><p id="e2f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[出于本文的目的，我们在BERT的上下文中交替使用训练和微调这两个词。]</p><p id="ebca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于训练/微调的目的，我们使用了一个非常用户友好的BERT API，名为<a class="ae kv" href="https://github.com/charles9n/bert-sklearn" rel="noopener ugc nofollow" target="_blank"> sklearn-bert </a>。这是一个无附加条件的API，我们可以像使用其他sklearn模型一样使用它。这就是我们安装sklearn-bert的方法。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="2bc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段显示了如何训练sklearn-bert:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="3305" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们的计算限制，我们将序列长度和batch_size分别限制为128和16。我们使用了最大序列长度为512的“bert-base-uncased”模型。我们可以只做一个模型。fit()然后瞧！我们已经完成了微调。我们可以做一个model.predict()来获得预测的标签。</p><p id="7b74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种API的优点是，我们不需要手动对BERT进行任何类型的标记化或最后一层添加——如果我们使用PyTorch或TensorFlow训练BERT，就必须这样做。</p><p id="eab4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们如何在这个BERT模型上做SSL呢？因为这是一个sklearn模型，所以我们认为可以使用以前使用过的来自scikeras.wrappers的相同的KerasClassifier。但不幸的是，这并没有奏效。我们认为这是因为BertClassifier没有实现我们的KerasClassifier需要的所有sklearn方法。因此，我们必须自己实现SSL逻辑——这相当简单。查看下面的代码片段，了解我们的BERT SSL实现:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="f5e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们快速回顾一下我们是如何实现SSL算法的。</p><p id="deab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nx"> X_u </em>表示未标记的数据集。迭代次数— <em class="nx"> num_iter </em>最多是<em class="nx"> X_u </em>和<em class="nx"> kbest </em>值的比值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ny nc l"/></div></figure><p id="03e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们做一个model.fit()，然后使用模型进行预测。然后，我们选择概率最大的预测作为最终的模型输出。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz nc l"/></div></figure><p id="9adf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们基于<em class="nx"> n_to_select </em>对其进行划分，这是<em class="nx"> kbest </em>值和<em class="nx"> X_u. </em>大小中较小的一个，它与已经存在的<em class="nx"> new_X </em>数据集连接在一起。最后一步，不管<em class="nx"> kbest </em>值是多少，我们都将剩下的内容(在<em class="nx">if not _ selected _ update = = ' yes ':</em>子句中完成)添加到数据中。</p><p id="4603" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是我们使用SSL和BERT获得的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/87f69f759b056be5d6d46596b5295ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFGW7KKmf6TtX9SYk19VJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">IMDB数据上的BERT</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/04f3288191998c8c700394f1e4d6a8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XhXzHgk9K7ExtnIILugXg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">新闻组数据上的BERT</p></figure><p id="352c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于内存和计算资源的限制，我们无法使用BERT来处理新闻组数据集中更多的标记数据。</p><h1 id="0c69" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">无监督数据增强(UDA)</h1><p id="5442" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">到目前为止，我们所做的一切只涉及改变模型和尝试不同的架构，以提高性能。但是现在，我们从根本上改变了处理这个问题的方式。</p><p id="2ed1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank"> UDA </a>中，我们使用数据扩充和一致性训练来进一步提高现有性能，而不是相同的SSL。我们现在将简要讨论UDA是如何工作的，特别是在NLP的环境中。</p><p id="9db2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无监督的数据增强试图通过使用一组数据增强来有效地增加标记数据的数量，如反向翻译、单词替换等。</p><h2 id="2c6a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">反向翻译</h2><p id="16a0" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">反向翻译把一个英语句子翻译成不同的语言——比如说，俄语、德语、汉语等等，然后再把它们翻译回英语，这样我们就会得到一个重新措辞的句子，意思相同，因此标签也相同。这些回译可以通过使用任何<em class="nx">机器翻译</em>模型作为服务来完成。</p><pre class="kg kh ki kj gt oa ng ob oc aw od bi"><span id="8034" class="ls lt iq ng b gy oe of l og oh">Sentence in English — S, Rephrased sentence — S”.</span><span id="5406" class="ls lt iq ng b gy oi of l og oh">S’ = EnglishToGerman(S);</span><span id="2407" class="ls lt iq ng b gy oi of l og oh">S” = GermanToEnglish(S’).</span></pre><p id="0b5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以控制回译的质量，得到不同种类的重组句子，它们的结构和措辞略有不同，但意思相似。在下图中，我们可以看到一个句子的三种不同的回译。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/74945b435fc9903f232f26b9da8f8e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_rNNLHcr5pB2ZZrdbRRjg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个反向翻译的例子。【<a class="ae kv" href="https://arxiv.org/abs/1812.00677" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><h2 id="8281" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">单词替换</h2><p id="bf77" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">单词替换是另一种数据扩充方法。我们通过从整个词汇表中取样单词来替换具有低TF-IDF分数的句子中的单词。这可能有助于生成多样且有效的示例。</p><p id="e7e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">UDA使用标记数据和扩充的未标记数据进行训练。我们可以使用任何通用的NLP模型来执行UDA，只需稍微改变损失函数。下图显示了损失函数是如何调整的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/4d0970a4d6cf3f5102da420d371582b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K-yoV5wwG_EhTVoz.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">UDA [ <a class="ae kv" href="https://arxiv.org/abs/1812.00677" rel="noopener ugc nofollow" target="_blank">图像源</a>概述</p></figure><p id="2221" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">损失函数有两个部分:有标记数据的监督交叉熵损失，这是传统的损失函数，以及无标记数据的<strong class="ky ir"> <em class="nx">无监督一致性损失</em> </strong>。</p><p id="50a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直观地说，无监督的一致性损失做了一件非常简单的事情:它迫使所有增强的例子具有相同的标签。这似乎是合理的，因为我们知道，我们所做的扩充给了我们具有相同含义的重组句子，因此，相同的标签。</p><p id="d33f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有一个稍微更新的UDA版本——叫做mixt，这就是我们已经实现的。现在就来看看吧。</p><h1 id="51ad" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">混合文本</h1><p id="f70f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/2004.12239.pdf" rel="noopener ugc nofollow" target="_blank">mixt</a>最初做的正是我们在UDA中讨论的事情——接受标记数据和未标记数据，进行扩充，并预测未标记数据的标签。但是在应用交叉熵和一致性损失之前，它执行了另一个称为<strong class="ky ir"> TMix的增强。</strong></p><h2 id="e95d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">TMix</h2><p id="c9be" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">TMix提供了一种扩充文本数据的通用方法，因此可以独立地应用于任何下游任务。TMix在文本隐藏空间做— <em class="nx">插值。</em>已经表明，从两个隐藏向量的插值中解码生成具有两个原始句子的混合含义的新句子。</p><p id="bc17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于具有L层的编码器，我们选择在第m层混合隐藏表示，其中m ∈ [0，L]。简单来说，TMix取两个带有标签<em class="nx"> y和y` </em>的文本样本<em class="nx"> x和x` </em>，将它们在层<em class="nx"> m </em>的隐藏状态表示<em class="nx"> h和h` </em>混合到<em class="nx">h"</em>中，然后继续向前传递以预测混合标签<em class="nx">y"</em>。混合隐藏状态(带有参数λ)的情况如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok nc l"/></div></figure><p id="4ff9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实验表明，一些特定的BERT层{3，4，6，7，9，12}最具代表性。基于此，mixt的作者选择了一些随机子集，并且{7，9，12}给出了最佳结果，因此，这就是我们所使用的。</p><p id="0dcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行MixText要求我们按照他们的<a class="ae kv" href="https://github.com/GT-SALT/MixText" rel="noopener ugc nofollow" target="_blank"> GitHub页面上的说明进行操作。</a>我们必须稍微修改code/read_data.py文件中的代码，以便读取我们的IMDB/新闻组数据格式。</p><p id="1e4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/671ef3de3fb482acab9f3ab679b5ebeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e92oSm2oc6PMwGsvyKYT5Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">IMDB数据上的mixt与BERT</p></figure><h1 id="a450" class="mq lt iq bd lu mr ms mt lx mu mv mw ma jw mx jx md jz my ka mg kc mz kd mj na bi translated">最终比较</h1><p id="04f9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们可以看到，BERT和CNN在SSL设置中表现得相当好。由于没有明确的赢家，我们可以根据手头的任务来测试和选择它们。</p><p id="37d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们固定的序列长度下，我们无法观察到mixt和BERT+SSL之间的任何显著差异。作者提到的结果可能是因为在更好的计算资源的帮助下，使用了更长的序列长度和更有效和多样的扩增。</p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><p id="46c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">包括本文在内的3部分系列是由Pilani-IN和热情的NLP数据科学家Sreepada Abhinivesh 共同完成的，Naveen Rathani 是一位拥有BITS硕士学位的应用机器学习专家和数据科学爱好者。</p></div></div>    
</body>
</html>