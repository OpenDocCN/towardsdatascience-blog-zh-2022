<html>
<head>
<title>Light on Math ML: Intuitive Guide to Matrix Factorization (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数学之光ML:矩阵分解直观指南(第1部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-matrix-factorization-bee5af0c01aa#2022-07-20">https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-matrix-factorization-bee5af0c01aa#2022-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3360" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你永远不会害怕在你的生活中看到一个据称令人生畏的矩阵分解方程！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cb47e2195634162fc837ce63f526bb66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XjPC4Nu9NIdmpweBWqZAQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我要让矩阵分解变得像这个巧克力棒一样甜蜜(图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2202141" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/wikimediaimages-1185597/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2202141" rel="noopener ugc nofollow" target="_blank"> WikimediaImages </a></p></figure><p id="7344" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">矩阵分解码</strong>:<a class="ae ky" href="https://github.com/thushv89/exercises_thushv_dot_com/blob/master/matrix_factorization_light_on_math_ml/matrix_factorization_part_1.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a></p><p id="2a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将了解矩阵分解，这是许多经典机器学习方法的基础。本文将重点解释矩阵分解(MF)的实际应用(带有代码示例)以及支持它的直觉。你有没有想过<a class="ae ky" href="https://academic.oup.com/bioinformatics/article/38/5/1369/6454947" rel="noopener ugc nofollow" target="_blank">矩阵分解可能被用于药物再利用</a> ❓</p><p id="6935" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你和我一样(希望不是)，在这篇文章结束时，你会后悔在你的本科代数课程中淡化矩阵分解，因为你从未想过你会用到它。</p><p id="a1a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要访问本系列中我以前的文章，请使用以下信件。</p><p id="4312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-ml-attention-with-keras-dc8dbc1fad39"><strong class="lb iu">A</strong></a><strong class="lb iu">B</strong><a class="ae ky" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">C</strong></a><strong class="lb iu"/><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><strong class="lb iu">D</strong></a><strong class="lb iu">* E F G H I J</strong><a class="ae ky" href="http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">K</strong></a><strong class="lb iu"/><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"><strong class="lb iu">L</strong></a><strong class="lb iu">* M</strong><a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee"><strong class="lb iu">N</strong></a><strong class="lb iu">O P Q</strong></p><h1 id="b3ab" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">序幕</h1><p id="d75a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我在大学时，数学从来都不是我的强项。我从来不明白，为什么曲线给定点处的切线角度很重要，或者为什么曲线上的积分导致曲线下的表面积很重要，或者为什么矩阵分解很重要？当然，我们在数字领域看到的许多数据都可以用矩阵(或一般的张量)来表示。但那对我来说不再有意义了。像这样的问题，如果你已经有了完整的基本事实矩阵，你为什么要把它分成两个？在代数课上困扰着我。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/175f98714dab24fc45f77215766dcebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIGryT54_8yoQelETRh89Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我大学时代的噩梦(图片由作者提供)</p></figure><p id="ad74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后是研究生学习。我正在攻读深度学习的博士学位，这是一场忽略矩阵分解等主题的完美风暴，因为这是深度学习文献中很少出现的东西。</p><p id="f5e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目前，作为一名从事推荐算法工作的MLE，我立刻有了迫切需要的顿悟。“啊哈，这就是为什么你把矩阵分解成两个更小的矩阵”。</p><p id="ced1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回首10年前那个天真的本科生，我对自己没有给予更多的关注感到深深的遗憾。为了补偿我自己，我将向你展示很酷的矩阵分解和它的概念的实际应用。我希望这将是那些可能在淹没数学和粗糙代数的沉船中的人的灯塔。</p><h1 id="d47d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么是矩阵？</h1><p id="d899" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">早在矩阵分解出现之前…矩阵。我们在数据科学中看到和使用的许多数据都可以用矩阵来表示。这里有几个例子。</p><ul class=""><li id="cf99" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">🖼灰度图像是按行(高度)和列(宽度)组织的像素矩阵</li><li id="72f9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">📹视频可以表示为矩阵，其行显示帧数，列是展开为1D向量的图像</li><li id="52e1" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">文档的语料库可以表示为矩阵，其中行是文档，列是语料库中存在的所有术语(即词汇)</li><li id="8229" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">👟项目评级(如电影或产品)可以表示为具有行(每个用户一个)和列(每个项目一个)的矩阵。</li></ul><p id="59e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这足以令人信服地理解我们每天遇到的数据中矩阵的广泛传播性质。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/39c62bbedd7fd6324b9746c4d30afdcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYafWiSsDrBM9vI1bimTsA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们周围的矩阵——我们处理的几乎所有数据都是矩阵(图片由作者提供)</p></figure><h1 id="ae28" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么要矩阵分解？</h1><p id="dd92" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们已经建立了矩阵没有缺席，我们的下一个目标是理解讨论中最重要的问题。</p><blockquote class="ni"><p id="bed7" class="nj nk it bd nl nm nn no np nq nr lu dk translated">为什么我们需要分解(即因式分解)矩阵？</p></blockquote><p id="e513" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">我无法给出一个通用的答案，因为MF的具体优势通常取决于应用程序和算法本身。所以让我激励我们如下。</p><p id="ea29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我上面概述的这些矩阵并不十分有用。换句话说，把它们想象成数据中底层模式之上的一层——这是我们通常感兴趣去发现的。就像你切割和打磨钻石一样🔹从地球上挖掘出来以增加它的吸引力和价值，这些观察到的矩阵需要以特定的方式被操纵以到达它的甜蜜中心。矩阵分解是这个旅程的容器。</p><blockquote class="ni"><p id="0406" class="nj nk it bd nl nm nn no np nq nr lu dk translated">我们观察到的数据矩阵包含生成这些数据的源的模式/行为</p></blockquote><p id="f474" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">更详细地说，通常数据都有内在的模式。例如，如果图像的一部分被遮挡，您仍然可以使用周围的信息来推断缺失的内容。另一个例子是，如果文档中的一些单词被修改/删除，你仍然可以理解该文档所传达的意思或信息。</p><p id="7c45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么可以这样做？这是因为我们看到的数据中有潜在的模式。图像上的像素不是随机的。文档中的文本不是随机的。通过矩阵分解，我们正试图通过我们观察到的数据/噪音来揭示这种模式。你可能会问，为什么学习模式很重要？获得这些模式是我们开发的决策支持系统的生命保障。毕竟，学习模式是机器学习模型的工作。</p><h1 id="9e12" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">MF是怎么做到的？</h1><p id="c16f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将数据分成多个部分来揭示数据中的基本模式，这听起来有点不可思议。事实证明，将数据投射到一个潜在空间会迫使生成的矩阵学习模式，分离出噪音。</p><p id="481f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多不同的MF算法，它们利用所得的分解矩阵的各种性质。有些可能会将值约束为非负，而有些则强制稀疏。因此，你使用的算法将决定你得到的结果的性质。在我们去它的中心的旅途中，我们会看到相当多的这样的东西！</p><h1 id="240c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">给我看看已经很酷的东西:MF的应用</h1><p id="90c3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在你可能厌倦了听我没完没了地谈论MF。是时候看看这些算法是如何运行的了。</p><h2 id="4317" class="nx lw it bd lx ny nz dn mb oa ob dp mf li oc od mh lm oe of mj lq og oh ml oi bi translated">图像压缩</h2><p id="fc0a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们看到的第一个例子是图像压缩。矩阵分解可用于存储稍后重建图像所需的重要图像内容(占用的内存更少)。在这里，我们将学习一种叫做奇异值分解(SVD)的技术。SVD背后的思想是用三个矩阵的乘法来表示矩阵<strong class="lb iu">A</strong>；<strong class="lb iu"> U </strong>、<strong class="lb iu">σ</strong>和<strong class="lb iu"> V </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e30db67f384c57031261c487bb8b0561.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*oopy_fWqYOyA6v1ggIT3EA.png"/></div></figure><p id="ddeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里A是<code class="fe ok ol om on b">n x m</code>，U是<code class="fe ok ol om on b">n x n</code>，σ是<code class="fe ok ol om on b">n x m</code>，V是<code class="fe ok ol om on b">m x m</code>。</p><p id="93f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矩阵σ是对角线矩阵，包含对角线上的奇异值，是SVD的重要副产品。这些奇异值表示<strong class="lb iu"> U </strong>和<strong class="lb iu"> V </strong>的每一行/列捕获了多少方差(或奇异向量)。捕捉到的差异越多，重建就越好。你可能认识到<a class="ae ky" href="https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" rel="noopener">这与PCA </a>背后的原理相同。</p><blockquote class="ni"><p id="3865" class="nj nk it bd nl nm nn no np nq nr lu dk translated">奇异值的平方与每个奇异向量捕获的信息量(即方差)成比例</p></blockquote><p id="46ca" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">另一件很棒的事情是奇异值是按降序排列的。这意味着要得到最重要的奇异值，你只需将矩阵切割成包含那么多奇异值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/1ffccb6572ed0cb26cee887c9330c19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQrImGuXkxN_I_mHtdUsxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SVD如何用于图像压缩(图片由作者提供)</p></figure><p id="16d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">奇异值分解有不同的变体。只得到最大奇异值而不计算全分解称为截断奇异值分解。一种更有效的近似变体称为<a class="ae ky" href="https://arxiv.org/pdf/0909.4061.pdf" rel="noopener ugc nofollow" target="_blank">随机化SVD </a>。这是我们用不同数量的随机奇异值分解得到的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/0cace9be26fddadc5e3ec065060d3d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYKUygewbP8BmPE_Pgkbvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用不同数量的奇异分量进行图像压缩(图片由作者提供)</p></figure><p id="47bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是在<code class="fe ok ol om on b">scikit-learn</code>中的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="fca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为<code class="fe ok ol om on b">512x512</code>图像计算<code class="fe ok ol om on b">k=50</code>的压缩率。</p><ul class=""><li id="9e0d" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">原始图像= 512x512 = 262144像素</li><li id="f5ad" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">重建= 512x10 + 10x10 + 10x512 = 10340像素(仅为原始图像的4%)</li></ul><p id="9b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不错！我们刚刚重建了一个近似值，它只占用了原来的4%的内存。你可以在这里找到完整的端到端代码<a class="ae ky" href="https://github.com/thushv89/exercises_thushv_dot_com/blob/master/matrix_factorization_light_on_math_ml/matrix_factorization_part_1.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="a997" class="nx lw it bd lx ny nz dn mb oa ob dp mf li oc od mh lm oe of mj lq og oh ml oi bi translated">前景检测</h2><p id="97c1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，我们进行前景检测。如果你认为压缩图像很酷，等着看这个吧！你可以用矩阵分解法来区分背景和前景🤯<strong class="lb iu">*插入心灵爆炸GIF* </strong>🤯。想想在视频监控中你能做的惊人的事情。你可以从视频中检测到人或车。让我们看看如何做到这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/009195498b84a79ff8351719a01c712b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*OVd99xWB2jMnIUvMB9SBsg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们将使用的视频(来自<a class="ae ky" href="http://backgroundmodelschallenge.eu/" rel="noopener ugc nofollow" target="_blank">http://backgroundmodelschallenge.eu/</a></p></figure><p id="7eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将视频表示为大小为<code class="fe ok ol om on b">l x f</code>的矩阵，其中<code class="fe ok ol om on b">l</code>是单帧的长度(即展开为1D矢量的灰度帧的高度和宽度)，而<code class="fe ok ol om on b">f</code>是视频中的帧数。在我们的例子中，我们有一个大小为<code class="fe ok ol om on b">76800 x 794</code>的矩阵(每个图像是320x240=76800像素)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将视频作为矩阵加载</p></figure><p id="f1f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以画出来，看看它是什么样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/c6e31844dc03c33d04492d502b8f557b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FP2QlhedFsAUNP2zZwDpYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">以矩阵形式组织的视频中的帧—这是实际矩阵的转置版本(即轴翻转)，以更好地利用空间(图片由作者提供)</p></figure><p id="0e60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你看到的曲线是视频中人物的动作。对于这项任务，我们将使用一种不同的矩阵分解技术，称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Robust_principal_component_analysis" rel="noopener ugc nofollow" target="_blank">鲁棒PCA </a> (rPCA)。想法是分解一个给定的矩阵<strong class="lb iu"> M </strong>如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/4897df2248e46fc6adc1274454904c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*nhFfdBTKi4jdSXLw27hPZQ.png"/></div></figure><p id="7e03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<strong class="lb iu"> L </strong>是低秩近似，<strong class="lb iu"> S </strong>是稀疏矩阵。霍勒普。🛑，你刚才不是说你不会用令人麻木的数学来恐吓我们吗？让我们直观地理解这意味着什么。</p><p id="1ab4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ov">矩阵的秩</em>称为线性无关列的个数。如果一列不能作为矩阵中其他列的线性变换导出，则该列是<em class="ov">线性无关的</em>。为什么这很重要？因为矩阵中的线性<em class="ov">相关</em>列越多，冗余信息就越多——因为它们是从独立的列中得到的。如果考虑来自CCTV摄像机的视频馈送，背景是静态的，因此包含非常少的信息(或低熵)。因此，我们可以用少量线性无关的列来表示背景。换句话说，如果我将M的内容表示为一个低秩矩阵<strong class="lb iu"> L </strong>，我将在其中捕获<strong class="lb iu"> M </strong>中存在的背景。</p><p id="39d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">附注:在奇异值分解中，非零奇异值的数量代表矩阵的秩。</p><blockquote class="ni"><p id="7631" class="nj nk it bd nl nm nn no np nq nr lu dk translated">低秩矩阵的一个例子是表示为矩阵的视频馈送的静态背景</p></blockquote><p id="1a87" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">稀疏矩阵呢。那个更有意义。在视频馈送中，静态背景将包含大部分数据(数据，而不是信息)。剩余的稀疏信息属于前景，因为前景通常在视频中占据很小的空间。因此，如果我试图迫使<strong class="lb iu"> M </strong>成为稀疏矩阵<strong class="lb iu"> S </strong>，我可能会在<strong class="lb iu"> S </strong>中捕捉前景运动。另一种思考方式是，<strong class="lb iu"> S </strong>捕捉数据中的异常值！</p><p id="26da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，将L和S相加应该可以得到原始视频，这就是鲁棒PCA方程的含义。现在说起来容易做起来难！我们如何确保这些矩阵的这些性质。这超出了本文的范围。但是为了给你一点提示，你可以使用像主成分<a class="ae ky" href="https://en.wikipedia.org/wiki/Augmented_Lagrangian_method#Alternating_direction_method_of_multipliers" rel="noopener ugc nofollow" target="_blank">追踪或者交替方向法</a>这样的算法。对于rPCA，我使用并修改了最初在找到的<a class="ae ky" href="https://freshprinceofstandarderror.com/ai/robust-principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">代码。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="8e35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在高层次上，让我们了解如何针对这些特殊属性进行优化。</p><p id="f397" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了最小化<strong class="lb iu"> L </strong>的秩，您可以使用核范数<strong class="lb iu">L</strong>T26】，它是秩的代理。另一方面，如果你从事过线性回归，你可能记得L1范数鼓励权重矩阵的稀疏性。他们在这里使用相同的原理，使<strong class="lb iu"> S </strong>成为稀疏矩阵。</p><p id="87a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看原著第250帧<code class="fe ok ol om on b">frames_t</code>、<code class="fe ok ol om on b">L</code>、<code class="fe ok ol om on b">S</code>给了我们以下三个支线剧情图(按顺序)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8a3b92565ff77dc8082952d3eff8e43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvMLaX2OZeNEj1ozVwUJRg.png"/></div></div></figure><p id="2822" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是视频。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/07723e4b97558a3791055a04ecff2e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*KMtNNaZPTyLoksZolCYT3g.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为所有帧提取的前景(图片由作者提供)</p></figure><p id="3abc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的端到端代码可在<a class="ae ky" href="https://github.com/thushv89/exercises_thushv_dot_com/blob/master/matrix_factorization_light_on_math_ml/matrix_factorization_part_1.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h2 id="de5e" class="nx lw it bd lx ny nz dn mb oa ob dp mf li oc od mh lm oe of mj lq og oh ml oi bi translated">很酷的把戏！让我们测试一下我们的知识</h2><p id="4c6f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">到目前为止，我们已经学习了两种技术；SVD和rPCA。我们还了解到，在以矩阵组织的视频中，低等级的组件捕获背景。</p><p id="a88c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，在奇异值分解中有一个有趣的性质，秩等于非零奇异值的个数。那么，如果我们用很少的分量执行截断SVD(在我们的例子中是随机化SVD)并重建图像，会发生什么呢？</p><p id="d0cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们应该能分离出背景。然后提取前景是琐碎的。只需从原始图像像素中减去背景像素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/10dbb161ab33a1a90f0ed41ce44adb98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntfQ5C-PiA5e4HmymLwcpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SVD前景检测(图片由作者提供)</p></figure><p id="dc79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来不错。我们可以看到，增加组件的数量会使重建的前景更加广阔。这是直觉多么强大的一个很好的例子，它让你自信而有效地结合不同的技术来得到你需要的东西！</p><p id="d4fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们为什么需要rPCA呢？这导致了更多的技术细节，超出了本介绍的范围。简而言之，不同的算法有不同的优缺点。它们并不总是在每一个用例中得到强调。我可以指出的一个明显的区别是，rPCA对于数据中的异常值更健壮。</p><h1 id="24c3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">现在再见</h1><p id="8318" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这里我要结束我们第一部分的讨论。我们首先理解了为什么矩阵是重要的，这也解释了为什么矩阵分解是重要的。然后我们讨论了计算机视觉中的两个应用；图像压缩和前景检测。我们讨论了两种算法，SVD和rPCA，甚至做了一个很酷的技巧，将SVD用于前景检测。</p><p id="fdf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章中，我们将探索机器学习领域的其他部分，矩阵分解已经统治了这个领域！我们会讨论，</p><ul class=""><li id="20a1" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">自然语言处理中如何使用矩阵分解来学习概念</li><li id="747f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">矩阵分解如何用于项目推荐</li></ul><p id="f9fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那之前，就再见了！</p><h1 id="b5ff" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">下一篇文章</h1><p id="6fd7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[TBA]第2部分—矩阵分解</p><h1 id="0b2c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">进一步阅读</h1><p id="caf5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[1] <a class="ae ky" href="https://github.com/fastai/numerical-linear-algebra-v2/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> fast.ai的线性代数课程</a>——非常全面地介绍了矩阵分解和Python中的大量应用</p><p id="8d3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" href="http://thecooldata.com/2018/08/matrix-decomposition-image-compression-and-video-background-removal-using-r-part-1/" rel="noopener ugc nofollow" target="_blank">奇异值分解</a></p><p id="8c15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" rel="noopener">奇异值分解和主成分分析的相似性</a></p><p id="e8d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] <a class="ae ky" href="https://freshprinceofstandarderror.com/ai/robust-principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">健壮的PCA </a></p></div></div>    
</body>
</html>