<html>
<head>
<title>Multilingual Speech Translation with Multi-Phase Pretraining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有多阶段预训练的多语言语音翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multilingual-speech-translation-with-multi-phase-pretraining-305d642b8a66#2022-04-04">https://towardsdatascience.com/multilingual-speech-translation-with-multi-phase-pretraining-305d642b8a66#2022-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fd7e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何将不同模态的预训练模型与单语数据混合成强大的多语言翻译模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fb78c1e9980afac1e7402ac2a5887af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KmdExHOK4jq1IhHf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@austindistel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Austin Distel </a>拍摄的照片</p></figure><p id="e069" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:这是本系列的第三篇文章。之前的文章解释了如何在双语和多语言环境下将海量预训练用于机器翻译:<br/>第1部分:<a class="ae ky" rel="noopener" target="_blank" href="/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432">用于双语机器翻译的海量预训练</a> <br/>第2部分:<a class="ae ky" rel="noopener" target="_blank" href="/mbart50-multilingual-fine-tuning-of-extensible-multilingual-pretraining-70a7305d4838"> mBART50:可扩展的多语言预训练的多语言微调</a> <br/> <strong class="lb iu">第3部分:具有多阶段预训练的多语言语音翻译</strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8b01" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">IWSLT</h1><p id="8074" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">自动语音翻译是从音频形式的语音输入开始，研究从一种人类语言到另一种语言的翻译的领域。它的研究跨越了几十年，多年来提出了许多不同的方法。</p><p id="d632" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">国际口语翻译会议(IWSLT)已经到了第19届，其目标是跟踪这一领域的进展。他们从两方面着手，一方面接受科学贡献，另一方面组织共同的任务，在不同的场景中，根据共同的基准比较真实的系统。</p><p id="58ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在以前的一篇文章中谈到过IWSLT:</p><div class="mz na gp gr nb nc"><a href="https://medium.com/machine-translation-fbk/machines-can-learn-to-translate-directly-your-voice-fbk-iwslt18-bb284ccae8bc" rel="noopener follow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">机器可以学习翻译你的声音</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">我们如何为IWSLT 2018评估活动构建我们的端到端语音到文本翻译系统。</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">medium.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ks nc"/></div></div></a></div><h1 id="c051" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">多语言语音翻译</h1><p id="ae9d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">去年的一个场景是多语言语音翻译，包括从西班牙语(es)、法语(fr)、葡萄牙语(pt)和意大利语(it)到英语(en)和西班牙语(es)的翻译。首先要注意的是，所有涉及到的语言都有些相似，英语是这个群体中唯一的非浪漫语言。这使得多语言翻译更加有效，因为对于模型来说，使用许多类似语言的数据进行改进相对容易。<br/>其次，英语只出现在目标端，而很多研究和很多数据集都集中在源端的英语上。</p><div class="mz na gp gr nb nc"><a href="https://medium.com/machine-translation-fbk/must-c-a-large-corpus-for-speech-translation-8e2350d01ea3" rel="noopener follow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">MuST-C:一个用于语音翻译的大型语料库</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">…你一定要看看！</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">medium.com</p></div></div><div class="nl l"><div class="nw l nn no np nl nq ks nc"/></div></div></a></div><p id="25fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该任务的三个平移方向(It-En、It-Es和Pt-Es)被认为是相对于受约束轨迹的零炮，这意味着没有专门针对这些方向的训练数据。在不受约束的赛道上，参与者可以使用他们想要的所有额外的训练数据，因此没有任何控制。</p><h1 id="98db" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">纸</h1><p id="be25" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这里描述的论文是去年(2021年)IWSLT多语言赛道的<a class="ae ky" href="https://scontent-dus1-1.xx.fbcdn.net/v/t39.8562-6/246840550_351924723290671_1968728059268811349_n.pdf?_nc_cat=100&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=LVLpkxx-5iQAX8qnfjp&amp;_nc_ht=scontent-dus1-1.xx&amp;oh=00_AT9rzfTqP2h69MMUnnnlHMNdz4oeBAtCyZyR-Dwt6utSdw&amp;oe=624E6380" rel="noopener ugc nofollow" target="_blank">获奖作品</a>，由脸书AI Research (FAIR)开发。他们提交的目标是探索用大量并行和未标记的数据预训练多模态翻译模型的解决方案。通过训练语音翻译、文本机器翻译和语音识别的模型，利用来自不同任务的数据。</p><h1 id="0d8e" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">数据</h1><p id="103a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">共享任务提供的训练集是TEDx、CoVoST和EuroParlST。</p><p id="4546" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TEDx是一个在TEDx上发表的演讲的集合，共有13个语言方向的翻译。这里，只考虑了任务的7个方向。<a class="ae ky" href="https://github.com/facebookresearch/covost" rel="noopener ugc nofollow" target="_blank"> CoVoST </a>是Mozilla Common Voice的扩展，提供大型数据集的翻译，包括11个翻译成英语的方向。<a class="ae ky" href="https://arxiv.org/abs/1911.03167" rel="noopener ugc nofollow" target="_blank">europarlist</a>是一个相对较小的多语言数据集，包含欧洲议会6种语言的翻译演讲，共11个翻译方向。</p><p id="009b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，作者使用两个已与单语文本对齐的多语转录音频数据集挖掘了并行数据。</p><p id="d0c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转录音频的两个多语种语料库分别是<a class="ae ky" href="https://commonvoice.mozilla.org/it" rel="noopener ugc nofollow" target="_blank"> CommonVoice </a> (29种语言)和<a class="ae ky" href="http://www.openslr.org/94/" rel="noopener ugc nofollow" target="_blank">Multilingual LibriSpeech</a>(8种语言的有声读物)，而<a class="ae ky" href="https://github.com/facebookresearch/cc_net" rel="noopener ugc nofollow" target="_blank"> CCNet </a>则作为多种语言的高质量单语文本大集合。</p><p id="0383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定这些数据，通过使用<a class="ae ky" href="https://engineering.fb.com/2019/01/22/ai-research/laser-multilingual-sentence-embeddings/" rel="noopener ugc nofollow" target="_blank">激光器</a>从CCNet中的源音频文本的抄本中提取句子嵌入以对齐具有相似语义的句子，如嵌入相似度所给出的，来获得另外的语音翻译数据。因为在源语言中音频和文本是对齐的，所以这个过程导致源音频与目标语言中的文本对齐。对于零炮点方向，得到的对准数据相当于几十个对准音频。</p><h1 id="074f" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">文本数据</h1><p id="af76" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">5种语言的单语数据用于训练<a class="ae ky" href="https://medium.com/towards-data-science/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432" rel="noopener"> mBART </a>。单语数据来自CC100。然后，mBART在从OPUS下载的7种语言的并行数据上进行微调。所得的微调模型将在以后用于初始化语音翻译模型。</p><h1 id="38dd" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">方法</h1><p id="ea41" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">模型培训遵循基于3个连续步骤的迁移学习方法:</p><ol class=""><li id="2cc7" class="nx ny it lb b lc ld lf lg li nz lm oa lq ob lu oc od oe of bi translated">经过自我监督学习预处理的单模态模块</li><li id="e987" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">多任务联合训练</li><li id="4ffe" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">特定于任务的微调</li></ol><p id="b920" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们的目标分别是:</p><ol class=""><li id="dbdc" class="nx ny it lb b lc ld lf lg li nz lm oa lq ob lu oc od oe of bi translated">从大量未标记数据中训练</li><li id="bb2d" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">从文本到文本再到语音到文本</li><li id="71d7" class="nx ny it lb b lc og lf oh li oi lm oj lq ok lu oc od oe of bi translated">对最终任务进行微调以获得更好的结果</li></ol><h2 id="0166" class="ol md it bd me om on dn mi oo op dp mm li oq or mo lm os ot mq lq ou ov ms ow bi translated">单一模态预训练</h2><p id="310a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Wav2vec 2.0用大量未标记音频数据训练，mBART用不同语言的大量单语文本训练。<a class="ae ky" href="https://arxiv.org/abs/2006.11477" rel="noopener ugc nofollow" target="_blank"> Wav2vec 2.0 </a>然后用于初始化第二训练阶段的语音编码器。mBART的编码器和解码器用于初始化下一阶段模型的编码器和解码器。</p><h2 id="3aa6" class="ol md it bd me om on dn mi oo op dp mm li oq or mo lm os ot mq lq ou ov ms ow bi translated">多任务联合训练</h2><p id="af7c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在第二阶段，语音到文本模型与文本到文本模型被联合学习。因此，整个模型由两个编码器和一个解码器组成。这两个编码器共享文本编码器的权重，但在处理音频输入时会使用附加层。适配器层用于促进在两个编码器之间共享的纯语音编码器和文本编码器权重之间的连接。一些培训<a class="ae ky" href="https://arxiv.org/abs/2107.05782" rel="noopener ugc nofollow" target="_blank">技巧</a>像交叉注意规则(CAR)和在线知识提炼(online KD)已经被用来促进任务间的知识转移。</p><h2 id="f3bd" class="ol md it bd me om on dn mi oo op dp mm li oq or mo lm os ot mq lq ou ov ms ow bi translated">特定于任务的微调</h2><p id="c9cd" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在最后阶段，文本编码器被丢弃，并且剩余的语音到文本翻译模型使用简单的交叉熵在并行音频翻译上被微调。</p><h1 id="46a2" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">结果</h1><p id="0cde" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">本文的主要结果是使用所描述的过程训练的三个系统的集合，但是具有稍微不同的编码器，比强语音翻译基线高出8.6个BLEU点。基线是使用上述所有数据建立的，但没有第二阶段的联合训练。</p><p id="929c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，该集合仅比翻译正确音频转录本的强MT模型弱3个BLEU点(在语言方向上平均),这代表了语音翻译性能的巨大进步。</p><h1 id="31af" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">结论和意见</h1><p id="664d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">这篇论文展示的结果无疑是惊人的，但不幸的是，并不是所有的小组都能够训练这种类型的模型。的确，虽然第2期和第3期相对便宜(5 + 2天8个NVidia V100 GPUs)，但像wav2vec和mBART这样从零开始的训练模型确实很昂贵，所需资源甚至没有在论文中提及。尽管如此，尽管共享任务的范围有限，但结果是显著的，并且清楚地表明大型预训练模型在跨模态设置中也是使能器。</p><p id="a1ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种强大的模型在该领域开辟了新的可能性，现在下一个前沿是在实时设置中也获得良好的结果，而该系统仅在批处理模式下工作。对于进一步的发展，我们只需要等待第19版IWS lt T1的结果，其中包括8个评估语音翻译不同方面的共享任务。</p><h1 id="4532" class="mc md it bd me mf nr mh mi mj ns ml mm jz nt ka mo kc nu kd mq kf nv kg ms mt bi translated">中等会员</h1><p id="d276" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">你喜欢我的文章吗？你是否正在考虑申请一个中级会员来无限制地阅读我的文章？</p><p id="3ac7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您通过此链接订阅，您将通过您的订阅支持我，无需为您支付额外费用【https://medium.com/@mattiadigangi/membership<a class="ae ky" href="https://medium.com/@mattiadigangi/membership" rel="noopener"/></p></div></div>    
</body>
</html>