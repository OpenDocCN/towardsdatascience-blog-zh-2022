<html>
<head>
<title>A batch too large: Finding the batch size that fits on GPUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量过大:寻找适合GPU的批量大小</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-batch-too-large-finding-the-batch-size-that-fits-on-gpus-aef70902a9f1#2022-10-19">https://towardsdatascience.com/a-batch-too-large-finding-the-batch-size-that-fits-on-gpus-aef70902a9f1#2022-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f034" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个简单的函数，用于确定PyTorch型号的批量大小，该批量可以填充GPU内存</h2></div><p id="fa98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我相信你们中的许多人都有以下痛苦的经历:你在你的GPU上开始多个ML实验以通宵训练，当你10个小时后回来检查时，你意识到进度条几乎没有移动，因为硬件利用不足，或者更糟的是，所有实验都因内存不足(OOM)错误而失败。在这个迷你指南中，我们将实现一个自动方法来为您的PyTorch模型找到批量大小，该批量大小可以充分利用GPU内存而不会导致OOM！</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/79856db973241155e687788e9aefc4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EwMByVB8ndcb5I4F"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lr" href="https://unsplash.com/@nanadua11?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">娜娜杜瓦</a>拍摄的照片</p></figure><p id="d3b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了模型架构和参数数量，批处理大小是控制实验使用的GPU内存量的最有效的超参数。找到可以充分利用加速器的最佳批量的适当方法是通过GPU配置文件，这是一种监控计算设备上的进程的过程。<a class="ae lr" href="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和<a class="ae lr" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>都提供了关于如何在其框架中执行概要分析的详细指南和教程。此外，批量大小会极大地影响模型的性能。例如，一个大的批量会导致较差的泛化能力，如果你对这个话题感兴趣的话，可以看看<a class="ae lr" href="https://medium.com/@kevinshen-57148?source=post_page-----21c14f7a716e--------------------------------" rel="noopener">沈世爱</a>关于<a class="ae lr" href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" rel="noopener">批量对训练动态的影响</a>的博客文章。</p><p id="4d4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果你只是想训练一个模型来测试一个想法，那么通过剖析或执行超参数搜索来寻找最佳批量可能是多余的，尤其是在项目的早期阶段。一种常见的方法是，在使用<code class="fe ls lt lu lv b">nvidia-smi</code>或<code class="fe ls lt lu lv b"><a class="ae lr" href="https://github.com/XuehaiPan/nvitop" rel="noopener ugc nofollow" target="_blank">nvitop</a></code>之类的工具监控GPU利用率的同时，用小批量训练模型，从而找到一个不需要OOM就可以拟合模型的值。然后，如果模型未充分利用GPU内存，则增加该值，并重复该过程，直到达到内存容量。但是，这种手动过程可能非常耗时。更令人恼火的是，当你不得不在不同内存大小的不同GPU上运行实验时，你必须为每台设备重复相同的过程。幸运的是，我们可以将这个繁琐的迭代过程转换成代码，并在实际实验之前运行它，这样您就知道您的模型不会导致OOM。</p><p id="031e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个想法很简单:</p><ol class=""><li id="7329" class="lw lx iq kh b ki kj kl km ko ly ks lz kw ma la mb mc md me bi translated">初始化您的模型。</li><li id="6850" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">将批次大小设置为2(对于BatchNorm)</li><li id="ec83" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">创建将样本形状作为真实数据的虚拟数据。</li><li id="7900" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">训练模型进行<code class="fe ls lt lu lv b">n</code>步骤(向前和向后传球)。</li><li id="21a8" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">如果模型运行时没有出现错误，则增加批量并转到步骤3。如果OOM升高(即PyTorch中的<code class="fe ls lt lu lv b">RuntimeError</code>),则将批量设置为之前的值并终止。</li><li id="198e" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">返回最终的批量大小。</li></ol><p id="8cc6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把它写成代码</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="423e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，该函数有7个参数:</p><ul class=""><li id="cbae" class="lw lx iq kh b ki kj kl km ko ly ks lz kw ma la mm mc md me bi translated"><code class="fe ls lt lu lv b">model</code> —你想要拟合的模型，注意，该模型将在函数结束时从内存中删除。</li><li id="b539" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">device</code> — <code class="fe ls lt lu lv b">torch.device</code>应该是CUDA设备。</li><li id="a4a4" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">input_shape</code> —数据的输入形状。</li><li id="6464" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">output_shape</code> —模型的预期输出形状。</li><li id="9105" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">dataset_size</code> —数据集的大小(当批处理大小已经大于数据集的大小时，我们不想继续搜索)。</li><li id="447a" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">max_batch_size</code> —设置要使用的最大批量的可选参数。</li><li id="428d" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><code class="fe ls lt lu lv b">num_iterations</code> —增加批量前更新模型的迭代次数，默认为5次。</li></ul><p id="3c43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们快速浏览一下函数中发生了什么。我们首先将模型加载到GPU，初始化Adam optimizer，并将初始批处理大小设置为2(如果您没有使用BatchNorm，可以从批处理大小1开始)。然后我们可以开始迭代过程。首先，我们检查当前的批处理大小是否大于数据集的大小或所需的最大批处理大小，如果是，我们就中断循环。否则，我们创建虚拟输入和目标，将它们移动到GPU并拟合模型。我们用5个步骤训练模型，以确保向前或向后传球都不会导致OOM。如果一切正常，我们将批量乘以2，并重新拟合模型。如果在上述步骤中发生了OOM，那么我们将批量减少2倍并退出循环。最后，我们从内存中清除模型和优化器，并返回最终的批量大小。就是这样！</p><p id="b41b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，在OOM的情况下，不是简单地将批量大小除以2，而是可以继续搜索最优值(即，二分搜索法批量大小，将批量大小设置为中断值和最后工作值之间的中点，并继续步骤3。)来找到最适合GPU的批量大小。然而，请记住PyTorch/TensorFlow或其他进程可能会在实验过程中请求更多的GPU内存，您可能会面临OOM的风险，因此我更喜欢有一些回旋的空间。</p><p id="2ba3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们把这个功能投入使用。在这里，我们将<a class="ae lr" href="https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>拟合在由<a class="ae lr" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.FakeData.html#torchvision.datasets.FakeData" rel="noopener ugc nofollow" target="_blank"> FakeData数据集</a>生成的大小为<code class="fe ls lt lu lv b">(3, 224, 224)</code>的1000个列车合成图像上。简单来说，我们首先调用<code class="fe ls lt lu lv b">get_batch_size=(model=ResNet(), input_shape=IMAGE_SHAPE, output_shape=(NUM_CLASSES,), dataset_size=DATASET_SIZE)</code>来获取能够充分填满GPU内存的批量大小。然后，我们可以初始化模型和数据加载器，并像平常一样训练模型！</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="a316" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面的GIF是在Nvidia RTX 2080 8GB上运行示例代码的屏幕记录。我在<code class="fe ls lt lu lv b">find_batch_size</code>函数中添加了一些打印语句，以显示它正在测试的批处理大小，并注意随着函数增加批处理大小，GPU内存使用量也在增加。我们的脚本确定批处理大小为16会导致OOM，并运行其余的训练代码，批处理大小为8，GPU内存利用率约为66.8%。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mn ml l"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">在Nvidia RTX 2080 8GB上运行示例代码，该脚本确定批量大小为16会导致OOM。[作者GIF</p></figure><p id="67fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们在Nvidia RTX 2080Ti 11GB上运行完全相同的代码时，我们能够以16的批处理大小和90.3%的GPU内存利用率运行。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mo"><img src="../Images/a1f08d1d50e62e10841cfb1dbf2d1382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6B30rzavL9o6fTcX-1WWA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">在Nvidia RTX 2080Ti 11GB上运行示例代码，该脚本确定批量大小为32会导致OOM。[图片由作者提供]</p></figure><p id="4094" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你有它！您可以将一个简单的函数添加到训练脚本的开头，以找到一个可以充分利用GPU内存而不用担心OOM错误的批处理大小。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="05ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，这里有一些关于批量大小及其对深度神经网络的影响的文章和论文。</p><ul class=""><li id="2b8e" class="lw lx iq kh b ki kj kl km ko ly ks lz kw ma la mm mc md me bi translated"><a class="ae lr" rel="noopener" target="_blank" href="/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9">历元对批量对迭代</a></li><li id="663b" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><a class="ae lr" href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e" rel="noopener">批量大小对训练动态的影响</a></li><li id="a3f8" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><a class="ae lr" href="https://wandb.ai/ayush-thakur/dl-question-bank/reports/What-s-the-Optimal-Batch-Size-to-Train-a-Neural-Network---VmlldzoyMDkyNDU" rel="noopener ugc nofollow" target="_blank">训练一个神经网络的最佳批量是多少？</a></li><li id="8fd4" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><a class="ae lr" href="https://arxiv.org/abs/1609.04836" rel="noopener ugc nofollow" target="_blank">关于深度学习的大批量训练:泛化差距和尖锐极小值</a></li><li id="1e6c" class="lw lx iq kh b ki mf kl mg ko mh ks mi kw mj la mm mc md me bi translated"><a class="ae lr" href="https://arxiv.org/abs/1711.00489" rel="noopener ugc nofollow" target="_blank">不衰减学习率，增加批量</a></li></ul></div></div>    
</body>
</html>