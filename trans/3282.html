<html>
<head>
<title>5 Probabilistic Training Data Sampling Methods in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的5种概率训练数据采样方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-probabilistic-training-data-sampling-methods-in-machine-learning-460f2d6ffd9#2022-07-21">https://towardsdatascience.com/5-probabilistic-training-data-sampling-methods-in-machine-learning-460f2d6ffd9#2022-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="900d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">合适的数据采样方法对于训练一个好的模型很重要</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a0dcbf7a539773fbb3d7eddf18f973f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NZUDtnDlJZXBHckM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@testalizeme?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Testalize.me </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e4c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型训练中的数据采样是机器学习中一个被忽视的领域。机器学习有两个方面:建模和数据。这两者同等重要，在现实生活的机器学习项目中应该仔细考虑。然而，许多教科书、论文、博客都谈到了建模方面，很少有人谈到数据方面。</p><p id="5357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里使用“数据”而不是“数据集”，因为它们的概念不同:“数据集”是有限的、固定的和静止的，而“数据”是无限的、不固定的和动态的。当我们训练一个模型的时候，我们没有办法接触到世界上所有相应的可能数据；要使用的数据是由一些抽样方法创建的子集。数据充满了潜在的偏差，这可能是由于收集、采样或标记数据过程中的一些人为因素造成的。由于偏差在基于此数据训练的模型中是永久存在的，因此对能够减少数据中偏差的适当子集进行采样是至关重要的。</p><p id="706f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，了解不同的抽样方法是很重要的，因为<strong class="lb iu">首先，它帮助我们避免数据中的潜在偏差，其次，它帮助我们提高训练中的数据效率</strong>。</p><p id="c0e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在数据采样期间，为了训练可靠的模型，采样的子集应该代表真实世界的数据，以减少选择偏差</strong>【1】。幸运的是，概率数据采样方法可以帮助我们认识到这一点。在这里，我将介绍五种在模型训练中有代表性的概率数据采样方法。</p><h2 id="c53e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">简单随机抽样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/21c6b1be869f1d022e0f8f5a38a21352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AT4wejM4lZKWUs9e"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Volodymyr Hryshchenko 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2ce8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最简单的概率抽样形式。总体中所有样本被抽样的机会相同，因此它们的概率形成均匀分布。例如，如果您想从10个样本中抽取5个样本，那么每个元素被选中的概率是0.5。</p><p id="f1a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该方法简单明了，易于实现，但总体中的稀有类可能不会在选择中被采样。假设您想从数据中抽取1%的样本，但是一个稀有类只出现在0.01%的总体中:这个稀有类的样本可能不会被选择。<strong class="lb iu">在这种情况下，用采样子集训练的模型可能不知道稀有类</strong>的存在。</p><h2 id="cc8d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">分层抽样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/971ed16806e7615b8ac6ddcaf660efc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*COf1VCpoqal27xII"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@kazto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> KAZTO TAKAHASHI </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="7f38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了避免简单随机抽样的缺点，您可以<strong class="lb iu">根据您的要求，例如标签，将人群分成几组，并从每组中单独抽样</strong>。每一组被称为一个阶层，这种方法被称为分层抽样。</p><p id="d263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，要从具有A类和B类的总体中抽取1%的样本，可以将总体分成两组，分别从这两组中抽取1%的样本。这样，<strong class="lb iu">无论A或B有多稀有，采样的子集都保证包含这两类</strong>。</p><p id="b368" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，分层抽样的一个缺点是总体并不总是可分的。例如，在每个样本具有多个标签的多标签学习任务中，根据不同的标签来划分群体是具有挑战性的。</p><h2 id="4cff" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">加权抽样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/b1217505ee234f7ca8f8a0465bad7150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aZ8vFdiP_PPSJaKz"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">廷杰伤害律师事务所</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3a0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在加权抽样中，每个样本都被赋予一个权重，即被抽样的概率。例如，对于包含A类和B类的总体，如果将权重0.8分配给A类，0.2分配给B类，则A类和B类被抽样的概率分别为80%和20%。</p><p id="7ccd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">权重抽样可以利用领域专业知识，这对减少抽样偏差很重要</strong>。例如，在训练一些在线学习模型时，最近的数据比旧数据重要得多。通过为最近的数据分配较大的权重，为旧数据分配较小的权重，可以更可靠地训练模型。</p><h2 id="57fc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">储层取样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/cee60928a35c3916af23d917992c1bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iFoPd0EdjRuJR0Xq"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乔纳森·比恩在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="df24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">油藏采样是一种有趣而优雅的算法，用于处理在线学习模型中的流数据，在产品中相当流行。</p><p id="8773" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设数据是以顺序流的方式产生的，比如一个时间序列，你不可能把所有的数据都装进内存，也不知道以后会产生多少数据。<strong class="lb iu">你需要用<em class="ms"> k个</em>样本对一个子集进行采样来训练一个模型，但是你不知道选择哪个样本，因为很多样本还没有生成</strong>。</p><p id="5a44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">储层采样可以处理这个问题，即<strong class="lb iu"> 1)所有样本以相等的概率被选择，以及2)如果你在任何时候停止算法，样本总是以正确的概率被选择</strong>。该算法包含3个步骤:</p><ol class=""><li id="02f3" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">将前<em class="ms"> k </em>个样本放入一个容器中，这个容器可以是一个数组或一个列表</li><li id="4c63" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">当产生第<em class="ms"> n </em>个样本时，在<em class="ms"> 1 </em>到<em class="ms"> n </em>的范围内随机选择一个数字<em class="ms"> m </em>。如果选择的数字<em class="ms"> m </em>在<em class="ms"> 1 </em>到<em class="ms"> k </em>的范围内，则用生成的第<em class="ms"> n </em>个样本替换库内的第<em class="ms"> m </em>个样本，否则不做任何操作。</li><li id="ecb8" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">重复2，直到算法停止。</li></ol><p id="a29e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我们很容易证明，对于每一个新生成的样本，被选入储层的概率是<em class="ms"> k </em> / <em class="ms"> n </em>。我们还可以证明，对于每一个已经在油藏中的样本，不被替换的概率也是<em class="ms"> k </em> / <em class="ms"> n </em> </strong>。因此，当算法停止时，以正确的概率选择储层中的所有样本。</p><h2 id="84fe" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">重要性抽样</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/01dc18d111c046993d67e5b34aa5910d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fPo8UgQx31cNRzS5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@freegraphictoday?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">absolute vision</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2738" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要抽样是最重要的抽样方法之一。当我们只能访问另一个发行版时，它允许我们从一个发行版中取样。</p><p id="7067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们想要从分布<em class="ms"> P(x) </em>中取样，但是不能访问它。然而，我们可以访问另一个分布<em class="ms"> Q(x) </em>。<strong class="lb iu">下面的等式表明，在期望中，从<em class="ms"> P(x) </em>采样的<em class="ms"> x </em>等于从<em class="ms"> Q(x) </em>采样的<em class="ms"> x </em>由<em class="ms"> P(x)/Q(x) </em> </strong>加权。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d3cff2b6850f7787d476d033129b3f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sCfTtMhz1aQThMagc5Occw.png"/></div></div></figure><p id="8cf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，<strong class="lb iu">可以替代从<em class="ms"> P(x) </em>采样，也可以从可访问的<em class="ms"> Q(x) </em>采样，并用<em class="ms"> P(x)/Q(x) </em> </strong> <em class="ms">对采样结果进行加权。</em>结果与我们直接从<em class="ms"> P(x) </em>取样的结果相同。</p><h2 id="bf82" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论</h2><p id="4211" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在这篇文章中，我介绍了5种用于模型训练的代表性概率数据采样方法。这些方法是任务不可知的，可以用于各种机器学习领域的所有任务:计算机视觉、NLP、强化学习和表格数据分析等。使用哪种采样方法取决于特定的任务、数据集、问题设置以及您希望从模型中得到什么。<strong class="lb iu">我建议先用一个简单的方法，比如简单的随机抽样，随着你理解的深入再换成其他方法</strong>。乍一看，您可能会发现这很难，但是随着模型训练和数据监控的进展，事情会变得清楚。</p><h2 id="37e3" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="dec6" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated"><a class="ae ky" href="https://www.jstor.org/stable/1912352" rel="noopener ugc nofollow" target="_blank">样本选择偏差作为规格错误，1979年</a></p><p id="2ee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/" rel="noopener ugc nofollow" target="_blank">设计机器学习系统，2022 </a></p><div class="no np gp gr nq nr"><a href="https://dushuchen.medium.com/membership" rel="noopener follow" target="_blank"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">加入我的介绍链接-陈数杜媒体</h2><div class="ny l"><h3 class="bd b gy z fp nw fr fs nx fu fw dk translated">阅读陈数·杜(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="nz l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">dushuchen.medium.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of ks nr"/></div></div></a></div></div></div>    
</body>
</html>