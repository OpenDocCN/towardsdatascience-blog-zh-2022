<html>
<head>
<title>Overfitting in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overfitting-in-deep-learning-what-is-it-and-how-to-combat-it-9760d25ad05b#2022-10-18">https://towardsdatascience.com/overfitting-in-deep-learning-what-is-it-and-how-to-combat-it-9760d25ad05b#2022-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3541" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于过度拟合问题和预防方法的简单指南。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/86c1c59b6efa0b5540b492bfa978031e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kYfpdQILWk2ucmbx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">国家癌症研究所</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="6088" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="fea5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">深度学习最近的成功是基于拥有数百万参数和大数据的巨大网络。这个简单的配方在许多领域，如图像分类或自然语言处理，给行业带来了革命性的变化。事实证明，这是一把双刃剑。太多的参数可能会导致过度拟合和对看不见的数据泛化能力差。因此，为什么在这篇文章中，我们将仔细看看这个问题，以及我们如何才能防止它。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="ea3a" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated"><strong class="ak">什么是过度拟合？</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/7b06bd3a6bc6f866bb1738aad24e01d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6Ym-LxaWmSnBA5Tg3ClMw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。图片作者。</p></figure><p id="3f25" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当网络具有太多参数并且夸大了数据中的潜在模式时，会发生过度拟合。尽管该模型完全符合数据点，但它不能很好地概括看不见的数据。</p><p id="ce23" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">另一方面，线性函数产生了过于简化的假设，导致<strong class="lt iu">对数据集</strong>的拟合不足。</p><p id="5d61" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">二次方程最适合我们的数据点。它捕捉点的一般形状，从而在测试集上产生良好的泛化。</p><p id="dc73" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">描述模型性能的常用方法是使用偏差和方差项。<strong class="lt iu">偏差</strong>代表输出和目标之间的距离，<strong class="lt iu">方差</strong>定义结果的分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/61ce2733e07ac1e0cac9a147f16eaf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14jqM5-Rgm-pWDrF29aftA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。图片作者。</p></figure><p id="c30e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">使用上面的例子，很明显欠拟合和过拟合取决于网络的容量。在这些统计玩具示例中，模型复杂性和性能之间的最佳平衡点相对容易建立，而深度学习则不是这样。</p><p id="145e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在下一部分，我们将戴上深度学习的帽子，看看如何在大型网络中发现这些问题。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="49e5" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">深度学习方法</h1><p id="02b5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">到目前为止，我所描述的是一种老式的机器学习方法，目标是找到模型复杂性和性能之间的最佳平衡点。</p><p id="2bb9" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当深度学习出现时，这种范式发生了转变。事实证明，当模型处于<strong class="lt iu">过度拟合状态</strong>时，性能会更好。在这种情况下，我们需要应用强大的<strong class="lt iu">规则</strong>并在训练期间监控模型的行为。</p><p id="8cf2" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">为了检查模型的性能，我们需要首先将数据分成3个子集:</p><ul class=""><li id="1764" class="ng nh it lt b lu na lx nb ma ni me nj mi nk mm nl nm nn no bi translated">训练集-模型训练所依据的数据(65–98)%</li><li id="7eb8" class="ng nh it lt b lu np lx nq ma nr me ns mi nt mm nl nm nn no bi translated">验证集-帮助评估模型在训练期间的性能(1–10)%</li><li id="a056" class="ng nh it lt b lu np lx nq ma nr me ns mi nt mm nl nm nn no bi translated">测试集—帮助评估模型在训练后的性能(1–25)%</li></ul><p id="cc3e" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">分割比例取决于数据集的大小。例如，ImageNet包含1000个类和120万个图像。当我们以98:1:1的方式分割它们时，我们仍然有24万个未见过的测试示例。这应该足以正确评估性能。</p><p id="fd85" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">就较小的数据集而言，最好保留较大块的不可见数据，以确保模型性能良好。此外，记住每个集合中的类的数量要平衡，这样评估就涵盖了所有的例子。</p><h2 id="cf77" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated">性能图</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/a0c9730032100ae1612616ffddbfdd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyRgUl2DeNASizGHCoUShA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。图片作者。</p></figure><p id="df81" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">泛化误差是训练误差和验证误差之间的差异。我们模型的最终目标是<strong class="lt iu">同时最小化训练和泛化错误</strong>。当泛化差距增大时，会发生过度拟合。</p><p id="ce47" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">监控两条曲线有助于发现任何问题，然后采取措施防止问题发生。在下一节中，我们将介绍用于对抗过度拟合的最流行的正则化技术。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="e981" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated"><strong class="ak">正规化</strong></h1><blockquote class="oh"><p id="59a8" class="oi oj it bd ok ol om on oo op oq mm dk translated"><em class="or">正则化是我们对学习算法进行的任何修改，旨在减少其泛化误差，而不是训练误差。</em><a class="ae ky" href="https://www.deeplearningbook.org/contents/regularization.html" rel="noopener ugc nofollow" target="_blank"><em class="or"/></a></p></blockquote><h2 id="1d68" class="nu la it bd lb nv os dn lf nx ot dp lj ma ou oa ll me ov oc ln mi ow oe lp of bi translated"><strong class="ak">重量衰减</strong></h2><p id="2caf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">明确地修改网络的复杂性不是一个容易的过程。幸运的是，我们可以通过惩罚大的权重来控制参数的大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/666181de7b3d45a363eaaf5826b8965b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U9xxGmUmtalmof3jo9lCAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。图片作者。</p></figure><p id="4886" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个简单的过程是基于在损失函数中加入惩罚项。现在，新模型的目标是最小化训练误差，并使权重更小。</p><p id="67ff" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">L2项是参数的平方和(点积),它严重影响了异常值。因此，权重分布更加均匀(图5)。</p><p id="48f5" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe oy oz pa pb b">lambda</code>参数定义了模型对重量的敏感度。该值越大，权重变化受到的惩罚就越小(图5)。在学术论文中，初始值通常被设置为<code class="fe oy oz pa pb b">0.0005</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/7307acdf413125682058e7ad9ab37d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9S1dOUb7QBtBaGv-fTGHdw.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。图片作者。多亏了Piotr的Skalski可视化技术。</p></figure><h2 id="55dd" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated"><strong class="ak">辍学</strong></h2><p id="a3f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">此技术在图层上应用随机采样零值的遮罩。该方案在模型中创建了多个子网组合(图6)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/ad2d2fb5dcc9a31a5811b2fb97ee847a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*VqLDG625j-2Rls6XTNgiCg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。图片作者。</p></figure><p id="a706" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">如你所见，单个节点不能再依赖来自其他神经元的信息。它迫使每个节点学习如何自己提取特征。当每个神经元变得更加自治时，整个网络就可以更好地泛化。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="46f4" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated">更多数据</h2><p id="6b05" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">扩大数据集是使网络更加稳定的最简单方法。模型接触到更多的例子，可以更好的泛化。尽管如此，在大多数情况下，现实生活中的样本数量是有限的。<em class="pd">数据扩充</em>基于添加已有数据的略微修改的副本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/d185ce9220e662d4d66e84ef0fc9ee0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iM8Lhxqep9YlDfYcRR9vww.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。图片作者。</p></figure><p id="9eb1" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个方案是许多计算机视觉和NLP任务的核心部分。我已经在我的<a class="ae ky" rel="noopener" target="_blank" href="/data-augmentations-in-torchvision-5d56d70c372e">上一篇文章</a>中深入讨论了这个话题，所以我强烈推荐你去看看。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="77e0" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated"><strong class="ak">随机深度</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/d13d6c3eca73f38896708cb50068f7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofS0KqFT886lrPM8E2QpzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。图片来自<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习。</a></p></figure><p id="6bd6" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这种方法仅适用于计算机视觉架构。2015年，ResNet论文介绍了剩余连接，它允许我们堆叠越来越多的层🍰相互叠加<strong class="lt iu">无信息丢失</strong> <em class="pd"> ( </em>图8 <em class="pd"> ) </em>。将多个卷积组合成一个块成为了计算机视觉任务中的一个新范例(图9)。我们获得了构建任意深度网络的能力，但过度拟合的主要问题仍然是一个障碍。随机深度通过随机丢弃块来解决这个问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/15fb6ce5900c1cab0418d36c44611570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3AHxejmrIIhedlEx-VR_AA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9。图片作者。</p></figure><p id="dae5" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">SD使网络更短而不是更薄。丢弃随机输出会给每个模块带来更多的自主权。除了正则化能力，与原始配置相比，它减少了25%的训练时间。</p><p id="46d7" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">SD仅在训练期间使用。在干涉期间，模型使用所有块。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="bfe2" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated"><strong class="ak">标签平滑</strong></h2><p id="b922" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在分类任务中，我们的模型正在优化权重以映射期望的独热编码概率分布<code class="fe oy oz pa pb b">[0, 0, 1]</code>。实际上，网络不能精确地预测值0或1，所以它开始<em class="pd">无休止的劳动</em>产生越来越大的权重以得到想要的结果。这个过程被称为<em class="pd">过度自信。这是过度合身的一个很好的指标。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/696c085bf45dd6985adf99fd2daa5213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4NTFbusle3cS7e3CJx5tFA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd pf"> K </strong> —例数。图10。图片作者。</p></figure><p id="b09f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">给标签添加噪声会阻止网络搜索理想的分布。参数<code class="fe oy oz pa pb b">alpha</code>控制噪音量。通常，<code class="fe oy oz pa pb b">0.1</code>值是一个很好的起点。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="73d2" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated">批量标准化</h2><p id="c399" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">BN的主要目的是加速收敛并减少网络中的不稳定性。这个附加层位于卷积层之后，以优化输出分布(图11)。除了计算上的提升，BN也可以作为一种正则化技术。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/6782b96f9bbb24d881d4f9cac378941e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNp_LHvbK937Rr7iz5Ag_A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11。图片作者。</p></figure><p id="89fa" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">数据集统计数据的近似值会给网络增加一些噪声。请记住，这些估计仅适用于较大的批次，对于较小的批次，性能会急剧下降。</p><h2 id="5215" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated">提前停止</h2><p id="73da" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们之前讨论过，监控损失函数有助于发现网络中的问题。手动照看模型是一项单调乏味的任务，可以自动完成。每当泛化差距增大时，提前停止算法就终止训练。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/ca4a60fd70b79836683ab58db20c556d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kpG8bY-uE-CNX4YQIj9IIQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12。图片作者。</p></figure><p id="c159" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在这种情况下，离开模型进行通宵训练，早上回来，并加载重量是安全的。有许多方法可以选择保存检查点，但最安全的选择是在每次错误比前一个时期更好时进行选择。</p><h2 id="021a" class="nu la it bd lb nv nw dn lf nx ny dp lj ma nz oa ll me ob oc ln mi od oe lp of bi translated">学习率计划</h2><p id="7ec1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在模型开始过度拟合之前，验证损失达到平稳阶段(图13)。与其停模型，不如降低学习率，让它训练更久。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/4e5813909e466b8aba3aff59a1274672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiJ64dVgoD_1QT263E9Icg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13。图片来自<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习。</a></p></figure><p id="51b0" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在这种情况下，我们可以使用更高的学习率，大大加快训练速度。要选择学习率下降的触发因素，最好先观察模型的行为。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="d08b" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated"><strong class="ak">结论</strong></h1><p id="1e91" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">祝贺你成功到达终点！在这篇文章中，我解释了过度拟合现象及其从网络的不想要的属性到深度学习的核心组件的进展。我还会给你大量的正规化工具，帮助你成功地训练你的模型。欢迎在评论中提问。</p><p id="e9df" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">查看我的<a class="ae ky" href="https://maciejbalawejder.medium.com/" rel="noopener"> <strong class="lt iu">中</strong> </a>和<a class="ae ky" href="https://github.com/maciejbalawejder" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">Github</strong></a><strong class="lt iu"/>简介如果你想看我的其他项目。</p></div></div>    
</body>
</html>