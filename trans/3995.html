<html>
<head>
<title>How to Make Your Models Available to the Public</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何向公众展示你的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-your-models-available-to-the-public-be782dcb9942#2022-09-06">https://towardsdatascience.com/how-to-make-your-models-available-to-the-public-be782dcb9942#2022-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9530" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">解释如何使用Docker、Flask和Gunicorn在线部署您的模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7fc556990d40c5f8e64a256aa9af7d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s01ydvDskmUix1DMMvBDOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MLOps作为交叉文氏图[ <a class="ae kv" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a> ]</p></figure><h1 id="793d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="7cae" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">端到端的机器学习解决方案是将人工智能带入生产并使其可供大众消费和使用的重要方式。但今天，大多数人工智能从业者只是简单地做预处理，训练，评估和调优阶段，并将剩下的部分留给DevOps工程师。</p><p id="3c35" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，一个名为<a class="ae kv" href="https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/" rel="noopener ugc nofollow" target="_blank"> MLOps </a>的新开发领域已经成为主流。重点已经从简单的培训和评估转移到将信息技术引入并整合到生产流程中。</p><p id="f4ad" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在个人层面上，知道如何将你的模型公之于众是人工智能从业者技能组合中的一个重要工具。</p><p id="6e03" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本文中，我们将探索如何使用<strong class="lq ir"> Keras、Flask、Gunicorn </strong>和<strong class="lq ir"> Docker </strong>以简单高效的方式执行<strong class="lq ir"> MLOps </strong>循环的一小部分。</p><p id="0d9a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你想跳过并直接进入代码，点击这里进入GitHub库。</p><h1 id="ae99" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">本教程涵盖了哪些内容？</h1><ol class=""><li id="6f95" class="mp mq iq lq b lr ls lu lv lx mr mb ms mf mt mj mu mv mw mx bi translated">使用<code class="fe my mz na nb b">Keras</code>及其现成组件创建一个定制模型</li><li id="fe48" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj mu mv mw mx bi translated">准备推理管道</li><li id="3d83" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj mu mv mw mx bi translated">开发一个简单的<code class="fe my mz na nb b">Flask</code>应用程序来公开用于推理的模型</li><li id="1a2b" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj mu mv mw mx bi translated">使用<code class="fe my mz na nb b">Gunicorn</code>定义一个<code class="fe my mz na nb b">Dockerfile</code></li><li id="3c30" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj mu mv mw mx bi translated">建立我们的形象</li><li id="c5a8" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj mu mv mw mx bi translated">定义一个简单的Github Actions工作流，在每次将图像推送到存储库时构建图像</li></ol><h1 id="7d04" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">1)使用Keras创建自定义模型</h1><p id="9e42" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">例如，我们将使用Keras Functional API和ImageNet上预训练的<code class="fe my mz na nb b">keras.applications</code>的现成<code class="fe my mz na nb b">MobileNetV2</code>模型创建一个简单的模型。</p><h1 id="68cc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">导入标题</h1><p id="f505" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本教程需要<code class="fe my mz na nb b">tensorflow</code>、<code class="fe my mz na nb b">keras</code>、<code class="fe my mz na nb b">Flask</code>、<code class="fe my mz na nb b">PIL</code>和<code class="fe my mz na nb b">os</code>。如果使用虚拟环境，您可以使用下面的<code class="fe my mz na nb b">requirements.txt</code>文件来准备您的env。</p><ul class=""><li id="0bce" class="mp mq iq lq b lr mk lu ml lx nh mb ni mf nj mj nk mv mw mx bi translated"><code class="fe my mz na nb b">tensorflow</code>:用于矩阵运算和keras后端</li><li id="c7c8" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">keras</code>:用于高级深度学习建模API，获取预训练模型</li><li id="18fc" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">Flask</code>:用于构建简单的API进行推理</li><li id="8343" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">PIL</code>:用于处理图像</li><li id="e48b" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">os</code>:用于设置环境变量</li></ul><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="b88c" class="np kx iq nb b gy nq nr l ns nt">import tensorflow <strong class="nb ir">as</strong> tf<br/>from tensorflow import keras<br/>from flask import Flask<br/>from flask import request<strong class="nb ir">,</strong> jsonify<br/>from PIL import Image<br/>import os</span></pre><h1 id="c12f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设置选项</h1><p id="1f59" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">由于GPU是一种很难获得的资源，我们设置了一个Tensorflow标志，以便首先使任何CUDA设备不可见。<em class="nu">如果你能在GPU上运行你的容器，请随意跳过这一行。</em></p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="7f99" class="np kx iq nb b gy nq nr l ns nt">os<strong class="nb ir">.</strong>environ<strong class="nb ir">[</strong>'CUDA_VISIBLE_DEVICES'<strong class="nb ir">]</strong> <strong class="nb ir">=</strong> '-1'</span></pre><h1 id="a7a7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型定义</h1><p id="3362" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个模型是使用Keras Functional API制作的。我们用一个简单的<code class="fe my mz na nb b">keras.Input</code>来接受任何大小的彩色(RGB)图像。<br/>输入通过以下层传递:</p><ul class=""><li id="d398" class="mp mq iq lq b lr mk lu ml lx nh mb ni mf nj mj nk mv mw mx bi translated"><code class="fe my mz na nb b">keras.layers.Resizing</code>:用于将图像张量大小调整为224x224x3张量。</li><li id="e956" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">keras.layers.Rescaling</code>:用于将图像张量值从[0，255]范围重新缩放到[0，1]范围。</li><li id="70c6" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">keras.applications.MobileNetV2</code>:用于从Keras导入MobileNetV2实例(在ImageNet上预训练)。</li></ul><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="a3d0" class="np kx iq nb b gy nq nr l ns nt">image_input <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>Input<strong class="nb ir">(</strong>shape<strong class="nb ir">=(None,None,</strong>3<strong class="nb ir">))</strong></span><span id="e0d5" class="np kx iq nb b gy nv nr l ns nt">x <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>layers<strong class="nb ir">.</strong>Resizing<strong class="nb ir">(</strong>height<strong class="nb ir">=</strong>224<strong class="nb ir">,</strong> width<strong class="nb ir">=</strong>224<strong class="nb ir">,</strong> interpolation<strong class="nb ir">=</strong>'lanczos3'<strong class="nb ir">,</strong> crop_to_aspect_ratio<strong class="nb ir">=False)(</strong>image_input<strong class="nb ir">)</strong></span><span id="dc2c" class="np kx iq nb b gy nv nr l ns nt">x <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>layers<strong class="nb ir">.</strong>Rescaling<strong class="nb ir">(</strong>scale<strong class="nb ir">=</strong>1.<strong class="nb ir">/</strong>255<strong class="nb ir">,</strong> offset<strong class="nb ir">=</strong>0.0<strong class="nb ir">)(</strong>x<strong class="nb ir">)</strong></span><span id="abcf" class="np kx iq nb b gy nv nr l ns nt">mobilenet <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>applications<strong class="nb ir">.</strong>MobileNetV2<strong class="nb ir">(</strong><br/>    alpha<strong class="nb ir">=</strong>1.0<strong class="nb ir">,</strong><br/>    include_top<strong class="nb ir">=True,</strong><br/>    weights<strong class="nb ir">=</strong>"imagenet"<strong class="nb ir">,</strong><br/>    input_tensor<strong class="nb ir">=</strong>image_input<strong class="nb ir">,</strong><br/>    classes<strong class="nb ir">=</strong>1000<strong class="nb ir">,</strong><br/>    classifier_activation<strong class="nb ir">=</strong>"softmax"<br/><strong class="nb ir">)</strong></span><span id="0996" class="np kx iq nb b gy nv nr l ns nt">model_output <strong class="nb ir">=</strong> mobilenet<strong class="nb ir">(</strong>x<strong class="nb ir">)</strong></span><span id="adfc" class="np kx iq nb b gy nv nr l ns nt">model <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>Model<strong class="nb ir">(</strong>inputs<strong class="nb ir">=</strong>image_input<strong class="nb ir">,</strong> outputs<strong class="nb ir">=</strong>model_output<strong class="nb ir">)</strong></span></pre><h1 id="8317" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">需求文件</h1><p id="edc7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><code class="fe my mz na nb b">Gunicorn</code>用于将API部署到多个工作线程上，以增加计算消耗为代价降低延迟。使用Gunicorn是因为它实现了WSGI。在生产环境中，像<a class="ae kv" href="https://www.nginx.com/" rel="noopener ugc nofollow" target="_blank"> NGINX </a>或<a class="ae kv" href="https://httpd.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Web Server </a>这样的前端服务器用于托管静态网页和负载平衡器，Gunicorn在该层后面运行以实现功能。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="c453" class="np kx iq nb b gy nq nr l ns nt">Flask<strong class="nb ir">==</strong>2.0<strong class="nb ir">.</strong>3<br/>Pillow<strong class="nb ir">==</strong>9.2<strong class="nb ir">.</strong>0<br/>tensorflow<strong class="nb ir">==</strong>2.9<strong class="nb ir">.</strong>1<br/>gunicorn<strong class="nb ir">==</strong>20.1<strong class="nb ir">.</strong>0</span></pre><h1 id="2427" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">2)准备推理管道</h1><p id="cd75" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们定义了一个简单的函数，它接受一个<code class="fe my mz na nb b">tf.Tensor</code>，并在模型中运行它，以返回一个最终的前5名预测字典结果。</p><h1 id="a2f3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">推理功能</h1><p id="0826" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">使用之前准备好的函数来推断作为<code class="fe my mz na nb b">tf.Tensor</code>接受的图像。然后提取张量的<code class="fe my mz na nb b">numpy</code>值，以获得每个类别的所有置信度得分。<br/>这个numpy数组然后被传递到<code class="fe my mz na nb b">keras.applications.imagenet_utils.decode_predictions</code>以获得前5个预测。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="2934" class="np kx iq nb b gy nq nr l ns nt"><strong class="nb ir">def</strong> <strong class="nb ir">inference(</strong>image<strong class="nb ir">:</strong> tf<strong class="nb ir">.</strong>Tensor<strong class="nb ir">):</strong><br/>    y <strong class="nb ir">=</strong> model<strong class="nb ir">(</strong>image<strong class="nb ir">).</strong>numpy<strong class="nb ir">()</strong><br/>    preds <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>applications<strong class="nb ir">.</strong>imagenet_utils<strong class="nb ir">.</strong>decode_predictions<strong class="nb ir">(</strong>y<strong class="nb ir">,</strong> top<strong class="nb ir">=</strong>5<strong class="nb ir">)</strong><br/>    result <strong class="nb ir">=</strong> <strong class="nb ir">{</strong>i<strong class="nb ir">[</strong>1<strong class="nb ir">]</strong> <strong class="nb ir">:</strong> <em class="nu">str</em><strong class="nb ir">(</strong>i<strong class="nb ir">[</strong>2<strong class="nb ir">])</strong> <strong class="nb ir">for</strong> i <strong class="nb ir">in</strong> preds<strong class="nb ir">[</strong>0<strong class="nb ir">]}</strong><br/>    result <strong class="nb ir">=</strong> <strong class="nb ir">{</strong>k<strong class="nb ir">:</strong> v <strong class="nb ir">for</strong> k<strong class="nb ir">,</strong> v <strong class="nb ir">in</strong> <em class="nu">sorted</em><strong class="nb ir">(</strong>result<strong class="nb ir">.</strong>items<strong class="nb ir">(),</strong> key<strong class="nb ir">=lambda</strong> item<strong class="nb ir">:</strong> item<strong class="nb ir">[</strong>1<strong class="nb ir">])}</strong><br/>    <strong class="nb ir">return</strong> result</span></pre><h1 id="66e2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">3)制作一个简单的Flask应用程序来公开模型以进行推理</h1><p id="e2d2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，我们在路线<code class="fe my mz na nb b">/</code>和<code class="fe my mz na nb b">/inference</code>上定义两个简单的端点。</p><ul class=""><li id="9a95" class="mp mq iq lq b lr mk lu ml lx nh mb ni mf nj mj nk mv mw mx bi translated"><code class="fe my mz na nb b">/</code> (GET):第一个端点充当健康检查，以确保API启动并运行。</li><li id="3a9b" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">/inference</code> (POST):第二个端点接受一个图像作为带有参数名<code class="fe my mz na nb b">image</code>的表单字段，并返回一个带有置信度得分和ImageNet类名的字典。</li></ul><h1 id="157c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">烧瓶应用程序定义</h1><p id="b45c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><code class="fe my mz na nb b">app</code>是稍后将由Gunicorn使用的WSGI callable的名称。要了解WSGI是什么，请查看下面有趣的链接部分。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="fb01" class="np kx iq nb b gy nq nr l ns nt">app <strong class="nb ir">=</strong> Flask<strong class="nb ir">(</strong><em class="nu">__name__</em><strong class="nb ir">)</strong></span></pre><h1 id="1083" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">健康检查端点的定义</h1><p id="caa7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了测试API是否启动并运行，我们只需在这个端点上点击一个GET请求来获得预期的输出。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="8222" class="np kx iq nb b gy nq nr l ns nt">@app<strong class="nb ir">.</strong>route<strong class="nb ir">(</strong>"/"<strong class="nb ir">,</strong> methods<strong class="nb ir">=[</strong>'GET'<strong class="nb ir">])</strong><br/><strong class="nb ir">def</strong> <strong class="nb ir">health_check():</strong><br/>    result <strong class="nb ir">=</strong> <strong class="nb ir">{</strong><br/>        'outcome'<strong class="nb ir">:</strong>'endpoint working successfully'<br/>    <strong class="nb ir">}</strong><br/>    <strong class="nb ir">return</strong> jsonify<strong class="nb ir">(</strong>result<strong class="nb ir">)</strong></span></pre><h1 id="c744" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">推理端点的定义</h1><p id="9a7f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这里，我们接受一个<code class="fe my mz na nb b">POST</code>请求，从请求中发送的文件中提取<code class="fe my mz na nb b">image</code>参数。这以文件流格式存储，然后传递到<code class="fe my mz na nb b">PIL.Image.open</code>以准备图像。最后，我们执行一些简单的预处理，将<code class="fe my mz na nb b">PIL</code>图像转换为<code class="fe my mz na nb b">tf.Tensor</code>图像，并准备一批1个图像传递给我们的推理函数。然后，返回的结果被传递到<code class="fe my mz na nb b">jsonify</code>中进行响应准备和执行。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="7222" class="np kx iq nb b gy nq nr l ns nt">@app<strong class="nb ir">.</strong>route<strong class="nb ir">(</strong>"/inference"<strong class="nb ir">,</strong> methods<strong class="nb ir">=[</strong>'POST'<strong class="nb ir">])</strong><br/><strong class="nb ir">def</strong> <strong class="nb ir">perform_inference():</strong><br/>    image <strong class="nb ir">=</strong> request<strong class="nb ir">.</strong>files<strong class="nb ir">[</strong>'image'<strong class="nb ir">]</strong><br/>    pil_img <strong class="nb ir">=</strong> Image<strong class="nb ir">.</strong>open<strong class="nb ir">(</strong>image<strong class="nb ir">.</strong>stream<strong class="nb ir">)</strong><br/>    tensor <strong class="nb ir">=</strong> keras<strong class="nb ir">.</strong>preprocessing<strong class="nb ir">.</strong>image<strong class="nb ir">.</strong>img_to_array<strong class="nb ir">(</strong>pil_img<strong class="nb ir">)</strong><br/>    tensor <strong class="nb ir">=</strong> tf<strong class="nb ir">.</strong>expand_dims<strong class="nb ir">(</strong>tensor<strong class="nb ir">,</strong> axis<strong class="nb ir">=</strong>0<strong class="nb ir">)</strong><br/>    result <strong class="nb ir">=</strong> inference<strong class="nb ir">(</strong>tensor<strong class="nb ir">)</strong><br/>    <strong class="nb ir">return</strong> jsonify<strong class="nb ir">(</strong>result<strong class="nb ir">)</strong></span></pre><h1 id="c6ce" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">4)定义一个使用Gunicorn进行部署的Dockerfile</h1><p id="a8d3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，我们已经完成了模型的定义，并使用一个简单的Flask应用程序为推断做好了准备。这里，我们开始编写一个<code class="fe my mz na nb b">Dockerfile</code>和一个<code class="fe my mz na nb b">.dockerignore</code>来构建一个定制的Docker映像。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="0421" class="np kx iq nb b gy nq nr l ns nt">FROM ubuntu<strong class="nb ir">:</strong>20.04</span><span id="cc04" class="np kx iq nb b gy nv nr l ns nt">RUN apt<strong class="nb ir">-</strong>get update <strong class="nb ir">&amp;&amp;</strong> apt<strong class="nb ir">-</strong>get install <strong class="nb ir">-</strong>y \<br/>git \<br/>curl \<br/>ca<strong class="nb ir">-</strong>certificates \<br/>python3 \<br/>python3<strong class="nb ir">-</strong>pip \<br/>sudo \<br/><strong class="nb ir">&amp;&amp;</strong> rm <strong class="nb ir">-</strong>rf <strong class="nb ir">/</strong>var<strong class="nb ir">/</strong>lib<strong class="nb ir">/</strong>apt<strong class="nb ir">/</strong>lists<strong class="nb ir">/*</strong></span><span id="cdfb" class="np kx iq nb b gy nv nr l ns nt">RUN useradd <strong class="nb ir">-</strong>m docker_runner</span><span id="3238" class="np kx iq nb b gy nv nr l ns nt">RUN chown <strong class="nb ir">-</strong>R docker_runner<strong class="nb ir">:</strong>docker_runner <strong class="nb ir">/</strong>home<strong class="nb ir">/</strong>docker_runner</span><span id="5841" class="np kx iq nb b gy nv nr l ns nt">COPY <strong class="nb ir">--</strong>chown<strong class="nb ir">=</strong>docker_runner <strong class="nb ir">*.*</strong> <strong class="nb ir">/</strong>home<strong class="nb ir">/</strong>docker_runner<strong class="nb ir">/</strong>flask_app<strong class="nb ir">/</strong>keras<strong class="nb ir">-</strong>docker<strong class="nb ir">-</strong>trial<strong class="nb ir">/</strong></span><span id="fe15" class="np kx iq nb b gy nv nr l ns nt">USER docker_runner</span><span id="f023" class="np kx iq nb b gy nv nr l ns nt">WORKDIR <strong class="nb ir">/</strong>home<strong class="nb ir">/</strong>docker_runner<strong class="nb ir">/</strong>flask_app<strong class="nb ir">/</strong>keras<strong class="nb ir">-</strong>docker<strong class="nb ir">-</strong>trial</span><span id="3f5f" class="np kx iq nb b gy nv nr l ns nt">ENV PATH<strong class="nb ir">=</strong>"${PATH}:/home/docker_runner/.local/bin"</span><span id="4019" class="np kx iq nb b gy nv nr l ns nt">RUN pip install <strong class="nb ir">--</strong>no<strong class="nb ir">-</strong>cache<strong class="nb ir">-</strong><em class="nu">dir</em> <strong class="nb ir">-</strong>r requirements<strong class="nb ir">.</strong>txt</span><span id="b14e" class="np kx iq nb b gy nv nr l ns nt">ENTRYPOINT <strong class="nb ir">[</strong>"gunicorn"<strong class="nb ir">,</strong> "--bind"<strong class="nb ir">,</strong> "0.0.0.0:5000"<strong class="nb ir">,</strong> "--workers=4"<strong class="nb ir">,</strong> "app:app"<strong class="nb ir">]</strong></span><span id="7ecf" class="np kx iq nb b gy nv nr l ns nt">EXPOSE 5000</span></pre><h1 id="abc2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">Dockerfile文件</h1><ul class=""><li id="b87b" class="mp mq iq lq b lr ls lu lv lx mr mb ms mf mt mj nk mv mw mx bi translated">第一行从Docker Hub中提取<code class="fe my mz na nb b">ubuntu:20.04</code>图像，准备一个容器，其中包含库存Ubuntu 20.04 Focal Fossa。</li><li id="1f8d" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated">第一个<code class="fe my mz na nb b">RUN</code>命令下载并安装我们稍后需要的几个基本包。</li><li id="4c46" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated">下一个<code class="fe my mz na nb b">RUN</code>命令添加一个名为docker_runner的用户，并为该用户创建一个主目录(使用-m选项)。</li><li id="57da" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated">下一个<code class="fe my mz na nb b">RUN</code>命令更改目录所有权，并以递归方式为所有文件和子目录指定docker_runner作为其主目录的所有者(使用-R选项)。</li><li id="7c4a" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">COPY</code>命令将Dockerfile所在的当前存储库中的所有文件移动到容器的目标目录中。</li><li id="201f" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">USER</code>命令用于将当前活动用户更改为<code class="fe my mz na nb b">docker_runner</code></li><li id="1b02" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">WORKDIR</code>命令用于将当前活动目录更改为<code class="fe my mz na nb b">/home/docker_runner/flask_app/keras-docker-trial</code></li><li id="6187" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">ENV</code>命令用于设置PATH环境变量，并将我们用户的<code class="fe my mz na nb b">/.local/bin</code>目录添加到其中。</li><li id="48e2" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">RUN</code>命令现在用于安装所有的需求，并且在安装时不使用任何缓存的目录或它们的SHA散列。</li><li id="9bbe" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">ENTRYPOINT</code>命令用于使用<code class="fe my mz na nb b">gunicorn</code>开始API部署。我们绑定本地主机的端口5000，并为此任务启动4个workers。我们将WSGI callable指定为<code class="fe my mz na nb b">app:app</code>左侧的<code class="fe my mz na nb b">app</code>。如果你在步骤3中更改了Flask应用程序的名称，那么你应该将这部分更改为<code class="fe my mz na nb b">{your_app_name}:app</code></li><li id="266d" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">EXPOSE</code>命令用于让容器监听端口5000。</li></ul><h1 id="1f25" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">。dockerignore</h1><p id="ceb7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们只是忽略了<code class="fe my mz na nb b">__pycache__/</code>目录，因为它从CPython生成中间文件。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="17fb" class="np kx iq nb b gy nq nr l ns nt">__pycache__<strong class="nb ir">/</strong></span></pre><h1 id="5af0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">5)建立我们的形象</h1><p id="dc52" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们现在构建我们的图像，并给它分配一个标签<code class="fe my mz na nb b">keras-docker-trial</code>。</p><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="0899" class="np kx iq nb b gy nq nr l ns nt">docker build <strong class="nb ir">.</strong> <strong class="nb ir">-</strong>t keras<strong class="nb ir">-</strong>docker<strong class="nb ir">-</strong>trial <strong class="nb ir">--</strong>file Dockerfile</span></pre><h1 id="8a46" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">6)定义一个简单的GitHub Actions工作流，以便在每次将图像推送到存储库时构建它</h1><p id="2f81" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这里，作为一个额外的步骤，我们使用GitHub Actions来构建我们的映像，作为每次对任何分支进行推送或者如果PR被合并到存储库中的测试。只有当您在GitHub上为您的模型准备一个存储库时，才需要添加这个。</p><ul class=""><li id="fc91" class="mp mq iq lq b lr mk lu ml lx nh mb ni mf nj mj nk mv mw mx bi translated"><code class="fe my mz na nb b">name</code>:为工作流程指定一个名称。</li><li id="7608" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">on</code>:定义何时使用工作流的触发器。</li><li id="7500" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">env</code>:设置环境变量。</li><li id="f2e4" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">jobs</code>:定义作为当前工作流的一部分运行的不同命令和工作流动作。</li><li id="6bb2" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">runs-on</code>:定义哪个GitHub托管的runner用于执行工作流。</li><li id="7aaa" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">actions/checkout@v3</code>:用于从仓库中签出代码。</li><li id="2f1f" class="mp mq iq lq b lr nc lu nd lx ne mb nf mf ng mj nk mv mw mx bi translated"><code class="fe my mz na nb b">Build Docker Image</code>:从存储库中存在的Dockerfile文件构建映像。</li></ul><pre class="kg kh ki kj gt nl nb nm nn aw no bi"><span id="1380" class="np kx iq nb b gy nq nr l ns nt">name<strong class="nb ir">:</strong> Docker CI</span><span id="6c90" class="np kx iq nb b gy nv nr l ns nt">on<strong class="nb ir">:</strong><br/>  push<strong class="nb ir">:</strong><br/>  pull_request<strong class="nb ir">:</strong><br/>    types<strong class="nb ir">:</strong> <strong class="nb ir">[</strong>'opened'<strong class="nb ir">,</strong> 'reopened'<strong class="nb ir">]</strong></span><span id="c397" class="np kx iq nb b gy nv nr l ns nt">env<strong class="nb ir">:</strong><br/>  BUILD_CONFIGURATION<strong class="nb ir">:</strong> Release</span><span id="4ebd" class="np kx iq nb b gy nv nr l ns nt">jobs<strong class="nb ir">:</strong><br/>  job1<strong class="nb ir">:</strong><br/>    runs<strong class="nb ir">-</strong>on<strong class="nb ir">:</strong> ubuntu<strong class="nb ir">-</strong>latest<br/>    steps<strong class="nb ir">:</strong><br/>      <strong class="nb ir">-</strong> name<strong class="nb ir">:</strong> Check<strong class="nb ir">-</strong>out the pushed code<br/>        uses<strong class="nb ir">:</strong> actions<strong class="nb ir">/</strong>checkout@v3</span><span id="3658" class="np kx iq nb b gy nv nr l ns nt">      <strong class="nb ir">-</strong> name<strong class="nb ir">:</strong> Build Docker image<br/>        run<strong class="nb ir">:</strong> docker build <strong class="nb ir">.</strong> <strong class="nb ir">-</strong>t keras<strong class="nb ir">-</strong>docker<strong class="nb ir">-</strong>trial <strong class="nb ir">--</strong>file Dockerfile</span></pre><h1 id="9fed" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">测试管道</h1><p id="019c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">上面，我们已经定义了模型，并使用Docker和Gunicorn部署了它。您可以通过下面的Postman API Explorer找到一些部署及其预测的示例截图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6865d6c3011282ad53a253db58021c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/0*5ZE3P-u-sna5DYAe.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">终端命令</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/bbf687b0ca422c15ab83a40916aa4b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6CzibOSYcul8fv9q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">获取健康检查请求</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/b57c102b27eaffc2e53a3381524930ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3GAsByOQ09Iy5FUa.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">获取推理请求</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/18978099299163def6a82b8c5596de13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*nrC8aqDCR959hze_.jpg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过邮递员请求发送的金鱼图像(根据<a class="ae kv" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a>许可)</p></figure><h1 id="dc09" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="b141" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">上面，我们已经完成了一个简单的Keras模型的开发，它通过Docker的部署和一个用于CI(持续集成)的GitHub Actions工作流。</p><h1 id="3da7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">未来范围</h1><p id="3682" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这只是作为一个简单的MLOps管道的一部分可以完成的工作的一小部分。CML(连续机器学习)和DVC(数据版本控制)是两个重要的概念，是每个自我维持的机器学习工作流的一个组成部分，可以进一步探索。有趣的链接部分提供了相关资源。</p><h1 id="bd24" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="74fe" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">1.)<a class="ae kv" href="https://docs.docker.com/docker-hub/" rel="noopener ugc nofollow" target="_blank"> Docker Hub文档</a> <br/> 2。)<a class="ae kv" href="https://keras.io/api/applications/mobilenet/#mobilenetv2-function" rel="noopener ugc nofollow" target="_blank"> Keras应用文档</a> <br/> 3。)<a class="ae kv" href="https://docs.gunicorn.org/en/stable/configure.html" rel="noopener ugc nofollow" target="_blank"> Gunicorn文档</a></p><h1 id="dca0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">有趣的链接</h1><p id="8615" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">1.)<a class="ae kv" href="https://cml.dev/" rel="noopener ugc nofollow" target="_blank">什么是CML？</a> <br/> 2。)<a class="ae kv" href="https://dvc.org/doc/user-guide/what-is-dvc" rel="noopener ugc nofollow" target="_blank">什么是DVC？</a> <br/> 3。)<a class="ae kv" href="https://wsgi.readthedocs.io/en/latest/what.html" rel="noopener ugc nofollow" target="_blank">什么是WSGI (Web服务器网关接口)？</a> <br/> 4。)<a class="ae kv" href="https://neptune.ai/blog/mlops" rel="noopener ugc nofollow" target="_blank">详细博客什么是MLOps？</a></p></div></div>    
</body>
</html>