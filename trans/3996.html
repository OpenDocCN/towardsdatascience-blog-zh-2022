<html>
<head>
<title>Global Context Vision Transformers (GC ViT)— Nvidia’s new SOTA Image Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">全球背景视觉变形金刚(GC ViT)——Nvidia的新SOTA图像模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/global-context-vision-transformers-nvidias-new-sota-image-model-2923bdaf438e#2022-09-06">https://towardsdatascience.com/global-context-vision-transformers-nvidias-new-sota-image-model-2923bdaf438e#2022-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e2e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入的解释和可视化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/20d3ed1fcb5021c2249339a1417869c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgQVeTLU8jqKagy_S3E-GA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">免费使用来自<a class="ae ky" href="https://www.pexels.com/photo/silhouette-of-woman-3862601/" rel="noopener ugc nofollow" target="_blank">像素</a>的图像</p></figure><h1 id="de15" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="be32" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Nvidia最近发布了一款新的视觉转换器，名为全球情境视觉转换器(<strong class="lt iu"> GC ViT </strong> ) ( <a class="ae ky" href="https://arxiv.org/abs/2206.09959" rel="noopener ugc nofollow" target="_blank"> Hatamizadeh等人，2022 </a>)。GC ViT引入了一种新的架构，它利用了全局注意力<em class="mn">和</em>局部注意力，允许它模拟短程和远程空间交互。</p><p id="be5e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Nvidia研究人员使用的巧妙技术使GC ViT能够模拟全球注意力，同时避免昂贵的计算。GC ViT在ImageNet-1K数据集上取得了最先进的(SOTA)结果，远远超过了Swin Transformer。</p><p id="15ea" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在本文中，我们将更深入地了解GC ViT的内部工作方式，以及使它能够实现这种结果的技术。</p><h1 id="6a40" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak"> GC ViT —改进Swin变压器</strong></h1><p id="55d7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自Swin Transformer ( <a class="ae ky" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">刘等人，2021 </a>)于2021年发表以来，它已成为最重要的基于Transformer的视觉模型之一。</p><p id="8287" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Swin Transformer引入了重要的技术，如分层特征映射和基于窗口的注意力，这使它能够实现与传统卷积神经网络相比具有竞争力的性能。如今，Swin变压器被用作各种视觉任务的主干架构，包括图像分类和物体检测。</p><blockquote class="mt mu mv"><p id="92fd" class="lr ls mn lt b lu mo ju lw lx mp jx lz mw mq mc md mx mr mg mh my ms mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-swin-transformer-64965f89d14c">注意:如果你需要复习Swin变压器，我在这里写了一个全面的指南，解释了Swin变压器的基本原理。</a></p></blockquote><p id="69c7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">尽管有所进步，Swin变压器仍有某些缺点。最值得注意的是，Swin Transformer中使用的基于窗口的注意力将交互的计算限制在每个窗口内，并限制跨窗口的交互。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/d48223baf548654bb42dbf2ba82315d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GpuOutJ1txCbOlby010uhA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在Swin Transformer中，注意力的计算被限制在局部窗口内。图像来自<a class="ae ky" href="https://www.pexels.com/photo/cute-purebred-dog-playing-with-ball-on-sandy-beach-3857521/" rel="noopener ugc nofollow" target="_blank">像素</a>。作者创建的图表。</p></figure><p id="699d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">上图显示了Swin Transformer中使用的基于窗口的注意力的局限性的示例。输入图像被分割成单独的窗口，然后只在每个窗口内计算自我注意。这限制了全局图像中不同对象之间的长距离相互作用的计算。例如，狗和球被分割到不同的窗口中，模型被限制学习两个对象之间的交互。缺乏跨窗口连接限制了模型捕捉长程相关性的能力，而长程相关性对于精确表示建模至关重要。</p><p id="d1e5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Swin转换器试图通过使用基于移位窗口的注意力来引入<em class="mn">一些</em>跨窗口连接。然而，这在计算上是昂贵的，并且它没有从根本上解决缺乏<strong class="lt iu">全球连接</strong>的问题。正如我们将在后面看到的，GC ViT通过在单个架构中提供本地和全局连接对此进行了改进。</p><h1 id="cd99" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">GC ViT的体系结构</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/5c5493049fe9ddc8508724aa09c1a333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXQnvRc4YewLkXN3kSF6Ng.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GC ViT的总体架构。改编自<a class="ae ky" href="https://arxiv.org/abs/2206.09959" rel="noopener ugc nofollow" target="_blank"> Hatamizadeh等人，2022 </a>。点击此处查看更高分辨率的图像。</p></figure><p id="9f9c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">GC ViT的整体架构如上图所示。正如我们所看到的，GC ViT由4个不同的阶段组成，每个阶段由局部和全局多头自我关注(MSA)层的交替块组成。本地MSA提取本地的短程信息，而全球MSA提取全球的远程信息。这使得GC ViT可以灵活地处理短期和长期依赖关系。在两个阶段之间，GC ViT使用下采样块来创建类似于Swin转换器的分层特征图。</p><p id="cf1e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">GC ViT的主要贡献是<strong class="lt iu">全局令牌生成器</strong>和<strong class="lt iu">全局MSA层</strong>。在下一节中，我们将更详细地研究它们。</p><h1 id="2f47" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">全球自我关注</h1><p id="ce6e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">全局自我关注的首要原则是在图像的每个区域之间建立全局联系。为了做到这一点，让我们来理解自我关注是如何将一幅图像分割成不同的碎片的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/a21781632f3abaf6c19d3e3fcaa03121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnN56y3siEs-rugFo5p_1w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为了计算自我注意，图像被分成窗口和小块</p></figure><p id="88aa" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">从上图中，我们看到每个图像被分割成独立的窗口(显示为紫色)。每个窗口被进一步分割成小块(以红色显示)。在局部自我注意中，小块之间的注意计算被限制在每个局部窗口内。换句话说，不同窗口中的补丁之间没有交叉连接，这限制了网络的建模能力。</p><p id="af36" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">全局自我关注旨在通过引入补丁和窗口之间的连接来解决这一问题，如下图动画所示。修补程序和窗口之间的这些全局连接允许GC ViT关注图像中的全局位置，有效地模拟长程相关性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7f09c8201bcc838de748398a45da5b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*3JlZuKVLsuTIipvtIjdpQg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">全局自我关注在每个补丁和每个窗口之间建立了联系</p></figure><p id="0fff" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在局部MSA(多头自关注)中，<em class="mn">查询</em>、<em class="mn">键</em>和<em class="mn">值</em>向量从局部窗口中的小块中导出，并且仅在每个局部窗口内计算关注度。相比之下，在全局MSA中，只有<em class="mn">键</em>和<em class="mn">值</em>向量是从局部窗口中的补丁中导出的。<em class="mn">查询</em>向量是从所有窗口导出的<strong class="lt iu">全局查询令牌</strong>。下图说明了本地MSA和全球MSA之间的区别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/f53b9a81264154359b248fb40cd0f93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFJD9QzhOVIa4EyFTTkVlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">本地管理服务协议与全球管理服务协议的比较</p></figure><p id="4a45" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">全局查询令牌由全局查询生成器在网络的每个阶段生成，全局查询生成器将特征地图作为输入，并提取全局特征作为全局查询令牌。全局查询令牌包含整个输入要素地图的信息，用于与本地键和值向量进行交互。</p><p id="dd52" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">全局查询生成器由一系列操作组成，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a15c13b5e8c2c1c3cba8bd59ef3bc14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRfJHVqZHfrm0_-THpQYpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">全局查询生成器中的操作</p></figure><p id="e94d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">全局查询令牌与本地键和值向量的交互允许计算全局注意力。这有效地扩大了感受野，并允许模型关注特征图中的各个区域。</p><h1 id="0b35" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">向下采样</h1><p id="6009" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">GC ViT还引入了一种新颖的方法，在创建分层特征图的阶段之间对特征图进行下采样。有趣的是，GC ViT使用卷积层进行下采样。作者认为，使用卷积进行下采样为网络提供了所需的属性，如位置偏差和跨通道交互。下采样块中使用的操作如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/757d70625457a05d3c7d969417913ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LEVvtDSyHsjTdMdmMxDXmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下采样块内的操作</p></figure><p id="9690" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意前4个操作与全局查询生成器的操作相似。事实上，作者将前4个操作称为“Fused-MBConv”块，它的灵感来自EfficientNetV2。</p><h1 id="851b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结果</h1><p id="6b1c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">GC ViT在ImageNet-1K数据集上进行图像分类训练。下表比较了GC ViT在ImageNet-1K数据集上与其他CNN和ViT(包括Swin Transformer)的性能。正如我们所看到的，GC ViT达到了一个新的最先进的基准。此外，就FLOPs的数量而言，GC ViT模型具有更好或相当的计算效率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/e943b18362b1df93653dcc635fce1e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gW5EivuR_ksb7-keKkipow.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ImageNet-1K数据集上的图像分类基准。来自<a class="ae ky" href="https://arxiv.org/abs/2206.09959" rel="noopener ugc nofollow" target="_blank"> Hatamizadeh等人，2022 </a>。</p></figure><p id="fc11" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">与Swin Transformer相比，GC ViT以更少的FLOPs实现了更好的性能，展示了结合局部和全局自我关注的好处。但是，请注意，Swin转换器具有无卷积架构，而GC ViT使用卷积运算来计算全局注意力和下采样。</p><h1 id="f61e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="743a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">GC ViT引入了一种结合局部自我关注和全局自我关注的新型架构，允许网络对短期和长期交互进行建模。通过删除其他ViT中所需的复杂而昂贵的操作和掩码，GC ViT在ImageNet-1K数据集上实现了新的SOTA，同时计算效率更高。</p><p id="aaf5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">不过，应当注意，与Swin变压器不同，GC ViT不是无卷积架构，其部分性能可能源自卷积的电感偏置。</p><h1 id="e927" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">喜欢这篇文章？</h1><p id="2d1b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">感谢您的阅读！我希望这篇文章对你有用。如果您想订阅中级会员，请考虑使用<a class="ae ky" href="https://medium.com/@jamesloyys/membership" rel="noopener">我的链接</a>。这有助于我继续创建对社区有用的内容！😄</p></div></div>    
</body>
</html>