<html>
<head>
<title>Into TheTransformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进入变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/into-the-transformer-5ad892e0cee#2022-10-26">https://towardsdatascience.com/into-the-transformer-5ad892e0cee#2022-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="26b5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">数据流、参数和维度</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0e34dc51dfdc14d1802ea784a04ab39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXdNv-exGw7jlpINb2KgUA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">约书亚·索蒂诺在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="7e0a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">简介:</h1><p id="8241" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Transformer是谷歌研究人员在2017年推出的一种神经网络架构，已被证明是自然语言处理(NLP)领域的最先进技术，随后进入了计算机视觉(CV)。</p><p id="d89d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">尽管网上有许多解释其体系结构的资源，但我还没有遇到一种资源明确地谈到数据以矩阵形式流经转换器时的更详细的细节。</p><p id="daaa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，本文涵盖了转换器中所有子层的维度(输入、输出和权重)。最后，计算出一个示例变压器模型中涉及的参数总数。</p><p id="3677" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对Transformer模型的基本熟悉有助于从本文中获益，但并不是必须的。需要进一步解释变压器基础知识的人，可以看看文末提到的参考资料。</p><p id="8ff9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本文组织如下:</p><ol class=""><li id="aa5b" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated"><a class="ae kv" href="#3cdc" rel="noopener ugc nofollow">变压器</a></li><li id="1e6a" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="#e40f" rel="noopener ugc nofollow">编码器</a></li><li id="0347" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="#2cd1" rel="noopener ugc nofollow">解码器</a></li><li id="7fa5" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="#5180" rel="noopener ugc nofollow">外围块</a></li><li id="c045" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><a class="ae kv" href="#c3d3" rel="noopener ugc nofollow">总结</a></li></ol><h1 id="3cdc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> 1。变压器:</strong></h1><p id="8b8e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">变换器由编码器和解码器组成，各重复N次(原研重复6次)，如图1所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c8d2e3c51bbd4dde4b0f0d9afde6149c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*CJt_f12Dh_1hUdw-S2HF1A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:Transformer——模型架构(来源:注意力是你所需要的)</p></figure><p id="aebd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">就机器翻译而言，输入(在编码器端)是来自源语言的单词标记，而输出(在解码器端)是来自目标语言的单词标记。</p><p id="99ba" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如图2所示，数据从编码器流向解码器<em class="ne">、</em>。</p><p id="c41e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">每个编码器的输出是下一个编码器的输入。最后一个编码器的输出馈入N个解码器中的每一个。除了最后一个编码器的输出，每个解码器还接收前一个解码器的输出作为其输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/781f9cc6c2df5fb849eb6286b8fd5122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ioAQFlBfXRXwdoukUIgQA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:编码器和解码器层之间的数据流(图片由作者提供)</p></figure><p id="b0e0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们看看编码器和解码器，看看它们如何通过接受相同维度的输入来产生维度为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的输出。这里，注意，馈送到编码器和解码器的输入数量(分别为<code class="fe ng nh ni nj b"><em class="ne">TE</em></code>和<code class="fe ng nh ni nj b"><em class="ne">TD</em></code> <em class="ne">、</em>)可以不同，而每个输入的维度(编码器和解码器)保持相同(即<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>)。关于这些尺寸的更多细节将在后面介绍。</p><p id="6b02" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些编码器和解码器层本身包含子层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e6780f8d098ad9bcfb57805af4a368e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rItz57rXZk9gDon55quWOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:编码器和解码器(作者图片)</p></figure></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="e40f" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated"><strong class="ak"> 2。编码器:</strong></h1><p id="15ba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">编码器内部有两个子层。</p><ul class=""><li id="9e0f" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">多头注意力</li><li id="b5d6" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">正向输送</li></ul><h2 id="2546" class="ny kx iq bd ky nz oa dn lc ob oc dp lg lx od oe li mb of og lk mf oh oi lm oj bi translated"><strong class="ak">编码器中的多头注意:</strong></h2><p id="86fa" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">多头注意力是Transformer架构中至关重要且计算量最大的模块。该模块将<code class="fe ng nh ni nj b"><em class="ne">T (=TE</em>)</code>个大小为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>的向量作为输入(打包成一个大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的矩阵)，并产生一个大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的输出矩阵(一包<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个大小为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>的向量)，如图<em class="ne">图4 </em>所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/6b8d9093b8efd295a4f2c9ad69c72e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2STO6Cpe2llN1mGC5vcxHQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/671facce03995e8d140068c36aae8600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vqDG--L6PDP9VWBeEenFA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:多头关注(图片由作者提供)</p></figure><p id="c447" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">注意头<code class="fe ng nh ni nj b"><em class="ne">Head_i</em></code>接受来自“输入嵌入+位置编码”的输出(或来自先前编码器的输出)作为输入，并通过将输入乘以相应的权重矩阵来产生<code class="fe ng nh ni nj b"><em class="ne">Query</em></code>、<code class="fe ng nh ni nj b"><em class="ne">Key</em></code>和<code class="fe ng nh ni nj b"><em class="ne">Value</em></code>矩阵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/8830cf7980f11fef2c62168f4518d713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*14uOh-dcR3rxgQ_yvv_8Cg.png"/></div></div></figure><p id="6123" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">q&lt;1&gt;</em></code>、<code class="fe ng nh ni nj b"><em class="ne">k&lt;1&gt;</em></code>、<code class="fe ng nh ni nj b"><em class="ne">v&lt;1&gt;</em></code>分别是<code class="fe ng nh ni nj b"><em class="ne">x&lt;1&gt;</em></code>通过投影矩阵<code class="fe ng nh ni nj b"><em class="ne">wQ</em></code>、<code class="fe ng nh ni nj b"><em class="ne">wK</em></code>、<code class="fe ng nh ni nj b"><em class="ne">wV</em></code>、<em class="ne">、</em>的投影。类似地，对于位置2到t。</p><p id="63c1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">并且两者的尺寸都是<code class="fe ng nh ni nj b"><em class="ne">1xdK</em></code>而尺寸都是<code class="fe ng nh ni nj b"><em class="ne">1xdV</em></code>。</p><p id="b0a0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">矩阵<code class="fe ng nh ni nj b"><em class="ne">A’</em></code>的元素是每个查询向量<code class="fe ng nh ni nj b"><em class="ne">q&lt;&gt;</em></code>相对于每个关键向量<code class="fe ng nh ni nj b"><em class="ne">k&lt;&gt;</em></code>的比例点积。(相同大小的两个向量<em class="ne"> a </em>和<em class="ne"> b </em>的点积为<code class="fe ng nh ni nj b"><em class="ne">a.b = abT</em></code>，其中<code class="fe ng nh ni nj b"><em class="ne">bT</em></code>为<code class="fe ng nh ni nj b"><em class="ne">b</em></code>的转置)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/b7e82a037dd92543afae995f384d0364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjrYvc4p-3FPe7fd2f6n9w.png"/></div></div></figure><p id="0563" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在矩阵<code class="fe ng nh ni nj b">A’</code>的每一行应用<em class="ne"> Softmax </em>得到矩阵<code class="fe ng nh ni nj b">A</code>。这就是<em class="ne">成比例的点积注意力。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/8d213a82bea67ad0f30d26e2492aca79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q3ynj3qpveTSELih-L_fKA.png"/></div></div></figure><p id="5597" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">A</em></code>的row-1中的元素表示query-1对从1到t的所有键的关注，Row-2是query-2对所有键的关注，以此类推。<code class="fe ng nh ni nj b"><em class="ne">A</em></code>中的每一行总计为Softmax的输出)。</p><p id="028b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">Head_i</em></code>的输出是矩阵<code class="fe ng nh ni nj b"><em class="ne">A</em></code>和<code class="fe ng nh ni nj b"><em class="ne">V</em></code>的乘积。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/1b33e4f6010460dcf282be09d02ecd36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qu6Vx2gq2UrDJHGqO6JD_w.png"/></div></div></figure><p id="626e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据矩阵乘法的定义，<code class="fe ng nh ni nj b"><em class="ne">Zi</em></code>的row-1是以<code class="fe ng nh ni nj b"><em class="ne">A</em></code>的row-1的元素为权重，对<code class="fe ng nh ni nj b"><em class="ne">V</em></code>的所有行进行加权求和。<code class="fe ng nh ni nj b"><em class="ne">Zi</em></code>的Row-2是<code class="fe ng nh ni nj b"><em class="ne">V</em></code>所有行的加权和，以<code class="fe ng nh ni nj b"><em class="ne">A</em></code>的row-2的元素为权重，以此类推。注意每排<code class="fe ng nh ni nj b"><em class="ne">z&lt;&gt;</em></code>的尺寸与<code class="fe ng nh ni nj b"><em class="ne">v&lt;&gt;</em></code>的尺寸相同。并且<code class="fe ng nh ni nj b"><em class="ne">Zi</em></code>中的行数和<code class="fe ng nh ni nj b"><em class="ne">A</em></code>中的行数一样多。</p><p id="e689" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，所有头的输出被连接起来形成<code class="fe ng nh ni nj b"><em class="ne">Z’</em></code>。并乘以<code class="fe ng nh ni nj b"><em class="ne">W⁰</em></code>以产生多头关注子层的最终输出，即<code class="fe ng nh ni nj b"><em class="ne">Z</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/44acc92544c85cd0a3508e2b9da8bac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O04hUjmU_5wFxVSBRtMBGw.png"/></div></div></figure><p id="e895" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b">Z’</code>中每一行的维数为<code class="fe ng nh ni nj b"><em class="ne">1xhdV</em></code> ( <code class="fe ng nh ni nj b"><em class="ne">h</em></code> <em class="ne"> </em>个大小为<code class="fe ng nh ni nj b"><em class="ne">1xdV</em></code>的向量串接而成)。矩阵<code class="fe ng nh ni nj b"><em class="ne">W⁰</em></code>的维度为<code class="fe ng nh ni nj b"><em class="ne">hdVxdm</em></code>，将每一行<code class="fe ng nh ni nj b">Z’</code>从<code class="fe ng nh ni nj b"><em class="ne">1xhdV</em></code>维度投影到<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>。</p><p id="6ea5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，编码器中的多头关注子层接受大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的输入(每个<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个输入的数量为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>个)，并产生相同大小的输出<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>(每个<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个输出的数量为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>)。这也被称为输入-输入注意或编码器自我注意，即输入句子的每个位置注意输入句子本身的所有其他位置。</p><h2 id="1d9d" class="ny kx iq bd ky nz oa dn lc ob oc dp lg lx od oe li mb of og lk mf oh oi lm oj bi translated">编码器中的前馈网络:</h2><p id="418f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">前馈网络接受大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的输入(每个<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个输入的数量为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>个)，并执行以下功能以产生相同大小的输出<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>。这里，<code class="fe ng nh ni nj b"><em class="ne">T=TE</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/9ae578d6c0bedb9618333a49792030d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuaGjnIE_gjRtaSnQJxaLA.png"/></div></div></figure><p id="a6f7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该网络执行两次线性变换(通过<code class="fe ng nh ni nj b"><em class="ne">W1</em></code>和<code class="fe ng nh ni nj b"><em class="ne">W2</em></code>)，其间具有ReLU非线性。<code class="fe ng nh ni nj b"><em class="ne">W1</em></code>将尺寸<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>的每个输入转换成尺寸<code class="fe ng nh ni nj b"><em class="ne">1xdff</em></code>，并将<code class="fe ng nh ni nj b"><em class="ne">1xdff</em></code>转换回另一个<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>尺寸。</p><p id="a14e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，前馈子层产生与输入维数相同的输出，即<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="2cd1" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">3.解码器:</h1><p id="9b46" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">解码器内部有三个子层。</p><ul class=""><li id="50e7" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">掩蔽的多头注意力</li><li id="c959" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">多头注意力</li><li id="65c8" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">正向输送</li></ul><h2 id="49b5" class="ny kx iq bd ky nz oa dn lc ob oc dp lg lx od oe li mb of og lk mf oh oi lm oj bi translated"><strong class="ak">解码器中屏蔽的多头注意力:</strong></h2><p id="c0b7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">解码器中的掩蔽多头注意力也称为输出-输出注意力或解码器自我注意力。该模块将<code class="fe ng nh ni nj b"><em class="ne">T (=TD</em>)</code>个大小为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>的向量作为输入，并产生一个大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>的输出矩阵<code class="fe ng nh ni nj b"><em class="ne">Z</em></code>。这与编码器中的多头注意力子层(参见图4)相同，除了一个变化——遮罩。此掩码防止查询位置关注未来位置的键，从而保留自回归属性。因此，允许查询<code class="fe ng nh ni nj b"><em class="ne">q&lt;t&gt;</em></code>只关注从<code class="fe ng nh ni nj b"><em class="ne">k&lt;1&gt;</em></code>到<code class="fe ng nh ni nj b"><em class="ne">k&lt;t&gt;</em></code>的键。这是通过在<code class="fe ng nh ni nj b"><em class="ne">A’</em></code>中将禁止的<em class="ne">查询键</em>组合的位置设置为<code class="fe ng nh ni nj b"><em class="ne">-infinity</em></code>来实现的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/7090b573f26ea4f71fba4d31459eff01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1m2zCcVB1BsP__sKLiqobg.png"/></div></div></figure><p id="f032" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，解码器中被屏蔽的多头注意力子层接受大小为<code class="fe ng nh ni nj b"><em class="ne">Txdm</em></code>(每个<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个输入的数量为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>个)的输入，并产生相同大小<code class="fe ng nh ni nj b"><em class="ne">Txdm</em> </code>(每个<code class="fe ng nh ni nj b"><em class="ne">T</em></code>个输入的数量为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>个)的输出。</p><h2 id="3c74" class="ny kx iq bd ky nz oa dn lc ob oc dp lg lx od oe li mb of og lk mf oh oi lm oj bi translated"><strong class="ak">解码器中的多头注意力:</strong></h2><p id="4bbd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">解码器中的多头注意力也称为输入-输出注意力或编码器-解码器注意力。这与编码器中的多头注意力子层(参见图4)相同，只是它从编码器堆栈接收一个额外的输入(称之为<code class="fe ng nh ni nj b">X<em class="ne">E</em></code>)。该额外输入(大小为<code class="fe ng nh ni nj b"><em class="ne">TExdm</em></code>)用于产生<code class="fe ng nh ni nj b">K</code>和<code class="fe ng nh ni nj b">V</code>，而解码器侧的输入(大小为<code class="fe ng nh ni nj b"><em class="ne">TDxdm</em></code>)用于产生<code class="fe ng nh ni nj b">Q</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/96f44ed3135045c70d3c8097e3154aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGgJh2c9E3JCE5UkbJBrYA.png"/></div></div></figure><p id="11c2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相应的，<code class="fe ng nh ni nj b"><em class="ne">A’</em></code>和<code class="fe ng nh ni nj b">A</code>的尺寸也会变成<code class="fe ng nh ni nj b"><em class="ne">TDxTE</em></code>。这表示来自解码器侧的<code class="fe ng nh ni nj b"><em class="ne">TD</em></code>个令牌中的每一个对来自编码器侧的<code class="fe ng nh ni nj b"><em class="ne">TE</em></code>个令牌中的每一个的关注。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/fea77ef372950a2e2314b287030b6e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPwlqX_c2z6Ny9q5UogMIA.png"/></div></div></figure><p id="4c81" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">Head_i</em></code>的输出尺寸为<code class="fe ng nh ni nj b"><em class="ne">TDxdV</em></code> <em class="ne">。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/9ecc9a3d102ac2934942fbee2b0a45bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fecYzLrk3CCUhLW3-6aACQ.png"/></div></div></figure><h2 id="2abb" class="ny kx iq bd ky nz oa dn lc ob oc dp lg lx od oe li mb of og lk mf oh oi lm oj bi translated"><strong class="ak">解码器中的前馈网络:</strong></h2><p id="65a0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">解码器中的前馈网络与编码器中的前馈网络相同。这里，<code class="fe ng nh ni nj b"><em class="ne">T=TD</em></code>。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="5180" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated"><strong class="ak"> 4。外围区块:</strong></h1><p id="e662" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">变压器模型中的其他外围模块是<em class="ne">输入嵌入、输出嵌入、线性、</em>和<em class="ne"> Softmax </em>模块。输入嵌入(输出嵌入)将输入标记(输出标记)转换成模型维度的向量<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>。输入和输出标记是来自输入和输出字典的一键编码。</p><p id="e5e4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">线性和Softmax模块从最后一个解码器获取尺寸为<code class="fe ng nh ni nj b"><em class="ne">1xdm</em></code>的输入，并将其转换为等于输出字典的一键编码的尺寸。这个输出代表概率分布。</p><p id="00c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ne">位置编码</em>既不包含任何可学习的参数，也不通过增加嵌入来改变尺寸。因此，不再进一步解释。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="c3d3" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">5.总结:</h1><p id="db34" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">具有重复6次的编码器-解码器并且在每个子层中具有8个注意头的变换器模型具有以下参数矩阵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/cc16938edf3982e657da05effb5cfc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxDQZsMJwcvIop6Aluoodg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:总参数矩阵(图片由作者提供)</p></figure><p id="67cf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于具有N个编码器-解码器层和<code class="fe ng nh ni nj b"><em class="ne">h</em></code> <em class="ne"> </em>注意力头的模型，概括上述内容:</p><ul class=""><li id="5c82" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">每个编码器的参数矩阵数量(MHA+FFN) = <code class="fe ng nh ni nj b"><em class="ne">3h+1 + 4 = 3h+5</em></code></li><li id="26d6" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">每个解码器的参数矩阵数量(MMHA+MHA+FFN) = <code class="fe ng nh ni nj b"><em class="ne">3h+1 + 3h+1 + 4 = 6h+6</em></code></li><li id="a71d" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">单个编码器-解码器对的参数矩阵的数量= <code class="fe ng nh ni nj b"><em class="ne">3h+5 + 6h+6 = 9h+11</em></code></li><li id="8dbc" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj nx mv mw mx bi translated">模型的参数矩阵总数(NxEnc-Dec+Linear+I . Emb+O . Emb)=<code class="fe ng nh ni nj b"><em class="ne">N(9h+11) + 3</em></code></li></ul><p id="2de4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">考虑到前面给出的所有参数矩阵的维数，模型的参数总数如下:</p><ul class=""><li id="452c" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">每个编码器的参数数量:</li></ul><p id="faf6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">MHA: (dmxdK + dmxdK + dmxdV)h + hdVxdm ----- (1)</em></code></p><p id="e6b4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">FFN: dmxdff + 1xdff + dffxdm + 1xdm ----- (2)</em></code></p><ul class=""><li id="b6c1" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">每个解码器的参数数量:</li></ul><p id="b137" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">MMHA: (dmxdK + dmxdK + dmxdV)h + hdVxdm ----- (3)</em></code></p><p id="bf43" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">MHA: (dmxdK + dmxdK + dmxdV)h + hdVxdm ----- (4)</em></code></p><p id="a6fa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">FFN: dmxdff + 1xdff + dffxdm + 1xdm ----- (5)</em></code></p><ul class=""><li id="2bb4" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">外围模块中的参数数量:</li></ul><p id="ba44" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe ng nh ni nj b"><em class="ne">Linear + I.Emb + O.Emb: I_dictxdm + O_dictxdm + dmxO_dict -- (6)</em></code></p><p id="815b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，<code class="fe ng nh ni nj b"><em class="ne">I_dict</em></code> <em class="ne"> </em>是机器翻译中的<em class="ne"> </em>输入语言词典大小，<em class="ne"> </em> <code class="fe ng nh ni nj b"><em class="ne">O_dict</em></code> <em class="ne"> </em>是输出语言词典大小。</p><ul class=""><li id="9328" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj nx mv mw mx bi translated">模型的参数总数= <code class="fe ng nh ni nj b"><em class="ne">N[(1)+(2)+(3)+(4)+(5)]+(6)</em></code></li></ul><p id="88b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">原变压器研究论文中提到的基础模型(注意是你所需要的)使用尺寸<code class="fe ng nh ni nj b"><em class="ne">dm = 512</em></code>、<code class="fe ng nh ni nj b"><em class="ne">dK = 64</em></code>、<code class="fe ng nh ni nj b"><em class="ne">dV = 64</em></code>、<code class="fe ng nh ni nj b"><em class="ne">dff = 2048</em></code>、<code class="fe ng nh ni nj b"><em class="ne">h = 8</em></code>、<code class="fe ng nh ni nj b"><em class="ne">N = 6</em></code>，共有<code class="fe ng nh ni nj b"><em class="ne">65 million</em></code>个参数。</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="f883" class="kw kx iq bd ky kz ns lb lc ld nt lf lg jw nu jx li jz nv ka lk kc nw kd lm ln bi translated">结论:</h1><p id="c647" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">转换器主要用于构建语言模型，帮助执行各种NLP任务，如机器翻译、自动摘要、对话管理、文本到图像的生成等。</p><p id="f49c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">几个拥有数十亿参数的大型语言模型，包括最近轰动一时的ChatGPT，展示了<a class="ae kv" href="https://sekharm.medium.com/list/in-conversation-with-ai-7a1d71751872" rel="noopener">非凡的对话能力</a>，都以变形金刚为构建模块。</p><p id="7c76" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我希望通过研究这个构件，您能够理解这些大型语言模型最终是如何包含数十亿个参数的。</p><h1 id="c2bc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考资料:</h1><p id="9a3d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">[1] Jay Alammar，<a class="ae kv" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图文并茂的变形金刚</a>博文，2018。</p><p id="356a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[2]瓦斯瓦尼等人，<a class="ae kv" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，2017。</p></div></div>    
</body>
</html>