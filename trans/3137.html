<html>
<head>
<title>Deep-dive on ML techniques for feature selection in Python — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探讨Python中特性选择的ML技术—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43#2022-07-10">https://towardsdatascience.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-2-c258f8a2ac43#2022-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2052" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">Python中AI驱动的特征选择！</h2><div class=""/><div class=""><h2 id="f4cd" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">基于最大似然的特征选择系列的第二部分，我们将讨论流行的嵌入和包装方法，如套索回归、贝塔系数、递归特征选择等。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/5b49a3867bf5d1eea0dbcbe75f56d807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QK9T6HALYxX8FAAO"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@ikukevk" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lf">凯文Ku </strong> </a> <strong class="bd lf"> </strong>上<a class="ae le" href="https://unsplash.com/photos/w7ZyuGYNpRQ" rel="noopener ugc nofollow" target="_blank">的Unsplash </a></p></figure><p id="7dde" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi mc translated">欢迎来到我关于基于ML的特征选择的博客系列的第二部分！我们都知道大数据时代已经到来，但有时很难理解大数据到底有多“大”。我最近读了一篇有趣的博客<a class="ae le" href="https://techjury.net/blog/how-much-data-is-created-every-day/#gref" rel="noopener ugc nofollow" target="_blank"/>,这篇博客很好地诠释了这一点:</p><ol class=""><li id="8fc8" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">2020年，每天会产生2.5万亿个数据字节</li><li id="040d" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">2020年，平均每个人每秒至少产生1.7 MB的数据</li><li id="4ce1" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">到2025年，云数据存储将达到200+ Zettabytes</li></ol><p id="5797" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">似乎有点令人生畏，对吧？我从数据量的快速增长中得出的一个结论是，我们不能再使用“厨房水槽”的方法来建模。这种方法仅仅意味着我们把“除了厨房水槽以外的所有东西”都扔进模型，希望找到某种模式。用这种方法，我们的模型根本无法理解大数据集。这就是使用特征选择方法首先剔除数据中的噪声，然后继续开发模型的关键所在。</p><p id="46b3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在第一篇<a class="ae le" href="https://indraneeldb1993ds.medium.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-1-3574269d5c69" rel="noopener">博客</a>中，我们概述了不同类型的特征选择方法，并讨论了一些过滤方法，如信息值。在第二部分，我们将深入探讨以下有趣的方法:</p><p id="a382" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">a)贝塔系数<br/> B)套索回归<br/> C)递归特征选择<br/> D)顺序特征选择器</p><p id="cf83" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">关于数据集的细节和整个代码(包括数据准备)可以在这个<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE/ML-based-feature-selection.git" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到。所以事不宜迟，让我们开始吧！</p><h1 id="622a" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated"><strong class="ak"> A)贝塔系数</strong></h1><p id="b9f8" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">关于贝塔系数，首先要理解的是，它是基于回归模型的。最简单的回归模型是线性回归，它试图拟合一个方程，该方程线性地解释目标变量和特征之间的关系。它表示为:</p><p id="095b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="nw"> Y = b0 + b1*X1 + b2*X2 + b3*X3 + …。bn*Xn + u </em></p><p id="0e70" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">其中Y是目标变量，X1…Xn是特征，b1…bn是系数，u是误差项。系数告诉我们，如果特性增加1个单位，那么对目标变量的边际(保持所有其他特性不变)影响是什么。深入回归模型(如假设、诊断检查等。)，我会建议通过以下方式:</p><ol class=""><li id="ddc9" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><a class="ae le" href="https://hbr.org/2015/11/a-refresher-on-regression-analysis" rel="noopener ugc nofollow" target="_blank">回归分析回顾</a></li><li id="0c6e" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://dss.princeton.edu/online_help/analysis/regression_intro.htm" rel="noopener ugc nofollow" target="_blank">线性回归假设</a></li><li id="23f4" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/" rel="noopener ugc nofollow" target="_blank">多元回归分析简介</a></li><li id="0fa2" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.amazon.in/Basic-Econometrics-Damodar-Gujarati/dp/0071333452" rel="noopener ugc nofollow" target="_blank">基础计量经济学(第2-13章)</a>(深入涵盖所有回归相关主题)</li></ol><p id="b5e8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">我们为什么要标准化特征？<br/> </em> </strong>关于系数，需要注意的一件有趣的事情是，它们受特征比例的影响。例如，如果我们的目标是GDP，而特征是以百万卢比为单位的预算赤字和以%为单位的中央银行回购利率，我们无法比较系数来判断哪个特征对GDP的影响更大。为了使系数具有可比性，我们需要将特征标准化。标准化特征包括以下步骤:</p><ol class=""><li id="7e6f" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><em class="nw">计算特征的平均值</em></li><li id="72ca" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><em class="nw">计算特征的标准偏差</em></li><li id="4ff8" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><em class="nw">对于每个观察值，减去平均值，然后除以标准偏差</em></li></ol><p id="589b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">标准化后，每个特征的平均值为0，标准偏差为1，因此具有可比性。<strong class="li ja">这种标准化特征的系数称为β系数。</strong>我们可以根据beta系数的绝对值对特性进行排序，并挑选出前n个特性(n可以由开发人员根据业务上下文和<a class="ae le" href="https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)" rel="noopener ugc nofollow" target="_blank">自由度</a>来决定)。</p><p id="3fea" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"><em class="nw">logistic回归的贝塔系数:<br/> </em> </strong>我们前面说的回归模型是一种线性回归模型，只能预测连续变量。为了计算分类问题的贝塔系数，我们需要使用逻辑回归。它可以表示为:</p><p id="9618" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">ln(pi/(1-pi))=<em class="nw">B0+B1 * X1+B2 * X2+B3 * X3+…。bn*Xn + u </em></p><p id="8ef5" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">其中一切都保持不变，除了不是预测目标变量(它将有不同的类别，如事件和非事件)，系数告诉我们，如果特征增加1个单位，那么对目标变量的概率对数的边际影响是什么。</p><p id="5e4e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">例如，假设我们的目标是政府是否会违约，特征是以百万卢比为单位的预算赤字和以%为单位的中央银行回购利率。预算赤字系数告诉我们，如果赤字增加100万卢比，那么对政府违约概率的对数，即log(违约概率/未违约概率)的边际影响是什么。</p><p id="dcfb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">尽管逻辑回归和线性回归之间存在许多其他差异(如使用OLS计算线性回归，而逻辑回归使用最大似然估计，因为后者是非线性的)，但我们在这里不会深入探讨这些差异。关于逻辑回归的更多细节，我建议浏览以下内容:</p><ol class=""><li id="4ebd" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated"><a class="ae le" href="https://www.ibm.com/in-en/topics/logistic-regression#:~:text=Logistic%20regression%20estimates%20the%20probability,bounded%20between%200%20and%201." rel="noopener ugc nofollow" target="_blank">什么是逻辑回归？</a></li><li id="a9d8" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/" rel="noopener ugc nofollow" target="_blank">逻辑回归的最大似然估计</a></li><li id="0532" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" rel="noopener" target="_blank" href="/logistic-regression-detailed-overview-46c4da4303bc">详细概述</a></li><li id="8d36" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.amazon.in/Basic-Econometrics-Damodar-Gujarati/dp/0071333452" rel="noopener ugc nofollow" target="_blank">基础计量经济学(第十五章)</a></li><li id="327f" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://francish.netlify.app/post/comparing-coefficients-across-logistic-regression-models/" rel="noopener ugc nofollow" target="_blank">逻辑的贝塔系数</a></li></ol><p id="7253" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">计算逻辑回归贝塔系数的Python函数:</em> </strong></p><p id="d3f0" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了使用逻辑回归计算β系数，我们首先使用<code class="fe nx ny nz oa b">StandardScaler</code>函数来标准化数据集，然后使用scikit-learn包中的<code class="fe nx ny nz oa b">LogisticRegression</code>函数来拟合没有惩罚项和截距的逻辑回归。函数<code class="fe nx ny nz oa b">beta_coeff</code>计算每个特征的贝塔系数，并根据贝塔系数的绝对值选择前n个(n由用户作为参数<code class="fe nx ny nz oa b">beta_threshold</code>提供)特征。然后，它将另一个套索回归与所选要素进行拟合，以允许开发人员查看所选要素的回归行为(如要素的符号和重要性)。</p><pre class="kp kq kr ks gt ob oa oc od aw oe bi"><span id="a00b" class="of na iq oa b gy og oh l oi oj">#3. Select  the top n features based on absolute value of beta coefficient of features</span><span id="3f4e" class="of na iq oa b gy ok oh l oi oj"># Beta Coefficients</span><span id="fb38" class="of na iq oa b gy ok oh l oi oj">beta_threshold = 10</span><span id="08cc" class="of na iq oa b gy ok oh l oi oj">################################ Functions #############################################################</span><span id="e2e2" class="of na iq oa b gy ok oh l oi oj">def beta_coeff(data, train_target,beta_threshold):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # beta_threshold - select n features with highest absolute beta coeficient value<br/>    <br/>    # Standardise dataset</span><span id="e04f" class="of na iq oa b gy ok oh l oi oj">    scaler = StandardScaler()<br/>    data_v2 = pd.DataFrame(scaler.fit_transform(data))<br/>    data_v2.columns = data.columns</span><span id="0299" class="of na iq oa b gy ok oh l oi oj"># Fit Logistic on Standardised dataset<br/>    # Manual Change in Parameters - Logistic Regression<br/>    # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>       <br/>    log = LogisticRegression(fit_intercept = False, penalty = 'none')<br/>    log.fit(data_v2, train_target)<br/>    coef_table = pd.DataFrame(list(data_v2.columns)).copy()<br/>    coef_table.insert(len(coef_table.columns), "Coefs",    log.coef_.transpose())<br/>    coef_table = coef_table.iloc[coef_table.Coefs.abs().argsort()]<br/>    sr_data2 = coef_table.tail(beta_threshold)<br/>    beta_top_features = sr_data2.iloc[:,0].tolist()<br/>    print(beta_top_features)<br/>    <br/>    beta_top_features_df = pd.DataFrame(beta_top_features,columns = ['Feature'])<br/>    beta_top_features_df['Method'] = 'Beta_coefficients'</span><span id="7a80" class="of na iq oa b gy ok oh l oi oj">    log_v2 = sm.Logit(train_target,\<br/>                     sm.add_constant(data[beta_top_features])).fit()<br/>    print('Logistic Regression with selected features')<br/>    print(log_v2.summary())<br/>    <br/>    return log,log_v2,beta_top_features_df</span><span id="2b99" class="of na iq oa b gy ok oh l oi oj">################################ Calculate Beta Coeff ################################################</span><span id="8ab9" class="of na iq oa b gy ok oh l oi oj">standardised_logistic,logistic_beta_features,beta_top_features_df = beta_coeff(train_features_v2,train_target,beta_threshold)</span><span id="6139" class="of na iq oa b gy ok oh l oi oj">beta_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/ebe0af5cfbda0828074c4b5b2abe221e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WTQ5cB9RCMBIuRLNuTgJgA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="fe2a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">计算线性回归贝塔系数的代码可以在<a class="ae le" href="https://medium.com/analytics-vidhya/demystifying-feature-engineering-and-selection-for-driver-based-forecasting-50ba9b5a0fbc" rel="noopener">这里</a>找到。</p><h1 id="79cc" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">拉索回归</h1><p id="2845" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">套索回归是可用于特征选择的少数嵌入式方法之一。这是线性/逻辑回归的自然延伸，其中使用惩罚项来选择特征。lasso回归的<a class="ae le" href="https://www.analyticsvidhya.com/blog/2021/03/data-science-101-introduction-to-cost-function/" rel="noopener ugc nofollow" target="_blank">成本函数</a>(需要最小化以获得最佳系数值的函数)可以表示为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6073dce0eb15eb07c9df0f1125588282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*eXXLDltJj0QfmLPVRDfbww.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="f09e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">附加罚项本质上是系数的绝对和乘以一个因子(称为调谐参数，我们将很快讨论)，这个过程称为L1正则化。因此，随着我们不断增加功能，我们会不断将功能的系数添加到惩罚项中，这将增加成本。这种类型的正则化(L1)导致一些最不重要的特征的系数为零，因此，我们可以使用这种模型来选择特征(要了解更多关于正则化的信息，请查看此<a class="ae le" rel="noopener" target="_blank" href="/intuitions-on-l1-and-l2-regularisation-235f2db4c261">博客</a>)。同样，可以将相同的惩罚项应用于逻辑回归成本函数来选择特征。</p><p id="55f8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">设置调整参数<br/> </em> </strong>使用L1正则化时要考虑的主要问题是调整参数λ的值，因为它控制惩罚的强度。从等式中可以看出，将其设置为0相当于线性回归。设置调节参数的一种方法是尝试一系列值，为每个值建立回归，并选择AIC分数最低的一个值。可以从测试0.01和1.0之间的值开始，网格间距为0.01。</p><p id="17f8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">关于lasso回归的更多细节，可以浏览下面的讲座<a class="ae le" href="https://www.stat.cmu.edu/~ryantibs/datamining/lectures/17-modr2.pdf" rel="noopener ugc nofollow" target="_blank">幻灯片</a>。</p><p id="5bc2" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">向逻辑回归添加L1正则化的Python函数(带逻辑回归的Lasso):</em></strong></p><p id="52eb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">函数<code class="fe nx ny nz oa b">lasso</code>用用户提供的调整参数值作为参数<code class="fe nx ny nz oa b">lasso_param</code>拟合带有l1惩罚项的逻辑回归。与上一个函数类似，它随后会用所选要素拟合另一个套索回归。</p><pre class="kp kq kr ks gt ob oa oc od aw oe bi"><span id="2f92" class="of na iq oa b gy og oh l oi oj">#4. Select the features identified by Lasso regression</span><span id="16c0" class="of na iq oa b gy ok oh l oi oj"># Lasso</span><span id="31ad" class="of na iq oa b gy ok oh l oi oj">lasso_param = .01</span><span id="6b8b" class="of na iq oa b gy ok oh l oi oj">################################ Functions #############################################################</span><span id="b313" class="of na iq oa b gy ok oh l oi oj">def lasso(data, train_target,lasso_param):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # lasso_param - Lasso l1 penalty term<br/>    <br/>    #Fit Logistic<br/>    # Manual Change in Parameters - Logistic Regression<br/>    # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>       <br/>    log = LogisticRegression(penalty ='l1', solver = 'liblinear',\<br/>                             C = lasso_param)<br/>    log.fit(data, train_target)<br/>    <br/>    #Select Features<br/>    lasso_df = pd.DataFrame(columns = ['Feature', 'Lasso_Coef'])<br/>    lasso_df['Feature'] = data.columns<br/>    lasso_df['Lasso_Coef'] = log.coef_.squeeze().tolist()<br/>    lasso_df_v2 = lasso_df[lasso_df['Lasso_Coef'] !=0]<br/>    lasso_top_features = lasso_df_v2['Feature'].tolist()<br/>    <br/>    lasso_top_features_df = pd.DataFrame(lasso_top_features,\<br/>                                         columns = ['Feature'])<br/>    lasso_top_features_df['Method'] = 'Lasso'</span><span id="f3c3" class="of na iq oa b gy ok oh l oi oj"># Logistic Regression with selected features<br/>    log_v2 = sm.Logit(train_target,\<br/>                   sm.add_constant(data[lasso_top_features])).fit()    <br/>    print('Logistic Regression with selected features')<br/>    print(log_v2.summary())<br/>    <br/>    return log_v2,lasso_top_features_df</span><span id="258b" class="of na iq oa b gy ok oh l oi oj">################################ Calculate Lasso ################################################</span><span id="05b5" class="of na iq oa b gy ok oh l oi oj">logistic_lasso_features,lasso_top_features_df = lasso(train_features_v2,train_target,lasso_param)</span><span id="10ed" class="of na iq oa b gy ok oh l oi oj">lasso_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/fe96ce79272f8728929a88bde37f2ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0NFWJMxTOyiYIC40xgc0w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="05d5" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">通过用<code class="fe nx ny nz oa b">LinearRegression</code>改变<code class="fe nx ny nz oa b">LogicticRegression</code>函数，用<code class="fe nx ny nz oa b">OLS</code>改变<code class="fe nx ny nz oa b">Logit</code>，同样的函数可以很容易地用于线性回归</p><h1 id="108c" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">c)递归特征消除(RFE)</h1><p id="63fb" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">这是python的<code class="fe nx ny nz oa b">Scikit-learn</code>包为特征选择提供的两种流行的特征选择方法之一。虽然RFE在技术上是一个包装器风格的方法，但它是基于基于过滤器的方法所使用的过程的。让我们看看怎么做。</p><p id="c487" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"><em class="nw">RFE是如何工作的？</em>T13】</strong></p><ol class=""><li id="4c1b" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">RFE首先在整个特征集上训练用户定义的模型。用户定义的模型可以是任何具有<code class="fe nx ny nz oa b">fit</code>方法的监督学习估计器，该方法提供关于特征重要性的信息。</li><li id="350b" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">然后，它根据训练好的模型计算每个特征的重要性。重要性可以通过任何特定于模型的属性(如<code class="fe nx ny nz oa b">coef_</code>、<code class="fe nx ny nz oa b">feature_importances_</code>)或可以为每个特性计算的用户定义的指标来获得。</li><li id="ca93" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">然后，它会丢弃最不重要的特征，并使用数量减少的特征重复步骤1和2</li><li id="49e3" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">重复步骤3，直到剩余用户定义的特征数量</li></ol><p id="0f46" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">RFE被认为是一种包装方法，因为它“包装”在外部估计量周围。但是它也是基于基于过滤器的方法的原理，这是由于基于重要性的度量对特征进行排序。</p><p id="2f97" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">如何使用RFE自动拾取特征的数量？<br/> </em> </strong>挑选特性的数量并不简单，值得庆幸的是<strong class="li ja"> <em class="nw"> </em> </strong> <code class="fe nx ny nz oa b">Scikit-learn</code>软件包有一个新的功能叫做<code class="fe nx ny nz oa b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">RFECV</strong></a></code>(带交叉验证的递归特性消除)，它把我们从这个负担中解放了出来。交叉验证是一种模型性能测试策略，最流行的版本是k-fold交叉验证。我们首先将数据分成k个随机部分，然后在除了一个(k-1)部分之外的所有部分上训练模型，最后，在不用于训练的部分上评估模型。这被重复k次，每次保留不同的部分用于评估。</p><p id="9a61" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果我们有n个特性，<code class="fe nx ny nz oa b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">RFECV</strong></a></code>实际上执行n-1次RFE和交叉验证(在每次迭代中丢弃最不重要的特性)。然后，它挑选交叉验证分数最大化的特征的数量。</p><p id="2f4a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw"> Python函数用于实现</em></strong><code class="fe nx ny nz oa b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">RFECV</strong></a></code><strong class="li ja"><em class="nw">:</em></strong></p><p id="3737" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们在这里使用scikit-learn的<code class="fe nx ny nz oa b">RFECV</code>函数，关于算法的选择，它的配置就像RFE类一样。功能<code class="fe nx ny nz oa b">rfecv_feature_selection</code>允许用户从5种流行的基于树的算法中进行选择:XG Boost、Random Forest、Catboost、Light GBM和决策树。我将很快就每种方法写一个新的博客系列。该函数从用户使用<code class="fe nx ny nz oa b">rfe_estimator</code>参数指定的列表中设置任何估计值。如果没有提到，默认情况下决策树是合适的。他们还可以更改许多其他功能，如评分标准，以选择最佳数量的功能(参见下面代码中函数的输入部分)。</p><pre class="kp kq kr ks gt ob oa oc od aw oe bi"><span id="a180" class="of na iq oa b gy og oh l oi oj">#5. Select features based on Recursive Feature Selection method</span><span id="817b" class="of na iq oa b gy ok oh l oi oj"># RFECV</span><span id="08d4" class="of na iq oa b gy ok oh l oi oj">rfe_estimator = "XGBoost"<br/>rfe_step = 2<br/>rfe_cv = 5<br/>rfe_scoring = 'f1'</span><span id="17b6" class="of na iq oa b gy ok oh l oi oj">################################ Functions #############################################################</span><span id="a9c4" class="of na iq oa b gy ok oh l oi oj">def rfecv_feature_selection(data, train_target,rfe_estimator,rfe_step,rfe_cv,rfe_scoring):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # rfe_estimator - base model (default: Decision Tree)<br/>    # rfe_step -  number of features to remove at each iteration<br/>    # rfe_cv - cross-validation splitting strategy<br/>    # rfe_scoring - CV performance scoring metric</span><span id="ab73" class="of na iq oa b gy ok oh l oi oj">## Initialize RFE</span><span id="db71" class="of na iq oa b gy ok oh l oi oj">    if rfe_estimator == "XGBoost":<br/>        # Manual Change in Parameters - XGBoost<br/>        # Link to function parameters - <a class="ae le" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/en/stable/parameter.html</a>       <br/>        estimator_rfe = XGBClassifier(n_jobs = -1, random_state=101)<br/>    elif rfe_estimator == "RandomForest":<br/>        # Manual Change in Parameters - RandomForest<br/>        # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a><br/>        estimator_rfe = RandomForestClassifier(n_jobs = -1, random_state=101)<br/>    elif rfe_estimator == "CatBoost":<br/>        # Manual Change in Parameters - CatBoost<br/>        # Link to function parameters - <a class="ae le" href="https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier" rel="noopener ugc nofollow" target="_blank">https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier</a><br/>        estimator_rfe = CatBoostClassifier(iterations=50,verbose=0,random_state=101)<br/>    elif rfe_estimator == "LightGBM":<br/>        # Manual Change in Parameters - LightGBM<br/>        # Link to function parameters - <a class="ae le" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html" rel="noopener ugc nofollow" target="_blank">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html</a><br/>        estimator_rfe = lgb.LGBMClassifier(n_jobs = -1, random_state=101)<br/>    else:<br/>        # Manual Change in Parameters - DecisionTree<br/>        # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a><br/>        estimator_rfe = DecisionTreeClassifier(random_state=101)</span><span id="86f8" class="of na iq oa b gy ok oh l oi oj"># Fit RFECV<br/>    # Manual Change in Parameters - RFECV<br/>    # Link to function parameters - <a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html</a><br/>    # Scoring metrics - <a class="ae le" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/model_evaluation.html</a><br/>    rfecv = RFECV(estimator = estimator_rfe, step = rfe_step, cv = rfe_cv, scoring = rfe_scoring)<br/>    rfecv.fit(data, train_target)</span><span id="8d65" class="of na iq oa b gy ok oh l oi oj"># Select feature based on RFE<br/>    print('Optimal number of features: {}'.format(rfecv.n_features_))<br/>    rfe_df = pd.DataFrame(columns = ['Feature', 'rfe_filter'])<br/>    rfe_df['Feature'] = data.columns<br/>    rfe_df['rfe_filter'] = rfecv.support_.tolist()<br/>    rfe_df_v2 = rfe_df[rfe_df['rfe_filter']==True]<br/>    rfe_top_features = rfe_df_v2['Feature'].tolist()<br/>    print(rfe_top_features)<br/>    <br/>    rfe_top_features_df = pd.DataFrame(rfe_top_features,columns = ['Feature'])<br/>    rfe_top_features_df['Method'] = 'RFECV'</span><span id="5b70" class="of na iq oa b gy ok oh l oi oj"># Plot CV results<br/>    %matplotlib inline<br/>    plt.figure(figsize=(16, 9))<br/>    plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)<br/>    plt.xlabel('Number of features selected', fontsize=14, labelpad=20)<br/>    plt.ylabel('f1 acore', fontsize=14, labelpad=20)<br/>    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)</span><span id="df87" class="of na iq oa b gy ok oh l oi oj">    plt.show()<br/>    <br/>    return rfe_top_features_df,rfecv</span><span id="64a4" class="of na iq oa b gy ok oh l oi oj">################################ Calculate RFECV #############################################################</span><span id="c597" class="of na iq oa b gy ok oh l oi oj">rfe_top_features_df,rfecv = rfecv_feature_selection(train_features_v2,train_target,rfe_estimator,rfe_step,rfe_cv,rfe_scoring)<br/>rfe_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/8a0fd0a68dc249542b7228a4397e20b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b2RC13BVGsDRyThZdIdzZw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="b35e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我个人不使用<code class="fe nx ny nz oa b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">RFECV</strong></a></code> <strong class="li ja"> </strong>带回归模型作为基于系数值的排序特征，如果特征不在同一尺度上是不可取的。</p><h1 id="5d67" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">d)顺序特征选择(SFS)</h1><p id="95fd" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">顺序特征选择(SFS)是<code class="fe nx ny nz oa b">Scikit-learn</code>包提供的另一种包装器类型的特征选择方法。RFE和SFS的区别在于，它不需要底层模型来计算特征重要性分数。它还可以灵活地进行向前(从1个特征开始，随后向模型添加特征)或向后(从所有特征开始，随后向模型移除特征)特征选择，而RFE只能进行向后选择。</p><p id="422b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">此外，SFS通常比RFECV慢。例如，在后向选择中，使用k重交叉验证从n个特征到n-1个特征的迭代需要拟合n*k个模型，而RFECV将只需要k次拟合。</p><p id="9cf1" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"><em class="nw">SFS是如何工作的？</em> </strong></p><p id="ec39" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="nw">前向SFS: </em></p><ol class=""><li id="7e6d" class="ml mm iq li b lj lk lm ln lp mn lt mo lx mp mb mq mr ms mt bi translated">SFS最初从没有特征开始，并找到使交叉验证分数最大化的特征</li><li id="df13" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">一旦选择了第一个特征，SFS通过向现有的所选特征添加新特征来重复该过程。</li><li id="c58d" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated">程序继续进行，直到达到由<code class="fe nx ny nz oa b">n_features_to_select</code>参数确定的所选特征的所需数量。如果用户没有指定要选择的功能的确切数量或性能改善的阈值，该算法会自动选择现有功能列表的一半。</li></ol><p id="b915" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="nw">向后SFS: </em></p><p id="84cc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">它具有相同的过程，但工作方向相反:它从所有特征开始，然后开始从集合中删除特征。</p><p id="7c79" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw">如何选择方向(向后vs向前)？</em> </strong></p><p id="e8ff" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><code class="fe nx ny nz oa b">direction</code>参数控制使用前向还是后向SFS。同样重要的是要注意向前和向后选择通常不会产生相同的结果。我们应该根据我们希望在模型中保留多少特征来选择方向。如果我们想保留大多数特征，我们应该选择“向后”，反之亦然。</p><p id="2bf9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> <em class="nw"> Python函数用于实现</em></strong><code class="fe nx ny nz oa b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector" rel="noopener ugc nofollow" target="_blank">SequentialFeatureSelector</a></code><strong class="li ja"><em class="nw">:</em></strong></p><p id="549f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们正在使用scikit的<code class="fe nx ny nz oa b">SequentialFeatureSelector</code>功能——点击此处了解。函数<code class="fe nx ny nz oa b">sfs_feature_selection</code>符合逻辑回归。我在这里使用逻辑回归，因为我们可以根据它们对<a class="ae le" href="https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit" rel="noopener ugc nofollow" target="_blank"> r平方</a>的影响来选择特征，并且我们已经在前面的方法中拟合了基于树的模型。用户还可以更改许多特性，例如要保留的特性数量(参见下面代码中函数的输入部分)。</p><pre class="kp kq kr ks gt ob oa oc od aw oe bi"><span id="cb5d" class="of na iq oa b gy og oh l oi oj">#6. Select features based on Sequential Feature Selector</span><span id="4908" class="of na iq oa b gy ok oh l oi oj"># Sequential Feature Selector</span><span id="05bc" class="of na iq oa b gy ok oh l oi oj">sfs_feature = 10<br/>sfs_direction = 'backward'<br/>sfs_cv = 2<br/>sfs_scoring = 'r2'</span><span id="cf11" class="of na iq oa b gy ok oh l oi oj">################################ Functions #############################################################</span><span id="6f24" class="of na iq oa b gy ok oh l oi oj">def sfs_feature_selection(data, train_target,sfs_feature,sfs_direction,sfs_cv,sfs_scoring):<br/>    <br/>    #Inputs<br/>    # data - Input feature data <br/>    # train_target - Target variable training data<br/>    # sfs_feature - no. of features to select<br/>    # sfs_direction -  forward and backward selection<br/>    # sfs_cv - cross-validation splitting strategy<br/>    # sfs_scoring - CV performance scoring metric</span><span id="227a" class="of na iq oa b gy ok oh l oi oj">    logistic = LogisticRegression(penalty = None)</span><span id="c681" class="of na iq oa b gy ok oh l oi oj">    sfs=SequentialFeatureSelector(estimator = logistic,<br/>                                  n_features_to_select=sfs_feature,   <br/>                                  direction = sfs_direction,<br/>                                  cv = sfs_cv,<br/>                                  scoring = sfs_scoring)<br/>    sfs.fit(train_features_v2, train_target)<br/>    sfs.get_support()</span><span id="9b69" class="of na iq oa b gy ok oh l oi oj">    sfs_df = pd.DataFrame(columns = ['Feature', 'SFS_filter'])<br/>    sfs_df['Feature'] = train_features_v2.columns<br/>    sfs_df['SFS_filter'] = sfs.get_support().tolist()</span><span id="6fc7" class="of na iq oa b gy ok oh l oi oj">    sfs_df_v2 = sfs_df[sfs_df['SFS_filter']==True]<br/>    sfs_top_features = sfs_df_v2['Feature'].tolist()<br/>    print(sfs_top_features)<br/></span><span id="9bde" class="of na iq oa b gy ok oh l oi oj">    x_temp = sm.add_constant(train_features_v2[sfs_top_features])</span><span id="4425" class="of na iq oa b gy ok oh l oi oj">    log_v2=sm.Logit(train_target,x_temp).fit()</span><span id="c2f1" class="of na iq oa b gy ok oh l oi oj">    print(log_v2.summary())<br/>    <br/>    sfs_top_features_df=pd.DataFrame(sfs_top_features\<br/>                                     ,columns = ['Feature'])<br/>    sfs_top_features_df['Method']='Sequential_feature_selector'</span><span id="c068" class="of na iq oa b gy ok oh l oi oj">    return sfs_top_features_df,sfs</span><span id="5260" class="of na iq oa b gy ok oh l oi oj">################################ Calculate RFECV #############################################################</span><span id="d1f3" class="of na iq oa b gy ok oh l oi oj">sfs_top_features_df,sfs = sfs_feature_selection(train_features_v2,train_target,sfs_feature,sfs_direction,sfs_cv,sfs_scoring)<br/>sfs_top_features_df.head(n=20)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/b0a2552334a2bcd3d6890505a0be5e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hmnuGDe2VhVTmaTtBdxnA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="603c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如前所述，用户可以通过更改代码的这一部分来适应任何其他方法:</p><pre class="kp kq kr ks gt ob oa oc od aw oe bi"><span id="6047" class="of na iq oa b gy og oh l oi oj">sfs=SequentialFeatureSelector(estimator = #add model here#<br/>                                  ,n_features_to_select=sfs_feature,   <br/>                                  direction = sfs_direction,<br/>                                  cv = sfs_cv,<br/>                                  scoring = sfs_scoring)</span></pre><h1 id="01eb" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">最后的话</h1><p id="1b8c" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">我们现在已经到了3部分博客系列的第2部分的结尾。读者现在应该熟悉6种不同的特征选择技术。他们还应该了解与其python实现相关的最佳实践。我想在这里强调的是，特征选择是艺术和科学的结合。虽然我们已经详细讨论了它背后的科学，但是随着经验的积累，人们会变得更好。</p><p id="d079" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">请务必阅读本系列的最后一篇<a class="ae le" href="https://indraneeldb1993ds.medium.com/deep-dive-on-ml-techniques-for-feature-selection-in-python-part-3-de2a7593247f" rel="noopener">部分</a>,在那里我们讨论了我们可以使用的最先进的技术(Borutapy，Borutashap ),并讨论了如何结合所有这些方法。最后，分析的全部代码可以在<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE/ML-based-feature-selection.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="5e6f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="8eed" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">参考材料</h1><ol class=""><li id="2c66" class="ml mm iq li b lj nr lm ns lp oq lt or lx os mb mq mr ms mt bi translated"><a class="ae le" href="https://hbr.org/2015/11/a-refresher-on-regression-analysis" rel="noopener ugc nofollow" target="_blank">回归分析的复习课</a></li><li id="548d" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://dss.princeton.edu/online_help/analysis/regression_intro.htm" rel="noopener ugc nofollow" target="_blank">线性回归假设</a></li><li id="660a" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/" rel="noopener ugc nofollow" target="_blank">多元回归分析简介</a></li><li id="fe96" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.amazon.in/Basic-Econometrics-Damodar-Gujarati/dp/0071333452" rel="noopener ugc nofollow" target="_blank">基础计量经济学(第2-13章)</a>(深入涵盖所有回归相关主题)</li><li id="4636" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.ibm.com/in-en/topics/logistic-regression#:~:text=Logistic%20regression%20estimates%20the%20probability,bounded%20between%200%20and%201." rel="noopener ugc nofollow" target="_blank">什么是逻辑回归？</a></li><li id="1ff2" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/" rel="noopener ugc nofollow" target="_blank">逻辑回归的最大似然估计</a></li><li id="6a1e" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" rel="noopener" target="_blank" href="/logistic-regression-detailed-overview-46c4da4303bc">详细概述</a></li><li id="d962" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://www.amazon.in/Basic-Econometrics-Damodar-Gujarati/dp/0071333452" rel="noopener ugc nofollow" target="_blank">基础计量经济学(第十五章)</a></li><li id="e570" class="ml mm iq li b lj mu lm mv lp mw lt mx lx my mb mq mr ms mt bi translated"><a class="ae le" href="https://francish.netlify.app/post/comparing-coefficients-across-logistic-regression-models/" rel="noopener ugc nofollow" target="_blank">逻辑斯谛的贝塔系数</a></li></ol><h1 id="1456" class="mz na iq bd nb nc nd ne nf ng nh ni nj kf nk kg nl ki nm kj nn kl no km np nq bi translated">我们连线吧！</h1><p id="b616" class="pw-post-body-paragraph lg lh iq li b lj nr ka ll lm ns kd lo lp nt lr ls lt nu lv lw lx nv lz ma mb ij bi translated">如果你和我一样，对人工智能、数据科学或经济学充满热情，请随时在<a class="ae le" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae le" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae le" href="https://medium.com/@indraneeldb1993ds" rel="noopener"> Medium </a>上添加/关注我。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b254b199af86c1d6521b398b1e5e73b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QGzYi9EOCJTtBXkl"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@howier" rel="noopener ugc nofollow" target="_blank"> Howie R </a>在<a class="ae le" href="https://unsplash.com/photos/CjI_2QX7hvU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>