<html>
<head>
<title>A Mathematical Breakdown of the Closed-Form Equation of Simple Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单线性回归封闭方程的数学分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-mathematical-breakdown-of-the-closed-form-equation-of-simple-linear-regression-8f8306127900#2022-07-15">https://towardsdatascience.com/a-mathematical-breakdown-of-the-closed-form-equation-of-simple-linear-regression-8f8306127900#2022-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1126" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">快速浏览，加强您的机器学习基础</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="19a6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">线性回归被商业、科学、工程等领域的专业人士广泛使用，但没有太多人理解(或关心)幕后的数学。这篇文章将引导读者进入数学领域，并希望在此过程中获得一些数学欣赏。</p><h1 id="a2dd" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">一.导言和问题陈述</h1><p id="2f4c" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">线性回归是一种统计模型，假设输入<strong class="kr iu"> x </strong>和输出<strong class="kr iu"> y </strong>之间的线性关系。目标是观察和预测。</p><p id="9b43" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">假设我们有n个观测值x_i，y_i其中i = 1，…，n，我们想得出一个可以根据x_i预测y_i的线性函数。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/672ce51c6bdb572deb2d8f52a44d3fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*llLpPIOBTYJbvVBxSGpRVA.png"/></div></div></figure><p id="0da7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在哪里</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mu"><img src="../Images/ce5e1366ce91dc38ebe77a0152318bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*li1yabXzVoaadPd-Es0dDw.png"/></div></div></figure><p id="b0a4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">每对β_0和β_1产生不同的谱线，但我们只对最好的谱线感兴趣。“最佳”线的定义是什么？我们如何找到最好的“β_0”和“β_1”？</p><h1 id="960f" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">二。剩余平方和</h1><p id="08e9" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">衡量线性回归性能的一种方法是找出它产生的误差。通过将预测值和实际值之间的所有差异(<strong class="kr iu">残差</strong>)相加，我们可以掌握什么是“坏的”，并从中尝试找到最佳模型。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mv"><img src="../Images/262630aa6b2e7d314f53def3f288f295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ms8kbUj87jfG7Mj-yLJRBA.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">估计回归线的残差(Huy Bui)</p></figure><p id="9534" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">换句话说，如果y_i是实际值，ŷ_i是预测值，那么成本函数<strong class="kr iu">的平方和</strong> ( <strong class="kr iu"> RSS </strong>)的残差被定义为</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi na"><img src="../Images/e04d2e2dbd3f3756ed30d857f9b33fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRXk2ooRcggyHFTgXwqpYw.png"/></div></div></figure><p id="2650" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们希望我们的预测是准确的，这意味着误差应该很小。因此，目标是找到β_0和β_1，使得RSS函数G(β_0，β_1)最小。</p><p id="2893" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">事实上，β_0和β_1有一个封闭的公式，</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nb"><img src="../Images/c19c375b34fe701ffd8eadb610bda2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGvdh176wwjWvPXvR_EwaQ.png"/></div></div></figure><p id="d9bb" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">让我们发现这个公式从何而来，从零开始推导！</p><h1 id="157c" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">三。求系数</h1><p id="3265" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">记得在微积分中，要求局部/全局极值，需要求导数，求临界点。</p><p id="2b8e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">类似地，我们可以使用偏导数找到成本函数G的临界点。这个临界点保证是<strong class="kr iu">全局最小值</strong>，因为G是凸函数。这里不讨论凸性的细节。然而，你可以把成本函数想象成一个溜冰场，全局最小值就是重力最终把你拉下来的地方。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nc"><img src="../Images/69444902fae7cb8d60a379054fc85141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6wLCv0nxI43jhHWOkhwCw.jpeg"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">滑板公园(资料来源:Unsplash)</p></figure><p id="5704" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在回到数学上来。如上所述，成本函数G定义如下:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi na"><img src="../Images/e04d2e2dbd3f3756ed30d857f9b33fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRXk2ooRcggyHFTgXwqpYw.png"/></div></div></figure><p id="f045" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">G对β_0的偏导数为:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nd"><img src="../Images/d415ae6e859bdb75c06f1c3bec2d168d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awgespmDbavxqo_2nwbOMg.png"/></div></div></figure><p id="c82f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">设置<strong class="kr iu"> ∂G/∂β_0 = 0 </strong>以获得</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ne"><img src="../Images/067add4e7ec5dc82cbbf9424ad3301c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZBaJoAwEI0zOHPToixLjg.png"/></div></div></figure><p id="a9a7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">将<strong class="kr iu"> β_0 = ȳ− β_1 x̄ </strong>代入我们得到的G(β_0，β_1)，</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nf"><img src="../Images/48d5f8f78e23fdbc35212f8ac7cbe183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZweO47LB9OtQrWyUZvjEvw.png"/></div></div></figure><p id="0754" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">类似地，对G相对于β_1求导得到，</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ng"><img src="../Images/4c4e42d9ac0a129ca69b941b2eef8bf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N8OB7mfDcaRyTR2W24eNfQ.png"/></div></div></figure><p id="6a26" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">设置<strong class="kr iu"> ∂G/∂β_0 = 0 </strong>获得，</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nh"><img src="../Images/fd0bf2d027858fc8437f82e47343616f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aadxlS9d7NRdD_dM-cedpQ.png"/></div></div></figure><h1 id="cf76" class="ll lm it bd ln lo lp lq lr ls lt lu lv jz lw ka lx kc ly kd lz kf ma kg mb mc bi translated">四。进一步讨论</h1><p id="e3b2" class="pw-post-body-paragraph kp kq it kr b ks md ju ku kv me jx kx ky mf la lb lc mg le lf lg mh li lj lk im bi translated">我们用微积分推导出了2D线性回归的公式。理论上，我们可以对3D或4D应用同样的程序。随着维数的增加，公式的复杂性呈指数增长，因此计算更高维的公式是不切实际的。</p><p id="cf71" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">已经开发了不同的优化技术来解决这个问题。其中一种叫做<strong class="kr iu">梯度下降</strong>。但这将是另一个不同的话题！</p><p id="6d39" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">感谢您的阅读。</p></div></div>    
</body>
</html>