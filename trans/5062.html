<html>
<head>
<title>Exploring Recommendation Systems: Review of Matrix Factorization &amp; Deep Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索推荐系统:矩阵分解和深度学习模型综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-recommendation-systems-review-of-matrix-factorization-deep-learning-models-74d51a3b4f20#2022-11-10">https://towardsdatascience.com/exploring-recommendation-systems-review-of-matrix-factorization-deep-learning-models-74d51a3b4f20#2022-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ece7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">推荐系统概述(替代最小二乘法、LightFM、神经网络矩阵分解和神经协同过滤)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/586795bb1fe4ca48cbaddc4e04f4a69c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UESgvZfLUxnjyJK3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">朱莉安娜·马尔他在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="2332" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.介绍</h1><p id="8d4e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在社交媒体网络上，有大量的半结构化数据。这项任务的数据集是从在线照片分享社交媒体网络 Flickr 上收集的。Flickr 允许用户分享照片并与其他人(朋友)交流。目标是向访问这个社交媒体平台的大量数据的每个用户推荐一个对象(图片)列表。训练数据集包含用于构建推荐系统的用户和项目(照片)之间的一组交互，并且包含评级的基本事实的验证数据被用于决定最终模型。除测试数据外，其余数据集不用于分析。</p><h1 id="1bae" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.推荐系统</h1><p id="7133" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">显式反馈是按特定比例排列的评级。大多数等级从 1 到 5 不等。隐式反馈以不同的形式收集，例如浏览、屏幕时间、点击、反应、查看等。，而明确的反馈是评级。当用户对一个事物的评价高于另一个时，它表达了他们的偏好。另一方面，用户的行为暗示了隐性反馈。“如果用户经常点击/查看/花费时间/对某个项目做出反应，这是他/她的兴趣的标志，”这是隐式反馈的基本思想。</p><p id="b647" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">显式反馈很简单，因为用户的评估可以很容易地理解为用户的偏好，使系统的预测更相关。然而，它远不如隐式反馈常见，因为用户不方便对他们交互的所有内容进行评分(他们很少这样做)。在巨大的数量中，隐式输入很容易收集。在这种情况下，评级列可以被认为是隐式反馈，因为它表示用户是否与项目交互。训练数据仅由 1 组成，表明所有可能的用户和项目组合都有交互。因此，我们将使用基于隐式的推荐系统作为初始模型。</p><h2 id="2c30" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2.1 矩阵分解</h2><p id="7ece" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">矩阵分解将一个大矩阵分解成两个乘积等于原矩阵的小矩阵。交替最小二乘法是一种矩阵分解，它将用户项目矩阵的维数减少到相当少的潜在或隐藏属性。它以非常高效的计算方式做到了这一点。</p><h2 id="f416" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2.1.1 交替最小二乘法(ALS)</h2><p id="2f24" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">替代最小二乘法是一个迭代优化过程，每次迭代的目的是创建原始数据的良好分解表示。让我们考虑一个大小为<strong class="lt iu"> u x i </strong>的矩阵<strong class="lt iu"> R </strong>，其中<strong class="lt iu"> u </strong>代表用户，<strong class="lt iu"> i </strong>代表一个项目。其思路是生成一个表示为<strong class="lt iu">U</strong><strong class="lt iu">x</strong><strong class="lt iu">f</strong>的<strong class="lt iu">矩阵 U </strong>其中<strong class="lt iu"> f </strong>表示隐藏特征，一个表示为<strong class="lt iu">f</strong>T22】x I 的矩阵<strong class="lt iu"> V </strong>。矩阵<strong class="lt iu"> U </strong>和<strong class="lt iu"> V </strong>由用户和物品如何与每个特征相关的权重组成。因此，如上所述，想法是计算<strong class="lt iu"> U </strong>和<strong class="lt iu"> V </strong>使得(Victor，2018)。</p><p id="e0bc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用最小二乘法随机分配<strong class="lt iu"> U </strong>和<strong class="lt iu"> V </strong>中的值，并且经过几次迭代，获得产生 R 的最佳近似值的最佳权重。<strong class="lt iu">对于交替最小二乘法，使用类似的想法，但是迭代优化 U 保持 V 固定，反之亦然</strong>。解决方案是合并一个项目的偏好<strong class="lt iu"> p </strong>与置信度<strong class="lt iu"> c </strong>其中用户<strong class="lt iu"> u </strong>对项目<strong class="lt iu"> i </strong>的偏好定义如下其中为用户对项目的未观察值(胡等 2008)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c8b43b185ed04463b74b6dd151128ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*O3pW_XzXD8OArS8p3Bd3UQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">配方一。</strong>偏好计算。作者使用 Jupyter 和 Latex 制作的图像。</p></figure><p id="4b54" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">信念与不同的置信水平相关联。0 值的𝑝ᵤᵢ与低信心相关联。用户没有对一个项目采取积极的行动可以归因于多种因素，而不是他们不喜欢它。同样，如果一件商品被消费，也不一定表明用户对它的偏好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/d4b5ff771d562a6f3385be36fa17c1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8HGd0m4pqKPdFPUV-nFlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图一。</strong>说明了用于 ALS 模型的成本函数。图片由作者使用 MS Word 编写。包括围绕成本函数和正则化的讨论的快照，以表示相关的公式。</p></figure><h2 id="ac6e" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">模型评估指标</h2><p id="30da" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">归一化贴现累积收益(NDCG) </strong> —使用以下断言衡量推荐的质量(Mhaskar，2015):</p><ul class=""><li id="9a7c" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">大多数相关结果比稍微相关的结果更有用，并且排序的结果独立于归一化</li><li id="6a69" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">如果相关结果出现在推荐列表中的较高位置，它们会更有用</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/cc07c7d05e32fa3cba6734759d83851d.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*EVrNPsOemuDx0hhmQ2aQ7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">配方二。</strong> NDCG 的计算。作者使用 Jupyter 和 Latex 制作的图像。</p></figure><p id="60f7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">贴现累积收益(DCG) </strong> —在推荐列表中出现在较低位置的相关文档受到惩罚，因为分级相关性值与结果的位置成对数比例地减少(Mhaskar，2015)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/6a651e21c424fe44ddb23b410ad31bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*hpZmhciWm0Q0MzFEckmtQA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">三级方程式。</strong> DCG 的计算。作者使用 Jupyter 和 Latex 制作的图像。</p></figure><p id="6610" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">在前 15 项推荐中被正确分类的验证项目</strong> —验证数据的想法是检查在使用用户和项目 id 时由 ALS 模型(根据训练数据训练)推荐的前 15 项是否来自包含用户交互的项目的验证数据，即来自验证数据的基本事实。在验证数据中，每个用户至少与一个项目有一次交互。因此，对于每个用户 id，我们取推荐的项目 id 和地面真值= 1 的项目 id 的交集，然后计算交集不为 0 的用户数量。我们可以将验证数据中正确预测的相互作用数量较多的模型视为最佳模型。</p><h2 id="3667" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2 . 1 . 3 ALS 的超参数调谐和输出</h2><p id="c4cf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因子(5，10，12，15，20)正则化(0.1)和 alpha (10，15，20 和 25)值的不同组合用于训练模型，然后在验证数据上进行测试，以分析 NDCG 并纠正项目的分类。结果如图 1 和图 2 所示。测试了 24 种不同的超参数组合(例如，下图中的 x 轴代表因子-正则化-α)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/551edff55107735208889ea3768d329f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7_URwN51lNfVJHH13na8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图二</strong>。从具有前 15 分的超参数验证数据中正确推荐的项目。具有最高 NDCG 分数的前三个参数用于产生测试建议并在 Kaggle 上验证。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/9a39bb405d9a4bfac74b6d0d60abd096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgPK3XNV-A1GwWnnStoNkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图 3 </strong>。来自验证数据的推荐项目的 NDCG 分数，跨具有前 15 分的超参数。</p></figure><h2 id="fc91" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2.2 LightFM</h2><p id="9bc7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Light FM 是一个针对隐式和显式反馈的开源推荐系统。使用 LightFM 创建的嵌入可以编码关于特性的有用语义信息，这些信息可以用于推荐任务。LightFM 模型的结构考虑了以下因素:</p><p id="3644" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该模型能够使用交互数据来学习用户和项目表示。如果用户喜欢不止一个项目，例如薯条和可乐，模型必须知道这两个项目是相似的。针对新用户和项目的模型扫描计算建议(Maciej &amp; Lyst，2015 年)</p><div class="nz oa gp gr ob oc"><a href="https://making.lyst.com/lightfm/docs/home.html" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">欢迎阅读 LightFM 的文档！- LightFM 1.16 文档</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">OSX 和 Windows 用户注意:默认情况下，LightFM 不会在 OSX 和 Windows 上使用 OpenMP，因此所有模型拟合…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">making.lyst.com</p></div></div></div></a></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/63b075ccb78fe39f7d5f21366e18994c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQ3_Y9cSQfD3gDnYQGH8FQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图 4。</strong>说明了用于 LightFM 模型的目标函数。图片由作者使用 MS Word 编写。包括围绕优化目标函数的讨论的快照，以表示相关的公式。</p></figure><p id="bd85" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">四个损失函数可用于 LightFM 逻辑损失函数、BPR 或贝叶斯个性化排序成对损失。弯曲或加权近似秩成对损失和 k-OS 弯曲或 k 阶统计损失。</p><p id="11ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">LightFM 模型对训练数据的输出在下面的表 1 中给出(库拉，2016)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/082f200df0d29d14acd11933af65272d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmYe4Ijw30IDIA01mAtoDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">表 1。</strong>使用 BPR、WARP 和 k-OS WARP 对训练数据建模验证指标和各自的分数。</p></figure><h2 id="1f0f" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2.3 用神经网络进行矩阵分解</h2><p id="78e7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">矩阵分解将用户和项目描绘成潜在特征的向量。这些特征被投影到一个共享的特征空间中，并从项目评级模式中推断出来。如果项目和用户之间的对应度高，则考虑向量<strong class="lt iu"> qᵢ ε Rᶠ </strong>。对于一个给定的项目<strong class="lt iu">I</strong>,<strong class="lt iu">qᵢ</strong>的元素衡量一个项目使用这些因素的好坏和程度。如果我们考虑一个用户<strong class="lt iu"> u </strong>，那么<strong class="lt iu"> pᵤ </strong>测量用户对相应因素高的项目的兴趣程度。<strong class="lt iu"> qᵢᵗ.的点积 pᵤ </strong>捕捉用户和物品的互动——用户对物品特征的整体兴趣。用户<strong class="lt iu"> u 对项目<strong class="lt iu"> i </strong>的</strong>评分用<strong class="lt iu">rᵤ<em class="on">ᵢ</em>t23】表示，等于<strong class="lt iu"> qᵢᵗ.pᵤ </strong>为了学习因子向量(<strong class="lt iu"> pᵤ &amp; q </strong> ᵢ)，已知评级集合上的正则化平方误差被最小化(Koren 等人，2009)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/b507d4499977f39f877c637efd475de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*oo96PSCNPQMdBd-mabik7A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">公式 4。</strong>误差函数。作者使用 Jupyter 和 Latex 制作的图像。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/fab56485fccc6ffd29791228ea3c94b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zvrVFb0ouCrqW3reu4gA2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图五。</strong>说明了使用神经网络的矩阵分解模型的训练功能。图片由作者使用 MS Word 编写。包括围绕优化目标函数的讨论的快照，以表示相关的公式。</p></figure><h2 id="fd49" class="ms la it bd lb mt mu dn lf mv mw dp lj ma mx my ll me mz na ln mi nb nc lp nd bi translated">2.4 神经协同过滤</h2><p id="ddd7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经协同过滤(NCF)捕捉用户和项目之间的交互。NCF 使用多层模型来学习用户和项目的交互功能。输入由用户<strong class="lt iu"> u (vᵤ) </strong>和项目<strong class="lt iu"> i (vᵢ) </strong>的稀疏特征向量组成，输出表示为<strong class="lt iu">(yᵤ<em class="on">ᵢ</em>)=f(pᵤ,q<em class="on">ᵢ</em>)</strong>。输入向量可以包括分类变量，如属性或上下文，而不仅仅是用户或项目。嵌入层由用户和项目潜在向量组成。神经协同过滤层由多个隐藏层组成，其中用户和项目潜在向量被连接。接着是多层感知器层，ReLU 作为激活函数，最后是输出层。多层感知器使用非线性函数来学习潜在因素不独立的交互作用，并且使用具有更好表示能力的数据来学习交互作用函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/b67fd5583e0956f408084913f8940b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xp1zpjugpoJLq_lVXmUa6Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图 6。</strong>NCF 框架的例子。该图摘自何湘南的<a class="ae ky" href="http://staff.ustc.edu.cn/~hexn/papers/www17-NCF.pptx" rel="noopener ugc nofollow" target="_blank">演示文稿</a>，由作者重新设计。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/65a29c9f6608533d133c2910136d5e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pi6x00jUMkAm-DHagINeYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">公式 5。</strong>神经网络架构。作者使用 Jupyter 和 Latex 制作的图像。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/0cb41e60a56184f540842237d6b8fb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lgbxl9Hz6T95ZebpEAbyNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">图 7 </strong>。使用 20 个时期的 NN 使用不同模型的训练损失。</p></figure><p id="f65b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于使用 Pytorch 的神经网络模型，使用不同的权重值、学习速率、时期和权重衰减来检查模型性能的改进。</p><div class="nz oa gp gr ob oc"><a href="https://d2l.ai/chapter_recommender-systems/neumf.html" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">17.6.个性化排序的神经协同过滤-深入学习…</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">这一节超越了显式反馈，介绍了神经协同过滤(NCF)框架…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">d2l.ai</p></div></div></div></a></div><h1 id="2a68" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.模型比较</h1><p id="d9c3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下表(表 2)提供了不同模型及其推荐准确度的概要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/911322db98ee87ef33b710d4c734f484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OeS066lIQFHmeeTTX7-8Fw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nf">表 2。</strong>展示了实现的模型、它们的用例以及验证指标。</p></figure><h1 id="e823" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak"> 4。结论</strong></h1><p id="1bb2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从分析中可以得出以下结论:</p><ol class=""><li id="f24a" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm ou nn no np bi translated">具有超参数因子α、正则化为 10、20 和 0.1 的隐式 ALS 模型为验证和测试数据产生最佳 NDCG</li><li id="ad3a" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">对于使用神经网络(NN)和神经协同滤波的矩阵分解(MF ),输出被认为是差的</li><li id="55de" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">神经网络模型易受初始权重、正则化、学习速率和权重衰减的影响，需要更多的微调以获得更好的输出</li><li id="fa57" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">使用 NN 向 MF 添加偏差提高了模型性能。这可以归因于模型的更好的概括能力，允许它捕捉观察到的信号。有偏见的 MF 可以受益于对用户行为的洞察</li><li id="3dc6" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">内积函数的选择会限制模型的表达能力，较高的潜在因子会降低泛化能力</li><li id="a2ef" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">何等人 2017 年进行的一项研究表明，如果堆叠更多的非线性层，更深的网络模型可以产生良好的性能。然而，当增加太多层时，优化困难会减少改进</li><li id="c35d" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm ou nn no np bi translated">神经协同过滤模型观察到由于线性层的堆叠而导致的性能下降</li></ol><p id="ec53" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在验证上具有最高 NDCG 分数的隐式 ALS 模型一起在训练和验证数据上被重新训练。从验证数据中保留的用户和交互项目被添加回训练数据。然后，使用上述参数的 ALS 模型(隐式 ALS —因子= 10，reg = 0.1，alpha = 20)用于产生最终建议。数据越多，测试集的性能越好。</p><h1 id="5fe8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">5.参考</h1><p id="4257" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.y .科伦、r .贝尔和 c .沃林斯基(2009 年)。推荐系统中的矩阵分解技术。计算机，42(8)，30–37。<a class="ae ky" href="https://doi.org/10.1109/mc.2009.263" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/mc.2009.263</a></p><p id="35c2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.胡，科伦，杨和沃林斯基(2008)。隐式反馈数据集的协同过滤。2008 年第八届 IEEE 数据挖掘国际会议。https://doi.org/10.1109/icdm.2008.22<a class="ae ky" href="https://doi.org/10.1109/icdm.2008.22" rel="noopener ugc nofollow" target="_blank"/></p><p id="2e47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.维克多。(2018 年 7 月 10 日)。ALS 隐式协同过滤。中等。<a class="ae ky" href="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe" rel="noopener">https://medium . com/radon-dev/als-implicit-collaborative-filtering-5ed 653 ba 39 Fe</a></p><p id="2ba9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">4.m .库拉(2016 年)。模型评估— LightFM 1.15 文档。Making.lyst.com。<a class="ae ky" href="https://making.lyst.com/lightfm/docs/lightfm.evaluation.html" rel="noopener ugc nofollow" target="_blank">https://making.lyst.com/light FM/docs/light FM . evaluation . html</a></p><p id="62f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">5.马切伊，k .，&amp; Lyst。(2015).用户和项目冷启动推荐的元数据嵌入。<a class="ae ky" href="https://arxiv.org/pdf/1507.08439.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1507.08439.pdf</a></p><p id="b7cd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">6.何，谢，廖，李，张，洪，聂，李，胡，谢，蔡东生(2017)。神经协同过滤。ArXiv:1708.05031 [Cs]。<a class="ae ky" href="https://arxiv.org/abs/1708.05031" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1708.05031</a></p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="831e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="on">关于作者:高级分析专家和管理顾问，帮助公司通过对组织数据的商业、技术和数学的组合找到各种问题的解决方案。一个数据科学爱好者，在这里分享、学习、贡献；你可以和我在</em> <a class="ae ky" href="https://www.linkedin.com/in/angel-das-9532bb12a/" rel="noopener ugc nofollow" target="_blank"> <em class="on">上联系</em> </a> <em class="on">和</em> <a class="ae ky" href="https://twitter.com/dasangel07_andy" rel="noopener ugc nofollow" target="_blank"> <em class="on">上推特</em></a><em class="on">；</em></p></div></div>    
</body>
</html>