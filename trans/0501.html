<html>
<head>
<title>2 Beautiful Ways to Visualize PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化PCA的两种美丽方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/2-beautiful-ways-to-visualize-pca-43d737e48ff7#2022-02-20">https://towardsdatascience.com/2-beautiful-ways-to-visualize-pca-43d737e48ff7#2022-02-20</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="2998" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">加上模型调整和评估，以选择最佳数量的组件。</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/367f662b873eb813db2310a42254ecd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Stvmp265pF2tJRWF"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated"><a class="ae kz" href="https://unsplash.com/@k8_iv?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> K8 </a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="5e6b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是PCA？</h1><p id="9af8" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><strong class="lu iv">主成分分析</strong>或<strong class="lu iv"> PCA </strong>是一种<em class="mo">降维技术</em>，用于具有许多特征或维度的数据集。它使用线性代数来确定数据集最重要的特征。在识别出这些特征后，您可以仅使用最重要的特征或那些<em class="mo">解释最大差异</em>的特征，来训练机器学习模型，并在不牺牲准确性的情况下提高模型的计算性能。</p><p id="69b9" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">PCA找到具有最大方差的轴，并将这些点投影到该轴上。PCA使用称为<strong class="lu iv">特征向量</strong>和<strong class="lu iv">特征值</strong>的线性代数概念。<a class="ae kz" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579" rel="noopener ugc nofollow" target="_blank">栈交换</a>上有个帖子很漂亮的解释了一下。</p><p id="31e7" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">这是一个用特征向量绘制的二维数据的快速视图。您可以看到其中一个向量是如何与方差最大的维度对齐的，而另一个向量是正交的。然后将数据投影到该轴上，减少数据量，但保持总体方差。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mu"><img src="../Images/e5bb88c597492edc39bcf7524d95feaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q-kOLSHBoQ4yMHqn.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="2efc" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">最近我写了一篇关于PCA如何用于图像压缩的文章。这次我想通过可视化来谈谈PCA的一些基础知识。当我在学习PCA以及它作为机器学习工具箱中的一个工具有多么强大时，我遇到了两种不同的方法来可视化PCA ，最终让它为我所用。我想我会与你分享这两种方法，并进一步展示模型如何在使用和不使用降维的情况下执行<em class="mo">和<em class="mo">。这两种方法是:</em></em></p><ol class=""><li id="7496" class="mv mw iu lu b lv mp ly mq mb mx mf my mj mz mn na nb nc nd bi translated"><strong class="lu iv">解释方差累积图</strong>:这个简单但是强大。它会立即告诉您数据中有多少方差是由每个分量解释的，以及不同分量的组合如何构成总方差。</li><li id="5fa8" class="mv mw iu lu b lv ne ly nf mb ng mf nh mj ni mn na nb nc nd bi translated"><strong class="lu iv">原始数据叠加的主成分</strong>:这个是我绝对喜欢的。你可以看到每一个主成分是如何带来稍微多一点的信息的，反过来，也就很难区分不同的成分。该图是解释方差累积图的完美伴侣。</li></ol><p id="63a4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">你听够了吗？我们走吧！</p><h1 id="99cc" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">入门指南</h1><p id="8f35" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们将使用的数据集是来自加州大学欧文分校机器学习知识库的<a class="ae kz" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">葡萄酒数据集</a>。数据集包含关于葡萄酒质量的数据。该数据集获得了<a class="ae kz" href="https://creativecommons.org/licenses/by/4.0/legalcode" rel="noopener ugc nofollow" target="_blank">知识共享署名4.0 </a>国际(CC BY 4.0)许可。</p><p id="04c6" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">当然，我们将从<strong class="lu iv">导入</strong>所需的包和<strong class="lu iv">加载数据</strong>开始。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="9522" class="no lb iu nk b be np nq l nr ns">import numpy as np<br/>import pandas as pd<br/>import warnings<br/>warnings.filterwarnings('ignore')<br/><br/># Machine Learning<br/>from sklearn import metrics<br/>from sklearn.metrics import ConfusionMatrixDisplay<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.pipeline import FeatureUnion<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.decomposition import PCA<br/>from sklearn.decomposition import IncrementalPCA<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/><br/># Plotting<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="0a9e" class="no lb iu nk b be np nq l nr ns">df = pd.read_csv("wine.csv")</span></pre><p id="e2ab" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">这个特殊的数据集有<strong class="lu iv"> 13个特征</strong>和一个名为<strong class="lu iv">类</strong>的目标变量，我们可以用它来进行<a class="ae kz" href="https://medium.com/@broepke/go-beyond-binary-classification-with-multi-class-and-multi-label-models-6ce91ca08264" rel="noopener">分类</a>。每个值都是连续的，因此我们可以直接将PCA应用于所有变量。通常我们会在更高维的数据集上使用PCA，但是这个数据集将显示概念。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="8847" class="no lb iu nk b be np nq l nr ns">df.info()</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="a526" class="no lb iu nk b be np nq l nr ns">[OUT]<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 178 entries, 0 to 177<br/>Data columns (total 14 columns):<br/> #   Column                Non-Null Count  Dtype  <br/>---  ------                --------------  -----  <br/> 0   Class                 178 non-null    int64  <br/> 1   Alcohol               178 non-null    float64<br/> 2   Malic acid            178 non-null    float64<br/> 3   Ash                   178 non-null    float64<br/> 4   Ash Alcalinity        178 non-null    float64<br/> 5   Magnesium             178 non-null    int64  <br/> 6   Total phenols         178 non-null    float64<br/> 7   Flavanoids            178 non-null    float64<br/> 8   Nonflavanoid phenols  178 non-null    float64<br/> 9   Proanthocyanins       178 non-null    float64<br/> 10  Color intensity       178 non-null    float64<br/> 11  Hue                   178 non-null    float64<br/> 12  OD280/OD315           178 non-null    float64<br/> 13  Proline               178 non-null    int64  <br/>dtypes: float64(11), int64(3)<br/>memory usage: 19.6 KB</span></pre><p id="a9e4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">快速检查不平衡数据向我们展示了这个数据集，不像大多数数据集，是相当平衡的。在这次演示中，我们将保持原样。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="636c" class="no lb iu nk b be np nq l nr ns">df['Class'].value_counts()</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="a25a" class="no lb iu nk b be np nq l nr ns">2    71<br/>1    59<br/>3    48<br/>Name: Class, dtype: int64</span></pre><p id="84ce" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">让我们创建我们的<code class="fe nu nv nw nk b">X</code>和<code class="fe nu nv nw nk b">y</code>变量，其中<code class="fe nu nv nw nk b">X</code>是除了类之外的每个特性，<code class="fe nu nv nw nk b">y</code>是类。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="0202" class="no lb iu nk b be np nq l nr ns">y = df['Class'].copy()<br/>X = df.drop(columns=['Class']).copy().values</span></pre><h1 id="31d5" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">查找解释的差异</h1><p id="ae32" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">第一步是找到每个主成分的<strong class="lu iv">解释方差</strong>。我们通过首先用<code class="fe nu nv nw nk b">StandardScalar</code>缩放我们的数据，然后用PCA模型拟合缩放后的数据来计算解释方差。<a class="ae kz" href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler" rel="noopener ugc nofollow" target="_blank">标准缩放器</a>是一个简单的转换，它将数据标准化为具有<code class="fe nu nv nw nk b">0</code>和<code class="fe nu nv nw nk b">0</code>单位方差的平均值，或<code class="fe nu nv nw nk b">1</code>的标准偏差。</p><p id="d97e" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">您还会注意到，主成分的总数<em class="mo">等于数据集中特征的总数<em class="mo"/>。然而，需要注意的一件重要事情是，主要组件不是数据集的列；相反，主成分分析正在构建新的特征来最好地解释数据。</em></p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="ac4b" class="no lb iu nk b be np nq l nr ns">def get_variance(X, n):<br/>    scaler = StandardScaler()<br/>    pca = PCA(n_components=n)<br/><br/>    pca.fit(scaler.fit_transform(X))<br/><br/>    return pca.explained_variance_ratio_.cumsum()[-1:]</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="eb82" class="no lb iu nk b be np nq l nr ns">for i in range(1,14):<br/>    print('Components:\t', i, '=\t', get_variance(X, i), <br/>          '\tCumulative Variance')</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="d82c" class="no lb iu nk b be np nq l nr ns">Components:  1 =     [0.36198848]   Cumulative Variance<br/>Components:  2 =     [0.55406338]   Cumulative Variance<br/>Components:  3 =     [0.66529969]   Cumulative Variance<br/>Components:  4 =     [0.73598999]   Cumulative Variance<br/>Components:  5 =     [0.80162293]   Cumulative Variance<br/>Components:  6 =     [0.85098116]   Cumulative Variance<br/>Components:  7 =     [0.89336795]   Cumulative Variance<br/>Components:  8 =     [0.92017544]   Cumulative Variance<br/>Components:  9 =     [0.94239698]   Cumulative Variance<br/>Components:  10 =    [0.96169717]   Cumulative Variance<br/>Components:  11 =    [0.97906553]   Cumulative Variance<br/>Components:  12 =    [0.99204785]   Cumulative Variance<br/>Components:  13 =    [1.]           Cumulative Variance</span></pre><p id="f97a" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">此外，每个主要分量加在一起，所有分量的总和将等于<code class="fe nu nv nw nk b">1</code>。当您浏览累积差异列表时，您会看到第<em class="mo">个组件是最重要的</em>，最后一个组件是最不重要的；换句话说，第一个成分对方差的贡献最大。随着我们包含越来越多的组件，每个组件的贡献量开始减少。让我们通过绘制累积方差来形象化这一点。</p><h1 id="cd01" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">绘制解释方差的阈值</h1><p id="3c6f" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">虽然这些值的打印输出是一个良好的开端，但我们可以通过绘制这些值来改进。此外，我们将绘制一条线，代表解释的方差的<code class="fe nu nv nw nk b">95%</code>。虽然没有规则规定模型中需要包含多少解释方差，但<code class="fe nu nv nw nk b">95%</code>是一个很好的起点。稍后，我们将执行网格搜索，以找到在模型中使用的最佳组件数量。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="7632" class="no lb iu nk b be np nq l nr ns">scaler = StandardScaler()<br/>data_rescaled = scaler.fit_transform(X)<br/><br/>pca = PCA().fit(data_rescaled)<br/><br/>plt.rcParams["figure.figsize"] = (8,5)<br/><br/>fig, ax = plt.subplots()<br/>xi = np.arange(1, 14, step=1)<br/>y = np.cumsum(pca.explained_variance_ratio_)<br/><br/>plt.ylim(0.0,1.1)<br/>plt.plot(xi, y, marker='o', linestyle='-', color='black')<br/><br/>plt.xlabel('Number of Components')<br/>plt.xticks(np.arange(1, 14, step=1)) <br/>plt.ylabel('Cumulative variance (%)')<br/>plt.title('The number of components needed to explain variance')<br/><br/>plt.axhline(y=0.95, color='grey', linestyle='--')<br/>plt.text(1.1, 1, '95% cut-off threshold', color = 'black', fontsize=16)<br/><br/>ax.grid(axis='x')<br/>plt.tight_layout()<br/>plt.savefig('pcavisualize_1.png', dpi=300)<br/>plt.show()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nx"><img src="../Images/f687e775c8a395023367c07796005adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FTp_6mBc-wRxxmml.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="0375" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">在绘制了累积解释方差后，我们可以看到曲线在<code class="fe nu nv nw nk b">6</code>或<code class="fe nu nv nw nk b">7</code>成分附近变得平缓。在我们为<code class="fe nu nv nw nk b">95%</code>绘制的线处，总的解释偏差大约在<code class="fe nu nv nw nk b">9</code>分量处。这就是解释差异可视化！让我们继续绘制实际的组件。</p><h1 id="01b8" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">绘制每个组件与原始数据的对比图</h1><p id="12da" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">下一步是可视化每个组件产生的数据，而不是解释的差异。我们将使用PCA模型的<code class="fe nu nv nw nk b">inverse_transform</code>方法，这将获取每个组件并将它们转换回原始数据规模。我们将绘制来自<code class="fe nu nv nw nk b">1-13</code>的整个元件范围，并用原始数据覆盖它们。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="0bbe" class="no lb iu nk b be np nq l nr ns"># Function to perform PCA with n-Components<br/>def transform_pca(X, n):<br/><br/>    pca = PCA(n_components=n)<br/>    pca.fit(X)<br/>    X_new = pca.inverse_transform(pca.transform(X))<br/><br/>    return X_new</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="eb96" class="no lb iu nk b be np nq l nr ns"># Plot the components<br/>rows = 4<br/>cols = 4<br/>comps = 1<br/><br/>scaler = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)<br/><br/>fig, axes = plt.subplots(rows, <br/>                         cols, <br/>                         figsize=(12,12), <br/>                         sharex=True, <br/>                         sharey=True)<br/><br/><br/>for row in range(rows):<br/>    for col in range(cols):<br/>        try:<br/>            X_new = transform_pca(X_scaled, comps)<br/>            ax = sns.scatterplot(x=X_scaled[:, 0], <br/>                                 y=X_scaled[:, 1], <br/>                                 ax=axes[row, col], <br/>                                 color='grey', <br/>                                 alpha=.3)<br/>            ax = sns.scatterplot(x=X_new[:, 0], <br/>                                 y=X_new[:, 1], <br/>                                 ax=axes[row, col], <br/>                                 color='black')<br/>            ax.set_title(f'PCA Components: {comps}');<br/><br/>            comps += 1<br/>        except:<br/>            pass<br/>plt.tight_layout()<br/>plt.savefig('pcavisualize_2.png', dpi=300)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mu"><img src="../Images/79286de6d6af14d564b9f857fa192888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gN2M95DNRLRPKsmX.png"/></div></div></figure><p id="a288" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">在我们的图中，灰色数据是原始数据，黑点是主成分。仅显示一个组件<strong class="lu iv">,它以一组点的形式投影在一条线上，这条线沿着原始数据中方差最大的轴。随着越来越多的组件被添加，我们可以看到数据开始与原始数据相似，即使显示的信息更少。当我们进入上限值时，数据开始与原始数据相匹配，最终，在所有13个分量上，它与原始数据相同。</strong></p><p id="a716" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">这是我最喜欢的视觉化PCA的方式！现在让我们用一个机器学习分类模型把它付诸行动。</p><h1 id="051d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">比较主成分分析和非主成分分析分类模型</h1><p id="4af5" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">让我们对数据集运行三个分类器，应用和不应用PCA，看看它们的性能如何。为了有趣起见，我们将只使用前两个组件(<code class="fe nu nv nw nk b">n_components=2</code>)来比较它们。我们将比较<code class="fe nu nv nw nk b">KNeighborsClassifier</code>、<code class="fe nu nv nw nk b">RandomForestClassifier</code>和<code class="fe nu nv nw nk b">LogisticRegression</code>作为我们的分类器，看看它们的表现如何。</p><p id="62cb" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">和往常一样，我们将使用一个<a class="ae kz" rel="noopener" target="_blank" href="/using-pipelines-in-sci-kit-learn-516aa431dcc5">管道</a>将我们的数据预处理和PCA合并到一个单独的步骤中。我们将使用之前使用的相同的<code class="fe nu nv nw nk b">StandardScaler</code>来缩放数据，然后用一定数量的组件来拟合数据。由于我们将在工作流程中多次使用该功能，因此我们将其设置为灵活的。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="9918" class="no lb iu nk b be np nq l nr ns">def create_pipe(clf, do_pca=False, n=2):<br/><br/>    scaler = StandardScaler()<br/>    pca = PCA(n_components=n)<br/><br/>    if do_pca == True:<br/>        combined_features = FeatureUnion([("scaler", scaler), <br/>                                          ("pca", pca)])<br/>    else:<br/>        combined_features = FeatureUnion([("scaler", scaler)])<br/><br/>    pipeline = Pipeline([("features", combined_features), <br/>                         ("clf", clf)])<br/><br/><br/>    return pipeline</span></pre><p id="cd25" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">接下来是循环不同的分类器并执行<strong class="lu iv">交叉验证</strong>。如上所述，我们将尝试三种不同的分类器，并在应用和不应用PCA的情况下分别测试它们。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="8a50" class="no lb iu nk b be np nq l nr ns">models = {'KNeighbors' : KNeighborsClassifier(),<br/>          'RandomForest' : RandomForestClassifier(random_state=42),<br/>          'LogisticReg' : LogisticRegression(random_state=42),<br/>          }<br/><br/><br/>def run_models(with_pca):<br/>    for name, model, in models.items():<br/>        clf = model<br/>        pipeline = create_pipe(clf, do_pca = with_pca, n=2)<br/>        scores = cross_val_score(pipeline, X, <br/>                                 y, <br/>                                 scoring='accuracy', <br/>                                 cv=3, n_jobs=1, <br/>                                 error_score='raise')<br/>        print(name, ': Mean Accuracy: %.3f and Standard Deviation: \<br/>             (%.3f)' % (np.mean(scores), np.std(scores)))<br/><br/>print(68 * '-')<br/>print('Without PCA')<br/>print(68 * '-')<br/>run_models(False)<br/>print(68 * '-')<br/>print('With PCA')<br/>print(68 * '-')<br/>run_models(True)<br/>print(68 * '-')</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="888c" class="no lb iu nk b be np nq l nr ns">--------------------------------------------------------------------<br/>Without PCA<br/>--------------------------------------------------------------------<br/>KNeighbors : Mean Accuracy: 0.944 and Standard Deviation: (0.021)<br/>RandomForest : Mean Accuracy: 0.961 and Standard Deviation: (0.021)<br/>LogisticReg : Mean Accuracy: 0.972 and Standard Deviation: (0.021)<br/>--------------------------------------------------------------------<br/>With PCA<br/>--------------------------------------------------------------------<br/>KNeighbors : Mean Accuracy: 0.663 and Standard Deviation: (0.059)<br/>RandomForest : Mean Accuracy: 0.955 and Standard Deviation: (0.021)<br/>LogisticReg : Mean Accuracy: 0.972 and Standard Deviation: (0.028)<br/>--------------------------------------------------------------------</span></pre><p id="4ca1" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">魔法！</strong>我们看到，在没有PCA的情况下，所有三个模型都表现得相当好。然而，只有前两个组件，<code class="fe nu nv nw nk b">RandomForest</code>的性能几乎相同，而<code class="fe nu nv nw nk b">LogisticRegression</code>的性能与原始组件完全相同！并不是所有的用例都是这样的，但是希望这能让我们一窥PCA的威力。如果你回头看看上面的图，每一个部分都重叠了，你可以看到需要的数据少了多少。</p><h1 id="cdb5" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">找到组件的最佳数量</h1><p id="4037" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">对于这一部分，我们将使用<code class="fe nu nv nw nk b">train_test_split</code>函数将数据集分割成<code class="fe nu nv nw nk b">70/30%</code>个分区，然后使用<code class="fe nu nv nw nk b">GridSearch</code>特性找到最佳参数。我们想要验证的最关键参数是各种<strong class="lu iv"> PCA组件号</strong>的性能。</p><p id="531d" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">正如我们从上面的散点图中看到的，当我们接近更高的数字(如<code class="fe nu nv nw nk b">9</code>)时，转换后的数据集看起来很像原始数据集，其中<code class="fe nu nv nw nk b">95%</code>是由转换后的数据集解释的变化。我们还看到只有<code class="fe nu nv nw nk b">2</code>组件表现良好，因此我们可以从<code class="fe nu nv nw nk b">1-13</code>开始测试整个范围并找到最佳设置。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="aa3f" class="no lb iu nk b be np nq l nr ns"># Make training and test sets <br/>X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.3, <br/>                                                    random_state=53)</span></pre><h1 id="246c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">调整模型</h1><p id="d86d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">接下来，我们将执行所谓的<strong class="lu iv">超参数调整</strong>。在利用默认参数基于交叉验证选择模型之后，我们执行网格搜索来微调模型并选择最佳参数。我们将循环三件事</p><p id="edba" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">查看<code class="fe nu nv nw nk b">LogisticRegression</code>的<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">文档</a>，了解更多参数信息。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="9855" class="no lb iu nk b be np nq l nr ns">def get_params(parameters, X, y, pipeline):<br/><br/>    grid = GridSearchCV(pipeline, <br/>                        parameters, <br/>                        scoring='accuracy', <br/>                        n_jobs=1, <br/>                        cv=3, <br/>                        error_score='raise')<br/>    grid.fit(X, y)<br/><br/>    return grid</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="fc02" class="no lb iu nk b be np nq l nr ns">clf = LogisticRegression(random_state=41)<br/>pipeline = create_pipe(clf, do_pca=True)<br/><br/>param_grid = dict(features__pca__n_components = list(range(2,14)),<br/>                 clf__C = [0.1, 1.0, 10, 100],<br/>                 clf__solver = ['liblinear', 'saga'],<br/>                 clf__penalty = ['l2', 'l1'])<br/><br/>grid = get_params(param_grid, X_train, y_train, pipeline)<br/><br/>print("Best cross-validation accuracy: {:.3f}".format(grid.best_score_))<br/>print("Test set score: {:.3f}".format(grid.score(X_test, y_test))) <br/>print("Best parameters: {}".format(grid.best_params_))</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="e817" class="no lb iu nk b be np nq l nr ns">Best cross-validation accuracy: 0.976<br/>Test set score: 0.944<br/>Best parameters: {'clf__C': 10, 'clf__penalty': 'l1', <br/>'clf__solver': 'liblinear', 'features__pca__n_components': 2}</span></pre><p id="9efa" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">就这么简单——在执行网格搜索后，我们可以看到一些执行得最好的设置。我们可以看到最好的参数是<code class="fe nu nv nw nk b">C=10</code>、<code class="fe nu nv nw nk b">penalty='l1'</code>、<code class="fe nu nv nw nk b">solver='liblinear'</code>和<code class="fe nu nv nw nk b">n_components=2</code>。我们还可以看到，测试集得分接近最佳交叉验证得分。我们还可以看到，从<code class="fe nu nv nw nk b">0.972</code>到<code class="fe nu nv nw nk b">0.976</code>，交叉验证的准确性有所提高。</p><p id="81d8" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">顺便提一下，这是典型的超参数调优。你<em class="mo">不会得到巨大的改进</em>；它是小的、渐进的改进，最大的收获是通过<em class="mo">特征工程</em>和<em class="mo">模型选择</em>。</p><h1 id="8a91" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">模型验证</h1><p id="c9f1" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">执行最终拟合，并用新参数进行测试。一个用于我们优化的组件数量(<code class="fe nu nv nw nk b">2</code>)，另一个用于一组没有进行PCA的数据。我们将打印出一份<strong class="lu iv">分类报告</strong>并生成一个<strong class="lu iv">混淆矩阵</strong>来比较结果。</p><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="f53b" class="no lb iu nk b be np nq l nr ns">def fit_and_print(pipeline):<br/><br/>    pipeline.fit(X_train, y_train)<br/>    y_pred = pipeline.predict(X_test)<br/><br/>    print(metrics.classification_report(y_test, y_pred, digits=3))<br/><br/>    ConfusionMatrixDisplay.from_predictions(y_test, <br/>                                            y_pred, <br/>                                            cmap=plt.cm.Greys)<br/><br/>    plt.tight_layout()<br/>    plt.ylabel('True Label')<br/>    plt.xlabel('Predicted Label')<br/>    plt.show;</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="07ac" class="no lb iu nk b be np nq l nr ns">clf = LogisticRegression(C=10, <br/>                         penalty='l1', <br/>                         solver='liblinear', <br/>                         random_state=41)<br/>pipeline = create_pipe(clf, do_pca=True, n=2)<br/>fit_and_print(pipeline)</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="c15d" class="no lb iu nk b be np nq l nr ns">              precision    recall  f1-score   support<br/><br/>           1      0.933     1.000     0.966        14<br/>           2      1.000     0.864     0.927        22<br/>           3      0.900     1.000     0.947        18<br/><br/>    accuracy                          0.944        54<br/>   macro avg      0.944     0.955     0.947        54<br/>weighted avg      0.949     0.944     0.944        54</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nx"><img src="../Images/880545410bd0c4730ce10282de44e574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t30rhEBSRmoJjtgB.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><pre class="kk kl km kn gu nj nk nl bn nm nn bi"><span id="9ce5" class="no lb iu nk b be np nq l nr ns">clf = LogisticRegression(C=10, <br/>                         penalty='l1', <br/>                         solver='liblinear', <br/>                         random_state=41)<br/>pipeline = create_pipe(clf, do_pca=False)<br/>fit_and_print(pipeline)</span></pre><pre class="nt nj nk nl bn nm nn bi"><span id="504b" class="no lb iu nk b be np nq l nr ns">              precision    recall  f1-score   support<br/><br/>           1      1.000     1.000     1.000        14<br/>           2      1.000     0.955     0.977        22<br/>           3      0.947     1.000     0.973        18<br/><br/>    accuracy                          0.981        54<br/>   macro avg      0.982     0.985     0.983        54<br/>weighted avg      0.982     0.981     0.982        54</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nx"><img src="../Images/54f1ad9861091523d87155dc10e90e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6QRc51toRKWsCTMJ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="cdb4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">最后，虽然PCA的性能低于完整数据集，但我们可以看到，仅使用前两个组件，我们就能够非常接近。如果我们有一个更大、更高维的数据集，我们可以显著提高性能，而不会显著降低精度。</p><h1 id="fecd" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="f0f4" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">PCA是一种不可思议的工具，可以自动将数据转换为更简单的表示，而不会丢失太多信息。您可以将PCA应用于数据集中的连续变量，然后通过网格搜索进行优化，为您的模型和用例找到最佳数量的组件。虽然这只是一个简单的概述，但我希望这对您有用！</p><p id="ac43" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">这篇文章的所有代码都可以在<a class="ae kz" href="https://github.com/broepke/PCA/blob/main/PCA.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="8663" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，考虑注册成为一名媒体会员。一个月5美元，让你可以无限制地访问成千上万篇文章。如果您使用 <a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener"> <em class="mo">【我的链接】</em> </a> <em class="mo">注册，我会为您赚取一小笔佣金，无需额外费用。</em></p><h1 id="76f8" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">参考</h1><p id="f48d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">Dua，d .，&amp; Graff，C. (2017)。UCI机器学习知识库。加州大学欧文分校信息与计算机科学学院。<a class="ae kz" href="http://archive.ics.uci.edu/ml" rel="noopener ugc nofollow" target="_blank">http://archive.ics.uci.edu/ml</a></p></div></div>    
</body>
</html>