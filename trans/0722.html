<html>
<head>
<title>The Transformer Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-is-all-you-need-e498378552f9#2022-03-01">https://towardsdatascience.com/attention-is-all-you-need-e498378552f9#2022-03-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="74a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">变压器编码器-解码器架构的逐步分解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cc43c91edc0ca8cc150bcdcf65bca303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2JTMOkabB7YkZ5slr8vvg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/x22UAIdif_k" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="09b0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="671a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">2017年，谷歌的研究人员和开发人员发布了一篇论文“<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">A<em class="mk">ttention is All You Need</em></a>”，该论文强调了Transformer模型的兴起。在他们的论文中，transformer在翻译任务方面达到了超越以前的自然语言处理(NLP)模型架构的新水平。鉴于它们目前在NLP领域的主导地位，本文深入探讨了transformer架构的细节，目的是强调是什么使它成为如此强大的模型。</p><h1 id="db68" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">总体架构</h1><p id="0ee9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">transformer模型的体系结构从RNNs中的编码器-解码器体系结构中使用的注意机制中得到启发，以处理序列到序列(seq2seq)任务，但通过消除顺序性的因素；这意味着，与RNNs不同，transformer不按顺序处理数据，这允许更多的并行化并减少训练时间。</p><p id="5306" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">图1展示了变压器的整体架构。变压器由两个主要部件组成:</p><ol class=""><li id="5058" class="mq mr iq lq b lr ml lu mm lx ms mb mt mf mu mj mv mw mx my bi translated">编码器堆栈— Nx层相同的编码器(在原始论文中Nx = 6)</li><li id="3fd5" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">解码器堆栈— Nx层相同的解码器(在原始论文中Nx =6)</li></ol><p id="9ea0" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">由于该模型不包含任何递归或卷积，因此它在编码器和解码器堆栈的底部添加了一个位置编码层，以利用序列的顺序。</p><div class="kg kh ki kj gt ab cb"><figure class="ne kk nf ng nh ni nj paragraph-image"><img src="../Images/d78fc1363cb03cea23ab31637a9a78fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*NdUKyp9ZUZQEWcSX2MxA-w.jpeg"/></figure><figure class="ne kk nk ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b724582045814bb92e861f0d76890d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*n-zvu8MAlDEZ0GN75LgogA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nl di nm nn translated">图1:变压器架构的总体概述—左边的图像是简化的形式(<a class="ae kv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">来源</a>)，右边的图像是详细的架构(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure></div><h1 id="d75f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">深入了解变压器</h1><p id="50ab" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">本节通过解释输入产生输出的步骤，详细介绍变压器的不同组件。</p><blockquote class="no np nq"><p id="54fa" class="lo lp mk lq b lr ml jr lt lu mm ju lw nr mn lz ma ns mo md me nt mp mh mi mj ij bi translated">在本文中，我们考虑了使用transformer将英语翻译成法语的经典示例。输入句子是这样的“我是学生”，预期输出是“我是学生”。</p></blockquote><h2 id="d64c" class="nu kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">编码器</h2><p id="652c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将从仔细观察编码器开始，并发现每一步都发生了什么。</p><p id="6ac4" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">输入</em> <br/> </strong>原始数据是一个英文文本，然而，与任何其他模型一样，转换器不理解英语，因此，文本被处理以将每个单词转换成唯一的数字ID。这是通过使用特定的词汇表来完成的，该词汇表可以从训练数据中生成，并将每个单词映射到一个数字索引。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/546e0f587ddbcb106234902fad0cda3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*X6EiM_7-k3xOYHvB0sAxQA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:原始文本的数字表示(图片由作者提供)</p></figure><p id="5b82" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">嵌入层</em> </strong></p><p id="afff" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">如同在其他模型中一样，转换器使用学习的嵌入将输入令牌转换成d = 512维的向量。在训练期间，模型更新向量中的数字，以更好地表示输入令牌。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/e1678e621b4cdd592b661f6669c3d47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDJOt3eJIwmmE0cNeM5pYA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:嵌入层对d=512的嵌入(图片由作者提供)</p></figure><p id="9e84" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">位置编码</em> </strong></p><p id="0498" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">将转换器与以前的序列模型区别开来的一个方面是，它不按顺序接受输入嵌入；相反，它一次接受所有的嵌入。这允许并行化并显著减少培训时间。然而，缺点是它丢失了与单词顺序相关的重要信息。为了使模型保持单词顺序的优势，位置编码被添加到输入嵌入中。由于位置编码和嵌入是相加的，所以它们都具有相同的维数d = 512。有不同的方法来选择位置编码；转换器的创建者使用正弦和余弦函数来获得位置编码。在偶数维指数上应用正弦公式，在奇数维指数上应用余弦公式。图4显示了用于获得位置编码的公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/5727126bfbca5a001e74be303de8589c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDVPDYorirT5WR-O4bCQ9A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:位置编码公式(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/a552e89c44dadcba8c6a296c50c7f9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*77i1zVQPCHmpRKmRKEqvuA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:将位置编码添加到嵌入中以生成位置嵌入(ep)</p></figure><p id="021f" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">多头关注层——自我关注</em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/cfea60f89999f302229aaeedb30e4471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3vLubZrFqXWeywSqHDXWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:多头关注层(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="1c94" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">在本节中有两个术语需要说明，<em class="mk">自我关注</em>和<em class="mk">多头。</em></p><p id="dd47" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><em class="mk">自我关注:</em></p><p id="e2f8" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们将从什么是自我关注以及如何应用自我关注开始。自我注意的目标是通过创建每个输入单词的基于注意的向量来捕捉句子中单词之间的上下文关系。基于注意力的向量有助于理解输入句子中的每个单词与句子中的其他单词(以及单词本身)的相关程度。</p><p id="fa11" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">应用图6左侧所示的标度点积注意力来计算基于注意力的向量。下面是如何从位置嵌入创建这些向量的详细解释。</p><p id="2ebe" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">第一步是获得查询(Q)、键(K)和值(V)。这是通过将位置嵌入的相同副本传递给三个不同的线性图层来实现的，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/386a4754bc02526b85555e1254791fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*7RpdMEDJ9Dvz21L3ogpS6g.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:生成查询、键和值(作者图片)</p></figure><p id="104a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">第二步是从查询(Q)和键(K)中创建一个注意力过滤器。注意力过滤器将指示每个单词在每个位置被关注的程度。它是通过应用图8中的公式创建的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/b57dc87eb00160d0db687d673129ef02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9V82IrwNi3KvOU3k6KZIQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:从查询(Q)和键(K)生成关注过滤器(图片由作者提供)</p></figure><p id="d39d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">最后，为了获得基于注意力的矩阵(自我注意力层的最终输出)，在注意力过滤器和先前生成的值(V)矩阵之间进行矩阵到矩阵乘法(matmul)。产生下面的最终公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/27ef84a4ddf55687085e53470bdb4b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*9c1qnBq4GzYh2CZ4aNNpyg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:比例点产品关注度(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d18c" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><em class="mk">多头注意:</em></p><p id="32ad" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">如图6的右侧所示，比例点积注意(即自我注意)不仅应用一次，而且应用多次(在原始论文中应用了8次)。目标是为同一个单词生成几个基于注意力的向量。这有助于模型在一个句子中有不同的单词关系表示。</p><p id="7eeb" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">从不同的头部生成的不同的基于注意力的矩阵被连接在一起，并通过线性层以将大小缩回到单个矩阵的大小。</p><p id="48df" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">残差连接，添加&amp;范数和</em>前馈网络</strong></p><p id="c4b6" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">从图1中可以看出，该架构包括剩余连接(RC)。剩余连接的目标是通过允许旧信息绕过多头关注层来避免这些信息丢失。因此，位置嵌入被添加到多头注意力的输出，然后在将其传递到常规前馈网络之前被归一化(Add &amp; Norm)。</p><h2 id="a04b" class="nu kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">解码器</h2><p id="f925" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">解码器端与编码器端有许多共享组件。因此，本节不会像上一节那样详细。解码器和编码器的主要区别在于，解码器接受两个输入，并两次应用多头注意力，其中一个被“屏蔽”。此外，解码器中的最终线性层的大小(即，单元的数量)等于目标字典(在这种情况下是法语字典)中的单词数量。每个单元将被分配一个分数；softmax用于将这些分数转换为概率，表示每个单词出现在输出中的概率。</p><p id="f86b" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">输入</em> </strong></p><p id="d67a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">解码器接受两个输入:</p><ol class=""><li id="aa72" class="mq mr iq lq b lr ml lu mm lx ms mb mt mf mu mj mv mw mx my bi translated">编码器的输出—这些是解码器执行多头关注(图1中的第二个多头关注)的键(K)和值(V)。在这个多头注意力层中，查询(Q)是被掩蔽的多头注意力的输出。</li><li id="8cb2" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">输出文本向右移动—这是为了确保特定位置“I”处的预测只能依赖于小于I的位置(参见图10)。因此，解码器将在位置I处要预测的实际字之前接收所有已经预测的字(位置0到i-1)。注意，传递到解码器的第一个生成的字是令牌<start>，预测过程继续，直到解码器生成特殊的结束令牌<eos>。</eos></start></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/d1452772e1093b9ccc573124bb9c1ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HDqn93FsA93o2z4bTwqUVQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10:在推理阶段，输出右移作为解码器的输入(图片由作者提供)</p></figure><p id="5321" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><strong class="lq ir"> <em class="mk">蒙面多头注意</em> </strong></p><p id="8ce1" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">掩蔽的多头注意的过程类似于常规的多头注意。唯一的区别是，在将矩阵Q和K相乘并对它们进行缩放之后，在应用softmax之前，会在生成的矩阵上应用一个特殊的遮罩(参见图6左图-遮罩选项)。).目标是使文本中特定位置“I”处的每个单词只关注文本中的每一个其他位置，直到包括其当前位置(位置0直到位置I)。这在训练阶段很重要，因为当预测位置i+1处的单词时，模型将只关注该位置之前的所有单词。因此，I之后的所有位置都会被屏蔽，并在传递给softmax运算之前设置为负无穷大，从而导致注意力滤波器中出现0(见图11)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8c429c8017ace6e75e0dd2ce8d5592aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*TGGeX4rKcll2Kg9Vofclaw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:屏蔽注意力过滤器(作者图片)</p></figure><h1 id="46d7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="7712" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Transformer模型是一种深度学习模型，已经在该领域存在了五年，并且已经产生了几个顶级性能和最先进的模型，如BERT模型。鉴于它在自然语言处理领域的主导地位及其在计算机视觉等其他领域的不断扩大的用途，理解它的体系结构是很重要的。本文涵盖了转换器的不同组件，并强调了它们的功能。</p><h1 id="3994" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">重要资源</h1><p id="5876" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部(A. Vaswani等人，2017) </a></p><p id="187a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><a class="ae kv" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《图解变压器》(J. Alammar，2018) </a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div></figure></div></div>    
</body>
</html>