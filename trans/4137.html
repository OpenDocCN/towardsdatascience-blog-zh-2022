<html>
<head>
<title>Distributed Parallel Training — Model Parallel Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式并行训练—模型并行训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distributed-parallel-training-model-parallel-training-a768058aa02a#2022-09-13">https://towardsdatascience.com/distributed-parallel-training-model-parallel-training-a768058aa02a#2022-09-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1d3a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">分布式培训</h2><div class=""/><div class=""><h2 id="832e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">PyTorch中大模型的分布式模型并行训练</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8f94603c221829a4eceabbba8118d329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hMWppImo2BJoXx8VLp_6Iw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">丹妮拉·奎瓦斯在<a class="ae le" href="https://unsplash.com/s/photos/landscape?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a658" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">近年来，深度学习模型的规模和分布式并行训练的挑战呈指数级增长。例如，著名的GPT-3有1750亿个参数和96个关注层，批量大小为3.2 M，字数为4990亿。亚马逊SageMaker训练平台在120ml . p4d . 24x大型实例和1750亿个参数上可以达到每秒32个样本的吞吐率。如果我们将此增加到240个实例，完整的模型将需要25天来训练。</p><p id="8bcb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于大型模型，在GPU上训练并行性变得非常必要。有三种典型的分布式并行训练类型:分布式数据并行、模型并行和张量并行。我们常常把后两种归为一类:模型并行，再分为流水线并行和张量并行两个亚型。我们将在这里重点介绍分布式模型并行训练，并演示如何在PyTorch中进行开发。</p><h2 id="4ecd" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">了解分布式模型并行训练</h2><p id="ec6c" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">模型并行性在多个GPU之间分割模型，这与数据并行性为所有训练GPU复制同一模型不同。模型并行训练有三个关键点:1 .如何有效地分割模型的层；2.如何并行而不是顺序地训练分片层；3.如何设计优秀的节点间吞吐量？如果我们不能平衡碎片或者并行运行它们，我们就不能实现模型并行的目标。</p><p id="ce42" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">分布式训练是一种在集群或资源池中具有多个节点的训练并行性。容器化使得扩展节点变得容易，Kubernetes很好地协调了它们。每个节点可以有多个GPU和多个容器。一个容器可以控制一个或多个GPU。模型并行性可以跨分布式GPU节点集群分派模型的各层。这种方法可以有效地扩展培训。</p><p id="e8a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以在下面举例说明分布式模型并行训练。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/bac202c8fa3623f03a4a869dad04423c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Jx4YzQcymZOkZ7jo2U10w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">模型并行性解释(作者)</p></figure><p id="a729" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的示例中，集群中有两个节点。每个节点有一个或两个容器，每个容器也有一个或两个GPU。模型的七层分布在这些GPU上。</p><p id="9c5c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">每个GPU或节点都有资源限制。模型并行性使培训能够并行化，而分布式培训最终可以横向扩展。</p><h2 id="4658" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">PyTorch中的模型并行性</h2><p id="eb6b" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">上面的描述表明，分布式模型并行训练有两个主要部分。为了实现这一点，有必要在多个GPU中设计模型并行性。PyTorch对此进行了包装并减轻了实现负担。PyTorch只有三个小变化。</p><ol class=""><li id="e0e3" class="mz na iq lh b li lj ll lm lo nb ls nc lw nd ma ne nf ng nh bi translated">使用“<code class="fe ni nj nk nl b">to(device)</code>”来标识模型的特定层(或子网)的特定设备(或GPU)。</li><li id="fe5f" class="mz na iq lh b li nm ll nn lo no ls np lw nq ma ne nf ng nh bi translated">相应地添加一个“<code class="fe ni nj nk nl b">forward</code>”方法，在设备间移动中间输出。</li><li id="0f79" class="mz na iq lh b li nm ll nn lo no ls np lw nq ma ne nf ng nh bi translated">调用损失函数时，指定同一设备上的标签输出。而“<code class="fe ni nj nk nl b">backward()</code>”和“<code class="fe ni nj nk nl b">torch.optim</code>”会像在单个GPU上运行一样自动处理渐变。</li></ol><p id="7cb6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们写一些伪代码来说明它。假设在两个GPU上运行一个简单的两层模型，将每个线性层放在每个GPU上，并将输入和中间输出相应地移动到相关层GPU上。我们可以如下定义虚拟模型:</p><pre class="kp kq kr ks gt nr nl ns nt aw nu bi"><span id="df14" class="mb mc iq nl b gy nv nw l nx ny"><strong class="nl ja">import</strong> <strong class="nl ja">torch</strong><br/><strong class="nl ja">import</strong> <strong class="nl ja">torch.nn</strong> <strong class="nl ja">as</strong> <strong class="nl ja">nn</strong></span><span id="524e" class="mb mc iq nl b gy nz nw l nx ny"><strong class="nl ja">class</strong> <strong class="nl ja">DummyModel(nn.Module):</strong><br/>    <strong class="nl ja">def</strong> __init__<strong class="nl ja">(</strong>self<strong class="nl ja">):</strong><br/>        super<strong class="nl ja">(DummyModel,</strong> self<strong class="nl ja">).</strong>__init__<strong class="nl ja">()</strong><br/>        self<strong class="nl ja">.net0</strong> <strong class="nl ja">=</strong> <strong class="nl ja">nn.Linear(</strong>20<strong class="nl ja">,</strong> 10<strong class="nl ja">).to(</strong>'cuda:0'<strong class="nl ja">)</strong><br/>        self<strong class="nl ja">.relu</strong> <strong class="nl ja">=</strong> <strong class="nl ja">nn.ReLU()</strong><br/>        self<strong class="nl ja">.net1</strong> <strong class="nl ja">=</strong> <strong class="nl ja">nn.Linear(</strong>10<strong class="nl ja">,</strong> 10<strong class="nl ja">).to(</strong>'cuda:1'<strong class="nl ja">)</strong><br/><br/>    <strong class="nl ja">def</strong> <strong class="nl ja">forward(</strong>self<strong class="nl ja">,</strong> <strong class="nl ja">x):</strong><br/>        <strong class="nl ja">x</strong> <strong class="nl ja">=</strong> self<strong class="nl ja">.relu(</strong>self<strong class="nl ja">.net0(x.to(</strong>'cuda:0'<strong class="nl ja">)))</strong><br/>        <strong class="nl ja">return</strong> self<strong class="nl ja">.net1(x.to(</strong>'cuda:1'<strong class="nl ja">))</strong></span></pre><p id="f297" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们可以用下面的损失函数添加训练代码:</p><pre class="kp kq kr ks gt nr nl ns nt aw nu bi"><span id="4fb6" class="mb mc iq nl b gy nv nw l nx ny"><strong class="nl ja">import</strong> <strong class="nl ja">torch</strong><br/><strong class="nl ja">import</strong> <strong class="nl ja">torch.nn</strong> <strong class="nl ja">as</strong> <strong class="nl ja">nn<br/>import</strong> <strong class="nl ja">torch.optim</strong> <strong class="nl ja">as</strong> <strong class="nl ja">optim<br/>import</strong> <strong class="nl ja">DummyModel</strong></span><span id="d362" class="mb mc iq nl b gy nz nw l nx ny"><strong class="nl ja">model</strong> <strong class="nl ja">=</strong> <strong class="nl ja">DummyModel()</strong><br/><strong class="nl ja">loss_fn</strong> <strong class="nl ja">=</strong> <strong class="nl ja">nn.MSELoss()</strong><br/><strong class="nl ja">optimizer</strong> <strong class="nl ja">=</strong> <strong class="nl ja">optim.SGD(model.parameters(),</strong> <strong class="nl ja">lr=</strong>0.001<strong class="nl ja">)</strong><br/><br/><strong class="nl ja">optimizer.zero_grad()</strong><br/><strong class="nl ja">outputs</strong> <strong class="nl ja">=</strong> <strong class="nl ja">model(torch.randn(</strong>30<strong class="nl ja">,</strong> 10<strong class="nl ja">))</strong><br/><strong class="nl ja">labels</strong> <strong class="nl ja">=</strong> <strong class="nl ja">torch.randn(</strong>30<strong class="nl ja">,</strong> 10<strong class="nl ja">).to(</strong>'cuda:1'<strong class="nl ja">)</strong><br/><strong class="nl ja">loss_fn(outputs,</strong> <strong class="nl ja">labels).backward()</strong><br/><strong class="nl ja">optimizer.step()</strong></span></pre><p id="62dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样的想法可以很快扩展到更复杂的模型。我们可以在一个<code class="fe ni nj nk nl b">Sequential</code>中分组多个层，通过<code class="fe ni nj nk nl b">to(device).</code>分配到一个特定的GPU上，当然还有更多优化并行效率的改进，这里就不赘述了。</p><h2 id="721f" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">TL；速度三角形定位法(dead reckoning)</h2><p id="3e99" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">分布式模型并行训练有两个主要概念。模型并行实现了训练无法在单个GPU或设备上运行的大型模型。分布式培训可以通过在分布式设备之间划分模型来有效地向外扩展。PyTorch和其他库(如SakeMaker)通过最小的修改使生活变得更容易，尽管它在内部实现起来很复杂。</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><h2 id="f9f7" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">参考</h2><div class="oh oi gp gr oj ok"><a href="https://arxiv.org/abs/2111.05972" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ja gy z fp op fr fs oq fu fw iz bi translated">Amazon SageMaker模型并行性:大型模型训练的通用灵活框架</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">随着深度学习模型的规模迅速增长，需要针对大模型训练的系统级解决方案。我们…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">arxiv.org</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ky ok"/></div></div></a></div><div class="oh oi gp gr oj ok"><a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ja gy z fp op fr fs oq fu fw iz bi translated">单机模型并行最佳实践- PyTorch教程1.12.1+cu102文档</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">模型并行广泛应用于分布式训练技术中。以前的帖子解释了如何使用数据并行…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">pytorch.org</p></div></div></div></a></div><div class="oh oi gp gr oj ok"><a href="https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ja gy z fp op fr fs oq fu fw iz bi translated">使用DJLServing和DeepSpeed模型并行推理在Amazon SageMaker上部署大型模型|…</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">近几年来，自然语言处理领域发展迅速。虽然硬件已经…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">aws.amazon.com</p></div></div><div class="ot l"><div class="oz l ov ow ox ot oy ky ok"/></div></div></a></div><div class="oh oi gp gr oj ok"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ja gy z fp op fr fs oq fu fw iz bi translated">可用的SageMaker Studio实例类型</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">以下亚马逊弹性计算云(Amazon EC2)实例类型可用于SageMaker Studio…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">docs.aws.amazon.com</p></div></div></div></a></div><div class="oh oi gp gr oj ok"><a href="https://aws.amazon.com/blogs/machine-learning/train-175-billion-parameter-nlp-models-with-model-parallel-additions-and-hugging-face-on-amazon-sagemaker/" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd ja gy z fp op fr fs oq fu fw iz bi translated">训练175+10亿个参数的NLP模型，在Amazon上使用模型并行添加和拥抱脸…</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">近几年来，自然语言处理领域发展迅速。虽然硬件已经…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">aws.amazon.com</p></div></div><div class="ot l"><div class="pa l ov ow ox ot oy ky ok"/></div></div></a></div></div></div>    
</body>
</html>