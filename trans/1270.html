<html>
<head>
<title>Introduction to SHAP Values and their Application in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SHAP 值及其在机器学习中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-shap-values-and-their-application-in-machine-learning-8003718e6827#2022-03-31">https://towardsdatascience.com/introduction-to-shap-values-and-their-application-in-machine-learning-8003718e6827#2022-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3370" class="pw-subtitle-paragraph jr is it bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">了解 SHAP 图书馆是如何运作的</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/8f3d427f7a1c55092147f3d3bce78dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*umNqrjPc_32AIVY5auy0zg.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">来源:图片由作者提供</p></figure><p id="51c5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP 是一种解释机器学习模型预测的数学方法。它基于博弈论的概念，可以通过计算每个特征对预测的贡献来解释任何机器学习模型的预测。SHAP 可以确定最重要的特征及其对模型预测的影响。SHAP 是一个数学话题，不解释背后的数学原理就无法完全理解。然而，我们试图通过解释数学题目背后的直觉并为每个题目提供一些例子来尽可能地简化数学题目。<em class="lv">读者也可以跳过可选章节或附录中介绍的更难的主题。</em>我们还将使用 Python 从头开始实现不同的 SHAP 算法，以帮助你完全理解它们是如何工作的。</p><p id="bf23" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Python 中的 SHAP 库</strong></p><p id="3011" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将研究可用于计算 SHAP 值的不同方法，每种方法都将从头开始用 Python 实现。然而，这些代码只是用来展示算法是如何工作的，对于现实世界的应用程序来说是没有效率的。Python 中的<a class="ae lw" href="https://pypi.org/project/shap/" rel="noopener ugc nofollow" target="_blank"> SHAP 库</a>可以用来高效地计算模型的 SHAP 值，在本文中，我们将简要地向您展示如何使用这个库。我们还用它来验证每一节中给出的 Python 脚本的输出(代码已经用<code class="fe lx ly lz ma b">shap 0.40.0</code>测试过)。</p><p id="2e95" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解释模型的预测</strong></p><p id="2a6c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数机器学习模型都是设计来预测目标的。这里预测的准确性非常重要，但是，我们还需要理解为什么一个模型会做出某种预测。所以，我们需要一个工具来<em class="lv">解释一个模型</em>。解释的意思是，我们希望对模型的预测和模型用来生成该预测的数据实例的组件之间的关系有一个定性的理解。这些组件可以是数据实例的一个特征(如果我们有一个表格数据集)，或者是图像中的一组像素，或者是文本文档中的一个单词。我们想知道这些成分的存在(或不存在)如何影响预测。</p><p id="2269" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习中的一些模型，如线性回归或决策树，是<em class="lv">可解释的</em>。这里的可解释性指的是人类理解模型用来做出预测的过程的容易程度。例如，如果我们绘制一个决策树分类器，我们可以很容易地理解它是如何做出某种预测的。另一方面，深度学习模型就像一个黑匣子，我们无法轻松理解这些模式如何做出预测。SHAP 是一个<em class="lv">个性化</em> <em class="lv">模型不可知</em> <em class="lv">讲解者</em>。模型不可知的方法假设要解释的模型是一个黑盒，并且不知道模型内部如何工作。因此，模型不可知方法只能访问输入数据和待解释模型的预测。一个个性化的模型不可知的解释者本身就是一个可解释的模型。解释者可以对一个特定的数据实例做出近似相同的预测。现在，我们可以假设这个可解释的模型正在模仿原始模型用来做出单一特定预测的过程。因此，我们说可解释模型可以解释原始模型。总之，SHAP 可以解释任何机器学习模型，而不知道该模型内部如何工作，它可以使用博弈论的概念来实现这一点。所以要理解它，首先，我们需要熟悉 Shapley 值。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mb"><img src="../Images/a0db6df0cb1f77046e1ab7dc02a31617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpdnnzbZuOk3aLm4IP6nrA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 1(来源:作者图片)</p></figure><p id="5d96" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">沙普利值</strong></p><p id="01e1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shapley 值是博弈论中的一个数学概念，由 Lloyd Shapley 于 1951 年提出。他后来因此获得了诺贝尔经济学奖。假设我们有一个合作博弈，有<em class="lv"> M </em>个玩家，编号从 1 到<em class="lv"> M </em>，设<strong class="lb iu">FT17】表示玩家集合，那么<strong class="lb iu">FT21】= { 1，2，.。。，<em class="lv"> M </em> }。一个联盟，<strong class="lb iu"> <em class="lv"> S </em> </strong>，定义为<strong class="lb iu"><em class="lv">f</em></strong>(<strong class="lb iu"><em class="lv">s</em></strong>⊆<strong class="lb iu"><em class="lv">f</em></strong>)的子集，我们还假设空集∅是一个没有玩家的联盟。例如，如果我们有 3 个玩家，那么可能的联盟是:</strong></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mc"><img src="../Images/cd6ed8c5b35da70a304c1b2a6ea3d5a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gatP1VlRtqR1LPj8xmXAYg@2x.png"/></div></div></figure><p id="639f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集合<strong class="lb iu"> <em class="lv"> F </em> </strong>也是联军，我们称之为<em class="lv">大联军</em>。很容易看出，对于 M 个玩家，我们有 2 个联盟。现在我们定义一个函数<em class="lv"> v </em>，它将每个联盟映射到一个实数。<em class="lv"> v </em>称为<em class="lv">特征函数</em>。所以，对于每个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>，数量<em class="lv">v</em>(<strong class="lb iu"><em class="lv">S</em></strong>)是一个实数，称为<em class="lv">联盟</em><em class="lv"/><strong class="lb iu"><em class="lv">S</em></strong>的价值。它被认为是联盟中的参与者在一起行动时可以获得的总收益或集体回报。由于空联盟没有玩家，我们可以假设</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi md"><img src="../Images/511de6d139dcdc4fe45087d691369b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*DES7ctDajafFgHdSGD93jQ@2x.png"/></div></figure><p id="ce2e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们想知道在一个有 M 个参与者的联盟博弈中，每个参与者对总收益的贡献是多少？换句话说，在玩家之间分配总收益最公平的方式是什么？</p><p id="40fa" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用一个例子来说明如何解决这个问题。假设我们有一个 5 人游戏，那么<strong class="lb iu"> <em class="lv"> F </em> </strong> = {1，2，...。。。，<em class="lv"> 5 </em> }。假设我们通过一次一个地将玩家添加到空集来形成大联盟<strong class="lb iu"> <em class="lv"> F </em> </strong>，那么每次我们添加一个新玩家，我们就形成一个新的<strong class="lb iu"> <em class="lv"> F </em> </strong>联盟。比如我们先把{1}加到空集，那么当前的玩家集是{1}这是一个<strong class="lb iu"> <em class="lv"> F </em> </strong>的联盟。然后我们加上{2}并且当前集合是联盟{1，2}并且我们继续直到我们得到<strong class="lb iu"> <em class="lv"> F </em> </strong> ={1，2，3，4，5}。随着每个玩家被添加到当前玩家集合中，他增加了先前联盟的总收益。例如，当当前设置为{1，2}时，总增益<em class="lv"> v </em> ({1，2})。加上{3}后，总增益变成<em class="lv"> v </em> ({1，2，3}) <em class="lv">。</em>现在我们可以假设{3}对当前联盟的贡献是当前联盟(包括{3})和不包括{3}的先前联盟的值之间的差:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi me"><img src="../Images/74e836f26bb0a1e62a4803b8bbf42211.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*2FuOcpdYONZJPZQSgmA0Lg@2x.png"/></div></figure><p id="f277" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">添加{3}后，我们可以添加{4}和{5}，它们也会改变总增益。但是不影响{3}的贡献，所以前面的等式还是给出了{3}的贡献(图 2)。但是，这里有一个问题。玩家的添加顺序也很重要。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mf"><img src="../Images/7b2a4211fd0c296b07e6859c5d31c6b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKwpif8jDCIlXhCNkF_95Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 2(来源:图片由作者提供)</p></figure><p id="40ad" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设这些玩家是某公司某部门的员工。公司首先聘用{1}。然后，他们发现自己缺少一套技能，于是雇佣了{2}。雇用{2}后，公司的总收益增加了 10000 美元，这是{2}加到{1}上的贡献。雇用{3}后，{3}的贡献仅为 2000 美元。此外，假设员工{2}和{3}具有相似的技能。现在，员工{3}可以声称，如果他早点被聘用，他将获得相同的缴款{2}。换句话说，{3}的贡献加上{1}也可能是 10000 美元。因此，为了对每个玩家的贡献有一个公平的评价，我们还应该考虑他们加入大联盟的顺序。</p><p id="c3c0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，要对玩家{ <em class="lv"> i </em> }的贡献有一个公正的评价，我们应该形成<strong class="lb iu"> <em class="lv"> F </em> </strong>的所有排列，并计算{ <em class="lv"> i </em> }在每个排列中的贡献，然后取这些贡献的平均值。例如，<strong class="lb iu"> <em class="lv"> F </em> </strong> ={1，2，3，4，5}的一种可能排列是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mg"><img src="../Images/4dfdfb6ba5df3a9aee36ca1d62b733d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*lAjBo6yD3TyTukbzVcsafw@2x.png"/></div></div></figure><p id="d6ff" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而{3}在这个排列中的贡献是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/38957661c6a9e7b0cf0e982608916919.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*V8QdYW-1J2qitHNpUqRNYQ@2x.png"/></div></figure><p id="6c27" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种排列可以是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mg"><img src="../Images/cba13baedbb19c3fa71898bf7dcf703d.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*sl5-ZXdNgBs4gqm9z_cOOQ@2x.png"/></div></div></figure><p id="a816" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而{3}在这个排列中的贡献是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mi"><img src="../Images/56e7b2621b2b82e78807a690f8fcc51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fa-Sz-RDW-bMGSUVs8dKYg@2x.png"/></div></div></figure><p id="205e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得注意的是，特征函数<em class="lv"> v </em>将联合作为其自变量，而不是排列。联合是一个集合，所以其中元素的顺序并不重要，但是排列是元素的有序集合。在类似[3，1，2，4，5]的排列中，3 是第一个玩家，5 是最后一个加入团队的玩家。因此，对于每个排列，元素的顺序可以改变它们对总增益的贡献，然而，总增益或排列的价值仅取决于元素，而不是它们的顺序。所以:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mj"><img src="../Images/940767e51338478ac543624ab835a71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU6JJ6GIbwniICpzeIHCYQ@2x.png"/></div></div></figure><p id="0626" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，对于每一个排列<strong class="lb iu"> <em class="lv"> P </em> </strong>，我们需要先计算{ <em class="lv"> i </em> }之前加入的玩家联盟的价值。姑且称这个联盟为<strong class="lb iu"><em class="lv"/></strong>。然后我们需要计算{ <em class="lv"> i </em> }加<strong class="lb iu"> <em class="lv"> S </em> </strong>形成的联盟的价值，我们把这个联盟叫做<strong class="lb iu"><em class="lv">S</em></strong>U {<em class="lv">I</em>}。现在由<em class="lv"> ϕᵢ </em>表示的玩家{ <em class="lv"> i </em> }的贡献是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mk"><img src="../Images/3bb50b3455e5900592ea2bcb264cf584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85vNgjLw9bz6OljA-xrj0g@2x.png"/></div></div></figure><p id="492c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大联盟<strong class="lb iu"> <em class="lv"> F </em> </strong>的排列总数是| <strong class="lb iu"> <em class="lv"> F </em> </strong> |！(这里| <strong class="lb iu"> <em class="lv"> F </em> </strong> |表示集合<strong class="lb iu"> <em class="lv"> F </em> </strong>)的元素个数，所以我们用贡献的总和除以那个，取{ <em class="lv"> i </em> }(图 3)的贡献的平均值。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ml"><img src="../Images/87313fb1d386607f8f7faed37fe01af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3-cOlijO6hQIxSwAclWLg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 3(来源:图片由作者提供)</p></figure><p id="5e65" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如图 3 所示，一些排列具有相同的贡献，因为它们的联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>和<strong class="lb iu"><em class="lv">S</em></strong><em class="lv"/>U<em class="lv"/>{<em class="lv">I</em>}是相同的。所以，一个更简单的计算情商的方法。1 是我们只计算贡献的不同值，并将它们乘以它们被重复的次数。要做到这一点，我们需要计算出每个联盟可以形成多少种排列。设<strong class="lb iu"><em class="lv">f</em></strong>-{<em class="lv">I</em>}为不包括玩家{ <em class="lv"> i </em> }的所有玩家的集合，<strong class="lb iu"> <em class="lv"> S </em> </strong>为<strong class="lb iu"><em class="lv">f</em></strong>-{<em class="lv">I</em>}(<strong class="lb iu"><em class="lv">s</em></strong><em class="lv">⊆</em><strong class="lb iu">的联盟之一例如对于<strong class="lb iu"> <em class="lv"> F </em> </strong> ={1，2，3，4，5}和{ <em class="lv"> i </em> }={3}，我们有</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mm"><img src="../Images/d2fd41c1cbabaca4f9f043eed3fb718d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0J7G_375Hh6m6d9NYsk5Q@2x.png"/></div></div></figure><p id="2d6a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv"> S </em> </strong>中的元素个数用| <strong class="lb iu"> <em class="lv"> S </em> </strong> |，我们可以有| <strong class="lb iu"> <em class="lv"> S </em> </strong> |！这些元素的排列。比如说如果<strong class="lb iu"> <em class="lv"> S </em> </strong> ={1，2}那么| <strong class="lb iu"> <em class="lv"> S </em> </strong> |=2 而我们有 2！=2 个排列:[1，2]和[2，1](图 4)。我们也知道从<strong class="lb iu"> <em class="lv"> S </em> </strong>形成的每一个排列的价值是<em class="lv">v</em>(<strong class="lb iu"><em class="lv">S</em></strong>)。现在我们在从<strong class="lb iu"> <em class="lv"> S </em> </strong>形成的每一个排列的末尾加上玩家{ <em class="lv"> i </em> }。所得排列的价值是<em class="lv">v</em>(<strong class="lb iu"><em class="lv">S</em></strong><em class="lv"/>U {<em class="lv">I</em>})因为它们都属于联盟<strong class="lb iu"><em class="lv">S</em></strong><em class="lv"/>U {<em class="lv">I</em>}。集合<strong class="lb iu"> <em class="lv"> F </em> </strong>有|<strong class="lb iu"><em class="lv">F</em></strong>|-|<strong class="lb iu">|<em class="lv">S</em></strong>|-1 剩余元素不包括<strong class="lb iu"><em class="lv">S</em></strong>U {<em class="lv">I</em>}可以加在<strong class="lb iu"> <em class="lv"> S </em>后面的元素所以，有(|<strong class="lb iu"><em class="lv">F</em></strong>|-|<strong class="lb iu"><em class="lv">S</em></strong>|-1)！将它们添加到<strong class="lb iu"><em class="lv">S</em></strong>U {<em class="lv">I</em>}的方法。</strong></p><p id="ef18" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在前面的示例中，<strong class="lb iu"> <em class="lv"> F </em> </strong>的其余元素是{4}和{5}。所以我们有(|<strong class="lb iu"><em class="lv">F</em></strong>|-|<strong class="lb iu"><em class="lv">S</em></strong>|-1)！= (5–2–1)!=使用这些剩余元素组成大联盟的 2 种方式。这样一来，我们就有了<strong class="lb iu"> <em class="lv"> S </em> </strong>！(|<strong class="lb iu">T115】FT117】|-|<strong class="lb iu">T119】ST121】|-1)！形成一个<strong class="lb iu"> <em class="lv"> F </em> </strong>的排列的方法，其中{ <em class="lv"> i </em> }在一个<strong class="lb iu"> <em class="lv"> S </em> </strong>的排列之后，其余的玩家在{ <em class="lv"> i </em> }之后(图 4)。</strong></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mn"><img src="../Images/da85d9af3f0355917c261f5ab594ec25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDaPmiFmWEx1aFdvC4Ay8w.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 4(来源:图片由作者提供)</p></figure><p id="dd5f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">{ <em class="lv"> i </em> }对每个排列的总增益的贡献为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/17ed1e3aa92637f6b02c42e55f95d618.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z5UxL2hYE8mZ0Z5v0CbuBw@2x.png"/></div></figure><p id="1d22" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且{ <em class="lv"> i </em> }对所有这些排列的总增益的总贡献是</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mp"><img src="../Images/aae314edc869ad1378fc99f32c9ead2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zbqyb1Yiy63ObfKMh2nZVQ@2x.png"/></div></div></figure><p id="99ea" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经涵盖了<strong class="lb iu"> <em class="lv"> F </em> </strong>中一个可能的联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>的排列，并计算了{ <em class="lv"> i </em> }对其总收益的总贡献。现在我们可以对其他联盟<strong class="lb iu"><em class="lv">F</em></strong>-{<em class="lv">I</em>}重复同样的过程，得到{ <em class="lv"> i </em> }在<strong class="lb iu"> <em class="lv"> F </em> </strong>所有排列中的贡献总和:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mq"><img src="../Images/c442447c7c70a741d3621a7bb01f81a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Ac8nmEMdU7kEyH-CMHLnQ@2x.png"/></div></div></figure><p id="e096" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们知道我们有| <strong class="lb iu"> <em class="lv"> F </em> </strong> |！排列为<strong class="lb iu"> <em class="lv"> F </em> </strong>。所以{ <em class="lv"> i </em> }对<strong class="lb iu"> <em class="lv"> F </em> </strong>所有排列的总增益的平均贡献可以通过将前一项除以| <strong class="lb iu"> <em class="lv"> F </em> </strong> |！</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mr"><img src="../Images/1740004da50a83227ba5d553bafa97d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*70YIMZd3H0CBJlFVN7IYng@2x.png"/></div></div></figure><p id="fcd4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> ϕᵢ </em>称为元素{ <em class="lv"> i </em> }的<em class="lv"> Shapley 值</em>，它是{ <em class="lv"> i </em> }在<strong class="lb iu"> <em class="lv"> F </em> </strong>所有排列中的平均贡献。这是玩家{ <em class="lv"> i </em> }对<strong class="lb iu"> <em class="lv"> F </em> </strong>中所有玩家总收益的数学公平份额。正如我们之前所展示的，每个联军<strong class="lb iu"><em class="lv"/></strong>，都能制造<strong class="lb iu"><em class="lv"/></strong>！(|<strong class="lb iu"><em class="lv">F</em></strong>|-|<strong class="lb iu"><em class="lv">S</em>|-1)！排列。由于排列总数是| <strong class="lb iu"> <em class="lv"> F </em> </strong> |！，我们可以写:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ms"><img src="../Images/1013d5616e405382a913c5b0ce784525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lkA5HrJv1PBTIrUDSl-_dg@2x.png"/></div></div></figure><p id="399c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shapley 值应该具有以下属性:</p><p id="9871" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1- <em class="lv">效率</em>:所有玩家的贡献总和应该给出总收益:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1f98da51e34852345dd9cd469858213c.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*1-y8JzBIAg837RPE9fIK-Q@2x.png"/></div></figure><p id="1bed" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1- <em class="lv">对称</em>:如果<em class="lv"> i </em>和<em class="lv"> j </em>是这样的<em class="lv">v</em>(<strong class="lb iu"><em class="lv">s</em></strong>∩{<em class="lv">I</em>})=<em class="lv">v</em>(<strong class="lb iu"><em class="lv">s</em></strong>∩{<em class="lv">j</em>这意味着，如果两个玩家在每个可能的联盟中增加相同的收益，那么他们应该有相同的贡献。</p><p id="0dcb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2- <em class="lv">哑元</em>:如果<em class="lv"> i </em>是这样的:对于每一个不包含<em class="lv"> i </em>的联盟<em class="lv"> S </em>，那么<em class="lv"> ϕᵢ </em> = 0。这意味着，如果一个玩家没有给任何可能的联盟增加任何收益，那么它的贡献为零。</p><p id="8cd7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3- <em class="lv">可加性</em>:假设<em class="lv"> u </em>和<em class="lv"> v </em>是一个游戏的两个不同的特征函数。设其中玩家<em class="lv"> i </em>的贡献分别为<em class="lv"> ϕᵢ </em> ( <em class="lv"> u </em>)和<em class="lv"> ϕᵢ </em> ( <em class="lv"> v </em>)(此处<em class="lv"> ϕᵢ </em> ( <em class="lv"> u </em>指<em class="lv"> ϕᵢ </em>是<em class="lv"> u </em>的函数)。然后我们有<em class="lv">ϕᵢ</em>(<em class="lv">u</em>+<em class="lv">v</em>)=<em class="lv">ϕᵢ</em>(<em class="lv">u</em>)+<em class="lv">ϕᵢ</em>(<em class="lv">v</em>)。让我们通过一个例子来阐明这个性质。假设一个员工团队从事两个不同的项目，他们对每个项目的总回报和贡献是不同的。那么如果我们把这些项目组合起来，一个员工在组合项目中的贡献就是他对每个项目贡献的总和。</p><p id="5fa1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以很容易地证明方程 3 中的 Shapley 值满足这些性质。</p><p id="9072" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">证明(<strong class="lb iu"> <em class="lv">可选</em> </strong>):</p><ul class=""><li id="6543" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">如果对于每一个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>，我们有<em class="lv">v</em>(<strong class="lb iu"><em class="lv">S</em></strong>)=<em class="lv">v</em>(<strong class="lb iu"><em class="lv">S</em></strong>∩{<em class="lv">I</em>})那么我们得到:</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nd"><img src="../Images/2fa956df3fd2bbae6701e644cfa5048c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XJwauPTEZITSGDSCjuo4g@2x.png"/></div></div></figure><p id="3a47" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，他们有虚拟财产。它们也满足效率特性。从等式 2 我们知道:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ne"><img src="../Images/635931b43e6a9e451b97daa9b6773eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQnCJC8-nJAaTmwHrvbAXA@2x.png"/></div></div></figure><p id="39af" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">c</em><strong class="lb iu"><em class="lv">ᵖ</em></strong><em class="lv">ᵢ</em>表示{ <em class="lv"> i </em> }对排列<strong class="lb iu"> <em class="lv"> P </em> </strong>总增益的贡献。现在假设这个排列的元素是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/33dcaf38fc278224e07d10810a8c47ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*g75j3LbGi-xXW9QTm7oT9w@2x.png"/></div></figure><p id="1d60" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">于是我们有了|<strong class="lb iu"><em class="lv">F</em></strong>| =<em class="lv">M</em>。我们可以计算出<em class="lv">c</em><strong class="lb iu"><em class="lv">ᵖ</em></strong><em class="lv">ᵢ</em>各个元素的值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ng"><img src="../Images/8b9a9e848d70f1319071992d104a658d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkXjnBwnH1HlnRBhF8Ro3Q@2x.png"/></div></div></figure><p id="2209" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在如果我们把所有玩家的<em class="lv">c</em><strong class="lb iu"><em class="lv">ᵖ</em></strong><em class="lv">ᵢ</em>的值相加，那么每个<em class="lv">c</em><strong class="lb iu"><em class="lv">ᵖ</em></strong><em class="lv">ᵢ</em>中的第一项就取消了<em class="lv"> c </em> <strong class="lb iu"> <em class="lv"> ᵖ </em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/212161d5bc675f2051d3b1110e63a2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*jXknwk2_to871ML1X4aEvg@2x.png"/></div></figure><p id="724c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，对于每个排列，所有参与者的贡献总和给出了大联盟的总收益。我们知道我们有| <strong class="lb iu"> <em class="lv"> F </em> </strong> |！排列。因此，使用等式 2，我们可以写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ni"><img src="../Images/54840f1d1747a27380e67e88edcd8215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XK97TF20geSQ2cf5_D7fUQ@2x.png"/></div></div></figure><p id="9262" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">机器学习中的 Shapley 值</strong></p><p id="993c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是我们如何将玩家的 Shapley 值与机器学习模型的特征联系起来呢？假设我们有一个数据集，有<em class="lv"> N </em>行和<em class="lv"> M </em>个特性，如图 5 所示。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nj"><img src="../Images/2355417747951916d20eaa2e0d772a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDFVhR-47_FJmtFvJq1auQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 5(来源:图片由作者提供)</p></figure><p id="b62f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> Xᵢ </em>是数据集的第<em class="lv"> i </em>个特征，<em class="lv"> xᵢ </em> ⁽ <em class="lv"> ʲ </em> ⁾是第<em class="lv"> j </em>个例子中第<em class="lv"> i </em>个特征的值，<em class="lv"> y </em> ⁽ <em class="lv"> ʲ </em> ⁾是第<em class="lv"> j </em>行的目标。这些特征的值可以形成一个<em class="lv">特征向量</em>，它由一个具有<em class="lv"> M </em>个元素的行向量表示:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/601ed4c7b62bbe88adde31b4e7678a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*G0f_5p-4OptkgNy-h4Lknw@2x.png"/></div></figure><p id="ba03" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们有<em class="lv"> X </em> ₁= <em class="lv"> x </em> ₁、<em class="lv"> X </em> ₂= <em class="lv"> x </em> ₂、… <em class="lv"> X_M </em> = <em class="lv"> x_M </em>(线性代数中向量通常被认为是列向量，但在本文中，我们假设它们是行向量)。特征向量也可以是数据集的第<em class="lv"> j </em>行。在这种情况下，我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e433e27f032a90e7544d1a50ab948d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*WQkqTTJqHuMedbpIH7dyiA@2x.png"/></div></figure><p id="1990" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也可以是数据集中不存在的测试数据点(本文中粗体小写字母(像<strong class="lb iu"> <em class="lv"> x </em> </strong>)指的是向量。粗体大写字母(如<strong class="lb iu"> <em class="lv"> A </em> </strong>)指矩阵，小写字母(如<em class="lv"> x </em> ₁)指标量值)。数据集的特征用大写字母表示，如(<em class="lv"> X </em> ₁).一对(<strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾，<em class="lv"> y </em> ⁽ <em class="lv"> ʲ </em> ⁾)被称为这个数据集的训练例子。现在我们可以使用一个模型来学习这个数据集:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9a9e13b9800fac87f1eaf0281d19baa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*G_AQfinR3IWUX0__cMJfuA@2x.png"/></div></figure><p id="758a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数采用特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>，这意味着它应用于<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有元素。例如，对于线性模型，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/013503af7b9356b3a99a53f143588a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*OELn8cMU_VkL898B1eWMNw@2x.png"/></div></figure><p id="c81c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，对于<strong class="lb iu"> <em class="lv"> x </em> </strong>的每个值，模型预测为<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)。如前所述，这个特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>可以是训练数据集的实例之一，也可以是训练数据集中不存在的测试数据实例。例如，使用此线性模型，我们可以预测其中一个训练示例的目标:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi no"><img src="../Images/39dd6270646c680ac2c99ce9f8734cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbD7mcxV7-rp7Kupo4P-Nw@2x.png"/></div></div></figure><p id="fd97" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，<em class="lv">f(</em><strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾<em class="lv">)</em>为数据集第<em class="lv">j</em>-行的模型预测，与<em class="lv">f(</em><strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾<em class="lv">)</em>和<em class="lv">y</em>⁽<em class="lv">ʲ<em class="lv">之差</em></em></p><p id="3715" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以假设一个机器学习模型是一个联盟博弈，<em class="lv"> M </em>特征就是这个博弈中的<em class="lv"> M </em>玩家。但是这个游戏的特色功能应该是什么呢？我们第一个猜测可以是<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)本身。但请记住，一个特征函数应该满足等式 1，这意味着当我们没有球员时，总增益为零。我们没有特色(玩家)怎么评价<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)？当一个特性不是游戏的一部分时，这意味着它的当前值是未知的，我们希望在不知道该特性的值的情况下预测模型的目标。当我们在游戏中没有特性时，意味着没有一个特性的当前值是已知的。在这种情况下，我们只能使用训练集进行预测。在这里，我们可以将训练样本的一个样本(或全部样本)的<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾)的平均值作为我们的最佳估计。所以，当我们没有特征时，我们的预测是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi np"><img src="../Images/9a5eea4cd87414fed0308685901dbba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4WelwQbaNmAlc0pXYazJUA@2x.png"/></div></div></figure><p id="44bb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> NA </em>表示不可用的特性(所以<em class="lv"> f </em>的参数在这里都不可用)。我们还从训练数据集(<em class="lv"> k </em> ≤ <em class="lv"> N </em>)中采样了<em class="lv"> k </em>个数据实例(特征向量)。现在我们将大联盟的特征函数定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nq"><img src="../Images/d355017834ed9b18ec7e2a2aed726866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5c4uAR1WcEawIqNCggkAPA@2x.png"/></div></div></figure><p id="e307" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们没有特征，那么使用等式 7 我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4a577d84dedec0a6b60f7eeef57a6494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*8VxIXHw4ro05QfM3S9J3yg@2x.png"/></div></figure><p id="6023" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个特征函数现在满足等式 1，可以给出大联盟<strong class="lb iu"><em class="lv">f</em></strong>= {<em class="lv">x</em>₁<em class="lv">，X </em> ₂ <em class="lv">，…，X_M </em> }。但是我们还需要<strong class="lb iu"><em class="lv">F</em></strong>-{<em class="lv">I</em>}的任何一个联盟的价值，才能够使用等式 3。我们如何将函数<em class="lv"> f </em>应用于其原始参数的子集？我们可以用两种方法来做这件事。首先，我们可以仅在原始特征的子集上重新训练相同的模型(具有相同的超参数)。例如，如果联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>包含了特性:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d063e52a0883915e7b7b15476626b3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*u63DVt2RlBggkZCsWxYjlA@2x.png"/></div></figure><p id="0a0c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要这些特征的<em class="lv"> f </em>的临界值，称为<em class="lv">f</em><strong class="lb iu"><em class="lv">【ₛ</em></strong>(<strong class="lb iu"><em class="lv">xₛ</em></strong>):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7609ed71c90e4b6bb6e337f1e1fbcd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*26kLZ27YbIIYGvfx7grpGg@2x.png"/></div></figure><p id="3693" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>是一个向量，只包含<strong class="lb iu"><em class="lv"/></strong>中出现的特性的值(请注意，一个联合是由特性组成的，但是一个函数取这些特性的值)。我们既可以对联盟中存在的特征<strong class="lb iu"> <em class="lv"> S </em> </strong>重新训练同类型的模型，得到<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)，也可以使用原始函数<em class="lv"> f </em>计算<em class="lv"> f </em> <strong class="lb iu"> ₛ </strong>。当一个特征在<strong class="lb iu"> <em class="lv"> S </em> </strong>中不存在时，那么就意味着我们不知道它的当前值，可以用<em class="lv"> NA </em>代替(不可用)。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mr"><img src="../Images/8cc3bfaea111e44e8a884cdc8a851b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRKmQniLhUyJTjYDgpU-_A@2x.png"/></div></div></figure><p id="5c2f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如说如果<strong class="lb iu"> <em class="lv"> F </em> </strong> ={X₁ <em class="lv">，X </em> ₂ <em class="lv">，X </em> ₃ <em class="lv">，X </em> ₄ <em class="lv">，X </em> ₅}，以及联军<strong class="lb iu"><em class="lv">s</em></strong>= {<em class="lv">x</em>₁<em class="lv">，X </em> ₂ <em class="lv">，X </em> ₅}，那么我们所以:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nu"><img src="../Images/74c86e73cd949dd15840e7f7cc33dada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z52pbZx64eclVCZ5Vl5HpA@2x.png"/></div></div></figure><p id="6bb1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们假设用<em class="lv"> f </em>表示的模型可以处理<em class="lv"> NA </em>值。因此，这个联盟的价值是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nv"><img src="../Images/3be4d1312506940f16cfaa19f00049b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*blYxr4LOIOX2I0rLexE_Ug@2x.png"/></div></div></figure><p id="8a0c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">f</em><strong class="lb iu"><em class="lv">ₛ</em></strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)<em class="lv"/>是通过对联合<strong class="lb iu"><em class="lv"/></strong>中存在的特征重新训练模型或者从等式 9 中获得的。例如对于<strong class="lb iu"><em class="lv">f</em></strong>= {<em class="lv">x</em>₁<em class="lv">，X </em> ₂ <em class="lv">，X </em> ₃ <em class="lv">，X </em> ₄ <em class="lv">，x</em>₅}<strong class="lb iu">和<em class="lv">s</em></strong>= {<em class="lv">x</em>₁<em class="lv">，X </em> ₂</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nw"><img src="../Images/18360787ae97d9c3e119dadb83b781ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRb2SOpc3olUURTxzKnASw@2x.png"/></div></div></figure><p id="52c0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以简单地使用等式 3 和等式 10 来计算特征<em class="lv"> Xᵢ </em>的 Shapley 值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nx"><img src="../Images/b2ce72f2bd3ad1dee03bc3405a4fddb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4oTYGUqTYVjRBK6l8M8q9Q@2x.png"/></div></div></figure><p id="2da2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在这个等式中，我们应该将<em class="lv">ϕ</em>t124】ₓt126】ᵢ写成与等式 3 一致。然而，为了简单起见，我们使用ϕᵢ。所以，在这个等式中，<em class="lv"> i </em>表示第<em class="lv"> i </em>个特征(<em class="lv"> Xᵢ </em>)。通过简化前面的等式，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ny"><img src="../Images/a7419f8824b8f9026cc3c8c869ac73e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z--zG0bAYtpLlbyypK4aVw@2x.png"/></div></div></figure><p id="768f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">f</em>T2】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)<em class="lv"/>为联盟中出现的特征<em class="lv">f</em>s<strong class="lb iu"><em class="lv"/></strong>，<em class="lv">f _</em><strong class="lb iu">s</strong>∩{<em class="lv">I</em>}(<strong class="lb iu"><em class="lv">x _ s<em class="lv">如果我们使用 Shapley 值的效率属性(等式 5)，我们可以写出:</em></em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nz"><img src="../Images/543d8ca46fd0ad6c81a9e61bb3cac915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gppxaSnXYRY8qxnand7xJA@2x.png"/></div></div></figure><p id="9765" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着所有特征的 Shapely 值的总和给出了具有当前特征值的模型的预测和所有训练示例的模型的平均预测之间的差异。</p><p id="00e2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">模型解释的数学描述</strong></p><p id="9783" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在讨论 SHAP 值之前，我们需要一个像 SHAP 这样的解释者模型的数学描述。设<em class="lv"> f </em>为待解释的原始模型，定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1ef3b0c91a3248ea56642e3b72f5cf07.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*y5nPOQZ8Xpe70oMmlex4wA@2x.png"/></div></figure><p id="3318" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，模型采用特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>和<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)作为该特征向量的模型预测。该特征向量可以是训练数据集的实例之一(<strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾)或者是训练数据集中不存在的测试特征向量。现在我们创建一组<em class="lv">简化输入特征</em>来显示特征向量中的特征是否存在:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e1bd0823cfc06e338791864cd01e23ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*-UDmFMW_CgG84rVL84_9fg@2x.png"/></div></figure><p id="c148" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向量<strong class="lb iu"><em class="lv">x</em></strong>’称为简化特征向量。每个<em class="lv"> x'ᵢ </em>都是一个二元变量，显示其对应的特征<em class="lv"> Xᵢ </em>是否在特征向量中被观察到(<em class="lv"> x'ᵢ </em> =1)或者是未知的(<em class="lv"> x'ᵢ </em> = 0)。例如，如果你的特征向量是</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oc"><img src="../Images/46f7116b892d10562a54278217c1ae95.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*A1-O-keBLiQnhNkbLsjjAQ@2x.png"/></div></div></figure><p id="6cdd" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi od"><img src="../Images/eb96f99b41ed407364657b99ee6e4d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*XVtzNgZTTXAxsovKsncArg@2x.png"/></div></figure><p id="24cb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以假设有一个映射函数将<strong class="lb iu"> <em class="lv"> x </em> </strong>映射到<strong class="lb iu"> <em class="lv"> x </em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oe"><img src="../Images/780e6385fc13489f9c9d0e4015bd054f.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*MCI0GVzXeA_wRwUwwPyufw@2x.png"/></div></div></figure><p id="4a6a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，它取简化的特征向量<strong class="lb iu"><em class="lv">x</em></strong>’并返回特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi of"><img src="../Images/5fdc95b0b96d9c80b055eb82071a76bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*ypt1oAfqF1afku_HeT-ENQ@2x.png"/></div></div></figure><p id="5c46" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解释器是一个可解释的模型<em class="lv"> g </em>，它采用了<em class="lv"> M </em>个二元变量:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi og"><img src="../Images/15b02acd46239de59149e8a3c6e9c60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*KhfKX-8vk3AixTGUw4UZRw@2x.png"/></div></figure><p id="37e3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> M </em>是等式 13 中简化输入特征的数量。行向量<strong class="lb iu"><em class="lv">z</em></strong>’表示<strong class="lb iu"> <em class="lv"> x </em> </strong>的可用值的联合。<em class="lv"> </em>所以<strong class="lb iu"> <em class="lv"> x </em> </strong>的零元素在<strong class="lb iu"> <em class="lv"> z </em> </strong>中总是零，而<strong class="lb iu"> <em class="lv"> x </em> </strong>的 1 元素在<strong class="lb iu"> <em class="lv"> z </em> </strong>中可以是 1 也可以是 0。我们把<strong class="lb iu"><em class="lv">z</em></strong>a<em class="lv">联军向量</em>。例如，如果特性的值为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1afb9a6e8e1eb02959d6578814bcd1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*aWBbbbzqeeqWNbB0rpZJOQ@2x.png"/></div></figure><p id="4fd2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么特征向量是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oi"><img src="../Images/3c83ed504431e0a66da4e81316e36690.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*BGY8tRWtoE5aKhC6E6jFag@2x.png"/></div></div></figure><p id="12b4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简化的特征是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d9f403f1e0cccdcaf41b11c28e0bbe1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*kFqx3PZegJW8ORwYDvC8Mw@2x.png"/></div></figure><p id="1fa3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在一个值为<strong class="lb iu"> <em class="lv"> z </em> </strong>比如</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e295abde456e103f801e42a653d81f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*t92ZS8nQ_h-8JylMxwS1vg@2x.png"/></div></figure><p id="7aac" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单的代表联盟<strong class="lb iu"><em class="lv">s</em></strong>= {<em class="lv">x</em>₁，<em class="lv"> X </em> ₃}既然只有这两个特征在<strong class="lb iu"> <em class="lv"> z </em> </strong>中有对应的 1。我们还可以断定，<strong class="lb iu"> <em class="lv"> x </em> </strong>'代表大联盟<strong class="lb iu"><em class="lv">f</em></strong>= {<em class="lv">x</em>₁，<em class="lv"> X </em> ₂，<em class="lv"> X </em> ₃}，所以<strong class="lb iu"> <em class="lv"> x </em> </strong>'也可以认为是联盟向量。如前所述，对于单个特征向量，我们希望解释器模型的预测接近原始模型的预测。假设我们从一个联盟向量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">’</em>开始，这个向量非常接近大联盟<strong class="lb iu"><em class="lv">x</em></strong>’。<em class="lv"> g </em>对于这个联盟的预测，简单来说就是<em class="lv">g</em>(<strong class="lb iu"><em class="lv">z</em></strong>’)。但是怎么才能用<strong class="lb iu"><em class="lv">z</em></strong>’得到<em class="lv"> f </em>的预测呢？问题是<em class="lv"> f </em>取的是特征向量，而不是联合向量。所以我们需要映射函数<em class="lv"> h </em> <strong class="lb iu"> <em class="lv"> x </em> </strong>来为<strong class="lb iu"> <em class="lv"> z </em> </strong>找到对应的特征值。这里<em class="lv">h</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)返回<strong class="lb iu"><em class="lv">z</em></strong>’中出现的特性的对应值，其他特性的值将为<em class="lv"> NA </em>。例如，如果我们有</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e295abde456e103f801e42a653d81f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*t92ZS8nQ_h-8JylMxwS1vg@2x.png"/></div></figure><p id="487b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/99b53040c2f8c282a5a5eb04822bc828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*VH9a8-5v4Ac-vS5dmwgHig@2x.png"/></div></figure><p id="776c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> f </em>对<strong class="lb iu"> <em class="lv"> z </em> </strong>中出现的特征的预测为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/029c33c67f370b908ef7a38e9d778c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*sJb2Pm74UnVsw9t5gojh_Q@2x.png"/></div></figure><p id="7ae1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还将<em class="lv"> f </em> <strong class="lb iu"> <em class="lv"> ₓ </em> </strong>定义为<strong class="lb iu"> <em class="lv"> z </em> </strong>中出现的特征的<em class="lv"> f </em>的临界值。所以我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ol"><img src="../Images/a4b4c6c7e6dd4ed705e61725fbaf4031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cjfip7mucmeq_EcBEiVsTQ@2x.png"/></div></div></figure><p id="65b4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在等式 8 中，<em class="lv"> f </em> <strong class="lb iu"> <em class="lv"> ₛ </em> </strong>表示联合<strong class="lb iu"> <em class="lv"> S </em> </strong>中存在的特征的<em class="lv"> f </em>的边缘值。不过，这里我们关注的是<strong class="lb iu"><em class="lv">z</em></strong>’而不是联盟，<em class="lv">f</em><strong class="lb iu"><em class="lv"/></strong>表示<em class="lv"> f </em>对于<strong class="lb iu"><em class="lv">z</em></strong>’中出现的特性的边际值。在这个例子中，我们用<strong class="lb iu"> <em class="lv"> z </em> </strong>代表联军<strong class="lb iu"><em class="lv">s</em></strong>= {<em class="lv">x</em>₁，<em class="lv"> x </em> ₃}.所以我们也可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi om"><img src="../Images/db5135f27593de790e31324d5fddab69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EqvXbJQb66GTYADPlwbNNw@2x.png"/></div></div></figure><p id="8590" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望<em class="lv"> f </em>的预测值(即<em class="lv">f</em>(<strong class="lb iu"><em class="lv">z</em></strong>)与<em class="lv"> g </em>(即<em class="lv">g</em>(<strong class="lb iu"><em class="lv">z</em></strong>’)非常接近，以确保<em class="lv"> g </em>模仿<em class="lv">f</em>的相同过程总而言之，我们想要</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi on"><img src="../Images/701cd8fd84bcc2859a7e400a50decf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fw0dPvmWTKymOZ2a6VkcRQ@2x.png"/></div></div></figure><p id="b997" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">能够声称<em class="lv"> g </em>能够解释<em class="lv"> f </em>。</p><p id="6574" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以根据<em class="lv"> g </em>对解释方法进行分类。<em class="lv">附加特征归因方法</em>有一个解释器模型，它是二元变量的线性函数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/93bd3eace325263233416a8bd1562227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*ByZgruGDu39aZlt_W8e6fQ@2x.png"/></div></figure><p id="3233" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> ci </em>是一些常数。正如我们后来看到的，SHAP 属于这一类方法。所以我们想用<strong class="lb iu"><em class="lv">z</em></strong>’来表示方程 11。设<strong class="lb iu"> <em class="lv"> x </em> </strong>为特征向量，<strong class="lb iu"><em class="lv">x</em></strong>’为其简化特征向量。我们可以证明，Shapley 值可以表示为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi op"><img src="../Images/d4fba7c02fbda767071c0b0317149972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVbKF4VvnMOoarWLhveAUg@2x.png"/></div></div></figure><p id="d36f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> ϕᵢ </em> ( <em class="lv"> f </em>，<strong class="lb iu"> <em class="lv"> x </em> </strong>)强调的是沙普利值是<em class="lv"> f </em>和<strong class="lb iu"> <em class="lv"> x </em> </strong>的函数。这里我们考虑<strong class="lb iu"><em class="lv">x</em></strong>’的所有可能的联盟向量(对应于<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有联盟)。对于每个联合向量<strong class="lb iu"><em class="lv">z</em></strong>’，我们计算特征<em class="lv"> i </em>的贡献。| <strong class="lb iu"> <em class="lv"> z </em> </strong> '|是<strong class="lb iu"> <em class="lv"> z </em> </strong>'(对应大联盟的大小)中非零项的个数，|<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">'</em>|是<strong class="lb iu"> <em class="lv"> x </em> </strong>'(大联盟的大小)。<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>表示对应的联盟不包含特征{ <em class="lv"> i </em> }。所以，<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>表示将<strong class="lb iu"> <em class="lv"> z </em> </strong>'的第<em class="lv"> i </em>个元素设置为 0(<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>= 0)得到的联合向量。例如，如果 3 代表第三个特征<em class="lv"> x </em> ₃，那么我们可以写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oq"><img src="../Images/502d55994d928bc21633e46966e1a403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jeNe1BQwt8IKDL9LC9atrg@2x.png"/></div></div></figure><p id="c8f1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很容易证明等式 11 和等式 18 是等价的。</p><p id="39f8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">证明(<strong class="lb iu"> <em class="lv">可选</em> </strong>):首先注意，等式 11 中的| <strong class="lb iu"> <em class="lv"> F </em> </strong> |等于等式 18 中的|<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">'</em>|因为它们都是指特征总数，<em class="lv"> M </em>。记住等式 11 中的每个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>不包括特征<em class="lv"> i </em>自<strong class="lb iu"><em class="lv">s</em></strong><em class="lv">⊆</em><strong class="lb iu"><em class="lv">f</em></strong>-{<em class="lv">I</em>}。我们很容易看到，方程 11 中的每个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>在方程 18 中都有对应的值<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>。例如，如果我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi or"><img src="../Images/41e68666c3d90b8ea57c95fc41de3fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WPD0gDsBKsTQnskmBUOmA@2x.png"/></div></div></figure><p id="0d07" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ea2c113294e6e6e9896c97ec65af7a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*XAkkyrqh3sHuMtef_469Sw@2x.png"/></div></figure><p id="0738" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>为<em class="lv"/><strong class="lb iu"><em class="lv">S</em></strong><em class="lv"/>的对应值为【1 1 0 1】。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/e8a94b537c4a1aba5adb09c19f7add1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*2uoxMYfi6jMssJauOOCrqQ@2x.png"/></div></figure><p id="74f4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过替换等式 11 中的这些值，该值的对应项<strong class="lb iu"> <em class="lv"> S </em> </strong>为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ou"><img src="../Images/f4e6d427289114e3ca1e4db28c8caa6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JwHDtelJ9g4_q_ydjPn6cw@2x.png"/></div></div></figure><p id="2175" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在等式 18 中，这个值<strong class="lb iu"><em class="lv"/></strong><em class="lv">’</em>的对应项是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ov"><img src="../Images/fe25bafd92c56dc4f77d372a19aff92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfvsCNahNDOWuwVG-EVJ2g@2x.png"/></div></div></figure><p id="bcb5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，根据等式 15，我们得出结论，等式 19 和等式 20 给出了相同的结果。一般来说，对于等式 11 中的每个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>，我们都有一个<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>即<em class="lv"> </em>来代表它。因此，<strong class="lb iu"> <em class="lv"> z </em> </strong>也代表<strong class="lb iu"><em class="lv">S</em></strong>∩{<em class="lv">I</em>}， 并且我们可以将<em class="lv">f</em>t30】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)和<em class="lv">f _</em><strong class="lb iu"><em class="lv">s</em></strong>∩{<em class="lv">I</em>}(<strong class="lb iu"><em class="lv">x _ s</em></strong>∩{<em class="lv">I</em>})替换为<em class="lv"> f </em> <strong class="lb iu"> 另外，| <strong class="lb iu"> <em class="lv"> S </em> </strong> |和| <strong class="lb iu"> <em class="lv"> F </em> </strong> |分别等于| <strong class="lb iu"> <em class="lv"> z </em> </strong> '|-1 和| <strong class="lb iu"> <em class="lv"> x </em> </strong> '|。因此，等式 11 中的每一项(对于不包括<em class="lv"> i </em>的联合<strong class="lb iu"> <em class="lv"> S </em> </strong>)在等式 18 中具有相应的项，其中联合向量<strong class="lb iu"><em class="lv">z</em></strong>’包括<em class="lv"> I </em>，并且这两项给出相同的值:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ow"><img src="../Images/b814df869e2cda12f059a2d24187d5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3lhhFtr3KAq7Mbmx6-7N6w@2x.png"/></div></div></figure><p id="3736" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但这只包括等式 18 中包含{ <em class="lv"> i </em> }的<strong class="lb iu">zT103】。在等式 18 中，我们还可以有不包括{ <em class="lv"> i </em> }的<strong class="lb iu"> <em class="lv"> z </em> </strong>。对于一个不包括{<strong class="lb iu"><em class="lv"/></strong>’的联合向量，我们可以写成<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)=<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ox"><img src="../Images/01c944f17d38b379ebe97071db473252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDRHTqd6k3mjg7AcTdZyqg@2x.png"/></div></div></figure><p id="6f9f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它没有给方程 18 增加任何东西。因此，我们得出结论，等式 11 和等式 18 是等价的，并给出相同的结果。∎</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="d022" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，在等式 18 中，空联盟的联盟向量(<strong class="lb iu"><em class="lv">z</em></strong>' =[0 0…0])不包括在求和中。对于这个联合向量，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b73c0687da582b53e44230d621dfbeed.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*PwB2CCZtay0bNcxOTx8g3g@2x.png"/></div></figure><p id="1d7d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是没有定义的。即使能计算出来，这个联盟也不会增加任何东西</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pf"><img src="../Images/88b82b33986602ce41d4b32ed68d67c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*WSNu6HpgWSyVDhjgt7Aadw@2x.png"/></div></div></figure><p id="89a8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还请注意，在介绍 SHAP 方法的原始论文中，等式 18 的写法不正确(参见[1]中的等式 8)。等式 11 是经典的 Shapley 值等式，在这个等式中，我们只关注可用的特性。等式 18 引入了缺失的概念。这里的大联盟向量<strong class="lb iu"> <em class="lv"> x </em> </strong>'可以有一些缺失值，其中<strong class="lb iu"> <em class="lv"> x </em> </strong> ' <em class="lv"> ᵢ </em> =1。等式 18 具有一些有趣的性质，描述如下:</p><p id="166d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性 1(局部精度)</strong></p><p id="bc58" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<em class="lv">g</em>(<strong class="lb iu"><em class="lv">x</em></strong>’)为解释模型，定义为</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/696b1a9f67198bbbd01f1bf89ad3c03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*ss6lSALCjJ09jcUa_gnIKw@2x.png"/></div></figure><p id="695e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)】、<em class="lv"> ϕᵢ </em>为<em class="lv"> f </em>的沙普利值。假设我们有一个特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>，它的简化特征向量是<strong class="lb iu"> <em class="lv"> x </em> </strong>'，那么我们有<strong class="lb iu"><em class="lv">x</em></strong>=<em class="lv">h</em><strong class="lb iu"><em class="lv">【ₓ</em></strong>(<strong class="lb iu"><em class="lv">x</em></strong>')。然后基于这个性质，预测的<em class="lv"> g 为</em><strong class="lb iu"><em class="lv">x</em></strong><em class="lv">'</em>匹配原模型的预测<em class="lv"> f </em>为<em class="lv"> </em> <strong class="lb iu"> <em class="lv"> x </em> </strong>。所以，我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/788be8fdc0091dd5406b25c8fd8c1cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*QpJ61YWzFiH9k4Wj7az6Zw@2x.png"/></div></figure><p id="0928" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">证明(<strong class="lb iu"> <em class="lv">可选</em> </strong>):假设|<strong class="lb iu"><em class="lv">f</em></strong>| =<em class="lv">m</em>(特征数)，并且所有特征都可用(<em class="lv"> x </em> ' <em class="lv"> ᵢ </em> =1 对所有<em class="lv"> i </em>)我们可以用等式 12 来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pi"><img src="../Images/608cee9f7a6deb68d3671dff258a19cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LjcxLZzhhyLjOJ_vR9JjNQ@2x.png"/></div></div></figure><p id="4823" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)】。∎</p><p id="c460" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，等式 11 仅考虑可用特征。因为等式 12 是从该等式导出的，所以它仅包括可用特征的 Shapley 值。例如，如果最后一个特征不可用，那么从等式 12 我们得到</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7be2d0740db256bd14768c611e2e1252.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*8uh1VdbbgTvqB-Yy059QZg@2x.png"/></div></figure><p id="5dfe" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这个结果仍然与等式 22 一致，因为对于最后一个特征，我们有<em class="lv"> x </em> '_ <em class="lv"> M </em> =0。</p><p id="a7bd" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性 2(缺失)</strong></p><p id="9138" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缺失要素的 Shapley 值应该为零。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8f47d2553eae2d364aba974f5a75202f.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*AMhQegtA_8xoz_TSe8nCJw@2x.png"/></div></figure><p id="a8d3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">证明(<strong class="lb iu"> <em class="lv">可选</em> </strong>):这不是方程 11 中经典 Shapley 值的必要条件。因为它不包括缺失的功能。然而，如果我们使用公式 18 计算 Shapley 值，那么我们可以证明它满足这一特性。考虑从<strong class="lb iu"> <em class="lv"> x </em> </strong>导出的所有联盟向量<strong class="lb iu"><em class="lv">z</em></strong>’。如果<em class="lv"> x </em> ' <em class="lv"> ᵢ </em> =0，那么<em class="lv"> z </em> ' <em class="lv"> ᵢ </em> =0。所以，对于这些联合向量我们得到<strong class="lb iu"><em class="lv">z</em></strong>' \<em class="lv">I</em>=<strong class="lb iu"><em class="lv">z</em></strong>'，我们可以写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pk"><img src="../Images/7d3e871ed2ddde587d17d1b0fbf3582d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vh-83RLMpzyP8q5--4TMRA@2x.png"/></div></div></figure><p id="d6bb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">属性 3(一致性)</strong></p><p id="ceaf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一致性意味着改变原始模型来增加一个特征对模型的影响将永远不会减少它的 Shapley 值。从数学上讲，如果我们有一个单一的特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>，以及两个模型<em class="lv"> f </em>和<em class="lv">f</em>’(两个模型<em class="lv"> f </em>和<em class="lv">f</em>’都是<strong class="lb iu"> <em class="lv"> x </em> </strong>的函数)，那么</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pl"><img src="../Images/b8b9747214078e8d3463e383f939500e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*Q-83wvV0j1nhIwy-Cl_4tA@2x.png"/></div></div></figure><p id="1b8c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有输入<strong class="lb iu"><em class="lv">z</em></strong>’，则</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pm"><img src="../Images/2475dd0c3ebb462bed79c7a07d9b267a.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*qGux8ZTXL2YTI0XpciBQMA@2x.png"/></div></div></figure><p id="8f59" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，在等式 18 中</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7614a2120b3c595b17855d0168c32ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*31HHRLZK6hYzFhBHpTJZ-Q@2x.png"/></div></figure><p id="efbf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与特征<em class="lv"> Xᵢ </em>对预测的贡献成比例。因此，如果将模型从<em class="lv"> f </em>改为<em class="lv">f’</em>，并且得到特征<em class="lv"> Xᵢ </em>对预测<strong class="lb iu"> <em class="lv"> x </em> </strong>增加的更高贡献(或保持不变)，那么<em class="lv"> Xᵢ </em>的 Shapley 值永远不会减小。</p><p id="5cb2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">证明(<strong class="lb iu"> <em class="lv">可选</em> </strong>):还是那句话，很容易说明等式 18 满足这个性质。因为我们对于两个模型具有相同的特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>，所以我们将具有相同的<strong class="lb iu"><em class="lv">x</em></strong>’。现在对于各联军的矢量<strong class="lb iu"><em class="lv"/></strong><strong class="lb iu"><em class="lv"/></strong>【我们有</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi po"><img src="../Images/22bc1acac3a104f1833c3d9521ff2ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAumlZ1TOyV9zbRSRepPGQ@2x.png"/></div></div></figure><p id="c660" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，通过为所有的<strong class="lb iu"><em class="lv"/></strong>增加这些术语，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pp"><img src="../Images/e5e880117551b767c1d02e5f90089c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63y5fdyC_ZbDeknKzShtpw@2x.png"/></div></div></figure><p id="10e3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经熟悉了 Shapley 值及其属性，我们可以看到它们如何解释机器学习模型。假设我们有一个模型<em class="lv"> f </em>，一个特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>。我们将模型<em class="lv"> g </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/5edd68e8a353dc56715066367a0b7cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*LrUfIUIvjQBJTsQxabpZFw@2x.png"/></div></figure><p id="0949" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)】,<em class="lv">ϕᵢ</em>为<em class="lv"> f </em>的 Shapley 值，<strong class="lb iu"> <em class="lv"> x </em> </strong>为<strong class="lb iu">x</strong>的简化特征向量(所以<strong class="lb iu"> <em class="lv"> x </em> </strong> = <em class="lv">模型<em class="lv"> g </em>是线性的，所以是可解释的。另外基于性质 1，<em class="lv">g</em>(<strong class="lb iu"><em class="lv">x</em></strong>')=<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)，因此<em class="lv"> g </em>可以完美地模拟<em class="lv"> f </em>进行单次预测<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)，并且可以作为<em class="lv">的解释器模型可以看出，对于附加特征归属方法，上面定义的模型<em class="lv"> g </em>是遵循等式 17 并且满足性质 1、2 和 3 的唯一可能的解释器模型。总之，Shapley 值可以提供线性模型的系数，该系数可以用作任何机器学习模型的完美解释器。</em></em></p><p id="6937" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">从沙普利值到 SHAP 值</strong></p><p id="75e2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shapley 值具有坚实的理论基础和有趣的性质，然而，在实践中，计算它们并不容易。为了计算它们，我们需要计算等式 18 中的<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)或等式 11 中的<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)。记住这一点</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/8bc2e5997166d2bbcd781632c14c8d5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*aYbPA1vP8ViaSMGg0uVZCw@2x.png"/></div></figure><p id="701b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，计算<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)或<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)意味着我们需要计算<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)带有一些<strong class="lb iu"> <em class="lv"> z </em>中没有的缺失特征问题是大多数模型不能处理缺失值。比如在线性模型中，我们需要<em class="lv"> xᵢ </em>的所有值来计算<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)。所以，我们需要一种方法来处理<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)中缺失的值。如前所述，对于每个联盟<strong class="lb iu"> <em class="lv"> S </em> </strong>，<strong class="lb iu"> <em class="lv"> x </em> </strong>的缺失元素是<strong class="lb iu"> <em class="lv"> S </em> </strong>中不存在的特征值。为了计算<em class="lv">f</em>t60】ₛ</strong>(<strong class="lb iu">t63】xₛ</strong>，我们假设:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/a556b3ed7c120781c290e2783a5f5f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*56g7w80diRrYWZS4yvCCEA@2x.png"/></div></figure><p id="76b3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)|<strong class="lb iu"><em class="lv">x</em>ₛ</strong>]是<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)的期望值，以<strong class="lb iu"><em class="lv"/></strong>中出现的特性为条件。同样，我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pt"><img src="../Images/e240aed35b1eff524bb539fe113bd801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIx3xCGOookpvf-luaNlKA@2x.png"/></div></div></figure><p id="3e6f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用等式 23 或等式 24 中的条件期望计算的 Shapley 值被称为<em class="lv"> SHAP </em> (SHapley 加法解释))<em class="lv">值</em>。因此，为了从等式 11 获得 SHAP 值，我们可以写为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pu"><img src="../Images/44cd0a26bc0edd8838fbfc8632a637c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1I93Y61nWWTOjQbbgObsw@2x.png"/></div></div></figure><p id="1771" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了从等式 18 计算 SHAP 值，我们可以写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pv"><img src="../Images/e1365ef7bb989ddb146d030954365f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMGJv3atKvQVibWwd0k9zg@2x.png"/></div></div></figure><p id="ed7c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP 是一种附加特征归因方法，其中我们有一个线性解释器模型。在本文中，我们讨论两种方法来计算方程 23 或方程 24 中的条件期望。第一个在本节中讨论，第二个适用于树结构数据的将在后面讨论。</p><p id="692b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>表示联军<strong class="lb iu"> <em class="lv"> S </em> </strong>的补充。所以，<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>表示<strong class="lb iu"><em class="lv"/></strong>中不存在的部分原始特征。现在我们可以用全概率定律来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pw"><img src="../Images/f27538faacfdf9846c963cb96fbbab87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v9288oA-XxU4pHHLA2A02w@2x.png"/></div></div></figure><p id="cd58" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> f </em> ( <strong class="lb iu"> <em class="lv"> x_S̅ </em> </strong>，<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>)表示<em class="lv"> f </em>的部分参数属于<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>，其余参数属于<strong class="lb iu"> <em class="lv"> x_S̅ </em> </strong>。当然，<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>或者<strong class="lb iu"> <em class="lv"> x_S̅ </em> </strong>的参数不一定是连续排列的。<em class="lv">p</em>(<strong class="lb iu"><em class="lv">x_s̅</em></strong>|<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)是<strong class="lb iu"> <em class="lv"> x_S̅ </em> </strong>给定<strong class="lb iu"> <em class="lv"> xₛ </em> </strong>的条件概率。所以，要计算<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)的值，我们需要条件概率<em class="lv">p</em>(<strong class="lb iu"><em class="lv">x_s̅</em></strong>|<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)。不幸的是，我们大部分时间都不知道这个分布。因此在 SHAP，我们假设这些特征是相互独立的，所以:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi px"><img src="../Images/85f89be2f504c6e36abe5160c95b12e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*UlsasLKP-rSeFN2_6Hx8SQ@2x.png"/></div></figure><p id="c139" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将该等式代入等式 27，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi py"><img src="../Images/63ef03177a3dab8562a85968f18e0bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xl8xBafMXRGP7Q57cw7eSg@2x.png"/></div></div></figure><p id="cda7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们有离散的数据点，我们可以用一个和来近似这个积分。我们从训练数据集(<em class="lv"> k </em> ≤ <em class="lv"> N </em>)中抽取<em class="lv"> k </em>个数据实例(特征向量)，并将它们分别称为<strong class="lb iu"><em class="lv">x</em></strong>⁽<em class="lv">ʲ</em>⁾.每个数据实例中的特征或者属于<strong class="lb iu"> <em class="lv"> S </em> </strong>或者属于<strong class="lb iu"> <em class="lv"> S̅ </em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/b1cc44f9290f9f3a11a7778123ecb84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*Zmy_RSB8n59W3whmvPsF9A@2x.png"/></div></figure><p id="3150" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，对于每个数据实例，我们将出现在<strong class="lb iu"><em class="lv"/></strong>中的特征值替换为它们在<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>中的对应值，并对该数据实例进行预测:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/900f5df14144aa66bea48a15f4b9fb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*nphPlEiEy8jfIeeBlxI05Q@2x.png"/></div></figure><p id="aa09" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，前面的积分可以用这些预测的平均值来近似:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qb"><img src="../Images/6ae480df7d91d08aeb180208ee71a956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9JjjbEkiH_pVInBlInXjw@2x.png"/></div></div></figure><p id="d086" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图 6 显示了计算该积分的示例。同样，我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qc"><img src="../Images/bc350713d085f4b8f292705d5d000e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgPmq7HvX_LLslDJUcZKUw@2x.png"/></div></div></figure><p id="64fc" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"><em class="lv"/></strong>是在<strong class="lb iu"> <em class="lv"> z </em> </strong>中具有非零索引的特征集合。图 6 显示了这种方法的一个例子。这里我们有一个模型<em class="lv"> f </em> ( <em class="lv"> x </em> ₁、<em class="lv"> x </em> ₂、<em class="lv"> x </em> ₃、<em class="lv"> x </em> ₄、<em class="lv"> x </em> ₅)其中<strong class="lb iu"><em class="lv">x</em>ₛ</strong>= {<em class="lv">x</em>₁、<em class="lv"> x </em> ₃、<em class="lv"> x </em>所以<strong class="lb iu"><em class="lv">x_s̅</em></strong>= {<em class="lv">x</em>₂、<em class="lv"> x </em> ₅}和特征<em class="lv"> X </em> ₂和<em class="lv"> X </em> ₅是<strong class="lb iu"> <em class="lv"> x </em> </strong>中的<em class="lv">娜</em>。特征向量 x 的形式为<strong class="lb iu"><em class="lv">x</em></strong>=【<em class="lv">x</em>₁<em class="lv">纳</em>x₃<em class="lv">x</em>₄<em class="lv">纳</em>。为了计算<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)，我们需要缺失特征的值，因此我们从训练数据集的样本中借用它们的值。对于该样本的第<em class="lv"> i </em>个数据实例，我们将特征向量<strong class="lb iu"><em class="lv">x</em></strong>(<em class="lv">x</em>₂，<em class="lv"> X </em> ₅)的缺失值替换为该实例中的对应值(<em class="lv"> x </em> ₂^( <em class="lv"> i </em>)，<em class="lv"> x </em> ₅^( <em class="lv"> i </em> <strong class="lb iu"><em class="lv">x</em>ₛ</strong>)=<em class="lv">f</em>(<em class="lv">x</em>₁，<em class="lv">x</em>₂<em class="lv">^</em>(<em class="lv">I</em>)，<em class="lv"> x </em> ₃，<em class="lv"> x </em> ₄，<em class="lv">x</em>₅<em class="lv">^</em>( 因此，对于每个数据实例，我们现在都有一个预测。最后我们取这些预测的平均值，报为我们对<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)的估计。现在，我们可以使用公式 29 和公式 30 给出的近似值来计算公式 25 或公式 26 中的 SHAP 值。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qd"><img src="../Images/ab07bfffccc9ba91a2d229b7ea3a85e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvXuAZuVwaj2kr1t18VsfQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 6(来源:图片由作者提供)</p></figure><p id="65f0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，等式 30 也与等式 7 一致。当<strong class="lb iu"> <em class="lv"> z </em> </strong>的所有元素都为零时，<strong class="lb iu"> <em class="lv"> S </em> </strong>成为空集，所以我们只对训练集的<em class="lv"> k </em>数据实例取模型预测的平均值。</p><p id="b91b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性 SHAP </strong></p><p id="7ba1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的预测模型<em class="lv"> f </em>是线性回归模型:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qe"><img src="../Images/5cf42e5d0d175a2c7ce91c1aa5e1666a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiSSXA_ruJkCYdwgnbfSXg@2x.png"/></div></div></figure><p id="45bc" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> x </em> ₀=1，特征<em class="lv"> Xᵢ </em>、<em class="lv"> i </em> = 1、…、<em class="lv"> M </em>相互独立。现在我们可以证明 SHAP 值由以下等式给出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/2dd3b1c797f34e075cecee4e47fd54af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*G9Y_d1vWTJ0dY9RDGkgkcg@2x.png"/></div></figure><p id="7d75" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> k </em>是我们用来计算 SHAP 值的训练数据集样本中的数据实例数量。你可以参考附录中的证明。我们也可以直观地驱动这个方程。在线性模型中，特征<em class="lv"> Xᵢ </em>对<em class="lv">f</em>(<strong class="lb iu">t20】xt22)的贡献简单来说就是<em class="lv"> cᵢxᵢ </em>。所以我们可以写:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qg"><img src="../Images/bfebee51cb066e068ca4edd91f5716a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*eqy5aJaZ0j7mF--8BdCumg@2x.png"/></div></div></figure><p id="c81b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，等式 22 增加了对 Shapley 值的约束:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/a343282bd4925a30e2d11888bf07d958.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*dksyXRLSI_Lj3mkVRX34Ew@2x.png"/></div></figure><p id="0c60" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们减去</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/20a627dd43a6dfbf93a219961f91608b.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*5p-lsyCyhP3OXZaoNt7jGQ@2x.png"/></div></figure><p id="a99e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从方程 31 中的<em class="lv"> cᵢxᵢ </em>来满足方程 22。如果我们将等式 31 中的 Shapley 值相加，我们会看到结果与等式 22 一致:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qj"><img src="../Images/7214a90a9a6dd232694d60a7b40ad4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*atTVInKQl8UFmDRn82zf1g@2x.png"/></div></div></figure><p id="e2fb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用 Python 计算 SHAP 值</strong></p><p id="8c29" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性 SHAP </strong></p><p id="650e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为我们的第一个例子，我们使用 Python 来计算虚拟数据集的线性 SHAP 值(等式 31)。我们首先定义数据集。我们只有两个填充了一些随机数的特征，目标被定义为这些特征的线性组合。这些特征是独立的。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="63c2" class="qo qp it ma b gy qq qr l qs qt"># Listing 1</span><span id="90f0" class="qo qp it ma b gy qu qr l qs qt"># Defining the dataset<br/>X = pd.DataFrame({'a': [2, 4, 8, 0, 3, 6, 9],<br/>  'b': [1, 5, 0, 7, 1, -2, 5]})<br/>y = 5*X['a'] + 2*X['b'] + 3</span></pre><p id="5ff5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们对该数据集使用线性回归模型，并计算该模型的系数，这些系数与用于定义目标的系数相同。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="3f92" class="qo qp it ma b gy qq qr l qs qt"># Listing 2</span><span id="ac28" class="qo qp it ma b gy qu qr l qs qt"># Defining a linear model<br/>linear_model = LinearRegression()<br/>linear_model.fit(X, y)</span><span id="2ae4" class="qo qp it ma b gy qu qr l qs qt">print("Model coefficients:")<br/>for i in range(X.shape[1]):<br/>   print(X.columns[i], "=", linear_model.coef_[i].round(4))</span></pre><p id="6610" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="aefa" class="qo qp it ma b gy qq qr l qs qt">Model coefficients:<br/>a = 5.0<br/>b = 2.0</span></pre><p id="748b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们使用等式 31 来计算该数据集第一个示例的 SHAP 值:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="cf25" class="qo qp it ma b gy qq qr l qs qt"># Listing 3</span><span id="34bd" class="qo qp it ma b gy qu qr l qs qt">shap_values = ((X[:1] — X.mean()) * linear_model.coef_)<br/>shap_values_table = shap_values.T<br/>shap_values_table.columns = ['SHAP_value']<br/>shap_values_table</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/fbd42fc9050b508cdcbb6ee6cff65a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*VpFWAsORSsMrOp9pBrKTjQ.png"/></div></figure><p id="5655" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/681dff41503fe0abeb76719b83374330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*YHq9Dol28r7iddK_VKaviQ@2x.png"/></div></figure><p id="3bdd" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中 1 和 2 分别指特征<em class="lv"> a </em>和<em class="lv"> b </em>。</p><p id="2760" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用 SHAP 库的线性 SHAP</strong></p><p id="641c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用 SHAP 库来计算清单 2 中定义的线性模型的 SHAP 值:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="602f" class="qo qp it ma b gy qq qr l qs qt">import shap<br/>explainer = shap.LinearExplainer(linear_model, X)<br/>shap_values = explainer.shap_values(X[:1])<br/>shap_values</span></pre><p id="2534" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="99ae" class="qo qp it ma b gy qq qr l qs qt">array([[-12.85714286,  -2.85714286]])</span></pre><p id="94ff" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP 图书馆中的类<code class="fe lx ly lz ma b">LinearExplainer()</code>采用训练模型和训练数据集。这个类中的方法<code class="fe lx ly lz ma b">shap_values()</code>获取要解释的行的数组，并返回它们的 SHAP 值。请注意，这里我们得到了与清单 3 相同的结果。</p><p id="004b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">精确的 SHAP 值</strong></p><p id="01a7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一个示例中，我们计算具有特征相关性的线性模型的 SHAP 值。所以，我们不能用等式 31。相反，我们使用等式 11，并使用所有可能的联合来计算 SHAP 值。这里我们使用<code class="fe lx ly lz ma b">scikit-learn</code>图书馆中的波士顿数据集:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="1da5" class="qo qp it ma b gy qq qr l qs qt"># Listing 4</span><span id="f1fa" class="qo qp it ma b gy qu qr l qs qt">d = load_boston()<br/>df = pd.DataFrame(d['data'], columns=d['feature_names'])<br/>y = pd.Series(d['target'])<br/>X = df[['LSTAT', 'AGE', 'RAD', 'NOX']]<br/>X100 = X[100:200]</span><span id="8b13" class="qo qp it ma b gy qu qr l qs qt">linear_model2 = LinearRegression()<br/>linear_model2.fit(X, y)</span></pre><p id="5087" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">波士顿数据集有 13 个特征，但我们只选择其中的 4 个(<code class="fe lx ly lz ma b">LSTAT</code>、<code class="fe lx ly lz ma b">AGE</code>、<code class="fe lx ly lz ma b">RAD</code>、<code class="fe lx ly lz ma b">NOX</code>)。我们还对该数据集的 100 行进行采样，以估计等式 29 中的<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)(因此<em class="lv"> k </em> =100)。我们将这些行存储在<code class="fe lx ly lz ma b">X100</code>中。然后我们用这个数据集来训练一个线性模型。现在我们需要定义一些函数来计算 Python 中的 SAHP 值:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="f3f5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">coalition_worth()</code>用于计算联盟的价值。它需要一个模型、一个训练数据集的样本(<code class="fe lx ly lz ma b">X_train</code>)、一个数据实例(<code class="fe lx ly lz ma b">x</code>)和一组联盟(<code class="fe lx ly lz ma b">coalition</code>)。这里，在等式 11 中，coalition 代表<strong class="lb iu"> <em class="lv"> S </em> </strong>。这个函数用联合集中给定的值替换<code class="fe lx ly lz ma b">X_train</code>的列，然后它使用模型来预测所有这些行的目标。最后，取所有这些预测的平均值，并将其作为<em class="lv">f</em>t35】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)的估计值返回。</p><p id="d75f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">coalitions()</code>返回一个数据实例的所有联合的集合，不包括特征<code class="fe lx ly lz ma b">col</code>。所以，它计算等式 11 中<strong class="lb iu"><em class="lv">F</em></strong>-{<em class="lv">I</em>}的所有联盟，其中 col 表示特征<em class="lv"> i </em>。</p><p id="aacb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">coalition_contribution()</code>计算等式 11 中每个联盟的贡献(等式 11 中求和的每一项)。这里我们用了这样一个事实:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/1c2d0e00fd50d296e8068f736e7a63fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*GyjEMisrK4cUAQDeMhloUA@2x.png"/></div></figure><p id="fe49" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此<code class="fe lx ly lz ma b">scipy</code>中的函数<code class="fe lx ly lz ma b">comb()</code>被用于计算二项式系数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ra"><img src="../Images/2166568d8e6be9b22439d3967c8f2197.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*etxhuKeG7OipAMIqo2N4UQ@2x.png"/></div></figure><p id="018e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，函数<code class="fe lx ly lz ma b">calculate_exact_shap_values()</code>获取待解释的特征向量(<code class="fe lx ly lz ma b">X_explain</code>)并计算其中每个特征向量的 SHAP 值。它将每个联盟的贡献相加，以计算特征向量中每个特征的 SHAP 值。现在，我们可以使用该函数，通过数据集行的样本来计算 Boston 数据集第一行的 SHAP 值(<code class="fe lx ly lz ma b">X100</code>):</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="b177" class="qo qp it ma b gy qq qr l qs qt"># Listing 6</span><span id="89ad" class="qo qp it ma b gy qu qr l qs qt">calculate_exact_shap_values(linear_model2, X100, X.iloc[0])</span></pre><p id="4bed" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="b655" class="qo qp it ma b gy qq qr l qs qt">(22.998930866827823,<br/> [[7.809214247585507,<br/>   -0.7308440229196315,<br/>   0.1290501127229501,<br/>   0.23758951510828266]])</span></pre><p id="42a9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用 SHAP 库计算 SHAP 值</strong></p><p id="7840" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">shap</code>库中的类<code class="fe lx ly lz ma b">Explainer()</code>接受模型预测函数(不仅仅是模型)和训练数据集，方法<code class="fe lx ly lz ma b">shap_values()</code>返回 SHAP 值。如果我们不传递特定算法的名称，它会根据给定的模型和训练数据集，尝试找到计算 SHAP 值的最佳算法。这里我们将它用于清单 4 中定义的同一个模型。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="96f5" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.Explainer(linear_model2, X100)<br/>shap_values = explainer.shap_values(X.iloc[0:1])<br/>shap_values</span><span id="e17f" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([[ 7.80921425, -0.73084402,  0.12905011,  0.23758952]])</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="0624" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">explainer.expected_value</span><span id="cbad" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>22.998930866827834</span></pre><p id="84d2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，数组 shap_values 给出了<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ_M </em>的值。<em class="lv"> ϕ </em> ₀的值存储在 Explainer 的 expected_value 字段中。这里我们得到了与清单 6 中的<code class="fe lx ly lz ma b">calculate_shap_values()</code>几乎相同的 SHAP 值。请务必注意，Explainer 类会自动对 100 行训练数据进行采样(如果行数大于 100)，并使用这些数据来计算 SHAP 值。因此，如果我们使用超过 100 行的训练数据集，<code class="fe lx ly lz ma b">Explainer</code>的输出将不再匹配<code class="fe lx ly lz ma b">calculate_exact_shap_values()</code>的输出:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="f0d3" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.Explainer(linear_model2, X[:150])<br/>shap_values = explainer.shap_values(X.iloc[0:1])<br/>shap_values</span><span id="f213" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([[ 8.88370884e+00, -2.97655621e-01,  1.17561972e-01,<br/>        -1.48202335e-03]])</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="2a8b" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">calculate_exact_shap_values(linear_model2, X[:150], X.iloc[0])</span><span id="4cf0" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>(22.521908424669917,<br/> [[7.993180897252836,<br/>   -0.11946396867250808,<br/>   0.11973195423751992,<br/>   -0.07141658816282939]])</span></pre><p id="f532" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">内核 SHAP </strong></p><p id="5b6e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用公式 11 或公式 18 计算模型的 SHAP 值，我们需要计算所有可能的联合。如前所述，对于<em class="lv"> M </em>特征，可能联盟的总数是 2<em class="lv">T17】。当然，在等式 25 中我们计算了<strong class="lb iu"><em class="lv">F</em></strong>-{<em class="lv">I</em>}的所有联盟。所以，我们需要计算每个 SHAP 值的实际联盟数是 2 <em class="lv"> ᴹ </em> ⁻，对于<em class="lv"> M </em>特征的时间复杂度是</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rg"><img src="../Images/c91cf9f0d6d2091ab83344c5ca02d993.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*dhishFDC1E229BrQ_q94Jw@2x.png"/></div></div></figure><p id="9000" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个联盟，我们需要估计<em class="lv">f</em>t30】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)和<em class="lv">f _</em><strong class="lb iu">t39】s</strong>∩{<em class="lv">I</em>}(<strong class="lb iu"><em class="lv">x _ s</em></strong>∩{<em class="lv">I</em>})<em class="lv"/>使用所以该算法的时间复杂度为<em class="lv">o</em>(<em class="lv">km</em>2<em class="lv">ᴹ</em>)。</p><p id="bf2c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这一结果，当<em class="lv"> M </em>增加时，可能联盟的数量呈指数增加，并且当我们具有多个特征时，使用这些方程来寻找 SHAP 值在计算上变得难以处理。核 SHAP 是一种近似方法，可以用来克服这个问题。这种方法首先是由 Lundberg 和 Lee [1]提出的。</p><p id="0451" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">石灰</strong></p><p id="4991" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解内核 SHAP，我们应该首先熟悉另一种称为 LIME(本地可解释模型不可知解释)的模型不可知解释方法。LIME 是在 SHAP 之前开发的，它的目标是为一个分类器确定一个可解释的模型，这个模型在本地是忠实的。假设您有一个模型<em class="lv"> f </em> ( <strong class="lb iu"> <em class="lv"> x </em> </strong>)，并且您想要为其找到最佳可解释模型<em class="lv"> g </em> ( <strong class="lb iu"> <em class="lv"> z' </em> </strong>)(记住<em class="lv"> g </em>是联合向量<strong class="lb iu"> <em class="lv"> z </em> </strong>')。设<strong class="lb iu"> <em class="lv"> x </em> </strong>为待解释的特征向量，<strong class="lb iu"> <em class="lv"> x </em> </strong>为其联盟向量。</p><p id="9846" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要在<strong class="lb iu"> x </strong>附近找到一些随机联盟向量。例如，我们可以选择<strong class="lb iu"> <em class="lv"> x </em> </strong>的一些非零分量，以 0.5 的概率将它们从 1 变为 0，以产生联盟向量<strong class="lb iu"><em class="lv">z</em></strong>’。结果，我们在<strong class="lb iu"><em class="lv">x</em></strong>’附近得到了一些联合向量。我们一般称这些联合向量中的每一个为<strong class="lb iu"><em class="lv">Z</em></strong>’，我们称所有这些向量的集合为<strong class="lb iu"> <em class="lv"> Z </em> </strong>。<strong class="lb iu"> <em class="lv"> </em> </strong>我们可以假设<strong class="lb iu"> <em class="lv"> x </em> </strong>'和<strong class="lb iu"> <em class="lv"> z </em> </strong>'是一个<em class="lv">M</em>-维空间中的一些点(图 7)。我们可以使用映射函数<em class="lv"> h </em> <strong class="lb iu"> <em class="lv"> ₓ </em> </strong>找到特征空间中每个<strong class="lb iu"><em class="lv">z</em></strong>’对应的向量(图 7):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rh"><img src="../Images/22bd3a1e2d409260fe249afe458239a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*QsDhz2Yxs6TKkNXukeVq2g@2x.png"/></div></figure><p id="620e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要对<strong class="lb iu"><em class="lv">x</em></strong>’和每个<strong class="lb iu"><em class="lv">z</em></strong>’之间的距离有一个量化的度量。于是，我们将函数π<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)定义为<strong class="lb iu"><em class="lv">x</em></strong>’与<strong class="lb iu"><em class="lv">【z</em></strong>’之间距离的度量。这个函数将联合向量<strong class="lb iu"><em class="lv">z</em></strong>’映射到一个非负实数({0，1} <em class="lv"> ᴹ </em> → R≥ 0)。π<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’)应该增加为<strong class="lb iu"><em class="lv">z</em></strong>‘越来越接近<strong class="lb iu"><em class="lv">x</em></strong>’。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ri"><img src="../Images/e14dca9b5203d7eb74316d1d65670081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7mShL9IZA6VD6l4i8fSIg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 7(来源:作者图片)</p></figure><p id="6ed0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在假设我们有一组可解释的讲解者模型<strong class="lb iu"> <em class="lv"> G </em> </strong>，我们想为<em class="lv">f(</em><strong class="lb iu"><em class="lv">x</em></strong><em class="lv">)</em>其中(<em class="lv">G</em>∈<strong class="lb iu"><em class="lv">G</em></strong>)找到最准确的讲解者模型<em class="lv"> g </em>。根据等式 16，我们需要:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rj"><img src="../Images/7142758287eabb16d719f516d0723e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58oTUjiaUuDGBwYrzvzQbQ@2x.png"/></div></div></figure><p id="7114" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以定义一个损失函数<em class="lv"> L </em> ( <em class="lv"> f，g，</em>π<strong class="lb iu"><em class="lv"/></strong>)<em class="lv"/>，它与<em class="lv">f</em>(<strong class="lb iu"><em class="lv">z</em></strong>)和<em class="lv">g</em>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">’</em>)之间的距离成正比我们希望<em class="lv">g</em>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">’</em>)非常接近<em class="lv">f</em>(<strong class="lb iu"><em class="lv">z</em></strong>)当<strong class="lb iu"><em class="lv">z</em></strong>’非常接近<strong class="lb iu"><em class="lv">x</em></strong>’。但是<strong class="lb iu"> <em class="lv"> Z </em> </strong>中的一些点可能与<strong class="lb iu"><em class="lv">×的</em></strong>’不是很接近，所以我们需要为它们增加一个惩罚项。由于π<strong class="lb iu"><em class="lv"/></strong>是<strong class="lb iu"><em class="lv">【x</em></strong>’和<strong class="lb iu"><em class="lv">【z</em></strong>’之间距离的<strong class="lb iu"> </strong>度量，我们可以将其作为参数添加到损失函数中。例如，我们可以将损失函数定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nz"><img src="../Images/4769b7a83fa0c691b60351788c7e47a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iAVoZrQAmqZDXa-CfGezMQ@2x.png"/></div></div></figure><p id="7c69" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过将π<strong class="lb iu"><em class="lv"/></strong><strong class="lb iu"><em class="lv"/></strong>添加到损失函数中，我们为<strong class="lb iu"><em class="lv"/></strong>中远离<strong class="lb iu"><em class="lv"/></strong>的点添加更高的惩罚，并且更近的点对于最小化变得更重要。因此，损失函数决定了解释函数<em class="lv"> g </em>在非常靠近<strong class="lb iu"> <em class="lv"> x </em> </strong>的点<strong class="lb iu"> <em class="lv"> z </em> </strong>【处】近似<em class="lv"> f </em>的程度。现在我们需要在<strong class="lb iu"><em class="lv">【G</em></strong>(我们想要尝试的所有可解释函数的集合)中找到函数<em class="lv"> g </em>，最小化这个损失函数。</p><p id="a5e2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也喜欢更简单和更易解释的函数，所以我们让<em class="lv">ω(g)</em>作为解释函数<em class="lv"> g </em>的复杂性(相对于可解释性)的度量。比如对于线性模型，<em class="lv">ω(g)</em>可以定义为非零权值的个数，或者对于决策树，可以定义为树的深度。因此<em class="lv">ω(G</em>)随着<em class="lv"> g </em>变得更简单从而更容易理解而减小，我们应该在<strong class="lb iu"> <em class="lv"> G </em> </strong>中寻找一个函数<em class="lv"> g </em>来最小化下面的目标函数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rk"><img src="../Images/a70e8b96fa8b9783683c685596dedc36.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*6WcaEYDQ001M0b359areVg@2x.png"/></div></figure><p id="274d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这相当于解决这个最小化问题:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rl"><img src="../Images/53f2fc3226dfd9fddfd117481c17c460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*85g37e59dHaDPnq0o4jB0g@2x.png"/></div></figure><p id="3aeb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以用下面的定理来定义核 SHAP 方法:</p><p id="efa8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">定理 1 </strong>。假设我们有一个具有<em class="lv"> M </em>个特征的特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>和一个以<strong class="lb iu"> <em class="lv"> x </em> </strong>为输入的模型<em class="lv"> f </em>。设<em class="lv"> g </em>为线性模型，定义如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/d9a03a38bb8dce8b42689801ece65505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*X8P3R_f8BetxpDHrkmXXOQ@2x.png"/></div></figure><p id="0d49" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> z </em> ' <em class="lv"> ᵢ </em>是<strong class="lb iu">z</strong>的第<em class="lv"> i </em>个元素，是<strong class="lb iu"> <em class="lv"> x </em> </strong>的联合向量；<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)](模型对训练数据集的一个样本的实例的平均预测)和<em class="lv"> ϕᵢ </em>(对于<em class="lv"> i </em> &gt; 0)是<em class="lv"> f </em>和<em class="lv"/><strong class="lb iu">的 Shapley 值当<em class="lv"> M </em>趋于无穷大时，方程 32 的解(函数<em class="lv"> g </em>)接近方程 33 给出的函数，如果<em class="lv"> L </em>、<em class="lv">ω</em>、π <strong class="lb iu">、<em class="lv"> ₓ </em> </strong>定义为:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rn"><img src="../Images/2e0bf11b7b2fa7631018b05ae0b4338b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24u3Skuj4IQzkLDNIIFEJw@2x.png"/></div></div></figure><p id="af45" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中| <strong class="lb iu"> <em class="lv"> z </em> </strong> '|是<strong class="lb iu"> <em class="lv"> z </em> </strong>'中非零项的个数。</p><p id="e28d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个定理的证明在附录中给出。有趣的是，在介绍 SHAP [2]的原始论文中，这个定理没有被正确证明(更多细节请参考附录)。让我们看看如何在实践中使用这种方法。我们假设我们有一个具有<em class="lv"> M </em>特征的特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>。我们计算简化的特征向量<strong class="lb iu"><em class="lv">x</em></strong>’。这里我们为了简单起见假设<strong class="lb iu"> <em class="lv"> x </em> </strong>中没有<em class="lv"> NA </em> s(在定理 1 中，<strong class="lb iu"> <em class="lv"> x </em> </strong>可以有<em class="lv"> NA </em>值)，那么<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有元素都是 1。然后我们计算<strong class="lb iu"><em class="lv">x</em></strong>’(它们也被称为<strong class="lb iu"><em class="lv">x</em></strong>’)的所有可能的联合向量。<strong class="lb iu"><em class="lv">x</em></strong>’的每个联合向量称为<strong class="lb iu"><em class="lv">z’</em></strong><em class="lv">ᵢ</em>，是一个有<em class="lv"> M </em>个元素的向量，其中每个元素可以是 0 也可以是 1。我们有 2 个<em class="lv"> ᴹ </em>联盟向量，我们将所有这些联盟向量放入 2 个<em class="lv"> ᴹ× M </em>矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/98c90dba69191c76c11bad148988e2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*5wkxYd-h_Y_2Oo11dT-wrw@2x.png"/></div></figure><p id="afc4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个矩阵叫做<em class="lv">联盟矩阵</em>。对于每个联盟向量<strong class="lb iu"><em class="lv">【z’</em></strong><em class="lv">ᵢ</em>，我们可以计算出模型预测<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong><em class="lv">(</em><strong class="lb iu"><em class="lv">z</em></strong><em class="lv">【'ᵢ】</em>。我们将列向量<strong class="lb iu"> <em class="lv"> y </em> </strong>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ro"><img src="../Images/67a88d564a9b956e7d5ea9e89563753f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_U_7tNiPllDG5GzyXp_RA@2x.png"/></div></div></figure><p id="6799" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"/><em class="lv">【ϕ】</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)](训练数据集样本中数据实例的预测值的平均值)。</p><p id="baee" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将 2 <em class="lv"> ᴹ× </em> 2 <em class="lv"> ᴹ </em>对角矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rp"><img src="../Images/ba9180df9dd2740a1424090e30288475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Mf7kDIs2YoEHeD_NLZcDA@2x.png"/></div></div></figure><p id="7b0a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rq"><img src="../Images/0030977cb07ebd71cb2a0e601eae3e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wLeYXsDeekW8ZaRGbDD0FQ@2x.png"/></div></div></figure><p id="315c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设特征向量为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rr"><img src="../Images/819c119deb93343d03096ced808742ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*Cw1jqcjlQRGGTovMteeFqA@2x.png"/></div></figure><p id="0361" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rs"><img src="../Images/328f4daa7b70b9a873b61ae8e7ac67f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*xDXksFOyn1ZtexlK5UtIng@2x.png"/></div></figure><p id="43bb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且联合矩阵将具有 2 =8 行:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/4fd34e9018228970caa410d94177483d.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*m1dfLnGvPHUf0t3p98uwCw@2x.png"/></div></figure><p id="3ddf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ru"><img src="../Images/1219d052ae29be55f81d905bb6859653.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*7zzmBhhBW-cytF7kivffrg@2x.png"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rv"><img src="../Images/acf2fd0531b849cb0649a28e527ed8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVq0VxQ8AEVkLPT1S2aqyQ@2x.png"/></div></div></figure><p id="1305" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据等式 32 和等式 34，我们知道需要解决这个最小化问题，以获得 Shapley 值的估计值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rw"><img src="../Images/ce574418baac1829f34f9977945e2f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArNic5XI9XgEXPND9oI-Og@2x.png"/></div></div></figure><p id="ef92" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将对最优函数<em class="lv"> g </em>的搜索限制为具有以下形式的线性函数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rx"><img src="../Images/1fd441043769e092c85865451968d957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jtNYKfTPSXIIhP1lBUWLA@2x.png"/></div></div></figure><p id="16d7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">列向量<strong class="lb iu"> <em class="lv"> c </em> </strong>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ry"><img src="../Images/61cd0571aa78685b4d5d0b2a0c290fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*pv5Q7SuJiN6VkmQUAL1fbA@2x.png"/></div></figure><p id="96ba" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在可以证明(详情见附录):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rz"><img src="../Images/2bb48597636616dfcc60015f350c6c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idoA6MBnEe-5uT0gK8xjtQ@2x.png"/></div></div></figure><p id="6468" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，最小化问题等价于</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3ca4390b59056d7c67099d8173c51282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*66yihcSDX4UP0715EBzwOg@2x.png"/></div></figure><p id="0861" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> g </em>只是<em class="lv"> cᵢ </em>的一个函数，所以我们不去找最小化目标函数的函数<em class="lv"> g </em>，而是去找最小化它的向量<strong class="lb iu"> <em class="lv"> c </em> </strong>的值。函数π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’<em class="lv">ᵢ</em>)也被称为<em class="lv">沙普利核权重</em>。基于(等式 40)，π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)的每个值就像一个重量为(<em class="lv">g</em>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)-<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z【z】 这也是联盟矢量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>的一个砝码，可见联盟是多么的重要。</em></strong></p><p id="bdf4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如附录所示，这个最小化问题的解决方案是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sa"><img src="../Images/99059601552625e621d37b90eeabb756.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Ehs6plXi6doFENTR2OhQiA@2x.png"/></div></figure><p id="4f39" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据定理 1，我们知道这个解是 Shapley 值的近似值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sb"><img src="../Images/1939fd7a3e19bf77152c7f814d60061e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ozEB1lJvSqULM7lfK6jHg@2x.png"/></div></div></figure><p id="af16" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)]，我们有所有的沙普利值。等式 41 中的<strong class="lb iu"> <em class="lv"> R </em> </strong>项并不依赖于一个具体的数据实例来解释，所以如果要解释多个数据实例，只需要计算一次<strong class="lb iu"> <em class="lv"> R </em> </strong>。然后，对于每个数据实例，计算新的值<strong class="lb iu"> <em class="lv"> y </em> </strong>并乘以<strong class="lb iu"> <em class="lv"> R </em> </strong>以获得 SHAP 值。</p><p id="f181" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是<strong class="lb iu"> <em class="lv"> X </em> </strong>中联盟向量的顺序并不重要。例如，在等式 39 中，联合向量[0 0 0]是第一行<strong class="lb iu"><em class="lv">×第一行</em> </strong>，然而，它可以是最后一行，等式 41 仍然有效(如果你看附录中定理 1 的证明，我们不对联合向量的顺序作任何假设<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>作为矩阵<strong class="lb iu"><em class="lv">×的行</em>它只需要拥有所有 2 个ᴹ联盟向量。</strong></p><p id="c227" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Python 中的内核 SHAP</strong></p><p id="bd8c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>具有一些不可用的特征，定理 1 也是有效的，但是在实践中，我们假设<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有特征都可用来实现 SHAP。所以，<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有元素都是一，并且|<strong class="lb iu"><em class="lv">x</em></strong>' | =<em class="lv">M</em>。现在让我们看看如何用 Python 实现内核 SHAP。为此，我们首先需要一个 Python 函数来计算π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="9497" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">pi_x()</code>获取一个联合向量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>(作为一个列表)，并基于等式 38 返回π<strong class="lb iu"><em class="lv">【ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)的值。函数<code class="fe lx ly lz ma b">generate_colaition_vectors()</code>获取特征的数量(<code class="fe lx ly lz ma b">num_features</code>，并为它们生成所有可能的联合向量。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="4235" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="fbd4" class="qo qp it ma b gy qq qr l qs qt">generate_coalition_vectors(num_features)</span></pre><p id="2862" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="2677" class="qo qp it ma b gy qq qr l qs qt">[[0.0, 0.0, 0.0],<br/> [1.0, 0.0, 0.0],<br/> [0.0, 1.0, 0.0],<br/> [0.0, 0.0, 1.0],<br/> [1.0, 1.0, 0.0],<br/> [1.0, 0.0, 1.0],<br/> [0.0, 1.0, 1.0],<br/> [1.0, 1.0, 1.0]]</span></pre><p id="2491" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要一个函数来生成等式 37 中矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong>的对角元素。这里我们有一个问题。通过看方程 38 我们看到如果|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>| = 0(当<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>都为零)和|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>| =<em class="lv">m</em>(当的所有元素请记住，我们希望最小化以下目标函数(等式 40):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sc"><img src="../Images/d377fc0f6f1316d98c43e532fce13d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bfWP8Pso6MFwHLaqHUIAMQ@2x.png"/></div></div></figure><p id="5151" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设第一个<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">’</em>₁是所有元素都为零的联盟:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sd"><img src="../Images/8583418dcac6a3c7c334262bfb7c0733.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*EkibwWwgVBGu1PXHtPD49g@2x.png"/></div></figure><p id="aa55" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让最后的联盟是所有元素是一个联盟:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi se"><img src="../Images/81dfbf6d52e69528a8d275e8cfb31606.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*uvayGkugzofGNbeUoRokNQ@2x.png"/></div></figure><p id="a38d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道，当我们没有可用的特征时，模型预测是训练数据集的样本中的实例的预测的平均值(等式 7)。所以，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sf"><img src="../Images/ff8c4453337d44d1566ea36337ab0b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JkldjA1-OEBqpeJUuXHCrQ@2x.png"/></div></div></figure><p id="c517" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还知道，当所有功能都可用时:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sg"><img src="../Images/717d3f21e7483f2f37de0abf0a8a3861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBjPVvulZpTfGI6pR3_sVQ@2x.png"/></div></div></figure><p id="98ff" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，对于最后一个联盟，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rz"><img src="../Images/1c8b3f03f6360cd9c4cd9a092cb334c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N6wc0QBfEJwhjQY003MBOQ@2x.png"/></div></div></figure><p id="c178" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在在方程 42 中，π<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong>【'₁】是这一项的权重:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sh"><img src="../Images/7ae385a102a6cb2f9316fcdbe5ca4987.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*BQnWSOlcGxXl6ueB32Jnfw@2x.png"/></div></figure><p id="35f7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'₁)是无穷大，我们需要这个项为零，这意味着我们应该有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi si"><img src="../Images/e7df5303d6714ec2013168838d182020.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*FagwFDBHeAx1ymjclOBfoA@2x.png"/></div></figure><p id="8e99" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是基于 Shapley 值的线性模型的属性(等式 22)。π<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’_ 2<em class="lv"/>)的无穷大值意味着我们应该有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/6e1c66ea1fddecb2628644a4462c29f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*bf5oNtNGZaHTj-Eby5tPOw@2x.png"/></div></figure><p id="001d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还知道，随着<em class="lv"> M </em>趋于无穷大，最小化目标函数的<em class="lv"> cⱼ </em>的值越来越接近于<em class="lv"> f </em>和<em class="lv"> </em> <strong class="lb iu"> <em class="lv"> x </em> </strong>的 Shapley 值。因此，上面的等式只是显示了基于 Shapley 值的线性模型的属性(等式 22)。现在我们来画π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)为一个特例，其中<em class="lv"> M </em> =13。为此，我们可以使用清单 9:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="427a" class="qo qp it ma b gy qq qr l qs qt"># Listing 9</span><span id="8cf0" class="qo qp it ma b gy qu qr l qs qt">pi_values = [1e7]<br/>for i in range(1, 14):<br/>    try:<br/>        pi_values.append(pi_x(13, i))<br/>    except:<br/>        pi_values.append(1e7)<br/>plt.plot(pi_values, marker=’o’)<br/>plt.xlabel(“$|\mathregular{z}’_i|$”, fontsize=14, weight=”bold”, <br/>   style=”italic”,)<br/>plt.ylabel(“$\pi_\mathregular{x}(\mathregular{z}’_i)$”, fontsize=14, <br/>   weight=”bold”, style=”italic”,)<br/>plt.ylim([0, 0.1])<br/>plt.show()</span></pre><p id="baf5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如图 8 所示。我们对π<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'₁)和π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>)都用了一个大常数(<code class="fe lx ly lz ma b">1e7</code>而不是无穷大，以便能够把它们画出来。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sj"><img src="../Images/d8b6343aa084e1f578889ae2c35a99fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jUgJpf6LJ7zVBwpd9cZZw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 8(来源:图片由作者提供)</p></figure><p id="125e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">剧情是对称的，随着|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>|越来越接近 0 或者<em class="lv"> M </em>、π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)增加。π的每一个值<strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)就像是一个重量对于联军向量<strong class="lb iu"> <em class="lv"> z </em> </strong> <em class="lv"> 'ᵢ.</em>如前所述，我们用等式 30 来估算<em class="lv">f</em><strong class="lb iu"><em class="lv"/></strong>(<strong class="lb iu"><em class="lv"/></strong>’)。对于<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁我们知道</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e60c07b13b7cbd107921e336c0391b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*3cdFP0yfBZxy8ylpFpZfPg@2x.png"/></div></figure><p id="e39d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而对于<strong class="lb iu"><em class="lv"/></strong>’_ 2<em class="lv"/>我们知道</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sk"><img src="../Images/e8ae85cd71bf936b6582c0f9a6b6eee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*TmnI7eNlbHdvfrocPBQJ2A@2x.png"/></div></figure><p id="6dfa" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个联合向量是最重要的，因为我们可以为它们精确计算出<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’<em class="lv">ᵢ</em>)的值。因此它们有无限的重量。但是对于其他联盟(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)我们只能估计<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>)。当联军<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>越来越接近<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁或者<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>、π <strong class="lb iu"> <em class="lv"> ₓ </em> </strong>赋予它高得多的权重。</p><p id="8f18" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">generate_pi_values()</code>生成<strong class="lb iu"> <em class="lv"> W </em> </strong>的对角元素。对于每个联盟矢量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>，这个函数使用<code class="fe lx ly lz ma b">pi_x()</code>计算π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)。我们不能用<code class="fe lx ly lz ma b">pi_x()</code>来表示<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">m</em>，因为它以被零除异常结束。相反，我们把π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)换成大常数<code class="fe lx ly lz ma b">1e7</code>用于其中的联盟|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv"/>| = 0 和|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>| =<em class="lv"/></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="3cb2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们需要一个 Python 函数为每个联盟向量计算<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>)。为此，我们可以使用等式 30。假设我们要说明的特征向量是<strong class="lb iu"> <em class="lv"> x </em> </strong>。我们把<strong class="lb iu"> <em class="lv"> x </em> </strong>中的所有特征都聚集到一个集合中，这个集合叫做<strong class="lb iu"><em class="lv"/></strong>’<em class="lv">【ᵢ】</em>xₛ。然后，对于训练数据集样本中的每个数据实例，我们将出现在<strong class="lb iu"><em class="lv"/></strong>中的特征值替换为它们在<strong class="lb iu"> <em class="lv"> x </em> ₛ </strong>中的对应值，并对该数据实例进行预测:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/900f5df14144aa66bea48a15f4b9fb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*nphPlEiEy8jfIeeBlxI05Q@2x.png"/></div></figure><p id="49b5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将训练集的所有实例的这些预测的平均值作为对<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>’<em class="lv">ᵢ</em>)的近似:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sl"><img src="../Images/0b4a7c27970202c026f9e9bfbc97868a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGbqNqXGlkYuyWAjNL1_Jg@2x.png"/></div></div></figure><p id="258c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python 函数<code class="fe lx ly lz ma b">f_x_z_prime()</code>使用这种方法为每个联盟向量计算<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>):</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="54fe" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">kernel_shap()</code>采用模型预测器、训练数据集、其权重数组和将由核 SHAP ( <code class="fe lx ly lz ma b">X_explain</code>)解释的特征向量。它生成联合向量，<em class="lv">E</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)]，以及矩阵的对角元素<strong class="lb iu"><em class="lv">【W</em></strong>。然后调用函数<code class="fe lx ly lz ma b">calculate_shap_values()</code>计算<code class="fe lx ly lz ma b">X_explain</code>中每个特征向量的 SHAP 值。对于要解释的每个特征向量，该函数使用等式 41 计算 SHAP 值。形成矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>和<strong class="lb iu"> <em class="lv"> W </em> </strong>和列向量<strong class="lb iu"> <em class="lv"> y </em> </strong>。然后用等式 41 计算出<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ_M </em>。它返回一个元组，其中第一个元素是<em class="lv"> ϕ </em> ₀，第二个元素是<code class="fe lx ly lz ma b">X_explain</code>中每个特征向量的 SHAP 值(<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ_M </em>)的数组。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="83d7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们在一个数据集上尝试<code class="fe lx ly lz ma b">kernel_shap()</code>。我们再次使用波士顿数据集，并且我们包括所有特征(13 个特征)。然后我们在那上面训练一个随机森林回归器。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="6f51" class="qo qp it ma b gy qq qr l qs qt"># Listing 13</span><span id="0c71" class="qo qp it ma b gy qu qr l qs qt">d = load_boston()<br/>df = pd.DataFrame(d['data'], columns=d['feature_names'])<br/>X = df[['AGE', 'RAD', 'TAX', 'DIS', 'RM', 'LSTAT', 'B', 'INDUS', <br/>   'CHAS']]<br/>y = pd.Series(d['target'])</span><span id="f20f" class="qo qp it ma b gy qu qr l qs qt">rf_model = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10).fit(X, y)</span></pre><p id="75bf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以在这个数据集上尝试<code class="fe lx ly lz ma b">kernel_shap()</code>。我们使用<code class="fe lx ly lz ma b">X</code>数据集的前 420 行作为训练数据集，然后尝试解释第 470 行。在这个例子中，训练数据集的所有元素(<code class="fe lx ly lz ma b">X_train</code>)具有相同的权重。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="ac6c" class="qo qp it ma b gy qq qr l qs qt"># Listing 14</span><span id="27dd" class="qo qp it ma b gy qu qr l qs qt">X_train = X.iloc[:100].copy()<br/>data_to_explain = X.iloc[470:471].copy()<br/>weights = np.ones(len(X_train)) / len(X_train)<br/>shap_values = kernel_shap(rf_model.predict, X_train.values, weights, <br/>   data_to_explain)<br/>shap_values</span></pre><p id="ca7a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="79fb" class="qo qp it ma b gy qq qr l qs qt">(22.74244968353333,<br/> array([[ 4.05579739e-02, -4.91062082e-02, -4.69741706e-01,<br/>          9.28299842e-02, -8.88366342e-01, -2.86693055e+00,<br/>          2.19117329e-01, -3.57934578e-02,  7.10305024e-09]]))</span></pre><p id="c044" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是一个元组。这个元组的第一个元素给出了<em class="lv"> ϕ </em> ₀ (22.742)，第二个元素是一个数组，分别给出了<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ </em> ₁₃的值。</p><p id="7874" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，训练数据集中的样本数量对内核 SHAP 的运行时间有很大的影响。为了计算等式 36 中的<strong class="lb iu"> <em class="lv"> y </em> </strong>，我们需要计算<em class="lv">I</em>= 2…2<em class="lv">ᴹ</em>的<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">【ᵢ</em>)并计算<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu">因此，大型训练集可能会降低计算速度，为此，我们仅使用训练集的一小部分样本。我们可以从训练数据集中随机抽取 k 个数据实例，或者使用聚类算法从训练数据集中抽取样本。例如，我们可以使用<em class="lv"> k </em> -means 在训练数据集中查找<em class="lv"> k </em>个聚类。每个聚类的权重与其中数据点的数量成比例。对于每个聚类，我们计算其均值和权重，并用一组加权均值来总结训练数据集。</strong></p><p id="bfc6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">核 SHAP 方程的替代形式</strong></p><p id="20eb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用一个技巧从目标函数中移除<strong class="lb iu"> <em class="lv"> z </em> </strong> '1 和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>。记住，目标函数中对应于<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>’_ 2<em class="lv">ᴹ</em>的项与等式 22 中描述的 Shapley 值的性质相关:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sm"><img src="../Images/27909cbc7167c74e37f8a40e201500e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgykgUADC-9rhoNBZzDJig@2x.png"/></div></div></figure><p id="dc4c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以去掉目标函数中对应于<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>的项，并加入上述方程作为单独的约束。因此，等式 40 中的目标函数变为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sn"><img src="../Images/7b57234890bff2e0dfed876ef2f5d06a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBFnki7-kv2owAeQynFHmw@2x.png"/></div></div></figure><p id="4853" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑等式 35 中定义的联盟矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>并且让<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>分别是全零和全一联盟。设<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ₜ </em>为 a (2 <em class="lv"> ᴹ </em> -2) <em class="lv"> ×M </em>矩阵，该矩阵是通过从联合矩阵<strong class="lb iu"><em class="lv"/></strong>z<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>x</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi so"><img src="../Images/60a138a46a90fd5c0e6be78d601b052d.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*W4svacmDOtxR7093SKeTGQ@2x.png"/></div></figure><p id="514c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并设<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ᵣ </em>为一个(2<em class="lv">ᴹ</em>-2)<em class="lv">×</em>(<em class="lv">m</em>-1)矩阵定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sp"><img src="../Images/5712465d80f3c1e599cef864aa5702a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zxsM2BG8VUBJPzBZTUAYlA@2x.png"/></div></div></figure><p id="48a3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<em class="lv"> *、i </em>是指<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ₜ </em>的第<em class="lv"> i </em>列。于是<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ᵣ </em>由第<em class="lv"> M </em> -1 列减去最后一列<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ₜ </em>而成。我们还将列向量<strong class="lb iu"> <em class="lv"> c </em> </strong> <em class="lv"> ᵣ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sq"><img src="../Images/442e2d4c7d261f74ed7384edb36397a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*ilBNT7zTmQCdtkNF6CdbqA@2x.png"/></div></figure><p id="4143" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将列向量<strong class="lb iu"> <em class="lv"> y </em> </strong> <em class="lv"> ᵣ </em>与 2 个<em class="lv"> ᴹ </em> -2 元素定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sr"><img src="../Images/7ba3d551632707e547375f05305ac5e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g77hBO_MPUhjjVDkna07eg@2x.png"/></div></div></figure><p id="a856" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> x </em> </strong>是应该说明的特征向量。最后，我们将(2<em class="lv">ᴹ</em>-2)<em class="lv">×</em>(2<em class="lv">ᴹ</em>-2)对角矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong> <em class="lv"> ᵣ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ss"><img src="../Images/52a83e8e95a0d43f598125c0b1074640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_G1mpetdB0TSun2v3TfqyQ@2x.png"/></div></div></figure><p id="9e2c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意<strong class="lb iu"> <em class="lv"> W </em> </strong> <em class="lv"> ᵣ </em>现在已经没有无限对角元素了。可以看出(细节在附录中给出),等式 45 中的目标函数可以写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rn"><img src="../Images/da8e61e94af8aff30318b87a96868b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbMUrSFH92xPoY0KiltN0g@2x.png"/></div></div></figure><p id="bdd3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们需要解决这个最小化问题:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi st"><img src="../Images/aa4625da86e7d7d44562a8c61d0da8ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZI8agGXKzYC95TNSoHzf6A@2x.png"/></div></figure><p id="d047" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而使这个目标函数最小化的<strong class="lb iu"> <em class="lv"> c </em> </strong> <em class="lv"> ᵣ </em>的值是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi su"><img src="../Images/6469efb32fbdc54daaa46fefa47584ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*A0V1Y-0GDQxw02g66z7WBg@2x.png"/></div></figure><p id="6798" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，基于定理 1，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ny"><img src="../Images/4e989ee731e52c1a01c97917d421a3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2Mqox0tZvSj4VeHWXnrEg@2x.png"/></div></div></figure><p id="536a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道这一点</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi si"><img src="../Images/e7df5303d6714ec2013168838d182020.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*FagwFDBHeAx1ymjclOBfoA@2x.png"/></div></figure><p id="d9be" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ_M </em> -1，我们就可以使用等式 22 计算<em class="lv"> ϕ_M </em>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sv"><img src="../Images/7c24645986421bc98646335dff43f738.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*7udCV36rebmkD9cyStY13g@2x.png"/></div></figure><p id="adab" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，用这种方法我们可以计算所有的 SHAP 值而不用处理<strong class="lb iu"> <em class="lv"> W </em> </strong>的无限元素。类似于等式 41，术语<strong class="lb iu"> <em class="lv"> R </em> </strong> <em class="lv"> ᵣ </em>并不依赖于一个具体的数据实例来解释，所以如果要解释多个数据实例，只需要计算一次即可。为了使用等式 47 实现内核 SHAP，我们需要更改一些 Python 函数。清单 15 中的函数<code class="fe lx ly lz ma b">generate_coalition_vectors2()</code>类似于清单 8 中定义的函数，但是它不生成全 0 和全 1 联盟。所以它只生成矩阵的行<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ₜ.</em></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="1a7d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">generate_pi_values2()</code>类似于清单 10 中定义的函数<code class="fe lx ly lz ma b">generate_pi_values()</code>，但是它没有异常处理，因为我们没有π<strong class="lb iu"><em class="lv">【ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)的无穷大值。此函数返回矩阵对角线矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong> <em class="lv"> ᵣ </em>的对角线元素列表。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="20da" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">kernel_shap2()</code>通过调用函数<code class="fe lx ly lz ma b">calculate_shap_values2()</code>计算 SHAP 值。它使用等式 47 计算出<em class="lv"> ϕ </em> ₁到<em class="lv"> ϕ_M </em> -1，然后使用它们计算出<em class="lv"> ϕ_M </em>。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="7c36" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在清单 13 中定义的之前的模型和数据集上尝试<code class="fe lx ly lz ma b">kernel_shap2()</code>。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="7d8a" class="qo qp it ma b gy qq qr l qs qt"># Listing 18</span><span id="68b2" class="qo qp it ma b gy qu qr l qs qt">shap_values = kernel_shap2(rf_model.predict, X_train, weights, <br/>   data_to_explain)<br/>shap_values</span></pre><p id="a549" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="5b64" class="qo qp it ma b gy qq qr l qs qt">(22.74244968353333,<br/> array([[ 4.05579906e-02, -4.91062100e-02, -4.69741717e-01,<br/>          9.28299916e-02, -8.88366357e-01, -2.86693056e+00,<br/>          2.19117328e-01, -3.57934593e-02,  4.44089210e-16]]))</span></pre><p id="ba35" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您看到的，返回的 SHAP 值几乎与清单 14 中的<code class="fe lx ly lz ma b">kernel_shap()</code>相同。</p><p id="cb90" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">内核 SHAP 与采样</strong></p><p id="b8b7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一个模型有如此多的特征时，计算等式 41 或等式 47 的右边仍然是计算上昂贵的。在这种情况下，我们可以使用联合向量的样本<strong class="lb iu"><em class="lv"/></strong>’<em class="lv">’ᵢ</em>来形成联合矩阵<strong class="lb iu"><em class="lv">×t40】。联盟矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>有 2 <em class="lv"> ᴹ </em>行，每一行都是一个联盟向量<strong class="lb iu"><em class="lv">z</em></strong>’<em class="lv">ᵢ.</em>沙普利内核权重，π<strong class="lb iu">t54】ₓ</strong></em>t56】(<strong class="lb iu">t58】z</strong>t61】'ᵢ，给出了联军向量的权重<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>。然而，<em class="lv"> </em>大多数联合向量具有非常小的 Shapley 核，这意味着它们对 Shapley 值没有贡献那么多。因此，我们可以忽略这些联合向量，并在没有它们的情况下近似 Shapely 值。</strong></p><p id="cf57" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们假设 Shapley 核权重给出了联合向量的概率分布，我们可以从不包括<strong class="lb iu"><em class="lv"/></strong>'₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>的原始 2 个<em class="lv"> ᴹ-2 </em>联合向量中采样(替换)D 联合向量的子集。我们将这些向量放在<em class="lv"> D×M </em>联合矩阵<strong class="lb iu"><em class="lv">X _</em></strong><em class="lv">p</em>中:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sw"><img src="../Images/4db10ede3b7aeb02640f8046ad3efb3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*FcIj2nf_VfE6Ac5LA5uDZw@2x.png"/></div></div></figure><p id="f52e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁...<strong class="lb iu"><em class="lv">z</em></strong>’<em class="lv">ᴅ</em>为采样的联合向量。现在，我们可以使用等式 47，通过这个新的联合矩阵来计算 Shapley 值。我们构成<em class="lv"> D× </em> ( <em class="lv"> M </em> -1)矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ᴅ </em>为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sx"><img src="../Images/73614477fb6d58b5d9361ffb54a2827a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EztxpBQPYUyHfyrN8FXQuQ@2x.png"/></div></div></figure><p id="cef1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">列向量<strong class="lb iu"> <em class="lv"> c </em> </strong> <em class="lv"> ᴅ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sy"><img src="../Images/2d7b669d72b3097220087831679027cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*rtMu_lq_cf7cgzPMxaEvJQ@2x.png"/></div></figure><p id="a859" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且我们用<em class="lv"> D </em>元素将列向量<strong class="lb iu">yt36】t37】ᴅ定义为:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sz"><img src="../Images/07a1f883757ade1c5c6ff68efd009f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJgvW3mxAVDp39BCO6WwrQ@2x.png"/></div></div></figure><p id="23ad" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将<em class="lv"> D×D </em>对角矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong> <em class="lv"> ᴅ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ta"><img src="../Images/981c4caa884fa05f970e6be53ab24ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*up7OhB1_QDW2ZWmqwVOtEw@2x.png"/></div></figure><p id="5634" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有 Shapley 核权重为 1 的原因是我们已经使用它们的 Shapley 核权重对联合向量进行了采样。因此，采样的联盟现在在新的联盟矩阵中被同等地加权。现在，我们可以使用等式 47，通过采样的联合向量来计算 Shapley 值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tb"><img src="../Images/78bee506214b84ce6c4d9d5c25f57936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bUdWX3fod0ch3X8HiQrFA@2x.png"/></div></div></figure><p id="925b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以使用公式 47 的修改版本，利用采样的联合向量来计算 Shapley 值。要用 Python 实现这个方法，我们只需要修改清单 17 中定义的函数:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="438c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<code class="fe lx ly lz ma b">kernel_shap3()</code>中，我们使用<code class="fe lx ly lz ma b">Numpy</code>中的函数<code class="fe lx ly lz ma b">choice()</code>来使用它们的归一化 Shaply 核权重对联合向量进行采样。这里，我们对原始联合向量的一半进行采样(但它可以是不同的数字)。采样的联合向量将被传递到<code class="fe lx ly lz ma b">calculate_shap_values3()</code>以计算 SHAP 值。这次我们不需要π<strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)的值，因为它们都等于 1。</p><p id="5af2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">SHAP 图书馆里的内核 SHAP</strong></p><p id="9130" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类<code class="fe lx ly lz ma b">kernelExplainer()</code>可用于使用核 SHAP 方法计算 SHAP 值。我们使用清单 13 中定义的相同数据集和模型。该类获取模型和训练数据集，其方法<code class="fe lx ly lz ma b">shap_values()</code>将<em class="lv"> ϕ </em> ₁返回到<em class="lv"> ϕ_M </em>。<em class="lv"> ϕ </em> ₀的值存储在字段<code class="fe lx ly lz ma b">expected_value</code>中。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="3662" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.KernelExplainer(rf_model.predict, X_train)<br/>k_shap_values = explainer.shap_values(data_to_explain)<br/>phi0 = explainer.expected_value<br/>k_shap_values</span><span id="7ef4" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([[ 0.04055799, -0.04910621, -0.46974172,  0.09282999, -0.88836636, -2.86693056,  0.21911733, -0.03579346,  0. ]])</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="c40e" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">phi0</span><span id="e603" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>22.742449683533327</span></pre><p id="dea0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，输出几乎等于清单 14 中的<code class="fe lx ly lz ma b">kernel_shap()</code>或清单 18 中的<code class="fe lx ly lz ma b">kernel_shap2()</code>。与类<code class="fe lx ly lz ma b">Explainer</code>不同，<code class="fe lx ly lz ma b">KernelExplainer</code>不会自动从训练数据集中采样，而是使用传递给它的整个训练数据集。我们既可以像上面的例子那样手动采样数据集，也可以使用方法<code class="fe lx ly lz ma b">shap.sample()</code>和<code class="fe lx ly lz ma b">shap.kmeans()</code>来完成。例如，我们可以使用<code class="fe lx ly lz ma b">shap.sample()</code>随机抽取 100 行训练数据集:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="7a0c" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.KernelExplainer(rf_model.predict, shap.sample(X, <br/>   100))</span></pre><p id="be70" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者我们可以使用<code class="fe lx ly lz ma b">shap.kmeans()</code>来总结训练数据集:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="bfba" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.KernelExplainer(rf_model.predict, shap.kmeans(X, <br/>   100))</span></pre><p id="7f1b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里使用<em class="lv"> k </em> -means 算法在训练数据集中寻找 100 个聚类，每个聚类的权重与其中的数据点数成正比。结果是 100 个聚类及其相应权重的平均值。现在，这个加权数据集被用作原始数据集的样本来计算 SHAP 值。下面的代码展示了如何在 Python 中实现<em class="lv"> k </em> -means 采样。我们首先使用<code class="fe lx ly lz ma b">scikit-learn</code>库中的类<code class="fe lx ly lz ma b">KMeans</code>将<code class="fe lx ly lz ma b">X</code>分成 100 个集群。然后根据每个集群上的数据点数量创建<code class="fe lx ly lz ma b">weights</code>数组。最后，聚类的中心和<code class="fe lx ly lz ma b">weights</code>数组被传递给<code class="fe lx ly lz ma b">kernel_shap2()</code>以生成 SHAP 值。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="db2d" class="qo qp it ma b gy qq qr l qs qt">kmeans = KMeans(n_clusters=100, random_state=10).fit(X)</span><span id="0ba5" class="qo qp it ma b gy qu qr l qs qt">cluster_size = np.bincount(kmeans.labels_)<br/>weights = cluster_size / np.sum(cluster_size)<br/>X_train_kmeans = kmeans.cluster_centers_<br/>shap_values = kernel_shap2(rf_model.predict, X_train_kmeans, <br/>   weights, data_to_explain)</span></pre><p id="fa4e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">基于树的模型的 SHAP 值</strong></p><p id="631c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，我们使用了这个等式:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oi"><img src="../Images/8bde19ba4799b166488b68d68474a824.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*EUbbXwbvFd6jFHsIVY-88A@2x.png"/></div></div></figure><p id="12d4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算 SHAP 值(参考等式 23)。为了计算上述等式中的条件期望，我们使用等式 29 中给出的近似值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tc"><img src="../Images/98f57de359140ddaf77be54d763e078b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*HNLYY3zArEr-12RYlTtVyA@2x.png"/></div></figure><p id="fbe1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在本节看到，对于基于树的模型(树和树的系综)，有一个更好的方法来计算<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)|<strong class="lb iu"><em class="lv">x</em>ₛ</strong>。</p><p id="f11f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们之前提到的，SHAP 是一个模型不可知的解释者，所以要解释的模型是一个黑盒，我们不知道它的类型。但是，在线性 SHAP 或树 SHAP 这样的方法中，我们应该知道模型的类型，所以这些方法不是真正的模型不可知的。然而，这不是一个重要的限制，因为在现实世界的应用中，我们通常知道要解释的模型的类型。</p><p id="47f4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先在波士顿数据集上尝试一个基于树的模型。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="e78e" class="qo qp it ma b gy qq qr l qs qt"># Listing 20</span><span id="2002" class="qo qp it ma b gy qu qr l qs qt">d = load_boston()<br/>df = pd.DataFrame(d['data'], columns=d['feature_names'])<br/>y = pd.Series(d['target'])<br/>X = df[['AGE', 'RAD', 'TAX', 'DIS']]<br/> <br/>tree_model = DecisionTreeRegressor(max_depth=3)<br/>tree_model.fit(X, y)</span><span id="0647" class="qo qp it ma b gy qu qr l qs qt">fig = plt.figure(figsize=(20, 10))<br/>plot_tree(tree_model, feature_names=X.columns, fontsize =16)<br/>plt.show()</span></pre><p id="cd06" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们使用决策树回归器(来自<code class="fe lx ly lz ma b">scikit-learn</code>库)来为具有 4 个特征(<code class="fe lx ly lz ma b">AGE</code>、<code class="fe lx ly lz ma b">RAD</code>、<code class="fe lx ly lz ma b">TAX</code>和<code class="fe lx ly lz ma b">DIS</code>)的波士顿数据集的子集建模。我们拟合模型并绘制出结果树，如图 9 所示。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi td"><img src="../Images/395bff8107ac1d8062732a5dc3b755e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKjqdZFB3IKDFFWNKo2Btg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 9(来源:图片由作者提供)</p></figure><p id="2ad3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该树中的每个内部节点都标有数据集的特征，并代表对该特征的测试。该节点的每个分支代表测试的结果。每个叶节点的值给出了通过从树根到叶的路径中的所有测试的特征向量的树模型的预测。例如，我们可以使用这个模型来预测清单 20 中第一行<code class="fe lx ly lz ma b">X</code>的目标值:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="374b" class="qo qp it ma b gy qq qr l qs qt">X.iloc[0]</span><span id="597b" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>AGE     65.20<br/>RAD      1.00<br/>TAX    296.00<br/>DIS      4.09</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="b80d" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">tree_model.predict(X.iloc[0:1])</span><span id="7cf7" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([23.02767857])</span></pre><p id="bc84" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这一行，我们有<code class="fe lx ly lz ma b">TAX &lt;=416.5</code>、<code class="fe lx ly lz ma b">TAX &lt;= 267.5</code>和<code class="fe lx ly lz ma b">RID &lt;= 7.5</code>，所以从树根开始，我们在一个值为 23.028 的叶子中结束(图 10)。该值是第一行<code class="fe lx ly lz ma b">X</code>的模型预测。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi td"><img src="../Images/9dd7f6e247e406d04ed244b454d4424c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5DvmqEhlS6bWhQeUqk7hA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 10(来源:图片由作者提供)</p></figure><p id="5991" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看如果我们在特征向量中有一个<em class="lv"> NA </em>值会发生什么。假设我们在<code class="fe lx ly lz ma b">X</code>的第一行没有<code class="fe lx ly lz ma b">DIS</code>的值。所以特征向量是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi te"><img src="../Images/0401e250561c49c87cb10699174a4d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBVfAzCUdzTa9FPIIRjz7Q.png"/></div></div></figure><p id="b100" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这些值，我们可以从根节点到内部节点进行测试<code class="fe lx ly lz ma b">RAD&lt;=7.5</code>(图 11)，但是我们不能更进一步，因为我们没有<code class="fe lx ly lz ma b">RAD</code>的值。然而，通过观察左边和右边的叶子，我们知道在这个模型中，对于这个特征向量，我们只有两个可能的预测。如果<code class="fe lx ly lz ma b">RAD&lt;=7.5</code>，那么预测是 23.028。否则预测 30.358。我们还知道在每片叶子中有多少训练数据集的数据样本。这里，248 个样本中有 224 个落在左叶，其余 24 个落在右叶(图 11)。落在每个节点中的样本数被称为该节点的<em class="lv">覆盖，因此具有测试<code class="fe lx ly lz ma b">RAD&lt;=7.5</code>的节点的覆盖是 248。基于这些结果，我们可以说，得到左节点值的概率是 224/248，得到右节点值的概率是 24/248。因此，该特征向量的模型预测值为:</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tf"><img src="../Images/ae01d58f191d035c4a17bdab7d255b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2xBke4Dull-0q_xhUQt2Q@2x.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tg"><img src="../Images/b97b0723132e59266bf7d9a4ecafaa91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBYgyRqGZQQzFezZc4xe3A.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 11(来源:图片由作者提供)</p></figure><p id="bca0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是左边和右边叶子的值的平均值。如果查看图 11，您会注意到测试<code class="fe lx ly lz ma b">RAD &lt;=7.5</code>的节点的值是同一个数字(23.737)。这里，<code class="fe lx ly lz ma b">scikit-learn</code>计算并显示树的所有内部节点的平均值。让我们看另一个例子。这里的特征向量是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi te"><img src="../Images/4da5523967a3ee580212878d1ee28c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtuHVlRaa5XnLgIfQ59fCQ.png"/></div></div></figure><p id="c973" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在根节点，我们不知道<code class="fe lx ly lz ma b">TAX</code>的值，所以我们取左右分支的加权平均值。在右边的分支中，我们没有在任何内部节点上测试的特性值，所以我们取所有叶子值的加权平均值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi th"><img src="../Images/d0c3e65a2db3678c520076685c0dada4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wkvaH9Nkz-6eBMS6IkqtA@2x.png"/></div></div></figure><p id="601c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">得到这个值的概率是 166/506。在根的左分支中，我们没有<code class="fe lx ly lz ma b">TAX</code>的值，所以我们取测试<code class="fe lx ly lz ma b">TAX&lt;=254.5</code>和<code class="fe lx ly lz ma b">RAD&lt;=7.5</code>的节点值的加权平均值。对于测试<code class="fe lx ly lz ma b">TAX&lt;=254.5</code>的节点，我们取两个叶子的加权平均值。我们知道 RAD 的值，所以对于测试<code class="fe lx ly lz ma b">RAD&lt;=7.5</code>的节点，值为 23.028，概率为 248/506。最后，这个特征向量的模型预测是(图 12):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ti"><img src="../Images/9f67f33a9aab06ad46c9b0d0e4da7a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrYFeaicHRH901ru56M2Gg@2x.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tj"><img src="../Images/731c03cf071686e30700b68cb5827f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ccaoe7sgqfNjyUVGBUxI8Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 12(来源:作者图片)</p></figure><p id="f9ba" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们知道了如何在基于树的模型中处理<em class="lv"> NA </em>特性，我们可以使用算法 1 中的递归算法来计算<em class="lv">f</em>ₛ(<strong class="lb iu">t13】xₛ)=<em class="lv">e</em><em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)|<strong class="lb iu"><em class="lv">x</em></strong>ₛ].</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tk"><img src="../Images/06b4b1f45e09219921cf5b4f6f60ae72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UiZrF7xlGwr2DXJYfQPHw.png"/></div></div></figure><p id="8f76" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">EXPVALUE()</code>取待解释的特征向量(<code class="fe lx ly lz ma b"><em class="lv">x</em></code>)、联盟<code class="fe lx ly lz ma b"><em class="lv">S</em> </code>(包含可用特征)和树模型，计算<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)≈<em class="lv">e</em><em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)|<strong class="lb iu"><em class="lv">x</em>在树形模型中，每个节点都有一个索引，根的索引为 1。<code class="fe lx ly lz ma b"><em class="lv">v</em></code>是节点值的向量。<code class="fe lx ly lz ma b"><em class="lv">vⱼ.type</em></code>表示节点是内部节点还是叶节点。向量<code class="fe lx ly lz ma b"><em class="lv">a</em></code>和<code class="fe lx ly lz ma b"><em class="lv">b</em></code>表示每个内部节点的左和右节点索引。向量<code class="fe lx ly lz ma b"><em class="lv">t</em></code>包含每个内部节点的阈值，并且<code class="fe lx ly lz ma b"><em class="lv">d</em></code>是用于在内部节点中分裂的特征的索引向量。向量<code class="fe lx ly lz ma b"><em class="lv">r</em></code>包含每个节点的覆盖。<code class="fe lx ly lz ma b"><em class="lv">x</em>.<em class="lv">feature</em>(<em class="lv">dⱼ</em>)</code>给出索引为<code class="fe lx ly lz ma b"><em class="lv">dⱼ</em></code>的<code class="fe lx ly lz ma b"><em class="lv">x</em></code>中特征的名称。图 13 显示了样本节点的这些向量的值。</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tl"><img src="../Images/c37236f4f3cd5cd6da780ebe0984a511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBJDYE-7_LydnTO95wHqWA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 13(来源:作者图片)</p></figure><p id="8a1c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe lx ly lz ma b">G()</code>获取节点和累加器的索引。对于一个内部节点，如果它的特征不是<code class="fe lx ly lz ma b">S</code>的成员(这意味着我们没有那个特征的值)，该函数递归计算左右分支的值，并返回它们的加权平均值。</p><p id="26d1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果节点的特征在<code class="fe lx ly lz ma b">S</code>中，则通过将该特征的值与阈值进行比较来选择左或右分支，并且递归地计算分支的值。最后，对于叶节点，其值乘以叶的覆盖并返回。清单 21 给出了该算法的 Python 实现。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="65b8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在图 11 和图 12 所示的联盟上测试这个函数:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="1755" class="qo qp it ma b gy qq qr l qs qt">expvalue(tree_model, x=X[:1].T.squeeze(), S=['TAX', 'AGE', 'DIS'])</span><span id="9cb5" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>23.73709677419353</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="9ccc" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">expvalue(tree_model, x=X[:1].T.squeeze(), S=['RAD'])</span><span id="b372" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>22.18510728402032</span></pre><p id="bc12" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们为<code class="fe lx ly lz ma b">S</code>尝试一个空列表会发生什么？</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="40d8" class="qo qp it ma b gy qq qr l qs qt">expvalue(tree_model, x=X[:1].T.squeeze(), S=[])</span><span id="d01e" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>22.532806324110666</span></pre><p id="db55" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是树中所有叶子的加权平均值。实际上，<code class="fe lx ly lz ma b">scikit-learn</code>已经将其计算为根节点的值(请看图 9)。这等于训练数据集中所有数据实例的预测平均值。这是因为，对于每个数据实例，模型预测只是其中一片叶子的值，我们知道整个训练数据集的每个值的权重。根据等式 7，当我们没有可用的特征时，模型预测是训练数据集的样本中的实例的预测的平均值，并且这里我们使用整个训练数据集而不是样本。从属性 1 我们还知道<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)】。所以带着一个空的联盟跑<code class="fe lx ly lz ma b">expvalue()</code>返回<em class="lv"> ϕ </em> ₀.此外，<code class="fe lx ly lz ma b">scikit-learn</code>计算的根节点的值也等于<em class="lv"> ϕ </em> ₀.</p><p id="b1cf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以用这个函数来计算决策树回归量的 SHAP 值。我们可以使用清单 5 中的相同代码，只是做了一些小的修改。在函数<code class="fe lx ly lz ma b">coalition_contribution()</code>中，我们使用<code class="fe lx ly lz ma b">expvalue()</code>来计算每个联盟的模型预测，在<code class="fe lx ly lz ma b">calculate_exact_tree_shap_values()</code>中，我们使用根节点的值作为<em class="lv"> ϕ </em> ₀.</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="9ba8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以测试这个函数来解释一行<code class="fe lx ly lz ma b">X</code>:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="e400" class="qo qp it ma b gy qq qr l qs qt"># Listing 23</span><span id="331d" class="qo qp it ma b gy qu qr l qs qt">calculate_exact_tree_shap_values(tree_model, X.iloc[470])</span></pre><p id="d5e5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="3cec" class="qo qp it ma b gy qq qr l qs qt">(22.532806324110698,<br/> [[0.17363555010556078,<br/>   1.6225955204216118,<br/>   -6.753886031609969,<br/>   1.1484597480832428]])</span></pre><p id="af2b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法 1 中定义的函数<code class="fe lx ly lz ma b">EXPVALUE()</code>是计算<em class="lv">f</em>t27】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)的好得多的选择。现在我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tm"><img src="../Images/914fc4295d6f3d32c49143e014e3da14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBQFKhHB0l301EV2WufZFA@2x.png"/></div></div></figure><p id="cbbe" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这使得我们可以为在某个数据集上训练的基于树的模型获得<em class="lv">f</em>t35】ₛ(<strong class="lb iu"><em class="lv">x</em>ₛ</strong>)的精确值，并且答案比在</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tn"><img src="../Images/fd67400270c8fc4fefeb6150c9acc028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQ-MUPnZQWlI1eZuXUz4PA@2x.png"/></div></div></figure><p id="97af" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，我们使用这种近似来计算使用精确和内核 SHAP 方法的 SHAP 值。</p><p id="7949" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法 1 的时间复杂度与树中的叶子数量成比例。这是因为最坏的情况是当我们有一个不包含树的内部节点的任何特征的联盟时。在这种情况下，算法需要计算所有叶子值的加权平均值。所以<code class="fe lx ly lz ma b">EXVALUE()</code>的时间共谋是<em class="lv"> O </em> ( <em class="lv"> L </em>)其中<em class="lv"> L </em>是离开的数字。我们有<em class="lv"> M </em>个特性，对于每个特性，我们必须评估 2 个<em class="lv"> ᴹ-1 </em>联盟(不包括那个特性)。因此，找到所有特征的 SHAP 值的时间复杂度为<em class="lv">o</em>(<em class="lv">lm</em>2<em class="lv">ᴹ</em>⁻)=<em class="lv">o</em>(<em class="lv">lm</em>2<em class="lv">ᴹ</em>)</p><p id="b5a4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，对于一个集合模型，我们可以在一个集合中使用<em class="lv"> T </em>树，这导致计算所有<em class="lv"> M </em>特征的 SHAP 值的时间复杂度为<em class="lv">o</em>(<em class="lv">tlm</em>2<em class="lv">ᴹ</em>)。结果，对于大值的<em class="lv"> M </em>，使用算法 1 计算基于树的模型的 SHAP 值在计算上是昂贵的。</p><p id="7fd8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">树 SHAP 算法</strong></p><p id="9de9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伦德伯格等人。al [3]提出了一种有效的算法，可以降低寻找基于树的模型的 SAHP 值的时间复杂度。可惜[3]中给出的算法错别字太多，没有解释清楚。因此，在这一节中，我将介绍这个算法的一个修正版本，并尝试解释它是如何工作的。算法 2 给出了[3]中原始算法的修正版本(修正用红色标记)。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi to"><img src="../Images/1dbee10f5b62bcb2a687260f01f1b279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MG3QLsDNAsIqV0Rx1j2P1Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">算法 2</p></figure><p id="78ee" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">清单 24 给出了该算法的 Python 实现(因为在 Python 中数组和数据帧是零索引的，所以需要做一些修改)。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="e2ac" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用清单 23 中使用的同一行测试<code class="fe lx ly lz ma b">tree_shap()</code>函数:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="d85a" class="qo qp it ma b gy qq qr l qs qt"># Listing 25<br/>tree_shap(tree_model, X.iloc[470])</span></pre><p id="f5a0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe lx ly lz ma b">Output:</code></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="29a5" class="qo qp it ma b gy qq qr l qs qt">(22.532806324110698,<br/> array([ 0.17363555,  1.62259552, -6.75388603,  1.14845975]))</span></pre><p id="3441" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们得到了与清单 23 相同的 SHAP 值。</p><p id="481d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">树 SHAP 算法是如何工作的？(<em class="lv">可选</em> ) </p><p id="c357" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解这个算法，我们首先需要理解它背后的数学[4]。这里我们使用算法 1 的相同符号，但是我们还需要引入一些额外的符号(图 14)。假设我们有一棵树有<em class="lv"> L </em>片叶子，这些叶子的值是 v₁… <em class="lv"> v_L </em>。路径<em class="lv"> Pₖ </em>被定义为从根开始到叶子结束的内部节点的集合，叶子的值为<em class="lv"> vₖ </em>(我们只包括内部节点，所以叶子本身不包括在路径中)。因此，我们在这个树中有<em class="lv"> L </em>条路径(<em class="lv"> P </em> ₁… <em class="lv"> P_L </em>)。</p><p id="1eac" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<em class="lv"> j </em>为路径<em class="lv"> Pₖ </em>中的第<em class="lv"> j </em>个内部节点(因此对于每条路径<em class="lv"> Pₖ </em>，<em class="lv"> j </em>对于根节点为 0<em class="lv"/>，对于路径中的下一个节点加 1)， 并设<strong class="lb iu"><em class="lv">d</em></strong><em class="lv">ₖ</em>= {<em class="lv">dⱼ</em>|<em class="lv">j</em>ϵ<em class="lv">pₖ</em>}为路径<em class="lv"> Pₖ </em>中内部节点用于拆分的唯一特征的索引集合(这里我们假设特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>中的每个特征都有一个索引)。 另外，<em class="lv"> tₖⱼ </em>表示路径<em class="lv"> Pₖ </em>中内部节点<em class="lv"> j </em>的阈值。因此{<em class="lv">tₖⱼ</em>|<em class="lv">j</em>ϵ<em class="lv">pₖ</em>}是路径<em class="lv"> Pₖ </em>中内部节点的阈值的集合。我们假设<em class="lv"> Tₖⱼ </em> = (-∞，<em class="lv"> tₖⱼ </em>)如果节点<em class="lv"> j </em>连接到其在路径<em class="lv"> Pₖ </em>中的左子节点，并且<em class="lv"> Tₖⱼ </em> = ( <em class="lv"> tₖⱼ </em>，∞)如果节点<em class="lv"> j </em>连接到其在路径<em class="lv"> Pₖ </em>中的右子节点。最后，沿着路径<em class="lv"> Pₖ </em>的覆盖率由集合{<em class="lv">rₖⱼ</em>|<em class="lv">j</em>ϵ<em class="lv">pₖ</em>}表示，其中<em class="lv">rₖⱼ</em>=<em class="lv">r _</em>(<em class="lv">k</em>，<em class="lv"> j </em> +1)/ <em class="lv"> rₖⱼ </em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tp"><img src="../Images/c15cd30d95594ff6214b0a4635e654fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOwEnrgmm3NSC-bGR1WVpw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 14(来源:作者图片)</p></figure><p id="38db" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在可以看出，值<em class="lv"> ϕᵢ </em>可以使用以下公式计算:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tq"><img src="../Images/f62d9ee6f626f2aa2e9ee1bae8b28a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4HMlmUExlAyftLYFlBMxw@2x.png"/></div></div></figure><p id="39f0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">𝟙{.在哪里} <strong class="lb iu"> </strong>是指标函数，所以</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tr"><img src="../Images/0a3baed68ad6c6a3a7fe86437f8c5924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*RcYTdYDGant0yM3alZuKgQ@2x.png"/></div></figure><p id="442b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个方程的证明在[4]中给出。</p><p id="ba59" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等式 48 中的第一个和是在节点包含特征<em class="lv"> i </em>的所有路径上。如前所述，<strong class="lb iu"> <em class="lv"> D </em> </strong> <em class="lv"> ₖ </em>是路径<em class="lv"> Pₖ </em>中所有独特特征的集合。这个等式中的第三个和是在不包含特征<em class="lv"> i </em>的<strong class="lb iu"><em class="lv">d</em></strong><em class="lv">ₖ</em>与<em class="lv"> m </em>元素的所有联盟之上。因此，对于每个特征<em class="lv"> i </em>，我们需要找到包含该特征的路径，并计算每个路径对该特征的 SHAP 值的贡献。基于等式 48，SHAP 值是所有这些贡献的总和:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ts"><img src="../Images/831327aa06c021a4af6ac95b81f1fabe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5zj2QJ8nS4PRiR98Gx4dg@2x.png"/></div></div></figure><p id="d2f9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看树 SHAP 算法是如何工作的。算法 2 递归查找树中的所有路径。在每条路径的末端，它计算该路径对所有特征的贡献，并将它们添加到变量<em class="lv"> ϕᵢ </em>(因此算法 2 中的每个<em class="lv"> ϕᵢ </em>是特征<em class="lv"> i </em>的形状值的累加器)。当算法覆盖所有路径时，<em class="lv"> ϕᵢ </em>等于特征<em class="lv"> i </em>的 SHAP 值。算法 2 中的变量<code class="fe lx ly lz ma b"><em class="lv">m</em></code>(相当于清单 24 中的数据帧<code class="fe lx ly lz ma b"><em class="lv">m</em></code>)是一个包含 4 个字段的表:<code class="fe lx ly lz ma b"><em class="lv">d</em></code>、<code class="fe lx ly lz ma b"><em class="lv">z</em></code>、<code class="fe lx ly lz ma b"><em class="lv">o</em></code>和<code class="fe lx ly lz ma b"><em class="lv">w</em></code>。这些字段保存了我们在每条路径中计算其对特征<em class="lv"> i </em>的 SHAP 值的贡献所需的信息。当算法沿着一条路径行进时，<code class="fe lx ly lz ma b"><em class="lv">m</em></code>包含了到目前为止在该路径中发现的所有独特特征的索引。这些索引存储在<code class="fe lx ly lz ma b"><em class="lv">m</em></code>的字段<code class="fe lx ly lz ma b"><em class="lv">d</em></code>中。在函数<code class="fe lx ly lz ma b">RECURSE()</code>中，如果我们找到一片叶子，那么我们使用这个循环:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tt"><img src="../Images/91fd1f12ad34a4fe3d92cf4a586f6b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pf4KRDAxw9ukBGOlLU-INA.png"/></div></div></figure><p id="d0b3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<code class="fe lx ly lz ma b"><em class="lv">w </em>= <em class="lv">sum</em>(UNWIND(<em class="lv">m</em>, <em class="lv">i</em>).<em class="lv">w</em>)</code>等于:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tu"><img src="../Images/e6ff10f9528436046b47701d7e072327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsANySxPdU3BtVGruZyB1Q@2x.png"/></div></div></figure><p id="1f90" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">表中的<code class="fe lx ly lz ma b"><em class="lv">mᵢ.o</em></code>给出了<code class="fe lx ly lz ma b"><em class="lv">m</em></code>的值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tv"><img src="../Images/547db1f2f9c448c1013eacf642a8e290.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*WXGVOdMIqMkceVtrxOTXPw@2x.png"/></div></figure><p id="719a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而<code class="fe lx ly lz ma b"><em class="lv">mᵢ.z</em></code> <em class="lv"> </em>给出了:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tw"><img src="../Images/f692b43cab83de151b6e7093008512e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*Mzbv0oxuxKMI0WjdgkchMg@2x.png"/></div></figure><p id="7c97" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以计算<code class="fe lx ly lz ma b"><em class="lv">w</em>(<em class="lv">mᵢ.o-mᵢ.z</em>)<em class="lv">.vⱼ</em></code>相当于计算:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tx"><img src="../Images/21f4a91262a042e54916a9b69d5f219d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUYQBJiTdUacvb6rTBVTgA@2x.png"/></div></div></figure><p id="5c5f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在等式 48 中。因此，For 循环会计算路径对该路径中存在的要素的 SHAP 值的影响。<code class="fe lx ly lz ma b">For</code>循环从 2 开始，因为我们从那个索引开始填充<code class="fe lx ly lz ma b"><em class="lv">m</em></code>。函数<code class="fe lx ly lz ma b">RECURSE(<em class="lv">j</em>, <em class="lv">m</em>, <em class="lv">pz</em> , <em class="lv">po</em>, <em class="lv">pᵢ</em>)</code>有五个参数。<code class="fe lx ly lz ma b"><em class="lv">j</em></code>是树中当前节点的索引(我们用 0 初始化，0 是根节点的索引)。用于沿路径分割前一个节点的特征索引存储在<code class="fe lx ly lz ma b"><em class="lv">pᵢ</em></code>中。用于分割当前节点的特征是<code class="fe lx ly lz ma b"><em class="lv">vⱼ</em></code>。基于这个特性的值，应该选择左边或右边的子节点。所选节点的索引称为热索引(<code class="fe lx ly lz ma b"><em class="lv">h</em></code>)，另一个子节点的索引将是冷索引(<code class="fe lx ly lz ma b">c</code>):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ty"><img src="../Images/e5e3940ab35ee1adf21b6a09e914e97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*VFUjoNUgR57ge78IS99hiA@2x.png"/></div></figure><p id="2730" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们只需要路径中的唯一特征集，算法检查<code class="fe lx ly lz ma b"><em class="lv">vⱼ</em></code>是否已经存储在<code class="fe lx ly lz ma b"><em class="lv">m</em></code>中？如果是重复特征，将使用<code class="fe lx ly lz ma b">UNWIND()</code>删除。函数<code class="fe lx ly lz ma b">RECURSE()</code>在热索引和冷索引结束时调用自身:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tz"><img src="../Images/c8b36928e3a7193cf705ca4c5fdf8a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrJJ1Rg879GyvnZsX4I8Lw@2x.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ua"><img src="../Images/48f22d7f7ab65ffa7b9deb2947cbb7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_BwRzQv4YhitbXT8BAq8aQ@2x.png"/></div></div></figure><p id="967c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果<code class="fe lx ly lz ma b"><em class="lv">dⱼ</em></code>是一个重复特征，则在<code class="fe lx ly lz ma b"><em class="lv">m</em></code>(字段<code class="fe lx ly lz ma b">mₖ<em class="lv">.z</em></code>)中找到它的覆盖比例，并存储在<code class="fe lx ly lz ma b"><em class="lv">iz</em></code>中。否则，我们有<code class="fe lx ly lz ma b"><em class="lv">iz</em></code> =1。然后将当前节点的覆盖率(或者是<code class="fe lx ly lz ma b"><em class="lv">r</em>ₕ/<em class="lv">rⱼ</em></code>或者是<code class="fe lx ly lz ma b"><em class="lv">r</em>𝒸/<em class="lv">rⱼ</em></code>)乘以<code class="fe lx ly lz ma b"><em class="lv">i</em>𝓏</code>并作为<code class="fe lx ly lz ma b"><em class="lv">p</em>𝓏</code>传递给<code class="fe lx ly lz ma b">RECURSE()</code>。于是，<code class="fe lx ly lz ma b"><em class="lv">p</em>𝓏</code> <em class="lv"> </em>累积起来</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tw"><img src="../Images/f692b43cab83de151b6e7093008512e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*Mzbv0oxuxKMI0WjdgkchMg@2x.png"/></div></figure><p id="ea65" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">沿着小路。保持对…的追踪</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi tv"><img src="../Images/547db1f2f9c448c1013eacf642a8e290.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*WXGVOdMIqMkceVtrxOTXPw@2x.png"/></div></figure><p id="b822" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于热索引，我们将<code class="fe lx ly lz ma b"><em class="lv">m</em>ₖ<em class="lv">.o</em></code>(如果<code class="fe lx ly lz ma b"><em class="lv">dⱼ</em></code>是重复特征)或 1 作为<code class="fe lx ly lz ma b"><em class="lv">p</em>ₒ</code>传递给<code class="fe lx ly lz ma b">RECURSE()</code>，对于冷索引，我们将 0 作为<code class="fe lx ly lz ma b"><em class="lv">p</em>ₒ</code>传递。调用函数<code class="fe lx ly lz ma b">EXTEND()</code>来存储沿路径找到的每个特征的信息。它分别存储了<code class="fe lx ly lz ma b"><em class="lv">mᵢ.d</em></code>、<em class="lv">、</em>、<em class="lv">、<em class="lv">、</em>、<code class="fe lx ly lz ma b"><em class="lv">mᵢ.o</em></code>、<em class="lv">、</em>中的<code class="fe lx ly lz ma b"><em class="lv">pᵢ</em></code>、</em>、、<code class="fe lx ly lz ma b"><em class="lv">p</em>ₒ</code>。它还计算</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ub"><img src="../Images/d13e6630f4baa2ce0568fdc3ff1cc8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*QbGKw_akLJEQWWFtJIyc-Q@2x.png"/></div></figure><p id="628e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将其存储在<code class="fe lx ly lz ma b"><em class="lv">mᵢ.w</em></code>中。</p><p id="7af6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是这些值是如何沿着路径产生的呢？函数<code class="fe lx ly lz ma b">EXTEND()</code>负责生成以下值</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uc"><img src="../Images/a1fc289ebe8756ee20eac89827ad3096.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iP81wUJU_wpBzXQiAZq2WA@2x.png"/></div></div></figure><p id="00a0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="lv"> h </em> =0，1，…，|<strong class="lb iu"><em class="lv">d</em></strong><em class="lv">ₖ</em>|。在叶节点处，<code class="fe lx ly lz ma b"><em class="lv">m</em></code>中的字段<code class="fe lx ly lz ma b"><em class="lv">w</em></code>包含等式 50 的值，用于<em class="lv"> h </em> =0，1，…，|<strong class="lb iu">|<em class="lv">d</em></strong><em class="lv">ₖ</em>|。请注意，等式 48 中第三个和超过了<strong class="lb iu"><em class="lv">s</em></strong>⊆<strong class="lb iu"><em class="lv">d</em></strong><em class="lv">ₖ</em>\ {<em class="lv">I</em>}，|<strong class="lb iu"><em class="lv">s</em></strong>| =<em class="lv">h</em>。因此，为了计算路径对 SHAP 值的贡献，首先我们需要从等式 50 中移除特征{ <em class="lv"> i </em> }。这就是为什么我们首先调用<code class="fe lx ly lz ma b">UNWIND(<em class="lv">m</em>, <em class="lv">i</em>).<em class="lv">w</em>)</code>，然后将<code class="fe lx ly lz ma b"><em class="lv">SUM</em>()</code>应用于结果。如前所述<code class="fe lx ly lz ma b"><em class="lv">sum</em>(UNWIND(<em class="lv">m</em>, <em class="lv">i</em>).<em class="lv">w</em>)</code>等于方程式 49。还请注意，<code class="fe lx ly lz ma b">EXTEND()</code>和<code class="fe lx ly lz ma b">UNWIND()</code>在更改和返回之前都会复制一份<code class="fe lx ly lz ma b"><em class="lv">m</em></code>。复制的<code class="fe lx ly lz ma b"><em class="lv">m</em></code>将用于当前节点，而原始的<code class="fe lx ly lz ma b"><em class="lv">m</em></code>对于前一个节点保持不变。因此，当我们沿着树行进时，<code class="fe lx ly lz ma b"><em class="lv">m</em></code>仅包含沿着当前路径的独特特征的集合。</p><p id="ac4c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法 2 将算法 1 的时间复杂度从指数降低到低阶多项式。函数<code class="fe lx ly lz ma b">EXTEND()</code>和<code class="fe lx ly lz ma b">UNWIND()</code>中的循环都以<code class="fe lx ly lz ma b"><em class="lv">m</em></code>的长度为界。因为<code class="fe lx ly lz ma b"><em class="lv">m</em></code>沿着一条路径跟踪独特的特征，所以它受到树的最大深度<em class="lv"> D、</em>的限制。因此，<code class="fe lx ly lz ma b">UNWIND()</code>和<code class="fe lx ly lz ma b">EXTEND()</code>的时间复杂度为<em class="lv"> O </em> ( <em class="lv"> D </em>)。函数<code class="fe lx ly lz ma b">RECURSE()</code>中的循环也受到<code class="fe lx ly lz ma b"><em class="lv">m</em></code>长度的限制。然而，对于一个叶子，函数<code class="fe lx ly lz ma b">UNWIND()</code>在那个循环中被调用，所以<code class="fe lx ly lz ma b">RECURSE()</code>的时间复杂度是<em class="lv"> O </em> ( <em class="lv"> D </em>),因为<code class="fe lx ly lz ma b">UNWIND()</code>嵌套在由<em class="lv"> D </em>限定的循环中。函数<code class="fe lx ly lz ma b">RECURSE()</code>要找到树的所有路径，路径数等于叶子数。所以假设<em class="lv"> L </em>是任意一棵树的最大叶子数，那么<code class="fe lx ly lz ma b">RECURSE()</code>对于整棵树的时间复杂度将是<em class="lv"> O </em> ( <em class="lv"> LD </em>)。最后，对于一个<em class="lv"> T </em>树的系综，时间复杂度变为<em class="lv"> O </em> ( <em class="lv"> TLD </em>)。</p><p id="995e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用于树形整体的树形 SHAP</strong></p><p id="6652" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以很容易地使用<code class="fe lx ly lz ma b">tree_shap()</code>函数来计算树集合模型的 SHAP 值。假设我们有一组 T42 树。我们可以分别计算每棵树的 SHAP 值，我们将第<em class="lv"> j </em>棵树<em class="lv"> ϕᵢ </em> ⁽ <em class="lv"> ʲ </em> ⁾.中的特征<em class="lv"> i </em>的 SHAP 值因为所有这些树都同样重要，所以整个集合的特征<em class="lv"> i </em>的 SHAP 值就是集合内所有树的<em class="lv"> i </em>的 SHAP 值的平均值:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ud"><img src="../Images/ffa1d28f7422e56b34f9ff3892545fe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*Jh5UNBZ7WAKpfAgW4prjaw@2x.png"/></div></figure><p id="d86e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">清单 26 中的函数<code class="fe lx ly lz ma b">tree_shap_ensemble()</code>可以用来计算一个树集合模型的 SHAP 值。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="qx qy l"/></div></figure><p id="84fa" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数使用树 SHAP 算法来计算集合中每棵树的 SHAP 值，然后取这些值的平均值。我们现在可以在一个随机森林模型上测试这个函数:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="348c" class="qo qp it ma b gy qq qr l qs qt"># <!-- -->Listing 27</span><span id="1931" class="qo qp it ma b gy qu qr l qs qt">d = load_boston()<br/>df = pd.DataFrame(d['data'], columns=d['feature_names'])<br/>y = pd.Series(d['target'])<br/>X = df[['AGE', 'RAD', 'TAX', 'DIS']]<br/> <br/>rf_model2 = RandomForestRegressor(random_state=0, n_estimators=4).fit(X, y)</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="f9f9" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">tree_shap_ensemble(rf_model2, X.iloc[470])</span><span id="ea17" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>(22.340563241106732,<br/> array([-0.60018849,  1.36914776, -6.2207912 ,  2.33626868]))</span></pre><p id="966c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">SHAP 图书馆里的树 SHAP</strong></p><p id="4f4f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP 库中的类<code class="fe lx ly lz ma b">TreeExplainer</code>可用于使用树 SHAP 方法为树和树集合计算 SHAP 值。这里我们使用这个类来计算清单 20 中定义的数据集和模型的 SHAP 值。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="dae7" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.TreeExplainer(tree_model)<br/>shap_values = explainer.shap_values(X.iloc[470])<br/>shap_values</span><span id="74f8" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([ 0.17363555,  1.62259552, -6.75388603,  1.14845975])</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="471e" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">explainer.expected_value</span><span id="843f" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([22.53280632])</span></pre><p id="d170" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，SHAP 值几乎与清单 23 中的 c <code class="fe lx ly lz ma b">alculate_shap_values()</code>和清单 25 中的<code class="fe lx ly lz ma b">tree_shap()</code>返回的 SHAP 值相同。这个类也可以用于清单 27 中定义的随机森林，其输出几乎与该清单的输出相同。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="97e7" class="qo qp it ma b gy qq qr l qs qt">explainer = shap.TreeExplainer(rf_model2)<br/>shap_values = explainer.shap_values(X.iloc[470])<br/>shap_values</span><span id="4634" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([-0.60018849,  1.36914776, -6.2207912 ,  2.33626868])</span></pre></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><pre class="qk ma ql qm aw qn bi"><span id="2d2b" class="qo qp it ma b gy rb rc rd re rf qr l qs qt">explainer.expected_value</span><span id="cc2e" class="qo qp it ma b gy qu qr l qs qt">### Output:<br/>array([22.34056324])</span></pre><p id="72f1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> SHAP 地块</strong></p><p id="2aaf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shap 库有一些很好的工具来可视化 SHAP 值。瀑布图可以显示各个特征向量的 SHAP 值。作为一个例子，我们在波士顿数据集上训练一个 XGBoost 模型，并显示一个<code class="fe lx ly lz ma b">X</code>实例的<em class="lv">瀑布图</em>:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="168b" class="qo qp it ma b gy qq qr l qs qt">d = load_boston()<br/>df = pd.DataFrame(d['data'], columns=d['feature_names'])<br/>X = df<br/>y = pd.Series(d['target'])</span><span id="8023" class="qo qp it ma b gy qu qr l qs qt">xgb_model = xgboost.XGBRegressor(random_state=1).fit(X, y)</span><span id="e3f8" class="qo qp it ma b gy qu qr l qs qt">explainer = shap.Explainer(xgb_model, X)<br/>explanation_object = explainer(X)<br/>shap_values = explainer.shap_values(X)</span><span id="4c12" class="qo qp it ma b gy qu qr l qs qt"># visualize the first prediction's explanation<br/>shap.plots.waterfall(explanation_object[0])</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ue"><img src="../Images/ac33399beca695a32a44c536bdc63852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ak9mZaBq1Zs4oWNKfTB9tA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 15(来源:作者图片)</p></figure><p id="3453" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，根据等式 22，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi uf"><img src="../Images/10e967a3371bf097130d17acaf9f44d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*uTHqvy8EhFiOqxbvJXPvgg@2x.png"/></div></figure><p id="99af" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瀑布图可以让我们看到这个等式。瀑布图的底部从<em class="lv">E</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)]= 22.343 开始，图中的每个箭头表示每个特征对预测的正(红色)或负(蓝色)贡献。每个箭头的长度等于其对应要素的绝对 SHAP 值。SHAP 值写在箭头上，相应特征的值也写在纵轴上(例如在上面的图中，特征<code class="fe lx ly lz ma b">LSTAT</code>的值是 4.98，其 SHAP 值是 4.64)。如果 SHAP 值为正，箭头为红色并向右。对于负的 SHAP 值，它是蓝色的并向左移动。顺着这些箭头，我们最终得出模型预测的值<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)= 21.019。要素按其 SHAP 值的绝对值排序，因此具有最大绝对 SHAP 值的要素位于顶部。请注意，我们需要将解释器对象直接传递给<code class="fe lx ly lz ma b">waterfall()</code>，而不是由<code class="fe lx ly lz ma b">Explainer</code>的<code class="fe lx ly lz ma b">shap_values()</code>方法返回的 SHAP 值。</p><p id="9496" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以使用<em class="lv">力图</em>来可视化 SHAP 值:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="0a74" class="qo qp it ma b gy qq qr l qs qt">shap.initjs()<br/>shap.plots.force(explanation_object[0])</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ug"><img src="../Images/707f2beefe1f14a840cf12f491c8581b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ctYLWPyuxy9C-sSN9BcwgA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 16(来源:作者图片)</p></figure><p id="470d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">力图类似于瀑布图，但是箭头是水平堆叠的。红色箭头和箭头根据它们的长度排序，因此红色箭头从左到右变长，蓝色箭头从右到左变长。每个要素的值(不是其 SHAP 值)写在其对应的箭头后面。最长的红蓝箭头相交于<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)= 21.019。<em class="lv">E</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)]= 22.343 的值也显示在标为基值的图中。</p><p id="9483" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以使用条形图来显示 SHAP 值。我们可以使用接受解释对象的函数<code class="fe lx ly lz ma b">shap.plots.bar()</code>。条形图没有显示箭头、<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)和<em class="lv">E</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)]的值以及特性的值。每个条形的长度等于其对应要素的绝对 SHAP 值。对于正/负 SHAP 值，该条被涂成红/蓝色，并且 SHAP 值被写在每个条的旁边。要素按其 SHAP 值的绝对值排序。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="8f96" class="qo qp it ma b gy qq qr l qs qt">shap.plots.bar(explanation_object[0])</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uh"><img src="../Images/abbe92ff591942f57de457dcdd3c3638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cz9bPtw5qejiGPGV9p3ohQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 17(来源:作者图片)</p></figure><p id="4b71" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以强制条形图仅显示绝对 SHAP 值:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="0fcc" class="qo qp it ma b gy qq qr l qs qt">shap.plots.bar(explanation_object[0].abs)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ui"><img src="../Images/f66060e4d47ae95ad289344d97beceab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JfFAXqJVm8zuHtdyKiaBTA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 18(来源:作者图片)</p></figure><p id="d9a0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">蜂群图<em class="lv">有助于总结大量实例的 SHAP 分析。首先，我们只在一个实例中使用它。函数<code class="fe lx ly lz ma b">shap.plots.beeswarm()</code>接受一个<code class="fe lx ly lz ma b">Explanation</code>对象。请注意，函数<a class="ae lw" href="https://stackoverflow.com/questions/68257249/why-are-shap-values-changing-every-time-i-call-shap-plots-beeswarm" rel="noopener ugc nofollow" target="_blank">有一个 bug </a>，当你调用它时，它会改变它所取的<code class="fe lx ly lz ma b">Explanation</code>对象的 SHAP 值，所以你应该总是传递一个<code class="fe lx ly lz ma b">Explanation</code>对象的深层副本给它。</em></p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="873f" class="qo qp it ma b gy qq qr l qs qt">import copy <br/>shap.plots.beeswarm(copy.deepcopy(explanation_object[0:1]))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uj"><img src="../Images/c5813ed451c7203c0ab4644b0570b188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZqUAS9qxVggxIOqRE2aYQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 19(来源:作者图片)</p></figure><p id="c9c4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图类似于条形图，但是，我们用点来表示每个 SHAP 值，而不是条形。每个点的<em class="lv"> x </em>位置给出了相应特征的 SHAP 值。因为我们只有一个实例，所以所有的点都有相同的颜色。现在我们尝试两种情况。现在，对于每个特性，我们有两个点，每个点代表一个实例。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="2527" class="qo qp it ma b gy qq qr l qs qt">shap.plots.beeswarm(copy.deepcopy(explanation_object[0:2]))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uk"><img src="../Images/51f724169711b11f098f44e0d3ab3dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Spuo4B1lnoMvroGTDWMcnA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 20(来源:作者图片)</p></figure><p id="72a5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些点根据相应要素的值进行着色。对于每个特性，具有较高值的实例是红色的，另一个是蓝色的。但是，当我们有多个实例时，这些特性是如何排序的呢？基于所有实例的绝对 SHAP 值的平均值对特征进行排序。现在，我们可以在更多的实例上进行尝试:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="7850" class="qo qp it ma b gy qq qr l qs qt">shap.plots.beeswarm(copy.deepcopy(explanation_object))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uj"><img src="../Images/23ad06795d6228119d25c159dd4fc626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZFVovqeVZlJ3eqQF58FzSQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 21(来源:图片由作者提供)</p></figure><p id="b4bd" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个图中，最大 SHAP 值属于特征<code class="fe lx ly lz ma b">DIS</code>，但是<code class="fe lx ly lz ma b">LSTAT</code>具有所有实例的最高平均值。我们可以改变这一点，并根据其最大绝对 SHAP 值对要素进行排序。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="bdb9" class="qo qp it ma b gy qq qr l qs qt">shap.plots.beeswarm(copy.deepcopy(explanation_object),    <br/>   order=explanation_object.abs.max(0))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uj"><img src="../Images/5c4c1a5ecd1e7df3a30966417d4bfcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PO6cQOeCceSbEBvqLFeLOQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 22(来源:作者图片)</p></figure><p id="d93d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在<code class="fe lx ly lz ma b">DIS</code>坐在上面。我们也可以在条形图中使用多个实例。在这种情况下，它将计算所有实例的绝对 SHAP 值的平均值，并基于此对要素进行排序。绝对 SHAP 值的平均值也显示在条形旁边。</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="752b" class="qo qp it ma b gy qq qr l qs qt">shap.plots.bar(explanation_object)</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ui"><img src="../Images/ef8866d873a79ebcef9604d0d191a8a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85oSDnwsdn1v9OZnH4GZSQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 23(来源:作者图片)</p></figure><p id="acc4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以显示多个实例的力图。以下是 3 个实例的示例:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="c869" class="qo qp it ma b gy qq qr l qs qt">shap.force_plot(explainer.expected_value, shap_values[0:3,:], <br/>   X.iloc[0:3,:], plot_cmap="DrDb")</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ul"><img src="../Images/887bce2c17dac158618e9e4aec56b768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BD_d70-Yup1Ek7XBQ96mjg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 24(来源:作者图片)</p></figure><p id="f38e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该图由 3 个单独的力图(如公式 14 中的图)组合而成，它们旋转 90 度并水平堆叠(参见图 16)。下面是另一个有更多实例的例子:</p><pre class="kk kl km kn gt qk ma ql qm aw qn bi"><span id="df5f" class="qo qp it ma b gy qq qr l qs qt">shap.force_plot(explainer.expected_value, shap_values, X, <br/>    plot_cmap="DrDb")</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi um"><img src="../Images/552fd8ad6382fc209c7293334028c8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39Vc_8MGorZYMz_Ve6Wicw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 25(来源:作者图片)</p></figure><p id="049f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以选择单个力图的堆叠方式。例如，在图 25 中，各个图根据相似性进行堆叠。如果我们假设每个实例的 SHAP 值形成了一个<em class="lv"> M </em>维空间中的一个点，那么相似度由这些点之间的欧几里德距离决定，更相似(距离更小)的点(或实例)被堆叠在一起。还有一些其他选项来堆叠实例。例如，我们可以根据每个实例的<em class="lv"> f </em> ( <strong class="lb iu"> <em class="lv"> x </em> </strong>)的值对它们进行排序。这如图 26 所示。这里，实例的<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)的值从左到右递减。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi un"><img src="../Images/020262f542dde5f86134975077e53659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhSEyp_z7gGBdDKx_X7wOQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">图 26(来源:作者图片)</p></figure></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="7681" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解释机器学习模型是一个重要的话题。解释的意思是，我们希望对模型的单个预测和用于生成该预测的特征之间的关系有一个定性的理解。我们可以用一个简单的、可解释的模型解释器来解释一个复杂的模型，并确定它的特性重要性。SHAP 是一个个性化的模型不可知的解释者，它是基于一个名为 Shapley 值的博弈论概念开发的。SHAP 值提供了线性模型的系数，该模型原则上可以解释任何机器学习模型。</p><p id="156f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SHAP 值具有一些理想的理论属性，但是，在实践中，计算精确的 SHAP 值在计算上是昂贵的，所以我们使用一些方法，如核 SHAP 来近似 SHAP 值。在本文中，我们首先解释了 Shapley 值的数学概念，以及它们如何用于解释机器学习模型。我们还讨论了 SHAP 值和可以用来估计它们的不同算法。所有这些算法都是在 Pythom 中从头开始实现的。最后，我们讨论了 Python 中的 SHAP 库及其提供的可视化 SHAP 值的绘图工具。</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="fa54" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果您有任何问题或建议，请告诉我。本文中的所有代码清单都可以从 GitHub 下载，网址是:<a class="ae lw" href="https://github.com/reza-bagheri/SHAP" rel="noopener ugc nofollow" target="_blank">https://github.com/reza-bagheri/SHAP</a></p><p id="e1a0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="598a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1-Scott M Lundberg 和 Su-In Lee。解释模型预测的统一方法。《第 31 届神经信息处理系统国际会议论文集》，第 4768–4777 页，2017 年。</p><p id="4dfa" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2-斯科特·M·伦德伯格和苏英·李。解释模型预测的统一方法。《第 31 届神经信息处理系统国际会议论文集》(2017)，补充材料(<a class="ae lw" href="https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Supplemental.zip" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/2017/file/8 a20 a 8621978632d 76 c 43 DFD 28 b 67767-supplemental . zip</a>)。</p><p id="e7c3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3-Scott M Lundberg、Gabriel G Erion 和 Su-In Lee。树集成的一致个性化特征属性。<a class="ae lw" href="https://arxiv.org/abs/1802.03888" rel="noopener ugc nofollow" target="_blank"> arXiv 预印本 arXiv:1802.03888 </a>，2018。</p><p id="6427" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4-吉磊杨。快速树型:加速树的 SHAP 值计算。<a class="ae lw" href="https://arxiv.org/abs/2109.09847" rel="noopener ugc nofollow" target="_blank"> <br/> arXiv 预印本 arXiv:2109.09847 </a>，2021。</p><p id="466b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">附录</strong></p><p id="943b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性 SHAP 方程的证明</strong></p><p id="d149" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的预测模型<em class="lv"> f </em>是一个线性回归模型:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qe"><img src="../Images/5cf42e5d0d175a2c7ce91c1aa5e1666a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiSSXA_ruJkCYdwgnbfSXg@2x.png"/></div></div></figure><p id="30a7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> x </em> ₀=1，特征<em class="lv"> Xᵢ </em>、<em class="lv"> i </em> = 1、…、<em class="lv"> M </em>相互独立。使用等式 28，我们可以写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uo"><img src="../Images/6c77947affaa090c12231de3a35a2f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFJfkWdm5I9PMNeYMOnAdw@2x.png"/></div></div></figure><p id="5a3d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设集合<strong class="lb iu"><em class="lv"/></strong>中有<em class="lv"> k </em>特征。因为所有这些功能都是独立的，所以我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi up"><img src="../Images/aaa2933e9519703cf98decef534d2fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bRevS4lQBv6Fgg0YANAXQ@2x.png"/></div></div></figure><p id="1251" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过替换上一个等式，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi po"><img src="../Images/86b4ec35562dabce102788359ecd5a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtUjCinvocjTEAUwKfqm_g@2x.png"/></div></div></figure><p id="c914" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于所有的概率都是归一化的，我们可以写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uq"><img src="../Images/ba00897fe5e78590c02812318ac45e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*rEx7KgzY4GPiOz3gEhi1RQ@2x.png"/></div></div></figure><p id="20ec" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们还有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8fc178dd5ef9c4c7b2071d2d7f3404cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*fyCsfPBTGutBP6JgvnckTQ@2x.png"/></div></figure><p id="3315" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ur"><img src="../Images/c6f0500ef1760ed7415af9bec5a38b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDs4yfIvAZTIZqytGy6Liw@2x.png"/></div></div></figure><p id="4225" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在要计算<em class="lv">f _</em><strong class="lb iu"><em class="lv">s</em></strong>∩{<em class="lv">I</em>}(<em class="lv">x _</em><strong class="lb iu"><em class="lv">s</em></strong>∩{<em class="lv">I</em>})从<em class="lv">f</em><strong class="lb iu">ₛ</strong>(<em class="lv">x</em><strong class="lb iu">ₛ</strong>，我们需要取特征<em class="lv"> i </em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uo"><img src="../Images/707170ec523a9b773735fce91b1c6948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NfpV3SeT2avJYjzUlXJ6Gw@2x.png"/></div></div></figure><p id="ed7b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将该等式代入等式 11 并使用等式 4，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uo"><img src="../Images/fdb0e87073c02b6949cdab881d800e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPuYyAShW7woOjeJBv58TA@2x.png"/></div></div></figure><p id="f930" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在等式 51 中，我们只能依靠训练数据集的样本来计算平均值，因此我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi us"><img src="../Images/7076186e68ae8cf7106c19649986030f.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*TFDHtvXMQbXL5i5PlFMWng@2x.png"/></div></figure><p id="1aa4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> E </em> [ <em class="lv"> xᵢ </em> ]是<em class="lv"> xᵢ </em>在训练数据集样本的所有实例(<em class="lv"> k </em>实例)上的平均值，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ut"><img src="../Images/a98d24ef207bee12d18c9d7e60254e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*TUEA5D2N_dSlRCJgH7s7qg@2x.png"/></div></figure><p id="2005" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">核 SHAP 定理的证明(定理 1) </strong></p><p id="dc10" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们假设我们有一个特征向量<strong class="lb iu"> <em class="lv"> x </em> </strong>和<em class="lv"> M </em>个特征。我们首先在没有缺失特征的情况下证明这一点，因此我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/76aa9bafe996d1ad28e2e8de5f8629b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*o8ibnmh3_-xFjhzNGul6FA@2x.png"/></div></figure><p id="5fd4" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们计算<strong class="lb iu"> <em class="lv"> x </em> </strong>的所有摄动。<strong class="lb iu"><em class="lv">x</em></strong>’的每一个扰动都是一个有<em class="lv"> M </em>个元素的联合向量，每个元素可以是 0 也可以是 1。所以我们有两个ᴹ微扰。我们把每一个摄动称为<strong class="lb iu"><em class="lv">【z’</em></strong><em class="lv">【ᵢ】</em>(这是一个行向量)。我们将所有这些扰动(或联合向量)放入 2 个ᴹ× M 个联合矩阵中:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/a72deb58808cf98c35230333c116e8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*lCxKbU2z8OLYkESHuMR7rA@2x.png"/></div></figure><p id="8902" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个联盟向量<strong class="lb iu"><em class="lv">z’</em></strong><em class="lv">ᵢ</em>，我们可以计算出模型预测<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong><em class="lv">(</em><strong class="lb iu"><em class="lv">z</em></strong><em class="lv">【ᵢ】</em>。我们将列向量<strong class="lb iu"> <em class="lv"> y </em> </strong>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/039ef51797a875dcf2a5eb0b3fa85498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*BkHBzBMmbFUBdlWoc95hYg@2x.png"/></div></figure><p id="2268" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">c</em>₀<em class="lv">t101】是常数。在介绍 SHAP [1]的原文中，<em class="lv">c</em>₀<em class="lv">t105】不包含在<strong class="lb iu">y</strong></em>t109】中，这是不正确的。我们假设</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rq"><img src="../Images/5f93f9e2056a48f5c784f46b545c8211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ji_YcaADdudB6X0fPZqgvA@2x.png"/></div></div></figure><p id="3e1f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而我们将 2 <em class="lv"> ᴹ× </em> 2 <em class="lv"> ᴹ </em>对角矩阵<strong class="lb iu">t115】wt117】定义为:</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rp"><img src="../Images/92cded4f0e67643e0acc1254ba766b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlBaNDkVwDCYkmtKU4ShWg@2x.png"/></div></div></figure><p id="f297" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在假设</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi uu"><img src="../Images/b7f39d69ff72d969f76838417f2314fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*xi2qYQp2Nj0OdHETcM4lzQ@2x.png"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uv"><img src="../Images/cac4266aa78ebe735c48f3b4c1f3e1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-nMsJbz_cyKzEEV-HooYQ@2x.png"/></div></div></figure><p id="fcb9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要证明当 M 趋于无穷大时</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi uw"><img src="../Images/3b014c8da0303f2cf9603ec013e36a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*dqIGeoH7EjxSNpVA4zcTLQ@2x.png"/></div></figure><p id="d918" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ux"><img src="../Images/2e7c83d9c3c3c33d35d107e555d70000.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*X_nj9AOTXlG4rMZZjw5NYQ@2x.png"/></div></figure><p id="d024" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> ϕᵢ </em>为等式 18 中定义的<em class="lv"> f </em>和<em class="lv"/><strong class="lb iu"><em class="lv">x</em></strong>的 Shapley 值。将等式 57 和等式 58 代入等式 59，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi uy"><img src="../Images/07c10214914b094199cc0cd1a9158dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IdUx8QvUZ9yt-giaYJRYQ@2x.png"/></div></div></figure><p id="eb8b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，要最小化的目标函数是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi uz"><img src="../Images/a870b09a0cd6f3f9aeed51db2d2d831a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*6Ud1xhGH5MK3sq3J58tghw@2x.png"/></div></figure><p id="8e73" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以把联合向量<strong class="lb iu"><em class="lv">【z’</em></strong><em class="lv">【ᵢ】</em>代入方程 60 得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi va"><img src="../Images/b78309f0fe76f0ccc978618187d68378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*hxLS-qBuR-bPapJauYw2Yg@2x.png"/></div></figure><p id="0609" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将对最优函数<em class="lv"> g </em>的搜索限制为具有以下形式的线性函数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi rx"><img src="../Images/c970e05311e0bf4d51c851afabef3b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*btGj2kj3RYJCTxb-f7ipyw@2x.png"/></div></div></figure><p id="64c1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的₀是等式 54 中使用的相同常数。对于每个联盟向量<strong class="lb iu"> <em class="lv"> z </em> </strong> <em class="lv"> ᵢ </em>，这个等式写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vb"><img src="../Images/2a3ac344cbe540015667fad9efb7e69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*5GEAk1S46jx6oHfTEBnZDA@2x.png"/></div></figure><p id="95bc" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em><em class="lv">ⱼ</em>是联合向量<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>的第<em class="lv"> j </em>个元素。将该等式代入等式 61，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vc"><img src="../Images/b2b767c94d4ad187bf2991368734fe3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0eu4BDT6z4G6qfmZCwHog@2x.png"/></div></div></figure><p id="659b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们用了这样一个事实:等式 53 中[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em><em class="lv">ⱼ</em>是矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>的<em class="lv"> ij </em>元素，等式 54 中<em class="lv"> yᵢ </em>是向量<strong class="lb iu"> <em class="lv"> y </em> </strong>的第<em class="lv"> i </em>元素。现在我们将列向量<strong class="lb iu"> <em class="lv"> c </em> </strong>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ry"><img src="../Images/61cd0571aa78685b4d5d0b2a0c290fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*pv5Q7SuJiN6VkmQUAL1fbA@2x.png"/></div></figure><p id="1a2d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们可以用矩阵乘法的定义来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vd"><img src="../Images/d4bc60a419aad84cf1a3ee4136eac2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*okHiuEYJnKGyGj3lcLO5Cw@2x.png"/></div></div></figure><p id="f046" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，<strong class="lb iu"> <em class="lv"> Xc </em> </strong>是一个列向量，【<strong class="lb iu"><em class="lv">xc</em></strong><em class="lv">ᵢ</em>是它的<em class="lv"> i </em> -th 元素。我们还知道<em class="lv">π</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)<em class="lv"/>是矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong>的第<em class="lv">I</em>-个对角元素(方程式 56)。所以，我们可以再次用矩阵乘法的定义来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ve"><img src="../Images/31ceb954d835de7adb5f1643a9a5f9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUbT9Yb2xqeGxjyOE2zaCw@2x.png"/></div></div></figure><p id="9cea" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们得出结论:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vf"><img src="../Images/39bb2eaa0e8e674b1cee6e076f087eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FF1WaeFDNO5nAB4H-_IL4Q@2x.png"/></div></div></figure><p id="de35" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们利用了<em class="lv"> g </em>只是<em class="lv"> cᵢ </em>的一个函数这一事实，所以我们不是去找最小化目标函数的函数<em class="lv"> g </em>，而是去找最小化它的向量<strong class="lb iu"> <em class="lv"> c </em> </strong>的值。实际上，这个目标函数类似于加权线性回归模型的损失函数。现在我们可以最小化这个目标函数:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vg"><img src="../Images/0049572d18321b06bba0ee7555df7e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bsnG5PD8cMBJ3RBoz7QYrw@2x.png"/></div></div></figure><p id="6426" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，使目标函数最小化的<strong class="lb iu"> <em class="lv"> c </em> </strong>的值为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vh"><img src="../Images/63decb2b398ae536d3174123eb5e860f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*W671UHocsISfvm-77luPqw@2x.png"/></div></figure><p id="0fbb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要简化这个等式。首先我们简化一下<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">w</em></strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/064ced86097b7c55a022f1c64c1da3be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dVzUR0DFoQguL-GdTRStDA@2x.png"/></div></div></figure><p id="7ea9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vj"><img src="../Images/1bb27aebed25e00bf573e6347af7215d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*sUibiuYEf8mjRvESQEd0og@2x.png"/></div></figure><p id="79ab" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从等式 64 我们可以得出，第<em class="lv"> i </em>行的<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">w</em></strong>为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vk"><img src="../Images/3460474188696df34a9301b6317440da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXVcjNSVbDPrXWcXck5z9Q@2x.png"/></div></div></figure><p id="6884" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv"> X </em> </strong>的第<em class="lv"> j </em>列为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vl"><img src="../Images/f203e7488b5e94600e6afca3f3012bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*BS5wRzzAAamLtEs_JqHGIw@2x.png"/></div></figure><p id="6f85" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在根据矩阵乘法的定义，<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>的<em class="lv"> ij </em>元素等于<strong class="lb iu"><em class="lv"/></strong><em class="lv">x</em><strong class="lb iu"><em class="lv">w</em></strong>和<em class="lv"> j </em>的第<em class="lv">行的点积</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vm"><img src="../Images/e783a6caad5dfbea58464e7e4848f0ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyL5WcVYg1kSN92p-AU2IA@2x.png"/></div></div></figure><p id="4cb7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们来看看<em class="lv">π</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em><em class="lv"><em class="lv">'ᵢ</em>):</em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vn"><img src="../Images/b5a4a202ac6c28ed0f512ba3dfd08a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*2nYmFgWarHx6oPUGE_XNQg@2x.png"/></div></figure><p id="899e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，它是<em class="lv"> M </em>和<strong class="lb iu"><em class="lv">z</em></strong>'(|<strong class="lb iu"><em class="lv">z</em></strong>' |)中非零条目数的函数。所以，对于同一个<strong class="lb iu"> <em class="lv"> x </em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a176c42fbac9e05f694f69abe98b693b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*1rHWHt0ngeDLfcgzXvi93A@2x.png"/></div></figure><p id="40d2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，我们知道|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv"/>|的最大值是<em class="lv"> M </em>。在等式 66 中，如果<em class="lv"> i </em> = <em class="lv"> j </em>，那么我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vo"><img src="../Images/6f1d52fef6dfdfff29ce5523bde03bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIqzjVRo1-8Pn1RzGgZhiA@2x.png"/></div></div></figure><p id="10a7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>是二元向量，【<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ₖ</em><em class="lv">ᵢ</em>不是零就是一。所以，我们只需要包括[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ₖ</em><em class="lv">ᵢ</em>= 1 的术语。现在在这个等式中，我们可以对所有的被加数进行分组，其中|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>|是某个数比如说<em class="lv"> m </em>。对于每组<em class="lv">π</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)是相同的(只有<em class="lv"> m </em>的功能):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vp"><img src="../Images/4b9c52ad5fdaa700d9dbb7f70f67aacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*_GySc7c35GkhUqLkV5QsNA@2x.png"/></div></figure><p id="3130" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，对于<em class="lv"> m </em> =0(当<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>的所有元素都是 0)和<em class="lv"> m </em> = <em class="lv"> M </em>(当<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>的所有元素都是 1)的情况下，<em class="lv">但是，不影响这个证明。对于<em class="lv"> m </em>的每个值，我们统计|<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>| = m 和[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ₖ</em>】<em class="lv">ᵢ</em>= 1 的项数，我们将此计数表示为:</em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vq"><img src="../Images/a4c8802e998e6a14cff6a4f354bad535.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*3JrEbQBA4GPT7w2OZPdkpQ@2x.png"/></div></figure><p id="ae80" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些术语都有相同的<em class="lv">π</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)。因此，等式 67 可以写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ov"><img src="../Images/7edc41bad01e3cefa10c9c2b15beae9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jl3IxPAwGbMzQRpb5zS2wQ@2x.png"/></div></div></figure><p id="6194" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似的，如果<em class="lv">I</em>≦<em class="lv">j</em>，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sp"><img src="../Images/4158097113c1c3e27fbf9196ae1a3ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6Hhi4yG3sRbzyJSnu-sxA@2x.png"/></div></div></figure><p id="9a74" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要计算</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vq"><img src="../Images/a4c8802e998e6a14cff6a4f354bad535.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*3JrEbQBA4GPT7w2OZPdkpQ@2x.png"/></div></figure><p id="99dc" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道联盟矢量<strong class="lb iu"> <em class="lv"> z </em> </strong> <em class="lv"> 'ₖ </em>有<em class="lv"> M </em>元素。我们希望第<em class="lv"> i </em>个元素为 1。而<strong class="lb iu"><em class="lv">z</em></strong><em class="lv"/>中 1 的总数应该是<em class="lv"> m </em>。所以除了第<em class="lv"> i </em> -th 元素，我们需要<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>的<em class="lv"> m </em> -1 个元素为 1，其余为 0。所以，这就像从<em class="lv"> M </em> -1 个元素中选择<em class="lv"> m </em> -1 个元素。所以:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vr"><img src="../Images/db6a55878724fbac57fb825e2211ab0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nanIQ19D8r2kLlgNE1tXlw@2x.png"/></div></div></figure><p id="d3d1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vs"><img src="../Images/5c7a1b3687d406c394bc6c7a5907456e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvxCzxgu2UfscM4JtvxqQg@2x.png"/></div></div></figure><p id="223a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这些等式与等式 68、等式 69 和等式 70 相结合，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vt"><img src="../Images/99ce57287a01872796a33bddb0f0e86e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9bz_uMOmqxFJAPub2nKnEA@2x.png"/></div></div></figure><p id="fc70" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于这个结果，<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>的所有对角元素等于<em class="lv"> E </em> 1，所有非对角元素等于<em class="lv"> E </em> 2。所以，我们可以把<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi su"><img src="../Images/8bb6db2c4bf9d6906d4f97fb7c529404.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*ZwFoPnshQbRsATfybUu7gw@2x.png"/></div></figure><p id="ff1d" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> I </em> </strong>为<em class="lv"> M </em> × <em class="lv"> M </em>单位矩阵，<strong class="lb iu"> <em class="lv"> J </em> </strong>为一个<em class="lv"> M </em> × <em class="lv"> M </em>的矩阵，其中每个元素都等于 1。现在我们需要计算这个等式中的 a 和 b。对于<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>的非对角元素我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vu"><img src="../Images/b24626be02ca8d228d512016c8509c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-I9xbuX_KH4fAp7OeKwxA@2x.png"/></div></div></figure><p id="16d1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于对角线元素:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vv"><img src="../Images/013ddd7d8275a83bcbc124ee4df41830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGWGQyHlS3q3nTBFRFPcXg@2x.png"/></div></div></figure><p id="aa21" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们有一个由两个方程组成的系统，通过求解它们，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vw"><img src="../Images/853bbba987ed2c09ed07f37c435a1989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HsHR-mjaFK8OYmWEEiO6HA@2x.png"/></div></div></figure><p id="e6c7" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="lv"> m </em> = <em class="lv"> M </em>，等式右边的最后两项相互抵消(尽管它们都趋向于无穷大):</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vw"><img src="../Images/963ab54967ef186c61bf4c11acdf9665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdbljxD8aPYqE9p2ciKcgg@2x.png"/></div></div></figure><p id="59eb" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以简化这个等式:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vx"><img src="../Images/5e38146b835f111ddb29bab2d7ba8c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NREf9akJLGBJjcuoCBDS2A@2x.png"/></div></div></figure><p id="12af" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将该等式代入等式 71，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi vy"><img src="../Images/937d91836c33e02c71a6480f8b66d7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*EpQcl-JcdC4-NN3DO1405g@2x.png"/></div></figure><p id="573a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们要计算(<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>)⁻。我们可以证明:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vz"><img src="../Images/c3e1be1ad8baca37fa8713752d53f6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89APoxQ3BEO1SD6houzAuw@2x.png"/></div></div></figure><p id="e9a2" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了证明我们应该记住，对于任何矩阵<strong class="lb iu"> <em class="lv">一</em> </strong>:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wa"><img src="../Images/5633339b769d8f91f0821041e10485dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*NKVPqCqoq3ygiF22Z0097w@2x.png"/></div></figure><p id="8936" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/4eb84a2c1b25882724fe21e15ff747bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZHqu68r3nFo1Xmg82wCmA@2x.png"/></div></div></figure><p id="6668" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> </strong>任何<em class="lv"> M </em> × <em class="lv"> M </em>矩阵<em class="lv"> </em>像<strong class="lb iu"> <em class="lv"> J </em> </strong>我们可以很容易地表明:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wb"><img src="../Images/4e5094091473a4fce81658ceac97ba6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*p4V3PDej5i50n_IX8kWiAw@2x.png"/></div></figure><p id="6d61" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这个等式代入上一个等式，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wc"><img src="../Images/2a4820023b25b38781a4a91afa8cdfb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZR6s6ONc6JLUJ4wncOQLQ@2x.png"/></div></div></figure><p id="430c" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以随着<em class="lv"> M </em>趋于无穷大，这个等式的右边就变成了<strong class="lb iu"> <em class="lv"> I </em> </strong>。同样，我们可以证明:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wd"><img src="../Images/074bf210afd45d7df0e1f96a8b0f7bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*xSvIfp6RPFHvfFhEeLVl3Q@2x.png"/></div></div></figure><p id="c6bf" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在介绍 SHAP [2]的原始论文中，<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>推导不当，然而(<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>))⁻仍然正确！</p><p id="7827" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们需要计算(<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>)⁻<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">w</em></strong>)。从等式 65 我们知道<em class="lv">j</em>-第列的<strong class="lb iu"><em class="lv"/></strong><em class="lv">x</em><strong class="lb iu"><em class="lv">w</em></strong>是<em class="lv">π</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv"/><strong class="lb iu"><em class="lv">z</em>的<em class="lv"> ij </em>元素(<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>)⁻<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">w</em></strong>是我<em class="lv">的内积</em> -th 从等式 73 中，我们知道</strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/2ab51742884563050610fcbc4c9d2a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jr6UX0d24-4uigB3Ngpcfg@2x.png"/></div></div></figure><p id="9ac1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中下标<em class="lv"> i </em>，*表示矩阵的第<em class="lv"> i </em>行。最后我们可以从方程 63 计算出矢量<strong class="lb iu"> <em class="lv"> c </em> </strong>。向量<strong class="lb iu"> <em class="lv"> c </em> </strong>的第<em class="lv"> j </em>个元素是矩阵第<em class="lv"> j </em>行(<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵀ</em><strong class="lb iu"><em class="lv">wx</em></strong>)⁻<strong class="lb iu"><em class="lv">x</em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vz"><img src="../Images/b71aa1c9341f94c9459dadd4e61511c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2A69_6xQ0Y2J5KgO5gRjow@2x.png"/></div></div></figure><p id="02a8" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们先考虑一个联盟向量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>对于其中的<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em><em class="lv">ⱼ</em>= 0。对于这样一个矢量，我们可以使用等式 55 和等式 74，并写出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi we"><img src="../Images/72c364f7dc04d462f165714ae56bea52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CdCmx1ylAgCtICifLMY9qQ@2x.png"/></div></div></figure><p id="ea66" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们考虑一个联合向量<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>₊<em class="lv"/>它等于<strong class="lb iu"> <em class="lv"> z </em> </strong> <em class="lv"> 'ᵢ </em>，其中<em class="lv"> j </em> -th 元素设置为 1。所以，我们可以写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wf"><img src="../Images/826532ba6377fcdaee89f7e2e356d8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*UCzn6rZILmmN2WPDz68suA@2x.png"/></div></div></figure><p id="a021" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还假设:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wg"><img src="../Images/be909e9d7a79bc8e29fd4ecc7d9546cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*k2ELGQsPS38EaApfAbroVg@2x.png"/></div></figure><p id="1c29" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以用等式 74 和等式 77 来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi tz"><img src="../Images/2d1f726590913e49fbee7861cc8f64d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TPolUUeJHcan7x0yyLzn1g@2x.png"/></div></div></figure><p id="358e" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结合等式 76 和等式 78，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/cdc30e3f6f72732116c4357fb59bfdcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*680YHg8rN7AtzZb1FIfPAQ@2x.png"/></div></div></figure><p id="ede1" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以用等式 75 来写:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/df48a83abf3f72286d39740214ab6540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3y3l3kLs8e_WDeNKqc20Fg@2x.png"/></div></div></figure><p id="d27b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与其求和超过<strong class="lb iu"><em class="lv"/></strong><em class="lv">'ᵢ</em>for which[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em><em class="lv">ⱼ</em>= 0，我们可以求和超过<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>₊<em class="lv">。</em>在这种情况下，使用等式 77，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi vi"><img src="../Images/ce06368bc8284be94df0131b80b03379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jipEBSui3XdkQz3qcZUV_w@2x.png"/></div></div></figure><p id="22f3" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住对于<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>₊<em class="lv"/>第<em class="lv"> j </em>个元素设置为 1。但是，我们也可以添加任何<strong class="lb iu"> z </strong> ' <em class="lv"> ᵢ </em>其中[<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em><em class="lv">ⱼ</em>= 0。那是因为对于这样一个联军向量，<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>₊)-<em class="lv">f</em><strong class="lb iu"><em class="lv">ₓ</em></strong>(<strong class="lb iu"><em class="lv">z</em></strong><em class="lv">'ᵢ</em>)</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wh"><img src="../Images/a745f975a0208ec60947a3e76594a06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBvtIdT2_MkCuLiWVhQuAA@2x.png"/></div></div></figure><p id="8a45" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们将该等式与等式 18 进行比较，我们得出结论:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wi"><img src="../Images/5f47c695e5cd83093cfbbabf31d810bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*FN4-tDGpQPFtzT3B4V1yQw@2x.png"/></div></figure><p id="cf8a" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">于是，向量<strong class="lb iu"> <em class="lv"> c </em> </strong>给出了<em class="lv"> f </em>和<strong class="lb iu"> <em class="lv"> x </em> </strong>的 Shapley 值，使等式 61 中的目标函数最小化的函数具有如下形式:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wj"><img src="../Images/aca25cd189a6f4c8b6c016bad3b96aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*jBkXJF0QnrZMBwTpUrjsIA@2x.png"/></div></figure><p id="fad5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般可以写成:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wk"><img src="../Images/2090b9fb9affb57b016ce017261f07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*8OQHPfcpmYuApvweAe91fg@2x.png"/></div></figure><p id="8be5" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，最小化并不能决定₀.的值(这就像最小化一个函数，比如<em class="lv">y</em>=<em class="lv">x</em>+<em class="lv">c</em>，其中无论<em class="lv"> c </em>的值是多少，最小点都在<em class="lv"> x </em> =0)。为了确定<em class="lv"> c </em> ₀，我们将所有特征设置为<em class="lv"> NA </em>，然后<em class="lv"> z </em> ' <em class="lv"> ᵢ </em> =0 对于<em class="lv"> i </em> =1..<em class="lv"> M </em>。因此，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wl"><img src="../Images/0575fcfc17115a0d7385fb99f758b893.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*PqFlJA72y1PksTLG5s3Xeg@2x.png"/></div></figure><p id="6bca" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还知道，在这种情况下，模型预测是训练数据集样本中实例预测的平均值(等式 22)。所以:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wm"><img src="../Images/8b71bb407c53e85c99bce2ad5dcbacb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*Q4Vy_MiNLWOFzuhzzwBYOA@2x.png"/></div></figure><p id="148b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，当 M 趋于无穷大时，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ux"><img src="../Images/2e7c83d9c3c3c33d35d107e555d70000.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*X_nj9AOTXlG4rMZZjw5NYQ@2x.png"/></div></figure><p id="656b" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv">ϕ</em>₀=<em class="lv">e</em>[<em class="lv">f</em>(<strong class="lb iu"><em class="lv">x</em></strong>)】和<em class="lv"> ϕᵢ </em>(对于<em class="lv"> i </em> =1..<em class="lv"> M </em>给出<em class="lv"> f </em>和<strong class="lb iu"> <em class="lv"> x </em> </strong>的沙普利值。为了这个证明，我们最初假设|<strong class="lb iu"><em class="lv">x</em></strong>' | =<em class="lv">M</em>，所以<strong class="lb iu"> <em class="lv"> x </em> </strong>'没有任何零元素，<strong class="lb iu"> <em class="lv"> x </em> </strong>没有缺失特征。如果我们在<strong class="lb iu"><em class="lv">x</em></strong>'(|<strong class="lb iu"><em class="lv">x</em></strong>' |&lt;<em class="lv">m</em>)中有一些零元素，我们只包括<strong class="lb iu"> <em class="lv"> x </em> </strong>'的元素，它们是 1 ( <em class="lv"> x </em> ' <em class="lv"> ᵢ </em> =1)，同样的证明也适用于它们。对于<strong class="lb iu"><em class="lv">x</em></strong>’(对应于<strong class="lb iu"> <em class="lv"> x </em> </strong>中缺失的特征)的零元素，我们假设<em class="lv"> ϕᵢ </em> =0，这与等式 18 一致。</p><p id="4fe6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">核 SHAP 解的简化方程</strong></p><p id="51ff" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑等式 35 中定义的联盟矩阵<strong class="lb iu"> <em class="lv"> X </em> </strong>，设<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>分别为全零和全一联盟。我们首先通过从联合矩阵<strong class="lb iu"><em class="lv">×x</em></strong>中消除<strong class="lb iu"><em class="lv"/></strong>'₁和<strong class="lb iu"><em class="lv">z</em></strong>’_ 2<em class="lv">ᴹ</em>来创建一个名为<strong class="lb iu">×t83】<em class="lv">的新矩阵:</em></strong></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi so"><img src="../Images/60a138a46a90fd5c0e6be78d601b052d.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*W4svacmDOtxR7093SKeTGQ@2x.png"/></div></figure><p id="fad0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们在等式 40 的目标函数中消除对应于<strong class="lb iu"> <em class="lv"> z </em> </strong> '₁和<strong class="lb iu"><em class="lv">z</em></strong>' _ 2<em class="lv">ᴹ</em>的项，我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wn"><img src="../Images/8e5164b747bd69ec65c54b72572cca7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GolBo2rl8qbW5by3zLwFOw@2x.png"/></div></div></figure><p id="5bef" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以使用等式 43 得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wo"><img src="../Images/cc7d51ea841e50aef12cd5deca039efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyagKXV8T3fpUWKn1hzcrQ@2x.png"/></div></div></figure><p id="f3f9" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意[<strong class="lb iu"><em class="lv">z</em></strong>'<em class="lv">ᵢ</em>]_<em class="lv">m</em>是矩阵<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ₜ</em>的第<em class="lv">m</em>-列。现在我们可以将(2<em class="lv">ᴹ</em>-2】<em class="lv">×</em>(<em class="lv">m</em>-1)矩阵<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ᵣ</em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi sp"><img src="../Images/5712465d80f3c1e599cef864aa5702a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zxsM2BG8VUBJPzBZTUAYlA@2x.png"/></div></div></figure><p id="3c38" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里[<strong class="lb iu"><em class="lv">x</em></strong><em class="lv">ₜ</em>_ *，I 是<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> t </em>的第<em class="lv">I</em>-列。所以<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ᵣ </em>是从第<em class="lv"> M </em> -1 列减去最后一列<strong class="lb iu"> <em class="lv"> X </em> </strong> <em class="lv"> ₜ </em>而成。现在你可以很容易地表明，<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>的<em class="lv"> ij </em>元素是:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wp"><img src="../Images/e1fbcab6592cd75ee547975921be0ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*JjfXl_3vyi8m7Yi95OyTUg@2x.png"/></div></figure><p id="d9a6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还将列向量<strong class="lb iu"> <em class="lv"> c </em> </strong> <em class="lv"> ᵣ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi sq"><img src="../Images/442e2d4c7d261f74ed7384edb36397a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*ilBNT7zTmQCdtkNF6CdbqA@2x.png"/></div></figure><p id="c003" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将列向量<strong class="lb iu"> <em class="lv"> y </em> </strong> <em class="lv"> ᵣ </em>与 2 个<em class="lv"> ᴹ </em> -2 元素定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wq"><img src="../Images/e291a4a8850ba9752c29b46da6e110ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mZ1ll7oUOiK0764uFWI5Cw@2x.png"/></div></div></figure><p id="e820" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，<strong class="lb iu"><em class="lv"/></strong><em class="lv">【ᵣ</em>的第<em class="lv">I</em>-t 列为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi wr"><img src="../Images/5ec84c141da550768873af8e7527ff47.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*YffyeRYIUN2dPg_nJ4jPaQ@2x.png"/></div></figure><p id="62a6" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在使用矩阵乘法的定义，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi wn"><img src="../Images/380c70d1fad2ccce90754f2cace75b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EeMtxPKf7-mbWa_dPU3fuA@2x.png"/></div></div></figure><p id="0342" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将(2<em class="lv">ᴹ</em>-2)<em class="lv">×</em>(2<em class="lv">ᴹ</em>-2)对角矩阵<strong class="lb iu"> <em class="lv"> W </em> </strong> <em class="lv"> ᵣ </em>定义为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ss"><img src="../Images/52a83e8e95a0d43f598125c0b1074640.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_G1mpetdB0TSun2v3TfqyQ@2x.png"/></div></div></figure><p id="f6d0" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果我们遵循用于简化等式 62 的相同程序，我们会得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ws"><img src="../Images/1bdf3cc008629862ac6897c7b5461076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAVplRTd9OdRefmWC0_lNw@2x.png"/></div></div></figure><p id="0809" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们需要解决这个最小化问题:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi st"><img src="../Images/aa4625da86e7d7d44562a8c61d0da8ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZI8agGXKzYC95TNSoHzf6A@2x.png"/></div></figure><p id="fb8f" class="pw-post-body-paragraph kz la it lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与方程 63 相似，使目标函数最小化的 c<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>的值为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi su"><img src="../Images/6469efb32fbdc54daaa46fefa47584ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*A0V1Y-0GDQxw02g66z7WBg@2x.png"/></div></figure></div></div>    
</body>
</html>