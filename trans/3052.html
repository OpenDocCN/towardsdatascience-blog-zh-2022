<html>
<head>
<title>K-Nearest Neighbors, Naive Bayes, and Decision Tree in 10 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">10分钟内k近邻、朴素贝叶斯和决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-naive-bayes-and-decision-tree-in-10-minutes-f8620b25e89b#2022-07-05">https://towardsdatascience.com/k-nearest-neighbors-naive-bayes-and-decision-tree-in-10-minutes-f8620b25e89b#2022-07-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4262" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="dd1b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过实践案例研究演练</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/876eb6b534268df61b7b700df5bc8fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wSRsSHYnIiGJFAqC"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lh" href="https://unsplash.com/@fabulu75?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Fabrice Villard </a>拍摄的照片</p></figure><p id="36eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">U</span>n像线性模型和SVM(见<a class="ae lh" rel="noopener" target="_blank" href="/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd"> <strong class="lk jd"> <em class="mn"> Part 1 </em> </strong> </a>)，一些机器学习模型真的很复杂，要从它们的数学公式中学习。幸运的是，通过在一个小的虚拟数据集上执行一步一步的过程，可以理解它们。这样，你可以在没有“数学瓶颈”的情况下发现机器学习模型。</p><p id="08a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<a class="ae lh" rel="noopener" target="_blank" href="/understanding-3-classical-machine-learning-models-once-and-for-all-part-1-32a1ac52c0fd"> <strong class="lk jd"> <em class="mn">第一部分</em> </strong> </a>之后你会在这个故事里学到更多的三个<em class="mn">模型:K-最近邻(KNN)、朴素贝叶斯、决策树。</em></p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="e660" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">Table of contents</strong></span><span id="626a" class="mt mu it mp b gy mz mw l mx my"><strong class="mp jd">· </strong><a class="ae lh" href="#164f" rel="noopener ugc nofollow"><strong class="mp jd">K-Nearest Neighbors (KNN)</strong></a><br/>  ∘ <a class="ae lh" href="#fd00" rel="noopener ugc nofollow">Classification</a><br/>  ∘ <a class="ae lh" href="#a6da" rel="noopener ugc nofollow">Case study</a><br/>  ∘ <a class="ae lh" href="#c483" rel="noopener ugc nofollow">Regression</a><br/><strong class="mp jd">· </strong><a class="ae lh" href="#c71a" rel="noopener ugc nofollow"><strong class="mp jd">Naive Bayes</strong></a><br/>  ∘ <a class="ae lh" href="#2c11" rel="noopener ugc nofollow">Bayes’ Theorem</a><br/>  ∘ <a class="ae lh" href="#fb64" rel="noopener ugc nofollow">Case study</a><br/>  ∘ <a class="ae lh" href="#552e" rel="noopener ugc nofollow">Laplace smoothing</a><br/><strong class="mp jd">· </strong><a class="ae lh" href="#f091" rel="noopener ugc nofollow"><strong class="mp jd">Decision Tree</strong></a><br/>  ∘ <a class="ae lh" href="#d971" rel="noopener ugc nofollow">Objective</a><br/>  ∘ <a class="ae lh" href="#909b" rel="noopener ugc nofollow">Impurity function</a><br/>  ∘ <a class="ae lh" href="#ea8f" rel="noopener ugc nofollow">Case study</a><br/><strong class="mp jd">· </strong><a class="ae lh" href="#4bbe" rel="noopener ugc nofollow"><strong class="mp jd">Conclusion</strong></a></span></pre><h1 id="164f" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">k-最近邻(KNN)</h1><h2 id="fd00" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">分类</h2><p id="b471" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">KNN是一个<em class="mn">非一般化</em>机器学习模型，因为它只是“记住”所有的训练数据。它并不试图构建一个通用的内部模型，而是简单地存储训练数据的实例。KNN没有真正的训练阶段。所以，我们直接去测试吧。</p><p id="6123" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给定来自测试数据的新观察值<em class="mn"> x </em>，KNN将从距离最接近<em class="mn"> x </em>的列车数据中找到预定义数量的观察值，并根据这些预测类别。最近列车观察值(也称为最近邻居)的数量是用户定义的常数，并成为模型的超参数<em class="mn"> k </em>。一般来说，距离可以是任何度量单位，但标准欧几里得距离是最常见的选择。</p><p id="14ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从<em class="mn"> x的最近邻居的简单多数投票计算分类，</em>即<em class="mn"> x </em>被分配在<em class="mn"> x </em>的最近邻居中具有最多代表的类别。通过这种方法，KNN支持多类分类。</p><p id="f3de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好，现在让我们把手弄脏。</p><h2 id="a6da" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">个案研究</h2><p id="17fb" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">假设你有以下数据。您将把KNN应用到有10个观测值的数据集中，其中7个用于训练，另外3个用于测试。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/e86d27d6ca725dd6821d4e850884f8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9lPKOx9alAP-FbVqk18fg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">KNN案例研究数据集|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>提供</p></figure><p id="0505" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于k =  3，KNN将考虑三个最近的邻居，其工作原理如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/97f457a768a7db924ff98d25097b9206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B80siWXo_aL31Hswhxdt5w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oi"> (a) </strong>绘制所有训练和测试数据| <strong class="bd oi"> (b) </strong>对于每个测试数据，KNN在训练数据中找到它的k个最近邻居| <strong class="bd oi"> (c) </strong>在每k个最近邻居中，计算有多少属于哪个类| <strong class="bd oi"> (d) </strong>给每个测试数据分配最主要的类|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="e7be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这很容易理解。问题是:如果有一个测试观测，它的最近邻有两个同样占优的类，你该怎么办？你把哪一类分配给测试观察？</p><p id="f8e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当然，如果只有两个类，可以通过选择奇数的<em class="mn"> k </em>来完全避免这个问题。但是，如果有两个以上的类，或者如果您坚持使用偶数k，您可以为每个最近的邻居分配一个权重，这样最近的邻居对拟合的贡献更大。分配的权重应该与测试观察距离的倒数成比例。这样，两个同样占主导地位的阶级几乎不存在了。</p><p id="4c19" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">值<em class="mn"> k </em>的最佳选择高度依赖于数据:通常，较大的<em class="mn"> k </em>会抑制噪声的影响，但会使分类边界不太明显。</p><h2 id="c483" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">回归</h2><p id="0463" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">对于回归问题，分配给测试观测值的值是根据其最近邻值的平均值<em class="mn">计算的。一个用k = </em> 5进行回归的KNN的例子看起来像这样。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/a39dab3ce9997093098e92ed951dd2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQRoTxPIeFi4V_kqoyp_xg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">KNN为回归|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="8f5e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管简单，KNN已经在大量的分类和回归问题上取得了成功。作为一种非参数方法，它通常在判定边界非常<strong class="lk jd"><em class="mn"/></strong>不规则的分类情况下是成功的。</p><p id="b4a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">KNN的一个缺点是它非常依赖于距离，因此您需要非常小心地预处理分类特征，找到一个适用于您的数据的<strong class="lk jd"> <em class="mn">距离函数</em> </strong>。出于同样的原因，如果某些特征的比例明显大于其他特征，您也需要<strong class="lk jd"> <em class="mn">标准化</em> </strong>您的数据。</p><h1 id="c71a" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">朴素贝叶斯</h1><h2 id="2c11" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">贝叶斯定理</h2><p id="82a8" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">先来个脑筋急转弯:</p><blockquote class="ok"><p id="518d" class="ol om it bd on oo op oq or os ot md dk translated">医生知道50%的情况下脑膜炎会导致颈部僵硬。任何患者患脑膜炎的概率为1/50000，任何患者患落枕的概率为1/20。如果患者颈部僵硬，他/她患脑膜炎的可能性有多大？</p></blockquote><figure class="ov ow ox oy oz kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/ade5de34b407d6ee99f5d847b26f07fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcWGQr3RQAaroBvQxE2L5w.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@freestockpro?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山大·巴甫洛夫·波德瓦尔尼</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3b81" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个问题不仅仅是文字游戏。暂时抛开原来的问题，让我们问另一个问题:一个病人患脑膜炎<em class="mn">和</em>落枕的概率有多大？</p><p id="9870" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以一步一步来计算。我们知道病人患脑膜炎的概率是1/50000。并且知道他/她患有脑膜炎，有50%的可能性患者也有落枕。所以，一个患者得脑膜炎<em class="mn">和</em>落枕的概率是50% × 1/50000 = 1/100000。</p><p id="76a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">脑膜炎和落枕没有先后顺序，可以翻转逻辑。我们知道患者落枕的概率是1/20。并且知道他/她有落枕，病人也有可能患有脑膜炎。所以，一个病人得脑膜炎<em class="mn">和</em>落枕的概率是<em class="mn"> x </em> × 1/20。</p><p id="6548" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">原问题问的是<em class="mn"> x </em>的值。但是从前面的论证我们知道<em class="mn"> x </em> × 1/20 = 1/100000。因此，我们发现，如果患者颈部僵硬，他/她患脑膜炎的概率为<em class="mn"> x </em> = 0.0002。</p><p id="dd4b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用简单的数学公式表达这个逻辑就简单多了。让</p><ul class=""><li id="2680" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated">P是病人患脑膜炎的概率，而</li><li id="df8f" class="pa pb it lk b ll pj lo pk lr pl lv pm lz pn md pf pg ph pi bi translated">P是患者落枕的概率。</li></ul><p id="b13c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们以上所做的实际上只是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/512781864244c55f8f97a0cf9709caa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*iTPVl6qOr7GNH5TFKki3SA.png"/></div></figure><p id="09a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是所谓的<a class="ae lh" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">贝叶斯定理</em> </strong> </a>。在机器学习设置中，你可以用你想要预测的目标变量<em class="mn"> y </em>代替<em class="mn"> M </em>，用特征向量<em class="mn"> x₁、…、xₙ </em>代替<em class="mn"> S </em>。这个定理变成了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/bc2981f8255e7817d46743fcb97fcdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*qyNmM6FsOqHIVGdoHCTywg.png"/></div></figure><p id="f36a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">计算<em class="mn"> P </em> ( <em class="mn"> x₁，x₂，…，xₙ </em> | <em class="mn"> y </em>)很难。所以，你做了一个<em class="mn">天真的</em>假设，在给定目标变量的值的情况下，每对特征之间存在条件独立性。因此对于每个<em class="mn">，i = 1，2，…，n </em>，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/c6f31fac6a4d559d76a90f77219861e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*54BaKZR6ZbIPJrCTpwe6tg.png"/></div></figure><p id="afd6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">朴素贝叶斯模型变成了</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/e3a14f35ae116b193960c6ee5dda75c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*R4Nh8WUOhWPu9bVILjTlWg.png"/></div></figure><p id="6cf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于<em class="mn"> P </em> ( <em class="mn"> x₁、x₂、…、xₙ </em>)是常数，所以可以简化如下进行预测<em class="mn"> ŷ </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/aacddd8320e2746761039ba774ac145c.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*ZcLdiKpQlR-MCzBFSddGvg.png"/></div></figure><h2 id="fb64" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">个案研究</h2><p id="4293" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">假设你有以下逃税数据。你的任务是根据应税收入(以美元计)和婚姻状况等特征，预测一个人是否愿意纳税。</p><p id="44f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您将使用10个观察值将朴素贝叶斯拟合到训练数据中，然后根据测试数据预测一个看不见的观察值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/6d30d7e3395213cee522c3d8873476b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zb7OQ3XSk-wZzpbqckXjLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">朴素贝叶斯案例研究数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="27d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">马上，您会看到有两个分类特征(退款和婚姻状况)和一个数字特征(应税收入)。当然，计数概率会因人而异。</p><p id="6244" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为方便起见，我们将退款、婚姻状况和应税收入分别称为<em class="mn"> x₁、</em>x₂和<em class="mn"> x₃ </em>。你可能还想用1000除x₃，这样你就不会处理太大或太小的数字了。另外，通过<em class="mn"> y </em>调用规避。</p><p id="1f43" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">朴素贝叶斯做了很多代数运算。对于分类特征，您有</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/0d02afc64260af89fe4aee49e98e4efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2hP9SaH_Cgp_WvWHhcd4w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算退款的条件概率给定逃避|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/2f308f80d2feb58e95240ff5f6b17767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4bZJOpRjbMpwU9Mch3lDw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算婚姻状况的条件概率</p></figure><p id="43ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了方便起见，我们对上面的训练数据进行了分类和颜色编码。</p><p id="30ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于数字特征，事情变得有点激动人心。您需要为每个特性假设一个基础分布。假设你假设x₃是正态分布的，那么</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/4eaabdf8fc26d9419fb3e15c499843d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*TEJjp23bjy1-SDXYvvDCKQ.png"/></div></figure><p id="e535" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="mn"> μ </em>和<em class="mn"> σ </em>分别代表均值和方差。让我们根据试验数据计算出<em class="mn"> x₃ </em> = 80。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/660a29dce3497daec8022abd880fabe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNTV3XmYbDNupVpF6_0hvQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">给定逃避|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>计算应税收入的条件概率</p></figure><p id="1ece" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，您已经准备好根据测试观察进行预测了。</p><ul class=""><li id="fea6" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><em class="mn"> y </em> =否</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/03fa496e528a3dde2dd4cea1861a0666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CoKRVgq_Wb5xF9tySp5etQ.png"/></div></div></figure><ul class=""><li id="952c" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated">y =是</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/58e5391fab7b2bbd7bca7adbb61d4828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIkrl-pErZz1ozaGoxPZjg.png"/></div></div></figure><p id="a2a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然5.39 × 10⁻⁴ &gt; 0，那么预测<em class="mn"> ŷ </em> =否</p><h2 id="552e" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">拉普拉斯平滑</h2><p id="65f4" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">在这个案例研究中，对于<em class="mn"> y </em> =是的，产品链中有一个术语的值为0。这是不好的，因为在产品链中可能还有另一个大到足以推翻<em class="mn"> y </em> = No的术语，但是当乘以0时，类<em class="mn"> y </em> = Yes总是输。</p><p id="e99c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了减轻这一点，可以使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">拉普拉斯平滑</em> </strong> </a>。规定</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/6ffb8fc8e9bb1d002e5c5362d71abb79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cqhop30R6MKeGWKGbBGZuQ.png"/></div></div></figure><p id="d37d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">代替像以前一样计算分类概率</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/44996828980843bcc095e7c33d4316c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*x1ORLzMH8RGQbsW2I5-weA.png"/></div></figure><p id="f591" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将拉普拉斯平滑应用为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/8aa9f43d3b6ee8d41f19fbb7efd64c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*FK-kD8eZ9YbCPnsKZEFYzg.png"/></div></figure><p id="0330" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们最后一次预测测试观察，现在平滑。</p><ul class=""><li id="77ab" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><em class="mn"> y </em> =否</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/5f8d1babbafb5691b29974dd8c97eafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vJ45IDHh0ys5yOnAA5DoQ.png"/></div></div></figure><ul class=""><li id="b846" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><em class="mn"> y </em> =是</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/a896274929e770cba1867555d7b14c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCvptU3NcFBl17N8hGtXHA.png"/></div></div></figure><p id="8ade" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然5.87 × 10⁻⁴ &gt; 3.24× 10⁻⁴，那么朴素贝叶斯还是预测<em class="mn"> ŷ </em> =否</p><p id="a9cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管朴素贝叶斯的假设明显过于简化，但它在许多现实世界的情况下工作得相当好，著名的是文本分类。即使具有强依赖性，朴素贝叶斯仍然工作得很好，即当那些依赖性相互抵消时，对分类没有影响。</p><h1 id="f091" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">决策图表</h1><h2 id="d971" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">目标</h2><p id="c5bf" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">决策树的目标是创建一个模型，通过学习从特征中推断出的简单决策规则来预测目标变量的值。一棵树可以被看作是一个分段常数近似。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/2ffe580fd8734a853d5716abe27fbc64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tmt2YlqD3FCiouLvqN7S8A.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从未有过更真实的迷因|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="78d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树划分特征空间，使得具有相同类别或相似目标值的观察值被分组在一起。由于对树的每个节点递归地进行划分，所以通过理解决策树在节点上的工作方式就足以理解决策树的工作方式。</p><p id="095f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，考虑一个节点<em class="mn"> m </em>。让节点<em class="mn"> m </em>处的列车数据由具有<em class="mn"> n </em>个观测值的<em class="mn"> Q </em>表示。对于每个候选分裂<em class="mn"> θ </em>(这是一个特征)，将数据划分为左右两个子集，<em class="mn"> Qₗ </em> ( <em class="mn"> θ </em>)和<em class="mn"> Qᵣ </em> ( <em class="mn"> θ </em>)，观测数分别为<em class="mn"> nₗ </em>和<em class="mn"> nᵣ </em>。</p><p id="a377" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，使用杂质函数<em class="mn"> H </em>计算节点<em class="mn"> m </em>的候选分裂的质量，其选择取决于被解决的任务(分类或回归)。最佳分割<em class="mn"> θ* </em>是基于<em class="mn"> nₗ </em>和<em class="mn"> nᵣ </em>最小化加权杂质的分割，即</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/edf8271c1f8c5d40a0545e31e4a93851.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*l1mPgjekhSHEcL7ET03h-w.png"/></div></figure><p id="28e5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对子集<em class="mn"> Qₗ </em> ( <em class="mn"> θ* </em>)和<em class="mn"> Qᵣ </em> ( <em class="mn"> θ* </em>)进行递归，直到达到最大允许深度，<em class="mn"> n </em>小于某个正阈值，或者<em class="mn"> n </em> = 1。</p><p id="af04" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在唯一剩下的就是定义杂质函数<em class="mn"> H </em>用于分类和回归。</p><h2 id="909b" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">杂质函数</h2><p id="62c2" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">如果目标变量<em class="mn"> y </em>是取值为<em class="mn"> k </em> ∈ {0，1，…，<em class="mn">K</em>1 }的分类结果，设<em class="mn"> I </em>为<a class="ae lh" href="https://en.wikipedia.org/wiki/Indicator_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> <em class="mn">指示函数</em> </strong> </a>和</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/6d2517ab8cfdb8f47efc3b07193dd0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*C4Qws88BnzPrxLn4K0Gveg.png"/></div></figure><p id="933b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">成为节点<em class="mn"> m </em>中类<em class="mn"> k </em>观测值的比例。常见的杂质分类方法如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/5fd5ec2740c9b21409f0e94ae1381bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*0RCI0P0LiD_OrxDCQtreAA.png"/></div></figure><p id="d551" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果目标变量<em class="mn"> y </em>是回归的连续结果，让</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/79cdfb75c5edfdeaed2a6651a249c333.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*N6jLvCYvDSJwmuUttmaUNQ.png"/></div></div></figure><p id="e465" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是节点<em class="mn"> m </em>中所有目标变量的平均值，中位数(<em class="mn"> y </em>)是它们的中位数。常见的回归杂质测量如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/8a1f2246c1c61dcbc7e331ee5dede78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*mpG_qS6TxmHcO3cAho5_Tg.png"/></div></figure><p id="1368" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您的目标变量是计数或频率(每单位的计数)，泊松偏差可能是一个不错的选择。</p><p id="88ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要有很好的理解感，你会看到为什么这些杂质函数的定义有意义，特别是对于分类。假设目标变量<em class="mn"> y </em>有两个类，节点<em class="mn"> m </em>中类1的比例为<em class="mn"> p₁ </em> = <em class="mn"> p </em>对于某些0 ≤ <em class="mn"> p </em> ≤ 1。然后<em class="mn">p₀</em>= 1<em class="mn">p</em>，您可以为每个<em class="mn"> p </em>绘制如下杂质函数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qi"><img src="../Images/73105af16ac16339114c408a10fa2551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MG-xj26Hpqopp886PPmV6Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">二元分类的杂质函数|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="7eaf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">杂质函数的思想是当决策树的划分导致最纯粹的分裂时，使其值最小。另一方面，如果分裂不纯(许多类仍然混在一个分裂中)，那么杂质函数应该给出一个高值。</p><p id="12e8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这正是上图中发生的情况。当<em class="mn"> p </em> = 0或<em class="mn"> p </em> = 1时，出现最纯粹的分裂，此时<em class="mn"> H </em>的值为零。当在最坏情况下分配不纯时<em class="mn"> p </em> = 0.5，则<em class="mn"> H </em>给出其最大值。</p><h2 id="ea8f" class="mt mu it bd nb nr ns dn nf nt nu dp nj lr nv nw nl lv nx ny nn lz nz oa np iz bi translated">个案研究</h2><p id="b018" class="pw-post-body-paragraph li lj it lk b ll ob kd ln lo oc kg lq lr od lt lu lv oe lx ly lz of mb mc md im bi translated">我们用之前的逃税数据。您将使用10个观察值将决策树拟合到训练数据中，然后根据测试数据预测一个看不见的观察值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/6d30d7e3395213cee522c3d8873476b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zb7OQ3XSk-wZzpbqckXjLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树案例研究数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="f983" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您将使用Gini杂质函数，并将树的最大深度设置为2，以避免过度拟合(两者都是模型的超参数)。由于在训练数据中有7个“否”观察值和3个“是”观察值，基尼系数的原始值为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/2c691ea083b4e6bb4dabfb8104453916.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*vFMVE6KOU0rFMsVCQ1KazA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/32cfc9f9a843077f48ce4c78458dd06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ri78r9K0G6ZtOv6jsQdTtA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树的根|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="2095" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您将使用候选分割<em class="mn"> θ </em>分割这些观察值，并找到最佳分割。</p><ul class=""><li id="ac9d" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =退款</strong>。那么，加权基尼系数的值为</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/d7a48ae6b2f3d0f63e98f31afff660eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*TXPn2xpotVm0At98BitWHw.png"/></div></figure><ul class=""><li id="23d9" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =婚姻状况</strong>。那么，有三种可能的拆分:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/b5095e3d6c5f963fed5fe4c9a6d2db30.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*IEh6g_RZf2VXOWgubw9bKQ.png"/></div></figure><ul class=""><li id="ad04" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =应纳税所得额</strong>。由于这是一个连续的特征，你需要确定几个分割位置(在我们的例子中是随机的)并计算每个分割的基尼系数。我们看到最低的加权基尼系数是在分离位置97获得的。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qn"><img src="../Images/ef8bf5cccaaa0ba6a755ef3701ed3b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tY1PLgL6As9yVA23o_upQg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算应税收入的基尼系数|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="be12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在所有这些候选分割中，最好的分割给出了最低的加权基尼系数，它要么是<strong class="lk jd"><em class="mn">【θ】</em>=婚姻状况</strong>({已婚}、{单身、离婚})分割，要么是<strong class="lk jd"> <em class="mn"> θ </em> =分割位置为97的应税收入</strong>。为了简单起见，我们选择前者。决策树如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/c9fb1848f366e1f420e7fbb77031c848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2slJtTEWxxf8_n4AjKmcZw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">深度为1的决策树|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="cac2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">左边的节点已经是纯规避=否，那么现在只需要拆分右边的节点。对其余的列车观测数据进行与之前相同的操作。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/613bf0fb173dd606e4812b263744a391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifjmnlq741Rs5vpo1CZ-Eg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树右侧节点的数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><ul class=""><li id="f6a2" class="pa pb it lk b ll lm lo lp lr pc lv pd lz pe md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =退款</strong>。加权基尼系数为0.25。</li><li id="1f0c" class="pa pb it lk b ll pj lo pk lr pl lv pm lz pn md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =婚姻状况</strong>。只剩下两类，所以很容易计算:加权基尼系数是0.50。</li><li id="b33d" class="pa pb it lk b ll pj lo pk lr pl lv pm lz pn md pf pg ph pi bi translated"><strong class="lk jd"> <em class="mn"> θ </em> =应纳税所得额</strong>。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qo"><img src="../Images/acce98701a0c58da63a99caaa4317a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w2JwPZ_23PpjblbDqU59OA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算应税收入的基尼系数|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="3e28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将选择<strong class="lk jd"> <em class="mn"> θ </em> =退款</strong>作为最佳选择，因为它在加权基尼系数最低的候选分割中非常简单。决策树如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/f896a4cf3fb9b67c8fc57a2118241f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wHX9pAhOOzq8LAgW5T2YQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">深度为2的决策树|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="a2f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您将在这里停止，因为树的最大深度已经达到(设置为2)。</p><p id="53e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要进行预测，只需使用测试数据遍历树即可。在你的测试数据中，<strong class="lk jd">婚姻状况=单身</strong>，<strong class="lk jd">退款=是</strong>，所以预测为<strong class="lk jd">逃避=否</strong>如下图橙色决策路径所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/12b48675d11d50bc31c3870d66af6b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A2jj9iv2BM8j-aRRWO_DQQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">对测试数据|图片进行决策树预测<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="adad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树因为可以可视化，所以<strong class="lk jd"> <em class="mn">理解</em></strong><strong class="lk jd"><em class="mn">解释</em> </strong>很简单。需要<strong class="lk jd"> <em class="mn">少量数据准备</em> </strong>:不需要数据归一化或虚拟变量。就像KNN和朴素贝叶斯一样，决策树能够处理<strong class="lk jd"> <em class="mn">多类</em> </strong>问题。</p><p id="4fd1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，决策树往往会对具有大量特征的<strong class="lk jd"> <em class="mn">数据进行过度拟合。诸如修剪、设置叶节点所需的最小样本数或设置树的最大深度之类的机制对于避免这个问题是必要的。也可以考虑事先进行<strong class="lk jd"> <em class="mn">降维</em> </strong>。</em></strong></p><p id="90ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树也可能不稳定，因为数据的微小变化可能导致生成完全不同的树。这个问题通过在<strong class="lk jd"> <em class="mn">集合</em> </strong>中使用决策树得以缓解。</p><h1 id="4bbe" class="na mu it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">结论</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qp"><img src="../Images/f48032c3c084979375b45cec96ac011b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dfCeNXgVdxU62WV2"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@heather_wilde?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">希瑟·王尔德</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="80a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你已经非常详细地学习了三个最基本的机器学习模型:<strong class="lk jd">K-最近邻</strong>、<strong class="lk jd">朴素贝叶斯</strong>和<strong class="lk jd">决策树</strong>。现在，您不仅可以使用已建立的库来构建它，还可以自信地知道它们是如何从内到外工作的，使用它们的最佳实践，以及如何提高它们的性能。</p><p id="b75b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恭喜你。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qq"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qr qs hx qt" role="separator"><span class="qu bw bk qv qw qx"/><span class="qu bw bk qv qw qx"/><span class="qu bw bk qv qw"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="mn">成为会员</em> </strong> </a> <em class="mn">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖<em class="mn">想了解更多经典机器学习模型的工作原理，以及它们是如何优化参数的？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="qy qz gp gr ra"><div role="button" tabindex="0" class="ab bv gv cb fp rb rc bn rd lb ex"><div class="re l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rf rg fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rf rg fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rj rk gw l"><h2 class="bd jd wn pa fp wo fr fs wp fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wq au wr ws wt td wu an eh ei wv ww wx el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wy l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rw dh rx fp ab ry fo di"><div class="di ro bv rp rq"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di ro bv rr rs rt"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv ru rv rt"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="qy qz gp gr ra"><div role="button" tabindex="0" class="ab bv gv cb fp rb rc bn rd lb ex"><div class="re l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rf rg fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rf rg fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rj rk gw l"><h2 class="bd jd wn pa fp wo fr fs wp fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wq au wr ws wt td wu an eh ei wv ww wx el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wy l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rw dh rx fp ab ry fo di"><div class="di ro bv rp rq"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di ro bv rr rs rt"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv ru rv rt"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="qy qz gp gr ra"><div role="button" tabindex="0" class="ab bv gv cb fp rb rc bn rd lb ex"><div class="re l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rf rg fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rf rg fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rj rk gw l"><h2 class="bd jd wn pa fp wo fr fs wp fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wq au wr ws wt td wu an eh ei wv ww wx el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wy l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="rw dh rx fp ab ry fo di"><div class="di ro bv rp rq"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di ro bv rr rs rt"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv ru rv rt"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="qy qz gp gr ra"><div role="button" tabindex="0" class="ab bv gv cb fp rb rc bn rd lb ex"><div class="re l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rf rg fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rf rg fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rj rk gw l"><h2 class="bd jd wn pa fp wo fr fs wp fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wq au wr ws wt td wu an eh ei wv ww wx el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wy l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="rw dh rx fp ab ry fo di"><div class="di ro bv rp rq"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di ro bv rr rs rt"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv ru rv rt"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="qy qz gp gr ra"><div role="button" tabindex="0" class="ab bv gv cb fp rb rc bn rd lb ex"><div class="re l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw rf rg fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l rf rg fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rj rk gw l"><h2 class="bd jd wn pa fp wo fr fs wp fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wq au wr ws wt td wu an eh ei wv ww wx el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----f8620b25e89b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wy l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rw dh rx fp ab ry fo di"><div class="di ro bv rp rq"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di ro bv rr rs rt"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv ru rv rt"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div><div class="ab cl qr qs hx qt" role="separator"><span class="qu bw bk qv qw qx"/><span class="qu bw bk qv qw qx"/><span class="qu bw bk qv qw"/></div><div class="im in io ip iq"><p id="e458" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1]<a class="ae lh" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">《sci kit-learn:Python中的机器学习》</a>，佩德雷戈萨，<em class="mn">等著</em>，JMLR 12，第2825–2830页，2011年。</p><p id="e122" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]<a class="ae lh" href="https://scikit-learn.org/stable/user_guide.html" rel="noopener ugc nofollow" target="_blank">sci kit-学习用户指南</a></p></div></div>    
</body>
</html>