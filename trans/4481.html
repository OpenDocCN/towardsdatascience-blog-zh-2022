<html>
<head>
<title>Auto Model Specification with ML Techniques for Time Series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于ML技术的时间序列自动模型描述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/auto-model-specification-with-ml-techniques-for-time-series-e7b9a90ae9d7#2022-10-04">https://towardsdatascience.com/auto-model-specification-with-ml-techniques-for-time-series-e7b9a90ae9d7#2022-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="180a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用Python库scalecast自动选择时间序列的最佳趋势、季节和自回归表示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/03e656c87105c7bf38005512188be0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UUCdK_yAgc78gBJr"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jakehills?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰克·希尔斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="e3e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据所采用的模型，有几种方法可以找到时间序列的最佳模型规格。对于arIMA模型，一种流行的方法是在搜索不同的AR、I和MA订单时监控信息标准。这已经被证明是一种有效的技术，流行的库<a class="ae ky" href="https://www.rdocumentation.org/packages/forecast/versions/8.5/topics/auto.arima" rel="noopener ugc nofollow" target="_blank"> R </a>和<a class="ae ky" href="https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html" rel="noopener ugc nofollow" target="_blank"> Python </a>提供了自动ARIMA模型供用户试验。类似的方法可以用于其他经典的统计时间序列方法，如霍尔特-温特斯指数平滑和TBATS。</p><p id="729c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于机器学习模型来说，它可以稍微复杂一些，除了复杂的深度学习模型(如<a class="ae ky" href="https://unit8co.github.io/darts/examples/07-NBEATS-examples.html" rel="noopener ugc nofollow" target="_blank"> N-Beats </a>、<a class="ae ky" href="https://unit8co.github.io/darts/generated_api/darts.models.forecasting.nhits.html?highlight=n%20hits" rel="noopener ugc nofollow" target="_blank"> N-HiTS </a>和其他几个)，没有多少自动化的纯ML方法持续优于经典模型(Makridakis等人，2020)。</p><p id="84bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python库<a class="ae ky" href="https://github.com/mikekeith52/scalecast" rel="noopener ugc nofollow" target="_blank"> scalecast </a>提供了一个名为 <code class="fe lv lw lx ly b"><a class="ae ky" href="https://scalecast.readthedocs.io/en/latest/Forecaster/Forecaster.html#src.scalecast.Forecaster.Forecaster.auto_Xvar_select" rel="noopener ugc nofollow" target="_blank">auto_Xvar_select()</a></code>的<a class="ae ky" href="https://scalecast.readthedocs.io/en/latest/Forecaster/Forecaster.html#src.scalecast.Forecaster.Forecaster.auto_Xvar_select" rel="noopener ugc nofollow" target="_blank">函数，可以使用来自</a><a class="ae ky" href="http://scikit-learn.org" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的模型自动选择任何给定系列的最佳趋势、季节性和回望表示(或滞后)。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="7e39" class="md me it ly b gy mf mg l mh mi">pip install --upgrade scalecast</span></pre><p id="10d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数的工作方式是首先搜索时间序列给定趋势的理想表示，然后是季节性，然后是回望，所有这些都是分别进行的。“理想”在这种情况下意味着使用选定的模型(默认为多元线性回归，或MLR)最小化一些样本外误差(或最大化R2)。在分别找到这些中的每一个之后，搜索所有上述表示的理想组合，并选择考虑不规则周期和用户认为合适的其他回归量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/a4c237857336f51268c1fe72a0af9484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*C8vWXGY2xl0WHaNE6QEkwQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者分类的图片scalecast如何使用auto_Xvar_select()函数为预测模型自动选择回归量</p></figure><p id="ec46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个有趣的功能。当应用于M4竞赛的100，000系列时，它返回具有不同准确度的结果，这取决于系列的频率。对于每小时频率组，使用KNN、LightGBM和XGBoost模型均可获得低于0.6的OWA，其中使用默认MLR模型搜索这些模型的表示。就背景而言，这意味着这些模型的表现有望超过一个简单的模型，其季节性调整平均超过40%(1–0.6)。这是一个非常可靠的结果，与sktime在同一系列中使用纯ML方法发布的结果相当(Loning等人，2019)。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="1209" class="md me it ly b gy mf mg l mh mi"><strong class="ly iu"># evaluate the hourly series<br/>for</strong> i <strong class="ly iu">in</strong> tqdm(Hourly<strong class="ly iu">.</strong>index):<br/>    y <strong class="ly iu">=</strong> Hourly<strong class="ly iu">.</strong>loc[i]<strong class="ly iu">.</strong>dropna()<br/>    sd <strong class="ly iu">=</strong> info<strong class="ly iu">.</strong>loc[i,'StartingDate']<br/>    fcst_horizon <strong class="ly iu">=</strong> info<strong class="ly iu">.</strong>loc[i,'Horizon']<br/>    cd <strong class="ly iu">=</strong> pd<strong class="ly iu">.</strong>date_range(<br/>        start <strong class="ly iu">=</strong> sd,<br/>        freq <strong class="ly iu">=</strong> 'H',<br/>        periods <strong class="ly iu">=</strong> len(y),<br/>    )<br/>    f <strong class="ly iu">=</strong> Forecaster(<br/>        y <strong class="ly iu">=</strong> y,<br/>        current_dates <strong class="ly iu">=</strong> cd,<br/>        future_dates <strong class="ly iu">=</strong> fcst_horizon,<br/>    )<br/>    <br/>    f<strong class="ly iu">.</strong>set_test_length(fcst_horizon)<br/>    f<strong class="ly iu">.</strong>integrate(critical_pval<strong class="ly iu">=</strong>.99,max_integration<strong class="ly iu">=</strong>1)<br/>    f<strong class="ly iu">.</strong>set_validation_length(fcst_horizon)<br/>    f<strong class="ly iu">.</strong>set_validation_metric('mae')<br/>    <strong class="ly iu">if</strong> len(f<strong class="ly iu">.</strong>y) <strong class="ly iu">&gt;</strong> 300:<br/>        f<strong class="ly iu">.</strong>auto_Xvar_select(<br/>            monitor<strong class="ly iu">=</strong>'LevelTestSetMAE',<br/>            max_ar <strong class="ly iu">=</strong> 48,<br/>            exclude_seasonalities <strong class="ly iu">=</strong> [<br/>              'quarter',<br/>              'month',<br/>              'week',<br/>              'day',<br/>            ]<br/>        )<br/>        f<strong class="ly iu">.</strong>determine_best_series_length(<br/>            monitor<strong class="ly iu">=</strong>'LevelTestSetMAE',<br/>            step<strong class="ly iu">=</strong>50,<br/>            min_obs <strong class="ly iu">=</strong> 300,<br/>        )<br/>    <strong class="ly iu">else</strong>:<br/>        f<strong class="ly iu">.</strong>auto_Xvar_select(<br/>            monitor<strong class="ly iu">=</strong>'LevelTestSetMAE',<br/>            irr_cycles <strong class="ly iu">=</strong> [168], <em class="mk"># weekly</em><br/>            exclude_seasonalities <strong class="ly iu">=</strong> [<br/>              'quarter',<br/>              'month',<br/>              'week',<br/>              'day',<br/>              'dayofweek',<br/>            ],<br/>            max_ar <strong class="ly iu">=</strong> 24,<br/>        )<br/>    f<strong class="ly iu">.</strong>tune_test_forecast(<br/>        models,<br/>        error<strong class="ly iu">=</strong>'ignore',<br/>    )</span></pre><p id="8b5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看用于在M4系列<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/m4/01%20-%20evaluate_models.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>运行所有带有scalecast的模型的笔记本，以及用于在相同过程中评估每个模型性能的笔记本<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/m4/02%20-%20evaluate_accuracy.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="7cc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的问题是，在<code class="fe lv lw lx ly b">auto_Xvar_select()</code>函数中使用不同于缺省值的估计量是否能导致持续更好的结果。以这种方式搜索模型规格可能很耗时，但使用MLR来做通常不会使事情变慢太多，这就是为什么它是该功能的默认设置。</p><p id="7437" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，这是一个漫长的过程来充分研究。为了简化流程，我将模型限制为:</p><ul class=""><li id="ba09" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">多元线性回归</li><li id="78f4" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">弹性网</li><li id="f544" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">梯度增强树(GBT)</li><li id="a2da" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">k-最近邻(KNN)</li><li id="7ddc" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">支持向量机</li><li id="2ead" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">多级感知器(MLP)</li></ul><p id="3c3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些模型中的每一个都被用来寻找代表和预测，尝试了所有的组合。我也只对每个模型使用默认参数，尽管我相信用网格搜索来调整它们可以显著提高性能。最后，我只使用了从小时组的414个系列中随机选择的50个样本。即使做了这些修改，这个过程在我的Windows电脑上运行了17个多小时。由此产生的预测通过各自的平均<a class="ae ky" href="https://scalecast.readthedocs.io/en/latest/Forecaster/Util.html#src.scalecast.util.metrics.smape" rel="noopener ugc nofollow" target="_blank">对称平均绝对百分比误差(SMAPE) </a>性能进行评估，这是M4竞赛中用于评估模型的指标之一。在这里找到完整的笔记本<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/misc/auto_Xvar/auto_Xvar.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kj kk kl km gt lz ly ma mb aw mc bi"><span id="d44c" class="md me it ly b gy mf mg l mh mi">for i in tqdm(Hourly.index):<br/>    y = Hourly.loc[i].dropna()<br/>    sd = info.loc[i,'StartingDate']<br/>    fcst_horizon = info.loc[i,'Horizon']<br/>    cd = pd.date_range(<br/>        start = sd,<br/>        freq = 'H',<br/>        periods = len(y),<br/>    )<br/>    f = Forecaster(<br/>        y = y,<br/>        current_dates = cd,<br/>        future_dates = fcst_horizon,<br/>    )<br/>    f.set_test_length(fcst_horizon)<br/>    f.integrate(critical_pval=.99,max_integration=1)<br/>    for xvm in models:<br/>        for fcstm in models:<br/>            f2 = f.deepcopy()<br/>            f2.auto_Xvar_select(<br/>                estimator = xvm,<br/>                monitor='LevelTestSetMAE',<br/>                max_ar = 48,<br/>                exclude_seasonalities = [<br/>                  'quarter',<br/>                  'month',<br/>                  'week',<br/>                  'day'<br/>                ],<br/>            )<br/>            f2.set_estimator(fcstm)<br/>            f2.proba_forecast(dynamic_testing=False) if fcstm in (<br/>                'mlp','gbt','xgboost','lightgbm','rf'<br/>            ) else f2.manual_forecast(dynamic_testing=False)<br/>            point_fcst = f2.export('lvl_fcsts')[fcstm]<br/>            results.loc[xvm,fcstm] += metrics.smape(<br/>                Hourly_test.loc[i].dropna().to_list(),<br/>                point_fcst.to_list(),<br/>            )</span></pre><p id="962d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这引出了几个值得观察的有趣观点:</p><ul class=""><li id="5a28" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">在衡量实际预测准确性时，KNN和GBT模型始终优于其他模型，无论哪个模型用于搜索最佳序列表示。这并不奇怪，因为这些是整个M4每小时系列中表现最好的车型。</li><li id="c244" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">表现最差的模型是SVR、ElasticNet和MLP，最好的模型和最差的模型平均相差60.8(！！)个百分点。</li><li id="e593" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">在寻找理想的系列表现方面最好的模型是ElasticNet和MLP，在这方面最好的和最差的模型之间只有4个百分点的差距。</li><li id="71c2" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">平均而言，表现最好的模型组合是KNN模型和MLP模型。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/9bc0349b78b355db6b243ecaac21e511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQOG5LUxkF7R7IpDEN-4tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="80b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，从该实验中可以看出，用于进行预测的模型比用于寻找理想表示的模型对获得高精度更重要。不过，这两方面的最佳模式似乎也是相互颠倒的。寻找表征的最佳模型在预测方面表现不佳，反之亦然。弱估计量可能更依赖于找到理想的趋势、季节性和回顾性，甚至有机会做出好的预测，因此将弱估计量与强估计量结合起来找到最佳表示以实际做出预测是一个好主意。但是，这只是一个想法，还需要更多的研究来证实。</p><h1 id="2338" class="na me it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">结论</h1><p id="e1fa" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">很难找到一种完全自动化的机器学习技术来预测持续优于经典统计方法的情况。很多时候，最好的结果来自于不同的最大似然估计的组合。在这种情况下，我发现使用较弱的模型来寻找给定序列的理想趋势、季节性和回望表示，并将该表示与较强的模型相结合来进行预测，通常会在我测试的小时序列样本上获得最佳结果。所有这些方法都用样本外数据进行了测试，scalecast软件包提供了一个非常简单的界面来执行这种分析。</p><h1 id="bfc6" class="na me it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">链接</h1><ul class=""><li id="3881" class="ml mm it lb b lc nr lf ns li nw lm nx lq ny lu mq mr ms mt bi translated">Scalecast: <a class="ae ky" href="https://github.com/mikekeith52/scalecast" rel="noopener ugc nofollow" target="_blank"> Github </a> / <a class="ae ky" href="https://scalecast.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">阅读文档</a></li><li id="74ee" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">带scalecast的M4车型:<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/m4/01%20-%20evaluate_models.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="9192" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">用scalecast评估M4模型:<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/m4/02%20-%20evaluate_accuracy.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="1529" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">使用scalecast对50系列进行Auto Xvar实验:<a class="ae ky" href="https://github.com/mikekeith52/scalecast-examples/blob/main/misc/auto_Xvar/auto_Xvar.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a> / <a class="ae ky" href="https://scalecast-examples.readthedocs.io/en/latest/misc/auto_Xvar/auto_Xvar.html" rel="noopener ugc nofollow" target="_blank">阅读文档</a></li></ul><h1 id="eded" class="na me it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated"><strong class="ak">作品引用</strong></h1><p id="b85a" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">马库斯·洛宁、安东尼·巴格纳尔、萨贾苏亚·加内什、维克多·卡萨科夫、杰森·莱恩斯和弗朗兹·基拉里。sktime:时间序列机器学习的统一接口。更正，abs/1909.07872，2019。网址<a class="ae ky" href="http://arxiv.org/abs/1909.07872." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1909.07872.</a><br/><br/>斯皮罗斯·马克里达基斯、伊万杰洛斯·斯皮里奥蒂斯和瓦西里奥斯·阿西马科普洛斯。M4竞赛:100，000个时间序列和61种预测方法。国际预测杂志，36(1):54–74，2020。doi:10.1016/j . ijforecast . 2019 . URL<a class="ae ky" href="https://ideas.repec.org/" rel="noopener ugc nofollow" target="_blank">https://ideas.repec.org/</a>a/eee/int for/v36y 2020 i1p 54–74 . html</p></div></div>    
</body>
</html>