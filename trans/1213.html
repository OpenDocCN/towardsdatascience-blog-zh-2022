<html>
<head>
<title>Geometric Priors I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几何先验 I</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/geometric-priors-i-cc9dc748f08#2022-03-28">https://towardsdatascience.com/geometric-priors-i-cc9dc748f08#2022-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1c4f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/geometric-deep-learning" rel="noopener" target="_blank">几何深度学习</a></h2><div class=""/><div class=""><h2 id="4376" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">群、表示、不变性和等方差</h2></div><p id="a16a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">一系列博文，总结了 AMMI 计划的</em> <a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">几何深度学习(GDL)课程</em></a><em class="lk">；</em> <a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">非洲机器智能硕士</em> </a> <em class="lk">，授课老师</em> <a class="ae ll" href="https://scholar.google.com/citations?user=UU3N6-UAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> <em class="lk">迈克尔·布朗斯坦</em> </a> <em class="lk">，</em> <a class="ae ll" href="https://cims.nyu.edu/~bruna/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">琼·布鲁纳</em> </a> <em class="lk">，</em> <a class="ae ll" href="https://tacocohen.wordpress.com/" rel="noopener ugc nofollow" target="_blank"> <em class="lk">塔科·科恩</em> </a> <em class="lk">，以及</em><a class="ae ll" href="https://petar-v.com/" rel="noopener ugc nofollow" target="_blank">T35】佩塔尔·韦利奇科维奇</a></p><p id="6f8c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">维数灾难是高维学习中的一个重大挑战。这篇帖子讨论了几何深度学习(GDL)中的一个主要概念，叫做<em class="lk">几何先验</em> ( <em class="lk">对称性和尺度分离</em>)。几何先验让我们能够解决维数灾难。我们将看到<em class="lk">域</em>和<em class="lk">信号</em>在这些域上的概念。我们将讨论<em class="lk">对称</em>以及它们是如何出现在 ML 和 GDL 中的。我们还将看看一些潜在的数学概念，如<em class="lk">群表示、不变量和等价变量</em>。最后，我们将看到如何在深度学习中使用<em class="lk">不变&amp;等变网络</em>的概念。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/85f312e57348346337ac4c1f98580d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5MWlAwS4GzpjsbZXJFa6Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">立方体的旋转对称性(Oₕ).群图片来自 GDL 课程第三讲。</p></figure><p id="df27" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">参见我上一篇文章</em> s <em class="lk">上一篇</em> <a class="ae ll" rel="noopener" target="_blank" href="/geometric-deep-learning-da09e7c17aa3"> <em class="lk"> Erlangen 程序的 ML </em> </a> <em class="lk">和</em> <a class="ae ll" rel="noopener" target="_blank" href="/high-dimensional-learning-ea6131785802"> <em class="lk">高维学习</em> </a> <em class="lk">。这些博文是根据四位导师的</em> <a class="ae ll" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank"> <em class="lk"> GDL 原书</em> </a> <em class="lk">和</em> <a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> <em class="lk"> GDL 课程</em></a><em class="lk">at</em><a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"><em class="lk">AMMI</em></a><em class="lk">改编的。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="dd74" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span><a class="ae ll" rel="noopener" target="_blank" href="/high-dimensional-learning-ea6131785802"><em class="lk">上一篇关于高维度学习的文章</em> </a>中，我们看到，由于维数灾难，在没有假设的情况下，高维度的学习是不可能的，也就是说，我们的学习问题所需的样本数量随着维数呈指数增长。我们还介绍了主要的几何函数空间，其中我们在高维空间中的点可以认为是低维几何域上的<em class="lk">信号</em>。从这个假设出发，并且为了使<em class="lk">学习</em>易于理解，我将呈现<em class="lk">对称性</em>(在这篇文章中)和<em class="lk">标度分离</em>(在下一篇文章中)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ms"><img src="../Images/4e876ebeba843b5fe4c993786c4de893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kbzUY1FdzyhEXHWOuNasw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">维度的诅咒。图片来自 GDL 课程第三讲。</p></figure><p id="b648" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，我们还讨论了我们需要注意的三种误差，即<em class="lk">近似误差、统计误差和优化误差。</em>如果我们的函数类减少(我们试图估计的真实函数远在这个类之外)，近似误差会增加，这表明有一个大的函数类。相比之下，统计误差意味着我们不可能基于有限数量的数据点找到真正的函数。随着函数类的增长，这种错误会增加。最后，我们有优化误差，它与我们在假设类中找到最优假设的程度有关(当经验风险最小化可以有效地解决时，误差很小)。</p><p id="2840" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们将会看到，通过使用<em class="lk">几何先验</em>和<em class="lk">等变网络</em>:尊重我们问题的<em class="lk">对称性</em>的网络，我们可以减少我们假设类的大小或复杂性。通过这样做，我们可以在不放弃有用假设的情况下减少统计误差，这意味着近似误差不会更差。(普适性结果:如果我们的函数类是等变函数的普适逼近器)。</p><h1 id="1c1f" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">几何域</h1><p id="0d0d" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>几何深度学习(GDL)中，数据存在于由ω表示的<em class="lk">域</em>中。域包括网格、组、图和流形(量规&amp;测地线)。定义域也是一个集合(在典型的数学中)，但是它可能有一种不同的<em class="lk">结构</em>。我们可以有一个<em class="lk">邻域结构</em>(在网格和图形中)。我们也可以有一个出现在流形中的<em class="lk">度量结构</em>(点之间的距离),因为我们可以测量表面上点之间的距离。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nq"><img src="../Images/4ed72aae722c40b0d12df8498f22ac55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9wmSaWj1l8a4Q4Y11mfcg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">几何领域:5 Gs。图片来自 GDL 课程第三讲。</p></figure><blockquote class="nr"><p id="b7cc" class="ns nt iq bd nu nv nw nx ny nz oa lj dk translated">GDL 的关键信息是，我们希望设计处理几何数据的神经网络，这些网络应该尊重域结构，无论它可能有许多选择。</p></blockquote><h2 id="b6d7" class="ob mu iq bd mv oc od dn mz oe of dp nd kx og oh nf lb oi oj nh lf ok ol nj iw bi translated"><strong class="ak">几何域上的信号空间</strong></h2><p id="481e" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>大多数情况下数据不是域本身，而是域上的<em class="lk">信号</em>。ω域上的信号是一个函数𝓍，它输入ω域上的一个元素，并将其作为向量空间𝒞中的一个向量输出，即𝓍∶ω→𝒞(𝒞的维数称为<em class="lk">通道</em>)。我们也可以把ω上的𝒞-valued 信号空间定义为所有函数(该空间上的各种信号)的集合，即𝒳(ω，𝒞)={𝓍∶ω→𝒞}.</p><p id="b69c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，如果我们的域是一个网格，数学上ω=ℤₙ×ℤₙ，其中ℤₙ是从 0 到 n-1 的整数集合。该信号是采用诸如(0，1)的网格点并将其映射到 rgb 值的空间，即信号𝓍∶ω→ℝ)的函数。类似地，在图中，我们的域是从 1 到 n 的节点集。对于分子图，信号将是从该图到𝑚维度的向量空间的函数，其中𝑚是我们关心的原子的数量(对于每个节点，我们分配一个热向量，该向量告诉该节点的原子类型)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi om"><img src="../Images/33a46ed5835bb7c05e465bf18c6274f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKLy8X3o5IGcZPYl1vIvGw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">网格和图形域中的信号。图片来自 GDL 课程第三讲。</p></figure><h2 id="1dc8" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">希尔伯特空间结构</strong></h2><p id="ce45" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">正如</span>所见，域可以是一个向量空间<em class="lk"> n </em>，比如用于建模图像的连续平面ℝ。所以我们可以在这个域里加点。但是在某些领域，我们没有这个向量空间(例如，在图中，我们没有任何节点添加的概念)。尽管如此，信号空间<em class="lk">总是</em>有一个向量空间结构，在这个结构中，我们可以将两个信号相加并乘以标量。从数学上讲，这由以下等式定义:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/6696228d8ea036628e6c1b46630042d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ydl6Vx1SQxPDcq2nh0e2OA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">信号的加法和标量乘法。来自 GDL 原型书第三章的方程式。</p></figure><p id="b2b8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">换句话说，如果我们有两个信号𝓍和𝓎(两个图像)，并且我们有两个标量<em class="lk"> α </em> &amp; <em class="lk"> β </em>，我们可以形成信号<em class="lk"> α </em> 𝓍 + <em class="lk"> β </em> 𝓎，并在点𝑢.对其求值这种加法是逐点定义的(在𝑢评估第一个函数，在𝑢评估第二个函数，将这些数与<em class="lk"> α </em>和<em class="lk"> β </em>线性组合，并将它们相加)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ot"><img src="../Images/6054680b7e0209a98e04220655874aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MV0Qd79H78NV5YZZ0Ujzkg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">两个信号的相加。图片来自 GDL 课程第三讲。</p></figure><p id="1426" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果信号空间有无穷多个点，那么它就是一个向量空间。那么，这将是一个无限维的向量空间(就计算而言，我们必须处理有限的东西)。而且，在这个向量空间中定义一个内积会给我们<em class="lk">希尔伯特</em>空间。</p><p id="9bc7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">给定一个内积〈 𝓋，𝓌〉on 𝒞 (𝓋，𝓌 ∈ 𝒞)和ω上的一个测度𝜇(关于它我们将定义积分)，我们通过下式得到𝒞𝒳(ω上的一个内积:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/f5726c36b1e14594bd0c6a049240c4b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHvvs6QUP17qBoZXvIL35Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">𝒞𝒳(ω上的内积定义)(关于测度𝜇).的积分)来自 GDL 原型书第三章的方程式。</p></figure><p id="95f5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">测量𝜇允许测量ω的集合和子集的大小。在大多数情况下，这种数学设置归结起来相当简单。具体来说，如果ω是一个有限集，那么<a class="ae ll" href="https://en.wikipedia.org/wiki/Counting_measure#:~:text=In%20mathematics%2C%20specifically%20measure%20theory,if%20the%20subset%20is%20infinite." rel="noopener ugc nofollow" target="_blank"> <em class="lk">计数度量</em> </a>可以用来计算积分，这意味着我们对ω上的点求和，所以当我们做内积时，我们只对向量空间的维数求和。</p><blockquote class="nr"><p id="a1f6" class="ns nt iq bd nu nv ou ov ow ox oy lj dk translated">我们需要内积的原因是我们想做<em class="oz">模式匹配</em>当我们使用线性/conv 层时，我们需要以某种方式比较我们的信号𝓍和滤波器𝓎.</p></blockquote><h2 id="e27c" class="ob mu iq bd mv oc od dn mz oe of dp nd kx og oh nf lb oi oj nh lf ok ol nj iw bi translated"><strong class="ak">几何特征领域</strong></h2><p id="c29d" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">到目前为止，我们已经看到信号𝓍是𝒞).域ω上的函数(取ω的一个元素𝑢并将其映射到向量𝑥(𝑢)稍微一般一点的东西，在物理学上叫做<em class="lk">场</em>，在数学上叫做(“<em class="lk">丛</em>”)。<em class="lk">字段</em>是一个函数的略微一般化，其中不是将ω中的𝑢映射到固定空间𝒞中的向量𝓍(𝑢，而是将其映射到向量空间𝒞的元素，该元素由𝑢进行子索引，即𝓍(𝑢) ∈ 𝒞 <em class="lk"> ᵤ </em>。例如，网格上的向量场为流形中的每个点分配一个向量，该向量位于该点的切平面上。所以流形上的每个点都有一个切面。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pa"><img src="../Images/3e3ed2dbaea03749a252d7273adf60de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HG53l1-COHP9lwh-gZMG6w.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">流形的向量场和切空间。图片来自 GDL 课程第三讲。</p></figure><blockquote class="nr"><p id="8dcc" class="ns nt iq bd nu nv nw nx ny nz oa lj dk translated">为了简单起见，我们现在只讨论𝒞𝒳(ω的函数空间。</p></blockquote><h2 id="747c" class="ob mu iq bd mv oc od dn mz oe of dp nd kx og oh nf lb oi oj nh lf ok ol nj iw bi translated"><strong class="ak">域为数据</strong></h2><p id="1124" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>有时候域上没有信号，但是<em class="lk">域本身</em>就是数据。这出现在网格、没有节点或边特征的图形以及点云中。有一些方法可以处理这个问题，比如我们可以用<em class="lk">图邻接矩阵</em>作为ω×ω上的信号。在流形或网格的情况下，我们可以使用<em class="lk">度量张量，</em>网格固有的量，它可以被视为信号。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d1d91274b7da8aa134d11119cae9c890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HGRgGq1FCpycbDd-Dgmgmg.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">域本身就是数据。图片来自 GDL 课程第三讲。</p></figure><h1 id="dc31" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated">对称性</h1><p id="248c" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">这里是我们岗位的核心。为了解释物体的对称性，让我们以三角形为例。我们可以对三角形做几个<em class="lk">不改变它</em>的变换。我们可以将它旋转 120 度，这将只是用另一个点替换三角形中的每个点，而不会改变整体形状。我们也可以在垂直线翻转它，它不改变三角形。这些变换叫做<em class="lk">对称</em>。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pc"><img src="../Images/fb4efda2f42ab592ab29f5c23612c0e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJ-EQpdzUR1f6gD0eThLRw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">三角形的对称性(由 120 度旋转 R 和翻转 F 产生)。图片来自 GDL 课程第三讲。</p></figure><blockquote class="nr"><p id="fab4" class="ns nt iq bd nu nv nw nx ny nz oa lj dk translated">一个对象的对称性是该对象保持不变的变换。</p></blockquote><p id="5ca3" class="pw-post-body-paragraph ko kp iq kq b kr pd ka kt ku pe kd kw kx pf kz la lb pg ld le lf ph lh li lj ij bi translated">我们将看看 ML 中出现的两种对称，<em class="lk">参数化的对称和标签函数的对称</em>。在参数化的<em class="lk">对称性中，设输入空间为𝒳，输出空间为𝒴(一个标签空间)，权重空间为𝒲(一个参数空间)，模型为𝑓，一个从输入空间𝒳和权重空间𝒲到标签空间𝒴的映射，即<em class="lk">f</em>:𝒳×𝒲→1。我们可以说，将权重𝓌映射到其自身的变换𝔤，即𝔤: 𝒲 → 𝒲，是参数化的对称，如果:</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/51acce6d8d47eba77ba928265b25375a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRahwKINCuCLyq4QVFu9jQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">参数化的对称性。来自 GDL 课程的方程式，第三讲。</p></figure><p id="d0cb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">比如交换神经网络同一层的两个神经元的进出连接，并不会改变输入输出映射<em class="lk"> f </em>(。, 𝑤).</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pi"><img src="../Images/99d0baf4c04b7429e2d599b863b26188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ng4z7ogaNM-ywYuzmjeXIQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">参数化的 S <em class="oz">对称性示例(s </em>交换两个神经元的输入和输出连接)。图片来自 GDL 课程第三讲。</p></figure><p id="1b13" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">ML 中出现的第二种对称，也是最会讲的，是标签函数的<em class="lk">对称。再次假设输入空间𝒳、输出空间𝒴和标签函数<em class="lk"> L </em>是地面实况标签函数，即从输入空间𝒳到输出空间𝒴.的映射变换𝔤是对称的，如果:</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/f4b2bd999b80fcccf5d8398d257fc54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GyjrSDjSxDH_zKkKxxyhA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">标签函数的对称性。来自 GDL 课程第三讲的方程。</p></figure><p id="2980" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">换句话说，如果我们进行转换，然后计算标签，这与只计算标签是一样的。例如，在图像分类任务中，𝔤可以是图像的旋转、平移或缩放。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pj"><img src="../Images/59b486b9404cae36796c29c8c7bf447c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-LtjS9WpJj-GYtBjqTarg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">标签函数的 S <em class="oz">对称性示例(如果我们旋转图像</em>，该类不会改变)。GDL 课程，第三讲。</p></figure><h2 id="cf7d" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">学习类≅学习对称性</strong></h2><p id="f32c" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>可以把所有的学习问题，或者至少是所有的监督学习，看作是关于对称性的学习。假设我们有两个类的输入空间𝒳。我们可以看到，𝒳中任何尊重类边界的可逆映射(它将某类中的一个点映射到该类中的另一个点)都是标号函数<em class="lk"> L </em>的对称。这意味着如果我们知道了所有的对称，我们只需要为每一个类标注一个标签！因为我们可以从一个点开始，应用一个对称很多次，到达这个类中的每一个点。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/ba86d66c164299e4b6eff843a8dffc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*iOlfUL60EpI95Y57P4ZXng.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">具有两个类的输入空间𝒳(如果我们知道<em class="oz">所有</em>l 的对称性，从某个类中的一点，我们到达该类中的每一个其他点)。图片来自 GDL 课程第三讲。</p></figure><blockquote class="nr"><p id="d2c5" class="ns nt iq bd nu nv nw nx ny nz oa lj dk translated">如果学习问题是非平凡的(如果我们相信我们需要学习来解决我们的问题)，我们不应该期望能够先验地找到完全对称群。</p></blockquote><h2 id="4d3e" class="ob mu iq bd mv oc od dn mz oe of dp nd kx og oh nf lb oi oj nh lf ok ol nj iw bi translated"><strong class="ak">结构域的对称性</strong></h2><p id="22b7" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>一大类问题中，我们称之为几何深度学习问题，对称性的来源来自于数据赖以生存的<em class="lk">域</em>。设ω表示一个几何域。变换𝔤∶ω→ω是对称的，如果它<em class="lk">保持</em>ω的结构。这些对称性的一些例子可能是:</p><ul class=""><li id="484a" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated">集合元素的排列保持了集合的成员关系。如果我们的定义域是一个集合，那么这个集合的结构就是一个集合成员关系(无论元素是不是集合的一部分，都不考虑集合中元素之间的顺序或关系)。然后，这个集合的元素的排列保持了这里的结构。</li><li id="0bd0" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">欧氏等距(旋转、平移、反射)保持了欧氏空间中的距离和角度</em>ω<em class="lk">=ℝᵈ</em>。如果我们有欧几里得空间，比方说一个平面，<em class="lk"> ℝᵈ是</em>一个度量空间(点与点之间的距离是空间结构的一部分)。然后，对称可以是欧几里得等距，即，诸如旋转、平移和反射的距离保持映射。</li><li id="d587" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">一般微分同胚保持流形ω的光滑结构。</em>如果我们把<em class="lk"> ℝ </em>或者任何流形仅仅看作一个光滑流形，我们不会以一个度量结束，所以域的任何微分同胚或者光滑翘曲都可以认为是一个对称性。</li></ul><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pz"><img src="../Images/4c3f51bc667c95c5736ac4055fe3ca18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTe-sWKqrOV7tl5aEqYp3g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">几何深度学习中对称性的例子。GDL 课程，第三讲。</p></figure><h2 id="204e" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称群</strong></h2><blockquote class="nr"><p id="083f" class="ns nt iq bd nu nv ou ov ow ox oy lj dk translated">给定物体的所有对称的集合称为对称群。</p></blockquote><p id="7c87" class="pw-post-body-paragraph ko kp iq kq b kr pd ka kt ku pe kd kw kx pf kz la lb pg ld le lf ph lh li lj ij bi translated">现在我们可以观察到一些关于对称性的东西:</p><ul class=""><li id="3681" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated"><em class="lk">恒等变换永远是对称的。</em></li><li id="1aee" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">给定两个对称变换，它们的合成(一前一后做)也是对称的。</em></li><li id="0f36" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">给定任何对称，它的逆也是对称。</em></li></ul><h2 id="0227" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">抽象群体</strong></h2><blockquote class="nr"><p id="ba96" class="ns nt iq bd nu nv ou ov ow ox oy lj dk translated"><em class="oz">群是一个集合</em> 𝔊 <em class="oz">连同一个二元运算</em> ⚬:𝔊×𝔊→𝔊 <em class="oz">称为</em>组成，<em class="oz">表示为</em> 𝔤⚬𝔥 = 𝔤𝔥(为简单起见)，<em class="oz">并满足以下公理:</em></p></blockquote><ul class=""><li id="5a5c" class="pl pm iq kq b kr pd ku pe kx qa lb qb lf qc lj pq pr ps pt bi translated"><em class="lk">关联性</em> : (𝔤𝔥) 𝔨 = 𝔤 (𝔥𝔨)适用于所有𝔤、𝔥、𝔨 ∈ 𝔊.</li><li id="1b04" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">恒等式</em>:存在唯一的𝔢 ∈ 𝔊满足𝔤𝔢 = 𝔢𝔤 = 𝔤.</li><li id="4491" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">逆</em>:对于每个𝔤 ∈ 𝔊都有一个唯一的逆𝔤⁻ ∈ 𝔊，这样。𝔤𝔤⁻ = 𝔤⁻ 𝔤 = 𝔢.</li><li id="5d05" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">结束:对于每一个𝔤，𝔥 ∈ 𝔊，我们有𝔤𝔥 ∈ 𝔊.</li></ul><p id="adf3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了简单起见，通常使用𝔊来指代组，而不仅仅是其元素的集合。注意，一个群是<em class="lk">而不是</em>必然可换的，即𝔤𝔥≠𝔥𝔤一般。具有交换运算的群称为<em class="lk">阿贝尔群。</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/85f312e57348346337ac4c1f98580d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5MWlAwS4GzpjsbZXJFa6Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">立方体的旋转对称性(Oₕ).群图片来自 GDL 课程第三讲。</p></figure><h2 id="350e" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称组，抽象组&amp;组动作</strong></h2><p id="519b" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>这一节，我们定义群论中的几个概念:</p><ul class=""><li id="d6bb" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated"><em class="lk">对称群</em>是其元素为变换𝔤∶<em class="lk">ω</em>→<em class="lk">ω的群。</em></li><li id="eafb" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">分组运算</em>是贴图(函数)的合成。</li><li id="f783" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">抽象组</em>是一组元素以及复合规则，满足组公理(如上定义)。</li><li id="d511" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">组动作</em>是<em class="lk"> </em>一个映射或函数，它获取组元素和域<em class="lk">ω</em>的元素，并将其映射到域<em class="lk">ω中的新元素。，即</em>𝔊×ω→ω(表示为(𝔤，𝑢) ↦ 𝔤𝑢).群作用应满足:𝔤(𝔥𝑢) = (𝔤𝔥) 𝑢 <em class="lk">和</em> 𝔢𝑢 = 𝑢，对于所有𝔤，𝔥 ∈ 𝔊和∈ω。</li></ul><p id="4f02" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">群体作用的例子:<em class="lk">作用于ℝ的欧几里得平面运动</em>。<em class="lk"> ℝ </em>中的欧几里得群具有带角度𝜃的旋转分量以及 x 和 y 方向的平移分量(tₓ，tᵧ)，可应用于二维点(𝓍，𝓎).</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qd"><img src="../Images/212344f1349f0d7adeef12ab2047649f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nh7RAmLrsdYLRUGc41eu8g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated"><em class="oz">作用于ℝ的欧几里得平面运动。</em> GDL 课程第三讲。</p></figure><h2 id="22fa" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">凯莱图表&amp;表格</strong></h2><p id="f340" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">如果</span>我们的群是有限且离散的，我们有两种方式来表示这个群的结构:<em class="lk">凯莱图&amp;表</em>。让我们以我们以前讨论过的三角形的对称群为例。在<em class="lk"> Cayley Diagrams </em>中，每个节点都是对象(三角形)的一个配置，但是我们也可以用一个组元素来标识它。我们可以对任何节点应用旋转或翻转，并移动到其他节点。相比之下，我们可以把群结构表示成一个<em class="lk">乘法表</em>。组中的每个元素都列在一行和一列中，表中的条目将是组元素的乘积。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qe"><img src="../Images/d63fe362fca3faebd5ec9c7de5beed12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QEZckRL2-gfRR4W20FTAJw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">凯莱图和乘法表(三角形的对称群)。GDL 课程，第三讲。</p></figure><h2 id="3b1f" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated">组的种类</h2><p id="c0eb" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">这里有一些不是有限的也不是离散的群。大体上，我们可以将群定义为两种类型，<em class="lk">离散群</em>和<em class="lk">连续群。</em> <em class="lk">离散组</em>可以是:</p><ul class=""><li id="6964" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated"><em class="lk">有限的</em>，如旋转对称的三角形。</li><li id="2e82" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">无限</em>，或<em class="lk">可数无限</em>，如整数集合的平移和无限扩展网格。</li></ul><p id="588a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，我们有<em class="lk">连续组</em>，它们可以是:</p><ul class=""><li id="c6ef" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated"><em class="lk">紧凑组</em>，如二维旋转。</li><li id="a188" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">局部紧群，</em>如连续平移和旋转平移。</li><li id="4ecb" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated"><em class="lk">非局部紧群，</em>如微分同胚的无限维群(作为数学句柄最难得到)。</li></ul><p id="bbcf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所有这些类型的群都可以是<em class="lk">可交换的</em>，例如顺序无关紧要的二维旋转群，或者是<em class="lk">不可交换的</em>，例如顺序有关的三维旋转群。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qf"><img src="../Images/06b8cbabb8ced2f99c609c7d9d96a2ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Ld2V_RSRmAKIy23C9vHPw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">各种团体。GDL 课程，第三讲。</p></figure><h2 id="c22b" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">ω作用于信号𝒳(ω的对称性，𝒞) </strong></h2><p id="561e" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">作用在域ω上的对称性也作用在ω上的信号上。对于群元素𝔤和信号𝓍，我们将𝔤对𝓍的新群作用定义为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/4a0e2747b28eeb0c0c10c1b89f20cde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqBS_bGL244IMcvrl72hng.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">𝔊对𝒞).𝒳(ω信号的行动 GDL 原型书，第 3.1 节。</p></figure><p id="a85a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">简而言之，在域中的点𝑢评估的变换信号等于在𝔤⁻ 𝑢.评估的原始信号𝓍</p><p id="7cc3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这方面的一个例子可以是翻译图像。𝔊是一个翻译小组，由𝑥和 y 两部分组成。在下图中，我们看到 bug(在右边的原图中)从左边移到了右边(<em class="lk"> t </em>是一个正数，沿 x 方向平移)。𝔤.翻译形象的价值在点𝑢的𝓍将是在点𝑢 - <em class="lk"> t，即</em> 𝔤.的原始图像中的相同值𝓍(𝑢) = 𝓍(𝑢 - <em class="lk"> t </em>。注意，如果没有<em class="lk">逆(减)，</em>我们<em class="lk">就无法满足群体行动</em>的公理。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qg"><img src="../Images/e81daee6d6c57003074d2d416a564fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ly6wgWJGCIRLdUjSYIbaeA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">翻译图像(右边:原始图像，左边:翻译图像)。GDL 课程，第三讲。</p></figure><p id="b7f2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如前所述，域可能缺少向量空间结构，但信号的空间总是具有向量空间结构。此外，我们刚刚在信号上定义的组动作是<em class="lk">线性</em>(如果我们有信号的线性组合，那么我们应用变换𝔤，这与对单个信号应用𝔤并线性组合它们是一样的)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/b98b7790b1dc5119d2f4ba566d683687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*so4hPTXPHre2gIDE85JXQQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">信号群作用的线性。GDL 原型书，第 3.1 节。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qh"><img src="../Images/f0235c5fded4296d3c784b0a4d6a6e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OgvXxGNWxBqqakkBrfxKqA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">信号上的群组动作的线性示例(两个图像)。GDL 课程，第三讲。</p></figure><h2 id="89a4" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">小组陈述</strong></h2><blockquote class="nr"><p id="52d1" class="ns nt iq bd nu nv ou ov ow ox oy lj dk translated">群𝔊的 n 维实表示是一个映射𝜌 ∶ 𝔊 → ℝⁿ*ⁿ，给每个𝔤 ∈ 𝔊指定一个可逆矩阵𝜌(𝔤，并且满足𝜌</p></blockquote><p id="9c00" class="pw-post-body-paragraph ko kp iq kq b kr pd ka kt ku pe kd kw kx pf kz la lb pg ld le lf ph lh li lj ij bi translated">例如，设群𝔊是 1 维中整数平移的集合，即𝔊 = (ℤ,+)，定义域ω=ℤ₅= { 0，1，2，3，4}(如短音频信号)，𝔤 = 𝑛对 u∈ω的作用定义为:𝑛，𝑢↦𝑛+5(mod 5)。那么，在𝒳(ω(5 维空间)上的表示就是移位算子:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qi"><img src="../Images/15290e10a67823c7a329c04517dc4c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKs_zyJMRpa8NoTbhfmI1A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">团体代表的例子。GDL 课程，第三讲。</p></figure><p id="4522" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">要验证这是群表示，我们得证明矩阵𝜌( <em class="lk"> n </em>可逆，𝜌 <em class="lk"> (n+m) = </em> 𝜌 <em class="lk"> (n)P(m)(群表示的条件)。</em>很明显，𝜌(n 的行列式)，即 det(𝜌( <em class="lk"> n </em>)，不等于 0；(det(𝜌(<em class="lk">n</em>)=det(𝜌(1))ⁿ，det(𝜌(1))不能为零，因为单位矩阵是𝜌(1)).的子矩阵因此，𝜌( <em class="lk"> n </em>是可逆的。𝜌(<em class="lk">n+m</em>)= 𝜌(<em class="lk">n</em>)𝜌(<em class="lk">m</em>)直截了当。</p><blockquote class="nr"><p id="8672" class="ns nt iq bd nu nv ou ov ow ox oy lj dk translated">重要的是要注意，如果一个组满足条件，我们可以对它有多个表示。例如，如果我们有旋转群 SO(2)，我们可以有一个作用于<em class="oz"> ℝ </em>中的点、位于圆上的点、平面上的函数/信号等的表示。在 GDL 蓝图中，每个特征空间都有一个组表示，每个组表示都可能不同。</p></blockquote><p id="fe98" class="pw-post-body-paragraph ko kp iq kq b kr pd ka kt ku pe kd kw kx pf kz la lb pg ld le lf ph lh li lj ij bi translated">在下面几节中，我们将讨论不同域中对称群的一些例子。</p><h2 id="a847" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称性:集合&amp;图形</strong></h2><p id="4827" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> S </span>从集合和图开始，对称群的一个例子是置换 s 的<em class="lk">群，即𝔊 = 𝕊ₙ(“对称群”)，定义域是<em class="lk">顶点</em>的集合，即ω=𝑉，(可能还有边)。图形的特征可以是标量、矢量或张量(𝕊ₙ).的三组表示</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qj"><img src="../Images/1fc76e5cfe18a7c06b2426b0683d3941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DAQAscpxeaB6OjTg3yXZJg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">三种图形特征(𝕊ₙ).的表示 GDL 课程，第三讲。</p></figure><p id="d429" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，我们有两种作用于图和集合的对称；<em class="lk">描述</em>的对称性和<em class="lk">对象</em>本身的对称性。首先，请注意:</p><ul class=""><li id="88f7" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated">一个图(甚至一个集合)是一个<em class="lk">抽象对象</em>(之前定义过)。</li><li id="4eb9" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">实际上，我们所能访问的只是计算机内存中的图/集合的<em class="lk">描述</em>(它有一些无关的属性，比如图/集合元素的节点顺序)。</li><li id="20e5" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">描述具有属性(例如顺序),这些属性不是对象的固有属性。</li></ul><p id="6b61" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通常，我们感兴趣的是描述的对称性，而不是物体本身的对称性。为了解释这一点，让我们讨论下图。对于中间的图，如果我们为每个节点选择一个标签，我们可以将图写成邻接矩阵。如果我们应用一个置换，我们可以得到一个不同的邻接矩阵(右边的那个)。这里是描述的对称性；我们对同一个图有不同的描述(我们希望我们的网络是不变的或等变的)。我们也可以看到这个图形本身有一种对称性。如果我们做另一个置换，例如交换节点 3 和 4，邻接矩阵将是相同的(左边的那个)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qk"><img src="../Images/d5db79a2d70395ea3b95d510abc9edf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PAMvi3vpc2espaJDNInJ5A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">D <em class="oz">描述</em>的对称性和 O <em class="oz">对象的对称性的例子。GDL 课程，第三讲。</em></p></figure><h2 id="cb06" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称性:网格</strong></h2><p id="09cb" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>网格中，定义域是<em class="lk">网格点的集合，</em>和对称群𝔊可以是<em class="lk">离散</em> <em class="lk">平移</em>、<em class="lk">旋转</em>、<em class="lk">翻转</em>。底层的连续空间可能有更多的对称性(例如缩放)。群体表征是有规律的；我们可以移动和旋转图像。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi os"><img src="../Images/79e8e1721386a1b29e6fdcb45598fc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfYLPbfrYpfazOqTv57r1g.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">常规组代表。GDL 课程，第三讲。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ql"><img src="../Images/bbbcc9755914beba10d9c56795276eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37V_XVcZEh2zzersAqycDQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图像的旋转和平移。GDL 课程，第三讲。</p></figure><h2 id="93cb" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称性:群&amp;齐次空间</strong></h2><p id="f139" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>可以将对称性推广到一般群和齐次空间的集合，其中域ω≅𝔊/ℌ，对于空间中的每两个点，至少有一个对称将一个点映射到另一个点(将在后面的帖子中涉及)。群𝔊是一个<em class="lk">局部紧群</em>。这个群的作用是传递的:对于任何𝓍，𝓎∈ω，存在𝔤 ∈ 𝔊使得𝔤.𝓍 = 𝓎.小组代表是正规的(𝜌(𝔤) 𝑥(𝑢) = 𝑥(𝔤⁻ 𝑢)).</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qm"><img src="../Images/e06c3a2db8d2447cadb9b91457f83cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KdDF7P5Vehwqrq1iE3bFg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">一般群和齐次空间。GDL 课程，第三讲。</p></figure><h2 id="c067" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">对称性:流形(测地线&amp;量规)</strong></h2><p id="4a2d" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">最后，我们有流形(测地线&amp;量规)，这也将在后面的帖子中详细介绍。群𝔊是群<em class="lk">规范变换</em>，它指的是特征空间的参考系中的变化。为了解释这一点，让我们看看下图。我们有一个向量场，它给流形上的每一个点分配一个切平面上的向量。如果我们想用数字表示矢量，我们必须在切平面上选择一个坐标系。改变这个框架将改变矢量的数值表示(这被称为规范变换)。制图表达𝜌取决于要素类型。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qn"><img src="../Images/b454fcbe4bcd4eae8e898692e8b908dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kaDbw93ONtPUM0qpw2B9Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">流形和网格。GDL 课程，第三讲。</p></figure><h2 id="0e1b" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">到目前为止的总结</strong></h2><p id="f473" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated">到目前为止，我们已经讨论了各种几何域上的信号空间及其希尔伯特空间结构。我们还看了几何先验中的几个概念，如对称性、对称群和群表示。<em class="lk">有趣的问题将是我们如何在深度学习中应用这些概念，以及我们如何建立包含它们的模型？</em></p><h1 id="2ea6" class="mt mu iq bd mv mw mx my mz na nb nc nd kf ne kg nf ki ng kj nh kl ni km nj nk bi translated"><strong class="ak">不变表示法</strong></h1><p id="fda8" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> A </span>假设我们想对字母“A”、“B”和“C”进行分类，它们可能以不同的字体和不同的方向出现(见下图)。然后，如果我们知道标签函数的对称性(在这种情况下，数字的旋转)，我们可以形成一个<em class="lk">不变表示</em>，其中同一字母的每个旋转版本由一种特征向量表示。因此，我们的学习问题变得更容易。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qo"><img src="../Images/5f43df3bc14e9c35752988e1c1cc2c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Znv_xfuJHeyXeUNt71AaRA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated"><em class="oz">数字分类问题中的不变表示。GDL 课程，第三讲。</em></p></figure><p id="5080" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，在深度学习中，过早地构建不变表示是不明智的。原因是:为了识别整个对象，我们需要首先识别这个对象的部分(这就是为什么神经网络应该是深度的)。但是如果我们让<em class="lk">的中间表示不变</em>，我们将<em class="lk">丢失关键信息</em>(看下图)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qp"><img src="../Images/3d859954c5fd1fe0c84e8e99be543c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuy-WH9_TKBXqWe5s4xCbQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated"><em class="oz">零件的不变表示将丢失关键信息(见右图)。GDL 课程，第三讲。</em></p></figure><h2 id="72a6" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">等变网络</strong></h2><p id="a4f6" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> T </span>上述问题的解决方案是使用<em class="lk">等变网络</em>，其具有以下成分:</p><ul class=""><li id="0591" class="pl pm iq kq b kr ks ku kv kx pn lb po lf pp lj pq pr ps pt bi translated">特征向量空间𝒳ᵢ。</li><li id="73c5" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">网络图层(地图)𝑓ᵢ。</li><li id="3e20" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">对称群𝔊。</li><li id="97cb" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">𝔊的𝜌ᵢ代表每个𝒳ᵢ。</li></ul><p id="6dac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最简单的例子可能是平面图像、RGB 输入(<em class="lk"> n × n </em>像素和 3 个通道)。特征空间𝒳₀将是<em class="lk"> n × n × 3 </em>。所以我们有一个<em class="lk"> n × n × 3 </em>维的𝔊群表示，它可以是平移。𝒳₁可能是具有 64 个通道的卷积层𝑓₁的输出(我们的特征空间将是<em class="lk"> n × n × 64 </em>)。这将是同一组𝔊的不同表现(翻译)。直到层数为止。如果满足下面的等式，我们说网络是等变的</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qq"><img src="../Images/d6c51c58a14ffc25db44807dd1231025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UulDXMtV5XgcQL_UukdY6Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">等方差方程。GDL 课程，第三讲。</p></figure><p id="3b9e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以从下图了解一下。如果我们从𝒳₀开始，使用𝜌₀进行变换(例如，移动图像)，然后应用第一层。如果我们首先应用图层，然后应用相同的变换𝜌₀，我们应该会得到相同的结果，但是现在通过特征空间中的表示来产生𝒳₁.如果所有的网络层都满足这个性质，我们说这个网络是<em class="lk">等变</em>。我们还可以证明，如果每一层都满足等方差，那么它们的合成也满足等方差。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi qr"><img src="../Images/883dbb354ee6ad35bc0e318328a769cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*wpXD_iY0e5wXTmYd4x73qA.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated"><em class="oz">等变网络图。GDL 课程，第三讲。</em></p></figure><h2 id="3f90" class="ob mu iq bd mv oc on dn mz oe oo dp nd kx op oh nf lb oq oj nh lf or ol nj iw bi translated"><strong class="ak">等方差作为对称一致的推广</strong></h2><p id="5776" class="pw-post-body-paragraph ko kp iq kq b kr nl ka kt ku nm kd kw kx nn kz la lb no ld le lf np lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> L </span> et 重复一下我们之前讨论过的分类问题(对数字“A”、“B”、“C”进行分类)。输入空间𝓍和𝓎(在下图中)是位于<em class="lk">轨道</em>上的两个信号(如果我们考虑字母“a”的所有旋转副本，这将形成称为“a”的<em class="lk">轨道的流形)。网络将该输入映射到由通用字母“A”符号化的特征向量。在非等变网络中，旋转版本可以被发送到特征空间中的不同点(这不可能发生，等变网络必须在整个轨道上一致地概括)。此外，等变网络已经以一种方式被一般化，即<em class="lk">符合对称性</em>。换句话说，如果𝓍和𝓎应该被映射到同一点，那么它们的变换版本也应该被映射到同一点。</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qs"><img src="../Images/bf7eda4ba8a049fe0ed560490293d4b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIPnvuopFhWvDqPU1cLCbw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">非等变网络(这不可能发生，等变网络必须在整个轨道上一致地推广)。GDL 课程，第三讲。</p></figure><blockquote class="nr"><p id="423d" class="ns nt iq bd nu nv nw nx ny nz oa lj dk translated">当我们深入研究对称和等变网络时，你可能想知道为什么不使用数据增强，它可以取代对称？</p></blockquote><p id="c99f" class="pw-post-body-paragraph ko kp iq kq b kr pd ka kt ku pe kd kw kx pf kz la lb pg ld le lf ph lh li lj ij bi translated">当然，数据增强在 ML 中应用广泛，简单易行，非常流行，但也有各种优缺点。支持等方差的最突出的论点，在某类问题中，我们有对称性，比如医学图像，真的有平移和旋转。在这些情况下，我们看到等变网有更好的性能。另一个相关的例子是，如果我们有一个大的对称群(如置换群)，实际上不可能使用数据扩充。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="da4e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> T </span>他的帖子讨论了<em class="lk">几何域</em>、<em class="lk">信号</em>，以及几何先验的第一要素(<em class="lk">对称性</em>)。我们还看到了对称背后的几个数学思想，以及几何领域的例子，以及它们如何用于深度学习。下一篇文章将介绍几何先验的第二个要素(<em class="lk">尺度分离</em>)。然后，我们将看到完整的<em class="lk">几何深度学习</em> ( <em class="lk"> GDL)蓝图。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="ad01" class="mt mu iq bd mv mw qt my mz na qu nc nd kf qv kg nf ki qw kj nh kl qx km nj nk bi translated">参考</h1><ul class=""><li id="0f08" class="pl pm iq kq b kr nl ku nm kx qy lb qz lf ra lj pq pr ps pt bi translated"><a class="ae ll" href="https://geometricdeeplearning.com/lectures/" rel="noopener ugc nofollow" target="_blank"> GDL 球场</a>，(<a class="ae ll" href="https://aimsammi.org/" rel="noopener ugc nofollow" target="_blank"> AMMI </a>，2021 年夏季)。</li><li id="1b1e" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">T. Cohen 的第 3 讲[ <a class="ae ll" href="https://youtu.be/fWBrupgU4X8" rel="noopener ugc nofollow" target="_blank">视频</a> | <a class="ae ll" href="https://bit.ly/3s1PACv" rel="noopener ugc nofollow" target="_blank">幻灯片</a>。</li><li id="d15c" class="pl pm iq kq b kr pu ku pv kx pw lb px lf py lj pq pr ps pt bi translated">米（meter 的缩写））m .布朗斯坦、j .布鲁纳、t .科恩和 p .韦利奇科维奇，<a class="ae ll" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank">几何深度学习:网格、组、图形、测地线和量规</a> (2021)。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="4973" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">我很感谢拉米·阿迈德、汉尼斯·斯特尔克和 M·埃尔法提赫·萨拉赫审阅了这篇文章并提供了有益的评论。</em></p></div></div>    
</body>
</html>