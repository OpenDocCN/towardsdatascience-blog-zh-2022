<html>
<head>
<title>Stop Using Elbow Method in K-means Clustering, Instead, Use this!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 K-means 聚类中不要再用肘法了，取而代之，用这个！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d#2022-11-17">https://towardsdatascience.com/elbow-method-is-not-sufficient-to-find-best-k-in-k-means-clustering-fc820da0631d#2022-11-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cd36" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何在 K-均值聚类中查找聚类数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e63f16578e099928702a3ae2905cb800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kaGK7mO81QqUcvKHOIFPxw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:Unsplash</p></figure><p id="cf91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> K-means 聚类</strong>是数据科学领域使用最多的聚类算法之一。为了成功实现 K-means 算法，我们需要确定使用 K-means 创建的聚类的数量。</p><p id="e904" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博客中，我们将探讨——对于你的 K-means 聚类算法来说，找到聚类数(或 K)的最<strong class="la iu">实用的方法是什么。</strong></p><p id="23f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">而肘法不是答案！</strong></p><p id="b642" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是我们将在本博客中涉及的主题:</p><ol class=""><li id="5160" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="md">什么是 K 均值聚类？</em></li><li id="fc9a" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><em class="md">什么是肘法及其弊端？</em></li><li id="f7a0" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><em class="md">如何求 K-means 中‘K’的值？</em></li><li id="3d94" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><em class="md"> Python 实现</em></li><li id="61e4" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><em class="md">结论</em></li></ol><p id="3f0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们开始吧！</p><h2 id="aecb" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">什么是 K-means 聚类？</h2><p id="5ea5" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated"><strong class="la iu"> K-means </strong>聚类是一种<strong class="la iu">基于距离的无监督聚类算法</strong>，其中彼此接近的数据点被分组到给定数量的聚类/组中。</p><p id="7154" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是 K-means 算法遵循的步骤:</p><ol class=""><li id="db63" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">初始化</strong>‘K’，即<strong class="la iu">要创建的集群数量</strong>。</li><li id="cddf" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated">随机<strong class="la iu">分配 K 个质心</strong>点。</li><li id="60f7" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><strong class="la iu">将</strong>每个<strong class="la iu">数据点分配到其最近的质心</strong>以创建 K 个聚类。</li><li id="4080" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><strong class="la iu">使用新创建的聚类重新计算质心</strong>。</li><li id="e73f" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><strong class="la iu">重复步骤 3 和 4 </strong>，直到质心固定。</li></ol><h2 id="ce18" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">什么是肘法及其弊端？</h2><p id="7bd3" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated"><strong class="la iu">肘方法是在 K 均值聚类中寻找最佳“K”</strong>的图形表示。其工作原理是寻找 WCSS(类内平方和)，即类中各点与类质心之间的平方距离之和。</p><p id="748c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">肘形图显示了对应于不同 K 值(在 x 轴上)的 WCSS 值(在 y 轴上)。当我们在图中看到一个<strong class="la iu">弯头形状时，我们选择创建弯头的 K 值</strong>。我们可以把这个点叫做<strong class="la iu">肘点</strong>。超过拐点后，增加“K”值不会导致 WCSS 显著降低。</p><p id="7232" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">肘部曲线预计是这样的😊</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/484e154bf8c49a0d8c1792e1971d0d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVzV1Q9nPREcnABJBD1zDg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预期肘曲线(图片由作者提供)</p></figure><p id="6964" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">什么样子！😒</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c10b25d8152914f38c6f1d933c5d3eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-VAFayGQhQE88LTSVFlSNQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际肘部曲线(图片由作者提供)</p></figure><p id="8c93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在大多数真实世界的数据集中，使用肘方法识别正确的“K”并不十分清楚。</p><h2 id="002f" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">那么，我们如何在 K-means 中找到“K”呢？</h2><p id="eed6" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在肘法不显示肘点的情况下，侧影评分是一个非常有用的求 K 数的方法。</p><p id="59b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">剪影得分的值范围从-1 到 1 </strong>。以下是剪影配乐解读。</p><ul class=""><li id="7041" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt ni ma mb mc bi translated"><strong class="la iu"> 1: </strong>点被<strong class="la iu">完美地分配在一个簇</strong>中，簇很容易区分。</li><li id="bc55" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt ni ma mb mc bi translated"><strong class="la iu"> 0: </strong>簇是<strong class="la iu">重叠</strong>。</li><li id="3b36" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt ni ma mb mc bi translated"><strong class="la iu"> -1: </strong>点被<strong class="la iu">错误分配到</strong>簇中。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/27d2c732332165c0f3b366337dbbb264.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*CRbxRQbT8y2whvr3RqBtEw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个聚类的轮廓分数</p></figure><p id="4a62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">剪影得分= (b-a)/max(a，b) </strong></p><p id="074f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中，a=平均类内距离，即类内每个点<strong class="la iu">之间的平均<strong class="la iu">距离</strong>。</strong></p><p id="5acc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">b=平均簇间距离，即所有簇之间的平均<strong class="la iu">距离</strong> <strong class="la iu">。</strong></p><h2 id="b842" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">Python 实现</h2><p id="aad6" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">让我们使用虹膜数据集来比较<strong class="la iu">肘部曲线</strong>和<strong class="la iu"/><strong class="la iu">轮廓得分</strong>。</p><p id="553b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用以下代码可以创建<strong class="la iu">弯头曲线</strong>:</p><pre class="kj kk kl km gt nk nl nm bn nn no bi"><span id="65b2" class="np mk it nl b be nq nr l ns nt">#install yellowbrick to vizualize the Elbow curve<br/>!pip install yellowbrick  <br/><br/>from sklearn import datasets<br/>from sklearn.cluster import KMeans<br/>from yellowbrick.cluster import KElbowVisualizer<br/><br/># Load the IRIS dataset<br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target<br/><br/># Instantiate the clustering model and visualizer<br/>km = KMeans(random_state=42)<br/>visualizer = KElbowVisualizer(km, k=(2,10))<br/> <br/>visualizer.fit(X)        # Fit the data to the visualizer<br/>visualizer.show()        # Finalize and render the figure</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4b81be79ceac953dd96cef7d270b0681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*BCdr-vPIEjG1VmHUHqUMUA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弯头图在 K=4 处找到弯头点</p></figure><p id="9d6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图在 K=4 处选择了一个弯头点，但是 K=3 看起来也是一个合理的弯头点。所以，<strong class="la iu">不清楚肘点</strong>应该是什么。让我们使用轮廓图验证<strong class="la iu"> K 的值(使用以下代码)。</strong></p><pre class="kj kk kl km gt nk nl nm bn nn no bi"><span id="8636" class="np mk it nl b be nq nr l ns nt">from sklearn import datasets<br/>from sklearn.cluster import KMeans<br/>import matplotlib.pyplot as plt<br/>from yellowbrick.cluster import SilhouetteVisualizer<br/><br/># Load the IRIS dataset<br/>iris = datasets.load_iris()<br/>X = iris.data<br/>y = iris.target<br/>  <br/>fig, ax = plt.subplots(3, 2, figsize=(15,8))<br/>for i in [2, 3, 4, 5]:<br/>    '''<br/>    Create KMeans instances for different number of clusters<br/>    '''<br/>    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)<br/>    q, mod = divmod(i, 2)<br/>    '''<br/>    Create SilhouetteVisualizer instance with KMeans instance<br/>    Fit the visualizer<br/>    '''<br/>    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])<br/>    visualizer.fit(X) </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/a5e27190984e21dcbb0f089cd8b4c21f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEVkyTxpjvbgrINiOaSkqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">K = 2 到 5 的轮廓图(图片由作者提供)</p></figure><p id="5d1b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 K = 2，轮廓分数是最大的(0.68)，但是这不足以选择最佳的 K。</p><p id="0380" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">应检查以下<strong class="la iu">条件</strong><strong class="la iu">以使用剪影图选择正确的‘K’</strong>:</p><ol class=""><li id="d66a" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">对于特定的 K，<strong class="la iu">所有的聚类都应该具有大于数据集</strong>的平均分数的轮廓分数(由红色虚线表示)。x 轴代表轮廓分数。K = 4 和 5 的聚类被消除，因为它们不符合这个条件。</li><li id="e15e" class="lu lv it la b lb me le mf lh mg ll mh lp mi lt lz ma mb mc bi translated"><strong class="la iu">星团的大小不应该有大的波动</strong>。聚类的宽度代表数据点的数量。对于 K = 2，蓝色簇的宽度几乎是绿色簇的两倍。对于 K = 3，这个蓝色簇被分解成 2 个子簇，从而形成大小一致的簇。</li></ol><p id="6e34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，<strong class="la iu">轮廓图方法给出了 K = 3 作为最佳值</strong>。</p><p id="e585" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 Iris 数据集上的最终聚类，我们应该选择 K = 3。</p><pre class="kj kk kl km gt nk nl nm bn nn no bi"><span id="4f0f" class="np mk it nl b be nq nr l ns nt">import plotly.graph_objects as go  #for 3D plot<br/><br/>## K-means using k = 3<br/>kmeans = KMeans(n_clusters=3)<br/>kmeans.fit(X)<br/>y_kmeans = kmeans.predict(X)<br/><br/>## 3D plot <br/>Scene = dict(xaxis = dict(title  = 'sepal_length --&gt;'),yaxis = dict(title  = 'sepal_width---&gt;'),zaxis = dict(title  = 'petal_length--&gt;'))<br/><br/>labels = kmeans.labels_<br/>trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))<br/>layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)<br/>data = [trace]<br/>fig = go.Figure(data = data, layout = layout)<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/363be00d60dc1b163e340c782abf6b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjChaWB2cY_ph50j8rycYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">聚类的三维图(图片由作者提供)</p></figure><p id="31b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还<strong class="la iu">通过索引/检查输入特征</strong>在集群内的分布来验证输出集群<strong class="la iu">。</strong></p><div class="nx ny gp gr nz oa"><a href="https://pub.towardsai.net/expectation-maximization-em-clustering-every-data-scientist-should-know-2b47fbd0dbc0" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">期望最大化(EM)聚类:每个数据科学家都应该知道</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">EM 聚类技术最直观的解释</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">pub.towardsai.net</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><h2 id="9da7" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">结论</h2><p id="0784" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">肘曲线和剪影图都是为 K-均值聚类寻找最佳 K 的非常有用的技术。在现实世界的数据集中，你会发现很多肘曲线不足以找到正确的“K”的情况。在这种情况下，您应该使用轮廓图来计算数据集的最佳聚类数。</p><p id="8331" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议您结合使用这两种技术来计算 K 均值聚类的最佳 K 值。</p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><h1 id="6b9a" class="ow mk it bd ml ox oy oz mo pa pb pc mr jz pd ka mu kc pe kd mx kf pf kg na pg bi translated">谢谢大家！</h1><p id="ab4c" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">你可以在收件箱里看到我所有的帖子。 <a class="ae ph" href="https://anmol3015.medium.com/subscribe" rel="noopener"> <strong class="la iu"> <em class="md">做到这里</em> </strong> <em class="md">！</em> </a> <em class="md"> <br/>如果你喜欢体验媒介的自己，可以考虑通过</em> <a class="ae ph" href="https://anmol3015.medium.com/membership" rel="noopener"> <strong class="la iu"> <em class="md">报名会员来支持我和其他成千上万的作家。它每个月只需要 5 美元，它极大地支持了我们，作家，你可以在媒体上看到所有精彩的故事。</em></strong></a></p><h2 id="6105" class="mj mk it bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated"><strong class="ak">你可能喜欢的故事！</strong></h2><div class="nx ny gp gr nz oa"><a href="https://medium.com/codex/feature-engineering-techniques-every-data-scientist-should-know-e40dc656c71f" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">每个数据科学家都应该知道的特征工程技术</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">最大限度地提高机器学习模型的性能！</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="pi l ol om on oj oo ks oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://medium.com/codex/24-powerful-must-know-pandas-functions-for-every-data-analysis-a1a9990d47c8" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">24 个强大的必知熊猫功能，用于每次数据分析</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">对每个数据科学专业人员来说，重要的熊猫功能</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="pj l ol om on oj oo ks oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://medium.com/codex/know-everything-about-bias-and-variance-7c7b9f9ee0ed" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">理解机器学习中的偏差和方差权衡</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">机器学习中一个必须知道的概念</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="pi l ol om on oj oo ks oa"/></div></div></a></div></div></div>    
</body>
</html>