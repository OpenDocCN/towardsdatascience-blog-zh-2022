<html>
<head>
<title>Build a deep neural network for the keyword spotting (KWS) task with nnAudio GPU audio processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用nAudio GPU音频处理为关键词识别(KWS)任务构建深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-deep-neural-network-for-the-keyword-spotting-kws-task-with-nnaudio-gpu-audio-processing-95b50018aaa8#2022-07-13">https://towardsdatascience.com/build-a-deep-neural-network-for-the-keyword-spotting-kws-task-with-nnaudio-gpu-audio-processing-95b50018aaa8#2022-07-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="af62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">处理音频会使任何机器学习任务变得复杂。在本教程中，我们将介绍如何在PyTorch中通过直接输入音频文件来构建神经网络，这些音频文件将直接转换为可微调的频谱图。为此，我们使用nnAudio [1]和PyTorch。</p><p id="42ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本教程将在谷歌语音命令数据集v2上为关键词识别(KWS)任务构建一个分类器。KWS是一个健全的分类问题。我们的模型将预测匹配输入声音文件的单词(文本)。在这个KWS任务中总共有12个不同的输出类。我们选择使用所有35个可用单词中的10个，其余25个单词归入“未知”类。从背景噪音中产生类别“寂静”。</p><blockquote class="kl km kn"><p id="7592" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated"><a class="ae ks" href="https://arxiv.org/pdf/1804.03209.pdf" rel="noopener ugc nofollow" target="_blank">谷歌语音命令数据集</a> v2有105829个话语，总共35个单词。该数据集包括一些词语，如“是”、“否”、“上”、“下”、“左”、“右”、“开”、“关”、“停止”和“开始”。每个话语长1秒。</p></blockquote><p id="19d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在决定这个问题的架构之前，重要的是考虑我们将如何处理音频。我们会使用储存在我们电脑上的光谱图图像，还是wavenet，还是其他什么？为了回答这个问题，让我们回到几年前，当我的博士生<a class="ae ks" href="https://scholar.google.com/citations?user=ZqlEvoMAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank">卓建伟</a>在SUTD的时候。Raven想要建立一个音频转录模型。他很快注意到，在训练模型之前，提取光谱图并将其存储在他的计算机上既麻烦又缓慢，使得不可能调整光谱图设置。因此，他开发了<a class="ae ks" href="https://ieeexplore.ieee.org/abstract/document/9174990" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">nn audio</strong></a><strong class="jp ir">【1】</strong>库，该库提供了一个有用的开源工具，可以将音频直接加载到PyTorch层，在其中音频被动态转换为声谱图表示。</p><p id="af38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">nAudio使用PyTorch 1D卷积神经网络作为其后端。因此，它优化了波形到声谱图的转换过程。这使得离散傅立叶变换的基函数可以被神经网络训练，这意味着它们可以针对手头的任务进行优化。这是可能的，因为短时傅立叶变换(STFT)和梅尔基函数被实现为神经网络的第一层(前端层)。在训练期间，模型能够将梯度反向传播到前端。因此，来自音频信号的最重要的特征可以通过定制的训练频谱图来“捕获”。正如我们将看到的，这为我们训练音频分类任务提供了一种又好又快的方法。</p><p id="cc12" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的关键词识别(KWS)教程由四部分组成:</p><ul class=""><li id="7cac" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated">第1部分:加载数据集&amp;简单的线性模型</li><li id="ba76" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">第2部分:用可训练的基函数训练线性模型</li><li id="7e12" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">第3部分:结果模型的评估</li><li id="398d" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">第4部分:使用更复杂的非线性模型</li></ul><p id="f569" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">开始吧！</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h2 id="f20d" class="lo lp iq bd lq lr ls dn lt lu lv dp lw jy lx ly lz kc ma mb mc kg md me mf mg bi translated">第1部分:加载数据集&amp;简单的线性模型</h2><p id="0d34" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">在本教程中，我们将使用光谱图。在音频深度学习中，频谱图扮演着重要的角色:连接音频文件和深度学习模型。前端工具如<a class="ae ks" href="https://librosa.org/doc/main/index.html" rel="noopener ugc nofollow" target="_blank"> librosa </a>和nAudio将音频波形(时域)转换为频谱图(时频域)，基本上可以采用与模型处理图像类似的方式进行处理。</p><p id="d126" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">加载数据集</strong></p><p id="85ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们需要从谷歌获取数据。我们使用AudioLoader来访问<a class="ae ks" href="https://github.com/KinWaiCheuk/AudioLoader/blob/master/AudioLoader/speech/speech_README.md#SpeechCommandsv2" rel="noopener ugc nofollow" target="_blank">语音命令12类数据集</a>。</p><blockquote class="kl km kn"><p id="5f3c" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated">AudioLoader 是一个一键式音频加载器，允许你一键下载、解压缩、数据分割和重新采样音频。它现在支持多个流行的数据集，如MusicNet，MusdbHQ，TIMIT。</p></blockquote><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="f870" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在传统的设置中，我们首先提取光谱图，将它们保存到我们的计算机中，然后将这些图像加载到模型中。这很慢，需要磁盘空间，并且很难根据手头的任务调整谱图特征。nAudio通过作为神经网络的一部分实时计算光谱图来解决这些问题。</p><p id="c850" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过利用PyTorch和GPU处理，nAudio可以计算不同类型的频谱图，如短时傅立叶变换(STFT)、梅尔频谱图和常数Q变换(CQT)。从原始nAudio论文[1]的下图中可以看出，在GPU上处理音频将计算时间缩短了100倍。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/66cff0605e6762b6fd6c023c836c4e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GiurCrK6Wp9qc9_A.jpeg"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">使用nAudio GPU(绿色)、nAudio CPU(橙色)和librosa(蓝色)计算不同类型光谱图的处理时间，以对数标度表示[1]。</p></figure><p id="ccf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">定义模型</strong></p><p id="5552" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将首先用一个简单的单层网络来演示我们的工作流程。模型被定义为<em class="ko">linear model _ n audio，</em>继承了类<em class="ko">speech command(lightning module)。</em>可以去<a class="ae ks" href="https://github.com/heungky/nnAudio_tutorial/blob/master/Part%201_Making%20on-the-fly%20audio%20processing%20possible.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ko">设置Lightning模块</em> </a>了解<em class="ko">父类的更多详情。</em></p><blockquote class="kl km kn"><p id="6fef" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated"><em class="iq"/><a class="ae ks" href="https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html" rel="noopener ugc nofollow" target="_blank"><em class="iq">闪电模块</em> </a> <em class="iq">是PyTorch闪电中的一个模块。它可以帮助您将PyTorch代码组织成6个部分，包括训练循环(training_step)、测试循环(test_step)、优化器和lr调度器(configure _ optimizers)。</em></p></blockquote><p id="ef75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个项目中，我们选择使用Mel-spectrogram，因为这些声谱图箱被缩放以匹配人类听觉频谱。因此，它们可能很好地代表了我们人类所获得的特征。</p><p id="1cb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个简单的模型将声音文件(x)作为输入。然后我们使用Linearmodel_nnAudio中的方法<em class="ko">n naudio . features . Mel . Mel spectrogram()</em>作为神经网络的第一层，将声音文件从音频波形转换成频谱图。</p><p id="415c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出于演示的目的，我们只定义了一个简单的模型，这里有一个额外的线性层。此KWS分类任务的输出包含12个类，因此图层的输出大小应为12。</p><p id="e962" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">nAudio的结果代码如下。请注意，<em class="ko">nnaudio . features . Mel . Mel spectrogram()</em>函数有许多附加参数，如果需要，这些参数将允许您对光谱图进行精细控制。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">带nnAudio的线性模型</p></figure><p id="9c0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练模型一个历元</strong></p><p id="2d56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面是用PyTorch Lightning为我们的简单线性模型训练1个时期的代码。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div><p class="na nb gj gh gi nc nd bd b be z dk translated">用nnAudio训练线性模型</p></figure><p id="0c53" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练上述模型导致完成一个历元的计算时间为17s，这比我们使用librosa快了大约95倍。</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h2 id="a849" class="lo lp iq bd lq lr ls dn lt lu lv dp lw jy lx ly lz kc ma mb mc kg md me mf mg bi translated">第2部分:用可训练的基函数训练线性模型</h2><p id="a9d0" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">在本节课中，我们将演示如何利用nAudio的可训练基函数来构建一个强大的分类器，在反向传播过程中，光谱图实际上会根据手头的任务进行微调。</p><p id="f039" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">设置基础功能</strong></p><p id="3ed1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在nAudio中，我们可以使傅立叶内核可训练，这允许它在反向传播期间根据我们手头的特定任务进行调整。</p><p id="0c5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以在下面的函数中修改Mel-spectrogram参数，用<em class="ko">可训练_mel </em>和<em class="ko">可训练_STFT </em>控制Mel和STFT基础是否可训练。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="f37f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练模特</strong></p><p id="d134" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用一个相当标准的训练循环，在此期间，训练好的模型权重将保存在<em class="ko"> lighting_logs </em>文件夹中。在本教程的下一步中，我们将进一步了解该模型的性能。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h2 id="9221" class="lo lp iq bd lq lr ls dn lt lu lv dp lw jy lx ly lz kc ma mb mc kg md me mf mg bi translated">第3部分:结果模型的评估</h2><p id="3068" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">您已经训练了一个线性模型，但现在是时候评估模型性能并进行一些可视化了。</p><p id="3243" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">将预训练的重量加载到模型上</strong></p><p id="ada0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每次训练一个模型，光照模块都会将训练好的权重保存在<em class="ko"> lightning_logs </em>文件夹中。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="bb60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">评估模型性能</strong></p><p id="6c73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以使用测试集上的以下度量来评估KWS任务上的模型性能:</p><ul class=""><li id="f862" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk ky kz la lb bi translated">测试/损失(交叉熵)</li><li id="b55a" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">测试/acc(精确度)</li><li id="8eaf" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">F1矩阵(F1分数)</li><li id="ee3f" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk ky kz la lb bi translated">困惑_矩阵</li></ul><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="1409" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">测试集的最终精度显示如下，其中的设置略有不同，以允许内核可训练。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/45a66633b871ceb6fd02467d79d3aec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vDVY-BoG0DUkAHXN.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">具有不同可训练基函数设置的线性模型的KWS任务中的测试/加速</p></figure><p id="2978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当查看这些结果时，请记住我们有一个12类问题，因此随机预言将有8.3%的准确性。从上表中，我们可以看到，通过使用可训练的基函数，与简单的线性模型相比，我们可以将KWS任务的准确性提高14.2个百分点。进一步调整超参数无疑会进一步提高性能。</p><h2 id="dfd7" class="lo lp iq bd lq lr ls dn lt lu lv dp lw jy lx ly lz kc ma mb mc kg md me mf mg bi translated">想象结果(奖金)</h2><p id="5923" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">当权重存储在我们的检查点文件中时，我们可以在我们的第一层nnAudio中可视化一些已学习的内核。检查点文件内部的结构如下所示:</p><pre class="mm mn mo mp gt ne nf ng nh aw ni bi"><span id="343d" class="lo lp iq nf b gy nj nk l nl nm">weight=torch.load('xxxx/checkpoints/xxxx.ckpt')<br/>├── epoch<br/>├── global_step<br/>├── pytorch-lightning_version<br/>│     <br/>├── state_dict<br/>│     ├─ mel_layer.mel_basis<br/>│     ├─ mel_layer.stft.wsin<br/>│     ├─ mel_layer.stft.wcos<br/>│     ├─ mel_layer.stft.window_mask   <br/>│     ├─ linearlayer.weight<br/>│     ├─ linearlayer.bias<br/>│     │<br/>│     <br/>├── callbacks<br/>├── optimizer_states<br/>├── lr_schedulers</span></pre><p id="d8bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ko"> state_dict </em>是检查点文件中的字典键之一。它是一个有序的检测，包括基函数(如梅尔箱，STFT) <strong class="jp ir"> </strong>和层权重(在这种情况下是线性层)<strong class="jp ir">的训练权重。</strong></p><p id="d342" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可视化蜂蜜罐</p><p id="e8c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ko"> mel_layer.mel_basis </em>的形状应该是[n_mels，(n_fft/2+1)]，其中n_mels是mel仓的数量，n_fft是指用零填充后的窗口信号的长度。在本教程示例中，<em class="ko"> mel_layer.mel_basis </em>的形状为【40，241】。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="1e3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是不可训练的Mel库和可训练的Mel库在200个时期的比较。40个Mel箱分别以不同的颜色显示如下:</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/d37c46bd11dc31087ee4526f1adcb436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pcLL4hJELdiRfaCv.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">trainable _ mel = False</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nn"><img src="../Images/263b4dc42a783e5ba770f46ee05811ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d6RWpZdBoBp3UfmM.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">trainable _ mel = True</p></figure><p id="9c7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意它们是如何变化很大的，并且在我们特定的任务中特别调谐到重要的频率和模式。</p><p id="c62f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">可视化STFT </strong></p><p id="9b3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ko"> mel_layer.stft.wsin </em>和<em class="ko"> mel_layer.stft.wcos </em>的形状应该是[(n_fft/2+1)，1，n_fft]。在本教程示例中，两者的形状都是[241，1，480]。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="3511" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是不可训练的STFT和可训练的STFT在200个时代的对比。从左到右，从上到下，分别是第2、第10、第20、第50傅立叶核。</p><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi no"><img src="../Images/1eb661ba5d14e9cfac16c3b10b472ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*v1Q7Sx-1G00ogkyp.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">可训练_STFT =假</p></figure><figure class="mm mn mo mp gt mq gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi np"><img src="../Images/386a09a762f6c913d68ad465a88329ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wIzzWKtpNGqhoPRJ.png"/></div></div><p class="na nb gj gh gi nc nd bd b be z dk translated">可训练_STFT =真</p></figure></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h2 id="5446" class="lo lp iq bd lq lr ls dn lt lu lv dp lw jy lx ly lz kc ma mb mc kg md me mf mg bi translated">第4部分:在更复杂的非线性模型中使用可训练的基函数</h2><p id="3fb4" class="pw-post-body-paragraph jn jo iq jp b jq mh js jt ju mi jw jx jy mj ka kb kc mk ke kf kg ml ki kj kk ij bi translated">在完成本教程的第1–3部分后，您现在对如何将nAudio与可训练基函数结合使用有了大致的了解。在本教程的最后一部分，您可以看到如何轻松地调整模型函数，以适应任何类型的深度、复杂的神经网络，从而满足您的需求。唯一需要更改的是模型定义，如下例所示，我们在nAudio声谱图层后使用广播残差网络。</p><figure class="mm mn mo mp gt mq"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="25ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">点击这里，查看完整、更复杂的教程示例<a class="ae ks" href="https://github.com/heungky/nnAudio_tutorial/blob/master/Part%204_Using%20nnAudio%20Trainable%20Basis%20Functions%20with%20more%20complex%20non-linear%20models.ipynb" rel="noopener ugc nofollow" target="_blank">！</a></p><p id="948d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论</strong></p><p id="c556" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经通过使用nAudio [1]可训练基函数，逐步完成了使用Google语音命令数据集v2构建关键字识别模型的教程。这种方法可以很容易地修改，以适应任何音频分类任务。请在评论中告诉我们您的反馈以及您对代码的处理方式！</p><p id="ede9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">致谢<br/> </strong>感谢我的研究助理香君怡准备代码片段，感谢卓建伟创建nAudio！</p><p id="a3b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong></p><ol class=""><li id="ebfe" class="kt ku iq jp b jq jr ju jv jy kv kc kw kg kx kk nq kz la lb bi translated">K.W. Cheuk，H. Anderson，K. Agres和D. Herremans，<a class="ae ks" href="https://ieeexplore.ieee.org/abstract/document/9174990" rel="noopener ugc nofollow" target="_blank">“nnAudio:一个使用1D卷积神经网络的动态GPU音频到频谱图转换工具箱”，<em class="ko"> IEEE Access </em>，第8卷，第161981–162003页，2020年，doi:10.11109/Access .</a></li><li id="a4d6" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk nq kz la lb bi translated">对于本教程的源代码，可以参考<a class="ae ks" href="https://github.com/heungky/nnAudio_tutorial" rel="noopener ugc nofollow" target="_blank">https://github.com/heungky/nnAudio_tutorial</a></li><li id="5475" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk nq kz la lb bi translated">nAudio源代码，可以参考<a class="ae ks" href="https://github.com/KinWaiCheuk/nnAudio" rel="noopener ugc nofollow" target="_blank">https://github.com/KinWaiCheuk/nnAudio</a></li><li id="96e3" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk nq kz la lb bi translated">对于nAudio文档，您可以参考https://kinwaicheuk.github.io/nnAudio/index.html<a class="ae ks" href="https://kinwaicheuk.github.io/nnAudio/index.html" rel="noopener ugc nofollow" target="_blank">的</a></li><li id="f749" class="kt ku iq jp b jq lc ju ld jy le kc lf kg lg kk nq kz la lb bi translated">1999年，香港大学，香港理工大学，香港理工大学，香港理工大学。<a class="ae ks" href="https://arxiv.org/pdf/2204.11437.pdf" rel="noopener ugc nofollow" target="_blank">通过可训练的基本功能理解音频特征。arXiv预印本arXiv:2204.11437 。</a>(关于可训练基函数的研究文章)</li></ol></div></div>    
</body>
</html>