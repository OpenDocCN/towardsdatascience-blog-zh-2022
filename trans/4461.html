<html>
<head>
<title>How to compute text similarity on a website with TF-IDF in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用Python中的TF-IDF计算网站上的文本相似度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-compute-text-similarity-on-a-website-with-tf-idf-in-python-680b3be06091#2022-10-04">https://towardsdatascience.com/how-to-compute-text-similarity-on-a-website-with-tf-idf-in-python-680b3be06091#2022-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b108" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">一种简单有效的TF-IDF和Pandas文本相似度方法</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/b9fa5bbbb020d927f0e972a3e21b4877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*o2ai2Ch57gGjLbfY.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0701" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在数据挖掘和自然语言处理领域，计算两个文本之间的相似度是一项非常有用的活动。这既允许<strong class="kx iu">隔离异常，又允许对特定问题</strong>进行诊断，例如博客上非常相似或非常不同的文本，或者将相似的实体归入有用的类别。</p><p id="b67d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在本文中，我们将使用发布的脚本<a class="ae lr" href="https://medium.com/mlearning-ai/how-to-scrape-a-blog-and-collect-its-articles-in-python-80895d8def66" rel="noopener">来抓取博客并创建一个小型语料库，在此基础上应用基于Python中TF-IDF的相似性计算算法。</a></p><p id="1738" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">具体来说，我们将使用一个名为<a class="ae lr" href="https://trafilatura.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Trafilatura </a>的库，通过网站地图从目标网站检索所有文章，并将它们放在Pandas数据框架中进行处理。</p><p id="72b8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我邀请读者阅读我在上面链接的文章，以更详细地了解提取算法是如何工作的。</p><p id="23a8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了简单起见，在这个例子中，我们将分析diariodiunanalista.it，这是我自己用意大利语写的关于数据科学的博客，以便了解是否有彼此过于相似的文章。在这个过程中，我会诊断我自己的工作，也许会提出一些很酷的见解！</p><p id="3f82" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这在搜索引擎优化上有很大的反响——事实上，类似的文章会引发<em class="ls">内容蚕食</em>的现象:当属于同一网站的两篇文章在谷歌上争夺相同的位置。<strong class="kx iu">我们希望避免这种情况，识别这些情况是避免这种情况的第一步。</strong></p><h1 id="1f44" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">要求</h1><p id="f4ba" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">我们需要的库是Pandas，Numpy，NLTK，Sklearn，TQDM，Matplotlib和Seaborn。</p><p id="34ab" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们将它们导入到Python脚本中。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="7a95" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">此外，我们需要运行<code class="fe ms mt mu mv b">nltk.download('stopwords')</code>命令来安装NLTK停用词。停用词是对句子的意义没有重要贡献的词，我们需要它们来预处理我们的文本。</p><h1 id="3960" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">创建数据集</h1><p id="e563" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">让我们运行上面提到的文章中的软件。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/cf6e654fa5ed0a4edbd27707b8c8e12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hJE-umXWfxM1XmDq.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据收集过程。图片作者。</p></figure><p id="9cbc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们来看看我们的数据集。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/05b1384e2ccce1ec4b644288f532d38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*FuIqVkojdcQ9pcOG.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们数据集的预览。图片作者。</p></figure><p id="69b9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从URL的分类中，我们注意到所有的帖子是如何被收集在<em class="ls"> /posts/ </em>下的——这允许我们只隔离实际的文章，而忽略页面、类别、标签等等。</p><p id="882e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们使用下面的代码来应用这个选择</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/a64d91adf662f8fc8e405b8109e0c357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3fiLgDyUtUkq0Tit.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们数据集的预览。图片作者。</p></figure><p id="c534" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是我们的语料库。在写这篇文章的时候，大约有30篇文章——因此它是一个非常小的语料库。对于我们的例子来说，它仍然是好的。</p><h1 id="54bf" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">文本预处理</h1><p id="2979" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">我们将应用一些简单的文本预处理来复制一个真实的应用程序管道。这可以扩展到补充读者的要求。</p><h2 id="1848" class="nc lu it bd lv nd ne dn lz nf ng dp md le nh ni mf li nj nk mh lm nl nm mj nn bi translated">预处理步骤</h2><p id="d6f5" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">我们将应用这些预处理步骤:</p><ul class=""><li id="5008" class="no np it kx b ky kz lb lc le nq li nr lm ns lq nt nu nv nw bi translated">删除标点符号</li><li id="27c2" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">小写应用程序</li></ul><p id="8cb4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这将在一个非常简单的函数中完成，该函数使用标准的<em class="ls">字符串</em>和NLTK库。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="d8d1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">TF-IDF矢量器(我们将很快定义)将使用这个函数来规范化文本。</p><h1 id="195e" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">相似度计算算法</h1><p id="61eb" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">首先，让我们通过将停用词保存在一个变量中来定义它们</p><p id="6a77" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe ms mt mu mv b">ita_stopwords = stopwords.words('italian')</code></p><p id="0df2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我们从Sklearn导入TfIdfVectorizer，向它传递预处理函数和停用词。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="66cb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">TF-IDF矢量器会将每个文本转换成它的矢量表示。这将允许我们将每个文本视为多维空间中的一系列点。</p><p id="41ae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们计算相似度的方法是通过对构成我们正在比较的文本的向量之间的余弦值进行<strong class="kx iu">计算。相似度值介于-1和+1之间。<strong class="kx iu">值+1表示两个基本相等的文本，而-1表示完全分离。</strong></strong></p><p id="2edc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我邀请感兴趣的读者在专门的维基百科页面上阅读更多关于这个主题的内容。</p><p id="addd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我们将定义一个名为<code class="fe ms mt mu mv b">compute_similarity </code>的函数，它将使用矢量器将文本转换为数字，并应用该函数计算TF-IDF向量的相似性余弦。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="a4f9" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">让我们测试代码</h1><p id="3f3d" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">我们以意大利语维基百科的两段文字为例。</p><blockquote class="oc od oe"><p id="6f01" class="kv kw ls kx b ky kz ju la lb lc jx ld of lf lg lh og lj lk ll oh ln lo lp lq im bi translated">Figlio第二次将giudice sardo Ugone II引入Arborea和Benedetta，推动了padre的文化和政治发展，并加强了Arborea和所有其他独立地区的自治。考虑到300年的一个重要数字，促进了农业组织的发展，颁布了《农村法典》,成功地完成了包括埃莉诺拉在内的立法工作</p><p id="9079" class="kv kw ls kx b ky kz ju la lb lc jx ld of lf lg lh og lj lk ll oh ln lo lp lq im bi translated">《绿巨人》是2008年路易斯·莱特里尔导演的一部电影。爱德华·诺顿的男主角，同样为车安和扎克·佩恩的风景作出了贡献；超级英雄的首要任务是完成所有提议中的最终版本，而不是在漫威传统大学里完成所有的投资。他的个人风格是古典风格的“巨人之歌”,在世界范围内独树一帜，不是最好的暗杀。</p></blockquote><p id="cbcd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们应用<code class="fe ms mt mu mv b">cosine_similarity </code>函数来测试这两个文本有多相似。我期待一个相当低的值，因为它们处理不同的主题，并且不使用相同的术语。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/0a7a65f4d08911752df995aea9cb44f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zx4-xV_znrZX6BIS.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该算法在两段文本上的应用。图片作者。</p></figure><p id="ead7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这两个文本显示出非常低的相似性，接近于0。现在让我们用两个相似的文本来测试一下。我将把第二个文本的一部分复制到第一个文本中，保持相似的长度。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oj"><img src="../Images/d6013bcde303f80331e59671fe46e524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XuIsBFG2bpAxmb17.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">该算法在两段文本上的应用。图片作者。</p></figure><p id="f427" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">相似度现在是0.33。它似乎工作得很好。<br/>现在，我们以成对的方式将该方法应用于所有语料库。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="21af" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们详细了解这段代码是做什么的。</p><ul class=""><li id="6bd1" class="no np it kx b ky kz lb lc le nq li nr lm ns lq nt nu nv nw bi translated">我们创建一个名为<code class="fe ms mt mu mv b">M</code>的30x30矩阵</li><li id="e298" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">我们在数据帧上逐行迭代以访问<code class="fe ms mt mu mv b">article_i</code></li><li id="aff3" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">我们再次在相同的数据帧上逐行迭代，以访问<code class="fe ms mt mu mv b">article_j</code></li><li id="8c94" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">我们在<code class="fe ms mt mu mv b">article_i</code>和<code class="fe ms mt mu mv b">article_j</code>上运行compute_similarity来获得相似性</li><li id="ca11" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">我们将这个值保存在M中的位置<code class="fe ms mt mu mv b">i</code>、<code class="fe ms mt mu mv b">j</code></li></ul><p id="01aa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">m可以很容易地转换成熊猫的数据帧，这使我们能够用Seaborn建立一个热图。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ok"><img src="../Images/7d1e1c6b3f3dc4befd4af443ce60942f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UW6scS99HKjhuAdJT7GKkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">相似性热图。图片作者。</p></figure><p id="9eda" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">热图根据获得的相似性值，使用更亮或更暗的颜色突出显示异常。</p><p id="6433" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们对代码做一个小小的修改，只选择相似度大于0.40的元素。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ok"><img src="../Images/9fd532ee797910b14ad8bf9b282b088b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-fItIPh7L_JaShYbvqLCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过高于0.4的值过滤的相似性热图。图片作者。</p></figure><p id="6af3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们看到相似性指数大于0.4的4个页面。</p><p id="c3e7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们特别看到这些组合:</p><ul class=""><li id="0b97" class="no np it kx b ky kz lb lc le nq li nr lm ns lq nt nu nv nw bi translated">训练模型前要做的6件事-&gt;机器学习的最大障碍——过度拟合</li><li id="065d" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">训练模型前要做的6件事-&gt;什么是机器学习中的交叉验证</li><li id="95da" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">什么是机器学习中的交叉验证-&gt;机器学习中最大的障碍——过拟合</li><li id="6de2" class="no np it kx b ky nx lb ny le nz li oa lm ob lq nt nu nv nw bi translated">什么是机器学习-&gt;机器学习和深度学习有什么区别</li></ul><p id="26d1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些对中的一些对之间的相似性也存在于显示高相似性的其他对中。</p><p id="0cb4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些文章分享了这个主题，即机器学习和一些最佳实践。</p><h1 id="4de8" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">结论</h1><p id="10bb" class="pw-post-body-paragraph kv kw it kx b ky ml ju la lb mm jx ld le mn lg lh li mo lk ll lm mp lo lp lq im bi translated">在这篇文章中，我们看到了一个简单而有效的算法来识别一个网站的相似页面或文章，用同样有效的方法刮。</p><p id="5db9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来的步骤将包括更深入的分析，以理解<em class="ls">为什么</em>这些文章有很高的相似性。数据挖掘和NLP工具，如Spacy，非常方便，允许进行词性(词性)和NER(命名实体识别)分析。</p><p id="1d5a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">研究最常用的关键词也会同样有效！</p><p id="e534" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">如果你想支持我的内容创作活动，欢迎点击我下面的推荐链接，加入Medium的会员计划</strong>。我将收到你投资的一部分，你将能够以无缝的方式访问Medium的大量数据科学文章。</p><div class="ol om gp gr on oo"><a href="https://medium.com/@theDrewDag/membership" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入Medium-Andrew D # data science</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">阅读Andrew D #datascience(以及媒体上成千上万的其他作者)的每一个故事。您的会员费直接…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">medium.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div><p id="b1bb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你的数据挖掘方法是什么？一般怎么发现内容相似等异常？用评论分享你的想法👇</p><p id="3a1f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">感谢您的关注，很快再见！👋</p></div></div>    
</body>
</html>