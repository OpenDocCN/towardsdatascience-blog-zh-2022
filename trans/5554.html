<html>
<head>
<title>Fine-Tune Transformer Models For Question Answering On Custom Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调变压器模型，用于回答有关自定义数据的问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80#2022-12-15">https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80#2022-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fc8e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于在自定义数据上微调拥抱脸 RoBERTa QA 模型并获得显著性能提升的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5290338623f452e7afd545ed882303cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ktn9Zcg-3JKWAu8MzSqIeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">问答精选|塞犍陀·维维克</p></figure><h1 id="3c4d" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">问答和变形金刚</h1><p id="f950" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">伯特是 2019 年风靡全球的变形金刚模型。通过屏蔽单词并训练模型基于上下文预测这些屏蔽单词，在未标记的数据上训练 BERT。伯特后来在多项任务上进行微调，并在许多特定的语言任务上取得了艺术级的表现。特别是，BERT 对来自 SQUAD 数据集的 100k+问题答案对进行了微调，这些问题由维基百科文章中提出的问题组成，其中每个问题的答案都是相应段落中的一段文本，或<em class="mm"> span </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/e846f639835989f9fcbdcff0f7a130c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqdgJOqpd2Zvv-uMLUJQAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自 https://arxiv.org/abs/1810.04805<a class="ae mo" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">的伯特变压器架构</a></p></figure><p id="03d9" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">不久后发布的 RoBERTa 模型通过修改关键超参数和改进训练建立在 BERT 的基础上。我们感兴趣的模型是 deepset 发布的 huggingface 上的<a class="ae mo" href="https://huggingface.co/deepset/roberta-base-squad2" rel="noopener ugc nofollow" target="_blank">微调过的 RoBERTA 模型，它在上个月被下载了 100 多万次。</a></p><p id="0628" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">例如，让我们使用 SubjQA 数据集的数据—包含来自 6 个不同领域的评论的 10，000 个问题:书籍、电影、杂货、电子产品、猫途鹰(即酒店)和餐馆。</p><div class="mu mv gp gr mw mx"><a href="https://github.com/megagonlabs/SubjQA" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">GitHub — megagonlabs/SubjQA:一个以主观信息为重点的问答数据集</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">SubjQA 是一个问题回答数据集，侧重于主观(而不是事实)问题和答案。的…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ks mx"/></div></div></a></div><p id="7263" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">特别是因为我正在说明微调的力量，我将从电影评论中产生的问题和答案。这些可以方便地分为两个 csv 文件，分别用于培训(train.csv)和测试(test.csv)。</p><h1 id="4d52" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">预处理数据</h1><p id="57c1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">有 4 个主要列— id、问题、上下文和答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/dd52db4392fbe9505514cd2183110e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBFbL0etlj-jl1Usc_8WOw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">微调预处理后的 subjQA 数据集|塞犍陀·维韦克</p></figure><p id="14e2" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">在这种情况下，Id 可以是一个虚拟索引，上下文指的是要从中提取问题的文本块。我发现这里的答案不仅指答案，还需要包含起始字符索引。例如这里:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/abef13a4e2bbe5cee074f5b048b1de58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*2nWF9wRmlg3Dw_KbzoKS7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于微调的典型问答格式|塞犍陀·维韦克</p></figure><p id="a014" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">一旦你有了这种格式的熊猫数据框架，无论 QA 数据集是什么，其他步骤都是相同的——基本上是将数据预处理成 HuggingFace 模型训练器的格式。我将链接到下面脚本的笔记本。</p><h1 id="a7a1" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">模特培训</h1><p id="3599" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我唯一真正改变的是这里的纪元数量。微调模型需要几分钟时间。其他耗时的步骤包括加载模型和推送到 HuggingFace hub。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/624a04120207cf3c787b6c902511391a.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*fRwynrlLihVqoDaWdiAGcA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型训练超参数|塞犍陀·维维克</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f98bee565c96e0c2991b597b3958170f.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*TolFztLdx-OusmZeVZ9PGw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">时代的损失|塞犍陀·维维克</p></figure><h1 id="0a15" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">模型评估和部署</h1><p id="864a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">用于评估问答模型性能的最常见指标是 F1 分数。关于验证数据，旧模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ec7ed154e7f1819b7c0e9fc44877c5b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*iNKDsy9SWRjI9iZHy7UsQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗伯塔基于 SubjQA 数据集的模型指标|塞犍陀·维韦克</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9b31b0a4ac3af31cb18d47a49c5523ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*b_xzM9Vp9Q2ntYrYqikBtA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗伯塔微调 SubjQA 数据集的模型指标|塞犍陀·维韦克</p></figure><p id="3ede" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">如您所见，微调<strong class="ls iu">将性能提高了 50%以上</strong>——这是一个巨大的性能提升！</p><p id="ab50" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">要部署模型，您只需运行:</p><p id="e066" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><code class="fe ns nt nu nv b">trainer.push_to_hub(commit_message=”Training complete”)</code></p><h1 id="a8d2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">模型推理</h1><p id="9148" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在是激动人心的部分——将模型部署到 huggingfacehub 后，您就可以从浏览器访问它了！</p><div class="mu mv gp gr mw mx"><a href="https://huggingface.co/skandavivek2/roberta-finetuned-subjqa-movies_2" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">skandavivek 2/Roberta-fine tuned-subj QA-movies _ 2 拥抱脸</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">编辑模型卡这个模型是在 None 数据集上的 deepset/roberta-base-squad2 的微调版本。更多信息…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">huggingface.co</p></div></div><div class="ng l"><div class="nw l ni nj nk ng nl ks mx"/></div></div></a></div><p id="4e18" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">也可以在笔记本中调用如下:</p><pre class="kj kk kl km gt nx nv ny bn nz oa bi"><span id="f780" class="ob kz it nv b be oc od l oe of"># Replace this with your own checkpoint <br/>model_checkpoint2 = “skandavivek2/roberta-finetuned-subjqa-movies_2”<br/>question_answerer = pipeline(“question-answering”, model=model_checkpoint2)</span></pre><p id="c088" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">作为健全性检查——您可以从训练数据集中的一个条目中查看微调后的转换器是否获得了正确的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b7d2b0b529a98a09b895967102d9e933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wy5uU5Cf9VZC4Uq7QGOfRg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">微调问答管道|塞犍陀·维维克</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/da0fb7090ddff284bd626bda7835d307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*CLWIHxBtr6jeECOlWLyiUA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基地问答管道|塞犍陀·维维克</p></figure><h1 id="5c5e" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">外卖食品</h1><p id="a2bd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一般来说，NLP 和 AI 的世界在不断发展，似乎每个月都有新的突破。在过去的几周里，OpenAI 的 ChatGPT 风靡了全世界。然而，麦肯锡最近的一份报告显示，即使是基于人工智能的公司，也只有 11%在积极使用变压器。</p><p id="98c7" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">虽然聊天功能和创造性的人工智能正在形成巨大的里程碑，但我相信，在信息提取和问题回答等任务中使用变形金刚，可以通过提供前所未有的快速准确的结果，为行业带来立竿见影的好处。</p><p id="d2a7" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">拥抱脸降低了进入门槛——现在不仅可以创建自己的微调变压器模型，而且它们使部署和可伸缩性更容易。</p><p id="c336" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated">您可以在这个 GitHub repo 中找到代码:</p><div class="mu mv gp gr mw mx"><a href="https://github.com/skandavivek/transformerQA-finetuning" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">GitHub—skandavivek/Transformer QA-Fine tuning:微调 HuggingFace Transformer…</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">github.com</p></div></div><div class="ng l"><div class="oi l ni nj nk ng nl ks mx"/></div></div></a></div><p id="2e9b" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><strong class="ls iu"> <em class="mm">更新:</em></strong><a class="ae mo" href="https://www.answerchatai.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu"><em class="mm">https://www.answerchatai.com/</em></strong></a><strong class="ls iu"><em class="mm">—我们使用生成式 AI 回答问题并从自定义文本中提取关键知识的 QA 引擎现已上线！回答特定领域的问题 3 个简单的步骤！</em> </strong></p><ol class=""><li id="d773" class="oj ok it ls b lt mp lw mq lz ol md om mh on ml oo op oq or bi translated"><strong class="ls iu"> <em class="mm">上传网址或粘贴文字，点击搜索按钮</em> </strong></li><li id="bd2c" class="oj ok it ls b lt os lw ot lz ou md ov mh ow ml oo op oq or bi translated"><strong class="ls iu"> <em class="mm">针对上下文提出问题并点击查询</em> </strong></li><li id="4900" class="oj ok it ls b lt os lw ot lz ou md ov mh ow ml oo op oq or bi translated"><strong class="ls iu"> <em class="mm">得到你的答案！</em> </strong></li></ol><p id="1887" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><strong class="ls iu"> <em class="mm">放心使用，在评论里告诉我你的反馈和任何建议！</em>T11】</strong></p><p id="35dd" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><em class="mm">参考文献:</em></p><ol class=""><li id="e7e8" class="oj ok it ls b lt mp lw mq lz ol md om mh on ml oo op oq or bi translated"><a class="ae mo" href="https://huggingface.co/docs/transformers/tasks/question_answering" rel="noopener ugc nofollow" target="_blank"><em class="mm">https://hugging face . co/docs/transformers/tasks/问答</em> </a></li><li id="4d57" class="oj ok it ls b lt os lw ot lz ou md ov mh ow ml oo op oq or bi translated"><a class="ae mo" href="https://github.com/megagonlabs/SubjQA/tree/master/SubjQA" rel="noopener ugc nofollow" target="_blank"><em class="mm">https://github.com/megagonlabs/SubjQA/tree/master/SubjQA</em></a></li></ol></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><p id="e454" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><em class="mm">如果你还不是中会员，想支持我这样的作家，可以通过我的推荐链接随意报名:</em><a class="ae mo" href="https://skanda-vivek.medium.com/membership" rel="noopener"><em class="mm">https://skanda-vivek.medium.com/membership</em></a></p><p id="a0b4" class="pw-post-body-paragraph lq lr it ls b lt mp ju lv lw mq jx ly lz mr mb mc md ms mf mg mh mt mj mk ml im bi translated"><em class="mm">获取每周数据透视</em> <a class="ae mo" href="https://skandavivek.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mm">订阅此处</em> </a> <em class="mm">！</em></p></div></div>    
</body>
</html>