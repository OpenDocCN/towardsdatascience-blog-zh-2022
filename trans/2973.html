<html>
<head>
<title>Best Practice to Calculate and Interpret Model Feature Importance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算和解释模型特征重要性的最佳实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/best-practice-to-calculate-and-interpret-model-feature-importance-14f0e11ee660#2022-06-29">https://towardsdatascience.com/best-practice-to-calculate-and-interpret-model-feature-importance-14f0e11ee660#2022-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8cab" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">以随机森林模型为例</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/44a11b0f334c6a4f666bad1cd8f09828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGHkpxzlnzwfR6yBG1JgWw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://unsplash.com/photos/w7ZyuGYNpRQ" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>(归功于凯文·Ku)</p></figure><p id="0deb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在机器学习中，大多数时候你希望模型不仅准确，而且可以解释。一个例子是客户流失预测-除了知道谁会流失，了解哪些变量在预测流失以帮助改进我们的服务和产品方面至关重要。</p><p id="8fba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scikit-learn等流行的机器学习包为模型解释提供了特征重要性的默认计算。然而，我们经常不能相信那些默认的计算。在本文中，我们将使用来自Kaggle 的著名的<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据和一个随机森林模型来说明:</a></p><ul class=""><li id="df73" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">为什么您需要一个<strong class="ky ir">健壮的</strong>模型和<strong class="ky ir">排列重要性</strong> <strong class="ky ir">分数</strong>来正确计算特性重要性。</li><li id="396f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">为什么您需要理解<strong class="ky ir">特性的相关性</strong>来正确解释特性的重要性。</li></ul><p id="ff42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中描述的实践也可以推广到其他模型。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="7fb9" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">计算要素重要性的最佳实践</h1><h2 id="f140" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">默认特征重要性的问题是</h2><p id="2565" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">我们将使用一个示例来展示Scikit-learn中为随机森林提供的默认基于杂质的特征重要性的<strong class="ky ir">问题。默认特征重要性是根据杂质的平均减少量(或基尼系数重要性)计算的，它衡量每个特征在减少不确定性方面的有效性。参见<a class="ae kv" rel="noopener" target="_blank" href="/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3">这篇伟大的文章</a>以获得特性重要性计算背后的数学的更详细的解释。</strong></p><p id="1570" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载著名的泰坦尼克号数据集。该数据集包含泰坦尼克号上1309名乘客的信息以及他们是否幸存。下面是对包含的列的简要描述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/2e922083bd6e435da869431b02ec5ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOJaoQIBR2dzenIWusxlSg.png"/></div></div></figure><p id="00f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们加载数据，并将其分为预测集和响应集。在预测器集合中，我们添加了两个随机变量<code class="fe nx ny nz oa b">random_cat</code>和<code class="fe nx ny nz oa b">random_num</code>。因为它们是随机生成的，所以这两个变量应该具有非常低的特征重要性分数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="04fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其次，我们对数据进行一些简单的清理和转换。这不是本文的重点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="c435" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三，我们建立一个简单的随机森林模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><pre class="kg kh ki kj gt od oa oe of aw og bi"><span id="ce1c" class="nf mo iq oa b gy oh oi l oj ok">RF train accuracy: 1.000<!-- -->.</span><span id="00a7" class="nf mo iq oa b gy ol oi l oj ok">RF test accuracy: 0.814</span></pre><p id="3bb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型在训练数据上略有过度拟合，但在测试集上仍有不错的性能。现在让我们使用这个模型来说明默认特性重要性计算的一些缺陷。让我们来看看默认的特性重要性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/04e7a64ee48157146053476c5ccba9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*RjpWU1ivU0p-IvWow2hphQ.png"/></div></figure><p id="b1a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从默认的特性重要性中，我们注意到:</p><ul class=""><li id="4b1f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">与<code class="fe nx ny nz oa b">random_cat</code>相比，<code class="fe nx ny nz oa b">random_num</code>具有更高的重要性分数，这证实了<strong class="ky ir">基于杂质的重要性偏向于高基数和数字特征</strong>。</li><li id="3c14" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">非预测性<code class="fe nx ny nz oa b">random_num</code>变量被列为最重要的特征之一，这没有意义。这反映了当您有一个过拟合模型时<strong class="ky ir">默认特征重要性不准确。当模型过度拟合时，它会从训练集中拾取太多的噪声，从而无法对测试集做出一般化的预测。当这种情况发生时，特征重要性是不可靠的，因为它们是基于训练集计算的。更一般地说，<strong class="ky ir">只有当您有一个可以合理预测的模型时，查看特性的重要性才有意义。</strong></strong></li></ul><h2 id="23cf" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">救援的排列重要性</h2><p id="d5d7" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">那么我们如何恰当地计算特性的重要性呢？一种方法是使用排列重要性分数。它通过以下步骤计算:</p><ol class=""><li id="9387" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr on ly lz ma bi translated">训练一个基线模型，并在验证集上记录分数(在这个例子中我们使用准确性)。</li><li id="d85a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr on ly lz ma bi translated">重新排列一个特征的值，使用模型再次进行预测，并计算验证集的得分。该特征的特征重要性是1中的基线和2中的置换得分之间的差异。</li><li id="19bf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr on ly lz ma bi translated">对所有特征重复该过程。</li></ol><p id="e68f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们利用2019年添加到Scikit-learn包中的permutation_importance函数。当调用该函数时，我们设置n_repeats = 20，这意味着对于每个变量，我们随机洗牌20次，并计算准确度的下降，以创建箱线图。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/0ff560feaf4f8d372fa7c35aa029d312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSKQFh7DR5PG37phLEKKyw.png"/></div></div></figure><p id="53e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到<code class="fe nx ny nz oa b">sex</code>和<code class="fe nx ny nz oa b">pclass</code>显示为最重要的特征，并且<code class="fe nx ny nz oa b">random_cat</code>和<code class="fe nx ny nz oa b">random_num</code>不再具有基于测试集上排列重要性的高重要性分数。箱线图显示了具有N次重复排列(在我们的例子中N = 20)的准确度分数降低的分布。</p><p id="1c1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们也计算训练集上的排列重要性。这表明<code class="fe nx ny nz oa b">random_num</code>和<code class="fe nx ny nz oa b">random_cat</code>获得了比在测试集上计算时显著更高的重要性排名，并且特性的排名看起来与测试集非常不同。如前所述，这是由于模型的过度拟合。</p><p id="daa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能想知道为什么Scikit-learn仍然包含默认的特性重要性，尽管它并不准确。RFs的发明者Breiman和Cutler指出，这种“将森林中所有树木的每个变量的基尼系数减少相加的方法给出了一个快速的变量重要性，这通常与排列重要性度量非常一致。”所以缺省值意味着置换重要性的代理。然而，正如施特罗布尔等人在<a class="ae kv" href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25" rel="noopener ugc nofollow" target="_blank">随机森林变量重要性测量的偏差</a>中指出的，“布雷曼的原始随机森林方法的变量重要性测量……在潜在预测变量的测量范围或类别数量发生变化的情况下是不可靠的。”</p><h2 id="cfaa" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">稳健的模型是获得准确重要性分数的先决条件</h2><p id="8f2b" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">我们已经看到，当模式过度拟合时，从训练集和预测集中生成的特征重要性可能会非常不同。让我们通过设置min_samples_leaf = 20而不是1来应用某种程度的正则化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><pre class="kg kh ki kj gt od oa oe of aw og bi"><span id="50d0" class="nf mo iq oa b gy oh oi l oj ok">RF train accuracy: 0.810</span><span id="de69" class="nf mo iq oa b gy ol oi l oj ok">RF test accuracy: 0.832</span></pre><p id="db0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们再次看看特性的重要性。修复过度拟合后，根据训练集和测试集计算的特征重要性看起来非常相似。这给了我们更多的信心，一个健壮的模型给出了准确的模型重要性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/40112fbad07acae2b894e5de695f329c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4oszHnWX0yEwLGUIWokcYg.png"/></div></div></figure><p id="8a80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一种方法是计算<strong class="ky ir">删除列重要性</strong>。这是计算特征重要性的最准确的方法。这个想法是用所有预测值计算模型性能，去掉一个预测值，然后观察性能的下降。特性越重要，我们看到的模型性能的下降就越大。</p><p id="3157" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到删除列重要性的高计算成本(我们需要为每个变量重新训练一个模型)，我们通常更喜欢排列重要性分数。但这是验证排列重要性的一个很好的方法。这两种策略的重要性值可能不同，但特征重要性的顺序应该大致相同。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/dfa241f2b72d2220b8a0d8bc6d91c155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*AJkAzE9b5flyUmZcie74ag.png"/></div></figure><p id="94ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征的排序类似于置换特征，尽管幅度不同。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="cdf0" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">解释特性重要性的最佳实践</h1><h2 id="406c" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">特征相关性的挑战</h2><p id="c9cb" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">在我们有了一个健壮的模型并正确地实现了正确的策略来计算特性重要性之后，我们就可以前进到解释部分了。</p><p id="63d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个阶段，相关性是我们解释特征重要性的最大挑战。到目前为止，我们所做的假设分别考虑了每个特性。如果所有的特征都是独立的，没有任何关联，那就很容易解释了。但是，如果两个或多个要素共线，将会影响要素重要性结果。</p><p id="eb80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了说明这一点，让我们使用一个极端的例子，复制列性别来重新训练模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><pre class="kg kh ki kj gt od oa oe of aw og bi"><span id="3b00" class="nf mo iq oa b gy oh oi l oj ok">RF train accuracy: 0.794<br/>RF test accuracy: 0.802</span></pre><p id="615f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们添加了一个没有添加任何信息的特征时，模型性能会稍微下降。</p><p id="c8c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在看到性特征的重要性现在分布在两个重复的性列之间。如果我们给复制的列添加一点噪声会发生什么？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/f639be28ad8ca29baa000ee934c32d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8smbm5kQIVavXlC2H0mv2A.png"/></div></div></figure><p id="4714" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们试着给性别添加0-1范围内的随机噪声。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/95c7ce905c20406724a4edacfd53dfa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*gG4fAa_3miG0PqsOIEMAiw.png"/></div></figure><p id="9404" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nx ny nz oa b">sex_noisy</code>现在是最重要的变量。如果我们增加噪声的数量会发生什么？让我们将随机变量的范围增加到0–3。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/799e0a1c4dddc19b8d5f185a41904b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*c-kbYbclIIMmpT0dNmn5Bg.png"/></div></figure><p id="ed69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以看到，随着更多的噪音加入，现在<code class="fe nx ny nz oa b">sex_noisy</code>不再是排名第一的预测因素，性别又回到了首位。结论是在随机森林模型<strong class="ky ir">上计算的排列重要性在共线变量</strong>上传播重要性。分享的数量似乎是两者之间有多少噪音的函数。</p><h2 id="4e40" class="nf mo iq bd mp ng nh dn mt ni nj dp mx lf nk nl mz lj nm nn nb ln no np nd nq bi translated">处理共线要素</h2><p id="c69d" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">我们来看看特征之间的相关性。我们使用rfpimp包中的feature_corr_matrix，它给出了Spearman相关性。Spearman相关与标准Pearson相关的区别在于，Spearman相关首先将两个变量转换为排名值，然后对排名变量运行Pearson相关。它没有假设变量之间的线性关系。</p><pre class="kg kh ki kj gt od oa oe of aw og bi"><span id="c7d4" class="nf mo iq oa b gy oh oi l oj ok">feature_corr_matrix(X_train)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/fc1395a0fa29b7b25be5d0036739c41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGzaemMR30BA8oeUZAhVzw.png"/></div></div></figure><pre class="kg kh ki kj gt od oa oe of aw og bi"><span id="bb95" class="nf mo iq oa b gy oh oi l oj ok">from rfpimp import plot_corr_heatmap<br/>viz = plot_corr_heatmap(X_train, figsize=(7,5))<br/>viz.view()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/931e2b32d6de4093eb8ee599c8ee1127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*OOmwoC8PeGHGcKacQ344VA.png"/></div></figure><p id="25d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nx ny nz oa b">pclass</code>与<code class="fe nx ny nz oa b">fare</code>高度相关，这并不奇怪，因为舱位等级取决于你支付的费用。在业务中，我们经常在预测模型中使用多个相互关联的特征。从前面的例子中，我们看到，当两个或多个变量共线时，根据信噪比，计算的重要性在共线变量之间共享。</p><p id="bac8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">策略1:组合共线特征</strong></p><p id="c394" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决这个问题的一种方法是将彼此高度共线的特征组合起来形成一个特征族，我们可以说这个特征族一起排列为X最重要。为此，我们将使用rfpimp包，它允许我们一次混洗两个变量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/87d63d562b805b430eece00d20a21d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*OLJUJV2boP-pDgeWJxVMjA.png"/></div></figure><p id="0d66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">策略2:删除高度共线变量</strong></p><p id="98a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果某个特征依赖于其他特征，这意味着可以使用所有其他特征作为独立变量来准确预测该特征。模型的R越高，特征的依赖性越强，我们就越有信心移除变量不会牺牲准确性。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/e3acab2c3203e5bb1b71f30242e44cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Lj6RuZQdSgL_IgHRRoPyQ.png"/></div></div></figure><p id="bcb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一列dependency显示了依赖分数。使用其他特征完全可预测的特征将具有接近1的值。在这种情况下，我们可能会丢弃<code class="fe nx ny nz oa b">pclass</code>和<code class="fe nx ny nz oa b">fare</code>中的一个，而不会影响太多精度。</p><h1 id="10f1" class="mn mo iq bd mp mq oy ms mt mu oz mw mx jw pa jx mz jz pb ka nb kc pc kd nd ne bi translated">最后</h1><p id="9c8b" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">一旦我们1)有了一个健壮的模型，并实现了正确的策略来计算排列的重要性，2)处理了特性的相关性，我们就可以开始精心制作我们的消息来与利益相关者分享。</p><p id="8963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于人们经常问的问题“特性1比特性2重要10倍吗？”，这时你可能明白了，只有当所有的特征都是独立的或者相关性很低的时候，我们才有信心进行论证。但在现实世界中，这种情况很少发生。建议的策略是将功能分配给高、中和低影响层，而不要过于关注确切的影响程度。如果我们需要显示特征之间的相对比较，请尝试将共线特征分组(或删除)到熟悉的特征，并基于分组进行解释，以使论点更加准确。</p><p id="0130" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的Github上找到这篇文章的代码。</p><h1 id="8e46" class="mn mo iq bd mp mq oy ms mt mu oz mw mx jw pa jx mz jz pb ka nb kc pc kd nd ne bi translated">参考</h1><p id="fefb" class="pw-post-body-paragraph kw kx iq ky b kz nr jr lb lc ns ju le lf nt lh li lj nu ll lm ln nv lp lq lr ij bi translated">[1] <a class="ae kv" href="https://explained.ai/rf-importance/" rel="noopener ugc nofollow" target="_blank">小心默认随机森林重要性</a></p><p id="3dbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] <a class="ae kv" href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html" rel="noopener ugc nofollow" target="_blank">排列重要性与随机森林重要性(MDI) </a></p><p id="620a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]<a class="ae kv" href="https://github.com/parrt/random-forest-importances" rel="noopener ugc nofollow" target="_blank">sci kit-Learn机器学习模型的特征重要性</a></p><p id="0d4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] <a class="ae kv" rel="noopener" target="_blank" href="/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3">决策树、随机森林的数学特性在Scikit-learn和Spark中的重要性</a></p><p id="f585" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] <a class="ae kv" rel="noopener" target="_blank" href="/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e">以随机森林为例解释特征重要性</a></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="44f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="pd">除特别注明外，所有图片均为作者所有。</em></p></div></div>    
</body>
</html>