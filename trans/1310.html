<html>
<head>
<title>Deep Learning for Dynamics — the Intuitions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动力学的深度学习——直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-dynamics-the-intuitions-20a67942dfbc#2022-04-02">https://towardsdatascience.com/deep-learning-for-dynamics-the-intuitions-20a67942dfbc#2022-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="013d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度学习在学习动力学方面已经显示出有希望的结果，这里是两种流行方法背后的直觉，用PyTorch编码的例子进行说明。</h2></div><p id="5588" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇博文假设了深度学习的一些基础知识。这篇文章中使用的所有代码都可以在我的GitHub中找到:<a class="ae lb" href="https://github.com/TomJZ/DeepLearningForDynamics-Intuition" rel="noopener ugc nofollow" target="_blank">https://github.com/TomJZ/DeepLearningForDynamics-Intuition</a></p><h1 id="0faa" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">微分方程</h1><p id="a98b" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">大多数物理模型都是用微分方程(DEs)编写的，具体来说就是常微分方程(ODEs)和偏微分方程(PDEs)。牛顿万有引力定律写道<em class="lz"> F = G*m1* m2/r = m*a </em>，其中<em class="lz"> a </em>为加速度，位置的二阶导数。爱因斯坦著名的广义相对论使用偏微分方程系统将时空、能量和动量联系起来。薛定谔方程使用线性PDE描述了量子力学系统的波函数，在量子尺度上模拟了概率分布的流动。毫不夸张地说，经典和现代物理学的成功都归功于DEs令人难以置信的表现力。</p><p id="6e96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DEs背后的假设很简单:时间和空间是连续的，因此它们可以被分成无限小的部分。通过恰当地描述这些部分在最小尺度下如何相互作用，观察到的宏观行为应该会浮现出来。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/d799ffdf3f00e0c248ca79de2c869743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6RwHrYams6lAo0DXmMcJw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">情商。1:颂歌的一般形式</p></figure><p id="7170" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的方程是ODEs的一般形式，它与大多数其他函数的明显区别在于<em class="lz"> f(。,.)</em>设为<em class="lz"> x_dot </em>，状态的导数。这个方程非常简单——它只是状态和时间的函数。这种简单的形式可以模拟从新冠肺炎传播到机器人动力学的各种各样的现象。</p><p id="62a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设系统从点<em class="lz"> x0 </em>在<em class="lz"> t=0 </em>开始，其在<em class="lz"> t=c </em>的状态由下式给出</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mq"><img src="../Images/0664227681f0a208d731253159b742e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wehazOW0B9gbW4ZYdxU5EQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">情商。2:给定初始条件时c时刻的状态</p></figure><p id="72c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过评估上面的等式，可以获得在<em class="lz"> t=c </em>的状态。关键是积分的计算。如果积分可以解析地计算出来，则称常微分方程有<em class="lz">解析解</em>。否则，积分可能需要使用数值方法进行评估，如<a class="ae lb" href="https://en.wikipedia.org/wiki/Euler_method" rel="noopener ugc nofollow" target="_blank">欧拉法</a>、<a class="ae lb" href="https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods" rel="noopener ugc nofollow" target="_blank">龙格-库塔法</a>或多步解算器。用这些方法我们得到了给定初始条件的常微分方程的数值解。著名的三体没有解析解，并且本质上是混沌的，这一点我们将在后面讨论。</p><p id="2437" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们深入研究机器学习之前，让我们先弄清楚一些术语。在这篇博文中，我将提到<em class="lz"> f(。,.)</em>作为<em class="lz">动力学</em>，它是支配一个系统如何进化的函数。然后是前面段落中描述的ODE的<em class="lz">解决方案</em>的概念。一种解决方案是使用ODE在初始条件下生成的状态序列(想象将时间<em class="lz"> c </em>设置为等式中的时间戳序列)。2，你就可以解出一系列的状态)。由微分方程表示的动力学有时更明确地表述为<em class="lz">矢量场</em>，相应地，微分方程的解可以称为<em class="lz">轨迹</em>。为了一致和清晰，我将成对使用这些术语:<em class="lz"> DEs </em>和它们的<em class="lz">解；矢量场</em>和这些场中的轨迹。</p><p id="72e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定系统演化的观测值，或其轨迹Z = [x(t1)，x(t2)，x(t3)，…，x(tn)]，其中采样时间戳是T = [t1，t2，…，tn]，因此机器学习任务是找到一个模型来拟合这些观测值。让我们在这里暂停一下，想一想到底如何处理这个问题。这就是哲学二分法的由来。一些人认为这个问题仅仅是学习一个时间序列轨迹，而另一些人认为学习潜在的向量场是重要的。</p><h1 id="f280" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">这些系统</h1><p id="8624" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在我们深入到不同的思想流派之前，这里有两个经典的动力系统我们将用来作为说明性的例子:(1)一个稳定的螺旋，和(2)洛伦兹系统。</p><p id="0d51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2D稳定螺旋，顾名思义，是一个稳定的系统。它的轨迹，随着时间的推移，收敛到一个固定点。</p><pre class="mb mc md me gt mr ms mt mu aw mv bi"><span id="9ced" class="mw ld iq ms b gy mx my l mz na"># Stable Spiral system dynamics<br/>class Spiral(nn.Module):<br/>    def __init__(self):<br/>        super(Spiral, self).__init__()<br/>        self.lin = nn.Linear(2, 2, bias=False)<br/>        W = torch.tensor([[-0.1, -2.0],<br/>                         [2.0, -0.1]])<br/>        self.lin.weight = nn.Parameter(W)</span><span id="d828" class="mw ld iq ms b gy nb my l mz na">    def forward(self, t, y):<br/>        return self.lin(y**3)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/b6fdc101ab77b22823c1d972df657080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*z06dOgsUQwqaNOrDavGtIQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图1:稳定的螺旋。轨迹最终收敛到一个固定点</p></figure><p id="a7a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，2D混沌的洛伦兹系统要有趣得多。它是由数学家和气象学家爱德华·洛伦茨在1963年发现的。它以<a class="ae lb" href="https://en.wikipedia.org/wiki/Chaos_theory#Chaotic_dynamics" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">混沌</strong> </a>而闻名，这意味着给定一个参考初始条件和由此产生的轨迹，对这个初始条件的任何微小扰动都会导致轨迹以指数速度发散。然而，这种分歧并没有使轨迹走向无穷:轨迹最终停留在一个称为吸引子的有界集合上。术语<em class="lz">蝴蝶效应</em>是洛伦兹吸引子的真实含义。</p><pre class="mb mc md me gt mr ms mt mu aw mv bi"><span id="d223" class="mw ld iq ms b gy mx my l mz na"># Lorenz system dynamics<br/>class Lorenz(nn.Module):<br/>    def __init__(self):<br/>        super(Lorenz, self).__init__()<br/>        self.lin = nn.Linear(5, 3, bias=False)<br/>        W = torch.tensor([[-10., 10., 0., 0., 0.],<br/>                          [28., -1., 0., -1., 0.],<br/>                          [0., 0., -8. / 3., 0., 1.]])<br/>        self.lin.weight = nn.Parameter(W)</span><span id="1669" class="mw ld iq ms b gy nb my l mz na">    def forward(self, t, x):<br/>        y = y = torch.ones([1, 5])<br/>        y[0][0] = x[0][0]<br/>        y[0][1] = x[0][1]<br/>        y[0][2] = x[0][2]<br/>        y[0][3] = x[0][0] * x[0][2]<br/>        y[0][4] = x[0][0] * x[0][1]<br/>        return self.lin(y)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/fb34887069466cca49547c84fbac2d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*M80P1pSFqaacZeGds2Futw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图2:洛伦兹系统。两个“翅膀”停留在一个吸引子上</p></figure><p id="5a34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面洛伦兹系统的图中，我根据时间对轨迹进行了颜色编码。轨迹从深蓝色开始，以黄色结束。可以观察到，在蝴蝶的两个“翅膀”上，颜色是混合的——这表明了轨迹的“随机性”。简单来说，你无法预测它会在未来的某个时间落脚在哪一翼。另一个有趣的事实是，混乱只可能在3D或更高的维度，这让我想起了道家的智慧:<em class="lz">道产生了一个；一个产生了两个；两个产生三个；三生万物。</em></p><p id="53a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，许多人将洛伦兹系统误认为非确定性系统。洛伦兹系统，像任何常微分方程一样，事实上完全是确定性的。它的“随机性”来源于指数级快速发散的轨迹，而不是任何随机性。混沌的本质使得洛伦兹系统更难学习，我们将在后面的学习示例中展示。</p><h1 id="e8c0" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">学习轨迹</h1><p id="20a6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">对于不熟悉动力系统的人来说，当一个类似Z = [x(t1)，x(t2)，x(t3)，…，x(tn)]的轨迹出现时，他们可能会把这看成一个时间序列建模问题。任务是直接模拟轨迹Z本身。像RNNs和transformers这样的经典时间序列建模工具自然就派上了用场。这些模型有许多变种，如油藏计算机、LSTMs、时空器等等。</p><p id="90bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将用最简单的RNN作为例证。不熟悉RNNs的可以在这里看一下<a class="ae lb" href="https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="mb mc md me gt mr ms mt mu aw mv bi"><span id="41a8" class="mw ld iq ms b gy mx my l mz na">class fcRNN(nn.Module):<br/>    def __init__(self, input_size, hidden_dim, output_size, n_layers):<br/>        super(fcRNN, self).__init__()<br/>        self.hidden_dim = hidden_dim<br/>        self.n_layers = n_layers<br/>        self.rnn = nn.RNN(input_size, <br/>                          hidden_dim, <br/>                          n_layers,<br/>                          nonlinearity='relu',<br/>                          batch_first=True) # RNN hidden units</span><span id="6922" class="mw ld iq ms b gy nb my l mz na">        self.fc = nn.Linear(hidden_dim, output_size) # output layer</span><span id="42e2" class="mw ld iq ms b gy nb my l mz na">    def forward(self, x):<br/>        bs, _, _ = x.shape<br/>        h0 = torch.zeros(self.n_layers, bs,<br/>                        self.hidden_dim).requires_grad_().to(device)<br/>        out, hidden = self.rnn(x, h0.detach())<br/>        out = out.view(bs, -1, self.hidden_dim)<br/>        out = self.fc(out)<br/>        return out[:, -1, :]</span></pre><p id="1d64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个RNN没有什么特别的。输入大小和输出大小与系统的维数相同(螺旋为2，洛伦兹为3)。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ne"><img src="../Images/76dc56a297e9bc71312899c4d1e16a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzTBt7J_PgaDFHhJxhxC7Q.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">情商。3:用RNN生成轨迹，假设训练时间窗口为3</p></figure><p id="f6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练RNN的方法如下:首先为输入定义一个时间窗口，然后RNN预测下一步，由此可以计算损失。更具体地说，给定轨迹Z = [x(t1)，x(t2)，x(t3)，…，x(tn)]，以及比如说3的时间窗，我们处理Z以给出训练数据S = {([x(t1)，x(t2)，x(t3)]，x(t4)，([x(t2)，x(t3)，x(t4)]，x(t5)，…，([x(tn-3)，x(tn-2)，x(tn-1)]，x(tn))}。以第一个训练样本为例，RNN取[x(t1)，x(t2)，x(t3)]并输出其预测。在预测和x(t4)之间计算损失。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nf"><img src="../Images/e5013e389fc50604de45c364bc50f3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*RjkhdJue6oJOULooGbToIg.gif"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图3:和RNN一起学习稳定螺旋</p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1438ae6505bc8247fca62009d5f606bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*qT8Zk3zuHeUWVUbxcU_aEQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图4:稳定螺旋的RNN预测</p></figure><p id="fab3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图4所示，得到的模型在预测稳定螺旋的轨迹方面做得相当不错。然而，从图3中可以看出，训练过程并不那么美好。在训练过程中，轨迹的某些部分经常变得不平滑。</p><p id="f273" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来让我们看看学习洛伦兹系统。在我下面的洛伦兹系统图中，深灰色散点图是训练数据，而渐变颜色线是模型预测。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/30119260a75df5f23bd469aaa973b5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AchDFDpM5KDYP5KPDtLU1g.gif"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图5:用RNN学习洛伦兹系统</p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/19c7be02e185475c3360d4aa262ff75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*oK8_2JoB_gNesLfMoSky6g.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图6:洛伦兹系统的RNN预测</p></figure><p id="ac2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练过程中我们仍然可以观察到轨迹并不是那么平滑。更糟糕的是，轨迹似乎在跳来跳去。得到的模型预测很难捕捉到系统的吸引子。根据颜色，系统错误地收敛到右“翼”中心的一个固定点。虽然我没有做大量的调整，但这种趋向于收敛到一个固定点或极限环的趋势经常被观察到。</p><h1 id="fabf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">学习向量场</h1><p id="925d" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">对于那些有一些动力系统背景的人来说，学习基础向量场是最自然的方法。毕竟，对动力系统的研究本身就是数学的一个分支，它本身就应该告知学习是如何进行的。换句话说，我们希望为未知的f(.,.)在Eq。1利用对其状态序列的观察。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nj"><img src="../Images/2c4d9298518ed34e11e8c74e33be1108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aeENZqwiU-H_AtLYawMr5g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">情商。4:生成带节点的轨迹。fNN(。,.)是描述向量场的神经网络</p></figure><p id="692d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多最近的作品[1，2，6]采用了这一观点，并展示了学习动力学的令人印象深刻的结果。这类神经网络称为神经常微分方程(节点)[1]。在加速和改善其优化收敛方面取得了很大进展。还有关于这些学习技巧的<a class="ae lb" rel="noopener" target="_blank" href="/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795">好教程</a>。</p><p id="b17b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将使用<a class="ae lb" href="https://github.com/rtqichen/torchdiffeq" rel="noopener ugc nofollow" target="_blank"> TorchDiffEq </a>库，它几乎是当今NODE的标准工具箱。它不仅提供了各种解算器，还集成了py torch<a class="ae lb" rel="noopener" target="_blank" href="/the-story-of-adjoint-sensitivity-method-from-meteorology-906ab2796c73">伴随灵敏度方法</a>，这是一种内存高效的反向传播替代方法。稳定螺旋和Lorenz系统的模型是具有ReLU激活功能的黑盒全连接神经网络。</p><p id="327e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与RNNs不同，我们不需要处理训练数据——观察到的轨迹可以直接用于学习。虽然大多数演示使用随机梯度下降(SGD)从训练数据中随机抽样批次进行训练，但我使用梯度下降并同时从所有批次中学习。我稍后会解释这样做的原因。</p><p id="8453" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">节点“批处理”的概念很重要。它是沿着训练轨迹的一批点，您可以将这些点用作生成神经网络模型预测的初始条件。还有一个至关重要的超参数，我称之为<strong class="kh ir">前瞻</strong>。前瞻是神经网络根据一批初始条件预测未来的步数。然后计算这些批预测和地面实况之间的损失。</p><p id="a578" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前瞻与你试图学习的系统动力学有很大关系，必须小心选择。对于大多数稳定的系统，更大的前瞻会起作用。然而，对于混乱和不稳定的系统，前瞻必须保持较小(您可以始终将其保持为1)。对于混沌/不稳定动力学保持较小的原因直接来自于这些动力学的定义。如果系统的轨迹随着不同的权重以指数方式快速发散，计算该轨迹上的损失将是没有意义的。简单来说，你不能强迫两个注定有分歧的东西变得一样。</p><pre class="mb mc md me gt mr ms mt mu aw mv bi"><span id="0f57" class="mw ld iq ms b gy mx my l mz na"># A 2D system for fixed point<br/>class FixedPointTrain(nn.Module):<br/>    """<br/>    neural network for learning the stable spiral<br/>    """<br/>    def __init__(self):<br/>        super(FixedPointTrain, self).__init__()<br/>        self.lin = nn.Linear(2, 128)<br/>        self.lin3 = nn.Linear(128, 2)<br/>        self.relu = nn.ReLU()</span><span id="8993" class="mw ld iq ms b gy nb my l mz na">def forward(self, t, x):<br/>        x = self.relu(self.lin(x))<br/>        x = self.lin3(x)<br/>        return x</span><span id="42dc" class="mw ld iq ms b gy nb my l mz na"># A 3D system for Lorenz system<br/>class LorenzTrain(nn.Module):<br/>    """<br/>    neural network for learning the chaotic lorenz system<br/>    """<br/>    def __init__(self):<br/>        super(LorenzTrain, self).__init__()<br/>        self.lin = nn.Linear(3, 256)<br/>        self.lin3 = nn.Linear(256, 3)<br/>        self.relu = nn.ReLU()</span><span id="f9b4" class="mw ld iq ms b gy nb my l mz na">def forward(self, t, x):<br/>        x = self.relu(self.lin(x))<br/>        x = self.lin3(x)<br/>        return x</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/e5795534e1baab6ec62bdb037ee0b79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-EYSidmOd6fd0D6RrHoTVg.gif"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图7:学习带有节点的稳定螺旋</p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nk"><img src="../Images/efae12d8e432a644a433d3f021bf24ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BCPgdEJyZjuBn08vnJDGA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图8:稳定螺旋的节点预测</p></figure><p id="7645" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">稳定螺旋的学习结果如图8所示，其中我也画出了矢量场。图7所示的学习过程在训练期间的任何时候都显示出平滑得多的轨迹。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/a953e05e79c889a91a038515e6a2303b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*whEbkNSxVKy1y7dAHt6s9A.gif"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图9:学习带有节点的洛伦兹系统</p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nl"><img src="../Images/7eba65c3f5556c0ffa3a65ecd12ba7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCoVsHJPKOGG4kBUrL5xEw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图10:洛伦兹系统的节点预测</p></figure><p id="ac77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于Lorenz系统，如图9所示，在训练期间，轨迹似乎仍在跳跃。然而，在整个训练过程中，轨迹要平滑得多。图10中得到的模型预测也很好地捕捉了系统的吸引子。虽然神经网络仍然表现出收敛到一个固定点或极限环的趋势，但NODE已经显示出产生真正的混沌轨迹[2]。</p><p id="f611" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有些人可能想知道为什么学习轨迹与训练数据不完全匹配。但任务首先不是要匹配这些轨迹，因为这些系统的混沌本质会让任何努力都变得徒劳。相反，神经网络正在学习矢量场，尽管预测轨迹存在抖动，但矢量场会随着时间的推移而收敛。如果向量场学习得足够好，混沌系统的吸引子将被很好地捕获，如图10所示。</p><p id="54ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一点是关于我使用梯度下降而不是SGD的原因。由于系统的混沌特性，使用神经网络生成的轨迹一旦被学习，不仅对初始条件敏感，而且对其自身的权重敏感。这意味着对神经网络权重的轻微更新将彻底改变所得系统的稳定性(例如稳定或混沌)，并进而改变其轨迹。SGD是随机的，除非学习率降到极低，否则它会放大这种效应。如果除了基础向量场之外，我们还想捕捉吸引子，那么减少训练中的随机性是很重要的。由于学习低维动态系统的大多数任务只涉及小数据集，使用梯度下降比SGD快得多。</p><h1 id="6999" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">那么有什么区别呢？</h1><h2 id="a765" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">时间嵌入</h2><p id="425a" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">RNNs可以被认为是<a class="ae lb" href="https://arxiv.org/pdf/1902.05198.pdf" rel="noopener ugc nofollow" target="_blank">时间延迟系统</a>，其中系统的动态被<em class="lz">嵌入到状态的时间历史中。这在我们处理用于训练RNNs的数据的方式中是明显的。还有诸如<a class="ae lb" href="https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,state%20of%20a%20dynamical%20system." rel="noopener ugc nofollow" target="_blank">图克嵌入定理</a>的理论，给出了混沌动力学可以从嵌入式系统中重构的条件。</em></p><p id="1a44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，NODE可以直接学习向量场。它不需要对系统进行任何变换，如时间嵌入。不过NODE也可以用来学习嵌入式系统，只要可以参数化为DEs。</p><h2 id="bfdc" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">隐式积分器</h2><p id="5bc4" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">另一个重要的区别是，RNNs可以被认为内部有一个隐式积分器。它直接从先前的状态预测下一个状态。情商。2显示了从动力系统生成轨迹的方式，并且存在积分。当学习动态系统时，RNNs隐式地执行这种集成。然而，这使RNNs处于不利地位，因为它只能以规定的时间间隔输出状态。对于以不规则时间间隔采样的观察值，RNNs可能难以应用。</p><p id="68bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相比之下，NODE输出状态的导数，并依靠外部积分器/解算器使用学习的场来生成轨迹。这不仅允许NODE使用不规则采样点对观察值进行训练，还允许它利用任何现有的求解器来满足我们的需求。</p><h2 id="c2f0" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">平滑</h2><p id="29d1" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">正如从前面的例子中观察到的，一个显著的区别是，在训练期间，NODE的轨迹比RNNs的轨迹平滑得多。</p><p id="0311" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是在节点外部执行集成的结果，这使得节点成为真正的动态系统。用数学术语来说，它构成了<a class="ae lb" href="https://en.wikipedia.org/wiki/Diffeomorphism#:~:text=In%20mathematics%2C%20a%20diffeomorphism%20is,and%20its%20inverse%20are%20differentiable." rel="noopener ugc nofollow" target="_blank">微分同胚</a>的一个<a class="ae lb" href="https://en.wikipedia.org/wiki/One-parameter_group" rel="noopener ugc nofollow" target="_blank">单参数群</a>，这里的“单参数”就是时间。微分同胚给了轨迹许多有趣的性质，我将遵从外部来源。因此，轨迹在时间上的平滑度可以被认为是这些属性之一。然而，对于RNNs，没有这样的理论性质。</p><h2 id="a891" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">在先的；在前的</h2><p id="d387" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">在建模方面，自上而下的深度学习方法和自下而上的第一原理方法之间经常存在争论。实际上，这两种方法都有各自的优点和缺点。自上而下的方法可以从数据中提取复杂的隐藏模式，但缺乏可解释性，而且通常缺乏可推广性；自下而上的方法产生可概括和可解释的模型，但需要对过程进行艰苦的分解，而且常常需要一点运气。作为一名机器人专家，在诉诸黑盒方法之前，我倾向于拥有尽可能多的第一原理知识。两者的有机结合当然是最理想的。</p><p id="7b9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">节点更“暴露”的性质使得合并物理先验更容易。毕竟我们大部分的物理模型都是常微分方程或者偏微分方程。当我们想在n ODE中加入一些已知的ODE结构时，这使得NODE成为一个更兼容的选择。其中一个例子可以在我以前的工作中找到，我试图结合两个世界的优点。然而，RNNs更具挑战性，因为动力学隐藏在其隐式积分中。</p><p id="db2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当学习动力系统时，一些显示良好结果的工作假设完全函数形式。注意，这使得它成为一个参数估计问题，而不是深度学习问题。参数估计问题比仅用黑盒神经网络学习要简单得多，因此这种工作的结果应该仔细评估。</p><h1 id="5eb9" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">挑战和机遇</h1><h2 id="8fdb" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">测试真正的混乱</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nx"><img src="../Images/46dccbe9afd64207a396628362215282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LX4pQZtG7ciw3JJWeSW-oQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图11:混沌洛伦兹系统与极限环的0–1测试。(a) Lorenz系统轨迹(b)Lorenz系统0–1测试的“pq”图，应类似于无界布朗运动(c)极限环轨迹(d)极限环的0–1测试的“pq”图，应是有界的</p></figure><p id="0220" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前还不知道神经网络是否能够“真正”代表混沌动力学。一些测试，如<a class="ae lb" href="https://www.maths.usyd.edu.au/u/gottwald/preprints/testforchaos_MPI.pdf" rel="noopener ugc nofollow" target="_blank">0–1测试</a>可以根据系统足够长的轨迹将系统归类为混沌系统。但是多长时间才算足够长呢？有人可能会说，总有一个时间点，轨迹收敛到一个固定点或极限环，特别是我们已经根据经验表明了他们的这种收敛趋势。</p><h2 id="4747" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">学习僵硬的系统</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ny"><img src="../Images/9ee5e4114fdde698e0f9cb8b45e0afd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpsKdeRN4lBzgBnHaOAvTg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图12:范德波尔振荡器。刚性系统的一个例子</p></figure><p id="2cdc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="http://www.scholarpedia.org/article/Stiff_systems" rel="noopener ugc nofollow" target="_blank">僵硬的系统</a>在化学和生物系统中普遍存在，并且是出了名的难以处理。它们生成的轨迹在时间尺度上有如此巨大的差异，以至于学习它们需要大量的调整。</p><h2 id="cece" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">学习高维动力系统</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nz"><img src="../Images/e62029a27e69525fe2e48a45f0230436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dkj_9Uwd_zDS4qz3QWYDmQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图13: 1D仓本-西瓦辛斯基方程。时空混沌的偏微分方程</p></figure><p id="a910" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然本文中的例子都是低维的常微分方程，但偏微分方程才是真正的数学瑰宝。海洋预报、气象学、传热学、量子力学等等，都可以用偏微分方程很好地建模。学习偏微分方程的一个巨大挑战在于它们的高空间维度，这通常需要离散化。学习和求解偏微分方程是一个活跃的研究领域，近年来产生了许多引人入胜的作品。</p><h2 id="82a9" class="mw ld iq bd le nm nn dn li no np dp lm ko nq nr lo ks ns nt lq kw nu nv ls nw bi translated">应用程序</h2><p id="ff4f" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">学习过的颂歌在各个领域的应用越来越多。我很幸运在计算机视觉[3]和机器人学[4，5]中应用了学习动力学的框架。在每一个项目之后，我都有了更多的直觉，对这个领域更加兴奋。与此同时，我也意识到我们还有很长的路要走，才能真正理解动力学和深度学习的相互作用。</p><p id="89ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的一些代码片段是根据<a class="ae lb" href="https://github.com/rtqichen/torchdiffeq/blob/master/examples/ode_demo.py" rel="noopener ugc nofollow" target="_blank">这个演示</a>修改的。</p><p id="8643" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><p id="c7e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] T. Q. Chen、Y. Rubanova、J. Bettencourt和D. Duvenaud，“神经常微分方程”载于NeurIPS，2018年，第6572–6583页。</p><p id="e61c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] T. Z .贾浩，M. A. Hsieh，E. Forgoston，“基于知识的非线性动力学和混沌的学习”，混沌，第31卷，第11期，第111101页，2021年。</p><p id="6326" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]吴、贾浩、王、谢美华、谢志伟，“基于神经常微分方程的可变形图像配准优化框架”，计算机视觉与模式识别(CVPR)IEEE会议，2022 .</p><p id="2e0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] K. Y. Chee*、T. Z. Jiahao*和M. A. Hsieh，“Knode-mpc:基于知识的数据驱动的空中机器人预测控制框架”，IEEE机器人与自动化快报(RA-L)，第7卷，第2期，第2819–2826页，2022年。</p><p id="4064" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]贾浩，潘立群，谢美华，“用基于知识的神经常微分方程学习群集”，IEEE机器人与自动化国际会议(ICRA)，2022年。</p><p id="d596" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6] S. Brunton，J. Proctor和J. Kutz，“从数据中发现控制方程:非线性动力系统的稀疏识别”，《美国国家科学院院刊》113，3932–3937(2015)。</p></div></div>    
</body>
</html>