<html>
<head>
<title>Useful Code Snippets for PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark的有用代码片段</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/useful-code-snippets-for-pyspark-c0e0c00f0269#2022-02-02">https://towardsdatascience.com/useful-code-snippets-for-pyspark-c0e0c00f0269#2022-02-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f9e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当您处理大数据时，您需要将这些代码放在手边</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6b31e8df70d8e3ee1c1adb69563c5076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxBRUnHoGIwt6jAHc4-lqw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@tvick?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">泰勒维克</a>在<a class="ae ky" href="https://unsplash.com/s/photos/network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="e159" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">PySpark</h2><p id="8952" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated"><em class="mv"> PySpark </em>是我们在Spark环境中使用Python语言编写分布式计算查询代码时调用的方式。这方面最著名的例子是专有框架Databricks。</p><p id="96d5" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">Databricks是由Apache Spark的创造者于2013年成立的公司，Apache Spark是分布式计算背后的技术。它目前正在持续增长，并成为该领域的主要解决方案。</p><p id="336a" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">然而，不利的一面是，即使语法是相似的，但它不是相同的，您必须了解代码中的一些特性，否则您将不得不等待很长时间来运行您的代码。</p><h2 id="e282" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">初始提示</h2><p id="2843" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">在我使用Databricks的第一年，我学会了一些技巧，我将在下面进行描述，这样您就不会遇到我在运行代码时遇到的同样的性能问题。</p><blockquote class="nb nc nd"><p id="40d4" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">使用筛选和选择数据启动查询，以缩短数据集的大小</p></blockquote><p id="c965" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">黄金法则:在创建脚本时，您总是希望只过滤和选择您实际使用的变量。这个简单的操作减少了数据的大小，从而转化为更快的脚本。</p><blockquote class="nb nc nd"><p id="673f" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">对于大型列表，join比isin()更快。</p></blockquote><p id="23b6" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">当您有一个数据框、一个值列表，并且您希望只为列表中的那些值过滤df时，如果您有一个大于一手值的列表，那么最好使用连接而不是<code class="fe nh ni nj nk b">isin()</code>。</p><blockquote class="nb nc nd"><p id="8205" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">groupBy对于大数据来说是痛苦的</p></blockquote><p id="5aa1" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">是的，这将是一个与数据集大小成比例的缓慢分组。即使这是一个懒惰的功能——意味着只有当你请求像<code class="fe nh ni nj nk b">display()</code>或<code class="fe nh ni nj nk b">count() </code>或<code class="fe nh ni nj nk b">collect()</code>这样的动作时，它才会被执行——它仍然需要读取所有的数据，这需要时间。</p><blockquote class="nb nc nd"><p id="e0eb" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">处理发生在内存上。内存越多，交付时间越快。</p></blockquote><p id="8601" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">Spark最大的优点是处理发生在内存上，而不是磁盘上，因此速度更快。因此，集群的内存越多，速度就越快。</p><blockquote class="nb nc nd"><p id="e87e" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">尽可能远离环路</p></blockquote><p id="1589" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">远离循环。但是跑大的时候！如果您习惯于在Python脚本中执行循环操作，要知道PySpark绝对不是运行循环的地方。考虑到数据的大小以及该命令将被拆分到许多节点中，它将永远运行下去。</p><blockquote class="nb nc nd"><p id="5c7d" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">总是按表的分区过滤</p></blockquote><p id="4a87" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">数据块中的许多表是由给定的字段划分的。如果你知道是哪一个，使用那个变量作为你的过滤器。</p><blockquote class="nb nc nd"><p id="1783" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">PySpark使用驼色表壳</p></blockquote><p id="2d4e" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">大部分功能将会是<code class="fe nh ni nj nk b">camelCase</code> | <code class="fe nh ni nj nk b">inThisFormat()</code></p><blockquote class="nb nc nd"><p id="be14" class="mc md mv me b mf mw ju mh mi mx jx mk ne my mm mn nf mz mp mq ng na ms mt mu im bi translated">在将大整数转换为字符串之前，不要过滤它们</p></blockquote><p id="7cae" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">如果你想过滤一个类型为<code class="fe nh ni nj nk b">long</code>的变量，你会看到PySpark不让你使用使用数字的过滤函数。它将要求你把你的过滤值作为一个字符串(如<code class="fe nh ni nj nk b">df.filter( col(‘x’) == ‘12345678900000000001’)</code>)。如果变量“x”没有被转换成字符串，您可能会看到多个结果，而不仅仅是一个过滤值，因为PySpark会寻找最接近那个大整数的第19位的匹配。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/1c4c7f72475c552afc401c18b110c898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hd5B35ZxqV7DxicH8jgmEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">过滤“long”类型变量，并将其转换为字符串或不转换为字符串。图片由作者提供。</p></figure><h2 id="ffda" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">有用的代码片段</h2><p id="56fc" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">这里有一些有用的代码片段，是我在今年使用Databricks时收集到的。</p><h2 id="42b7" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">进口</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="3cc8" class="lg lh it nk b gy nq nr l ns nt"># Basic functions<br/>from pyspark.sql import functions as F</span><span id="fe68" class="lg lh it nk b gy nu nr l ns nt"># These ones I use the most<br/>from pyspark.sql.functions import col, sum, max, min, countDistinct, datediff, when</span><span id="1b4b" class="lg lh it nk b gy nu nr l ns nt"># To create Loops, use Windows<br/>from pyspark.sql.window import Window</span><span id="f0bc" class="lg lh it nk b gy nu nr l ns nt"># For datetime transformations<br/>from datetime import timedelta, date</span></pre><h2 id="1498" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">列出、保存、删除命令</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="06a7" class="lg lh it nk b gy nq nr l ns nt"># List files<br/>%fs ls dbfs:/your mount point address</span><span id="460d" class="lg lh it nk b gy nu nr l ns nt"># Save a file to dbfs<br/>df.write.format('parquet').save('address')</span><span id="96f6" class="lg lh it nk b gy nu nr l ns nt"># Save with Overwrite<br/>df.write.mode('overwrite').csv(path)</span><span id="87db" class="lg lh it nk b gy nu nr l ns nt"># SIZE OF A FILE<br/>display( dbutils.fs.ls("file_address") )</span><span id="6430" class="lg lh it nk b gy nu nr l ns nt"># Remove (Delete) a file<br/>dbutils.fs.rm('file_address',True)</span></pre><p id="b8f2" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">要保存文件并将其导出到你的本地机器上，<a class="ae ky" href="https://tinyurl.com/2p8k97n9" rel="noopener ugc nofollow" target="_blank">阅读这篇文章</a>。</p><h2 id="c6a1" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">基本命令</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="e000" class="lg lh it nk b gy nq nr l ns nt"># Display a table<br/>df.display()<br/>display( df.select('col1') )</span><span id="a2c9" class="lg lh it nk b gy nu nr l ns nt"># SELECT<br/>df.select('col1', 'col2', 'col3')</span><span id="ba12" class="lg lh it nk b gy nu nr l ns nt"># LIMIT Number of rows<br/>df.limit(n)</span><span id="8632" class="lg lh it nk b gy nu nr l ns nt"># FILTER<br/>df.filter( df.col1 == xxx )</span><span id="a872" class="lg lh it nk b gy nu nr l ns nt"># COUNT<br/>df.select('col1').count()</span><span id="e1af" class="lg lh it nk b gy nu nr l ns nt"># COUNT DISTINCT<br/>df.select('col1').distinct().count()</span><span id="2ea1" class="lg lh it nk b gy nu nr l ns nt"># JOIN<br/>df1.join( df2, df1.column == df2.column, 'jointype')<br/>df1.join( df2, on='column', how='inner')</span><span id="37b7" class="lg lh it nk b gy nu nr l ns nt"># Join Left_anti: <br/># It is like df1-df2: selects rows from df1 NOT present in df2<br/>df1.join(df2, on=['key'], how='left_anti')</span><span id="64eb" class="lg lh it nk b gy nu nr l ns nt"># SORT<br/>df.sort('column', ascending=False)</span><span id="5324" class="lg lh it nk b gy nu nr l ns nt"># Show NULL values<br/>display( df.filter(df.col_name.isNull() )</span><span id="ec75" class="lg lh it nk b gy nu nr l ns nt"># CAST VALUES TO OTHER TYPES<br/>df = df.withColumn("col_name", df.col_name.cast('type'))</span></pre><h2 id="1ebb" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">填充NAs</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="7478" class="lg lh it nk b gy nq nr l ns nt"># FILL NAs<br/>df = df.fillna(value)<br/>df = df.fillna(value, subset=['col1', 'col2'])</span></pre><h2 id="ed0b" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">价值观在</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="15b9" class="lg lh it nk b gy nq nr l ns nt"># Value IS IN<br/>df.filter( df.col_name.isin(listed_values) )</span></pre><h2 id="2fc8" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">值介于</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="5418" class="lg lh it nk b gy nq nr l ns nt">df.filter(col('xx').between("2020-01-01", "2020-01-02") ) </span></pre><h2 id="6518" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">日期差异</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="f485" class="lg lh it nk b gy nq nr l ns nt">from pyspark.sql.functions import datediff, col</span><span id="341a" class="lg lh it nk b gy nu nr l ns nt">df1.withColumn( "diff_in_days", <br/>                 datediff(col("col_date1"),<br/>                          col("col_date2")) )<br/>   .show()</span></pre><h2 id="e08b" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">列出列中的值</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="bcd1" class="lg lh it nk b gy nq nr l ns nt"># LIST VALUES<br/>data = [row.col for row in df.select('col').distinct().collect()]</span></pre><h2 id="9782" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分组依据和聚集</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="c8ea" class="lg lh it nk b gy nq nr l ns nt"># GROUP BY<br/>df.groupBy('col1').sum()</span><span id="5d9d" class="lg lh it nk b gy nu nr l ns nt"># Aggregate<br/>df.groupBy(['col1', 'col2']).agg({'col1':'sum', 'col2':'mean')</span><span id="53c5" class="lg lh it nk b gy nu nr l ns nt"># Put a List of grouped values in a single column<br/>df.groupBy('colx').agg(F.collect_list('colA'),  <br/>                       F.collect_list('colB')).show()</span></pre><h2 id="be75" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创建新列</h2><p id="3ccf" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">使用<code class="fe nh ni nj nk b">withColumn('col_name', operation or condition)</code></p><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="30a7" class="lg lh it nk b gy nq nr l ns nt"># Create column C as the sum of A + B<br/>df.withColumn( 'C', col('A') + col('B') )</span></pre><p id="319e" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">创建具有常数值的新列。您可以使用pyspark函数中的函数<code class="fe nh ni nj nk b">lit</code>并在其中添加任何值。</p><pre class="kj kk kl km gt nm nk nv bn nw nx bi"><span id="64ae" class="ny lh it nk b be nz oa l ob nt"># New column with a constant text value<br/>df_1.withColumn( 'new_col', F.lit('text') )<br/><br/># New column with a constant number value<br/>df_1.withColumn( 'new_col', F.lit(10) )</span></pre><h2 id="368e" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Spark数据帧中的转换列表</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="f771" class="lg lh it nk b gy nq nr l ns nt"># Import Row<br/>from pyspark.sql import Row</span><span id="6232" class="lg lh it nk b gy nu nr l ns nt"># Create a list<br/>my_list = [1,2,3,4]</span><span id="9b3a" class="lg lh it nk b gy nu nr l ns nt"># Parallelize abuse cards list<br/>rdd1 = sc.parallelize(my_list)<br/>row_rdd = rdd1.map(lambda x: Row(x))<br/>my_df = sqlContext.createDataFrame(row_rdd,['variable_name'])</span></pre><h2 id="3d61" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">何时/否则</h2><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="a34a" class="lg lh it nk b gy nq nr l ns nt"># Single condition<br/>df.withColumn('my_col', when(col('A').isNull(),'Nothing')\<br/>                        .otherwise('Something')</span><span id="ac41" class="lg lh it nk b gy nu nr l ns nt"># Multiple conditions<br/>df.withColumn( 'Result', <br/>               when( (col('A') == 0), 'Zero')<br/>              .when( (col('A') &lt; 0) , 'Negative')<br/>              .when( (col('A') &gt; 0) , 'Positive') <br/>              .otherwise('NA')  )</span></pre><h2 id="5857" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">计算列的百分比</h2><pre class="kj kk kl km gt nm nk nv bn nw nx bi"><span id="15f7" class="ny lh it nk b be nz oa l ob nt">from pyspark.sql.window import Window<br/><br/>df.withColumn('col_pct', col('col_A')/F.sum('col_A')<br/>  .over(Window.partitionBy()) *100 )</span></pre><h2 id="0b7c" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">随机抽样调查</h2><p id="d467" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">根据数据大小的百分比获取数据的随机样本。在本例中，50%的数据没有替换—一旦选择了数据点，就不能再次选择。</p><pre class="kj kk kl km gt nm nk nn no aw np bi"><span id="93ba" class="lg lh it nk b gy nq nr l ns nt">df.sample(withReplacement=False, fraction=0.5, seed=None)</span></pre><h2 id="4757" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在你走之前</h2><p id="964c" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">很明显，这只是使用PySpark可以完成的一小部分工作。此外，最近推出了Spark的熊猫，因此它将变得更好。</p><p id="38c5" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">我知道这些代码片段每天都给我很大帮助，因此我相信它也会帮助你。</p><p id="e299" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">别忘了跟着我。</p><div class="oc od gp gr oe of"><a href="https://gustavorsantos.medium.com/" rel="noopener follow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">古斯塔沃·桑托斯-中等</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">阅读古斯塔夫·桑托斯在媒介上的作品。数据科学家。我从数据中提取见解，以帮助个人和公司…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">gustavorsantos.medium.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><p id="8811" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">或者使用我的<a class="ae ky" href="https://gustavorsantos.medium.com/membership" rel="noopener">推荐代码</a>订阅Medium。</p></div></div>    
</body>
</html>