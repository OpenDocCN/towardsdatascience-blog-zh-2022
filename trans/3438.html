<html>
<head>
<title>Activation Functions in Neural Networks: What You May Not Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活功能:你可能不知道的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/activation-functions-in-neural-networks-what-you-may-not-know-2d99ad093246#2022-07-29">https://towardsdatascience.com/activation-functions-in-neural-networks-what-you-may-not-know-2d99ad093246#2022-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c56" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何识别最佳(和最差)激活函数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/71456a6c58d0c80ceca1a5b605996eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykKb1xNcvIPZa52h612hfg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/es/@jjying?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae ky" href="https://unsplash.com/s/photos/neural-network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ebca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我第一次开始学习ML时，我被告知激活是用来模仿大脑中的“神经元激活”，因此得名神经网络。直到很久以后，我才了解到这个积木的更精细的复杂性。</p><p id="48a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将解释深层神经网络中激活函数背后的两个关键概念和直觉:</p><ul class=""><li id="a5ee" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">为什么我们需要它们，</li><li id="4832" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">为什么我们不能选择任何非线性函数作为激活函数。</li></ul><blockquote class="mj mk ml"><p id="1956" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">本文中的所有数字和结果都是作者创作的:等式是使用<a class="ae ky" href="https://www.texstudio.org/" rel="noopener ugc nofollow" target="_blank"> TeXstudio </a>编写的；使用<a class="ae ky" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>创建模型，使用<a class="ae ky" href="https://github.com/lutzroeder/netron" rel="noopener ugc nofollow" target="_blank"> Netron </a>可视化；使用<a class="ae ky" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> matplotlib </a>绘制图形。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/bd26a44cbadf49f636ab284a4d48f1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*6DLe4NYxCtiIztMLhH5I-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Keras支持的一些激活</p></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="da4d" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">为什么我们需要激活函数？</h1><p id="edec" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated"><em class="mm">术语</em>:激活是一个非线性函数。简单地说，它是不属于形式<code class="fe nv nw nx ny b">y = mx + b</code>的任何函数。</p><p id="b7ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在神经网络中，这些函数通常放在每个卷积或全连接(又称密集)层的输出之后。他们的主要工作是“激活神经元”，即<strong class="lb iu">捕捉模型的非线性</strong>。</p><p id="2dd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是什么是非线性，为什么它很重要？</p><p id="f256" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑以下具有两个密集图层的简单网络(如果您使用过MNIST数据集，可能会觉得很熟悉):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/14e6c013fe4a2ed301efea393342cf9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FkJWNDYaOfcrVlOjU3xPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">简单的双层网络</p></figure><p id="bec6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设输入为<code class="fe nv nw nx ny b">x</code>，第一层的权重和偏差为<code class="fe nv nw nx ny b">W_1, b_1</code>，第二层的权重和偏差为<code class="fe nv nw nx ny b">W_2, b_2</code>。用激活功能<code class="fe nv nw nx ny b">σ_1, σ_2</code>，第一层的输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/4808b45a1b39b935045c725c88ea89c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*hnn7APX8o0qRb9P7BivH2A.png"/></div></figure><p id="c840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个模型的输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/58ca70febb69ae4a613f2cb71e0c45e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*YqV9ssIdd2bM8eRfvmcmdQ.png"/></div></figure><p id="9dd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是如果我们不使用任何激活功能呢？如果没有<code class="fe nv nw nx ny b">σ_1</code>和<code class="fe nv nw nx ny b">σ_2</code>，新的输出将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a3bde9d8cb0e29ee51ce13bdfebb0688.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*QYTY-9tke3oZ-0sYgWElrg.png"/></div></figure><p id="afc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，这个等式可以简化为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/40424e6753791093ae79afc5ccd19e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*0jVoFqQDjvj4JwVI0eeuUw.png"/></div></figure><p id="1afa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这相当于具有一个密集层的浅网络。简单来说，第二层根本没有添加任何有用的信息。我们的模型现在相当于:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/cb94babb190c0737d9aaf7e96ba755c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*3OJdZ-svCQsALp2w1NRwug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">浅层单层网络</p></figure><p id="a0c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将这种分析推广到任意层数，结果仍然成立。(微积分类似的结果:线性函数合成还是线性函数。)</p><blockquote class="of"><p id="cdda" class="og oh it bd oi oj ok ol om on oo lu dk translated">因此，为了让深层网络有意义，我们必须对每个隐藏的输出进行激活。</p></blockquote><p id="5232" class="pw-post-body-paragraph kz la it lb b lc op ju le lf oq jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">否则，我们的网络将会很浅，学习能力将会受到严重限制。</p><p id="f922" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个关于MNIST的快速实验来进一步说明这个结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b8ba2dd4870f494babe39119f9a7bd4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*3n1vtf3oiC78vGqEWUMcUw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST的一些快速结果</p></figure><h1 id="7fc4" class="my mz it bd na nb ov nd ne nf ow nh ni jz ox ka nk kc oy kd nm kf oz kg no np bi translated">一些流行的激活功能</h1><p id="04ca" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">对于大多数实际系统/模型，激活是以下三种之一:ReLU、Sigmoid或Tanh(双曲正切)。</p><h2 id="37ee" class="pa mz it bd na pb pc dn ne pd pe dp ni li pf pg nk lm ph pi nm lq pj pk no pl bi translated">热卢</h2><p id="aa6e" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">最简单的(也可以说是最好的)激活。如果隐藏层的输出为负，我们只需将其设置为零:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/9e4eef9a0379414f6aa2a5bc227faa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*Guqx6-nLY8XU-5g9IsZQEA.png"/></div></figure><p id="791b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ReLU的图形是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/2909cdafe03bb0e21cce3af7174c45b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*fqDNgu1OEHZDTx242dSZhQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ReLU激活功能</p></figure><p id="aaaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="85b5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">高效的反向传播计算。</li><li id="5f52" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">没有消失梯度问题。</li><li id="95d4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">在许多情况下收敛得更快。</a></li></ul><p id="10c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="b600" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">Dead ReLU:如果所有输出都是负的，梯度将被削波为零。这可以通过更好的权重初始化来避免。</li></ul><h2 id="407a" class="pa mz it bd na pb pc dn ne pd pe dp ni li pf pg nk lm ph pi nm lq pj pk no pl bi translated">乙状结肠和Tanh</h2><p id="9f89" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">这些函数具有以下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/899716369bb5b478cbdb48510dc8ce29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*iCxMfo4UhYagKe2EHe0mtQ.png"/></div></figure><p id="0017" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(你能推导出这些函数的导数吗？不要担心，PyTorch和TensorFlow等现代ML框架免费提供这些激活，反向传播已经内置并优化。)</p><p id="c4d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们的图表是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/a949332108d5590b4ed82d55b995a198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*c73HHgxIRYx2fYdDiTEVGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Sigmoid和Tanh激活函数</p></figure><p id="cb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们的行为非常相似，只是Sigmoid的输出上限在[0，1]之间，Tanh的输出上限在[-1，1]之间。</p><p id="2527" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="2327" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">不会破坏激活。</li><li id="6568" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Sigmoid适合捕捉“概率”，因为输出上限在0和1之间。(这些概率的总和不是1，为此我们需要Softmax激活。)</li></ul><p id="47bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="3487" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">较慢的反向传播计算。</li><li id="b0ed" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">很多情况下收敛较慢。</a></li><li id="3d7c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">消失梯度:当输入远离零时，图形是平坦的。这可以通过使用正则化来避免。</li></ul><h1 id="0fb5" class="my mz it bd na nb ov nd ne nf ow nh ni jz ox ka nk kc oy kd nm kf oz kg no np bi translated">为什么不是其他非线性函数？</h1><p id="700c" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">说到非线性，我会想到其他一些函数:二次函数、平方根函数、对数函数…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/d272067b92c7268cdbc9b3615069822d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*TNvCCZ26DyfD6-uYf1H6bA.png"/></div></figure><p id="c0a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么我们在实践中不使用这些函数？</p><p id="586a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据经验，良好的激活:</p><ul class=""><li id="230c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">应该为所有实数定义，</li><li id="2754" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">是可微分的，并且可以有效地实现反向传播，</li><li id="3d64" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">可以“启发式”解释。</li></ul><p id="f3ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述三个函数的一些问题是:</p><ul class=""><li id="faa7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">二次:没有给出有意义的信号。输出-2或2将给出相同的结果。</li><li id="beb3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">平方根:没有为<code class="fe nv nw nx ny b">x &lt; 0</code>定义。</li><li id="2f40" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对数:没有为<code class="fe nv nw nx ny b">x &lt;= 0</code>定义，这个函数在零附近也是无界的。</li></ul><p id="6e56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为练习，可以考虑微积分/线性代数中遇到的其他非线性函数。想想我们为什么不使用它们。这是一个很好的练习，可以提高你对深度学习的直觉。</p><p id="d9e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如:我能不能把平方根函数扩展到负值(通过对称绘制)，并以此作为激活函数？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/baa0cd2ec71b805f2bf4df9aa047d551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ZiKBxjkYGJH3fkAGqq2V5g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">扩展平方根函数</p></figure><p id="2453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mm">(提示:这个激活功能很有问题！)</em></p><h1 id="1dfa" class="my mz it bd na nb ov nd ne nf ow nh ni jz ox ka nk kc oy kd nm kf oz kg no np bi translated">结论</h1><p id="d4a9" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在建立深度学习模型时，激活功能通常是事后想起的。然而，在它的机制中有一些微妙之处，你应该知道。希望这篇文章能让你更好地理解激活的基本概念，以及为什么我们会选择一些功能而不是其他。</p><p id="0a55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快乐学习！</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="68d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你愿意支持Thu，你可以考虑注册成为一名中等会员。每月支付5美元，你就可以无限制地阅读媒体上的故事。如果你使用他的推荐链接，Thu也会得到一小笔佣金。</p><div class="pq pr gp gr ps pt"><a href="https://medium.com/@tdinh15/membership" rel="noopener follow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">通过我的推荐链接加入媒体- Thu Dinh</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">阅读Thu Dinh的每一个故事(以及媒体上成千上万的其他作家)。你的会员费直接支持周四…</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">medium.com</p></div></div><div class="qc l"><div class="qd l qe qf qg qc qh ks pt"/></div></div></a></div></div></div>    
</body>
</html>