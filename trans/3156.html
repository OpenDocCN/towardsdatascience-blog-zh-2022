<html>
<head>
<title>Flux.jl on MNIST — What about ADAM?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MNIST——亚当呢？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/flux-jl-on-mnist-what-about-adam-fb78e7f02acd#2022-07-11">https://towardsdatascience.com/flux-jl-on-mnist-what-about-adam-fb78e7f02acd#2022-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/aa149c2bf05080953516d44cff962416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUA5KfVAzETOPc70-kQHTg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@simonppt?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">西蒙·李</a>在<a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="eff9" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">到目前为止，我们已经看到了使用标准梯度下降优化器的性能分析。但是，如果我们使用像亚当这样更复杂的方法，我们会得到什么结果呢？</h2></div><p id="110c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在关于MNIST的<a class="ae jd" rel="noopener" target="_blank" href="/flux-jl-on-mnist-variations-of-a-theme-c3cd7a949f8c">flux . JL——主题的变体</a>中，我展示了三种用于识别手写数字的神经网络，以及三种用于训练这些网络的梯度下降算法(GD)。</p><p id="fbaf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于MNIST的后续文章<a class="ae jd" rel="noopener" target="_blank" href="/flux-jl-on-mnist-a-performance-analysis-c660c2ffd330">flux . JL——性能分析</a>显示了使用不同参数的每个模型的性能以及训练它们需要付出的努力，这取决于所应用的GD变体。GD算法使用标准的GD学习公式，在该分析中利用恒定的学习速率。</p><p id="3d94" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们将使用高级学习公式(现在称为“优化器”)执行此分析，并将结果与使用标准公式实现的性能进行比较。我们将使用的优化器名为ADAM (= <em class="lr">自适应矩估计</em>)，由<em class="lr"> Diederik P. Kingma </em>和<em class="lr"> Jimmy Ba </em> 于<a class="ae jd" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> 2015年发表。它适应每次迭代的学习速率，是目前该领域最复杂的方法之一。</a></p><p id="3d81" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与最后的分析类似，我们将应用所有三种GD变量进行训练:批量GD、随机GD和小批量GD。</p><h1 id="f911" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">批量梯度下降</h1><h2 id="d48b" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">4LS-型号</h2><p id="b2df" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">使用batch GD将ADAM应用于4LS模型，导致了显著的改进:仅用500次迭代，我们就获得了74.16%的准确率，提高到91%以上。相比之下，标准的<code class="fe nb nc nd ne b">Descent</code>优化器停留在11.35%。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/7aa1395f10ee1fbbc9021775f1e10eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crDjT-gSxdDTP77_mvB9Rg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批量GD—4LS[图片由作者提供]</p></figure><p id="c3e7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习曲线说明了这种行为:左边是标准优化器，迭代500次，右边是ADAM，迭代1000次。ADAM非常快地将损失降低到低于0.1的值，然后稍微停滞，直到大约迭代300，在那里它再次开始降低。<code class="fe nb nc nd ne b">Descent</code>也相当快，直到低于0.1(请注意y轴上的不同缩放比例)，然后停滞不前。除此之外，使用<code class="fe nb nc nd ne b">Descent</code>减少损失并不能转化为准确性的提高(在测试数据集上)。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/357eebb3994246b613f449801aa74f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcCIqCRi7hTWW0VCZXqPGA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1-500(下降)和1-1000(亚当)[图片由作者提供]</p></figure><h2 id="c971" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">3LS型号</h2><p id="2e6e" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于3LS模型，我们得到了类似的结果。ADAM在这里也显著地改进了结果:在500次迭代时，我们获得了95.66%的准确度，在1000次迭代时，准确度可以略微提高到96%以上。另一方面，标准优化器只能产生低于20%的精度。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/cb446ba076bdfdf01684f6074fb356c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NpBkcoRL3d45NrE6BiA4A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批量GD—3LS[图片由作者提供]</p></figure><p id="e46f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习曲线也显示了这一点:两个优化器都非常快地将损耗降低到低于0.1的值(再次注意y轴上的不同缩放)，ADAM甚至比<code class="fe nb nc nd ne b">Descent</code>更快，然后降低得更多。</p><p id="c049" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与4LS-model相比，ADAM在这里停滞得更快，但在这一点上已经取得了比4LS更好的准确性。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/9217f3fff932f2444fb21a663c9044a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9hpkFxPYR6WqVHohE72-A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1-500(下降)和1-1000(亚当)[图片由作者提供]</p></figure><h2 id="c823" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">2LR型号</h2><p id="1b82" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">在2LR-model上，我们已经用标准优化器得到了很好的结果，并且我们用ADAM达到了相同的准确度水平(超过96%)。但是ADAM已经迭代了500次(然后略有下降)，而标准优化器需要4，000次迭代才能达到这个水平。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/de9d61ccb3dd13e77178e49662b01f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPNuBNgnEfRVjjznIkASJA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">批量GD—2LR[图片由作者提供]</p></figure><p id="89e3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，除了ADAM在开始时下降较快之外，学习曲线非常相似:</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/87a7e88f095dbf3afd0484a8563df746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NfWgiIsRocsR-LzKCoJ4w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1-1，000次(下降和亚当)[作者图片]</p></figure><h1 id="8811" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">随机梯度下降</h1><p id="16d8" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于随机GD，标准优化器在4LS-和3LS-模型上提供了相当好的结果。让我们看看，亚当是否能做得更好。</p><h2 id="b9b0" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">4LS-型号</h2><p id="48e3" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于两个优化器，4LS模型的学习曲线具有相同的基本形状，但是ADAM导致更强的振荡。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/10fabe1f140e38d934234f4d584cd639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iga4FaZxcd4CgMcXvIgSXQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–1，024，000次(下降和亚当)[图片由作者提供]</p></figure><p id="ab63" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ADAM用更少的迭代给了我们更高的精度，但最终我们没有得到从<code class="fe nb nc nd ne b">Descent</code>得到的质量(在200万次迭代中，94.16%对92.12%)。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/ed7c2ea8af93809d57ced94c53d11029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yroPQuNl0qYjphb3aZ8rg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">随机GD—4LS[图片由作者提供]</p></figure><h2 id="f2bf" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">3LS型号</h2><p id="423a" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于3LS模型，我们得到了几乎相同的情况:基本上相似的曲线，其中ADAM在开始时更快地收敛到较低的损失值(并且以这种方式振荡得更多):</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/6548f335ce25af6f1befe95de44e6c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EVe1FMwR10zRn57GSnwjmA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–1，024，000次(下降和亚当)[图片由作者提供]</p></figure><p id="10aa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这反映在我们能够达到的精度上。同样，ADAM也没有达到我们用标准优化器达到的水平(在200万次迭代中，93.73%对97.64%):</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/aed4abc4cc8aa967ccf42667c7a68a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*joRdbH_ireScLTeNg-q7_w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">随机GD—3LS[图片由作者提供]</p></figure><h2 id="ede7" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">2LR型号</h2><p id="5d1e" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">用标准优化器训练2LR模型是失败的。我们从来没有达到20%以上的准确率。在这种情况下，ADAM是一个真正的改进，在大约50万次迭代后达到了87%以上的精度(然后又变得更差)。但是我们不能像其他配置那样达到95%以上。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/7a81f583cb5f285f09d3c342effa1b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRA5ot2tkKOzlgJ5PNv7Xg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">随机GD—2LR[图片由作者提供]</p></figure><p id="8558" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用ADAM的学习曲线显示出剧烈的波动，它并不真正符合上表中的数字，因为我们做的迭代次数越多，损失似乎越大。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/e58feff43ff00673f3dd97919b0efe9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CaxoBN-K2MYwtimDscVu8A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代1–1，024，000次(下降和亚当)[图片由作者提供]</p></figure><p id="80bf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">前128，000次迭代的100移动平均线更好地显示了该部分发生的情况。至少在最初的40，000次迭代中，损耗显著降低。但是这仍然不能解释为什么精度在高达大约50万次迭代的范围内提高。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/acd343180fd31045a7f0f608736a4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whzK73tmfT4AKTwL8PTCKg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">移动100-平均值-迭代1–128，000(ADAM)[图片由作者提供]</p></figure><h1 id="3fbd" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">小批量梯度下降</h1><p id="b6a8" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">小批量GD在所有三种模型上都取得了非常好的结果。所以留给亚当的空间并不多。让我们来看看不同之处。</p><h2 id="3e91" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">4LS-型号</h2><p id="45a5" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">正如上面在其他配置中看到的，ADAM在开始时收敛得更快(500次迭代时92.46%的准确性)，但没有达到标准优化器的水平(200万次迭代时94.49%对93.28%):</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/126821cd5671713096a90dda94068784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeJxKP2WNgqmvZ9hvi2GxA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">小批量GD—4LS[图片由作者提供]</p></figure><p id="2318" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习曲线反映了这些发现(同样，亚当导致了更多的振荡):</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/cff706e36189e22df8dfbc9b60594a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMvHbKS1jzXyCSTYD4wfrA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代4，001–512，000(下降)和1–512，000(亚当)[图片由作者提供]</p></figure><h2 id="3e5a" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">3LS型号</h2><p id="76ac" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">3LS型号也是如此。ADAM在500次迭代时已经达到了94.47%的准确度，但是在200万次迭代时最终稍差(97.68%对96.17%)。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/b071dc30459aa4aea2e1f2b27edfdcae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5j3Lq3wkONpxLydR5QKYdA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">小批量GD-3LS[图片由作者提供]</p></figure><p id="43e3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以类似的方式，这反映在学习曲线中:</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/10832ed512e61ced2cc6a2bc88253052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*788VTjYxgR_JeDIbGxcp-w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代4，001–512，000(下降)和1–512，000(亚当)[图片由作者提供]</p></figure><h2 id="a71a" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">2LR型号</h2><p id="e9e8" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">对于2LR型号，情况有点不同。亚当在开始时并没有收敛得更快。同样，它也达不到与标准优化器相同的准确性水平(97.0%对91.33%)。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/08645f4c4f9d9efc373344983ff78fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyvJslEtmbHFZEAaBKIB6g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">小批量GD—2LR[图片由作者提供]</p></figure><p id="f3fc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习曲线在这里显示了(与随机GD一样)显著的振荡:</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/90800ddfde5f7c77c253ab450292406d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49TkFtOyAN8Jji7cUcKWHA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">迭代4，001–512，000(下降)和1–512，000(亚当)[图片由作者提供]</p></figure><h1 id="c1ed" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">结论</h1><h2 id="0955" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">血统vs .亚当</h2><p id="8870" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">下图概述了使用标准优化器(左)和ADAM(右)获得的最佳结果。它显示了与训练时间相关的准确性。在这里，我们可以看到，亚当做得很好:几乎所有的结果都在左上角。即以相对较小的努力实现了高精度。标准优化器提供了更广泛的结果。但最终它产生了一些最好的结果。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/319b0cac51970cf1405da909d42e6935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RmRo5CFggbFiTxFqujTRmQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">准确度与训练时间(下降和亚当)[图片由作者提供]</p></figure><h2 id="1b01" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">顶级表现者</h2><p id="6aee" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">下图显示了表现最好的人(准确率&gt; 90%)。最好的结果是3LS/下降(亮蓝色)，然后是2LR/下降(亮黄色)。4LS变异体(绿色)通常不太好。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/dd4b68ded32d0630a662609a9db5451e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYzBga_eh-vae2H-t--BQw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">准确性与训练时间——最佳表现者[图片由作者提供]</p></figure><h2 id="74d0" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">努力</h2><p id="f16c" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">最后，在最近的分析中，我计算了一个名为“努力”的性能指标，这是训练时间和准确性之间的比率(描述了达到一定水平的准确性需要多少训练时间)。所以你也可以看到这方面的佼佼者:</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/f8112fb40138b6862b0761d20bfab6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VvR_wUMVJb5ep6kdbHGuA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">按“努力”排名[图片由作者提供]</p></figure><h2 id="f932" class="mk lt jg bd lu ml mm dn ly mn mo dp mc le mp mq me li mr ms mg lm mt mu mi mv bi translated">摘要</h2><p id="aabb" class="pw-post-body-paragraph kv kw jg kx b ky mw kh la lb mx kk ld le my lg lh li mz lk ll lm na lo lp lq ij bi translated">有趣的是，在标准优化器失败的三种情况下，我们都可以通过ADAM获得明显更好的结果。但在所有其他情况下，亚当都没有达到我们用<code class="fe nb nc nd ne b">Descent</code>获得的水平。在这些情况下，迭代次数越少，收敛速度越快，但最终还是于事无补。因此，我们可以得出结论，这两种优化器可以很好地互补，但没有一种优化器可以适用于所有情况。</p></div></div>    
</body>
</html>