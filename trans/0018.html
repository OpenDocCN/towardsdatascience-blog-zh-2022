<html>
<head>
<title>Logistic Regression and the Feature Scaling Ensemble</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归和特征尺度集成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-and-the-feature-scaling-ensemble-e78a56fc6c1#2022-02-01">https://towardsdatascience.com/logistic-regression-and-the-feature-scaling-ensemble-e78a56fc6c1#2022-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dd5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">线性分类的一个新的优化凹槽</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f6618d6a64d3dcf65f2e9127abb5cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vfO8LckOyXPQIaQDr7K9vw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔希·索伦森在<a class="ae kv" href="https://unsplash.com/s/photos/ensemble-music?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="93fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">首席研究员:戴夫·古根海姆/撰稿人:乌特萨维·瓦赫哈尼</em> </strong></p><p id="da67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">向下跳动</strong></p><p id="471c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这项工作是我们对特征缩放的早期研究的延续(见这里:<a class="ae kv" rel="noopener" target="_blank" href="/the-mystery-of-feature-scaling-is-finally-solved-29a7bb58efc2">特征缩放的秘密最终被解开|由Dave Guggenheim |走向数据科学</a>)。在这个项目中，我们将使用岭正则化逻辑回归在60个数据集上检验15种不同缩放方法的效果。</p><p id="4272" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将展示正则化的强大功能，许多数据集的准确性不受要素比例选择的影响。但是，与原始工作一样，特征缩放集合提供了显著的改进，在这种情况下，尤其是对于多类目标。</p><p id="c60d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">型号定义</strong></p><p id="bf50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们选择L2(岭或吉洪诺夫-米勒)正则化逻辑回归，以满足规模数据的要求。它增加了一个惩罚，该惩罚是系数的平方值之和。这在处理多重共线性时特别有用，并在惩罚模型中不太重要的变量时考虑变量的重要性。</p><p id="2407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本研究中的所有模型都是使用sci-kit学习库中的LogisticRegressionCV算法构建的。所有模型也用分层抽样进行了10倍交叉验证。每个二元分类模型使用以下超参数运行:</p><p id="d15a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)惩罚= 'l2 '</p><p id="ea96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2) cs = 100</p><p id="6545" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3)求解器= 'liblinear '</p><p id="6f2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4) class_weight = 'balanced '</p><p id="924c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5) cv = 10</p><p id="a0ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6) max_iter = 5000</p><p id="f4b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7)得分=“准确性”</p><p id="9118" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8)随机状态= 1</p><p id="e2bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多类分类模型(在结果表中用星号表示)以这种方式进行调整:</p><p id="fc95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)惩罚= 'l2 '</p><p id="ae92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2) cs = 100</p><p id="bd7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3)求解器= 'lbfgs '</p><p id="aa44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4) class_weight = 'balanced '</p><p id="e024" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5) cv = 10</p><p id="fd8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6) max_iter = 20000</p><p id="7eeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7)得分=“准确性”</p><p id="a7cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8)随机状态= 1</p><p id="9db8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">9)多类= '多项'</p><p id="3e41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里的L2惩罚因子解决了使用训练数据和测试数据时预测模型的低效问题。由于具有较高数量预测值的模型面临过度拟合问题，使用L2正则化的岭回归可以利用平方系数惩罚来防止它。</p><p id="fc4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有其他超参数保留各自的默认值。所有模型都是使用这些缩放算法(sci-kit学习包在括号中命名)用特征缩放数据构建的:</p><p id="3915" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a.标准化(标准缩放器)</p><p id="cd22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">b.L2归一化(归一化器；norm='l2 ')</p><p id="45e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">c.鲁棒(RobustScalerquantile_range=(25.0，75.0)，with_centering=True，with_scaling=True)</p><p id="ae8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d.规范化(MinMaxScalerfeature_range =多个值(见下文))</p><p id="2d38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d1。特征范围= (-1，1)</p><p id="0f51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d2。特征范围= (0，1)</p><p id="95fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d3。特征范围= (0，2)</p><p id="b3ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d4。特征范围= (0，3)</p><p id="98b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d5。feature_range = (0，4)</p><p id="ae01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d6。feature_range = (0，5)</p><p id="7008" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d7。特征范围= (0，6)</p><p id="6ca2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d8。feature_range = (0，7)</p><p id="2608" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d9。feature_range = (0，8)</p><p id="6a84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d10。feature_range = (0，9)</p><p id="a359" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">e.具有StackingClassifier的集成:StandardScaler + Norm(0，9)[有关更多信息，请参见特征缩放集成]</p><p id="befb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f.具有堆叠分类器的集成:标准缩放器+鲁棒缩放器[参见特征缩放集成了解更多信息]</p><p id="9833" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了系综之外，使用“fit_transform”将缩放方法应用于训练预测器，然后使用“transform”将缩放方法应用于测试预测器，如许多来源所指定的(例如，Géron，2019，第。66;穆勒和圭多，2016年，第。139;Shmueli，Bruce等人，2019年，第。33;是否应该对机器学习的训练数据和测试数据都进行缩放？— Quora )并由scikit learn为所有特征缩放算法提供。然而，由于它们的设计，集成被迫对原始测试数据进行预测，因为顺序链接缩放算法仅导致最终阶段作为结果出现，并且这甚至没有解决通过建模将两个并行缩放路径组合成一个的复制条件。</p><p id="0cb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每个预测值的样本少于12个的情况下，我们将测试分区限制为不少于10%的总体(Shmueli，Bruce等人，2019，第。29).如果有足够的样本达到合理的预测精度(由样本复杂性泛化误差确定),我们使用统一的50%测试分区大小。在这两个界限之间，我们调整了测试规模，以限制泛化测试误差，与训练样本规模进行权衡(Abu-Mostafa，Magdon-Ismail，&amp; Lin，2012，第。57).对于多类数据，如果目标变量中每个分类级别的样本少于12个，则在建模前会删除这些级别。</p><p id="63db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任何缺失值都使用MissForest算法进行估算，因为该算法在多重共线性、异常值和噪声面前具有很强的鲁棒性。分类预测因子是使用pandas get_dummies函数和丢弃的子类型(drop_first=True)进行一次性编码的。低信息变量(如身份证号码等。)在训练/测试分区之前被丢弃。</p><p id="c6fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建这些模型的目的是比较特征缩放算法，而不是调整模型以获得最佳结果。出于这个原因，我们在模型中引入了尽可能多的默认值，为上述比较创造了一个平台。<strong class="ky ir">所有的性能指标都是根据测试数据</strong>的预测的总体准确度来计算的，并且该指标通过两个阈值来检验:1)作为概化测量的最佳性能的3%以内，以及2)作为预测准确度测量的最佳性能的0.5%以内。这就是我们使用许多数据集的原因——因为方差及其固有的随机性是我们研究的一切的一部分。</p><p id="d474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集</strong></p><p id="3d90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本分析中使用的60个数据集如表1所示，具有广泛的预测器类型和类(二元和多元类)。大多数数据集可以在UCI索引(UCI机器学习库:数据集)中找到。不在UCI索引中的数据集都是开源的，可以在Kaggle:</p><p id="e989" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">波士顿住房:波士顿住房| KaggleHR员工流失:员工流失| KaggleLending Club:Lending Club | ka ggle；电信客户流失:电信客户流失| Kaggle丰田卡罗拉:丰田卡罗拉|卡格尔</p><p id="511c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">38个数据集是二项式的，22个是多项式分类模型。所有模型都是根据所有数据集创建和检查的。表中列出的预测数是未编码的(分类的)和所有原始变量，包括排除前的非信息变量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/373dc03730ee7378e12ad85eb12ab1b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*6gzpGo-pzrVU3rzB0Q7TOg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">表1数据集(图片由作者提供)</strong></p></figure><p id="c3e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">特征缩放失配探索</strong></p><p id="2454" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们之前的研究表明，对于预测模型，特征缩放算法的正确选择包括找到与学习模型的不匹配，以防止过度匹配。下面是定义对数损失成本函数的等式，其中添加了L2惩罚因子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/1245ac8bd862607efcab9bcd2afd53c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*tJlnE24zz_khSiX5YNxKXg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图1对数损失成本函数(图片由作者提供)</strong></p></figure><p id="3902" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与基于距离的测量方法不同，对于基于距离的测量方法，归一化是合适的(通过保持相对间距)，而标准化是不合适的，正则化测井损失成本函数不容易确定。不管嵌入的logit函数和它在不匹配方面可能指示什么，增加的惩罚因子应该最小化关于模型性能的任何差异。</p><p id="c974" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了测试这种偏差控制条件，我们构建了相同的归一化模型，从feature_range = (0，1)到feature_range = (0，9)依次循环。每个阶段的训练和测试集精度被捕获并标绘，训练用蓝色，测试用橙色。</p><p id="e6e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个比较图的左侧面板显示了使用<strong class="ky ir">非正则化</strong>支持向量分类器将特征范围从零增加一个单位到九的效果。右图显示了相同的数据和模型选择参数，但使用了<strong class="ky ir"> L2正则化</strong>逻辑回归模型。</p><p id="6cbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们在不改变数据或模型的任何其他方面的情况下增加特征范围时，较低的偏差是非正则化学习模型的结果，而对正则化版本几乎没有影响。请注意，y轴并不相同，应单独参考。请参考图2–7，了解这种现象的示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lw"><img src="../Images/f158893cd2bc67d8e64b8ffb1ffa4330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFb5zBtnRaqM2ESdvvY1GA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图2澳洲信用二元模型对比(图片由作者提供)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/4cedde54fdf0a3b3aafa3f63d948bbbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzqykpZfQDbZwQfd8yW1Cw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图3汽车多类车型对比(图片由作者提供)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ly"><img src="../Images/ee612072cbaceb5dd34a2307424f62ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOV5BUm7FSVaY8v34Xoedw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图4信贷审批二元模型对比(图片由作者提供)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/7412f6d4f8fd2dd895a8666751712f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N77mo2J1fRd4kH7sAY__tQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图5德国信用二元模型对比(图片由作者提供)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/32733ac08d8040873f7b0d5bc29a569d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*410JtZDL0622C0vAU1fcMg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图6字母识别多类模型对比(图片由作者提供)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ma"><img src="../Images/d2b696cf568db7dbd3e4ada01ea2738e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Q3rSWBLQpjwWx-_ITkovw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图7人力资源流失(员工流失)二元模型对比(图片由作者提供)</strong></p></figure><p id="316a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">诚然，这两种不同的学习模型可能不会以相同的方式对归一化范围的扩展做出响应，但是不管怎样，正则化模型确实展示了一种偏差控制机制。</p><p id="51ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">特征缩放集合</strong></p><p id="1e33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于13个单一特征缩放模型生成的结果，这两个集合被构建为满足概括和预测性能结果(见图8)。使用scikit learn中的make_pipeline函数构建缩放路径，用于创建三个估计量:1)标准化+L2逻辑回归，2)范数(0，9)+L2逻辑回归，以及3)稳健缩放+L2逻辑回归。投票分类器作为最后一级进行了测试，但由于性能不佳而被拒绝，因此对两个集成使用堆叠分类器作为最终估计器。除了对每个管道进行10重交叉验证之外，还对堆叠分类器进行了10重交叉验证。所有其他超参数被设置为其先前指定的值或默认值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/149078906156faf1de8205a144137fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gm2SwyEjxOHGOwho6HSyZg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图8特征缩放系综构造</strong></p></figure><p id="fcc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">初步结果</strong></p><p id="deee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关15个特征缩放算法(13个单独算法和2个集合算法)的综合性能的详细信息，请参考图9。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/14301dd48e71d6a8568eb60bd0958495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*WJ6Bg817W-tGeS729QtxmA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图9概括表现(图片由作者提供)</strong></p></figure><p id="418f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如所料，单个特征缩放算法在<strong class="ky ir">广义性能</strong>方面几乎没有差异。在图9中，可以看到通过正则化实现的等式，因此，除了L2归一化，性能最低的solo算法(Norm(0，9) = 41)和性能最佳的solo算法(Norm(0，4) = 45)之间只有四个数据集的差异。STACK_ROB特征缩放集成将最佳计数提高了另外8个数据集，达到53个，代表集成推广的60个数据集的88%。</p><p id="e4b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">预测性能</strong>的情况下，单独特征缩放算法之间存在较大差异。在图10中，可以看到跨数据集的更大范围的计数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/c4cdf932d92b23104f9b0306bf42ffd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*bYi86hgnGwYV_2437M61YQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图10预测性能(图片由作者提供)</strong></p></figure><p id="06f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">排除L2归一化，最低性能的solo算法和最佳solo算法之间的最大差异是11个数据集((StandardScaler = 21)和(Norm(0，5))= 32)，而不是由泛化度量表示的4个。STACK_ROB特征缩放集成将最佳计数提高了另外12个数据集，达到44个，或者在所有60个数据集上比最佳solo算法提高了20%。</p><p id="b7da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种不寻常的现象，即预测性能的提高，无法通过检查特征缩放组合的整体性能图来解释(见图11)。相反，为了更好地理解，我们需要深入研究原始的比较数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi md"><img src="../Images/4bb6b87b919715a428a91d077b620101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*Gq4w4YXMhG7-Z9MAkHMRWA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图11特征缩放综合整体性能(图片由作者提供)</strong></p></figure><p id="5c1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">您所看到的是正确的——在这项研究中，特征缩放集成为超过一半的数据集提供了新的最佳准确性指标！</em></p><p id="9d4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">全部结果和新发现</strong></p><p id="9407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">表2以多种方式进行颜色编码。首先，左边的列表示60个数据集，多类目标用黄色突出显示并在数据集名称后有一个星号来标识。接下来，彩色编码的单元格表示与最佳独奏方法的百分比差异，该方法是100%点。为了清楚起见，颜色编码的单元格并不显示绝对差异，而是显示百分比差异。单元格中的绿色表示相对于最佳独奏方法实现了最佳情况下的性能，或者在最佳独奏精度的0.5%以内。单元格中的黄色表示泛化性能，或在最佳独奏精度的3%以内。如果数据集一直显示为绿色或黄色，则表明正则化的有效性，因为性能差异极小。单元格中的红色显示超出3%阈值的性能，单元格中的数字显示它与最佳独奏方法的目标性能的差距，以百分比表示。最后，蓝色，超级性能者，以百分比显示高于和超过最佳solo算法的性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/ccc0e7cf559e8a88d6561a4de884cf1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*gb5w53AZ6B7hbvkPu-Zs1g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">表2完整对比结果(图片由作者提供)</strong></p></figure><p id="a5fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在回顾对比数据时，我们注意到一些有趣的事情——在多类目标变量上的正微分预测性能。</p><p id="e2c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">二进制对多类性能</strong></p><p id="3ce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在38个二进制分类数据集中，STACK_ROB特征缩放集成对33个数据集的泛化性能和26个数据集的预测性能进行了评分(见表3)。对于二元目标，这些结果代表87%的概括和68%的预测性能，或者这两个度量之间的19点差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/883c36f76c3549ed922755d0ed227338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*GAGXMIHVICymJUHEm7BdIw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">表3二进制分类对比结果(图片由作者提供)</strong></p></figure><p id="f5e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在22个多类数据集中，特征缩放集成对20个数据集的泛化性能进行了评分，仅比大多数solo算法多一个(见图12)。然而，这些相同的特征缩放集成对18个数据集的预测性能进行了评分(见图13)，从而缩小了泛化和预测之间的差距。换句话说，特征缩放集成在22个多类数据集上实现了91%的泛化和82%的预测准确性，这是一个9点的差异，而不是二进制目标变量的19点差异。</p><p id="4543" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这18个处于最佳性能的数据集当中，有15个提供了新的最佳准确性指标(超级性能)。多类比较分析见表4。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/60314d5631e0fb2a6105daf201a37d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*46_Si-MEHOmMx3rNlgzX_w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图13多类的泛化性能(按作者分类的图像)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/01b61cd526f3e57705a7ddd3c453a0d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3RkSlWN89wq6kHFyEQtk_Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">图14多类预测性能(按作者分类的图像)</strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/61b6411430039bdc7bbb17420cb15cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3Noip4d6QYUt_AmY268XBg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd lu">表4多类比较结果(图片由作者提供)</strong></p></figure><p id="56cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">STACK _ ROB特征缩放集合</strong></p><p id="926b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图15显示了通过在所有60个数据集上结合标准化和稳健扩展所提供的性能对比检查。小于零的数字表示STACK_ROB无法满足以最佳solo算法的百分比表示的缩放精度的数据集。数字为零表示达到100%的最佳独奏准确度，而数字大于零表示表现优异，y轴表示相对于最佳独奏方法的改进百分比。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/b2cd0de9b4cb8996f16d1ed6f37ebbd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*s1A7wsuWZ8AxEtaBOxm6pw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图15 STACK_ROB性能(图片由作者提供)</p></figure><p id="4591" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="9ec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的工作表明，正则化在最小化特征缩放模式之间的精度差异方面是有效的，因此缩放的选择不像非正则化模型那样重要。尽管正则化有偏差控制效应，但预测性能结果表明，对于逻辑回归，标准化是合适的，而归一化是不合适的。</p><p id="ba5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，正如我们在之前的研究中所证实的那样，特性扩展集成，尤其是STACK_ROB，能够带来显著的性能提升。在这种情况下，多类预测准确性有了决定性的提高，预测性能缩小了与通用指标的差距。如果您尚未考虑使用逻辑回归来解决多项式问题，STACK_ROB要素缩放集成可能会改变您的想法。</p><p id="a003" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您的L2正则化逻辑回归模型不支持处理要素缩放集合所需的时间，那么使用0到4或5个要素范围(Norm(0，4)或Norm(0，5))的归一化对于概化和预测都具有不错的性能。至少，这是你寻找最优的一个好的起点。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="8768" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">致谢</strong></p><p id="e6ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想对Utsav Vachhani一年多来为解决特征缩放之谜所付出的不懈努力表示最深切的感谢，这导致了特征缩放合集的创建。很简单，没有他的贡献，这篇论文和所有未来的功能缩放集成工作将不会存在。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="3c90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="8889" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Abu-Mostafa，Y. S .、Magdon-Ismail，m .、和Lin，H.-T. (2012年)。<em class="ls">从数据中学习</em>(第4卷)。美国纽约AMLBook</p><p id="608c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Géron，A. (2019)。<em class="ls">使用Scikit-Learn、Keras和TensorFlow进行动手机器学习:构建智能系统的概念、工具和技术</em>。奥莱利媒体。</p><p id="20d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">米勒和圭多(2016年)。<em class="ls">Python机器学习简介:数据科学家指南</em>。奥赖利媒体公司。</p><p id="b8d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Shmueli，g .，Bruce，P. C .，Gedeck，p .，&amp; Patel，N. R. (2019)。<em class="ls">商业分析的数据挖掘:Python中的概念、技术和应用</em>。约翰·威利的儿子们。</p><p id="d022" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scikit-learn_developers。(未注明)。<code class="fe ml mm mn mo b"><a class="ae kv" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model" rel="noopener ugc nofollow" target="_blank">sklearn.linear_model</a></code>。后勤注册文件。从<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression" rel="noopener ugc nofollow" target="_blank"> sklearn.linear_model中检索。LogisticRegressionCV—sci kit-learn 1 . 0 . 2文档</a></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="bf88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集来源</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6ccbef8519d01d4e2640a69d45e4a3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*ugfBANknIT5Ni3DcdtIvfA.png"/></div></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="6057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">联系方式:</strong></p><p id="7643" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">戴夫·古根汉姆:参见作者简介和简历，<a class="ae kv" href="mailto:dguggen@gmail.com" rel="noopener ugc nofollow" target="_blank">dguggen@gmail.com</a></p><p id="764f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">乌特萨维·瓦赫哈尼:<a class="ae kv" href="https://www.linkedin.com/in/uv29/" rel="noopener ugc nofollow" target="_blank">领英简历</a>，<a class="ae kv" href="mailto:uk.vachhani@gmail.com" rel="noopener ugc nofollow" target="_blank">uk.vachhani@gmail.com</a></p></div></div>    
</body>
</html>