<html>
<head>
<title>Residual Blocks in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的剩余块</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/residual-blocks-in-deep-learning-11d95ca12b00#2022-07-11">https://towardsdatascience.com/residual-blocks-in-deep-learning-11d95ca12b00#2022-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="91a8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">ResNet论文中首次引入的残差块解决了神经网络退化问题</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8a1e79aded050142126812ad9252982a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snW_pNG60JGLWsPNrPoBgA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图0:随着深度神经网络的深入，其退化的真实生活模拟(图片由作者提供)</p></figure><p id="5f76" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">深度神经网络是主要机器学习算法的发电站。这些神经网络是堆叠层(每层有一些神经元)的集合，它们组合起来执行给定的任务。因此，随着我们将更多的层堆叠在一起，即更深或增加模型的深度，我们期望提高性能。</p><p id="62e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，这些深度神经网络受到退化问题的困扰。那么神经网络中的退化到底是什么？如何解决这个问题？这些是残余块破译的一些问题。</p><p id="1275" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇博客中，我们将学习以下一组基本概念(按顺序排列),这些概念增强了数据科学领域的创新能力。</p><ol class=""><li id="8457" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">深度神经网络简介</li><li id="6abb" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">深度神经网络中的退化问题</li><li id="02c8" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">剩余块(解决问题的方法)</li><li id="d58e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">身份功能</li><li id="694e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">不同类型神经网络中的剩余块</li><li id="4631" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">结论</li></ol><p id="91bc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们开始…</p><p id="58b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，在开始之前，如果你希望更深入地了解卷积(残差块是在牢记卷积神经网络的基础上开发的)，请阅读下面的博客:<a class="ae mf" rel="noopener" target="_blank" href="/computer-vision-convolution-basics-2d0ae3b79346">https://towardsdatascience . com/computer-vision-convolution-basics-2d 0 AE 3b 79346</a></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="33aa" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">深度神经网络简介</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/d88467cbb643e9dc7fcba2deb8aa0774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NDJp6fsFy3uaKJhzHvIUQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:用于二元分类的深度神经网络(图片由作者提供)</p></figure><p id="8e68" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图代表了一个由层叠CNN组成的深度神经网络，对输入图像进行二值分类(是或否)。</p><p id="bd72" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们把输入看作<strong class="kx ir">‘x’</strong>，输出看作<strong class="kx ir">“H(x)</strong>。CNN操作本质上是线性的，并且激活函数(假设relu)用于学习抽象特征(非线性)。这个网络的全部目的是找到最佳映射(拟合)，即函数</p><blockquote class="ng nh ni"><p id="8063" class="kv kw nj kx b ky kz jr la lb lc ju ld nk lf lg lh nl lj lk ll nm ln lo lp lq ij bi translated">F(x) -&gt; H(x)</p></blockquote><p id="733f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，如果我们想提高模型性能，堆叠更多的层有帮助吗？理想情况下，它应该提高性能，因为更多的神经元可用于学习抽象特征。但这并不是每次都管用。该网络面临多个问题，第一个是消失/爆炸梯度的问题。这个问题可以通过规格化初始化和中间规格化层、权重裁剪等等来有效解决。它使模型能够开始收敛于具有反向传播的随机梯度下降(或任何其他优化器)。</p><blockquote class="ng nh ni"><p id="0c07" class="kv kw nj kx b ky kz jr la lb lc ju ld nk lf lg lh nl lj lk ll nm ln lo lp lq ij bi translated">一旦更深的模型开始收敛(在考虑消失/爆炸梯度之后)，我们可能会有另一个问题。是<strong class="kx ir">退化</strong>。</p></blockquote><h1 id="34e3" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated">深度神经网络中的退化问题</h1><p id="ceeb" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">那么到底什么是退化呢？</p><p id="688a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着我们增加网络的深度，精确度会饱和。也许这些层已经充分了解了我们数据的所有复杂性。但是，我们对精度不满意，我们又叠加了几层。现在，模型开始退化。这是意料之外的，但这种退化不是因为过拟合(训练误差增加)。附加层导致更高的训练误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/aee72066408c25f055266b9c10922d66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZOtURyfn7cWpWjyk7bCpw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:训练错误(左)和测试错误(右)。Red是更深的网络，并且具有更高的训练和测试误差。(图片由<a class="ae mf" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">明凯</a>提供)</p></figure><p id="9820" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">退化间接暗示了并非所有的系统都是相似的或容易优化的。如果浅层网络正在学习特征，假设获得90%的准确性，那么同一模型的深层变体甚至可能获得低于90%的准确性。这是因为随着网络深度的增加，优化变得非常困难。</p><p id="eb20" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，如何解决这个问题呢？</p><h1 id="abca" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated">残留块(消除退化的方法)</h1><p id="a3ec" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">让我们考虑两个网络，网络<strong class="kx ir">“A”</strong>:浅层网络和网络<strong class="kx ir">“B”</strong>:网络A的更深层对等物。现在，如果不是更好，至少我们希望实现与网络B相同的性能。因此，给定网络A，我们将添加身份映射作为额外的层来构造B。这种设置应确保更深层的模型B不会比其更浅层的对等物产生更高的训练错误。但是实验表明，这种设置无法找到相对更好或相同性能的解决方案。</p><p id="4013" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这证明了模型在优化期间学习身份映射是非常困难的。</p><p id="d35c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了解决退化的问题，我们有一个叫做<strong class="kx ir">跳跃连接</strong>的设置。或者称为快捷连接，它跳过架构中的一些层，并将前一层的输出提供给当前位置，从而有助于优化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/8fcb9b96238145c7a1f836e13e72b0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Zm7h19WrRlNuMNBBeqi00w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:跳过连接的剩余块(图片由<a class="ae mf" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">明凯</a>提供)</p></figure><p id="4480" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上图中，skip-connection跳过两层，直接将输入“x”作为输出。这被称为快捷方式/跳过连接，因为它不涉及任何额外的参数，因为我们只是将先前的信息传递给层。</p><p id="2854" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个称为深度剩余学习的框架用于解决退化问题。</p><p id="c89e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在前面的部分中，我们学习了网络学习正确的映射，即F(x) -&gt; H(x)，其中x是输入，H(x)是要拟合的底层映射(预期输出)，我们尝试拟合网络并获得类似H(x)的F(x)。</p><p id="1d2a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，如果我们在同一个设置中添加一个跳过连接(作为身份函数)，我们会得到以下等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/def099730c470339d02237ddf75bddab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qo9ashvLDzARkmYVtc0Rrg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:带有残差块的网络的更新方程(图片由作者提供)</p></figure><p id="0997" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上图中，我们有了一个新的F(x)，即残差(预期输出和输入之间的差)。堆叠层尝试学习残差和ta-da的映射！这就是它被称为残余块的原因。</p><p id="3226" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">原来的映射F(x) -&gt; H(x)现在是H(x)-x(剩余)。堆叠的非线性层试图拟合残差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/43f411a0fc0ba0c686fb4983b70b3179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJk8kG8H_An1E1wjtvnITw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:带有残余块的前向传播(图片由作者提供)</p></figure><p id="2ddf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图5证明了跳过连接只是执行身份映射。它们的输出被添加到堆叠层的输出中，并且由于某种原因，如果F(x)趋向于零，则由于身份映射，我们的模型将仍然具有非零权重。这消除了退化。</p><p id="7306" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑一个神经网络，其中一些堆叠的层增加了值，而一些层仅仅为零。但是由于残留块，我们能够保持权重并不断优化和获得更好的精度。为此，将残差F(x)推至零比通过一堆非线性层拟合身份映射更容易。跳过连接是消除更深层神经网络退化的一种幸事。</p><blockquote class="ng nh ni"><p id="4c50" class="kv kw nj kx b ky kz jr la lb lc ju ld nk lf lg lh nl lj lk ll nm ln lo lp lq ij bi translated">残差块的整个思想来源于这样一个假设，如果多个非线性层可以逼近复杂函数，那么等价于假设它们也可以逼近残差函数，即H(x)-x。</p></blockquote><p id="a62f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，如果x和F(x)的维数不同呢？</p><h1 id="79cd" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated">身份映射</h1><p id="ba62" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">快捷连接(身份映射)既不引入额外的参数，也不增加计算复杂度。</p><p id="786e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nj"> y = </em> F(x，{Wi}) <em class="nj"> + x，</em>其中x是身份映射</p><p id="0de4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，如果身份映射和堆叠层的维度不同，那么我们就不能添加它们。为了解决这个问题，我们在身份函数中执行线性投影(使用CNN)或额外的零条目(称为填充)来匹配维度。我们有以下等式:</p><p id="01ba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">y = F(x，{Wi}) + W*x，其中W是线性投影</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/74089b24a058a54ad718cf63a4538987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rQwjjDPus8UY6-dzqCBPw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:跳过连接的残差块(左)和线性变换跳过连接的残差块(右)以匹配维度(图片由作者提供)</p></figure><p id="87af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常，残差函数具有两个或三个堆叠层，然而更多层是可能的，但是它可以减少残差块的影响。但是，如果F(x)只有一层，它非常类似于线性层，因此:</p><p id="27e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nj"> y = W1*x + x </em></p><p id="62a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种设置无助于解决退化问题。因此，我们可以假设，通常一个残差块有2-3个堆叠层，并有一个跳跃连接。我们可以使用堆叠的剩余块来创建更深层次的神经网络。</p><p id="ed20" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">终于！！我们已经学习了使更深层次的神经网络能够优化和继续学习的残差块。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="80d6" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">不同神经网络中的剩余块</h1><p id="4d96" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">所以，我们只是在一个简单的神经网络上探索了剩余块。但是，我们如何应用相同的块来解决计算机视觉任务，准确地说是卷积神经网络，或者解决使用顺序网络的任务。</p><p id="0f8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">残差块可以在没有任何修改的情况下与卷积神经网络一起使用。在CNN中，堆叠层的输出会发生变化，但方法完全相同。</p><p id="6afa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于时序网络，我们有一个称为<strong class="kx ir">高速公路网络的网络。</strong>高速公路网是附加大门的捷径连接。这些门是数据相关的，并且有一些参数，而剩余网络中的快捷连接没有任何参数。这些门决定是否传递信息。它作为一个调制跳跃连接来调节信息流。这些网络受<strong class="kx ir"> LSTM式</strong>网络的启发，在那里我们有多个门来遗忘、更新和输出信息。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/d9060a0d16474e00b446f6f835bcc98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OaCzVQ9toZ7Bin3b.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:使用循环跳跃层的LSTNet架构，类似于具有跳跃连接的剩余块的功能(图片由<a class="ae mf" href="https://arxiv.org/abs/1703.07015" rel="noopener ugc nofollow" target="_blank">郭坤</a>提供)</p></figure><p id="0e56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">或者，对于时序网络，让我们以时间序列数据为例。就像残差块一样，我们可以创建一个跳过RNN连接来跳过输入中的几个时间步长(通过以有序的方式跳过一些时间步长)。LSTNet是一个最先进的模型，为时间序列数据实现了跳跃RNN连接。人们可以通过看一看来加深对斯基普-RNN的理解。</p><h1 id="cfe3" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated"><strong class="ak">结论</strong></h1><p id="2dfa" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">残差块是ResNet的基本单元，ResNet是用于从图像提取特征的SOTA模型。它继续被用于解决深度神经网络中的退化问题。在当今世界，超过90%的架构使用基于跳过连接的网络来开发特征嵌入。从ResNet到Transformer再到BERT，残差块的重要性已经被证明是非常重要的，这将继续成为未来许多创新的一部分。</p><p id="2a5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">读完整篇博客后，你应该能够回答以下问题:</p><ol class=""><li id="dcd6" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">什么是神经网络中的退化？</li><li id="d646" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">降级和过度拟合有什么不同？</li><li id="c8a3" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">如何解决深度网络中的退化问题？</li><li id="8c60" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">什么是跳过连接？</li><li id="5e61" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">什么是残块？</li><li id="ae01" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">为什么残差块被称为残差块？</li><li id="1d50" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">什么是高速公路网？</li><li id="650b" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">跳跃连接和高速公路网有什么关系？</li><li id="1d06" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">我们能在顺序网络中使用剩余块吗？</li><li id="ae40" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">剩余块中的恒等式(函数)是什么？</li><li id="ec69" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">叠加层的同一性和输出的维度总是一样的吗？如果没有，如何处理这种情况？</li></ol><p id="d0de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是这篇博客回答的几个问题。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="7fce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望这个博客解决了你追求知识的愿望。如果你喜欢这个博客，你可能会喜欢TDS上的一些其他读物。</p><ol class=""><li id="38da" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/computer-vision-convolution-basics-2d0ae3b79346">计算机视觉:卷积基础知识</a></li><li id="4c95" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/dropout-in-neural-networks-47a162d621d9">神经网络辍学</a></li><li id="a2d6" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/preserving-data-privacy-in-deep-learning-part-1-a04894f78029">联邦学习基础</a></li><li id="b4d3" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/preserving-data-privacy-in-deep-learning-part-2-6c2e9494398b">非IID/真实世界数据集</a></li><li id="2ab7" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/preserving-data-privacy-in-deep-learning-part-3-ae2103c40c22">基于CIFAR10的加权平均联合学习</a></li></ol><p id="dd7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">再见…直到下一个博客:D</p><h1 id="5ed6" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated">参考</h1><p id="48bc" class="pw-post-body-paragraph kv kw iq kx b ky ns jr la lb nt ju ld le nu lg lh li nv lk ll lm nw lo lp lq ij bi translated">[1]何，深度残差学习用于图像识别，<a class="ae mf" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="b897" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] Prem Oommem，ResNets —残差块&amp;深度残差学习，<a class="ae mf" rel="noopener" target="_blank" href="/resnets-residual-blocks-deep-residual-learning-a231a0ee73d2">https://towards data science . com/ResNets—残差块—深度残差学习-a231a0ee73d2 </a></p><p id="d5d4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] Sabyasachi，ResNet的剩余块-构建块，<a class="ae mf" rel="noopener" target="_blank" href="/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec">https://towards data science . com/Residual-blocks-Building-blocks-of-ResNet-FD 90 ca 15d 6 EC</a></p><p id="51d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]赖国坤，用深度神经网络建模长短期时间模式，<a class="ae mf" href="https://arxiv.org/abs/1703.07015" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>