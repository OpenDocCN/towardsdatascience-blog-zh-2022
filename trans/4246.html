<html>
<head>
<title>Smart Distributed Training on Amazon SageMaker with SMD: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带SMD的Amazon SageMaker上的智能分布式培训:第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee#2022-09-20">https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee#2022-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d908" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何选择与您的训练实例的功能相一致的分发算法来增加吞吐量和降低成本</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/85d4b7462f5f8b42eb1a7fb80ce14346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QuK1ce-io3h16fNe"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">雅尼克·菲舍尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f733" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">优化训练作业运行时性能的一个关键步骤是调整算法，以便最大限度地利用训练环境中的资源。这需要彻底了解你的资源，(计算设备的数量和类型，可用内存，通信带宽等。)以及分析其利用情况的适当工具。调整您的训练算法以充分利用您的资源可以提高您的训练速度并降低您的训练成本。这尤其适用于您的<strong class="ky ir">分布式训练算法</strong>，它严重依赖于运行在多个处理器(例如，GPU)上的多个进程之间的高速通信。未能考虑培训环境的具体情况会导致沟通瓶颈、高延迟、培训速度降低和成本增加。</p><p id="3fbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是关于优化分布式培训主题的三篇文章的第一部分。在第一部分中，我们将简要回顾执行分布式培训的不同方法，同时强调它们对底层培训环境的依赖性。在第二和第三部分中，我们将展示两个例子，说明分布式训练方法和算法的选择如何影响训练速度。运行分布式培训有许多不同的流行框架。在本帖中，我们将使用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker的分布式培训库</a> (SMD)进行演示。虽然我们的重点是Amazon SageMaker环境，但是这篇文章的内容同样适用于任何其他培训环境和分发算法。这篇文章中的框架选择不应该被看作是一种认可。对你来说，正确的选择将高度依赖于你的项目的细节，并应考虑到最新的技术和实用工具。</p><h1 id="cda4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">分布式培训概述</h1><p id="aa3f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">有关分布式培训的更详细概述，请参见<a class="ae kv" href="https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/distributed-training.html" rel="noopener ugc nofollow" target="_blank">此处</a>和<a class="ae kv" href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="235d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在分布式培训作业中，对多名员工进行培训。在这篇文章中，我们将假设工人是GPU。分配培训工作有两个主要原因:</p><ol class=""><li id="218d" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir">训练加速</strong>:通过结合多个GPU的力量，我们可以加快训练过程。</li><li id="2a66" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated"><strong class="ky ir">大模型尺寸</strong>:当一个模型太大而不适合单个GPU的内存时，我们需要多个GPU来训练。</li></ol><p id="809a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分布式训练有两种，<strong class="ky ir">数据并行</strong>训练(又名<strong class="ky ir">数据分发</strong>)和<strong class="ky ir">模型并行</strong>训练(又名<strong class="ky ir">模型分发</strong>)。</p><p id="c899" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">数据并行</strong>分布式训练中，每个GPU维护其自己的完整模型副本，并对训练数据的不同子集(本地批次)执行每个训练步骤。在每个训练步骤之后，它发布其结果梯度，并考虑所有GPU学习的组合知识来更新其自己的模型。用<em class="nd"> k </em>表示GPU的数量，用<em class="nd"> b </em>表示<strong class="ky ir">局部批量</strong>，对<em class="nd">k</em>GPU进行分布式训练的结果是，在每一个训练步骤中，模型都在<em class="nd"> k*b </em>样本的<strong class="ky ir">全局批量</strong>上进行训练。理想情况下，在<em class="nd"> k </em>个GPU上执行数据分布式训练会将训练速度提高数倍<em class="nd"> k </em>。然而，由于<em class="nd">梯度共享</em>的额外开销，这种线性比例加速不应被视为理所当然。实际的训练加速将取决于许多因素，包括GPU间的通信带宽、用于共享梯度的算法以及模型架构。查看<a class="ae kv" rel="noopener" target="_blank" href="/a-guide-to-highly-distributed-dnn-training-9e4814fb8bd3">这篇博文</a>了解更多关于数据分布式培训的信息。</p><p id="ffb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">模型并行</strong>分布式训练中，模型分布在几个GPU上。有几种方法可以分布模型，包括:垂直(管道并行)、水平(张量并行)和通过模型分片。</p><p id="f419" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">流水线并行</strong>解决方案中，模型的单个副本将被分成<em class="nd"> N </em>个部分，每个部分将包含一个或多个层。每个部分都将存储在其中一个GPU上。训练数据将被输入到托管输入层的GPU中，并将向前流经模型的<em class="nd"> N </em>部分，然后向后用于梯度计算。在向前和向后传递期间，模型部分之间的数据流在宿主GPU之间传递。流水线并行化的主要挑战之一是试图减少GPU空闲时间。在一个简单的实现中，您会发现在训练步骤中的任何给定点只有一个GPU是活动的。在实践中，现代流水线算法将输入样本分成微批，并使用复杂的调度算法来减少空闲时间。</p><p id="e9ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">张量并行</strong>解决方案中，<strong class="ky ir"> </strong>有时被称为<strong class="ky ir">张量切片</strong>，模型张量的子集将跨GPU划分，并且将添加适当的通信操作以跨GPU传输输入和输出数据。请注意，虽然被归类为模型并行技术，但张量并行与数据并行有一些共同的属性，即每个GPU都有自己独特的小批量数据，并且没有并行化的张量是重复的。参见<a class="ae kv" href="https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/model-parallel-extended-features-pytorch-tensor-parallelism-how-it-works.html" rel="noopener ugc nofollow" target="_blank">此处</a>了解张量并行性如何工作的更多细节。</p><p id="fe1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<strong class="ky ir">分片数据并行</strong>中，有时也称为<strong class="ky ir">零功耗数据并行</strong>，模型参数在所有GPU之间进行分片。每个参数将驻留在单个GPU上，由单个GPU拥有和更新。与标准数据并行性一样，完整的训练步骤将在每个独立的小批量GPU上执行。当GPU达到存储在<em class="nd"> gpu_j </em>上的<em class="nd">参数_i </em>时，它将需要从<em class="nd"> gpu_j </em>中提取其权重来执行所需的计算，但它随后会立即删除权重，以便它不会占用任何本地内存。这在向前和向后传球中都发生。一旦计算出梯度更新，GPU需要将它们传达给它们各自参数的所有者。注意，这种方法有时(例如，这里的<a class="ae kv" href="https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/" rel="noopener ugc nofollow" target="_blank">是</a>)被归类为数据并行方法，而不是模型并行方法。</p><p id="fd9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3D并行</strong>是在<a class="ae kv" href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/" rel="noopener ugc nofollow" target="_blank">这篇论文</a>中引入的一个术语，指的是上面讨论的数据并行和模型并行策略的任意组合。3D并行技术的目标是结合各种技术的优势。</p><p id="b73a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">所有方法的共同点是数据在GPU之间传递。然而，GPU之间的通信路径的数量和细节、每个训练步骤的数据跳数以及传递的数据量可能会有很大差异。</strong></p><p id="94cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你可能会对自己说，“哇，这么多进行分布式培训的方法，多么好的概述啊，但是我怎么知道该用哪一种呢？”。我想让你知道我能感受到你，我的朋友。事实是，弄清楚这一点并不简单。(我想说这是NP难的，但我手头没有资料支持这一点。)最佳选择将取决于您的模型的细节以及您的基础培训环境的细节。如果你的模型足够小，可以放入单个GPU的内存中，那么你可能会选择<strong class="ky ir">数据并行分布式</strong>算法。如果您的模型太大，不适合单个GPU，但GPU之间的通信带宽特别小，那么您可能会发现管道并行化是最佳选择。</p><p id="c303" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多资源(例如这里的<a class="ae kv" href="https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/#deciding-which-technology-to-use" rel="noopener ugc nofollow" target="_blank">和这里的</a><a class="ae kv" href="https://medium.com/pytorch/pytorch-data-parallel-best-practices-on-google-cloud-6c8da2be180d" rel="noopener">试图根据模型和环境的细节来提供决策指南。一旦选择了高级策略，包括SMD在内的一些库将自动执行一些特定的配置。在撰写本文时，还没有API(据我们所知)能够自动完成寻找和构建最佳分布式训练拓扑的整个过程。我们只能希望在某个地方，有人正在研究这样的解决方案。</a></p><p id="3dc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一节中，我们将深入研究在选择分布式训练方法和算法时应该考虑的基础训练环境的一个特定方面。</p><h1 id="d7f8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一些GPU到GPU的链接与其他链接不同</h1><p id="c255" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如上所述，所有分布式训练算法都依赖于GPU之间的数据通信。该数据可以是参数权重、梯度和/或激活。该通信可以是直接的或者经由某种形式的中介(例如，参数服务器)。它可以通过几种不同的介质，如以太网、pci或<a class="ae kv" href="https://en.wikipedia.org/wiki/NVLink" rel="noopener ugc nofollow" target="_blank"> NVLink </a>。在大多数现代训练系统中，NVLink支持同一实例中GPU之间的最高数据传输速率。因此，许多分布式训练算法将选择直接的GPU到GPU数据传输。在许多情况下——当然，如果所有的GPU都在一个实例上——这确实可能是最佳选择。然而，当在多个实例上训练时，每个实例都有多个GPU，<strong class="ky ir">重要的是要意识到这样一个事实，即两个不同实例上的两个GPU之间的数据传输速度和延迟可能会与同一实例上的两个GPU有很大不同</strong>。这有几个原因，包括:</p><ol class=""><li id="eee4" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">计算实例之间的网络带宽往往比单个实例上直接GPU到GPU链接的带宽低得多。</li><li id="2863" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">网络带宽由与分布式训练算法无关的其他类型的数据通信共享，例如训练样本加载、检查点保存等。</li><li id="e6af" class="mp mq iq ky b kz my lc mz lf na lj nb ln nc lr mu mv mw mx bi translated">实例之间的距离会导致一定量的延迟，这会对分布式训练算法的速度产生负面影响。(注意，在撰写本文时，SageMaker APIs不允许您为所有实例强制使用一个单一的<a class="ae kv" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">集群放置组</a>。)</li></ol><p id="eff6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在某些情况下，您可能有一个专用的网络接口来促进节点间的通信，比如<a class="ae kv" href="https://aws.amazon.com/hpc/efa/" rel="noopener ugc nofollow" target="_blank"> Amazon EFA </a>。然而，它仍然不是单实例直接GPU到GPU链接的对手。</p><p id="d5f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理想情况下，我们希望我们的分布式训练算法能够考虑到不同类型的GPU到GPU连接之间的差异。我们希望这种方法优于一种简单的方法，在这种方法中，所有GPU到GPU的连接都以相同的方式处理。在接下来的部分中，我们将用两种方式对此进行测试，首先使用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank"> SageMaker分布式数据并行</a> (SDP)，然后使用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html" rel="noopener ugc nofollow" target="_blank"> SageMaker分布式模型并行</a> (SMP)。</p><h1 id="2b00" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">后续步骤</h1><p id="a9ca" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">请务必查看帖子的第二部分，我们将在其中演示<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank">亚马逊SageMaker的分布式数据并行库</a>如何以区分节点内和节点间GPU对的方式支持数据分发。</p></div></div>    
</body>
</html>