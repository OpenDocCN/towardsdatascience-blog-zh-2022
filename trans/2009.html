<html>
<head>
<title>How Policy Gradients can get you to the Moon</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">政策梯度如何让你登上月球</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-policy-gradients-in-reinforcement-learning-can-get-you-to-the-moon-15940cbc076a#2022-05-06">https://towardsdatascience.com/how-policy-gradients-in-reinforcement-learning-can-get-you-to-the-moon-15940cbc076a#2022-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0994" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">RL实践课程—第7部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/29355cd757253c813a73f8faab317e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GX6widaYlFYjqjKRlo0hgA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">墨西哥霍尔博克斯的贾戈达和凯(图片由作者提供)</p></figure><p id="519f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">策略梯度</strong>是一系列强大的强化学习算法，可以解决复杂的控制任务。在今天的课程中，我们将从头开始实现普通的渐变政策和<strong class="la iu">登月</strong>🌗。</p><p id="5387" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您是强化学习的新手，请查看<a class="ae lu" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-1-269b50e39d08"> <strong class="la iu">课程简介</strong> </a>以了解基础知识和术语。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/hands-on-reinforcement-learning-course-part-1-269b50e39d08"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">实践强化学习课程—第1部分</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">从零到英雄。循序渐进。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="a630" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，你可以在<a class="ae lu" href="https://github.com/Paulescu/hands-on-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">这个资源库</strong> </a>中找到今天课程的所有代码👇🏽</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://github.com/Paulescu/hands-on-rl"><div class="gh gi mn"><img src="../Images/77c41afc75aaeb0a65be17882dae8ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qQ5wWUOyL9J8vU8LBfGuYw.jpeg"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">喜欢吗？在GitHub上给它一颗星</p></figure><h1 id="bd32" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">内容</h1><ol class=""><li id="d52d" class="ng nh it la b lb ni le nj lh nk ll nl lp nm lt nn no np nq bi translated">政策梯度如何上月球？🚀🌙</li><li id="5ca8" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated"><code class="fe nw nx ny nz b">LunarLander</code>环境</li><li id="bc97" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">基线代理</li><li id="3583" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">欢迎政策梯度🤗</li><li id="5576" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">策略梯度代理</li><li id="ef16" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">关键要点</li><li id="47d1" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">家庭作业📚</li><li id="dc09" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">下一步是什么？</li></ol><h1 id="9be9" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">1.政策梯度如何上月球？🚀🌙</h1><p id="c114" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">为了登上月球，我们使用宇宙飞船，这是一个巨大的、模块化的工程，结合了令人难以置信的力量和温和的精度。</p><p id="0c80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们旅程的开始，我们使用强大的火箭(如下图所示)来帮助宇宙飞船摆脱地球的引力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/d1eb6343b3cbdda5cf92188b27175e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-jFETNHYzU25wpzlkuiIg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两枚火箭将我们的宇宙飞船推入太空</p></figure><p id="d375" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦火箭耗尽，宇宙飞船将它们安全地释放到地球的海洋中，并开始其385，000公里(239，000英里)的月球之旅。</p><p id="8afa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">三天后，我们的宇宙飞船接近月球，并开始绕月球运行。这部精湛戏剧的最后一幕是<strong class="la iu">登陆</strong>。</p><p id="ed7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">宇宙飞船释放了一个装置，月球着陆器，月球的引力场向下拉向它的表面。我们整个任务的成功取决于这个装置的平稳着陆。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/d904b8b5d04564ac47eeff37ad82db2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2ooH2NjOt8J8F8-N.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">月球着陆器(<a class="ae lu" href="https://commons.wikimedia.org/wiki/File:NASA_Selects_First_Commercial_Moon_Landing_Services_for_Artemis_Program_(47974915541).jpg" rel="noopener ugc nofollow" target="_blank">维基共享资源</a>)</p></figure><p id="5558" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在今天的讲座中，我们将使用强化学习来训练一个代理(又名控制器)将这个设备降落在月球上。</p><p id="6955" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，我们将使用OpenAI中的<code class="fe nw nx ny nz b">LunarLander</code>环境作为着陆问题的一个方便(和简化)的抽象。</p><p id="0ad7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，这就是您今天要实现的代理如何在月球上着陆。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><h1 id="717a" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">2.<code class="fe nw nx ny nz b">LunarLander</code>环境</h1><p id="1263" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">在进入建模部分之前，让我们快速熟悉一下<code class="fe nw nx ny nz b">LunarLander</code>环境。</p><h2 id="7d0b" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">状态</h2><p id="97f8" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">代理获得具有8个分量的状态向量:</p><ul class=""><li id="19ca" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">目前在2D世界的位置:<strong class="la iu"> x </strong>和<strong class="la iu"> y </strong></li><li id="1abf" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">沿这两个轴的速度:<strong class="la iu"> vx </strong>和<strong class="la iu"> vy </strong></li><li id="c656" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">相对于垂直方向的角度位置<strong class="la iu"> θ </strong>:</li><li id="32b3" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">角速度<strong class="la iu"> ω </strong>，捕捉设备绕自身轴的旋转。</li><li id="1a1d" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">两个二进制信号(<strong class="la iu"> s1 </strong>和<strong class="la iu"> s2 </strong>)指示设备的两个腿中的哪一个(如果有的话)与地板接触。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/1f1ddae6b90d63bc7afe9da582b4bfce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ys9ZApPIEI_iyQePgEbJbw.png"/></div></div></figure><h2 id="b963" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated"><strong class="ak">动作</strong></h2><p id="1567" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">有4种可能的操作:</p><ul class=""><li id="e11c" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated"><code class="fe nw nx ny nz b">0</code>无所事事。</li><li id="3c83" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">启动左引擎。</li><li id="1347" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated"><code class="fe nw nx ny nz b">2</code>点燃主引擎(中央)。</li><li id="abe9" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated"><code class="fe nw nx ny nz b">3</code>点燃右侧发动机。</li></ul><h2 id="b7d4" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated"><strong class="ak">奖励</strong></h2><p id="ba1b" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">在这种环境下，奖励函数被设计成向代理人提供足够的信号。这使得训练更加容易。</p><p id="4ca2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是着陆器在每个时间步得到的奖励:</p><ul class=""><li id="b383" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">如果着陆器远离着陆垫，它会得到负奖励。</li><li id="2ce6" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">如果发生事故，它将获得额外的-100分。</li><li id="0d0f" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">如果是休息，它会获得额外的+100点。</li><li id="a8e7" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">每条腿着地+10分。</li><li id="057b" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">启动主引擎每帧-0.3分。</li><li id="6aa6" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">发射侧边引擎每帧-0.03分。</li><li id="7556" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">最后，解决了200分。</li></ul><blockquote class="ox oy oz"><p id="e566" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">奖励工程的利弊</em> </strong></p><p id="7e64" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><strong class="la iu">奖励工程</strong>是一种建模技巧，通过向代理提供频繁的反馈来帮助RL代理学习。奖励很少的环境更难解决。</p><p id="c35e" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">然而，它也有两个缺点:</p><p id="753a" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">👉🏽设计奖励功能很难<strong class="la iu"/>，就像你看到<code class="fe nw nx ny nz b">LunarLander-v2</code>奖励后可能想象的那样。在人们找到鼓励代理学习正确行为的奖励函数之前，通常要进行大量的反复试验。</p><p id="1ad5" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">👉🏽奖励塑造是<strong class="la iu">环境特定的</strong>，它将被训练的代理的泛化限制在稍微不同的环境中。此外，这限制了在模拟上训练的RL代理到真实世界环境的可移植性。</p></blockquote><h1 id="a752" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">3.基线代理</h1><p id="2696" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><a class="ae lu" href="https://github.com/Paulescu/hands-on-rl/blob/main/04_lunar_lander/notebooks/01_random_agent_baseline.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/01 _ random _ agent _ baseline . ipynb</a></p><p id="1e6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像往常一样，我们使用一个随机代理来获得问题的快速基线。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><p id="63e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们使用100集来评估这个代理，以获得着陆器的总奖励和成功率:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><p id="4c9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们画出结果</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><p id="d605" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">终于意识到我们的代理人是一个彻底的灾难</p><pre class="kj kk kl km gt pf nz pg ph aw pi bi"><span id="fba6" class="og mp it nz b gy pj pk l pl pm">Reward average -198.35, std 103.93<br/>Succes rate = 0.00%</span></pre><p id="71d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pa">*由于代理固有的随机性，当您在笔记本电脑上运行笔记本时，这些数字不会完全相同。无论如何，成功率几乎肯定是0%。</em></p><p id="7153" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另外，你可以看一段这个随机代理的视频，来说服自己我们还远远没有解决问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">失败时间</p></figure><p id="0a78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实证明，登上月球并不是一件容易的事(多么令人惊讶)</p><p id="6dc2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看<strong class="la iu">政策梯度</strong>如何帮助我们登上月球。</p><h1 id="c00c" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">4.欢迎政策梯度🤗</h1><p id="50e2" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">任何强化学习问题的目标都是找到一个最大化累积回报的<strong class="la iu">最优策略</strong>。</p><p id="4273" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，在课程中，我们已经使用了基于<strong class="la iu">值的</strong>方法，如<a class="ae lu" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-2-1b0828a1046b"> <strong class="la iu"> Q-learning </strong> </a>或<a class="ae lu" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-3-5db40e7938d4"> <strong class="la iu"> SARSA </strong> </a>。这些方法以间接的方式找到最优策略，首先找到最优Q函数，然后使用这个Q函数按照ε-贪婪策略选择下一个最佳动作。这样他们就能得出最优策略。</p><p id="4f38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">另一方面，基于策略的</strong>方法直接找到最优策略，而不必估计最优Q函数。</p><p id="829e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于策略的方法的两个基本步骤如下:</p><h2 id="8b2c" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">第一步。政策参数化</h2><p id="2900" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">我们使用神经网络模型来<em class="pa">参数化</em>最优策略。这个政策被建模为一个<strong class="la iu">随机</strong>政策，与确定性政策相反。我们称之为策略网络，我们将其表示为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/d4c5d5400337f79b244f0481050e3ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/0*bRSQUuNQmrKUYAUf.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略取决于参数<strong class="bd po"> <em class="pp"> θ </em> </strong></p></figure><blockquote class="ox oy oz"><p id="e1f7" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><strong class="la iu">随机政策和勘探与开采的权衡</strong></p><p id="09f0" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">随机策略将一个给定的观察(输入)映射到一个概率列表，每个概率对应一个可能的动作。</p><p id="a77b" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">为了选择下一步要采取的操作，代理会根据操作的概率对其进行采样。因此，随机策略自然地处理探索与利用的权衡，而没有像ε这样的超参数技巧(如Q-learning中使用的ε-贪婪技巧)。</p></blockquote><p id="f4d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe nw nx ny nz b">LunarLander-v2</code>环境中，我们的随机策略将有<strong class="la iu"> 8个输入</strong>和<strong class="la iu"> 4个输出。</strong>4个输出中的每一个都代表采取4个行动中的每一个的概率。</p><p id="7a89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">策略网络的架构是一个您需要试验的超参数。我们今天要用的是一个有64个单元的隐藏层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/21e57e01e603089cfdb0003c73509956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*7hQ3xuV6syLl6QWc7iACbw.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">政策网(图片由作者提供)</p></figure><p id="4c73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解决我们的RL问题就相当于找到了这个神经网络的最优参数。</p><p id="153b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们如何找到这个网络的最佳参数呢？</p><h2 id="a4ce" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">第二步。寻找最佳参数的政策梯度</h2><p id="5b53" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">评估一项政策有多好很容易:</p><ul class=""><li id="6ca6" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">首先，我们对<strong class="la iu"> <em class="pa">、N </em> </strong>随机剧集遵循该策略，并收集每集的总报酬</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/416f7a77d09cce4c47ccf50f4d007fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/0*NGnJp3s_mK_gMsHs.png"/></div></figure><ul class=""><li id="ee43" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">第二，我们平均这个<strong class="la iu"> N </strong>总报酬，得到一个整体绩效指标。这个表达式就是总报酬的<strong class="la iu">经验平均值</strong>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/ea11e71c9609567bde5fabdc90ddcabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T1fb0KXGirSh94Sp.png"/></div></div></figure><p id="6b78" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，总报酬以及J取决于策略参数<strong class="la iu"> <em class="pa"> θ，a </em> </strong>我们的目标是找到使<strong class="la iu"> J </strong>最大化的参数<strong class="la iu"> θ </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/0a5455fd970f26d23d3d36e86231be6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/0*z_8BYDXFEpFL-sCE.png"/></div></figure><p id="c056" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，有不同的算法来解决这个最大化问题(例如，遗传优化)，但通常在深度学习中，基于<strong class="la iu">梯度</strong>的方法是王道👑。</p><h2 id="999c" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">基于梯度的方法是如何工作的？</h2><p id="d048" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">我们从政策参数θ₀的初始猜测开始，我们希望找到一组更好的参数，θ₁</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/995ab8ba3b36dd39cecabd71894e4310.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/0*tJuZOXPhl64nczOU.png"/></div></figure><p id="0806" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">θ₁和θ₀的区别在于我们在参数空间中行进的方向。</p><p id="31f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问题是，增加<strong class="la iu"> <em class="pa"> J </em> </strong>的最佳方向是什么？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/67e4ac56f9346b9c2463bca55b063598.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/0*Z7Kft663JZ9qGThS.png"/></div></figure><p id="efed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而答案就是……这个<strong class="la iu">渐变。</strong></p><p id="a9f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pa">什么事？</em></p><p id="357a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个函数<strong class="la iu"> <em class="pa"> J </em> </strong>相对于一个参数向量<strong class="la iu"> <em class="pa"> θ </em> </strong>的梯度就是我们需要从<strong class="la iu"> <em class="pa"> θ </em> </strong>到<strong class="la iu"> <em class="pa"> </em> </strong>产生J的最大增量的方向。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/7a7612b15dc7d411e7e29a97330592f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/0*JSJYyBFzHjJr8b6K.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">J相对于θ的梯度</p></figure><p id="34db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，<em class="pa">我们在梯度方向上应该移动的距离呢？</em></p><p id="4197" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是另一个(非常流行的)超参数:<strong class="la iu">学习率α </strong></p><p id="8c2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">配备了梯度和适当调整的学习率，我们做一步梯度上升，以提出一个更好的参数θ₁向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/14d2df4cc7ff15769dbde96a5202d57a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vc36cjeVWfsCzYkU.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度上升的一步</p></figure><p id="b4db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们迭代重复这个公式，我们将获得更好的政策参数，并有望解决环境问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/d1dd2d0c3336247f817e032124d377c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NMEKI-MK3it5OyQv.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/007c860aef54033cee82501585694f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p0WVM1pHk-IbQk9S.png"/></div></div></figure><p id="ab4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是梯度上升如何帮助我们找到最优的政策网络参数。</p><p id="73ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">剩下的问题是，我们如何计算这些梯度？</p><h2 id="b89d" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">估计政策绩效的梯度</h2><p id="bd48" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">为了实现这个算法，我们需要一个可以数值计算的策略性能梯度的表达式。</p><p id="16b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">利用一点微积分和统计学，我们可以从数学上证明，政策参数可以用轨迹样本来近似，这些轨迹样本是用当前政策收集的，如下<strong class="la iu"> : </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/e3cf3db376331ea6862b48c57bfa1fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PAVuhlCoSiPS1c-B8zeF-A.png"/></div></div></figure><p id="31a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中轨迹𝛕是直到情节结束所观察到的状态和所采取的动作的序列</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qb"><img src="../Images/91e091a412cf44c144d87fbb49e772bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bob4CfbARI34iQ3B.png"/></div></div></figure><p id="1c14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">重要的是要强调这样一个事实，即这些轨迹<strong class="la iu">必须使用当前政策参数</strong>来收集。</p><blockquote class="ox oy oz"><p id="0b5f" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><strong class="la iu">符合策略与不符合策略的算法</strong></p><p id="25bc" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">策略梯度方法是基于策略的算法，意味着我们只能使用在当前策略下收集的经验来改进当前策略。</p><p id="40c4" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">另一方面，使用<strong class="la iu">非策略</strong>算法，如Q-learning，我们可以保留使用旧版本策略收集的轨迹的回放记忆，并使用它们来改进当前策略。</p></blockquote><p id="ed19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，上面的公式对应于最简单的政策梯度方法。然而，有这个公式的变化，取代每轨迹总回报R(τᵢ)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/d5d68cdd26f951b27e4fa658d6a2de0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*veumCRartu5wY3B8.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每集总奖励</p></figure><p id="2c0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他<em class="pa">成功</em>指标。</p><p id="8f6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特别是，我们将在本课程中看到两个版本，一个在今天，另一个在下节课:</p><ul class=""><li id="5d66" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">给定动作后的累积奖励，又名<strong class="la iu">奖励-继续</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/7caa94091db42d76fbfe97e7c65a4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Bk5W0g3mZCkvfuy.png"/></div></div></figure><ul class=""><li id="6b4a" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated"><strong class="la iu">优势函数</strong>，是当前策略的Q函数和V函数的区别。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/13c8045c85dce819b089f674163fedd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pGq94MQilCAb9C7P.png"/></div></div></figure><p id="8ae4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pa">如果我们已经有了一个公式，为什么还需要其他公式来估计梯度？</em></p><p id="080e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">政策梯度公式的所有3个变量都有相同的期望值，所以如果你使用一个<strong class="la iu">非常大的</strong>轨迹样本，这3个数字将非常相似。然而，就计算性能而言，我们只能负担得起一小组轨迹，因此这些估计的可变性很重要。</p><p id="3dc3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特别</p><ul class=""><li id="1478" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">奖励去的版本有一个比原来的公式更低的方差</li><li id="7d71" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">并且优势功能版本甚至比奖励去版本具有更小的方差。</li></ul><p id="d4ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">较低的方差导致更快和更稳定的策略学习。</p><p id="7dea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">今天，我们将实现奖励到去的版本，因为它几乎与原始的累积奖励的香草政策梯度公式一样复杂。在下一讲中，我们将看到如何估计优势函数并得到更好的结果。</p><blockquote class="ox oy oz"><p id="dde9" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">想了解细节吗？T25】</p><p id="47e8" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">由于本课程的目的是给你一个实践这个主题的方法，我将数学术语保持在最低限度。</p><p id="f943" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><em class="it">如果你想了解从上面推导政策梯度公式的所有步骤，我推荐你阅读Wouter van hees wijk</em><em class="it">的这篇</em> <a class="ae lu" href="https://medium.com/towards-data-science/policy-gradients-in-reinforcement-learning-explained-ecec7df94245" rel="noopener"> <em class="it">优秀博文📝。</em></a></p></blockquote><h1 id="e576" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">5.策略梯度代理</h1><p id="a84f" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><a class="ae lu" href="https://github.com/Paulescu/hands-on-rl/blob/main/04_lunar_lander/notebooks/03_vanilla_policy_gradient_with_rewards_to_go_as_weights.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">👉🏽notebooks/03 _ vanilla _ policy _ gradient _ with _ rewards _ to _ go . ipynb</strong></a></p><p id="ea03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们实现最简单的策略梯度代理，其中策略梯度公式中的权重是阶段性的回报。</p><p id="0117" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">超参数只有3:</p><ul class=""><li id="259e" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated"><code class="fe nw nx ny nz b">learning_rate</code>是政策梯度更新的大小。</li><li id="2cdc" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated"><code class="fe nw nx ny nz b">hidden_layers</code>定义策略网络架构。</li><li id="2770" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated"><code class="fe nw nx ny nz b">gradient_weights</code>是每个时间步的奖励:即从当前时间步到剧集结束的奖励总和。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><p id="fdfe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用Tensorboard设置了基本的日志记录，并开始训练我们的VPGAgent:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><p id="f26d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练逻辑封装在<code class="fe nw nx ny nz b"><a class="ae lu" href="https://github.com/Paulescu/hands-on-rl/blob/main/04_lunar_lander/src/vpg_agent.py" rel="noopener ugc nofollow" target="_blank">src/vgp_agent.py</a></code>中的<code class="fe nw nx ny nz b">agent.train()</code>函数中。该函数本质上是一个具有<code class="fe nw nx ny nz b">n_policy_updates</code>次迭代的循环，在每次迭代中，我们:</p><ol class=""><li id="cc84" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt nn no np nq bi translated">收集<code class="fe nw nx ny nz b">batch_size</code>轨迹的样本</li><li id="6481" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">使用随机梯度上升更新策略参数。</li><li id="4922" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">将度量记录到Tensorboard</li><li id="32a8" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt nn no np nq bi translated">可能评估当前代理，如果它是我们目前找到的最好的代理，将其保存到磁盘。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><blockquote class="ox oy oz"><p id="975e" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated"><strong class="la iu">什么是正确的</strong> <code class="fe nw nx ny nz b"><strong class="la iu">batch_size</strong></code> <strong class="la iu">使用方法？</strong></p><p id="01c9" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">每次策略更新的轨迹数量(又名<code class="fe nw nx ny nz b">batch_size</code>)是一个<strong class="la iu">超参数</strong>，你需要调整它以确保你的训练在合理的时间内收敛到最优解。</p><p id="45e2" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">👉🏽如果<code class="fe nw nx ny nz b">batch_size</code>太低，您将得到非常嘈杂的梯度估计，并且训练将不会收敛到最优策略。</p><p id="539f" class="ky kz pa la b lb lc ju ld le lf jx lg pb li lj lk pc lm ln lo pd lq lr ls lt im bi translated">👉🏽相反，如果<code class="fe nw nx ny nz b">batch_size</code>太大，训练循环会不必要地变慢，而你会坐着等🥱</p></blockquote><h2 id="155f" class="og mp it bd mq oh oi dn mu oj ok dp my lh ol om na ll on oo nc lp op oq ne or bi translated">标绘训练结果</h2><p id="d050" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">当模型正在训练时，我们到终端并按如下方式启动Tensorboard服务器:</p><pre class="kj kk kl km gt pf nz pg ph aw pi bi"><span id="5221" class="og mp it nz b gy pj pk l pl pm">$ tensorboard --logdir tensorboard_logs</span></pre><p id="901d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您访问控制台上打印的URL，您会看到一个类似于此的图表，其中横轴是代理到目前为止收集的轨迹总数，纵轴是每集的总奖励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/85375c2ebbefeffbdb04efb08923f7d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZcknAMhAGome0Wj7iM2Dw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">1M步后每集总奖励(图片由作者提供)</p></figure><p id="b468" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们清楚地看到代理正在学习，并在100万步后实现了大约250英镑的平均总回报。</p><p id="1459" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于总报酬不是一个非常直观的衡量标准，我们在<code class="fe nw nx ny nz b">100</code>新剧集的培训结束时直接评估代理，并检查登陆成功率:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pe of l"/></div></figure><pre class="kj kk kl km gt pf nz pg ph aw pi bi"><span id="5f43" class="og mp it nz b gy pj pk l pl pm">Reward average 245.86, std 30.18<br/>Succes rate = 99.00%</span></pre><p id="ba36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">哇哦！99%的准确率已经很不错了吧？</p><p id="d9f7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们快速休息一下，回顾一下，然后展示作业。我们走吧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qg"><img src="../Images/82b632ab3fd76dc59f590d983d90f84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMTCb1tf9Rom4TSCxxke0A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">休息和补水的时间到了(图片由作者提供)</p></figure><h1 id="9c5e" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">6.关键要点</h1><p id="a2d0" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">今天我想让你睡在这三样东西上:</p><ul class=""><li id="e04d" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">首先，策略梯度(PG)算法直接参数化最优策略，并使用梯度上升算法调整其参数。</li><li id="8ff6" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">第二，与深度Q学习相比，PG算法更<strong class="la iu">稳定</strong>(即对超参数不太敏感)，这很好。</li><li id="694a" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">最后，PG算法的数据效率<strong class="la iu">更低</strong>，因为它们不能重用旧的轨迹来更新当前的策略参数(即它们是基于策略的方法)。这是我们需要改进的地方</li></ul><h1 id="3cb5" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">7.家庭作业</h1><p id="df04" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><a class="ae lu" href="https://github.com/Paulescu/hands-on-rl/blob/main/04_lunar_lander/notebooks/03_vanilla_policy_gradient_with_rewards_to_go_as_weights.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">👉🏽笔记本/04 _作业. ipynb </strong> </a></p><ul class=""><li id="f448" class="ng nh it la b lb lc le lf lh os ll ot lp ou lt ov no np nq bi translated">你能找到一个更小的网络来解决这种环境吗？我用了一个隐藏层64个单位，但我觉得这是矫枉过正。</li><li id="2021" class="ng nh it la b lb nr le ns lh nt ll nu lp nv lt ov no np nq bi translated">你能通过适当调整<code class="fe nw nx ny nz b">batch_size</code>来加速收敛吗？</li></ul><h1 id="438f" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">8.下一步是什么</h1><p id="57bf" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">在下一课中，我们将介绍一种新的技巧来提高策略梯度方法的数据效率。</p><p id="c539" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">敬请关注。</p><p id="e6e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">爱情，</p><p id="2550" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">活下去。</p><p id="2743" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">学习。</p><h1 id="22eb" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">想支持我吗？</h1><p id="0e29" class="pw-post-body-paragraph ky kz it la b lb ni ju ld le nj jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">你喜欢阅读和学习关于ML、AI和数据科学的知识吗？</p><p id="76e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">无限制地访问我在Medium上发布的所有内容，并支持我的写作。</p><p id="774d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉🏽今天使用我的<a class="ae lu" href="https://pau-labarta-bajo.medium.com/membership" rel="noopener"> <strong class="la iu">推荐链接</strong> </a>成为会员。</p><div class="lv lw gp gr lx ly"><a href="https://pau-labarta-bajo.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">成为一个媒体成员来阅读我在媒体上分享的一切。</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">你的会员费的一部分给了所有你喜欢阅读的作家。希望是我。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">pau-labarta-bajo.medium.com</p></div></div><div class="mh l"><div class="qh l mj mk ml mh mm ks ly"/></div></div></a></div><p id="ccdc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉🏽订阅<a class="ae lu" href="https://datamachines.xyz/subscribe/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="pa"> datamachines </em>简讯</strong> </a> <strong class="la iu">。</strong></p><p id="cb00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">👉🏽<a class="ae lu" href="https://medium.com/@pau-labarta-bajo" rel="noopener"> <strong class="la iu">跟着我</strong> </a>上媒。</p><p id="7bce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">祝你愉快🤗</p><p id="39b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">避寒胜地</p></div></div>    
</body>
</html>