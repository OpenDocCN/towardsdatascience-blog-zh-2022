<html>
<head>
<title>Q-Learning Algorithm: How to Successfully Teach an Intelligent Agent to Play A Game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q-学习算法:如何成功地教一个智能代理玩游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/q-learning-algorithm-how-to-successfully-teach-an-intelligent-agent-to-play-a-game-933595fd1abf#2022-10-04">https://towardsdatascience.com/q-learning-algorithm-how-to-successfully-teach-an-intelligent-agent-to-play-a-game-933595fd1abf#2022-10-04</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="06f4" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="e7b6" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">通过一个循序渐进的Python示例详细解释了一种称为Q-Learning的强化学习算法</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/b272d98a63b44233055de6a4f995d88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGhII3GlrepClsQ1NtJ58g.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Q学习的强化学习。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="fbf5" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">简介</strong></h1><p id="0b34" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated"><strong class="md je">强化学习(RL) </strong>是机器学习的一个令人兴奋的部分，它帮助我们创造出能够执行各种任务的<strong class="md je">智能代理</strong>。</p><p id="2245" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在这一系列文章中，我将深入研究不同的RL技术，并向您展示如何使用Python代码来应用它们。这一次我将讨论Q-Learning(Q代表质量)，这是最直接的RL方法之一。</p><p id="e78d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果你对RL的主要思想不熟悉，可以随意参考我的介绍文章:<a class="ae li" rel="noopener" target="_blank" href="/reinforcement-learning-rl-what-is-it-and-how-does-it-work-1962cf6db103">强化学习(RL)——它是什么，是如何工作的？</a></p><h1 id="7941" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">内容</strong></h1><ul class=""><li id="2dad" class="nc nd iu md b me mf mh mi mk ne mo nf ms ng mw nh ni nj nk bi translated">Q-Learning在更广泛的机器学习(ML)宇宙中处于什么位置</li><li id="7c37" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">强化学习术语</li><li id="afa3" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">Q-Learning是如何工作的？<br/> -基于策略和基于值的方法之间的区别<br/> - Q函数和Q表<br/> - Q学习算法<br/> - Q学习输出</li><li id="496a" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">Python示例向您展示了如何训练一个代理来玩一个冰湖游戏</li></ul><h1 id="31dc" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">机器学习领域中的q学习</h1><p id="719b" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">下图是我对最常用的机器学习算法进行分类的尝试。当然，它永远不会是完美的，因为一些算法可以分为多个类别。然而，观想ML宇宙是非常有帮助的，尤其是在学习过程中。</p><p id="a6bc" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">Q-Learning属于强化学习算法家族，更具体地说，属于基于值的方法分支(在下面的章节中有更多的介绍)。</p><p id="05f2" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面的<strong class="md je">图是交互式的</strong>，请点击👇在不同的类别上探索和揭示更多。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nq nr l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。互动图表由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创作。</p></figure><p id="f9ef" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="ns">如果你喜欢数据科学和机器学习</em> </strong> <em class="ns">，请</em> <a class="ae li" href="https://bit.ly/3sItbfx" rel="noopener ugc nofollow" target="_blank"> <em class="ns">订阅</em> </a> <em class="ns">获取我的新文章邮件。如果不是中等会员，可以在这里</em>  <em class="ns">加入</em> <a class="ae li" href="https://bit.ly/36Mozgu" rel="noopener ugc nofollow" target="_blank"> <em class="ns">。</em></a></p><h1 id="f81f" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">强化学习术语</h1><p id="7718" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在我们研究Q-Learning之前，先快速回顾一下强化学习的基本要素:</p><ul class=""><li id="4660" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">代理</strong>——可以与其环境交互的“智能角色”，例如游戏中的玩家。</li><li id="70c4" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">环境</strong>——代理“生活”或操作的“世界”。</li><li id="b1ec" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">动作空间</strong> —代理可以执行的动作的列表或范围。</li><li id="9918" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">状态/观察空间</strong> —可能的环境配置的列表或范围。状态/观察向代理提供关于其环境的信息(例如，其位置)。</li><li id="f66b" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">奖励</strong> —当代理在不同状态下执行期望的(不期望的)行为时，我们给予代理的激励(或抑制)。我们可以通过使用<strong class="md je">贴现因子{gamma(𝛾)} </strong>来减少相对于当前奖励的未来奖励。</li><li id="d0f8" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">探索/开发{epsilon(𝜖)} </strong> —使我们能够设置代理应该花费多少时间来探索环境和开发其关于环境的现有知识。</li><li id="8ccb" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">第</strong>集——从开始位置到结束位置的一个完整循环。例如，在游戏的上下文中，一集将从你的代理开始新的级别的时刻持续到它死亡或完成该级别。</li><li id="c8a9" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je"> Alpha(𝛼) </strong> —学习率，它影响学习速度和向最优策略的收敛。</li><li id="1096" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">policy(𝜋)</strong>—代理人追求目标的策略。</li></ul><h1 id="e0c1" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">Q-Learning是如何工作的？</h1><h2 id="64a6" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">基于政策和基于价值的方法之间的差异</strong></h2><p id="5fce" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">Q-Learning属于<strong class="md je">基于价值的方法</strong>的范畴，所以让我们从理解基于价值的方法和基于政策的方法之间的区别开始。</p><ul class=""><li id="728d" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">基于策略的方法</strong> —我们直接训练代理<strong class="md je">在哪个状态下采取什么行动</strong>。它由一个<strong class="md je">策略函数</strong>描述，该函数可以是<strong class="md je">确定性的</strong>(给出每个状态的精确动作)或<strong class="md je">随机性的</strong>(提供动作的概率分布)。</li><li id="213c" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">基于价值的方法</strong> —我们通过教代理<strong class="md je">识别哪些状态(或状态-动作对)更有价值来间接训练代理</strong>，以便它可以被价值最大化所引导。它由一个<strong class="md je">价值函数</strong>描述，其中一个状态的价值是代理在该状态开始时可以获得的预期贴现回报。</li></ul><p id="467a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">不管我们使用哪种方法来训练我们的代理，找到最优策略函数或最优值函数等同于发现<strong class="md je">最优policy(𝜋).</strong></p><h2 id="6db2" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak"> Q功能和Q表</strong></h2><p id="bd9a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">由于Q-Learning是一种基于价值的方法，我们必须有一个<strong class="md je">价值函数</strong>，我们称之为<strong class="md je"> Q函数</strong>。在内部，它将有一个包含每个<strong class="md je">状态动作对</strong>的<strong class="md je"> Q表</strong>。</p><p id="671a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">训练一个Q函数就是简单地找到与存储在Q表中的每个状态-动作对相关联的值。了解这些值使代理能够在每个状态下选择最佳操作。</p><p id="8ba8" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在后面的Python部分，我们将教代理玩一个冰湖游戏，所以让我们用同一个游戏来演示Q表是什么样子的。</p><p id="cdc4" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们的冰湖环境将是一个4x4的网格，由冰冻的正方形和带孔的正方形组成，总共16个正方形。每个方块代表一种可能的状态，我们可以通过给它们编号来标记。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/22e133238f9e22a88a4dfe59923a3a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_XgKRu0jD0Y7_65R1hb2xg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖游戏</a>状态空间，每个状态分配有数字。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="13d4" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je">动作空间</strong>将由我们的代理可以采取的4个不同动作组成:<strong class="md je">向左(0)、向下(1)、向右(2)、向上(3) </strong>。</p><p id="5df2" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">现在我们可以形成一个Q表，状态作为行，动作作为列。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oi"><img src="../Images/e5a5acc767ad9b25967b97c752814ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMRni3eCM_AgUVqolYsIRA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">空Q表。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="fb16" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">游戏的目标是让代理人从起点(方块0)导航到终点(包含礼物的方块15)而不掉进洞里。因此，训练一个Q函数将教会代理如何避免漏洞，找到到达目标的最快路线。</p><p id="594a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><em class="ns">请注意，当我们第一次初始化Q-table时，它在每个单元格中都有零，因为代理在开始探索它之前不知道任何关于它的环境的信息。</em></p><h2 id="7fb9" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">Q-学习算法</strong></h2><p id="db06" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在我们研究实际的Q-Learning算法之前，还有几件事需要注意:</p><ul class=""><li id="967a" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">On-policy vs Off-policy:</strong>Q-Learning是一种<strong class="md je"> off-policy </strong>算法，这意味着在训练过程中，我们使用不同的策略让agent进行动作(acting policy)和更新Q-function (updating policy)。同时，On-policy意味着相同的策略用于动作和更新。关于这一点，请参阅Python部分。</li><li id="58ca" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">时间差分(TD) vs蒙特卡罗:</strong> Q-Learning使用一种<strong class="md je"> TD方法</strong>，这意味着在训练过程中，它在每一步之后更新Q-function。同时，蒙特卡罗方法是等到一集结束后再对价值函数进行更新。</li></ul><p id="12be" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">现在让我们来看看Q学习算法，看看它如何训练Q函数(更新Q表):</p><ul class=""><li id="1c0e" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">Q-Q函数</li><li id="3f33" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝑆𝑡——现状(观察)</li><li id="c827" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝐴𝑡——当前行动</li><li id="7175" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝑅𝑡+1 —当前行动后收到的奖励</li><li id="1c88" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝑆𝑡+1——下一个国家(观察)</li><li id="de82" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝛼(阿尔法)—学习率参数</li><li id="3207" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝛾(伽玛)-折扣系数参数</li><li id="31bc" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">𝑚𝑎𝑥𝑎𝑄(𝑆𝑡+1,𝑎) —在可能的动作空间中，下一个状态(观察)的最大值</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oj"><img src="../Images/c246f888df41b0efcd55a1d905af30ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*go03SQ3GyxGeJnJBhX6H9g.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">q-学习算法。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><ul class=""><li id="e356" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">左边Q(𝑆𝑡,𝐴𝑡的项)是特定状态-动作对的新值<strong class="md je">。</strong></li><li id="5f59" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">右手边的第一项，Q(𝑆𝑡,𝐴𝑡)，是同一状态-动作对的<strong class="md je">电流值</strong>。</li><li id="fc05" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">为了修改当前值，我们将代理𝑅𝑡+1采取行动后的<strong class="md je">奖励</strong>加上我们可以从下一个状态𝛾𝑚𝑎𝑥𝑎𝑄(𝑆𝑡+1,𝑎<strong class="md je">中得到的<strong class="md je">最大值乘以γ</strong>，并减去<strong class="md je">当前值</strong> Q(𝑆𝑡,𝐴𝑡).</strong></li><li id="3140" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">因此，方括号中的项可能导致正值、零值或负值。因此，该状态-动作对的新Q值将增加、保持不变或减少。请注意，我们还应用了一个学习率来控制每次更新的“大小”。</li></ul><p id="fa8e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">由于Q-Learning使用<strong class="md je">时间差异(TD)方法</strong>，该算法将在每一步之后不断更新Q-table，直到我们到达一个不能再进行更新的位置，即它已经收敛到一个最优解。</p><h2 id="64be" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated">q-学习输出</h2><p id="cc3c" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">得出q函数的最优解意味着我们已经找到了最优的<strong class="md je">policy(𝜋】</strong>。然后，代理可以使用此策略成功地导航其环境，并根据起始状态获得可能的最高累积奖励。</p><p id="3205" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面是一个冰湖游戏的最佳q表，使用0.95的𝛾 (gamma)和游戏的默认奖励函数:</p><ul class=""><li id="085d" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">实现目标(G): +1</li><li id="5828" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">到达孔(H): 0</li><li id="30f1" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">达到冻结状态(F): 0</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ok"><img src="../Images/9618d799b14d07482848b7aca471cb57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyIE_430xbBRzrrjUdYLQw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">为<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖游戏</a>优化的q表。图片来自<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><p id="5476" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们将在Python部分看到如何得到这个结果，但是现在，让我们看看它告诉我们什么。</p><p id="d1fd" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果我们的代理使用上述Q表中描述的政策，那么:</p><ul class=""><li id="aa04" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">从状态0开始，向下或向右都同样有价值。根据Q表中的动作顺序，代理将进入状态4。</li><li id="53a3" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">从状态4开始，最高值动作是再次下降，因此它将这样做，到达状态8。</li><li id="cd09" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">从状态8开始，最高值动作是向右，将代理带到状态9。</li><li id="583b" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">从状态9开始，向下或向右同样有价值。但是，由于基于Q-表中的动作顺序，down首先出现，代理进入状态13。</li><li id="46dc" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">从状态13，它将向右到状态14，然后再向右到状态15，达到目标。</li></ul><p id="5294" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">这是遵循上述政策的代理的gif图像。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ol"><img src="../Images/52e55dcfcec399778cdfe8761d984578.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/1*-JdsniM1sZ18YpFHcDUJcw.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Gif图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖游戏</a>的组件创建。</p></figure><div class="kt ku kv kw gu ab cb"><figure class="om kx on oo op oq or paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/63320331b74bd98eea6402472b4209ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qkXay39OnVc2IosW6rkxtw.png"/></a></figure><figure class="om kx on oo op oq or paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="d631" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak"> Python示例向你展示了如何训练一个代理玩冰湖游戏</strong></h1><p id="5e7a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">现在让我们使用Python来实现我们刚刚学到的内容。我们将使用<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank"> Open AI的健身房Python库</a>包含各种游戏环境，包括冰封湖。</p><h2 id="0153" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated">设置</h2><p id="3bce" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们需要获得以下库:</p><ul class=""><li id="02d2" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><a class="ae li" href="https://www.gymlibrary.dev/" rel="noopener ugc nofollow" target="_blank">为<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖</a>游戏环境打开艾的健身房库</a></li><li id="316d" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a>用于数据操作</li><li id="a401" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://matplotlib.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>和内置的<strong class="md je"> IPython </strong>和<strong class="md je"> time </strong>库，用于显示代理如何导航其环境</li></ul><p id="8729" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们导入库:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="a397" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码打印了本例中使用的包版本:</p><pre class="kt ku kv kw gu ot ou ov ow aw ox bi"><span id="1d37" class="nw lk iu ou b gz oy oz l pa pb">numpy: 1.23.3<br/>gym: 0.26.0<br/>matplotlib: 3.6.0</span></pre><p id="9250" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，我们建立一个冰湖环境:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="541d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">注意，我们在这个例子中使用了一个不滑的游戏版本。同时，渲染选项如下:</p><ul class=""><li id="9d6e" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">无(默认):</strong>不计算渲染。</li><li id="e1ab" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">人类:</strong>渲染返回无。环境在当前显示器或终端中被连续渲染，通常用于人类消费。</li><li id="0aff" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je"> rgb_array: </strong>返回代表环境状态的帧。帧是具有shape (x，y，3)的numpy.ndarray，表示x乘y像素图像的RGB值。</li><li id="c9bc" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je"> ansi: </strong>返回字符串列表(str)或StringIO。StringIO，包含每个时间步长的终端样式文本表示。文本可以包括换行符和ANSI转义序列(例如颜色)。</li></ul><p id="4f8f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们检查我们在上面设置的环境的环境描述、状态空间和动作空间:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><pre class="kt ku kv kw gu ot ou ov ow aw ox bi"><span id="195f" class="nw lk iu ou b gz oy oz l pa pb">Environment Array: <br/>[[b'S' b'F' b'F' b'F']<br/> [b'F' b'H' b'F' b'H']<br/> [b'F' b'F' b'F' b'H']<br/> [b'H' b'F' b'F' b'G']]</span><span id="2f5b" class="nw lk iu ou b gz pc oz l pa pb">State(Observation) space: Discrete(16)<br/>Action space: Discrete(4)</span></pre><p id="3c7c" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">您可以看到环境与上一节中显示的布局相匹配。但是，您可以在gym.make()中使用<strong class="md je"> desc </strong>选项来指定您自己的地图。提醒:S =开始，F =冻结，H =球洞，G =进球。</p><p id="8c04" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">同样，正如所料，状态空间包含16个离散状态(4x4)，动作空间有4个离散动作(0:左，1:下，2:右，3:上)。</p><p id="0138" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">作为最后一项检查，在我们进行任何培训之前，我们将让我们的代理在其环境中自由活动(在每个步骤中采取随机行动)，并将其呈现在Jupyter笔记本中。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="34d9" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">以下是上述代码的输出:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ol"><img src="../Images/1c38c992107b9a41a51711613d21de34.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/1*mzY3hQpmWJJHFHA1vWzQ3A.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖游戏</a>的组件创建的Gif图像。</p></figure><h2 id="8459" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">训练Q函数以找到最佳策略</strong></h2><p id="90aa" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">设置完成后，让我们使用<strong class="md je"> Q-Learning </strong>为我们的代理在这个游戏中找到最好的<strong class="md je"> policy(𝜋) </strong>。</p><p id="3ecc" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们首先初始化几个参数:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="21d3" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请注意，我们将在整个培训过程中改变epsilon。我们将从ε= 1开始，这意味着我们的代理人的行为在开始时都是随机的。但是，我们会随着每一集而衰减epsilon，所以我们的代理人逐渐从纯探索走向剥削。</p><p id="ecca" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">接下来，让我们初始化Q表。正如我们在上一节中看到的，这将是一个16x4的表格，其中16行代表16个状态，4列代表4个可能的动作。我们用全0初始化Q表，因为在开始训练之前，我们不知道每个状态的价值。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="ea34" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上述代码显示初始化的Q表:</p><pre class="kt ku kv kw gu ot ou ov ow aw ox bi"><span id="3f0b" class="nw lk iu ou b gz oy oz l pa pb">array([[0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.],<br/>       [0., 0., 0., 0.]])</span></pre><p id="e189" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">回想一下，Q-Learning是一种<strong class="md je">非策略</strong>算法。因此，我们将为<strong class="md je">动作</strong> (epsilon_greedy)定义一个函数，为<strong class="md je">更新Q表(update_Q)定义另一个函数。更新策略使用贪婪方法，即没有探索。</strong></p><p id="ca0d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">您应该能够发现update_Q函数包含前一节中分析的Q学习算法等式。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="229f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">最后，让我们定义我们的培训功能:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="ad78" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">现在让我们调用训练函数，看看结果:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="05f6" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在训练之后，我们得到了优化的Q表，它与我们在上一节中展示的结果相匹配。现在，代理可以使用它来始终达到目标，而不会掉进洞里。</p><pre class="kt ku kv kw gu ot ou ov ow aw ox bi"><span id="b199" class="nw lk iu ou b gz oy oz l pa pb">array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],<br/>       [0.73509189, 0.        , 0.81450625, 0.77378094],<br/>       [0.77378094, 0.857375  , 0.77378094, 0.81450625],<br/>       [0.81450625, 0.        , 0.77378094, 0.77378094],<br/>       [0.77378094, 0.81450625, 0.        , 0.73509189],<br/>       [0.        , 0.        , 0.        , 0.        ],<br/>       [0.        , 0.9025    , 0.        , 0.81450625],<br/>       [0.        , 0.        , 0.        , 0.        ],<br/>       [0.81450625, 0.        , 0.857375  , 0.77378094],<br/>       [0.81450625, 0.9025    , 0.9025    , 0.        ],<br/>       [0.857375  , 0.95      , 0.        , 0.857375  ],<br/>       [0.        , 0.        , 0.        , 0.        ],<br/>       [0.        , 0.        , 0.        , 0.        ],<br/>       [0.        , 0.9025    , 0.95      , 0.857375  ],<br/>       [0.9025    , 0.95      , 1.        , 0.9025    ],<br/>       [0.        , 0.        , 0.        , 0.        ]])</span></pre><h2 id="644a" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated">估价</h2><p id="3092" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">让我们通过运行一些模拟来评估这个策略，并检查代理是否总是设法获得最大的回报。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="2118" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">上面的代码打印出以下结果:</p><pre class="kt ku kv kw gu ot ou ov ow aw ox bi"><span id="36fc" class="nw lk iu ou b gz oy oz l pa pb">Mean Reward = 1.00 +/- 0.00</span></pre><p id="2860" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如你所见，在测试的100集每集中，代理人设法获得了最高奖励(1.00)。</p><p id="91a5" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们通过让代理遵循策略并将其呈现在屏幕上来直观地评估它:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="os nr l"/></div></figure><p id="2e45" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">正如我们在上一节中看到的那样，结果是意料之中的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ol"><img src="../Images/52e55dcfcec399778cdfe8761d984578.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/1*-JdsniM1sZ18YpFHcDUJcw.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Gif图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用来自<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰湖游戏</a>的组件创建。</p></figure><h1 id="f77e" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">结束语</strong></h1><p id="b3d0" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们已经成功地使用Q-Learning为代理人找到了在冰湖游戏中使用的最佳策略。我希望这款游戏的简单本质能让你很容易理解Q-Learning是如何工作的。</p><p id="7ac2" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请不要忘记<a class="ae li" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank">订阅</a>，这样您就可以<strong class="md je">从我即将发表的文章</strong>中学习其他强化学习算法，并了解如何使用<strong class="md je"> Python将它们应用到不同的环境中！</strong></p><p id="e75f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">你可以在我的<a class="ae li" href="https://github.com/SolClover/Art056_RL_Q_Learning" rel="noopener ugc nofollow" target="_blank"> <strong class="md je"> GitHub库</strong> </a>上找到本文使用的完整Python代码，作为Jupyter笔记本。</p><p id="1c07" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">干杯！🤓<br/> <strong class="md je">索尔·多比拉斯</strong></p><p id="1918" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果你已经花光了这个月的学习预算，下次请记得我。  <em class="ns">我的个性化链接加入媒介:</em></p><div class="pd pe gq gs pf pg"><a href="https://bit.ly/3J6StZI" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd je gz z fq pl fs ft pm fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="pn l"><h3 class="bd b gz z fq pl fs ft pm fv fx dk translated">阅读索尔·多比拉斯和媒体上成千上万的其他作家的每一个故事。你的会员费直接支持索尔…</h3></div><div class="po l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">solclover.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lc pg"/></div></div></a></div></div></div>    
</body>
</html>