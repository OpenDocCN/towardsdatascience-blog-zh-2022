<html>
<head>
<title>Prediction intervals in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的预测区间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prediction-intervals-in-python-64b992317b1a#2022-08-01">https://towardsdatascience.com/prediction-intervals-in-python-64b992317b1a#2022-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/0e05d9a378669285c1b549a7d6a9e5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C4QIFDQPnRXfGewh"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图为<a class="ae jg" href="https://unsplash.com/@theshubhamdhage?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">舒巴姆·达吉</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="b5fc" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="532d" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">了解获得预测区间的三种方法</h2></div><p id="bb83" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果让你猜我过去一周看了多少部电影，你会觉得猜“2到6”还是“3”更有把握？我们可能会同意，猜测一个范围比猜测一个数字更有可能是正确的。同样，与单值预测相比，预测区间为我们提供了更可靠、更透明的估计。在本帖中，我们将学习在Python中获得预测区间的三种方法。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/48927058485663340650e7b27bdd1ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SjX5qxstStiJZV3D"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法库里安设计</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="2664" class="mp mq jj bd mr ms mt mu mv mw mx my mz ky na kz nb lb nc lc nd le ne lf nf ng bi translated">📦 0.设置</h1><p id="e1f9" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">我们将从加载必要的库和样本数据开始。我们将使用Scikit-learn关于<a class="ae jg" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset" rel="noopener ugc nofollow" target="_blank">糖尿病</a>的内置数据集(<em class="nm">该数据在BSD许可下可用</em>)。如果您想了解更多关于数据集的信息，请使用<code class="fe nn no np nq b">print(diabetes[‘DESCR’])</code>查看它的描述。</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="3f45" class="nv mq jj nq b gy nw nx l ny nz">import numpy as np<br/>np.set_printoptions(<br/>    formatter={'float': lambda x: "{:.4f}".format(x)}<br/>)<br/>import pandas as pd<br/>pd.options.display.float_format = "{:.4f}".format<br/>from scipy.stats import t<br/>import statsmodels.api as sm<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.ensemble import GradientBoostingRegressor</span><span id="faf8" class="nv mq jj nq b gy oa nx l ny nz">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style='darkgrid', context='talk')</span><span id="9294" class="nv mq jj nq b gy oa nx l ny nz">diabetes = load_diabetes(as_frame=True)<br/>df = diabetes['data']<br/>df['target'] = diabetes['target']<br/>df.info()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/16e6fe25115ba6d9b2c5420668ce5ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*gqJ6I1-Nbk4BuFqvQRzRvQ.png"/></div></figure><p id="f469" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们将数据分成训练集和测试集:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="d7f3" class="nv mq jj nq b gy nw nx l ny nz">train, test = train_test_split(df, test_size=0.1, random_state=42)<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    df.drop(columns='target'), df['target'], test_size=0.1, <br/>    random_state=42<br/>)<br/>x_train = X_train['bmi']<br/>x_test = X_test['bmi']</span><span id="1f33" class="nv mq jj nq b gy oa nx l ny nz">print(f"X_train shape: {X_train.shape}")<br/>print(f"x_train shape: {x_train.shape}")<br/>print(f"y_train shape: {y_train.shape}")<br/>print("\n========== Training data ==========")<br/>display(train[['target']].describe().T)</span><span id="b893" class="nv mq jj nq b gy oa nx l ny nz">print(f"X_test shape: {X_test.shape}")<br/>print(f"x_test shape: {x_test.shape}")<br/>print(f"y_test shape: {y_test.shape}")</span><span id="a6d4" class="nv mq jj nq b gy oa nx l ny nz">print("\n========== Test data ==========")<br/>test[['target']].describe().T</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/34c458ce7d4bb5cd2a0c7014071dd98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*E3Y7Sdw3mjCVjFeEg4jY5g.png"/></div></figure><p id="8e24" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标范围从25到350，平均值约为150，中位数约为130-140。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="7dbf" class="mp mq jj bd mr ms mt mu mv mw mx my mz ky na kz nb lb nc lc nd le ne lf nf ng bi translated">📍 1.预测数的变化范围</h1><p id="e288" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">我们现在来看看获得预测区间的三种方法。</p><h2 id="de83" class="nv mq jj bd mr od oe dn mv of og dp mz lq oh oi nb lu oj ok nd ly ol om nf jp bi translated">💡 1.1.使用标准误差</h2><p id="e5c1" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">让我们使用<code class="fe nn no np nq b">bmi</code>建立一个简单的线性回归来预测<code class="fe nn no np nq b">target</code>。</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="7458" class="nv mq jj nq b gy nw nx l ny nz">model = LinearRegression()<br/>model.fit(x_train.values.reshape(-1, 1), y_train)<br/>print(f"Intercept: {model.intercept_:.2f}")<br/>print(f"Slope: {model.coef_[0]:.2f}")<br/>print(model.predict(x_test.values.reshape(-1, 1))[:5])</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4eaeecbe11a5068883ae0a2f80e78961.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*b44ZG7LjhN6hljlRZaR17A.png"/></div></figure><p id="f17e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到预测，我们最好的猜测。使用下面的公式，我们可以计算标准误差并获得预测区间:</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/2a35cf7c5a59facd1eeb587e985e1c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*jjzjZvSFDqvKf3oBmfXEJg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">绿框中的变量是针对我们正在进行预测的特定观察的，而其余的是根据训练数据计算的。</p></figure><p id="d594" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该公式可以翻译成如下代码。我们使用自定义对象，因为它比函数更灵活:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="8834" class="nv mq jj nq b gy nw nx l ny nz">class CustomLinearRegression:<br/>    def __init__(self):<br/>        pass<br/>    <br/>    def fit(self, x, y):<br/>        # Calculate stats<br/>        self.n = len(x)<br/>        self.x_mean = np.mean(x)<br/>        self.y_mean = np.mean(y)<br/>        self.x_gap = x-self.x_mean<br/>        self.y_gap = y-self.y_mean<br/>        self.ss = np.square(self.x_gap).sum()<br/>        <br/>        # Find coefficients<br/>        self.slope = np.dot(self.x_gap, self.y_gap)/self.ss<br/>        self.intercept = self.y_mean-self.slope*self.x_mean<br/>        <br/>        # Find training error<br/>        y_pred = self.intercept+self.slope*x<br/>        self.se_regression = np.sqrt(<br/>            np.square(y-y_pred).sum()/(self.n-2)<br/>        )<br/>    <br/>    def predict(self, x):<br/>        y_pred = self.intercept+self.slope*x<br/>        return y_pred<br/>    <br/>    def predict_interval(self, x, alpha=0.1):<br/>        t_stat = t.ppf(1-alpha/2, df=self.n-2)<br/>        <br/>        # Calculate interval upper and lower boundaries<br/>        df = pd.DataFrame({'x': x})<br/>        for i, value in df['x'].iteritems():<br/>            se = self.se_regression * np.sqrt(<br/>                1+1/self.n+np.square(value-self.x_mean)/self.ss<br/>            )<br/>            df.loc[i, 'y_pred'] = self.intercept+self.slope*value<br/>            df.loc[i, 'lower'] = df.loc[i, 'y_pred']-t_stat*se<br/>            df.loc[i, 'upper'] = df.loc[i, 'y_pred']+t_stat*se<br/>        return df<br/>        <br/>custom_model = CustomLinearRegression()     <br/>custom_model.fit(x_train, y_train)<br/>print(f"Intercept: {custom_model.intercept:.2f}")<br/>print(f"Slope: {custom_model.slope:.2f}")<br/>custom_pred = custom_model.predict_interval(x_test)<br/>custom_pred.head()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/febf74ba914527d63c4ad242aeeb019f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*s6-q99xBKKjWYHgMxoImDw.png"/></div></figure><p id="d15b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来理解这个输出。在线性回归中，预测代表条件平均目标值。因此<code class="fe nn no np nq b">y_pred</code>，我们的预测栏，告诉我们给定特征的估计平均目标。预测区间告诉我们<em class="nm">对于一个给定的记录</em>目标可以取值的范围。我们可以从<code class="fe nn no np nq b">lower</code>和<code class="fe nn no np nq b">upper</code>列中看到预测区间的上下边界。这是一个90%的预测区间，因为我们选择了<code class="fe nn no np nq b">alpha=0.1</code>。我们将在这篇文章的剩余部分使用相同的alpha值。</p><p id="dae5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你很好奇，这里有一些解释预测区间的方法:</p><ul class=""><li id="1531" class="oq or jj lj b lk ll ln lo lq os lu ot ly ou mc ov ow ox oy bi translated">记录287的实际目标值有90%的可能性在42.8550和249.4799之间。</li><li id="c192" class="oq or jj lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">根据记录287的<code class="fe nn no np nq b">bmi</code>值，我们有90%的把握认为其实际目标值将介于42.8550和249.4799之间。</li><li id="a744" class="oq or jj lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">大约90%的预测区间将包含实际值。</li></ul><p id="cdad" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们检查一下测试数据中的目标值在预测区间内的百分比:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="17c0" class="nv mq jj nq b gy nw nx l ny nz">custom_correct = np.mean(<br/>    (custom_pred['lower']&lt;y_test) &amp; (y_test&lt;custom_pred['upper'])<br/>)<br/>print(f"{custom_correct:.2%} of the prediction intervals contain true target.")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/c01e853802ca426ad35208eb5af60845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*Bx1lPJcy11CdYMkp5EBj0g.png"/></div></figure><p id="1a9e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这大概是90%。虽然手动计算有助于我们理解幕后发生的事情，但更实际的是，我们可以使用库并简化我们的工作。下面是我们如何使用<code class="fe nn no np nq b">statsmodels</code>包来获得相同的预测间隔:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="6be7" class="nv mq jj nq b gy nw nx l ny nz">sm_model = sm.OLS(y_train, sm.add_constant(x_train)).fit()<br/>print(f"Intercept: {sm_model.params[0]:.2f}")<br/>print(f"Slope: {sm_model.params[1]:.2f}")</span><span id="cb2f" class="nv mq jj nq b gy oa nx l ny nz">sm_pred = sm_model.get_prediction(sm.add_constant(x_test))\<br/>                  .summary_frame(alpha=0.1)<br/>sm_pred.head()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7b4865924b23a50686de2dd3f5d5e15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*VWnNx6dsPkWnhItmkEoxyg.png"/></div></figure><p id="34aa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个输出提供了一些额外的输出，让我们来理解其中的关键:<br/> ◼️ <code class="fe nn no np nq b">mean</code>:预测，与前面的<code class="fe nn no np nq b">y_pred</code>相同。<br/> ◼️ <code class="fe nn no np nq b">mean_ci_lower</code> &amp; <code class="fe nn no np nq b">mean_ci_upper</code>:置信区间边界<br/> ◼️ <code class="fe nn no np nq b">obs_ci_lower</code> &amp; <code class="fe nn no np nq b">obs_ci_upper</code>:预测区间边界，同前面的<code class="fe nn no np nq b">lower</code>和<code class="fe nn no np nq b">upper</code>。</p><p id="fb89" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以检查预测和预测间隔是否与手动计算的相匹配:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="7ff0" class="nv mq jj nq b gy nw nx l ny nz">np.mean(np.isclose(<br/>    custom_pred.drop(columns='x').values, <br/>    sm_pred[['mean', 'obs_ci_lower', 'obs_ci_upper']]), <br/>        axis=0)</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/298c680923d221239a7bd33b465686d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*0SEDia-JQ_3a8seYsAwlbw.png"/></div></figure><p id="2b74" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">可爱，很配！</p><p id="7e84" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，你可能想知道置信区间和预测区间的区别。虽然这些术语相互关联，听起来有些相似，但它们指的是两个不同的区间，不应互换使用:<br/> ◼️置信区间用于均值预测。与预测区间不同，置信区间并没有告诉我们<em class="nm">一个观察可以采用的目标值的范围</em>。相反，它告诉我们目标平均值的范围。下面是一个解释示例:有90%的可能性，特征值与记录287相同的记录的平均目标值将落在140.9452和151.3897之间。<br/> ◼️虽然两个区间都以预测为中心，但预测区间的标准误差大于置信区间的标准误差。因此，预测区间比置信区间宽。</p><p id="f672" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们通过简单的线性回归来了解这个概念，但是在实践中更常见的是有多个特征。是时候扩展我们的示例以使用全部功能了:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="63d7" class="nv mq jj nq b gy nw nx l ny nz">ols = sm.OLS(y_train, sm.add_constant(X_train)).fit()<br/>test[['ols_lower', 'ols_upper']] = (ols<br/>    .get_prediction(sm.add_constant(X_test))<br/>    .summary_frame(alpha=0.1)[['obs_ci_lower', 'obs_ci_upper']])<br/>columns = ['target', 'ols_lower', 'ols_upper']<br/>test[columns].head()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/8a8a9a108db59e033899acdfb9c2950d.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*LjjZ_flTDamCjZbZANaCSg.png"/></div></figure><p id="39d3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">流程与使用单个特征完全相同。如果你很想知道在多重特征存在的情况下公式是如何变化的，请查看本指南。让我们评估一下我们的时间间隔:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="40b6" class="nv mq jj nq b gy nw nx l ny nz">ols_correct = np.mean(<br/>    test['target'].between(test['ols_lower'], test['ols_upper'])<br/>)<br/>print(f"{ols_correct:.2%} of the prediction intervals contain true target.")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/83bf758d7052de704fad75a5f5110857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*SextLdoEFRXh2xlV44DCHA.png"/></div></figure><h2 id="11c5" class="nv mq jj bd mr od oe dn mv of og dp mz lq oh oi nb lu oj ok nd ly ol om nf jp bi translated">1.2.来自分位数回归</h2><p id="0372" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">使用分位数回归，我们可以预测目标的<em class="nm">条件分位数，而不是条件均值目标。为了获得90%的预测区间，我们将构建两个分位数回归，一个预测第5个百分位数，另一个预测第95个百分位数。</em></p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="115d" class="nv mq jj nq b gy nw nx l ny nz">alpha = 0.1<br/>quant_lower = sm.QuantReg(<br/>    y_train, sm.add_constant(X_train)<br/>).fit(q=alpha/2)<br/>test['quant_lower'] = quant_lower.predict(<br/>    sm.add_constant(X_test)<br/>)</span><span id="058d" class="nv mq jj nq b gy oa nx l ny nz">quant_upper = sm.QuantReg(<br/>    y_train, sm.add_constant(X_train)<br/>).fit(q=1-alpha/2)<br/>test['quant_upper'] = quant_upper.predict(<br/>    sm.add_constant(X_test)<br/>)<br/>columns.extend(['quant_lower', 'quant_upper'])<br/>test[columns].head()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/44aa289def441991d092e05c64121d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*HmPsu6vhvC9JyolzkNOf2g.png"/></div></figure><p id="e045" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，<code class="fe nn no np nq b">quant_lower</code>模型预测不足，而<code class="fe nn no np nq b">quant_upper</code>模型预测过度。现在让我们检查在新间隔内的预测的百分比:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="c958" class="nv mq jj nq b gy nw nx l ny nz">quant_correct = np.mean(<br/>    test['target'].between(test['quant_lower'], test['quant_upper'])<br/>)<br/>print(f"{quant_correct:.2%} of the prediction intervals contain true target.")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/dba617a258302bf8f5e3035717b91307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*N3sxzkxGC2H6Nz5Irgcn0g.png"/></div></figure><p id="4758" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">略低于90%，覆盖率略低于以前。</p><h2 id="e62d" class="nv mq jj bd mr od oe dn mv of og dp mz lq oh oi nb lu oj ok nd ly ol om nf jp bi translated">1.3.从有分位数损失的GBM</h2><p id="9581" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">最后一种方法与前一种方法非常相似。我们将使用分位数损失的<code class="fe nn no np nq b">GradientBoostingRegressor</code>,通过两个模型获得90%的预测区间:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="5011" class="nv mq jj nq b gy nw nx l ny nz">gbm_lower = GradientBoostingRegressor(<br/>    loss="quantile", alpha=alpha/2, random_state=0<br/>)<br/>gbm_upper = GradientBoostingRegressor(<br/>    loss="quantile", alpha=1-alpha/2, random_state=0<br/>)</span><span id="96db" class="nv mq jj nq b gy oa nx l ny nz">gbm_lower.fit(X_train, y_train)<br/>gbm_upper.fit(X_train, y_train)</span><span id="5b87" class="nv mq jj nq b gy oa nx l ny nz">test['gbm_lower'] = gbm_lower.predict(X_test)<br/>test['gbm_upper'] = gbm_upper.predict(X_test)<br/>columns.extend(['gbm_lower', 'gbm_upper'])<br/>test[columns].head()</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/127919da5d6f79158c97a52dde1aa62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*_gggEoR3Y76C8kpnyfBvMw.png"/></div></figure><p id="985d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">评估时间间隔覆盖范围的时间:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="2800" class="nv mq jj nq b gy nw nx l ny nz">gbm_correct = np.mean(<br/>    test['target'].between(test['gbm_lower'], test['gbm_upper'])<br/>)<br/>print(f"{gbm_correct:.2%} of the prediction intervals contain true target.")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/ee47d0b2e451cecfd0feeee97c45abf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*wn7r8Rd-FwS132aFN6KDKQ.png"/></div></figure><p id="1690" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与以前的区间相比，这一数值较低。查看上面的示例记录，与前两种方法相比，间隔看起来稍微窄一些。对于我们在这篇文章中学到的任何方法，我们可以通过减少alpha来增加覆盖率。然而，这意味着间隔可能变得太宽，以至于它们可能不太能提供信息或对决策没有帮助。因此，在使用预测区间时，我们必须找到正确的平衡点。</p><p id="d3d0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们直观地比较所有三个区间:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="8440" class="nv mq jj nq b gy nw nx l ny nz">test = test.sort_values('target').reset_index()</span><span id="fd7f" class="nv mq jj nq b gy oa nx l ny nz">plt.figure(figsize=(10,6))<br/>sns.scatterplot(data=test, x=test.index, y='target',<br/>                color='grey')<br/>sns.lineplot(data=test, x=test.index, y='ols_lower', <br/>             color='hotpink')<br/>sns.lineplot(data=test, x=test.index, y='ols_upper', <br/>             color='hotpink', label='Linear Regression')</span><span id="af11" class="nv mq jj nq b gy oa nx l ny nz">sns.lineplot(data=test, x=test.index, y='quant_lower', <br/>             color='blue')<br/>sns.lineplot(data=test, x=test.index, y='quant_upper', <br/>             color='blue', label='Quantile Regression')</span><span id="b409" class="nv mq jj nq b gy oa nx l ny nz">sns.lineplot(data=test, x=test.index, y='gbm_lower', <br/>             color='green')<br/>sns.lineplot(data=test, x=test.index, y='gbm_upper', <br/>             color='green', label='Gradient Boosting Machine')<br/>plt.xticks([]);</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/cd0371a213395442dd947df7b8da899b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*lpRd6iwz5VOATQb4S2ycEg.png"/></div></figure><p id="40ce" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">越接近最小值和最大值的值越有可能超出预测区间。让我们进一步研究这些错误。我们将查看具有最高目标的5项记录:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="a6af" class="nv mq jj nq b gy nw nx l ny nz">test[columns].nlargest(5, 'target')</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/ee423087d78d6a91e244b34dd929cc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*GGuPqmJEaUCWwU_TdFw_gQ.png"/></div></figure><p id="65d8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们看看另一端:</p><pre class="me mf mg mh gt nr nq ns nt aw nu bi"><span id="1ddb" class="nv mq jj nq b gy nw nx l ny nz">test[columns].nsmallest(5, 'target')</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/ed2b87e21fab1d2477c7cdaf59da9e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*E7E9D7MUFmRs5bhhGr3mfg.png"/></div></figure><p id="6de9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性回归预测区间的下限有时是负值。这是需要注意的事情。如果我们知道目标总是正的，我们可以使用包装函数用最低可能的值覆盖这些负边界。</p><p id="535d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">瞧，这就是计算预测区间的三种方法！希望您能在下一个回归用例中使用这些方法。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/c2a299a427c63a87baafa6a1c214d624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ixcc9vWsKZqwqo5l"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">塞巴斯蒂安·斯文森在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d252" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="nm">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="nm">我的推荐链接</em></a><em class="nm">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="6e0e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谢谢你看我的帖子。如果你感兴趣，这里有我的一些帖子的链接:</p><p id="92d6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道、ColumnTransformer和FeatureUnion解释</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6"> FeatureUnion、ColumnTransformer &amp;管道用于预处理文本数据</a> <br/> ◼️ <a class="ae jg" rel="noopener" target="_blank" href="/enrich-your-jupyter-notebook-with-these-tips-55c8ead25255">用这些提示丰富您的Jupyter笔记本</a> <br/> ◼️ <a class="ae jg" rel="noopener" target="_blank" href="/organise-your-jupyter-notebook-with-these-tips-d164d5dcd51f">用这些提示组织您的Jupyter笔记本</a> <br/> ◼️ <a class="ae jg" rel="noopener" target="_blank" href="/explaining-scikit-learn-models-with-shap-61daff21b12a">解释Scikit-用SHAP学习模型</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/feature-selection-in-scikit-learn-dc005dcf38b7">在scikit中选择特性</a></p><p id="ce7c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">再见🏃💨</p></div></div>    
</body>
</html>