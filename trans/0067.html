<html>
<head>
<title>Principal Component Analysis (PCA) Explained Visually with Zero Math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">零数学直观解释主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d#2022-02-03">https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d#2022-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d417" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">主成分分析(PCA)是数据科学可视化和降维的一个不可或缺的工具，但通常被隐藏在复杂的数学中。至少可以说，我很难理解为什么会这样，这使得我很难欣赏它的全部魅力。</p><p id="d7d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然数字对于证明一个概念的有效性很重要，但我相信分享数字背后的故事也同样重要——用一个故事。</p><ul class=""><li id="f0e8" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated"><a class="ae kx" href="#0226" rel="noopener ugc nofollow">什么是 PCA？</a></li><li id="ab10" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><a class="ae kx" href="#9b5c" rel="noopener ugc nofollow">PCA 是如何工作的？</a></li><li id="79f8" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><a class="ae kx" href="#8b6b" rel="noopener ugc nofollow">逃脱的个人电脑</a></li><li id="fba7" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated"><a class="ae kx" href="#37f8" rel="noopener ugc nofollow">在 Python 中实现 PCA</a></li></ul></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><h1 id="0226" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">什么是 PCA？</h1><p id="d8c7" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">主成分分析(PCA)是一种将高维数据转换为低维数据，同时保留尽可能多的信息的技术。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/83166b7a69321052b47ac010b0e85e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*QinDfRawRskupf4mU5bYSA.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">原始三维数据集。红色、蓝色、绿色箭头分别是第一、第二和第三主分量的方向。图片由作者提供。</p></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/6df563b3befb236cb0902384bb106305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKTwaVmP4Dqxb-N3iD3CHw.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">PCA 后的散点图从三维减少到二维。图片由作者提供。</p></figure><p id="a17e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在处理具有大量特征的数据集时，PCA 非常有用。图像处理、基因组研究等常见应用总是需要处理成千上万的列。</p><p id="c270" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然拥有更多的数据总是好的，但有时它们包含了太多的信息，我们将有不可能的长模型训练时间，并且<a class="ae kx" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-50dc6e49aa1e">维数灾难</a>开始成为一个问题。有时候，少即是多。</p><p id="7de9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我喜欢把 PCA 比作写一本书的总结。</p><p id="2a31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">找时间读一本 1000 页的书是一种奢侈，很少有人能负担得起。如果我们能在两三页内总结出最重要的要点，这样即使是最忙的人也能很容易地理解这些信息，这不是很好吗？在这个过程中，我们可能会丢失一些信息，但至少我们了解了全局。</p><h1 id="9b5c" class="lk ll it bd lm ln ne lp lq lr nf lt lu lv ng lx ly lz nh mb mc md ni mf mg mh bi translated">PCA 是如何工作的？</h1><p id="00b6" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">这是一个两步走的过程。如果我们没有阅读或理解书的内容，我们就不能写一个书的总结。</p><p id="faf6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PCA 的工作方式是一样的——理解，然后总结。</p><h2 id="6c6e" class="nj ll it bd lm nk nl dn lq nm nn dp lu kb no np ly kf nq nr mc kj ns nt mg nu bi translated">用主成分分析法理解数据</h2><p id="b5f9" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">人类通过使用表达性语言来理解故事书的含义。可惜 PCA 不会说英语。它必须通过自己喜欢的语言，数学，在我们的数据中寻找意义。</p><p id="143e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个百万美元的问题是…</p><ul class=""><li id="e221" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">PCA 能理解我们数据的哪一部分重要吗？</li><li id="dced" class="ko kp it js b jt ky jx kz kb la kf lb kj lc kn kt ku kv kw bi translated">我们能从数学上量化数据中包含的信息量吗？</li></ul><p id="89f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，<em class="nv">方差</em>可以。</p><p id="268d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">方差越大，信息越多。反之亦然。</p><p id="32fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于大多数人来说，方差并不是一个陌生的术语。我们在高中学过，方差衡量的是每个点与均值的平均差异程度。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ab8409be294837190d998893f18de245.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*00VCpG1yrY0ZglbBbQ1iGg.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">方差公式。</p></figure><p id="59f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是它没有把变化和信息联系起来。那么这种联想从何而来呢？为什么会有意义？</p><p id="a19a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们正在和朋友玩猜谜游戏。游戏很简单。我们的朋友会遮住他们的脸，我们需要仅仅根据他们的身高来猜测谁是谁。作为好朋友，我们记得每个人有多高。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1b062f016e704499542d50c27ecce9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*lH2mo7BUdZnpfye-5U0PjA.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">记忆中我们朋友的身高。图片由作者提供。</p></figure><p id="6899" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我先来。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ny"><img src="../Images/73032981c7bcf71dd214ff6ac78a298d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_-eUfpayQ0hRcxSF80ceQ.jpeg"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">三个相同的朋友的剪影，我们需要根据他们的身高差异来识别他们。图片由<a class="ae kx" href="https://pixabay.com/users/7089643-7089643/" rel="noopener ugc nofollow" target="_blank"> 7089643 </a>来自<a class="ae kx" href="https://pixabay.com/vectors/man-silhouette-man-alone-freedom-5403280/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>，经作者许可编辑。</p></figure><p id="6445" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">毫无疑问，我会说 A 是克里斯，B 是亚历克斯，C 是本。</p><p id="63bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们试着猜一组不同的朋友。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f391355d557aca04486b61055a82384f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*2EaIutltzy_0USP3TFoMWQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">另一组我们铭记于心的朋友身高。图片由作者提供。</p></figure><p id="353d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">轮到你了。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ny"><img src="../Images/2d30e6f373d0eebab792f89a3ff55875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBdInUbKGxeaQvI3hkD_8w.jpeg"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">三个同样高的朋友的剪影，我们需要确认他们的身份。图片由<a class="ae kx" href="https://pixabay.com/users/7089643-7089643/" rel="noopener ugc nofollow" target="_blank"> 7089643 </a>来自<a class="ae kx" href="https://pixabay.com/vectors/man-silhouette-man-alone-freedom-5403280/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>，经作者许可编辑。</p></figure><p id="e457" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你能猜出谁是谁吗？当他们的身高非常接近时，这很难。</p><p id="8f64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">早些时候，我们很容易区分 185 厘米的人和 160 厘米和 145 厘米的人，因为他们的身高差异很大。</p><p id="0be7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，当我们的数据具有较高的方差时，它包含更多的信息。这就是为什么我们在同一个句子中不断听到 PCA 和最大方差。我想引用维基百科的一个片段来正式说明这一点。</p><blockquote class="nz oa ob"><p id="b243" class="jq jr nv js b jt ju jv jw jx jy jz ka oc kc kd ke od kg kh ki oe kk kl km kn im bi translated">PCA 被定义为正交线性变换，其将数据变换到新的坐标系，使得数据的某个标量投影的<strong class="js iu">最大方差</strong>位于第一坐标(称为第一主分量)，第二最大方差位于第二坐标，依此类推。</p></blockquote><p id="2279" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 PCA 看来，方差是一种客观的数学方法，可以量化我们数据中的信息量。</p><p id="c976" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">差异<em class="nv">是</em>信息。</p><p id="c49a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了说明这一点，我提议再进行一次猜谜游戏，只是这一次，我们要根据身高和体重来猜谁是谁。</p><p id="8ddb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二回合。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f2f8ce519a90ecb7e5daa84cbf7c21db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*S0pYcfmLJ2jM4pHXF5R5wA.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">同一套朋友和各自的身高体重。图片由作者提供。</p></figure><p id="28b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一开始，我们只有身高。现在，我们基本上已经将朋友的数据量增加了一倍。这会改变你的猜测策略吗？</p><p id="170b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是进入下一节的一个很好的小插曲 PCA 如何总结我们的数据，或者更准确地说，如何降低维数。</p><h2 id="d37f" class="nj ll it bd lm nk nl dn lq nm nn dp lu kb no np ly kf nq nr mc kj ns nt mg nu bi translated">使用 PCA 汇总数据</h2><p id="c52e" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">就我个人而言，体重差异很小(也就是一个小的方差)，这根本不能帮助我区分我们的朋友。我仍然不得不主要依靠身高来进行猜测。</p><p id="7938" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">直观地说，我们刚刚将数据从二维减少到一维。这个想法是，我们可以选择性地保留方差较高的变量，然后忘记方差较低的变量。</p><p id="333b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是如果，如果身高和体重有相同的方差呢？这是否意味着我们不能再降低这个数据集的维度？我想用一个样本数据集来说明这一点。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi of"><img src="../Images/773e96825db9740b61dde2fdd4e0e799.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*4CRSHwsXZjEuQCCAIYLbUw.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">虚线代表身高和体重的差异。图片由作者提供。</p></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8eb5c5c553c0000bb5fd7cdab5c6af42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*EpAI6DKFzOIvLZx4aRF9MQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">为了公平比较，所有特征都被标准化为相同的尺度。图片由作者提供。</p></figure><p id="d6c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种情况下，很难选择我们想要删除的变量。如果我丢弃任何一个变量，我们就丢弃了一半的信息。</p><p id="9bbe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们能同时保留<em class="nv">和</em>吗？</p><p id="5562" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或许，换个角度。</p><p id="6001" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最好的故事书总是有隐藏的主题，这些主题没有写出来，但是<em class="nv">暗示了</em>。单独阅读每一章是没有意义的。但是如果我们全部读完，它给了我们足够的背景来拼凑这些谜题——潜在的情节出现了。</p><p id="fde6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我们只是分别研究了身高和体重的变化。与其限制我们只能选择其中之一，为什么不把它们结合起来呢？</p><p id="8fa4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们仔细观察我们的数据时，方差的最大值不在 x 轴上，也不在 y 轴上，而是在一条对角线上。第二大方差是一条与第一个方差成 90 度的线。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi og"><img src="../Images/68f50c3a0e8b286981ef4cb6dad7e7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*MzFj06hkX1OLzuEzOuSE7g.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">虚线表示最大方差的方向。图片由作者提供。</p></figure><p id="0dd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了表示这两条线，PCA 结合了身高和体重来创建两个全新的变量。可能是 30%的身高和 70%的体重，或者 87.2%的身高和 13.8%的体重，或者任何其他组合，这取决于我们现有的数据。</p><p id="be61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这两个新变量被称为<strong class="js iu">第一主成分(PC1) </strong>和<strong class="js iu">第二主成分(PC2) </strong>。我们可以分别使用 PC1 和 PC2，而不是在两个轴上使用身高和体重。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oh"><img src="../Images/aa294d12b3150974fae3f38a553d39a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oF9D29AhygWCWbV3qFTSNQ.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">(<strong class="bd oi">左</strong>)红色和绿色箭头是原始数据中的主轴。图片由作者提供。| ( <strong class="bd oi">右</strong>)主轴的方向已经旋转成为新的 x 轴和 y 轴。图片由作者提供。</p></figure><p id="bb10" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在所有的恶作剧之后，让我们再来看看差异。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oj"><img src="../Images/0e6fe04e77d83647a295374b0a2bca8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhF4p6gENa4-oT_LspJxYA.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">(<strong class="bd oi">左</strong>)原始数据中身高体重方差相近。图片由作者提供。| ( <strong class="bd oi">右</strong>)PCA 变换后，所有的方差都显示在 PC1 轴上。图片由作者提供。</p></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/36356f55b8d9596de07d489bf89f7199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*64K39wB_y1S1RIvJEMnmng.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">为了公平比较，所有变量都被标准化为相同的尺度。图片由作者提供。</p></figure><p id="622f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PC1 可以单独获取身高和体重的总方差。因为 PC1 拥有所有信息，所以您已经知道了该步骤—我们可以轻松地删除 PC2，并且知道我们的新数据仍然代表原始数据。</p><p id="a16e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当涉及到真实数据时，通常情况下，我们不会得到一个主成分来捕捉 100%的方差。执行 PCA 将给出 N 个主成分，其中 N 等于原始数据的维数。从这个主成分列表中，我们通常选择最少数量的主成分来解释我们原始数据的最大数量。</p><p id="1458" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">帮助我们做出这个决定的一个很好的视觉辅助工具是<strong class="js iu"> Scree Plot </strong>。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/623818105aa759b963836148613ea4c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*FTHaLTSEGLQQ9ygNPd3HyQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">三维数据集的碎石图示例。图片由作者提供。</p></figure><p id="cd21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">条形图告诉我们由每个主成分解释的方差的比例。另一方面，叠加的折线图给出了解释方差的累积和，直到第 N 个主成分。理想情况下，我们希望仅用 2 到 3 个分量就能获得至少 90%的方差，以便保留足够的信息，同时我们仍能在图表上可视化我们的数据。</p><p id="5da6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看着图表，我会觉得使用 2 个主要成分很舒服。</p><h1 id="8b6b" class="lk ll it bd lm ln ne lp lq lr nf lt lu lv ng lx ly lz nh mb mc md ni mf mg mh bi translated">逃脱的个人电脑</h1><p id="70ad" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">由于我们没有选择所有的主成分，我们不可避免地会丢失一些信息。但是我们还没有确切地描述我们正在失去什么。让我们用一个新的玩具例子来深入探讨这个问题。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7fce7cc2fd07fc547a0e6ba959f77136.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*sfJ1VCTT2CrxX2M6-A-DhQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">这些点是分散的，但我们仍然可以在对角线上看到一些正相关。图片由作者提供。</p></figure><p id="1f17" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们通过主成分分析模型输入数据，它将首先绘制第一个主成分，然后绘制第二个主成分。当我们将原始数据从二维转换到二维时，除了方向之外，一切都保持不变。我们刚刚旋转了数据，使最大方差出现在 PC1 中。这里没什么新鲜的。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi om"><img src="../Images/f2f798b2f93df75131a14681a42df948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ET5JFZMHbP4RxIwIqZRLyQ.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">(<strong class="bd oi">左</strong>)虚线是第一和第二主成分的方向。图片由作者提供。| ( <strong class="bd oi">右</strong> ) PCA 旋转数据，因此将最大方差放在 PC1 上，然后是 PC2。图片由作者提供。</p></figure><p id="4951" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，假设我们决定只保留第一个主成分，我们必须将所有数据点投影到第一个主成分上，因为我们不再有 y 轴。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi on"><img src="../Images/1a80d34b3429a0fd5b5d25d9577ed710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gmJrZS1MCD2vz6_aErm3g.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">(<strong class="bd oi">左</strong>)虚线是第一和第二主成分的方向。图片由作者提供。| ( <strong class="bd oi">右</strong>)现在所有的点都在虚线上，因为我们去掉了第二个主成分。图片由作者提供。</p></figure><p id="d4fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们会丢失第二个主成分中的距离，下面用红色线突出显示。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8ab5c0b3975c3ab329cb4c77b893cfe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*wyNzDZA6swVMJOcTeGOGtg.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">所有红线都是第二主成分中的值，它们已被移除。图片由作者提供。</p></figure><p id="903d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这对每个数据点的感知距离有影响。如果我们查看两个特定点之间的欧几里德距离(也称为成对距离)，您会注意到原始数据中的一些点比转换数据中的要远得多。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi op"><img src="../Images/2573a306b32754380e61adc0543550b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w97bieYmdWhv_Olrxe-Ibw.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">在(<strong class="bd oi">左</strong>)和(<strong class="bd oi">右</strong>)维度之前和之后的两个点之间的成对欧几里德距离的比较从 2 维减少到 1 维。图片由作者提供。</p></figure><p id="f968" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">主成分分析是一种线性变换，因此它本身不会改变距离，但是当我们开始移除维度时，距离就会发生扭曲。</p><p id="a01f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事情变得更棘手了——并不是所有成对距离都受到同等影响。</p><p id="9fdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们取最远的两个点，你会看到它们几乎平行于主轴。虽然它们的欧几里得距离仍然是扭曲的，但程度要小得多。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi oq"><img src="../Images/1ba0148e48ea5c4b64503a51dd9ed438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wivMlZPGciDshoQluvENOg.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">在降维之前(<strong class="bd oi">左</strong>)和之后(<strong class="bd oi">右</strong>)这两个点之间的成对欧几里德距离保持相当一致。图片由作者提供。</p></figure><p id="f068" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">原因是主成分轴被画在方差最大的方向上。根据定义，当数据点相距较远时，方差会增加。因此，自然地，相距最远的点会更好地与主轴对齐。</p><p id="36bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总而言之，用 PCA 降维改变了我们数据的距离。它以一种比小的成对距离更好地保持大的成对距离的方式来做到这一点。</p><p id="b4ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是使用 PCA 降低维数的少数缺点之一，我们需要意识到这一点，尤其是在使用基于欧氏距离的算法时。</p><p id="a636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有时，在原始数据上运行您的算法可能更有好处。这就是你，一个数据科学家需要根据你的数据和用例做出决定的地方。</p><p id="6022" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">毕竟，数据科学既是科学，也是艺术。</p><h1 id="37f8" class="lk ll it bd lm ln ne lp lq lr nf lt lu lv ng lx ly lz nh mb mc md ni mf mg mh bi translated">在 Python 中实现 PCA</h1><p id="5a1d" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">除了本文的前提之外，PCA 还有更多内容。真正领略 PCA 之美的唯一途径就是亲身体验。因此，我很乐意在这里分享一些代码片段给任何想动手的人。完整的代码可以通过 Google Colab 在<a class="ae kx" href="https://colab.research.google.com/drive/1RC_XulRdrqpYRq4h8pRl22cfg9a-_FvS?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>访问。</p><p id="852c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们把导入的东西去掉，并生成一些我们将要使用的数据。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="954f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的玩具数据集有 3 个变量——x0、x1 和 x2，它们分布在 3 个不同的簇中。“cluster_label”告诉我们数据点属于哪个集群。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/304b6295f9c05d65d3d0f12f7f0fb354.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*YL5ccA9BMnC81mSsSNv_TQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">玩具示例数据集的前 5 行。图片由作者提供。</p></figure><p id="3a55" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">只要有可能，将它们形象化总是一个好主意。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/832500eaae2a1848897629524110f49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*RTuxwyj4TDZWLCEw1tWeyg.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">三维图表上的玩具数据。图片由作者提供。</p></figure><p id="0d44" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据似乎已准备好进行主成分分析。我们将尝试降低它的维度。幸运的是，Sklearn 使得 PCA 非常容易执行。尽管我们花了 2000 多字来解释 PCA，但我们只需要 3 行代码来运行它。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="2f72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里有几个活动部件。当我们将我们的数据拟合到 Sklearn 的 PCA 函数时，它会执行所有繁重的工作来返回 PCA 模型和转换后的数据。</p><p id="d0ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该模型让我们获得了众多的属性，如特征值，特征向量，原始数据的平均值，方差解释，等等。如果我们想了解 PCA 对我们的数据做了什么，这些非常有见地。</p><p id="d5ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我想强调的一个属性是<code class="fe ov ow ox oy b">pca.explained_variance_ratio_</code>，它告诉我们由每个主成分解释的方差的比例。我们可以用碎石图来想象这一点。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4ada91a9fe970ac695efb1c67d4e086d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*o-qrkKUflz_xF9tzH4BQZQ.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">叠加在条形图上的线形图，显示每台电脑的差异比例。图片由作者提供。</p></figure><p id="7e6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图表告诉我们，使用 2 个主成分而不是 3 个主成分很好，因为它们可以捕捉 90%以上的方差。</p><p id="18cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">除此之外，我们还可以看看用<code class="fe ov ow ox oy b">pca.components_**2</code>创建每个主成分的变量组合。我们可以用热图来展示这一点。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e45319653c132d9763331c65d0745f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*YElUyZ5o9JeKNbgR7vKFCw.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">每台电脑都是由多个变量组合而成的。图片由作者提供。</p></figure><p id="2326" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的示例中，我们可以看到 PCA1 由 34%的 x0、30%的 x1 和 36%的 x2 组成。PCA2 主要受 x1 支配。</p><p id="1a7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Sklearn 提供了许多更有用的属性。对于感兴趣的人，我推荐看看<a class="ae kx" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> Sklearn 文档</a>上 PCA 的属性部分。</p><p id="ab73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们对主成分有了更好的理解，我们可以最终决定我们想要保留的主成分的数量。在这种情况下，我觉得 2 个主成分就足够了。</p><p id="1b9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，我们可以重新运行 PCA 模型，但是这次使用了<code class="fe ov ow ox oy b">n_components=2</code>参数，它告诉 PCA 只为我们保留前 2 个主成分。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="37aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将返回给我们一个带有前两个主要成分的数据帧。最后，我们可以绘制一个散点图来可视化我们的数据。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi pb"><img src="../Images/55da0fd8afff77a18ab97045f0eed307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFpLzGeW1wpVzgL5x49B5w.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">(<strong class="bd oi">左</strong>)原始数据。图片由作者提供。| ( <strong class="bd oi">右</strong>)相同的数据，但用 PCA 简化为 2-D。图片由作者提供。</p></figure><h1 id="7943" class="lk ll it bd lm ln ne lp lq lr nf lt lu lv ng lx ly lz nh mb mc md ni mf mg mh bi translated">结束语</h1><p id="a3f7" class="pw-post-body-paragraph jq jr it js b jt mi jv jw jx mj jz ka kb mk kd ke kf ml kh ki kj mm kl km kn im bi translated">PCA 是一个数学上美丽的概念，我希望我能够以一种随意的语气来表达它，这样就不会感到势不可挡。对于那些渴望了解本质细节的人，我在下面附上了一些有趣的讨论/资源供你阅读。</p><p id="7621" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">谢谢你的时间，并有一个伟大的一天。</p></div><div class="ab cl ld le hx lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="im in io ip iq"><p id="9afb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]:中等，法尔哈德·马利克(2019 年 1 月 7 日)。<em class="nv">什么是特征值和特征向量？<br/></em><a class="ae kx" href="https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47" rel="noopener">https://medium . com/fintech explained/what-are-特征值和特征向量-机器学习的必备概念-80d0fd330e47 </a></p><p id="d340" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]: GitHub。<em class="nv">深入:主成分分析<br/></em><a class="ae kx" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html" rel="noopener ugc nofollow" target="_blank">https://jakevdp . github . io/python datascience handbook/05.09-Principal-Component-Analysis . html</a></p><p id="d844" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]: StackExchange，whuber(2013 年 2 月 21 日)。<em class="nv">在进行主成分分析之前，是否应该去除高度相关的变量？<br/></em><a class="ae kx" href="https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/50537/should-one-remove-high-correlated-variables-before-do-PCA</a></p><p id="e674" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]: StackExchange，ttnphns(2017 年 4 月 13 日)。<em class="nv"> PCA 和方差比例解释</em><a class="ae kx" href="https://stats.stackexchange.com/questions/22569/pca-and-proportion-of-variance-explained" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/22569/PCA-和-方差比例解释</a></p><p id="dcf1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]: StackExchange，阿米巴原虫(2017 . 04 . 13)。<em class="nv">PCA 仅保留大的成对距离是什么意思？<br/></em><a class="ae kx" href="https://stats.stackexchange.com/questions/176672/what-is-meant-by-pca-preserving-only-large-pairwise-distances" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/176672/what-is-mean-by-PCA-preserving-only-large-pair-distance</a></p><p id="34b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[6]: StackExchange，阿米巴原虫(2015 年 3 月 6 日)。<em class="nv">主成分分析的意义，特征向量&amp;特征值<br/></em><a class="ae kx" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/2691/Making-sense-of-main-component-analysis-特征向量-特征值/140579#140579 </a></p></div></div>    
</body>
</html>