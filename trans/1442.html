<html>
<head>
<title>Simple Regularized Lasso Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单正则化套索回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-regularized-lasso-regression-31ef20eb3c21#2022-04-08">https://towardsdatascience.com/simple-regularized-lasso-regression-31ef20eb3c21#2022-04-08</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="0e4e" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">L1正则化</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/77226ad9209fadb633aefd32dbae07bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1vA4-K7e4NdwHI4VhK9rg.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><blockquote class="kz la lb"><p id="de6b" class="lc ld le lf b lg lh jv li lj lk jy ll lm ln lo lp lq lr ls lt lu lv lw lx ly in bi translated"><strong class="lf iv">简介</strong></p></blockquote><p id="907f" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">当我们从使用训练数据拟合的模型中观察到测试数据中的高方差时，我们知道需要在模型中引入一些偏差。否则，从长期来看，它将始终产生很高的方差，因此被认为是一个弱模型。实现这一点的几种方法之一是套索回归。套索回归与岭回归非常相似，但也有一个非常重要的区别。读者可以查看关于岭回归的文章，因为我将使用相同的例子来实现套索回归。</p><div class="mc md gq gs me mf"><a rel="noopener follow" target="_blank" href="/simple-regularized-linear-and-polynomial-regression-37d0d634ece3"><div class="mg ab fp"><div class="mh ab mi cl cj mj"><h2 class="bd iv gz z fq mk fs ft ml fv fx it bi translated">简单正则化线性和多项式回归</h2><div class="mm l"><h3 class="bd b gz z fq mk fs ft ml fv fx dk translated">L2正则化</h3></div><div class="mn l"><p class="bd b dl z fq mk fs ft ml fv fx dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt kt mf"/></div></div></a></div><blockquote class="kz la lb"><p id="bd89" class="lc ld le lf b lg lh jv li lj lk jy ll lm ln lo lp lq lr ls lt lu lv lw lx ly in bi translated"><strong class="lf iv">山脊vs拉索</strong></p></blockquote><p id="446e" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">先说简单的线性回归。在这种情况下，我们尝试最小化残差的平方，从而得到一条生成最小误差平方和的线。线性拟合的示例如下所示。这是根据以下字典创建的合成数据集。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mu mv l"/></div></figure><p id="5d3b" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">以下示例中的均方误差为0.055，这是所有其他可能的线中最小的。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mw"><img src="../Images/68aa315486fa2fc2881fa3b0278eb7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoQTpUkxA6A4J8fYgyp9ow.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">线性拟合的训练数据[图片由作者提供]</p></figure><p id="3f2b" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">在岭回归中，我们以训练数据中的方差为代价引入了一个小的偏差，但它减少了测试数据中的长期方差，这是偏差-方差权衡的一个典型场景。岭回归的核心是最小化残差平方和以及λ乘以斜率的平方。然而，套索的目标是相同的，但程序是不同的。这里，要最小化的项是残差的平方和加上λ乘以斜率的绝对值。就是这样。这是核心区别。这些回归模型中要最小化的项总结如下。当然，lambda项可以通过交叉验证来确定，并针对给定数据集取最佳值。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mx"><img src="../Images/02465457892d367b69890af937446565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQl1PlyHqA3MLgd9gvDNEg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">简单线性回归vs岭回归vs套索回归</p></figure><blockquote class="kz la lb"><p id="f101" class="lc ld le lf b lg lh jv li lj lk jy ll lm ln lo lp lq lr ls lt lu lv lw lx ly in bi translated"><strong class="lf iv">套索在行动</strong></p></blockquote><p id="0310" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">为了便于比较，让我们使用用于演示岭回归的同一个数据集。蓝点是训练数据，红点是测试数据。测试数据集也是从以下字典创建的合成数据集。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mu mv l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj my"><img src="../Images/1484154672571ccf2c3f7aa97884ae64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjgga6p1QtAHEKWgjmktJg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">训练(蓝色)和测试(红色)数据[图片由作者提供]</p></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mu mv l"/></div></figure><p id="fc7c" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">在上面的例子中，当lasso回归被实施并且模型被拟合在训练数据上时，测试数据的均方误差是5.41，这略小于岭的情况，在岭的情况下，我们获得的值是6.88的均方误差，并且也明显小于简单的线性回归结果，简单的线性回归结果以8.41的均方误差结束。同样，alpha值越高，自变量对回归模型的影响越小。这基本上意味着当我们增加α值时，模型的系数接近零。对于非常高的α值，它将是完全水平的，对独立变量没有依赖性。因此，需要优化以通过交叉验证来确定alpha值。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mz"><img src="../Images/15e4c364cbec134f4a2b0bd9f2ddb1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*ww5YsvtflwSPHZtEAUqgBQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">套索回归的交叉验证</p></figure><p id="f4da" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">我们可以从很低到很高随机选择α的值。这里，我们扫描了特定数据集的alpha值，并确定了每种情况下的MSE(均方误差)值。最小的MSE是α= 0.1，然后当使用LassoCV时获得相同的结果。</p><blockquote class="kz la lb"><p id="7348" class="lc ld le lf b lg lh jv li lj lk jy ll lm ln lo lp lq lr ls lt lu lv lw lx ly in bi translated"><strong class="lf iv">结论</strong></p></blockquote><p id="4249" class="pw-post-body-paragraph lc ld iu lf b lg lh jv li lj lk jy ll lz ln lo lp ma lr ls lt mb lv lw lx ly in bi translated">岭回归和套索回归都是非常有用的工具，可以有效地消除训练数据集的过度拟合。这两种方法都惩罚模型系数，并试图最小化误差平方和加上惩罚项。只有这个罚项在山脊法和套索法之间有所不同。随着我们对高维回归分析的深入研究，我们将会看到，当存在大量不必要的独立变量时，岭方法可以降低不必要变量的系数，而lasso可以有效地使它们为零，从而完全消除它们对结果的影响。</p><div class="mc md gq gs me mf"><a href="https://mdsohel-mahmood.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fp"><div class="mh ab mi cl cj mj"><h2 class="bd iv gz z fq mk fs ft ml fv fx it bi translated">用我的推荐链接加入媒体</h2><div class="mm l"><h3 class="bd b gz z fq mk fs ft ml fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fq mk fs ft ml fv fx dk translated">mdsohel-mahmood.medium.com</p></div></div><div class="mo l"><div class="na l mq mr ms mo mt kt mf"/></div></div></a></div></div></div>    
</body>
</html>