<html>
<head>
<title>Explainable Machine Learning for Models Trained on Text Data: Combining SHAP with Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于文本数据训练的模型的可解释机器学习:结合SHAP和变压器模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8#2022-05-25">https://towardsdatascience.com/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8#2022-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="445c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释文本分类器变得容易与拥抱脸零杆学习与SHAP</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8f50ffdfecc025ece1dc186ef492ebc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XqsJMzGyqZaUdgBB"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像来源— <a class="ae ky" href="https://images.unsplash.com/photo-1586769506823-483a8228a6eb?ixlib=rb-1.2.1&amp;raw_url=true&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;auto=format&amp;fit=crop&amp;w=1170" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="96a3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">给你一个背景</h1><p id="ed7a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://www.amazon.com/dp/1803246154/ref=as_sl_pc_as_ss_li_til?tag=adib0073-20&amp;linkCode=w00&amp;linkId=74817cc3f53ad04fe861a6ed9f619830&amp;creativeASIN=1803246154" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">【XML】</strong></a>或<strong class="lt iu"> </strong> <a class="ae ky" href="https://www.amazon.com/dp/1803246154/ref=as_sl_pc_as_ss_li_til?tag=adib0073-20&amp;linkCode=w00&amp;linkId=74817cc3f53ad04fe861a6ed9f619830&amp;creativeASIN=1803246154" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">可讲解人工智能(XAI) </strong> </a>是所有工业级<strong class="lt iu">机器学习(ML) </strong>或<strong class="lt iu">人工智能(AI) </strong>系统的必需品。没有可解释性，ML总是被怀疑地采用，从而限制了在业务用例中使用ML的好处。</p><div class="mn mo gp gr mp mq"><a href="https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&amp;pd_rd_w=Wr6SJ&amp;content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_r=6P2PM599T97MRG7NZD9J&amp;pd_rd_wg=m4qUW&amp;pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&amp;linkCode=li3&amp;tag=adib0073-20&amp;linkId=35506e1847de5c011fc57aa66c2b1d8e&amp;language=en_US&amp;ref_=as_li_ss_il" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">应用机器学习可解释技术:使ML模型可解释和可信…</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">应用机器学习可解释技术:使ML模型可解释和可信赖的实践…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.amazon.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="6ccf" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">由于文本数据非常丰富，因此有很大的机会解决涉及文本数据的数据科学问题。但与此同时，文本数据非常嘈杂，为了概括文本模型，需要大量的数据。最近，transformer模型在文本数据方面显示出了很好的结果，而不是传统的<strong class="lt iu">自然语言处理(NLP) </strong>方法。然而，在大量的原始文本数据上训练transformer模型是麻烦且资源昂贵的。幸运的是，随着预先训练的transformer模型的出现，使用原始文本数据构建AI应用程序的过程已经得到了极大的简化。<strong class="lt iu">抱脸变形金刚模型</strong>(<a class="ae ky" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a>)被认为是在海量文本数据上训练出来的最先进的预训练模型。就个人而言，我是拥抱脸框架的忠实粉丝，因为我可以使用几行代码在各种应用程序上利用预先训练的模型。预先训练的模型可以很容易地下载并在任何自定义数据集上进行微调，并且可以非常容易地与其他生产级应用程序集成。</p><p id="9b35" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">不幸的是，变压器是非常复杂的模型，很难解释。因此，为了简化模型解释过程，我们将探索最流行的XAI框架之一<strong class="lt iu">SHAP</strong>(<a class="ae ky" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">https://github.com/slundberg/shap</a>)。在这篇文章中，我们将详细讨论如何使用SHAP来解释<strong class="lt iu">抱脸零拍学习模型</strong>并附带代码教程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/bc834bd64e74f2a568e4ffee39e9e8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ip-mn6QrxDRuEKLd"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae ky" href="https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter07/images/Shap_HF.jpg" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></figure><h1 id="82ae" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是零拍学习？</h1><p id="e411" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">零镜头学习(Zero shot learning)(ZSL)是一个机器学习过程，其中在推理时间，学习者观察来自类的样本，这些样本在<a class="ae ky" href="https://en.wikipedia.org/wiki/Machine_learning#Training_models" rel="noopener ugc nofollow" target="_blank"> t </a> he训练过程中没有观察到，但它需要预测它们所属的类。ZSL使我们能够应用一个经过训练的模型，而无需对任何带标签的样本进行微调。从技术上来说，这个概念对于文本模型和生产系统来说是一个游戏规则的改变者，有了ZSL，推广任何定制的应用程序就容易多了。看看这个:<a class="ae ky" href="https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681" rel="noopener ugc nofollow" target="_blank">https://discuse . hugging face . co/t/new-pipeline-for-zero-shot-text-class ification/681</a>了解更多关于使用拥抱脸变形金刚使用ZSL的信息。</p><h1 id="f6cc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">什么是SHAP </strong>？</h1><p id="9a13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如https://github.com/slundberg/shap在SHAP的GitHub知识库中所描述的那样:</p><blockquote class="nl nm nn"><p id="dc40" class="lr ls no lt b lu nf ju lw lx ng jx lz np nh mc md nq ni mg mh nr nj mk ml mm im bi translated">SHAP(SHapley Additive explaints)是一种解释任何机器学习模型输出的博弈论方法。它将最优信用分配与使用博弈论及其相关扩展的经典Shapley值的本地解释联系起来。</p></blockquote><p id="214d" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这是最流行的XAI方法之一，因为它可以为不同类型的数据(如表格、文本和图像)提供模型不可知的解释能力。</p><div class="mn mo gp gr mp mq"><a href="https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&amp;pd_rd_w=Wr6SJ&amp;content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_r=6P2PM599T97MRG7NZD9J&amp;pd_rd_wg=m4qUW&amp;pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&amp;linkCode=li3&amp;tag=adib0073-20&amp;linkId=35506e1847de5c011fc57aa66c2b1d8e&amp;language=en_US&amp;ref_=as_li_ss_il" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">应用机器学习可解释技术:使ML模型可解释和可信…</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">应用机器学习可解释技术:使ML模型可解释和可信赖的实践…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.amazon.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="91fc" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">如果你对XAI概念不太熟悉，我强烈推荐你观看过去在2021年APAC人工智能加速器节上发表的关于XAI的演讲:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="feb6" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">我想让这篇文章更加实用，因为网上有大量的资源可以用来了解概念和算法。在paper with code(<a class="ae ky" href="https://paperswithcode.com/search?q_meta=&amp;q_type=&amp;q=zero+shot+learning" rel="noopener ugc nofollow" target="_blank">https://paperswithcode.com/search?q_meta=&amp;q _ type =&amp;q = zero+shot+learning</a>)上可以找到的研究文献将是了解更多关于ZSL的一个很好的起点。让我们开始用SHAP为ZSL模型应用模型可解释性。</p><h1 id="a9a0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">设置</h1><p id="fa19" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在来自Google Colab或您的本地环境的Python Jupyter笔记本中安装以下库，以成功执行代码:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b4ac" class="nz la it nv b gy oa ob l oc od"><strong class="nv iu">!</strong>pip install --upgrade numpy shap transformers</span></pre><p id="33d5" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">请确保在继续之前导入以下Python库:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="6849" class="nz la it nv b gy oa ob l oc od"><strong class="nv iu">import</strong> numpy <strong class="nv iu">as</strong> np<br/><strong class="nv iu">import</strong> shap<br/><strong class="nv iu">import</strong> transformers<br/><strong class="nv iu">from</strong> transformers <strong class="nv iu">import</strong> ZeroShotClassificationPipeline<br/><strong class="nv iu">from</strong> typing <strong class="nv iu">import</strong> Union, List</span></pre><p id="73d8" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">有时，这些Python框架的不同版本会产生不同的结果。因此，请检查您的环境中安装的版本，我们可以在评论中讨论，以防您在安装的版本中遇到任何问题。我在这个例子中使用的版本是:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c028" class="nz la it nv b gy oa ob l oc od">Shap version used: 0.40.0<br/>Hugging Face transformer version used: 4.14.1</span></pre><h1 id="33ce" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">加载拥抱脸预训练模型</h1><p id="9362" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们将使用来自拥抱脸的预先训练的变形金刚模型。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="718a" class="nz la it nv b gy oa ob l oc od">model <strong class="nv iu">=</strong> AutoModelForSequenceClassification<strong class="nv iu">.</strong>from_pretrained("valhalla/distilbart-mnli-12-3")<br/>tokenizer <strong class="nv iu">=</strong> AutoTokenizer<strong class="nv iu">.</strong>from_pretrained("valhalla/distilbart-mnli-12-3")</span></pre><p id="50fc" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated"><strong class="lt iu"> DistilBart-MNLI </strong>主要是一个<strong class="lt iu"> BART汇总模型</strong>(<a class="ae ky" href="https://huggingface.co/valhalla/distilbart-mnli-12-1" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/valhalla/distilbart-mnli-12-1</a>)但是我们将把它用于我们的ZSL模型。</p><h1 id="f231" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">设置模型管线</h1><p id="d0f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们可以使用以下方法设置模型管线:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0991" class="nz la it nv b gy oa ob l oc od"><em class="no"># Create a custom pipeline that only requires the text parameter </em><br/><em class="no"># for the __call__ method and provides a method to set the labels</em><br/><strong class="nv iu">class</strong> ZeroShotModelPipeline(ZeroShotClassificationPipeline):<br/>    <em class="no"># Overwrite the __call__ method</em><br/>    <strong class="nv iu">def</strong> __call__(self, <strong class="nv iu">*</strong>args):<br/>        out <strong class="nv iu">=</strong> super()<strong class="nv iu">.</strong>__call__(args[0], self<strong class="nv iu">.</strong>set_labels)[0]<br/><br/>        <strong class="nv iu">return</strong> [[{"label":x[0], "score": x[1]}  <strong class="nv iu">for</strong> x <strong class="nv iu">in</strong> zip(out["labels"], out["scores"])]]<br/><br/>    <strong class="nv iu">def</strong> set_labels(self, labels: Union[str,List[str]]):<br/>        self<strong class="nv iu">.</strong>set_labels <strong class="nv iu">=</strong> labels</span><span id="660f" class="nz la it nv b gy oe ob l oc od"><strong class="nv iu">def</strong> score_and_visualize(text, shap_values):<br/>    prediction <strong class="nv iu">=</strong> pipe(text)<br/>    print(f"Model predictions are: {prediction}")<br/>    shap<strong class="nv iu">.</strong>plots<strong class="nv iu">.</strong>text(shap_values)</span><span id="ff75" class="nz la it nv b gy oe ob l oc od"><em class="no"># Assign the labels for the classification model</em><br/>model<strong class="nv iu">.</strong>config<strong class="nv iu">.</strong>label2id<strong class="nv iu">.</strong>update({v:k <strong class="nv iu">for</strong> k,v <strong class="nv iu">in</strong> enumerate(labels)})<br/>model<strong class="nv iu">.</strong>config<strong class="nv iu">.</strong>id2label<strong class="nv iu">.</strong>update({k:v <strong class="nv iu">for</strong> k,v <strong class="nv iu">in</strong> enumerate(labels)})</span><span id="a9a7" class="nz la it nv b gy oe ob l oc od">pipe <strong class="nv iu">=</strong> ZeroShotModelPipeline(model<strong class="nv iu">=</strong>model, tokenizer<strong class="nv iu">=</strong>tokenizer, return_all_scores<strong class="nv iu">=True</strong>)</span><span id="7104" class="nz la it nv b gy oe ob l oc od"># Setting up the inference text data and the corresponding labels<br/>text = ["You can run faster than Lions"]<br/>labels = ["bird", "animal", "insects"]</span><span id="09fc" class="nz la it nv b gy oe ob l oc od"># Setting the on-the-fly labels to the model pipeline<br/>pipe<strong class="nv iu">.</strong>set_labels(labels)</span></pre><h1 id="1295" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">解释的SHAP</h1><p id="e854" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">接下来，我们将使用SHAP模型的可解释性。我们将使用SHAP可解释对象，并应用于管道模型。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="5b57" class="nz la it nv b gy oa ob l oc od"><em class="no"># SHAP Explainer</em><br/>explainer <strong class="nv iu">=</strong> shap<strong class="nv iu">.</strong>Explainer(pipe)<br/>shap_values <strong class="nv iu">=</strong> explainer(text)</span><span id="dea3" class="nz la it nv b gy oe ob l oc od">score_and_visualize(text, shap_values)</span></pre><p id="a467" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这将为您提供以下结果和力图可视化:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b4d0" class="nz la it nv b gy oa ob l oc od">Model predictions are: [[{'label': 'animal', 'score': 0.8714343905448914}, {'label': 'bird', 'score': 0.10309296101331711}, {'label': 'insect', 'score': 0.025472665205597878}]]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/138910228d08855a1c564dc103b3df41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oi_RwIpYyLQzYIoc81-sQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从SHAP框架的力图生成的输出(图片由作者提供)</p></figure><p id="c1fe" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">你也可以得到条形图，找出Shapley值对句子中单词的影响。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="2565" class="nz la it nv b gy oa ob l oc od"><em class="no"># Let's visualize the feature importance towards the outcome - animal</em><br/>shap<strong class="nv iu">.</strong>plots<strong class="nv iu">.</strong>bar(shap_values[0,:,'<em class="no">animal</em>'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/485d7e09ddd05f334797398d3a7cb478.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*AXHTJjuz7o9HSpLdYJdjBg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从SHAP框架的条形图生成的输出(图片由作者提供)</p></figure><p id="f794" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">如你所见，使用SHAP，你可以很容易地突出那些对模型的决策有正面或负面影响的单词。将SHAP用于模型不可知可解释性的过程类似于任何其他的ML模型。我发现这非常有趣，因为即使复杂的变压器模型也可以用几行代码来解释。执行时间也很快！</p><p id="08d3" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">完整的笔记本可以在GitHub资源库中找到—<a class="ae ky" href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/packt publishing/Applied-Machine-Learning-explability-Techniques/blob/main/chapter 07/Explaining _ transformers . ipynb</a>。</p><h1 id="c45e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">更多关于应用机器学习可解释技术</h1><p id="30bc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你喜欢这篇文章，并且想了解更多关于可解释AI (XAI)的知识，请看看我的书<strong class="lt iu"> " </strong> <a class="ae ky" href="https://www.amazon.com/dp/1803246154/ref=as_sl_pc_as_ss_li_til?tag=adib0073-20&amp;linkCode=w00&amp;linkId=74817cc3f53ad04fe861a6ed9f619830&amp;creativeASIN=1803246154" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">应用机器学习可解释技术</strong> </a> <strong class="lt iu"> " </strong>，这是GitHub知识库，其中包含了许多关于书中各个章节的实践教程:<a class="ae ky" href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/packt publishing/Applied-Machine-Learning-explability-Techniques</a>。如果你喜欢GitHub资源库中提供的教程，请在资源库中做fork和star，以示你对这个项目的支持！这本书现在可以在亚马逊上买到。</p><div class="mn mo gp gr mp mq"><a href="https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&amp;pd_rd_w=Wr6SJ&amp;content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&amp;pf_rd_r=6P2PM599T97MRG7NZD9J&amp;pd_rd_wg=m4qUW&amp;pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&amp;linkCode=li3&amp;tag=adib0073-20&amp;linkId=35506e1847de5c011fc57aa66c2b1d8e&amp;language=en_US&amp;ref_=as_li_ss_il" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">应用机器学习可解释技术:使ML模型可解释和可信…</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">应用机器学习可解释技术:使ML模型可解释和可信赖的实践…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.amazon.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="0ba3" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">本文到此为止！希望你觉得有用。你愿意分享反馈或讨论想法吗？请看下一部分。</p><h1 id="3753" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">让我们连接</h1><p id="3765" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你想联系我分享任何反馈，请随时过来，在LinkedIn上说<em class="no">嗨</em>——<a class="ae ky" href="https://www.linkedin.com/in/aditya-bhattacharya-b59155b6/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/aditya-bhattacharya-b59155b6/</a>，我们也可以通过我网站上提到的其他方式联系:<a class="ae ky" href="https://aditya-bhattacharya.net/contact-me/" rel="noopener ugc nofollow" target="_blank">https://aditya-bhattacharya.net/contact-me/</a>。我的其他媒介物品也可以从这里轻松获得:<a class="ae ky" href="https://adib0073.medium.com/" rel="noopener">https://adib0073.medium.com/</a>。快乐学习:)</p></div></div>    
</body>
</html>