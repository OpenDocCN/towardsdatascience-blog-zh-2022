<html>
<head>
<title>Image Super-Resolution: An Overview of the Current State of Research</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像超分辨率:研究现状综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-super-resolution-an-overview-of-the-current-state-of-research-94294a77ed5a#2022-10-20">https://towardsdatascience.com/image-super-resolution-an-overview-of-the-current-state-of-research-94294a77ed5a#2022-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="03e9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对流行技术和剩余挑战的回顾</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/dd1bc3df8d40e1f90ad26dd34c2c816d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*W_ioPRwCxM464cVGvb7eGQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">两幅图像，其中左半部分对应于低分辨率图像，右半部分描绘高分辨率图像。超分辨率方法的目的是改善低分辨率图像，使其尽可能接近高分辨率图像。由<a class="ae kr" href="https://unsplash.com/es/@lbnielsen?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">拉斯·博·尼尔森</a>(左)和<a class="ae kr" href="https://unsplash.com/@boontohhgraphy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">索拉萨克</a>(右)在<a class="ae kr" href="https://unsplash.com/s/photos/building?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的原始图像</p></figure><p id="5ace" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在<a class="ae kr" rel="noopener" target="_blank" href="/what-is-image-super-resolution-and-why-do-we-need-it-9c3bd9dc233e">之前的一篇文章</a>中，给出了超分辨率(SR)的概述以及它为什么会成为一个重要的研究课题。概括地说，它是对低分辨率(LR)图像进行上采样以恢复基础高质量图像的过程。除了分辨率，SR方法通常需要考虑其他因素，如模糊、传感器噪声和压缩，这些因素也可能降低图像质量。</p><p id="0b1f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">存在许多解决SR的方法，这些方法可以大致分为“<em class="lo">非盲</em>”，其中假定影响图像的退化是已知的，或者“<em class="lo">盲</em>”，其中影响图像的确切退化是未知的。</p><p id="542d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">无论采用哪种方法，它们都必须定义一个退化模型，用于确定要使用的LR图像的类型。鉴于大多数现代方法都是基于深度学习的，因此已经对要使用的损失函数和评估指标进行了大量研究，这些损失函数和评估指标会显著影响最终结果的外观和质量。</p><p id="9e45" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们将更详细地讨论所有这些，并概述软件无线电领域最流行和最先进的方法。</p><p id="657f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">概括来说，本文将涉及的主题如下:</p><ol class=""><li id="301c" class="lp lq iq ku b kv kw ky kz lb lr lf ls lj lt ln lu lv lw lx bi translated"><a class="ae kr" href="#dd8f" rel="noopener ugc nofollow">退化模型</a></li><li id="e39a" class="lp lq iq ku b kv ly ky lz lb ma lf mb lj mc ln lu lv lw lx bi translated"><a class="ae kr" href="#2449" rel="noopener ugc nofollow">非盲SR方法</a></li><li id="4fc4" class="lp lq iq ku b kv ly ky lz lb ma lf mb lj mc ln lu lv lw lx bi translated"><a class="ae kr" href="#1aa0" rel="noopener ugc nofollow">盲SR方法</a></li><li id="3911" class="lp lq iq ku b kv ly ky lz lb ma lf mb lj mc ln lu lv lw lx bi translated"><a class="ae kr" href="#5890" rel="noopener ugc nofollow">损失函数和评估指标</a></li><li id="3aa5" class="lp lq iq ku b kv ly ky lz lb ma lf mb lj mc ln lu lv lw lx bi translated"><a class="ae kr" href="#4183" rel="noopener ugc nofollow">结论和未来方向</a></li></ol><h1 id="dd8f" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">退化模型</h1><p id="4516" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">任何SR算法的关键组成部分之一实际上与方法本身无关，而是与所使用的数据有关。具体来说，应用了一个<em class="lo">退化模型</em>,以便制定要使用哪种数据。</p><p id="c47b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">大多数方法倾向于基于<em class="lo">监督</em>学习的方法，需要使用地面真实信息(在SR的情况下对应于原始HR图像),以便算法知道当超解析给定的LR图像时最终结果应该是什么样子。因此，这种退化模型被直接应用于HR图像以产生低分辨率图像。典型地，诸如所考虑的退化幅度的参数被改变以产生多个LR图像。</p><p id="9a65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">“经典”退化模型是最常用的，它考虑了下采样、模糊和噪声:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/987b766e4caac5cecc4c91f10591713f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKK3ELu3EvynR98WggBzjg.png"/></div></div></figure><p id="4c71" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中⊗表示卷积运算，<em class="lo"> k </em>是核(通常是高斯模糊核，但是它也可以表示诸如点扩展函数(PSF)之类的其他函数)，<em class="lo"> n </em>表示加性噪声，并且↓ <em class="lo"> s </em>是缩减操作，其通常被假定为具有比例因子<em class="lo"> s </em>的双三次下采样。</p><p id="d784" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">该模型被批评为过于简单，利用它的方法不能很好地概括更复杂的退化，这种退化通常在现实世界的图像中发现。因此，最近的努力试图设计更真实的退化模型，例如通过包括由图像中非常普遍的压缩引起的伪像:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nf"><img src="../Images/ddc9c51be0a4593eac0d1c8573034ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBa2ETO-u6IRwoNdIQPKZA.png"/></div></div></figure><p id="6762" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中<em class="lo"> C </em>是JPEG等压缩方案。</p><p id="3c84" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">还提出了其他方案，例如通过不止一次地应用经典退化模型的<a class="ae kr" href="https://doi.org/10.1109/ICCVW54120.2021.00217" rel="noopener ugc nofollow" target="_blank">、</a><a class="ae kr" href="https://doi.org/10.1109/ICCV48922.2021.00475" rel="noopener ugc nofollow" target="_blank">随机打乱应用退化的顺序的</a>，以及最近的<a class="ae kr" href="https://doi.org/10.1109/CVPRW56347.2022.00068" rel="noopener ugc nofollow" target="_blank">使用“随机门控制器”，其随机选择要使用哪些基础退化</a>。</p><p id="723d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">SR的目的是然后反转所考虑的任何退化过程，以恢复原始的基本高保真图像。现在将提供执行该任务的流行方法的概述。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="2449" class="md me iq bd mf mg nn mi mj mk no mm mn jw np jx mp jz nq ka mr kc nr kd mt mu bi translated">非盲随机共振方法</h1><p id="25af" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">大多数作品倾向于了解使用了哪种退化模型，因此被称为“非盲”方法。这与“盲目”模型相反，盲目模型不能“看见”，因此对可能影响图像的退化一无所知。虽然非盲方法不太现实，但它们已经形成了包括盲SR方法在内的更复杂方法的基础。</p><p id="1d4f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">超分辨率卷积神经网络(SRCNN) [1，2]被认为是使用深度学习和卷积神经网络完成SR任务的开创性工作。它仅由三层组成，要求LR图像在被网络处理之前使用双三次插值进行上采样，但它的性能优于当时的最新方法，如A+ [3]和基于稀疏表示的方法[4]。研究还表明，基于稀疏编码的方法等效于卷积神经网络，这影响了SRCNN的超参数设置。</p><p id="9671" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">虽然SRCNN已被许多研究人员用作基准，但它现在已被广泛超越，不再经常用于比较目的。然而，由于其相对简单，它可以作为任何对基于深度学习的人工智能领域感兴趣的人的良好起点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ns"><img src="../Images/f2819a46922d5ac58a919cb1cf9e04d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVPwfYfN6GQDBjRmDx8heA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">SRCNN方法的架构[1，2]</p></figure><p id="081d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">残差网络(ResNet)架构[5，6]主要用于随着层数的增加而简化网络的训练。事实上，虽然文献中的许多作品都表明，更深的网络提供了更好的性能，但准确性会达到饱和，然后迅速下降。研究表明，这不是由于过拟合，而是由于优化和训练非常深的网络的困难。特别是，有人指出，深层网络可能会发现很难学习身份功能。</p><p id="e761" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了解决这个问题，许多跳过/快捷连接被用来将网络的任何给定级别的特征地图直接馈送到更高层，因此对应于身份功能。这样，网络将只需要学习抑制中间层的响应(将输出驱动到零)的函数。</p><p id="0a12" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在[7]中，ResNet被应用于SR域以创建SRResNet，其中它也被用作基于生成对抗网络(GAN)的网络(称为SRGAN)的基础。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/6f3c4b2f8cb82df296d692b6cb14d896.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/0*mMdfQrFQvyFm-vBm"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">ResNet中剩余学习的构建模块[5，6]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nu"><img src="../Images/a41097de7a3e1aca7c248468f5f521d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DYSpZMu_9OKM_8fE"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">SRGAN的生成器(基于SRResNet)和鉴别器网络的架构，其中<em class="nv"> k </em>表示内核大小，<em class="nv"> n </em>是特征映射的数量，<em class="nv"> s </em>是步距[7]</p></figure><p id="1c18" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">增强型深度超分辨率(EDSR)方法[8]也是基于ResNet，并结合了SRResNet等先前工作中报告的结论。所提出的方法的贡献之一是去除了<a class="ae kr" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批标准化</a>层，以禁用特征值的限制并减少训练期间的内存使用，从而允许使用更多的层和过滤器。残差缩放还用于确保网络的稳定性(尤其是在使用大量要素时)。</p><p id="f9e8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">还设计了称为多尺度深度超分辨率(MDSR)的多尺度网络，其本质上包括用于三个不同上采样因子(2，3，4)的公共网络，以及在预处理阶段的尺度特定模块和在由卷积和“洗牌”层组成的网络末端的上采样模块。因此，所提出的方法不需要像在一些其他工作中(例如SRCNN)那样对输入图像进行双三次插值。提出的方法被证明超过了NTIRE 2017竞赛中评估的方法的性能[9]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3e65ec0e466140d7cdf58132fe978d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/0*oMdd65HTsZZkHbqb"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">EDSR建筑[8]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/dbb4b570eab678cee336c8eeac566c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/0*zrjTaAibxnB7uuHE"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">ResNet(左)、SRResNet(中)和EDSR(右)中使用的残差块之间的差异[8]</p></figure><p id="856c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">剩余通道注意力网络(RCAN)是在[10]中提出的，以抵消训练非常深的CNN的困难和跨通道的低频信息的同等重要性的分配。因此，RCAN由“剩余组”组成，每个“剩余组”包含多个“剩余频道注意块”。“长跳跃连接”与残差组一起使用，使得信息能够从网络的早期阶段传播到最终阶段，而“短跳跃连接”在残差块内使用，用于以更精细的级别传播信息。作者还指出，这种残差中残差的架构能够训练非常深的CNN(超过400层)。</p><p id="a9d7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">RCAN的表现优于美国有线电视新闻网(SRCNN)和EDSR等方法，并与更现代的方法保持相当的竞争力。它还被证明对于物体识别是有效的，其中当与通过其他方法上采样的图像相比时，由RCAN输出的上采样图像能够实现更高的精度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ny"><img src="../Images/1d21176fc4840786781079d81699535a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*85ThQt4PnROB4BNU"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">RCAN建筑[10]</p></figure><p id="00f1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">SRGAN在[11]中进行了改进，以产生增强的SRGAN (ESRGAN)，改进集中在:(I)网络架构，其中去除了批量归一化(类似于EDSR ),并且残差中残差密集块(RRDB)被提议作为网络的基本构建块，以实现更高的网络容量并促进训练，(ii)对抗损失，其被修改以确定图像的相对“真实度”(即，如果图像比假数据更真实或者比真实数据更不真实， 而不是简单的真实或虚假)，以及(iii)感知损失，通过使用在卷积发生之后(即在激活之前)立即计算的特征来减少特征的稀疏性并更好地监控亮度一致性和纹理恢复。 “网络插值”也使用户能够平衡感知质量和PSNR之间的折衷。</p><p id="1f3f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">与包括SRCNN [1，2]，EDSR [8]，RCAN [10]和SRGAN [7]在内的其他方法相比，结果表明，ESRGAN产生的图像边缘更清晰，纹理更好，同时减少了不希望的伪影。ESRGAN还赢得了PIRM2018-SR挑战赛[12]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nz"><img src="../Images/8d9dc13dcc5ddffe99bee210a57b0bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PwfSBQppr8pQWEb6"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">在ESRGAN中，移除了SRGAN中存在的批量标准化层，同时使用了新的RRDB模块。还采用了剩余比例参数<em class="nv">β</em>[11]。</p></figure><p id="95a7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><a class="ae kr" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank"> Transformers </a>，最初提出用于自然语言处理(NLP)领域，是深度学习模型，它使用一种注意机制来决定序列中的哪些部分最重要。它们已经成为一个热门的研究课题，并已被应用于计算机视觉任务，视觉变压器(ViT)被认为是这一领域的开创性工作[13]。最近，在[14]中提出了基于变换器的方法的应用，命名为高效超分辨率变换器(ESRT)。</p><p id="6aed" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">轻量级Transformer Backbone (LTB)的任务是捕获图像中跨局部区域的长期空间依赖性。如果图像在不同位置包含相似的补丁，这将特别有用。CNN也被用作轻量级CNN主干(LCB)的一部分，它可以动态调整特征大小以提取深层特征，同时保持低计算成本。实验表明，ESRT可以在性能和计算复杂度之间取得很好的平衡。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oa"><img src="../Images/9eec303a0159c019fc9a066eff16b817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MJQV2rAwROlx3ed7"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ob"><img src="../Images/3792822b2147864dfe6a4e85c284a527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/0*teOqkbx1NVU_oQwx"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">ESRT的建筑(上)和高保护街区(HPB)(下)。LCB、LTB、HPB和ET分别代表轻量级CNN主干、轻量级变压器主干、高保护块和高效变压器。在HPB架构中，HFM和ARFB代表高频滤波模块和自适应残差特征块[14]。</p></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="1aa0" class="md me iq bd mf mg nn mi mj mk no mm mn jw np jx mp jz nq ka mr kc nr kd mt mu bi translated">盲随机共振方法</h1><p id="b983" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">虽然上面提到的非盲方法为SR的网络开发提供了许多有用的见解，但是它们往往不完全适用于真实世界的场景。这是因为这些方法假定了一个已知的固定退化过程，因此当遇到与它们专门训练的退化不同且更复杂的退化时，往往会失败，这在现实世界中是常见的。此外，影响图像的退化类型通常是未知的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oc"><img src="../Images/a97ba6024e8999fb4ae0f10856e72e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JBZLgiflUtKi-OioHqvJUg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">非盲SR方法的失败案例，在模糊输入的情况下无法锐化纹理，在有噪声输入的情况下保持噪声[15]</p></figure><p id="8494" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">盲SR方法试图通过减少所做的假设来对这个问题更加鲁棒，即使大多数方法仍然对输入降级做一些假设。刘等人[15]的一项调查将开创性的SRCNN的第一作者(C. Dong)列为其作者之一，该调查提出了一种基于所用数据类型及其建模方式的分类法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi od"><img src="../Images/d335eb9c2df3a9faa2e0f9190ab5fb1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QyNVYS3u7ukxYejm"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">盲SR方法的分类和一些相应的代表性方法，如[15]中所述，其中也揭示了研究缺口。</p></figure><p id="db32" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一组方法通过除了LR图像之外还允许输入表示估计退化的其他特征来执行SR。因此，这些方法的主要任务是如何最好地利用这些附加信息。</p><p id="f1b2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">用于多重降级的12层超分辨率网络(SRMD) [16]就是这样一种方法，它考虑了高斯模糊、噪声和下采样。使用“维度拉伸”方法，其中使用<a class="ae kr" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>将模糊内核矢量化并投影到更小的维度，然后将其与降级图像的噪声级别连接。然后复制该向量中的每个维度(元素)以获得其宽度和高度与输入LR图像的宽度和高度相同的矩阵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oe"><img src="../Images/47cc0609528b0a312ce78302bb975485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Sjb15pWcZ4a6s5xH"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">SRMD建筑[16]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/8e255a3367e10dadd896d3cc3c4b0a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/0*qNRp7fBnekIb59Ij"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">SRMD提出的维度拉伸方法[16]</p></figure><p id="3d37" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">还提出了扩展现有非盲方法以利用退化信息的技术[17，18]。这种方法的缺点是，它们需要精确的退化信息(这不是一项简单的任务)，因为估计输入的任何偏差都会导致内核不匹配，从而对性能有害。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi og"><img src="../Images/ca068eeb69dbfb1c34bc04304d29bb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QWPVK97QSJo_3Dk1"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">[18]中提出的“元注意力”块，以及在典型的单幅图像超分辨率网络中的放置</p></figure><p id="19ab" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了缓解这些问题，已经提出了在SR过程中估计退化核的方法，例如迭代核校正(IKC) [19]。这种方法利用了任何核不匹配都可能导致规则模式的观察结果，实现了对核的估计，并使用校正器网络以迭代方式对其进行校正，以逐步改善超分辨率图像。结合非盲网络(也在[19]中提出)，空间特征变换多重降级(SFTMD)网络，据报道实现了优于诸如SRMD的方法的性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oh"><img src="../Images/e11a828f5a09bda15a28e4a8e5105abc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R95jlS3z3xqslSPk"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">SFTMD(上图)、预测器和校正器网络(下图)的架构[19]</p></figure><p id="50e4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">深度交替网络(DAN) [20](也称为DANv1)及其更新版本DANv2 [21]通过在单个端到端可训练网络内统一SR和内核校正器网络来构建IKC，这可以使两个网络更有效地相互操作。为了提高核估计的鲁棒性，校正器也被修改为使用以中间SR结果为条件的LR输入，而不是像在IKC中执行的那样，以估计的核为条件来调节超分辨率图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oi"><img src="../Images/e342407f86e657cbc73133132e0cefaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YWZ11643bpp732nd"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">丹建筑[20]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oj"><img src="../Images/d025b94826455502a36552e2e7a2d355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0A11Mdv0af9V_uSAU35r6w.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">DANv2 [21]中的(a)双路径条件块(DPCB)、(b)双路径条件组(DPCG)、(c)恢复器和(d)估计器的架构。‘GAP’表示全局平均池，f_basic表示基本输入，f_cond表示条件输入。</p></figure><p id="d1cd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">面向内核的自适应局部调整网络(KOALAnet) [22]考虑图像内的空间变化特征，并因此尝试执行局部自适应。这使得能够区分由于不期望的效果引起的模糊和出于美学目的故意引起的模糊(例如<a class="ae kr" href="https://en.wikipedia.org/wiki/Bokeh" rel="noopener ugc nofollow" target="_blank">散景</a>效果)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ok"><img src="../Images/2f726a60e5d2cba49381124a7800c488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ChGpwXCmCyRoFVSp"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">KOALAnet架构[22]</p></figure><p id="2bce" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如果LR图像包含与训练模型时考虑的退化不同的退化，则上述方法可能仍然表现出较差的性能，假设它们依赖于核估计。因此，另一组方法，如KernelGAN [23]和“零炮”SR (ZSSR) [24]利用自然图像的内部统计和观察，一些模式在尺度内(即在图像内的不同位置)和跨尺度(即本质上不同的大小)重复。这使得模型可以自我监督，一次适应一幅图像。</p><p id="6333" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，有争议的是，在尺度内和跨尺度具有重复出现的斑块的假设可能并不适用于所有图像(例如，那些包含各种各样的内容或者相反地具有大部分同质区域的图像)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ol"><img src="../Images/2532fb23b5882bc9ffa61a2ae4625a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PmxCuxqqlDF4J-Eb"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">KernelGAN建筑[23]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi om"><img src="../Images/654bf46f605947ed8ed7eb20a987becc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7-uY_yHV0y6jyVtI"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">在外部数据集上训练的网络(a)和ZSSR方法(b)之间的差异，在后者中，网络在图像上训练，以自我超分辨[24]</p></figure><p id="f1ab" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">另一组方法试图隐式地对潜在的退化模型进行建模，以便对真实世界的LR图像更加鲁棒，其中HR图像不可用并且因此是未知的。循环中循环GAN (CinCGAN) [25]就是这样一种方法，它由两个循环GAN [26]网络组成。第一个网络试图对输入图像去噪(以产生“干净的LR”图像)，而第二个网络则学习从LR空间到HR空间的映射，以及基于EDSR的SR网络[8]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi on"><img src="../Images/7c3ff7157deeee59c95d55d0f3087270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FD7M87Sq1IM-U8qJ"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">CinCGAN架构，其中G1-G3是生成器，D1-D2是鉴别器，SR是超分辨率网络[25]</p></figure><p id="e78f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">另一种方法Real-ESRGAN [27]是ESRGAN [11]的扩展，它使用“高阶”退化过程生成合成图像，其中退化模型实际上应用了两次。这种方法的主要缺点是需要大量的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oo"><img src="../Images/0291a7aea9006d41a3d172af8be0d174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Td7tdAs02_X52Q2N"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">Real-ESRGAN架构[27]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi op"><img src="../Images/eaa4eed549fa73ecce5192ccd3a0b113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AisTDAGKjZa0x73B"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">用于Real-ESRGAN的合成数据生成管道[27]</p></figure><p id="19fd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">虽然退化GAN [17]等方法试图通过学习HR到LR的退化过程来解决这一问题，从而允许在SR模型的训练过程中生成和使用真实的LR样本，但大多数为隐式退化建模而设计的模型倾向于使用不易训练的GAN，并可能引入对取证或旧照片恢复等应用有害的假纹理或伪影。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oq"><img src="../Images/dd7ec02faf35c0e8e813aeccf4653990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vVE2XI99w0TmLr5I"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">退化GAN的架构和训练管道[17]</p></figure><p id="5663" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一些方法还考虑了多种建模模式和数据源，例如混合专家(MoESR) [28]，其中不同的退化内核分别由特定的SR网络(称为“专家”)处理。然后，最好的专家被用于内核预测，而图像的内部统计数据则被用于执行微调。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ol"><img src="../Images/9f0db3386099db60f1fd1a225b980142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*N2d_-uEs8xzObzFh"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">教育和科学研究部各种网络的培训计划[28]</p></figure><p id="0491" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最近重新兴起的一股研究潮流是<em class="lo">对比学习</em>，它也被应用于诸如降解感知SR (DASR)网络【29】和为遥感图像设计的方法【30】等方法中的SR。对比学习是自我监督学习的一种形式，其中从表现出与“锚”表示相似的特征的图像样本(通常称为“正”样本)获得的特征表示被拉近，而从具有与锚不同的特征的样本(通常称为“负”样本)获得的表示被推开。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi or"><img src="../Images/82ef23facb590a8014ea3bf039783cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wzd1wGFN5up1dGO-"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">DASR建筑(下)[29]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4fbf9980d24c06c1d38a96db473e5b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/0*NwK5HmBjUc18vecI"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">DASR使用的无监督退化表示学习方案的示例[29]</p></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="5890" class="md me iq bd mf mg nn mi mj mk no mm mn jw np jx mp jz nq ka mr kc nr kd mt mu bi translated">损失函数和评估指标</h1><p id="51a2" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">上述所有方法都使用机器学习，其中需要一些方法来确定学习的参数是否朝着正确的方向移动。然后进行任何调整以进一步优化这些参数，并进而产生(希望)更令人满意的结果。这是使用所谓的<em class="lo">损失函数</em>来执行的，该函数测量网络在训练阶段输出的结果的质量。</p><p id="7e46" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最常用的损失函数直接比较LR图像和目标(HR)图像，例如L2损失，其形成了<a class="ae kr" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差(MSE) </a>和密切相关的<a class="ae kr" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="noopener ugc nofollow" target="_blank">峰值信噪比(PSNR) </a>的基础。这些所谓的全参考图像质量评估(FR-IQA)度量本质上是在超分辨率的情况下测量HR和LR图像的对应像素之间的差异。训练函数的任务是理想地最小化MSE到零，相反地尽可能最大化PSNR。</p><p id="d82c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，这些指标长期以来一直被批评为与主观质量感觉不太相关，因为它们对可能的解决方案进行像素平均，从而导致模糊的结果。因此，与看起来质量较低的图像相比，在感知上看起来更令人愉悦的图像可能产生较低的PSNR值。此外，即使在垂直或水平方向上移动图像一个像素也会导致MSE变大(并且PSNR非常低)，即使要评估的图片与原始图像相同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ot"><img src="../Images/c89b9b0185941caf2dda7c23645cfb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9h1j35pr6mhq0S3KrsWigA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">MSE / PSNR的问题:可能的解决方案被平均，导致模糊的结果；换句话说，由于MSE [31]固有的平均特性，模糊性受到鼓励</p></figure><p id="42b6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">上述问题引发了对<em class="lo">感知指标</em>的使用，该指标旨在衡量与人类感知质量相关的图像整体质量。结构相似性指数(SSIM) [32]旨在解决这一问题，在SR方法的开发和评估中非常常用。</p><p id="c9d8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，SSIM仍然将被评估的图像与原始HR图像进行比较，因此仍然对一些因素敏感。此外，自从SSIM创立以来，已经开发了许多方法，这些方法被证明与人类对图像质量的感知更好地相关。</p><p id="59eb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">那么为什么不使用这些相关性更好的方法呢？嗯，MSE、PSNR和SSIM是简单的指标，计算速度非常快(当你需要评估数千甚至数百万张图像时很方便)，并且在许多常见的编程库中实现，使它们非常方便易用。此外，它们是可微分的，这是使深度学习方法能够更新其参数所必需的属性。</p><p id="18a3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">还设计了不需要原始HR图像的感知度量，称为无参考IQA (NR-IQA)度量。例子包括BRISQUE [33]和NIQE [34]，但这些往往主要用于评估，因为它们是不可微的。</p><p id="e236" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">一种在增强感知愉悦图像方面非常流行的方法是从其他网络中提取和使用中间特征，以产生一种类型的<em class="lo">感知损失</em>。</p><p id="9702" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">例如，在人脸SR的情况下，为人脸识别而设计和训练的诸如VGG-Face [35]的网络可以用于强制在处理HR图像时获得的中间特征和在使用超分辨率图像时获得的特征彼此相似。这样，损失是在特征级而不是图像级计算的。</p><p id="51a8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">此外，假设为面部训练的网络提取与辨别面部最相关的显著特征，那么特征相似性的实施也将面部的身份和特征实施为相似的。这有助于防止网络“幻觉”原始图像中可能不存在的新细节，从而防止身份的改变，这种改变在一些应用中是有害的，如前一篇文章中的<a class="ae kr" rel="noopener" target="_blank" href="/what-is-image-super-resolution-and-why-do-we-need-it-9c3bd9dc233e">所述。</a></p><p id="c0db" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">感知损失函数可以用于几乎任何SR应用，而不仅仅是人脸。事实上，感知损失最初应用于普通图像内容的超分辨率[36]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ou"><img src="../Images/881af31d63bb111d02417581d3ec34d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSgPxqGzylm7QZGPlPuUpg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">双三次插值、使用基于像素的损失的超分辨率、SRCNN [1，2]和使用特征重建损失(一种感知损失函数)的超分辨率之间的比较。每个图像还显示了PSNR / SSIM。从[36]获取的图像。</p></figure><p id="d99b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在上面的图像中，可以注意到使用感知损失获得的方法的PSNR值比使用每像素损失获得的结果低(差)，而SSIM值仅略高。此外，PSNR和SSIM都低于不仅通过SRCNN超分辨的图像获得的值，而且还低于通过基本双三次插值上采样的图像获得的值。</p><p id="6c2d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，可以认为，与使用基于像素的损失和SRCNN导出的图像相比，基于感知损失的结果实际上更清晰且更接近地面真实图像。[36]的作者指出，与基线方法相比，放大后可见的轻微交叉图案会损害PSNR和SSIM值。这突出了使用与主观质量感觉更好相关的IQA度量的需要。</p><p id="9862" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">尽管PSNR等度量标准存在缺点，但它们仍然有助于确保超分辨率图像的内容与原始图像保持相似。换句话说，如果不进行逐像素的比较，如果图像只是好看，网络可以彻底改变图像内容而不会受到惩罚。事实上，这是GANs的缺点之一(在某些应用中也是一个优点),它往往以合成原始图像中可能不存在的纹理和内容为代价来产生好看的图像。</p><p id="3d3b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如上所述，这在一些应用中可能是有害的，例如在人脸识别中，如果身份被改变，错误的人可能被定罪，而实际的犯罪者可能无法被识别。例如，在由<a class="ae kr" href="https://basurafernando.github.io/papers/XinYuCVPR18.pdf" rel="noopener ugc nofollow" target="_blank">于<em class="lo">等人</em> (2018) </a>提出的工作的图5中，所提出的方法的超分辨率结果看起来相当好，特别是考虑到要超分辨率的低分辨率图像的大小仅为16 × 16像素。然而，有人可能会说，一些人的身份与原始图像中的身份不同。</p><p id="a208" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">针对一种损失函数类型调整网络往往会降低另一种损失函数类型的性能，反之亦然。例如，SRResNet在PSNR方面优于SRGAN，但在感知质量方面不如SRGAN。因此，通常需要在这两种度量之间进行权衡，以确保图像内容与原始内容相似，同时使结果在感觉上令人满意。显然，这取决于目标应用程序，因此可能需要给予一种类型的度量比其他类型的度量更大的重要性。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="4183" class="md me iq bd mf mg nn mi mj mk no mm mn jw np jx mp jz nq ka mr kc nr kd mt mu bi translated">结论和未来方向</h1><p id="ede2" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">可以看出，在软件无线电领域已经做了许多工作，但仍然存在许多需要克服的挑战。其中一个主要问题是SR方法的开发，该方法真正能够在盲设置中对真实世界的图像进行操作，在盲设置中没有关于可能影响图像的退化的信息被假定。</p><p id="04dd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">此外，以上讨论集中在通用图像内容上；有一些方法是为特定应用设计的，例如人脸SR。这些方法也是一次对一幅图像进行操作，因此被称为单幅图像SR (SISR)方法；虽然也做了工作来使用多幅图像(例如视频和多光谱卫星图像)提取更多信息，从而提高性能，但与SISR相比，这方面的进展更为有限。</p><p id="458d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">总之，未来的研究有很多途径，这至少保证了一件事:软件无线电的未来是光明的(即使有时事情看起来模糊和扭曲😉)!</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><p id="8430" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="lo">你对这篇文章有什么想法吗？欢迎直接在</em> <a class="ae kr" href="https://www.linkedin.com/in/cgalea" rel="noopener ugc nofollow" target="_blank"> <em class="lo"> LinkedIn </em> </a>上留言、评论或给我发消息！</p><p id="0594" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="lo">此外，确保</em> <a class="ae kr" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9be78db0c9b&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-super-resolution-an-overview-of-the-current-state-of-research-94294a77ed5a&amp;user=Christian+Galea&amp;userId=a9be78db0c9b&amp;source=post_page-a9be78db0c9b--three_column_layout_sidebar-----------------------follow_profile-----------" rel="noopener"> <strong class="ku ir"> <em class="lo">关注</em> </strong> </a> <em class="lo"> me，以确保您在未来文章发表时得到通知。</em></p><p id="9577" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="lo">作者是马耳他大学</em> <a class="ae kr" href="https://www.um.edu.mt/" rel="noopener ugc nofollow" target="_blank"> <em class="lo">博士后研究员</em> </a> <em class="lo">在</em> <a class="ae kr" href="https://www.um.edu.mt/projects/deep-fir/" rel="noopener ugc nofollow" target="_blank"> <em class="lo">深松</em> </a> <em class="lo">项目，该项目是与</em> <a class="ae kr" href="https://www.ascent.io" rel="noopener ugc nofollow" target="_blank"> <em class="lo"> Ascent软件</em> </a> <em class="lo">合作完成的，由马耳他科学委员会&amp;技术(MCST)资助，并代表科学基金会&amp;技术，通过</em></p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="5416" class="md me iq bd mf mg nn mi mj mk no mm mn jw np jx mp jz nq ka mr kc nr kd mt mu bi translated">参考</h1><p id="b753" class="pw-post-body-paragraph ks kt iq ku b kv mv jr kx ky mw ju la lb mx ld le lf my lh li lj mz ll lm ln ij bi translated">[1] C. Dong、C. C. Loy、K. He和X. Tang，<a class="ae kr" href="https://doi.org/10.1007/978-3-319-10593-2_13" rel="noopener ugc nofollow" target="_blank">学习用于图像超分辨率的深度卷积网络</a> (2014)，欧洲计算机视觉会议(ECCV，2014)</p><p id="bc62" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[2]董，卢，何，唐，<a class="ae kr" href="https://arxiv.org/abs/1501.00092" rel="noopener ugc nofollow" target="_blank">基于深度卷积网络的图像超分辨率处理</a> (2016)，IEEE模式分析与机器智能汇刊</p><p id="3fba" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[3] R .蒂莫夫特、V. D .斯梅特和L. V .古尔，<a class="ae kr" href="https://doi.org/10.1007/978-3-319-16817-3_8" rel="noopener ugc nofollow" target="_blank"> A+:用于快速超分辨率的调整锚定邻域回归</a> (2014)，亚洲计算机视觉会议(ACCV，2014)</p><p id="5512" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[4] J. Yang，J. Wright，T. S. Huang，和Y. Ma，<a class="ae kr" href="https://doi.org/10.1109/TIP.2010.2050625" rel="noopener ugc nofollow" target="_blank">基于稀疏表示的图像超分辨率技术</a> (2010)，IEEE图像处理汇刊</p><p id="eb00" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[5]何国光，张，任，孙，<a class="ae kr" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">深度残差学习用于图像识别</a> (2016)，IEEE计算机视觉与模式识别会议(2016)</p><p id="ebe2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[6]何国光，张，任，孙，<a class="ae kr" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">深度剩余网络中的身份映射</a> (2016)，欧洲计算机视觉会议(ECCV，2016)</p><p id="62ff" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[7] C. Ledig，L. Theis，F. Huszr，J. Caballero，A. Cunningham，A. Acosta，A. Aitken，A. Tejani，J. Totz，Z. Wang和W. Shi，<a class="ae kr" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">使用生成式对抗网络的照片真实感单幅图像超分辨率</a> (2017)，IEEE计算机视觉和模式识别会议(CVPR 2017)</p><p id="0fe2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[8] B. Lim、S. Son、H. Kim、S. Nah和K. M. Lee，<a class="ae kr" href="https://arxiv.org/abs/1707.02921" rel="noopener ugc nofollow" target="_blank"/>(2017)，IEEE计算机视觉和模式识别研讨会会议(CVPRW 2017)</p><p id="fdfd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[9] E. Agustsson和r .蒂莫夫特，<a class="ae kr" href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.html" rel="noopener ugc nofollow" target="_blank">2017年单幅图像超分辨率挑战:数据集和研究</a> (2017)，IEEE计算机视觉和模式识别研讨会会议(CVPRW 2017)</p><p id="68f0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[10] Y. Zhang，K. Li，K. Li，L. Wang，B. Zhong，和Y. Fu，<a class="ae kr" href="https://arxiv.org/abs/1807.02758" rel="noopener ugc nofollow" target="_blank">使用极深残差通道注意网络的图像超分辨率</a> (2018)，欧洲计算机视觉会议(ECCV，2018)</p><p id="95ce" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[11] X. Wang，K. Yu，S. Wu，J. Gu，Y. Liu，C. Dong，Y. Qiao，C. C. Loy，<a class="ae kr" href="https://arxiv.org/abs/1809.00219" rel="noopener ugc nofollow" target="_blank"> ESRGAN:增强型超分辨率生成对抗网络</a> (2018)，欧洲计算机视觉会议(ECCV，2018)</p><p id="afdd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[12] Y. Blau，R. Mechrez，r .蒂莫夫特，T. Michaeli和L. Zelnik-Manor，<a class="ae kr" href="https://arxiv.org/abs/1809.07517" rel="noopener ugc nofollow" target="_blank">2018年感知图像超分辨率PIRM挑战赛</a> (2018)，arXiv</p><p id="a741" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[13] A. Dosovitskiy，L. Beyer，a .科列斯尼科夫，D. Weissenborn，X. Zhai，T. Unterthiner，M. Dehghani，M. Minderer，G. Heigold，S. Gelly，J. Uszkoreit和N. Houlsby，<a class="ae kr" href="https://iclr.cc/virtual/2021/poster/3013" rel="noopener ugc nofollow" target="_blank">一幅图像相当于16x16个词:大规模图像识别的变形金刚</a> (2021)，国际学习表征会议(ICLR 2021)</p><p id="b407" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[14] <a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu%2C+Z" rel="noopener ugc nofollow" target="_blank"> Z. Lu </a>，<a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J" rel="noopener ugc nofollow" target="_blank"> J. Li </a>，<a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+H" rel="noopener ugc nofollow" target="_blank"> H .刘</a>，<a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+C" rel="noopener ugc nofollow" target="_blank"> C .黄</a>，<a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+L" rel="noopener ugc nofollow" target="_blank"> L .张</a>，<a class="ae kr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng%2C+T" rel="noopener ugc nofollow" target="_blank"> T .曾</a>，<a class="ae kr" href="https://arxiv.org/abs/2108.11084" rel="noopener ugc nofollow" target="_blank">单幅图像超分辨率变换</a> (2022)，IEEE计算机视觉与模式识别研讨会(CVPRW 2022)</p><p id="41f5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[15] A. Liu，Y. Liu，J. Gu，Y. Qiao，C. Dong，<a class="ae kr" href="https://arxiv.org/abs/2107.03055" rel="noopener ugc nofollow" target="_blank">盲图像超分辨率:综述与超越</a>，(2022)，IEEE模式分析与机器智能汇刊</p><p id="c460" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[16] K. Zhang，W. Zuo，和L. Zhang，<a class="ae kr" href="https://arxiv.org/abs/1712.06116" rel="noopener ugc nofollow" target="_blank">学习单个卷积超分辨率网络的多重退化</a> (2018)，IEEE/CVF计算机视觉和模式识别会议(CVPR，2018)</p><p id="6fe0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[17] A. Bulat，J. Yang，G. Tzimiropoulos，<a class="ae kr" href="https://arxiv.org/abs/1807.11458" rel="noopener ugc nofollow" target="_blank">学习图像超分辨率，先用GAN学习如何做图像退化</a> (2018)，欧洲计算机视觉会议(ECCV 2018)</p><p id="3dfc" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[18] M. Aquilina，C. Galea，J. Abela，K. P. Camilleri和R. A .法鲁吉亚，<a class="ae kr" href="https://arxiv.org/abs/2110.14638" rel="noopener ugc nofollow" target="_blank">使用元注意层提高超分辨率性能</a> (2021)，IEEE信号处理快报</p><p id="2e2c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[19]顾，陆，左，董，<a class="ae kr" href="https://arxiv.org/abs/1904.03377" rel="noopener ugc nofollow" target="_blank">迭代核校正的盲超分辨率算法</a> (2019)，IEEE/CVF计算机视觉与模式识别会议(CVPR，2019)</p><p id="a15f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[20]罗，黄，李，王，谭，<a class="ae kr" href="https://arxiv.org/abs/2010.02631" rel="noopener ugc nofollow" target="_blank"/>(2020)，神经信息处理系统进展</p><p id="679f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[21]罗，黄，李，王，谭，<a class="ae kr" href="https://arxiv.org/abs/2105.06878" rel="noopener ugc nofollow" target="_blank">盲超分辨率的端到端交替优化</a> (2021)，arXiv</p><p id="f688" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[22] S. Y. Kim，H. Sim，M. Kim，<a class="ae kr" href="https://arxiv.org/abs/2012.08103" rel="noopener ugc nofollow" target="_blank"> KOALAnet:使用面向内核的自适应局部调整的盲超分辨率</a> (2021)，IEEE/CVF计算机视觉和模式识别会议(CVPR 2021)</p><p id="3ec4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[23] S. Bell-Kligler、A. Shocher和m .伊拉尼，<a class="ae kr" href="https://arxiv.org/abs/1909.06581" rel="noopener ugc nofollow" target="_blank">使用内部g an的盲超分辨率核估计</a> (2019)，神经信息处理系统进展(NeurIPS)</p><p id="527b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[24] A. Shocher，N. Cohen和m .伊拉尼，<a class="ae kr" href="https://arxiv.org/abs/1712.06087" rel="noopener ugc nofollow" target="_blank">“零射击”使用深度内部学习的超分辨率</a> (2018)，IEEE/CVF计算机视觉和模式识别会议(CVPR 2018)</p><p id="7df1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[25]Yuan，S. Liu，J. Zhang，Y. Zhang，C. Dong和L. Lin，<a class="ae kr" href="https://arxiv.org/abs/1809.00437" rel="noopener ugc nofollow" target="_blank">使用循环生成对抗网络的无监督图像超分辨率</a> (2018)，IEEE/CVF计算机视觉和模式识别研讨会(CVPRW 2018)</p><p id="7118" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[26] J. Zhu，T. Park，P. Isola和A. A. Efros，<a class="ae kr" href="https://arxiv.org/abs/1703.10593" rel="noopener ugc nofollow" target="_blank">使用循环一致对抗网络的不成对图像到图像翻译</a> (2017)，IEEE计算机视觉国际会议(ICCV，2017)</p><p id="7322" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[27] X. Wang，L. Xie，C. Dong，和Y. Shan，<a class="ae kr" href="https://arxiv.org/abs/2107.10833" rel="noopener ugc nofollow" target="_blank"> Real-ESRGAN:用纯合成数据训练真实世界盲超分辨率</a> (2021)，国际计算机视觉研讨会会议(ICCVW 2021)</p><p id="5459" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[28] M .艾玛德、m .皮曼和H. Corporaal，<a class="ae kr" href="https://openaccess.thecvf.com/content/WACV2022/html/Emad_MoESR_Blind_Super-Resolution_Using_Kernel-Aware_Mixture_of_Experts_WACV_2022_paper.html" rel="noopener ugc nofollow" target="_blank"> MoESR:使用核感知专家混合的盲超分辨率</a> (2022)，IEEE/CVF计算机视觉应用冬季会议(WACV 2022)</p><p id="6891" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[29] L. Wang，Y. Wang，X. Dong，Q. Xu，J. Yang，W. An，和Y. Guo，<a class="ae kr" href="https://arxiv.org/abs/2104.00416" rel="noopener ugc nofollow" target="_blank">用于盲超分辨率的无监督退化表示学习</a> (2021)，IEEE/CVF计算机视觉和模式识别会议(CVPR 2021)</p><p id="083f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[30] G. Yin，W. Wang，z . Yu，W. Ji，D. Yu，S. Sun，t-s . Chua，和C. Wang，<a class="ae kr" href="https://arxiv.org/abs/2104.03926" rel="noopener ugc nofollow" target="_blank">用于多降级的盲超分辨率的条件超网络</a> (2022)，IEEE图像处理汇刊</p><p id="e83c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[31] M. S. M. Sajjadi、B. Schlkopf和M. Hirsch，<a class="ae kr" href="https://arxiv.org/abs/1612.07919" rel="noopener ugc nofollow" target="_blank"> EnhanceNet:通过自动纹理合成实现单幅图像超分辨率</a> (2017)，IEEE计算机视觉国际会议(ICCV，2017)</p><p id="841a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[32]纣王，A. C .博维克，H. R .谢赫和E. P .西蒙切利，<a class="ae kr" href="https://www.cns.nyu.edu/pub/lcv/wang03-preprint.pdf" rel="noopener ugc nofollow" target="_blank">图像质量评估:从错误可见性到结构相似性</a> (2004)，IEEE图像处理汇刊</p><p id="88f8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[33] A. Mittal，A. K. Moorthy和A. C. Bovik，<a class="ae kr" href="https://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf" rel="noopener ugc nofollow" target="_blank">空间域中的无参考图像质量评估</a> (2012)，IEEE图像处理汇刊</p><p id="b358" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[34] A. Mittal、R. Soundararajan和A. C. Bovik，<a class="ae kr" href="http://live.ece.utexas.edu/research/quality/niqe_spl.pdf" rel="noopener ugc nofollow" target="_blank">制作“完全盲”图像质量分析仪</a> (2013)，IEEE信号处理快报</p><p id="8a3c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[35] O. M. Parkhi，A. Vedaldi和A. Zisserman，<a class="ae kr" href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/" rel="noopener ugc nofollow" target="_blank">深度人脸识别</a> (2015)，英国机器视觉大会(BMVC 2015)</p><p id="ab42" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">[36] J. Johnson，A. Alahi，l .飞飞，<a class="ae kr" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a> (2016)，欧洲计算机视觉会议(ECCV，2016)</p></div></div>    
</body>
</html>