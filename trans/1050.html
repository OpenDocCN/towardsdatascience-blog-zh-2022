<html>
<head>
<title>Sentiment Analysis on News Headlines: Classic Supervised Learning vs Deep Learning Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新闻标题的情感分析:经典监督学习与深度学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-on-news-headlines-classic-supervised-learning-vs-deep-learning-approach-831ac698e276#2022-03-19">https://towardsdatascience.com/sentiment-analysis-on-news-headlines-classic-supervised-learning-vs-deep-learning-approach-831ac698e276#2022-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="19c5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用经典机器学习和深度学习技术开发二元分类器以检测正面和负面新闻标题的解释性指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7e0624168a0e40bfb3d5b675d47eb6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XAEqbL2TWPrd3SPizI6Ww.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">图片取自</strong> <a class="ae kw" href="https://unsplash.com/photos/Mwuod2cm8g4" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv"> Unsplash </strong> </a></p></figure><h1 id="e9f7" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">介绍</h1><blockquote class="lp lq lr"><p id="8c9a" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">本文将使您能够构建一个二元分类器，使用两种不同的方法对<strong class="lv ir">未标记的数据</strong>执行情感分析:</p><p id="f2e5" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">1-通过<strong class="lv ir"> scikit-learn 库和 nlp 包进行监督学习</strong></p><p id="8b8a" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">2-使用<strong class="lv ir"> TensorFlow 和 Keras 框架的深度学习</strong></p></blockquote><p id="d024" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">然而，挑战在于我们正在处理未标记的数据，因此我们将利用一种<strong class="lv ir"> <em class="lu">弱监督技术</em> </strong>即<strong class="lv ir"> <em class="lu">通气管</em> </strong>为我们的训练数据点创建标记 0(负)或 1(正)。</p><p id="d861" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">项目的视觉计划如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a702c6fdaa5fa64364a069abdef385cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*LSczfBZLWtO0-j74o5_UQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">项目概述</strong></p></figure><p id="4c3c" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">该项目中使用的数据集被称为“百万新闻标题”数据集，可在<a class="ae kw" href="https://www.kaggle.com/therohk/million-headlines" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上获得。我也将使用谷歌 Colab，随时设置您的笔记本，复制和运行自己的代码。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="0d71" class="my ky iq mu b gy mz na l nb nc">#install needed packages<br/>!pip install snorkel<br/>!pip install textblob<br/>#import libraries and modules<br/>from google.colab import files<br/>import io<br/>import pandas as pd<br/>#Snorkel<br/>from snorkel.labeling import LabelingFunction<br/>import re<br/>from snorkel.preprocess import preprocessor<br/>from textblob import TextBlob<br/>from snorkel.labeling import PandasLFApplier<br/>from snorkel.labeling.model import LabelModel<br/>from snorkel.labeling import LFAnalysis<br/>from snorkel.labeling import filter_unlabeled_dataframe<br/>from snorkel.labeling import labeling_function<br/>#NLP packages<br/>import spacy<br/>from nltk.corpus import stopwords<br/>import string<br/>import nltk<br/>import nltk.tokenize<br/>punc = string.punctuation<br/>nltk.download('stopwords')<br/>stop_words = set(stopwords.words('english'))<br/>#Supervised learning<br/>from tqdm import tqdm_notebook as tqdm<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import classification_report<br/>##Deep learning libraries and APIs<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><h1 id="a6ce" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">1.加载数据</h1><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="9de5" class="my ky iq mu b gy mz na l nb nc">#uplaod the data from your local directory<br/>uploaded = files.upload()</span><span id="bb76" class="my ky iq mu b gy nd na l nb nc"># store the dataset as a Pandas Dataframe<br/>df = pd.read_csv(io.BytesIO(uploaded['data.csv']))</span><span id="846f" class="my ky iq mu b gy nd na l nb nc">#conduct some data cleaning<br/>df = df.drop(['publish_date', 'Unnamed: 2'], axis=1)<br/>df = df.rename(columns = {'headline_text': 'text'})<br/>df['text'] = df['text'].astype(str)</span><span id="663f" class="my ky iq mu b gy nd na l nb nc">#check the data info<br/>df.info()</span></pre><p id="d58f" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">在这里，我们可以检查我们的数据集有 63821 个实例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e64c43c3d881ebe8e2e50afe3d66df3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*4T1TWMZ6Elf1QGlsV61kRQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">数据集信息</strong></p></figure><h1 id="d5f7" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">2.创建标签:浮潜技术</h1><blockquote class="lp lq lr"><p id="38f5" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">因为数据集是无标签的，所以我们将使用 sluck，使用分配两类标签的函数来提出启发式和编程规则，这两类标签区分标题是正面的(1)还是负面的(0)。有关通气管标签的更多详情，请访问此<a class="ae kw" href="https://www.snorkel.org/use-cases/01-spam-tutorial#3-writing-more-labeling-functions" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></blockquote><p id="d3f7" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">下面，您可以找到生成的标签函数。第一个函数查看标题中的输入单词，第二个函数根据正面和负面列表中的预定义单词分配适当的标签。例如，如果“有前途”一词出现在标题中，那么它将被分配一个积极的标签，而“咄咄逼人”一词则导致一个消极的标签。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="b44d" class="my ky iq mu b gy mz na l nb nc">#define constants to represent the class labels :positive, negative, and abstain<br/>POSITIVE = 1<br/>NEGATIVE = 0<br/>ABSTAIN = -1<br/>#define function which looks into the input words to represent a proper label<br/>def keyword_lookup(x, keywords, label):  <br/>    if any(word in x.text.lower() for word in keywords):<br/>        return label<br/>    return ABSTAIN<br/>#define function which assigns a correct label<br/>def make_keyword_lf(keywords, label=POSITIVE):<br/>    return LabelingFunction(<br/>        name=f"keyword_{keywords[0]}",<br/>        f=keyword_lookup,<br/>        resources=dict(keywords=keywords, label=label))<br/>#resource: <a class="ae kw" href="https://www.snorkel.org/use-cases/01-spam-tutorial#3-writing-more-labeling-functions" rel="noopener ugc nofollow" target="_blank">https://www.snorkel.org/use-cases/01-spam-tutorial#3-writing-more-labeling-functions</a></span><span id="c828" class="my ky iq mu b gy nd na l nb nc">#these two lists can be further extended <br/>"""positive news might contain the following words' """<br/>keyword_positive = make_keyword_lf(keywords=['boosts', 'great', 'develops', 'promising', 'ambitious', 'delighted', 'record', 'win', 'breakthrough', 'recover', 'achievement', 'peace', 'party', 'hope', 'flourish', 'respect', 'partnership', 'champion', 'positive', 'happy', 'bright', 'confident', 'encouraged', 'perfect', 'complete', 'assured' ])</span><span id="373c" class="my ky iq mu b gy nd na l nb nc">"""negative news might contain the following words"""<br/>keyword_negative = make_keyword_lf(keywords=['war','solidiers', 'turmoil', 'injur','trouble', 'aggressive', 'killed', 'coup', 'evasion', 'strike', 'troops', 'dismisses', 'attacks', 'defeat', 'damage', 'dishonest', 'dead', 'fear', 'foul', 'fails', 'hostile', 'cuts', 'accusations', 'victims',  'death', 'unrest', 'fraud', 'dispute', 'destruction', 'battle', 'unhappy', 'bad', 'alarming', 'angry', 'anxious', 'dirty', 'pain', 'poison', 'unfair', 'unhealthy'<br/>                                              ], label=NEGATIVE)</span></pre><p id="6cb4" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">另一组标记功能通过 TextBlob 工具实现，这是一个预训练的情感分析器。我们将创建一个预处理程序，在我们的标题上运行 TextBlob，然后提取极性和主观性得分。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="a594" class="my ky iq mu b gy mz na l nb nc">#set up a preprocessor function to determine polarity &amp; subjectivity using textlob pretrained classifier <br/><a class="ae kw" href="http://twitter.com/preprocessor" rel="noopener ugc nofollow" target="_blank">@preprocessor</a>(memoize=True)<br/>def textblob_sentiment(x):<br/>    scores = TextBlob(x.text)<br/>    x.polarity = scores.sentiment.polarity<br/>    x.subjectivity = scores.sentiment.subjectivity<br/>    return x</span><span id="4019" class="my ky iq mu b gy nd na l nb nc">#find polarity<br/><a class="ae kw" href="http://twitter.com/labeling_function" rel="noopener ugc nofollow" target="_blank">@labeling_function</a>(pre=[textblob_sentiment])<br/>def textblob_polarity(x):<br/>    return POSITIVE if x.polarity &gt; 0.6 else ABSTAIN</span><span id="864e" class="my ky iq mu b gy nd na l nb nc">#find subjectivity <br/><a class="ae kw" href="http://twitter.com/labeling_function" rel="noopener ugc nofollow" target="_blank">@labeling_function</a>(pre=[textblob_sentiment])<br/>def textblob_subjectivity(x):<br/>    return POSITIVE if x.subjectivity &gt;= 0.5 else ABSTAIN</span></pre><p id="07fe" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">下一步是组合所有的标记函数，并将其应用于我们的数据集。然后，我们拟合 label_model 来预测和生成正类和负类。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="c83a" class="my ky iq mu b gy mz na l nb nc">#combine all the labeling functions <br/>lfs = [keyword_positive, keyword_negative, textblob_polarity, textblob_subjectivity ]</span><span id="cb20" class="my ky iq mu b gy nd na l nb nc">#apply the lfs on the dataframe<br/>applier = PandasLFApplier(lfs=lfs)<br/>L_snorkel = applier.apply(df=df)</span><span id="5fd5" class="my ky iq mu b gy nd na l nb nc">#apply the label model<br/>label_model = LabelModel(cardinality=2, verbose=True)<br/>#fit on the data<br/>label_model.fit(L_snorkel)<br/>#predict and create the labels<br/>df["label"] = label_model.predict(L=L_snorkel)</span></pre><p id="5dea" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">我们可以注意到，在去掉未标记的数据点之后(如下所示)，我们有大约 12300 个正面标签和 6900 个负面标签，这将足以构建我们的情感分类器。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="8512" class="my ky iq mu b gy mz na l nb nc">#Filtering out unlabeled data points<br/>df= df.loc[df.label.isin([0,1]), :]</span><span id="f0c4" class="my ky iq mu b gy nd na l nb nc">#find the label counts <br/>df['label'].value_counts()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a1cc3399038475e539273ea582a745e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*F2C5Tg5tZ7wF-KGMJAUutw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">正负标签</strong></p></figure><h1 id="539e" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">3.应用监督学习方法:逻辑回归</h1><blockquote class="lp lq lr"><p id="ac85" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">我们将用来构建情感分类器的第一种方法是经典的监督方法，即<strong class="lv ir">逻辑回归</strong>，它被认为是一种强大的二元分类器，可以估计某个实例属于某个类别的概率，并相应地做出预测。但是，在训练模型之前，我们应该首先预处理数据并创建矢量表示。</p></blockquote><h2 id="b91d" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">3.1 文本预处理</h2><p id="b50a" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">预处理是自然语言处理中为训练准备文本数据的基本任务。它将原始文本的输入转换成作为单个单词或字符的净化标记。主要的预处理方法总结如下:</p><p id="e042" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">1- <strong class="lv ir">标记化</strong>:将单词拆分成标记</p><p id="5b0e" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">2- <strong class="lv ir">词汇化</strong>:将单词分解成它们的根格式</p><p id="a21b" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">3- <strong class="lv ir">去掉停用词:</strong>去掉不必要的词，如 the、he、she 等。</p><p id="2ac9" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">4- <strong class="lv ir">去掉标点符号</strong>:去掉不重要的单词元素，如逗号、句号、括号、圆括号等。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="b42a" class="my ky iq mu b gy mz na l nb nc">#make a copy of the dataframe<br/>data = df.copy()</span><span id="9241" class="my ky iq mu b gy nd na l nb nc">#define a function which handles the text preprocessing <br/>def preparation_text_data(data):<br/>    """<br/>    This pipeline prepares the text data, conducting the following steps:<br/>    1) Tokenization<br/>    2) Lemmatization<br/>    4) Removal of stopwords<br/>    5) Removal of punctuation<br/>    """<br/>    # initialize spacy object<br/>    nlp = spacy.load('en_core_web_sm')<br/>    # select raw text<br/>    raw_text = data.text.values.tolist()<br/>    # tokenize<br/>    tokenized_text = [[nlp(i.lower().strip())] for i in tqdm(raw_text)]<br/>    #define the punctuations and stop words<br/>    punc = string.punctuation <br/>    stop_words = set(stopwords.words('english'))<br/>    #lemmatize, remove stopwords and punctuationd<br/>    corpus = []<br/>    for doc in tqdm(tokenized_text):<br/>        corpus.append([word.lemma_ for word in doc[0] if (word.lemma_ not in stop_words and word.lemma_ not in punc)])<br/>    # add prepared data to df<br/>    data["text"] = corpus<br/>    return data</span><span id="9007" class="my ky iq mu b gy nd na l nb nc">#apply the data preprocessing function<br/>data =  preparation_text_data(data)</span></pre><p id="7e3a" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">我们可以在下图中注意到，数据已经被适当地清理，显示为单独的标记，每个标记都位于其原始词根，没有停用词和标点符号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/b84cc5d3b7f32a21ef91813469d5d37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kOUtpxRj2HehyLhuYpOAwQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">文本准备结果</strong></p></figure><h2 id="87dc" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">3.2 文本表示</h2><p id="61de" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">第二步包括将文本数据转换成 ML 模型可以理解的有意义的向量。我应用了<strong class="lv ir"> TF-IDF(术语频率(TF) —逆密集频率(IDF)) </strong>，它基于整个语料库中的单词出现次数为输入数据创建计数权重。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="91f2" class="my ky iq mu b gy mz na l nb nc">def text_representation(data):<br/>  tfidf_vect = TfidfVectorizer()<br/>  data['text'] = data['text'].apply(lambda text: " ".join(set(text)))<br/>  X_tfidf = tfidf_vect.fit_transform(data['text'])<br/>  print(X_tfidf.shape)<br/>  print(tfidf_vect.get_feature_names())<br/>  X_tfidf = pd.DataFrame(X_tfidf.toarray())<br/>  return X_tfidf</span><span id="eb6a" class="my ky iq mu b gy nd na l nb nc">#apply the TFIDV function<br/>X_tfidf = text_representation(data)</span></pre><p id="04c6" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">下面，我们可以找到 text_representation 函数的结果，由此我们可以看到单词已经被转换为有意义的向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/129691c6f6d78ef0c28fbb3f22682d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFkKaRxshj6rl_jURVj8MQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">文本表示 TFIDV 结果</strong></p></figure><h2 id="2e43" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">3.3 模型培训</h2><p id="1018" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">在这个阶段，我们准备建立和训练 ML 逻辑回归模型。我们将数据集分为训练和测试，我们拟合模型并根据数据点进行预测。我们可以发现该模型的<strong class="lv ir">准确率为 92%。</strong></p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="377a" class="my ky iq mu b gy mz na l nb nc">X= X_tfidf<br/>y = data['label']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span><span id="7fd3" class="my ky iq mu b gy nd na l nb nc">#fit Log Regression Model<br/>clf= LogisticRegression()<br/>clf.fit(X_train,y_train)<br/>clf.score(X_test,y_test)<br/>y_pred = clf.predict(X_test)<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d3c92fa439e53b74fe70cc21e2cb8825.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*X2F_gceXrWs9kVCEgYrx-A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv"> Logistic 回归分类报告</strong></p></figure><h2 id="4f8d" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">3.4 预测新实例</h2><blockquote class="lp lq lr"><p id="6cb1" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">我们可以预测如下所示的新实例，我们用新标题填充模型，我们预测标签在我们的示例中是负面的，因为我们传达的是战争和制裁。</p></blockquote><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="6628" class="my ky iq mu b gy mz na l nb nc">new_data = ["The US imposes sanctions on Rassia because of the Ukranian war"]<br/>tf = TfidfVectorizer()<br/>tfdf = tf.fit_transform(data['text'])<br/>vect = pd.DataFrame(tf.transform(new_data).toarray())<br/>new_data = pd.DataFrame(vect)<br/>logistic_prediction = clf.predict(new_data)<br/>print(logistic_prediction)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/91f01c3e18c9433b73cc7c05fcd1a786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hbq3tsSOnC1pMvwbsVIXgQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">方法一:对新实例的预测</strong></p></figure><h1 id="5fb3" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">4.深度学习方法:张量流和 Keras</h1><blockquote class="lp lq lr"><p id="5c51" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated"><strong class="lv ir">神经网络</strong>是一种深度学习算法，由多层互连的神经元组成，由激活函数提供动力。它考虑每个输入的加权和，然后对该和应用阶跃函数，并输出显示实例类的结果。</p></blockquote><p id="64f0" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">事实上，就深度学习而言，<strong class="lv ir"> Keras 和 TensorFlow </strong>是最受欢迎的框架之一。具体来说，Keras 是一个运行在 TensorFlow 之上的高级神经网络库，而 TensorFlow 是一个用于机器学习的端到端开源平台，由工具、库和其他资源组成，为工作流提供高级 API。</p><blockquote class="lp lq lr"><p id="3c6a" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">在我们的项目中，我们将利用 TensorFlow 来预处理数据，并使用<strong class="lv ir">标记器类</strong>填充数据，我们将使用 Keras 来加载和训练<strong class="lv ir">顺序模型(神经网络)</strong>，它适用于各层的简单堆叠，其中每层都有一个输入张量和一个输出张量。要了解更多关于顺序模型的信息，请访问此<a class="ae kw" href="https://www.tensorflow.org/guide/keras/sequential_model" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></blockquote><h2 id="7d2e" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">4.1 训练和测试分割</h2><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="9329" class="my ky iq mu b gy mz na l nb nc">##store headlines and labels in respective lists<br/>text = list(data['text'])<br/>labels = list(data['label'])</span><span id="2094" class="my ky iq mu b gy nd na l nb nc">##sentences<br/>training_text = text[0:15000]<br/>testing_text = text[15000:]</span><span id="760b" class="my ky iq mu b gy nd na l nb nc">##labels<br/>training_labels = labels[0:15000]<br/>testing_labels = labels[15000:]</span></pre><h2 id="4ead" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">4.2 从张量设置记号化器，对数据进行预处理。</h2><p id="3ce4" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">在这一步中，我们使用 tensorflow.keras 中的<strong class="lv ir">单词标记器，使用 texs_to_sequences 实例</strong>创建<em class="lu">单词编码</em>(具有键值对的字典)和<em class="lu">序列</em> <strong class="lv ir">，然后我们<em class="lu">使用<strong class="lv ir"> pad_sequences 实例填充这些序列</strong></em>，使其长度相等。</strong></p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="9770" class="my ky iq mu b gy mz na l nb nc">#preprocess <br/>tokenizer = Tokenizer(num_words=10000, oov_token= "&lt;OOV&gt;")<br/>tokenizer.fit_on_texts(training_text)</span><span id="2edc" class="my ky iq mu b gy nd na l nb nc">word_index = tokenizer.word_index</span><span id="81d5" class="my ky iq mu b gy nd na l nb nc">training_sequences = tokenizer.texts_to_sequences(training_text)<br/>training_padded = pad_sequences(training_sequences, maxlen=120, padding='post', truncating='post')</span><span id="7ce6" class="my ky iq mu b gy nd na l nb nc">testing_sequences = tokenizer.texts_to_sequences(testing_text)<br/>testing_padded = pad_sequences(testing_sequences, maxlen=120, padding='post', truncating='post')</span><span id="a8dd" class="my ky iq mu b gy nd na l nb nc"># convert lists into numpy arrays to make it work with TensorFlow <br/>training_padded = np.array(training_padded)<br/>training_labels = np.array(training_labels)<br/>testing_padded = np.array(testing_padded)<br/>testing_labels = np.array(testing_labels)</span></pre><h2 id="1bfd" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">4.3 定义并训练<strong class="ak">顺序模型</strong></h2><p id="549a" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">我们使用 vocab 大小、嵌入维度和输入长度的<strong class="lv ir">嵌入层</strong>来构建模型。我们还添加了一个<strong class="lv ir">密集层 RelU </strong>，它要求模型将实例分为正面或负面两类，以及另一个最终的<strong class="lv ir"> sigmoid 层</strong>，它输出 0 或 1 之间的概率。您可以简单地使用每一层中的超参数来提高模型性能。然后，我们用优化器和度量性能编译该模型，并在我们的数据集上训练它。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="e825" class="my ky iq mu b gy mz na l nb nc">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(10000, 16, input_length=120),<br/>    tf.keras.layers.GlobalAveragePooling1D(),<br/>    tf.keras.layers.Dense(24, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])</span><span id="9335" class="my ky iq mu b gy nd na l nb nc">##compile the model<br/>model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])<br/> <br/>model.summary()</span></pre><p id="49c8" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">我们可以在下图中看到，我们有 4 层，max_length 为 120，密集层节点为 16 和 24，以及 160，433 个可训练参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7b465970d1c837a45b9157adc2223f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*hGF_Dla-VMt6AAQonXoUXw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">顺序模型总结</strong></p></figure><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="137c" class="my ky iq mu b gy mz na l nb nc">num_epochs = 10<br/>history = model.fit(training_padded, <br/>                    training_labels, <br/>                    epochs=num_epochs, <br/>                    validation_data=(testing_padded, testing_labels), <br/>                    verbose=2)</span></pre><p id="0b50" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">我们可以进一步检查我们建立的具有 10 个运行时期的神经网络模型具有 99%的<strong class="lv ir"/><strong class="lv ir"/><strong class="lv ir">非常好的准确性，减少验证损失并增加验证准确性</strong>，这确保了强大的预测性能和泛化(过拟合)错误的低风险。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/13a867c5d098b5030d7ea671da230f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NAnJkbSaC8fQzMCY5o8Mw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">神经网络训练结果</strong></p></figure><h2 id="6753" class="my ky iq bd kz ng nh dn ld ni nj dp lh mp nk nl lj mq nm nn ll mr no np ln nq bi translated">4.4 预测新实例</h2><blockquote class="lp lq lr"><p id="d096" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">现在，我们将使用这个特定的模型来预测同一个标题。同样，输出接近于零，这也表明这个标题是负面的。</p></blockquote><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="00a1" class="my ky iq mu b gy mz na l nb nc">new_headline = ["The US imposes sanctions on Rassia because of the Ukranian war"]</span><span id="1531" class="my ky iq mu b gy nd na l nb nc">##prepare the sequences of the sentences in question<br/>sequences = tokenizer.texts_to_sequences(new_headline)<br/>padded_seqs = pad_sequences(sequences, maxlen=120, padding='post', truncating='post')</span><span id="c9c0" class="my ky iq mu b gy nd na l nb nc">print(model.predict(padded_seqs))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/88a6c83dc6611c8bc22ff55fbdb86b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYboGnM1gZUi_W37bg7ENw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">方法二:对新实例的预测</strong></p></figure><h1 id="c2cc" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">5.结论</h1><p id="cbdd" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">在本文中，我们建立了一个二元分类器来检测新闻标题的情感。然而，我们首先使用了一些启发式规则，使用 sluck 方法来创建分类负面和正面标题的标签。我们使用监督 ML 和深度学习方法创建了情感预测。这两种方法都成功地预测了新给定实例上的正确标题，并且对于逻辑回归和深度神经网络，它们都分别具有合理的 92%和 96%的高准确度分数。</p><p id="60c3" class="pw-post-body-paragraph ls lt iq lv b lw lx jr ly lz ma ju mb mp md me mf mq mh mi mj mr ml mm mn mo ij bi translated">你可能会问自己，在我的下一个数据科学预测任务中，哪种方法更好或更容易使用；然而，答案完全取决于项目的范围和复杂性以及数据的可用性，有时我们可能会选择使用 scikit-learn 中众所周知的算法的简单解决方案，预测系统利用数学直觉为给定的输入分配所需的输出值。另一方面，深度学习试图模仿人脑的功能，从给定的输入中执行规则(作为输出)。我们应该记住，神经网络通常需要大量的数据和高计算能力来完成任务。</p><blockquote class="lp lq lr"><p id="c0b7" class="ls lt lu lv b lw lx jr ly lz ma ju mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">我希望你喜欢阅读这篇文章，并期待更多丰富的文章，这些文章串联了我关于数据科学和机器学习主题的知识。</p></blockquote><h1 id="2bdd" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">参考</h1><p id="990f" class="pw-post-body-paragraph ls lt iq lv b lw nr jr ly lz ns ju mb mp nt me mf mq nu mi mj mr nv mm mn mo ij bi translated">[1]100 万条新闻标题，18 年间发布的新闻标题，许可 CCO: Public Domain，<a class="ae kw" href="https://www.kaggle.com/therohk/million-headlines" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></div></div>    
</body>
</html>