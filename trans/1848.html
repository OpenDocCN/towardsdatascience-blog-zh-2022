<html>
<head>
<title>Bias-Variance Decomposition for Model Assessment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模型评估的偏差-方差分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-and-variance-for-model-assessment-a2edb69d097f#2022-04-29">https://towardsdatascience.com/bias-and-variance-for-model-assessment-a2edb69d097f#2022-04-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7936" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习算法的偏差-方差分解及其在Python中的实际应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/603539f21cbed439567ffdcab5ac1c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Hh5yTB-MfgA2_nCz.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/@goumbik" rel="noopener ugc nofollow" target="_blank">卢卡斯</a>在<a class="ae ky" href="https://www.pexels.com/photo/chart-close-up-data-desk-590022/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄</p></figure><p id="9cdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">偏差</em>和<em class="lv">方差</em>是机器学习模型评估中的两个关键概念，因为它们与模型在未知数据上的性能密切相关。偏差和方差都是预测误差的误差类型。第三种误差是不可约误差，这是数据中固有的误差，无论使用什么算法都无法减少。</p><p id="c0dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家在实施新模型时面临的主要困难之一是所谓的<em class="lv">偏差-方差困境</em>或<em class="lv">偏差-方差问题</em>。这包括在监督学习算法中最小化两个误差源的冲突，可以用<em class="lv">偏差-方差分解</em>方法进行评估。</p><p id="683d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在整篇文章中，我们将浏览这些概念，并以本文的主要贡献结束，本文解释了偏差-方差分解，并为任何对实现模型分解感兴趣的人提供了一个Python实践示例。</p><h1 id="c936" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">偏差和方差</h1><p id="9433" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">偏差</em>被定义为模型预测和实际情况之间的差异。高偏差会导致算法错过特征和目标输出之间的相关关系(欠拟合)。</p><p id="9f99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">方差</em>定义为对训练集中波动的敏感度。换句话说，就是指当训练数据发生变化时，结果的变化有多大。高方差表明随着训练数据集的变化，目标函数的估计值会有大的变化(过度拟合)。</p><p id="a153" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用数学表示的两个术语都对应于以下公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/2ad47ce693e484cecfe2aa25f57d7b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NfHpLgpVpz5gTCMqCf3Vg.png"/></div></div></figure><p id="11f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两项都可以很容易地从均方误差(MSE)公式中导出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/e006f3708eb3d984ddb89e28ac4c49c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iosggQBi1SG9r3DNkItfDw.png"/></div></div></figure><blockquote class="mv mw mx"><p id="9f9e" class="kz la lv lb b lc ld ju le lf lg jx lh my lj lk ll mz ln lo lp na lr ls lt lu im bi translated">如何解释这些公式的例子显示在文章的结尾。</p></blockquote><p id="6ca5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个图表，包含高低偏差和高低方差的四种不同情况。考虑到我们收集的训练数据中的机会可变性，每个命中代表我们模型的一个单独实现[1]。目标的中心意味着模型完美地预测了这些值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/483f2b6737648a992b494a8bdd89228b.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*aMj8GPI-ektxgkBiHh9Eug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nc">图一。</strong>偏差-方差。参考:图片由作者提供。</p></figure><h1 id="dece" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">偏差-方差权衡</h1><p id="31dd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">尽管最佳任务是尽可能使偏差和方差最小，但在实践中，两种误差之间存在明显的权衡。在这两个术语之间找到一个平衡点就是所谓的<em class="lv">偏差-方差权衡</em>。</p><p id="82a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于机器学习模型，偏差和方差与模型的复杂性密切相关，然后与模型何时过度拟合或欠拟合训练数据相关联。如图2所示，当模型复杂性超过最佳点时，我们的模型会过度拟合训练数据，而如果模型复杂性不足，则模型会对数据拟合不足。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/b011b188873da31ecb299a37bb3b3783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n16BhE2yk1yM3eRHBNxorQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nc">图二</strong>。偏差和方差随模型复杂性的变化。<strong class="bd nc">参考号</strong>:图片由作者提供。</p></figure><p id="701d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实际场景中，没有找到最佳点的分析方法，因此需要测试几个具有不同复杂性的模型，并选择一个使总体误差最小的模型。</p><h1 id="09b1" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">偏差-方差分解</h1><p id="4679" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">偏差-方差分解是理解算法性能的一种有用方法。</p><p id="d956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法背后的主要思想是当用不同的训练集训练相同的模型并在相同的测试集上测试它时，测量偏差和方差。</p><p id="2798" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了实现这一点，用来对数据进行子采样的方法是<em class="lv">自举</em>(名字<em class="lv">打包</em>来源于<em class="lv">自举</em> + <em class="lv">合计</em>)。这种方法包括对数据进行随机采样和替换，这意味着训练数据的子集将重叠，因为我们不是分割数据，而是对其进行重采样。</p><p id="e67e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，通过迭代运行<em class="lv"> bootstrapping </em>方法并获得测试集模型的准确性，我们可以获得我们迭代的所有回合的平均偏差和方差。</p><p id="3fe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是回归任务的部分代码，总结了该方法背后的主要逻辑。所有代码都是从<a class="ae ky" href="https://github.com/rasbt/mlxtend" rel="noopener ugc nofollow" target="_blank"> MLxtend </a>库中获得的。</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="6d05" class="nj lx it nf b gy nk nl l nm nn">Input:<br/>- X_train<br/>- y_train<br/>- X_test<br/>- y_test<br/>- num_rounds: Number of iterations<br/>Output:<br/>- avg_expected_loss: Average MSE loss for all the rounds<br/>- avg_bias: Average bias for all the rounds<br/>- avg_var: Average variance for all the rounds<br/>(avg_expected_loss = avg_bias^2 + avg_var)</span><span id="0371" class="nj lx it nf b gy no nl l nm nn">(1) Iterate for <em class="lv">num_rounds</em>, in each implementing bootstrapping, training the model and getting the predictions<br/>for i in range(num_rounds):<br/>- X_boot, y_boot = _draw_bootstrap_sample(rng, X_train, y_train)<br/>- pred = estimator.fit(X_boot, y_boot).predict(X_test)<br/>- all_pred[i] = pred</span><span id="1353" class="nj lx it nf b gy no nl l nm nn">(2) Obtain the average MSE error<br/><strong class="nf iu">avg_expected_loss</strong> = np.apply_along_axis(lambda x: ((x — y_test)**2).mean(), axis=1, arr=all_pred).mean()</span><span id="87f4" class="nj lx it nf b gy no nl l nm nn">(3) Obtain the average bias and variance<br/>main_predictions = np.mean(all_pred, axis=0)<br/><strong class="nf iu">avg_bias</strong> = np.sum((main_predictions — y_test)**2) / y_test.size<br/><strong class="nf iu">avg_var</strong> = np.sum((main_predictions — all_pred)**2) / all_pred.size</span></pre><p id="0139" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，这里有一个Python实践示例，展示了如何实现偏差-方差分解。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="c8aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为展示，我们使用了免费提供的波士顿住房数据集[2]，其任务是使用回归变量预测房价。我们首先将数据分为训练集和测试集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="b3f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">例1:决策树</em> </strong></p><p id="99c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了分析偏差-方差分解，我们首先实现了一个决策树回归器，并通过<em class="lv"> bias_variance_decomp </em>函数运行它，其伪代码如上所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="1648" class="nj lx it nf b gy nk nl l nm nn">Average expected loss: 32.419<br/>Average bias: 14.197<br/>Average variance: 18.222</span></pre><p id="ad21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了将这个输出与另一个模型进行比较，我们还使用决策树回归器运行了一个bagging集成方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="ed0f" class="nj lx it nf b gy nk nl l nm nn">Average expected loss: 18.693<br/>Average bias: 15.292<br/>Average variance: 3.402</span></pre><p id="26ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与之前的结果相比，我们可以观察到偏差是如何增加的，这意味着bagging回归模型的表现比决策树模型差。然而，方差严重下降，这表明该模型与其预测更加一致。</p><p id="7de8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">例2:神经网络</em> </strong></p><p id="53f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还评估了在Keras中实现的基于神经网络的模型的性能，因为据我们所知，这个功能不能在PyTorch中实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="4d8f" class="nj lx it nf b gy nk nl l nm nn">Average expected loss: 25.470<br/>Average bias: 19.927<br/>Average variance: 5.543</span></pre><p id="b0b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与第一个模型相比，我们通过增加每层的神经元数量来增加模型的复杂性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="np nq l"/></div></figure><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="3b07" class="nj lx it nf b gy nk nl l nm nn">Average expected loss: 23.458<br/>Average bias: 17.608<br/>Average variance: 5.850</span></pre><p id="351a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如预期的那样，偏差的减少是以增加模型的方差为代价的。</p><h1 id="bf0c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">管理偏差和差异的方法</h1><p id="3b3b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这里有一些技巧来管理偏差和方差误差。</p><p id="2bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先也是最重要的，不要只关注偏差的最小化，或者换句话说，不要忘记方差。对于一个健壮的模型来说，这两者同样重要。</p><p id="515e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，通过(1)实施<em class="lv">增强</em>集成方法，或(2)添加更多特征或进行特征工程来增加复杂性，可以减少模型的偏差。</p><p id="f9b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，方差可以通过(1)实施<em class="lv"> bagging </em>集合方法，或者(2)通过正则化来约束或收缩估计的系数来减小。</p><p id="47f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关如何使用集成方法管理偏差和方差的更多详细信息，我建议阅读文章<a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-ensemble-methods-in-machine-learning-e72c6b9ff4bc"> <em class="lv">机器学习集成方法简介</em> </a>。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="2b6b" class="nj lx it bd ly ny nz dn mc oa ob dp mg li oc od mi lm oe of mk lq og oh mm oi bi translated">如何解释偏差和差异的详细说明</h2><p id="34e8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如果我们有下面的分布，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/4dac8901ec8ac3092ee4959153dbd790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3PnpXpfaW2VvGb96dBoqoA.png"/></div></div></figure><p id="f756" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">期望值和方差的计算如下:</p><ul class=""><li id="fe4c" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">期望值</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/0b484bfa2970fb5f854d7c33aefce966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RtV1BVXBrCICVkUVd5FQw.png"/></div></div></figure><ul class=""><li id="d176" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">差异</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/4b5a8f96378917707b4120e55e41b250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9FTlqTSNsDR_8iqf9orFg.png"/></div></div></figure></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="ba73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">如果你喜欢这篇文章，请考虑</em> </strong> <a class="ae ky" href="https://javiferfer.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">订阅</em> </strong> </a> <strong class="lb iu"> <em class="lv">。你将获得我所有的内容+所有其他来自牛逼创作者的文章！</em> </strong></p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h1 id="1477" class="lw lx it bd ly lz ov mb mc md ow mf mg jz ox ka mi kc oy kd mk kf oz kg mm mn bi translated">参考</h1><p id="f0f0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] Scott Fortmann-Roe，<a class="ae ky" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">了解偏差-方差权衡</a></p><p id="3b10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]Harrison d .和Rubin feld d . l .，《享乐价格和对清洁空气的需求》，环境杂志。经济学与管理学，第5卷，81–102页，1978年。</p><p id="cf94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]机器学习者，<a class="ae ky" href="https://www.themachinelearners.com/tradeoff-bias-variance/" rel="noopener ugc nofollow" target="_blank">Entiende de una vez quées El trade off Bias-Variance</a></p><p id="471a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] GitHub，<a class="ae ky" href="https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/bias_variance_decomp.py" rel="noopener ugc nofollow" target="_blank">函数bias_variance_decomp.py </a></p><p id="ebb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【5】栈交换，<a class="ae ky" href="https://math.stackexchange.com/questions/2005675/what-is-ex2-mean-in-literal-terms" rel="noopener ugc nofollow" target="_blank">𝐸(𝑋)从字面上是什么意思？</a></p><p id="5551" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6]中等，<a class="ae ky" href="https://medium.com/@pardeshi.vishwa25/bias-variance-tradeoff-explained-7f18ebbef020" rel="noopener">偏倚-方差权衡解释</a></p><p id="90bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7]机器学习掌握，<a class="ae ky" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习中偏差-方差权衡的温和介绍</a></p><p id="a8bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8] GitHub，<a class="ae ky" href="https://github.com/christianversloot/machine-learning-articles/blob/main/machine-learning-error-bias-variance-and-irreducible-error-with-python.md" rel="noopener ugc nofollow" target="_blank">用python实现机器学习误差偏差-方差和不可约误差</a></p><p id="7972" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9] GitHub，<a class="ae ky" href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" rel="noopener ugc nofollow" target="_blank">分类和回归损失的偏差-方差分解</a></p><p id="9a70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[10]天才博客，<a class="ae ky" href="https://kindsonthegenius.com/blog/bias-variance-trade-off-in-classificationmachine-learning/" rel="noopener ugc nofollow" target="_blank">什么是偏差-方差权衡？</a></p></div></div>    
</body>
</html>