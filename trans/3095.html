<html>
<head>
<title>Everything you need to know about ALBERT, RoBERTa, and DistilBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你需要知道的关于艾伯特，罗伯塔和迪沃伯特的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-albert-roberta-and-distilbert-11a74334b2da#2022-07-07">https://towardsdatascience.com/everything-you-need-to-know-about-albert-roberta-and-distilbert-11a74334b2da#2022-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6410" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">回顾不同BERT变形金刚的异同，以及如何从拥抱脸变形金刚库中使用它们</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ad3a83b17f4ecaf1b635da90d0e55e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qD_4ALUyvJAZXesr"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内特·雷菲尔德在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="00fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将解释你需要知道的关于Albert、Roberta和Distilbert的一切。如果你不能从名字上看出来，这些模型都是原始最先进的变压器BERT的修改版本。这三个型号，连同伯特，是目前最受欢迎的变形金刚。我将回顾这些模型与BERT的不同(和相似)之处，对于每个模型，我将包含代码片段，演示如何使用拥抱面部变形库中的每个模型。注意，这篇文章写于2022年7月，所以拥抱脸的早期/未来版本可能不起作用。还要注意的是，本文假设您对BERT transformer有所了解，所以在阅读本文之前要先了解这一点。然而，我将在本文中快速回顾BERT，作为一个简短的概述。</p><h2 id="1761" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">伯特</h2><p id="6bcc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">BERT——或来自Transformers的双向编码器表示——是第一个建立在原始编码器-解码器转换器基础上的转换器，它使用掩蔽语言建模和下一句预测任务的自我监督训练来学习/产生单词的上下文表示。BERT的核心架构由12个编码器模块堆叠而成(来自原始编码器-解码器转换纸)。为了对其他任务进行微调，例如(但不限于)问题回答、摘要和序列分类，BERT在堆叠编码器的顶部添加了额外的线性层。这些额外的层用于根据BERT正在解决的任务生成特定的输出。然而，重要的是要记住，BERT的原始的、核心的、不可改变的部分是来自堆叠的双向编码器的输出。这些模块使得BERT如此强大:通过定制/添加任何特定的层组合，您几乎可以配置BERT来解决任何任务。在本文中，我将向您展示这样配置BERT的代码。你可以从拥抱脸变形库<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209">这里</a>找到如何使用BERT的代码。</p><p id="bc8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了方便起见，下面是如何使用BERT完成任何通用任务的代码片段:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8ff9" class="lv lw it mu b gy my mz l na nb">from transformers import BertModel<br/>class Bert_Model(nn.Module):<br/>   def __init__(self, class):<br/>       super(Bert_Model, self).__init__()<br/>       self.bert = BertModel.from_pretrained('bert-base-uncased')<br/>       self.out = nn.Linear(self.bert.config.hidden_size, classes)<br/>   def forward(self, input):<br/>       _, output = self.bert(**input)<br/>       out = self.out(output)<br/>       return out</span></pre><p id="f058" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码可以用来构建一个通用的Pytorch BERT模型，该模型可以在任何其他未指定的任务上进行微调。正如你所看到的，我没有下载一个已经为特定任务设计的特定的BERT模型，比如BERTForQuestionAnswering或BERTForMaksedLM，而是下载了一个未经训练的BERT模型，它没有附带任何“头”。相反，我在上面添加了我自己的线性层，然后可以配置为其他任务，这些任务没有列在HuggingFace transformer库已经完成的任务中，你可以在这里找到<a class="ae ky" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank"/>。虽然上面的代码不一定是您想要的，但是您可以浏览hugging face提供的模型列表并使用它们的API。例如，下面是如何为BERT建立一个屏蔽语言模型。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="64fb" class="lv lw it mu b gy my mz l na nb">from transformers import BertTokenizer, BertForMaskedLM<br/>from torch.nn import functional as F<br/>import torch<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')<br/>model = BertForMaskedLM.from_pretrained('bert-base-uncased',    return_dict = True)<br/>text = "The capital of France, " + tokenizer.mask_token + ", contains the Eiffel Tower."<br/>input = tokenizer.encode_plus(text, return_tensors = "pt")<br/>mask_index = torch.where(input["input_ids"][0] == tokenizer.mask_token_id)<br/>output = model(**input)<br/>logits = output.logits<br/>softmax = F.softmax(logits, dim = -1)<br/>mask_word = softmax[0, mask_index, :]<br/>top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]<br/>for token in top_10:<br/>   word = tokenizer.decode([token])<br/>   new_sentence = text.replace(tokenizer.mask_token, word)<br/>   print(new_sentence)</span></pre><p id="dcaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">屏蔽语言建模本质上是一个“填空任务”，其中模型屏蔽一个标记，并训练自己使用屏蔽标记周围的上下文来准确预测屏蔽标记是什么。在上面的示例中，代码使用BERT列出屏蔽令牌的前10个候选令牌。你可以在这里阅读<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209">发生的事情的更详细的描述。该代码片段的输出是:</a></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="39b0" class="lv lw it mu b gy my mz l na nb">The capital of France, paris, contains the Eiffel Tower. <br/>The capital of France, lyon, contains the Eiffel Tower. <br/>The capital of France, lille, contains the Eiffel Tower. <br/>The capital of France, toulouse, contains the Eiffel Tower. <br/>The capital of France, marseille, contains the Eiffel Tower. <br/>The capital of France, orleans, contains the Eiffel Tower. <br/>The capital of France, strasbourg, contains the Eiffel Tower. <br/>The capital of France, nice, contains the Eiffel Tower. <br/>The capital of France, cannes, contains the Eiffel Tower. <br/>The capital of France, versailles, contains the Eiffel Tower.</span></pre><h2 id="5f42" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">罗伯塔</h2><p id="5785" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">罗伯塔是伯特的一个简单但非常受欢迎的替代者/继承者。它主要通过仔细和智能地优化BERT的训练超参数来改进BERT。几个简单明了的变化一起增强了Roberta的性能，使它在BERT设计解决的几乎所有任务上都优于BERT。值得注意的一个有趣事实是，在Roberta出版的时候，另一个流行的新变形金刚，<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-xlnet-from-the-hugging-face-transformer-library-ddd0b7c8d0b9"> XLNet </a>，也在一篇研究论文中发表/介绍。然而，与XLNet不同的是，XLNet引入的变化比Roberta引入的变化更难实现，这只会增加Roberta在AI/NLP社区中的受欢迎程度。</p><p id="2e3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我之前提到的，罗伯塔实际上使用了与伯特相同的架构。然而，与BERT不同的是，在预训练期间，它只通过掩蔽语言建模进行预训练(BERT也通过下一句预测进行预训练)。下面是Roberta用来获得更好性能的一些超参数变化。</p><ul class=""><li id="66d8" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">更长的训练时间和更大的训练数据(从16GB到160GB增加10倍)</li><li id="df5e" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">从256到8000的更大批量和从30k到50k的更大词汇量</li><li id="4b6f" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">使用更长的序列作为输入，但是Roberta仍然像BERT一样有512个标记的最大标记限制</li><li id="8a46" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">动态掩蔽允许每次将序列输入模型时掩蔽模式不同，这与使用相同掩蔽模式的BERT相反。</li></ul><p id="d427" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">知道如何使用拥抱脸变形库中的BERT确实有助于理解Roberta(以及本文中描述的所有模型)是如何编码的。你可以从拥抱脸变形库<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209">这里</a>学习如何使用BERT。按照那篇文章中的代码，使用拥抱脸中的Roberta非常简单。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9f16" class="lv lw it mu b gy my mz l na nb">from transformers import RobertaModel<br/>import torch<br/>import torch.nn as nn</span><span id="26bd" class="lv lw it mu b gy nq mz l na nb">class RoBERTa_Model(nn.Module):<br/>  def __init__(self, classes):<br/>    super(RoBERTa_Model, self).__init__()<br/>    self.roberta = RobertaModel.from_pretrained('roberta-base')<br/>    self.out = nn.Linear(self.roberta.config.hidden_size, classes)<br/>    self.sigmoid = nn.Sigmoid()<br/>  def forward(self, input, attention_mask):<br/>    _, output = self.roberta(input, attention_mask = attention_mask)<br/>    out = self.sigmoid(self.out(output))<br/>    return out</span></pre><p id="e093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码展示了如何构建一个通用的Roberta Pytorch模型。如果您将其与基于BERT的模型的代码进行比较，您确实可以看到我们实际上只是用Roberta替换了BERT！这确实有道理——毕竟，罗伯塔更像伯特，但受过更好的训练。但是你很快就会发现，阿尔伯特和迪翁伯特也是如此。因为这些模型都是BERT的修改版本，所以拥抱脸代码的工作方式是，使用任何模型时，您只需从上面获取BERT代码，然后用Roberta替换所有BERT术语(即，改为导入Roberta模型，使用正确的模型id“Roberta-base”，并导入正确的Roberta tokenizer)。因此，如果你想用Roberta做屏蔽语言建模、抽取式问题回答或其他任何事情，你可以使用上面的BERT代码或这里的<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209"/>并直接用Roberta、Distilbert或Albert(你想用哪个就用哪个)替换BERT术语。</p><h2 id="24c8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">蒸馏啤酒</h2><p id="7ccc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Distilbert的目标是通过减小bert的大小和提高BERT的速度来优化训练，同时尽可能地保持最佳性能。具体来说，Distilbert比最初的BERT-base模型小40%，比它快60%，并且保留了97%的功能。</p><p id="49f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Distilbert是如何做到这一点的？它使用与BERT大致相同的通用架构，但只有6个编码器模块(回想一下，BERT base有12个)。这些编码器块也通过仅从每2个预训练的BERT编码器块中取出1个来初始化。此外，BERT的令牌类型嵌入和池功能也从Distilbert中移除。</p><p id="1281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与bert不同，Distilbert仅使用掩蔽语言建模进行预训练(回想一下，BERT是使用MLM和下一句预测进行训练的)。使用三重损失/三重损失函数训练Distilbert:</p><ul class=""><li id="35a6" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">伯特使用的相同的语言模型损失</li><li id="85c4" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">蒸馏损失衡量蒸馏器和bert之间输出的相似性。</li><li id="42b3" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">余弦距离损失衡量蒸馏伯特和伯特的隐藏状态有多相似。</li></ul><p id="e79d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些损失函数的组合模拟了Distilbert和bert之间的学生-教师学习关系。Distilbert还使用了几个与Roberta相同的超参数，比如更大的批量，动态屏蔽，以及我之前提到的，没有对下一句预测进行预训练。从上面看Roberta(和BERT)的代码，使用拥抱脸的Distilbert非常容易。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8a12" class="lv lw it mu b gy my mz l na nb">from transformers import DistilBertModel<br/>import torch<br/>import torch.nn as nn</span><span id="36f6" class="lv lw it mu b gy nq mz l na nb">class DistilBERT_Model(nn.Module):<br/> def __init__(self, classes):<br/>   super(DistilBERT_Model, self).__init__()<br/>   self.distilbert = DistilBertModel.from_pretrained('distilbert<br/>                                                     base-uncased')<br/>   self.out = nn.Linear(self.distilbert.config.hidden_size, classes)<br/>   self.sigmoid = nn.Sigmoid()<br/> def forward(self, input, attention_mask):<br/>   _, output = self.distilbert(input, attention_mask <br/>                                      = attention_mask)<br/>   out = self.sigmoid(self.out(output))<br/>   return out</span></pre><h2 id="3bda" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">艾伯特</h2><p id="1806" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Albert与Distilbert大约在同一时间出版/推出，也有一些与论文中介绍的相同的动机。就像Distilbert一样，Albert减少了bert的模型大小(参数减少了18倍)，训练速度也提高了1.7倍。然而，与Distilbert不同的是，Albert在性能上没有折衷(Distilbert在性能上确实有轻微的折衷)。这来自于Distilbert和Albert实验构造方式的核心差异。Distilbert的训练方式是将bert作为其训练/蒸馏过程的老师。另一方面，艾伯特和伯特一样是从零开始训练的。更好的是，Albert优于所有以前的模型，包括bert、Roberta、Distilbert和XLNet。</p><p id="bdd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Albert能够通过这些参数缩减技术获得较小模型架构的结果:</p><ul class=""><li id="f9db" class="nc nd it lb b lc ld lf lg li ne lm nf lq ng lu nh ni nj nk bi translated">因式分解的嵌入参数化:为了确保隐藏层的大小和嵌入维度是不同的，Alberta将嵌入矩阵解构为2块。这允许它实质上增加隐藏层的大小，而不真正修改实际的嵌入尺寸。在分解嵌入矩阵之后，在嵌入阶段完成之后，Alberta将线性层/全连接层添加到嵌入矩阵上，并且这映射/确保嵌入维度的维度是相同正确的。你可以在这里阅读更多关于这个<a class="ae ky" href="https://tungmphung.com/a-review-of-pre-trained-language-models-from-bert-roberta-to-electra-deberta-bigbird-and-more/#albert" rel="noopener ugc nofollow" target="_blank">的内容。</a></li><li id="b845" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">跨层参数共享:回想一下，BERT和Alberta各有12个编码器模块。在阿尔伯塔省，这些编码器块共享所有参数。这将参数大小减少了12倍，并且还增加了模型的正则化(正则化是在校准用于防止过拟合/欠拟合的ML模型时的一种技术)</li><li id="2ba6" class="nc nd it lb b lc nl lf nm li nn lm no lq np lu nh ni nj nk bi translated">Alberta删除了辍学层:辍学层是一种技术，其中随机选择的神经元在训练过程中被忽略。这意味着他们不再被训练，基本上暂时无用。</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="cf1e" class="lv lw it mu b gy my mz l na nb">from transformers import AlbertModel<br/>import torch<br/>import torch.nn as nn</span><span id="4abf" class="lv lw it mu b gy nq mz l na nb">class ALBERT_Model(nn.Module):<br/> def __init__(self, classes):<br/>   super(ALBERT_Model, self).__init__()<br/>   self.albert = AlbertModel.from_pretrained('albert-base-v2')<br/>   self.out = nn.Linear(self.albert.config.hidden_size, classes)<br/>   self.sigmoid = nn.Sigmoid()<br/> def forward(self, input, attention_mask): <br/>   _, output = self.albert(input, attention_mask = attention_mask)<br/>   out = self.sigmoid(self.out(output))<br/>   return out</span></pre><h2 id="6e65" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">其他类似变压器</h2><p id="161a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然Albert、Roberta和Distilbert可能是最受欢迎的三种变形金刚(bert的所有修改/版本/改进),但其他几种受欢迎的变形金刚也实现了类似的一流性能。这些包括但不限于XLNet、BART和Mobile-BERT。<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-xlnet-from-the-hugging-face-transformer-library-ddd0b7c8d0b9"> XLNet </a>是一个自回归语言模型，建立在Transformer-XL模型的基础上，使用<a class="ae ky" rel="noopener" target="_blank" href="/permutative-language-modeling-explained-9a7743d979b4">置换语言建模</a>来实现与Roberta类似的最先进的结果。Mobile-BERT类似于DistilBERT:它主要是为速度和效率而设计的。与BERT-base相比，它的体积小4.3倍，速度快5.5倍，但性能相当/相似。BART是另一个预训练的模型，在NLU(自然语言理解)任务上取得了与Roberta相似的性能。除此之外，BART还可以在NLG(自然语言生成)任务上表现出色，如抽象摘要，这就是它的独特之处。你可以在这里阅读更多关于他们<a class="ae ky" href="https://tungmphung.com/a-review-of-pre-trained-language-models-from-bert-roberta-to-electra-deberta-bigbird-and-more/#albert" rel="noopener ugc nofollow" target="_blank">的信息。</a></p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="cf25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您觉得这些内容很容易理解。如果你认为我需要进一步阐述或澄清什么，请在下面留言。</p><h2 id="ad00" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="64d7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">预训练语言模型回顾:从BERT、RoBERTa到ELECTRA、DeBERTa、BigBird等等:<a class="ae ky" href="https://tungmphung.com/a-review-of-pre-trained-language-models-from-bert-roberta-to-electra-deberta-bigbird-and-more/#distilbert" rel="noopener ugc nofollow" target="_blank">https://tungmphung . com/a-review-of-pre-trained-language-models-from-BERT-RoBERTa-to-ELECTRA-DeBERTa-big bird-and-more/# distill BERT</a></p><p id="6597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特:用于语言理解的深度双向转换器的预训练:<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a></p><p id="727e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ALBERT:一个用于语言表征自我监督学习的Lite BERT:<a class="ae ky" href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2019/12/ALBERT-Lite-BERT-for-Self-Supervised . html</a></p><p id="334e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">蒸馏伯特，伯特的蒸馏版本:更小、更快、更便宜、更轻:<a class="ae ky" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1910.01108</a></p><p id="0824" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">抱抱脸变形金刚库:<a class="ae ky" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/index</a></p></div></div>    
</body>
</html>