<html>
<head>
<title>Feature Importance in Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树中的特征重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-importance-in-decision-trees-e9450120b445#2022-06-02">https://towardsdatascience.com/feature-importance-in-decision-trees-e9450120b445#2022-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="347c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于树的机器学习算法中测量特征重要性背后的计算的完整Python实现和解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ac32eb909f1819837dd55ed39062ee1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q6j3aSPSEjrgLbQv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约书亚·戈尔德在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d59d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的目的是让读者熟悉如何在决策树中计算特征的重要性。就我个人而言，我没有找到对这个概念的深入解释，因此本文诞生了。</p><p id="63ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文中使用的所有代码都是公开的，可以通过以下途径找到:</p><p id="edd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【https://github.com/Eligijus112/gradient-boosting T4】</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="eb25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入研究特性重要性计算之前，我强烈建议您复习一下什么是树，以及我们如何使用这些文章将它们组合成随机森林的知识:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的决策树算法从零开始</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">用Python编写只使用NumPy和Pandas的流行算法，并解释其中的内容</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/regression-tree-in-python-from-scratch-9b7b64c815e3"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的回归树从头开始</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">用Python编写流行的回归树算法，并解释其本质</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/random-forest-algorithm-in-python-from-scratch-487b314048a8"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的随机森林算法</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">使用(主要)数组和循环在python中编写强大的算法</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="6dfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用决策树模型，使用各种回归变量(<strong class="lb iu"> X </strong>)来创建加利福尼亚州中值房价(<strong class="lb iu"> Y </strong>)之间的关系。可以使用scikit-learn包加载数据集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加载加州住房数据集；作者代码</p></figure><p id="7d4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在模型中使用的特性<strong class="lb iu"> X </strong>是:</p><p id="9496" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">* <strong class="lb iu"> MedInc </strong> —过去12个月的家庭收入中位数(十万)</p><p id="9178" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">* <strong class="lb iu">房龄</strong> —房龄(年)</p><p id="b680" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">* <strong class="lb iu">平均房间数</strong>——每个住所的平均房间数</p><p id="630b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">* <strong class="lb iu"> AveBedrms </strong> —每个住宅的平均卧室数量</p><p id="9668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">*<strong class="lb iu">ave occupation</strong>——平均家庭成员数</p><p id="44ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">响应变量<strong class="lb iu"> Y </strong>是加州各区的中值房价，以几十万美元表示。</p><p id="d015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据的形状如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1f38e185dab3ac41f52f7276100d03ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*it5mcWm1h70dR6d_SOIEhQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据的形状；按作者分类的表格</p></figure><p id="0335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速浏览特征变量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/90778270f4e8dfb7949d90a6f70bfbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*GHObzAl6bfpnQOEyfcqbhg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">X变量的头；按作者分类的表格</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b9d364d6815dbdbd1e2dc17306453b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*ZgK97y8sX8_y-HxnSALEvw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征的描述性统计；按作者分类的表格</p></figure><p id="e87d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Y变量的分布:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/607f62b50cabdda49b50e98e305bb154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*x9s7_06LYOvRKsHiKmRx7g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">响应变量的分布；按作者分类的图表</p></figure><p id="779b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了匿名化数据，数据中有一个50万美元的收入上限:超过这个上限的仍被标记为50万美元的收入。这是为了确保没有人可以识别具体的家庭，因为早在1997年没有多少家庭是如此昂贵。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3e89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将数据分为训练集和测试集，拟合回归树模型，并推断训练集和测试集的结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拟合回归树；作者代码</p></figure><p id="739f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印输出如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/54eece25c6f8bd461b82a1d3444a2b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*InihOiyk3C4hg5HrkhE1bQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">准确性结果；作者照片</p></figure><p id="d696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">长成的树不会过度生长。这篇文章是关于特征的推断，所以我们不会尽力减少错误，而是试图推断哪些特征是最有影响的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e1da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练集上的成年树:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/c8f2e24156c60e18f643786c5751656a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xy7nB4vRply2ER7QumMzuQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">全回归树；按作者分类的图表</p></figure><p id="ab69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树由<strong class="lb iu">个节点、</strong>个节点组成，每个节点由一个分割规则链接。分割规则涉及一个要素和它应该分割的值。</p><p id="2448" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">术语“分割”是指如果满足分割规则，数据集的观测值将位于节点的左侧。如果不满足规则，则观察向右进行。</p><p id="329b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从1到15枚举每个节点。</p><p id="5a10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查第一个节点和其中的信息。<strong class="lb iu">所有节点的逻辑都是一样的。</strong></p><p id="ecef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> MedInc ≤ 5.029 </strong> —节点的拆分规则。如果一个观察的MedInc值小于或等于5.029，那么我们向左遍历树(转到节点2)，否则，我们转到右边的节点(节点3)。</p><p id="2f35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> squared_error </strong> —用作分割标准的统计数据。squared_error的计算公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/7c240527b671af0fc3d0bdcced199f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/0*4p6GhgCopvaW7KbG"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">均方误差公式</p></figure><p id="b7a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一个节点中，统计值等于1.335。</p><p id="a7e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">样本</strong> —节点中观察值的数量。因为这是根节点，所以15480对应于整个训练数据集。</p><p id="8358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">值</strong> —节点的预测值。换句话说，如果观察路径在此节点停止，则该节点的预测值将为2.074。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="d810" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们创建一个保存所有节点中所有观察值的字典:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="c737" class="nk nl it ng b gy nm nn l no np">n_entries = {<br/>“node 1”: 15480,<br/>“node 2”: 12163,<br/>“node 3”: 3317,<br/>“node 4”: 5869,<br/>“node 5”: 6294,<br/>“node 6”: 2317,<br/>“node 7”: 1000,<br/>“node 8”: 2454,<br/>“node 9”: 3415,<br/>“node 10”: 1372,<br/>“node 11”: 4922,<br/>“node 12”: 958,<br/>“node 13”: 1359,<br/>“node 14”: 423,<br/>“node 15”: 577<br/>}</span></pre><p id="6b1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算特征重要性时，使用的度量之一是观察值落入某个节点的概率。概率是为决策树中的每个节点计算的，计算方法是将节点中的样本数除以数据集中的观察总数(在我们的例子中是15480)。</p><p id="c6ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将字典表示为<strong class="lb iu"> n_entries_weighted </strong>:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="4a60" class="nk nl it ng b gy nm nn l no np">n_entries_weighted = {<br/>'node 1': 1.0,  <br/>'node 2': 0.786,  <br/>'node 3': 0.214,  <br/>'node 4': 0.379,  <br/>'node 5': 0.407,  <br/>'node 6': 0.15,  <br/>'node 7': 0.065,  <br/>'node 8': 0.159,  <br/>'node 9': 0.221,  <br/>'node 10': 0.089,  <br/>'node 11': 0.318,  <br/>'node 12': 0.062,  <br/>'node 13': 0.088,  <br/>'node 14': 0.027,  <br/>'node 15': 0.037<br/>}</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="2dfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使特性重要性的定义具有数学上的严格性，让我们在本文中使用数学符号。</p><p id="f3bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将每个节点表示为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/859b63fd6dbd895cba6a14b1884f2381.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/0*etevXTjQWUC20AIL"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点符号</p></figure><p id="e622" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个节点，直到最终深度，都有一个左子节点和一个右子节点。让我们将它们表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1ed9f4590d56f85d7efda9a7ed883b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/0*-zZSdiwnpGKiGyHS"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左右子符号</p></figure><p id="edb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个节点都有特定的属性。让我们将上一节中计算的重量表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d2469faa8ce79c5ced4f5b0a6e07b2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/0*UbQQIawmpyZiXR-b"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点权重</p></figure><p id="5c70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将均方误差(MSE)统计量表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fd80a6441c2675b1af271f7d811a1474.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/0*EDDx89LG98eIt0aJ"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点均方误差统计</p></figure><p id="888d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有子节点的节点的一个非常重要的属性是所谓的节点重要性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f9e73cda8cbe38ee5d56ec518e9a0b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/0*byiGK_SPiOgbAryw"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点计算的重要性</p></figure><p id="06e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述等式的直觉是，如果子节点中的MSE较小，则该节点尤其是其分裂规则特征的重要性较大。</p><p id="cb3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用每个节点的MSE统计数据创建一个字典:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="603b" class="nk nl it ng b gy nm nn l no np">i_sq = {<br/>“node 1”: 1.335,<br/>“node 2”: 0.832,<br/>“node 3”: 1.214,<br/>“node 4”: 0.546,<br/>“node 5”: 0.834,<br/>“node 6”: 0.893,<br/>“node 7”: 0.776,<br/>“node 8”: 0.648,<br/>“node 9”: 0.385,<br/>“node 10”: 1.287,<br/>“node 11”: 0.516,<br/>“node 12”: 0.989,<br/>“node 13”: 0.536,<br/>“node 14”: 0.773,<br/>“node 15”: 0.432<br/>}</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="96ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者Trevor Hastie、Robert Tibshirani和Jerome Friedman在他们的巨著<strong class="lb iu"><em class="nv">The Elements of Statistical Learning:Data Mining，Inference，and Prediction </em> </strong>中用以下等式定义了特征重要性计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3122c9529cd8df377d22ccc7e2a3453c.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/0*V-NCqb0Ri5VmwNDr"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布雷曼特征重要性方程</p></figure><p id="7d19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><p id="70bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t——是整个决策树</p><p id="9ddb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">l —有问题的特征</p><p id="d8f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">J —决策树中内部节点的数量</p><p id="9a03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">I——用于分割的度量的减少</p><p id="b4aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">II —指示器功能</p><p id="c495" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">v(t)-用于分割节点的特征t用于分割节点</p><p id="10db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个等式背后的直觉是，对树中所有特征的度量的所有减少进行求和。</p><p id="3280" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn使用前面提出的节点重要性公式。主要区别在于，在scikit-learn中，引入了节点权重，即观察值落入树中的概率。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f4e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们放大一点，进一步检查节点1到3。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/413d75928e968e73ad528e84c2b413d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6x630W2XGE7ghq1_TTBrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">前三个节点；按作者分类的图表</p></figure><p id="02ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">节点重要性(以及要素重要性)的计算一次只考虑一个节点。为节点号1解释的后续逻辑适用于以下级别的所有节点。</p><p id="909f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">只有具有分裂规则的节点才有助于特征重要性的计算。</strong></p><p id="5ed0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二个节点是节点1的左子节点，第三个节点是右子节点。</p><p id="b377" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征重要性背后的直觉始于分裂标准中的总体减少的想法。换句话说，我们想要测量给定的特征及其分裂值(尽管该值本身在任何地方都不使用)如何减少系统中的均方误差。上一节中定义的节点重要性等式反映了这种影响。</p><p id="d4b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们在根节点中使用MedInc，将有12163个观察值到达第二个节点，3317个观察值到达右节点。这转化为左节点的权重为0.786 (12163/15480)，右节点的权重为0.214 (3317/15480)。左侧节点中的均方误差等于0.892，右侧节点中的均方误差为1.214。</p><p id="30c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要计算节点重要性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/371cb24ba3c5a2fb60bbfc3b9c162a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/0*6BMWyYi-zQ1pwZWK"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/1f13a3b00473eb62271394057b83d879.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/0*ZOiMo2csQF3mI6wZ"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/5227d3d37673513da9219050b39007a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/0*gv8ifEzg0N_ZQSWo"/></div></figure><p id="40b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以将节点重要性保存到字典中。字典键是在节点的分裂标准中使用的特征。这些值是节点的重要性。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="5a7f" class="nk nl it ng b gy nm nn l no np">{<br/>  "<strong class="ng iu">MedInc</strong>": 0.421252,<br/>  "<strong class="ng iu">HouseAge</strong>": 0,<br/>  "<strong class="ng iu">AveRooms</strong>": 0,<br/>  "<strong class="ng iu">AveBedrms</strong>": 0,<br/>  "<strong class="ng iu">AveOccup</strong>": 0</span><span id="4e77" class="nk nl it ng b gy ob nn l no np">}</span></pre><p id="a6ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">上述计算过程需要对所有具有分裂规则的节点重复进行。</strong></p><p id="eac3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们再做一些节点计算，以完全掌握算法:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/ede40d1047425a7d17a567cbfab15c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVYI1Dijqu0OzElO5BDkFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更多的平方误差减少；按作者分类的图表</p></figure><p id="2734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们在节点2中使用MedInc特性，误差平方为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/942b914ba967bfc399216e20a6a563f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/0*tIez6EUDP5sMLM4F"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第二节点重要性</p></figure><p id="49ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征重要性字典现在变成了:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="a71f" class="nk nl it ng b gy nm nn l no np">{<br/>  "<strong class="ng iu">MedInc</strong>": 0.421 + 0.10758<br/>  "<strong class="ng iu">HouseAge</strong>": 0,<br/>  "<strong class="ng iu">AveRooms</strong>": 0,<br/>  "<strong class="ng iu">AveBedrms</strong>": 0,<br/>  "<strong class="ng iu">AveOccup</strong>": 0</span><span id="f7e6" class="nk nl it ng b gy ob nn l no np">}</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/666df2e75ab2f46e96687a017e56ed9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*zJJhZ_8eZQo9hejwK500_w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">要拆分的新功能；按作者分类的图表</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fb57981c2c590f28fe3d8860e7cbbd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/0*X_paiN7Id9OZXIOi"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点4的重要性</p></figure><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="661f" class="nk nl it ng b gy nm nn l no np">{<br/>  "<strong class="ng iu">MedInc</strong>": 0.558,<br/>  "<strong class="ng iu">HouseAge</strong>": 0,<br/>  "<strong class="ng iu">AveRooms</strong>": 0.018817,<br/>  "<strong class="ng iu">AveBedrms</strong>": 0,<br/>  "<strong class="ng iu">AveOccup</strong>": 0</span><span id="5958" class="nk nl it ng b gy ob nn l no np">}</span></pre><p id="3733" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不能更进一步，因为节点8和9没有分裂规则，因此不会进一步减少均方误差统计。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对上面几节中给出的实现进行编码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于重要性计算的数据</p></figure><p id="f889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们定义一个计算节点重要性的函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点重要性计算函数</p></figure><p id="4cf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将所有这些放在一起:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算节点的代码段</p></figure><p id="479a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述代码片段中的内容如下:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="efad" class="nk nl it ng b gy nm nn l no np">Node importance: {<br/>‘node 1’: 0.421, <br/>‘node 2’: 0.108, <br/>‘node 3’: 0.076, <br/>‘node 4’: 0.019, <br/>‘node 5’: 0.061, <br/>‘node 6’: 0.025, <br/>‘node 7’: 0.013<br/>} </span><span id="b60d" class="nk nl it ng b gy ob nn l no np">Feature importance before normalization: {<br/>‘MedInc’: 0.618, <br/>‘AveRooms’: 0.019, <br/>‘AveOccup’: 0.086, <br/>‘HouseAge’: 0, <br/>‘AveBedrms’: 0<br/>} </span><span id="dc21" class="nk nl it ng b gy ob nn l no np"><strong class="ng iu">Feature importance after normalization: {<br/>‘MedInc’: 0.855, <br/>‘AveRooms’: 0.026, <br/>‘AveOccup’: 0.119, <br/>‘HouseAge’: 0.0, <br/>‘AveBedrms’: 0.0<br/>}</strong></span></pre><p id="adbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">归一化后的最终特征字典是具有最终特征重要性的字典。根据字典，到目前为止最重要的特征是MedInc，其次是ave occupation和AveRooms。</p><p id="04e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征HouseAge和AveBedrms没有在任何分割规则中使用，因此它们的重要性为0。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9d6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将我们的计算与功能重要性计算的scikit-learn实现进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Sklearn特性重要性的实现</p></figure><p id="d2cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面写着:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="3c0d" class="nk nl it ng b gy nm nn l no np">Feature importance by sklearn: <br/>{<br/>‘MedInc’: 0.854, <br/>‘HouseAge’: 0.0, <br/>‘AveRooms’: 0.027, <br/>‘AveBedrms’: 0.0, <br/>‘AveOccup’: 0.12<br/>}</span></pre><p id="c42c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与我们的计算相比:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2e99ad2792d65976f8ecf22e046a0b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_sSj4zB9PaZqRgQrtFE4-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征重要性比较</p></figure><p id="1498" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">差异很小，但这是由于舍入误差。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="0218" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我非常详细地演示了决策树的特性重要性计算。一个非常相似的逻辑适用于分类中使用的决策树。唯一的区别是度量——我们不使用平方误差，而是使用GINI杂质度量(或其他分类评估度量)。所有关于节点重要性的计算保持不变。</p><p id="79dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望在读完所有这些之后，你会对如何解释和计算特性的重要性有一个更清晰的了解。</p><p id="c07f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">编码快乐！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="688c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]作者:<strong class="lb iu">佩斯，r .凯利和罗纳德·巴里</strong> <br/>年份:<strong class="lb iu"> 1997年</strong> <br/>书名:<strong class="lb iu">稀疏空间自回归</strong> <br/>网址:<strong class="lb iu"/><a class="ae ky" href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">http://archive.ics.uci.edu/ml</strong></a><br/>期刊:<strong class="lb iu">统计与概率快报</strong></p><p id="4390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]作者:<strong class="lb iu">特雷弗·哈斯蒂、罗伯特·蒂布拉尼和杰罗姆·弗里德曼</strong> <br/>年份:<strong class="lb iu"> 2017 </strong> <br/>书名:<strong class="lb iu">统计学习的要素:数据挖掘、推断和预测</strong> <br/>网址:<strong class="lb iu"/><a class="ae ky" href="https://hastie.su.domains/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">http://archive.ics.uci.edu/ml</strong></a><br/>页面:<strong class="lb iu">368–370</strong></p></div></div>    
</body>
</html>