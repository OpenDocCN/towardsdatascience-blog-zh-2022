<html>
<head>
<title>Partial dependence plots with Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scikit-learn绘制部分相关性图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/partial-dependence-plots-with-scikit-learn-966ace4864fc#2022-07-04">https://towardsdatascience.com/partial-dependence-plots-with-scikit-learn-966ace4864fc#2022-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7112" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="f83c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">走向可解释的人工智能</h2></div><p id="f76e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">部分相关图(PDP)是深入了解特征和预测之间关系的有用工具。它有助于我们理解特定特征的不同值如何影响模型的预测。在本帖中，我们将学习PDP的基本知识，并熟悉一些使用Scikit-learn绘制PDP的有用方法。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/8538c410497107cc18deede5a80c0430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZgCjNovbtBGT6II6"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">特里·维里斯迪斯的照片</p></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="9d3a" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">📦数据</h1><p id="3ac3" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">在这篇文章中，我们将使用<a class="ae md" href="https://github.com/mwaskom/seaborn-data/blob/master/titanic.csv" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a> ( <em class="ni">该数据可通过Seaborn获得，并持有BSD-3许可证</em>)。让我们导入库并加载数据集。然后，我们将训练一个随机森林模型，并评估其性能。</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="7091" class="no mm it nk b gy np nq l nr ns">import numpy as np<br/>import pandas as pd</span><span id="2f03" class="no mm it nk b gy nt nq l nr ns"># sklearn version: v1.0.1<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import (RandomForestClassifier, <br/>                              AdaBoostClassifier)<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.inspection import (partial_dependence, <br/>                                PartialDependenceDisplay)</span><span id="9c5e" class="no mm it nk b gy nt nq l nr ns">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style='darkgrid', context='talk', palette='Set2')</span><span id="7642" class="no mm it nk b gy nt nq l nr ns">columns = ['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare', <br/>           'adult_male']<br/>df = sns.load_dataset('titanic')[columns].dropna()<br/>X = df.drop(columns='survived')<br/>y = df['survived']<br/>X_train, X_test, y_train, y_test =  train_test_split(<br/>    X, y, random_state=42, test_size=.25<br/>)</span><span id="8713" class="no mm it nk b gy nt nq l nr ns">rf = RandomForestClassifier(random_state=42)<br/>rf.fit(X_train, y_train)</span><span id="3a59" class="no mm it nk b gy nt nq l nr ns">def evaluate(model, X_train, y_train, X_test, y_test):<br/>    name = str(model).split('(')[0]<br/>    print(f"========== {name} ==========")<br/>    y_train_pred = model.predict_proba(X_train)[:,1]<br/>    roc_auc_train = roc_auc_score(y_train, y_train_pred)<br/>    print(f"Train ROC AUC: {roc_auc_train:.4f}")<br/>    <br/>    y_test_pred = model.predict_proba(X_test)[:,1]<br/>    roc_auc_test = roc_auc_score(y_test, y_test_pred)<br/>    print(f"Test ROC AUC: {roc_auc_test:.4f}")<br/>    <br/>evaluate(rf, X_train, y_train, X_test, y_test)</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5ffe161258dd63ec6e103ac14523a007.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*6AUhiHpINwDePCJIijFC_g.png"/></div></figure><p id="243a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，让我们来学习PDP的基础知识。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="d4c4" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">📊部分相关图介绍</h1><p id="b798" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">让我们以PDP为例开始讨论。我们将使用<code class="fe nv nw nx nk b"><a class="ae md" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html" rel="noopener ugc nofollow" target="_blank">PartialDependenceDisplay</a></code>为<code class="fe nv nw nx nk b">pclass</code>绘制一个:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="58b1" class="no mm it nk b gy np nq l nr ns">var = 'pclass'<br/>PartialDependenceDisplay.from_estimator(rf, X_train, [var]);</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/bab15e752055ff12a6e8c4027c12e1ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Olor3-ncQ4pBiTs6c60J_A.png"/></div></figure><blockquote class="nz oa ob"><p id="ceed" class="kr ks ni kt b ku kv kd kw kx ky kg kz oc lb lc ld od lf lg lh oe lj lk ll lm im bi translated">随着特征值的变化，PDP显示了对预测的平均影响。</p></blockquote><p id="6ceb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上图中，纵轴显示预测概率，横轴显示<code class="fe nv nw nx nk b">pclass</code>值。绿线表示平均预测概率随着<code class="fe nv nw nx nk b">pclass</code>值的变化而变化。我们看到，随着乘客等级从1增加到3，平均生存概率降低。</p><p id="3a2a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了更好地理解PDP，让我们简单地看一下如何手动构建先前的情节。我们首先会发现<code class="fe nv nw nx nk b">pclass</code>的独特价值。Then️对于每个唯一值，我们将用它替换训练数据中的<code class="fe nv nw nx nk b">pclass</code>列中的值，并记录预测如何变化。</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="22e6" class="no mm it nk b gy np nq l nr ns">values = X_train[var].sort_values().unique()<br/>print(f"Unique values: {values}")<br/>individual = np.empty((len(X_train), len(values)))<br/>for i, value in enumerate(values):<br/>    X_copy = X_train.copy()<br/>    X_copy[var] = value<br/>    individual[:, i] = rf.predict_proba(X_copy)[:, 1]<br/>individual</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi of"><img src="../Images/cc8bddf8f4a685afdae68e402107a19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*BYf0f8DZpYet_okoJnV1fg.png"/></div></div></figure><p id="f51c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里我们可以看到，如果我们更改<code class="fe nv nw nx nk b">pclass</code>的值，训练数据集中每条记录的个体预测(<em class="ni">也称为个体条件期望，ICE </em>)将如何变化。通过平均这些预测值(<em class="ni">部分相关，PD </em>)，我们得到PDP的输入值。</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="7239" class="no mm it nk b gy np nq l nr ns">individual.mean(axis=0)</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4735496c8d54d4cbc769b717b4ed764c.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*HOJVD5U-5ZPVQeQXbQFjyA.png"/></div></figure><p id="56ad" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过绘制这些值和<code class="fe nv nw nx nk b">pclass</code>的唯一值，我们可以重现PDP。与使用<code class="fe nv nw nx nk b">PartialDependenceDisplay</code>相比，我们自己从原始值中绘图给了我们更多的灵活性和对如何可视化PDP的控制。</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="6aba" class="no mm it nk b gy np nq l nr ns">sns.lineplot(x=values, y=individual.mean(axis=0), style=0, <br/>             markers=True, legend=False)<br/>plt.ylim(0.2,0.6)<br/>plt.ylabel("Partial dependence")<br/>plt.xlabel(var);</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8ea98dfcf3509fa0e265873cac100b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*5sZO4yktOZDsQOWHJTfUHA.png"/></div></figure><p id="2e4d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如上所述的手动计算对于学习和理解概念非常有用，但是，在实际用例中继续使用这种方法是不现实的。在实践中，我们将使用Scikit-learn更高效的<code class="fe nv nw nx nk b">partial_dependence</code>函数来提取原始值。</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="0620" class="no mm it nk b gy np nq l nr ns">raw_values = partial_dependence(rf, X_train, var, kind='both')<br/>raw_values</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/84a6eb6c1249976d6abfceae97d9c1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*DqntFGkkvpSQZk4SMyxSCA.png"/></div></figure><p id="869a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里，我们指定了<code class="fe nv nw nx nk b">kind='both'</code>来查看单个预测以及平均预测。如果我们只是在平均预测之后，我们可以用<code class="fe nv nw nx nk b">kind='average'</code>:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="72a0" class="no mm it nk b gy np nq l nr ns">partial_dependence(rf, X_train, var, kind='average')</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8170c3d6efd72eed3c2c98ad74f33fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*DTcyIfi5G1KoAvWKzj0B6A.png"/></div></figure><p id="cde4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">值得注意的是，<code class="fe nv nw nx nk b">partial_dependence(…, kind='both')</code>和<code class="fe nv nw nx nk b">partial_dependence(…, kind='average')</code>的平均预测可能不总是与一些机器学习算法完全匹配，其中更有效的<code class="fe nv nw nx nk b">recursion</code>方法可用于后者。</p><p id="8c8b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们检查一下我们手动计算的值是否与Scikit-learn的版本相匹配:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="f2d4" class="no mm it nk b gy np nq l nr ns">print(np.array_equal(raw_values['individual'][0], individual))<br/>print(np.isclose(raw_values['average'][0], <br/>                 np.mean(individual, axis=0)))</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d2c0018a7a25692a3a3a2e3bb7e5f796.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*bCSBqU5VYHg42BWtuOW-xg.png"/></div></figure><p id="aca8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">很好，他们很配！</p><p id="d8a0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><code class="fe nv nw nx nk b">PartialDependenceDisplay</code>允许我们绘制单个预测的子集以及平均值，以更好地理解数据:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="e9ce" class="no mm it nk b gy np nq l nr ns">n = 50<br/>PartialDependenceDisplay.from_estimator(<br/>    rf, X_train, ['pclass'], kind="both", n_jobs=3, subsample=n<br/>)<br/>plt.legend(bbox_to_anchor=(1,1));</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/312c26e0e731c09436d270581b1e27d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*eNICz38OZb3r108PZbGkJg.png"/></div></figure><p id="a9f9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这提供了更多的上下文。我们可以从原始值中自己复制一个类似的图表:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="6d3a" class="no mm it nk b gy np nq l nr ns">sns.lineplot(x=values, y=individual.mean(axis=0), style=0, <br/>             markers=True, legend=False)<br/>sns.lineplot(data=pd.DataFrame(individual, columns=values)\<br/>                    .sample(n).reset_index().melt('index'), <br/>             x='variable', y='value', style='index', dashes=False, <br/>             legend=False, alpha=0.1, size=1, color='#63C1A4')<br/>plt.ylabel("Partial dependence")<br/>plt.xlabel(var);</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b003514cdd07f1e8a24c39a21843a50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*8Z8HFQfA0YgnTCP6wKzgng.png"/></div></figure><p id="c812" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于像<code class="fe nv nw nx nk b">pclass</code>这样的离散变量，我们不必局限于线形图，甚至可以使用条形图，因为我们有充分的自由从原始值构建任何图表:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="b274" class="no mm it nk b gy np nq l nr ns">raw_df = pd.DataFrame(raw_values['individual'][0], <br/>                      columns=raw_values['values'])<br/>sns.barplot(data=raw_df.melt(var_name=var), x=var, y='value')<br/>plt.ylabel("Partial dependence");</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi on"><img src="../Images/45c5ddf9d1a475c74cdfda99a71e835a.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*cpRt1E0TCpZNxqhN4Hkz4w.png"/></div></figure><p id="8fbd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们很可能会考虑PDP的多个变量。了解了基础知识之后，让我们来看看绘制多个变量的PDP的几种方法。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="7822" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">📈多个变量的PDP</h1><p id="6064" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">由于我们的玩具数据集有少量的功能，让我们为每个功能绘制PDP。我们将首先使用<code class="fe nv nw nx nk b">PartialDependenceDisplay</code>:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="f301" class="no mm it nk b gy np nq l nr ns">n_cols = 2<br/>n_rows = int(len(X_train.columns)/n_cols)</span><span id="ec79" class="no mm it nk b gy nt nq l nr ns">fig, ax = plt.subplots(n_rows, n_cols, figsize=(10, 12))<br/>PartialDependenceDisplay.from_estimator(rf, X_train, X_train.columns, ax=ax, n_cols=n_cols)<br/>fig.suptitle('Partial Dependence Plots')<br/>fig.tight_layout();</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/8f9640d8d58be4e20bcccc50ff8afa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*yN6sGIXaH5ZptLsRd1vYsw.png"/></div></figure><p id="84a5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从这些图中，我们可以看到特征和预测之间的关系类型。一些关系看起来是线性的，而另一些则更复杂。</p><p id="8604" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，让我们根据用<code class="fe nv nw nx nk b">partial_dependence</code>提取的原始值绘制PDP:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="8f2f" class="no mm it nk b gy np nq l nr ns">fig, ax = plt.subplots(n_rows, n_cols, figsize=(10,12), sharey=True)<br/>for i, x in enumerate(X_train.columns):<br/>    raw_values = partial_dependence(rf, X_train, i, kind='average')<br/>    loc = i//n_cols, i%n_cols<br/>    sns.lineplot(x=raw_values['values'][0], <br/>                 y=raw_values['average'][0], ax=ax[loc], style=0, <br/>                 markers=True, legend=False)<br/>    ax[loc].set_xlabel(x)<br/>    if i%n_cols==0:<br/>        ax[loc].set_ylabel('Partial dependence')<br/>fig.suptitle('Partial Dependence Plots')<br/>fig.tight_layout()</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi op"><img src="../Images/da613f39a98c2ae7e0f05479564c8752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*DWwXSYowGkr6onGBG2HF1A.png"/></div></figure><p id="5c91" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">或者，我们也可以绘制单个预测的子集，为我们提供平均值背后的更多背景信息:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="e035" class="no mm it nk b gy np nq l nr ns">plt.figure(figsize=(10,12))<br/>for i, x in enumerate(X_train.columns):<br/>    raw_values = partial_dependence(rf, X_train, i, kind='both')<br/>    ax = plt.subplot(n_rows, n_cols, i+1)<br/>    sns.lineplot(x=raw_values['values'][0], y=raw_values['average'][0], <br/>                 style=0, markers=True, legend=False, ax=ax)<br/>    sns.lineplot(data=pd.DataFrame(raw_values['individual'][0], <br/>                                   columns=raw_values['values'][0])\<br/>                        .sample(n).reset_index().melt('index'), <br/>                 x='variable', y='value', style='index', dashes=False, <br/>                 legend=False, alpha=0.1, size=1, color='#63C1A4')<br/>    ax.set_xlabel(x)<br/>    ax.set_ylabel('Partial dependence')<br/>plt.suptitle('Partial Dependence Plots')<br/>plt.tight_layout()</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5b2599eeb55514b42ee94e5158ec656a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*MIYJ6dcbp2EQYFQHfyLqxg.png"/></div></figure><p id="8cfb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这些图帮助我们理解特征之间的关系以及它们对目标预测的影响，并检测模型学习的模式是否合理和可解释。PDP还可以用来直观地评估和比较模型。在下一节中，我们将了解如何为多个模型绘制PDP。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="60ec" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">📉多个型号的PDP</h1><p id="29a5" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">让我们再构建两个模型，并提取所有三个模型的原始值:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="2b72" class="no mm it nk b gy np nq l nr ns">pclass_df = pd.DataFrame(columns=values)<br/>pclass_df.loc['rf'] = partial_dependence(<br/>    rf, X_train, var, kind='average'<br/>)['average'][0]</span><span id="0f04" class="no mm it nk b gy nt nq l nr ns">ada = AdaBoostClassifier(random_state=42)<br/>ada.fit(X_train, y_train)<br/>evaluate(ada, X_train, y_train, X_test, y_test)</span><span id="60d3" class="no mm it nk b gy nt nq l nr ns">pclass_df.loc['ada'] = partial_dependence(<br/>    ada, X_train, var, kind='average'<br/>)['average'][0]</span><span id="57e6" class="no mm it nk b gy nt nq l nr ns">knn = KNeighborsClassifier()<br/>knn.fit(X_train, y_train)<br/>evaluate(knn, X_train, y_train, X_test, y_test)<br/>pclass_df.loc['knn'] = partial_dependence(<br/>    knn, X_train, var, kind='average'<br/>)['average'][0]<br/>pclass_df</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c7d9cd4123df66a56937e7a6a9573849.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*V5gRamCivGXGfvWlksY7HQ.png"/></div></figure><p id="7cd1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，我们可以绘制每个模型的部分相关性:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="9af0" class="no mm it nk b gy np nq l nr ns">pclass_df = pclass_df.reset_index().melt('index')<br/>sns.lineplot(data=pclass_df, x='variable', y='value', <br/>             hue='index');<br/>sns.scatterplot(data=pclass_df, x='variable', y='value', <br/>                hue='index', legend=False)<br/>plt.legend(bbox_to_anchor=(1, 1))<br/>plt.ylabel("Partial dependence")<br/>plt.xlabel(var);</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi os"><img src="../Images/e730a610da73270da9b64a2f97ae012c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*vcN8HUKlNUWMJ9EBY17XEg.png"/></div></figure><p id="b00b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于AdaBoost和K-最近邻分类器，预测概率几乎与乘客类别无关。</p><p id="e4c1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们对所有变量进行类似的比较:</p><pre class="lo lp lq lr gt nj nk nl nm aw nn bi"><span id="aa6f" class="no mm it nk b gy np nq l nr ns">summary = {}<br/>fig, ax = plt.subplots(n_rows, n_cols, figsize=(10,12), sharey=True)</span><span id="7a41" class="no mm it nk b gy nt nq l nr ns">for i, x in enumerate(X_train.columns):<br/>    summary[x] = pd.DataFrame(columns=values)<br/>    raw_values = partial_dependence(rf, X_train, x, kind='average')<br/>    summary[x] = pd.DataFrame(columns=raw_values['values'][0])<br/>    summary[x].loc['rf'] = raw_values['average'][0]<br/>    summary[x].loc['ada'] = partial_dependence(<br/>        ada, X_train, x, kind='average'<br/>    )['average'][0]<br/>    summary[x].loc['knn'] = partial_dependence(<br/>        knn, X_train, x, kind='average'<br/>    )['average'][0]<br/>    <br/>    data = summary[x].reset_index().melt('index')<br/>    loc = i//n_cols, i%n_cols<br/>    if i==1:<br/>        sns.lineplot(data=data, x='variable', y='value', <br/>                     hue='index',ax=ax[loc]);<br/>        ax[loc].legend(bbox_to_anchor=(1, 1));<br/>    else: <br/>        sns.lineplot(data=data, x='variable', y='value', <br/>                     hue='index', ax=ax[loc], legend=False);<br/>    sns.scatterplot(data=data, x='variable', y='value', <br/>                    hue='index', ax=ax[loc], legend=False)<br/>    ax[loc].set_xlabel(x)<br/>    if i%n_cols==0:<br/>        ax[loc].set_ylabel('Partial dependence')</span><span id="df2a" class="no mm it nk b gy nt nq l nr ns">fig.suptitle('Partial Dependence Plots')<br/>fig.tight_layout()</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a34c466614e2418904068347a746b590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*1iIInaKhnhxYX_1ghg_kHw.png"/></div></figure><p id="64c3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过不同的模型查看PDP有助于选择一个更合理、更易于解释的模型。</p><p id="2546" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">部分相关图提供了预测如何受特征变化影响的见解。PDP的一个缺点是它假设特性是相互独立的。虽然我们已经看到了分类用例，但是PDP也可以用于回归。在这篇文章中，我们主要关注最简单的PDP形式:单向PDP。对于渴望了解PDP更多信息的学习者来说，能够洞察特性之间交互的双向和/或三向PDP是值得研究的有趣主题。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ou"><img src="../Images/2957b470d3cb8ee044c86d389c4e1ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6RNcTLDvQTQcJ3gj"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">照片由<a class="ae md" href="https://unsplash.com/@callumshaw?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡勒姆·肖</a>在<a class="ae md" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="83b0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ni">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae md" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="ni">我的推荐链接</em></a><em class="ni">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="fd43" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢您阅读这篇文章。如果你感兴趣，这里有我的一些其他帖子的链接:<br/> ◼️️ <a class="ae md" rel="noopener" target="_blank" href="/explaining-scikit-learn-models-with-shap-61daff21b12a">解释scikit-learn models with shap</a><br/>◼️️<a class="ae md" rel="noopener" target="_blank" href="/meet-histgradientboostingclassifier-54a9df60d066?source=your_stories_page-------------------------------------">会见histgradientsboostingclassifier</a><br/>◼️️<a class="ae md" rel="noopener" target="_blank" href="/from-ml-model-to-ml-pipeline-9f95c32c6512?source=your_stories_page-------------------------------------">从ML模型到ML管道</a> <br/> ◼️️ <a class="ae md" rel="noopener" target="_blank" href="/4-simple-tips-for-plotting-multiple-graphs-in-python-38df2112965c">用Python绘制多个图形的4个简单技巧</a> <br/> ◼️ <a class="ae md" rel="noopener" target="_blank" href="/prettifying-pandas-dataframes-75c1a1a6877d">美化熊猫数据帧</a> <br/> ◼ <a class="ae md" rel="noopener" target="_blank" href="/simple-data-visualisations-in-python-that-you-will-find-useful-5e42c92df51e">简单的数据可视化</a></p><p id="ef2f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>