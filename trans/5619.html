<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-9462c15c2a64#2022-12-19">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-9462c15c2a64#2022-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="ir is gp gr it iu gh gi paragraph-image"><div class="ab gu cl iv"><img src="../Images/a42e9b4e9510f434241ac07a2aabc2d6.png" data-original-src="https://miro.medium.com/v2/format:webp/0*g0HJqbQTtFkPy4nh.jpg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated"><a class="ae jc" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="4647" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">第 10 部分:预测</h2></div><p id="8759" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们到了这一系列文章的最后一部分，在这里我将使用 ML 模型对司法判决进行分类。为此，我将使用已经标记的记录作为训练集来执行监督学习任务，以便将模型应用于那些没有标记的记录。理想情况下，这种模型可以极大地帮助在记录到达官员手中时自动对记录进行分类。</p><p id="390c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将深入探究技术:</p><ul class=""><li id="f07c" class="lq lr jf kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">最频繁基线</li><li id="d805" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">逻辑回归</li><li id="761e" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">支持向量分类器</li><li id="938c" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">带有 Keras 的深度神经网络</li></ul><p id="bf64" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们首先导入所需的库:</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="ac38" class="mn mo jf mj b be mp mq l mr ms"># Libraries we will use in this section<br/>from sklearn.metrics import precision_score, classification_report, f1_score<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.feature_selection import SelectKBest, chi2<br/><br/>from sklearn.multiclass import OneVsRestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import LinearSVC<br/><br/>from scipy.sparse import random<br/><br/>import sklearn as skl<br/>skl.warnings.filterwarnings("ignore")</span></pre><p id="713e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了有一个基准来比较我们的模型结果，我将首先使用最频繁的基线作为所有文档的预测，这样我们就有了与之相关的性能指标。这个想法是，如果一个模型的性能比最频繁的基线差，它就不值得。</p><h2 id="b9dd" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">最频繁标签基线</h2><p id="f062" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">在将<em class="nq"> df_factor </em>分成训练集和测试集之前，我将删除没有主题的行。为此，我将创建一个遮罩并将其应用于 df。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9ede" class="mn mo jf mj b be mp mq l mr ms">#I will split first the dataset into a test set (20%) and a temporary set (80%). <br/>#Then, I will split the latter into train set (80% of temporary set) <br/>#and development set (20% of temporary set)<br/><br/>from sklearn.model_selection import train_test_split<br/><br/>tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability<br/>train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability<br/><br/>vectorizer_logit = TfidfVectorizer(ngram_range = (2,6), min_df = 0.001, max_df = 0.75, stop_words = 'english')<br/><br/>X_train = vectorizer_logit.fit_transform(train.Lemmas)<br/><br/>#we cannot refit the vectorizer<br/>X_dev = vectorizer_logit.transform(dev.Lemmas)<br/>X_test = vectorizer_logit.transform(test.Lemmas)<br/><br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)</span></pre><p id="b6ea" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从描述性统计部分，我们知道最常见的类别是“税”。因此，我们将只预测所有观测值的税收作为基线模型。让我们检索该类别的索引。作为一个参考指标，我将主要依靠微观平均 f1 分数，其中微观停留在“计算总的真阳性、假阴性和假阳性。”事实上，如果我们考虑宏观平均指标，我们不会考虑数据不平衡的事实。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="722a" class="mn mo jf mj b be mp mq l mr ms">ind = y_train.columns.get_loc("Tax")<br/>#print(ind)<br/><br/>most_frequent = np.zeros(42)<br/>most_frequent[ind] = 1<br/>#print(most_frequent)<br/><br/>most_frequent_prediction = [most_frequent for i in range(y_dev.shape[0])]<br/>most_frequent_prediction = pd.DataFrame(most_frequent_prediction, columns = y_dev.columns)<br/><br/>print(classification_report(y_dev, most_frequent_prediction, target_names =  y_dev.columns))</span></pre><h2 id="2a4c" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">逻辑回归基线</h2><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9c20" class="mn mo jf mj b be mp mq l mr ms">logit = OneVsRestClassifier(LogisticRegression())<br/>logit.fit(X_train, y_train)</span></pre><p id="6db2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们将交叉验证应用到培训中(如果你想了解更多关于交叉验证的知识，你可以在这里阅读我以前的文章<a class="ae jc" href="https://medium.com/dataseries/cross-validation-for-model-selection-5e843c71553d" rel="noopener"/>):</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="0e5e" class="mn mo jf mj b be mp mq l mr ms"># estimating the (test) F1 score via cross validation<br/>for k in [2, 5, 10]:<br/>    cv = cross_val_score(OneVsRestClassifier(LogisticRegression()), X_train, y = y_train, cv = k, n_jobs = -1, scoring = "f1_micro")<br/>    fold_size = X_train.shape[0]/k<br/>    <br/>    print("F1 with {} folds for bag-of-words is {}".format(k, cv.mean()))<br/>    print("Training on {} instances/fold, testing on {}".format(round(fold_size*(k-1)), round(fold_size)))<br/>    print()</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/af67d31ac69ff9fe6b9fdcb34101729d.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*mzSwlM2cXZicsxRFnAXhCQ.png"/></div></figure><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="3bba" class="mn mo jf mj b be mp mq l mr ms">y_pred_logit_baseline = logit.predict(X_dev)<br/>print(classification_report(y_dev, y_pred_logit_baseline, target_names = y_dev.columns))</span></pre><p id="9884" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管“幼稚”,这个模型在最频繁基线的开发集中表现得更好。我们的微观 F1 得分为 54%,低于 16%。</p><p id="a65d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，假设我们决定使用这个模型。它在测试数据上的表现如何？</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="eea4" class="mn mo jf mj b be mp mq l mr ms">y_pred_logit_baseline_test = logit.predict(X_test)<br/>print(classification_report(y_test, y_pred_logit_baseline_test, target_names = y_test.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/acf5e0d4f00888f9e08a560d7044e253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*P1O93U1glEb25R5NoZxzig.png"/></div></figure><p id="d491" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">正规化强度</em> </strong></p><p id="57ac" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我将改进我的逻辑模型。</p><p id="14ee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每当我们训练一个模型时，我们都必须考虑方差偏差的权衡和过度拟合的风险:事实上，增加参数的数量总是会导致训练误差的减少，但不会导致测试误差的减少，因为(平方)偏差会减少，但方差会增加。因此，在训练模型时，我们需要考虑当我们增加参数数量时增加损失函数的惩罚项。在逻辑回归中，我们使用参数 c。这个想法是，降低 c 将加强𝜆调节器。的确，c 对 1/𝜆.来说是奇怪的因此，𝜆越高，惩罚项越高，模型的参数就越少。</p><p id="741d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注:想了解更多关于正规化的内容，可以在这里阅读我之前的文章<a class="ae jc" href="https://medium.com/dataseries/preventing-overfitting-regularization-5eda7d5753bc" rel="noopener">。</a></p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="69c1" class="mn mo jf mj b be mp mq l mr ms">from sklearn.metrics import f1_score<br/>best_c = None<br/>best_performance = 0.0<br/><br/>for c in [20, 10, 5, 2, 0.5, 0.1, 0.05, 0.01]:<br/>    print(c)<br/>    classifier_c = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                             class_weight='balanced',<br/>                                             C=c<br/>                                     ))<br/>    classifier_c.fit(X_train, y_train)<br/>    predictions_c = classifier_c.predict(X_dev)<br/>    score = f1_score(y_dev, predictions_c, average='micro')<br/>    if score &gt; best_performance:<br/>        best_performance = score<br/>        best_c = c<br/>        print("New best performance: {}".format(score))<br/>        <br/>    #print(classification_report(y_dev, predictions_c, target_names = y_dev.columns))<br/>    </span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1c44ed91cdf60b911f10d4746a1e35f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*Wtt7_012zm3So6QIajmB3A.png"/></div></figure><p id="1883" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从上面的结果，我们可以说，最好的模型输出等于 81%的微 f1 分数。让我们看看它在测试集上的表现:</p><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/88ae29b2f2a99a78bfcdd129dc65522b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*3Nd3kfVMSTfJ4qDgP82H4A.png"/></div></figure><p id="b51f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">功能选择</em> </strong></p><p id="a8c0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们把特征的数量减少到 4500 个。我会把这个模型和之前 c 的最佳值的结果结合起来。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7527" class="mn mo jf mj b be mp mq l mr ms">from sklearn.feature_selection import SelectKBest, chi2<br/><br/>selector = SelectKBest(chi2, k=4500).fit(X_train, y_train)<br/><br/>X_train_sel = selector.transform(X_train)<br/>X_dev_sel = selector.transform(X_dev)<br/>X_test_sel = selector.transform(X_test)<br/><br/>classifier_sel = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                    class_weight='balanced', C = best_c))<br/>classifier_sel.fit(X_train_sel, y_train)<br/><br/>predictions_sel = classifier_sel.predict(X_dev_sel)<br/>print(classification_report(y_dev, predictions_sel, target_names = y_dev.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4965c03d1e4715eb0d09d9a470eef668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*KFOyqX4-33teeetglSOIVg.png"/></div></figure><p id="8424" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">看来特征选择并没有带来提升(现在微 f1 评分更低，72%)。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="4151" class="mn mo jf mj b be mp mq l mr ms">#let's also evaluate the model in the test set<br/>predictions_sel_test = classifier_sel.predict(X_test_sel)<br/>print(classification_report(y_test, predictions_sel_test, target_names = y_test.columns))</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/affcff6b6828ded145515b3ddc8c1adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*5RTd3W49oe1ZAN2qWZsXgA.png"/></div></figure><p id="ec93" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg"> <em class="nq">降维</em> </strong></p><p id="76f2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，我们不在特征中进行选择，而是创建新的低维特征，作为原始特征的线性组合。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="8834" class="mn mo jf mj b be mp mq l mr ms">from sklearn.decomposition import TruncatedSVD<br/><br/>best_performance=0.0<br/>best_k = None<br/><br/>for k in [300, 500, 1000, 1500,2000, 2500]:<br/>    print(k)<br/>    svd = TruncatedSVD(n_components=k)<br/><br/>    X_train_dim = svd.fit_transform(X_train_sel)<br/>    X_dev_dim = svd.transform(X_dev_sel)<br/>    X_test_dim = svd.transform(X_test_sel)<br/><br/>    classifier_dim = OneVsRestClassifier(LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', <br/>                                        class_weight='balanced', C = best_c)) #still including the parameter c<br/>    classifier_dim.fit(X_train_dim, y_train)<br/>    predictions_dim = classifier_dim.predict(X_dev_dim)<br/>    score = f1_score(y_dev, predictions_dim, average='micro')<br/>    if score &gt; best_performance:<br/>        best_performance = score<br/>        best_k = k<br/>        print("New best performance: {}".format(score))<br/><br/>    #print(classification_report(y_dev, predictions_dim, target_names = y_dev.columns))<br/>    print()</span></pre><p id="0e2b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它导致模型的最佳性能等于 72%。</p><h2 id="b3da" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">支持向量分类器</h2><p id="d640" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">对于我的第二个分类模型，我决定使用一个支持向量分类器(一对一对全部)，像以前一样使用调优参数 C。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="953a" class="mn mo jf mj b be mp mq l mr ms">svc_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df = 0.25, stop_words = 'english')<br/><br/>tmp, test = train_test_split(df_factor_label, test_size=0.2, random_state=123) #for replicability<br/>train, dev = train_test_split(tmp, test_size=0.2, random_state=123) #for replicability<br/><br/>X_train_svc = svc_vectorizer.fit_transform(train.text)<br/>X_dev_svc = svc_vectorizer.transform(dev.text)<br/>X_test_svc = svc_vectorizer.transform(test.text)<br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_dev = dev.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/><br/>best_c = None #same ratio as above<br/>best_f1_score = 0.0<br/>for c in [5, 1.0, 0.5]:<br/>    print(c)<br/>    svc_clf = OneVsRestClassifier(LinearSVC(C = c, class_weight = 'balanced')).fit(X_train_svc, y_train)<br/>    cv_reg = cross_val_score(svc_clf, X_train_svc, y = y_train, cv = 5, n_jobs = -1, scoring = "f1_micro")<br/><br/>    new_predictions_regularized = svc_clf.predict(X_dev_svc)<br/>    f1 = f1_score(y_dev, new_predictions_regularized, average='micro')<br/>    print("5-CV on train at C={}: {}".format(c, cv_reg.mean()))<br/>    print(classification_report(y_dev, new_predictions_regularized, target_names = y_dev.columns))<br/>    print()    <br/>    if f1 &gt; best_f1_score:<br/>        best_f1_score = f1<br/>        best_c = c<br/>        #print("New best performance: {}".format(f1))</span></pre><p id="3306" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这种情况下，最佳性能是 C=1 的那个，微 f1=0.84。</p><p id="9387" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，让我们回顾一下所有上述模型的指标(在开发集中):</p><figure class="me mf mg mh gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/af4984cf13c9c57166dbec4348ac2146.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*1FJAR1ziyQD8u5fGfO6FVQ.png"/></div></div></figure><p id="c1fe" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，在所有模型中，我假设最好的是 SVC:这是我要与两个基线进行比较的模型。但是在进入引导部分之前，让我们看看它在测试集上的性能。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7be6" class="mn mo jf mj b be mp mq l mr ms">lsvc = OneVsRestClassifier(LinearSVC(C = best_c, class_weight = "balanced"))<br/>lsvc.fit(X_train_svc, y_train)<br/><br/>y_pred_svc = lsvc.predict(X_test_svc)<br/>print(classification_report(y_test, y_pred_svc, target_names = y_test.columns))<br/><br/>#finally, let's store the dev predictions to be used in the next section.<br/>best_preds = lsvc.predict(X_dev_svc)</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e1ee306f2b5916ae1d1af183f678b7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*u0e44y1AhxKNPvN88j4Mog.png"/></div></figure><h2 id="a784" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">Keras 神经网络</h2><p id="9ac7" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">Keras 是一个开源软件库，为 ann(人工神经网络)提供了 Python 接口。它可以运行在 TensorFlow、微软认知工具包、Theano 或 PlaidML 之上。</p><p id="2205" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Keras 的开发是为了让研究人员和开发人员更容易原型化、构建和实验深度学习模型。它具有用户友好的界面，允许您轻松创建和训练各种类型的深度学习模型，包括卷积神经网络(CNN)、递归神经网络(RNNs)和长短期记忆(LSTM)网络。</p><p id="5ecf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Keras 的设计是灵活和模块化的，因此您可以轻松地配置、组合和微调您创建的模型。它还具有各种预训练的模型，可以用于一系列任务，如图像分类、自然语言处理和时间序列预测。</p><p id="a7a4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为此，我将使用 Keras 构建一个深度神经网络来预测与司法判决相关的标签。</p><p id="a45a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了简单起见，我将只考虑那些具有单个标签的记录，这样任务将归结为一个多类任务，而不是多类、多标签任务。</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="21bb" class="mn mo jf mj b be mp mq l mr ms">import tensorflow as tf<br/>from keras.models import Sequential<br/>from keras.layers import InputLayer, Input<br/>from keras.layers import Reshape, MaxPooling2D<br/>from keras.layers import Conv2D, Dense, Flatten, Activation, Dropout<br/>from keras.optimizers import SGD, RMSprop, Adagrad #want to try different optimizers<br/><br/>#downloading df<br/>df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/><br/>#creating a df containing only records with one label<br/>m=[len(df_factor.category[i])==1 for i in range(len(df_factor))]<br/>df_factor_single_label=df_factor[m]<br/><br/><br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>train, test = train_test_split(df_factor_single_label, test_size=0.2, random_state = 123) #here, we don't need a dev set<br/>                                                                                        #since it can be specified direclty<br/>                                                                                        #during training<br/>#for this purpose, I will use the TfIdf vecotrizer.<br/><br/>vectorizer_nn = TfidfVectorizer(ngram_range = (1, 2), min_df = 0.001, max_df = 0.25, stop_words = 'english')<br/><br/>#let's also store the full dataset into a X_nn variable, so that we will be able to plot the training history.<br/><br/>X_nn = vectorizer_nn.fit_transform(df_factor_single_label.text)<br/>X_train_nn = vectorizer_nn.fit_transform(train.text)<br/>X_test_nn = vectorizer_nn.transform(test.text)<br/><br/>y_train = train.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y_test = test.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)<br/>y = df_factor_single_label.drop(["titles", "date", "text", "category", "component", "Tokens", "Lemmas", "Stems"], axis = 1)</span></pre><p id="2e28" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们初始化模型:</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="1c94" class="mn mo jf mj b be mp mq l mr ms">model = Sequential()<br/>model.add(Dense(3000, activation='relu', input_dim = X_train_nn.shape[1]))<br/>model.add(Dropout(0.1))<br/>model.add(Dense(600, activation='relu'))<br/>model.add(Dropout(0.1))<br/>model.add(Dense(200, activation='relu'))<br/>model.add(Dense(y_train.shape[1], activation='softmax'))<br/><br/>sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)<br/>#rms = RMSprop(learning_rate=0.001, rho=0.9)<br/>#adam = Adagrad(learning_rate=0.01)<br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer='adam',<br/>              metrics=['accuracy'])<br/><br/>history = model.fit(X_train_nn, y_train, epochs = 5, batch_size = 100, verbose = 1, validation_split=0.2)<br/><br/>#score = model.evaluate(X_test_nn, y_test, batch_size = 100)<br/>#score</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi od"><img src="../Images/353e20bcfb4d79b235ed698622985376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rsbWlp-pexRSKqLgb-tm0Q.png"/></div></div></figure><p id="4eb2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们最终得到了一个验证/开发准确率为 91.23%的模型，还不错！</p><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="9d74" class="mn mo jf mj b be mp mq l mr ms">from keras.models import load_model<br/><br/>model.save('Keras Models/NN_labels.h5')  # creates a HDF5 file 'NN_labels.h5'<br/><br/>%matplotlib inline<br/>import pandas as pd<br/>import seaborn<br/><br/>df = pd.DataFrame(history.history)<br/>df[['val_accuracy', 'accuracy']].plot.line()<br/>df[['val_loss', 'loss']].plot.line()</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/56f0d5971583d872fd678fd8ff0ae6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*4gRIk5qaRJWsRjdg"/></div></figure><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c53c1141454977c9c9299882bfdf6b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*2-WqQTQTzOnyj32D"/></div></figure><pre class="me mf mg mh gt mi mj mk bn ml mm bi"><span id="7bfc" class="mn mo jf mj b be mp mq l mr ms">#downloading model<br/>from keras.models import load_model<br/><br/>model = load_model('Keras Models/NN_labels.h5')<br/><br/>#using it to predict on new, never-seen-before data.<br/><br/>loss, accuracy = model.evaluate(X_test_nn, y_test, batch_size = 100)<br/><br/>print("test loss: ", loss)<br/>print("test accuracy: ", accuracy)</span></pre><figure class="me mf mg mh gt iu gh gi paragraph-image"><div class="gh gi og"><img src="../Images/47769b24b32f83087e8a220bdf180210.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*6R9EEWzOD_-AgFw2PO65MQ.png"/></div></figure><p id="e63f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">测试精度相当令人满意(91.3%)，因此我相信这个模型能够正确地标记新文章。</p><p id="8a4e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有了这个模型，我有信心在新文章被插入到数据库中时自动标记它们。这可能是实现能够解决原始任务(即多类、多标签分类问题)的解决方案的起点。这个模型的一个可能的改进是:训练一组不同的分类器(可以是具有不同结构的神经网络，或者不同的模型，如逻辑回归、SVC 等等)。).然后使用它们来预测文章的测试序列，并且每当模型输出不同的标签时，将它们都归因于该观察/文本。也就是说，可以使用逻辑回归、SVC 和 NN 来预测新文章标签，如果它们都返回不同的结果，则将所有三个标签归属于该文章。</p><h2 id="e57b" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">结论</h2><p id="5b5a" class="pw-post-body-paragraph ku kv jf kw b kx nl kg kz la nm kj lc ld nn lf lg lh no lj lk ll np ln lo lp im bi translated">我们到了关于司法判决的 NLP 系列文章的最后一部分。如果尽管我写的东西很无聊，你还是设法来了😃谢谢大家！这对我太重要了。</p><p id="2920" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我总是感谢任何建设性的反馈，所以请随时通过 Medium 或 Linkedin 联系我。</p><p id="bbf4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下一篇文章再见！</p><h2 id="0253" class="mt mo jf bd mu mv mw dn mx my mz dp na ld nb nc nd lh ne nf ng ll nh ni nj nk bi translated">参考</h2><ul class=""><li id="cb7d" class="lq lr jf kw b kx nl la nm ld oh lh oi ll oj lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="aea8" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">Python 中的 spaCy 工业级自然语言处理</a></li><li id="8579" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="1ac4" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009–2018 年新闻发布| Kaggle </a></li><li id="4d3e" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://spacy.io/usage/linguistic-features#named-entities" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/linguistic-features#named-entities</a></li><li id="1fab" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated"><a class="ae jc" href="https://medium.com/p/d81bdfa14d97/edit" rel="noopener">https://medium.com/p/d81bdfa14d97/edit</a></li><li id="2a71" class="lq lr jf kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">https://www.nltk.org/api/nltk.sentiment.vader.html<a class="ae jc" href="https://www.nltk.org/api/nltk.sentiment.vader.html" rel="noopener ugc nofollow" target="_blank"/></li></ul></div></div>    
</body>
</html>