<html>
<head>
<title>How to augment ADAS on your Car using Pi?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Pi增强汽车上的ADAS？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adas-collision-avoidance-system-on-indian-cars-bac64cc8a863#2022-08-17">https://towardsdatascience.com/adas-collision-avoidance-system-on-indian-cars-bac64cc8a863#2022-08-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4421" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">启用防撞系统(CAS) &amp;使用Pi在您的汽车上实现智能环绕视野。这种低成本的解决方案，即T2使用的技术，如激光雷达-相机低空传感器融合技术，非常适合发展中国家。</em></h2></div><p id="10f9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">虽然印度只占1%的车辆，<a class="ae lc" href="https://timesofindia.indiatimes.com/city/pune/pune-students-create-ai-powered-driverless-car-to-help-curb-accidents-deaths/articleshow/85454149.cms" rel="noopener ugc nofollow" target="_blank">世界银行的调查</a>报告<strong class="ki ir">全球</strong><strong class="ki ir"/><strong class="ki ir">道路死亡的11%发生在印度！</strong>在发展中国家，加强道路安全势在必行，在这些国家，大多数车辆没有先进的驾驶辅助功能，他们也负担不起为提高安全性而升级汽车的费用。此外，这些国家提出了一系列独特的挑战。这些包括过时的车辆，缺乏人行车道，混乱的交通，动物过马路，等等。</p><p id="0d8f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这种背景下，<strong class="ki ir">该解决方案旨在为印度最便宜的汽车配备</strong> <strong class="ki ir">超便宜的ADAS级，即防撞和智能全景。</strong>配有前方碰撞预警系统(FCW)或自主紧急制动系统(AEB)的现代汽车价格昂贵，但<strong class="ki ir">我们可以以较低的成本在旧车上增加这些功能。</strong></p><h1 id="8d92" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">项目演示</h1><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">ADAS-CAS小工具通过声光指示器警告驾驶员</p></figure><p id="0af3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">此解决方案的完整源代码可在</strong> <a class="ae lc" href="https://github.com/AdroitAnandAI/ADAS-Collision-Avoidance-System-on-Indian-Roads" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">此处</strong> </a>获得</p><p id="0590" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这个想法是使用电池供电的Pi连接到安装在汽车引擎盖上的激光雷达、Pi Cam、LED SHIM和NCS 2，以感知前方物体的深度和方向。这个<strong class="ki ir">不仅启用了</strong> <strong class="ki ir">前向碰撞预警系统，还启用了</strong> <strong class="ki ir">智能驾驶辅助，对交通标志</strong> <strong class="ki ir">或行人、沿路边行走</strong>、<strong class="ki ir">或过马路</strong>发出警报。</p><p id="462b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">激光雷达使用激光束根据反射时间计算距离。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/77ddb6e3cb4e3655f6aad4c60308fc6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/0*VceTgg1iSpyC1oaB"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">激光测距[5]。c =光速</p></figure><p id="b762" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">相机的分辨率通常比激光雷达高，但相机的FOV有限，无法估计距离。旋转<strong class="ki ir">激光雷达</strong>有<strong class="ki ir"> 360 </strong>视场，<strong class="ki ir"> Pi Cam </strong>只有<strong class="ki ir"> 62x48度</strong> <strong class="ki ir">水平x垂直视场。</strong>当我们在这里处理多个传感器时，我们需要使用<strong class="ki ir">视觉融合技术</strong>来整合传感器输出，即获得车辆前方障碍物的距离和角度。在动手实施之前，我们先讨论一下传感器融合的理论基础。</p><h1 id="e19a" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">传感器融合的思想</h1><p id="46dd" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">每种传感器都有自己的优缺点。以雷达为例，它的分辨率很低，但却擅长在没有视线的情况下进行测量。在自动驾驶汽车中，通常使用激光雷达、雷达和摄像头的组合来感知环境。这样，我们可以通过结合所有传感器的优点来弥补缺点。</p><ul class=""><li id="abbb" class="mo mp iq ki b kj kk km kn kp mq kt mr kx ms lb mt mu mv mw bi translated"><strong class="ki ir">相机:</strong>非常适合理解场景或<strong class="ki ir">对物体进行分类</strong></li><li id="641f" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">激光雷达</strong>:优于<strong class="ki ir">利用<strong class="ki ir">脉冲激光波</strong>估计距离</strong></li><li id="7abe" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">雷达:</strong>可以利用<strong class="ki ir">多普勒效应</strong>测量<strong class="ki ir">障碍物的速度</strong></li></ul><p id="615a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">该相机是一个2D传感器，通过它可以识别边界框、交通灯、车道划分等特征。激光雷达是一种输出一组点云的3D传感器。<strong class="ki ir">融合技术找到激光雷达探测到的点和<strong class="ki ir">摄像机</strong>探测到的</strong>点之间的对应关系。为了统一使用激光雷达和相机来构建ADAS，<strong class="ki ir">需要通过以下步骤将3D传感器输出与2D传感器输出</strong>融合。</p><p id="0dd3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">1.<strong class="ki ir">将激光雷达点云</strong> (3D)投影到2D图像上。</p><p id="4935" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">2.<strong class="ki ir">使用YOLOv4等算法进行物体检测</strong>。</p><p id="218f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">3.<strong class="ki ir">匹配ROI </strong>找到感兴趣的激光雷达投影点。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/3870b5b04af73d8b8699319ccdc55024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ESOjdMVynknEl1f1"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">激光雷达-相机传感器融合[2]</p></figure><p id="f433" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">通过以上3个步骤，<strong class="ki ir">周围的</strong> <strong class="ki ir">物体将</strong><strong class="ki ir"/><strong class="ki ir">使用激光雷达-相机融合进行测量和分类。</strong></p><h1 id="099b" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">激光雷达-相机传感器融合考虑</h1><p id="98dc" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">当来自cam的原始图像与来自雷达或激光雷达的原始数据融合时，这被称为<strong class="ki ir">低级融合或早期融合</strong>。在后期融合中，检测在融合之前完成。请注意，在2D影像上投影3D激光雷达点云存在许多挑战。在执行融合时，必须考虑两个传感器之间的<strong class="ki ir">相对方位和平移</strong>。</p><ul class=""><li id="90ef" class="mo mp iq ki b kj kk km kn kp mq kt mr kx ms lb mt mu mv mw bi translated"><strong class="ki ir">旋转:</strong>激光雷达和相机的<strong class="ki ir">坐标系可以不同</strong>。激光雷达上的距离可能在z轴上，而相机上的距离在x轴上。<strong class="ki ir">我们需要在激光雷达点云上应用旋转</strong>以使坐标系相同，即<strong class="ki ir">将每个激光雷达点乘以</strong> <strong class="ki ir">旋转矩阵。</strong></li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3d43eb4364adc8ba317a9de151461c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/0*O4xDfHDrHnNUvS2s"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">我们需要旋转，使激光雷达和相机坐标系相同</p></figure><ul class=""><li id="d2ed" class="mo mp iq ki b kj kk km kn kp mq kt mr kx ms lb mt mu mv mw bi translated"><strong class="ki ir">翻译:</strong>在自动驾驶汽车中，激光雷达可以位于中央顶部，摄像头位于两侧。每个装置中激光雷达和摄像机的<strong class="ki ir">位置可以不同。</strong>基于相对传感器位置，<strong class="ki ir">通过乘以平移矩阵平移激光雷达点。</strong></li><li id="00f7" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">立体校正:</strong>对于立体摄像机设置，我们<strong class="ki ir">需要</strong> <strong class="ki ir">做立体校正</strong> <strong class="ki ir">使左右图像共面。</strong>因此，我们需要乘以矩阵R0来沿着<strong class="ki ir">水平核线</strong>对齐所有东西。</li><li id="8ac2" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><strong class="ki ir">内在校准:</strong>校准是告诉你的相机如何将3D世界中的点转换成像素的步骤。为此，我们需要乘以包含工厂校准值的固有校准矩阵。</li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a8c4ecf47547f607ba876ae21409bbee.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/0*LbLhvskvSX9H_YyD"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">内在校准矩阵。f =焦距。c =光学中心</p></figure><p id="24df" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">综上所述，<strong class="ki ir">我们需要将激光雷达点乘以所有的4个矩阵</strong>才能投影到相机图像上。</p><p id="d0f7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">将3D中的点X投影到2D的点Y上，</strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8e6f503aefd2b523d61371142c4a2b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*25An4T6wJnVOyvXi"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">激光雷达-相机投影公式</p></figure><ul class=""><li id="e612" class="mo mp iq ki b kj kk km kn kp mq kt mr kx ms lb mt mu mv mw bi translated"><em class="nk"> P =摄像机内部校准矩阵</em></li><li id="ab54" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><em class="nk"> R0 =立体校正矩阵</em></li><li id="0b8a" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><em class="nk"> R|t =旋转&amp;从激光雷达到摄像机的平移</em></li><li id="d7f5" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><em class="nk"> X =三维空间中的点</em></li><li id="02ef" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated"><em class="nk">Y = 2D图像中的点</em></li></ul><p id="5403" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">请注意，我们将刚体变换、旋转和平移组合在一个矩阵R|t中。将这三个矩阵P、R0和R|t放在一起，说明<strong class="ki ir">外部和内部校准</strong>将激光雷达点投影到相机图像上。然而，<strong class="ki ir">矩阵值高度依赖于我们定制的传感器安装。</strong></p><p id="5978" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这只是拼图的一部分。我们的目标是为任何廉价汽车增加一个<strong class="ki ir">端到端防撞系统</strong> <strong class="ki ir">和智能全景。</strong>这将包括我们选择的传感器、传感器位置、数据采集、自定义视觉融合和物体检测，以及一个数据分析节点，以实现传感器间的同步，从而触发驾驶员辅助警告以避免危险。</p><h1 id="21b6" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">RPi和RPLIDAR A1的实际实施</h1><p id="89d7" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">首先，我们需要用RPLIDAR A1、Pi Cam、LED垫片和NCS 2组装Pi。<strong class="ki ir"> 2D激光雷达被用来代替3D激光雷达</strong>，因为我们的目标是使这个小玩意<strong class="ki ir">最便宜</strong>成为可能。该装置由5V 3A 10K毫安电池供电。为了便于组装，<strong class="ki ir">激光雷达支架是3D打印的</strong>并附在RPi上。挂载设计的一部分取自从<a class="ae lc" href="https://www.thingiverse.com/thing:3970110" rel="noopener ugc nofollow" target="_blank">这里</a>获得的STL文件</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/04b54bd4f884c6a67910a7ce2353f59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/0*1GPW6ZQ9VwjRBaHi"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">STL可视化:Raspberry Pi的激光雷达挂载</p></figure><p id="50d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">使用微型USB电缆将RPLIDAR A1与连接到Pi USB的USB适配器连接。激光雷达的适配器提供电源，并将激光雷达的内部UART串行接口转换为USB接口。使用Aux-to-Aux电缆将RPi连接到扬声器。由于物理限制，使用LED垫片代替Blinkt来发出警告信息。虽然<strong class="ki ir">ADAS小工具的总成本约为150-200美元，</strong>人们可能需要至少再支付<strong class="ki ir">10-20000美元，</strong> <strong class="ki ir">才能获得具有如此先进功能的汽车模型。</strong></p><p id="8daf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">让我们想象一下，3D激光雷达的连接方式与上述相同。首先，我们将尝试在上面的小工具上解决<strong class="ki ir"> 3D激光雷达-相机传感器融合</strong>。然后我们将看到2D激光雷达-相机融合的<strong class="ki ir">变化，以便使其在RPLIDAR A1上工作。</strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8d134ae28896a37649480fd1aad9ce86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/1*7dpMRwe3-l9usFtUfX3aEw.gif"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">安装在汽车引擎盖上的ADAS设备</p></figure><h1 id="7bcd" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">3D激光雷达-相机传感器融合</h1><p id="dfae" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">从上面的讨论中可以清楚地看到，我们需要进行<strong class="ki ir">旋转、平移、立体校正</strong>、<strong class="ki ir">和内部校准</strong>来将激光雷达点投影到图像上。我们将基于我们构建的自定义小工具尝试应用上述公式。</p><p id="9d72" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">从上面的图像中，您可以估计Pi Cam位于激光雷达扫描平面下方10 mm处。即沿着3D轴的[0，-10，0] 的<strong class="ki ir">平移。考虑将威力登HDL-64E作为我们的3D激光雷达，其<strong class="ki ir">需要180°旋转</strong>以将坐标系与Pi Cam对齐。<strong class="ki ir">我们现在可以计算</strong> <strong class="ki ir"> R|t矩阵</strong>了。</strong></p><p id="21e2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">当我们在这里使用一个<strong class="ki ir">单目摄像机</strong>时，立体校正矩阵将是一个<strong class="ki ir">单位矩阵。</strong>我们可以根据V2 Pi Cam的<strong class="ki ir">硬件规格制作内部校准矩阵。</strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/276cd5d715c2ee9eeb3d456087f3f217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qvSNqruW5v5Q6qVZ"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">我们可以输入“n”个点，而不是1个点</p></figure><p id="c974" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">为</strong><strong class="ki ir">树莓派V2相机，</strong></p><ul class=""><li id="2275" class="mo mp iq ki b kj kk km kn kp mq kt mr kx ms lb mt mu mv mw bi translated">焦距(FL) = 3.04毫米</li><li id="5740" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated">FL像素=焦距* sx，其中sx =真实世界与像素的比率</li><li id="11e3" class="mo mp iq ki b kj mx km my kp mz kt na kx nb lb mt mu mv mw bi translated">焦距* sx = 3.04mm毫米* (1/ 0.00112毫米每像素)= 2714.3像素</li></ul><p id="2bab" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">由于<strong class="ki ir">形状不匹配，矩阵无法相乘</strong>。为了使它工作，我们<strong class="ki ir">需要通过添加0和1作为最后一行或一列来从欧几里得坐标转换到齐次坐标</strong>。完成乘法运算后，我们需要将<strong class="ki ir">转换回齐次</strong>坐标。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/67b049a749416b6d90f78aca8cf2a0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*Xz6I9CeOJ4rkPeBW"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">欧几里德到齐次的相互转换</p></figure><p id="59dd" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在3D点云上应用投影公式后，可以看到<strong class="ki ir"> 3D LIDAR-CAM传感器融合投影输出</strong>。从360威力登HDL-64E和摄像机下载<strong class="ki ir">输入传感器数据</strong>并输入。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="0820" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">然而，<strong class="ki ir"> 3D激光雷达的成本是</strong>构建廉价解决方案的一个障碍。我们可以使用便宜的2D激光雷达，并做必要的调整，因为它只能扫描一条水平线。</p><h1 id="5bc2" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">2D激光雷达-相机传感器融合</h1><p id="fc7e" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">我们的小工具配备了2D反相激光雷达A1，以最大限度地降低成本。这台激光雷达<strong class="ki ir">在2D平面上扫描环境，与相机平面</strong>正交。旋转扫描将对从0°<strong class="ki ir"/>到360°<strong class="ki ir"/>的每个角度估计到障碍物的距离。由于激光雷达w.r.t. Pi Cam在小工具中的位置，<strong class="ki ir">相机在激光雷达几何图形上处于+90°</strong>。但是请注意Pi凸轮V2的视野在水平x垂直方向分别为<strong class="ki ir">62 x48</strong>。</p><p id="e8a2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">设备的集成前视图如下所示。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c87b05e1af50a1a7d0794a8bf8c744f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/0*PrvxImbHPRjGWkge"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">激光雷达-照相机几何学</p></figure><p id="5032" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">由于激光雷达和相机传感器数据都可以在<strong class="ki ir">正面62弧</strong>中获得，我们需要融合这些数据。在激光雷达扫描平面中，相机数据从<strong class="ki ir"> </strong> +59到+59<strong class="ki ir"/>+62<strong class="ki ir"/>=<strong class="ki ir">121。</strong>我们可以<strong class="ki ir">在图像上运行对象检测</strong>来获得感兴趣对象的边界框。<strong class="ki ir">例如</strong>:人、汽车、自行车、红绿灯等。由于2D激光雷达只有宽度信息，因此只考虑每个边界框的x_min和x_max。</p><p id="8676" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们需要<strong class="ki ir">计算对应于图像像素的激光雷达角度</strong>，以便估计到像素的距离。为了找到到边界框内物体的距离，<strong class="ki ir">使用下面的公式计算对应于x_min &amp; x_max的θ_min和θ_max </strong>，<strong class="ki ir">基于上图，</strong></p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4a8e22d63e43a111c5b2052f5108904d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*YifxMan5ieGRVLOX"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">w =图像宽度。对θ_max也应用相同的公式</p></figure><p id="59a0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，您可以根据最新的激光雷达扫描数据找到θ_min和θ_max 之间每个角度的<strong class="ki ir">距离。然后<strong class="ki ir">计算对着物体边界框</strong>的所有激光雷达点的中间距离</strong> <strong class="ki ir">以估计物体深度。如果距离低于阈值，则基于角度触发警告。如果在后续帧中长方体中心移动了很大距离，则重复警告。</strong></p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="nq mb l"/></div></figure><p id="1c63" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了构建无阻塞的系统流程，采用了模块化架构，其中，<strong class="ki ir">每个独立节点依赖于不同的硬件组件。</strong>即<strong class="ki ir">“物体检测”节点使用Movidius </strong>进行推断，而<strong class="ki ir">“距离估计”节点将LIDAR数据</strong>作为输入，而<strong class="ki ir">“警报”模块向Pimoroni Blinkt和</strong> <strong class="ki ir">扬声器发送信号。</strong>模块通过MQTT 消息就各自的主题进行<strong class="ki ir">交流。</strong></p><h1 id="6fd8" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">架构图</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/00acadb926fe2d8a6bfb68b582a63a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/0*5B3NKQqn2XDYl_pc"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">3个独立的MQTT节点，每个都链接到不同的硬件，运行在Pi上</p></figure><p id="c934" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">时间同步</strong>模块负责传感器融合的<strong class="ki ir">“数据相关性因子”。</strong>对于ADAS,“节点1”检测到的物体的位置可能会随着物体的移动而改变。因此，<strong class="ki ir">到</strong><strong class="ki ir"/><strong class="ki ir">边界框的距离估计可能会在2-3秒</strong>后出错(而消息可能会保留在MQTT队列中)。为了同步，<strong class="ki ir">当前时间= 60 *分钟+秒</strong>被附加到消息中(忽略滞后的消息)。</p><p id="7a54" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">模型输出从节点1发送到<strong class="ki ir">节点2，在此发生LiDAR-Cam传感器融合，</strong>进一步将消息推送到节点3。为了使系统正常运行，<strong class="ki ir">3个MQTT节点应该协同工作，由MQTT消息</strong>进行编排，发布和订阅各自的主题。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="nq mb l"/></div></figure><p id="66d6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这篇文章的顶部，你可以看到这个小工具<strong class="ki ir">行驶在印度的道路上，在感知到周围的物体以及它们的深度和方向后，给出驾驶员辅助警报。扬声器内部连线</strong>或集成蓝牙扬声器，使驾驶员更容易听到通知。</p><p id="f366" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">这个ADAS设备可以连接到can总线</strong>上，让它加速、转向或刹车。RPi没有内置的CAN总线，但<strong class="ki ir">它的GPIO包括SPI总线，它受到许多CAN控制器</strong>如MCP2515的支持。因此，通过将该设备连接到can总线，可以实现自动紧急制动(AEB)和防撞系统(CAS)。</p><h1 id="0cd7" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">适应印度的条件</h1><p id="7411" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">印度的交通难题如此独特，以至于需要定制的解决方案。首先，我们需要<strong class="ki ir">用印度的交通工具训练物体检测模型，比如卡车、tempos、货车、汽车、人力车等等。</strong></p><p id="e040" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">此外，为了增强智能环绕视图，我们<strong class="ki ir">需要用印度交通标志和标牌</strong>训练 <strong class="ki ir">型号</strong> <strong class="ki ir">，以便在印度道路上给出更有意义的驾驶员辅助警告。这在印度是常见的景象，像牛、猪、水牛、山羊、狗等动物。</strong>，穿越道路和高速公路。因此，检测它们也是有益的。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/286466084554b5aed8000c30c127a845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*2z588a45tRhy8sWWxRmjGw.gif"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">YOLO能够对行走在印度道路上的奶牛进行分类</p></figure><p id="a530" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于概念验证，请参见<strong class="ki ir"> SSD-Mobilenet模型的输出，该模型被训练用于根据印度招牌对印度交通标志进行分类。</strong>你可以进一步对交通标志进行分类，以破译标志的确切含义。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1a20f0f825ec4f2db50acef47a28fc76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/1*uTM4MRPxm0aFlrZ5pR_1JA.gif"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">SSD-MobileNet模型能够对印度交通标志(黄色Bbox)和标志牌(绿色Bbox)进行分类</p></figure><p id="5b96" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="nk">带注释的印度交通标志数据集由印度Datacluster实验室提供。他们还没有完成</em>“<em class="nk">印度车辆”数据库的注释。</em><strong class="ki ir"><em class="nk">只是训练时间问题</em> </strong> <em class="nk">制造这个小玩意，为印度量身定做。</em></p><p id="e267" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">为了从图像中找出ROI，我们使用了<strong class="ki ir"> SSD MobileNet </strong>对COCO进行了训练，过滤了潜在的对象。为了只检测人和车辆，你可以使用<a class="ae lc" href="https://github.com/openvinotoolkit/training_extensions/tree/develop/models/object_detection/model_templates/person-vehicle-bike-detection/person-vehicle-bike-detection-2000" rel="noopener ugc nofollow" target="_blank">这种</a>模式来获得更好的速度和准确性。</p><p id="7430" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">更重要的是，<strong class="ki ir">可以用你定制的带注释的数据训练一个对象检测模型，用OpenVINO </strong>做硬件优化，部署在Pi上，只要这些层受OpenVINO支持。通过这样做，我们可以在RPi上部署一个对象检测模型来定位定制对象。</p><p id="2225" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">1.首先，<strong class="ki ir">选择针对低功耗硬件的高效对象检测模型</strong>，如<strong class="ki ir"> Efficientdet、SSD-Mobilenet、Tiny-YOLO、YOLOX </strong>等。<strong class="ki ir">我在RPi 4B和SSD-Mobilenet上对所有提到的模型进行了实验</strong> <strong class="ki ir">获得了最高的FPS。</strong></p><p id="81d3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">2.<strong class="ki ir">使用您的自定义数据进行对象检测模型<strong class="ki ir">的迁移学习</strong>。</strong></p><p id="df65" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">3.<strong class="ki ir">转换</strong>经过培训的*。pb文件<strong class="ki ir">到中间表示</strong> — *。xml和*。使用<strong class="ki ir">模型优化器的bin。</strong></p><blockquote class="nt nu nv"><p id="2903" class="kg kh nk ki b kj kk jr kl km kn ju ko nw kq kr ks nx ku kv kw ny ky kz la lb ij bi translated">export PATH = "<omz_dir>/deployment _ tools/inference _ engine/demos/common/python/:$ PATH "</omz_dir></p><p id="1482" class="kg kh nk ki b kj kk jr kl km kn ju ko nw kq kr ks nx ku kv kw ny ky kz la lb ij bi translated">python 3<strong class="ki ir">&lt;OMZ _目录&gt;/部署_工具/模型_优化器/mo_tf.py </strong> —输入_模型&lt;冻结_图形. pb &gt; —反向_输入_通道—输出_目录&lt;输出_目录&gt;—tensor flow _ object _ detection _ API _ pipeline _ config&lt;到SSD _ mobilenet _ v2 _ coco . config&gt;的位置—tensor flow _ use _ custom _ operations _ config【T61</p><p id="e6f5" class="kg kh nk ki b kj kk jr kl km kn ju ko nw kq kr ks nx ku kv kw ny ky kz la lb ij bi translated">python 3 object _ detection _ demo . py-d CPU-I<input_video>—labels labels . txt-m<location of="" frozen_inference_graph.xml="">—在ssd</location></input_video></p></blockquote><p id="1b5c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">4.最后，<strong class="ki ir">在Pi上部署硬件优化模型</strong>。</p><p id="a9fe" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">此解决方案的完整源代码可在</strong> <a class="ae lc" href="https://github.com/AdroitAnandAI/ADAS-Collision-Avoidance-System-on-Indian-Roads" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir">此处</strong> </a>获得</p><p id="ac3b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="nk">如有任何疑问或建议，可在此</em> </strong> 联系我  <a class="ae lc" href="https://www.linkedin.com/in/ananduthaman/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir"> <em class="nk"/></strong></a></p><h1 id="1ce1" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">参考</h1><p id="cc45" class="pw-post-body-paragraph kg kh iq ki b kj mj jr kl km mk ju ko kp ml kr ks kt mm kv kw kx mn kz la lb ij bi translated">1.<strong class="ki ir">激光雷达-相机传感器融合高水平:【https://www.thinkautonomous.ai/blog/?】T43</strong>p =自动驾驶汽车中的激光雷达和摄像头传感器融合</p><p id="e575" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">2.<strong class="ki ir">Adafruit的激光雷达数据扫描码存根:</strong><a class="ae lc" href="https://learn.adafruit.com/remote-iot-environmental-sensor/code" rel="noopener ugc nofollow" target="_blank">https://learn . Adafruit . com/remote-IOT-environmental-sensor/code</a></p><p id="e4b7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">3.<strong class="ki ir">摄像机标定和内禀矩阵估计:</strong><a class="ae lc" href="https://www.cc.gatech.edu/classes/AY2016/cs4476_fall/results/proj3/html/agartia3/index.html" rel="noopener ugc nofollow" target="_blank">https://www . cc . gatech . edu/classes/ay 2016/cs 4476 _ fall/results/proj 3/html/agartia 3/index . html</a></p><p id="08b7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">4.<strong class="ki ir">自动驾驶汽车的视觉融合</strong>PyImageSearch大学的课程:【https://www.pyimagesearch.com/pyimagesearch-university/ T2】</p><p id="6ca0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">5.【https://en.wikipedia.org/wiki/Lidar】激光雷达距离估计:<a class="ae lc" href="https://en.wikipedia.org/wiki/Lidar" rel="noopener ugc nofollow" target="_blank"/></p><p id="9deb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">6.<strong class="ki ir"> RPLIDAR A1 M8硬件规格:</strong><a class="ae lc" href="https://www.generationrobots.com/media/rplidar-a1m8-360-degree-laser-scanner-development-kit-datasheet-1.pdf" rel="noopener ugc nofollow" target="_blank">https://www . generation robots . com/media/RP lidar-A1 M8-360度激光扫描仪-开发-套件-数据表-1.pdf </a></p><p id="ff95" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">7.<strong class="ki ir">模型训练，数据清洗&amp;增强:</strong><a class="ae lc" href="http://www.roboflow.com" rel="noopener ugc nofollow" target="_blank">www.roboflow.com</a></p><p id="85f5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 8。</strong>印度交通标志模型已经使用由印度<strong class="ki ir">数据集群实验室提供的<strong class="ki ir">交通数据集</strong>进行了训练。</strong></p></div></div>    
</body>
</html>