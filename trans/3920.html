<html>
<head>
<title>Dealing with Outliers Using Three Robust Linear Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用三种稳健线性回归模型处理异常值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dealing-with-outliers-using-three-robust-linear-regression-models-544cfbd00767#2022-08-31">https://towardsdatascience.com/dealing-with-outliers-using-three-robust-linear-regression-models-544cfbd00767#2022-08-31</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><figure class="iu iv gp gr iw ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi it"><img src="../Images/214217788f536657612a72c828ec968e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hB8ZK8URMh0H00Ha.jpeg"/></div></div><p class="je jf gj gh gi jg jh bd b be z dk translated">照片由<a class="ae ji" href="https://unsplash.com/@rgaleria?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jj">里卡多·戈麦斯天使</strong> </a> <strong class="bd jj">上</strong> <a class="ae ji" href="https://unsplash.com/s/photos/stand-out?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jj">下</strong> </a></p></figure><div class=""/><div class=""><h2 id="1ec4" class="pw-subtitle-paragraph kj jl jm bd b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la dk translated">通过使用Huber、RANSAC和Theil-Sen回归算法的实际例子</h2></div><p id="0189" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">线性回归是最简单的机器学习模型之一。它通常不仅是学习数据科学的起点，也是构建快速简单的最小可行产品(MVP)的起点，这些产品随后将作为更复杂算法的基准。一般来说，线性回归拟合一条线(在二维中)或一个超平面(在三维和更多维中),它们最好地描述了特征和目标值之间的线性关系。</p><p id="7008" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">异常值是位于预期分布之外很远的值。它们导致特性的分布不太好。因此，该模型可能会偏向异常值，正如我们已经确定的，这些异常值远离观察值的中心。自然地，这导致线性回归发现更差和更有偏差的拟合，具有较差的预测性能。</p><p id="5261" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">重要的是要记住，异常值可以在要素和目标变量中找到，并且所有场景都可能使模型的性能恶化。</p><p id="99c4" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">处理异常值有许多可能的方法:从观测值中去除异常值，处理异常值(例如，将极端观测值限制在一个合理的值)，或者使用非常适合处理这些值的算法。这篇文章主要关注这些健壮的方法。</p><h1 id="d2d4" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">设置</h1><p id="ff43" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">我们使用相当标准的库:<code class="fe mu mv mw mx b">numpy</code>、<code class="fe mu mv mw mx b">pandas</code>、<code class="fe mu mv mw mx b">scikit-learn</code>。我们在这里工作的所有模型都是从<code class="fe mu mv mw mx b">scikit-learn</code>的<code class="fe mu mv mw mx b">linear_model</code>模块导入的。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="b804" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">数据</h1><p id="d64c" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">假设目标是显示不同的稳健算法如何处理异常值，第一步是创建一个定制的数据集，以清楚地显示行为的差异。为此，我们使用了<code class="fe mu mv mw mx b">scikit-learn</code>中的可用功能。</p><p id="a1f7" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们首先创建一个包含500个观察值的数据集，其中有一个信息丰富的特征。只有一个特征和目标，我们绘制数据，以及模型的拟合。此外，我们指定噪声(应用于输出的标准偏差)并创建一个包含基础线性模型系数的列表，也就是说，如果线性回归模型适合生成的数据，系数将是多少。在这个例子中，系数的值是64.6。我们为所有模型提取这些系数，然后用它们来比较它们与数据的拟合程度。</p><p id="9207" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">接下来，我们将前25个观察值(5%的观察值)替换为离群值，远远超出生成的大量观察值。请记住，先前存储的系数来自没有异常值的数据。包括他们会有所不同。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/6097d419aeac72fe5fdb77868203d829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*Gfe6tvK02j6jKo9D.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图1</p></figure><h1 id="cab4" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">线性回归</h1><p id="6cb7" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">我们从好的旧线性回归模型开始，它很可能受到异常值的影响。我们使用以下示例来拟合模型和数据:</p><pre class="my mz na nb gt nf mx ng nh aw ni bi"><span id="d27a" class="nj ly jm mx b gy nk nl l nm nn">lr = LinearRegression().fit(X, y)<br/>coef_list.append(["linear_regression", lr.coef_[0]])</span></pre><p id="d159" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，我们准备一个对象用于绘制模型的拟合。<code class="fe mu mv mw mx b">plotline_X</code>对象是一个2D数组，包含由生成的数据集指定的间隔内均匀分布的值。我们使用这个对象来获取模型的拟合值。它必须是一个2D数组，因为它是<code class="fe mu mv mw mx b">scikit-learn</code>中模型的预期输入。然后，我们创建一个<code class="fe mu mv mw mx b">fit_df</code>数据框架，在其中存储拟合值，通过将模型拟合到均匀分布的值来创建。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="e48e" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">准备好数据框架后，我们绘制线性回归模型与异常值数据的拟合图。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="5e03" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">图2显示了异常值对线性回归模型的重大影响。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5358e188b329813128952aae1abf9528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*d135uxRcQKv1JVc8.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图2</p></figure><p id="4db5" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">使用线性回归获得基准模型。现在是时候转向稳健的回归算法了。</p><h1 id="0bf3" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">胡伯回归</h1><p id="4e3e" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated"><a class="ae ji" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html" rel="noopener ugc nofollow" target="_blank">休伯回归</a>是稳健回归算法的一个例子，该算法对被识别为异常值的观察值分配较少的权重。为此，它在优化例程中使用了Huber损失。Huber回归最小化以下损失函数:</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi no"><img src="../Images/0c96330af6d688f570dbac822816c8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tf-5Jh0B7oaTqtpEWKB-gQ.png"/></div></div></figure><p id="abde" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">其中σ表示标准偏差，X_i表示特征集，y_i是回归的目标变量，ω <em class="np"> </em>是估计系数的向量，α是正则化参数。该公式还表明，根据Huber损失，异常值的处理方式不同于常规观测值:</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi nq"><img src="../Images/2393c3d80af634abfce0b8f07b723033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fq8XZHJjZVdiyqICKbXP0Q.png"/></div></div></figure><p id="66bb" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">Huber损失通过考虑由z表示的残差来识别异常值。如果观察值被认为是规则的(因为残差的绝对值小于某个阈值𝜖)，则我们应用平方损失函数。否则，观察值被认为是异常值，我们应用绝对损失。话虽如此，Huber损失基本上是平方损失函数和绝对损失函数的组合。</p><p id="7b15" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">好奇的读者可能会注意到，第一个方程类似于<a class="ae ji" href="https://en.wikipedia.org/wiki/Ridge_regression" rel="noopener ugc nofollow" target="_blank">岭回归</a>，也就是说，包括L2正则化。Huber回归和岭回归的区别在于对异常值的处理。</p><p id="ce21" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通过分析两个流行的回归评估指标之间的差异，您可能会认识到这种损失函数方法:均方误差(MSE)和平均绝对误差(MAE)。与Huber损失的含义相似，建议在处理异常值时使用MAE，因为它不会像平方损失那样严重地惩罚那些观察值。</p><p id="50bc" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">与前一点相关的事实是，优化平方损失会导致均值附近的无偏估计量，而绝对差会导致中位数附近的无偏估计量。对于异常值，中位数比平均数更稳健，所以我们期望这能提供一个更少偏差的估计。</p><p id="8b6e" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们对𝜖使用默认值1.35，这决定了回归对异常值的敏感度。Huber (2004)表明，当误差遵循σ = 1的正态分布时，相对于OLS回归，𝜖 = 1.35产生95%的效率。对于您自己的用例，我建议使用网格搜索之类的方法来调优超参数<code class="fe mu mv mw mx b">alpha</code>和<code class="fe mu mv mw mx b">epsilon</code>。</p><p id="9490" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">然后，我们使用以下示例对数据进行Huber回归拟合:</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="75f2" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">图3显示了拟合模型的最佳拟合线。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div role="button" tabindex="0" class="iy iz di ja bf jb"><div class="gh gi nr"><img src="../Images/953106b9a9195ccfbee90afb91c35a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*61Lhuye4yQ8_RLB_.png"/></div></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图3</p></figure><h1 id="9a2b" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">RANSAC回归</h1><p id="2b61" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated"><a class="ae ji" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html" rel="noopener ugc nofollow" target="_blank">随机样本一致性(RANSAC)回归</a>是一种非确定性算法，它试图将训练数据分为内点(可能会受到噪声的影响)和离群点。然后，它只使用内联器来估计最终的模型。</p><p id="8414" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">RANSAC是一种迭代算法，其中迭代由以下步骤组成:</p><ol class=""><li id="c8af" class="ns nt jm ld b le lf lh li lk nu lo nv ls nw lw nx ny nz oa bi translated">从初始数据集中选择一个随机子集。</li><li id="de9a" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw nx ny nz oa bi translated">使模型适合所选的随机子集。默认情况下，该模型是线性回归模型；但是，我们可以将其更改为其他回归模型。</li><li id="0938" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw nx ny nz oa bi translated">使用估计模型计算初始数据集中所有数据点的残差。绝对残差小于或等于所选阈值的所有观测值都被视为内点，并创建所谓的共识集。默认情况下，阈值被定义为目标值的中值绝对偏差(MAD)。</li><li id="1408" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw nx ny nz oa bi translated">如果足够多的点已经被分类为共识集的一部分，则拟合的模型被保存为最佳模型。如果当前估计的模型与当前最佳模型具有相同数量的内联器，则只有当它具有更好的分数时才被认为是更好的。</li></ol><p id="d23a" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这些步骤迭代执行最大次数，或者直到满足特定的停止标准。这些标准可以使用三个专用的超参数来设置。正如我们前面提到的，最终的模型是使用所有的内层样本来估计的。</p><p id="7e76" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在下面的代码片段中，我们用RANSAC回归模型来拟合数据。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="a61a" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">正如您所看到的，恢复系数的过程有点复杂，因为我们首先使用<code class="fe mu mv mw mx b">estimator_</code>访问模型的最终估计器(使用所有识别的内联器训练的那个)。由于它是一个<code class="fe mu mv mw mx b">LinearRegression</code>对象，我们继续像前面一样恢复系数。然后，我们绘制RANSAC回归拟合图(图4)。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/8d9cd86fc5383a76f3fdaa702b10efff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*MRNyKX_iHsJCGJfQ.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图4</p></figure><p id="cee2" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">使用RANSAC回归，我们还可以检查模型认为是内部值和异常值的观察值。首先，我们检查模型总共识别出多少异常值，然后检查手动引入的异常值中有多少与模型的决策重叠。训练数据的前25个观察值都是已经引入的异常值。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0a9d" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">运行该示例将打印以下摘要:</p><pre class="my mz na nb gt nf mx ng nh aw ni bi"><span id="7262" class="nj ly jm mx b gy nk nl l nm nn">Total outliers: 51<br/>Outliers you added yourself: 25 / 25</span></pre><p id="a508" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">大约10%的数据被确定为异常值，所有引入的观察值都被正确地归类为异常值。这样，我们可以快速地将内标值与外标值进行比较，以查看标记为外标值的其余26个观察值。</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="a61b" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">图5显示了距离原始数据的假设最佳拟合线最远的观察值被认为是异常值。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/8ef5ca81ee804a3a1d9d3ffd37f8ffb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*FrlOYvbiAdmx8hxj.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图5</p></figure><h1 id="1de5" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">泰尔-森回归</h1><p id="950b" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">在<code class="fe mu mv mw mx b">scikit-learn</code>可用的最后一种稳健回归算法是<a class="ae ji" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html" rel="noopener ugc nofollow" target="_blank">泰尔森回归</a>。它是一种非参数回归方法，这意味着它对基础数据分布不做任何假设。简而言之，它包括在训练数据的子集上拟合多重回归模型，然后在最后一步聚合系数。</p><p id="9a87" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">算法是这样工作的。首先，它计算从训练集x中的所有观察值创建的大小为<em class="np"> p </em>(超参数<code class="fe mu mv mw mx b">n_subsamples</code>)的子集的最小二乘解(斜率和截距)，如果我们计算截距(它是可选的)，则必须满足以下条件:<code class="fe mu mv mw mx b">p &gt;= n_features + 1</code>。线的最终斜率(可能还有截距)被定义为所有最小二乘解的(空间)中值。</p><p id="9220" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">该算法的一个可能的缺点是其计算复杂性，因为它可以认为最小二乘解的总数等于<code class="fe mu mv mw mx b">n_samples choose n_subsamples</code>，其中<code class="fe mu mv mw mx b">n_samples</code>是<em class="np"> X </em>中的观测值的数量。考虑到这个数字会迅速膨胀，我们可以做一些事情:</p><ul class=""><li id="4b70" class="ns nt jm ld b le lf lh li lk nu lo nv ls nw lw og ny nz oa bi translated">仅将该算法用于样本和特征数量方面的小问题。然而，由于显而易见的原因，这并不总是可行的。</li><li id="2b98" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">调整<code class="fe mu mv mw mx b">n_subsamples</code>超参数。较低的值以较低的效率为代价导致对异常值的较高鲁棒性，而较高的值导致较低的鲁棒性和较高的效率。</li><li id="11fb" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">使用<code class="fe mu mv mw mx b">max_subpopulation</code>超参数。如果<code class="fe mu mv mw mx b">n_samples choose n_subsamples</code>的总值大于<code class="fe mu mv mw mx b">max_subpopulation</code>，该算法只考虑给定最大尺寸的随机子群体。自然地，只使用所有可能组合的随机子集会导致算法失去一些数学特性。</li></ul><p id="9e4e" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">此外，我们应该意识到，随着问题的维数增加，估计量的稳健性会迅速下降。为了了解这在实践中是如何实现的，我们使用下面的代码片段来估计Theil-Sen回归:</p><figure class="my mz na nb gt ix"><div class="bz fp l di"><div class="nc nd l"/></div></figure><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/01a1aefb2ad89525caf260bdb9357be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*Wgu1ti3-KBzYC7dA.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图6</p></figure><h1 id="08e8" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">模型的比较</h1><p id="a7be" class="pw-post-body-paragraph lb lc jm ld b le mp kn lg lh mq kq lj lk mr lm ln lo ms lq lr ls mt lu lv lw io bi translated">到目前为止，我们已经对包含异常值的数据拟合了三种稳健的回归算法，并且我们已经确定了各个最佳拟合线。现在是比较的时候了。</p><p id="2786" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们从图7的目视检查开始。为了不显示太多的线条，我们没有打印原始数据的拟合线。然而，给定大多数数据点的方向，很容易想象它看起来像什么。显然，RANSAC和Theil-Sen回归得到了最准确的最佳拟合线。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/5410f27fd21dd79ef27aa7c4fb092d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*NVA8y2KGycHCOjjH.png"/></div><p class="je jf gj gh gi jg jh bd b be z dk translated">图7</p></figure><p id="f2c1" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">更准确地说，我们看估计的系数。下表显示RANSAC回归的结果与原始数据最接近。观察5%的异常值对常规线性回归拟合的影响有多大也很有趣。</p><figure class="my mz na nb gt ix gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ff285b8852365230b93f1ad1dd73425a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*NDWcVh6r0SjelVn3wQCNFQ.png"/></div></figure><p id="0e8b" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">你可能会问哪个稳健回归算法最好？通常情况下，答案是:“视情况而定。”以下是一些指导原则，可能有助于您针对具体问题找到合适的模型:</p><ul class=""><li id="be4d" class="ns nt jm ld b le lf lh li lk nu lo nv ls nw lw og ny nz oa bi translated">一般来说，在高维设置中的稳健拟合是困难的。</li><li id="754d" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">与Theil-Sen和RANSAC不同，Huber回归没有试图完全过滤掉异常值。相反，它减少了他们对适合的影响。</li><li id="474a" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">Huber回归应该比RANSAC和Theil-Sen更快，因为后者适合数据的较小子集。</li><li id="733c" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">Theil-Sen和RANSAC不太可能像使用默认超参数的Huber回归那样稳健。</li><li id="414e" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">RANSAC比Theil-Sen更快，并且随着样本数量的增加，它的伸缩性更好。</li><li id="ff2a" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">RANSAC应该更好地处理y方向上的大异常值，这是最常见的情况。</li></ul><p id="7ba5" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">考虑到上述所有信息，您还可以根据经验试验所有三种稳健的回归算法，看看哪一种最适合您的数据。</p><p id="bf5c" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">你可以在我的<a class="ae ji" href="https://github.com/erykml/nvidia_articles/blob/main/robust_regression.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到这篇文章中使用的代码。一如既往，我们非常欢迎任何建设性的反馈。你可以在<a class="ae ji" href="https://twitter.com/erykml1?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">推特</a>或评论中联系我。</p><p id="d5cb" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><em class="np">喜欢这篇文章？成为一个媒介成员，通过无限制的阅读继续学习。如果你使用</em> <a class="ae ji" href="https://eryk-lewinson.medium.com/membership" rel="noopener"> <em class="np">这个链接</em> </a> <em class="np">成为会员，你就支持我，不需要你额外付费。提前感谢，再见！</em></p><p id="721e" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">您可能还会对以下内容感兴趣:</p><div class="iu iv gp gr iw oi"><a rel="noopener follow" target="_blank" href="/r-shiny-is-coming-to-python-1653bbe231ac"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jn gy z fp on fr fs oo fu fw jl bi translated">R Shiny要来Python了</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">Shiny正在加入Streamlit和Dash等网络应用工具的行列</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow jc oi"/></div></div></a></div><div class="iu iv gp gr iw oi"><a rel="noopener follow" target="_blank" href="/three-approaches-to-feature-engineering-for-time-series-2123069567be"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jn gy z fp on fr fs oo fu fw jl bi translated">时间序列特征工程的三种方法</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">使用虚拟变量、循环编码和径向基函数</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow jc oi"/></div></div></a></div><div class="iu iv gp gr iw oi"><a href="https://medium.com/geekculture/investigating-the-effects-of-resampling-imbalanced-datasets-with-data-validation-techniques-f4ca3c8b2b94" rel="noopener follow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jn gy z fp on fr fs oo fu fw jl bi translated">利用数据验证技术研究不平衡数据集重采样的效果</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">了解流行的重采样方法对处理类别不平衡的影响</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">medium.com</p></div></div><div class="or l"><div class="oy l ot ou ov or ow jc oi"/></div></div></a></div><h1 id="9867" class="lx ly jm bd lz ma mb mc md me mf mg mh ks mi kt mj kv mk kw ml ky mm kz mn mo bi translated">参考</h1><ul class=""><li id="b023" class="ns nt jm ld b le mp lh mq lk oz lo pa ls pb lw og ny nz oa bi translated">菲施勒，硕士和博尔斯，R. C. (1981年)。随机样本一致性:模型拟合范例及其在图像分析和自动制图中的应用。<em class="np">ACM的通信</em>，<em class="np"> 24 </em> (6)，381–395。</li><li id="88bf" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">Peter J. Huber，Elvezio M. Ronchetti，稳健统计伴随规模估计，第172页</li><li id="485e" class="ns nt jm ld b le ob lh oc lk od lo oe ls of lw og ny nz oa bi translated">胡伯，P. J. (1992年)。位置参数的稳健估计。在<em class="np">统计学的突破</em>(第492–518页)。纽约州纽约市斯普林格</li></ul></div><div class="ab cl pc pd hz pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="io ip iq ir is"><p id="5b57" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">所有图片，除非特别注明，均为作者所有。</p><p id="d540" class="pw-post-body-paragraph lb lc jm ld b le lf kn lg lh li kq lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated"><em class="np">最初发表于</em> <a class="ae ji" href="https://developer.nvidia.com/blog/dealing-with-outliers-using-three-robust-linear-regression-models/" rel="noopener ugc nofollow" target="_blank"> <em class="np"> NVIDIA的开发者博客</em></a><em class="np">2022年7月20日</em></p></div></div>    
</body>
</html>