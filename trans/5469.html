<html>
<head>
<title>An Intuitive Comparison of MCMC and Variational Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MCMC 与变分推理的直观比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-comparison-of-mcmc-and-variational-inference-8122c4bf37b#2022-12-08">https://towardsdatascience.com/an-intuitive-comparison-of-mcmc-and-variational-inference-8122c4bf37b#2022-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="26b8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">估计未观测变量的两种巧妙方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/05ddd8ec99bcbfa71a121da08d7ba0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g_ZbwVgUBfbGsX2G"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Yannis Papanastasopoulos 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="e0d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近开始研究“<a class="ae kv" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" rel="noopener ugc nofollow" target="_blank">概率编程和黑客贝叶斯方法</a>”，这在我的任务清单上已经有很长时间了。作为一个参加过统计学和机器学习课程(包括贝叶斯统计)的人，我发现我正在通过这种编码优先的方法理解一些以前从来不清楚的事情。我强烈推荐这本书！</p><p id="e824" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些慷慨的人更新了代码以使用 TensorFlow 概率库，这是我在工作中经常使用的。因此，我的大脑终于开始通过反向传播在贝叶斯潜在变量分析和机器学习之间建立联系。我正在将马尔可夫链蒙特卡罗(MCMC)的世界与神经网络联系起来。回想起来，这似乎是我应该早点明白的事情，但是，好吧。如果你和我一样，脑子里没有这种联系，让我们现在就解决它。</p><p id="c238" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">贝叶斯潜在变量分析</strong></p><p id="8be7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">“潜变量”=隐藏；没有被直接观察或者测量。</p><p id="51ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，“分析”是建立概率模型来表示数据生成过程的过程。一个常见的例子是开关点分析。假设我们有一段时间间隔内的统计数据(例如，一个世纪以来每年发生的<a class="ae kv" href="https://www.tensorflow.org/probability/examples/Bayesian_Switchpoint_Analysis" rel="noopener ugc nofollow" target="_blank">起矿难</a>)。我们想估计新的安全条例实施的年份。我们预计事故率将在这一点后下降，之前给出一个事故率，之后给出一个不同的(希望更低的)事故率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/42975de8fadc0b4be6100a961456373a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rr9T3eEqfV6fLOGy"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矿难发生率变化的例子。图片作者。</p></figure><p id="54dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是与神经网络的第一个联系:我们用来拟合数据的模型在某种程度上是任意的。“任意”可能是一个强烈的词——你可以说模型结构是合理选择的，带有一些艺术许可。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/4f7b0d048e7f171ef15728ae05a3b0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vL7kpTJHlgw-jnoK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卡通模特。图片作者。</p></figure><p id="cbfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们观察的是计数数据，因此泊松分布对于生成我们观察的数据是有意义的。但是，实际上，有两个泊松分布，或者至少有两个利率参数——一个在转换点之前，一个在转换点之后。然后是切换点本身，它是我们数据范围内的一个时间值。</p><p id="cff9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是像任何建模一样，这是一个任意的模型结构。我们希望它足够简单，具有有用的表达能力，但仅此而已。这非常类似于聚类(想想<a class="ae kv" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> K-means </a>或者<a class="ae kv" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> EM 算法</a>，所以你可以想象添加更多的开关点(即更多的聚类)。你可以像对待线性函数而不是固定比率一样对待之前和之后的事故率，把它变成一个在转换点斜率发生变化的回归问题。使这个模型变得复杂或丰富的选择是无穷无尽的。神经网络也是如此。选择正确的模型架构和大小是一个反复试验的问题，艺术许可，避免过度拟合，但允许足够丰富的模型表达。</p><p id="bbb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一个相似性，我们有我们想要估计的隐藏变量。从某种意义上来说，我们希望为数据“拟合一个模型”。我们希望选择最大化观察数据可能性的参数。我们有一个模型，允许我们计算<em class="lu"> p(X|Z) </em>(给定一些参数<em class="lu"> Z </em>，数据<em class="lu"> X </em>的概率)，我们想要<em class="lu"> p(Z|X) </em>(也就是“后验概率”)。这可以用一种经典的统计方法来完成，即受人喜爱的 MCMC。等效地，可以在称为变分推理的过程中使用梯度下降来学习参数。我们将从较高的层面来讨论这两个问题。</p><p id="bc21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> MCMC:采样(非“拟合”)模型参数</strong></p><p id="4169" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MCMC 是一种抽样方法。对于从潜在(未观察到的)模型参数的分布中取样，这是一个非常聪明的算法。重要的是，它不是一种估计参数化分布的方法。MCMC 只是生成参数样本，然后您可以将其绘制成直方图，并以分布图的形式查看。但是你没有“拟合”数据。您正在对可能生成数据的模型参数进行采样。</p><p id="d843" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种工作方式非常巧妙。更多细节，请查看 RitVikMath 的视频或阅读<a class="ae kv" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="noopener ugc nofollow" target="_blank">大都会算法</a>。你从产生一些随机的候选人开始。一般来说，这不是完全随机的，尽管它可能是。通常你是使用先验分布来生成这些值的(现在是贝叶斯)。例如，为了成为有效的泊松参数，速率参数必须是非负的，并且它们应该生成看起来类似于我们的输入数据的计数，因此一个好的起点是以观察到的计数数据的平均值为中心的指数分布。但是，最终，它只是一种生成速率的开始猜测的方法。开关点也是如此。它可以由我们数据的起始年和结束年之间的均匀分布产生。</p><p id="797e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们设置了一些合理的先验，并为模型中的每个参数随机抽取了一个候选样本(在我们的例子中是 3 个)。这允许我们估计给定这些随机生成的值<em class="lu"> p(X|Z) </em>的数据的可能性。在下面的漫画中，图中的红线表示事故率转换点，如果您沿着这条线在每个数据点可视化泊松分布，您可以想象如何在此模型下获得数据的可能性。在这种情况下，第二个速率(切换点右侧)看起来太高，数据将被评定为不太可能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/ab1d115462f5f4c40fcc0e285ef846a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PLeeaYrhS9_D5dUj"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">生成模型的动画。有两个速率参数，分布为指数分布(因为它们必须为正)和切换点，分布为均匀分布。每个分布下的小红线显示当前的样本。散点图中的红线显示了这些参数如何转化为计数数据。在这种情况下，模型不能很好地解释数据，这意味着样本还不是很好。图片作者。</p></figure><p id="e363" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是事情变得有趣的地方。从最后一个样本(完全随机的)开始，我们将一个核函数(通常是正态分布)集中在这些点上，并再次采样，但现在我们从核分布中采样。如果数据的可能性比上次猜测的高，我们立即接受新的参数样本。如果数据的可能性等于或低于我们当前的步骤，我们按比例接受概率。换句话说，我们绝对、永远、永远接受能更好地解释我们数据的可变样本。我们只是有时会接受更差的。</p><p id="ce99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着时间的推移，你可以看到马尔可夫链是如何从最合理的参数分布开始采样的，即使我们不知道那个分布是什么。因为我们可以评估数据的可能性，所以我们能够越来越接近对参数的良好猜测。因为我们会在 100%的时间里接受更好的猜测，只是有时会接受不太好的猜测，所以总的趋势是更好的猜测。因为我们仅基于最后的猜测进行采样，所以趋势是样本向相对于数据更可能的参数漂移。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/47b3b0400126a442193e6e179e56b1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TO3h_PhVcn9QAatk"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有更好的可变样本的卡通模型。这些数据比第一张图中的数据更能说明问题。图片作者。</p></figure><p id="b38a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为进入“合理猜测”的空间需要几个样本，通常 MCMC 在使用前要被采样很多次。这就是所谓的“老化”，老化的适当长度也是一种猜测(就像许多建模是…<em class="lu">*叹息* </em>)。您还可以看到核函数的选择如何影响最终结果。比如你的核函数是超分散的，那么采样的参数也会更分散。</p><p id="4cce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终结果是，您可以对合理的参数进行采样，这些参数与您的数据非常“吻合”。你可以将这些采样参数绘制成直方图，估计它们的平均值或中值，以及所有这些好东西。你要去比赛了！</p><p id="8260" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">变分推理:新算法，同样的目标</strong></p><p id="9595" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你花时间在神经网络，反向传播领域，那么像我一样，你可能会看到各种各样的相似之处。向损失函数的最优值迈进的一小步？听起来像梯度下降！事实上，同样的模型可以在一个叫做<a class="ae kv" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">变分推理</a> (VI)的过程中使用梯度下降来“拟合”。</p><p id="cef8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，如前所述，我们有一个模型可以用来计算<em class="lu"> p(X|Z) </em>，我们想要的是逆运算:<em class="lu"> p(Z|X) </em>。后验描述了使观察数据的可能性最大化的参数。VI 背后的思想是通过最大化<a class="ae kv" href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="noopener ugc nofollow" target="_blank">证据下限(ELBO) </a>损失函数来拟合<em class="lu"> p(Z|X) </em>的代表性分布。我们将依次讨论这些作品。</p><p id="d4a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，我们不知道后验概率，<em class="lu"> p(Z|X) </em>，提前，所以我们用一个足够灵活的分布来表示它。在<a class="ae kv" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">本教程</a>中，他们展示了几个选项，包括独立正态分布(模型中的每个潜在变量都有一个)，多元正态分布(分布及其协方差是可学习的)，以及最奇特的选项，作为替身的神经网络，也称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Flow-based_generative_model" rel="noopener ugc nofollow" target="_blank">自回归流</a>。关键是，很多分布可以用来近似后验概率。</p><p id="44e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">厄尔巴岛本身就是一个话题，值得关注。<a class="ae kv" href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="noopener ugc nofollow" target="_blank">维基百科的文章挺好的</a>，还有机器学习&amp;模拟的<a class="ae kv" href="https://www.youtube.com/watch?v=HxQ94L8n0vU&amp;list=PLISXH-iEM4JloWnKysIEPPysGVg4v3PaP" rel="noopener ugc nofollow" target="_blank">系列 YouTube 视频。从发展直觉的角度来看，ELBO 是 Z 的先验估计值和最优点估计值之间的平衡行为，可最大化观测数据 x 的似然性。这是通过损失函数同时激励高数据似然性和惩罚偏离先验的大偏差来实现的。</a></p><p id="7329" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过最大化 ELBO，我们可以从后面采样，类似于 MCMC 方法。</p><p id="6338" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">示例</strong></p><p id="e7a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你已经知道 MCMC 和 VI 是什么了，这里有一个使用张量流概率的例子。我将在这里强调一些片段。完整的例子是 GitHub 上的<a class="ae kv" href="https://github.com/mbi2gs/mcmc_and_vi" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="a427" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我坚持使用上面描述的灾难计数模型。在 MCMC 示例中，模型是这样建立的:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="5232" class="ma mb iq lw b be mc md l me mf">disaster_count = tfd.JointDistributionNamed(<br/>    dict(<br/>        early_rate=tfd.Exponential(rate=1.),<br/>        late_rate=tfd.Exponential(rate=1.),<br/>        switchpoint=tfd.Uniform(low=tf_min_year, high=tf_max_year),<br/>        d_t=lambda switchpoint, late_rate, early_rate: tfd.Independent(<br/>            tfd.Poisson(<br/>                rate=tf.where(years &lt; switchpoint, early_rate, late_rate),<br/>                force_probs_to_zero_outside_support=True<br/>            ),<br/>            reinterpreted_batch_ndims=1<br/>        )<br/>    )<br/>)</span></pre><p id="45e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果是一个联合分布，在给定一些参数的情况下，它可以评估我们的数据的概率。我们根据一组特定的参数评估数据的对数概率，如下所示:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="6132" class="ma mb iq lw b be mc md l me mf">model.log_prob(<br/>  switchpoint=switchpoint,<br/>  early_rate=early_rate,<br/>  late_rate=late_rate,<br/>  d_t=disaster_data<br/>)</span></pre><p id="d34c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们建立一个 MCMC 对象，如下所示:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="0371" class="ma mb iq lw b be mc md l me mf">tfp.mcmc.HamiltonianMonteCarlo(<br/>    target_log_prob_fn=target_log_prob_fn,<br/>    step_size=0.05,<br/>    num_leapfrog_steps=3<br/>)</span></pre><p id="83fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用一个<code class="fe mg mh mi lw b">TransformedTransitionKernel</code>包装 MCMC 对象，这样我们就可以在连续的空间中进行采样，同时将学习限制在支持我们的模型(例如，没有负的年份值)。我们定义了烧入步骤的数量，以及我们希望抽取的样本数量。然后，不到一分钟后，我们就有了样品。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/985b99109df003b8a72a528df541e7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EeCyDJI3QaMG421gW-yfAg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MCMC 抽样的模型变量的分布。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/c80f32213326678babf84bbd98919b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOIHPihKCS4gt68tMr1Z6Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MCMC-根据灾难数据绘制的估计开关点。图片作者。</p></figure><p id="efe7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 VI 示例中，我们再次设置了一个联合分布:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="6fd2" class="ma mb iq lw b be mc md l me mf">early_rate = yield tfd.Exponential(rate=1., name='early_rate')<br/>late_rate = yield tfd.Exponential(rate=1., name='late_rate')<br/>switchpoint = yield tfd.Uniform(low=tf_min_year, high=tf_max_year, name='switchpoint')<br/>yield tfd.Poisson(<br/>    rate=tf.where(years &lt; switchpoint, early_rate, late_rate),<br/>    force_probs_to_zero_outside_support=True,<br/>    name='d_t'<br/>)</span></pre><p id="2bf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这次它被设置为一个生成器(因此有了<code class="fe mg mh mi lw b">yield</code>语句)。我们确保模型通过锁定来评估数据的对数似然性:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="21d4" class="ma mb iq lw b be mc md l me mf">target_model = vi_model.experimental_pin(d_t=disaster_data)</span></pre><p id="38a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们建立了一个灵活的分布，可以优化，以代表后验。我选择了自回归流(一种神经网络)，但正如所讨论的，有许多选项。</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="90f9" class="ma mb iq lw b be mc md l me mf">tfb.MaskedAutoregressiveFlow(<br/>    shift_and_log_scale_fn=tfb.AutoregressiveNetwork(<br/>        params=2,<br/>        hidden_units=[hidden_size]*num_hidden_layers,<br/>        activation='relu'<br/>    )<br/>)</span></pre><p id="c382" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，我们使用双投影器将连续的数值优化转化为我们模型的支持。最后，我们像这样拟合我们的模型:</p><pre class="kg kh ki kj gt lv lw lx bn ly lz bi"><span id="6506" class="ma mb iq lw b be mc md l me mf">optimizer = tf.optimizers.Adam(learning_rate=0.001)<br/>iaf_loss = tfp.vi.fit_surrogate_posterior(<br/>    target_model.unnormalized_log_prob,<br/>    iaf_surrogate_posterior,<br/>    optimizer=optimizer,<br/>    num_steps=10**4,<br/>    sample_size=4,<br/>    jit_compile=True<br/>)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/57e2028b2e5567eccae5f361f541a8e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kPsQQNJG3rj2EhTdo2Vckw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">VI 抽样的模型变量的分布。图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/7887fe794a0fff27b806a47b915476d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrigRAHk_woFr9tkrZPf2g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">VI-灾难数据上绘制的估计切换点。图片作者。</p></figure><p id="22bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，VI 不像 MCMC 那样适合切换点，我也不完全确定为什么。如果您有任何想法，请对本文进行评论或在 GitHub 上提出问题！但是，无论如何，你明白了。</p><p id="f983" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="f178" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这个简短的练习有启发性。MCMC 用于从潜在变量中取样。VI 用于使用梯度下降来拟合潜在变量。两者都可以用张量流概率来完成。现在我们知道了。</p></div></div>    
</body>
</html>