<html>
<head>
<title>CatBoost vs. LightGBM vs. XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CatBoost与LightGBM与XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924#2022-05-05">https://towardsdatascience.com/catboost-vs-lightgbm-vs-xgboost-c80f40662924#2022-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="206c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">哪个算法最好？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e4fa9c3e868cade63cc3de431160fe6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HWRT6au2SxhwdFV6"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">廷杰伤害律师事务所</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="16a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"><em class="lv">【CatBoost】</em></strong>(类别提升)<strong class="lb iu"> <em class="lv"> LightGBM </em> </strong>(光梯度提升机)<strong class="lb iu"> <em class="lv"> XGBoost </em> </strong>(极限梯度提升)都是梯度提升算法。在深入研究它们在特征和性能方面的相似性和差异之前，我们必须了解术语集成学习以及它与梯度增强的关系。</p><h1 id="b38d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">目录</h1><ol class=""><li id="4877" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated"><a class="ae ky" href="https://medium.com/p/c80f40662924/#d9ec" rel="noopener">集成学习</a></li><li id="b834" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="https://medium.com/p/c80f40662924/#5d39" rel="noopener">Catboost vs . light GBM vs . XGBoost特性</a></li><li id="e11e" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="https://medium.com/p/c80f40662924/#72c7" rel="noopener">提高精度、速度和控制过拟合</a></li><li id="7e67" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="https://medium.com/p/c80f40662924/#1801" rel="noopener">性能对比</a></li></ol></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="d9ec" class="lw lx it bd ly lz nl mb mc md nm mf mg jz nn ka mi kc no kd mk kf np kg mm mn bi translated">集成学习</h1><p id="6aa9" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">集成学习是一种结合来自多个模型的预测以获得更稳定和更好概括的预测的技术。这个想法是平均不同模型的个别错误，以减少过度拟合的风险，同时保持强大的预测性能。</p><p id="d844" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回归中，总体预测通常是单个树预测的平均值，而在分类中，总体预测基于加权投票，概率是所有树的平均值，概率最高的类是最终预测的类。</p><p id="4e22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种主要的集成学习方法，即bagging和boosting，尽管ML(机器学习)算法可以是两者的组合，但有一定的变化。</p><ul class=""><li id="d5af" class="mo mp it lb b lc ld lf lg li nt lm nu lq nv lu nw mw mx my bi translated"><strong class="lb iu"> Bagging </strong>方法使用数据的随机子集(替换取样)并行构建模型，并汇总所有模型的预测</li><li id="8aef" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated"><strong class="lb iu"> Boosting </strong>方法利用全部数据依次建立模型，每个模型在前一个模型的误差上进行改进</li></ul><p id="f3cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CatBoost、LightGBM和XGBoost都是梯度增强算法的变体。现在，您已经理解了打包和提升之间的区别，我们可以继续讨论算法如何实现梯度提升的区别。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="5d39" class="lw lx it bd ly lz nl mb mc md nm mf mg jz nn ka mi kc no kd mk kf np kg mm mn bi translated">Catboost与LightGBM和XGBoost特性的关系</h1><p id="f204" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">下表总结了这三种算法之间的差异，请继续阅读，以了解详细的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/08583c72e30546a35a0d86329c930c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PJXOO3x2HC_XfnxvjN-dEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表CatBoost、LightGBM和XGBoost的特征——按作者分类的图像</p></figure><h2 id="8fc8" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated">树对称</h2><p id="80f0" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在CatBoost中，对称树或平衡树指的是在树的同一深度上所有节点的分裂条件是一致的。另一方面，LightGBM和XGBoost会产生不对称的树，这意味着相同深度的每个节点的分割条件可能不同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/a54cb5ff58767e3989240cf5da60bdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9ZpKrigM0r8NIXhskJjHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:不对称树与对称树——作者图片</p></figure><p id="e32c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于对称树，这意味着分裂条件必须导致相同深度的所有节点的最低损失。平衡树架构的好处包括更快的计算和评估以及控制过拟合。</p><p id="223b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管LightGBM和XGBoost都是非对称树，但是LightGBM是按叶生长的，而XGBoost是按层生长的。简而言之，我们可以认为LightGBM是选择性地生长树，从而产生比XGBoost更小更快的模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/671fccca2bf4f13dbc6e4e54df54fc95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5oaBmMXLOrF2uI5xY1OrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2: LightGBM(左)与XGBoost(右)——作者图片</p></figure><h2 id="931c" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated">分裂法</h2><p id="bd59" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">拆分方式是指如何确定拆分条件。</p><p id="57cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在CatBoost中，使用贪婪方法，从而将特征分裂对的可能候选列表分配给叶子作为分裂，并且选择导致最小惩罚的分裂。</p><p id="2b95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在LightGBM中，基于梯度的单侧采样(GOSS)保留所有具有大梯度的数据实例，并对具有小梯度的数据实例执行随机采样。梯度是指损失函数切线的斜率。具有较大梯度的数据点具有较高的误差，并且对于找到最佳分裂点是重要的，而具有较小梯度的数据点具有较小的误差，并且对于保持学习决策树的准确性是重要的。这种采样技术导致训练模型的数据实例更少，因此训练时间更快。</p><p id="7093" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在XGBoost中，预排序算法考虑所有特征，并根据特征值对它们进行排序。之后，进行线性扫描，以确定导致最大信息增益的特征和特征值的最佳分割。基于直方图的算法以相同的方式工作，但是它不是考虑所有特征值，而是将特征值分组到离散的箱中，而是基于离散的箱找到分裂点，这比预先排序的算法更有效，尽管仍然比GOSS慢。</p><h2 id="d0e2" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated">增压类型</h2><p id="8e8a" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">选择数据进行训练的方式有所不同。有序推进指的是每个模型训练一个数据子集并评估另一个数据子集的情况。有序提升的好处包括增强对未知数据的稳健性。</p><h2 id="b402" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated">分类列</h2><p id="cc17" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">不同算法的分类列的参数如下:</p><ul class=""><li id="ea36" class="mo mp it lb b lc ld lf lg li nt lm nu lq nv lu nw mw mx my bi translated">CatBoost: <code class="fe om on oo op b">cat_features</code>，<code class="fe om on oo op b">one_hot_max_size</code></li><li id="2574" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">LightGBM: <code class="fe om on oo op b">categorical_feature</code></li><li id="6f75" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">XGBoost: NA</li></ul></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="72c7" class="lw lx it bd ly lz nl mb mc md nm mf mg jz nn ka mi kc no kd mk kf np kg mm mn bi translated">提高精度、速度和控制过拟合</h1><p id="aac7" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在集成学习中，对不同模型的预测进行平均有助于过拟合。然而，与任何基于树的算法一样，仍然存在过度拟合的可能性。可以在将数据集分成训练集、验证集和测试集的过程中处理过拟合，从而实现交叉验证、提前停止或树修剪。为了比较不同的算法，我们将集中于使用模型参数控制过拟合。</p><p id="eca7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，为了控制模型的复杂性，XGBoost使用参数<code class="fe om on oo op b">max_depth</code>(因为它是逐层增长的)，而LightGBM使用参数<code class="fe om on oo op b">num_leaves</code>(因为它是逐叶增长的)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/cf54f0a75ccd4e2f5884f8b702b7de7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHGth3DPDTf3WqTVBZLBSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表2:用于调整精度、速度和过度拟合的参数—图片由作者提供</p></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="1801" class="lw lx it bd ly lz nl mb mc md nm mf mg jz nn ka mi kc no kd mk kf np kg mm mn bi translated"><strong class="ak">性能对比</strong></h1><p id="d76f" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">在不同的数据集上，有各种关于准确性和速度的基准测试。我发现在几个数据集上概括算法性能是草率的，特别是如果过度拟合和数字/分类变量没有得到适当的考虑。</p><p id="5cf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，一般来说，从文献来看，XGBoost和LightGBM产生的性能相似，CatBoost和LightGBM的性能比XGBoost快得多，尤其是对于较大的数据集。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="ffd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望您对三种最流行的ML boosting算法有更好的理解——CatBoost、LightGBM和XGBoost，它们主要在结构上有所不同。在实践中，数据科学家通常会对他们的数据尝试不同类型的ML算法——所以现在还不要排除任何算法！除了在不同算法之间进行选择时的可理解性、性能和时序考虑之外，通过超参数调整来微调模型以及通过流水线架构或超参数来控制过拟合也是至关重要的。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="2658" class="lw lx it bd ly lz nl mb mc md nm mf mg jz nn ka mi kc no kd mk kf np kg mm mn bi translated">相关链接</h1><h2 id="7410" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated"><strong class="ak"> CatBoost </strong></h2><ul class=""><li id="4911" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu nw mw mx my bi translated">文档:<a class="ae ky" href="https://catboost.ai/en/docs/" rel="noopener ugc nofollow" target="_blank">https://catboost.ai/en/docs/</a></li><li id="9dab" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">官方GitHub:<a class="ae ky" href="https://github.com/catboost/catboost" rel="noopener ugc nofollow" target="_blank">https://github.com/catboost/catboost</a></li><li id="fc26" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">论文:<a class="ae ky" href="http://learningsys.org/nips17/assets/papers/paper_11.pdf" rel="noopener ugc nofollow" target="_blank">http://learningsys.org/nips17/assets/papers/paper_11.pdf</a></li></ul><h2 id="2226" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated"><strong class="ak"> LightGBM </strong></h2><ul class=""><li id="2e54" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu nw mw mx my bi translated">文件:<a class="ae ky" href="https://lightgbm.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">https://lightgbm.readthedocs.io/</a></li><li id="6082" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">官方GitHub:<a class="ae ky" href="https://github.com/microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/LightGBM</a></li><li id="5cfb" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">论文:<a class="ae ky" href="https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings . neur IPS . cc/paper/2017/file/6449 f 44 a 102 FDE 848669 BDD 9 EB 6b 76 fa-paper . pdf</a></li></ul><h2 id="6638" class="ny lx it bd ly nz oa dn mc ob oc dp mg li od oe mi lm of og mk lq oh oi mm oj bi translated"><strong class="ak"> XGBoost </strong></h2><ul class=""><li id="0158" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu nw mw mx my bi translated">文档:【https://xgboost.readthedocs.io/ T2】</li><li id="d2d6" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">官方GitHub:【https://github.com/dmlc/xgboost T4】</li><li id="a20a" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nw mw mx my bi translated">论文:<a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1603.02754.pdf</a></li></ul></div></div>    
</body>
</html>