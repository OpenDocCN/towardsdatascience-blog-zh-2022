<html>
<head>
<title>Semantic Textual Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语义文本相似度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-textual-similarity-83b3ca4a840e#2022-04-25">https://towardsdatascience.com/semantic-textual-similarity-83b3ca4a840e#2022-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="53b2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从Jaccard到OpenAI，为您的语义文本相似性项目实现最佳的NLP算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6fff6c433caada4eef2e2cacc15138b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mlZET9sS-ZZVtvPb"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@inakihxz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">伊尼基·德尔·奥尔莫</a>拍摄的照片</p></figure><h1 id="8677" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🎬介绍</h1><p id="4909" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自然语言处理(NLP)在信息提取、自然语言理解和自然语言生成方面有着巨大的<a class="ae ky" href="https://zilliz.com/learn/top-5-nlp-applications" rel="noopener ugc nofollow" target="_blank">现实世界应用</a>。比较自然语言文本之间的相似性对于许多信息提取应用来说是必不可少的，例如<a class="ae ky" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">谷歌搜索</a>、<a class="ae ky" href="https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/" rel="noopener ugc nofollow" target="_blank"> Spotify的播客搜索</a>、<a class="ae ky" href="https://www.datanami.com/2022/03/15/home-depot-finds-diy-success-with-vector-search/" rel="noopener ugc nofollow" target="_blank">家得宝的产品搜索</a>等。语义文本相似性(STS)问题试图比较两个文本，并确定它们在意义上是否相似。由于自然语言的细微差别，这是一个众所周知的难题，两个文本可能是相似的，尽管没有一个共同的单词！</p><p id="ddc5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然这个挑战已经存在很长时间了，但是NLP的最新进展已经开发了许多算法来解决这个问题。一些方法接受两个文本作为输入，并直接提供两个文本相似程度的分数。其他一些技术只是将每个文本转换成一个称为嵌入的数字向量，然后我们可以通过数学计算向量之间的距离来进行比较。回想一下，</p><blockquote class="ms"><p id="93ac" class="mt mu it bd mv mw mx my mz na nb mm dk translated">💡“嵌入”向量是我们自然语言文本的数字表示，以便我们的计算机可以理解我们文本的上下文和含义。</p></blockquote><p id="7955" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">这篇文章将介绍几种在不同场景下解决STS问题的技术。我的目标是尽可能多地使用预先训练好的模型，这些模型提供了开箱即用的优秀结果。在文章的最后，我附上了一个流程图，帮助你决定解决定制问题的最佳方法。具体来说，在这篇文章中，我们将讨论以下内容。点击链接，跳转到以下部分:</p><ol class=""><li id="5056" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated"><a class="ae ky" href="https://medium.com/p/83b3ca4a840e#45f9" rel="noopener">设置环境并下载一些数据</a></li><li id="76d0" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="https://medium.com/p/83b3ca4a840e#4031" rel="noopener">用经典的非上下文算法测量文本相似性</a></li><li id="b8ce" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="https://medium.com/p/83b3ca4a840e#b3f1" rel="noopener">用现代上下文算法测量文本相似度</a></li><li id="f477" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="https://medium.com/p/83b3ca4a840e#012f" rel="noopener">结果&amp;结论</a></li></ol><p id="aaf1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我们将使用<a class="ae ky" href="https://huggingface.co/datasets/stsb_multi_mt" rel="noopener ugc nofollow" target="_blank"> Huggingface datasets </a>上可用的stsb_multi_mt数据集。该数据集包含成对的句子和一个正实数标签，该标签指示每对句子的相似度，范围从0(最不相似)到5(最相似)。作者已经在<a class="ae ky" href="https://github.com/PhilipMay/stsb-multi-mt/blob/main/LICENSE" rel="noopener ugc nofollow" target="_blank">Commons Attribution-Share like 4.0国际许可</a>下许可了数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/df466738ba6b605bd182da00309d93f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZrmtoJWys3g4iHlkEBoHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">STSB数据集的前五行。图片作者。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/f99e90304fcd0b2442b2e214adae8d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzEJPK3izRQxkb1Z17cPwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">STSB测试数据集相似性得分分布。图片作者。</p></figure><h1 id="45f9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🧩设置环境并下载一些数据</h1><p id="27b0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们首先需要安装所有必要的库来测试各种嵌入策略。我们将使用来自<a class="ae ky" href="https://github.com/conda-forge/miniforge" rel="noopener ugc nofollow" target="_blank"> mini-forge </a>的conda来管理这个项目的虚拟环境。</p><h2 id="60b7" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">设置环境并安装必要的库</h2><p id="ffcf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因为我们将在这篇文章中讨论几种不同的算法，所以我们需要安装各种各样的库来让它们都工作。不幸的是，安装pip时，WMD算法所必需的pyemd库无法工作，因此我们需要使用conda来完成这个项目。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><h2 id="d6bf" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">加载STSB数据集</h2><p id="04dd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用<a class="ae ky" href="https://huggingface.co/docs/datasets/index" rel="noopener ugc nofollow" target="_blank"> Huggingface的数据集库</a>将STSB数据集快速加载到熊猫数据帧中。STSB数据集由一个<code class="fe ol om on oo b">train</code>表和一个<code class="fe ol om on oo b">test</code>表组成。我们将这两个表分成它们各自的数据帧<code class="fe ol om on oo b">stsb_train</code>和<code class="fe ol om on oo b">stsb_test</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><h2 id="e3aa" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">创建一些助手函数</h2><p id="2508" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们为我们将在这篇文章中重复执行的操作创建两个助手函数。第一个功能是对文本进行预处理，将字母符号化、小写化，并删除数字和停用词。第二个函数接受两列文本嵌入，并返回两列之间的行余弦相似度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="64c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们有了环境和数据，让我们开始讨论算法吧！</p><h1 id="4031" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用经典的非上下文算法进行文本相似性的🧮度量</h1><p id="f1a1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本节将讨论一些使用经典的非上下文方法来测量文本间相似性的技术。在这些算法中，我们只使用实际的单词进行相似度计算，而不考虑每个单词出现的上下文。正如所料，这些技术通常比更现代的上下文方法性能更差。</p><h2 id="b6da" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">雅克卡相似性</h2><p id="c617" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">比较两篇文章最简单的方法是计算它们共有的独特单词的数量。然而，如果我们仅仅计算独特的常用词的数量，那么较长的文档将具有更多的常用词。为了克服这种对较长文档的偏见，在Jaccard相似度中，我们将两个文本中共同的唯一单词的数量标准化为唯一单词的总数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/14e54c4c9287f00d51fba49dbaa7072e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*5RhwzNt5oSdWJB1aShGUHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Jaccard相似方程。图片作者。</p></figure><p id="021e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Jaccard相似性是Python中使用<a class="ae ky" href="https://github.com/life4/textdistance" rel="noopener ugc nofollow" target="_blank"> textdistance </a>库可以轻松计算的几种距离之一。请注意，在运行Jaccard similarity之前，要对文本进行预处理，以删除停用词、小写字母并对其进行词条归类，以确保在计算中仅使用信息词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="d023" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在上面的代码中，我们仅使用单词(1-gram)来计算Jaccard相似性。然而，该技术也可以很容易地扩展到任何N元文法。使用N-gram而不是单词(1-gram)的Jaccard相似度被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/W-shingling" rel="noopener ugc nofollow" target="_blank"> w-shingling </a>。</p><p id="c513" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然Jaccard相似性和w-shingling是测量文本相似性的简单方法，但它们在实践中表现得相当不错，如本文末尾的<a class="ae ky" href="https://medium.com/p/83b3ca4a840e#012f" rel="noopener">结果部分所示！</a></p><h2 id="3d74" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">单词袋(蝴蝶结)</h2><p id="c2fd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="noopener ugc nofollow" target="_blank"> Bag of Words </a>是从文本中提取特征并将其转换为数字嵌入向量的经典方法的集合。然后，我们通过计算它们之间的余弦相似性来比较这些嵌入向量。使用单词包方法有两种流行的方式:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TFIDF矢量器</a>。</p><p id="8b5f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="oq">计数矢量器</em> </strong></p><p id="655b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该算法将整个文本语料库中的每个唯一单词映射到一个唯一的向量索引。每个文档的向量值是每个特定单词在该文本中出现的次数。因此，向量可以由整数值组成，包括0，这表示该单词没有出现在文本中。虽然计数矢量器易于理解和实现，但它的主要缺点是，不管单词的实际重要性如何，它对所有单词都一视同仁。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e846d0b0fb2a0112fcc6043042e517e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*JR96pQC21utQ8zUVUVWM3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计数矢量器示例。图片作者。</p></figure><p id="e8a9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="oq"> TFIDF矢量器</em> </strong></p><p id="dc2d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了克服计数矢量器的缺点，我们可以使用TFIDF矢量器。该算法还将整个文本语料库中的每个唯一单词映射到一个唯一的向量索引。但是每个文档的向量值不是简单的计数，而是两个值的乘积:术语频率(TF)和逆文档频率(IDF)。</p><ol class=""><li id="8666" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">词频(TF):每个单词的TF是该单词在该文档中出现的次数，与来自计数矢量器的值相同。这是衡量该单词对文档有多重要的标准。</li><li id="f17a" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">逆文档频率(IDF):另一方面，一个单词的IDF是该单词出现的文档部分的倒数的对数。它衡量这个词在整个语料库中的稀有程度。</li></ol><p id="d034" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，每个文档中每个单词的TFIDF值是单个TF和IDF分数的乘积。这里的直觉是，一个文档中在整个语料库中相对罕见的频繁词是该文档的关键词，并且具有高TFIDF分数。TFIDF的大多数实现都将这些值标准化为文档长度，这样较长的文档就不会在计算中占主导地位。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/6dec7175fa9ee208506ede187b485de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SW80ThHVY7UndJRCF_DEFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TFIDF矢量器示例。图片作者。</p></figure><p id="1159" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用sklearn库在代码中实现TFIDF很简单，它以稍微复杂一点的方式计算IDF，实现规范化并防止0除法；你可以在这里了解更多关于<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2021/11/how-sklearns-tfidfvectorizer-calculates-tf-idf-values/" rel="noopener ugc nofollow" target="_blank">的信息。TFIDF是这篇文章中我们需要训练的唯一模型，以学习语料库中所有独特的单词及其相关的IDF值。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="21dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管单词包方法很直观，并为我们提供了文本的矢量表示，但它们在现实世界中的表现却千差万别。在STSB任务中，TFIDF没有Jaccard相似性做得好，正如在<a class="ae ky" href="https://www.notion.so/The-State-of-the-Art-for-Semantic-Textual-Similarity-366cf60864404eff9d6cc70274b3ccbe" rel="noopener ugc nofollow" target="_blank">结果部分</a>中看到的。</p><p id="1b82" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">词汇袋方法的其他潜在陷阱是</p><ol class=""><li id="72f4" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">如果文档的数量很大，那么由这种方法生成的向量将具有非常高的维数，因为在语料库中会有许多独特的单词。</li><li id="c1c2" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">这些向量非常稀疏，因为大多数单词不会出现在大多数文档中。</li></ol><p id="1a5f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下测量文本相似性的方法通过使用预先训练的单词嵌入克服了这些限制。</p><h2 id="89e5" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">字移动距离(WMD)</h2><p id="07b1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Jaccard相似度和TFIDF假设相似的文本有许多共同的单词。但是，情况可能并不总是如此，因为即使没有任何常用的不间断单词的文本也可能是相似的，如下所示。解决这个问题的一种方法是使用预先训练的单词嵌入。</p><pre class="kj kk kl km gt ot oo ou ov aw ow bi"><span id="332e" class="nx la it oo b gy ox oy l oz pa"><em class="oq">document1: “Obama speaks to the media in Illinois” <br/>document2: “The president greets the press in Chicago”</em></span></pre><blockquote class="ms"><p id="d3d3" class="mt mu it bd mv mw pb pc pd pe pf mm dk translated">💡单词嵌入是将单词编码成数字向量的模型，使得<a class="ae ky" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">相似的单词具有在向量空间</a>中彼此靠近的向量。</p></blockquote><p id="f718" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">生成单词嵌入有几种方式，最突出的是<a class="ae ky" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>、<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>和<a class="ae ky" href="https://fasttext.cc/docs/en/unsupervised-tutorial.html" rel="noopener ugc nofollow" target="_blank"> FastText </a>。</p><p id="18ee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于我们需要比较包含多个单词的文本之间的相似性，从单个单词嵌入到单个句子嵌入的最简单方法是计算该文本中所有单词嵌入的元素平均值。然而，有一种更好的方法可以直接从单词嵌入中计算文本之间的相似性，这种方法称为单词移动距离(WMD)。</p><p id="8953" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="http://proceedings.mlr.press/v37/kusnerb15.pdf" rel="noopener ugc nofollow" target="_blank"> WMD </a>基于<a class="ae ky" href="https://www.cs.jhu.edu/~misha/Papers/Rubner98.pdf" rel="noopener ugc nofollow" target="_blank">推土机距离</a>的概念，是一个文档中的单词嵌入需要“行进”到我们与之比较的文档的单词嵌入的最小距离。由于每个文档包含多个单词，WMD计算需要计算每个单词到其他单词的距离。它还根据每个单词的词频对“旅行”进行加权。令人欣慰的是，<a class="ae ky" href="https://github.com/RaRe-Technologies/gensim" rel="noopener ugc nofollow" target="_blank"> gensim </a>库使用<a class="ae ky" href="https://www.cs.huji.ac.il/w~werman/Papers/ICCV2009.pdf" rel="noopener ugc nofollow" target="_blank">快速WMD算法</a>有效地实现了这个复杂的计算。我们只需一行代码就可以轻松使用它！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/425c59b208315b3225e7737ec6145648.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*t2D29DYW1TP6cNqQr84YcQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">大规模杀伤性武器在行动。两对句子都没有共同的词。WMD仍然可以找到一个相似的句子，因为它们的单词更接近。图片来自<a class="ae ky" href="https://arxiv.org/pdf/2105.14403.pdf" rel="noopener ugc nofollow" target="_blank"> Arxiv论文</a>。</p></figure><p id="6045" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然我们可以在WMD中使用任何单词嵌入模型，但我决定使用在维基百科上预先训练的FastText模型，主要是因为<a class="ae ky" href="https://arxiv.org/pdf/1607.04606.pdf" rel="noopener ugc nofollow" target="_blank"> FastText使用子单词</a>信息，并且永远不会遇到Word2Vec或GloVe可能会遇到的词汇问题。注意对文本进行预处理，删除停用词、小写字母，并对其进行词条归类，以确保WMD计算仅使用信息词。最后，由于WMD是一个距离度量，而我们正在寻找一个相似性度量，我们将WMD值乘以-1(负WMD ),这样更多的相似文本在数字上具有更大的值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="1b6e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在STSB数据集上，负的WMD分数仅比Jaccard相似度稍好，因为该数据集中的大多数句子都有许多相似的词。NegWMD的性能在文本之间有较少共同词的数据集上将比Jaccard好得多。</p><p id="6d6a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">WMD的限制之一是WMD中使用的单词嵌入是非上下文相关的，其中每个单词获得相同的嵌入向量，而不管它出现在句子的其余部分的上下文。本文其余部分的算法也可以使用上下文来克服这个问题。</p><h1 id="b3f1" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🚀用现代上下文算法度量文本相似性</h1><p id="4a6a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本节将讨论几种测量语义文本相似性的技术，考虑不同单词出现的上下文。这些方法通常比非上下文方法更准确。</p><h2 id="6ffc" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">通用句子编码器(使用)</h2><p id="f639" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在<a class="ae ky" href="http://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">使用</a>中，谷歌的研究人员首先在多任务目标上预先训练了一个基于Transformer的模型，然后将其用于迁移学习。为了计算文本相似性，我们首先使用预先训练的使用模型来计算句子中每个单词的上下文单词嵌入。然后，我们通过执行所有单词向量的元素式求和并除以句子长度的平方根来标准化句子长度，从而计算句子嵌入。一旦我们有了每个句子的使用嵌入，我们就可以使用本文开头定义的帮助函数来计算余弦相似度。研究人员在Tensorflow hub上开源了预训练模型，我们将直接使用它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="c8f2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与NegWMD度量相比，USE在STSB数据集上实现了显著的10点跳跃。它显示了在迁移学习环境中使用变形金刚生成的上下文句子嵌入的潜力。随后，研究人员开发了更先进的方法，使用度量学习来预训练基于变压器的模型，以获得更好的性能！</p><h2 id="57d6" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">交叉编码器</h2><p id="2f80" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">2018年来自变压器(BERT) 模型的<a class="ae ky" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">双向编码器表示的出现，通过击败几个基准，开创了NLP的新时代。随着时间的推移，研究人员继续改进香草伯特模型，产生了几个值得注意的变体，如</a><a class="ae ky" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>、<a class="ae ky" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">迪翁伯特</a>、<a class="ae ky" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">艾伯特</a>等。，正如本帖中讨论的<a class="ae ky" href="https://zilliz.com/learn/7-nlp-models" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="8d72" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT的强大功能来自其自我监督的预训练任务，称为掩蔽语言建模(MLM)，我们随机隐藏一些单词，并训练模型预测丢失的单词，同时给出丢失单词之前和之后的单词。对大规模文本语料库的训练允许BERT学习语言中各种单词之间的语义关系。</p><p id="42c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以通过向BERT模型的输出添加分类头来使用BERT作为交叉编码器。交叉编码器模型将一对文本文档作为输入，直接输出两个文档相似的概率。通过在带标签的<a class="ae ky" href="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016" rel="noopener ugc nofollow" target="_blank"> STS </a>数据集上微调预训练的BERT模型，我们可以在STS任务上获得最先进的结果！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/28088dc0b9fe12cf0b2e64ad5e5cf126.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*T6XILGOFvbIvLaNSa9VjSA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉编码器。图片作者。</p></figure><p id="ac4b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将使用<a class="ae ky" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank"> sentence_transformers库</a>来有效地使用在<a class="ae ky" href="https://paperswithcode.com/dataset/snli" rel="noopener ugc nofollow" target="_blank"> SNLI </a>和<a class="ae ky" href="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016" rel="noopener ugc nofollow" target="_blank"> STS </a>数据集上训练的各种开源交叉编码器模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="d64d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">交叉编码器不输出任何嵌入向量，因此在几千个文档之外不具有很好的可扩展性。但总的来说，BERT交叉编码器在大多数句子相似性任务中提供了最好的性能。在我们的STSB数据集中，BERT Cross编码器给出了最好的分数！</p><h2 id="86a3" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">度量学习</h2><p id="1e55" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://paperswithcode.com/task/metric-learning?page=6" rel="noopener ugc nofollow" target="_blank">度量学习</a>是生成嵌入的最有前途的方法之一，尤其是对于相似性搜索应用。在最基本的层面上，在度量学习中，</p><ol class=""><li id="b99a" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">我们使用像BERT这样的神经网络将文本转换成嵌入。</li><li id="262b" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">我们构建这些嵌入，使得语义相似的文本彼此聚得更近，而不相似的文本则相距更远。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/2aba651000cb0f657dd3f5e7f4547b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ae5MjNxxpw0GX1eHTf95BA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">向量空间中的自然语言文本示例。图片来自作者，灵感来自<a class="ae ky" rel="noopener" target="_blank" href="/contrastive-loss-explaned-159f2d4a87ec">媒体博客文章</a>作者<a class="ae ky" href="https://medium.com/@rantlab?source=post_page-----159f2d4a87ec-----------------------------------" rel="noopener">布莱恩·威廉斯</a>。</p></figure><p id="a030" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练度量学习模型需要在如何处理数据和如何训练模型方面进行创新，正如在<a class="ae ky" rel="noopener" target="_blank" href="/powering-semantic-similarity-search-in-computer-vision-with-state-of-the-art-embeddings-f6c183fff134">我的上一篇文章</a>中详细描述的那样。在用这种方法训练模型之后，我们可以通过数学上计算两个文本向量之间的余弦相似性来找到两个文本之间的相似性。</p><h2 id="3677" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">SBERT双编码器</h2><p id="41e5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="http://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">句子转换器</a>(也称为SBERT)是当前最先进的NLP句子嵌入。它使用<a class="ae ky" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>及其变体作为基础模型，并利用一种称为对比学习的度量学习进行预训练。在对比学习中，对比损失函数比较两个嵌入是相似的(0)还是不相似的(1)。</p><p id="e904" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">句子变形金刚的核心思想如下。</p><ol class=""><li id="995b" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">使用带标签的<a class="ae ky" href="https://paperswithcode.com/dataset/snli" rel="noopener ugc nofollow" target="_blank"> SNLI数据集</a>或<a class="ae ky" href="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016" rel="noopener ugc nofollow" target="_blank"> STS数据集</a>作为训练数据。这些数据集包含数千对被标记为相似或不相似的句子。</li><li id="c73f" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">对于训练数据集中的每个文本，使用任何预先训练的BERT模型作为编码器来计算该文本的上下文单词嵌入。</li><li id="b00a" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">计算所有标记嵌入的元素平均值，以获得整个文本的单个固定维度句子嵌入。这种操作称为平均池。</li><li id="76ab" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">使用具有对比损耗的<a class="ae ky" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">连体网络架构</a>训练模型。该模型的目标是将相似文本的嵌入移得更近，使得它们之间的距离接近0。相反，该模型旨在将来自不同文本的嵌入彼此远离，使得它们之间的距离很大。</li><li id="13c2" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">在我们完成模型的训练之后，我们可以通过计算任意两个文本的嵌入之间的余弦相似性来比较这两个文本。</li></ol><p id="bb79" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">双编码器句子转换器模型一次接收一个文本作为输入，并输出一个固定维度的嵌入向量作为输出。然后，我们可以通过计算任意两个文档的嵌入之间的余弦相似性来比较这两个文档。</p><p id="9156" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管在我们的STSB数据集上，双编码器句子转换器的性能略低于交叉编码器，但通过与向量搜索数据库(如<a class="ae ky" href="https://milvus.io/" rel="noopener ugc nofollow" target="_blank"> Milvus </a>)相结合，双编码器在扩展到数十亿甚至数万亿个文档时大放异彩！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/f32de3826df0ac35b01962fee57e97ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*2GzHnkb4DpQ1jAix1PAC3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双编码器。图片作者。</p></figure><p id="eed9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将使用<a class="ae ky" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">句子_变形金刚库</a>来有效地使用在<a class="ae ky" href="https://paperswithcode.com/dataset/snli" rel="noopener ugc nofollow" target="_blank"> SNLI </a>和<a class="ae ky" href="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016" rel="noopener ugc nofollow" target="_blank"> STS </a>数据集上训练的各种开源SBERT双编码器模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="b832" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管在STSB数据集上表现很好，但不幸的是，句子转换器是完全监督的模型，需要大量带标签的句子对语料库来训练。因此，在新的领域采用句子转换器是一个耗时且昂贵的收集大量高质量标记数据的过程。幸运的是，在半监督和自监督学习方面的一些前沿研究显示了有希望的结果！</p><h2 id="373c" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">希姆策</h2><p id="a3c1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我的<a class="ae ky" rel="noopener" target="_blank" href="/powering-semantic-similarity-search-in-computer-vision-with-state-of-the-art-embeddings-f6c183fff134">上一篇关于计算机视觉嵌入的文章</a>中，我介绍了<a class="ae ky" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank"> SimCLR </a>，一种使用对比损失学习图像嵌入的自我监督算法。在这篇文章中，我们来讨论SimCSE，它是SimCLR的NLP等价物。</p><p id="154e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">SimCSE代表句子嵌入的简单对比学习。我们可以将它训练成一个有监督的模型(如果有标签数据的话)或者一个完全无监督的模型！</p><p id="155c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">SimCSE的核心思想如下。</p><ol class=""><li id="32a7" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">给定一个文本文档，使用任何预先训练的BERT模型作为编码器来计算该文本的嵌入，并获取<a class="ae ky" href="https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important" rel="noopener ugc nofollow" target="_blank">【CLS】令牌</a>的嵌入。</li><li id="b4f4" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">通过对原始嵌入应用两个不同的<a class="ae ky" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">缺失</a>蒙版，创建同一文本嵌入的两个噪点版本。从相同输入文本生成的这两个有噪声的嵌入被认为是“正”对，并且模型期望它们具有0的余弦距离。在<a class="ae ky" href="https://aclanthology.org/2021.emnlp-main.552" rel="noopener ugc nofollow" target="_blank"> SimCSE论文</a>中的实验发现，对于STSB数据集，0.1的辍学率是最佳的。</li><li id="0724" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">我们认为这一批中所有其他文本的嵌入是“否定的”该模型期望“负片”到来自前一步骤的目标文本嵌入的余弦距离为1。损失函数然后更新编码器模型的参数，使得嵌入更接近我们的期望。</li><li id="5ae6" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">监督SimCSE有一个额外的步骤，我们使用一个<a class="ae ky" href="https://paperswithcode.com/task/natural-language-inference" rel="noopener ugc nofollow" target="_blank">自然语言推理(NLI)标记的数据集</a>从标记为“蕴涵”的文本中获得“肯定”对，从标记为“矛盾”的文本中获得“否定”对</li></ol><p id="597f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图从概念上解释了整个过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/258cf520c519c4c4ab3b8acaae3e309b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9OWdjRkCXBBXglIZwoGyYQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">无监督和有监督SimCSE。图片来自<a class="ae ky" href="https://arxiv.org/pdf/2104.08821.pdf" rel="noopener ugc nofollow" target="_blank"> arxiv论文</a>。</p></figure><p id="8726" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">SimCSE模型是使用SimCSE方法训练的双编码器句子转换器模型。因此，我们可以直接重用来自双编码器句子转换器模型的所有代码，但是将预训练模型更改为SimCSE模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="de3c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从STSB数据集上的结果，我们看到，与有监督的SimCSE和其他有监督的句子转换器模型相比，无监督的SimCSE模型具有显著的性能下降。然而，尽管完全在无监督的情况下训练，仅使用Dropout来创建“正”对，但无监督的SimCSE可以轻松击败其他方法，如WMD和USE。因此，在没有足够的标记数据或收集这些数据的成本很高的领域，无监督SimCSE将是首选方法。</p><h2 id="b2d5" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">OpenAI</h2><p id="ef54" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所有基于BERT的模型(如句子转换器和SimCSE)的一个重大限制是，它们只能对最长512个标记的文本进行编码。这个限制是因为BERT系列模型有512个令牌输入限制。此外，由于BERT的子单词标记器可能会将每个单词分成多个标记，因此可以使用这些技术转换为嵌入的文本需要少于512个单词。如果您需要比较较长文档之间的相似性，这可能会带来问题。非基于BERT的模型不会面临这种限制，但它们的性能比基于BERT的模型差，因此如果有更好的替代方案，我们宁愿避免使用它们。</p><p id="bf78" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">生成最先进嵌入的最后一种方法是使用付费托管服务，如<a class="ae ky" href="https://beta.openai.com/docs/guides/embeddings/what-are-embeddings" rel="noopener ugc nofollow" target="_blank"> OpenAI的嵌入端点</a>。它支持多达2048个标记的文本，因此非常适合比BERT的512个标记限制更长的文本文档。然而，OpenAI端点价格昂贵，尺寸较大(12288维，而基于BERT的模型为768维)，与同类最佳的免费开源句子转换器模型相比，性能有所下降。</p><p id="8bae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了让你知道它有多贵，我花了大约<strong class="lt iu">20美元</strong>在这个小的STSB数据集上生成OpenAI Davinci嵌入，甚至在确保每个唯一文本只生成一次嵌入之后！即使对于大型组织来说，将这一代嵌入扩展到一个巨大的语料库也是非常昂贵的。收集标记数据并在内部训练模型可能更便宜。因此，我认为这种技术在现实世界中的用途有限，但是我仍然将它包含在本文中以完善它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><h1 id="012f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">🏅结果和结论</h1><p id="9690" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最后，让我们比较一下我在这篇文章中提到的各种文本相似性方法的结果。许多关于语义文本相似性的论文使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> Spearman等级相关系数</a>来衡量模型的性能，因为它对离群值、非线性关系或非正态分布数据不敏感，如本文中的<a class="ae ky" href="https://aclanthology.org/C16-1009.pdf" rel="noopener ugc nofollow" target="_blank">所述。</a></p><p id="d94d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我们将计算每种方法的相似性得分与STSB数据集提供的实际<code class="fe ol om on oo b">similarity_score</code>标签之间的Spearman等级相关性。使用<code class="fe ol om on oo b">pandas</code>中内置的<code class="fe ol om on oo b">corr</code>方法，计算相对简单，如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><p id="0509" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的Spearman等级相关分数显示SBERT交叉编码器具有最好的性能，紧随其后的是SBERT双编码器。无监督SimCSE的性能非常有前途，因为它比Jaccard、TFIDF、WMD和USE等其他方法好得多。最后，OpenAI Davinci显示了良好的性能，但是它的成本超过了接受超过512个令牌的文本的大多数好处。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/184014f25decba3e64a92c6db8217374.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*PKsVhPUDjL5Yds2WfwutMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">STSB数据集上各种算法的Spearman秩相关性能。图片作者。</p></figure><p id="93a0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们绘制了实际相似性分数和来自各种算法的预测相似性之间的相关性。视觉上，SBERT和SimCSE的相关性看起来相当强！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者代码。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/394913217be9a85544b4514b0dbda6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SNKwc34LHObQnBT-iU-PAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Spearman等级相关系数和本帖中介绍的各种方法的实际与预测相似性得分。图片作者。</p></figure><p id="a979" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我开发了下面的流程图来帮助你选择你自己的语义文本相似性任务的方法。我希望它能让你为你的用例选择最好的技术！感谢您的阅读。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/ad3ffc9570f6111fc35a47b6161eb145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EtKt9AQIY1y1t5YDwHuZg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择语义文本相似度算法的流程图。图片作者。</p></figure></div><div class="ab cl po pp hx pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="im in io ip iq"><p id="6fe1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要了解如何比较图像之间的相似性，请阅读下面的帖子。</p><div class="pv pw gp gr px py"><a rel="noopener follow" target="_blank" href="/powering-semantic-similarity-search-in-computer-vision-with-state-of-the-art-embeddings-f6c183fff134"><div class="pz ab fo"><div class="qa ab qb cl cj qc"><h2 class="bd iu gy z fp qd fr fs qe fu fw is bi translated">用最先进的嵌入技术支持计算机视觉中的语义相似性搜索</h2><div class="qf l"><h3 class="bd b gy z fp qd fr fs qe fu fw dk translated">执行图像到图像和文本到图像相似性搜索的最简单方法</h3></div><div class="qg l"><p class="bd b dl z fp qd fr fs qe fu fw dk translated">towardsdatascience.com</p></div></div><div class="qh l"><div class="qi l qj qk ql qh qm ks py"/></div></div></a></div><p id="a2b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请阅读下面的帖子，了解如何在产品中部署文本图像多模态搜索引擎。</p><div class="pv pw gp gr px py"><a href="https://blog.milvus.io/supercharged-semantic-similarity-search-in-production-f2a3c35c4e00" rel="noopener  ugc nofollow" target="_blank"><div class="pz ab fo"><div class="qa ab qb cl cj qc"><h2 class="bd iu gy z fp qd fr fs qe fu fw is bi translated">生产中的超级语义相似性搜索</h2><div class="qf l"><h3 class="bd b gy z fp qd fr fs qe fu fw dk translated">利用剪辑嵌入和Milvus实现超高速、高度可扩展的文本到图像搜索</h3></div><div class="qg l"><p class="bd b dl z fp qd fr fs qe fu fw dk translated">blog.milvus.io</p></div></div><div class="qh l"><div class="qn l qj qk ql qh qm ks py"/></div></div></a></div></div></div>    
</body>
</html>