<html>
<head>
<title>Findings from benchmarking churn prediction methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基准测试流失预测方法的发现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/findings-from-benchmarking-churn-prediction-methods-95940683523d#2022-09-23">https://towardsdatascience.com/findings-from-benchmarking-churn-prediction-methods-95940683523d#2022-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e407" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过比较广泛使用的客户流失预测方法获得的见解</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1075f8fe587aac4eb32bbd95e1007b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tqbPxaMsc_h6-o9n"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/photos/Y20k53JEpUI" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@robertbye" rel="noopener ugc nofollow" target="_blank"> Robert Bye </a>拍摄的照片。</p></figure><p id="4b1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> T </span>他的文章展示了我上一篇(技术性更强)文章中的结果和发现，我在文章中解释了如何构建一个<strong class="lb iu">通用管道来对流失预测方法进行基准测试。</strong>这篇文章背后的动机是来自<a class="ae ky" href="https://link.springer.com/article/10.1007/s41060-022-00312-5#citeas" rel="noopener ugc nofollow" target="_blank"> Geiler等人(2022) </a>的一篇论文，该论文对不同的流失预测方法进行了基准测试。</p><h1 id="cda0" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">TL；博士；医生</h1><ul class=""><li id="17e3" class="mx my it lb b lc mz lf na li nb lm nc lq nd lu ne nf ng nh bi translated">根据我的<a class="ae ky" rel="noopener" target="_blank" href="/a-pipeline-for-benchmarking-churn-prediction-approaches-f5b533c53e30">上一篇文章</a>，常见的客户流失方法在<strong class="lb iu">五个免费可用的客户流失数据集</strong>上进行了基准测试。</li><li id="5fe6" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">由于样本量小(只有五个数据集)，这些发现<strong class="lb iu">可能不具有代表性</strong>，但可以为您提供关于<strong class="lb iu">在您的下一个流失预测项目中考虑</strong>哪些方法的想法。</li><li id="8568" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">在处理<strong class="lb iu">高等级不平衡</strong>时，<strong class="lb iu">采样方法并不总能提高你的模型性能</strong>。</li><li id="5827" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">根据您偏好的误差度量，最佳方法是没有采样(PR AUC)的<strong class="lb iu">软投票分类器模型</strong>(逻辑回归+ XGB +随机森林)或具有<strong class="lb iu">过采样</strong> (SMOTE)和<strong class="lb iu">随机过采样</strong> (RND)方法(F2得分)的<strong class="lb iu">逻辑回归</strong>。</li></ul></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="ff80" class="mf mg it bd mh mi nu mk ml mm nv mo mp jz nw ka mr kc nx kd mt kf ny kg mv mw bi translated">回顾和方法</h1><p id="bf8d" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">在讨论结果之前，我想给你一个快速回顾所使用的管道和方法。图1显示了所使用的基准测试管道的整体结构，从通过预清理步骤加载数据开始，到实际的基准测试和结果的最终可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/412f1025daab52bad5bb2a7bc3d467fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPmURKvcfMXUfNZYwF0NXg@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。基准测试流程概述(图片由作者提供)。</p></figure><p id="3241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“动态部分”(绿色)表示不同的方法或机器学习模型和采样方法的组合，它们“动态地”附加到流水线的静态部分(估算器、缩放器和编码器)。</p><h2 id="c4ec" class="od mg it bd mh oe of dn ml og oh dp mp li oi oj mr lm ok ol mt lq om on mv oo bi translated">🛢数据集</h2><p id="9817" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">对于这篇文章，我只使用了免费使用的客户流失数据集，并且有一个“真实的”流失率。我所理解的“现实流失率”是20%或更少的高等级不平衡。如果你的数据有40%的流失率，你应该考虑你是否真的想做流失率预测或更好地分析你的商业模式，因为几乎一半的客户正在离开。</p><p id="9877" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表1总结了所使用的数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/384f86a7de3f482ad8826c7f4ffe4686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7d21woNHj9J1yqW---vfg@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1。所用数据集的总结(图片由作者提供)。</p></figure><p id="aab2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前两行显示预清理步骤后每个数据集的行数或观察值，以及相关的流失率。以下各行显示了每个数据集的列数或特征数及其数据类型数。</p><p id="e193" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所述的预清洁步骤<strong class="lb iu">移除立柱</strong> …</p><ul class=""><li id="a8b3" class="mx my it lb b lc ld lf lg li oq lm or lq os lu ne nf ng nh bi translated">丢失超过20%的值</li><li id="a8c7" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">仅常量值</li><li id="0417" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">像唯一标识符(例如，用户id或地址)</li></ul><blockquote class="ot ou ov"><p id="1adc" class="kz la me lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">除了这些规则，我还删除了KDD数据集中包含1000多个不同类别值的列。否则，在应用一次热编码步骤时，我会遇到一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维数灾难</a>问题。</p></blockquote><p id="1915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文使用的数据集如下:</p><ul class=""><li id="25de" class="mx my it lb b lc ld lf lg li oq lm or lq os lu ne nf ng nh bi translated"><strong class="lb iu"> ACM KDD杯— 2009 ( </strong> <code class="fe oz pa pb pc b">kdd</code> <strong class="lb iu"> ) </strong>:来自法国电信公司Orange的营销数据库，用于预测客户更换提供商的倾向(<a class="ae ky" href="https://www.openml.org/search?type=data&amp;status=active&amp;id=1112&amp;sort=runs" rel="noopener ugc nofollow" target="_blank"> CC0:公共领域</a>)。它是具有最多观察和特征的数据集。</li><li id="0343" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu"> IBM HR Analytics员工流失&amp;绩效(</strong> <code class="fe oz pa pb pc b">ibm_hr</code> <strong class="lb iu"> ) </strong>:由IBM数据科学家创建的虚构数据集，其中包含导致员工流失的因素(<a class="ae ky" href="https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset" rel="noopener ugc nofollow" target="_blank">数据库内容许可证(DbCL) </a>)。</li><li id="1b8f" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu">2020年客户流失预测(</strong> <code class="fe oz pa pb pc b">ccp</code> <strong class="lb iu"> ) </strong>:流失数据(基于与真实世界相似的说法人工得出)。数据也是<a class="ae ky" href="https://rdrr.io/cran/regclass/man/CHURN.html" rel="noopener ugc nofollow" target="_blank"> R包</a>(<a class="ae ky" href="https://rdrr.io/cran/regclass/" rel="noopener ugc nofollow" target="_blank">GPL(&gt;= 2 license</a>)的一部分。</li><li id="d6aa" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu">葡萄牙银行营销数据集(</strong> <code class="fe oz pa pb pc b">prt_bank</code> <strong class="lb iu"> ) </strong>:一家葡萄牙银行的电话直销营销活动数据集(<a class="ae ky" href="https://archive-beta.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank"> CC BY 4.0 </a>)。</li><li id="e1b7" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu">报纸流失</strong> <strong class="lb iu"> ( </strong> <code class="fe oz pa pb pc b">news</code> <strong class="lb iu"> ): </strong>关于报纸订阅用户的数据集(<a class="ae ky" href="https://www.kaggle.com/datasets/andieminogue/newspaper-churn" rel="noopener ugc nofollow" target="_blank"> CC0:公共域</a>)。</li></ul><h2 id="3c75" class="od mg it bd mh oe of dn ml og oh dp mp li oi oj mr lm ok ol mt lq om on mv oo bi translated">⚖️阶级不平衡</h2><p id="560f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">在现实生活中，处理接近50%的类平衡是非常罕见的。特别是在客户流失预测领域，人们通常不得不处理高度不平衡的问题(流失者占少数)。imblearn包提供了一系列不同的方法来处理这个问题。使用了以下几种(及其组合):</p><ul class=""><li id="a69a" class="mx my it lb b lc ld lf lg li oq lm or lq os lu ne nf ng nh bi translated"><strong class="lb iu">不取样</strong>(<code class="fe oz pa pb pc b">no_sampling</code>)——我们不采用任何方法</li><li id="765a" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>(<code class="fe oz pa pb pc b">o_SMOTE</code>)—过采样</li><li id="2f09" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.ADASYN.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>(<code class="fe oz pa pb pc b">o_ADASYN</code>)—过采样</li><li id="ba3b" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.TomekLinks.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>(<code class="fe oz pa pb pc b">u_TomekLinks</code>)—欠采样</li><li id="5974" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.NeighbourhoodCleaningRule.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">【NCR】</strong></a>(<code class="fe oz pa pb pc b">u_NCR</code>)—欠采样</li><li id="b97c" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu"> SMOTE和</strong><a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">RND</strong></a>(<code class="fe oz pa pb pc b">h_SMOTE_RND</code>)—过采样和欠采样</li><li id="7082" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu"> SMOTE和TomekLinks </strong> ( <code class="fe oz pa pb pc b">h_SMOTE_TomekLinks</code> ) —过采样和欠采样</li><li id="097a" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><strong class="lb iu">斯莫特和NCR </strong> ( <code class="fe oz pa pb pc b">h_SMOTE_NCR</code> ) —过采样和欠采样</li></ul><h2 id="6d90" class="od mg it bd mh oe of dn ml og oh dp mp li oi oj mr lm ok ol mt lq om on mv oo bi translated">📦模型</h2><ul class=""><li id="233b" class="mx my it lb b lc mz lf na li nb lm nc lq nd lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">逻辑回归</strong> </a> ( <code class="fe oz pa pb pc b">lr</code>)</li><li id="1d21" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a><strong class="lb iu"/>(<code class="fe oz pa pb pc b">rf</code>)</li><li id="d4a4" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/stable/python/python_api.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> XGB量词</strong> </a> ( <code class="fe oz pa pb pc b">xgb</code>)</li><li id="3f0a" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> SVC分类器</strong> </a> ( <code class="fe oz pa pb pc b">svc</code>)</li><li id="3ea1" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">高斯朴素贝叶斯</strong> </a> ( <code class="fe oz pa pb pc b">gnb</code>)</li><li id="e7f3" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">light GBM</strong></a>(<code class="fe oz pa pb pc b">lgb</code></li><li id="ffda" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">knighborsclassifier</strong></a><strong class="lb iu"/>(<code class="fe oz pa pb pc b">knn</code>)</li><li id="03c6" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/lhagiimn/GEV-NN-A-deep-neural-network-architecture-for-class-imbalance-problem-in-binary-classification" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">GEV-NN</strong></a><strong class="lb iu"/>(<code class="fe oz pa pb pc b">gev_nn</code>)</li><li id="ff88" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://github.com/naomifridman/Neural-Network-Churn-Prediction/blob/master/FFNN_churn_predict_0_12174.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">前馈神经网络</strong> </a> ( <code class="fe oz pa pb pc b">ffnn</code>)</li><li id="4055" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">voting Classifier 1</strong></a>:逻辑回归、XGB分类器、随机森林(<code class="fe oz pa pb pc b">lr_xgb_rf</code>)</li><li id="03ed" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">voting Classifier 2</strong></a>:逻辑回归、XGB分类器、随机森林、前馈神经网络(<code class="fe oz pa pb pc b">lr_xgb_rf_ffnn</code>)</li></ul><blockquote class="ot ou ov"><p id="4a29" class="kz la me lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">投票分类器使用<a class="ae ky" href="https://www.geeksforgeeks.org/ml-voting-classifier-using-sklearn/" rel="noopener ugc nofollow" target="_blank">软投票</a>，这意味着它们的结果是它们使用的模型预测的平均值。</p></blockquote><h2 id="1b41" class="od mg it bd mh oe of dn ml og oh dp mp li oi oj mr lm ok ol mt lq om on mv oo bi translated">⏱️误差度量</h2><p id="2ac8" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">使用了以下误差指标:</p><ul class=""><li id="cb6a" class="mx my it lb b lc ld lf lg li oq lm or lq os lu ne nf ng nh bi translated"><a class="ae ky" href="https://rasbt.github.io/mlxtend/user_guide/evaluate/lift_score/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">提升得分</strong> </a>(将模型预测与随机生成的预测进行比较)</li><li id="6814" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> ROC AUC </strong> </a></li><li id="585f" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> F1分数</strong> </a>(用于真实类和宏)</li><li id="3171" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">F2</strong>得分</a></li><li id="c5cf" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">召回</strong> </a>(在客户流失预测中，我们通常在漏报上有更高的成本)</li><li id="6ca1" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">精度</strong> </a></li><li id="92a5" class="mx my it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">平均精度</strong> </a> * (PR AUC)</li></ul><p id="5866" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多关于“正确”误差度量的讨论。首先，<strong class="lb iu">没有银弹</strong>。选择正确的错误度量标准不仅取决于您的<strong class="lb iu">用例</strong>或<strong class="lb iu">优先级</strong>(例如，您在假阴性或假阳性上的成本更高吗？)还取决于您的数据(例如，处理<strong class="lb iu">强烈的类别不平衡</strong>)以及您是否想要<strong class="lb iu">预测类别</strong>(与阈值相关)或<strong class="lb iu">概率</strong>(与阈值无关)。因此，更有意义的是<strong class="lb iu">考虑几个指标</strong>来更清楚地了解你的模型的性能。</p><p id="f831" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> F1分数</strong>同等对待精度和召回<strong class="lb iu"/>。然而，在客户流失预测的情况下，我们通常会有<strong class="lb iu">更高的假阴性成本</strong>(获取客户的成本通常远高于留住客户的成本)。<strong class="lb iu"> F2分数</strong>在召回上增加了<strong class="lb iu">更高的权重，在等式的精度部分增加了更低的权重。</strong></p><p id="3ebb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我们将分类器的默认阈值(0.5)更改为0.34，因为我们认为这是一个更好的阈值，我们的<strong class="lb iu"> F2分数</strong>和<strong class="lb iu">召回度量</strong>将<strong class="lb iu">更改它们的值</strong>。阈值独立指标是<strong class="lb iu"> ROC AUC </strong>和<strong class="lb iu"> PR AUC </strong>。对于不平衡数据集，Saito和Rehmsmeier (2015)和Czakon (2022)更喜欢PR AUC而不是ROC AUC。</p><blockquote class="ot ou ov"><p id="17bb" class="kz la me lb b lc ld ju le lf lg jx lh ow lj lk ll ox ln lo lp oy lr ls lt lu im bi translated">*人们也可以<strong class="lb iu">认为PR AUC是为每个回忆阈值</strong>计算的精确度分数的平均值(Czakon，2022)。</p></blockquote><h2 id="9dee" class="od mg it bd mh oe of dn ml og oh dp mp li oi oj mr lm ok ol mt lq om on mv oo bi translated">🔬交叉验证</h2><p id="50ff" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">为了计算分数，我使用了<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html" rel="noopener ugc nofollow" target="_blank">重复分层k倍交叉验证</a>，其中n_repeats和n_splits = 5。这种方法通常在处理不平衡数据集时使用。在每个折叠中，每个目标类的样本百分比大致相同。</p><h1 id="12a1" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结果</h1><p id="a808" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">结果可以通过使用像箱线图或可视化表格这样的图表来解释。我将在下面使用两者，从F2开始。下面的图2显示了通过对所有五个数据集使用不同的采样方法得到的每个模型的F2分数。平均值由绿色三角形标记表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/0fa838e53fbc31f05f32137751a32e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnZSJr9PfapZDIVwdZcU8Q@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。对所有数据集使用不同采样方法的每个模型的F2分数的箱线图(图片由作者提供)。</p></figure><p id="d1b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从方框图中看到，与深度学习模型(<code class="fe oz pa pb pc b">gev_nn</code>、<code class="fe oz pa pb pc b">ffnn</code>)相比，基于树的模型(<code class="fe oz pa pb pc b">rf</code>、<code class="fe oz pa pb pc b">lgb</code>、<code class="fe oz pa pb pc b">xgb</code>)总体上显示出更广泛的分布。我们还可以观察到，当使用混合采样方法(<code class="fe oz pa pb pc b">h_SMOTE_RND</code>、<code class="fe oz pa pb pc b">h_SMOTE_Tomek</code>、<code class="fe oz pa pb pc b">h_SMOTE_NCR</code>)时，我们的线性模型(<code class="fe oz pa pb pc b">lr</code>)的分布很小，与其他方法相比，其性能相当好。</p><p id="5c36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下表(表2)显示了每种方法的平均F2分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/58ab39ba2733237ab2a215e1fadc3a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gy5ctTFSLfAN0MSxGt-qpg@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表二。对所有数据集使用不同采样方法的每个模型的平均F2值(图片由作者提供)。</p></figure><p id="a29a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该表可以按行读取。粗体显示的F2分数是相应模型采样方法的最高(最好)分数。<strong class="lb iu">粗体和</strong> <strong class="lb iu">绿色高亮</strong>的单元格是具有<strong class="lb iu">最佳综合得分</strong>的方法。当使用<strong class="lb iu"> SMOTE+RND </strong>过采样方法时，逻辑回归(<code class="fe oz pa pb pc b">lr</code>)模型得到了<strong class="lb iu">总体最高的F2分数</strong>。</p><p id="24ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，F2分数取决于阈值。一个独立于阈值的指标是PR AUC指标，如下图所示(图3)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/d62e1abafa33a8151c95cd7ad4a69df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMpy26RoZRf_LqMfFqQYqg@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。对所有数据集使用不同采样方法的每个模型的PR AUC分数的箱线图(图片由作者提供)。</p></figure><p id="0b76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，基本上高斯朴素贝叶斯(<code class="fe oz pa pb pc b">gnb</code>)显示了最大的扩散，而投票分类器方法(<code class="fe oz pa pb pc b">lr_xgb_rf</code>、<code class="fe oz pa pb pc b">lr_xgb_rf_ffnn</code>)显示了最小的扩散。通过查看表格(表3)，我们可以看到，最好的方法是没有抽样方法的投票分类器模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/e5143d233928ea54a2aff4533ee30b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxJ_Ly5Gz-knTeCwoQYXnw@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表3。在所有数据集上使用不同取样方法的每个模型的平均PR AUC值(图片由作者提供)。</p></figure><p id="3a8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的特定情况下，使用逻辑回归、XGB分类器、随机森林(<code class="fe oz pa pb pc b">lr_xgb_rf</code>)的软投票分类器比使用前馈神经网络(<code class="fe oz pa pb pc b">lr_xgb_rf_ffnn</code>)的软投票分类器表现稍好。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="f778" class="mf mg it bd mh mi nu mk ml mm nv mo mp jz nw ka mr kc nx kd mt kf ny kg mv mw bi translated">结论</h1><p id="5d34" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">在这篇文章中，我们使用了五个免费的客户流失数据集对不同的方法进行了基准测试。需要指出的是，少量的数据集可能没有足够的代表性来得出可靠的结论。</p><p id="7fe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我希望这篇文章能让你知道在你的下一个客户流失预测项目中，你可以尝试哪些方法(例如，没有抽样方法的软分类器)。模型的代码和更详细的解释可以在我的<a class="ae ky" rel="noopener" target="_blank" href="/a-pipeline-for-benchmarking-churn-prediction-approaches-f5b533c53e30">上一篇文章</a>中找到。</p><p id="1cce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对提到的其他指标的值感兴趣，请参见下面的附录。</p><h1 id="a1f3" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">来源</h1><p id="1c9b" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">盖勒，l .，阿费尔特，s .，纳迪夫，m .，2022。<em class="me">流失预测的机器学习方法综述</em>。<em class="me"> </em> Int J Data Sci Anal。<a class="ae ky" href="https://doi.org/10.1007/s41060-022-00312-5" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s41060-022-00312-5</a></p><p id="0626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">蒙赫达莱，l，蒙赫达莱，t，刘，K.H，2020。<em class="me"> GEV-NN: </em> <em class="me">针对二元分类中类别不平衡问题的深度神经网络架构</em>。基于知识的系统。【https://doi.org/10.1016/j.knosys.2020.105534 T4】</p><p id="74e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">j .布朗利(2020年2月)。<em class="me">机器学习Fbeta-Measure的温和介绍</em>。机器学习掌握。<a class="ae ky" href="https://machinelearningmastery.com/fbeta-measure-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/fbeta-measure-for-machine-learning/</a></p><p id="ff53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Czakon，J. (2022年7月21日)。<em class="me"> F1评分vs ROC AUC vs准确性vs PR AUC:应该选择哪种评价指标？</em>Neptune . ai . 2022年9月18日检索，来自<a class="ae ky" href="https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc" rel="noopener ugc nofollow" target="_blank">https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc</a></p><p id="5b6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Saito和m . rehms Meier(2015年3月4日)。<em class="me">在不平衡数据集上评估二元分类器时，精确召回图比ROC图提供的信息更多</em>。PLOS一号，10(3)，e0118432。<a class="ae ky" href="https://doi.org/10.1371/journal.pone.0118432" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1371/journal.pone.0118432</a></p><h1 id="07eb" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">附录</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/523e26b71da83a1eddf717d7aeb55baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzqHcz6I5SWX4nGm9g7aOA@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A1。F1宏观评分(图片由作者提供)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/932964a27f2935e4e15ae1e4654df323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_tnAi_1MN3qhNHxamSB2Q@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A2。F2分数(图片由作者提供)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/604d301398d4b1402aa0f6db6cf2750c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhLs0SolZBB-8Z10_w4FOQ@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A3。Lift评分(图片由作者提供)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/053ca91339aa225850f774ec8005495f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXORezdcB8bVXY1swB0ZFg@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A4。精度(图片由作者提供)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/708576820873adfce49971d0c511826a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C8wbcpRFqBhIwRGPYyg9aA@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A5。回忆(图片由作者提供)。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/1888120ab8940498c11ccebb26958b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCYjZdKySRYa9noS5_M-xw@2x.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表A6。ROC AUC(图片由作者提供)。</p></figure></div></div>    
</body>
</html>