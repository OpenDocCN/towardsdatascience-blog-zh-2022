<html>
<head>
<title>Image Classification with No Data?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">没有数据的图像分类？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-classification-with-no-data-b44089f8dc28#2022-12-07">https://towardsdatascience.com/image-classification-with-no-data-b44089f8dc28#2022-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b928" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用较少数据的机器学习算法的理论综述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ca724b0fbb3aeea85edefcf318c43d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2PUZgPrdqafLu_G-VzoEg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com/photos/jw3GOzxiSkw" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@rohanmakhecha" rel="noopener ugc nofollow" target="_blank"> R. Makhecha </a>拍摄的照片</p></figure><p id="4375" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你想建立一个没有多少数据的机器学习模型？众所周知，机器学习需要大量数据，而收集和标注数据需要时间，而且成本高昂。这篇文章介绍了一些用更少的数据建立一个有效的图像分类器的方法！</p><h1 id="4483" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">目录</h1><p id="7cbb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><a class="ae kv" href="#5d01" rel="noopener ugc nofollow">简介</a></p><p id="7712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#9313" rel="noopener ugc nofollow"> 1。转移学习</a></p><p id="1080" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#3bc2" rel="noopener ugc nofollow"> 2。利用未标记的数据</a></p><p id="9bc4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#0652" rel="noopener ugc nofollow"> 3。少数镜头学习</a></p><p id="dc9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#6a3a" rel="noopener ugc nofollow"> 4。弱监督学习和基于文本的零触发分类器</a></p><p id="3d42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#c771" rel="noopener ugc nofollow">结论</a></p><p id="cd0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#6405" rel="noopener ugc nofollow">参考文献</a></p><h1 id="5d01" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">介绍</h1><p id="d237" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">训练机器学习模型总是需要数据的。庞大的数据集对于构建强大的管道、避免过度拟合以及很好地推广到新图像是必要的。</p><p id="178a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，给定一些假设，您可以极大地减少所需的数据量——不管是否带注释。本文介绍了一些在没有大量数据或标记数据的情况下进行图像分类的方法。我将介绍<em class="mp">迁移学习</em>、<em class="mp">自监督学习</em>、<em class="mp">半监督学习</em>、<em class="mp">主动学习</em>、<em class="mp">少镜头图像分类</em>、<em class="mp">基于文本的零镜头分类器</em>中最重要的几个方面。</p><p id="270d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，我提出的每种方法都可以用于其他机器学习问题。</p><p id="99a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第 1 节和第 2 节介绍了用经过充分研究的方法用大约一千个带注释的样本构建分类器的方法。第 3 节和第 4 节通过热门研究主题将数据效率推向极限。</p><p id="f067" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了简洁起见，我不再谈论<em class="mp">数据增强</em>和<em class="mp">生成网络</em>。出于同样的原因，我不会深入数学或实现细节。要了解更多信息，我建议你直接阅读参考资料部分的论文。话虽如此，我们还是开始吧！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/00fc522abbbd4e363b34b80b1e423329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwv7gok_fEl72arwdvePMw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="9313" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">迁移学习</h1><p id="9876" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">当你从零开始训练一个深度神经网络时，你经常会随机初始化你的权重。这是初始化神经网络的最佳方式吗？答案一般是否定的。</p><p id="b3b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，深度学习是关于表现的。在经典的机器学习中，特征需要手工制作。深度学习背后的想法是，你让你的神经网络在训练时自己学习一种特征表示。</p><p id="a4c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在神经网络的每一层之间，都有一个输入数据的表示。你越深入你的神经网络，你的表现就应该越全面。通常，已知分类器神经网络的第一层能够检测颜色和边缘的斑点。中间层将第一层表示作为输入来计算比第一层更复杂的概念。例如，他们可能会发现猫眼或狗耳的存在。最后一层给出了图像来自每个类的概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/8105bf2ef4c383ca9b762a5108da3e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AyjuCeyAXBCMyFTNy56Ukg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">深层特征表现——作者的形象</p></figure><p id="1a7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">迁移学习</em> [ <a class="ae kv" href="#47d7" rel="noopener ugc nofollow"> 1 </a>背后的想法是，从另一个分类任务中学到的一些表征可能对你的任务有用。<em class="mp">迁移学习</em>是将预先训练好的网络的第一层用于另一项任务，在其上添加新层，并在感兴趣的数据集上微调整个网络。</p><p id="d796" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相比之下，如果你的目标是学会赢得足球比赛，<em class="mp">转移学习</em>将包括首先学会打篮球，以适应移动你的身体，锻炼你的耐力，等等。，才开始玩足球游戏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/a5c7d7a8f430e4c71f9dfc0b650764e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a2qRSyXVpRjHqiVZXI9DBg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">转移学习管道—图片由作者提供</p></figure><p id="cdb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它将如何影响最终网络的性能？你应该在哪里切断你预先训练的网络？这些问题在[ <a class="ae kv" href="#47d7" rel="noopener ugc nofollow"> 1 </a>中有详细阐述。</p><p id="cbfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下最重要的观点:</p><ul class=""><li id="1200" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">神经网络的第一层非常一般，而最深层是预训练任务中最专业的。因此，你可以预期，如果你的预训练任务接近你的目标任务，那么保持更多的层将更有益。</li><li id="11a8" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">在中间层进行切割通常会导致性能下降。这是由于通过微调在中间层获得了脆弱的平衡。</li><li id="5460" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">使用预先训练的权重总是比使用随机初始化的权重更好。这是因为，通过先在另一项任务上进行训练，你的模型学会了用其他方法学不到的特性。</li><li id="0826" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">当重新训练这些预训练的权重时，可以获得更好的性能-最终对它们使用较低的学习速率，或者在几个时期后解冻它们。</li></ul><h1 id="3bc2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">利用未标记的数据</h1><p id="78ae" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与已标记的数据相比，未标记的数据通常更容易访问。不利用这一点就太浪费了！</p><h2 id="7723" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">自我监督学习</h2><p id="350f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><em class="mp">自我监督学习</em> [ <a class="ae kv" href="#d398" rel="noopener ugc nofollow"> 2 </a> ]解决了从未标记数据中学习深度特征的问题。在训练了一个自我监督的模型之后，可以像在<em class="mp">迁移学习</em>中一样使用特征提取器，所以你仍然需要一些带注释的数据来微调。</p><p id="4413" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，如何从未标记的数据中训练深度特征提取器呢？总之，你需要一个足够难的借口任务，使你能够为你的分类任务学习有趣的特征。</p><p id="5b0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果你想在不玩真实游戏的情况下赢得足球比赛，你可以尽可能多地训练玩杂耍球。玩杂耍球会提高你的控球技术，这在玩游戏时会派上用场。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/0d4720ef4f9b6cadce7641d43754b71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIKo9Bux8neut5TozK1uWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自我监督学习管道[ <a class="ae kv" href="#d398" rel="noopener ugc nofollow"> 2 </a></p></figure><p id="0e65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">借口任务的一个例子是预测图像的旋转角度。基本上，对于每个图像，你应用一个旋转<em class="mp"> z </em>来得到旋转后的图像<em class="mp"> x </em>。然后你训练一个神经网络从<em class="mp"> x 预测<em class="mp">z</em></em>这个旋转预测任务迫使你的网络深入理解你的数据。事实上，要预测一幅狗的图像的旋转，你的网络首先需要了解图像中有一只狗，并且一只狗应该以特定的方式定向。</p><div class="kg kh ki kj gt ab cb"><figure class="nu kk nv nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/3b4b5f907352c4f97f789d80f5c6b6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*afiovzcmxeqf7CgCaqUULg.png"/></div></figure><figure class="nu kk oa nw nx ny nz paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/64f233f6950b902e0a484d0638226fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*FgPC9NiVYXFN-3nOutb4iw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk ob di oc od translated">托辞任务示例:旋转变换预测—(左)[ <a class="ae kv" href="#d398" rel="noopener ugc nofollow"> 2 </a> ] |(右)作者图片</p></figure></div><p id="c2c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">借口任务可以根据你的具体目标有很大的不同。常用的借口任务包括:</p><ul class=""><li id="0490" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">变换预测:数据集中的样本被变换修改，您的网络学习预测该变换。</li><li id="3031" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">屏蔽预测:输入图像的随机正方形被屏蔽，网络必须预测图像的屏蔽部分。</li><li id="a486" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">实例辨别:学习一种分离所有数据样本的表示法。例如，每个数据点可以被认为是一个类，并且可以在这个任务上训练一个分类器。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/d9aa59fda00de9357404b760d2a461fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NHoA6w9D6EM72OVdT9u_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自我监督借口任务的示例[ <a class="ae kv" href="#d398" rel="noopener ugc nofollow"> 2 </a></p></figure><h2 id="cf28" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">半监督学习</h2><p id="9f91" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><em class="mp">半监督学习</em> [ <a class="ae kv" href="#c211" rel="noopener ugc nofollow"> 4 </a> ]非常类似于<em class="mp">自监督学习</em>，因为它也涉及到拥有一个小的带注释的数据集以及一个大的未标记的数据集。根据你所取的定义，<em class="mp">自监督学习</em>可以看作是<em class="mp">半监督学习</em>的一个特例。然而，在更严格的定义中，<em class="mp">半监督学习</em>在于直接利用未标记的数据作为训练样本，而不是作为学习有意义特征的方法。</p><p id="22e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最流行的做<em class="mp">半监督学习</em>的方式是<em class="mp">自学</em> [ <a class="ae kv" href="#fda4" rel="noopener ugc nofollow"> 3 </a>。基本上，您在已标记的数据上训练基线，在未标记的数据上推断以获得伪标签，仅保留具有最高置信度的伪标签，并在已标记和剩余伪标记数据的联合上训练。</p><p id="e8f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，<em class="mp">自我训练</em>在于根据标记数据训练一个模型，然后这个第一个模型成为学生模型的老师。然后，学生模型可以成为老师，等等。至于迭代次数，通常情况下，一两次迭代就足够让你的度量最大化了。</p><p id="7709" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继续与赢得足球比赛进行比较，<em class="mp">半监督学习</em>就是你观看足球比赛的录像，教练分析每场比赛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/08c7bbc233503cbffe1a61f130a25bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*1zA4mTbRixKx7thUGu6F9Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带自我训练的半监督学习[ <a class="ae kv" href="#fda4" rel="noopener ugc nofollow"> 3 </a></p></figure><p id="45f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">半监督学习</em>和<em class="mp">自我监督学习</em>比<em class="mp">迁移学习</em>更有效。这仅仅是因为<em class="mp">半监督</em>或<em class="mp">自监督</em>设置中的数据更接近目标任务。尽管如此，在<em class="mp">半监督</em>和<em class="mp">自我监督</em>之间并没有明显的赢家。</p><h2 id="306a" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">主动学习</h2><p id="c258" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">上面介绍的所有方法都假设您已经注释了一些数据，并且您不能或不想注释更多的数据。<em class="mp">主动学习</em> [ <a class="ae kv" href="#7989" rel="noopener ugc nofollow"> 5 </a> ]给你一套规则，让你选择标注哪些数据，以最少的标注样本获得最好的结果。</p><p id="3940" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要注释哪些数据？答案是相当自然的:您应该总是为您的模型注释硬样本。这样，您就迫使您的模型学习更精确的特征来识别您的类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a2df1d1ba4df69fa14faa6ef6c83b51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*LcAMNhpugbv-grfbE4OBOw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用主动学习进行注解。“先知”是一个人类注释者。</p></figure><p id="d906" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在学习赢得足球比赛时，经典训练集中在你的缺陷上。假设你是前锋，你会希望掌握控球，达到良好的身体爆发力，并有很大的投篮能力。这些都要掌握，而不是直接打游戏，最有效率的方法就是一个一个的训练每个技能，先把重点放在自己最差的技能上。</p><p id="60e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以现在，你会问如何确定这些“硬样本”。这就是事情变得复杂的地方。一个简单的基线是随机标记一小部分数据，在此基础上训练一个模型，在此基础上进行推断，并标注那些你最没有信心的数据。但是，因为大型网络通常过于自信，所以通常建议估计不确定性，而不是直接使用神经网络的输出。为了得到不确定性估计，你可以使用<em class="mp">高斯过程</em>或者<em class="mp">贝叶斯网络</em>作为例子。</p><h1 id="0652" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">少量学习，将数据效率推向极限</h1><p id="cd0d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><em class="mp">少量学习</em>是从很少的样本中学习。典型的数量级是每类 1 到 20 个样本。虽然深度学习通常需要至少数千个样本，但<em class="mp">少量学习</em>解决了根本没有数据的问题。这个领域目前是一个热门的研究课题，所以你可能不想马上投入生产。</p><p id="134a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它是如何工作的？怎么能从这么少的数据中学会分类呢？执行<em class="mp">少镜头图像分类</em>的方法有很多种。我将只介绍其中的几个，主要是那些对研究有最重要影响的。</p><h2 id="fa1b" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">基础设置:公制学习和质子</h2><p id="6924" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">还记得我说过深度学习都是关于表象的吗？因为你只有几十个样本，你不能训练一个神经网络去理解每一类，并对其中一类图像进行分类。其思想是通过利用预先训练的特征提取器来找到到特定类的距离。可以用上面提到的任何方法来训练特征提取器。</p><p id="8577" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设每个类别只有一个图像，并且您想对一个新的未标记图像进行分类。给定一个特征提取器，您可以提取每个标记图像以及未标记图像的特征表示。如果提取的特征足够好，则从未标记图像中提取的特征应该与来自同一类别的图像的特征最相似。然后，您可以计算到每个类的距离。您可以使用神经网络或常规距离，如<em class="mp">余弦距离</em>或<em class="mp">均方误差</em>。那就是所谓的<em class="mp">公制学习</em> [ <a class="ae kv" href="#9597" rel="noopener ugc nofollow"> 6 </a> ][ <a class="ae kv" href="#866f" rel="noopener ugc nofollow"> 7 </a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/90282522931e114c557b601bd6816223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*nktJCCqB_4g31eXISLVUFA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">公制学习设置[ <a class="ae kv" href="#9597" rel="noopener ugc nofollow"> 6 </a></p></figure><p id="1db3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你每门课都有几张图像，研究论文通常会使用质子设置[ <a class="ae kv" href="#2396" rel="noopener ugc nofollow"> 8 </a> ]。它包括为每个类计算一个原型。基本上，对于每个类，你用预先训练好的特征提取器计算所有的特征，并采取措施得到每个类的原型。那么一切工作就好像你只有一个图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/3b182e15201a77a6e988c5929d429c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qS1uJjry6bYz-GSnLFO__A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原型网络[8]</p></figure><h2 id="5b79" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">元学习</h2><p id="39ec" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与<em class="mp">度量学习</em>，<em class="mp">元学习</em> [ <a class="ae kv" href="#995e" rel="noopener ugc nofollow"> 9 </a> ]是一种完全不同的方式来执行<em class="mp">少量学习</em>。这里的想法是，首先，训练一个模型，以便在经过几个时期的微调后，它可以获得最佳指标。</p><p id="ff6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，您希望找到最佳的权重初始化，这样您只需在几个时期内使用少量数据进行微调。为了找到最佳的初始化，使用学习模式。这就是为什么<em class="mp">元学习</em>又叫<em class="mp">学习学习</em>的原因。</p><p id="5730" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种学会学习的想法在开始时有点令人不安。足球的元学习将是你在尽可能多的运动中训练，网球、篮球、橄榄球、排球、肌肉等等。正因为如此，你现在可以适应任何不需要训练的运动，包括足球。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3d10c99f2603a4261e58e27ce8fbffd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*k6dBz5J2f7tN2R5iov5X8w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">快速适应目标任务的元学习</p></figure><p id="5b49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设你有三个不同的任务，数据很少。<em class="mp"> θ* </em>代表每项任务的理论最佳值。经过几次微调后，你应该从哪里开始训练以获得最佳效果？元学习试图找到最佳的初始化<em class="mp"> θ </em>以在微调后最接近所有任务的最优。</p><p id="22fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了避免让你迷路，我就不细说数学了。我建议你阅读这篇论文，以便更好地理解它。</p><p id="97b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">元学习</em>的想法似乎真的很有趣，但在实践中，与其他方法相比，它没有达到良好的性能指标。此外，为了元训练您的模型，您需要几个不同的任务，这在实践中很少出现。</p><h2 id="59b4" class="nh lt iq bd lu ni nj dn ly nk nl dp mc lf nm nn me lj no np mg ln nq nr mi ns bi translated">最新技术和局限性</h2><p id="358b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">目前排行榜上最好的模特是<em class="mp">P&gt;M&gt;F</em>[<a class="ae kv" href="#c5a7" rel="noopener ugc nofollow">10</a>。本文结合了许多其他论文，研究了全球管道各部分的影响。特征提取器是一个预先训练好的<em class="mp">视觉转换器</em>，使用<em class="mp">屏蔽自我监督</em> <em class="mp">方法</em>。然后用<em class="mp">质子</em>设置对模型<em class="mp">进行元训练</em>。为了执行特定的分类任务，需要对它进行微调。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/7625d4a1e17b8798f255dfae98dc4324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpcYJmFFvqFTzrrmV4eOmA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://paperswithcode.com/sota/few-shot-image-classification-on-mini-2" rel="noopener ugc nofollow" target="_blank">在 Mini-Imagenet 5 路单镜头上进行代码基准测试的纸张</a></p></figure><p id="01d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最好的模型达到 95%的准确率！是不是意味着我们不再需要带标签的数据了？不幸的是，目前的研究集中在不切实际的学术数据集上。基准测试通常是用少量的类(图中的基准测试中有 5 个类)来计算的，并且测试样本是从均匀分布中抽取的。这使得目前的方法在实际应用中不可靠。少镜头图像分类仍然是一个具有挑战性的领域，它肯定会随着时间的推移而改进！</p><h1 id="6a3a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">弱监督学习和基于文本的零镜头分类器</h1><p id="e2a8" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">本节呈现来自 OpenAI [ <a class="ae kv" href="#c73c" rel="noopener ugc nofollow"> 12 </a> ]的片段。这种架构通过将图像和文本结合在一起，在 2021 年彻底改变了深度学习。这里，零拍摄意味着该模型能够对图像进行分类，而无需使用标记类进行专门训练。</p><p id="7647" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CLIP 由两个编码器或特征提取器组成:一个用于图像，另一个用于文本。两个编码器在相同的潜在空间中分别变换图像和文本，使得能够在文本和图像之间用提取的特征的简单余弦距离进行比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/e516485f7c64bf9781a44847703fbb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iQttTaO9p4ADLQn4djdCQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f3fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是怎么训练出来的？使用弱监督学习来训练 CLIP。它包含了大量的噪声数据。它利用了你可以从互联网上截取的大量图片说明数据。即使来自互联网的数据极其嘈杂，但事实证明 CLIP 相当准确，具有非常强的零拍能力。</p><p id="4515" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于其大规模和嘈杂的训练数据集，CLIP 可以在不进行任何微调的情况下对常见图像进行分类，同时比经典分类器更健壮、更准确。OpenAI 在其<a class="ae kv" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">博客</a>上展示了一份更加准确的分析。</p><p id="fc19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CLIP 有什么缺点吗？！嗯，凡事都有代价…</p><blockquote class="om on oo"><p id="4a64" class="kw kx mp ky b kz la jr lb lc ld ju le op lg lh li oq lk ll lm or lo lp lq lr ij bi translated">最大的 ResNet 模型 RN50x64 在 592 个 V100 GPU 上训练了 18 天，而最大的 Vision Transformer 在 256 个 V100 GPU 上训练了 12 天。</p></blockquote><p id="b9b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不是每个人——实际上，几乎没有人——都有能力训练如此庞大的神经网络。我甚至不会谈论训练数据集的大小。就推理而言，这完全是关于计算资源和准确性之间的权衡。CLIP 有几种较小的版本。</p><p id="826c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管巨大而缓慢，CLIP 可以用来生成高质量的伪标签，然后训练一个较小的网络。此外，当伪标签可用时，人工注释通常会更快，因为注释器只需在几个类之间进行选择，而不是几十或几百个类。</p><h1 id="c771" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="4c45" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">研究太令人兴奋了！我发现在没有太多数据的情况下能够做得这么好简直是疯了。没有创造力，这些方法是不可能的。创造力开启了如此多的可能性！</p><p id="733c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下这些想法:</p><ul class=""><li id="0cc0" class="mt mu iq ky b kz la lc ld lf mv lj mw ln mx lr my mz na nb bi translated">当你有一个巨大的标注数据集接近你的目标任务时，一定要迁移学习。</li><li id="b3d5" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">当你有大量的未标注数据和标注数据的子集时，进行自监督学习或半监督学习。</li><li id="b01b" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">如果您想高效地注释数据，请使用主动学习。</li><li id="9e15" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">少镜头图像分类可以在几乎没有标注数据的情况下对图像进行分类。尽管它仍在研究中，但目前的基准是令人鼓舞的。</li><li id="32b2" class="mt mu iq ky b kz nc lc nd lf ne lj nf ln ng lr my mz na nb bi translated">基于文本的零起点学习者在未来也可能发挥主导作用。它们可以开箱即用，不需要任何数据。</li></ul><p id="5fdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我没有在少量学习中展示<em class="mp">直推模型，因为我个人认为这些在实际应用中是不可用的。你可能想看看他们，因为他们曾经统治了代码为</em>的<em class="mp">报纸的排行榜。</em></p><p id="ce1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不要忘记查看<em class="mp">正则化、</em> <em class="mp">数据增强、</em>和<em class="mp">生成模型</em>。这些可能会更容易解决你的问题。</p><p id="8fc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">恭喜你。您已到达这篇文章的结尾。我希望你喜欢读它。你现在可以毫无畏惧地阅读所有的报纸了。不要犹豫评论，伸手！</p><h1 id="6405" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="47d7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] J. Yosinski，J. Clune，Y. Bengio，et al .<a class="ae kv" href="https://arxiv.org/abs/1411.1792" rel="noopener ugc nofollow" target="_blank">深度神经网络中的特征有多大的可转移性？</a> (2014 年)，NIPS 2014 年</p><p id="d398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] L. Ericsson，H. Gouk，C. C. Loy 等.<a class="ae kv" href="https://arxiv.org/abs/2110.09327" rel="noopener ugc nofollow" target="_blank">自我监督表征学习:介绍、进展与挑战</a> (2021)，IEEE 2022</p><p id="fda4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]谢，梁，何，等.<a class="ae kv" href="https://arxiv.org/abs/1911.04252" rel="noopener ugc nofollow" target="_blank">噪声学生自我训练改善图像网络分类</a>(2020)2020</p><p id="c211" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] Y .奥阿利，c .胡德洛，和 m .塔米，<a class="ae kv" href="https://arxiv.org/abs/2006.05278" rel="noopener ugc nofollow" target="_blank">深度半监督学习概述</a> (2020)，arXiv 预印本 arXiv:2006.05278</p><p id="7989" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]任平，肖，常，等.<a class="ae kv" href="https://arxiv.org/abs/2009.00236" rel="noopener ugc nofollow" target="_blank">深度主动学习综述</a> (2021)，ACM 计算综述 2022</p><p id="9597" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6] O. Vinyals，C. Blundell，T. Lillicrap 等，<a class="ae kv" href="https://arxiv.org/abs/1606.04080" rel="noopener ugc nofollow" target="_blank">一次学习的匹配网络</a> (2017)，NIPS 2016</p><p id="866f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]宋凤英，杨玉英，张立荣等.<a class="ae kv" href="https://arxiv.org/abs/1711.06025" rel="noopener ugc nofollow" target="_blank">学会比较:少投学习的关系网络</a>(2018)2018</p><p id="2396" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8] J. Snell，K. Swersky 和 R. S. Zemel，<a class="ae kv" href="https://arxiv.org/abs/1703.05175" rel="noopener ugc nofollow" target="_blank">用于少量学习的原型网络</a> (2017)，NIPS 2017</p><p id="995e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9] C. Finn，P. Abbeel 和 S. Levine，<a class="ae kv" href="https://arxiv.org/abs/1703.03400" rel="noopener ugc nofollow" target="_blank">用于深度网络快速适应的模型不可知元学习</a> (2017)，ICML 2017</p><p id="c5a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[10]胡世祥，李，施蒂默等，<a class="ae kv" href="https://arxiv.org/abs/2204.07305" rel="noopener ugc nofollow" target="_blank">挑战简单流水线的极限:外部数据和微调有所作为</a> (2022)，2022</p><p id="2fab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[11] E. Bennequin，m .塔米，A. Toubhans，等人，<a class="ae kv" href="https://arxiv.org/abs/2205.05155" rel="noopener ugc nofollow" target="_blank">少拍图像分类基准离现实太远:用语义任务采样重建得更好</a> (2022)，CVPR 2022</p><p id="c73c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[12] A .拉德福德，J. W .金，c .哈勒西等，<a class="ae kv" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型</a> (2021)，PMLR 2021</p></div></div>    
</body>
</html>