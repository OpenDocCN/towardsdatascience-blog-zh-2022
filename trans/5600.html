<html>
<head>
<title>Make Your Art Move with Stable Diffusion Animations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用稳定的扩散动画让你的艺术动起来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/make-your-art-move-with-stable-diffusion-animations-80de62eec633#2022-12-19">https://towardsdatascience.com/make-your-art-move-with-stable-diffusion-animations-80de62eec633#2022-12-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e454" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍 Giffusion，这是一个简单的 web UI，用于创建具有稳定扩散的动画 gif 和视频</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/1ea2e45faa6fb27b9673f2a7d3b04774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*9zrKz-NS-ZYrWAH2xW0mIg.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="311a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着稳定的传播，互联网变得热闹起来——它正在慢慢成为创造美丽视觉效果的必备工具。如果你想让你的视觉效果更上一层楼，那么用稳定扩散制作的动画是个不错的选择。</p><p id="83b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您不再需要花费数小时来创建传统动画，通过稳定的扩散，您可以立即让您的视觉效果栩栩如生。所以，让我们打开这个神奇的工具，看看如何使用它来创建令人惊叹的动画！</p><p id="06cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这篇文章中，我将介绍一些用于创建稳定扩散动画的技术。</p><h1 id="3ddb" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">一些背景信息</h1><p id="5c02" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">在事情开始之前，我们需要熟悉一些概念。</p><p id="742a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">第一个是扩散模型如何工作。简单来说，这些模型把噪音变成图像。在训练过程中，这些模型学习反转一个<em class="mk">去噪</em> <em class="mk">过程</em>，该过程通过一个<a class="ae ml" href="https://huggingface.co/docs/diffusers/v0.10.2/en/api/schedulers#what-is-a-scheduler" rel="noopener ugc nofollow" target="_blank">调度函数</a>以固定的步数逐渐将噪声添加到图像中，直到图像与随机噪声无法区分。</p><p id="7224" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一旦经过训练，我们就可以通过传入图像整形噪声张量来使用该模型生成图像，并通过使其迭代通过该模型来将其去噪到图像中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/54e4c2787a71fe62a5bd9862f6f11ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oh5Vog78ER6xurxTzkx5uQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">正向和反向扩散过程(<a class="ae ml" href="https://arxiv.org/pdf/2006.11239.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="1a57" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上图中，密度函数<strong class="kt ir"> <em class="mk"> q </em> </strong>指的是正向去噪过程。这意味着，前向过程基于时间步长<strong class="kt ir"><em class="mk">t-1</em></strong><em class="mk">处的样本和时间步长</em><strong class="kt ir"><em class="mk"/></strong>处的噪声调度函数的值来估计时间步长<em class="mk"/><strong class="kt ir"><em class="mk">t</em></strong><em class="mk">处的噪声样本</em></p><p id="dece" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">密度函数<strong class="kt ir">重复该过程，直到创建图像。</strong></p><p id="17f7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们需要掌握的下一个概念是潜在扩散。这就是稳定扩散模型的动力。</p><p id="433d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在普通扩散模型中，正向和反向过程对与图像张量具有相同形状的噪声张量进行操作。潜在扩散使用 VAE 模型，在执行正向和反向过程之前，首先将图像编码成潜在代码。</p><p id="ded3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这使得该过程更快，因为该模型在低维潜在代码上操作，而不是在完整的图像成形张量上操作。</p><p id="7d91" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">关于稳定扩散的另一件事是，它是文本引导的。这意味着图像生成过程由剪辑编码器产生的文本嵌入来指导。来自文本嵌入的信息通过交叉注意被结合到扩散过程中。</p><p id="83de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在我们已经讨论过了，让我们继续制作一些动画。</p><h1 id="48f0" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">在两个图像之间过渡</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/13046c9631ca0444bccb57c894f22efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*K4BKcS8O5brY1Xl61OFhfA.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="eea7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以制作的最基本的动画是两幅图像之间的过渡。在上面的动画中，我们在提示<code class="fe ms mt mu mv b">a picture of a corgi</code>和<code class="fe ms mt mu mv b">a picture of a lion</code>之间创建了一个过渡。为了创建这个转换，我们需要定义几个值。</p><ol class=""><li id="5e74" class="mw mx iq kt b ku kv kx ky la my le mz li na lm nb nc nd ne bi translated">图像之间要生成的帧数。<code class="fe ms mt mu mv b">num_frames</code></li><li id="338d" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated">动画的帧速率，以每秒帧数定义。<code class="fe ms mt mu mv b">fps</code></li></ol><p id="c810" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">帧数将让我们知道我们期望在两帧之间生成多少个点，而帧速率将决定动画移动的速度。</p><p id="0780" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们已经介绍过潜在扩散是通过在图像的潜在代码上运行扩散过程来工作的。为了生成新的图像，我们创建一个与我们的潜在代码形状相同的噪声张量，并通过我们的模型将其与我们的剪辑文本一起嵌入。这将导致一个潜在的代码，我们可以传递到我们的解码器模型来创建图像。</p><p id="db3c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这里有一些伪代码来演示这一点。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="1bff" class="no lo iq mv b be np nq l nr ns">seed = 42<br/>latent = torch.randn(<br/>            (1, self.pipe.unet.in_channels, height // 8, width // 8),<br/>            device=self.pipe.device,<br/>            generator=generator.manual_seed(),<br/>        )<br/>text_embedding = get_text_embedding(["a photo of a corgi"])<br/>output_latent = diffuse(text_embedding, latent)<br/>image = decode_latent(latent)</span></pre><p id="c365" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">文本嵌入和潜在都是张量。我们将为我们的开始和结束帧创建这些张量，并使用<em class="mk">插值</em>来填充帧之间的内容。插值是一种在两个已知数据点之间创建新数据点的方法。</p><p id="0a6f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了在开始帧和结束帧之间创建指定数量的帧，我们将使用<a class="ae ml" href="https://minibatchai.com/sampling/2022/04/24/Slerp.html" rel="noopener ugc nofollow" target="_blank">球形线性插值</a>，因为<a class="ae ml" href="https://github.com/soumith/dcgan.torch/issues/14" rel="noopener ugc nofollow" target="_blank">考虑了潜在空间的形状。</a>你可以把球面线性插值想象成沿着两个单位法向量之间的一个角度生成向量。</p><p id="338a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">插值函数接受两个张量和一个插值权重，插值权重决定了结果张量与起始值和结束值的接近程度。插值权重为 0.1 将产生更类似于起始张量的插值张量，而插值权重为 0.9 将产生更类似于结束张量的张量。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="ee6b" class="no lo iq mv b be np nq l nr ns">import numpy as np<br/><br/><br/>def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):<br/>    """helper function to spherically interpolate two arrays v1 v2"""<br/>    # from https://gist.github.com/nateraw/c989468b74c616ebbc6474aa8cdd9e53<br/><br/>    if not isinstance(v0, np.ndarray):<br/>        inputs_are_torch = True<br/>        input_device = v0.device<br/>        v0 = v0.cpu().numpy()<br/>        v1 = v1.cpu().numpy()<br/><br/>    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))<br/>    if np.abs(dot) &gt; DOT_THRESHOLD:<br/>        v2 = (1 - t) * v0 + t * v1<br/>    else:<br/>        theta_0 = np.arccos(dot)<br/>        sin_theta_0 = np.sin(theta_0)<br/>        theta_t = theta_0 * t<br/>        sin_theta_t = np.sin(theta_t)<br/>        s0 = np.sin(theta_0 - theta_t) / sin_theta_0<br/>        s1 = sin_theta_t / sin_theta_0<br/>        v2 = s0 * v0 + s1 * v1<br/><br/>    if inputs_are_torch:<br/>        v2 = torch.from_numpy(v2).to(input_device)<br/><br/>    return v2<br/><br/><br/>start_seed = 42<br/>latent_start = torch.randn(<br/>    (1, 4, 64, 64),<br/>    generator=generator.manual_seed(start_seed),<br/>)<br/><br/>end_seed = 43<br/>latent_end = torch.randn(<br/>    (1, 4, 64, 64),<br/>    generator=generator.manual_seed(start_seed),<br/>)<br/><br/>num_frames = 60<br/>schedule = np.linspace(0, 1, num_frames)<br/><br/>in_between_latents = []<br/>for t in range(schedule):<br/>    in_between_latents.append(slerp(float(t), start_latent, end_latent))</span></pre><p id="2ea4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们也将同样的插值应用到文本嵌入中。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="487e" class="no lo iq mv b be np nq l nr ns">start_text_embedding = get_text_embedding(["a picture of a corgi"])<br/>end_text_embedding = get_text_embedding(["a picture of a lion"])<br/><br/>for t in range(schedule):<br/>    in_between_latents.append(slerp(float(t), start_text_embedding, end_text_embedding))</span></pre><p id="92b7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这导致生成的图像之间的平滑过渡。</p><h1 id="a9cd" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">保留动画中的主题</h1><p id="d11d" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">你可能已经注意到，在早期的动画中，我们的柯基在动画过程中发生了巨大的变化。如果我们试图在我们的动画中创建一个焦点，这可能是非常不可取的。</p><p id="9397" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了避免这一点，我们可以使用<a class="ae ml" href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" rel="noopener ugc nofollow" target="_blank">可组合扩散</a>。这种技术允许以保留单个组件的方式组合提示。</p><p id="ddcc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了做到这一点，Composable Diffusion 根据分隔符分割输入提示，比如<code class="fe ms mt mu mv b">|</code></p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="7bc0" class="no lo iq mv b be np nq l nr ns">'a red house | a house in a lightning storm'</span></pre><p id="26e1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该提示将基于分隔符被分成两个提示，并被馈入具有相同潜在成形噪声张量的扩散模型。由该模型产生的结果潜在值将被平均为单个张量，并被发送到解码器模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/0b09f49e97b24f2d2ada5a0112d587a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*6XQPNUyFXtsmyAq07a4X6Q.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="07f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请注意，上面动画中的房子在整个动画过程中变化不大。这是因为提示符的各个组成部分是通过组合来组合的。</p><p id="21f4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种方法的伪代码:</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="c3a4" class="no lo iq mv b be np nq l nr ns">prompt = ['a red house | a house in a lightning storm']<br/>prompts = prompt.split('|')<br/><br/>text_embeddings = [get_text_embedding(prompt) for prompt in prompts]<br/>latent_noise_tensor = torch.randn(<br/>    (1, 4, 64, 64),<br/>    generator=generator.manual_seed(start_seed),<br/>)<br/><br/>output_latents = [diffuse(text_embedding, latent_noise_tensor) for text_embedding in text_embeddings]<br/>final_latent = mean(output_latents)<br/><br/>image = decode_latent(final_latent)</span></pre><h1 id="91b3" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">音频反应动画</h1><p id="3285" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">到目前为止，我们已经讨论了如何使用球形线性插值在图像之间进行过渡。这种类型的插值在各个帧上产生彼此等距的点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/223afea59c6c2f66e52565e7e82223aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTFaJsK55RuhgVw44zxkPg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="8965" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请注意，X 方向上的每一步如何导致 Y 方向上的等量移动。</p><p id="c1db" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当尝试创建音频反应动画时，我们希望当有特定类型的音频事件(如鼓声)时，在我们的视觉效果中产生戏剧性的变化，否则保持我们的变化相对恒定。</p><p id="00c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要产生这样的变化，我们需要创建一个插值权重表，当帧包含音频事件时，它会快速变化，而当帧不包含音频事件时，它会保持相对恒定。</p><p id="e413" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用像<code class="fe ms mt mu mv b">librosa</code>这样的音频分析库可以帮助我们检测这些音频事件。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="32bb" class="no lo iq mv b be np nq l nr ns">import librosa<br/><br/>audio_input = "my_audio_file.mp3"<br/><br/>audio_array, sr = librosa.load(audio_input) <br/><br/>onset_env = librosa.onset.onset_strength(audio_array, sr=sr)<br/>onset_env = librosa.util.normalize(onset_env)</span></pre><p id="7364" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这个片段加载到一个音频文件中，并提取一个开始强度包络，向您显示音频事件发生的位置。此外，我们对包络进行了归一化，以便将值限制在 0.0 和 1.0 之间</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/54cd210dc6213a29dfaf646c97f2920c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HqGyeyhL2tAaxvnejOXNw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="257d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图表中的峰值对应于一段时间内音频文件中的事件。直接使用这个数组作为我们的插值权重调度将导致两个提示之间的非常抖动的运动，因为我们随着时间的推移不断地在 0.0-1.0 之间振荡。这无疑是有趣的，但对于更复杂的音频，它可能最终看起来非常嘈杂。我们希望能够驱动从一帧到下一帧的转换，这样我们就可以清楚地看到提示之间的转换。我们需要一个从 0.0 到 1.0 的单调递增函数</p><p id="27e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用该数组的归一化累积和将产生我们需要的效果。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="2df8" class="no lo iq mv b be np nq l nr ns">import numpy as np<br/><br/>cumulative = np.cumsum(onset_env)<br/>cumulative = cumulative / max(cumulative)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/b5ba35b962cc539a2869fc18b0976817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QefBVOZMzXJFOWcTF2vcvA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="a5c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当我们从起点移动到终点时，插值时间表会随着时间的推移而增加。我们需要调整这个数组的大小，以匹配我们想要生成的帧数。</p><pre class="kg kh ki kj gt nk mv nl bn nm nn bi"><span id="2858" class="no lo iq mv b be np nq l nr ns">import numpy as np<br/><br/>def resize(onset_env):<br/>    x = np.linspace(0, len(onset_env), len(onset_env))<br/>    resized_schedule = np.linspace(0, len(onset_env), num_frames)<br/>    return np.interp(resized_schedule, x, onset_env)<br/><br/>resized = resize(cumulative, num_frames) </span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nt"><img src="../Images/de390371e83d1ab17ef3095f1f47289c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6AUSwRqMwp2Vh4JaRTj3lg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="ff95" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，当我们在之间插值时，我们在音频开始事件期间获得快速过渡，否则获得平滑过渡。我们可以通过将音频信号分解为打击乐和谐波分量，并基于这些分量之一构建插值权重，来进一步改善转场。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nu nv l"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h1 id="d9d8" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">使用初始视频输入创建动画</h1><p id="8900" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">到目前为止，我们一直使用噪声张量作为输入来创建动画。但是也有可能需要利用现有的视频作为我们动画的基础。</p><p id="e9d2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">稳定扩散通过图像到图像的转换支持这一工作流程。</p><p id="1df2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图像到图像的工作流程不是使用随机采样的噪声张量，而是首先将初始图像(或视频帧)编码成潜在代码。我们将噪声添加到这个潜在代码中，并将其用作我们的输入噪声张量，同时运行稍微修改的扩散过程。</p><p id="7ee3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们通过引入一个新的参数来修改扩散过程；实力。</p><p id="4258" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">“强度”参数控制在扩散过程中保留多少原始图像内容，其范围在 0.0 和 1.0 之间。较低的值(高达 0.5)倾向于更多地保留原始图像内容，而较高的值会改变图像语义的一致性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/65e1df95ee135b5b488084f07d20ef8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/1*PszQenZdotpxoU1KTGS_7Q.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="1c50" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么强度参数到底在做什么呢？</p><p id="5863" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">去噪过程是迭代的，并且基于调度器运行固定数量的步骤。这意味着，在这个迭代过程中，我们可以在任何点自由地引入噪声张量。强度参数控制我们在这个过程中多晚引入噪声张量。</p><p id="cb40" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">假设我们正在运行 50 个推理步骤的去噪过程。如果我们将强度值设置为<strong class="kt ir"> 0.8 </strong>，我们将为初始潜像添加 40 步的噪声，然后对图像进行 40 步的降噪处理。</p><p id="6597" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">较高的值意味着运行更多步骤的去噪过程，导致原始图像语义消失。值越小，去噪步骤越少，保留的原始图像越多。</p><h1 id="e878" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">临别赠言</strong></h1><p id="8c5f" class="pw-post-body-paragraph kr ks iq kt b ku mf jr kw kx mg ju kz la mh lc ld le mi lg lh li mj lk ll lm ij bi translated">如果你对应用这些技术感兴趣，试试我过去几周一直在做的这个项目，<a class="ae ml" href="https://github.com/dn6/giffusion" rel="noopener ugc nofollow" target="_blank"> Giffusion </a>。</p><p id="63cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">Giffusion 应用了我在这篇文章中描述的所有技术，并提供了一个简单的 WebUI，您可以在 Google Colab 中运行它来创建具有稳定扩散的动画 gif 和视频。</p><p id="6293" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你对我在这篇文章中是如何制作动画感到好奇，你可以看看我的<a class="ae ml" href="https://www.comet.com/team-comet-ml/giffusion/view/CzxqbNrydKqHCaYhNEnbyrpnz/panels?utm_source=tds&amp;utm_medium=social&amp;utm_campaign=stable_diffusion" rel="noopener ugc nofollow" target="_blank"> Comet 项目，里面有所有的提示、参数和生成的动画</a>。</p><p id="7f77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">所以，你有它！通过稳定的扩散，您可以创建令人惊叹的动画，并使您的项目栩栩如生。现在你知道了这个神奇工具的力量，是时候激发你的创造力，做一些漂亮的东西了！</p><h1 id="8230" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">参考</h1><ol class=""><li id="5843" class="mw mx iq kt b ku mf kx mg la nx le ny li nz lm nb nc nd ne bi translated"><a class="ae ml" href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/" rel="noopener ugc nofollow" target="_blank">机器学习的扩散模型介绍</a></li><li id="1e22" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work" rel="noopener ugc nofollow" target="_blank">用扩散器稳定扩散</a></li><li id="0d6d" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/" rel="noopener ugc nofollow" target="_blank">可组合扩散</a></li><li id="471d" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="https://github.com/nateraw/stable-diffusion-videos" rel="noopener ugc nofollow" target="_blank">针对音频反应的稳定扩散视频</a></li><li id="6f28" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="http://How img2img Diffusion Works" rel="noopener ugc nofollow" target="_blank">图像到图像的工作原理</a></li><li id="c742" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="https://github.com/DN6/giffusion" rel="noopener ugc nofollow" target="_blank"> Giffusion:生成稳定扩散动画的项目</a></li><li id="6540" class="mw mx iq kt b ku nf kx ng la nh le ni li nj lm nb nc nd ne bi translated"><a class="ae ml" href="https://www.comet.com/team-comet-ml/giffusion/view/CzxqbNrydKqHCaYhNEnbyrpnz/panels?utm_source=tds&amp;utm_medium=social&amp;utm_campaign=stable_diffusion" rel="noopener ugc nofollow" target="_blank">带有我的所有提示和生成图像的彗星项目</a></li></ol></div></div>    
</body>
</html>