<html>
<head>
<title>How to Interpret Linear Regression, Lasso, and Decision Tree with Python (easy)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Python 解释线性回归、套索、决策树(易)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-interpret-machine-learning-models-part-1-easy-978ddade7ada#2022-03-06">https://towardsdatascience.com/how-to-interpret-machine-learning-models-part-1-easy-978ddade7ada#2022-03-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7d6f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最重要的特性(特性重要性)是什么？为什么模型会做出这个特定的决定？</h2></div><h1 id="0aed" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.介绍</h1><p id="c66e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这篇文章中，我将尝试解释<strong class="lc iu">线性回归、Lasso 和决策树</strong>模型，这些模型本身都是可以解释的。我将分析<em class="lw">全局可解释性</em>——分析总体预测的最重要特征，以及<em class="lw">局部可解释性</em>——解释个别预测结果。</p><p id="a8c2" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">机器学习模型用于银行交易中的欺诈和风险检测、语音助手、推荐系统、聊天机器人、自动驾驶汽车、社交网络分析等应用中。然而，有时很难解释它们，因为算法代表了一个黑盒(例如神经网络)，人类很难理解为什么模型做出了特定的决定。所以我们需要额外的技术来分析黑盒决策。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/f5d1dc8e503366bea8273dca9c0f679c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnEIOvLZ6O2jSiemhWgy_Q.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">来源:作者照片</p></figure><h1 id="8d03" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.什么是可解释性？</h1><blockquote class="ms mt mu"><p id="8171" class="la lb lw lc b ld lx ju lf lg ly jx li mv lz ll lm mw ma lp lq mx mb lt lu lv im bi translated">可解释性是人类能够理解决策原因的程度[Miller，Tim 2017]。</p></blockquote><p id="4afc" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们可以互换使用<strong class="lc iu">可解释性</strong>和<strong class="lc iu">可解释性</strong>。然而，术语<strong class="lc iu">解释</strong>用于解释单个预测，而非整个模型。如上所述，一些算法很容易解释，然而，在算法预测准确性和可解释性之间存在粗略的相关性。模型越复杂，解释力就越弱。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi my"><img src="../Images/1d9c2d41e6b59602193b903f90267713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ff7AT9VN8fGUnLBw2cCHUA.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><h1 id="0157" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.为什么可解释性很重要？</h1><p id="2c62" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">以下是几个原因:</p><ul class=""><li id="6623" class="mz na it lc b ld lx lg ly lj nb ln nc lr nd lv ne nf ng nh bi translated">解释为什么模型做出特定的决定。</li><li id="cf64" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">建立对关键应用程序模型的信任。</li><li id="ddac" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated">调试和改进模型。</li></ul><h1 id="6395" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.线性回归、套索和要素重要性</h1><h2 id="b662" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">4.1 线性回归</h2><p id="51d3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">线性模型预测是特征输入的加权和。</p><p id="3b34" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">y' = W0 + W1*x1 + W2*x2 + … +WnXn</p><p id="c789" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">其中 Wj 为特征，W0 为截距。</p><p id="8a08" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">使用线性模型时，数据集上有一些<strong class="lc iu">约束</strong>:</p><ul class=""><li id="0f24" class="mz na it lc b ld lx lg ly lj nb ln nc lr nd lv ne nf ng nh bi translated"><strong class="lc iu">线性:</strong>目标变量是输入特征的线性组合。</li><li id="344d" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">正态性:</strong>给定目标变量，假设特征遵循正态分布。</li><li id="422e" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">同方差:</strong>假设误差项的方差在整个特征空间上是恒定的</li><li id="b723" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">独立性:</strong>每个实例都独立于任何其他实例。</li><li id="adb8" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">无多重共线性</strong>:输入要素的相关性应较弱，否则会影响预测。</li></ul><p id="4001" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">实用建议</strong>:如果只有几个特征，线性模型就不错。</p><p id="f7b6" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">问题</strong>:当我们拥有很多功能时会发生什么？线性回归发现很难处理它们并相应地解释特征。这就是为什么我们可以使用套索功能选择。</p><h2 id="a481" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">4.2 套索</h2><p id="0fe7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Lasso 是采用 L1 正则化的线性回归，负责使用惩罚权重进行要素选择，这意味着模型的大权重会缩小，小权重会变为零。alpha 参数定义权重的惩罚。</p><p id="ee84" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">注</strong>:如果想了解更多关于<strong class="lc iu">线性回归</strong>或者<strong class="lc iu">套索</strong>的内容，可以参考我之前关于线性模型的文章。</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/fish-weight-prediction-regression-analysis-for-beginners-part-1-8e43b0cb07e"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">鱼重预测(初学者回归分析)——第一部分</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">如何使用顶级线性 ML 算法(线性回归、套索回归和岭回归)构建 ML 回归模型</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq mm oc"/></div></div></a></div><h2 id="7a9f" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">4.3 特征在线性回归和套索中的重要性</h2><p id="9867" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">特征重要性是其<a class="ae or" href="https://en.wikipedia.org/wiki/T-statistic#:~:text=In%20statistics%2C%20the%20t%2Dstatistic,or%20reject%20the%20null%20hypothesis." rel="noopener ugc nofollow" target="_blank"> t 统计量</a>的绝对值。因此，如果特性的权重增加，其重要性也会增加。然而，如果标准误差增加(我们不太确定正确的值)，重要性就会降低。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi os"><img src="../Images/ea488e935114a0e722f94e1fccecbda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*XyqF4KQ_gsTPWXC0D2oGig.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">t 统计公式</p></figure><h2 id="1913" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">4.4 线性模型解释评价</h2><p id="68ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">用线性来解释对人们来说更简单自然。因此，如果线性回归或 Lasso 是数据的良好预测器，这意味着数据满足上面提到的大多数重要约束，那么解释将是真实的。在另一种情况下，如果数据中存在非线性相互作用，线性模型将越不准确，因此，解释将越不真实。</p><h1 id="d1af" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.决策树和特征重要性</h1><h2 id="b3c7" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">5.1 决策树</h2><p id="6a58" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当数据不是线性的、要素和结果之间的关系是非线性的或者存在多重共线性时，线性回归模型会失败，因此输入要素会相互影响。这就是为什么在这种情况下可以使用基于<strong class="lc iu">树的</strong>算法。</p><p id="d1b2" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">决策树背后的直觉是，数据根据某些临界值被分割成不同的子集，预测结果是每个叶节点。</p><p id="7712" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">注意</strong>:如果你想了解更多关于<strong class="lc iu">决策树的知识，</strong>请参考我之前关于基于树的方法的文章。</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/regression-analysis-for-beginners-using-tree-based-methods-2b65bd193a7"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">初学者回归分析—第二部分</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">使用基于树的算法(决策树、随机森林、XGboost)构建 ML 回归模型</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ot l on oo op ol oq mm oc"/></div></div></a></div><h2 id="8f4a" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">5.2 决策树中的特征重要性:</h2><p id="c8f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">解释起来很容易。决策树的总体特征重要性可以通过以下方式计算。浏览所有分割，并注意每个要素分割相对于父节点减少了多少方差(对于回归)或基尼指数(对于分类)。</p><h2 id="8501" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">5.3 决策树解释评估:</h2><p id="ecc5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">预测的真实性取决于树的预测性能。对短树的解释非常简单和通用，因为对于每一次分裂，实例要么落入一片叶子，要么落入另一片叶子。二元决策很容易理解。</p><h1 id="9764" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak"> 6。总结</strong></h1><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ou"><img src="../Images/821cb44ab343ecb3b98f84bbcb9dfebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WEfyLSokUB1mQMoJH2NhBw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><h1 id="5eda" class="ki kj it bd kk kl pc kn ko kp pd kr ks jz pe ka ku kc pf kd kw kf pg kg ky kz bi translated">使用 Python 实现可解释性</h1><p id="a089" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu"> GitHub 代码可用:</strong><a class="ae or" href="https://github.com/gurokeretcha/ML_interpretability/blob/main/ML_model_interpretability.ipynb" rel="noopener ugc nofollow" target="_blank">T5】此处 </a></p><p id="d04f" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在这一节中，我将向您展示线性回归、Lasso 和决策树解释的实际实现。<strong class="lc iu">全局解释</strong>表示每个特征对于总体预测的重要性。<strong class="lc iu">局部解释</strong>试图帮助我们理解机器学习模型的个体预测。</p><p id="efe0" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">来自 Kaggle </strong>的数据集:<a class="ae or" href="https://www.kaggle.com/mirichoi0218/insurance" rel="noopener ugc nofollow" target="_blank">医疗费用个人数据集</a></p><p id="ffee" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">共 7 项功能</strong>:</p><ul class=""><li id="f589" class="mz na it lc b ld lx lg ly lj nb ln nc lr nd lv ne nf ng nh bi translated"><strong class="lc iu">年龄</strong>:保险承包人年龄</li><li id="8eb8" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">性别</strong>:保险签约人性别，女，男</li><li id="c474" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu"> BMI </strong>:身体质量指数，提供对身体的了解，体重相对于身高相对较高或较低，体重的客观指数(kg / m ^ 2)使用身高与体重的比率，理想值为 18.5 至 24.9</li><li id="e5d4" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">儿童</strong>:健康保险覆盖的儿童人数/受抚养人人数</li><li id="1536" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">吸烟者</strong>:吸烟</li><li id="d514" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">地区</strong>:受益人在美国的居住区域，东北、东南、西南、西北。</li><li id="80f6" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">费用</strong>:由健康保险支付的个人医疗费用</li></ul><p id="a896" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">我们的目标</strong>是<strong class="lc iu"> </strong>建立一个预测个体承包商费用的 ML 模型，然后解释结果。</p><p id="236d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">第 0 步</strong>:导入库</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="5736" class="nn kj it pi b gy pm pn l po pp"><strong class="pi iu">import</strong> pandas <strong class="pi iu">as</strong> pd<br/><strong class="pi iu">import</strong> numpy <strong class="pi iu">as</strong> np<br/><strong class="pi iu">import</strong> matplotlib.pyplot <strong class="pi iu">as</strong> plt<br/><strong class="pi iu">import</strong> statsmodels.api <strong class="pi iu">as</strong> sm<br/><strong class="pi iu">from</strong> sklearn.linear_model <strong class="pi iu">import</strong> LinearRegression,Lasso<br/><strong class="pi iu">from</strong> sklearn.preprocessing <strong class="pi iu">import</strong> MinMaxScaler, StandardScaler, LabelEncoder,OneHotEncoder<br/><strong class="pi iu">from</strong> sklearn.metrics <strong class="pi iu">import</strong> mean_squared_error, mean_absolute_error, r2_score<br/><strong class="pi iu">from</strong> sklearn.model_selection <strong class="pi iu">import</strong> train_test_split,GridSearchCV<br/><strong class="pi iu">from</strong> sklearn.pipeline <strong class="pi iu">import</strong> Pipeline<br/><strong class="pi iu">from</strong> sklearn.tree <strong class="pi iu">import</strong> DecisionTreeRegressor<br/><strong class="pi iu">from</strong> sklearn <strong class="pi iu">import</strong> tree</span></pre><h1 id="95b6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak"> 1。线性回归:</strong></h1><h2 id="2306" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated"><strong class="ak">步骤 1 </strong>:导入数据预处理数据——对分类列进行编码，并进行标准缩放。</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="3136" class="nn kj it pi b gy pm pn l po pp"># 1. import data<br/>data <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>read_csv("insurance.csv")</span><span id="a671" class="nn kj it pi b gy pq pn l po pp"># 2. categorical encoding<br/>data_linear_reg <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>get_dummies(data,columns<strong class="pi iu">=</strong>['sex', 'smoker','region'], drop_first<strong class="pi iu">=True</strong>)</span><span id="1393" class="nn kj it pi b gy pq pn l po pp"># 3. scalling<br/>col_names <strong class="pi iu">=</strong> ['age', 'bmi']<br/>features <strong class="pi iu">=</strong> data_linear_reg[col_names]<br/>scaler <strong class="pi iu">=</strong> StandardScaler()<strong class="pi iu">.</strong>fit(features<strong class="pi iu">.</strong>values)<br/>features <strong class="pi iu">=</strong> scaler<strong class="pi iu">.</strong>transform(features<strong class="pi iu">.</strong>values)<br/>data_linear_reg[col_names] <strong class="pi iu">=</strong> features</span><span id="e957" class="nn kj it pi b gy pq pn l po pp"># 4.train test split<br/>x_train, x_test, y_train, y_test <strong class="pi iu">=</strong> train_test_split(data_linear_reg<strong class="pi iu">.</strong>drop(columns<strong class="pi iu">=</strong>["charges"]), data_linear_reg["charges"], test_size<strong class="pi iu">=</strong>0.2, random_state<strong class="pi iu">=</strong>42)</span><span id="ba72" class="nn kj it pi b gy pq pn l po pp">#5. linear regression model <br/>linear_reg <strong class="pi iu">=</strong> sm<strong class="pi iu">.</strong>OLS(y_train, x_train)<strong class="pi iu">.</strong>fit()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pr"><img src="../Images/07b8e0f48229ae45d8963947d958a994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adQ0-n5NhfArsg3NfjxCrw.png"/></div></div></figure><h2 id="0dc7" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated"><strong class="ak">第二步:预测结果</strong></h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="d094" class="nn kj it pi b gy pm pn l po pp"><strong class="pi iu">def</strong> pred_result(pred,y_test):<br/>  score_MSE <strong class="pi iu">=</strong> round(mean_squared_error(pred, y_test))<br/>  score_MAE <strong class="pi iu">=</strong> round(mean_absolute_error(pred, y_test))<br/>  score_r2score <strong class="pi iu">=</strong> round(r2_score(pred, y_test),2)<br/>  print(f"MSE: {score_MSE} | MAE: {score_MAE} | R2score: {score_r2score}")</span><span id="8a10" class="nn kj it pi b gy pq pn l po pp">linear_pred <strong class="pi iu">=</strong> linear_reg<strong class="pi iu">.</strong>predict(x_test)<br/>pred_result(linear_pred,y_test)</span></pre><p id="e34b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">Out[]:MSE:46124128 | MAE:5032 | R2 score:0.67</strong></p><p id="764d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如您所见，均方误差为 46124128，平均绝对误差为 5032，R2 分数为 0.67。</p><h2 id="bc86" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated"><strong class="ak">第三步:全局可解释性</strong></h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="f988" class="nn kj it pi b gy pm pn l po pp">err <strong class="pi iu">=</strong> linear_reg<strong class="pi iu">.</strong>params <strong class="pi iu">-</strong> linear_reg<strong class="pi iu">.</strong>conf_int()[0]<br/>coef_df <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>DataFrame({'coef': round(linear_reg<strong class="pi iu">.</strong>params),<br/>                        'Standard Error': round(linear_reg<strong class="pi iu">.</strong>bse),<br/>                        't_Stats': round(linear_reg<strong class="pi iu">.</strong>tvalues,1),<br/>                        'error': round(err)<br/>                       })<strong class="pi iu">.</strong>reset_index()<strong class="pi iu">.</strong>rename(columns<strong class="pi iu">=</strong>{"index":"columns"})<br/>coef_df</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/616a6abaec06c4dbd0715afc795a9f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*uNEbpGlnCYYhym1V3HBXWA.png"/></div></figure><p id="3212" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> Coef </strong> -代表特征的权重。</p><p id="30f0" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">标准误差</strong>:砝码的标准误差。</p><p id="ee0d" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> t_Stats </strong>:每个特征的 t 统计量。</p><p id="844a" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">误差</strong>:权重与其第一置信区间水平之差。</p><p id="b3ce" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">Coef(权重)背后的直觉</strong>:</p><ul class=""><li id="f931" class="mz na it lc b ld lx lg ly lj nb ln nc lr nd lv ne nf ng nh bi translated"><strong class="lc iu">数字特征(例如:年龄)</strong>:年龄增加 1 岁，在所有其他特征保持不变的情况下，预测费用数增加 3598。</li><li id="e60f" class="mz na it lc b ld ni lg nj lj nk ln nl lr nm lv ne nf ng nh bi translated"><strong class="lc iu">分类特征</strong>(例如:吸烟者):在所有其他特征保持不变的情况下，当一个人吸烟时，收费值比不吸烟的人高 25153。</li></ul><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="fb7c" class="nn kj it pi b gy pm pn l po pp">coef_df<strong class="pi iu">.</strong>plot(y<strong class="pi iu">=</strong>'coef', x<strong class="pi iu">=</strong>'columns', kind<strong class="pi iu">=</strong>'bar', color<strong class="pi iu">=</strong>'none', yerr<strong class="pi iu">=</strong>'error', legend<strong class="pi iu">=False</strong>, figsize<strong class="pi iu">=</strong>(14,6))<br/>plt<strong class="pi iu">.</strong>scatter(x<strong class="pi iu">=</strong>np<strong class="pi iu">.</strong>arange(coef_df<strong class="pi iu">.</strong>shape[0]), s<strong class="pi iu">=</strong>100, y<strong class="pi iu">=</strong>coef_df['coef'], color<strong class="pi iu">=</strong>'green')<br/>plt<strong class="pi iu">.</strong>axhline(y<strong class="pi iu">=</strong>0, linestyle<strong class="pi iu">=</strong>'--', color<strong class="pi iu">=</strong>'black', linewidth<strong class="pi iu">=</strong>1)<br/>plt<strong class="pi iu">.</strong>title("Coefficient and Standard error")<br/>plt<strong class="pi iu">.</strong>grid()<br/>plt<strong class="pi iu">.</strong>yticks(fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>xticks(fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pt"><img src="../Images/d538f879230e43f5c7de3819bb98f8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeAfiugPQqzqOerPBg2WYg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><p id="b00a" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们可以在图中观察每个特征的权重和标准误差。值得注意的是，标准误差可以忽略，因为它们很小，这意味着模型确定每个特征的权重。</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="4b34" class="nn kj it pi b gy pm pn l po pp">plt<strong class="pi iu">.</strong>figure(figsize<strong class="pi iu">=</strong>(14,6))<br/>plt<strong class="pi iu">.</strong>bar(linear_reg<strong class="pi iu">.</strong>tvalues<strong class="pi iu">.</strong>keys(), abs(linear_reg<strong class="pi iu">.</strong>tvalues<strong class="pi iu">.</strong>values))<br/>plt<strong class="pi iu">.</strong>title("Feature Importance",fontsize<strong class="pi iu">=</strong>25)<br/>plt<strong class="pi iu">.</strong>ylabel("t-statistic (absolute value)",fontsize<strong class="pi iu">=</strong>18)<br/>plt<strong class="pi iu">.</strong>grid()<br/>plt<strong class="pi iu">.</strong>xticks(rotation<strong class="pi iu">=</strong>90,fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>yticks(range( 0,int(max(round(linear_reg<strong class="pi iu">.</strong>tvalues)))<strong class="pi iu">+</strong>5,5) ,fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pu"><img src="../Images/ac7ff03cca88339e623b75300ac65b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOWPlq70fkjOQ_iiCyrkBg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><p id="0027" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如上所述，可以使用<em class="lw"> t 统计量</em>来测量线性回归特征的重要性。所以基于线性回归<strong class="lc iu">最重要的特征</strong>是<strong class="lc iu">吸烟者 _ 是</strong>(它定义了这个人是否吸烟)。第二个最重要的特征是年龄。最不重要的特征是<strong class="lc iu">性别 _ 男性</strong>和<strong class="lc iu">体重指数</strong>。</p><h2 id="6d63" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated"><strong class="ak">步骤 4:局部可解释性</strong></h2><p id="8198" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们检查一个单独的预测(例如，第 5 个样本点)和它的预测是什么。</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="4aa2" class="nn kj it pi b gy pm pn l po pp">forth_sample <strong class="pi iu">=</strong> x_test<strong class="pi iu">.</strong>iloc[4]<br/>forth_sample</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/54d31adcb57dbeaf0d8d1d63db2a9952.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*jPHtYLaEyz1cEzy4HpuqLg.png"/></div></figure><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="5190" class="nn kj it pi b gy pm pn l po pp">print("predicted: ",int(linear_reg<strong class="pi iu">.</strong>predict(forth_sample<strong class="pi iu">.</strong>values)))<br/>print("actual: ",int(y_test<strong class="pi iu">.</strong>iloc[4]))</span></pre><p id="b825" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> Out[]:预测:28147 实际:33750 </strong></p><p id="8861" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">注</strong>:该样本点性别为男性，吸烟。<strong class="lc iu">想象一下</strong>如果那个人<strong class="lc iu">不吸烟</strong>会有什么预测？</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="f3ae" class="nn kj it pi b gy pm pn l po pp">forth_sample<strong class="pi iu">.</strong>values[4] <strong class="pi iu">=</strong> 0 # make a person non-smoker<br/>print("predicted non smoker: ",int(linear_reg<strong class="pi iu">.</strong>predict(forth_sample<strong class="pi iu">.</strong>values)))</span></pre><p id="9377" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> Out[]:预计不吸烟人数:2994 人</strong></p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="e383" class="nn kj it pi b gy pm pn l po pp">print("difference : ", 28147 <strong class="pi iu">-</strong> 2994 ) <em class="lw"># same as smoker_yer weight.</em></span></pre><p id="827e" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> Out[]:差额:25153 </strong></p><p id="ddf0" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">注意</strong>:正如我们从特征重要性图中看到的，smoker_yes 特征是线性回归模型中最重要的特征。事实确实如此。第五种情况的预测是 28 147，而当我们改变它的值并使一个人不吸烟时，预测值变成 2 994。<strong class="lc iu">差 25153 与吸烟者体重相同 _ 是特征！</strong></p><h1 id="6d03" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.套索</h1><h2 id="c213" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 1:导入数据预处理数据—对分类列进行编码并进行标准缩放。</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="f287" class="nn kj it pi b gy pm pn l po pp">data <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>read_csv("insurance.csv")</span><span id="5527" class="nn kj it pi b gy pq pn l po pp">data_lasso <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>get_dummies(data,columns<strong class="pi iu">=</strong>['sex', 'smoker','region'], drop_first<strong class="pi iu">=True</strong>)</span><span id="1522" class="nn kj it pi b gy pq pn l po pp">col_names <strong class="pi iu">=</strong> ['age', 'bmi']<br/>features <strong class="pi iu">=</strong> data_lasso[col_names]<br/>scaler <strong class="pi iu">=</strong> StandardScaler()<strong class="pi iu">.</strong>fit(features<strong class="pi iu">.</strong>values)<br/>features <strong class="pi iu">=</strong> scaler<strong class="pi iu">.</strong>transform(features<strong class="pi iu">.</strong>values)<br/>data_lasso[col_names] <strong class="pi iu">=</strong> features</span><span id="ef43" class="nn kj it pi b gy pq pn l po pp">search <strong class="pi iu">=</strong> GridSearchCV(Lasso(),<br/>                      {'alpha':np<strong class="pi iu">.</strong>arange(0.1,200,1)},<br/>                      cv <strong class="pi iu">=</strong> 5, scoring<strong class="pi iu">=</strong>"neg_mean_squared_error",verbose<strong class="pi iu">=</strong>0<br/>                      )</span><span id="5eaf" class="nn kj it pi b gy pq pn l po pp">x_train, x_test, y_train, y_test <strong class="pi iu">=</strong> train_test_split(data_lasso<strong class="pi iu">.</strong>drop(columns<strong class="pi iu">=</strong>["charges"]), data_lasso["charges"], test_size<strong class="pi iu">=</strong>0.2, random_state<strong class="pi iu">=</strong>42)</span><span id="f0f5" class="nn kj it pi b gy pq pn l po pp">search<strong class="pi iu">.</strong>fit(x_train,y_train)</span><span id="27c8" class="nn kj it pi b gy pq pn l po pp">search<strong class="pi iu">.</strong>best_params_</span></pre><p id="f5fd" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu"> Out[ ]: {'alpha': 74.1} </strong></p><p id="e7ce" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">alpha 参数定义了权重惩罚能力。如果它很高，权重就变成了 0。在这种情况下，我进行了交叉验证的网格搜索，以找到最佳的 alpha 参数，在这种情况下是 74.1。</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="ca26" class="nn kj it pi b gy pm pn l po pp">lasso_reg <strong class="pi iu">=</strong> Lasso(alpha <strong class="pi iu">=</strong> 74.1)<br/>lasso_reg<strong class="pi iu">.</strong>fit(x_train,y_train)</span></pre><h2 id="437f" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated"><strong class="ak">第二步:预测结果</strong></h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="72b7" class="nn kj it pi b gy pm pn l po pp">lasso_pred <strong class="pi iu">=</strong> lasso_reg<strong class="pi iu">.</strong>predict(x_test)<br/>pred_result(lasso_pred, y_test)</span></pre><p id="cdc0" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">Out[]:MSE:34153021 | MAE:4237 | R2 score:0.69</strong></p><h2 id="df7f" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 3:全局可解释性</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="de4d" class="nn kj it pi b gy pm pn l po pp">coefficients <strong class="pi iu">=</strong> lasso_reg<strong class="pi iu">.</strong>coef_<br/>importance <strong class="pi iu">=</strong> np<strong class="pi iu">.</strong>abs(coefficients)</span><span id="0c0f" class="nn kj it pi b gy pq pn l po pp">plt<strong class="pi iu">.</strong>figure(figsize<strong class="pi iu">=</strong>(14,6))<br/>plt<strong class="pi iu">.</strong>bar(x_train<strong class="pi iu">.</strong>columns, importance)<br/>plt<strong class="pi iu">.</strong>title("Feature Importance (Lasso)",fontsize<strong class="pi iu">=</strong>25)<br/>plt<strong class="pi iu">.</strong>ylabel("t-statistic (absolute value)",fontsize<strong class="pi iu">=</strong>18)<br/>plt<strong class="pi iu">.</strong>grid()<br/>plt<strong class="pi iu">.</strong>xticks(rotation<strong class="pi iu">=</strong>90,fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi pw"><img src="../Images/adcf04922f8a1a7111843cf5d8ebc996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WRafFS7Jq0f13TcpQdKOZg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><p id="1b3b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">结果显示，<strong class="lc iu">套索</strong>使得<strong class="lc iu">性别 _ 男性，地域 _ 西北，地域 _ 西南</strong>特征权重<strong class="lc iu"> </strong>到<strong class="lc iu">为零</strong>和<strong class="lc iu">地域 _ 西南特征和子特征(几乎为零)。</strong>所以，模型<strong class="lc iu"> </strong>几乎用 3 个特征:年龄、bmi、吸烟者 _ 是作为预测指标。和之前一样，smoker_yes 特征最重要，其次是年龄和 bmi 特征。值得一提的是，仅使用这 3 个特征，该模型比使用所有特征的简单线性回归做出了更好的预测。</p><h2 id="b73b" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 4:本地可解释性</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="cc7b" class="nn kj it pi b gy pm pn l po pp">forth_sample <strong class="pi iu">=</strong> x_test<strong class="pi iu">.</strong>iloc[4]<br/>lasso_reg<strong class="pi iu">.</strong>coef_</span><span id="19af" class="nn kj it pi b gy pq pn l po pp">print("predicted: ",int(lasso_reg<strong class="pi iu">.</strong>predict([forth_sample<strong class="pi iu">.</strong>values])))<br/>print("actual: ",int(y_test<strong class="pi iu">.</strong>iloc[4]))</span></pre><p id="31d4" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">预测:26662 <br/>实际:33750</p><h1 id="824c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.决策图表</h1><h2 id="721c" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 1:导入数据预处理数据—对分类列进行编码并进行标准缩放。</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="bca9" class="nn kj it pi b gy pm pn l po pp">data <strong class="pi iu">=</strong> pd<strong class="pi iu">.</strong>read_csv("insurance.csv")<br/>encoder <strong class="pi iu">=</strong> LabelEncoder()<br/>data[['age','sex','smoker','region']] <strong class="pi iu">=</strong> data[['age','sex','smoker','region']]<strong class="pi iu">.</strong>apply(encoder<strong class="pi iu">.</strong>fit_transform)<br/>data_tree <strong class="pi iu">=</strong> data<strong class="pi iu">.</strong>copy()</span><span id="2f26" class="nn kj it pi b gy pq pn l po pp">tree_reg <strong class="pi iu">=</strong> DecisionTreeRegressor(random_state<strong class="pi iu">=</strong>42,max_leaf_nodes<strong class="pi iu">=</strong>5) <em class="lw"># max_leaf_node =5 for simlicity</em></span><span id="708e" class="nn kj it pi b gy pq pn l po pp">x_train, x_test, y_train, y_test <strong class="pi iu">=</strong> train_test_split(data_tree<strong class="pi iu">.</strong>drop(columns<strong class="pi iu">=</strong>["charges"]), data_tree["charges"], test_size<strong class="pi iu">=</strong>0.2, random_state<strong class="pi iu">=</strong>42)</span><span id="d5c6" class="nn kj it pi b gy pq pn l po pp">tree_reg<strong class="pi iu">.</strong>fit(x_train,y_train)</span></pre><p id="e563" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">为了简单起见，我将 max_leaf_nodes 设置为 5，以便更好地查看可视化效果。</p><h2 id="eaf8" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">第二步:预测结果</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="b7a1" class="nn kj it pi b gy pm pn l po pp">tree_pred <strong class="pi iu">=</strong> tree_reg<strong class="pi iu">.</strong>predict(x_test)<br/>pred_result(tree_pred, y_test)</span></pre><p id="4208" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">MSE:26368109 | MAE:3325 | R2 score:0.8</strong></p><p id="43eb" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">结果远比简单的线性回归和套索要好</p><h2 id="18d3" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 3:全局可解释性</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="98ee" class="nn kj it pi b gy pm pn l po pp">importance <strong class="pi iu">=</strong> tree_reg<strong class="pi iu">.</strong>feature_importances_<br/>plt<strong class="pi iu">.</strong>figure(figsize<strong class="pi iu">=</strong>(14,6))</span><span id="e8eb" class="nn kj it pi b gy pq pn l po pp"><strong class="pi iu">for</strong> i,v <strong class="pi iu">in</strong> enumerate(importance):<br/>	print('Feature: %0d, Score: %.5f' <strong class="pi iu">%</strong> (i,v))<br/><em class="lw"># plot feature importance</em><br/>plt<strong class="pi iu">.</strong>bar(x_train<strong class="pi iu">.</strong>columns, importance)<br/>plt<strong class="pi iu">.</strong>yticks(fontsize<strong class="pi iu">=</strong>15)<br/>plt<strong class="pi iu">.</strong>show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi px"><img src="../Images/df9f06ee2230b44eedea1fd45ad5432c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pmrWFUQAJjMGvlPRw9q8w.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><p id="8ae4" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如决策树特征重要性图所示，最<strong class="lc iu">重要的特征</strong>是<strong class="lc iu">吸烟者</strong>，其次是 bmi 和年龄。这些都是和以前一样重要的特性。同样值得注意的是，特征重要性分数的总和是 1。</p><h2 id="085c" class="nn kj it bd kk no np dn ko nq nr dp ks lj ns nt ku ln nu nv kw lr nw nx ky ny bi translated">步骤 4:本地可解释性</h2><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="af9a" class="nn kj it pi b gy pm pn l po pp">forth_sample <strong class="pi iu">=</strong> x_test<strong class="pi iu">.</strong>iloc[4]<br/>print("predicted: ",int(tree_reg<strong class="pi iu">.</strong>predict([forth_sample<strong class="pi iu">.</strong>values])))<br/>print("actual: ",int(y_test<strong class="pi iu">.</strong>iloc[4]))</span></pre><p id="7441" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">出[]:预测:36691 实际:33750 </strong></p><p id="de28" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">这个个人预测的解释是如此简单。正如我们看到的，第 5 个样本点预测是 36691。我们可以在下面直观地检查为什么我们会得到这样的结果。</p><pre class="md me mf mg gt ph pi pj pk aw pl bi"><span id="b4dc" class="nn kj it pi b gy pm pn l po pp"><strong class="pi iu">from</strong> sklearn.tree <strong class="pi iu">import</strong> export_graphviz<br/><strong class="pi iu">from</strong> IPython.display <strong class="pi iu">import</strong> Image<br/><strong class="pi iu">from</strong> subprocess <strong class="pi iu">import</strong> call<br/>export_graphviz(tree_reg, out_file<strong class="pi iu">=</strong>'tree.dot', <br/>                feature_names <strong class="pi iu">=</strong> x_train<strong class="pi iu">.</strong>columns,<br/>                rounded <strong class="pi iu">=</strong> <strong class="pi iu">True</strong>, proportion <strong class="pi iu">=</strong> <strong class="pi iu">False</strong>, <br/>                precision <strong class="pi iu">=</strong> 2, filled <strong class="pi iu">=</strong> <strong class="pi iu">True</strong>)<br/>call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600']) <em class="lw"># Convert to png using system command (requires Graphviz)</em><br/>Image(filename <strong class="pi iu">=</strong> 'tree.png')</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi py"><img src="../Images/eb29c7b64aa2f9d58a94ce85147317c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2f17w8NMuCq8_VHh8KEpg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">作者照片</p></figure><p id="aada" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在右侧，我们可以看到第 5 个样本特征值。吸烟者=1，bmi=31.92，年龄=1。如果我们沿着树从上到下，我们可以很容易地评估预测。</p><h1 id="ac50" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论:</h1><p id="4f61" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以我建立了线性回归、套索和决策树的可解释性。当我们改变机器学习算法时，可解释性和最重要的特征会改变，因为它们的性质不同。</p><p id="c220" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在下面的文章中，我将使用部分依赖图、累积局部效应、置换特征重要性和许多其他方法来解释更复杂的 ML 方法。</p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="d207" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如果你想了解更多关于<strong class="lc iu">应用数据科学</strong>的信息，这里是<strong class="lc iu">我的新 YouTube 频道</strong>——<a class="ae or" href="https://www.youtube.com/channel/UCvlF0PPaQ2GAuqYKJT4UpJQ" rel="noopener ugc nofollow" target="_blank">AI 学院与朋友</a></p><div class="nz oa gp gr ob oc"><a href="https://www.youtube.com/channel/UCvlF0PPaQ2GAuqYKJT4UpJQ" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">和朋友一起的 AI 学院</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">与朋友、家人和全世界分享您的视频</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">www.youtube.com</p></div></div><div class="ol l"><div class="pz l on oo op ol oq mm oc"/></div></div></a></div></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><blockquote class="ms mt mu"><p id="3169" class="la lb lw lc b ld lx ju lf lg ly jx li mv lz ll lm mw ma lp lq mx mb lt lu lv im bi translated"><em class="it">您可以在</em> <a class="ae or" href="https://medium.com/@gkeretchashvili" rel="noopener"> <em class="it">中</em> </a> <em class="it">上关注我，为即将到来的文章保持更新。</em></p></blockquote><div class="nz oa gp gr ob oc"><a href="https://medium.com/@gkeretchashvili" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">gurami keretcashvili-培养基</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">阅读 Gurami Keretchashvili 在介质上的作品。数据科学家| AI 理工学院硕士研究生…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">medium.com</p></div></div><div class="ol l"><div class="qa l on oo op ol oq mm oc"/></div></div></a></div><h1 id="391b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><p id="5b1f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1]克里斯托弗·莫尔纳尔，<a class="ae or" href="https://christophm.github.io/interpretable-ml-book/tree.html" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a> (2022)</p><p id="b7c3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">[2]马勒·杨奇煜，<a class="ae or" href="https://www.explorium.ai/blog/interpretability-and-explainability-part-1/" rel="noopener ugc nofollow" target="_blank">可解释性与可解释性</a> (2019)</p><p id="10fd" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">[3] Will Koehrsen，<a class="ae or" rel="noopener" target="_blank" href="/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c">如何使用 Scikit-Learn 在 Python 中可视化来自随机森林的决策树</a> (2018)</p><p id="6ced" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">[4] <a class="ae or" href="https://scikit-learn.org/stable/modules/linear_model.html" rel="noopener ugc nofollow" target="_blank"> Sklearn 线性模型</a></p></div></div>    
</body>
</html>