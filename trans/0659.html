<html>
<head>
<title>Paper explained: Pushing The Limits Of Self-Supervised ResNets: Can We Outperform Supervised Learning Without Labels On ImageNet?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释:推动自我监督网络的极限:我们能胜过ImageNet上没有标签的监督学习吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-explained-pushing-the-limits-of-self-supervised-resnets-can-we-outperform-supervised-2ba322dd6409#2022-02-26">https://towardsdatascience.com/paper-explained-pushing-the-limits-of-self-supervised-resnets-can-we-outperform-supervised-2ba322dd6409#2022-02-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="02af" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索ReLICv2中的新方法</h2></div><p id="5f64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，我们将看看最近的一篇论文，这篇论文推动了自我监督学习的发展，由DeepMind发表，昵称为ReLICv2。</p><p id="9a1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在他们的出版物<em class="le">“推动自我监督网络的极限:我们能胜过ImageNet上没有标签的监督学习吗？”</em>、Tomasev等人提出了对他们在ReLIC后面的名为<em class="le">“通过不变因果机制的表征学习”</em>的论文中提出的技术的改进。他们方法的核心是增加了一个Kullback-Leibler-Divergence损失，这是用经典对比学习目标的概率公式计算出来的。不仅如此，他们还使用了一个完善的增强方案，并借鉴了其他相关出版物的成功经验。</p><p id="74c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我试图保持文章的简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/df5cca607553a617624b4bd5ea706db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pv8i_1XXjgp-Xg5luFLHDA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">遗迹训练管道的插图。来源:<a class="ae lv" href="https://arxiv.org/pdf/2010.07922.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="6186" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:计算机视觉的自我监督和非监督预训练</h1><p id="cdfe" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们深入讨论之前，有必要快速回顾一下自我监督预培训是怎么回事。如果你一直在阅读我的其他自我监督学习的故事，或者你熟悉自我监督预培训，请随意跳过这一部分。</p><p id="eef3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统上，计算机视觉模型总是使用<strong class="kk iu">监督学习</strong>来训练。这意味着人类看着这些图像，并为它们创建了各种各样的标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。</p><p id="8594" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，<strong class="kk iu">自我监督学习不需要任何人为创造的标签</strong>。顾名思义，<strong class="kk iu">模特学会自我监督</strong>。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，<strong class="kk iu">我们让模型知道这些图像仍然包含相同的视觉信息</strong>，即相同的对象。<strong class="kk iu">这导致模型学习相同对象的相似潜在表示(输出向量)。</strong></p><p id="c9bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。</p><h1 id="8e50" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">新颖的贡献和知识的结合</h1><p id="d164" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">正如许多其他自我监督的预训练技术一样，ReLICv2训练过程的第一步是数据扩充。在论文中，作者首先提到了以前成功的增强方案的使用。</p><p id="6b24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个是在SwAV中使用的增强。与以前的工作相反，SwAV不仅创建了输入图像的两种不同的裁剪，而且多达6种裁剪。这些可以制成不同的尺寸，如224x244和96x96，最成功的数量是两个大作物和6个小作物。如果你想了解更多关于SwAV的增强计划，请务必<a class="ae lv" href="https://medium.com/towards-data-science/paper-explained-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-f9e87db3cb9b" rel="noopener">阅读我关于它的故事</a>。</p><p id="5dca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">前面描述的第二组扩充来自SimCLR。这个方案现在被这个领域几乎所有的报纸所采用。通过应用随机水平翻转、颜色失真、高斯模糊和曝光来处理图像。如果你想了解更多关于SimCLR的信息，请务必去<a class="ae lv" href="https://medium.com/towards-data-science/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703" rel="noopener">看看我的文章</a>。</p><p id="c7cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是ReLICv2还使用了另一种增强技术:从图像中的对象中移除背景。为了实现这一点，他们以无人监管的方式在一些ImageNet数据上训练了一个<strong class="kk iu">显著背景去除</strong>模型。作者发现，当以10%的概率应用时，这种增加是最有效的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mt"><img src="../Images/ad231a73e0d9e647dac1df8597bc388a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JZZ6yW6vGgfXODW6aGkX8g.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">使用无监督DeepUSPS的显著背景去除增强。来源:<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="80fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦图像被增强并且进行了多次裁剪，输出就通过编码器网络和目标网络，它们输出相同维度的特征向量。<strong class="kk iu">当使用反向传播更新编码器网络时，目标网络通过类似于<a class="ae lv" rel="noopener" target="_blank" href="/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb">MoCo框架</a>的动量计算</strong>接收更新。</p><p id="d57f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ReLICv2的总体目标是学习编码器网络，以便为相同的类产生一致的输出向量。为了实现这一点，作者制定了一个新的损失函数。他们从<strong class="kk iu">标准对比负对数似然</strong>开始，其核心是一个相似性函数，将锚视图(主输入图像)与正样本(图像的增强版本)和负样本(同一批中的其他图像)进行比较。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/9b6936ab5b9bfc88d3f99324121f3eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dL5TcDyjC5ciDKFDEYUiow.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">ReLICv2损失函数由负对数似然性以及锚视图和正视图的Kullback-Leibler散度组成。来源:<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="89b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种损失通过<strong class="kk iu">对比目标的概率公式</strong> : <strong class="kk iu">锚图像和正面</strong>的可能性之间的Kullback-Leibler差异而扩展。这种<strong class="kk iu">强制网络不学习积极的靠近和消极的远离，但是在避免可能导致学习崩溃的极端聚类时，在集群之间创建更平衡的景观</strong>。因此，这个附加损失项可以被视为类似于<strong class="kk iu">调节</strong>。这两项都有一个α和β超参数，允许对这两项损失进行单独加权。</p><p id="fef9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有这些新奇事物的加入证明是成功的。为了找出是在哪些方面，让我们更仔细地看看论文中给出的结果。</p><h1 id="888a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="17f8" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated"><strong class="kk iu">relic v2试图证明的要点，正如其在论文标题中所说的，是自监督预训练方法只有在它们都为编码器网络使用相同的网络架构时才具有可比性。</strong>对于他们的工作，他们选择使用经典的ResNet-50。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mv"><img src="../Images/57e4c4c69c80b0f28c803dba3dbfcf7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HP0PtEEfpE2nwosi8EIqMQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">在ImageNet线性评估方案下使用不同的预训练ResNet-50的结果。来源:<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="a2f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用相同的ResNet-50并在ImageNet-1K上训练其线性层，同时冻结所有其他权重时，ReLICv2的输出远远超过现有方法。引入的改进甚至导致相对于原始遗物纸的性能优势。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mw"><img src="../Images/e71754333c83a101164547114a86295b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hbbG2e-gyGEyoZjWUj7WCg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">与不同数据集上的监督预训练模型相比，准确性有所提高。来源:<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="d986" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当在其他数据集上比较迁移学习性能时，ReLICv2与其他方法(如NNCLR和BYOL)相比，继续表现出令人印象深刻的性能。这进一步表明ReLICv2是一种新型的自我监督预训练方法。对其他数据集的评估在其他论文中不常提及。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mx"><img src="../Images/c9f355c180eddbf220e7a80d9115da71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O9aa0GAiQvw4TB_2b_85qA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">ReLICv2和BYOL对习得集群的阐释。点越蓝，学习到的越接近对应的类簇。来源:<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="eda0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个生动的图形显示了ReLICv2学习的职业比其他框架如BYOL学习的职业更接近。这再次表明，与其他方法相比，该技术具有创建更细粒度集群的潜力。</p><h1 id="f40d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="d7b4" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您已经了解了ReLICv2，这是一种用于自我监督预训练的新方法，已经显示出有希望的实验结果。</p><p id="f15b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过结合对比学习目标的概率公式，并通过添加经过验证的增强方案，该技术已经能够将视觉中的自我监督预训练空间向前推进。</p><p id="7ff7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然我希望这个故事让你对ReLICv2有了一个很好的初步了解，但仍有很多东西有待发现。因此，即使你是这个领域的新手，我也会鼓励你自己阅读这些论文。你必须从某个地方开始；)</p><p id="6fe8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在Twitter上给我留言，我的账户链接在我的媒体简介上。</p><p id="8ed9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="f3db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="87f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="a96e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]米特洛维奇、约瓦娜等人，“通过不变因果机制的表征学习”arXiv预印本arXiv:2010.07922  (2020)。<a class="ae lv" href="https://arxiv.org/pdf/2010.07922.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.07922.pdf</a></p><p id="7790" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] Tomasev，Nenad，等人“推动自我监督的结果网的极限:在ImageNet上没有标签的情况下，我们能胜过监督学习吗？."<em class="le"> arXiv预印本arXiv:2201.05119 </em> (2022)。<a class="ae lv" href="https://arxiv.org/pdf/2201.05119.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2201.05119.pdf</a></p></div></div>    
</body>
</html>