<html>
<head>
<title>Logistic Regression and the Missing Prior</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归和缺失先验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-and-the-missing-prior-7c36039af851#2022-03-10">https://towardsdatascience.com/logistic-regression-and-the-missing-prior-7c36039af851#2022-03-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0c16" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何利用 Jeffreys 先验减少 logistic 回归的偏倚</h2></div><p id="03dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设<strong class="kh ir"> X </strong>表示回归量矩阵，而<strong class="kh ir"> y </strong>，</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/01a85a545a275d20bf9c3b29b1760bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkRADfHKAZrkA5Es4kGXVw@2x.png"/></div></div></figure><p id="e19f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">表示目标值的向量。</p><p id="0fa1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们假设数据是从逻辑回归模型生成的，那么我们在看到数据后对权重<strong class="kh ir"> w </strong>的信任由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/7dd46f178520f48a73dc087003b25e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1YYoaI099Gv-MOLIqgoWQ@2x.png"/></div></div></figure><p id="3275" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，P( <strong class="kh ir"> y </strong> | <strong class="kh ir"> X </strong>，<strong class="kh ir"> w </strong>)称为可能性，由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2ab48a38812e729e3b8f33fde8ccf25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6QjgRL5Ux2BmjuMD_b3uQ@2x.png"/></div></div></figure><p id="af91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而 P( <strong class="kh ir"> w </strong>)称为先验，在看到数据之前量化我们对<strong class="kh ir"> w </strong>的信念。</p><p id="6559" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，逻辑回归适合最大化 P( <strong class="kh ir"> y </strong> | <strong class="kh ir"> X </strong>，<strong class="kh ir"> w </strong>)，而不考虑先验</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/0bbc7ad6b24a8513093ca19e5b653dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQ6VQTWIg9POTDjQlyZlAA@2x.png"/></div></div></figure><p id="ae82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而<strong class="kh ir"> w </strong> _ML 就是我们所说的权重的<em class="ln">最大似然</em>估计。</p><p id="ed44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是无视先验通常是个坏主意。我们来看看为什么。</p><h2 id="8483" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">最大似然模型有什么问题</h2><p id="60d3" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">考虑一个具体的例子。</p><p id="5ade" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你是一名研究狼的生物学家。作为你工作的一部分，你一直在跟踪狼群，观察它们的捕猎行为。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mm"><img src="../Images/48af220fe9138943ce51f8cf0e6cf20c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rJPM84L8qsYaC3GMdVCqw.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">托马斯·博诺梅蒂在 Unsplash 的照片</p></figure><p id="e343" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你想知道是什么让狩猎成功。为此，您需要建立一个逻辑回归模型来预测搜寻是成功还是失败。考虑像群体大小和猎物类型这样的因素会很有趣，但是为了简单起见，你只从一个偏见术语开始。</p><p id="1a37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让<em class="ln"> n </em>表示狩猎的总次数。如果<em class="ln"> b </em>表示我们的<br/>偏置项的值，那么观察到<em class="ln"> k </em>成功搜索的可能性由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/b8bf27cc4c148ff2241ab287452291ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktYaxZbGaDyl7WCJrcypaw@2x.png"/></div></div></figure><p id="8a9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们假设我们观察了 8 次狩猎，没有记录到成功的狩猎。(狼的捕猎成功率实际上相当低，所以这样的结果并不令人惊讶)。对于<em class="ln"> b </em>来说，什么是好的工作估算？最大似然估计是-∞，但是我们知道这没有意义…如果没有成功的机会，为什么还要去狩猎呢？</p><p id="2596" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们不选择<em class="ln"> b </em>来最大化可能性，而是选择<em class="ln"> b </em>来最大化后验分布的概率，我们可以获得更好的工作估计</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/aa50d0a0cafd7f5f6eafea052758efa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKMqOZYVyLtZn9_2y6U32A@2x.png"/></div></div></figure><p id="87fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具有适当选择的先验 P(b)。我们稍后将讨论选择先验的过程，但现在考虑先验</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/07e097b8f3c8ef2d3a8683c0155de8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RcPZossYhsTZ7aNDN3F0lw@2x.png"/></div></div></figure><p id="7c42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个先验的后验概率是</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/134040fc6be87803364739c574fb3034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SoenVLOYFngc-7SvOdslRg@2x.png"/></div></div></figure><p id="ad7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">放 p = (1 + exp(-b))⁻。我们可以通过对 0 取对数、求导和求解来找到 MAP 估计</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/735cc5582f5be75b9a9085cd4ed1de18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2lVhaAHCNTxfXaunL5sfg@2x.png"/></div></div></figure><p id="51e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">求解 0，我们有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d809e6a606fff6aed7cbbc5954f09f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2fXq9vVpccYeg3fTnWd_2g@2x.png"/></div></div></figure><p id="faf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，这给了我们更合理的估计</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/027c9661008906f8ecde3b9ec973d90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHYDrAK2fc-O5cAz8NRPZA@2x.png"/></div></div></figure><p id="17f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用的先验是无信息先验的一个例子。</p><h2 id="b409" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">什么是无信息先验，我们如何发现它们</h2><p id="01b4" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">注意:这里对非信息先验的介绍紧跟 Box &amp; Tiao 的书<a class="ae mr" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118033197" rel="noopener ugc nofollow" target="_blank">统计分析中的贝叶斯推断</a>的第 2 章。</p><p id="1b4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们给定一个模型的似然函数 L(θ| <strong class="kh ir"> y </strong>，该模型有参数θ和数据<strong class="kh ir"> y </strong>。当我们没有关于θ的特定知识时，什么是好的使用前？</p><p id="06d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们来看一个具体的例子。假设数据由方差σ已知但均值θ未知的正态分布生成。该模型的似然函数为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/95b2696eb6d6200ee1465870b0682b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usqPd4tHm3EfvIA_dOMUgg@2x.png"/></div></div></figure><p id="ea79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是 n=5 和不同 y̅:值的概率图</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/afaa6795b530c5f476416af956f5befa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*NeHKPdcJRZsXuwmBfoly-g.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">方差为 1，<br/> n=5，y̅.值不同的正态分布数据均值的似然函数图片作者。图片作者。</p></figure><p id="3e73" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，对于所有的 y̅值，似然曲线的形状都是相同的，只有平移不同。当似然曲线具有这种性质时，我们说它是经过<em class="ln">转换的</em>数据。</p><p id="e19e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在考虑两个大小相同的范围[a，b]和[a + t，b + t]。如果观测数据由 y̅+t 而不是 y̅.转换，则两者相对于似然曲线的位置是相等的因此，我们应该</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2e80570ce6514d9c5fd1e612deede6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMO-OxArFVHuMlbiiT-MaQ@2x.png"/></div></div></figure><p id="dc19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者制服先验</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1bac855a7c0e5edef11fa33b40733c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXrXQwNYO3lyVKx0zREKzg@2x.png"/></div></div></figure><p id="04cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们对似然函数 L(θ| <strong class="kh ir"> y </strong>)使用另一种参数化:我们通过<em class="ln"> u </em> = ϕ(θ)进行参数化，并在<em class="ln"> u </em>之前应用均匀化，其中ϕ是某种单调内射变换。累积分布函数由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1075f20475560d768306347df5137c24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KCgKiUTNIaHxIb81rkAdsA@2x.png"/></div></div></figure><p id="0379" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="ln"> Z </em>是某个归一化常数。进行替换<em class="ln"> θ = ϕ⁻ (u) </em>以回到原始参数化，然后我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/12140d4170d69af88a42c3d9c5665b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIInDK3my_7u6xMRpXln7A@2x.png"/></div></div></figure><p id="9a31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们看到在<em class="ln"> u </em>上的均匀先验等价于在<em class="ln"> θ </em>上的先验<em class="ln"> P(θ) ∝ ϕ'(θ) </em>。找到θ的无信息先验等价于找到一个变换<em class="ln"> ϕ </em>来转换似然函数数据。</p><p id="4077" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们回到只有偏差项的逻辑回归模型的似然函数</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/22aa52fa69b6a8efe377b93334b61947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBWg6XJrOSeUaUDbwYAPpg@2x.png"/></div></div></figure><p id="4cd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绘制<em class="ln"> n=8 </em>和不同的<em class="ln"> k 值</em>的可能性给出了</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/2de372eea519425094426f33037d571d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*MRjEmKb_JmkZrpXOujsIbw.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">只有一个偏差项的逻辑回归模型的似然函数，<br/> n=8，k 值不同。</p></figure><p id="bfd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图中可以很清楚地看到，可能性函数没有经过数据转换。对于<em class="ln"> k=4 </em>的曲线比其他的更为陡峭，对于<em class="ln"> k=1 </em>和<em class="ln"> k=6 </em>的曲线是歪斜的。</p><p id="6cca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，对于适当表现的似然函数，我们可以使用 log L(θ| <strong class="kh ir"> y </strong>)关于<br/> θ_ML 的二阶泰勒级数展开，通过<br/> a 高斯来近似 L(θ| <strong class="kh ir"> y </strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1e9d9c1bfdaee61b816178c366f117db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaLUGMwaXeuTj7cVuqgGrw@2x.png"/></div></div></figure><p id="84ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/f3f1cb6a9a0522e3bb0d7dd20d278130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHRHf27B-k19bqu3zefMjQ@2x.png"/></div></div></figure><p id="578d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着 n →∞，近似变得越来越精确。</p><p id="b55c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看这是如何为我们的简化逻辑回归模型工作的。</p><p id="b184" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对 log L(b|k)求微分得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/c2ee36e5bcff7d3102f7922963dd63de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ExQiYn0nfCjpHtiUGARQSw@2x.png"/></div></div></figure><p id="206f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次微分给了我们</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/4eda2f4e6e54ef82e9cec4b83b042c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cNIcV45c1Rvoy179GDYlfg@2x.png"/></div></div></figure><p id="e899" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是高斯近似的样子</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/9cafe8e904f7e5931e30ff613f24047c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*v1K4wKtL5QFVXFc9hEpWaw.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">仅含偏倚项的逻辑回归模型的似然函数<br/>及其高斯近似(n=8，k=1)。图片作者。</p></figure><p id="c86d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，考虑当我们用θ = ϕ⁻ (u)重新参数化时，L(θ，<strong class="kh ir"> y </strong>)的高斯近似会发生什么。</p><p id="80c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">差异化给了我们</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/83afc9f5526fc14a9b779c8aeaf4aa06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADo8qIPG_jc-l741ji-mfw@2x.png"/></div></div></figure><p id="f756" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次微分给了我们</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/1052d3e809999ebd20ec0e1252a658bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xVr3Iww6Uiv_JuZm2F7_IA@2x.png"/></div></div></figure><p id="6adf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以最大值 u_ML= ϕ(θ_ML 进行评估)，我们有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/a2b4c9bc0079ee1ad0e41ae30a2d276d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vjdBKcPZQ-v4B2UqWpZrw@2x.png"/></div></div></figure><p id="7857" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们申请的地方</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/3822f480a910c8cf360863d4db73a112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kcsvxh82p0f8t2SBPDtLA@2x.png"/></div></div></figure><p id="7147" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回到逻辑回归，把</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2dd6499f0ff6d21d9f555e596ec74df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-VyKuh1sECeypXVnb34Wg@2x.png"/></div></div></figure><p id="70bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么我们有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/eb9e7466cfcc6efc192ee3ed305ef7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8o-w5mahEcFhEEATUuakQ@2x.png"/></div></div></figure><p id="5336" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，关于 u_ML 的高斯近似变成</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/beedc6e62012fbd2a6190a46ba6542be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HLFFSv9Tdcu7qdRG3b6prA@2x.png"/></div></div></figure><p id="26e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此似然函数现在是近似的数据转换。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/070bb6adf12a75ed7b1b4e06aef91b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*vEDnGKDzSR01R7C68uMoDQ.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">只有一个偏差项的逻辑回归模型的重新参数化似然函数，<br/> n=8，k 值不同。</p></figure><h2 id="b670" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">杰弗里斯先验</h2><p id="fe94" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">现在让我们对多参数情况下的 Jeffreys 先验作一个简要的概述。<br/>设<strong class="kh ir"> θ </strong> = (θ₁，…，θ_k)^T 表示参数的向量。</p><p id="0c84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于单参数情况，似然函数函数可以由关于最优值的高斯函数来近似</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/94ce0ebb04e838b4cf2f9fbebe985ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KD-cFLr2lHI8YG_9vd9rBg@2x.png"/></div></div></figure><p id="4970" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/54897bc21ad8e2741cab55e73a217f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrlDPvAx5HL4S8onXMxywg@2x.png"/></div></div></figure><p id="2927" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，假设我们用<strong class="kh ir"> u </strong> = ϕ( <strong class="kh ir"> θ </strong>重新参数化。那么高斯近似的更新的 hessian 变成</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d45280bd382fa102c5c9df86db55861a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAF64IQy91pxeDCAVVn5pw@2x.png"/></div></div></figure><p id="9d59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/55e81d9633f870c36ce5091a1f68e85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O70PBuJKI9XLJvZOQqeMMA@2x.png"/></div></div></figure><p id="9cb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在一般的多变量情况下，我们可能无法以高斯近似变成数据转换的方式重新参数化，但是假设我们选择ϕ，使得</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/6086e5e1f9e70dd515050dc4ee0b99c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8HcD565cQBZ918PV2cbbVQ@2x.png"/></div></div></figure><p id="caf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/274745077b42d4d5c7c36c38244a8cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlLl8vXc4MgE7AgYJdjNdw@2x.png"/></div></div></figure><p id="a880" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一个常数行列式，意味着区域的体积将保持不变。</p><p id="ef0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设 S ∈ ℝᵏ是关于原点的区域，K 是关于<strong class="kh ir"> θ </strong> _ML 的重新参数化高斯近似的协方差矩阵<br/>。如果 L 是<strong class="kh ir"> K </strong>的乔莱斯基分解</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d1c3ea36e18b3616f2cdf75987f241ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xoYjznAFNIlV96WhRVZG1g@2x.png"/></div></div></figure><p id="0bb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">则 s 对应于 L⁻ S+ <strong class="kh ir"> θ </strong> _ML 关于<strong class="kh ir"> θ </strong> _ML 的区域，该区域具有固定的概率质量和体积</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/30938d06da50b9c0cf77755b277d5986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTFJJW7Ot0TLxcqYQM9DYw@2x.png"/></div></div></figure><p id="45e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> u </strong>上均匀先验的<strong class="kh ir"> θ </strong>对应的先验为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2f8e2f46d18ba749d080628759d352f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBJN1ZNj3ojpCMRg2UwiZQ@2x.png"/></div></div></figure><p id="f4ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而这叫做<em class="ln">杰弗里斯在先</em>。</p><p id="2809" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们将此应用于带权重的完全二项逻辑回归模型。负对数似然函数由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2aa1a8c6a51dde176151976d3e6c1042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DaNNHFGE1Zhqqws2iV662w@2x.png"/></div></div></figure><p id="e3e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/9aeb4306bb51fa108073a881901acdfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Xq_qtqp4BBpjaYc1dTU7Q@2x.png"/></div></div></figure><p id="7377" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">差异化给了我们</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/49179648fe4b031bc4fd4d57c2c319c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-eAnAg3B7z5jxqwBlWz-4g@2x.png"/></div></div></figure><p id="35b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/da715ae9f2266804b2d3b34d9b912abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oh3FlFu4EddA4YWDPv8hqg@2x.png"/></div></div></figure><p id="5350" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次微分给了我们</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/a09b86869e5d10e1196e1e5a83aa4ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqV3Ex1W5Dajj3piIvQ4yw@2x.png"/></div></div></figure><p id="027c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/f3bb9a2e7d22f0319cbbb7dea360e54e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrNN1BiqW7wpJB_ubF9KRA@2x.png"/></div></div></figure><p id="97ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以把黑森写成</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/7122437f9a42a7a99ecf332ee02e0c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEPdQcTggsGhcTogm7V6Fw@2x.png"/></div></div></figure><p id="8566" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> A </strong>是对角矩阵</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/42d318c7e058339adbde4117ab25f833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVpOs2CqOfrsMM6PoUZemA@2x.png"/></div></div></figure><p id="5e6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">杰弗里斯副院长是</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/27a20aaec7e414e1c27918f9b94583e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zlk_0S3TuFVMHlW5WUxJIA@2x.png"/></div></div></figure><h2 id="d5fb" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">用 Jeffreys 先验拟合逻辑回归</h2><p id="f6ba" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">用 Jeffreys 先验找到逻辑回归的映射模型相当于解决优化问题</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/0aad382bc308955bd02e3514ac15aa15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pf0-EAxODHGuueEKwm9gIw@2x.png"/></div></div></figure><p id="4535" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设π表示杰弗里斯先验的对数</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/cf22548fd3966b061402ffdde0eff5a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYVZpKgj1vH1WxsvvLXa9Q@2x.png"/></div></div></figure><p id="8cbc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们可以找到π的梯度和 hessian 的方程，那么我们可以应用标准的二阶优化算法来找到<strong class="kh ir"> w </strong> _MAP。</p><p id="a55b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从梯度开始，我们可以应用<a class="ae mr" href="https://en.wikipedia.org/wiki/Jacobi%27s_formula" rel="noopener ugc nofollow" target="_blank">雅可比公式</a>对行列式进行微分</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/3e1d65061a6c687388b3fc2fd76242d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xP9teC4NGL_MHky-A_2vQ@2x.png"/></div></div></figure><p id="9ad4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/66474f207948e06501248792da918fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fP4jIuglsUKTCzsL-EKRsA@2x.png"/></div></div></figure><p id="8ab0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kh ir"> A_w </strong>的导数是对角矩阵</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/16e045b3b684c98e5fb3c8877a2bef34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3tgymtPaaxSnZYzvDq_SQ@2x.png"/></div></div></figure><p id="0a91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 hessian，我们应用微分逆矩阵的公式</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/51b1fd82645282cf4c5d205624dc8ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2g1jy28fSoXJarN2TskcAw@2x.png"/></div></div></figure><p id="5e92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d031499bde16cfa23482932d643080ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OlTuYJvbH3BUeoJSp4GOZA@2x.png"/></div></div></figure><p id="ce57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/6c293e09d1358640193df62f67ef9264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ewsrqy_otYaDPMYQjPxEw@2x.png"/></div></div></figure><p id="1701" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过以有效的方式评估梯度和 hessian 计算并应用二阶信赖域优化器，我们可以快速迭代以找到<strong class="kh ir"> w </strong> _MAP。使用这种方法的 Python 包可以在 https://github.com/rnburn/bbai<a class="ae mr" href="https://github.com/rnburn/bbai" rel="noopener ugc nofollow" target="_blank">获得。</a></p><p id="4322" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们在一些例子中看看这是如何工作的。</p><h2 id="be16" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">示例 1:单变量模拟数据</h2><p id="3d1f" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">我们将从查看模拟数据集的先验和后验开始，其中<br/>只有一个回归变量，没有偏差。</p><p id="8157" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从生成数据集开始。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="5d87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来我们添加一个函数来计算先验</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="18a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并绘制出一系列的值</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/1445252da7ad1c26ecf6451508e548e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNtACehncdAG1Q96VwnJtg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">Jeffreys prior 对具有单个回归量的模拟数据集进行逻辑回归。图片作者。</p></figure><p id="a91c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们拟合逻辑回归图模型，并将 w_MAP 与<br/> w_true 进行比较。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="6e91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">印刷品</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/86d6a2b081090f0be47affe24b326bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9fEOzyWxILEMbJaEArFD7g@2x.png"/></div></div></figure><p id="fb6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们画出后验分布。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/63d82817dc6a48fc016be2731b5bb9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7ZwoJpseJ4h3UtO2ucVTQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">使用单个回归变量和 w_MAP 对模拟数据集进行逻辑回归的后验概率。图片作者。</p></figure><p id="e89d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的例子可以在 github.com/rnburn/bbai/example/05-jeffreys1.ipynb 找到。</p><h2 id="b640" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">示例 2:具有两个变量的模拟数据</h2><p id="db4d" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">接下来让我们看一个有两个变量的例子。</p><p id="dccb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们生成一个包含两个变量的数据集</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="0756" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们计算先验，并在一系列值上绘制出来</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/be3ee8b16bc973c88c6407fd3a2a80f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cL0a2UhlFco7KINdDsYb_g.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">Jeffreys 先验用于具有两个回归量的模拟数据集上的逻辑回归。图片作者。</p></figure><p id="668b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将拟合一个模型来查找<strong class="kh ir"> w </strong> _MAP。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="c3ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">印刷品</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/0c31da7a3bc343239254dec2e054e8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ow_WWP-a0A9ub8u6gnl-LA@2x.png"/></div></div></figure><p id="e034" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们绘制出以<strong class="kh ir"> w </strong> _MAP 为中心的后验分布。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mv"><img src="../Images/0d2f290c92cdeb2af39af33f3203c457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loTDDwnkOTZ5sxYO8BUXTw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">具有 Jeffreys 先验的逻辑回归的后验概率。图片作者。</p></figure><p id="f736" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的例子可以在 github.com/rnburn/bbai/example/06-jeffreys2.ipynb 找到。</p><h2 id="ff22" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">示例 3:乳腺癌数据集</h2><p id="b2db" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">现在，我们将根据现实世界的乳腺癌数据集拟合一个模型。</p><p id="6dbb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从加载和预处理数据集开始。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="67e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们拟合一个逻辑回归图模型，并使用费雪信息矩阵来估计标准误差。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="234c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们打印出重量和它们的标准误差。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="e0e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的例子可以在 github.com/rnburn/bbai/example/07-jeffreys-breast-cancer.py 找到。</p><h2 id="ec85" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">结论和进一步阅读</h2><p id="c318" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">与普通最小二乘法不同，逻辑回归的似然函数不需要数据转换。搜索数据转换转换会将我们带到 Jeffreys 先验，一种自然收缩先验。</p><p id="2e3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Firth 1993 [1]和 Kosmidis &amp; Firth 2021 [2]分析了 Jeffreys 先验惩罚最大化似然估计量的统计性质，发现它比标准的最大似然估计量具有更小的渐近偏差阶。Jeffreys prior 降低偏差的特性结合其处理分离问题的能力[3]，使其成为拟合逻辑回归模型的标准方法的绝佳替代品。</p><p id="e48c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ln">博文原载于</em><a class="ae mr" href="https://buildingblock.ai/logistic-regression-jeffreys" rel="noopener ugc nofollow" target="_blank"><em class="ln">【https://buildingblock.ai/logistic-regression-jeffreys】</em></a><em class="ln">。</em></p><h2 id="2b3f" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">参考</h2><p id="05b2" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">[1]弗思，D. (1993 年)。<a class="ae mr" href="https://www2.stat.duke.edu/~scs/Courses/Stat376/Papers/GibbsFieldEst/BiasReductionMLE.pdf" rel="noopener ugc nofollow" target="_blank">最大似然估计值的偏差减少</a>。<em class="ln"> Biometrika </em> 80，27–38。</p><p id="03ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]科斯米迪斯本人和弗斯博士(2021 年)。<a class="ae mr" href="https://academic.oup.com/biomet/article/108/1/71/5880219" rel="noopener ugc nofollow" target="_blank">Jeffreys-二项响应广义线性模型中的先验惩罚、有限性和收缩</a>。<em class="ln"> Biometrika </em>，第 108 卷，第 1 期，2021 年 3 月，第 71–82 页。</p><p id="1631" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]:海因策，g .和谢姆佩尔，M. (2002 年)。逻辑回归中分离问题的一种解决方法。<em class="ln">统计学家。医学。</em>212409–19。</p></div></div>    
</body>
</html>