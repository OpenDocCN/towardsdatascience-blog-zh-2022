<html>
<head>
<title>How to use the Empirical Distribution of your Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用你的数据的经验分布</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/empirical-distribution-6af5142eed09#2022-09-15">https://towardsdatascience.com/empirical-distribution-6af5142eed09#2022-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9723" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过从数据的经验分布中提取的随机变量来改进模拟建模</h2></div><p id="0456" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据机器学习和统计学，我们主要建立模型和理论，这些模型和理论假设数据的基本分布。这可能并不总是合适的，但是如何利用数据的经验分布来解决分析问题呢？</p><h2 id="0495" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">动机</h2><p id="a83d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">概率分布是<strong class="kk iu">统计</strong>、<strong class="kk iu">机器学习、</strong>的核心元素，在<strong class="kk iu">模拟</strong>中<strong class="kk iu">随机数生成</strong>的过程中也起着非常关键的作用。虽然现代统计库和软件完全能够从期望的分布中抽取随机数，如正态分布、指数分布或均匀分布(仅举几个例子)，但当基础数据<strong class="kk iu">不符合任何这些分布</strong>时，这项任务就变得更加复杂。为了预测核心信息<strong class="kk iu">，逆变换定理</strong>在这一切中扮演了巨大的角色。</p><blockquote class="mc md me"><p id="30d5" class="ki kj mf kk b kl km ju kn ko kp jx kq mg ks kt ku mh kw kx ky mi la lb lc ld im bi translated">本文要回答的主要问题是:“<strong class="kk iu">如何从任何经验分布中生成随机变量(RV)？</strong></p></blockquote><p id="188c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了激发你的兴趣，让我给你提供一个实际的问题:假设你经营自己的餐馆，你想模拟顾客到达的数量。正如你在《统计学101》一书中所述，为了模拟这样的问题，你可以从<strong class="kk iu">泊松分布</strong>(比率为λ)中提取到达次数，而到达间隔时间则通过<strong class="kk iu">指数分布</strong>(1/λ)来建模。然而，在实践中，<strong class="kk iu">你观察到这几乎从来都不是真的</strong>——不管出于什么原因，你的客人只是以一种你所研究的任何分布都无法捕捉的方式来来去去，留给你的是到达数据的经验分布——或者换句话说，你通过查看手头的数据直接推断出的<strong class="kk iu">分布</strong>。</p><p id="f98d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了模拟真实的游客到达，您希望直接从给定的底层分布中对随机事件进行采样，但是如何做到这一点呢？</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mj"><img src="../Images/0c4c23ece79da639ff103bf888f19c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Rb4rx6WYwQI7sC8mfOsSlg.gif"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">我们将在这篇文章中讨论什么</p></figure><p id="0534" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文探讨了任意<strong class="kk iu">经验分布的作用以及逆变换定理</strong>的作用，逆变换定理允许我们从给定的数据分布中生成随机变量:</p><ol class=""><li id="1ee0" class="mz na it kk b kl km ko kp kr nb kv nc kz nd ld ne nf ng nh bi translated"><a class="ae ni" href="#df69" rel="noopener ugc nofollow">经验数据分布</a></li><li id="0dda" class="mz na it kk b kl nj ko nk kr nl kv nm kz nn ld ne nf ng nh bi translated"><a class="ae ni" href="#cca3" rel="noopener ugc nofollow">逆变换定理</a></li><li id="8893" class="mz na it kk b kl nj ko nk kr nl kv nm kz nn ld ne nf ng nh bi translated"><a class="ae ni" href="#7ca9" rel="noopener ugc nofollow">从经验分布中抽取随机变量</a>(证明为什么所有这些真的有效)</li><li id="d646" class="mz na it kk b kl nj ko nk kr nl kv nm kz nn ld ne nf ng nh bi translated"><a class="ae ni" href="#dcd1" rel="noopener ugc nofollow">结论</a></li></ol><h2 id="df69" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">经验数据分布</h2><p id="626e" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">经验示例数据是通过具有不同参数(均值、方差)和不同抽取样本数量的两个正态分布生成的。这给我们留下了一个双峰正态分布。现在，假设对于我们的分析问题，我们感兴趣的是通过从N(0，1)分布中抽取来初始化权重矩阵。这可能很简单，但导致的问题是我们不会考虑<strong class="kk iu">沉重的右尾巴</strong>。因此，我们将从右尾提取很少的值，但是在平均值0附近的值要多得多。</p><p id="194d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，我们会取消<strong class="kk iu">对经验分布右侧的低估</strong>，并比我们应该看到的更少看到4左右的值。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi no"><img src="../Images/76f23e8740ac4cce28f43dd373d89f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FRN-QCnFmohbEdVVatFkQ.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">数据的直方图和密度</p></figure><p id="3b27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的直方图被分成20个区间。每个箱代表一个值范围，对于每个范围，我们计算落入其中的值的数量。</p><p id="313c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变量“bin”说明了每个值存储桶的边界，因此，为了找到直方图中第一个条形的高度，我们需要对落入范围从-3.310到-2.863的存储桶中的所有值进行计数。在经验数据中，有12个值可以分配到该范围。自然地，给定用于生成数据的基本分布，大多数出现在0和4均值附近。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="83c7" class="nu lf it nq b be nv nw l nx ny">bins =<br/>array([-3.310 , -2.863, -2.417, -1.971, -1.524, -1.078, -0.632, -0.186, 0.261, 0.707, 1.153, 1.599, 2.046, 2.492, 2.938, 3.384, 3.831, 4.277, 4.723, 5.169, 5.616])</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="2fab" class="nu lf it nq b be nv nw l nx ny"># bold: 0.707/1.153<br/># The bold printed numbers are used for explanation later on</span></pre><p id="6c8d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变量“counts”简单地计算落入每个相应仓中的事件的数量。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="3a5c" class="nu lf it nq b be nv nw l nx ny">counts = </span></pre><pre class="nz np nq nr bn ns nt bi"><span id="b8c8" class="nu lf it nq b be nv nw l nx ny">[  12,   52,  172,  374,  801, 1237, 1657, 1788, 1540, 1117,  697,<br/>        343,  145,   73,  193,  535,  666,  430,  148,   20]</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="21c3" class="nu lf it nq b be nv nw l nx ny"># value related to bin [0.707, 1.153]: 1117</span></pre><p id="9e6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">概率密度函数</strong></p><p id="487c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用从直方图获得的值，我们可以直接推断出经验数据的<strong class="kk iu">概率密度函数</strong> ( <strong class="kk iu"> PDF </strong>)。为此，我们只需将每个计数除以值的总数:</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="6173" class="nu lf it nq b be nv nw l nx ny">pdf = <br/>array([0.001, 0.004, 0.014, 0.031, 0.067, 0.103, 0.138, 0.149, 0.128, 0.093, 0.058, 0.029, 0.012, 0.006, 0.016, 0.045, 0.056, 0.036, 0.012, 0.002])</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="6ca5" class="nu lf it nq b be nv nw l nx ny"># p for value 1117: 0.093</span></pre><p id="4443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你观察PDF值，你会发现它们先增加，然后减少，再稍微增加(大约是x=4的值)，然后再减少到接近初始值。根据我们的直觉，这与我们之前看到的直方图/密度图非常吻合。</p><p id="9cf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，不需要手动进行拆分和计数；Numpy的<em class="mf"> histogram() </em>函数就是这样做的。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="b175" class="nu lf it nq b be nv nw l nx ny">values, bins = np.histogram(data, bins=20)</span></pre><p id="7bd6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">累积分布函数</strong></p><p id="e6ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们已经<strong class="kk iu">能够给一个给定的输入值</strong>分配一个概率。例如，如果我们定义我们的示例X=0.8，我们可以将它分配给边界为<strong class="kk iu"> <em class="mf"> 0.707 </em>到<em class="mf"> 1.153 </em> </strong>的bin。有<strong class="kk iu"> 1117 </strong>个值落入此区间，假设总共有<strong class="kk iu"> 12000个观察值</strong>，在该区间中看到值的概率约为1117 / 12000 = 0.093或<strong class="kk iu"> 9.3% </strong>。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="ed8f" class="nu lf it nq b be nv nw l nx ny">"bounds: [0.707, 1.153]" : {"counts" : 1117, "probability" : 0.093}</span></pre><p id="c17b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">累积分布函数(CDF)基于从PDF获得的知识。参考上面的例子，CDF表示<strong class="kk iu"> X ≤0.8 </strong>的概率。这意味着我们不仅要查看函数在<strong class="kk iu"> X=0.8 </strong>的精确值，还要查看到该点为止的所有值。</p><p id="5607" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了用简单的代码片段来表达这一点，我们可以直接从PDF获取CDF:</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="4e95" class="nu lf it nq b be nv nw l nx ny">np.cumsum( counts / np.sum(counts) ) # counts-&gt; no of values per bin</span></pre><p id="d83a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了查看从已知分布计算<strong class="kk iu"> PDF值的上下文中的一个示例，</strong>我可以参考下面的帖子(它使用超几何分布作为基础分布，并将其放在A/B测试的上下文中):</p><div class="oa ob gp gr oc od"><a rel="noopener follow" target="_blank" href="/fishers-exact-fb49432e55b5"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd iu gy z fp oi fr fs oj fu fw is bi translated">完全掌握费希尔的A/B检验的精确检验</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">虽然费希尔精确测试是A/B测试的一个方便工具，但测试的思想和结果往往很难…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">towardsdatascience.com</p></div></div><div class="om l"><div class="on l oo op oq om or mt od"/></div></div></a></div><p id="6a17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基本情况:正态分布</strong></p><p id="3b51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们先来看看标准正态分布。如您所见，CDF中的值范围是从0到1——从PDF的左侧开始移动，将所有值添加到右侧。考虑到CDF总是随着我们从左向右移动而累加值，我们可以确信CDF是严格单调递增的。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi os"><img src="../Images/e311f096ff1ef74a0bd11b3dc8367638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVpSZ9NARwdRiXinIPqKTQ.png"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">标准正态分布的直方图和CDF(模拟10，000个随机变量)</p></figure><blockquote class="mc md me"><p id="03fb" class="ki kj mf kk b kl km ju kn ko kp jx kq mg ks kt ku mh kw kx ky mi la lb lc ld im bi translated"><strong class="kk iu">为什么我们关心经验分布的CDF？</strong></p></blockquote><p id="4b06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个有限的范围对于进一步的考虑非常有价值，因为它允许我们在我们的经验CDF中的这个值上画出范围[0，1]中的任何随机均匀数<strong class="kk iu">点，并且<strong class="kk iu">将其“反向”映射到它在经验PDF </strong>中各自的X值——这个过程通常被称为逆变换定理(ITT)。</strong></p><h2 id="cca3" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">逆变换定理</h2><p id="d14c" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">正如刚才所描述的，ITT允许我们使用CDF来将聚合概率映射到其各自的X值(PDF的)。这意味着我们可以选择任何随机的统一值，在CDF的垂直线上找到它的点，并在水平轴上找到X的对应值——这个映射值对应于我们的PDF中感兴趣的变量。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="8e7b" class="nu lf it nq b be nv nw l nx ny">Uniform(0,1) -&gt; CDF (y-axis) -&gt; PDF (x-axis) # done</span></pre><p id="914e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的图表中，我们对两个不同的均匀随机变量进行了采样，并试图找到我们的基本分布的各自的X值。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b413cfdfb2a7c26550285137d08804f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*6_DGQ-cGQJbm7MhljvG98w.png"/></div></figure><p id="3367" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">红线对应U=0.8 </strong>，而<strong class="kk iu">金线对应U=0.15 </strong>(将y轴视为U的范围)。我们现在来看看CDF函数的截取。通过观察，我们已经可以假设红线的X值大约是0.9，而金线的X值似乎是-1左右。</p><p id="797e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了检查这一观察结果，我们可以将百分比值插入<strong class="kk iu">百分点函数(ppf) </strong>。这将为我们插入的总概率提供X的精确值。正如我们可以看到的，随机采样数据及其观察到的映射与标准正常CDF 的<strong class="kk iu">精确值非常一致:</strong></p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="70ce" class="nu lf it nq b be nv nw l nx ny">norm.ppf(0.8) = <strong class="nq iu">0.8416212</strong>, norm.ppf(0.15) = <strong class="nq iu">-1.0364334</strong></span></pre></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="e987" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了我们刚刚研究的，我们的<strong class="kk iu">主要兴趣</strong>是如何从<strong class="kk iu">经验分布</strong>中抽取随机数。为了回答这个问题，我们可以利用与ITT完全相同的想法。在前面的步骤中，我已经导出了数据的经验PDF。这些PDF值的累积和将产生<strong class="kk iu">经验CDF </strong>，如下所示:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/795ce836e20f97fdeae8ad4c00bc0bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*5U9lsGM7jQyGkTwE4ROxFQ.png"/></div></figure><p id="a0b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，经验CDF偏离了标准正态分布过程中所讨论的内容。然而，应用ITT来获得X值的想法也是适用的。</p><p id="f3d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在不使用外部包/库的情况下，可以查看CDF并识别累积概率小于或等于给定均匀随机变量(U)的索引:</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="160f" class="nu lf it nq b be nv nw l nx ny">idx=np.argmin(mycdf &lt;= 0.5) # provides the first index where "True"<br/>pdf[idx] # desired bin which is ≈ X</span></pre><p id="69d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用该索引，我们可以找到相应的感兴趣的容器，并将值U映射到该容器。这当然不是很准确，实际上需要在面元内进行插值，以使分布足够连续。</p><p id="29cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">找到对应于我们的统一U的X的更好和更准确的方法是使用<em class="mf"> statmodels的experimental _ distribution</em>。<em class="mf"> ECDF() </em>为您提供了x和y属性，使索引变得非常简单。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="b1c2" class="nu lf it nq b be nv nw l nx ny">from statsmodels.distributions.empirical_distribution import ECDF<br/>ecdf = ECDF( data )</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="6e4a" class="nu lf it nq b be nv nw l nx ny"># ecdf.x, ecdf.y</span></pre><p id="7212" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考我们之前用于标准正常CDF的例子，我们寻找对应于U=0.8的X值。</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/38762837a003b14b2970bd12fdb73cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*-sdKndv8BEjbuNuT79MNTA.png"/></div><p class="mv mw gj gh gi mx my bd b be z dk translated">U=0.8 ≈ X=1.73</p></figure><p id="39b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与U=0.8导致X约为0.84的标准正态分布的结果相反，经验分布指定了一个大得多的值X约为1.73。这是有意义的，因为我们<strong class="kk iu">知道经验分布有一个很大的右尾</strong>，或者更确切地说是在X=4 的值周围添加了一个“附加正态分布<strong class="kk iu">。</strong></p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h2 id="7ca9" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">从经验分布中提取随机变量</h2><p id="a810" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">为了结束这个话题，我想说明ITT的概念确实提供了<strong class="kk iu">一致和正确的结果</strong>。这个绘制U并找到其正确bin位置的迭代过程称为“<strong class="kk iu">自举。</strong>”</p><p id="8d9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我将采用经验CDF并抽取5000个独立的随机均匀分布(U)。对于每个U，我将找到CDF ≤ U为真的第一个索引，并且<strong class="kk iu">递增该索引位置的bin计数</strong>。最终，绝对出现次数将根据总值的数量进行归一化，从而获得PDF。</p><p id="40fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，对于这个总结步骤，我们只有经验CDF，我们使用bootstrapping来导出PDF，它应该看起来与我们开始时的经验PDF相似。</p><pre class="mk ml mm mn gt np nq nr bn ns nt bi"><span id="5827" class="nu lf it nq b be nv nw l nx ny">pdf = np.zeros_like(bins)</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="4494" class="nu lf it nq b be nv nw l nx ny">for step in np.arange(1000):<br/>    u = np.random.uniform()<br/>    p = np.argmin(mycdf &lt;= u) # p is the index<br/>    # Increment the counter at bin position of index<br/>    pdf[p-1]+=1</span></pre><pre class="nz np nq nr bn ns nt bi"><span id="843e" class="nu lf it nq b be nv nw l nx ny"># Calculating the PDF<br/>pdf = pdf / np.sum(pdf)</span></pre><p id="f22e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用经验CDF和1000个随机抽取的均匀随机变量，我们能够近似我们在这篇文章的第一步<a class="ae ni" href="#df69" rel="noopener ugc nofollow">中创建的经验PDF。这个总结性实验表明，应用ITT是从任意经验分布中生成<strong class="kk iu">随机变量的一种直接方式。</strong></a></p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pc"><img src="../Images/6b48a3122a4a4e9fb528046816e56eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdzyXw9wSRnE-Waaza3Svw.png"/></div></div></figure><h2 id="dcd1" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">结论</h2><p id="4be6" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">虽然统计和机器学习课程主要关注众所周知的概率分布的推导和应用，但使用经验分布似乎超出了范围。</p><p id="dde7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在现实世界中，尤其是在模拟领域，我们可能会观察到无法映射到标准、指数、均匀或其他分布的数据分布。对于这些情况，从观察到的经验数据分布中简单地提取随机变量是非常方便的。</p><p id="66fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你觉得这篇文章有趣，我会很感激“关注”🫀，直到那时:</p><p id="ecca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mf">照顾好自己，如果可以的话，也照顾好别人。</em></p><p id="ad67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mf"> —借用史蒂芬·都伯纳</em></p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="d2d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上所有图片均由作者创作。这篇文章的灵感来自于佐治亚理工学院教授戴夫·戈德曼杰出的IYSE 6644“模拟”课程。</p><p id="22a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">标题图片由Ruben Gutierrez在Unsplash上提供:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi pd"><img src="../Images/fd7cfcfade9f8f6ecf7d10bc843830a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D6Nk1GhEvu9MlISx"/></div></div><p class="mv mw gj gh gi mx my bd b be z dk translated">照片由<a class="ae ni" href="https://unsplash.com/@collectivecreatorsco?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">鲁本·古铁雷斯</a>在<a class="ae ni" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>