<html>
<head>
<title>Reward Is Not Enough for Risk-Averse Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对于规避风险的强化学习，奖励是不够的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reward-is-not-enough-for-risk-averse-reinforcement-learning-64133b63950b#2022-11-25">https://towardsdatascience.com/reward-is-not-enough-for-risk-averse-reinforcement-learning-64133b63950b#2022-11-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="476e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么我们不能通过合理设置奖励来解决风险敏感性问题呢？</h2></div><p id="d74a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">TL；博士</strong>:风险规避在许多 RL 应用中是必不可少的(例如，驾驶、机器人手术和金融)。一些修改后的 RL 框架考虑了风险(例如，通过优化回报的风险度量而不是其预期)，但提出了新的算法挑战。相反，人们通常建议坚持旧的良好的 RL 框架，只设定奖励，这样负面结果就会被放大。不幸的是，如下所述，使用对重新定义的回报的期望来模拟风险通常是不自然的、不切实际的，甚至在数学上是不可能的，因此不能取代风险度量的显式优化。这与决策理论的类似结果一致，在决策理论中，风险优化不等同于期望效用最大化。</p><p id="da5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">另见</strong>我的相关帖子关于<a class="ae lb" rel="noopener" target="_blank" href="/efficient-risk-averse-reinforcement-learning-29fbd0c4a906">如何实际做避险 RL</a>【neur IPS 2022】，以及<a class="ae lb" rel="noopener" target="_blank" href="/detecting-rewards-deterioration-in-reinforcement-learning-92338e8740d1">如何反正知道什么时候出问题</a>【ICML 2021】。</p><h1 id="48af" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">背景:为什么是风险厌恶型 RL？</h1><p id="d244" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">强化学习(RL)是机器学习的一个子领域，专注于顺序决策。在典型的设定中，一个<em class="lz">特工</em>被训练在<em class="lz">集</em>中操作；每一集都需要做出一系列的决定(<em class="lz">动作</em>)，每一个决定都会导致<em class="lz">奖励</em>，而<strong class="kh ir">累积的奖励形成了该集的结局(<em class="lz">回报</em> ) </strong>。主体的标准目标是找到<strong class="kh ir">最大化期望收益<em class="lz">E【R】</em></strong>的决策策略。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/2117cfaa2771fecfb5b6107727409c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/0*XOwc6YrG7Y_OajIV.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">强化学习图解(图片由<a class="ae lb" href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/" rel="noopener ugc nofollow" target="_blank"> L. Weng </a>提供)</p></figure><p id="4f2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然 RL 已经在各种<a class="ae lb" href="https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">游戏</a>和模拟中展示了令人印象深刻的结果，但它仍在努力为现实世界的应用服务。一个挑战是 RL 的许多自然应用(如驾驶、机器人手术和金融)的高风险敏感性:虽然允许游戏机器人偶尔出现故障，但自动驾驶汽车必须在任何情况下都能可靠地运行。对风险的敏感性也反映了许多应用中自然的用户偏好[ <a class="ae lb" href="https://www.jstor.org/stable/1914185" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，因为规避风险是一种常见和普遍的行为，被认为根植于人类进化[ <a class="ae lb" href="https://www.nature.com/articles/srep08242" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p><p id="2879" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">风险规避优化的必要性推动了对设置的研究，在设置中<strong class="kh ir">收益的风险度量被最大化，而不是其期望值</strong>。特别令人感兴趣的是<em class="lz">风险条件值</em>(<a class="ae lb" href="https://en.wikipedia.org/wiki/Expected_shortfall" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir"><em class="lz">CVaR</em></strong></a>，也称为<em class="lz">预期短缺</em>)，它衡量回报率分布中<em class="lz"> α </em>最低分位数(0 &lt; <em class="lz"> α &lt; 1 </em>)的平均值(与预期相反，它衡量整个分布的平均值)。它是一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Coherent_risk_measure" rel="noopener ugc nofollow" target="_blank">连贯的</a>风险度量(事实上，任何连贯的风险度量都可以写成 CVaR 度量的组合[ <a class="ae lb" href="https://www.sciencedirect.com/science/article/abs/pii/S037842660400144X" rel="noopener ugc nofollow" target="_blank"> 3 </a> ])。CVaR 广泛应用于金融风险管理[ <a class="ae lb" href="http://www.smartquant.com/references/VaR/var20.pdf" rel="noopener ugc nofollow" target="_blank"> 4 </a>、<a class="ae lb" href="https://link.springer.com/chapter/10.1007/978-1-4757-3150-7_15" rel="noopener ugc nofollow" target="_blank"> 5 </a>、<a class="ae lb" href="https://www.sciencedirect.com/science/article/abs/pii/S0378426602002650" rel="noopener ugc nofollow" target="_blank"> 6 </a>甚至<a class="ae lb" href="https://www.msci.com/expected-shortfall-story" rel="noopener ugc nofollow" target="_blank">银行业监管</a>、<a class="ae lb" href="https://www.msci.com/documents/10199/22aa9922-f874-4060-b77a-0f0e267a489b" rel="noopener ugc nofollow" target="_blank">8</a>；最近在医药、能源生产、供应链管理和调度方面也有所应用。CVaR 优化也在 RL [ <a class="ae lb" href="https://arxiv.org/abs/1404.3862" rel="noopener ugc nofollow" target="_blank"> 10 </a>，<a class="ae lb" href="https://arxiv.org/abs/1506.02188" rel="noopener ugc nofollow" target="_blank"> 11 </a>，<a class="ae lb" href="https://arxiv.org/abs/2205.05138" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]的框架下进行了研究，例如针对驾驶[ <a class="ae lb" href="https://arxiv.org/abs/1911.03618" rel="noopener ugc nofollow" target="_blank"> 13 </a> ]和机器人[ <a class="ae lb" href="https://arxiv.org/abs/1910.02787" rel="noopener ugc nofollow" target="_blank"> 14 </a>。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/23af0a1879f5e8e13e7d6b5687fd200e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yfBHmvVOdQQlOjmD.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">CVaR 图解:分布尾部的平均值(图片由作者提供)</p></figure><h1 id="a98c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">但是 CVaR 优化更难！我们不能减少到标准 RL 吗？</h1><p id="a7f2" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">优化像 CVaR 这样的风险衡量标准是一个巨大的挑战。在政策梯度方法中，关注最糟糕的情景会导致代理人忽略大部分一般数据(<em class="lz">样本无效率</em>)和特定的成功策略(<em class="lz">对成功的盲目</em> ) [ <a class="ae lb" href="https://arxiv.org/abs/2205.05138" rel="noopener ugc nofollow" target="_blank"> 12 </a>)。在基于价值的方法中，对风险度量的天真规划会导致不一致的风险规避(即，通过在每个时间步合理地规避风险，我们最终会在整个事件中过于保守)。</p><p id="0fb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，如果我们希望优化 CVaR，我们应该怎么做呢？虽然某些解决方案解决了上述限制，但是一些解决方案建议简单地绕过复杂性:代替</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b5ae924da9c9d2c38db80dbd817e441c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/0*2u52gQ_QB_z1WSDq.png"/></div></figure><p id="9683" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">建议<strong class="kh ir">简单地用一些放大负面结果的<em class="lz"> r' </em>代替奖励<em class="lz"> r </em>，解决标准 RL 问题 wrt <em class="lz"> r' </em> </strong>:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5968742d2dd132827e217a620deb1fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/0*eh0Aa7ayGpFM67dX.png"/></div></figure><p id="a3a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，当训练一个汽车司机时，不要优化 CVaR——只给事故分配非常负面的奖励！这遵循了<em class="lz">奖励足够了</em> [ <a class="ae lb" href="https://www.sciencedirect.com/science/article/pii/S0004370221000862" rel="noopener ugc nofollow" target="_blank"> 15 </a> ]的更一般方法的精神，并且可能被认为既简化了算法又反映了我们的“真实”效用函数。在决策理论中，这种方法被称为<em class="lz">期望效用最大化</em><a class="ae lb" href="https://www.researchgate.net/publication/5119917_Comparative_Analyses_of_Expected_Shortfall_and_Value-at-Risk_2_Expected_Utility_Maximization_and_Tail_Risk" rel="noopener ugc nofollow" target="_blank">16</a>。</p><h1 id="d005" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">奖励是不够的</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mt"><img src="../Images/4cd9dd3930364eee2d481d7670128907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g33xPXDqSk99ZfSsgK3T8w.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">作者图片</p></figure><p id="cd00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在不同的应用中，平均回报和 CVaR 回报都是合理的目标。如上所述，CVaR 优化在许多风险敏感问题中有很强的动机。在这种情况下，<strong class="kh ir">不幸的是，通过替换奖励来将 CVaR 优化简化为均值问题通常是不可行的</strong>。这个结果在决策理论中是众所周知的，其中期望效用最大化不同于 CVaR 优化(除非有一个策略严格支配所有其他策略)[ <a class="ae lb" href="https://www.researchgate.net/publication/5119917_Comparative_Analyses_of_Expected_Shortfall_and_Value-at-Risk_2_Expected_Utility_Maximization_and_Tail_Risk" rel="noopener ugc nofollow" target="_blank"> 16: Section IV </a> ]。我们在 RL 的上下文中讨论这种差异，其中时间结构形成了一个额外的因素。具体来说，我们提出以下主张:</p><ol class=""><li id="40b0" class="mu mv iq kh b ki kj kl km ko mw ks mx kw my la mz na nb nc bi translated">很难量化我们“真正的”效用函数。</li><li id="9947" class="mu mv iq kh b ki nd kl ne ko nf ks ng kw nh la mz na nb nc bi translated">量化效用函数目标是不够的:我们的目标是根据<em class="lz">回报</em>定义的，而简化为均值优化要求我们重新定义<em class="lz">回报</em>。从回报到回报的效用转换通常是不切实际的，甚至在数学上是不可能的(见上面 MDP 的例子)。</li><li id="617d" class="mu mv iq kh b ki nd kl ne ko nf ks ng kw nh la mz na nb nc bi translated"><strong class="kh ir">即使我们能够设计出忠实反映我们目标的<em class="lz">奖励</em>，它们的优化仍然会受到与风险度量优化类似的限制。</strong></li></ol><p id="3bc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了方便起见，下面我们以不同的顺序(1，3，2)讨论索赔。</p><h2 id="5081" class="ni ld iq bd le nj nk dn li nl nm dp lm ko nn no lo ks np nq lq kw nr ns ls nt bi translated">(1)“真正的”奖励是未知的</h2><p id="aba6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">设计正确反映我们风险厌恶的奖励是一项艰巨的任务。考虑一下驾驶问题:为了防止一起车祸，你愿意在路上多花多少分钟？注意，答案不是无限的，否则汽车就会灭绝。“正确”的答案是未知的，通过猜测，我们可能会鼓励不受欢迎的行为——要么太大胆，要么太保守。</p><p id="b237" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，CVaR 优化也需要我们量化自己的风险厌恶水平(<em class="lz"> α </em>)。然而，这种选择可以说是更加直观和普遍的:而不是“一次事故和一分钟的交通流量之间有多少个数量级？”，我们有“我们应该根据<em class="lz"> α=10% </em>最坏情况，还是<em class="lz"> α=1% </em>最坏情况来做决策？”。</p><p id="4c58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，我们的实际驾驶实践可以说更类似于 CVaR 优化:在我们做出道路决策之前，没有我们隐含分配给事故和浪费时间的内部价值。相反，我们想象结果并据此行动。如果我们厌恶风险，我们会想象更糟糕的结果(例如，害怕另一条车道的车突然向我们的车道移动)，从而导致更安全的行为。这正是 CVaR 优化的工作方式:代理学习在最差结果的条件下行动。</p><h2 id="3fb8" class="ni ld iq bd le nj nk dn li nl nm dp lm ko nn no lo ks np nq lq kw nr ns ls nt bi translated">(3)重新设计的奖励可能只是将问题转移到其他地方</h2><p id="56f1" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">因此，低于事故成本会容忍太多的事故，而高于事故成本可能会导致我们呆在家里。但是，如果我们神奇地知道一次事故的“正确”成本，即在路上浪费的时间，会怎么样呢？由于事故本来就很少发生且代价高昂，安全驾驶的天真优化仍然会受到有效回报稀少和样本低效的影响。因此，即使我们这次没有将问题公式化为 CVaR 优化，来自风险厌恶 RL 的方法可能仍然是令人感兴趣的(例如，在[ <a class="ae lb" href="https://arxiv.org/abs/2205.05138" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]中对某些类型的事件进行过采样)。</p><h2 id="cda9" class="ni ld iq bd le nj nk dn li nl nm dp lm ko nn no lo ks np nq lq kw nr ns ls nt bi translated">(2)规避风险的回报<em class="nu">≦</em>规避风险的回报</h2><p id="c08c" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">考虑一个金融投资组合，回报是年利润，回报是日利润。想想你的厌恶风险的朋友，他讨厌亏损:每年的亏损让他付出的心理健康代价是年利润的三倍。与其纠结于风险度量，为什么我们不能简单地用平凡的效用函数转换结果:<em class="lz"> U(利润)=利润如果利润≥0，否则 3 *利润</em>？</p><p id="d649" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了回答这个问题，我们应该明确我们是将这个结果转换应用于回报还是奖励。如果我们转换回报，我们不再遵循预期累积回报的标准目标，事实上我们优化了回报的风险度量。如果我们转换奖励，那么我们就变成了一个标准的 RL 问题，但它不再考虑我们的实际目标，因为效用是根据年利润定义的，而奖励是根据日来定义的。今天亏了 10 美元，明天赚了 20 美元，仍然为年利润贡献<em class="lz"> -10+20=+10 美元</em>——而不是<em class="lz"> -3*10+20=-10 美元</em>！也就是说，<strong class="kh ir">对回报的风险厌恶不会直接转化为回报，而一个天真的转化通常会导致过度保守的政策</strong>(例如，每天避免损失<em class="lz"/>)<strong class="kh ir">。</strong></p><p id="2881" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的例子中，还不清楚如何定义一个回报函数来考虑风险规避。期望的回报函数应该取决于环境内的结构(例如，几天内的相关性)。从这个意义上说，我们需要提前解决问题，这样我们就可以定义奖励，让我们再次解决问题…</p><p id="8d2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，即使这种不切实际的解决方案也不总是可行的。</p><p id="64c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">索赔</strong>:存在一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank"> MDP </a> <em class="lz"> (S，A，P，r，H) </em>，使得对于任何重新定义的报酬函数<em class="lz">r’</em>，CVaR 最优策略 wrt 到<em class="lz"> r </em>不同于均值最优策略 wrt<em class="lz">r’</em>。</p><p id="7ef8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">证明</strong>:技术证明在<a class="ae lb" href="https://docs.google.com/document/d/1ahbHz964-FUA2Uef_a7niYyOIYz0Csa8h-rg1wy1BeM/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">单独文件</a>中提供。上图中的 MDP 抓住了主要思想。</p><p id="d7bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">备注</strong>:如果状态除了奖励还可以修改，那么在某些情况下，CVaR <em class="lz">可以</em>化简为一个扩展状态空间上的优化[ <a class="ae lb" href="https://arxiv.org/abs/1506.02188" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]。自然，这带来了新的困难，特别是增加了样本的复杂性，从而限制了可伸缩性。替代方法通常可以优化 CVaR，而不会显著增加样本复杂性[ <a class="ae lb" href="https://arxiv.org/abs/2205.05138" rel="noopener ugc nofollow" target="_blank"> 12 </a> ]。这可以被视为类似于非马尔可夫决策过程:这样的问题可以在数学上简化为 MDP(通过扩展状态以包括历史)；然而，实际方法使用专用算法(例如，通过向代理添加存储单元)来解决这些问题。</p><h2 id="d0ae" class="ni ld iq bd le nj nk dn li nl nm dp lm ko nn no lo ks np nq lq kw nr ns ls nt bi translated">当* <strong class="ak"> <em class="nu">我们可以* </em> </strong>化简为均值优化吗？</h2><p id="ea0f" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">根据上面的讨论，我们可以标记 CVaR 优化可以安全地简化为标准 RL 问题的某些情况。首先，当然，我们必须相信我们的效用函数——我们赋予各种结果的价值。其次，高风险回报和高风险回报之间的不一致必须可以忽略不计。例如，如果极端负回报主要是由单个极端负回报(如车祸)而不是一系列负回报(如财务损失)引起的，那么这是正确的。或者，在小规模问题中，我们可以通过扩展状态空间[ <a class="ae lb" href="https://arxiv.org/abs/1506.02188" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]来应用缩减。</p><p id="c5bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，虽然 CVaR 优化非常有用，但并不是每个看似风险敏感的问题实际上都遵循 CVaR 目标。例如，如果奖励确实是可加的，并且可以可靠地量化，并且 i.i.d .测试集的数量足够大，那么预期回报就是一个合理的优化目标(这要归功于集的中心极限定理)。</p><h1 id="1666" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="eba1" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">以符合标准算法的方式编写问题是应用机器学习领域中的一个关键步骤，因为它允许我们利用该领域中的伟大算法。然而，在某些情况下，问题不能适应，算法必须改为修改。在本文中，我们解释了为什么风险厌恶 RL 是这些情况之一。幸运的是，最近的工作提出了 RL [ <a class="ae lb" href="https://arxiv.org/abs/2205.05138" rel="noopener ugc nofollow" target="_blank"> 12 </a>，<a class="ae lb" href="https://arxiv.org/abs/1911.03618" rel="noopener ugc nofollow" target="_blank"> 13 </a>，<a class="ae lb" href="https://arxiv.org/abs/1910.02787" rel="noopener ugc nofollow" target="_blank"> 14 </a>，<a class="ae lb" href="http://proceedings.mlr.press/v80/dabney18a.html" rel="noopener ugc nofollow" target="_blank"> 17 </a>中风险度量优化的有效算法。</p><p id="490b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我要感谢阏氏·曼诺尔、尤里·加多和伊莱·梅洛姆对本文的反馈和有益建议。</p><h2 id="171d" class="ni ld iq bd le nj nk dn li nl nm dp lm ko nn no lo ks np nq lq kw nr ns ls nt bi translated">参考</h2><p id="cbf7" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">[1] <em class="lz">前景理论:风险下的决策分析</em>，卡内曼和特沃斯基，<em class="lz">计量经济学 1979 </em></p><p id="7418" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <em class="lz">风险敏感性作为一种进化适应</em>，辛慈、奥尔森、阿达米和赫特维格，<em class="lz">自然 2015 </em></p><p id="7ce8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <em class="lz">关于预期缺口作为连贯风险度量的意义</em>，Inui 和 Kijima，<em class="lz">银行杂志&amp;金融 2005 </em></p><p id="146e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] <em class="lz">条件风险值的优化</em>，Rockafellar 和 Uryasev，<em class="lz">风险杂志 2000 </em></p><p id="06f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] <em class="lz">关于在险价值和条件在险价值的几点注记</em>，Pflug，<em class="lz"> 2000 </em></p><p id="c009" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6] <em class="lz">风险值与非独立信用风险投资组合的预期短缺:概念和实践见解</em>，Frey 和 McNeil，<em class="lz">银行杂志&amp;金融 2002 </em></p><p id="216b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[7] <em class="lz">预期亏空及超出</em>，Tasche，<em class="lz">银行学杂志&amp;金融 2002 </em></p><p id="62bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8] <em class="lz">回测预期缺口</em>，亚齐德和策克利，<em class="lz">2014 年风险</em></p><p id="c1fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[9] <em class="lz">金融之外的条件风险价值:一项调查</em>，菲利皮、瓜斯塔罗巴和斯佩兰萨，<em class="lz">运筹学国际交易 2019 </em></p><p id="4756" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[10] <em class="lz">通过采样优化 CVaR</em>，Tamar，Glassner 和 Mannor，<em class="lz"> AAAI 2015 </em></p><p id="673b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[11] <em class="lz">风险敏感和稳健决策:CVaR 优化方法</em>，Chow，Tamar，Mannor 和 Pavone，<em class="lz"> NIPS 2015 </em></p><p id="dcb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[12] <em class="lz">高效的风险厌恶强化学习</em>，格林伯格，周，加瓦姆扎德和曼诺尔，<em class="lz"> NeurIPS 2022 </em></p><p id="d439" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[13] <em class="lz">最坏情况政策梯度</em>，唐，张，Salakhutdinov，<em class="lz"> CoRL 2019 </em></p><p id="2f03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[14] <em class="lz">基于视觉的机器人抓取的分位数 QT-Opt</em>，博德纳尔等人，<em class="lz">机器人科学与系统 2020 </em></p><p id="0f0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【15】<em class="lz">奖励够了</em>，西尔弗，辛格，普雷科普和萨顿，<em class="lz">人工智能 2021 </em></p><p id="f5e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[16] <em class="lz">预期缺口与风险价值的比较分析(2):预期效用最大化与尾部风险</em>，Yamai and Yoshiba，<em class="lz">货币与经济研究 2002 </em></p><p id="c811" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[17] <em class="lz">分布强化的隐式分位数网络 learning‏ </em>，达布尼，奥斯特罗夫斯基，西尔弗和穆诺斯，<em class="lz"> ICML 2018 </em></p></div></div>    
</body>
</html>