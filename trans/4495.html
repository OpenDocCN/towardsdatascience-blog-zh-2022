<html>
<head>
<title>Applied Reinforcement Learning I: Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用强化学习I:Q-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applied-reinforcement-learning-i-q-learning-d6086c1f437#2022-10-05">https://towardsdatascience.com/applied-reinforcement-learning-i-q-learning-d6086c1f437#2022-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1f65" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">逐步理解Q学习算法，以及任何基于RL的系统的主要组件</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3670f21681b1e0fd708e5653105e0c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tTDbj7htk5Pc2ZoC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@deepmind?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="90f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">我们</span>都经历过这样的情况，我们做了错事，为此受到惩罚，这让我们不会再犯。同样的，很多时候好的行为会得到回报，鼓励我们在更多的场合重复这些行为。遵循这种并行性，强化学习代理将遵循策略/政策采取某些行动，并根据所采取的行动是否有益来接收正面或负面的反馈。这个奖励然后被用来更新策略，整个过程被重复，直到达到一个最优策略，如图<em class="me">图1 </em>所示。</p><p id="9c7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在心理学中，有一种学习程序叫做<em class="me">工具性或操作性条件反射</em><strong class="lb iu">【1】</strong>，它是基于给定行为发生的概率取决于预期的后果。这个学科的主要目标是增加或减少一个行为被重复的概率，这与强化学习的方法完全相同！</p><blockquote class="mf mg mh"><p id="bb46" class="kz la me lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">强化学习代理的目标是通过代理与动态环境的连续交互，优化所采取的行动，以获得尽可能高的回报。</p></blockquote><p id="4cd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，可以看出，强化学习的主要前提是如何基于生物学习过程的，以及这些过程如何在整个人类历史中被证明是有效的，这使其成为机器学习最令人兴奋和有趣的分支之一。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/e33836de957c61e47c88fb74a5e842ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*nhV0RGi3PHW2Rhqq8uoycQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。强化学习流程。作者图片</p></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h2 id="78bf" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">无模型和基于模型的RL算法</h2><p id="a43d" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">在继续解释和实现Q学习算法之前，重要的是要注意RL算法分为两大类:<strong class="lb iu">基于模型的</strong>算法和<strong class="lb iu">无模型的</strong>算法。</p><p id="75c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前者旨在通过与环境的交互来学习环境模型，这样代理就能够在采取给定行动之前预测该行动的回报(通过拥有环境模型，它可以预见每个行动之后会发生什么)，从而允许进行行动规划。另一方面，无模型算法必须采取一个行动，以便观察其后果，然后从中学习(见<em class="me">图2 </em>)。需要注意的是，‘<em class="me">模型’</em>这个词并不是指机器学习模型，而是指环境本身的一个模型。为了更深入地解释这两个群体之间的差异，看看这篇文章<strong class="lb iu">【2】</strong>。</p><p id="93fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如后面将要看到的，Q-Learning是一种无模型算法，因为它的学习包括采取行动、接受奖励以及从采取这些行动的结果中学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2fada302fc4c73bd7ae68bb0f51963ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*IaFFm7ToF7Sr5Dn4BGa6bA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。作者图片</p></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="df03" class="ns mu it bd mv nt nu nv my nw nx ny nb jz nz ka ne kc oa kd nh kf ob kg nk oc bi translated">q学习</h1><p id="05d9" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Q学习算法利用包含状态-动作对的<strong class="lb iu"> Q表</strong> (2D矩阵)，使得表/矩阵中的每个值<strong class="lb iu"> Q(S，A) </strong>对应于在状态<strong class="lb iu"> A </strong>中采取动作<strong class="lb iu"> S </strong>的<strong class="lb iu"> Q值</strong>估计值(Q值将在后面介绍)。当代理与环境交互时，Q表的Q值将收敛到它们的最优值，直到找到最优策略。</p><p id="ed4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一开始理解所有这些术语很复杂，出现许多问题是正常的:什么是Q值？Q表是如何构造的？Q值是如何更新的？</p><p id="ffbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解算法流程所需的所有概念，以及上面的问题，都在下面解释。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h2 id="896e" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">q表结构</h2><p id="5f9e" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">如前所述，Q表是一个矩阵，其中每个元素对应一个状态-动作对。因此，Q表将是一个<strong class="lb iu"> m </strong> x <strong class="lb iu"> n </strong>矩阵，其中<strong class="lb iu"> m </strong>是可能状态的数量，而<strong class="lb iu"> n </strong>是可能动作的数量。Q表的Q值必须有一个初始值，因此该表将被初始化，所有值都被设置为零。</p><p id="dadf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="me">举例:</em> </strong></p><p id="a1b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简单起见，环境将是一个有4种可能状态(A、B、C、D)的房间，如图所示。此外，代理将能够执行4个可能的动作:向上、向下、向左和向右。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7318f2d236498ac5d7a695d44ac1c77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*IR_NTQS6PdDEaY6CHfBG2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例环境。作者图片</p></figure><p id="2acd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到上面提到的代理和环境，Q表将是4x4矩阵，4行对应于4种可能的状态，4列对应于4种可能的动作。从下面可以看出，所有的值都被初始化为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/0e09c1337f60f2003301e428b16e07e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*NlslsvUg6ic7OOqIx_XIIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">初始化Q表。作者图片</p></figure></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h2 id="40a2" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">q值</h2><p id="a711" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">一旦Q表被初始化，代理可以开始与环境交互，并更新Q值以实现最优策略。但是，Q值是如何更新的？</p><p id="d7d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，重要的是介绍一下<strong class="lb iu">值函数的概念，</strong>更早的出现在<em class="me">图2 </em>中。价值函数是对代理处于给定状态或给定状态-动作对的益处的度量。</p><p id="82e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">价值函数有两种:<strong class="lb iu">状态-价值函数</strong>，<strong class="lb iu"> v(S) </strong>，决定在遵循某一政策的同时处于某一特定状态的利益；以及<strong class="lb iu">行动-价值函数</strong>，<strong class="lb iu"> q(S，A) </strong>，其确定在遵循某一政策的同时从特定状态采取特定行动的收益。更具体地说，这些函数返回从一个状态(对于状态-值函数)或一个状态-动作对(对于动作-值函数)开始并遵循给定策略的预期收益。</p><p id="787f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面显示了这两种功能，以及一篇深入解释它们的重要性和存在理由的文章的链接。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/309040b51b5cd04cb084588247fc9f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeC3A2Zp626imvS6GYCiEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">期望回报、状态价值和行动价值函数的方程式。作者图片</p></figure><div class="og oh gp gr oi oj"><a href="https://www.analyticsvidhya.com/blog/2021/02/understanding-the-bellman-optimality-equation-in-reinforcement-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">强化学习中的贝尔曼最优方程</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">这篇文章作为数据科学博客的一部分发表。在本文中，首先，我们将讨论一些…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ks oj"/></div></div></a></div><p id="0f4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">动作值函数的结果称为Q值，如前所述，它是构成Q表的每个像元。因此，Q表为我们提供了从某个状态采取某个动作的预期好处，代理将使用该信息在环境中最优地移动。因此，代理的目标将是找到<strong class="lb iu">最优动作值函数</strong>，<strong class="lb iu"> q*(S，A) </strong>，(最优Q值)，使得它遵循任何策略从任何状态-动作对返回最高可能的回报。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h2 id="e16b" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">q值更新</h2><p id="e20a" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">最优动作值函数q*(S，A)的一个性质是它满足贝尔曼最优性方程，如下所示。知道了这一点，贝尔曼方程就可以用来迭代地推断出最优的行动值函数，这是代理人的主要目标。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/d29c578919fd9a3ec04dec79c91788c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*_a96YtXDnA2wGuGwn3jIOg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝尔曼最优方程。作者图片</p></figure><p id="f7d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Q学习的情况下，图3 中<em class="me">所示的贝尔曼最优方程的修改用于迭代更新Q表的Q值。该等式用于在每次迭代中将当前Q值与最佳值进行比较，以减少误差，并寻求使两者相等。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/e3640ebeb4960e989a6200cbd543cf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*ExizJWewz2nqLqFjVc822g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q值更新方程。作者图片</p></figure><p id="6505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，Q值更新方程利用了一个被称为<strong class="lb iu">学习率</strong>的<strong class="lb iu"> α </strong>参数，该参数表示新Q值在每次更新/迭代中的权重。学习率为0将导致代理从不更新其动作值函数，而学习率为1将导致仅考虑新学习的Q值。通过反复试验找到该参数的理想值是常见的。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h2 id="249b" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">算法流程</h2><p id="2b87" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">现在已经解释了算法的所有组件和步骤，是时候将它们放在一起并让代理学习了。下面是算法的伪代码，它将在Q-Learning的实现过程中用作参考。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/f67cd6b0d941bdcc951d1955a0eb1c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*kXzYZEf95OrRv0NvjlQphA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q-学习伪代码。摘自萨顿和巴尔托:《强化学习:<br/>导论<strong class="bd pb">【3】</strong></p></figure><h2 id="de73" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">1.初始化Q表</h2><p id="1e10" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">如前所述，用依赖于可能的状态和动作的数量的形状来初始化Q表，并且将其所有值设置为零。</p><h2 id="af93" class="mt mu it bd mv mw mx dn my mz na dp nb li nc nd ne lm nf ng nh lq ni nj nk nl bi translated">2.<strong class="ak">开始第一集</strong></h2><p id="222c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">每集都将运行，直到代理达到终端/目标状态。代理从随机状态开始剧集，对于剧集中的每个时间步长，它将:</p><p id="26ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1) </strong>根据策略采取行动(该算法最常用的是<strong class="lb iu"> ɛ-greedy策略)。</strong></p><blockquote class="mf mg mh"><p id="6f7f" class="kz la me lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">ɛ-greedy策略试图在随机探索环境以学习和通过根据q值选择最佳行动来利用环境之间取得平衡。政策对环境的勘探或开发的倾向由ε<strong class="lb iu">ɛ</strong>参数给出，允许政策从勘探行为开始，并动态地将其改变为开发行为。有关此政策的更多信息，请参见<strong class="lb iu">【4】</strong>。</p></blockquote><p id="c961" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2) </strong>根据前面提到的Q值更新公式，根据达到的新状态和获得的奖励计算新的Q值。</p><p id="befc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从达到的新状态开始下一个时间步。</p><p id="6736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有剧集完成后，训练将会结束。在这一点上，Q表的Q值将是最优的(只要训练有效)，这意味着如果代理人选择具有最高Q值的行动A，他将在每个状态S中获得最大回报。</p><p id="9e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，为了在非训练环境中使用受过训练的代理，只需要使它在每个时间步中选择具有最高Q值的动作，因为Q表已经在训练期间被优化。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="8a9f" class="ns mu it bd mv nt nu nv my nw nx ny nb jz nz ka ne kc oa kd nh kf ob kg nk oc bi translated">密码</h1><p id="e363" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">为了便于理解和学习，Q-Learning算法的完整实现，以及受过训练的代理的可视化和评估，可以在我的GitHub存储库中找到，作为一个Jupyter笔记本。</p><div class="og oh gp gr oi oj"><a href="https://github.com/JavierMtz5/ArtificialIntelligence" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">GitHub-Javier mtz 5/人工智能</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">github.com</p></div></div><div class="os l"><div class="pc l ou ov ow os ox ks oj"/></div></div></a></div><p id="26ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我不得不提到，这篇文章仅涉及算法的理论和介绍性概念，并不关注它在代码中的实现。这是因为这个月我将发表另一篇文章，专门关注该算法的实现及其在一个已知的OpenAI健身房环境中的实际应用。如果你有兴趣，请跟我来，这样你就不会错过了！</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><h1 id="2a3b" class="ns mu it bd mv nt nu nv my nw nx ny nb jz nz ka ne kc oa kd nh kf ob kg nk oc bi translated">参考</h1><p id="cf00" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><strong class="lb iu">【1】</strong>施塔顿，约翰·尔；丹尼尔·t·塞鲁蒂《操作性条件反射》。<em class="me">《心理学年刊》</em>，2003年，第54卷，第115页</p><p id="16d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【2】</strong>基于模型和无模型的强化学习<a class="ae ky" href="https://neptune.ai/blog/model-based-and-model-free-reinforcement-learning-pytennis-case-study" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/Model-Based-and-Model-Free-Reinforcement-Learning-py tennis-case-study</a></p><p id="63a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【3】</strong>萨顿，理查德·s；强化学习:介绍。麻省理工学院出版社，2018</p><p id="4bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【4】</strong>Epsilon-Greedy算法在强化学习中的应用<a class="ae ky" href="https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/Epsilon-Greedy-Algorithm-in-Reinforcement-Learning/</a></p></div></div>    
</body>
</html>