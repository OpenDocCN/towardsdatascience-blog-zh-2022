<html>
<head>
<title>Where Do Loss Functions Come From?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失函数从何而来？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/where-do-loss-functions-come-from-c93bc34c0bc5#2022-06-07">https://towardsdatascience.com/where-do-loss-functions-come-from-c93bc34c0bc5#2022-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="771d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用极大似然估计量推导线性回归损失函数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/34bfd6ede8892c839dc4923b16c62835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qlNBVVsRnyLdiDmg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰斯温·托马斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="bff6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="0645" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们都知道，在线性回归中，我们的目标是最小化误差平方和(SSE)。然而，为什么是上交所，这种说法是从哪里来的？</p><p id="9c67" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我希望用最大似然估计来回答这个问题。</p><h1 id="4782" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最大似然估计器</h1><p id="55f2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在数据科学社区呆上足够长的时间，我相信你会遇到术语<strong class="lt iu">最大似然估计器(MLE)。</strong></p><p id="0733" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我不打算给出MLE的详细分析，主要是因为它已经以不同的方式做了很多次，可能比我能解释的还要好。此处链接的是我所见过的Josh Starmer的StatQuest的最佳解释，它真是太棒了。</p><p id="27f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，为了完整起见，我将给出一个简短的描述。MLE是一种计算最佳参数的方法，这些参数最好地描述了给定的数据或统计模型。</p><p id="9ba0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可以这样想，给定我们观察到的数据，什么参数最适合这个数据。或者，什么参数最有可能给出这种数据分布。</p><p id="69d3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">例如，假设我们有一些数据<strong class="lt iu"> <em class="ms"> X </em> </strong>，我们想要找到可能生成这些数据的参数<strong class="lt iu"> <em class="ms"> θ </em> </strong>的最佳值。我们可以根据<strong class="lt iu"> <em class="ms"> θ: </em> </strong>的值，通过计算数据点个体概率的乘积来计算“可能性”</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/8a51abdfeadb118077f81a0b78a9841b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlZV_ToGHf1VtpeY873CpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="4261" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> L(θ | X) </em> </strong>是这个值的可能性<strong class="lt iu"> <em class="ms"> θ </em> </strong>产<strong class="lt iu"> <em class="ms"> </em> </strong>我们的观测数据<strong class="lt iu"><em class="ms">X</em></strong><strong class="lt iu"><em class="ms"/></strong><strong class="lt iu"><em class="ms">f(X _ n |θ)</em></strong>是概率<a class="ae ky" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="noopener ugc nofollow" target="_blank">(概率密度函数)</a>T49】</p><p id="d41d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所以，我们想找到最大化<strong class="lt iu"><em class="ms">【L(θ| X)</em></strong>的<strong class="lt iu"> <em class="ms"> θ </em> </strong>的值，因此它是最大似然估计量！</p><h1 id="8d73" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简单线性回归</h1><p id="56d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们以最基本的线性模型，简单的线性回归为例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/310423194aaec02cca65719385389dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*Dm3ymxHWiu6rUuI6u3uIEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="a62f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> y_i </em> </strong>为给定观测值的响应<strong class="lt iu"> <em class="ms"> x_i </em> </strong>，<strong class="lt iu"> <em class="ms"> β_0 </em> </strong>和<strong class="lt iu"> <em class="ms"> β_1 </em> </strong>为我们想要拟合的参数，<strong class="lt iu"> ϵ </strong>为正态分布误差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ad04480ad683f198d5a2d42af348431a.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*2XwKOYJ4BEFsnYf1qpr4gg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="0e63" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> σ </strong>为方差。</p><p id="0838" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个等式可以用下面的方式重写:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b17bcd2f4280ba9c5b9aff19ee143cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*vNuf2ThG_V5h125wXB7tmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="0d0c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> y </em> </strong>对<strong class="lt iu"> <em class="ms"> x </em> </strong>的条件分布假设为正态分布。</p><blockquote class="mx my mz"><p id="e2e3" class="lr ls ms lt b lu mn ju lw lx mo jx lz na mp mc md nb mq mg mh nc mr mk ml mm im bi translated">请记住，假设条件分布是正态分布，而不是响应的边际分布。<a class="ae ky" href="https://stats.stackexchange.com/questions/327427/how-is-y-normally-distributed-in-linear-regression" rel="noopener ugc nofollow" target="_blank">这里链接的一个很棒的线程解释了这个概念。</a></p></blockquote><h1 id="9c40" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">应用MLE</h1><p id="e555" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">单个响应和可观测值的概率密度函数(PDF)是以我们的预测为平均值的<a class="ae ky" href="https://www.investopedia.com/terms/n/normaldistribution.asp" rel="noopener ugc nofollow" target="_blank">正态分布</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/50718f52ccb92fcc98eeb22dc0d5c816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*rNa5XmJ4sMBXaOCTJMPAeQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="fa87" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用分号来区分参数和随机变量。</p><p id="7d3d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我们的可能性函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/e54be26199d651b3f9db71cb0be6c0a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUIRl9Pwe_kubZElVMD7iQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="ef5f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一般来说，人们更喜欢使用对数似然法，因为在数学上/计算上使用总和比乘积更方便:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/5b5d13f4aeceee29a4c0faf9ebe3e04f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-FTnLhkMoPMmgDvygrFMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="4dbd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正态分布PDF中的副标题:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b83637622709a322b76a9edf8cd06fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*BAjanjgIscyEr6BW0Ya2kQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="9db1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这等于:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/7f2a2ec0c43f5eb9a4369ece011b481b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4BkrShgk-snGDsg4Lba-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="6730" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你注意到上学期有什么熟悉的东西吗？</p><blockquote class="mx my mz"><p id="3fe2" class="lr ls ms lt b lu mn ju lw lx mo jx lz na mp mc md nb mq mg mh nc mr mk ml mm im bi translated">对于感兴趣的读者来说，这里有一个更完整的推导，链接到这些表达的出处。</p></blockquote><h1 id="eced" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">机器学习中的MLE</h1><p id="9724" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在在机器学习中我们只对<strong class="lt iu"> <em class="ms"> β_0 </em> </strong>和<strong class="lt iu"> <em class="ms"> β_1的参数感兴趣。</em> </strong>因此，对于机器学习中的简单线性回归模型，我们希望最大化的对数似然是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/152e56ededc48d307649cbb65d6d0fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*k4O6O2boqF4Mk4e4ta1Rtg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="376f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于我们通常认为它是最小化损失函数，因此我们取对数似然的负值，并找到最小化该函数的参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/7908caea4252b624a07453b09b863f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYDBbZFKkBTODrTrzQvy0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><blockquote class="mx my mz"><p id="81e8" class="lr ls ms lt b lu mn ju lw lx mo jx lz na mp mc md nb mq mg mh nc mr mk ml mm im bi translated">最小化负对数似然性与最大化对数似然性是一样的。<a class="ae ky" href="https://stats.stackexchange.com/questions/141087/why-do-we-minimize-the-negative-likelihood-if-it-is-equivalent-to-maximization-o" rel="noopener ugc nofollow" target="_blank">这里链接的线程很好地解释了这一点。</a></p></blockquote><p id="154e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，<strong class="lt iu"/><a class="ae ky" href="https://www.statology.org/constant-variance-assumption/" rel="noopener ugc nofollow" target="_blank">在假设的线性回归中被说成是常数。</a> <strong class="lt iu"> </strong>考虑到这一点<strong class="lt iu">，</strong>对于简单的线性回归，我们需要最小化以下表达式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4a01f6da3a185b0766062391d1c88390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*u8x1S0KKEdV_LVjHeU42Xw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中创建的方程。</p></figure><p id="ceef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">也就是我们都知道的经典<strong class="lt iu">平方和</strong>！</p><h1 id="1fde" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="72f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我们已经展示了如何使用线性回归的最大似然估计来导出平方和。这就是为什么它被用作我们机器学习问题的损失函数。</p><p id="f994" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以对二项式分布进行类似的推导，并得出用于逻辑回归的对数损失！</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="f035" class="kz la it bd lb lc ns le lf lg nt li lj jz nu ka ll kc nv kd ln kf nw kg lp lq bi translated">和我联系！</h1><ul class=""><li id="ca40" class="nx ny it lt b lu lv lx ly ma nz me oa mi ob mm oc od oe of bi translated">要在媒体上阅读无限的故事，请务必在此注册！  <em class="ms"> </em>💜</li><li id="9786" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="/subscribe/@egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="ms">在我发布注册邮件通知时获得更新！</em> </a> <em class="ms"> </em>😀</li><li id="b8dc" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://www.linkedin.com/in/egor-howell-092a721b3/" rel="noopener ugc nofollow" target="_blank"><em class="ms">LinkedIn</em></a><em class="ms"/>👔</li><li id="a7a0" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://twitter.com/EgorHowell" rel="noopener ugc nofollow" target="_blank"> <em class="ms">推特</em> </a> <em class="ms"> </em> 🖊</li><li id="fb18" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://github.com/egorhowell" rel="noopener ugc nofollow" target="_blank"><em class="ms">github</em></a><em class="ms"/>🖥</li><li id="87b0" class="nx ny it lt b lu og lx oh ma oi me oj mi ok mm oc od oe of bi translated"><a class="ae ky" href="https://www.kaggle.com/egorphysics" rel="noopener ugc nofollow" target="_blank"><em class="ms"/></a><em class="ms"/>🏅</li></ul><blockquote class="mx my mz"><p id="72f1" class="lr ls ms lt b lu mn ju lw lx mo jx lz na mp mc md nb mq mg mh nc mr mk ml mm im bi translated">(所有表情符号都是由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>设计的——开源的表情符号和图标项目。许可证:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></blockquote></div></div>    
</body>
</html>