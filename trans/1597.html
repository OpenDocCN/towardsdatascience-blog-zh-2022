<html>
<head>
<title>Linear and Logistic Regressions as Degenerate Neural Networks in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras中作为退化神经网络的线性和逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-and-logistic-regressions-as-degenerate-neural-networks-in-keras-c9b309cbd80a#2022-04-17">https://towardsdatascience.com/linear-and-logistic-regressions-as-degenerate-neural-networks-in-keras-c9b309cbd80a#2022-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5556" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络是线性回归和逻辑回归的超集。使用Keras快速有效地建立回归模型，并根据需要在它们和神经网络之间切换。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e251d6af152ce5f0fe0e33c19440bbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FXfacxxYz9-RrhSajqEww.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片基于Ashkan Forouzani在Unsplash上拍摄的照片</p></figure><p id="bade" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您的任务是为某个度量创建预测，您可能想知道简单的线性或多元回归是否足够(或者，如果我们想要预测二进制值，逻辑回归)，或者可能使用神经网络。以及在尝试不同的模型时需要多少编码。好消息是，高级神经网络框架Keras足以满足所有这些目的，我将用一个简单的例子来说明。</p><p id="1c95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> Aaron Zhu对这里使用的回归模型的概述</a>是一个很好的开始，所以这里我们只提一下基础知识。</p><h1 id="c429" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">线性回归</h1><p id="c3ae" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">简单线性回归的目标是使用两个常数将一个结果(Y[i])(一个连续变量)建模为连续输入变量(X[i])的线性函数:</p><p id="56e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">𝛃X[i] + 𝛂 = Y[i]*</p><p id="229a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测中的误差(Y[i]* — Y[i])通常使用误差平方和来测量，因为最小化该误差恰好最大化𝛂和𝛃提供给定x和y的观测数据的基础模型的可能性</p><p id="ef70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，从X到Y*的映射实际上与神经网络中全连接(或密集)层提供的映射相同。这个密集层非常简单:它有一个输入和一个输出；𝛃是与单个输入相关的权重，𝛂是偏差。</p><p id="3435" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然简单的线性回归可以直接求解，也就是说，𝛂和𝛃的值可以使用一些代数来找到，但如果我们决定使用梯度下降来优化它们的值，我们将得到一个非常简单的退化神经网络，具有单个密集层和平方误差损失函数。这两者在喀拉斯都很容易买到。我称之为神经网络退化，因为它没有激活功能，所以它甚至没有实际的感知器。</p><p id="081b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，为什么要在这里停下来呢？如果我们有多个输入变量，X1，X2，… Xn，我们可以考虑多元回归，其中单个结果(Y)使用一个因子向量进行预测:</p><p id="f5f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">𝛃1x1[i]+𝛃2x2[i]+…+𝛃nxn[i]+𝛂= y[I]*</p><p id="d3dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">获得𝛃n和𝛂的直接代数方法是解线性方程组。另一种选择是，使用上述梯度下降法，可以证明一般来说资源不太密集，这又产生了退化的神经网络:这一次是具有n个输入和单个输出的密集层，其中𝛃1、𝛃2、…、𝛃n是权重，𝛂是偏差。出于与简单线性回归相同的原因，误差函数的选择是误差平方和。</p><h1 id="fa22" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">逻辑回归</h1><p id="7b73" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">虽然有时我们需要预测的不是一个连续的值，而是一个真或假的值，这就是逻辑回归的作用。这是一个线性回归模型，其结果被输入逻辑函数𝝈(x) = 1/(1+exp(-x))，这是一个将所有实数映射到0–1区间的sigmoid函数。这使得逻辑回归的输出可以解释为一种概率:如果我们试图预测一个人会买红色还是绿色的气球，这可能是他们选择红色气球的概率。</p><p id="901c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">𝝈(𝛃1x1[i]+𝛃2x2[i]+…+𝛃nxn[i]+𝛂)= y[I]* = prob(person[I]购买红色气球)</p><p id="46ca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，通常的误差平方和损失不会最大化𝛃1、𝛃2、…、𝛃n、𝛂直接成为潜在模型的可能性。我们需要使用的正确的损失函数叫做二进制交叉熵。(这一点的推导见朱的文章。)逻辑函数增加了直接找到最优解的复杂性，因此，与多元回归类似，梯度下降可以提供更快且资源更少的替代方案。</p><p id="0f3a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">毫不奇怪，使用梯度下降，优化逻辑回归相当于训练一个简单的神经网络，使用所有相关软件包中可用的组件。实际上，逻辑回归表示单层感知器，在Keras中，它可以被建模为具有sigmoid激活的密集层。使用二进制交叉熵损失函数来训练这个模型正好给出了我们想要的。</p><h1 id="6170" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">一个例子</h1><p id="1901" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">也就是说，不需要编写单独的代码或调用单独的库来尝试线性和逻辑回归，或者成熟的神经网络。简单地改变神经网络模型允许我们快速且容易地尝试所有三种类型的模型。</p><p id="15bc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了演示这一点，我们创建了一个玩具示例。给定一个人的年龄、关系状况和孩子数量，我们试图预测他们买了多少气球，以及这些气球是红色还是绿色。我们将关系状态表示为单个值，对于合作伙伴为-0.5，对于单身为+0.5。</p><p id="c4fb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的玩具示例中，我们选择了一个不完全是线性的基础模型，来看看全神经网络是否比线性模型更好。我们使用统一的随机值生成输入。我们从[-0.5，+0.5]的区间开始这样做，因此输入已经被归一化:具有零均值和均匀方差。然后我们计算:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="832d" class="mu lt iq mq b gy mv mw l mx my">if relationship == -0.5:<br/>    number_of_balloons = 1. * children - .2 * age<br/>    balloon_color = 1 if (.8 * children + .2 * age &gt; 0) else 0 <br/>else:<br/>    number_of_balloons = .8 * children + .5 * age<br/>    balloon_color = 1 if (.5 * children + .5 * age &gt; 0) else 0</span></pre><p id="d055" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们创建了四个模型:一个多元线性回归和一个末端没有sigmoid函数的神经网络来预测气球的数量；这里我们用的是损失平方和。然后是逻辑回归模型和具有最终sigmoid函数的神经网络来预测气球的颜色；这里我们使用二进制交叉熵作为损失。</p><p id="1c9c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有关训练这些模型的代码，请参见此要点，对于回归模型，还会显示密集层的权重(对应于𝛃和𝛂):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mz na l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">四种模式的实施和培训</p></figure><p id="1125" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个示例运行产生了以下输出—为便于阅读，此处对其进行了细微的修改:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="a702" class="mu lt iq mq b gy mv mw l mx my">======= Output type: num_balloons Model type: regression<br/>Epoch 1/1000 loss: 0.0526 - val_loss: 0.0205<br/>Epoch 2/1000 loss: 0.0205 - val_loss: 0.0201<br/>Epoch 3/1000 loss: 0.0204 - val_loss: 0.0207 </span><span id="a145" class="mu lt iq mq b gy nb mw l mx my">Weights: <br/>[&lt;'dense/kernel:0' ([[-0.00324172], [0.43457505], [0.1423042]])&gt;,<br/>&lt;'dense/bias:0' ([0.00032589])&gt;] </span><span id="a9cb" class="mu lt iq mq b gy nb mw l mx my">======= Output type: num_balloons Model type: neural <br/>Epoch 1/1000 loss: 0.0206 - val_loss: 1.1117e-04 <br/>Epoch 2/1000 loss: 1.2853e-04 - val_loss: 1.1776e-04 </span><span id="15b2" class="mu lt iq mq b gy nb mw l mx my">======= Output type: color Model type: regression <br/>Epoch 1/1000 loss: 0.5130 - val_loss: 0.2387 <br/>Epoch 2/1000 loss: 0.2213 - val_loss: 0.2005 <br/>Epoch 3/1000 loss: 0.2009 - val_loss: 0.1935 <br/>Epoch 4/1000 loss: 0.1965 - val_loss: 0.1969 </span><span id="89b7" class="mu lt iq mq b gy nb mw l mx my">Weights: <br/>[&lt;'dense/kernel:0' ([[0.02176554], [14.170614], [8.668548]])&gt;,<br/>&lt;'dense/bias:0' ([0.02175274])&gt;] </span><span id="be1c" class="mu lt iq mq b gy nb mw l mx my">======= Output type: color Model type: neural <br/>Epoch 1/1000 loss: 0.4589 - val_loss: 0.0651 <br/>Epoch 2/1000 loss: 0.0518 - val_loss: 0.0345 <br/>Epoch 3/1000 loss: 0.0271 - val_loss: 0.0199 <br/>Epoch 4/1000 loss: 0.0161 - val_loss: 0.0147 <br/>Epoch 5/1000 loss: 0.0119 - val_loss: 0.0098 <br/>Epoch 6/1000 loss: 0.0104 - val_loss: 0.0098</span></pre><p id="cd76" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个观察结果是神经模型在两种情况下都比回归结果好(0.001178验证损失对0.0207；0.0098损失对0.1969)。正如所料，他们可以模拟非线性关系。</p><p id="c994" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回归返回的权重值得进行更多的分析和健全性检查。对于气球的数量，多元回归预测</p><p id="0d7c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">-0.003关系+ 0.435儿童数量+ 0.142年龄+ 0.000 =气球数量</p><p id="2c2c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为孩子的数量和年龄的平均值都为零，所以对于两种关系状态，基础模型返回的气球数量的平均值也为零。这反映在关系输入的因子和偏差实际上都为零。</p><p id="8181" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基础模型为两种关系状态生成相同数量的示例，因此我们期望近似其组合的最佳线性模型位于基础模型中两个线性表达式的中间。事实上，我们发现:孩子数量的因子接近(1.0 + 0.8) / 2 = 0.45，年龄的因子接近(0.5–0.2)/2 = 0.15。</p><p id="c759" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于气球的颜色，逻辑回归给出了模型</p><p id="fce5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">𝝈(0.0218关系+14.171 num _儿童+ 8.669年龄+ 0.0218) = prob(红色)</p><p id="35b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样的对称性适用于以前，我们再次发现，关系和偏差的因素接近于零，特别是如果与其他两个数字相比。我们再次期望线性模型介于两个线性表达式之间。对于我们期望的孩子数(0.8 + 0.5) / 2 = 0.65，对于年龄(0.2 + 0.5) / 2 = 0.35。然而，由于sigmoid函数，该模型只对输出是负还是正感兴趣，因此存在任意的比例因子；考虑到这一点，我们看到这些因素确实有意义:14.171 / 8.669 = 1.635，接近0.65 / 0.35 = 1.857。</p><p id="c2b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事实上，比例因子不是任意的:它们是大数字的事实使得sigmoid函数的输入绝对值很大，这迫使其输出非常接近零或一。</p><h1 id="e92f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="243d" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们已经看到了神经网络如何成为线性和逻辑回归的超集，以及如何使用现有的软件组件来构建神经网络，我们可以非常容易地实现回归模型。以这种方式实现回归模型可以很容易地在必要时将它们升级为神经网络。</p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="5423" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nj">图片基于Ashkan Forouzani在Unsplash上的照片，</em><a class="ae lr" href="https://unsplash.com/photos/_Y82jqFIBgw" rel="noopener ugc nofollow" target="_blank"><em class="nj"/></a></p></div><div class="ab cl nc nd hu ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="ij ik il im in"><p id="3c7b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nj">原载于</em> <a class="ae lr" href="https://www.furnitureferret.com/blog/2022/05/19/linear-and-logistic-regressions-as-degenerate-neural-networks-in-keras/" rel="noopener ugc nofollow" target="_blank"> <em class="nj">家具雪貂博客</em> </a></p></div></div>    
</body>
</html>