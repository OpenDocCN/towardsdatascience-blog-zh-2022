<html>
<head>
<title>AdaBoost, Step-by-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AdaBoost，循序渐进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaboost-in-7-simple-steps-a89dc41ec4#2022-08-03">https://towardsdatascience.com/adaboost-in-7-simple-steps-a89dc41ec4#2022-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a353" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">升压和AdaBoost简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6ffca8a95043afbe8a1a9629c7f19fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoxIDKx-fgK-XI0xFb1F_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AdaBoost—作者图片</p></figure><p id="0a84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AdaBoost属于集成学习方法，并模仿“群体智慧”的原则:单独表现不佳的模型在组合时可以形成强大的模型。</p><p id="0660" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2021年发表的一项麻省理工学院的研究描述了人群如何识别假新闻。没有背景知识或事实核查，个人往往很难可靠地识别假新闻。然而，根据我们的经验，我们通常至少能够给出一个趋势，这通常比随机猜测要好。如果我们想知道一个给定的标题描述的是事实还是包含假新闻，我们可以简单地随机问100个人。如果超过50个说头条含有假新闻，我们就归类为假新闻。</p><p id="29e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">几个单个弱学习器的预测组合起来可以产生一个强学习器，该强学习器能够以很高的准确度区分真伪。</p><p id="7995" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lv">用集成学习，我们模仿这个概念</em> </strong></p><p id="f025" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lv"> Boosting </em> </strong>是最流行的集成学习技术之一。建立一组所谓的弱学习器，即性能略好于随机猜测的模型。单个弱学习器的输出被组合为加权和，并代表增强分类器的最终输出。AdaBoost代表“自适应增压”。适应性，因为模型是一个接一个地建立的，并且前面模型的性能影响后面模型的模型建立过程。</p><p id="972d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在学习过程中，AdaBoost算法还会为每个弱学习者分配一个权重。因此，不是每个弱学习者对集成模型的预测都有相同的影响。这个计算整体模型预测的过程称为<strong class="la iu"> <em class="lv">软表决</em> </strong>。另一方面，如果每个弱学习者的结果被同等地加权，我们会说<strong class="la iu"> <em class="lv">硬投票</em> </strong>。[Kum21]</p><p id="2987" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">仅次于<strong class="la iu"> <em class="lv">装袋</em> </strong>，<strong class="la iu"> <em class="lv">助推</em> </strong>是最广为人知的合奏方法。</p><ul class=""><li id="4b6f" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">在<strong class="la iu"> <em class="lv">装袋中，</em> </strong>我们训练一组相互独立的个体模型。各个模型彼此不同，因为它们是用训练数据集的不同随机子集训练的。随机森林就是基于这个原理。一组单独的决策树形成集合模型的预测。</li><li id="662b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">另一方面，在<strong class="la iu"> <em class="lv">增强</em> </strong>中的训练是连续的。各个模型的建模过程一个接一个地进行，由此模型预测的准确性影响后续模型的训练过程。AdaBoost算法是如何做到这一点的，将在本文中逐步解释。模型由弱学习器、深度为1的简单决策树，即所谓的“决策树桩”来表示。</li></ul><p id="0d99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文旨在逐步解释AdaBoost算法背后的概念。为此，我搜索了一个简单的分类数据集，找到了“成人”数据集。<a class="ae lu" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">【KOH 96】</a></p><h1 id="8363" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">使用的数据集</h1><p id="4d1a" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">“成人”数据集(也称为“人口普查收入”数据集)用于二元分类任务。该数据集包含描述生活在美国的人的数据，如性别、年龄、婚姻状况和教育程度等属性。目标变量区分年收入低于和高于50，000美元。</p><p id="68bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了说明AdaBoost算法是如何工作的，我简化了数据集，只使用了其中的一小部分。我已经将代码片段打包到Jupyter笔记本中描述的步骤中。如果您想自己遵循这些步骤，可以随意克隆repo或使用文本中的代码片段。</p><p id="aceb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://github.com/polzerdo55862/Ada-Boost-Tutorial" rel="noopener ugc nofollow" target="_blank">https://github.com/polzerdo55862/Ada-Boost-Tutorial</a></p><h1 id="2c3f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">加载数据集</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="7d03" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">准备数据集</h1><p id="8b42" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我直接从UCI加载数据集，并准备3个简单的二进制特征:</p><ul class=""><li id="f31f" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">男性(是或否)</li><li id="5737" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">每周超过40小时(是或否)</li><li id="03a7" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">50岁以上(是或否)</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="8e4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下文中，我将使用这个简单的数据集来解释AdaBoost算法的工作原理。</p><h1 id="3d39" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">1.建造第一个WeakLearner:找到性能最好的树桩</h1><p id="9695" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">第一步，找一个WeakLearner，可以对目标变量做一个陈述(&gt; 50k收入)，至少比随机猜测略胜一筹。</p><p id="6c26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AdaBoost可以与几种机器学习算法结合使用。在这种情况下，我们选择决策树作为弱学习器，这是AdaBoost算法最流行的应用。</p><p id="7d42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">只是作为一个简短的复习:</em></p><p id="994d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树在所谓的<strong class="la iu"> <em class="lv">节点</em> </strong>逐步分割整个数据集。树中的第一个节点叫做<strong class="la iu"> <em class="lv">根节点</em> </strong>，所有跟随<strong class="la iu"> <em class="lv">决策的节点</em> </strong>。不再发生数据集分割的节点被称为<strong class="la iu"> <em class="lv">终端节点</em> </strong>或<strong class="la iu"> <em class="lv">叶节点</em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/1516aee93f625a3d7d86005f473a1dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uKb6ZwZUMqXWRwcMDaGjAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树—作者图片</p></figure><p id="f08a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在这里描述了用于回归任务的决策树和随机森林的工作原理:</p><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">7种最常用的回归算法以及如何选择正确的算法</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">线性和多项式回归、RANSAC、决策树、随机森林、高斯过程和支持向量回归</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ks nn"/></div></div></a></div><p id="999a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您对如何使用决策树执行异常检测任务感兴趣，您可以在此处找到对<strong class="la iu"> <em class="lv">隔离林</em> </strong>的介绍:</p><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/a-comprehensive-beginners-guide-to-the-diverse-field-of-anomaly-detection-8c818d153995"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">异常检测多样化领域的初学者综合指南</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">隔离森林，局部异常因子，一类SVM，自动编码器，稳健协方差估计和时间序列…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob ks nn"/></div></div></a></div><p id="8041" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们试图找到在预测输出变量方面表现最佳的第一个决策树桩。为此，我们遵循与学习决策树相同的过程。第一步，我们确定数据集中最有可能区分高于和低于50k收入的特征。</p><p id="ca2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了有某种度量来评估不同的特征，我们使用了基尼系数。我们想找到叶子杂质最低的节点。</p><p id="d369" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从随机选择的特征开始；这里的特征是“男性”。属性只区分这个人是不是男人。根节点将整个数据集分割成一个子集，其中仅包含男性和所有其他人的实例。</p><p id="8962" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果一个子集(例如D1)包含k个不同的类，那么一条记录属于I类的概率可以描述为p_i. [Kar22]下图中，我描述了如何计算左边终端节点的<strong class="la iu"> <em class="lv">基尼杂质</em> </strong>。</p><p id="681e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在下面使用的简单数据集只包含10个实例，6个实例描述男性，4个描述女性。如果我们查看包含6个“男性”数据样本的子集D1，我们会看到6个人中有4个人的收入在5万英镑或以下。只有2张唱片的收入超过50k。</p><p id="10e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，子集D_1的随机样本显示收入高于50k的概率<strong class="la iu">是2/6或<strong class="la iu"> 1/3 </strong>，样本显示收入低于50k的概率是4/6或<strong class="la iu"> 2/3 </strong>。因此，叶1(子集D_1)基尼系数为0.444 </strong>。</p><p id="43a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们对叶子2做同样的事情，我们得到的杂质为<strong class="la iu"> 0.375 </strong>。</p><p id="6a3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于我们想要比较根节点“雄树”、“40小时以上”和“50年以上”的树桩的性能/基尼指数，我们首先计算根节点“雄树”的加权基尼系数，作为单个树叶的加权和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/a99b944a994262aa401d734649e7a940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMLncNS21IqOA0rbEu2_yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算基尼指数-作者图片</p></figure><p id="dfde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在下面的Python片段中绘制了基尼指数的计算，它只是简单地迭代数据框的所有列，并执行上述基尼指数计算:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8bf41cdeb0558d6b4a04c6c7d457bf7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*2l3hiaJzl1_jJUxw8kVtMQ.png"/></div></figure><p id="9821" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目标是最小化叶子的基尼杂质，从而最大化基尼增益，基尼增益被计算为两个分裂子集的加权杂质和整个数据集的杂质之间的差。</p><p id="9947" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">工作时间特征为" &gt; 40小时"的树桩显示出最高的基尼系数，因此被用于构建第一个树桩。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b9f94471de5372e9b09883304daab098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgGpN2bcJCJEtyHBi3QBwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者计算残肢图像的误差</p></figure><p id="2ed9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AdaBoost现在正在继续按顺序建造树桩。AdaBoost的特别之处在于，第一个树桩产生的误差会影响下一个树桩的建模过程。第一个树桩错误分类的实例将在下一个树中被赋予更大的权重。单个残肢的结果在集合模型中的权重取决于残肢的误差有多高。</p><h1 id="1e36" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">2.计算残肢的误差</h1><p id="7563" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在上图中，我已经画出了正确和错误分类的实例数量。据此，在为训练数据集的记录预测目标变量时，stump仅产生一个错误。数据集中只有一个例子，每周工作时间超过40小时，但每年收入不到5万英镑。数据集中的所有其他实例都已经用这个简单的<em class="lv">决策树桩</em>正确分类了。</p><p id="80c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于误差值的计算，我们为数据集的每个记录引入权重。在第一个树桩形成之前，每个实例的<em class="lv">权重w </em>为<em class="lv"> w=1/n </em>，其中n对应数据集的大小。因此，所有权重的总和为1。</p><p id="a1a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，决策残肢产生的误差被简单地计算为残肢错误分类目标变量的所有样本权重的总和。由于所选的决策树桩仅错误分类了一个实例，<strong class="la iu">第一次运行的错误是1/10。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/43433e9c2391ff2024746ec4b5e99851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wFaaZEReuZV3pJxqYSinWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算加权误差-作者图片</p></figure><p id="b9c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">函数“<em class="lv">calculate _ error _ for _ choosed _ stump</em>”计算我选择的特征作为根节点的树桩的加权误差(<em class="lv">selected _ root _ node _ attribute</em>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="9807" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">3.计算树桩的权重，即“话语权”</h1><p id="b6e8" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">正如已经提到的，集合模型的结果是所有树桩预测的加权和。因此，在下文中，我们感兴趣的是刚刚构建的树的预测对于最终的集合模型有多重要。或者换句话说，say (alpha)  的<strong class="la iu"> <em class="lv">量有多高。</em></strong></p><p id="0184" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们计算出<strong class="la iu"><em class="lv">α</em></strong>的金额如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/438aa0f4b0582f5ab1f3c639c095fe2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JO5JoTgJDAK74cjexBfRpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算作者的say-Image数量</p></figure><p id="9eef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图左边部分显示了<strong class="la iu"><em class="lv">say</em></strong>(alpha)量与残肢加权误差的关系。对于小误差<strong class="la iu"> <em class="lv">量大的说</em> </strong>。对于较大的误差，α甚至可以取负值。对于0.1的第一个残端的误差，<strong class="la iu"><em class="lv">alpha/量说</em> </strong>大约是<strong class="la iu"> 1.1 </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="7587" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">4.样品重量的调整</h1><p id="68f0" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">由于我们希望在对第二个树桩建模时考虑第一个树桩的结果，所以我们根据第一个树桩的误差来调整样本权重。第一个树桩预测错误的数据集记录应该在下一个树桩的构建过程中发挥更大的作用。为此，我们首先调整单个样本的权重。</p><p id="9bad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用<strong class="la iu"> <em class="lv"> alpha </em> </strong>计算样品的新重量，如下所示:</p><ol class=""><li id="a331" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt oi mc md me bi translated">左图显示了正确分类样本的缩放比例。</li><li id="9577" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">右图显示了错误分类样本的缩放比例</li><li id="7246" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">然后，新的样本权重被归一化</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/50a1c2e8f8ba6ee550e149fb8fa7fbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAUuJHtyHw3Yl5EbKfrU2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算样品重量—图片由作者提供</p></figure><p id="2276" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">计算并绘制新的重量标度:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="2d6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用刚刚定义的函数更新样品重量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="652a" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">5.第二次运行:形成新的引导数据集</h1><p id="5f16" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">对于第二次运行，我可以简单地为每个属性构建一个树桩，并选择加权误差最小的树桩。</p><p id="7d75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者，我们可以使用样本权重来生成一个新的数据集，其中权重较大的样本在统计上更常见。为此，我使用样本权重将范围0–1划分为多个区间。</p><p id="bfeb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在图像中看到描述为“累计_总和_下限”和“累计_总和_上限”的箱范围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/0f877825473bd87583e824cba30d0019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2xtb6MjR3ha-BtQzEMV6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">定义范围-作者提供的图像</p></figure><p id="8afb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我在0和1之间选择一个随机数，并寻找该数所在的范围/bin。我复制已识别的记录，并将其添加到新的数据集中。</p><p id="cbc9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我重复这个过程N次，直到我得到一个新的数据集，它是我的原始数据集的两倍长。</p><p id="8d55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于权重较大的样本在权重范围(从0到1)中占据较大的范围，因此权重较大的样本通常会在新数据集中出现多次。这正是您在下图的数据集中看到的内容。被第一个树桩错误分类的样本权重更大(这里权重为0.5)，最终更频繁地出现在新数据集中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/1162e8af45e869b896972da9c69eb1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ETtOAeirtHdF41ENl4F3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成新数据集-作者提供的图像</p></figure><p id="7cff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于新数据集，我们重复第一步:</p><ol class=""><li id="dd4d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt oi mc md me bi translated">计算所有特征的基尼系数，并选择该特征作为显示最大基尼系数的第二个树桩的根节点</li><li id="9f6f" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">建造第二个树桩</li><li id="3bfb" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">将加权误差计算为错误分类样本的样本权重之和</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="ac05" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">找到第二个树桩的根节点:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="db1f" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">6.重复该过程，直到达到终止条件</h1><p id="830e" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">该算法现在重复刚刚描述的步骤，直到达到某个终止条件，例如直到所有特征已经被用作根节点一次。</p><p id="b3d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">总结</strong></p><p id="6096" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于1中的t…t:</p><ol class=""><li id="9923" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt oi mc md me bi translated">找到最大化基尼增益(或者最小化错误分类实例的误差)的弱学习器h_t(x)</li><li id="dc85" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">将弱学习器的加权误差计算为误分类样本的样本权重之和。</li><li id="069a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">将分类器添加到集成模型中。模型的结果是个别弱学习者的结果的总结。使用上面确定的“说的量”/加权错误率α对弱学习者进行加权。</li><li id="32e4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">更新权重:在加权和误差的帮助下，我们将α计算为刚形成的弱学习者的“发言权数量”。</li><li id="b624" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">我们使用这个alpha来重新调整样本权重。</li><li id="4cd5" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">新的样本权重被归一化，使得权重之和再次为1</li><li id="c790" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt oi mc md me bi translated">使用新的权重，我们通过在0和1之间选择N次随机数来生成新的数据集，然后将数据样本添加到代表该数据集的新数据集。</li></ol><h1 id="18e3" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">7.将弱学习者组合成一个集成模型</h1><p id="fdd8" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">集合模型计算预测如下:</p><ul class=""><li id="256d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">对每个树桩做预测</strong>。</li></ul><p id="8d3a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">举个简单的例子，一个30岁的人每周工作42小时，第一个树桩的结果是收入高于50k，第二个树桩的结果是收入低于50k。</p><ul class=""><li id="7f93" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">将<em class="lv">的量累加到</em>的每一个输出值</strong>。</li></ul><p id="8a6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，我们的简单集合模型只包含两个树桩。我们将收入分类在50k以上的树桩和收入分类在50k以下的树桩的权重相加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/60343048f4c419d1a99f0c9916eaae81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEKcmNapRdDGFze4IouzIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">建立集合模型——作者的图像</p></figure><p id="3790" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们简单的比较一下权重之和。因为在我们的例子中，第一个树桩的重量占优势，所以集合模型的预测是收入50k。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/152ae0336a1d55865794d50c208c2268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*kPNA1B4o9dULl1xv3CG7FQ.png"/></div></figure><h1 id="e852" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">摘要</h1><p id="8bcc" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">近年来，Boosting方法在许多数据竞赛中表现出了优异的结果，然而它们背后的概念却非常简单。简单易懂的步骤构建简单可解释的模型。只有简单模型的组合才能产生强大的学习者。</p><p id="080d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://dmnkplzr.medium.com/membership" rel="noopener">如果您还不是中级高级会员，并且想要成为会员，您可以通过使用此推荐链接注册来支持我。</a></p><p id="9de5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><h1 id="db63" class="mk ml it bd mm mn ou mp mq mr ov mt mu jz ow ka mw kc ox kd my kf oy kg na nb bi translated">参考</h1><p id="0dd7" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">彼得·迪兹克斯:麻省理工学院新闻，2021年。<a class="ae lu" href="https://news.mit.edu/2021/crowd-source-fact-checking-0901" rel="noopener ugc nofollow" target="_blank">https://news.mit.edu/2021/crowd-source-fact-checking-0901</a></p><p id="fe3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[Kar22] <a class="ae lu" href="https://www.learndatasci.com/author/FatihKarabiber" rel="noopener ugc nofollow" target="_blank"> Fatih Karabiber </a>:基尼杂质，2022。https://www.learndatasci.com/glossary/gini-impurity/<a class="ae lu" href="https://www.learndatasci.com/glossary/gini-impurity/" rel="noopener ugc nofollow" target="_blank"/></p><p id="3486" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[Kum21] Ajitesh Kumar:硬投票与软投票分类器Python示例，2021。<a class="ae lu" href="https://vitalflux.com/hard-vs-soft-voting-classifier-python-example/" rel="noopener ugc nofollow" target="_blank">https://vital flux . com/hard-vs-soft-voting-classifier-python-example/</a></p><p id="b608" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[Koh96] Kohavi，Ronny和Becker，Barry:成人数据集，1996年。<a class="ae lu" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/adult</a>(CC乘4.0)</p><p id="8d38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[Wiq20] Wiqaas:推进决策树桩，2020年。<a class="ae lu" href="https://github.com/wiqaaas/youtube/blob/master/Machine_Learning_from_Scratch/AdaBoosting/AdaBoosting_Decision_Tree.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/wiq AAS/YouTube/blob/master/Machine _ Learning _ from _ Scratch/ada boosting/ada boosting _ Decision _ tree . ipynb</a></p></div></div>    
</body>
</html>