<html>
<head>
<title>What is a Tensor in Machine Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是机器学习中的张量？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507#2022-03-29">https://towardsdatascience.com/what-is-a-tensor-in-deep-learning-6dedd95d6507#2022-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9d1d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">张量、数组和矩阵的区别</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/843152310a361dcb12a8c933104c3ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyiDSD4C40VA0nJnuMcUYg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3915" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">张量到底是什么？</p><p id="f391" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">大多数深度学习实践者都知道它们，但无法精确指出一个<strong class="kx ir">精确的定义</strong>。</p><p id="9673" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">TensorFlow，PyTorch:每个深度学习框架都依赖于同一个基本对象:<strong class="kx ir"> tensors </strong>。它们被用来存储深度学习中的几乎所有东西:输入数据、权重、偏见、预测等。</p><p id="8282" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，他们的定义非常模糊:仅<a class="ae lr" href="https://en.wikipedia.org/wiki/Category:Tensors" rel="noopener ugc nofollow" target="_blank">维基百科类别</a>就有<strong class="kx ir">超过 100 页</strong>与张量相关。</p><p id="4241" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将对以下问题给出一个明确的答案:什么是神经网络中的张量？</p><h1 id="9a44" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">💻计算机科学中的张量</h1><p id="75de" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">那么为什么会有这么多定义呢？</p><p id="4b63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">很简单:不同的字段有不同的定义。<strong class="kx ir">数学</strong>中的张量与<strong class="kx ir">物理</strong>中的张量不太一样，与<strong class="kx ir">计算机科学</strong>中的张量不同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/1f62d70f55bfd0bbb59a524f6abc915f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igbaqgEGXxGulR-KgPPeKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="315a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些定义可以分为两类:作为数据结构的张量或作为对象的张量(在<a class="ae lr" href="https://en.wikipedia.org/wiki/Object-oriented_programming" rel="noopener ugc nofollow" target="_blank">面向对象编程</a>的意义上)。</p><ul class=""><li id="06c0" class="mq mr iq kx b ky kz lb lc le ms li mt lm mu lq mv mw mx my bi translated"><strong class="kx ir">数据结构</strong>:这是我们在计算机科学中使用的定义。张量是存储特定类型值的多维数组。</li><li id="3a66" class="mq mr iq kx b ky mz lb na le nb li nc lm nd lq mv mw mx my bi translated"><strong class="kx ir">对象</strong>:这是其他领域使用的定义。在<a class="ae lr" href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)" rel="noopener ugc nofollow" target="_blank">数学</a>和<a class="ae lr" href="https://en.wikipedia.org/wiki/Infinitesimal_strain_theory" rel="noopener ugc nofollow" target="_blank">物理</a>中，张量不仅仅是一个数据结构:它们也有一个属性列表，就像一个特定的产品。</li></ul><p id="7a70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是为什么你看到很多人(有时相当学究气地)说“<em class="ne">张量是</em> <strong class="kx ir"> <em class="ne">不是</em> </strong> <em class="ne"> n 维数组/矩阵</em>”:他们说的不是数据结构，而是具有属性的<strong class="kx ir">对象。</strong></p><p id="ec5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">甚至同样的单词也有<strong class="kx ir">不同的意思</strong>。例如，在计算机科学中，2D 张量是一个矩阵(它是一个秩为 2 的张量)。在线性代数中，二维张量意味着它只存储两个值。秩也有一个完全不同的定义:它是其线性无关的列(或行)向量的最大数量。</p><p id="0c92" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在计算机科学中，我们只对专注于<strong class="kx ir">数据结构</strong>的定义感兴趣。从这个角度来看，张量确实是矩阵的 n 维<em class="ne">的推广。</em></p><p id="dc90" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是当我们在深度学习的背景下谈论张量时，我们仍然忽略了一个重要的细微差别...</p><h1 id="8885" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">深度学习中的🧠张量</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/31ef0863324d706fc2db801a86109dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AZyHlg_Mgd2qfv0UxR87Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="ng">Freepik 和 smashingstocks 创建的图标— </em> <a class="ae lr" href="https://www.flaticon.com/free-icons/" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> Flaticon </em> </a></p></figure><p id="4a9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么为什么它们被称为“张量”而不是“多维数组”呢？好吧，是短了点，但这就是全部了吗？实际上，人们在谈论张量时会做一个隐含的假设。</p><p id="4d6c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PyTorch 的<a class="ae lr" href="https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_tensor.html#:~:text=PyTorch%3A%20Tensors,-A%20third%20order&amp;text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation." rel="noopener ugc nofollow" target="_blank">官方文档</a>给了我们一个实用的答案:</p><blockquote class="nh ni nj"><p id="5cc2" class="kv kw ne kx b ky kz jr la lb lc ju ld nk lf lg lh nl lj lk ll nm ln lo lp lq ij bi translated"><em class="iq">numpy 数组和 PyTorch 张量最大的区别是 PyTorch 张量既可以运行在</em> <strong class="kx ir"> <em class="iq"> CPU 上，也可以运行在</em></strong><em class="iq">GPU 上。</em></p></blockquote><p id="753a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在深度学习中，我们需要以高度并行的方式计算大量矩阵乘法的性能。这些矩阵(以及一般的 n 维数组)通常在 GPU 上存储和处理，以加快训练和推理时间。</p><p id="742c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我们之前的定义中所缺少的:深度学习中的张量不仅仅是 n 维数组，还有一个隐含的假设，即它们可以在 GPU 上运行。</p><h1 id="3802" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">⚔️·努皮 vs 皮托奇</h1><p id="82e3" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们看看 NumPy 数组和 PyTorch 张量的区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/d6f41523f07b7da0da5320f1c40a8cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4j9a--rOjr9wYfBgNffpyg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1258" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两个对象非常相似:我们可以用几乎相同的语法初始化一个<strong class="kx ir"> 1D 数组</strong>和一个<strong class="kx ir"> 1D 张量</strong>。它们还共享许多方法，可以很容易地相互转换。</p><p id="8f94" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以在这个地址找到这篇文章<a class="ae lr" href="https://github.com/mlabonne/how-to-data-science/blob/main/What_is_a_Tensor_in_Deep_Learning.ipynb" rel="noopener ugc nofollow" target="_blank">中使用的代码。</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="f76b" class="nv lt iq nr b gy nw nx l ny nz"><strong class="nr ir">NumPy Array</strong>: [1 2 3]</span><span id="29dc" class="nv lt iq nr b gy oa nx l ny nz"><strong class="nr ir">PyTorch Tensor</strong>: tensor([1, 2, 3])</span></pre><p id="cd8f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">初始化 2D 数组和 2D 张量并不复杂。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="2aab" class="nv lt iq nr b gy nw nx l ny nz"><strong class="nr ir">NumPy Array</strong>: [[1 2 3]<br/>              [4 5 6]]</span><span id="db1e" class="nv lt iq nr b gy oa nx l ny nz"><strong class="nr ir">PyTorch Tensor</strong>: tensor([[1, 2, 3],<br/>                        [4, 5, 6]])</span></pre><p id="ab83" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们说过张量和数组的唯一区别是张量可以在 GPU 上运行。所以说到底，这种区分是基于性能的。但这种推动有那么重要吗？</p><p id="defe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们比较一下 NumPy 数组和 PyTorch 张量在矩阵乘法上的性能。在下面的例子中，我们随机初始化<strong class="kx ir"> 4D 数组/张量，并将它们乘以</strong>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="46b5" class="nv lt iq nr b gy nw nx l ny nz">&gt;&gt;&gt;<strong class="nr ir"> 1.32 s</strong></span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="154b" class="nv lt iq nr b gy nw nx l ny nz">&gt;&gt;&gt;<strong class="nr ir"> 25.2 ms</strong></span></pre><p id="6633" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们所看到的，PyTorch 张量完成的性能优于 NumPy 数组:它们完成乘法<strong class="kx ir">的速度比</strong>快 52 倍！</p><p id="c220" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以将这种表现归因于不同的因素，例如:</p><ul class=""><li id="cb70" class="mq mr iq kx b ky kz lb lc le ms li mt lm mu lq mv mw mx my bi translated">NumPy 数组使用<em class="ne"> float64 </em>格式，而 PyTorch tensors 则利用了更高效的<em class="ne"> float32 </em>格式。然而，即使当 NumPy 数组转换为<em class="ne"> float32 </em>时，PyTorch 张量仍然快 40 倍。</li><li id="4f65" class="mq mr iq kx b ky mz lb na le nb li nc lm nd lq mv mw mx my bi translated">PyTorch 张量存储在 GPU 上，不像 NumPy 数组。但是，如果我们在 CPU 上重复同样的实验，PyTorch 张量仍然能够平均快 2.8 倍。</li></ul><p id="021a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">即使将这两个因素结合起来，PyTorch 张量也被证明要快 1.4 倍，这表明 NumPy 数组对于矩阵乘法来说性能确实较差。</p><p id="7b56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是张量的真正力量:它们快得惊人！性能可能会因维度、实现<strong class="kx ir">、</strong>和硬件而异，但这种速度是张量(而不是数组)在深度学习中如此常见的原因。</p><h1 id="260c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">📝结论</h1><p id="a173" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在本文中，我们根据以下内容编写了张量的定义:</p><ol class=""><li id="f991" class="mq mr iq kx b ky kz lb lc le ms li mt lm mu lq ob mw mx my bi translated">它们在<strong class="kx ir">计算机科学</strong>(数据结构)中的用途；</li><li id="c402" class="mq mr iq kx b ky mz lb na le nb li nc lm nd lq ob mw mx my bi translated">更具体的说，在<strong class="kx ir">深度学习</strong>(他们可以在 GPU 上运行)。</li></ol><p id="f572" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以用一句话来总结:</p><blockquote class="nh ni nj"><p id="6ecd" class="kv kw ne kx b ky kz jr la lb lc ju ld nk lf lg lh nl lj lk ll nm ln lo lp lq ij bi translated"><em class="iq">张量是</em> <strong class="kx ir"> <em class="iq"> n 维数组</em> </strong> <em class="iq">隐含假设它们可以</em> <strong class="kx ir"> <em class="iq">在 GPU 上运行。</em> </strong></p></blockquote><p id="e6b5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们看到了张量和数组之间的性能差异，这激发了深度学习对张量的需求。</p><p id="226f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以下一次有人试图向你解释张量不完全是矩阵的推广，你会知道他们在张量的特定定义中是正确的，但在计算机科学/深度学习中不是。</p><p id="1339" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你在寻找更多 n 维的数据科学和机器学习内容，请<strong class="kx ir">在 twitter 上关注我</strong><a class="ae lr" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">@ maxime labanne</strong></a>。你可以在这个地址找到本文<a class="ae lr" href="https://colab.research.google.com/drive/1azq12DApWgLgdWuYB3wh0TcwJVAfVxCL?usp=sharing" rel="noopener ugc nofollow" target="_blank">中使用的代码。📣</a></p></div></div>    
</body>
</html>