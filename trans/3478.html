<html>
<head>
<title>Using Python UDF’s and Snowflake’s Snowpark to build and deploy Machine Learning Models, Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python UDF和Snowflake的Snowpark构建和部署机器学习模型，第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-python-udfs-and-snowflake-s-snowpark-to-build-and-deploy-machine-learning-models-part-2-2fe40d382ae7#2022-08-02">https://towardsdatascience.com/using-python-udfs-and-snowflake-s-snowpark-to-build-and-deploy-machine-learning-models-part-2-2fe40d382ae7#2022-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="88a8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何训练和预测所有上传到Snowpark的UDF</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9992df25990339888414021238a4d77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPZzUG8ThgTVsUptcihFzg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://unsplash.com/@anthi98" rel="noopener ugc nofollow" target="_blank"> Anthi K </a>通过<a class="ae kv" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="3416" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">**********</p><p id="1fc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新:</strong>在与雪花交谈后，我在这里尝试复制的这个方法是为了一次训练多个模型，每个模型高达16mb的数据。同样，使用雪花进行计算，最大文件大小为5GB。所以任何大数据还是需要在Spark这样的东西里做。他们有一个更大文件的私人预览，但目前还没有对公众开放。</p><p id="3444" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我认为python存储过程或第1部分是使用雪花构建和部署机器学习模型的最佳选择。</p><p id="13c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">**********</p><p id="88fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://medium.com/@chriskuchar/using-python-udfs-and-snowflake-s-snowpark-to-build-and-deploy-machine-learning-models-a3c160c06d85" rel="noopener">第1部分</a>中，我展示了如何训练一个本地模型，将其包装在Python udf中，使用Snowpark将其推送到雪花，并使用Snowpark或雪花Sql使用该udf进行预测。</p><p id="be96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，本指南将向您展示如何使用Snowpark和Snowflake compute创建一个Python udf来构建、训练和预测模型。我们将在这里使用回归，而第1部分使用分类。</p><p id="8261" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文还将强调我在使用Snowpark和Python udf时发现的一些局限性。</p><p id="8a3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用了Github 上的<a class="ae kv" href="https://github.com/Snowflake-Labs/sfguide-citibike-ml-snowpark-python/blob/main/03_ML_Engineering.ipynb" rel="noopener ugc nofollow" target="_blank"> snowflake's guide中的这个指南来获得构建它的主要框架。</a></p><p id="54ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">导入库</strong></p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="66ae" class="lx ly iq lt b gy lz ma l mb mc">import snowflake.connector<br/>import pandas as pd<br/>import numpy as np<br/>from sqlalchemy import create_engine<br/>from snowflake.sqlalchemy import URL</span><span id="1960" class="lx ly iq lt b gy md ma l mb mc">from snowflake.connector.pandas_tools import write_pandas<br/>from snowflake.snowpark.functions import udf<br/>from snowflake.snowpark.types import IntegerType, StringType, StructType, FloatType<br/>from snowflake.snowpark.session import Session<br/>from snowflake.snowpark import Session<br/>import snowflake.snowpark.functions as F<br/>from snowflake.snowpark import types as T<br/>from snowflake.snowpark import Window<br/>from snowflake.snowpark.functions import udf, max, min, count, avg, sum, col, lit, listagg<br/>import mlxtend<br/>from mlxtend.feature_selection import ColumnSelector</span><span id="e4af" class="lx ly iq lt b gy md ma l mb mc">import lightgbm as lgb<br/>from sklearn.model_selection import GridSearchCV, train_test_split<br/>from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion<br/>from sklearn import datasets</span></pre><p id="8c86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据</strong></p><p id="29c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将解决一个回归问题，使用来自sklearn数据集的<a class="ae kv" href="https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html" rel="noopener ugc nofollow" target="_blank">糖尿病</a>数据集。阿尔2004年)。这个数据集是在<a class="ae kv" href="https://creativecommons.org/licenses/by/4.0/legalcode" rel="noopener ugc nofollow" target="_blank">知识共享署名4.0国际</a> (CC BY 4.0)许可下许可的。</p><p id="49c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这允许为任何目的共享和改编数据集，只要给予适当的信任。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="2fe3" class="lx ly iq lt b gy lz ma l mb mc">#Load the features<br/>db = pd.DataFrame(datasets.load_diabetes().data, columns=datasets.load_diabetes().feature_names)</span><span id="d926" class="lx ly iq lt b gy md ma l mb mc">#Load the response variable<br/>db['target'] = datasets.load_diabetes().target</span><span id="c032" class="lx ly iq lt b gy md ma l mb mc">#Convert to upper for preparation for uploading to Snowflake columnnames=[x.upper() for x in db.columns] <br/>db.columns=columnnames<br/>db.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/e05a429daa8a0d23ae9165cd5eafb9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRE8O5LS1W8BXPFyQKjJ0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9f1d" class="lx ly iq lt b gy lz ma l mb mc">print(datasets.load_diabetes().DESCR)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mf"><img src="../Images/57950a454e2e6d187dfe9779f78fb04e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iXRhUIbjYAWwaOBmOKo9iA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/4153b0ad50d929b479a2ff2a04f2a1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9LFjxkC677vJPu6plRpBeQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ae57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们将糖尿病数据保存为雪花中的一个表。我们稍后可以使用Snowpark连接到它:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="c4b8" class="lx ly iq lt b gy lz ma l mb mc">#Set up the snowflake connection<br/>ctx = snowflake.connector.connect(<br/>    user='&lt;user&gt;',<br/>    password='&lt;password&gt;',<br/>    account='&lt;account-identifier&gt;',<br/>    database='&lt;database&gt;',<br/>    warehouse='&lt;warehouse&gt;',<br/>    role='&lt;role&gt;',<br/>    schema='&lt;schema&gt;'<br/>    )</span><span id="10db" class="lx ly iq lt b gy md ma l mb mc">#Create the input string for the diabetes data <br/>snowflake_cols=[str(x) + ' float' for x in db.columns]<br/>s = ', '.join(snowflake_cols)</span><span id="38ae" class="lx ly iq lt b gy md ma l mb mc">#Create the empty table in Snowflake<br/>ctx.cursor().execute(<br/>"""CREATE OR REPLACE TABLE<br/>DIABETES_DATA(""" + s + """)""")</span><span id="9e60" class="lx ly iq lt b gy md ma l mb mc">#Copy the table into snowflake<br/>write_pandas(ctx, db, 'DIABETES_DATA')</span></pre><p id="742f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">连接到Snowpark中的数据:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="f881" class="lx ly iq lt b gy lz ma l mb mc">#Create snowpark session to connect to saved table. <br/>def create_session_object():<br/>   connection_parameters = {<br/>      "account": "&lt;account-identifier&gt;",<br/>      "user": "&lt;user&gt;",<br/>      "password": "&lt;password&gt;",<br/>      "role": "&lt;role&gt;",<br/>      "warehouse": "&lt;warehouse&gt;",<br/>      "database": "&lt;database&gt;",<br/>      "schema": "&lt;schema&gt;"<br/>   }<br/>   session = Session.builder.configs(connection_parameters).create()<br/>   print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())<br/>   return session</span><span id="e8f2" class="lx ly iq lt b gy md ma l mb mc">#Create two sessions, one for doing initial testing, and one for pushing the udf to snowpark. I've found the session tends to fail if I use just one session for both. </span><span id="e008" class="lx ly iq lt b gy md ma l mb mc">session=create_session_object()</span><span id="4f94" class="lx ly iq lt b gy md ma l mb mc">session2=create_session_object()</span><span id="9ac4" class="lx ly iq lt b gy md ma l mb mc">cols=session.table('DIABETES_DATA')<br/>cols.schema.fields</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/89f1a4dbb703ebfe75bbb58410bf6ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wjxQedEInoLdnDVDhVXDiA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4f67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">造型</strong></p><p id="04bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，构建udf时要记住的一件重要事情是，数据必须作为array_construct()或所有特性的单个列传入。这类似于Spark中的features列输入。</p><p id="92aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，为了准备我们的udf，让我们看看是否可以使用Snowpark array_construct作为输入来构建一个本地模型。这样我们就能知道我们的代码是否能在我们上传到Snowpark的udf中工作。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0f12" class="lx ly iq lt b gy lz ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="90ee" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct of the data to be used for modeling. digits=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="ab69" class="lx ly iq lt b gy md ma l mb mc">#Collect the data from Snowpark<br/>tbl=pd.DataFrame(digits.collect())</span><span id="a0ab" class="lx ly iq lt b gy md ma l mb mc">tbl.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/417ccf0fbeb23cecae32d8a15ab74780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEyO7jtV-nI0sqk9xqTVnw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ccb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以看到，将它读入我们的本地环境并尝试将其转换为数据帧是行不通的。它仍然将它保存为一列。让我们使用Python中的str.split方法将数据分离出来。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="d61d" class="lx ly iq lt b gy lz ma l mb mc">tbl=tbl['INPUT_DATA'].str.split(',\n', expand=True)<br/>tbl.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/1df73562da3312661082981ea34c1c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXtmdT0JL1AmKnr42mvrSg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="84b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们仍然有一些时髦的新的行字符和括号，所以让我们去掉它们。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="2ad4" class="lx ly iq lt b gy lz ma l mb mc">tbl[0]=[x.replace('\n','').replace('[  ','').replace(']','') for x in tbl[0]]</span><span id="1025" class="lx ly iq lt b gy md ma l mb mc">tbl[10]=[x.replace('\n','').replace('[  ','').replace(']','') for x in tbl[10]]</span><span id="18b8" class="lx ly iq lt b gy md ma l mb mc">tbl.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/735a0bb70734c5c00ef81ded897c4c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nM7DdtA2rMrJNQusfFdFXA.png"/></div></div></figure><p id="73c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的数据现在看起来不错，让我们看看能否构建一个模型:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0a25" class="lx ly iq lt b gy lz ma l mb mc">#Check NAs<br/>count_nas=pd.DataFrame(tbl.isna().sum())<br/>count_nas[count_nas[0]&gt;0]</span><span id="89e9" class="lx ly iq lt b gy md ma l mb mc">#Fill in NAs if there were any <br/>tbl=tbl.fillna(0)</span><span id="b0c0" class="lx ly iq lt b gy md ma l mb mc">#Set the column names to what our original table in Snowflake has <br/>tbl.columns=columns_list</span><span id="bf25" class="lx ly iq lt b gy md ma l mb mc">#Split into features, target, training and validation<br/>X=tbl.drop(columns='TARGET')<br/>y=tbl['TARGET']<br/>X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1234, test_size=.33)</span><span id="167e" class="lx ly iq lt b gy md ma l mb mc">#Build a pipeline <br/>numeric_features=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']</span><span id="b54d" class="lx ly iq lt b gy md ma l mb mc">numeric_cols = Pipeline(steps=[<br/>    ('selector', ColumnSelector(numeric_features))])</span><span id="0f66" class="lx ly iq lt b gy md ma l mb mc"># Combine categorical and numerical pipeline with FeatureUnion<br/>preprocessor = FeatureUnion([<br/>    ('select_numeric_cols',numeric_cols)<br/>])</span><span id="5b38" class="lx ly iq lt b gy md ma l mb mc">pipe_feat_un = Pipeline(steps=[('preprocessor', preprocessor)])</span><span id="d2ad" class="lx ly iq lt b gy md ma l mb mc">#Light gbm<br/>clf = make_pipeline(lgb.LGBMRegressor())</span><span id="c058" class="lx ly iq lt b gy md ma l mb mc">#Add the model to the pipeline<br/>model = make_pipeline(pipe_feat_un, clf)</span><span id="d9d7" class="lx ly iq lt b gy md ma l mb mc">#Fit on the training data<br/>model.fit(X_train,y_train)</span><span id="6a02" class="lx ly iq lt b gy md ma l mb mc">#Predict on the validation data<br/>preds=model.predict(X_valid)</span><span id="fa3d" class="lx ly iq lt b gy md ma l mb mc">#Look at the predictions<br/>preds</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/085007f3afa5fc84cf50cb2f40861cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gsHZjm4LQx-Ffnuk7Fe-cQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="bdca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是我们所期望的，连续范围内的预测。</p><p id="6aad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们把它包装成一个函数，这样我们就可以把它作为一个udf上传:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="85c0" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict_udf(db, columns_list):<br/>    import pandas as pd<br/>    import numpy as np<br/>    from lightgbm import LGBMRegressor<br/>    from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion<br/>    from mlxtend.feature_selection import ColumnSelector<br/>    import mlxtend</span><span id="307d" class="lx ly iq lt b gy md ma l mb mc">    #Read in the data <br/>    tbl=pd.DataFrame(db, columns=['INPUT_DATA'])<br/>    tbl=tbl['INPUT_DATA'].str.split(',\n', expand=True)<br/>    tbl[0]=[x.replace('\n','').replace('[  ','').replace(']','') for x in tbl[0]]<br/>    tbl[10]=[x.replace('\n','').replace('[  ','').replace(']','') for x in tbl[10]]</span><span id="235d" class="lx ly iq lt b gy md ma l mb mc">    #Fill in NAs if there are any<br/>    tbl=tbl.fillna(0)</span><span id="fee3" class="lx ly iq lt b gy md ma l mb mc">    #Change the column names to what our table in Snowflake has<br/>    tbl.columns=columns_list</span><span id="7795" class="lx ly iq lt b gy md ma l mb mc">    #Split into features, target, training and validation<br/>    X=tbl.drop(columns='TARGET')<br/>    y=tbl['TARGET']<br/>    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1234, test_size=.33)</span><span id="bbb1" class="lx ly iq lt b gy md ma l mb mc">    #Build a pipeline<br/>    numeric_features=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']<br/>    numeric_cols = Pipeline(steps=[<br/>        ('selector', ColumnSelector(numeric_features))])</span><span id="500d" class="lx ly iq lt b gy md ma l mb mc">    # Combine categorical and numerical pipeline with FeatureUnion<br/>    preprocessor = FeatureUnion([<br/>        ('select_numeric_cols',numeric_cols)<br/>    ])<br/>    pipe_feat_un = Pipeline(steps=[('preprocessor', preprocessor)])</span><span id="4251" class="lx ly iq lt b gy md ma l mb mc">    #Light gbm<br/>    clf = make_pipeline(lgb.LGBMRegressor())</span><span id="b311" class="lx ly iq lt b gy md ma l mb mc">    #Add the model to the pipeline<br/>    model = make_pipeline(pipe_feat_un, clf)</span><span id="ce47" class="lx ly iq lt b gy md ma l mb mc">    #Fit on the training data<br/>    model.fit(X_train,y_train)</span><span id="5b10" class="lx ly iq lt b gy md ma l mb mc">    #Predict on the validation data<br/>    preds=model.predict(X_valid)<br/>    return(preds)</span></pre><p id="8ece" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们将它上传为一个udf:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="a043" class="lx ly iq lt b gy lz ma l mb mc">dep_imports=['/opt/anaconda3/lib/python3.8/site-packages/mlxtend']</span><span id="180b" class="lx ly iq lt b gy md ma l mb mc">lightgbm_train_predict_udf = session2.udf.register(lightgbm_train_predict_udf, name="lightgbm_train_predict_udf",is_permanent=True,                      stage_location='MODELSTAGE', packages=['numpy','scikit-learn','lightgbm','pandas'], imports=dep_imports, input_types [T.ArrayType(), T.ArrayType()],return_type=T.ArrayType(), replace=True)</span></pre><p id="2172" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想调出session.udf.register函数的导入参数。如果Snowpark中没有您需要的库，您可以很容易地添加它们。这增加了使用Snowpark的灵活性。在本例中，我正在导入mlxtend，这允许我在管道中使用ColumnSelector函数。</p><p id="eefc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里需要注意的另一个要点是，我们只能使用在<a class="ae kv" href="https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/_autosummary/snowflake.snowpark.types.html#module-snowflake.snowpark.types" rel="noopener ugc nofollow" target="_blank"> Snowpark类型文档</a>中列出的类型。具体来说，我们必须使用数组构造作为udf的输入。</p><p id="befe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们测试我们上传的udf:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="92c0" class="lx ly iq lt b gy lz ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="060a" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct for input into the udf diabetes=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="c38d" class="lx ly iq lt b gy md ma l mb mc">#Create an array construct of the column names to be fed in the function <br/>input_column_names = F.array_construct(*[F.lit(x) for x in columns_list])</span><span id="12ad" class="lx ly iq lt b gy md ma l mb mc">#Call the udf and look at the results output=diabetes.select(F.call_udf('lightgbm_train_predict_udf', F.col('INPUT_DATA'), input_column_names).alias('ALL_ONE_UDF_PREDICTED')).collect()</span><span id="ef7a" class="lx ly iq lt b gy md ma l mb mc">output</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/1c26e0d08e2fb1ab9f09554c37435d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4hNacd8B6_9OWVO5c6Rb1w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="56e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此代码在' TBL = TBL[' INPUT _ DATA ']. str . split('，\n '，expand=True)处失败。这是没有意义的，因为第一次测试时，它在我们的本地环境中工作。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="2c40" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict_udf(db, columns_list):<br/>    tbl=pd.DataFrame(db, columns=['INPUT_DATA'])<br/>    tbl=tbl['INPUT_DATA'].str.split(',\n', expand=True)</span></pre><p id="9d70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们对此进行调整，并再次测试udf，看看第一次运行' tbl=pd时它会返回什么。DataFrame(db，columns=['INPUT_DATA'])'。由于我们只能根据Snowpark的文档返回数据类型，所以我找到的最简单的方法是np.array()。这与Snowpark ArrayType()无缝地相互转换。返回DataFrame不起作用，也不受支持。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="a8ca" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict(diabetes, columns_list):<br/>    import pandas as pd<br/>    import numpy as np<br/>    from lightgbm import LGBMRegressor<br/>    from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion<br/>    from mlxtend.feature_selection import ColumnSelector<br/>    import mlxtend<br/>    tbl=pd.DataFrame(diabetes, columns=['INPUT_DATA'])<br/>    return(np.array(tbl))<br/>    <br/>dep_imports=['/opt/anaconda3/lib/python3.8/site-packages/mlxtend']</span><span id="77c0" class="lx ly iq lt b gy md ma l mb mc">#Register the udf to Snowflake/Snowpark<br/>lightgbm_train_predict_udf = session2.udf.register(lightgbm_train_predict, name="lightgbm_train_predict_udf", is_permanent=True, stage_location='MODELSTAGE', <br/>packages=['numpy','scikit-learn','lightgbm','pandas'], <br/>imports=dep_imports, input_types=[T.ArrayType(), T.ArrayType()],<br/>return_type=T.ArrayType(), replace=True) </span><span id="1303" class="lx ly iq lt b gy md ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="c305" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct for input into the udf diabetes=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="fac1" class="lx ly iq lt b gy md ma l mb mc">#Create an array construct of the column names to be fed in the function <br/>input_column_names = F.array_construct(*[F.lit(x) for x in columns_list])</span><span id="f0f2" class="lx ly iq lt b gy md ma l mb mc">#Call the udf and look at the results output=diabetes.select(F.call_udf('lightgbm_train_predict_udf', F.col('INPUT_DATA'), input_column_names).alias('ALL_ONE_UDF_PREDICTED')).collect()<br/>output</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/a9f3324b6bf361e83bb9c7c2ed167e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ml4vfmVrQNrKFU3ml29LIQ.png"/></div></div></figure><p id="5fa2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将返回我们所期望的结果。但是这意味着我们的split方法在udf中不起作用。这让我相信Snowpark中有一些我无法理解的数据类型变化。我的假设是，这与让Python在雪花上运行的约束有关。</p><p id="8e7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为前面的另一个示例做准备，让我们使用以下内容对此数组进行子集划分:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="b28c" class="lx ly iq lt b gy lz ma l mb mc">output[0:1]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/459eb3b897604e512a9fecbaef689b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bKmuH8nEGtsPqZgiR6Ctag.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ebc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">output[0:1]返回数组的第一行。</p><p id="8135" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，下面是我想到的最好的解决方法，可以让udf在读入数据和格式化数据以进行建模时工作。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="22d0" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict(db, columns_list):<br/>    db=np.array(db)<br/>    objects=[pd.DataFrame(db[i:i+1]) for i in range(0,11)]<br/>    tbl=pd.concat(objects, axis=1)</span><span id="cb5d" class="lx ly iq lt b gy md ma l mb mc">    #Fill in NAs if there are any<br/>    tbl=tbl.fillna(0)</span><span id="6b1a" class="lx ly iq lt b gy md ma l mb mc">    #Change the column names to what our table in Snowflake has<br/>    tbl.columns=columns_list<br/>    return(np.array(tbl))</span><span id="67e7" class="lx ly iq lt b gy md ma l mb mc">dep_imports=['/opt/anaconda3/lib/python3.8/site-packages/mlxtend']</span><span id="2942" class="lx ly iq lt b gy md ma l mb mc">#Register the udf to Snowpark/Snowflake<br/>lightgbm_train_predict_udf = session2.udf.register(lightgbm_train_predict, name="lightgbm_train_predict_udf", is_permanent=True, stage_location='MODELSTAGE', packages=['numpy','scikit-learn','lightgbm','pandas'], imports=dep_imports, input_types=[T.ArrayType(), T.ArrayType()], return_type=T.ArrayType(), replace=True)</span><span id="6b22" class="lx ly iq lt b gy md ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="0494" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct for input into the udf diabetes=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="1eab" class="lx ly iq lt b gy md ma l mb mc">#Create an array construct of the column names to be fed in the function <br/>input_column_names = F.array_construct(*[F.lit(x) for x in columns_list])</span><span id="4f38" class="lx ly iq lt b gy md ma l mb mc">#Call the udf and look at the results output=diabetes.select(F.call_udf('lightgbm_train_predict_udf', F.col('INPUT_DATA'), input_column_names).alias('ALL_ONE_UDF_PREDICTED')).collect()</span><span id="a520" class="lx ly iq lt b gy md ma l mb mc">output</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/a9f3324b6bf361e83bb9c7c2ed167e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ml4vfmVrQNrKFU3ml29LIQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a0e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为这返回了我们所有数据的数组，所以我们可以知道将它转换为DataFrame并重命名所有列是有效的。否则，它就会出错。</p><p id="072d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想指出的上面代码中最大的标注是这三行代码:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="94ae" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict(diabetes, columns_list):<br/>    diabetes=np.array(diabetes)<br/>    objects=[pd.DataFrame(diabetes[i:i+1]) for i in range(0,11)]</span></pre><p id="6e0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们将它转换成一个np.array，使用digits[0:1]对它进行子集化会返回第一个<em class="mo">列</em>，而当我们将它作为子集输出[0:1]带到本地环境时，它会返回第一个<em class="mo">行。</em>我真的不确定为什么Snowpark的Python实例会这样，但我希望它不会这样。找出这种差异就像戴上眼罩，试图根据颜色的感觉来解开魔方。我不推荐这样做，也不想再做了。</p><p id="ccdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们尝试重新注册udf并构建模型:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="125a" class="lx ly iq lt b gy lz ma l mb mc">def lightgbm_train_predict(db, columns_list):<br/>    import pandas as pd<br/>    import numpy as np<br/>    from lightgbm import LGBMRegressor<br/>    from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion<br/>    from mlxtend.feature_selection import ColumnSelector<br/>    import mlxtend</span><span id="37e4" class="lx ly iq lt b gy md ma l mb mc">    #Read in the data <br/>    db=np.array(db)<br/>    objects=[pd.DataFrame(db[i:i+1]) for i in range(0,11)]<br/>    tbl=pd.concat(objects, axis=1)</span><span id="17c5" class="lx ly iq lt b gy md ma l mb mc">    #Fill in NAs if there are any<br/>    tbl=tbl.fillna(0)</span><span id="263a" class="lx ly iq lt b gy md ma l mb mc">    #Change the column names to what our table in Snowflake has<br/>    tbl.columns=columns_list</span><span id="ca13" class="lx ly iq lt b gy md ma l mb mc">    #Split into features, target, training and validation<br/>    X=tbl.drop(columns='TARGET')<br/>    y=tbl['TARGET']<br/>    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1234, test_size=.33)</span><span id="6444" class="lx ly iq lt b gy md ma l mb mc">    #Build a pipeline<br/>    numeric_features=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']<br/>    numeric_cols = Pipeline(steps=[<br/>        ('selector', ColumnSelector(numeric_features))])</span><span id="85ef" class="lx ly iq lt b gy md ma l mb mc">    # Combine categorical and numerical pipeline with FeatureUnion<br/>    preprocessor = FeatureUnion([<br/>        ('select_numeric_cols',numeric_cols)<br/>    ])<br/>    pipe_feat_un = Pipeline(steps=[('preprocessor', preprocessor)])</span><span id="34d5" class="lx ly iq lt b gy md ma l mb mc">    #Light gbm<br/>    clf = make_pipeline(lgb.LGBMRegressor())</span><span id="e913" class="lx ly iq lt b gy md ma l mb mc">    #Add the model to the pipeline<br/>    model = make_pipeline(pipe_feat_un, clf)</span><span id="989e" class="lx ly iq lt b gy md ma l mb mc">    #Fit on the training data<br/>    model.fit(X_train,y_train)</span><span id="156a" class="lx ly iq lt b gy md ma l mb mc">    #Predict on the validation data<br/>    preds=model.predict(X_valid)<br/>    return(preds)</span><span id="8211" class="lx ly iq lt b gy md ma l mb mc">dep_imports=['/opt/anaconda3/lib/python3.8/site-packages/mlxtend']</span><span id="3f46" class="lx ly iq lt b gy md ma l mb mc">#Register the udf to Snowpark/Snowflake<br/>lightgbm_train_predict_udf = session2.udf.register(lightgbm_train_predict, name="lightgbm_train_predict_udf", is_permanent=True, stage_location='MODELSTAGE', packages=['numpy','scikit-learn','lightgbm','pandas'], imports=dep_imports, input_types=[T.ArrayType(), T.ArrayType()], return_type=T.ArrayType(), replace=True)</span><span id="d76d" class="lx ly iq lt b gy md ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="81e9" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct for input into the udf diabetes=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="7d55" class="lx ly iq lt b gy md ma l mb mc">#Create an array construct of the column names to be fed in the function <br/>input_column_names = F.array_construct(*[F.lit(x) for x in columns_list])</span><span id="5778" class="lx ly iq lt b gy md ma l mb mc">#Call the udf and look at the results output=diabetes.select(F.call_udf('lightgbm_train_predict_udf', F.col('INPUT_DATA'), input_column_names).alias('ALL_ONE_UDF_PREDICTED')).collect()</span><span id="6e97" class="lx ly iq lt b gy md ma l mb mc">output</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/9d2dc058bcfaf8cc5054b5848ba74d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsrwKMo2qmivKr3We-roXg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="331a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">耶，又一个错误。这个错误是sklearn train_test_split函数。不管是什么原因，sklearn的train_test_split在Snowpark内部是行不通的。此外，使用light gbm时出现错误。但是我将跳过这个例子。</p><p id="0434" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我修复这些错误的方法是使用np.random.rand和xgboost手动分成训练和测试。这是我的最终解决方案，我甚至不确定这是否正确。尽管如此，它确实完成了整个过程，并且预测没有显示错误。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="6da1" class="lx ly iq lt b gy lz ma l mb mc">def xgboost_train_predict(db, columns_list):<br/>    import pandas as pd<br/>    import numpy as np<br/>    from xgboost import XGBRegressor<br/>    from sklearn.model_selection import GridSearchCV, train_test_split<br/>    from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, FunctionTransformer<br/>    from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion<br/>    from sklearn.compose import ColumnTransformer<br/>    from mlxtend.feature_selection import ColumnSelector<br/>    import mlxtend</span><span id="bc04" class="lx ly iq lt b gy md ma l mb mc">    #Read in the data<br/>    db=np.array(db)<br/>    objects=[pd.DataFrame(db[i:i+1]) for i in range(0,11)]<br/>    tbl=pd.concat(objects, axis=1)</span><span id="67e8" class="lx ly iq lt b gy md ma l mb mc">    #Fill in NAs if there are any<br/>    tbl=tbl.fillna(1)</span><span id="88c1" class="lx ly iq lt b gy md ma l mb mc">    #Change the column names to what our table in Snowflake has<br/>    tbl.columns=columns_list<br/><br/>    # #Split into training and validation<br/>    tbl['prob'] = np.random.rand(len(tbl))<br/>    tbl['counter'] = [1 if x &lt;= .65 else 0 for x in tbl['prob']]<br/>    # return(np.array(tbl))<br/>    training_data = tbl.loc[tbl['counter']==1,].reset_index(drop=True)<br/>    testing_data = tbl.loc[tbl['counter']==0,].reset_index(drop=True)<br/>    X_train=training_data.drop(columns=['TARGET','prob','counter'])<br/>    y_train=training_data['TARGET']<br/>    X_valid=testing_data.drop(columns=['TARGET','prob','counter'])<br/>    y_valid=testing_data['TARGET']</span><span id="e65d" class="lx ly iq lt b gy md ma l mb mc">    #Build a pipeline<br/>    numeric_features=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']<br/>    numeric_cols = Pipeline(steps=[<br/>        ('selector', ColumnSelector(numeric_features))])<br/>    # Combine categorical and numerical pipeline with FeatureUnion<br/>    preprocessor = FeatureUnion([<br/>        ('select_numeric_cols',numeric_cols)<br/>    ])<br/>    pipe_feat_un = Pipeline(steps=[('preprocessor', preprocessor)])</span><span id="608c" class="lx ly iq lt b gy md ma l mb mc">    #Light gbm<br/>    clf = make_pipeline(XGBRegressor(n_estimators=5))</span><span id="bde2" class="lx ly iq lt b gy md ma l mb mc">    #Add the model to the pipeline<br/>    model = make_pipeline(pipe_feat_un, clf)</span><span id="0b67" class="lx ly iq lt b gy md ma l mb mc">    #Fit on the training data<br/>    model.fit(X_train, y_train)</span><span id="ad1d" class="lx ly iq lt b gy md ma l mb mc">    #Predict on the validation data<br/>    preds=model.predict(X_valid)<br/>    return(preds)</span><span id="f308" class="lx ly iq lt b gy md ma l mb mc">dep_imports=['/opt/anaconda3/lib/python3.8/site-packages/mlxtend']</span><span id="ec68" class="lx ly iq lt b gy md ma l mb mc">xgboost_train_predict_udf = session2.udf.register(xgboost_train_predict,name="xgboost_train_predict_udf",is_permanent=True,stage_location='MODELSTAGE',packages=['numpy','scikit-learn','xgboost','pandas'], imports=dep_imports, input_types=[T.ArrayType(),T.ArrayType()], return_type=T.ArrayType(),replace=True)</span><span id="9525" class="lx ly iq lt b gy md ma l mb mc">columns_list=['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6','TARGET']</span><span id="8aa9" class="lx ly iq lt b gy md ma l mb mc">#Connect to the table in Snowflake and create an array construct for input into the udf diabetes=session2.table('DIABETES_DATA').select(F.array_construct(*columns_list).alias('INPUT_DATA'))</span><span id="5692" class="lx ly iq lt b gy md ma l mb mc">#Create an array construct of the column names to be fed in the function <br/>input_column_names = F.array_construct(*[F.lit(x) for x in columns_list])</span><span id="71dd" class="lx ly iq lt b gy md ma l mb mc">#Call the udf and look at the results output=diabetes.select(F.call_udf('xgboost_train_predict_udf', F.col('INPUT_DATA'), input_column_names).alias('ALL_ONE_UDF_PREDICTED')).collect()</span><span id="8e48" class="lx ly iq lt b gy md ma l mb mc">output</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/1db83eef142879a8be59e3e9b4fb9f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GgYFEpQmCdz6yOKvUAeeAw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b083" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看这些结果，Snowpark处理模型的方式有明显的错误。首先，并不是所有的预测都应该是0.5。当我们在本地建立模型时，它应该在一个连续的范围内。其次，它带来了空数组，因为Snowpark不会在训练和测试分割期间使用data frame . reset _ index(drop = True)重新索引数据帧或数组。请参见下面的代码:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="fbde" class="lx ly iq lt b gy lz ma l mb mc">training_data = tbl.loc[tbl['counter']==1,].reset_index(drop=True)<br/>testing_data = tbl.loc[tbl['counter']==0,].reset_index(drop=True)</span></pre><p id="7965" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论:</strong></p><p id="f717" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我认为最好在本地构建一个模型，然后像我们在本系列第1部分中所做的那样，将它作为udf发送到雪花的Snowpark。我的观点是，Snowpark的Python仍然处于早期阶段，还不具备使用本地环境的Python所具备的许多功能的能力。</p><p id="cd8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在Snowpark的Python中引入错误的函数，这些函数在我的本地环境中使用相同的进程和数据:</strong></p><ol class=""><li id="fd4c" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">相同的数组子集/维度数组。例如，上例中的output[0:1]返回本地环境中的第一行，以及Snowpark中的第一列</li><li id="eb40" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">从Snowpark阵列或数据帧转换到pandas数据帧</li><li id="cce0" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">Sklearn的train_test_split()</li><li id="00d9" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">Lightgbm</li><li id="69f6" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">重新索引数据帧或数组</li><li id="3734" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">Tfidf()和countvectorizer()</li></ol><p id="f3f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我希望Snowpark中的udf具有的功能:</strong></p><ol class=""><li id="10af" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">可以选择返回数据帧作为上传udf的结果</li><li id="0cc2" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">将模型作为上传的udf的输出返回</li></ol><p id="641c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总的来说，我看到了Snowpark的可用性，我对即将到来的变化和改进感到兴奋。如果雪花修复了这些错误，我将发布一个更新的指南和一个工作实例。</p><p id="9606" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献:</strong></p><p id="5727" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Bradley Efron，Trevor Hastie，Iain Johnstone和Robert Tibshirani (2004)“最小角度回归”，统计年鉴(附讨论)，407–499。(https://web . Stanford . edu/~ hastie/Papers/LARS/leas tangle _ 2002 . pdf)</p></div></div>    
</body>
</html>