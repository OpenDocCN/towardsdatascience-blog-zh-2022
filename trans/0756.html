<html>
<head>
<title>7 Most Asked Questions on K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于K均值聚类的7个最常见问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explain-ml-in-a-simple-way-k-means-clustering-e925d019743b#2022-03-03">https://towardsdatascience.com/explain-ml-in-a-simple-way-k-means-clustering-e925d019743b#2022-03-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="80ae" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最常用的无监督聚类算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dc0b0e739d2a4053c2e093de7533833f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLbVK900-7YyEhkqPLw3HA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="ab2b" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">背景</h2><p id="c7be" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我们每天大约产生2.5万亿字节的数据。这些数据采用字符、数字、文本、图像、声音等形式。不足为奇的是，这些数据中的大部分都没有标签，有用的见解被淹没在数据的大山中。<strong class="lw iu">聚类</strong>是一种<strong class="lw iu">无监督学习</strong>方法，广泛用于寻找具有相似特征<em class="mn">的数据点组(称为聚类),而不需要现有的标记数据</em>。</p><p id="3e0d" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">借助聚类方法，我们可以将原始数据转化为可操作的见解。例如，我们可以对共享相同主题的消息进行聚类，对属于同一对象的图像进行分组，将具有相似行为的客户归类到同一组中。在本文中，我们将讨论最常用的聚类方法:<strong class="lw iu"> K-Means聚类</strong>。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="d347" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">#1:什么是K均值？</h2><p id="6fb1" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">用通俗的语言来说，K-Means的目的就是把具有相似特征的数据点放在同一个聚类中(即内部内聚)，把具有不同特征的数据点分离到不同的聚类中(即外部分离)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/b98ff51a37e0a1e6a0ba76b130ee7ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nm43wXkfUKrEjF9DLJcapA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a503" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">从技术上讲，我们需要数学公式来量化内部内聚和外部分离。</p><ul class=""><li id="6947" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated"><strong class="lw iu">群内方差</strong>(也称为平方误差函数或内平方和(SSW)或平方和误差(SSE))用于量化内部凝聚力。它被定义为平均点(称为<strong class="lw iu">质心</strong>)和簇中每个点之间的平方距离之和。<em class="mn">值越小，聚类越好。</em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/231b236a43eeb60769790c9b4f4c441d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EEF63qfm2MPA6NXZ_6DS3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ul class=""><li id="1b0c" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated"><strong class="lw iu">组间方差</strong>(又名,( SSB)之间的平方和)用于量化外部分离。它被定义为全局平均点和每个质心之间的平方距离之和。<em class="mn">值越大，聚类越好</em>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/16ff6e053707da11886ef6ed022a2cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iN1I8hZ3V695szmzldvXlQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="23f6" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在实践中，我们只需要最小化组内方差，因为最小化SSW(组内平方和)必然会最大化SSB(组间平方和)</p><p id="206b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们用一个简单的例子来证明。在下面的例子中，我们希望根据得分值创建聚类。如果我们简单地将前三个观察值归入第一组，后三个观察值归入第二组。第一组的平均分数是25分，第二组是16分。我们知道，不管集群是如何创建的，全局平均值(20.5)总是保持不变。所以<strong class="lw iu"> <em class="mn"> SST </em> </strong> <em class="mn">(每个点与全局平均点之间距离的平方和)也将一直保持不变</em>。数学上，证明SST = SSW + SSB并不难。因此，<em class="mn">找到最小化SSW的簇将间接最大化SSB </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/5e9f026575864abbfe9eb4c88c26947d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jroslpfI84sIdFwEkHBWMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="cd98" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"># 2:K-Means聚类是如何工作的？</h2><p id="ad3b" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">步骤1:通过随机选取K个起始点来初始化聚类质心</p><p id="acfa" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">步骤2:将每个数据点分配到最近的质心。K-Means聚类常用的距离计算是<strong class="lw iu">欧几里德距离，这是一个度量两个数据点之间距离的尺度值。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/5be19e8b8436bcf1bd3cd6546c36d211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-qVF7cApg8b8ysMS_S4mQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="67d1" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">步骤3:更新聚类质心。质心计算为聚类中数据点的平均值。<em class="mn">更新的质心可能是也可能不是实际的数据点。如果他们是，那将是一个巧合。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/ef6a8632db89783ddc525157d53db470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-Urw_dzufrRsLr5jI4F8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="bbcb" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">步骤4:重复步骤2-3(将每个数据点分配给新的质心并更新聚类质心)，直到满足其中一个停止条件。</p><ul class=""><li id="b077" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated">更新后的质心与前一次迭代中的质心保持一致(这是一种理想的情况，但在实践中，这可能太耗时了)</li><li id="0b64" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">上证指数至少没有提高x %</li><li id="78eb" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">达到了最大迭代次数(明智地选择最大迭代次数，否则，我们会得到较差的聚类。)</li></ul></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="dc9a" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">#3:如何对K-Means的原始数据进行预处理？</h2><p id="9965" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">K-Means使用基于距离的测量(例如欧几里德距离)来计算每个数据点与使用所有特征的值的质心的相似程度。这些特征通常采用<em class="mn">不可比单位</em>的值(例如，以美元表示的货币、以千克表示的重量、以华氏度表示的温度)。为了产生公平的结果，建议将原始数据标准化。我们可以转换原始数据，使<strong class="lw iu">的平均值为0，标准偏差为1</strong>，这样所有特征的权重<strong class="lw iu">都相等</strong>。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="2ecd" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">#4: <strong class="ak">如何挑选K-Means中的K值？</strong></h2><p id="0429" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">如果我们事先知道K值或者从领域专家那里得到建议，那将是理想的。如果没有，我们将需要依靠替代方法。虽然没有关于如何选择K-Means聚类的K值的最终方法，但是有一些流行的方法可以用来估计最佳的聚类数。</p><p id="3221" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">肘法</strong>:用SSE(又名，<strong class="lw iu">集群惯性</strong>)来评价拆分的好坏。然后，我们为范围从2到N的K值创建一个SSE肘图(您可以为您的研究设置N的值)。随着K的增加，对应的SSE会减少。我们将观察K和SSE之间的权衡(我们希望SSE低，同时将K保持在合理的值)。<em class="mn">当我们看到上证综指开始变平并形成肘形时，我们通常会选择K的最佳值</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/28da9655e98f1a52ea0b9d001a1e3251.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/0*9ltCooMB4QSJnR0O.png"/></div></figure><p id="7e9b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">剪影分析</strong>:用<strong class="lw iu">剪影系数</strong>来评价分割的好坏。轮廓系数计算如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/beb2a9797d54cd3531fc0265a182acb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBP3ccl6y-lqYgoWf292JQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4659" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">S(i)是给定数据点的轮廓系数。a(i)是该给定数据点和同一聚类中所有其他数据点之间的平均距离。b(i)是该给定数据点和来自最近聚类的所有数据点之间的平均距离。S(i)的范围可以从-1到1。</p><ul class=""><li id="60be" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated">如果S(i) = 1，则意味着该数据点靠近同一聚类内的点，而远离相邻聚类。</li><li id="0fd6" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">如果S(i) = 0，则意味着该数据点接近其聚类的边界。</li><li id="a2d1" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">如果S(i) = -1，则意味着该数据点被分配到错误的簇。</li></ul><p id="7a5b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">最终轮廓系数计算为所有数据点的平均轮廓系数。然后，我们计算范围从2到n的K值的轮廓系数。<em class="mn">轮廓系数越高，聚类可能越好。</em></p><p id="3f80" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">戴维斯-波尔丁指数:</strong>用<strong class="lw iu">戴维斯-波尔丁指数</strong>来评价分割的好坏。戴维斯-波尔丁指数<strong class="lw iu"> </strong>计算如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/5b59fab2924f47971454cf694ee935b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lG-IjMQKVsP1ZZjHo2FIbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="25de" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">D(i，j)是给定的一对簇(例如，簇I和j)的Davies-Bouldin指数。d(i)和d(j)分别是聚类I和聚类j中每个点与其质心之间的平均距离。d(i，j)是簇I和j的质心之间的距离。</p><p id="03f4" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">对于一个给定的簇，我们将计算它自己和所有其他簇之间的Davies-Bouldin指数。然后我们取这个集群的最大戴维斯-波尔丁指数。最后，我们将最终的戴维斯-波尔丁指数计算为这些最大值的平均值。然后我们计算K值范围从2到n的Davies-Bouldin指数<em class="mn">Davies-Bouldin指数越小，这些聚类越远，聚类越好。</em></p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="6b3c" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">#5:如何挑选K均值的起点？</h2><p id="9ae0" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">即使我们选择了最佳的K值，K-Means方法也不一定能产生最佳的聚类。由于K-Means算法很可能陷入局部最优，并且永远不会收敛到全局最优，因此得到的聚类可能会基于不同的起点而变化。因此，强烈建议使用不同的随机起点集运行K-Means，并根据上面提到的三种评估方法选择最佳结果。</p><p id="f497" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">有一种先进的初始化方法，如<strong class="lw iu"> K-Means++和</strong>，这使得它能够克服陷入局部最优的问题，并提高聚类的质量。直觉很简单。<em class="mn">我们将挑选彼此远离的初始质心，这样更有可能从不同的簇中挑选点。</em> K-Means++可以通过以下步骤实现。</p><ul class=""><li id="35d8" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated">步骤1:我们随机选取一个数据点作为第一个质心。</li><li id="f3b6" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">步骤2:我们用最近的质心计算剩余点之间的距离。</li><li id="ff68" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">步骤3:我们选取下一个质心，使得选取给定点作为质心的概率与这个给定点和它最近的所选质心之间的距离成比例。换句话说，<em class="mn">一个点离选择的质心越远，它就越有可能被选为下一个质心。</em></li><li id="e670" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">重复步骤2–3，直到拾取K个质心。</li></ul></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="a275" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">#6:如何用Python实现K-Means聚类？</h2><p id="af1a" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在下面的例子中，我将用Python在Iris数据集上实现K-Means聚类。虹膜数据集包括4个特征变量(例如，“萼片长度”、“萼片宽度”、“花瓣长度”、“花瓣宽度”)和1个描述鸢尾花种类的变量(例如，Setosa、Versicolor、Virginica)。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="2955" class="ky kz it ny b gy oc od l oe of">from sklearn.preprocessing import StandardScaler<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import matplotlib.cm as cm<br/>import seaborn as sns<br/>from sklearn.cluster import KMeans<br/>from sklearn.metrics import davies_bouldin_score, silhouette_score, silhouette_samples<br/>import numpy as np</span><span id="e23c" class="ky kz it ny b gy og od l oe of">df = pd.read_csv("<a class="ae oh" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>", names = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species'])<br/># standardize the data to have a mean of zero and a standard deviation of one<br/>df.iloc[:,:4] = StandardScaler().fit_transform(df.iloc[:,:4])<br/># Exploratory Data Analysis<br/>sns.pairplot(df, diag_kind= 'kde')<br/>sns.pairplot(df, hue="Species", diag_kind= 'kde')</span></pre><p id="7a7b" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在图1中，我们可以看到数据点的分离。我们希望在应用K-means聚类之后，能够得到尽可能接近图2的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/eec5d39818674e1ad97858ddb8d9bd8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KjymVHLIzWImOn-nZg77gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:没有标签的原始数据(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/13025b545da3bad7007643d10ed1ae16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WfropeCZ3yQ__1CJbCoLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:带有正确标签的原始数据(图片由作者提供)</p></figure><p id="bcca" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">我们将使用Python中“sklearn”库中的“KMeans”算法。“n_clusters”表示要形成的簇的数量。“max_iter”表示单次运行中执行的最大迭代次数。“n_init”表示K-Means将在不同的起始点集合上运行的次数。init = "random|k-means++ "将指示是使用随机初始化方法还是k-means++初始化。“random_state”用于确保结果是可重复的。</p><p id="9315" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">要计算SSE，我们可以使用"。惯性_ "来自K-Means输出。“戴维斯-波尔丁分数”和“侧影分数”分别用于计算戴维斯-波尔丁指数和侧影分数。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="2b8b" class="ky kz it ny b gy oc od l oe of">x = df.iloc[:,:4]<br/>sse, db, slc = {}, {}, {}<br/>for k in range(2, 10):<br/>    kmeans = KMeans(n_clusters = k, max_iter = 1000, n_init = 10, init = 'k-means++', random_state=123).fit(x)<br/>    clusters = kmeans.labels_<br/>    sse[k] = kmeans.inertia_<br/>    db[k] = davies_bouldin_score(x, clusters)<br/>    slc[k] = silhouette_score(x, clusters)</span></pre><p id="5622" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">肘方法</strong>:在图3中，我们有一个SSE与集群数量的关系图。该图表明，弯头的K值约为3-5。K=5后，上证指数开始缓慢下降。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="b078" class="ky kz it ny b gy oc od l oe of">plt.figure(figsize=(8, 6))<br/>plt.plot(list(sse.keys()), list(sse.values()), marker='o')<br/>plt.xlabel('Number of Clusters', fontsize=24)<br/>plt.ylabel('SSE', fontsize=24)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0e4bfbda4042835a9afe9b3fb83e205b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*WCZiazFF7frhLD7ch5I7fg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3(作者图片)</p></figure><p id="12f5" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">侧影分析:</strong>在图4中，我们有一个侧影指数与聚类数的关系图。该图表明高轮廓指数出现在较低的K值(例如，2，3)。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="c4d1" class="ky kz it ny b gy oc od l oe of">plt.figure(figsize=(8, 6))<br/>plt.plot(list(slc.keys()), list(slc.values()), marker='o')<br/>plt.xlabel('Number of Clusters', fontsize=24)<br/>plt.ylabel('Silhouette Score', fontsize=24)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/fe9c269ff3aaae4f41f666c5f1e8bbd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*XjigUlacgxM6U0B7duVppA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4(作者图片)</p></figure><p id="e179" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">戴维斯-波尔丁指数:</strong>在图5中，我们绘制了戴维斯-波尔丁指数与集群数量的关系图。该图还表明，低戴维斯-波尔丁指数出现在较低的K值(例如2，3)。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="0eb6" class="ky kz it ny b gy oc od l oe of">plt.figure(figsize=(8, 6))<br/>plt.plot(list(db.keys()), list(db.values()), marker='o')<br/>plt.xlabel('Number of Clusters', fontsize=24)<br/>plt.ylabel('Davies-Bouldin Values', fontsize=24)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/00f1d208eb797c6a455f80b855ee0f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*4fHfC7xaNg0PJgtYjo9GGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5(作者图片)</p></figure><p id="7b0a" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated"><strong class="lw iu">侧影图</strong>:我们可以创建另一个信息图来确定K的最佳值，这就是侧影图。它为不同聚类中的所有点绘制轮廓系数。该图包括每个集群的一个刀形。宽度代表每个点的轮廓系数。高度表示给定聚类中的点数。我们可以用下面的标准来选择最佳K值。</p><ul class=""><li id="6d0d" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated">平均轮廓指数高。</li><li id="7a00" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">聚类大致平衡，即聚类具有大致相同的点数。</li><li id="092c" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">大多数点的轮廓系数高于平均轮廓指数。</li></ul><p id="15c6" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在图6中，K=2，3具有相对较高的轮廓指数。但是K=3具有更平衡的集群。所以3更有可能是最优K值。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="0645" class="ky kz it ny b gy oc od l oe of">def make_Silhouette_plot(X, n_clusters):<br/>    plt.xlim([-0.1, 1])<br/>    plt.ylim([0, len(X) + (n_clusters + 1) * 10])</span><span id="b7b2" class="ky kz it ny b gy og od l oe of">clusterer = KMeans(n_clusters=n_clusters, max_iter = 1000, n_init = 10, init = 'k-means++', random_state=10)<br/>    cluster_labels = clusterer.fit_predict(X)<br/>    silhouette_avg = silhouette_score(X, cluster_labels)<br/>    print(<br/>        "For n_clusters =", n_clusters,<br/>        "The average silhouette_score is :", silhouette_avg,<br/>    )</span><span id="040a" class="ky kz it ny b gy og od l oe of"># Compute the silhouette scores for each sample<br/>    sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><span id="c943" class="ky kz it ny b gy og od l oe of">y_lower = 10<br/>    for i in range(n_clusters):<br/>        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]<br/>        ith_cluster_silhouette_values.sort()<br/>        size_cluster_i = ith_cluster_silhouette_values.shape[0]<br/>        y_upper = y_lower + size_cluster_i<br/>        color = cm.nipy_spectral(float(i) / n_clusters)<br/>        plt.fill_betweenx(<br/>            np.arange(y_lower, y_upper),<br/>            0,<br/>            ith_cluster_silhouette_values,<br/>            facecolor=color,<br/>            edgecolor=color,<br/>            alpha=0.7,<br/>        )</span><span id="51c2" class="ky kz it ny b gy og od l oe of">plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))<br/>        y_lower = y_upper + 10</span><span id="e681" class="ky kz it ny b gy og od l oe of">plt.title(f"The Silhouette Plot for n_cluster = {n_clusters}", fontsize=26)<br/>    plt.xlabel("The silhouette coefficient values", fontsize=24)<br/>    plt.ylabel("Cluster label", fontsize=24)<br/>    plt.axvline(x=silhouette_avg, color="red", linestyle="--")<br/>    plt.yticks([])  <br/>    plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])<br/>    <br/>range_n_clusters = [2, 3, 4, 5]<br/>for n_clusters in range_n_clusters:<br/>    make_Silhouette_plot(x, n_clusters)   <br/>    plt.savefig('Silhouette_plot_{}.png'.format(n_clusters))<br/>    plt.close()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/369533317b2a28ec55deaf24200d1f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XZphYktmmcpcYJZnbzQAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6(作者图片)</p></figure><p id="4de0" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">基于所有不同的度量，3似乎是K均值聚类的最佳K值。最后，让我们使用K=3来产生K均值输出。在图7中，考虑到K-Means没有使用任何预先标记的训练数据，预测的聚类看起来相当准确。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="b01c" class="ky kz it ny b gy oc od l oe of">kmeans = KMeans(n_clusters = 3, max_iter = 1000, n_init = 10, init = 'k-means++', random_state=123).fit(x)<br/>clusters = kmeans.labels_<br/>x['K-Means Predicted'] = clusters<br/>sns.pairplot(x, hue="K-Means Predicted", diag_kind= 'kde')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ee8a39e519d4a83efb471138c68d2bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpKPjUoqfKpE2W6OWMwsQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7(作者图片)</p></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="cdc5" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"># 7:K均值的优缺点是什么？</h2><p id="bbd4" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">K-Means是最常用的聚类算法，因为它非常容易实现和解释。只有一个超参数(K值)需要优化。它是一个有效的工具，可以应用于几乎所有不同的数据类型。</p><p id="fd00" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">然而，K-Means有一些明显的缺点。它假设</p><ul class=""><li id="5f5e" class="nb nc it lw b lx mo ma mp lh nd ll ne lp nf mm ng nh ni nj bi translated">不同的簇有不同的质心，它们彼此相距很远。</li><li id="6e1e" class="nb nc it lw b lx np ma nq lh nr ll ns lp nt mm ng nh ni nj bi translated">如果点(A)比点(B)离给定质心更远，则点(A)比点(B)更不可能属于该给定聚类</li></ul><p id="4778" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在图8的第一个例子中，内圈应该属于一个集群，外圈应该属于另一个集群。但是K-Means不能正确地聚类这些点，因为它不能克服质心重叠的问题。</p><p id="1dba" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">在第二个例子中，两个半圆应该属于两个不同的聚类，K-Means再次未能识别出明显的模式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/42120d254d5c98151fbed729499b7102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_JYWVYXvKw1v1TmiQLDgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8(作者图片)</p></figure><p id="58d7" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">现实生活中的数据几乎总是复杂的，因为它们由噪声和异常组成。虽然K-Means聚类是一个强大的工具，但我们也应该意识到它的局限性。</p><h1 id="5b77" class="oo kz it bd la op oq or ld os ot ou lg jz ov ka lk kc ow kd lo kf ox kg ls oy bi translated">感谢您的阅读！！！</h1><p id="a742" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">如果你喜欢这篇文章，并且想<strong class="lw iu">请我喝杯咖啡，</strong>请<a class="ae oh" href="https://ko-fi.com/aaronzhu" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</p><p id="bd9a" class="pw-post-body-paragraph lu lv it lw b lx mo ju lz ma mp jx mc lh mq me mf ll mr mh mi lp ms mk ml mm im bi translated">您可以注册一个<a class="ae oh" href="https://aaron-zhu.medium.com/membership" rel="noopener"> <strong class="lw iu">会员</strong> </a>来解锁我的文章的全部访问权限，并且可以无限制地访问介质上的所有内容。如果你想在我发表新文章时收到电子邮件通知，请订阅。</p></div></div>    
</body>
</html>