<html>
<head>
<title>How to Fix Can't assign requested address: Service 'sparkDriver' failed after 16 retries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何修复无法分配请求的地址:服务“sparkDriver”在16次重试后失败</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-fix-cant-assign-driver-32406580375#2022-04-08">https://towardsdatascience.com/spark-fix-cant-assign-driver-32406580375#2022-04-08</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="a0cf" class="pw-subtitle-paragraph jv it iu bd b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km dk translated">了解如何解决最常见的PySpark问题之一</h2></div><figure class="ko kp kq kr gu ks gi gj paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gi gj kn"><img src="../Images/0604ae9c6b51435a5f275d491e7decd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kdRrAtHy3D8c3W76v4219g.jpeg"/></div></div><p class="kz la gk gi gj lb lc bd b be z dk translated">艾蒂安·吉拉尔代在<a class="ae ld" href="https://unsplash.com/s/photos/error?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="69c2" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">介绍</h2><p id="26a7" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">当涉及到配置时，Spark networking有时会变得相当具有挑战性，有时可能会出现一些奇怪的错误。我个人看到的最常见的错误之一是<code class="fe jr js jt ju b">Service 'sparkDriver' failed after 16 retries</code>错误。错误的完整追溯分享如下。</p><pre class="ko kp kq kr gu mt ju mu mv aw mw bi"><span id="56a6" class="le lf iu ju b gz mx my l mz na">Exception in thread "main" java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.<br/>    at sun.nio.ch.Net.bind0(Native Method)<br/>    at sun.nio.ch.Net.bind(Net.java:433)<br/>    at sun.nio.ch.Net.bind(Net.java:425)<br/>    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)<br/>    at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:127)<br/>    at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:501)<br/>    at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1218)<br/>    at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:496)<br/>    at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:481)<br/>    at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:965)<br/>    at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:210)<br/>    at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:353)<br/>    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)<br/>    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:446)<br/>    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)<br/>    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)<br/>    at java.lang.Thread.run(Thread.java:745)<br/><br/>Process finished with exit code 1</span></pre><p id="7ec2" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">在今天的简短教程中，我们将探索一些潜在的解决方法，最终可以帮助您处理这个错误。</p></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><h2 id="0571" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">设置SPARK_LOCAL_IP环境变量</h2><p id="ac01" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">Spark使用驱动程序中指定的配置参数或作为位于安装目录中的<code class="fe jr js jt ju b">conf/spark-env.sh</code>(或<code class="fe jr js jt ju b">conf/spark-env.cmd</code>)脚本的一部分加载的环境变量，确定JVM如何在工作节点上初始化。</p><p id="a876" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">在运行本地Spark应用程序或提交脚本时，也会用到这个脚本文件。注意安装Spark时<code class="fe jr js jt ju b">conf/spark-env.sh</code>默认不存在。然而，GitHub上有一个名为<code class="fe jr js jt ju b"><a class="ae ld" href="https://github.com/apache/spark/blob/master/conf/spark-env.sh.template" rel="noopener ugc nofollow" target="_blank">conf/spark-env.sh.template</a></code>的模板文件，你可以用它作为起点。您还必须确保该文件是可执行的。</p><p id="1695" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated"><code class="fe jr js jt ju b">SPARK_LOCAL_IP</code>环境变量对应于要绑定到的机器的IP地址。因此，您可以通过向<code class="fe jr js jt ju b">conf/spark-env.sh</code>脚本添加以下命令来指定Spark在该节点上绑定的IP地址</p><pre class="ko kp kq kr gu mt ju mu mv aw mw bi"><span id="98e0" class="le lf iu ju b gz mx my l mz na">export SPARK_LOCAL_IP="127.0.0.1"</span></pre><p id="c2d5" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">或者，由于<code class="fe jr js jt ju b">conf/spark-env.sh</code>是一个shell脚本，您可以通过查找特定网络接口的IP来编程计算<code class="fe jr js jt ju b">SPARK_LOCAL_IP</code>，而不是将其硬编码为一个特定的值。</p></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><h2 id="a71c" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">配置spark.driver.bindAddress</h2><p id="9f7e" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">如前一节所述，Spark还可以使用驱动程序中指定的配置来确定JVM如何在工作节点上初始化。</p><p id="876f" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">换句话说，通过为<code class="fe jr js jt ju b">spark.driver.bindAddress</code>提供一个配置，您可以在初始化<code class="fe jr js jt ju b">SparkSession</code>时以编程方式设置绑定监听套接字的主机名或IP地址。</p><p id="4301" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">但是请注意，这种方法将覆盖<code class="fe jr js jt ju b">SPARK_LOCAL_IP</code>环境变量的值，如果它已经在我们在上一节描述的<code class="fe jr js jt ju b">conf/spark-env.sh</code>脚本中导出的话。</p><blockquote class="nn no np"><p id="69ef" class="ma mb nq mc b md nb jz mf mg nc kc mi nr nd mk ml ns ne mn mo nt nf mq mr ms in bi translated"><code class="fe jr js jt ju b"><em class="iu">spark.driver.bindAddress</em></code>:绑定监听套接字的主机名或IP地址。这个配置覆盖了<code class="fe jr js jt ju b"><em class="iu">SPARK_LOCAL_IP</em></code>环境变量。</p><p id="abaf" class="ma mb nq mc b md nb jz mf mg nc kc mi nr nd mk ml ns ne mn mo nt nf mq mr ms in bi translated">它还允许将与本地地址不同的地址公布给执行器或外部系统。例如，当运行具有桥接网络的容器时，这是有用的。为了正常工作，驱动程序使用的不同端口(RPC、块管理器和UI)需要从容器的主机转发。</p><p id="01f5" class="ma mb nq mc b md nb jz mf mg nc kc mi nr nd mk ml ns ne mn mo nt nf mq mr ms in bi translated">—来源:<a class="ae ld" href="https://spark.apache.org/docs/latest/configuration.html" rel="noopener ugc nofollow" target="_blank">火花文件</a></p></blockquote><p id="86b4" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">在Scala中，您可以配置IP地址来绑定监听套接字，如下所示。</p><pre class="ko kp kq kr gu mt ju mu mv aw mw bi"><span id="e3f7" class="le lf iu ju b gz mx my l mz na">import org.apache.spark.sql.SparkSession</span><span id="e26b" class="le lf iu ju b gz nu my l mz na">val spark: SparkSession = SparkSession.builder() \<br/>    .appName("Test application") \<br/>    .master("local[*]") \<br/>    .config("spark.driver.bindAddress", "127.0.0.1") \<br/>    .getOrCreate()</span></pre><p id="8c66" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">PySpark中的等效表达式是</p><pre class="ko kp kq kr gu mt ju mu mv aw mw bi"><span id="5cb8" class="le lf iu ju b gz mx my l mz na">from pyspark.sql import SparkSession</span><span id="8ffc" class="le lf iu ju b gz nu my l mz na">spark = SparkSession.builder \<br/>    .appName("Test application") \    <br/>    .master("local[1]") \<br/>    .config("spark.driver.bindAddress", "127.0.0.1") \<br/>    .getOrCreate()</span></pre></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><h2 id="1fac" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">在本地机器上设置主机名</h2><p id="824f" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">第三个选项是通过命令行在本地机器上手动指定<code class="fe jr js jt ju b">hostname</code>。主机名是作为计算机或服务器名称的唯一标识符。</p><p id="6d03" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">可以使用以下命令调整该名称:</p><pre class="ko kp kq kr gu mt ju mu mv aw mw bi"><span id="5afa" class="le lf iu ju b gz mx my l mz na">sudo hostname -s 127.0.0.1</span></pre></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><h2 id="a277" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">检查您的VPN连接</h2><p id="3d6f" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">最后，如果您在本地机器上运行Spark，另一种可能性是您的虚拟专用网络可能会影响绑定监听套接字的主机名或IP地址。</p><p id="2889" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">如果是这种情况，您可以通过简单地断开VPN或禁用任何其他可能影响网络的工具来解决这个问题。</p></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><h2 id="78f3" class="le lf iu bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph ma mb iu mc b md me jz mf mg mh kc mi ln mj mk ml lr mm mn mo lv mp mq mr ms in bi translated">在今天的简短教程中，我们讨论了Spark中一个常见的报告错误，即与Spark网络相关的<code class="fe jr js jt ju b">java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries</code>。</p><p id="6821" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">我们展示了可以采取哪些措施来解决这个问题。总而言之，这可以通过以下方式实现</p><ul class=""><li id="ac0c" class="nv nw iu mc b md nb mg nc ln nx lr ny lv nz ms oa ob oc od bi translated">导出在工作节点上初始化JVM时加载的相应的<code class="fe jr js jt ju b">SPARK_LOCAL_IP</code>环境变量</li><li id="353d" class="nv nw iu mc b md oe mg of ln og lr oh lv oi ms oa ob oc od bi translated">在<code class="fe jr js jt ju b">SparkSession</code>中设置相应的<code class="fe jr js jt ju b">spark.driver.bindAddress</code>配置(注意这种方法将覆盖<code class="fe jr js jt ju b">SPARK_LOCAL_IP</code>环境变量)</li><li id="97dd" class="nv nw iu mc b md oe mg of ln og lr oh lv oi ms oa ob oc od bi translated">更新本地机器上的<code class="fe jr js jt ju b">hostname</code></li><li id="6a58" class="nv nw iu mc b md oe mg of ln og lr oh lv oi ms oa ob oc od bi translated">检查您是否启用了虚拟专用网络(VPN)或任何其他可能影响本地计算机联网的工具，因为这有时可能会影响绑定地址</li></ul><p id="c581" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated">一旦应用了本文中描述的任何解决方案，您最终应该能够顺利运行Spark应用程序。</p></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated"><a class="ae ld" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="mc iv">成为会员</strong> </a> <strong class="mc iv">阅读媒体上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p><div class="oj ok gq gs ol om"><a href="https://gmyrianthous.medium.com/membership" rel="noopener follow" target="_blank"><div class="on ab fp"><div class="oo ab op cl cj oq"><h2 class="bd iv gz z fq or fs ft os fv fx it bi translated">通过我的推荐链接加入Medium-Giorgos Myrianthous</h2><div class="ot l"><h3 class="bd b gz z fq or fs ft os fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ou l"><p class="bd b dl z fq or fs ft os fv fx dk translated">gmyrianthous.medium.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa kx om"/></div></div></a></div></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph ma mb iu mc b md nb jz mf mg nc kc mi ln nd mk ml lr ne mn mo lv nf mq mr ms in bi translated"><strong class="mc iv">相关文章你可能也喜欢</strong></p><div class="oj ok gq gs ol om"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"><div class="on ab fp"><div class="oo ab op cl cj oq"><h2 class="bd iv gz z fq or fs ft os fv fx it bi translated">加快PySpark和Pandas数据帧之间的转换</h2><div class="ot l"><h3 class="bd b gz z fq or fs ft os fv fx dk translated">将大火花数据帧转换为熊猫时节省时间</h3></div><div class="ou l"><p class="bd b dl z fq or fs ft os fv fx dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa kx om"/></div></div></a></div></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><div class="ko kp kq kr gu om"><a rel="noopener follow" target="_blank" href="/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a"><div class="on ab fp"><div class="oo ab op cl cj oq"><h2 class="bd iv gz z fq or fs ft os fv fx it bi translated">spark session vs spark context vs SQLContext vs hive context</h2><div class="ot l"><h3 class="bd b gz z fq or fs ft os fv fx dk translated">SparkSession、SparkContext HiveContext和SQLContext有什么区别？</h3></div><div class="ou l"><p class="bd b dl z fq or fs ft os fv fx dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pc l ox oy oz ov pa kx om"/></div></div></a></div></div><div class="ab cl ng nh hy ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="in io ip iq ir"><div class="ko kp kq kr gu om"><a rel="noopener follow" target="_blank" href="/apache-spark-3-0-the-five-most-exciting-new-features-99c771a1f512"><div class="on ab fp"><div class="oo ab op cl cj oq"><h2 class="bd iv gz z fq or fs ft os fv fx it bi translated">Apache Spark 3.0:5个最激动人心的新特性</h2><div class="ot l"><h3 class="bd b gz z fq or fs ft os fv fx dk translated">Apache Spark 3.0新版本中最激动人心的5个特性</h3></div><div class="ou l"><p class="bd b dl z fq or fs ft os fv fx dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pd l ox oy oz ov pa kx om"/></div></div></a></div></div></div>    
</body>
</html>