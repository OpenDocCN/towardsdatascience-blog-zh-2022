<html>
<head>
<title>Reinforcement Learning for Inventory Optimization Series II: An RL Model for A Multi-Echelon Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">库存优化的强化学习系列 II:多级网络的强化学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00#2022-12-30">https://towardsdatascience.com/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00#2022-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="86f2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">建立近似策略优化(PPO)模型来优化多级供应链网络的库存运作</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1d211c24856f49da9ea5ba86207733d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pisX04SEqtugLmMZ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">纳斯蒂亚·杜尔希尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="4255" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更新:本文是我的博客系列<em class="ls">库存优化的强化学习</em>的第二篇文章。下面是同一系列中其他文章的链接。如果你感兴趣，请去看看。</p><p id="54b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="noopener"> <em class="ls">库存优化的强化学习系列 I:单一零售商的 RL 模型</em> </a></p><p id="28bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d" rel="noopener"> <em class="ls">库存优化强化学习系列之三:RL 模型的虚拟到真实传递</em> </a></p><p id="4f7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我的<a class="ae kv" href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="noopener">上一篇文章</a>中，我提出了一个优化单个零售商库存控制策略的 DQN 模型，其客户需求模式可以用混合正态分布来描述。与传统的(<em class="ls"> s </em>，<em class="ls"> S </em>)库存控制策略相比，DQN 模型取得了明显更好的结果。在这篇文章中，正如我在上一篇文章的结尾提到的，我将在库存优化的 RL 方面更进一步，并专注于为更复杂的供应链网络(多级网络)构建 RL 模型。</p><h1 id="7895" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">多级网络 RL 建模的挑战</h1><p id="0459" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">毫无疑问，为多级网络构建 RL 模型远比单个零售商复杂。复杂之处在于供应链中实体的增加导致了可伸缩性问题。在前一篇文章的单个零售商的例子中，我们只关心一个实体——零售商的库存控制策略。因此，只需要建立一个代理，其动作空间表示零售商的订货量。然而，多级网络是一个更复杂的系统，通常涉及不同类型的实体(例如，制造厂、配送中心、零售商等)。).</p><p id="824b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了建立这样一个系统的模型，我们有两种可能的方法。第一种方法是将每个实体建模为独立的代理，并建立多代理 RL 模型。在这种方法中，每个代理只关心网络中一个实体的动作，这就限制了动作空间的大小。然而，与单代理 RL 模型相比，多代理 RL 模型通常更难训练和调整，因为整个模型的成功取决于每个代理的良好训练和调整，以及代理之间的合作。第二种方法是使用单个代理对整个网络进行建模，其动作空间足够灵活，可以同时描述所有实体的排序决策，例如，一个<em class="ls"> n </em>维动作空间，每个维度对应于每个实体的排序决策。然而，这种方法的缺点是随着实体数量的增加，动作空间的大小急剧增加。动作空间的大小随着实体的数量呈指数增长。</p><h1 id="6709" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">简单多级网络的 RL 模型</h1><p id="c157" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们从<a class="ae kv" href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="noopener">上一篇文章</a>中的例子开始。假设有一家零售公司，在全国不同地区开设零售店，销售可乐盈利。在<a class="ae kv" href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="noopener">前一篇文章</a>中，我们没有关注单个零售店的库存运作，而是假设零售公司拥有自己的配送中心(DC)来满足其零售店的需求，我们寻求优化由配送中心和零售店组成的整个网络的库存控制策略。</p><p id="01db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使例子更简单，我们考虑一个仅由两个配送中心和四个零售商组成的多级网络。下图描述了配送中心和零售商之间的运输联系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/b46bb4bf19abaec68fa015ae383da2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VRytjtF4hc9MAP0sqXxN-Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正在调查的简单多级供应链网络(图片由作者提供)</p></figure><p id="6218" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，零售公司首先从供应商那里购买可乐，然后将可乐运输到 DC 1 号和 DC 2 号。DC 1 存储库存以满足零售商 1 和零售商 2 的订单，DC 2 存储库存以满足零售商 3 和零售商 4 的订单。在本例中，让我们进一步假设所有四家零售商都有与<a class="ae kv" href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="noopener">上一篇文章</a>中相同的客户需求模式。具体来说，所有四个零售商的客户需求都遵循混合正态分布，其中周一至周四的需求遵循具有最低均值的正态分布，周五的需求遵循具有中等均值的正态分布，周六至周日的需求遵循具有最高均值的正态分布。</p><p id="da9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个特殊的问题，需要注意的是整个网络可以分解成两个子网络，一个由 DC 1、零售商 1 和零售商 2 组成，另一个由 DC 2、零售商 3 和零售商 4 组成。这两个子网络具有相同的客户需求分布。因此，为了使训练过程更容易，我们可以在这里采用分而治之的方法。我们不是对整个网络建模，而是仅通过一个 RL 代理对子网络建模，并且我们依次使用来自第一和第二子网络的数据来训练 RL 代理。RL 模型的状态、动作和奖励定义如下:</p><ol class=""><li id="a52d" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">State: ( <em class="ls"> i_pt_dc1 </em>，<em class="ls"> i_pt_r1，i_pt_r2，dow_t </em>)，其中<em class="ls"> i_pt_dc1 </em>为 DC 1 <em class="ls"> </em>在<em class="ls"> t </em>日结束时的库存位置<em class="ls">，<em class="ls"> i_pt_r1 </em>为零售商 1 的库存位置，<em class="ls"> i_pt_r2 </em>为</em></li><li id="6e0d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">动作:(<em class="ls"> a_t_dc1 </em>，<em class="ls"> a_t_r1 </em>，<em class="ls"> a_t_r2 </em>)，其中<em class="ls"> a_t_dc1 </em>是 DC 1 <em class="ls"> </em>在<em class="ls"> </em>第<em class="ls"> t </em>天结束时的订单数量，<em class="ls"> a_t_r1 </em>是零售商 1 在<em class="ls"> </em>时的订单数量注意，如果<em class="ls"> a_t_dc1 </em>、<em class="ls"> a_t_r1 </em>或<em class="ls"> a_t_r2 </em>为 0 <em class="ls">、</em>那么我们当时不下单对应的实体。行动空间受到最大订货量的限制，最大订货量由供应商或运输车辆的容量决定。</li><li id="c76b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">奖励:<em class="ls">r _ t = r _ t _ R1+r _ t _ R2-h _ t _ R1-h _ t _ R2-h _ t _ dc1-f _ t _ R1-f _ t _ R2-f _ t _ dc1-v _ t _ dc1，</em>其中<em class="ls"> r_t_r1 </em>和<em class="ls"> r_t_r2 </em>为第(<em class="ls">t+1)</em><em class="ls">日白天在零售商 1 和 2 处销售产品所获得的收益 <em class="ls"> </em>零售商 1、2 和 DC 1 在第<em class="ls"> t </em>个决策时期产生的配送费用，以及<em class="ls"> v_t_dc1 i </em> s 在第<em class="ls"> t </em>天结束时在 DC 1 <em class="ls"> </em>产生的可变订购成本(供应商收取的产品购买成本)。 因为我们假设可变订购成本仅指购买成本，我们只需要在 DC 1 计算这个成本。很容易看出，报酬<em class="ls"> r_t </em>就是在第<em class="ls"> t </em>个决策历元从 DC 1、零售商 1 和零售商 2 组成的子网络中获得的利润。</em></li></ol><p id="403f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，这个建模框架遵循上一节提到的第二种建模方法，它可能会遭受维数灾难。如果我们假设每个实体的动作取从 0 到<em class="ls"> a_max </em>(最大订货量)的离散值，那么动作空间的大小随着网络中实体的数量呈指数增长。这给有效训练 RL 代理造成了困难。缓解这个问题的一种方法是将一个动作可以采取的值限制在[0，<em class="ls">a _ max</em><em class="ls"/>区间<em class="ls">内。</em>例如<em class="ls">，</em>如果<em class="ls"> a_max </em> = 20 <em class="ls">，</em>我们可以限制动作只能取值 0、5、15 或 20，而不是允许它取 0 到 20 之间的每个整数。这可能在某种程度上破坏从 RL 代理获得的结果策略的最优性，但是它可以显著地减小动作空间的大小。</p><h1 id="885f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数值实验</h1><p id="c88e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">假设有一家小型零售公司向其客户销售可乐。该零售公司有两个配送中心和四个零售商来满足客户需求。每当该公司想在任何 DC 或零售商那里补充库存时，该公司必须订购整数箱可乐(一箱装 24 罐)。假设对于零售商，可乐的单位售价为每箱 30 美元，持有成本为每箱每晚 3 美元，固定订购成本为每订单 50 美元，库存能力为 50 箱，每订单允许的最大订购量为 20 箱，在一个周日结束时初始库存为 25 箱，提前期为 2 天。周一至周四的需求服从正态分布<em class="ls"> N </em> (3，1.5)，周五的需求服从正态分布<em class="ls"> N </em> (6，1)，周六至周日的需求服从正态分布<em class="ls"> N </em> (12，2)。对于配送中心，每晚每箱的持有成本为 1 美元，固定订购成本为每订单 75 美元，库存能力为 200 箱，每订单允许的最大订单数量为 100 箱，周日结束时的初始库存为 100 箱，提前期为 5 天。我们从混合分布中生成 52 <em class="ls"> </em>周的历史需求<em class="ls"> </em>样本，并将其用作 RL 模型的训练数据集。</p><p id="26ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于要使用的特定 RL 算法的选择，我采用了近似策略优化(PPO)算法，因为它是一种最先进的基于策略的 RL 算法，能够给出随机策略。我还尝试使用基于值的算法来生成确定性策略(例如，DQN)，但发现 PPO 对于这个特殊的例子更有效。这里，我省略了对 PPO 算法的详细解释，因为这不是本文的重点。感兴趣的读者可以参考<a class="ae kv" href="https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12" rel="noopener">这篇文章</a>了解更多细节。</p><p id="eb3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为基准，我们将使用用于训练 PPO 模型的相同数据集来优化经典(<em class="ls"> s </em>，<em class="ls"> S </em>)库存控制策略，并在测试集中将其性能与 PPO 进行比较。</p><h2 id="3ea3" class="nf lu iq bd lv ng nh dn lz ni nj dp md lf nk nl mf lj nm nn mh ln no np mj nq bi translated">PPO 模型的定型代码</h2><p id="22a9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">首先，我们生成包含四家零售商 52 周历史需求记录的训练数据集。请注意，非整数需求数据会四舍五入为最接近的整数。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="ebf1" class="nw lu iq ns b be nx ny l nz oa">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import random<br/>np.random.seed(0)<br/>demand_hist_list = []<br/>for k in range(4):<br/>    demand_hist = []<br/>    for i in range(52):<br/>        for j in range(4):<br/>            random_demand = np.random.normal(3, 1.5)<br/>            if random_demand &lt; 0:<br/>                random_demand = 0<br/>            random_demand = np.round(random_demand)<br/>            demand_hist.append(random_demand)<br/>        random_demand = np.random.normal(6, 1)<br/>        if random_demand &lt; 0:<br/>            random_demand = 0<br/>        random_demand = np.round(random_demand)<br/>        demand_hist.append(random_demand)<br/>        for j in range(2):<br/>            random_demand = np.random.normal(12, 2)<br/>            if random_demand &lt; 0:<br/>                random_demand = 0<br/>            random_demand = np.round(random_demand)<br/>            demand_hist.append(random_demand)<br/>    demand_hist_list.append(demand_hist)</span></pre><p id="201a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们定义了 PPO 代理交互的库存优化问题的环境。该环境包含一个有一个 DC 和两个零售商的子网络。在本例中，零售商不考虑延期交货，因为我们假设如果顾客在零售店没有看到任何剩余的可乐，他们会去其他商店购买可乐。但是，DC 会考虑延期交货。当 DC 没有足够的库存来满足零售商的订单时，就会发生延期交货，而 DC 会在补充库存后尽快满足延期交货。为了限制行动空间的大小，我们允许 DC 的订货量取值为[0，10，20，30，40，50，60，70，80，90，100]，两个零售商取值为[0，5，10，15，20]。动作空间的大小现在是 11*5*5 = 275。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="fc7e" class="nw lu iq ns b be nx ny l nz oa">import itertools<br/>action_lists = [[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],[0, 5, 10, 15, 20],[0, 5, 10, 15, 20]]<br/>action_map = [x for x in itertools.product(*action_lists)]<br/><br/>class Retailer():<br/>    def __init__(self, demand_records):<br/>        self.inv_level = 25<br/>        self.inv_pos = 25<br/>        self.order_quantity_limit = 20<br/>        self.holding_cost = 3<br/>        self.lead_time = 2<br/>        self.order_arrival_list = []<br/>        self.backorder_quantity = 0<br/>        self.capacity = 50<br/>        self.demand_list = demand_records<br/>        self.unit_price = 30<br/>        self.fixed_order_cost = 50<br/>    <br/>    def reset(self):<br/>        self.inv_level = 25<br/>        self.inv_pos = 25<br/>        self.order_arrival_list = []<br/>        self.backorder_quantity = 0<br/>        <br/>    def order_arrival(self, current_period):<br/>        n_orders = 0<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            index_list = []<br/>            for j in range(len(self.order_arrival_list)):<br/>                if current_period == self.order_arrival_list[j][0]:<br/>                    self.inv_level = min(self.capacity, self.inv_level + self.order_arrival_list[j][1])<br/>                    n_orders += 1<br/>                    index_list.append(j)<br/>            self.order_arrival_list =  [e for i, e in enumerate(self.order_arrival_list) if i not in index_list]<br/>        holding_cost_total = self.inv_level*self.holding_cost<br/>        return n_orders, holding_cost_total<br/>    <br/>    def satisfy_demand(self, demand):<br/>        units_sold = min(demand, self.inv_level)<br/>        self.inv_level = max(0,self.inv_level-demand)<br/>        self.inv_pos = self.inv_level<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            for j in range(len(self.order_arrival_list)):<br/>                self.inv_pos += self.order_arrival_list[j][1]<br/>        revenue = units_sold*self.unit_price<br/>        return revenue<br/><br/>class DistributionCenter():<br/>    def __init__(self):<br/>        self.inv_level = 100<br/>        self.inv_pos = 100<br/>        self.order_quantity_limit = 100<br/>        self.holding_cost = 1<br/>        self.lead_time = 5<br/>        self.order_arrival_list = []<br/>        self.capacity = 200<br/>        self.fixed_order_cost = 75<br/>    <br/>    def reset(self):<br/>        self.inv_level = 100<br/>        self.inv_pos = 100<br/>        self.order_arrival_list = []<br/>        <br/>    def place_order(self, order_quantity, current_period):<br/>        if order_quantity &gt; 0:<br/>            self.order_arrival_list.append([current_period+self.lead_time, order_quantity])<br/>            <br/>    def order_arrival(self, retailers, current_period):<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            if current_period == self.order_arrival_list[0][0]:<br/>                self.inv_level = min(self.capacity, self.inv_level+self.order_arrival_list[0][1])<br/>                self.order_arrival_list.pop(0)<br/>        holding_cost_total = self.inv_level*self.holding_cost<br/>        return holding_cost_total<br/>        <br/>    def satisfy_demand(self, retailers, actions, current_period):<br/>        quantity_satisfied = [0,0]<br/>        total_backorder = np.sum([retailer.backorder_quantity for retailer in retailers])<br/>        if total_backorder &gt; 0:<br/>            if self.inv_level &lt;= retailers[0].backorder_quantity:<br/>                retailers[0].backorder_quantity -= self.inv_level<br/>                quantity_satisfied[0] += self.inv_level<br/>                self.inv_level = 0<br/>            if self.inv_level &gt; retailers[0].backorder_quantity and self.inv_level &lt;= total_backorder:<br/>                if retailers[0].backorder_quantity == 0:<br/>                    retailers[1].backorder_quantity -= self.inv_level<br/>                    quantity_satisfied[1] += self.inv_level<br/>                else:<br/>                    quantity_left = self.inv_level - retailers[0].backorder_quantity<br/>                    quantity_satisfied[0] += retailers[0].backorder_quantity<br/>                    retailers[0].backorder_quantity = 0<br/>                    quantity_satisfied[1] += quantity_left<br/>                    retailers[1].backorder_quantity -= quantity_left<br/>                self.inv_level = 0<br/>            if self.inv_level &gt; total_backorder:<br/>                if retailers[0].backorder_quantity == 0 and retailers[1].backorder_quantity != 0:<br/>                    quantity_satisfied[1] += retailers[1].backorder_quantity<br/>                    retailers[1].backorder_quantity = 0<br/>                if retailers[0].backorder_quantity != 0 and retailers[1].backorder_quantity == 0:<br/>                    quantity_satisfied[0] += retailers[0].backorder_quantity<br/>                    retailers[0].backorder_quantity = 0<br/>                if retailers[0].backorder_quantity != 0 and retailers[1].backorder_quantity != 0:<br/>                    quantity_satisfied[0] += retailers[0].backorder_quantity<br/>                    quantity_satisfied[1] += retailers[1].backorder_quantity<br/>                    retailers[0].backorder_quantity = 0<br/>                    retailers[1].backorder_quantity = 0<br/>                self.inv_level -= total_backorder<br/>                        <br/>        if self.inv_level &gt; 0:<br/>            if self.inv_level &lt;= actions[0]:<br/>                quantity_satisfied[0] += self.inv_level<br/>                retailers[0].backorder_quantity += actions[0] - self.inv_level<br/>                self.inv_level = 0    <br/>            if self.inv_level &gt; actions[0] and self.inv_level &lt;= np.sum(actions):<br/>                if actions[0] == 0:<br/>                    quantity_satisfied[1] += self.inv_level<br/>                    retailers[1].backorder_quantity += actions[1] - self.inv_level<br/>                else:<br/>                    inv_left = self.inv_level-actions[0]<br/>                    quantity_satisfied[0] += actions[0]<br/>                    quantity_satisfied[1] += inv_left<br/>                    retailers[1].backorder_quantity += actions[1] - inv_left<br/>                self.inv_level = 0<br/>            if self.inv_level &gt; np.sum(actions): <br/>                if actions[0] == 0 and actions[1] != 0:<br/>                    quantity_satisfied[1] += actions[1]<br/>                if actions[0] != 0 and actions[1] == 0:<br/>                    quantity_satisfied[0] += actions[0]<br/>                if actions[0] != 0 and actions[1] != 0:    <br/>                    quantity_satisfied[0] += actions[0]<br/>                    quantity_satisfied[1] += actions[1]<br/>                self.inv_level -= np.sum(actions)   <br/>        else:<br/>            retailers[0].backorder_quantity += actions[0]<br/>            retailers[1].backorder_quantity += actions[1]  <br/>        <br/>        for i in range(len(retailers)):<br/>            quantity_left = quantity_satisfied[i]<br/>            while quantity_left &gt; 0:<br/>                if quantity_left &gt; retailers[i].order_quantity_limit:<br/>                    retailers[i].order_arrival_list.append([current_period+retailers[i].lead_time, retailers[i].order_quantity_limit])<br/>                    quantity_left -= retailers[i].order_quantity_limit<br/>                else:<br/>                    retailers[i].order_arrival_list.append([current_period+retailers[i].lead_time, quantity_left])<br/>                    quantity_left = 0<br/>                         <br/>        self.inv_pos = self.inv_level<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            for j in range(len(self.order_arrival_list)):<br/>                self.inv_pos += self.order_arrival_list[j][1]<br/>        for retailer in retailers:<br/>            self.inv_pos -= retailer.backorder_quantity<br/><br/><br/>class MultiEchelonInvOptEnv():<br/>    def __init__(self, demand_records):<br/>        self.n_retailers = 2<br/>        self.n_DCs = 1<br/>        self.retailers = []<br/>        for i in range(self.n_retailers):<br/>            self.retailers.append(Retailer(demand_records[i]))<br/>        self.DCs = []<br/>        for i in range(self.n_DCs):<br/>            self.DCs.append(DistributionCenter()) <br/>        self.n_period = len(demand_records[0])<br/>        self.current_period = 1<br/>        self.day_of_week = 0<br/>        self.state = np.array([DC.inv_pos for DC in self.DCs] + [retailer.inv_pos for retailer in self.retailers] + \<br/>                              self.convert_day_of_week(self.day_of_week))<br/>        self.variable_order_cost = 10<br/>        self.demand_records = demand_records<br/>        <br/>    def reset(self):<br/>        for retailer in self.retailers:<br/>            retailer.reset()<br/>        for DC in self.DCs:<br/>            DC.reset()<br/>        self.current_period = 1<br/>        self.day_of_week = 0 <br/>        self.state = np.array([DC.inv_pos for DC in self.DCs] + [retailer.inv_pos for retailer in self.retailers] + \<br/>                              self.convert_day_of_week(self.day_of_week))<br/>        return self.state<br/>    <br/>    def step(self, action):<br/>        action_modified = action_map[action]<br/>        y_list = []<br/>        for i in range(self.n_DCs):<br/>            y = 1 if action_modified[i] &gt; 0 else 0    <br/>            y_list.append(y)<br/>        for DC,order_quantity in zip(self.DCs,action_modified[:self.n_DCs]):<br/>            DC.place_order(order_quantity,self.current_period)<br/>        sum_holding_cost_DC = 0<br/>        for i in range(self.n_DCs):<br/>            holding_cost_total = self.DCs[i].order_arrival(self.retailers,self.current_period)<br/>            sum_holding_cost_DC += holding_cost_total<br/>            self.DCs[i].satisfy_demand(self.retailers,action_modified[i*2+1:i*2+3],self.current_period)<br/>        sum_n_orders = 0<br/>        sum_holding_cost_retailer = 0<br/>        sum_revenue = 0<br/>        for retailer,demand in zip(self.retailers,self.demand_records):<br/>            n_orders, holding_cost_total = retailer.order_arrival(self.current_period)<br/>            sum_n_orders += n_orders<br/>            sum_holding_cost_retailer += holding_cost_total<br/>            revenue = retailer.satisfy_demand(demand[self.current_period-1])<br/>            sum_revenue += revenue    <br/>        reward = sum_revenue - sum_holding_cost_retailer - sum_holding_cost_DC - sum_n_orders*self.retailers[0].fixed_order_cost - \<br/>                 np.sum(y_list)*self.DCs[0].fixed_order_cost - np.sum(action_modified[:self.n_DCs])*self.variable_order_cost<br/><br/>        self.day_of_week = (self.day_of_week+1)%7<br/>        self.state = np.array([DC.inv_pos for DC in self.DCs] + [retailer.inv_pos for retailer in self.retailers] + \<br/>                              self.convert_day_of_week(self.day_of_week))<br/>        self.current_period += 1<br/>        if self.current_period &gt; self.n_period:<br/>            terminate = True<br/>        else: <br/>            terminate = False<br/>        return self.state, reward, terminate<br/>    <br/>    def convert_day_of_week(self,d):<br/>        if d == 0:<br/>            return [0, 0, 0, 0, 0, 0]<br/>        if d == 1:<br/>            return [1, 0, 0, 0, 0, 0] <br/>        if d == 2:<br/>            return [0, 1, 0, 0, 0, 0] <br/>        if d == 3:<br/>            return [0, 0, 1, 0, 0, 0] <br/>        if d == 4:<br/>            return [0, 0, 0, 1, 0, 0] <br/>        if d == 5:<br/>            return [0, 0, 0, 0, 1, 0] <br/>        if d == 6:<br/>            return [0, 0, 0, 0, 0, 1] </span></pre><p id="fe9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们开始用 PyTorch 构建 PPO 模型。这部分 PPO 的实现是基于<a class="ae kv" href="https://github.com/nikhilbarhate99/PPO-PyTorch" rel="noopener ugc nofollow" target="_blank">这个资源库</a>的。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="30b7" class="nw lu iq ns b be nx ny l nz oa">import torch<br/>import torch.nn as nn<br/>from torch.distributions import MultivariateNormal<br/>from torch.distributions import Categorical<br/><br/>################################## set device ##################################<br/>print("============================================================================================")<br/># set device to cpu or cuda<br/>device = torch.device('cpu')<br/>if(torch.cuda.is_available()): <br/>    device = torch.device('cuda:0') <br/>    torch.cuda.empty_cache()<br/>    print("Device set to : " + str(torch.cuda.get_device_name(device)))<br/>else:<br/>    print("Device set to : cpu")<br/>print("============================================================================================")<br/><br/><br/>################################## PPO Policy ##################################<br/>class RolloutBuffer:<br/>    def __init__(self):<br/>        self.actions = []<br/>        self.states = []<br/>        self.logprobs = []<br/>        self.rewards = []<br/>        self.is_terminals = []<br/>    <br/>    def clear(self):<br/>        del self.actions[:]<br/>        del self.states[:]<br/>        del self.logprobs[:]<br/>        del self.rewards[:]<br/>        del self.is_terminals[:]<br/><br/><br/>class ActorCritic(nn.Module):<br/>    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):<br/>        super(ActorCritic, self).__init__()<br/><br/>        self.has_continuous_action_space = has_continuous_action_space<br/>        <br/>        if has_continuous_action_space:<br/>            self.action_dim = action_dim<br/>            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)<br/>        # actor<br/>        if has_continuous_action_space :<br/>            self.actor = nn.Sequential(<br/>                            nn.Linear(state_dim, 256),<br/>                            nn.Tanh(),<br/>                            nn.Linear(256,256),<br/>                            nn.Tanh(),<br/>                            nn.Linear(256, action_dim),<br/>                            nn.Sigmoid()<br/>                        )<br/>        else:<br/>            self.actor = nn.Sequential(<br/>                            nn.Linear(state_dim, 384),<br/>                            nn.Tanh(),<br/>                            nn.Linear(384, 384),<br/>                            nn.Tanh(),<br/>                            nn.Linear(384, action_dim),<br/>                            nn.Softmax(dim=-1)<br/>                        )<br/>        # critic<br/>        self.critic = nn.Sequential(<br/>                        nn.Linear(state_dim, 128),<br/>                        nn.Tanh(),<br/>                        nn.Linear(128, 128),<br/>                        nn.Tanh(),<br/>                        nn.Linear(128, 1)<br/>                    )<br/>        <br/>    def set_action_std(self, new_action_std):<br/>        if self.has_continuous_action_space:<br/>            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)<br/>        else:<br/>            print("--------------------------------------------------------------------------------------------")<br/>            print("WARNING : Calling ActorCritic::set_action_std() on discrete action space policy")<br/>            print("--------------------------------------------------------------------------------------------")<br/><br/>    def forward(self):<br/>        raise NotImplementedError<br/>    <br/>    def act(self, state):<br/>        if self.has_continuous_action_space:<br/>            action_mean = self.actor(state)<br/>            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)<br/>            dist = MultivariateNormal(action_mean, cov_mat)<br/>        else:<br/>            action_probs = self.actor(state)<br/>            dist = Categorical(action_probs)<br/><br/>        action = dist.sample()<br/>        action_logprob = dist.log_prob(action)<br/>        <br/>        return action.detach(), action_logprob.detach()<br/>    <br/>    def evaluate(self, state, action):<br/><br/>        if self.has_continuous_action_space:<br/>            action_mean = self.actor(state)<br/>            <br/>            action_var = self.action_var.expand_as(action_mean)<br/>            cov_mat = torch.diag_embed(action_var).to(device)<br/>            dist = MultivariateNormal(action_mean, cov_mat)<br/>            <br/>            # For Single Action Environments.<br/>            if self.action_dim == 1:<br/>                action = action.reshape(-1, self.action_dim)<br/>        else:<br/>            action_probs = self.actor(state)<br/>            dist = Categorical(action_probs)<br/>        action_logprobs = dist.log_prob(action)<br/>        dist_entropy = dist.entropy()<br/>        state_values = self.critic(state)<br/>        <br/>        return action_logprobs, state_values, dist_entropy<br/><br/><br/>class PPO:<br/>    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):<br/><br/>        self.has_continuous_action_space = has_continuous_action_space<br/><br/>        if has_continuous_action_space:<br/>            self.action_std = action_std_init<br/><br/>        self.gamma = gamma<br/>        self.eps_clip = eps_clip<br/>        self.K_epochs = K_epochs<br/>        <br/>        self.buffer = RolloutBuffer()<br/><br/>        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)<br/>        self.optimizer = torch.optim.Adam([<br/>                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},<br/>                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}<br/>                    ])<br/><br/>        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)<br/>        self.policy_old.load_state_dict(self.policy.state_dict())<br/>        <br/>        self.MseLoss = nn.MSELoss()<br/><br/>    def set_action_std(self, new_action_std):<br/>        if self.has_continuous_action_space:<br/>            self.action_std = new_action_std<br/>            self.policy.set_action_std(new_action_std)<br/>            self.policy_old.set_action_std(new_action_std)<br/>        else:<br/>            print("--------------------------------------------------------------------------------------------")<br/>            print("WARNING : Calling PPO::set_action_std() on discrete action space policy")<br/>            print("--------------------------------------------------------------------------------------------")<br/><br/>    def decay_action_std(self, action_std_decay_rate, min_action_std):<br/>        print("--------------------------------------------------------------------------------------------")<br/>        if self.has_continuous_action_space:<br/>            self.action_std = self.action_std - action_std_decay_rate<br/>            self.action_std = round(self.action_std, 4)<br/>            if (self.action_std &lt;= min_action_std):<br/>                self.action_std = min_action_std<br/>                print("setting actor output action_std to min_action_std : ", self.action_std)<br/>            else:<br/>                print("setting actor output action_std to : ", self.action_std)<br/>            self.set_action_std(self.action_std)<br/><br/>        else:<br/>            print("WARNING : Calling PPO::decay_action_std() on discrete action space policy")<br/>        print("--------------------------------------------------------------------------------------------")<br/><br/>    def select_action(self, state):<br/><br/>        if self.has_continuous_action_space:<br/>            with torch.no_grad():<br/>                state = torch.FloatTensor(state).to(device)<br/>                action, action_logprob = self.policy_old.act(state)<br/><br/>            self.buffer.states.append(state)<br/>            self.buffer.actions.append(action)<br/>            self.buffer.logprobs.append(action_logprob)<br/><br/>            return action.detach().cpu().numpy().flatten()<br/>        else:<br/>            with torch.no_grad():<br/>                state = torch.FloatTensor(state).to(device)<br/>                action, action_logprob = self.policy_old.act(state)<br/>            <br/>            self.buffer.states.append(state)<br/>            self.buffer.actions.append(action)<br/>            self.buffer.logprobs.append(action_logprob)<br/><br/>            return action.item()<br/><br/>    def update(self):<br/>        # Monte Carlo estimate of returns<br/>        rewards = []<br/>        discounted_reward = 0<br/>        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):<br/>            if is_terminal:<br/>                discounted_reward = 0<br/>            discounted_reward = reward + (self.gamma * discounted_reward)<br/>            rewards.insert(0, discounted_reward)<br/>            <br/>        # Normalizing the rewards<br/>        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)<br/>        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)<br/><br/>        # convert list to tensor<br/>        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)<br/>        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)<br/>        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)<br/><br/>        # Optimize policy for K epochs<br/>        for _ in range(self.K_epochs):<br/><br/>            # Evaluating old actions and values<br/>            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)<br/><br/>            # match state_values tensor dimensions with rewards tensor<br/>            state_values = torch.squeeze(state_values)<br/>            <br/>            # Finding the ratio (pi_theta / pi_theta__old)<br/>            ratios = torch.exp(logprobs - old_logprobs.detach())<br/><br/>            # Finding Surrogate Loss<br/>            advantages = rewards - state_values.detach()   <br/>            surr1 = ratios * advantages<br/>            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages<br/><br/>            # final loss of clipped objective PPO<br/>            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy<br/>            <br/>            # take gradient step<br/>            self.optimizer.zero_grad()<br/>            loss.mean().backward()<br/>            self.optimizer.step()<br/>            <br/>        # Copy new weights into old policy<br/>        self.policy_old.load_state_dict(self.policy.state_dict())<br/><br/>        # clear buffer<br/>        self.buffer.clear()    </span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="0ac1" class="nw lu iq ns b be nx ny l nz oa">import os<br/>import glob<br/>import time<br/>from datetime import datetime<br/><br/>import torch<br/>import numpy as np<br/><br/>################################### Training ###################################<br/>def train():<br/>    print("============================================================================================")<br/><br/><br/>    has_continuous_action_space = False # continuous action space; else discrete<br/><br/>    max_ep_len = 364                   # max timesteps in one episode<br/>    max_training_timesteps = int(364*15000)   # break training loop if timeteps &gt; max_training_timesteps<br/><br/>    print_freq = max_ep_len * 10        # print avg reward in the interval (in num timesteps)<br/><br/>    action_std = 0.6            # starting std for action distribution (Multivariate Normal)<br/>    action_std_decay_rate = 0.03       # linearly decay action_std (action_std = action_std - action_std_decay_rate)<br/>    min_action_std = 0.03               # minimum action_std (stop decay after action_std &lt;= min_action_std)<br/>    action_std_decay_freq = int(1e5)  # action_std decay frequency (in num timesteps)<br/>    #####################################################<br/><br/>    ## Note : print/log frequencies should be &gt; than max_ep_len<br/><br/>    ################ PPO hyperparameters ################<br/>    update_timestep = max_ep_len/2       # update policy every n timesteps<br/>    K_epochs = 20               # update policy for K epochs in one PPO update<br/><br/>    eps_clip = 0.2          # clip parameter for PPO<br/>    gamma = 0.99            # discount factor<br/><br/>    lr_actor = 0.00005       # learning rate for actor network<br/>    lr_critic = 0.0001       # learning rate for critic network<br/><br/>    random_seed = 0         # set random seed if required (0 = no random seed)<br/>    #####################################################<br/><br/>    state_dim = 9<br/>    action_dim = 275<br/><br/>    torch.manual_seed(random_seed)<br/>    np.random.seed(random_seed)<br/><br/>    ################# training procedure ################<br/><br/>    # initialize a PPO agent<br/>    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)<br/><br/>    # track total training time<br/>    start_time = datetime.now().replace(microsecond=0)<br/>    print("Started training at (GMT) : ", start_time)<br/><br/>    print("============================================================================================")<br/><br/><br/>    # printing and logging variables<br/>    print_running_reward = 0<br/>    print_running_episodes = 0<br/><br/>    time_step = 0<br/>    i_episode = 0<br/>    <br/>    # training loop<br/>    for i in range(2):<br/>        env = MultiEchelonInvOptEnv(demand_hist_list[i*2:i*2+2])<br/>        while time_step &lt;= max_training_timesteps:<br/><br/>            state = env.reset()<br/>            current_ep_reward = 0<br/><br/>            for t in range(1, max_ep_len+1):<br/><br/>                # select action with policy<br/><br/>                action = ppo_agent.select_action(state)<br/>                state, reward, done = env.step(action)<br/><br/>                # saving reward and is_terminals<br/>                ppo_agent.buffer.rewards.append(reward)<br/>                ppo_agent.buffer.is_terminals.append(done)<br/><br/>                time_step +=1<br/>                current_ep_reward += reward<br/><br/>                # update PPO agent<br/>                if time_step % update_timestep == 0:<br/>                    ppo_agent.update()<br/><br/>                # if continuous action space; then decay action std of ouput action distribution<br/>                if has_continuous_action_space and time_step % action_std_decay_freq == 0:<br/>                    ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)<br/><br/><br/>                # printing average reward<br/>                if time_step % print_freq == 0:<br/><br/>                    # print average reward till last episode<br/>                    print_avg_reward = print_running_reward / print_running_episodes<br/>                    print_avg_reward = round(print_avg_reward, 2)<br/><br/>                    print("Episode : {} \t\t Timestep : {} \t\t Average Reward : {}".format(i_episode, time_step, print_avg_reward))<br/><br/>                    print_running_reward = 0<br/>                    print_running_episodes = 0<br/><br/>                # break; if the episode is over<br/>                if done:<br/>                    break<br/><br/>            print_running_reward += current_ep_reward<br/>            print_running_episodes += 1<br/><br/>            i_episode += 1<br/>    torch.save(ppo_agent.policy.state_dict(), desired_path)<br/><br/>if __name__ == '__main__':<br/><br/>    train()</span></pre><h2 id="e3c3" class="nf lu iq bd lv ng nh dn lz ni nj dp md lf nk nl mf lj nm nn mh ln no np mj nq bi translated">用于优化(S，S)策略的代码</h2><p id="3534" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">对于这个特定的数值实验，可能的(<em class="ls"> s </em>、<em class="ls"> S </em> ) <em class="ls"> </em>组合的数量大得惊人。因此，列举所有可能的组合来优化(<em class="ls"> s </em>，<em class="ls"> S </em>)策略是不切实际的。这里，我们采用贝叶斯优化(BO)来获得最优(<em class="ls"> s </em>，<em class="ls"> S </em>)策略。BO 是一种黑盒优化方法。它通过从目标函数顺序采样并更新逼近目标函数的代理模型来优化目标函数。BO 使用一个获取函数来确定在哪里采样下一个点，并且随着更多的点被采样，代理模型的精度不断提高。最后，BO 返回所有先前采样点中具有最佳目标函数值的点作为最优解。关于 BO 更详细的介绍，感兴趣的读者可以参考本文。这部分业务对象的实现基于<a class="ae kv" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">这个库</a>。</p><p id="838a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在代码中，我们定义了一个有 6 个参数的目标函数，分别是<em class="ls"> s_DC，s_DC，s_r1，S_r1，s_r2，S_r2。</em>目标函数计算在历史需求数据集上获得的利润，<em class="ls"> s_DC </em>和<em class="ls"> S_DC </em>定义两个<em class="ls"/>DC 的<em class="ls"> </em> ( <em class="ls"> s，S </em> ) <em class="ls"> </em>策略，<em class="ls"> s_r1 </em>和<em class="ls"> S_r1 </em>定义<em class="ls"> </em> ( <em class="ls"> s，S </em> ) <em class="ls"> </em>策略然后我们使用 BO 优化这个函数。我们将<em class="ls"> S_DC </em>的上限设置为 210，留出一点额外的空间来允许<em class="ls"> S </em>到<em class="ls">T39】取一个高于容量的值。我尝试将<em class="ls"> S_r1 </em>和<em class="ls"> S_r2 </em>到<em class="ls"> </em>每个值的上限设置为【60，70，80，90，100】，发现 90 给出了最佳目标值。因此，我选择 90 作为上限。历史需求数据集上的最优解竟然是(<em class="ls"> s_DC，s_DC，s_r1，S_r1，s_r2，S _ R2</em>)<em class="ls"/>=<em class="ls"/>(62，120，15，41，17，90) <em class="ls">。</em></em></p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="77da" class="nw lu iq ns b be nx ny l nz oa">def MultiEchelonInvOpt_sS(s_DC,S_DC,s_r1,S_r1,s_r2,S_r2):<br/>    if s_DC &gt; S_DC-1 or s_r1 &gt; S_r1-1 or s_r2 &gt; S_r2-1:<br/>        return -1e8<br/>    else:<br/>        n_retailers = 4<br/>        n_DCs = 2<br/>        retailers = []<br/>        for i in range(n_retailers):<br/>            retailers.append(Retailer(demand_hist_list[i]))<br/>        DCs = []<br/>        for i in range(n_DCs):<br/>            DCs.append(DistributionCenter()) <br/>        n_period = len(demand_hist_list[0])<br/>        variable_order_cost = 10<br/>        current_period = 1<br/>        total_reward = 0<br/>        while current_period &lt;= n_period:<br/>            action = []<br/>            for DC in DCs:<br/>                if DC.inv_pos &lt;= s_DC:<br/>                    action.append(np.round(min(DC.order_quantity_limit,S_DC-DC.inv_pos)))<br/>                else:<br/>                    action.append(0)<br/>            for i in range(len(retailers)):<br/>                if i%2 == 0:<br/>                    if retailers[i].inv_pos &lt;= s_r1:<br/>                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r1-retailers[i].inv_pos)))<br/>                    else:<br/>                        action.append(0)<br/>                else:<br/>                    if retailers[i].inv_pos &lt;= s_r2:<br/>                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r2-retailers[i].inv_pos)))<br/>                    else:<br/>                        action.append(0)<br/>            y_list = []<br/>            for i in range(n_DCs):<br/>                y = 1 if action[i] &gt; 0 else 0    <br/>                y_list.append(y)<br/>            for DC,order_quantity in zip(DCs,action[:n_DCs]):<br/>                DC.place_order(order_quantity,current_period)<br/>            sum_holding_cost_DC = 0<br/>            for i in range(n_DCs):<br/>                holding_cost_total = DCs[i].order_arrival(retailers[i*2:i*2+2],current_period)<br/>                sum_holding_cost_DC += holding_cost_total<br/>                DCs[i].satisfy_demand(retailers[i*2:i*2+2],action[i*2+2:i*2+4],current_period)<br/>            sum_n_orders = 0<br/>            sum_holding_cost_retailer = 0<br/>            sum_revenue = 0<br/>            for retailer,demand in zip(retailers,demand_hist_list):<br/>                n_orders, holding_cost_total = retailer.order_arrival(current_period)<br/>                sum_n_orders += n_orders<br/>                sum_holding_cost_retailer += holding_cost_total<br/>                revenue = retailer.satisfy_demand(demand[current_period-1])<br/>                sum_revenue += revenue    <br/>            reward = sum_revenue - sum_holding_cost_retailer - sum_holding_cost_DC - sum_n_orders*retailers[0].fixed_order_cost - \<br/>                     np.sum(y_list)*DCs[0].fixed_order_cost - np.sum(action[:n_DCs])*variable_order_cost<br/><br/>            current_period += 1<br/>            total_reward += reward<br/>        return total_reward</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="b404" class="nw lu iq ns b be nx ny l nz oa">from bayes_opt import BayesianOptimization<br/>pbounds = {'s_DC': (0,210), 'S_DC': (0, 210), 's_r1': (0, 90), 'S_r1': (0, 90), 's_r2': (0, 90), 'S_r2': (0, 90)}<br/>optimizer = BayesianOptimization(<br/>    f=MultiEchelonInvOpt_sS,<br/>    pbounds=pbounds,<br/>    random_state=0,<br/>)<br/>optimizer.maximize(<br/>    init_points = 100,<br/>    n_iter=1000<br/>)<br/>print(optimizer.max)</span></pre><h2 id="294d" class="nf lu iq bd lv ng nh dn lz ni nj dp md lf nk nl mf lj nm nn mh ln no np mj nq bi translated">测试 PPO 策略的代码</h2><p id="f50c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们首先创建 100 个客户需求数据集进行测试。100 个数据集中的每一个都包含四家零售商 52 周的需求数据。我们可以将每个数据集视为未来 1 年需求的可能情景。然后，我们在每个需求数据集上评估 PPO 策略，并收集每个数据集的总回报。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="ad5c" class="nw lu iq ns b be nx ny l nz oa">np.random.seed(0)<br/>demand_test = []<br/>for k in range(100,200):<br/>    demand_list = []<br/>    for k in range(4):<br/>        demand = []<br/>        for i in range(52):<br/>            for j in range(4):<br/>                random_demand = np.random.normal(3, 1.5)<br/>                if random_demand &lt; 0:<br/>                    random_demand = 0<br/>                random_demand = np.round(random_demand)<br/>                demand.append(random_demand)<br/>            random_demand = np.random.normal(6, 1)<br/>            if random_demand &lt; 0:<br/>                random_demand = 0<br/>            random_demand = np.round(random_demand)<br/>            demand.append(random_demand)<br/>            for j in range(2):<br/>                random_demand = np.random.normal(12, 2)<br/>                if random_demand &lt; 0:<br/>                    random_demand = 0<br/>                random_demand = np.round(random_demand)<br/>                demand.append(random_demand)<br/>        demand_list.append(demand)<br/>    demand_test.append(demand_list)</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="9afe" class="nw lu iq ns b be nx ny l nz oa">import os<br/>import glob<br/>import time<br/>from datetime import datetime<br/><br/>import torch<br/>import numpy as np<br/><br/>has_continuous_action_space = False # continuous action space; else discrete<br/>action_std = 0.6            # starting std for action distribution (Multivariate Normal)<br/>action_std_decay_rate = 0.03       # linearly decay action_std (action_std = action_std - action_std_decay_rate)<br/>min_action_std = 0.03               # minimum action_std (stop decay after action_std &lt;= min_action_std)<br/>action_std_decay_freq = int(1e5)  # action_std decay frequency (in num timesteps<br/>eps_clip = 0.2          # clip parameter for PPO<br/>gamma = 0.99            # discount factor<br/>K_epochs = 20<br/>lr_actor = 0.00005      # learning rate for actor network<br/>lr_critic = 0.0001       # learning rate for critic network<br/><br/>random_seed = 0         # set random seed if required (0 = no random seed)<br/>#####################################################<br/><br/>state_dim = 9<br/>action_dim = 275<br/><br/>torch.manual_seed(random_seed)<br/>np.random.seed(random_seed)<br/><br/>ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)<br/>ppo_agent.policy_old.load_state_dict(torch.load(desired_path))<br/>ppo_agent.policy.load_state_dict(torch.load(desired_path))<br/><br/>reward_RL = []<br/>for demand in demand_test:<br/>    reward_total = 0<br/>    for i in range(2):<br/>        env = MultiEchelonInvOptEnv(demand[i*2:i*2+2])<br/>        state = env.reset()<br/>        done = False<br/>        reward_sub = 0<br/>        while not done:<br/>            action = ppo_agent.select_action(state)<br/>            state, reward, done = env.step(action)<br/>            reward_sub += reward<br/>            if done:<br/>                break<br/>        reward_total += reward_sub<br/>    reward_RL.append(reward_total)</span></pre><h2 id="a842" class="nf lu iq bd lv ng nh dn lz ni nj dp md lf nk nl mf lj nm nn mh ln no np mj nq bi translated">测试(S，S)策略的代码</h2><p id="9917" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们在同一个测试集上评估(<em class="ls"> s </em>，<em class="ls"> S </em>)策略。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="067d" class="nw lu iq ns b be nx ny l nz oa">def MultiEchelonInvOpt_sS_test(s_DC,S_DC,s_r1,S_r1,s_r2,S_r2,demand_records):<br/>    if s_DC &gt; S_DC-1 or s_r1 &gt; S_r1-1 or s_r2 &gt; S_r2-1:<br/>        return -1e8<br/>    else:<br/>        n_retailers = 4<br/>        n_DCs = 2<br/>        retailers = []<br/>        for i in range(n_retailers):<br/>            retailers.append(Retailer(demand_records[i]))<br/>        DCs = []<br/>        for i in range(n_DCs):<br/>            DCs.append(DistributionCenter()) <br/>        n_period = len(demand_records[0])<br/>        variable_order_cost = 10<br/>        current_period = 1<br/>        total_reward = 0<br/>        total_revenue = 0<br/>        total_holding_cost_retailer = 0<br/>        total_holding_cost_DC = 0<br/>        total_variable_cost = 0<br/>        while current_period &lt;= n_period:<br/>            action = []<br/>            for DC in DCs:<br/>                if DC.inv_pos &lt;= s_DC:<br/>                    action.append(np.round(min(DC.order_quantity_limit,S_DC-DC.inv_pos)))<br/>                else:<br/>                    action.append(0)<br/>            for i in range(len(retailers)):<br/>                if i%2 == 0:<br/>                    if retailers[i].inv_pos &lt;= s_r1:<br/>                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r1-retailers[i].inv_pos)))<br/>                    else:<br/>                        action.append(0)<br/>                else:<br/>                    if retailers[i].inv_pos &lt;= s_r2:<br/>                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r2-retailers[i].inv_pos)))<br/>                    else:<br/>                        action.append(0)<br/>            y_list = []<br/>            for i in range(n_DCs):<br/>                y = 1 if action[i] &gt; 0 else 0 <br/>                y_list.append(y)<br/>            for DC,order_quantity in zip(DCs,action[:n_DCs]):<br/>                DC.place_order(order_quantity,current_period)<br/>            sum_holding_cost_DC = 0<br/>            for i in range(n_DCs):<br/>                holding_cost_total = DCs[i].order_arrival(retailers[i*2:i*2+2],current_period)<br/>                sum_holding_cost_DC += holding_cost_total<br/>                DCs[i].satisfy_demand(retailers[i*2:i*2+2],action[i*2+2:i*2+4],current_period)<br/>                <br/>            sum_n_orders = 0<br/>            sum_holding_cost_retailer = 0<br/>            sum_revenue = 0<br/>            for retailer,demand in zip(retailers,demand_records):<br/>                n_orders, holding_cost_total = retailer.order_arrival(current_period)<br/>                sum_n_orders += n_orders<br/>                sum_holding_cost_retailer += holding_cost_total<br/>                revenue = retailer.satisfy_demand(demand[current_period-1])<br/>                sum_revenue += revenue  <br/>            reward = sum_revenue - sum_holding_cost_retailer - sum_holding_cost_DC - sum_n_orders*retailers[0].fixed_order_cost - \<br/>                     np.sum(y_list)*DCs[0].fixed_order_cost - np.sum(action[:n_DCs])*variable_order_cost<br/>            <br/>            current_period += 1<br/>            total_reward += reward<br/>            <br/>        return total_reward</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="3f04" class="nw lu iq ns b be nx ny l nz oa">reward_sS = []<br/>for demand in demand_test:<br/>    reward = MultiEchelonInvOpt_sS_test(62, 120, 15, 41, 17, 90, demand)<br/>    reward_sS.append(reward)</span></pre><h2 id="f230" class="nf lu iq bd lv ng nh dn lz ni nj dp md lf nk nl mf lj nm nn mh ln no np mj nq bi translated">对数值结果的讨论</h2><p id="55d0" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">PPO 策略在 100 个需求数据集上的平均利润为$31187.03，(<em class="ls"> s </em>，<em class="ls"> S </em>)策略的平均利润为$26390.87，这表明利润增加了 18.17%。PPO 和(<em class="ls"> s </em>，<em class="ls"> S </em>)政策在 100 个需求数据集上获得的利润箱线图如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1dc6facb9f35733213d14732c45866a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*RLKzRXMeyD7B4UIwqOgdDA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试集中 PPO 策略和(S，S)策略获得的利润的箱线图(图片由作者提供)</p></figure><p id="fca3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经看到，PPO 政策在目前的表述下优于(<em class="ls"> s </em>，<em class="ls"> S </em>)政策。其性能可能还有进一步提升的空间。例如，我们可以将动作建模为[0，1]之间的连续值<em class="ls"> x </em>。因此，每个实体的订单数量将是<em class="ls">x</em>*最大订单数量。在我以后的文章中，我可能会朝着这个方向更进一步，看看这种方法的效果如何。</p><p id="08dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>