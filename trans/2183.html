<html>
<head>
<title>Neural Sheaf Diffusion for deep learning on graphs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于图形深度学习的神经层扩散</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6#2022-05-16">https://towardsdatascience.com/neural-sheaf-diffusion-for-deep-learning-on-graphs-bfa200e6afa6#2022-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a74e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">从代数拓扑的角度看GNNs</h2><div class=""/><div class=""><h2 id="1a05" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">图形神经网络(GNNs)连接到扩散方程，在图形的节点之间交换信息。作为纯粹的拓扑对象，图被隐含地假定为具有平凡的几何。使用一种更一般的结构称为细胞绞线，我们可以赋予图形一个可学的几何结构。我们证明了细胞束上的扩散过程可以解决任何节点分类任务，并且可以提供关于过度平滑、处理嗜异性数据的问题以及GNNs的表达能力的新见解。</strong></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a468cc6ab32ad585886ece76ba3b43ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6Ms8tzZ-Yk80dOA1hfiJg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">基于Shutterstock的图像。</p></figure><p id="2013" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">本文由克里斯蒂安·博德纳尔和弗朗切斯科·迪·乔瓦尼合著，基于c·博德纳尔、f·迪·乔瓦尼等人的论文</em>、<em class="ma"> </em> <a class="ae mb" href="https://arxiv.org/pdf/2202.04579.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ma">【神经束扩散:对GNNs </em> </a> <em class="ma">中异嗜性和过度光滑的拓扑透视】(2022) arXiv:2202.04579。它是通过微分几何和代数拓扑</em>  <em class="ma">的透镜的</em> <a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-through-the-lens-of-differential-geometry-and-algebraic-topology-3a7c3c22d5f?source=your_stories_page----------------------------------------"> <em class="ma">图神经网络系列的一部分。另请参见之前讨论</em> </a><a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00"> <em class="ma">神经扩散偏微分方程</em> </a> <em class="ma">、</em> <a class="ae mb" rel="noopener" target="_blank" href="/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16?sk=f5cf01cbd57b4fee8fb3a89d447e56a9"> <em class="ma">图重布线与瑞奇流</em> </a>、<em class="ma"/><a class="ae mb" href="https://michael-bronstein.medium.com/a-new-computational-fabric-for-graph-neural-networks-280ea7e3ed1a?sk=3d4185067ef3c1cf81793deada08e18f" rel="noopener"><em class="ma">拓扑消息传递</em> </a> <em class="ma">的系列帖子。看一段Aleksa gordi</em><em class="ma">详细讲解论文的</em> <a class="ae mb" href="https://www.youtube.com/watch?v=JiQmkhsbRwk" rel="noopener ugc nofollow" target="_blank"> <em class="ma">视频和Cristian在AIMS 2020几何深度学习课程中的</em> </a><a class="ae mb" href="https://www.youtube.com/watch?v=TgPMuvH5j1I" rel="noopener ugc nofollow" target="_blank"> <em class="ma">嘉宾谈</em> </a> <em class="ma">。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="f611" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">图形神经网络(GNNs)已经成为机器学习领域的“一等公民”[1]，并在从粒子物理到纯数学定理证明等问题中获得了成功应用[2]。大多数gnn基于消息传递[3]，通过它，相邻节点之间的信息在图上传播。</p><p id="5885" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">两种现象通常与某些gnn有关:在<em class="ma">嗜异性</em>设置中表现不佳【4】和<em class="ma">过度平滑</em>【5–6】。前者源于gnn通常建立在<em class="ma">同向性</em>(即相邻节点是相似的)的假设上，这通常看起来是不现实的【7】。后一种现象指的是应用多个消息传递层来产生接近常数的特性的趋势，导致性能随着深度而下降，以及深度GNNs的不可行性[8]。</p><p id="9281" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">在最近的一篇论文[9]中，我们认为这两个问题在底层图的“几何”中有一个共同的根源。像流形一样，图也是拓扑对象[10]:它们有邻域的概念(在图上，这是由一对节点之间的边捕获的)，但没有距离或方向。我们可以通过用向量空间结构装饰每个节点和边(这种向量空间被称为<em class="ma">茎</em>)以及每个关联节点和边的茎之间的一组线性变换(<em class="ma">限制映射</em>)来定义图上的一些几何概念。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/c0858226b18e9e0b9c7fecfd7c56b4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WTfIRJGMR91IikPe"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">通过将向量空间与图中的每个节点和边(分别为“梗”ℱ(u)、ℱ(v和ℱ(e=(u,v)相关联，并在这些空间之间为每个关联节点-边对(u⊴e和v⊴e).)建立“限制图”，在图上构建细胞层</em></p></figure><p id="9782" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这个图，连同其上构建的茎和限制性图谱，被称为<em class="ma">细胞束</em>。<a class="ae mb" href="https://en.wikipedia.org/wiki/Sheaf_%28mathematics%29" rel="noopener ugc nofollow" target="_blank">束</a>是代数拓扑中的主要研究对象，可以粗略地认为是描述定义在拓扑空间上的数据的一种非常通用的方式【11】。虽然它们在连续情况下的定义有些技术性和复杂性，但细胞束是离散拓扑空间(细胞复形，其中图是一个特例)上的一个类似的和更容易掌握的结构。1977-1978年，<a class="ae mb" href="https://en.wikipedia.org/wiki/Robert_MacPherson_%28mathematician%29" rel="noopener ugc nofollow" target="_blank">罗伯特·麦克弗森</a>在布朗大学的一次研讨会上首次发现了细胞束。麦克弗森的学生艾伦·谢泼德在1985年完成的博士论文[12]中对该理论进行了第一次全面的发展。这项工作，然而，几乎没有人注意到；最近，Robert Ghrist和他以前的博士生Justin Curry和Jakob Hansen已经复活并将其引入应用领域。</p><p id="307e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">在</span>我们讨论的上下文中，细胞层的概念与微分几何中的<em class="ma">连接</em>的概念密切相关，顾名思义，微分几何“连接”光滑流形上两个相邻点的切空间【16】。这种机制规定了切向量如何局部移动，允许执行<em class="ma">并行传输</em>【17】——从而赋予流形(拓扑对象)一种几何结构。连接的各种选择导致流形的不同性质和其上发生的物理过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/5148f3051016d0e2f31c2a48d13aa963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UdH7qV2Afk_dWyK8"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">在一个管汇上平行运输。在向量空间ℱ(w和ℱ(u之间移动向量的结果通常是路径相关的(在这个例子中，从w到u沿着e₃或者沿着e₁和e₂到v的路径产生不同的结果)。</em></p></figure><h1 id="4a06" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">层的选择决定了表现力</h1><p id="5d78" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">本着类似的精神，我们证明了通过适当构造限制图来选择图上的层结构，会导致图上扩散过程的不同行为。由于GNNs可以解释为离散化的扩散方程[18]，这种形式允许我们研究不同GNN模型在不同环境下的表达能力和局限性。</p><p id="6aea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一层上的扩散受微分方程支配</p><p id="8323" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">ẋ</strong>(<em class="ma">t</em>)=<strong class="lg ja">δx</strong>(<em class="ma">t</em>)</p><p id="6372" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从一些初始条件开始<strong class="lg ja"> x </strong> (0)= <strong class="lg ja"> x </strong>。<strong class="lg ja"> </strong>这看起来像一个标准的图形扩散方程，有一个重要的区别:我们现在有了维度为<em class="ma"> d </em>的<em class="ma">矢量</em>数据(排列成大小为<em class="ma">和</em>×1】<em class="ma"/>的块矢量<strong class="lg ja"> x </strong>，编码层结构【19】的<em class="ma">层拉普拉斯</em><strong class="lg ja">δ</strong>是<em class="ma">和</em> × </p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b2ba9f5f5c17b381236aaf7571ca1579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/0*QeUjQcdGqKaBU9Il"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">与图的节点相关联的空间一起形成0-共链C⁰的空间(“节点信号”</em> <strong class="bd nt"> <em class="mt"> x </em> </strong> <em class="mt">)和图的边上的空间1-共链c(“边信号”</em> <strong class="bd nt"> <em class="mt"> y </em> </strong> <em class="mt">)。共同边界图δ:C⁰→C是测量节点空间之间“不一致”的梯度算子的一般化；类似地，地图δᵀ:C →C⁰相当于散度算子。谢夫拉普拉斯被定义为</em><strong class="bd nt"><em class="mt">δ</em></strong><em class="mt">=δᵀδ，是微分几何中使用的霍奇拉普拉斯的离散版本。</em></p></figure><p id="f4c4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在极限<em class="ma"> t </em> →∞中，层扩散方程的解属于层拉普拉斯算子的<em class="ma">调和空间</em>:该空间包含符合层沿所有边的限制图【21】的信号，使得<strong class="lg ja">δx</strong>= 0<strong class="lg ja"/>【22】。这是对谐波信号的经典定义的概括，谐波信号在每个节点上等于其邻居的平均值。</p><p id="e660" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">任何节点分类任务都可以被描述为寻找正确的层，在该层上，层扩散的极限能够线性地分离节点特征。这种观点可以作为Weisfeiler-Lehman图同构测试层次结构[23，48]的替代方法，用于表征从扩散过程中获得的gnn的表达能力。扩散的分离能力通常取决于层的构造以及关于基础图的结构和节点特征或标签的假设(嗜同性对嗜异性)。</p><p id="eaf8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">具有对称和非零限制图的<em class="ma"> d </em> =1维(标量)茎的选择对应于具有严格正边权重的标准加权图拉普拉斯算子。从这种扩散过程中产生的GNN架构是GCN类型的图形卷积模型[24–25]。这种模型可以在某些同质假设下分离两类节点[26]；然而，这类滑轮在嗜异性环境中不够强大[27]。我们表明，为了解决嗜异性的情况，必须放弃对称假设，允许非对称限制图[28]，这在标量情况下相当于允许负边权重。这为为什么最近的论文[29–31]显示负权重对处理异性很有用提供了理论依据。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/47c9b261dfc0dcc6af7b390f74ad1bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v1uhZpNF-1ddrbQS"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">具有两个分别包含正负标量特征的不同节点(红色和绿色)的嗜异性设置示例。不对称限制图(在这种情况下，+1或-1边权重)是调和它们所必需的。</p></figure><p id="2c11" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">考虑到两个以上的节点类，我们表明更高维的主干(提供一定的“表示宽度”)是分离这种情况所必需的[32]。这种额外的“宽度”概念可以对特征通道的数量起到补充作用，以增加GNN模型的表达能力。我们表明使用<em class="ma"> d </em> × <em class="ma"> d </em>对角可逆限制图允许分离<em class="ma"> d </em>类【33】。使用正交限制图(相当于节点向量的旋转和反射)会产生更好的表达能力:对于<em class="ma"> d </em> =2或4，相关的扩散过程可以分离至少2个<em class="ma"> d </em>类。正交设置特别有趣，因为它与同步问题有关[21]，最近以正交GNNs [34]和Taco Cohen在其博士论文[35]中开发的规范等变卷积模型的形式进行了研究。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/7b8a8ea1aeb6833c2b151fbec76bed97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vM-K7HbL0p6D7oIS"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mt">在具有四个节点类(颜色编码)的合成嗜异性数据集上通过层扩散进行节点分类的示例。节点特征是二维的(由平面中的节点坐标表示)，并且限制图是正交的(旋转+反射矩阵)。与我们的理论一致，层扩散能够线性地分离极限中的节点。</em></p></figure><h1 id="0a91" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">层卷积网络和能量最小化</h1><p id="1980" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">为了将层理论与图形上的深度学习联系起来，我们考虑以下GNN层，该层最初由Hansen和Gebhart提出[36]:</p><p id="c96a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">y</strong>=σ((<strong class="lg ja">I</strong>—<strong class="lg ja">δ</strong>)(<strong class="lg ja">I</strong>⊗<strong class="lg ja">w</strong>₁)<strong class="lg ja">xw</strong>₂).</p><p id="31af" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里<strong class="lg ja"> X </strong>是具有<em class="ma"> f </em>个通道的输入特征的<em class="ma"> nd </em> × <em class="ma"> f </em>矩阵，<strong class="lg ja"> W </strong> ₁和<strong class="lg ja"> W </strong> ₂是<em class="ma"> d </em> × <em class="ma"> d </em>和<em class="ma"> f </em> × <em class="ma"> f </em>可学习的权重矩阵(前者作用于<strong class="lg ja">x的每个块 和GCN [25]中一样，<strong class="lg ja">δ</strong>是大小为<em class="ma"> nd </em> × <em class="ma"> nd，</em>t44】I</strong>是适当维数的单位矩阵，⊗表示<a class="ae mb" href="https://en.wikipedia.org/wiki/Kronecker_product" rel="noopener ugc nofollow" target="_blank">克罗内克乘积</a>【37】，σ是ReLU或Leaky ReLU等非线性。 通过类比GCN [38]，我们将这一层称为<em class="ma">层卷积网络</em> (SCN)。SCN模型可以被视为层扩散方程的离散的、参数化的和非线性的版本。</p><p id="b5c3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一个关键问题是SCNs是否以及何时在一定程度上趋向于像层扩散一样。为了分析这一点，我们检查SCN层如何影响<em class="ma">层狄利克雷能量</em>ℰ(<strong class="lg ja">x</strong>)= trace(<strong class="lg ja">x</strong>ᵀ<strong class="lg ja">δx</strong>)。这是一种类似于经典狄利克雷能量的二次形式，测量<strong class="lg ja"> X </strong>与层拉普拉斯的调和空间的距离(即信号与层结构的不一致程度)。扩散方程是狄利克雷能量的梯度流，使其随时间最小化。</p><p id="ec22" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在GCNs(SCN[38]的标量版本)的特殊情况下，蔡和王[6]表明，一个gcn层的应用必然<em class="ma">降低</em>狄利克雷能量，即<strong class="lg ja"> Y </strong>、<strong class="lg ja"> X </strong>。因此，随着深度的增加，特征往往会变得更平滑，这种现象在这种类型的图形神经网络中很常见。</p><p id="7e8c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在我们更一般的设置中，这种行为可能会根据层和相关层拉普拉斯<strong class="lg ja">δ</strong>的选择而变化。特别地，该不等式在沿所有边都相等的<em class="ma"> d </em>维正交限制图的情况下成立【39】。此外，沿所有边的关系的对称性是一个必要条件:只要我们有一个边，其中限制图是不同的，我们就可以找到一个任意小的线性变换，增加狄利克雷能量[40]。</p><p id="870f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在ReLU型σ假设下的<em class="ma"> d </em> =1维(标量)情况下，具有正号边是狄利克雷能量降低的必要条件。这是Oono和[5]以及蔡和王[6]关于广义神经网络过光滑性的结果的推广。有趣的是，如果σ被假设为线性的，那么即使是带负号的边，狄利克雷能量也会最小化。</p><h1 id="d586" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">从数据中学习束允许特征与图形几何一起进化</h1><p id="7ceb" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">最后，我们提出了一个基于层扩散(SCN型层)的GNN架构，其中层可以从数据中学习。具体地，该层由形式为𝚽( <strong class="lg ja"> x </strong> ᵤ、<strong class="lg ja"> x </strong> ᵥ、<em class="ma"> d </em> × <em class="ma"> d </em>矩阵值函数的参数限制图定义，该矩阵值函数可以进一步被约束为正交或对角的。这允许GNN在图上“发现”正确的层结构，以最适合下游任务的方式修改扩散的属性。</p><p id="13e5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">此外，GNN的每一层中的层都可能不同(通过其对当前图层的要素的依赖性)这一事实是域和数据的一种<em class="ma">共同进化</em>形式。在GNN的文献中，类似的想法是将计算图与输入图分离，这通常以<em class="ma">图重新布线</em>(改变图作为预处理步骤【41】)或<em class="ma">潜在图学习</em>(在学习期间改变图【42】)的形式实现。</p><p id="d8ec" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们最近在非欧几里德神经扩散[43]的工作中研究了图形“几何”演变的另一种方法，其中图形被嵌入到联合位置特征空间中。这个空间上的一个可学习的扩散过程，称为<em class="ma"> graph Beltrami flow </em>，导致同时的特征扩散和图形重布线。学习一层的优势在于它是“内在地”完成的，并且不需要在环境空间中嵌入任何类型的图。</p><h1 id="602e" class="mv mw iq bd mx my mz na nb nc nd ne nf kf ng kg nh ki ni kj nj kl nk km nl nm bi translated">结束语</h1><p id="02d8" class="pw-post-body-paragraph le lf iq lg b lh nn ka lj lk no kd lm ln np lp lq lr nq lt lu lv nr lx ly lz ij bi translated">神经层扩散遵循了图ML架构设计的一个新趋势:首先将输入图“提升”到一个结构更丰富的对象中，然后在这个对象上进行操作。这样，GNNs可以避免使用输入图作为“计算结构”，这仍然是当前结合消息传递范例使用的主要方法。</p><p id="99d7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们遵循这一理念的工作包括最近关于拓扑消息传递的论文[44–45](其中图被制成单纯或细胞复合体，在其上适当定义的消息传递被证明比经典的Weisfeiler-Lehman更具表达性)，图Ricci流(在图上构建Ricci曲率的离散类比，以量化和消除造成过度挤压的瓶颈[46]并将其用于节点嵌入[47])，以及前面提到的图Beltrami流[43]。</p><p id="1610" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第二，有限层扩散的节点分离能力为研究GNNs的表达能力提供了一个新的框架，该框架脱离了传统的Weisfeiler-Lehman层级[48]。层形式主义允许做出明智的架构选择——例如，证明在某些嗜异性设置或正交消息传递函数中使用负权重的合理性。</p><p id="30e9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，gnn是<a class="ae mb" rel="noopener" target="_blank" href="/geometric-foundations-of-deep-learning-94cdd45b451d?sk=184532175cb936d7b25d9adebd512629">几何深度学习蓝图</a>的一个特例，将Felix Klein的几何群论处理(“<a class="ae mb" href="https://en.wikipedia.org/wiki/Erlangen_program" rel="noopener ugc nofollow" target="_blank">埃尔兰根程序</a>”)引入机器学习【49】。正如埃尔兰根计划的精神蔓延到其他学科，并在微分几何，代数拓扑和物理学中产生了新的理论[50]，我们相信几何深度学习的下一步发展应该从这些类比中获得灵感。通过求助于上述领域的强大工具，可以得出新的理论见解和计算模型[51]，到目前为止，这些工具在最大似然法中探索不足，在应用领域中被认为是“奇异的”。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="d437" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[1]引用Petar Velič ković在我的<a class="ae mb" rel="noopener" target="_blank" href="/predictions-and-hopes-for-graph-ml-in-2021-6af2121c3e3d?sk=e2dfa5d2f63dab879a2d097382a3cf66"> 2021年预测帖子</a>中的话。</p><p id="b72c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2] A. Davies等.用人工智能指导人类直觉推进数学(2021)，自然600:70–74。</p><p id="52ed" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] J. Gilmer等人，<a class="ae mb" href="https://arxiv.org/pdf/1704.01212.pdf" rel="noopener ugc nofollow" target="_blank">量子化学的神经信息传递</a> (2017) ICML。</p><p id="5109" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4]朱等，<a class="ae mb" href="https://proceedings.neurips.cc/paper/2020/file/58ae23d878a47004366189884c2f8440-Paper.pdf" rel="noopener ugc nofollow" target="_blank">超越图神经网络中的同伦现象:当前的局限与有效设计</a> (2020) NeurIPS。</p><p id="6682" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5] K. Oono和t .铃木，<a class="ae mb" href="https://openreview.net/pdf?id=S1ldO2EFPr" rel="noopener ugc nofollow" target="_blank">图神经网络对节点分类的表达能力呈指数下降</a> (2020) ICLR。</p><p id="390a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]蔡志勇，王等，<a class="ae mb" href="https://arxiv.org/pdf/2006.13318.pdf" rel="noopener ugc nofollow" target="_blank">关于图神经网络过光滑问题的一个注记</a> (2020) arXiv:2006.13318 .</p><p id="ced2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7]早期的graph ML基准测试，如Cora、Citeseer和Pubmed是高度同质化的，这给GNNs的性能带来了一些错误的印象。最近专门针对这个问题开发的数据集指出，一些流行的GNN模型在嗜异性环境中的表现令人失望。</p><p id="b337" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8]参见我那篇有争议的博文<a class="ae mb" rel="noopener" target="_blank" href="/do-we-need-deep-graph-neural-networks-be62d3ec5c59?sk=8daa06935676e78bdb229017d3c4bac9">我们需要深度图神经网络吗？</a>在走向数据科学。</p><p id="0cda" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9] C .博德纳尔，f .迪·乔瓦尼等人，N <a class="ae mb" href="https://arxiv.org/pdf/2202.04579.pdf" rel="noopener ugc nofollow" target="_blank"> eural层扩散:GNNs</a>(2022)arXiv:2202.04579中关于异质和过度光滑的拓扑透视。</p><p id="9800" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[10]拓扑学和几何学区别的一个典型例子是关于一个拓扑学家的早餐有一个油炸圈饼和一杯咖啡的笑话，它们在拓扑学上是等价的(两者都同胚于一个环面)。这个例子借用了拓扑学中“橡胶片几何”的比喻:一个可拉伸的圆环面可以变形为一个咖啡杯，这个过程将影响点之间的局部距离和角度(几何)，但不会影响点之间的连接方式(拓扑学)。在这篇博文的<a class="ae mb" href="https://www.cantorsparadise.com/what-is-topology-963ef4cc6365" rel="noopener ugc nofollow" target="_blank">中可以看到一个流行的拓扑介绍。</a></p><p id="8941" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[11]层理论的发展归功于法国数学学派，它始于代数拓扑学家让·勒雷在战争时期的工作，随后在20世纪50年代由昂利·嘉当、让·皮埃尔·塞尔、奥斯卡·扎里斯基、亚历山大·格罗滕迪克和其他人发展。H. Miller，<a class="ae mb" href="https://klein.mit.edu/~hrm/papers/ss.pdf" rel="noopener ugc nofollow" target="_blank"> Leray在of lag XVIIA:The origins of sheaf theory，sheaf上同调，and spectral sequences </a>，1999中把sheaf theory的诞生归功于让·勒雷在二战期间被囚禁的一个戏剧性插曲。战争爆发时他是一名军官，1940年法国军队被击败后，他被囚禁在德国T2的T3战俘营里。勒雷和其他战友被允许举办一个数学研讨会，他是主席。由于担心他在力学和流体动力学方面的专业知识会让德国人强迫他为纳粹战争效力，Leray选择代数拓扑作为最抽象和“无害”的教学科目。他的工作后来分两部分出版，分别是《J. Leray，Sur la forme des espaces topologiques et Sur les points fixes des representations 》( 1945年)和《J. Math》。pures et appl . 24:95–167和169–199，副标题为“<em class="ma">un cours de topologie algébrique professoréen captivité</em>”(囚禁期间教授的代数拓扑课程)。</p><p id="6827" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[12] A. Shepard，<a class="ae mb" href="http://justinmcurry.com/wp-content/uploads/2022/02/shepard.pdf" rel="noopener ugc nofollow" target="_blank">分层空间导出范畴的细胞描述</a>，博士论文，布朗大学，1985年。</p><p id="ccc0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[13] J. M. Curry，<a class="ae mb" href="https://arxiv.org/pdf/1303.3255.pdf" rel="noopener ugc nofollow" target="_blank">滑轮、共沉和应用</a> (2014)。宾夕法尼亚大学博士论文。参见<a class="ae mb" href="https://twitter.com/currying/status/1496201117163794432?s=20&amp;t=oD-9NWs3k9L2hC1h3A0m9A" rel="noopener ugc nofollow" target="_blank">贾斯汀关于蜂窝绳轮历史的推特帖子</a>。</p><p id="9b5b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[14] J. Hansen和R. Ghrist，<a class="ae mb" href="https://arxiv.org/pdf/1808.01513.pdf" rel="noopener ugc nofollow" target="_blank">迈向细胞束的光谱理论</a> (2019)。应用与计算拓扑学杂志，3(4):315–358。</p><p id="8e74" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[15] J. Hansen和R. Ghrist，<a class="ae mb" href="https://arxiv.org/pdf/2005.12798.pdf" rel="noopener ugc nofollow" target="_blank">关于话语束的意见动态</a> (2021)。暹罗应用数学杂志，81(5):2033–2060。</p><p id="2e87" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[16]严格地说，限制图从节点的向量空间到边的向量空间。更正确的连接类比是<em class="ma">运输图</em> —对于一条边<em class="ma"> e </em> =( <em class="ma"> u </em>，<em class="ma"> v </em>)，从<em class="ma"> u </em>到<em class="ma"> e </em>的限制图与从<em class="ma"> e </em>到<em class="ma"> v </em>的转置图的组合。注意，虽然连接必须是可逆映射，但限制映射不是必须的。</p><p id="8785" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[17]光滑流形上的一个连接规定了一种对切向量场进行微分的方式(因此又有一个名字，<a class="ae mb" href="https://en.wikipedia.org/wiki/Covariant_derivative" rel="noopener ugc nofollow" target="_blank"> <em class="ma">协变导数</em> </a>)。有多种方法可以做到这一点。如果流形额外配备了黎曼度量，则存在一个唯一的“规范”连接，称为<a class="ae mb" href="https://en.wikipedia.org/wiki/Levi-Civita_connection" rel="noopener ugc nofollow" target="_blank"> <em class="ma">李维-奇维塔连接</em> </a>。</p><p id="0346" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[18] B. Chamberlain，J. Rowbottom等人，<a class="ae mb" href="https://arxiv.org/abs/2106.10934" rel="noopener ugc nofollow" target="_blank">GRAND:Graph Neural Diffusion</a>(2021)ICML。另见<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00">附带的博文</a>。</p><p id="de69" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[19]我们使用归一化的层拉普拉斯算子；参见我们论文中的定义4。</p><p id="f878" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[20]注意柄的尺寸<em class="ma"> d </em>与特征通道的数量不同。一个类似的例子是微分几何中的切向量场；在这种情况下，<em class="ma"> d </em>对应于流形的维度(及其切空间)。</p><p id="3231" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[21]因此，层扩散可以被看作是一个全球性的“同步”过程。A. Singer和H.-T. Wu在《纯数学和应用数学通讯》65(8)(2012)中的文章<a class="ae mb" href="https://arxiv.org/pdf/1102.0075.pdf" rel="noopener ugc nofollow" target="_blank">《向量扩散图和连接拉普拉斯算子》以及我的博士生A. Kovnatsky等人的文章</a><a class="ae mb" href="https://arxiv.org/pdf/1505.07676.pdf" rel="noopener ugc nofollow" target="_blank"> MADMM:流形上非光滑优化的一般算法</a> (2016)研究了图拉普拉斯算子与正交限制映射(称为<em class="ma">连接拉普拉斯算子</em>)的同步问题</p><p id="cb9c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[22]或者，<strong class="lg ja">x</strong>∈ker(δ)，即调和空间是拉普拉斯算子的核。</p><p id="d66f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[23] B. Weisfeiler和A. Lehman，将图简化为标准形式以及其中出现的代数(1968)。nauchno-Technicheskaya informatisia 2(9):12–16介绍了图同构测试，这种测试最近在GNN文献中很流行，因为它等价于某些形式的消息传递。参见<a class="ae mb" rel="noopener" target="_blank" href="/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49?sk=5c2a28ccd38db3a7b6f80f161e825a5a">我的帖子</a>和C. Morris等人，<a class="ae mb" href="https://arxiv.org/pdf/2112.09992.pdf" rel="noopener ugc nofollow" target="_blank"> Weisfeiler和Leman围棋机器学习:迄今为止的故事</a> (2021) arXiv:2112.09992的历史笔记和深入综述。</p><p id="c266" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[24] M. Defferrard等人<a class="ae mb" href="https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf" rel="noopener ugc nofollow" target="_blank">图的卷积神经网络与快速局部谱滤波</a> (2016) NIPS。</p><p id="2c8e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[25] T. Kipf和M. Welling，<a class="ae mb" href="https://arxiv.org/pdf/1609.02907.pdf" rel="noopener ugc nofollow" target="_blank">使用图卷积网络的半监督分类</a> (2017) ICLR。描述了GCN的建筑。</p><p id="2733" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[26]我们论文中的命题13。</p><p id="1421" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[27]我们论文中的命题14。</p><p id="25ad" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[28]见我们论文中的命题16。此外，在命题18中，我们陈述了在具有正边权重的图上的扩散不能分离仅具有来自通过边连接的不同类的两个节点的平凡的异嗜情况。不对称边权重有时在图ML文献中被称为“各向异性”，类似于连续扩散方程(其中该术语表示依赖于方向的扩散率)。</p><p id="fae7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[29] Y. Yan等，<a class="ae mb" href="https://arxiv.org/pdf/2102.06462.pdf" rel="noopener ugc nofollow" target="_blank">同一枚硬币的两面:图卷积神经网络中的异嗜性和过度光滑</a> (2021) arXiv:2102.06462。</p><p id="607b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[30] E. Chien等，<a class="ae mb" href="https://openreview.net/pdf?id=n6jl7fLxrP" rel="noopener ugc nofollow" target="_blank">自适应通用广义PageRank图神经网络</a> (2021) ICLR。</p><p id="779e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[31] D. Bo等，<a class="ae mb" href="https://ojs.aaai.org/index.php/AAAI/article/view/16514/16321" rel="noopener ugc nofollow" target="_blank">超越图卷积网络中的低频信息</a> (2021).</p><p id="3f41" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[32]我们论文中的命题19。</p><p id="29ce" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[33]我们论文中的命题21。</p><p id="0a87" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[34]郭，正交图神经网络(2021) arXiv:2109.11338 .</p><p id="93be" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[35]t . Cohen研究了连续O( <em class="ma"> d </em>)向量束，等变卷积网络(2021)，阿姆斯特丹大学博士论文。2021年4月，我是他的博士委员会的成员，一生中只有一次机会看到他穿着白色领带和燕尾服。这个框架的离散实例由m .魏勒等人进一步发展，<a class="ae mb" href="https://arxiv.org/pdf/2106.06020.pdf" rel="noopener ugc nofollow" target="_blank">坐标独立卷积网络</a> (2021) arXiv:2106.06020和P. de Haan等人，<a class="ae mb" href="https://openreview.net/pdf?id=Jnspzp-oIZE" rel="noopener ugc nofollow" target="_blank">规范等变网格CNN:几何图形上的各向异性卷积</a> (2021) ICLR。</p><p id="ca0e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[36] J. Hansen和T. Gebhart，Sheaf neuro networks(2020)neur IPS拓扑数据分析及其他研讨会。</p><p id="ffb7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">【37】克罗内克乘积<strong class="lg ja"> I </strong> ⊗ <strong class="lg ja"> W </strong> ₁的an <em class="ma"> n </em> × <em class="ma"> n </em>单位矩阵I和可学习的<em class="ma"> d </em> × <em class="ma"> d </em>权重矩阵<strong class="lg ja"> W </strong> ₁产生大小为<em class="ma"> nd </em> × <em class="ma"> nd </em>的块对角矩阵作用在<em class="ma"> nd </em>的每个块上</p><p id="c8fb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[38]对于<em class="ma"> d </em> =1，获得GCN的残差变量，在这种情况下，该层的形式为<strong class="lg ja">Y</strong>=σ((<strong class="lg ja">I</strong>-<strong class="lg ja">δ</strong>)<strong class="lg ja">XW</strong>)。</p><p id="628e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[39]本文中的定理25。</p><p id="b8e9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[40]即，对于任何ε&gt;0，存在一个矩阵<strong class="lg ja"> W </strong>，其中‖<strong class="lg ja">w</strong>&lt;ε使得ℰ((<strong class="lg ja">I</strong>⊗<strong class="lg ja">w</strong>)<strong class="lg ja">x</strong>)&gt;ℰ(<strong class="lg ja">x</strong>。参见我们论文中的27号提案。</p><p id="5f68" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[41]图形重新布线通常对图形进行预处理，以使其对消息传递更友好(例如，消除瓶颈和减少过度挤压现象)以及降低计算复杂度(例如，通过邻居采样)。</p><p id="e1e5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[42]“潜在图形学习”是GNN型架构的通称，该架构从数据构建和更新图形，以指导其发展。第一个这样的架构之一是由Y. Wang等人开发的<a class="ae mb" href="https://arxiv.org/pdf/1801.07829.pdf" rel="noopener ugc nofollow" target="_blank">用于在点云上学习的动态图形CNN</a>(2019)。ACM Trans图形38(5):146。看到我关于这个话题的<a class="ae mb" rel="noopener" target="_blank" href="/manifold-learning-2-99a25eeb677d?sk=1c855a020f09b72edfa50a8aba5f24a0">博文</a>。</p><p id="2f2c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[43] B. P. Chamberlain等人，<a class="ae mb" href="https://arxiv.org/pdf/2110.09443.pdf" rel="noopener ugc nofollow" target="_blank"> Beltrami流和图形上的神经扩散</a> (2021) NeurIPS。</p><p id="1ed2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[44] C .博德纳尔，f .弗拉斯卡，等，<a class="ae mb" href="http://proceedings.mlr.press/v139/bodnar21a/bodnar21a.pdf" rel="noopener ugc nofollow" target="_blank">魏斯费勒和雷曼go拓扑:消息传递单纯网络</a> (2021) ICML。</p><p id="d59e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[45] C .博德纳尔、f .弗拉斯卡等人，<a class="ae mb" href="https://arxiv.org/pdf/2106.12575.pdf" rel="noopener ugc nofollow" target="_blank">魏斯费勒和雷曼go cellular:CW Networks</a>(2021)neur IPS。</p><p id="5f29" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[46] J. Topping，F. Di Giovanni等人，<a class="ae mb" href="https://arxiv.org/pdf/2111.14522.pdf" rel="noopener ugc nofollow" target="_blank">通过曲率理解图的过度挤压和瓶颈</a> (2022) ICLR。见<a class="ae mb" rel="noopener" target="_blank" href="/over-squashing-bottlenecks-and-graph-ricci-curvature-c238b7169e16?sk=f5cf01cbd57b4fee8fb3a89d447e56a9">附随博文</a>。</p><p id="a6b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[47] F. Di Giovanni，G. Luise和M. M. Bronstein，<a class="ae mb" href="https://arxiv.org/pdf/2202.01185.pdf" rel="noopener ugc nofollow" target="_blank">曲率感知图嵌入的异构流形</a> (2022) arXiv:2202.01185。</p><p id="070d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[48]我们研究了节点分类问题中的表达能力，而Weisfeiler-Lehman形式主义通常应用于图分类。这两个框架都适用于这两种环境。</p><p id="3818" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[49]参见我们的“原型书”M. M. Bronstein等人的<a class="ae mb" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank">几何深度学习:网格、组、图、测地线和量规</a> (2021)以及我在2021年ICLR的<a class="ae mb" href="https://www.youtube.com/watch?v=w6Pw4MOzMuo" rel="noopener ugc nofollow" target="_blank">主题演讲</a>。</p><p id="9de0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[50]埃尔兰根方案对整个几何学和数学产生了深远的方法和文化影响。最初从克莱因的蓝图中排除的黎曼几何，在五十年后被卡坦纳入，导致微分几何的现代公式。用20世纪40年代范畴理论的创始人艾伦伯格和麦克莱恩的话来说，诸如代数拓扑、纤维束和束的理论以及范畴理论(粗略地说，是物体之间关系的理论)等数学新领域是“埃尔兰根计划的延续”。</p><p id="164c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[51]参见<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a?sk=3a058b427798d601ebb2d618fca89a8f">我的博客文章</a>更详细地描述了“物理启发”图形ML的想法。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="24ad" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作者感谢Taco Cohen、Justin Curry和Nils Hammerla对这篇文章的校对。关于图形深度学习的其他文章，请参见我在《走向数据科学》中的 <a class="ae mb" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="ma">其他帖子</em> </a> <em class="ma">，</em> <a class="ae mb" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="ma">订阅我的帖子</em> </a> <em class="ma">和</em> <a class="ae mb" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> YouTube频道</em> </a> <em class="ma">，获取</em> <a class="ae mb" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="ma">中等会员</em> </a> <em class="ma">，或者关注我的</em> <a class="ae mb" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> Twitter </em> </a></p></div></div>    
</body>
</html>