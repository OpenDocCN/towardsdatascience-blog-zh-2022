<html>
<head>
<title>Calculating Data Drift in Machine Learning using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python计算机器学习中的数据漂移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculating-data-drift-in-machine-learning-53676ff5646b#2022-10-24">https://towardsdatascience.com/calculating-data-drift-in-machine-learning-53676ff5646b#2022-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4169" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习模型的漂移检测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c982bb191ba169d264285054f3632518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-Xoxu7faRXZKIoOJ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/-EXF9shcTO0" rel="noopener ugc nofollow" target="_blank">Unsplash</a>Ralfs Blumbergs</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="b113" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文旨在提供与Python中的数据漂移相关的直觉和实现。它将涵盖两种计算漂移的方法之间的实现和差异，即交叉熵和KL散度。以下是这篇文章的提纲。</p><h2 id="5062" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">目录</h2><ul class=""><li id="97f4" class="mv mw it li b lj mx lm my lp mz lt na lx nb mb nc nd ne nf bi translated">什么是数据漂移？</li><li id="846f" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">漂移指标<br/> -交叉熵<br/> - KL散度</li><li id="b3b0" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">解决方案架构<br/> -需求</li><li id="9582" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">实现<br/> -生成数据<br/> -训练模型<br/> -生成观察值<br/> -计算漂移<br/> -可视化随时间的漂移</li><li id="6787" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">计算漂移的障碍</li><li id="0463" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">结束语</li><li id="173f" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">资源</li></ul><h1 id="fe95" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">什么是数据漂移？</h1><p id="874e" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">MLOps是构建成功的机器学习模型并将其部署到生产中的一个不可或缺的组件。数据漂移可以归入MLOps中的模型监控类别。它指的是量化观察数据相对于训练数据的变化。随着时间的推移，这些变化的影响可能会对模型生成的预测质量产生巨大影响，通常会变得更糟。跟踪与训练特征和预测相关联的漂移对于模型监控和识别何时应该重新训练模型应该是不可或缺的。</p><p id="bb07" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你可以参考我的另一篇文章，了解在生产环境中监控机器学习模型背后的概念和架构的更多细节<a class="ae ky" href="https://pub.towardsai.net/monitoring-machine-learning-models-in-production-1633f23d1e0b" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="65ca" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">您可能不希望监控与模型预测/特征相关联的漂移的唯一情况是，在生成预测的基础上定期重新训练模型。这可能是与时间序列模型的许多应用相关的常见事件。然而，还有各种其他的东西可以跟踪，以确定您正在生成的模型的质量。本文将主要关注与经典机器学习(分类、回归和聚类)相关的模型。</p><h1 id="c290" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">漂移度量</h1><p id="c115" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">下面概述的两个度量标准都是量化一对概率分布相似程度的统计度量。</p><h2 id="9aec" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">交叉熵</h2><p id="e6c6" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">交叉熵可以由以下公式定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b2ba8de78e8d6df7a1fced5bf0c80d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*dsK2lf5vwSHYhs0FOkVJRA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉熵公式。图片取自<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵维基百科</a>【2】。</p></figure><ul class=""><li id="5f5c" class="mv mw it li b lj lk lm ln lp oa lt ob lx oc mb nc nd ne nf bi translated">p:真实概率分布</li><li id="b29c" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">问:估计概率分布</li></ul><p id="2e5a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从信息论的角度来看，熵反映了消除不确定性所需的信息量[3]。请注意，分布A和B的交叉熵将不同于分布B和A的交叉熵。</p><h2 id="d34e" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">KL散度</h2><p id="d3d8" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">Kullback Leibler散度，也称为KL散度，可通过以下公式定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e560477fee3837d0e4abff889f9f0429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*BuAK1aK5-wE5c-yw7jlUFg.png"/></div></figure><ul class=""><li id="ed9a" class="mv mw it li b lj lk lm ln lp oa lt ob lx oc mb nc nd ne nf bi translated">p:真实概率分布</li><li id="8a16" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">问:估计概率分布</li></ul><p id="b803" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然后，kull back–lei bler散度被解释为使用针对Q优化的代码而非针对P优化的代码对P的样本进行编码所需的比特数的平均差异[1]。请注意，分布A和B的KL散度将不同于分布B和A的KL散度。</p><p id="1078" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">请注意，这两种度量都不是距离度量。这是因为度量缺乏对称性。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="828f" class="mc md it of b gy oj ok l ol om">entropy / KL divergence of A,B != entropy / KL divergence of B,A</span></pre><h2 id="d86f" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">解决方案架构</h2><p id="81b4" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">下图概述了机器学习生命周期在合并模型监控时的运行方式。如上面的需求所示，为了监控模型的性能，应该从训练阶段保存各种数据。即用于训练模型的特征和目标数据。这将提供一个比较新观察结果的基础事实数据源。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/0dc76f2b6f3b6a5095780bb62092106a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2B28cT-NCHMEDJOk.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型监控架构。图片由作者提供。</p></figure><h2 id="8fad" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">要求</h2><p id="13a6" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">以下模块和版本用于在本地运行实现中显示的代码。它们都是著名的数据科学/分析/机器学习库，因此安装特定版本对大多数用户来说应该不是问题。</p><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="b13c" class="mc md it of b gy oj ok l ol om">Python=3.9.12<br/>pandas&gt;=1.4.3<br/>numpy&gt;=1.23.2<br/>scipy&gt;=1.9.1<br/>matplotlib&gt;=3.5.1<br/>sklearn&gt;=1.1.2</span></pre><h1 id="1db9" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">履行</h1><p id="455c" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">我将通过一个使用合成数据的例子来展示如何计算数据随时间的漂移。请注意，为我生成的值将与您生成的值不一致，因为它们是随机生成的。此外，由于它是随机生成的，所以从提供的可视化/数据中没有任何真正有意义的结果可以解释。目的是为您的应用程序提供可重用和可重新配置的代码。</p><h2 id="38c3" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">生成数据</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="a8dc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上面的脚本将生成一个由1000行和列<code class="fe oq or os of b">uuid, feature1, feature2, feature3, target</code>组成的合成数据集。这将是我们训练模型的基础数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/104f191b5cec5e81a3391769d6f2749f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SiYEHSCUD7IXxsrxIn081Q.png"/></div></div></figure><h2 id="a3eb" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">火车模型</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="0746" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">出于本教程的目的，上面的脚本将允许您根据我们上面生成的特征和目标创建一个随机森林回归模型。假设这个模型将被推送到生产环境中，并且每天都会被调用。</p><h2 id="3ae6" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">生成观察结果</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="fbcb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上面的脚本将在模型被生产和调用的第一天生成与特性相关的观察。我们现在可以将地面实况训练数据相对于观察数据的差异可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/d65857841e1512e478dd9f88b4e50a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zY4A5tFLOsvcBbM3EV9_XQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">可视化特征1的训练数据与观察数据。图片由作者提供。</p></figure><p id="acca" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从上图中可以看出，在模型生产的第一天，对特征的观察比地面实况多。这是一个问题，因为我们不能比较两个长度不同的值列表。如果我们试图比较两个不同长度的数组，将会产生错误的结果。现在为了计算漂移，我们需要使观测值的长度等于地面真实数据的长度。我们可以通过创建N个桶，并确定每个桶中的观察频率来做到这一点。实质上是创建一个直方图。下面的代码片段将可视化这些分布的相互关系。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/6e1648643edafbe71cb09d60aed3b53c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dXOQhc8j8bHKmPnPWlCOpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">地面实况数据的特征分布与特征1的观测数据。图片由作者提供。</p></figure><p id="4e9d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">既然两个数据集大小相同，我们可以比较两个分布中的漂移。</p><h2 id="6603" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">计算漂移</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="41b2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上面的脚本概述了如何计算与观察数据相关的相对于训练数据的漂移(使用<code class="fe oq or os of b">scipy</code>中的<code class="fe oq or os of b">entropy</code>实现)。它首先通过<code class="fe oq or os of b">matplotlib</code>中的<code class="fe oq or os of b">hist</code>方法将输入向量的大小标准化为相同长度，通过<code class="fe oq or os of b">softmax</code>函数将这些值转换为概率，然后通过<code class="fe oq or os of b">entropy</code>函数最终计算漂移。</p><h2 id="ba1d" class="mc md it bd me mf mg dn mh mi mj dp mk lp ml mm mn lt mo mp mq lx mr ms mt mu bi translated">可视化随时间的漂移</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8cb6ec2f8480d481fa3f4bb780c2873a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiL-Kf_Q1bDI-U6weSZ96w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随着模型在生产中使用时间的推移，可视化与模型中每个特征相关联的漂移分数。图片由作者提供。</p></figure><p id="e11a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">根据您为数据集生成的结果，建议确定某个阈值，如果模型所依赖的大多数<strong class="li iu">重要</strong>特征的漂移分数超过该阈值，这将是重新训练模型的有力指标。通过<code class="fe oq or os of b">sklearn</code>或SHAP之类的东西，可以很容易地识别基于树的模型的特征重要性。</p><h1 id="d858" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">计算漂移的障碍</h1><p id="291d" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">在计算你正在使用的机器学习模型的数据漂移时，你可能会遇到各种各样的障碍。</p><ol class=""><li id="af7b" class="mv mw it li b lj lk lm ln lp oa lt ob lx oc mb ox nd ne nf bi translated">处理值为0的特征/预测。这将产生与两种漂移实现相关的零分频误差。解决这个问题的一个快速简单的方法是用一个非常接近零的小值来代替零。由于监控漂移可能是逐案进行的，所以要主动了解这可能对您正在处理的问题产生什么影响。</li><li id="d503" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb ox nd ne nf bi translated">比较一对不同长度的分布。假设您根据与每个功能和目标相关联的1，000次观察来训练模型。然而，当您每天生成预测时，根据平台获得的流量，功能和目标的观察量从1，000到10，000不等。这是有问题的，因为你不能比较两个不同大小的分布。要解决这个问题，您可以使用上述实现中的宁滨方法，将训练数据和观察值归入相同大小的组中，然后计算该数据之上的漂移。这可以通过<code class="fe oq or os of b">matplotlib</code>库中的<code class="fe oq or os of b">histogram</code>方法轻松完成。</li><li id="ba54" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb ox nd ne nf bi translated">使用<code class="fe oq or os of b">softmax</code>函数将频率转换为概率时，获取<code class="fe oq or os of b">NaN</code>值。这是因为<code class="fe oq or os of b">softmax</code>函数依赖于指数。您将在softmax的输出中收到<code class="fe oq or os of b">NaN</code>结果，因为计算机无法计算大数的指数。如果是这种情况，您可能希望研究另一个不是softmax的实现，或者研究将您传入的值规范化，以便softmax可以工作。</li></ol><h1 id="44e4" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">结束语</h1><p id="0a54" class="pw-post-body-paragraph lg lh it li b lj mx ju ll lm my jx lo lp nw lr ls lt nx lv lw lx ny lz ma mb im bi translated">本文重点讨论了在经典机器学习的应用中如何计算数据漂移。我回顾了KL散度和交叉熵等常见漂移计算指标背后的直觉和实现。</p><p id="294d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，本文概述了个人在试图计算漂移时会遇到的常见障碍。即，当有零值时被零除的误差，以及关于比较一对大小不同的分布的问题。请注意，这篇文章主要对那些不经常重新训练您的模型的人有帮助。模型监控将作为一种手段来衡量模型的成功，并确定它何时因漂移而表现倒退。两者都是模型开发阶段再培训或重复的指标。</p><p id="ec5f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你可以在我的GitHub页面<a class="ae ky" href="https://github.com/vatsal220/medium_articles/tree/main/data_drift" rel="noopener ugc nofollow" target="_blank">这里</a>查看与本教程相关的资源库。</p><p id="b18c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你想转型进入数据行业，并希望得到经验丰富的导师的指导和指引，那么你可能想看看最敏锐的头脑。Sharpest Minds是一个导师平台，导师(他们是经验丰富的实践数据科学家、机器学习工程师、研究科学家、首席技术官等。)将有助于你的发展和学习在数据领域找到一份工作。点击查看<a class="ae ky" href="https://www.sharpestminds.com/?r=vatsal-patal" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="dce6" class="nl md it bd me nm nn no mh np nq nr mk jz ns ka mn kc nt kd mq kf nu kg mt nv bi translated">资源</h1><ul class=""><li id="42e4" class="mv mw it li b lj mx lm my lp mz lt na lx nb mb nc nd ne nf bi translated">[1]https://en.wikipedia.org/wiki/Kullback-Leibler_divergence<a class="ae ky" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"/></li><li id="30ed" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">[2]https://en.wikipedia.org/wiki/Cross_entropy<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank"/></li><li id="bd1e" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated">[3]<a class="ae ky" href="https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/357963/what-the-difference-cross-entropy-and-KL-divergence</a></li><li id="ea5b" class="mv mw it li b lj ng lm nh lp ni lt nj lx nk mb nc nd ne nf bi translated"><a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html" rel="noopener ugc nofollow" target="_blank">https://docs . scipy . org/doc/scipy/reference/generated/scipy . stats . entropy . html</a></li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="8f9c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你觉得这篇文章有用，这里有一些我写的其他文章，你可能也会觉得有用。</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/word2vec-explained-49c52b4ccb71"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">Word2Vec解释道</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">解释Word2Vec的直观性&amp;用Python实现它</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/link-prediction-recommendation-engines-with-node2vec-c97c429351a8"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">使用Node2Vec的链接预测推荐引擎</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">在Python中使用节点嵌入进行链接预测</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/recommendation-systems-explained-a42fc60591ed"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">推荐系统解释</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">用Python解释和实现基于内容的协同过滤和混合推荐系统</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">用Jaro-Winkler和PageRank实现Python中的文本摘要</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">用Jaro-Winkler和PageRank构建一个文本摘要器</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="ps l pm pn po pk pp ks pb"/></div></div></a></div></div></div>    
</body>
</html>