<html>
<head>
<title>PyTorch vs. TensorFlow for Transformer-Based NLP Applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch与TensorFlow在基于变压器的NLP应用中的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-vs-tensorflow-for-transformer-based-nlp-applications-b851bdbf229a#2022-06-07">https://towardsdatascience.com/pytorch-vs-tensorflow-for-transformer-based-nlp-applications-b851bdbf229a#2022-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="259d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">当使用基于BERT的模型时，部署考虑应该是首要的</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79dd8ac1969c12066ff2432ae2efbaf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ogh1fUyExFYtXx4NMh7GiA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">皮查拜</a>从<a class="ae kv" href="https://www.pexels.com/photo/gray-scale-photo-of-gears-159298/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄。</p></figure><p id="bcfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">TL；DR: BERT是NLP中一个不可思议的进步。两个主要的神经网络框架都成功地完全实现了BERT，尤其是在HuggingFace的支持下。然而，尽管乍一看TensorFlow更容易进行原型设计和部署，但PyTorch在量化和一些GPU部署方面似乎更有优势。在开始一个基于BERT的项目时，应该考虑到这一点，这样你就不必像我们一样中途重新构建代码库。</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="0c9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像人工智能领域的许多事情一样，机会在于你能多快地改变和适应改进的性能。伯特及其衍生物无疑建立了一个新的基线。它很大，而且掌管一切。(事实上，<a class="ae kv" href="https://lemay.ai" rel="noopener ugc nofollow" target="_blank">我们最近有</a>这么多基于BERT的项目同时启动，我们需要全公司范围的培训，以确保每个人都有相同的编程风格。)</p><p id="ff68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://auditmap.ai" rel="noopener ugc nofollow" target="_blank">我们的另一家公司</a>最近遇到了一些与基于Tensorflow的模型相关的令人头疼的问题，希望你能从中学习。以下是我们在这个项目中学到的一些方面。</p><h1 id="4c28" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">模型可用性和存储库</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/72a7b6674f0d1319b8f25ad2752d072b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHzmIZ4oCqIh8YV5lTBHEA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">购买模特有时感觉像逛市场。<a class="ae kv" href="https://www.pexels.com/photo/photo-of-a-pathway-with-gray-and-black-buildings-792832/" rel="noopener ugc nofollow" target="_blank">好人</a>在<a class="ae kv" href="https://pexels.com" rel="noopener ugc nofollow" target="_blank">Pexels.com</a>上的照片。</p></figure><p id="b8bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想使用那些出版物刚出版的模特，你仍然需要通过GitHub。否则可以直接进入transformer模型库hubs，比如<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>、<a class="ae kv" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>和<a class="ae kv" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> PyTorch Hub </a>。</p><p id="a00b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT发布</a>几个月后，让它运行起来有点笨拙。自从<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>推动整合<a class="ae kv" href="https://arxiv.org/abs/1910.03771" rel="noopener ugc nofollow" target="_blank">变形金刚模型库</a>以来，这就有点悬而未决了。由于大多数(几乎所有)模型都可以在HuggingFace上正确检索，hugging face是任何变形金刚的第一和主要来源，所以这些天关于模型可用性的问题越来越少。</p><p id="96a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在某些情况下，模型只能在专有的存储库中获得。例如，Google 的<a class="ae kv" href="https://arxiv.org/pdf/1803.11175.pdf" rel="noopener ugc nofollow" target="_blank">通用句子编码器似乎仍然只能在</a><a class="ae kv" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>上使用。(在<a class="ae kv" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">发布</a>的时候，这是最好的单词和句子嵌入模型之一，所以这是一个问题，但是它已经被类似<a class="ae kv" href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" rel="noopener ugc nofollow" target="_blank"> MPNet </a>和<a class="ae kv" href="https://huggingface.co/sentence-transformers/sentence-t5-base" rel="noopener ugc nofollow" target="_blank"> Sentence-T5 </a>的东西取代了。)</p><p id="8831" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在撰写本文时，HuggingFace 上有<a class="ae kv" href="https://huggingface.co/models?library=tf&amp;sort=downloads" rel="noopener ugc nofollow" target="_blank">2669个Tensorflow模型，相比之下</a><a class="ae kv" href="https://huggingface.co/models?library=pytorch&amp;sort=downloads" rel="noopener ugc nofollow" target="_blank">有31939个PyTorch模型</a>。这主要是由于较新的模型首先作为PyTorch模型发布；学术界偏爱PyTorch模型，尽管这不是一个普遍的模型。</p><p id="347d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">要点:PyTorch有更多的模型，但是主要的模型在两个框架上都可用。</em> </strong></p><h1 id="7ea8" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">跨库GPU争用</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/72ae0c3462b25c583137c15208f11dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B0mtp-mfF0i91bvUb8VwRg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">纯粹的火力。照片由<a class="ae kv" href="https://www.pexels.com/photo/industry-technology-grey-music-8622912/" rel="noopener ugc nofollow" target="_blank">娜娜·杜瓦</a>在<a class="ae kv" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">Pexels.com</a>拍摄</p></figure><p id="4b0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">毫不奇怪，这些利维坦模型有巨大的计算需求，GPU将在训练和推理周期的不同点上涉及。此外，您可能将这些模型用作NLP/文档智能管道的一部分，其他库在预处理或自定义分类器期间会争夺GPU空间。</p><p id="e6fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">幸运的是，有许多流行的库已经在它们的后端使用Tensorflow和PyTorch，所以与其他模型友好相处应该很容易。例如，两个流行的NLP库，主要运行在Torch上(<a class="ae kv" href="https://github.com/explosion/spacy-transformers/blob/5a36943fccb66b5e7c7c2079b1b90ff9b2f9d020/requirements.txt#L3" rel="noopener ugc nofollow" target="_blank"> 1 </a>，<a class="ae kv" href="https://github.com/flairNLP/flair/blob/016cd5273f8f3c00cac119debd1a657d5f86d761/requirements.txt#L2" rel="noopener ugc nofollow" target="_blank"> 2 </a>)。</p><p id="c454" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">* <em class="ls">注意:SpaCy使用</em><a class="ae kv" href="https://thinc.ai/docs/usage-frameworks" rel="noopener ugc nofollow" target="_blank"><em class="ls">Thinc</em></a><em class="ls">来实现框架之间的可互换性，但是我们注意到，如果我们坚持使用基本PyTorch模型，会获得更多的稳定性、原生支持和可靠性。</em></p><p id="f4e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于单个框架，在定制的BERT模型和特定于库的模型之间共享GPU要容易得多。如果您可以共享一个GPU，那么部署成本就会降低。(稍后在“<a class="ae kv" href="#9499" rel="noopener ugc nofollow">量子化</a>”中对此有更多介绍。)在理想的部署中，有足够的资源来有效地扩展每个库；事实上，计算与成本之间的制约发生得非常快。</p><p id="eee3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您正在运行一个多步骤部署(比如说文档智能)，那么您将拥有一些通过将它们转移到GPU而得到改进的功能，比如句子化和分类。</p><p id="cb4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch具有本机GPU增量使用，通常为给定模型保留正确的内存边界。从他们的<a class="ae kv" href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management" rel="noopener ugc nofollow" target="_blank">到<em class="ls"> CUDA语义</em>文档</a>:</p><blockquote class="mu mv mw"><p id="b094" class="kw kx ls ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated">PyTorch使用缓存内存分配器来加速内存分配。这允许在没有设备同步的情况下快速释放内存。然而，由分配器管理的未使用的内存仍然会显示为在<code class="fe na nb nc nd b">nvidia-smi</code>中使用过。可以使用<code class="fe na nb nc nd b"><a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated" rel="noopener ugc nofollow" target="_blank">memory_allocated()</a></code>和<code class="fe na nb nc nd b"><a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated" rel="noopener ugc nofollow" target="_blank">max_memory_allocated()</a></code>来监控张量占用的内存，使用<code class="fe na nb nc nd b"><a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" rel="noopener ugc nofollow" target="_blank">memory_reserved()</a></code>和<code class="fe na nb nc nd b"><a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" rel="noopener ugc nofollow" target="_blank">max_memory_reserved()</a></code>来监控缓存分配器管理的内存总量。调用<code class="fe na nb nc nd b"><a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" rel="noopener ugc nofollow" target="_blank">empty_cache()</a></code>从PyTorch释放所有<strong class="ky ir">未使用的</strong>缓存内存，以便其他GPU应用程序可以使用这些内存。但是，张量占用的GPU内存不会被释放，因此它不能增加PyTorch可用的GPU内存量。</p></blockquote><p id="eb44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与TensorFlow相比，tensor flow具有默认的完全内存接管，您需要<a class="ae kv" href="https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth" rel="noopener ugc nofollow" target="_blank">来指定</a> <code class="fe na nb nc nd b"><a class="ae kv" href="https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth" rel="noopener ugc nofollow" target="_blank">incremental_memory_growth()</a></code>:</p><blockquote class="mu mv mw"><p id="3171" class="kw kx ls ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated">默认情况下，TensorFlow会映射进程可见的所有GPU的几乎所有GPU内存(以<code class="fe na nb nc nd b"><a class="ae kv" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars" rel="noopener ugc nofollow" target="_blank">CUDA_VISIBLE_DEVICES</a></code>为准)。这样做是为了通过减少内存碎片来更有效地使用设备上相对珍贵的GPU内存资源。要将TensorFlow限制到一组特定的GPU，请使用<code class="fe na nb nc nd b"><a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices" rel="noopener ugc nofollow" target="_blank">tf.config.set_visible_devices</a></code>方法。</p></blockquote><p id="e296" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">外卖:两个框架都在单个GPU上具备多模型部署能力，但Tensorflow的管理稍逊一筹。小心使用。</em>T3】</strong></p><h1 id="9499" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">量化</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/06f5b9c1c7c7aca55dff7d49a07147e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7AMvkW8tlgiWRwB4-TNTQA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://pexels.com" rel="noopener ugc nofollow" target="_blank">Pexels.com</a>上<a class="ae kv" href="https://www.pexels.com/photo/low-angle-view-of-lighting-equipment-on-shelf-257904/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄的照片。</p></figure><p id="2109" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Quantization" rel="noopener ugc nofollow" target="_blank">量化</a>主要涉及将<a class="ae kv" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.double" rel="noopener ugc nofollow" target="_blank"> Float64 </a>转换为<a class="ae kv" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.ubyte" rel="noopener ugc nofollow" target="_blank"> Unsigned Int8 </a>或<a class="ae kv" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.ushort" rel="noopener ugc nofollow" target="_blank"> UInt16 </a>以减少模型大小和完成单次计算所需的位数，这也是<a class="ae kv" href="https://arxiv.org/abs/1710.09282" rel="noopener ugc nofollow" target="_blank">一种广为接受的模型压缩技术</a>。这类似于图像的像素化和颜色损失。它还考虑了权重的分布，在它们的通用模型生成管道中，<a class="ae kv" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>和<a class="ae kv" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>都支持固定和动态范围量化。</p><p id="1ec0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">量化在模型性能优化中是一个值得采取的步骤，其主要原因是<em class="ls">性能</em>随时间推移的典型损失(由于延迟增加)比<em class="ls">质量</em>随时间推移的损失(例如F1下降)更昂贵。对此的另一种解释是“现在好不如以后更好”。</p><p id="e371" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有趣的是，我们看到训练后量化的F1平均分数下降了0.005(相比之下，训练中量化的F1平均分数下降了0.03–0.05)，对于我们的大多数客户和主要应用程序来说，这是一个可以接受的质量下降，特别是如果这意味着在更便宜的基础设施上和在合理的时间框架内运行的话。</p><p id="436e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">举个例子:考虑到我们在</em> <a class="ae kv" href="https://auditmap.ai" rel="noopener ugc nofollow" target="_blank"> <em class="ls">审计图</em> </a> <em class="ls">应用程序中分析的文本量，我们发现的大部分风险洞察都是有价值的，因为我们能够快速检索它们，向我们的审计师和风险经理客户传达他们的风险状况。我们大多数模型的F1分数在0.85到0.95之间，完全可以接受基于大规模分析的决策支持。</em></p><p id="b377" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些模型确实需要在GPU上训练和(<a class="ae kv" href="https://paperswithcode.com/paper/distilbert-a-distilled-version-of-bert" rel="noopener ugc nofollow" target="_blank">通常是</a>)运行才能有效。然而，如果我们想只在CPU上运行这些模型，我们需要从Float64表示转移到int8或uint8，以便在可接受的时间框架内运行。根据我的实验和检索到的例子，我将把我的观察范围限制在以下方面:</p><p id="0c38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我还没能找到一个简单或直接的机制来量化基于张量流的拥抱脸模型。</strong></p><p id="0790" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与PyTorch相比:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nf ng l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我在PyTorch中写的动态量化的一个简单例子。</p></figure><p id="079a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">要点:PyTorch中的量化是单行代码，可以部署到CPU机器上。Tensorflow不太流线型。</em> </strong></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="87bb" class="ma mb iq bd mc md nh mf mg mh ni mj mk jw nj jx mm jz nk ka mo kc nl kd mq mr bi translated">奖励类别:圈复杂度和编程风格</h1><p id="0b74" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">那么，如果PyTorch在它所提供的东西上有如此好的差异化，为什么TensorFlow仍然是一个考虑因素？在我看来，这是因为用TensorFlow编写的代码具有更少的活动部分，也就是说圈复杂度更低。</p><p id="19f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Cyclomatic_complexity" rel="noopener ugc nofollow" target="_blank">圈复杂度</a>是一种软件开发度量，用于评估一段代码中所有可能的代码路径。它被用作全面性、可维护性和每行代码的<a class="ae kv" href="https://en.wikipedia.org/wiki/Cyclomatic_complexity#Correlation_to_number_of_defects" rel="noopener ugc nofollow" target="_blank">错误</a>的代理。考虑到代码的可读性，类继承是一个循环步骤，而内置函数不是。从机器学习的角度来看，圈复杂度可以用来评估模型训练和推理代码的可读性。</p><p id="5f97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继续往下，PyTorch受面向对象编程的影响很大，而Tensorflow(通常，不总是)在其模型生成流程中更程序化。</p><p id="2066" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们为什么关心？因为复杂会滋生bug。库越简单，故障诊断和修复就越容易。简单的代码是可读的代码，可读的代码是可用的代码。</p><p id="258b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在PyTorch BERT管道中，圈复杂度随着数据加载器、模型实例化和训练而增加。</p><p id="c645" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看FashionMNIST数据加载器的公开例子。</p><p id="2368" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是PyTorch :</p><pre class="kg kh ki kj gt nr nd ns nt aw nu bi"><span id="bd32" class="nv mb iq nd b gy nw nx l ny nz"># PyTorch Example</span><span id="716f" class="nv mb iq nd b gy oa nx l ny nz">import os<br/>import pandas as pd<br/>from torchvision.io import read_image</span><span id="4730" class="nv mb iq nd b gy oa nx l ny nz">class CustomImageDataset(Dataset):<br/>    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):<br/>        self.img_labels = pd.read_csv(annotations_file)<br/>        self.img_dir = img_dir<br/>        self.transform = transform<br/>        self.target_transform = target_transform</span><span id="6f3c" class="nv mb iq nd b gy oa nx l ny nz">def __len__(self):<br/>        return len(self.img_labels)</span><span id="b924" class="nv mb iq nd b gy oa nx l ny nz">def __getitem__(self, idx):<br/>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])<br/>        image = read_image(img_path)<br/>        label = self.img_labels.iloc[idx, 1]<br/>        if self.transform:<br/>            image = self.transform(image)<br/>        if self.target_transform:<br/>            label = self.target_transform(label)<br/>        return image, label</span></pre><p id="9a38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是<a class="ae kv" href="https://www.tensorflow.org/tutorials/keras/classification" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>预建加载器:</p><pre class="kg kh ki kj gt nr nd ns nt aw nu bi"><span id="eaea" class="nv mb iq nd b gy nw nx l ny nz"># TensorFlow Example</span><span id="8f4c" class="nv mb iq nd b gy oa nx l ny nz">import tensorflow as tf<br/>import numpy as np</span><span id="389f" class="nv mb iq nd b gy oa nx l ny nz">fashion_mnist = tf.keras.datasets.fashion_mnist<br/><br/>(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span></pre><p id="ec3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这是TensorFlow中的一个预建函数，但它是典型的训练/测试拆分的示例。</p><p id="8b37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(<em class="ls">奖励:有人在TensorFlow中用PyTorch编写代码:</em> <a class="ae kv" rel="noopener" target="_blank" href="/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"> <em class="ls">使用BERT和TensorFlow </em> </a> <em class="ls"> ) </em></p><h1 id="9498" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结论</h1><p id="9b79" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">如果您有可用的GPU，您通常不会看到任何框架之间的任何重大差异。但是，请记住上面提到的边缘情况，因为您可能会发现自己正在重建从一个框架到另一个框架的整个管道。就像我一样。</p><p id="0ba9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">省时快乐！</p><p id="eaca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">马特。</p><p id="a26a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">如果您对本文或我们的AI咨询框架有其他问题，请通过</em><a class="ae kv" href="https://www.linkedin.com/in/mnlemay/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ls">LinkedIn</em></strong></a><strong class="ky ir"><em class="ls"/></strong><em class="ls">或通过</em><a class="ae kv" href="mailto:matt@lemay.ai" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="ls">email</em></strong></a><em class="ls">联系。</em></p><h1 id="c5f7" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">你可能喜欢的其他文章</h1><ul class=""><li id="ea91" class="ob oc iq ky b kz nm lc nn lf od lj oe ln of lr og oh oi oj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/mlops-for-batch-processing-running-airflow-on-gpus-dc94367869c6">用于批处理的MLOps:在GPU上运行气流</a></li><li id="400a" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated">数据集偏差:制度化的歧视还是足够的透明度？</li><li id="b92c" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated"><a class="ae kv" href="https://medium.com/@lsci/how-does-artificial-intelligence-create-value-bec14c785b40" rel="noopener">AI如何创造价值？</a></li><li id="05b8" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/implementing-a-corporate-ai-strategy-a64e641384c8">实施企业人工智能战略</a></li><li id="f789" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/outlier-aware-clustering-beyond-k-means-76f7bf8b4899">离群点感知聚类:超越K均值</a></li><li id="f10c" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/rorschach-tests-for-deep-learning-image-classifiers-68c019fcc9a9">深度学习图像分类器的罗夏测试</a></li></ul></div></div>    
</body>
</html>