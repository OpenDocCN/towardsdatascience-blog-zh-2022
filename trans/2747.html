<html>
<head>
<title>Building NLP Powered Applications with Hugging Face Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用拥抱脸变压器构建NLP供电的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-nlp-powered-applications-with-hugging-face-transformers-9f561effc84f#2022-06-14">https://towardsdatascience.com/building-nlp-powered-applications-with-hugging-face-transformers-9f561effc84f#2022-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="56d2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">和Docker部署在Google Chrome上</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/33ea56e253604646fcd81d06b2b6d7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RCoZ4AFNHslsmP-J"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布雷特·乔丹在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="41a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我最近完成了拥抱脸团队的几个人写的关于变形金刚的奇妙的新自然语言处理的书，并受到启发，将我的一些新知识用于一个基于NLP的小项目。在寻找一些想法的时候，我看到了泰赞·萨胡的一篇<a class="ae kv" href="https://medium.com/data-science-at-microsoft/developing-microsoft-edge-extensions-powered-by-sota-nlp-models-f3991f18daa4" rel="noopener">优秀博客文章</a>，他在文章中构建了一个微软Edge扩展来解释你屏幕上高亮显示的文本。我想更进一步，通过:</p><ul class=""><li id="1b35" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">a)用<strong class="ky ir"> ONNX运行时</strong>和<strong class="ky ir">量化</strong>优化模型推理</li><li id="44a5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">b)包括诸如摘要、命名实体识别(NER)和关键词提取的特征。</li></ul><p id="3db0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个想法是，这创造了最终的随笔伴侣，因为它可以帮助快速理解摘要和NER的文本，它可以让那些创造性的汁液随着转述的文本和关键词同义词流动。显然，我希望人们用它来重写他们自己的作品，而不是其他人的…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/eb3b2a6baec27db11a32bc32d4a07ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nn7HmZcgxgeDgU3585UUXw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">谷歌Chrome上的随笔伴侣演示(图片由作者提供_</p></figure><blockquote class="mh mi mj"><p id="28bb" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated"><strong class="ky ir"><em class="iq">TL；DR: </em> </strong> <a class="ae kv" href="https://github.com/jackmleitch/EssayCompanion" rel="noopener ugc nofollow" target="_blank"> <em class="iq">这个库</em> </a> <em class="iq">包含了本文提到的所有代码。ML的东西可以在</em><a class="ae kv" href="https://github.com/jackmleitch/EssayCompanion/tree/main/src" rel="noopener ugc nofollow" target="_blank"><em class="iq">src</em></a><em class="iq">文件夹中找到，Chrome扩展的东西在</em> <a class="ae kv" href="https://github.com/jackmleitch/EssayCompanion/tree/main/extension" rel="noopener ugc nofollow" target="_blank"> <em class="iq">扩展</em> </a> <em class="iq">文件夹中。</em></p></blockquote><h1 id="e07d" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">模型</h1><p id="d0ef" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">在这个扩展中有四个不同的模型(和tokenizers)在运行，其中三个是在拥抱脸中发现的！</p><h2 id="6b4e" class="nl mp iq bd mq nm nn dn mu no np dp my lf nq nr na lj ns nt nc ln nu nv ne nw bi translated"><a class="ae kv" href="https://huggingface.co/ramsrigouthamg/t5-large-paraphraser-diverse-high-quality" rel="noopener ugc nofollow" target="_blank"> T5释义模式</a></h2><p id="7661" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">这是所使用的最大型号，达到2.75Gb(！)，并根据拉姆斯里·古塔姆的解释进行了微调。<strong class="ky ir"> T5 </strong>是在非监督和监督任务的多任务混合上预先训练的编码器-解码器模型，并且对于该模型，每个任务被转换成文本到文本的格式。</p><p id="c801" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先使用<strong class="ky ir"> NLTK </strong>的句子分词器<a class="ae kv" href="https://www.nltk.org/api/nltk.tokenize.html" rel="noopener ugc nofollow" target="_blank"> sent_tokenize </a>将文本拆分成句子。然后，每个句子通过T5模型，每个句子的释义输出被连接起来，得到一个新的释义段落。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">释义模型(图片由作者提供)</p></figure><p id="b815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些示例输出:</p><p id="46b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对你的知识的最终考验是你将它传递给另一个人的能力。你将它从一个人传递给另一个人的能力是对你智力的最终衡量。</p><p id="6692" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很好，我们可以看到我们的转述文本是连贯的，并且具有与原文不同的结构！</p><h2 id="7570" class="nl mp iq bd mq nm nn dn mu no np dp my lf nq nr na lj ns nt nc ln nu nv ne nw bi translated"><a class="ae kv" href="https://huggingface.co/facebook/bart-large-cnn" rel="noopener ugc nofollow" target="_blank"> BART汇总模型</a></h2><p id="fcd4" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated"><strong class="ky ir"> BART </strong>也是一个编码器-解码器(seq2seq)模型，具有双向(像BERT)编码器和自回归(像GPT)解码器。通过(1)用任意噪声函数破坏文本，以及(2)学习模型以重建原始文本，来预训练BART。</p><p id="9a36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，最近一篇关于欧盟USB-C规则实施的250字长的新闻文章<a class="ae kv" href="https://www.eurogamer.net/portable-electronic-devices-sold-in-the-eu-will-require-usb-c-charging-by-autumn-2024" rel="noopener ugc nofollow" target="_blank">用55个字来概括:</a></p><blockquote class="mh mi mj"><p id="62c3" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated"><em class="iq">到2024年秋季，所有在欧盟销售的便携式电子设备都需要使用USB Type-C进行充电。其目的是通过只有一个“通用充电器”来减少电子垃圾，对消费者更加友好。受影响最大的将是苹果的iPhones和iPads，它们将不再能够使用lightning线缆。</em></p></blockquote><p id="20f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇！它工作得非常出色，简明扼要地挑出了文章中的所有要点。</p><h2 id="7b0a" class="nl mp iq bd mq nm nn dn mu no np dp my lf nq nr na lj ns nt nc ln nu nv ne nw bi translated"><a class="ae kv" href="https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english" rel="noopener ugc nofollow" target="_blank">NER模型</a></h2><p id="f8f1" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">使用了使用<a class="ae kv" href="https://huggingface.co/datasets/conll2003" rel="noopener ugc nofollow" target="_blank"> conll03英语数据集</a>为<strong class="ky ir"> NER </strong>微调的<strong class="ky ir"> DistilBERT </strong> base无壳模型。DistilBERT是一个比<strong class="ky ir"> BERT </strong>更小更快的模型，后者以自我监督的方式在相同的语料库上进行预训练，使用BERT基础模型作为教师。NER模型只是一个蒸馏编码器，在末尾添加了一个标记分类头来预测每个实体:人、位置、组织和杂项。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NER模型(作者图片)</p></figure><p id="c2f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在chrome扩展中，使用<strong class="ky ir"> SpaCy </strong>在HTML中呈现NER结果。下面是一些输出示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/71a2e319a6172f3c60752b2c8f165ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oxYlkFglB3vtsa1E.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">NER输出示例(图片由作者提供)</p></figure><h2 id="dec6" class="nl mp iq bd mq nm nn dn mu no np dp my lf nq nr na lj ns nt nc ln nu nv ne nw bi translated"><a class="ae kv" href="https://maartengr.github.io/KeyBERT/" rel="noopener ugc nofollow" target="_blank"> KeyBERT关键词提取模型</a></h2><p id="a760" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">该模型利用BERT文本嵌入和余弦相似性来寻找文档中与文档本身最相似的子短语。这个想法是这些子短语是文本中最重要的短语。</p><p id="3f5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先使用<strong class="ky ir"> KeyBERT </strong>模型从文本中提取关键词。一旦找到关键词，WordNet 就会被用来寻找每个关键词的同义词。值得注意的是，在WordNet中，相似的单词被分组到一个称为Synset的集合中，Synset中的<strong class="ky ir"> </strong>单词被词条化。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">KeyBERT模型(图片由作者提供)</p></figure><p id="c29a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在句子中找到的关键词和相关同义词'<em class="mk">最终考验你的</em> <strong class="ky ir"> <em class="mk">知识</em> </strong> <em class="mk">是你的能力</em> <strong class="ky ir"> <em class="mk">传达</em> </strong> <em class="mk">它到另一个'</em>是:</p><ul class=""><li id="1e21" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">知识:理解、认知、掌握</li><li id="7dae" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">运送:运输、搬运、交付</li></ul><h1 id="8942" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">模型优化</h1><p id="c105" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">在GPU上运行推理时，模型的初始性能是<em class="mk"> OK </em>，但在CPU上运行时性能非常差。甚至GPU推理也没有我希望的那么快，因为解码器模型必须顺序解码模型输出，而这是无法并行化的。内存也是一个问题，因为我希望能够在内存和计算资源有限的设备上使用这一扩展(我想到了我的<em class="mk">老式</em>2015 MacBook pro…)，其中两个型号的大小超过了1.5Gb！我的目标是尽可能减少这些模型的内存占用，优化CPU推理的性能，同时保持模型的性能。</p><p id="653e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">量化</strong>和<strong class="ky ir">蒸馏</strong>是两种常用于应对规模和性能挑战的技术。蒸馏已经用于NER模型，因为DistilBERT是O.G. BERT模型的蒸馏版本。在我的例子中，由于我有限的计算资源，T5/BART的提取是不可能的。</p><p id="6ac3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我采取的第一步是将PyTorch模型转换为ONNX，这是一种用于机器学习算法的开放表示格式，它允许我们为目标设备(本例中为CPU)优化推理。拥抱脸变压器库包括一个<a class="ae kv" href="https://github.com/huggingface/transformers/blob/main/src/transformers/convert_graph_to_onnx.py" rel="noopener ugc nofollow" target="_blank">工具</a>来轻松地转换模型到ONNX，这是用来转换蒸馏模型。转换编码器-解码器模型有点棘手，因为seq2seq转换目前不被Hugging Face的ONNX转换器支持。然而，我能够利用<a class="ae kv" href="https://github.com/Ki6an/fastT5" rel="noopener ugc nofollow" target="_blank">这个</a>奇妙的GitHub库，它分别转换编码器和解码器，并将两个转换后的模型包装在拥抱面Seq2SeqLMOutput类中。</p><p id="9d64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将模型转换为ONNX后，QInt8量化用于以较低位宽数逼近浮点数，显著减少了模型的内存占用，并提高了性能，因为计算可以进一步优化。然而，量化会引入性能损失，因为我们在转换中丢失了信息，但是已经广泛证明(例如，参见此处的<a class="ae kv" href="https://arxiv.org/pdf/2103.13630.pdf" rel="noopener ugc nofollow" target="_blank"/>)权重可以用8位整数表示，而性能没有显著下降。毫不奇怪，ONNX模型的量化超级容易！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型量化(图片作者提供)</p></figure><p id="439e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以针对CPU使用率优化模型推断！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">优化的NER推理(图片由作者提供)</p></figure><p id="1820" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型优化的结果如下所示。我们可以看到，我们的努力导致了大约2倍的大小缩减和大约3倍的延迟提升！</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk od oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/f3455fc697a400181f442074fcb0622e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*FG3bPPc-ALrYY0wOAZJVhg.png"/></div></figure><figure class="oc kk oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/99d26e0eb9c5e9185d61db1a65353c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*P02MwHTPxYRuCnQwIRFPvw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk oj di ok ol translated">PyTorch、ONNX和ONNX +量化模型(6核英特尔酷睿i7)之间的比较(图片由作者提供)</p></figure></div><h1 id="7449" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">使用FastAPI + Docker部署模型端点</h1><p id="b7bb" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">为了部署我们的应用程序，我使用了两个工具作为主要构建模块:<a class="ae kv" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> FastAPI </strong> </a>和<a class="ae kv" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Docker </strong> </a>。FastAPI使围绕模型构建web框架变得非常容易，Docker是一个容器化工具，允许我们在任何环境中轻松打包和运行应用程序。</p><p id="e4e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用FastAPI，我们将模型打包并构建一个API来与它通信。我们还使用Pydantic来验证用户输入和模型输出，我们必须非常小心！例如，我们确保输入文本是一个字符串，摘要模型的响应是一个字符串，关键字提取模型返回一个包含字符串列表的字典。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">FastAPI应用程序(图片由作者提供)</p></figure><p id="4678" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Docker然后被用来容器化API服务器，这允许我们在任何机器上运行应用程序，而不用担心复制我的确切环境的麻烦。为了构建服务器的Docker映像，我在项目的根文件夹中创建了一个Docker文件。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建Docker图像(由作者创建的图像)</p></figure><p id="4045" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在线托管扩展，我们可以使用Azure的容器服务或者T2 AWS的弹性容器服务。我还没有抽出时间来做这件事，但是计划在不久的将来做这件事！</p><h1 id="db8c" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">构建Google Chrome扩展</h1><p id="49a6" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">拼图的最后一块是构建由3部分组成的chrome扩展:</p><ul class=""><li id="111f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> manifest.json </strong>包含扩展的配置</li><li id="2759" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"><em class="mk">&amp;style . CSS</em></strong>定义用户界面</li><li id="a8c4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> popup.js </strong>与API通信，实现扩展的实际功能。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自扩展的API调用(图片由作者提供)</p></figure><p id="0300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我按照<a class="ae kv" href="https://medium.com/data-science-at-microsoft/developing-microsoft-edge-extensions-powered-by-sota-nlp-models-f3991f18daa4" rel="noopener">这个</a>神奇的教程来构建web扩展，所以如果你想做类似的事情，请查看一下。我自己做的唯一更改是更改输出以包含更多模型，并以HTML格式呈现NER结果。这是在网上找到的一些示例文本的最终产品的样子！</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk om oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/31c1ed778d70ea730520e6ea8e678a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*ub_81ZnHMaSN6DNHEzyhzA.png"/></div></figure><figure class="oc kk on oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ab32000ebc589508e22db9e667420aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*wE-hMXLbjsPT_HDW4bmW5A.png"/></div></figure></div><div class="ab cb"><figure class="oc kk oo oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/27521e7d5b0cbf58793b57a464977b08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*XAilzwEKf61jUDJVfK2W9w.png"/></div></figure><figure class="oc kk op oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/72121333eb8dc8ce59d0f23c9266cfee.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*o_sfqXFydpsrVI7Kwsa4LA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk oq di or ol translated">Essay Companion的输出示例(图片由作者提供)</p></figure></div><h1 id="cf5f" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">结束语</h1><p id="0483" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf ni lh li lj nj ll lm ln nk lp lq lr ij bi translated">在这篇博文中，我展示了如何利用最先进的transformer模型来构建“智能”的基于文本的应用程序。在CPU上优化模型推理之前，我首先为解释、摘要、名称实体识别和关键字提取找到合适的模型。然后，我使用FastAPI在API端点部署模型，并为了可再现性将应用程序容器化。</p><blockquote class="mh mi mj"><p id="428a" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated"><em class="iq">这个项目的所有代码都可以在</em> <a class="ae kv" href="https://github.com/jackmleitch/EssayCompanion" rel="noopener ugc nofollow" target="_blank"> <em class="iq">这个GitHub资源库</em> </a> <em class="iq">中找到。</em></p></blockquote><p id="5d09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mk">随时联系我</em><a class="ae kv" href="https://www.linkedin.com/in/jackmleitch/" rel="noopener ugc nofollow" target="_blank"><em class="mk">LinkedIn</em></a><em class="mk">！</em></p></div></div>    
</body>
</html>