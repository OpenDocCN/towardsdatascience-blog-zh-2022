<html>
<head>
<title>A Gentle Introduction to Self-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自学入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-self-learning-5d6d40349f7c#2022-10-25">https://towardsdatascience.com/a-gentle-introduction-to-self-learning-5d6d40349f7c#2022-10-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7c95" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/072fe01bfde18c3a669a0bdf75797987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iXDeCGNiMGQp14hL"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">格伦·卡斯滕斯-彼得斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c311" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将介绍自我学习的主题，然后从头开始实施两个简单的自我学习策略(几乎)来巩固我们的知识。</p><p id="c440" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个帖子是基于[1]和[2]的天方夜谭。</p><p id="7828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但首先，让我们谈谈什么是自我学习，以及作为一名数据科学家，为什么它是一个需要添加到您的工具箱中的相关工具。</p><p id="89ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在都知道，可用数据的数量已经增长了很多，这导致了深度学习模型的重生，使机器学习领域作为一个整体达到了一个新的性能目标。</p><p id="1b29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，即使有大量数据可用，标记的数据也很少。这是因为标注数据困难、昂贵且缓慢。因此，我们有大量未标记的数据，我们必须对它们有所了解。</p><p id="85e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，机器学习最成熟的领域是监督学习。正如我们许多人所知，处理无监督学习可能相当困难。正因为如此，什么是更好的，有一点点标记的数据还是去一个完全无监督的方法？</p><p id="f8e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，我没有能力回答这个问题，但因为它，半监督学习的领域已经到来。这个领域的重点是通过使用少量的已标记数据来理解大量的未标记数据。这种情况通常出现在图形问题中。</p><p id="7ac7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自我训练是进行半监督学习的一种方法。让我们深入研究一下。</p><p id="f18a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的代码可以在我的<a class="ae kv" href="https://github.com/TNanukem/paper_implementations/blob/main/Self-Training%20-%20Introduction.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>和<a class="ae kv" href="https://www.kaggle.com/tiagotoledojr/an-introduction-to-self-training" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。</p><h1 id="6fac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">谈论自我训练</h1><p id="4e99" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">自我训练背后的主要思想是使用标记和未标记的数据，让您的模型(或多个模型)从自身(或彼此)学习。</p><p id="07f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个过程中，你不仅要教会模型数据的模式，还要教会它如何使用未标记的数据来继续学习。</p><p id="9e3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将探讨两种自我训练的方法:伪标记和双分类器自我训练。</p><h1 id="6f12" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">伪标记</h1><p id="f4b1" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">伪标记的思想是为未标记的数据生成标记，然后使用这个新的标记数据用比以前看到的更多的信息来重新训练模型。</p><p id="7d47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，当您的模型出错时，这可能会有问题，这就是为什么我们不预测整个测试集，然后重新训练模型。我们将通过仅添加我们的模型更有信心的预测来迭代。我们通过选择最高的分类概率来做到这一点，因此，为了做到这一点，我们必须使用校准的分类器[2]。</p><p id="e8d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从定义要使用多少次伪标签迭代开始我们的代码。我们还将加载葡萄酒数据集，并定义一些参数供我们测试，以查看该方法如何改善或恶化结果:</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="e546" class="mu lt iq mq b gy mv mw l mx my">pseudo_label_iters = 20<br/>thresholds = [0.4, 0.6, 0.8, 1]<br/>null_percs = [0.2, 0.5, 0.7]</span><span id="de59" class="mu lt iq mq b gy mz mw l mx my"># Data Loading<br/>df = datasets.load_wine()</span></pre><p id="c9d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将把结果与没有在伪标记数据上训练的分类器进行比较。我们将迭代空值的百分比和阈值，以定义哪个是高置信度预测。</p><p id="6d1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还将校准我们的分类器，并用GridSearch超参数优化器对其进行优化。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="8c42" class="mu lt iq mq b gy mv mw l mx my">rf = RandomForestClassifier(n_jobs=-1)</span><span id="8b46" class="mu lt iq mq b gy mz mw l mx my">parameters = {<br/>    'n_estimators': [10, 50],<br/>    'class_weight': [None, 'balanced'],<br/>    'max_depth': [None, 5, 10]<br/>}</span><span id="d9df" class="mu lt iq mq b gy mz mw l mx my">results = pd.DataFrame()</span><span id="5b80" class="mu lt iq mq b gy mz mw l mx my">for threshold in tqdm(thresholds):<br/>    for null_perc in null_percs:<br/>        <br/>        # Creating a test set for us to validate our results (and compare to a non-self-learning classifier)<br/>        X_train, X_test, y_train, y_test = train_test_split(<br/>            df.data, df.target, test_size=0.3, shuffle=True)<br/>        <br/>        # Randomly removing null_perc % of labels from training set<br/>        rng = np.random.RandomState()<br/>        random_unlabeled_points = rng.rand(y_train.shape[0]) &lt; null_perc</span><span id="02a5" class="mu lt iq mq b gy mz mw l mx my">y_train[random_unlabeled_points] = -1<br/>        new_y_train = y_train.copy()</span><span id="9aee" class="mu lt iq mq b gy mz mw l mx my"># Training loop<br/>        for i in range(pseudo_label_iters):</span><span id="504b" class="mu lt iq mq b gy mz mw l mx my"># Select the labeled set<br/>            X = X_train[np.where(new_y_train != -1)]<br/>            y = new_y_train[np.where(new_y_train != -1)]</span><span id="f445" class="mu lt iq mq b gy mz mw l mx my"># Select the unlabeled set<br/>            X_un = X_train[np.where(new_y_train == -1)]<br/>            y_un = new_y_train[np.where(new_y_train == -1)]</span><span id="f6ac" class="mu lt iq mq b gy mz mw l mx my">if len(y_un) == 0:<br/>                break</span><span id="87be" class="mu lt iq mq b gy mz mw l mx my"># Hyperparameter optimization<br/>            rf_ = GridSearchCV(rf, parameters, cv=2).fit(X, y).best_estimator_</span><span id="99de" class="mu lt iq mq b gy mz mw l mx my"># Probability Calibration    <br/>            calibrated_clf = CalibratedClassifierCV(base_estimator=rf_,<br/>                                                    cv=2,<br/>                                                    ensemble=False)<br/>            calibrated_clf.fit(X, y)<br/>            preds = calibrated_clf.predict_proba(X_un)</span><span id="20ff" class="mu lt iq mq b gy mz mw l mx my"># Adding the high confidence labels<br/>            classes = np.argmax(preds, axis=1)<br/>            classes_probabilities = np.max(preds, axis=1)</span><span id="9fb0" class="mu lt iq mq b gy mz mw l mx my">high_confidence_classes = classes[np.where(classes_probabilities &gt;= threshold)]</span><span id="472d" class="mu lt iq mq b gy mz mw l mx my">y_un[np.where(classes_probabilities &gt;= threshold)] = high_confidence_classes</span><span id="78a8" class="mu lt iq mq b gy mz mw l mx my">new_y_train[np.where(new_y_train == -1)] = y_un</span><span id="4469" class="mu lt iq mq b gy mz mw l mx my"># Validation<br/>        X = X_train[np.where(new_y_train != -1)]<br/>        y = new_y_train[np.where(new_y_train != -1)]<br/>        calibrated_clf.fit(X, y)</span><span id="96f9" class="mu lt iq mq b gy mz mw l mx my">y_pred_self_learning = calibrated_clf.predict(X_test)</span><span id="f146" class="mu lt iq mq b gy mz mw l mx my">X = X_train[np.where(y_train != -1)]<br/>        y = y_train[np.where(y_train != -1)]</span><span id="b506" class="mu lt iq mq b gy mz mw l mx my">calibrated_clf.fit(X, y)<br/>        y_pred = calibrated_clf.predict(X_test)<br/>        <br/>        results = pd.concat([results, pd.DataFrame([{<br/>            'threshold': threshold, 'null_perc': null_perc,<br/>            'normal_acc': accuracy_score(y_test, y_pred),<br/>            'pseudo_acc': accuracy_score(y_test, y_pred_self_learning)<br/>        }])])</span></pre><p id="aa7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们可以看到改变数据中的空值百分比后的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/dca8f66e8da0d23cb924561d3b7d91c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*d02jsWwpnhvkRnFBUY5evg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伪标签的零百分比变化。由作者开发</p></figure><p id="a6b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们有阈值变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c189a5ffa25ff2e22fcc6a3eb38ffeda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*xbHjV5x4FdfFxIBdJ0_0Rg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伪标记的置信阈值变化。由作者开发</p></figure><p id="3291" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，自我训练方法开始产生更好的平均结果，并且变化越少，空值越多，我们对预测越有信心。尽管如此，差异并不明显，主要是因为我们处理的是一个相当小的“简单”数据集。</p><h1 id="56ea" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">两个分类器自训练</h1><p id="a47f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这种类型的自我训练看起来像伪标记，但我们不是每次都使用同一个分类器，而是使用两个分类器。在每一步，我们将在可用数据上训练一个分类器，然后我们将预测下一批新数据。然后，我们切换我们正在使用的分类器，这样做几次。</p><p id="8f1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们试图使模型更加健壮，因为我们总是根据前一个分类器的输入来提供新的分类器。</p><p id="1732" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个实现，我们将依赖于我们为伪标签制作的概念。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="1d8e" class="mu lt iq mq b gy mv mw l mx my">from sklearn.neural_network import MLPClassifier<br/>rf = RandomForestClassifier(n_jobs=-1)<br/>mlp = MLPClassifier()</span><span id="13d4" class="mu lt iq mq b gy mz mw l mx my">rf_param = {<br/>    'n_estimators': [10, 50],<br/>    'class_weight': [None, 'balanced'],<br/>    'max_depth': [None, 5, 10]<br/>}</span><span id="7406" class="mu lt iq mq b gy mz mw l mx my">mlp_param = {<br/>    'hidden_layer_sizes': [(50,), (50, 50), (5, 50, 50)],<br/>    'alpha': [0.0001, 0.001, 0.01]<br/>}</span><span id="3aab" class="mu lt iq mq b gy mz mw l mx my">results = pd.DataFrame()</span><span id="f016" class="mu lt iq mq b gy mz mw l mx my">for threshold in tqdm(thresholds):<br/>    for null_perc in null_percs:<br/>        <br/>        # Creating a test set for us to validate our results (and compare to a non-self-learning classifier)<br/>        X_train, X_test, y_train, y_test = train_test_split(<br/>            df.data, df.target, test_size=0.3, shuffle=True)<br/>        <br/>        # Normalizing the data<br/>        scaler = StandardScaler()<br/>        X_train = scaler.fit_transform(X_train)<br/>        X_test = scaler.transform(X_test)<br/>        <br/>        # Randomly removing null_perc % of labels from training set<br/>        rng = np.random.RandomState()<br/>        random_unlabeled_points = rng.rand(y_train.shape[0]) &lt; null_perc</span><span id="e1e9" class="mu lt iq mq b gy mz mw l mx my">y_train[random_unlabeled_points] = -1<br/>        new_y_train = y_train.copy()</span><span id="ee51" class="mu lt iq mq b gy mz mw l mx my"># Training loop<br/>        for i in range(pseudo_label_iters):<br/>            # Choose the classifier to use<br/>            if i % 2 == 0:<br/>                clf = rf<br/>                parameters = rf_param<br/>            else:<br/>                clf = mlp<br/>                parameters = mlp_param</span><span id="9625" class="mu lt iq mq b gy mz mw l mx my"># Select the labeled set<br/>            X = X_train[np.where(new_y_train != -1)]<br/>            y = new_y_train[np.where(new_y_train != -1)]</span><span id="6e11" class="mu lt iq mq b gy mz mw l mx my"># Select the unlabeled set<br/>            X_un = X_train[np.where(new_y_train == -1)]<br/>            y_un = new_y_train[np.where(new_y_train == -1)]</span><span id="85dd" class="mu lt iq mq b gy mz mw l mx my">if len(y_un) == 0:<br/>                break</span><span id="24a4" class="mu lt iq mq b gy mz mw l mx my"># Hyperparameter optimization<br/>            clf_ = GridSearchCV(clf, parameters, cv=2).fit(X, y).best_estimator_</span><span id="8ede" class="mu lt iq mq b gy mz mw l mx my"># Probability Calibration    <br/>            calibrated_clf = CalibratedClassifierCV(base_estimator=clf_,<br/>                                                    cv=2,<br/>                                                    ensemble=False)<br/>            calibrated_clf.fit(X, y)<br/>            preds = calibrated_clf.predict_proba(X_un)</span><span id="f6f9" class="mu lt iq mq b gy mz mw l mx my"># Adding the high confidence labels<br/>            classes = np.argmax(preds, axis=1)<br/>            classes_probabilities = np.max(preds, axis=1)</span><span id="35bd" class="mu lt iq mq b gy mz mw l mx my">high_confidence_classes = classes[np.where(classes_probabilities &gt;= threshold)]</span><span id="ebc6" class="mu lt iq mq b gy mz mw l mx my">y_un[np.where(classes_probabilities &gt;= threshold)] = high_confidence_classes</span><span id="249c" class="mu lt iq mq b gy mz mw l mx my">new_y_train[np.where(new_y_train == -1)] = y_un</span><span id="49c5" class="mu lt iq mq b gy mz mw l mx my"># Validation<br/>        X = X_train[np.where(new_y_train != -1)]<br/>        y = new_y_train[np.where(new_y_train != -1)]<br/>        calibrated_clf.fit(X, y)</span><span id="2024" class="mu lt iq mq b gy mz mw l mx my">y_pred_self_learning = calibrated_clf.predict(X_test)</span><span id="f786" class="mu lt iq mq b gy mz mw l mx my">X = X_train[np.where(y_train != -1)]<br/>        y = y_train[np.where(y_train != -1)]</span><span id="d6b3" class="mu lt iq mq b gy mz mw l mx my">calibrated_clf.fit(X, y)<br/>        y_pred = calibrated_clf.predict(X_test)<br/>        <br/>        results = pd.concat([results, pd.DataFrame([{<br/>            'threshold': threshold, 'null_perc': null_perc,<br/>            'normal_acc': accuracy_score(y_test, y_pred),<br/>            'pseudo_acc': accuracy_score(y_test, y_pred_self_learning)<br/>        }])])</span></pre><p id="d075" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">至于结果，我们有零百分比变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/b6f98b10b02481fb5b7f575313b40434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*DsB-Z6heoLtvob7krnUy_Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两个分类器的百分比变化为零。由作者开发</p></figure><p id="4425" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阈值变化:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b8910bc2daeec9c4a7237161783cb7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*zdhd2KKOuVyRv855zIY5sw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两个分类器的置信阈值变化。由作者开发</p></figure><p id="5899" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于在这种情况下，我们使用了两个分类器，因此可以合理地预期数据量越大越好，即使它是未标记的。正如我们从结果中看到的，自我训练方法勉强能够克服默认分类器。</p><h1 id="3142" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="875f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如我常说的，数据科学是关于使用正确的工具解决业务问题，而自我培训是你工具箱中应该有的另一种工具，可以帮助你处理一系列新问题。</p><p id="3d5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于它还有很多内容要讲，但是我希望这个初步的介绍可以帮助您入门。</p><p id="93f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]阿米尼，马西-雷扎&amp;费奥法诺夫，瓦西里&amp;保莱托，洛伊奇&amp;德维杰夫，艾米莉&amp;马克西莫夫，尤里。(2022).自我训练:一项调查。</p><p id="d36d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://scikit-learn.org/stable/modules/calibration.html#calibration" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/calibration . html # calibration</a></p></div></div>    
</body>
</html>