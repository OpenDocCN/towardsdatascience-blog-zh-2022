<html>
<head>
<title>Papers Simplified: »Anticorrelated Noise Injection for Improved Generalization«</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文简化:反相关噪声注入用于改进推广</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/papers-simplified-anticorrelated-noise-injection-for-improved-generalization-8dd379af059c#2022-08-18">https://towardsdatascience.com/papers-simplified-anticorrelated-noise-injection-for-improved-generalization-8dd379af059c#2022-08-18</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="986e" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/machine-learning-research" rel="noopener">机器学习研究</a></h2><div class=""/><div class=""><h2 id="e2d9" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">了解在梯度更新期间添加噪声如何有助于概化，包括在TensorFlow中的实现</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/bef1884bfea07b8d7cd7cfb4582901fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-lbSr2hQLR0kYjPh"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@wrongtog?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在</a><a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5936" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在本文中，我们将查看安东尼奥·奥维多、汉斯·克斯汀、弗兰克·普罗斯克、弗朗西斯·巴赫和奥雷连·卢奇于2022年2月6日发表的一篇论文。它被称为<strong class="ll je">反相关噪声注入，用于改进泛化</strong>【1】，而<a class="ae li" href="https://arxiv.org/abs/2202.02831" rel="noopener ugc nofollow" target="_blank">你可以在这里找到</a>。</p><p id="79d2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我会这样总结这篇论文:</p><blockquote class="mf"><p id="02bc" class="mg mh iu bd mi mj mk ml mm mn mo me dk translated">当使用梯度下降优化复杂函数时(即，当训练神经网络时)，在梯度更新期间以智能方式添加噪声。</p></blockquote><p id="29a6" class="pw-post-body-paragraph lj lk iu ll b lm mp ke lo lp mq kh lr ls mr lu lv lw ms ly lz ma mt mc md me in bi translated">让我们深入研究一下这张纸，看看与不太聪明的方法相比，聪明的方法是什么。在摘要中，作者写了以下内容:</p><blockquote class="mv mw mx"><p id="b10d" class="lj lk mu ll b lm ln ke lo lp lq kh lr my lt lu lv mz lx ly lz na mb mc md me in bi translated">将人工噪声注入梯度下降(GD)通常被用来提高机器学习模型的性能。<strong class="ll je">通常，在这种扰动梯度下降(PGD) </strong>方法中使用 <strong class="ll je">不相关的噪声。但是，不知道这是否是最佳的，也不知道其他类型的噪声是否可以提供更好的泛化性能。在本文中，我们放大了<strong class="ll je">关联连续PGD步骤</strong>的扰动的问题。我们考虑了各种目标函数，发现带有反相关扰动的GD<strong class="ll je">(“反PGD”)比GD和标准(不相关的)PGD </strong>更具普遍性。为了支持这些实验发现，我们还得出了一个理论分析，表明<strong class="ll je">反PGD运动到更宽的最小值，而广东和PGD仍然停留在次优区域甚至发散。</strong>反相关噪声和泛化之间的这种新联系为利用噪声来训练机器学习模型的新方法开辟了领域。</strong></p></blockquote><p id="1750" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这真的让我开始思考。在梯度下降过程中添加噪声不仅有帮助。如果该噪声在梯度下降步骤中<strong class="ll je">相关</strong>，则优化结果在某些方面甚至更好。这种方法被作者称为<strong class="ll je">反PGD </strong>。</p></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="87e5" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在这篇文章中，我不会向你解释所有的(令人兴奋！)数学细节。相反，我将为您提供一些实现和图片，以便理解本文的主旨。我还尽力创建了论文中提到的优化器的<strong class="ll je">实现，但是要小心使用代码，因为我在这方面也不是专家。<strong class="ll je">但是我很高兴得到任何反馈</strong>！</strong></p><h1 id="231d" class="ni nj iu bd nk nl nm nn no np nq nr ns kj nt kk nu km nv kn nw kp nx kq ny nz bi translated">先决条件</h1><p id="f6b9" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">为了理解<strong class="ll je">反PGD(反扰动梯度下降)</strong>是什么，让我们简单回顾一下GD和衍生算法如SGD和PGD是如何工作的。我在本文的先决条件部分解释了关于GD和SGD的重要内容</p><div class="of og gq gs oh oi"><a rel="noopener follow" target="_blank" href="/papers-simplified-gradients-without-backpropagation-96e8533943fc"><div class="oj ab fp"><div class="ok ab ol cl cj om"><h2 class="bd je gz z fq on fs ft oo fv fx jd bi translated">论文简化:无反向传播的梯度</h2><div class="op l"><h3 class="bd b gz z fq on fs ft oo fv fx dk translated">如何评估一个函数并一次计算其梯度的近似值</h3></div><div class="oq l"><p class="bd b dl z fq on fs ft oo fv fx dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow lc oi"/></div></div></a></div><p id="9263" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">但是让我给你一个简略的版本。</p><h2 id="1a18" class="ox nj iu bd nk oy oz dn no pa pb dp ns ls pc pd nu lw pe pf nw ma pg ph ny ja bi translated">梯度下降</h2><p id="7362" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">让我们假设我们想要最小化一个函数<em class="mu"> f </em>，其梯度表示为∇ <em class="mu"> f </em> ( <em class="mu"> θ </em>)。然后，GD随机初始化模型的参数<em class="mu"> θ </em>，然后用简单的更新规则<strong class="ll je">t15】θ←<em class="mu">θ—α</em>∇<em class="mu">f</em>(<em class="mu">θ</em>)</strong>进行迭代更新，直到收敛，即<em class="mu"> θ </em>或<em class="mu">f</em>(<em class="mu">θ</em>)<em class="mu"/>do<em class="mu"> α </em>是学习率<em class="mu">。</em></p><p id="d91b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对于<strong class="ll je">凸函数</strong>，如果学习率足够小，GD保证找到全局——也是唯一的局部——最小值。例如，对L2损失进行线性回归就是这种情况。对于<strong class="ll je">非凸函数</strong>，GD可能会陷入(许多)局部极小值之一。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pi"><img src="../Images/0684d35063a57df2cd77ffe6d3b5a4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8bAa0NHwFHOXuvZcd7g9WQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><h2 id="360d" class="ox nj iu bd nk oy oz dn no pa pb dp ns ls pc pd nu lw pe pf nw ma pg ph ny ja bi translated">随机梯度下降</h2><p id="b2c0" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">SGD也做同样的事情，但是它<strong class="ll je">估计</strong> ∇ <em class="mu"> f </em> ( <em class="mu"> θ </em>)而不是像GD那样精确地计算它。这有几个优点:</p><ul class=""><li id="62d2" class="pj pk iu ll b lm ln lp lq ls pl lw pm ma pn me po pp pq pr bi translated"><strong class="ll je">使用较少的RAM </strong>，因为我们不需要解析完整的数据集来计算梯度</li><li id="8e35" class="pj pk iu ll b lm ps lp pt ls pu lw pv ma pw me po pp pq pr bi translated"><strong class="ll je">更快的收敛</strong> <strong class="ll je">(在历元数中)</strong>因为在每个历元中关于dataset_size / batch_size的梯度更新发生</li><li id="ab7a" class="pj pk iu ll b lm ps lp pt ls pu lw pv ma pw me po pp pq pr bi translated">根据罗伯特·克莱恩伯格、李沅芷和杨远[2]的说法，与GD相比，它倾向于找到更好的局部极小值。</li></ul><p id="56d2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第三点肯定不是显而易见的，你应该查看<a class="ae li" href="https://arxiv.org/abs/1802.06175" rel="noopener ugc nofollow" target="_blank">这篇论文</a>来更深入地探讨这个话题。基本上，从GD到SGD <strong class="ll je">平滑</strong>我们想要优化的功能(假设<strong class="ll je">从现在开始最小化</strong>):</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj px"><img src="../Images/4436de6df708601bb20761dd767d7415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFHUOYKUqILcEVlNnwtcyg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><h2 id="baa2" class="ox nj iu bd nk oy oz dn no pa pb dp ns ls pc pd nu lw pe pf nw ma pg ph ny ja bi translated">扰动梯度下降</h2><p id="3073" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">这里，我们稍微改变了更新规则:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj py"><img src="../Images/64772122bdd7333b446d46b342ef5e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1oXfoZlfcUKm-F6KuE9Jw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="c254" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">其中<em class="mu">ξₙ</em>(<strong class="ll je">扰动</strong> ) <em class="mu"> </em>来自一个<strong class="ll je">居中的正态分布<em class="mu"> N </em> (0，<em class="mu"> σ </em>)，并且是随机独立的</strong>。<strong class="ll je"> </strong>在梯度更新期间添加这种噪声可能有助于优化器摆脱局部最小值。然而，作者无法观察到使用PGD训练的模型实现了更好的泛化性能。尽管如此，通过给这个想法一点小小的改变，作者使它变得更加有用。</p><h1 id="077e" class="ni nj iu bd nk nl nm nn no np nq nr ns kj nt kk nu km nv kn nw kp nx kq ny nz bi translated"><strong class="ak">反扰动梯度下降</strong></h1><p id="209f" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">更准确地说，作者认为拥有反相关扰动是有益的。他们伪造它们的方式再简单不过了:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pz"><img src="../Images/882401654740881f151a4b3f47b58425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pSGEGo_5x7GrHaVfGuVZQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="ca9a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里——再一次——ξₙ独立于一个<em class="mu"> N </em> (0，<em class="mu"> σ </em>分布。这已经足以产生完美的反相关扰动，这意味着</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qa"><img src="../Images/9867329eb6ebc9cb68b5e85bf5baa838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*mQj6GiF7XCus5xelOr8xFg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片来自他们的论文。</p></figure><p id="e788" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里，<em class="mu"> I </em>只是单位矩阵。为了得到为什么这是真的直觉，想象一下ξₙ是一维的。然后我们可以忽略换位，可以计算</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qb"><img src="../Images/8befd97e2bf2b332f382ef1f008b2898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vATNQmbLHLcvDca4ZxTlDQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="598f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这同样适用于更高维度。不错！</p></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="c48f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后作者推断这些扰动对损失函数<em class="mu"> f </em> = <em class="mu"> L </em>的最小化有以下影响:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qc"><img src="../Images/6e23d0bfd7745a5e63f161e17945629d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHotN8WVYnT_h3fFspVCyw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的报纸上。</p></figure><blockquote class="mv mw mx"><p id="5d33" class="lj lk mu ll b lm ln ke lo lp lq kh lr my lt lu lv mz lx ly lz na mb mc md me in bi translated"><strong class="ll je"> <em class="iu">注:</em> </strong> <em class="iu">一个函数的</em> <a class="ae li" href="https://en.wikipedia.org/wiki/Hessian_matrix" rel="noopener ugc nofollow" target="_blank"> <em class="iu">黑森</em> </a> <em class="iu">只是广义二阶导数。例如，如果有函数</em> f <em class="iu"> ( </em> x <em class="iu">，</em> y <em class="iu"> ) = </em> xy <em class="iu">，则给出的黑纱为</em></p></blockquote><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qb"><img src="../Images/b44a60c094ec130559469b29826808ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2UQqST2GJpw156ZqrnR45w.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><blockquote class="mv mw mx"><p id="980a" class="lj lk mu ll b lm ln ke lo lp lq kh lr my lt lu lv mz lx ly lz na mb mc md me in bi translated">轨迹Tr就是主对角线的和——从左上到右下，即0 + 6  xy <em class="iu">。</em></p></blockquote><p id="c354" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">主要要点是:</p><blockquote class="mf"><p id="cb39" class="mg mh iu bd mi mj mk ml mm mn mo me dk translated">我们不是试图最小化损失函数，而是试图最小化损失加上其二阶导数的大小，二阶导数充当正则化项。</p></blockquote><p id="3ee6" class="pw-post-body-paragraph lj lk iu ll b lm mp ke lo lp mq kh lr ls mr lu lv lw ms ly lz ma mt mc md me in bi translated">这也与从GD切换到SGD如何平滑损失函数有关。</p><h2 id="c539" class="ox nj iu bd nk oy oz dn no pa pb dp ns ls pc pd nu lw pe pf nw ma pg ph ny ja bi translated">反PGD如何对抗广东和PGD</h2><p id="f08d" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">好的，我们加入了一些粗麻布和一条现在用于正规化的痕迹。但是为什么呢？为什么有这个正则化子是有益的？我们将在本节中找到答案！</p></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="8e26" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">为了给出一个直觉，作者在所谓的<strong class="ll je">加宽谷函数</strong>上测试他们的反PGD</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qd"><img src="../Images/facd23bc17937fb8fe6b820f7c6cd0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jfhd3z1OBeN3OgGJZ6rAVA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="18b8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">他们试图将其最小化。虽然最小化这个函数是微不足道的(设置v=0或所有u = 0)，但作者(还有我)认为，例如，我们在(0，0，…，0)处的最小值比在(0，100，100，…，100)处的最小值更好一些。要了解原因，请看他们论文的截图:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qe"><img src="../Images/fcae0456b80a8651a0c5a62cc6371212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cltm1F7ia4WJqbeEueYxOQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的报纸上。</p></figure><p id="fe78" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在左边，你可以看到一个函数图。原点(0，…，0)的最小值被<strong class="ll je">平地</strong>包围，而远离原点的最小值则在<strong class="ll je">越来越陡</strong>的山谷中。这是图表中的大图:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qf"><img src="../Images/5bd4c33930eb4740df40b47fa54339a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FapME8wfjA5ZvD9jIIpO1A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的论文中，加上我的注解。</p></figure><p id="7687" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">因此，在某种意义上，我们希望我们的优化器在原点找到最小值，因为它<em class="mu">感觉更安全</em>。如果我们在原点稍微摆动一下最小值，我们仍然会有一点损失。然而，如果我们达到最小值(0，100，…，100)，稍微改变<em class="mu">v</em>——例如从0到0.1——会使<strong class="ll je">损失飙升到100 <em class="mu"> d </em> </strong>，维度<em class="mu"> d </em>越高，损失越大。</p><blockquote class="mf"><p id="27cc" class="mg mh iu bd mi mj mk ml mm mn mo me dk translated">当然，反PGD在起源中找到了好的最小值。</p></blockquote><p id="587f" class="pw-post-body-paragraph lj lk iu ll b lm mp ke lo lp mq kh lr ls mr lu lv lw ms ly lz ma mt mc md me in bi translated">GD似乎找到了退而求其次的东西，<strong class="ll je"> PGD甚至走向被高山环绕的<em class="mu">险峻</em>极小</strong>。😅</p></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="a861" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">反PGD发现它是因为黑森的痕迹告诉它:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qg"><img src="../Images/00093eaf00f743839c1b5c68420c2b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b4Sm9wGqWXogdSRnhAgszA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的报纸上。</p></figure><p id="d189" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在哪里</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qe"><img src="../Images/72b04611b34eb75e968b90d51620c4a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02gqKZ5CRgMEpla_McHkPg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="53e3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果我们将此作为正则项添加到<em class="mu"> L </em>中，优化器将被迫寻找最小值(0，…，0)，因为优化目标变为</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qh"><img src="../Images/50e94820594e417b127a45ab47b39f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXOMJQB3bHHO8K1lr09qOQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure><p id="de82" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">而这个函数最小化正好是对于<em class="mu"> u </em> = (0，… 0)和<em class="mu"> v </em> = 0，没有别的。</p></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="1f06" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">作者声称，这不仅适用于加宽谷函数，而且更是一个一般概念。从他们的论文来看:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qi"><img src="../Images/193ecb83ae45f59463c2935f66b71e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BgMrBu--NbQKpfy9YMzh1Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的报纸上。</p></figure><h2 id="1285" class="ox nj iu bd nk oy oz dn no pa pb dp ns ls pc pd nu lw pe pf nw ma pg ph ny ja bi translated">履行</h2><p id="c54c" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">你甚至可以在实践中尝试一下，因为据我所知，我<strong class="ll je">实现了SGD </strong>(让你开始了解如何编写优化器)<strong class="ll je">、PGD </strong>和反PGD<em class="mu"/>。如有错误，请写评论！</p><pre class="kt ku kv kw gu qj qk ql bn qm qn bi"><span id="cf01" class="qo nj iu qk b be qp qq l qr qs">import tensorflow as tf<br/><br/><br/>class SGD(tf.keras.optimizers.Optimizer):<br/>    def __init__(self, learning_rate=0.01, name="SGD", **kwargs):<br/>        super().__init__(name, **kwargs)<br/>        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))<br/><br/>    @tf.function<br/>    def _resource_apply_dense(self, grad, var):<br/>        var_dtype = var.dtype.base_dtype<br/>        lr_t = self._decayed_lr(var_dtype)  # learning rate decay<br/>        new_var = var - grad * lr_t<br/>        var.assign(new_var)<br/><br/>    def get_config(self):<br/>        base_config = super().get_config()<br/>        return {<br/>            **base_config,<br/>            "learning_rate": self._serialize_hyperparameter("learning_rate"),<br/>        }<br/><br/><br/>class PGD(tf.keras.optimizers.Optimizer):<br/>    def __init__(self, learning_rate=0.01, std=0.5, name="PGD", **kwargs):<br/>        super().__init__(name, **kwargs)<br/>        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))<br/>        self._set_hyper("std", std)<br/><br/>    @tf.function<br/>    def _resource_apply_dense(self, grad, var):<br/>        var_dtype = var.dtype.base_dtype<br/>        lr_t = self._decayed_lr(var_dtype)<br/>        perturbation = tf.random.normal(var.shape)<br/>        new_var = var - grad * lr_t + self._get_hyper("std", dtype=tf.float32) * lr_t * perturbation<br/>        var.assign(new_var)<br/><br/>    def get_config(self):<br/>        base_config = super().get_config()<br/>        return {<br/>            **base_config,<br/>            "learning_rate": self._serialize_hyperparameter("learning_rate"),<br/>            "std": self._serialize_hyperparameter("std")<br/>        }<br/><br/><br/>class AntiPGD(tf.keras.optimizers.Optimizer):<br/>    def __init__(self, learning_rate=0.01, std=0.5, name="AntiPGD", **kwargs):<br/>        super().__init__(name, **kwargs)<br/>        self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))<br/>        self._set_hyper("std", std)<br/><br/>    def _create_slots(self, var_list):<br/>        for var in var_list:<br/>            self.add_slot(var, "previous_perturbation", initializer="random_normal")<br/><br/>    @tf.function<br/>    def _resource_apply_dense(self, grad, var):<br/>        var_dtype = var.dtype.base_dtype<br/>        lr_t = self._decayed_lr(var_dtype)<br/>        previous_perturbation = self.get_slot(var, "previous_perturbation")<br/>        current_perturbation = tf.random.normal(var.shape)<br/>        perturbation_diff = current_perturbation - previous_perturbation<br/>        new_var = var - grad * lr_t + self._get_hyper("std", dtype=tf.float32) * lr_t * perturbation_diff<br/>        previous_perturbation.assign(current_perturbation)<br/>        var.assign(new_var)<br/><br/>    def get_config(self):<br/>        base_config = super().get_config()<br/>        return {<br/>            **base_config,<br/>            "learning_rate": self._serialize_hyperparameter("learning_rate"),<br/>            "std": self._serialize_hyperparameter("std")<br/>        }</span></pre><h1 id="3e15" class="ni nj iu bd nk nl nm nn no np nq nr ns kj nt kk nu km nv kn nw kp nx kq ny nz bi translated">结论</h1><p id="4040" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">在他们的论文中，Antonio Orvieto、Hans Kersting、Frank Proske、Francis Bach和Aurelien Lucchi已经表明，在梯度更新期间添加噪声可能有利于找到更好的最小值。在机器学习的设置中，这些是具有良好泛化能力的最小值。</p><p id="e651" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，像PGD那样添加独立噪声的天真方法不足以找到这些最小值——它甚至<strong class="ll je">适得其反</strong>，而(S)GD似乎是更好的选择。作者推荐反相关噪声，产生<strong class="ll je">反PGD </strong>优化器。</p><p id="7b88" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">作者声称<em class="mu">可能</em>运行良好是因为:</p><blockquote class="mf"><p id="64fe" class="mg mh iu bd mi mj mk ml mm mn mo me dk translated">平坦最小值=小Hessian =良好的概括</p></blockquote><p id="8020" class="pw-post-body-paragraph lj lk iu ll b lm mp ke lo lp mq kh lr ls mr lu lv lw ms ly lz ma mt mc md me in bi translated">但是还有很多研究要做。他们甚至说SGD之所以如此有效是因为同样的效果:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qt"><img src="../Images/e1d812acc48a41f3e02479cc53fe356a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jh0aM9-5KemYRTpTjhrrng.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从他们的报纸上。</p></figure><p id="597d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">所以，似乎即使对于像SGD这样简单的算法，还是有那么多不清楚的地方。我们甚至还没有达到RMSprop、Adam或<a class="ae li" rel="noopener" target="_blank" href="/papers-simplified-gradients-without-backpropagation-96e8533943fc"> Fadam </a>的水平。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qu"><img src="../Images/d1d43e36aed05a283aed626bf884fc1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqITeoj5PxcFYeYDcXee4Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由我提供。</p></figure></div><div class="ab cl nb nc hy nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="in io ip iq ir"><p id="997e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="8052" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">作为最后一点，如果你</strong></p><ol class=""><li id="c98a" class="pj pk iu ll b lm ln lp lq ls pl lw pm ma pn me qv pp pq pr bi translated"><strong class="ll je">想支持我多写点机器学习和</strong></li><li id="e53c" class="pj pk iu ll b lm ps lp pt ls pu lw pv ma pw me qv pp pq pr bi translated"><strong class="ll je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="ed3d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">为什么不做</strong> <a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="ll je">通过这个环节</strong> </a> <strong class="ll je">？这将对我帮助很大！😊</strong></p><p id="137d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="eb77" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="mf"><p id="34ca" class="mg mh iu bd mi mj mk ml mm mn mo me dk translated"><em class="qw">有问题就在</em><a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="qw">LinkedIn</em></a><em class="qw">上写我！</em></p></blockquote><h1 id="9c54" class="ni nj iu bd nk nl nm nn no np nq nr ns kj qx kk nu km qy kn nw kp qz kq ny nz bi translated">参考</h1><p id="6a4e" class="pw-post-body-paragraph lj lk iu ll b lm oa ke lo lp ob kh lr ls oc lu lv lw od ly lz ma oe mc md me in bi translated">[1]奥维多，a .，克尔斯汀，h .，普罗斯克，f .，巴赫，f .和卢奇，a .，2022。<a class="ae li" href="https://arxiv.org/abs/2202.02831" rel="noopener ugc nofollow" target="_blank">反相关噪声注入，以提高泛化能力</a>。<em class="mu"> arXiv预印本arXiv:2202.02831 </em></p><p id="a71e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[2] Kleinberg，b .，Li，y .和Yuan，y . 2018年7月。<a class="ae li" href="https://arxiv.org/abs/1802.06175" rel="noopener ugc nofollow" target="_blank">另一种观点:SGD何时逃离局部极小值？</a>。在<em class="mu">机器学习国际会议</em>(第2698–2707页)。PMLR。</p></div></div>    
</body>
</html>