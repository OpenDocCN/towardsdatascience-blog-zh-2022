<html>
<head>
<title>Detect emotion in speech data: Fine-tuning HuBERT using Huggingface</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检测语音数据中的情感:使用Huggingface微调HuBERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-hubert-for-emotion-recognition-in-custom-audio-data-using-huggingface-c2d516b41cd8#2022-09-16">https://towardsdatascience.com/fine-tuning-hubert-for-emotion-recognition-in-custom-audio-data-using-huggingface-c2d516b41cd8#2022-09-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c562" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习—音频的自然语言处理</h2><div class=""/><div class=""><h2 id="be74" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">构建定制的数据加载器、实验日志、改进指标的技巧以及GitHub repo，如果您愿意的话</h2></div><h1 id="fe9d" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">为什么是音频数据？</h1><p id="ac39" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">与文本和计算机视觉任务的NLP相比，音频数据的NLP没有得到足够的重视。是时候改变了！</p><h1 id="c01f" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">工作</h1><p id="64f4" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">情绪识别——识别语音是否表现出<em class="mf">愤怒</em>、<em class="mf">快乐</em>、<em class="mf">悲伤</em>、<em class="mf">厌恶</em>、<em class="mf">惊讶</em>或<em class="mf">中性</em>情绪。</p><p id="c9d3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注意:一旦我们完成教程，你应该能够重用任何音频分类任务的代码。</em></p><h1 id="d948" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">资料组</h1><p id="3c53" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">对于本教程，我们将使用Kaggle上公开可用的<a class="ae ml" href="https://www.kaggle.com/datasets/ejlok1/cremad" rel="noopener ugc nofollow" target="_blank"> Crema-D </a>数据集。(非常感谢<a class="ae ml" href="https://github.com/CheyneyComputerScience/CREMA-D" rel="noopener ugc nofollow" target="_blank">大卫·库珀·切尼</a>收集了这个令人敬畏的数据集)。所以请点击<a class="ae ml" href="https://www.kaggle.com/datasets/ejlok1/cremad?resource=download" rel="noopener ugc nofollow" target="_blank">链接</a>上的<em class="mf">下载</em>按钮。您应该看到包含Crema-D音频文件的<em class="mf"> archive.zip </em>开始下载。它包含7k+音频文件，格式为<code class="fe mm mn mo mp b">.wav</code>。</p><p id="754b" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注意:请随意使用您收集的任何音频数据，而不是CremaD数据集。</em></p><p id="8495" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">如果你想跟随这个教程，这里有GitHub repo。</p><div class="mq mr gp gr ms mt"><a href="https://github.com/V-Sher/Audio-Classification-HF/tree/main/src" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jd gy z fp my fr fs mz fu fw jc bi translated">GitHub—V-Sher/Audio-Classification-HF at主音频情感识别使用HuggingFace库</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">基于人脸库的音频情感识别——音频分类——HF/src主…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ni mt"/></div></div></a></div><h1 id="5d4a" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">拥抱脸库和教练API</h1><p id="115d" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">正如标题中提到的，我们将使用拥抱人脸库来训练模型。特别是，我们将使用它的<a class="ae ml" href="https://huggingface.co/docs/transformers/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">训练器</a>类API。</p><p id="80a6" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">为什么是培训师？为什么不用PyTorch写一个标准的训练循环呢？</em></p><p id="7cbd" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">下面是标准样板代码在<strong class="ll jd"> Pytorch </strong>中的样子:</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nj"><img src="../Images/2b6eeadfb55e92e4bc8618d2ed0f0f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekG5svIvUkFPUzPO_4iP6Q.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">摘自我给PyTorch的<a class="ae ml" rel="noopener" target="_blank" href="/recreating-keras-code-in-pytorch-an-introductory-tutorial-8db11084c60c">入门教程</a></p></figure><p id="55cb" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">相比之下，<strong class="ll jd">训练器</strong>大大简化了编写训练循环的复杂性，使得训练可以在一行中完成:</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="18a1" class="oc ks it mp b gy od oe l of og">trainer.train()</span></pre><p id="2169" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">除了支持基本的训练循环，它还允许在多个GPU/TPU上进行分布式训练、回调(例如提前停止)、评估测试集的结果等。所有这些都可以通过在初始化培训师课程时简单地设置几个参数来实现。</p><p id="656f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">如果不是因为任何事情，我觉得用Trainer代替香草PyTorch肯定会导致一个更有组织，看起来更干净的代码库。</p><h1 id="5c4e" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">让我们开始吧…</h1><h2 id="7279" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">装置</h2><p id="9d2f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">虽然是可选的，但我强烈建议通过创建和激活一个新的虚拟环境来开始本教程，在这个环境中我们可以完成所有的<code class="fe mm mn mo mp b">pip install ...</code>。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="77db" class="oc ks it mp b gy od oe l of og">python -m venv audio_env<br/>source activate audio_env/bin/activate</span></pre><h2 id="3c70" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">正在加载数据集</h2><p id="d16f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">与任何数据建模任务一样，我们首先需要使用<a class="ae ml" href="https://huggingface.co/docs/datasets/tutorial" rel="noopener ugc nofollow" target="_blank">数据集</a>库加载数据集(我们将把它传递给培训师类)。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="d9ba" class="oc ks it mp b gy od oe l of og">pip install datasets</span></pre><p id="c7ca" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">鉴于我们正在使用自定义数据集(与该库附带的预安装数据集相反)，我们需要首先编写一个<strong class="ll jd">加载脚本</strong>(让我们称之为<code class="fe mm mn mo mp b">crema.py</code>)，以教练可接受的格式加载数据集。</p><p id="7397" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在之前的一篇<a class="ae ml" rel="noopener" target="_blank" href="/how-to-turn-your-local-zip-data-into-a-huggingface-dataset-43f754c68f82">文章</a>中，我已经讲述了如何创建这个脚本(非常详细)。(我强烈建议通过它来了解<code class="fe mm mn mo mp b">config</code>、<code class="fe mm mn mo mp b">cache_dir</code>、<code class="fe mm mn mo mp b">data_dir</code>等的用法。在下面的代码片段中)。数据集中的每个示例都有两个特征:<code class="fe mm mn mo mp b">file</code>和<code class="fe mm mn mo mp b">label</code>。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="ff02" class="oc ks it mp b gy od oe l of og">dataset_config = {<br/>  "LOADING_SCRIPT_FILES": os.path.join(PROJECT_ROOT, "<strong class="mp jd">crema.py</strong>"),<br/>  "CONFIG_NAME": "clean",<br/>  "DATA_DIR": os.path.join(PROJECT_ROOT, "data/archive.zip"),<br/>  "CACHE_DIR": os.path.join(PROJECT_ROOT, "cache_crema"),<br/>}</span><span id="db1f" class="oc ks it mp b gy or oe l of og">ds = load_dataset(<br/>  dataset_config["LOADING_SCRIPT_FILES"],<br/>  dataset_config["CONFIG_NAME"],<br/>  <em class="mf">data_dir</em>=dataset_config["DATA_DIR"],<br/>  <em class="mf">cache_dir</em>=dataset_config["CACHE_DIR"]<br/>)</span><span id="819e" class="oc ks it mp b gy or oe l of og">print(ds)</span><span id="6a47" class="oc ks it mp b gy or oe l of og">********* OUTPUT ********DatasetDict({<br/>    train: Dataset({<br/>        features: ['<strong class="mp jd">file</strong>', '<strong class="mp jd">label</strong>'],<br/>        num_rows: 7442<br/>    })<br/>})</span></pre><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi os"><img src="../Images/00a53b43ef1976baccdf854eab1be513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t--VGU5ty1guVBR2wy8FCw.png"/></div></div></figure><p id="4c11" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf"> P.S:虽然我们为CremaD数据集创建了一个</em> <code class="fe mm mn mo mp b"><em class="mf">datasets.Dataset</em></code> <em class="mf">对象(传递给Trainer类)，但它不一定必须是这样。我们也可以定义并使用</em> <code class="fe mm mn mo mp b"><em class="mf">torch.utils.data.Dataset</em></code> <em class="mf">(类似于我们在</em> <a class="ae ml" rel="noopener" target="_blank" href="/recreating-keras-code-in-pytorch-an-introductory-tutorial-8db11084c60c"> <em class="mf">本</em> </a> <em class="mf">教程中创建的CSVDataset)。</em></p><h1 id="f2ba" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">编写模型培训脚本</h1><p id="fe47" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><a class="ae ml" href="https://github.com/V-Sher/Audio-Classification-HF" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中的目录结构:</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="34f5" class="oc ks it mp b gy od oe l of og">Audio-Classification-Medium<!-- -->  <br/>│<br/>└───src<br/>│   │<br/>│   └───data<br/>│       │   crema.py<br/>│   <br/>└───data<br/>|   │   archive.zip<!-- --> <br/>|<br/>└───scripts<br/>    │   audio_train.py</span></pre><p id="bab0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">让我们开始写我们的<code class="fe mm mn mo mp b">audio_train.py</code>剧本吧。</p><h2 id="356d" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">装置</h2><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="bfc4" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">实验跟踪(可选)</h2><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="90d2" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们使用<a class="ae ml" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">权重&amp;偏差</a>进行实验跟踪，因此请确保您已经创建了一个帐户，然后根据您的详细信息更新<code class="fe mm mn mo mp b">USER</code>和<code class="fe mm mn mo mp b">WANDB_PROJECT</code>。</p><h2 id="babe" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">加载特征提取器</h2><blockquote class="ov ow ox"><p id="fabe" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">问题:从广义上讲，什么是特征提取器？<br/>答:<a class="ae ml" href="https://huggingface.co/docs/transformers/main_classes/feature_extractor#feature-extractor" rel="noopener ugc nofollow" target="_blank">特征提取器</a>是一个负责为模型准备输入特征的类。例如，在<strong class="ll jd">图像</strong>的情况下，这可以包括裁剪图像、填充，或者在<strong class="ll jd">音频</strong>的情况下，这可以包括将原始音频转换成频谱特征、应用归一化、填充等。</p><p id="c1cb" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">图像数据的特征提取器示例:<br/> &gt; &gt; &gt;从变形金刚导入ViTFeatureExtractor<br/>&gt;&gt;&gt;vit _ extractor = ViTFeatureExtractor()<br/>&gt;&gt;&gt;print(vit _ extractor)</p><p id="7f04" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">ViTFeatureExtractor {<br/>" do _ normalize ":true，<br/> "do_resize": true，<br/>" feature _ extractor _ type ":" ViTFeatureExtractor "，<br/> "image_mean": [0.5，0.5，0.5]，<br/> "image_std": [0.5，0.5，0.5]，<br/> "resample": 2，<br/> "size": 224 <br/> }</p></blockquote><p id="3f3a" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">更具体地说，我们将使用<code class="fe mm mn mo mp b"><a class="ae ml" href="https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor" rel="noopener ugc nofollow" target="_blank">Wav2Vec2FeatureExtractor</a></code>。这是从<a class="ae ml" href="https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor" rel="noopener ugc nofollow" target="_blank">sequence feature extractor</a>派生的一个类，它是一个通用的语音识别特征提取类，由Huggingface提供。</p><p id="1e33" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">使用<a class="ae ml" href="https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor" rel="noopener ugc nofollow" target="_blank">wav2 vec 2 feature extractor</a>有三种方法:</p><ul class=""><li id="c5fb" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">选项1 —使用默认值。</li></ul><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="edab" class="oc ks it mp b gy od oe l of og">from transformers import Wav2Vec2FeatureExtractor</span><span id="6536" class="oc ks it mp b gy or oe l of og">feature_extractor = Wav2Vec2FeatureExtractor()<br/>print(feature_extractor)</span><span id="02aa" class="oc ks it mp b gy or oe l of og">**** OUTPUT ****<br/>Wav2Vec2FeatureExtractor {<br/>   "do_normalize": true,<br/>   "feature_extractor_type": "Wav2Vec2FeatureExtractor",<br/>   "feature_size": 1,<br/>   "padding_side": "right",<br/>   "padding_value": 0.0,<br/>   "return_attention_mask": false,<br/>   "sampling_rate": 16000 <br/>}</span></pre><ul class=""><li id="6916" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">选项2-修改任何<code class="fe mm mn mo mp b">Wav2Vec2FeatureExtractor</code> <a class="ae ml" href="https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor" rel="noopener ugc nofollow" target="_blank">参数</a>来创建您的自定义特征提取器。</li></ul><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="b032" class="oc ks it mp b gy od oe l of og">from transformers import Wav2Vec2FeatureExtractor</span><span id="9551" class="oc ks it mp b gy or oe l of og">feature_extractor = Wav2Vec2FeatureExtractor(<br/>          <strong class="mp jd"><em class="mf">sampling_rate</em>=24000,<br/>          <em class="mf">truncation</em>=True</strong><br/>)<br/>print(feature_extractor)</span><span id="f626" class="oc ks it mp b gy or oe l of og">**** OUTPUT ****<br/>Wav2Vec2FeatureExtractor {<br/>   "do_normalize": true,<br/>   "feature_extractor_type": "Wav2Vec2FeatureExtractor",<br/>   "feature_size": 1,<br/>   "padding_side": "right",<br/>   "padding_value": 0.0,<br/>   "return_attention_mask": false,<br/><strong class="mp jd">   "sampling_rate": 24000,<br/>   "truncation": true</strong><br/>}</span></pre><p id="9d12" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">选项3:因为我们不寻求任何定制，我们可以使用<code class="fe mm mn mo mp b">from_pretrained()</code>方法来加载预训练模型的默认特征提取器参数(通常存储在名为<code class="fe mm mn mo mp b">preprocessor_config.json</code>的文件中)。由于我们将使用<code class="fe mm mn mo mp b">facebook/hubert-base-ls960</code>作为我们的基础模型，我们可以获得它的特征提取器参数(可用于视觉检查<a class="ae ml" href="https://huggingface.co/facebook/wav2vec2-base-960h/tree/main" rel="noopener ugc nofollow" target="_blank">这里</a>下<code class="fe mm mn mo mp b">preprocessor_config.json</code>)。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="4e29" class="oc ks it mp b gy od oe l of og">from transformers import Wav2Vec2FeatureExtractor</span><span id="6d88" class="oc ks it mp b gy or oe l of og"><strong class="mp jd">model = "facebook/hubert-base-ls960"</strong><br/>feature_extractor = Wav2Vec2FeatureExtractor<strong class="mp jd">.from_pretrained(model)</strong></span><span id="291b" class="oc ks it mp b gy or oe l of og">print(feature_extractor)</span><span id="d82a" class="oc ks it mp b gy or oe l of og">*** OUTPUT ***</span><span id="b95f" class="oc ks it mp b gy or oe l of og">Wav2Vec2FeatureExtractor {<br/>   "do_normalize": true,<br/>   "feature_extractor_type": "Wav2Vec2FeatureExtractor",<br/>   "feature_size": 1,<br/>   "padding_side": "right",<br/>   "padding_value": 0,<br/>   "return_attention_mask": false,<br/>   "sampling_rate": 16000<br/> }</span></pre><p id="14ab" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">为了查看特征提取器的运行情况，让我们将一个虚拟音频文件作为<code class="fe mm mn mo mp b">raw_speech</code>输入到Wav2Vec2FeatureExtractor:</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="24ad" class="oc ks it mp b gy od oe l of og">model_id = "facebook/hubert-base-ls960"<br/>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_id)</span><span id="2db9" class="oc ks it mp b gy or oe l of og">audio_file = "dummy1.wav"<br/>audio_array = librosa.load(audio_file, <em class="mf">sr</em>=16000, <em class="mf">mono</em>=False)[0]</span><span id="2104" class="oc ks it mp b gy or oe l of og">input = <em class="mf">feature_extractor</em>(<br/>       <strong class="mp jd">raw_speech=audio_array</strong>,<br/>       <em class="mf">sampling_rate</em>=16000,<br/>       <em class="mf">padding</em>=True,<br/>      <em class="mf">return_tensors</em>="pt"<br/>)</span><span id="51f6" class="oc ks it mp b gy or oe l of og">print(input)<br/>print(input.shape)<br/>print(audio_array.shape)</span><span id="6460" class="oc ks it mp b gy or oe l of og">***** OUTPUT ******<br/>{'input_values': tensor([[-0.0003, -0.0003, -0.0003,  ...,  0.0006, -0.0003, -0.0003]])}</span><span id="1bf9" class="oc ks it mp b gy or oe l of og">torch.Size([1, 36409])</span><span id="b3bd" class="oc ks it mp b gy or oe l of og">(36409,)</span></pre><p id="6db7" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">需要注意的几件事:</p><ul class=""><li id="6da2" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">特征提取器的输出是一个包含<code class="fe mm mn mo mp b">input_values</code>的字典。它的值只是应用于<code class="fe mm mn mo mp b">audio_array</code>的<a class="ae ml" href="https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L224" rel="noopener ugc nofollow" target="_blank">标准化</a>，即来自<code class="fe mm mn mo mp b">librosa</code>库的输出。事实上，<code class="fe mm mn mo mp b">input.input_values</code>和<code class="fe mm mn mo mp b">audio_array</code>的形状是一样的。</li><li id="a4c5" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">当调用特征提取器时，确保您使用的<code class="fe mm mn mo mp b">sampling_rate</code>与基础模型用于其训练数据集的<code class="fe mm mn mo mp b">sampling_rate</code>相同。我们使用<a class="ae ml" href="https://huggingface.co/facebook/hubert-base-ls960" rel="noopener ugc nofollow" target="_blank">这个</a> facebook模型进行训练，它的模型卡明确声明以16Khz采样语音输入。</li><li id="e46b" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated"><code class="fe mm mn mo mp b">return_tensors</code>可以分别为PyTorch张量、TensorFlow对象和NumPy数组取值“pt”、“tf”和“np”。</li><li id="11ba" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated"><code class="fe mm mn mo mp b">padding</code>在单个音频文件的情况下没有太大意义，但当我们进行批处理时，它有意义，因为它填充较短的音频(带有额外的0或-1)以具有与最长音频相同的长度。以下是用不同长度填充音频文件的示例:</li></ul><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="04d7" class="oc ks it mp b gy od oe l of og">audio_file_1 = "dummy1.wav"<br/>audio_file_2 = "dummy2.wav"</span><span id="c003" class="oc ks it mp b gy or oe l of og">audio_array_1 = librosa.load(audio_file_1, <em class="mf">sr</em>=16000, <em class="mf">mono</em>=False)[0]<br/>audio_array_2 = librosa.load(audio_file_2, <em class="mf">sr</em>=16000, <em class="mf">mono</em>=False)[0]</span><span id="a651" class="oc ks it mp b gy or oe l of og">input_with_one_audio = <em class="mf">feature_extractor</em>(<br/>       <strong class="mp jd">audio_array_1</strong>,<br/>       <em class="mf">sampling_rate</em>=16000,<br/>       <em class="mf">padding</em>=True,<br/>      <em class="mf">return_tensors</em>="pt"<br/>)</span><span id="28d3" class="oc ks it mp b gy or oe l of og">input_with_two_audio = <em class="mf">feature_extractor</em>(<br/>       <strong class="mp jd">[audio_array_1, audio_array_2]</strong>,<br/>       <em class="mf">sampling_rate</em>=16000,<br/>       <em class="mf">padding</em>=True,<br/>      <em class="mf">return_tensors</em>="pt"<br/>)</span><span id="7c9f" class="oc ks it mp b gy or oe l of og">print(input_with_one_audio.input_values.shape)<br/>print(input_with_two_audios.input_values.shape)</span><span id="2f1d" class="oc ks it mp b gy or oe l of og">***** OUTPUT ****<br/>torch.Size([1, 36409])<br/>torch.Size([2, 37371])</span></pre><p id="1080" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">既然我们知道特征提取器模型的输出可以根据输入音频的不同而改变形状，那么在将一批输入推送到模型进行训练之前，填充为什么重要就变得很清楚了。当处理批次时，我们可以(a)将所有音频填充到训练集中最长音频的长度，或者(b)将所有音频截断到最大长度。(a)的问题是，我们不必要地增加了存储这些额外填充值的内存开销。(b)的问题是，由于截断，可能会丢失一些信息。</p><p id="f317" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">有一个更好的选择——使用<a class="ae ml" href="https://huggingface.co/docs/transformers/main_classes/data_collator" rel="noopener ugc nofollow" target="_blank">数据整理器</a>在模型训练期间应用动态填充。我们将很快看到他们的行动！</p><blockquote class="ov ow ox"><p id="05e9" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">在构建批次(用于训练)时，<strong class="ll jd">数据整理器</strong>可以只对特定批次的输入进行预处理(比如填充)。</p></blockquote><h2 id="af4b" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">分类的加载基础模型</h2><p id="1c6b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">如前所述，我们将使用脸书的休伯特模型来分类音频。如果你对休伯特的内部工作方式感兴趣，可以看看这篇由乔纳森·Bgn撰写的关于休伯特的很棒的<a class="ae ml" href="https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html" rel="noopener ugc nofollow" target="_blank">入门教程。</a></p><p id="2ae1" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">裸露的<a class="ae ml" href="https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertModel" rel="noopener ugc nofollow" target="_blank"> HubertModel </a>是24个transformer编码器层的堆栈，并为这24层中的每一层输出原始隐藏状态(顶部没有任何特定的头用于分类)。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="11f6" class="oc ks it mp b gy od oe l of og">bare_model = HubertModel.from_pretrained("facebook/hubert-large-ls960-ft")</span><span id="da52" class="oc ks it mp b gy or oe l of og">last_hidden_state = bare_model(input.input_values).last_hidden_state<br/>print(last_hidden_state.shape)</span><span id="5cb3" class="oc ks it mp b gy or oe l of og">*** OUTPUT ***</span><span id="0fa7" class="oc ks it mp b gy or oe l of og">torch.Size([1, 113, 1024]) <em class="mf"># the hidden size i.e. 113 can vary depending on audio</em></span></pre><p id="1d2d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们需要在这个裸模型之上的某种分类头，它可以将最后一个隐藏层的输出馈送到一个线性层，最终输出6个值(6个情感类别中的每一个)。这正是<a class="ae ml" href="https://huggingface.co/docs/transformers/model_doc/hubert#transformers.HubertForSequenceClassification" rel="noopener ugc nofollow" target="_blank">Hubert for sequence classification</a>所做的事情。它的顶部有一个分类头，用于音频分类等任务。</p><p id="9af9" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">然而，与上面解释的特征提取器配置类似，如果您从预训练模型中获得HubertForSequenceClassification的默认配置，您会注意到，由于其默认配置的定义方式，它仅适用于二进制分类任务。</p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="aac3" class="oc ks it mp b gy od oe l of og">model_path = ""facebook/hubert-large-ls960-ft""</span><span id="0b1f" class="oc ks it mp b gy or oe l of og">hubert_model = HubertForSequenceClassification.from_pretrained(model_path)</span><span id="26d2" class="oc ks it mp b gy or oe l of og">hubert_model_config = hubert_model.config</span><span id="82e6" class="oc ks it mp b gy or oe l of og">print("Num of labels:", hubert_model_config.num_labels)</span><span id="92f9" class="oc ks it mp b gy or oe l of og">**** OUTPUT ****</span><span id="9ce8" class="oc ks it mp b gy or oe l of og"><strong class="mp jd">Num of labels: 2</strong></span></pre><p id="2d1c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">对于我们的6类分类，我们需要使用<a class="ae ml" href="https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/configuration#transformers.PretrainedConfig" rel="noopener ugc nofollow" target="_blank"> PretrainedConfig </a>更新要传递给Hubert模型的配置(查看部分— <a class="ae ml" href="https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/configuration#transformers.PretrainedConfig.architectures" rel="noopener ugc nofollow" target="_blank">参数进行微调</a>)。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="1f86" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">需要注意的几件事:</p><ul class=""><li id="532e" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">在第5行，<code class="fe mm mn mo mp b">from_pretrained()</code>从<code class="fe mm mn mo mp b">facebook/hubert-base-ls960</code>加载模型架构<strong class="ll jd">和</strong>模型权重(即所有24个变压器层+线性分类器的权重)。<br/> <em class="mf">注意:如果你简单地做了</em> <code class="fe mm mn mo mp b"><em class="mf">hubert_model = HubertForSequenceClassification()</em></code> <em class="mf">，变换编码器和分类器权重被随机初始化</em>。</li><li id="4a06" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">将<code class="fe mm mn mo mp b">ignore_mismatched_sizes</code>参数设置为<code class="fe mm mn mo mp b">True</code>很重要，因为如果没有它，你会因为尺寸不匹配而得到一个错误(见下图)——作为<code class="fe mm mn mo mp b">facebook/hubert-base-ls960</code>一部分的分类器权重具有形状<code class="fe mm mn mo mp b">(2, classifier_proj_size)</code>，而根据我们新定义的配置，我们的权重应该具有形状<code class="fe mm mn mo mp b">(6, classifier_proj_size)</code>。鉴于我们无论如何都要从头重新训练线性分类器层，我们可以选择忽略不匹配的大小。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pp"><img src="../Images/bdba50bf14ff7f1a4878450c63db228c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1V-el3p9RyjBZ3yWPTT9Sw.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">分类器大小不匹配导致的错误</p></figure><h2 id="db4e" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">冻结图层进行微调</h2><blockquote class="ov ow ox"><p id="e1ec" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">根据一般经验，如果训练预训练模型的基础数据集与您正在使用的数据集有显著不同，最好取消冻结并重新训练顶层的几个层。</p></blockquote><p id="049d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">首先，我们解冻顶部两个编码器层(最接近分类头)的权重，同时保持所有其他层的权重冻结。为了冻结/解冻权重，我们将<code class="fe mm mn mo mp b">param.require_grad</code>设置为假/真，其中<code class="fe mm mn mo mp b">param</code>指的是模型参数。</p><blockquote class="ov ow ox"><p id="f7c0" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">在模型训练期间解冻权重意味着这些权重将照常更新，以便它们可以达到手头任务的最佳值。</p></blockquote><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="ca58" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注意:虽然在训练过程的最开始解冻许多层看起来很直观，但这不是我推荐的。我实际上是通过冻结所有层并只训练分类器头来开始我的实验的。因为结果很差(这点不奇怪)，我解冻两层恢复训练。</em></p><h2 id="4996" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">正在加载数据集</h2><p id="28cc" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">使用我们的定制加载脚本<code class="fe mm mn mo mp b">crema.py</code>，我们现在可以使用<code class="fe mm mn mo mp b">load_dataset()</code>方法从<a class="ae ml" href="https://huggingface.co/docs/datasets/v2.4.0/en/index" rel="noopener ugc nofollow" target="_blank">数据集</a>库中加载我们的数据集。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="3f0f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">接下来，我们使用<code class="fe mm mn mo mp b">map()</code>将数据集中的所有原始音频(以<code class="fe mm mn mo mp b">.wav</code>格式)转换成数组。</p><blockquote class="ov ow ox"><p id="cded" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">一般来说，<code class="fe mm mn mo mp b">map</code>对数据集中的所有行/样本重复应用一个函数。</p></blockquote><p id="c58f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">这里，函数(定义为一个<a class="ae ml" href="https://www.w3schools.com/python/python_lambda.asp" rel="noopener ugc nofollow" target="_blank"> lambda函数</a>)接受一个参数<code class="fe mm mn mo mp b">x</code>(对应于数据集中的一行)，并使用<code class="fe mm mn mo mp b">librosa.load()</code>将该行中的音频文件转换为一个数组。如上所述，确保采样率(<code class="fe mm mn mo mp b">sr</code>)合适。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="f568" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注意:如果你在这个阶段做</em> <code class="fe mm mn mo mp b"><em class="mf">print(ds)</em></code> <em class="mf">，你会注意到数据集中的三个特征:</em></p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="c071" class="oc ks it mp b gy od oe l of og"><em class="mf">print(ds)</em></span><span id="1110" class="oc ks it mp b gy or oe l of og"><em class="mf">***** OUTPUT *****</em></span><span id="647e" class="oc ks it mp b gy or oe l of og"><em class="mf">DatasetDict({<br/>    train: Dataset({<br/>        features: ['file', 'label', 'array'],<br/>        num_rows: 7442<br/>    })<br/>})</em></span></pre><p id="058a" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">一旦我们生成了数组，我们将再次使用<code class="fe mm mn mo mp b">map</code>——这一次使用辅助函数<code class="fe mm mn mo mp b">prepare_dataset()</code>来准备输入。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="83c8" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><code class="fe mm mn mo mp b">prepare_dataset</code>是一个助手函数，它将处理函数应用于数据集中的每个示例(或者一组示例，也就是<code class="fe mm mn mo mp b">batch</code>)。更具体地说，该函数做两件事— (1)读取出现在<code class="fe mm mn mo mp b">batch["array"]</code>的音频数组，并使用上面讨论的<code class="fe mm mn mo mp b">feature_extractor</code>从中提取特征，并将其存储为名为<code class="fe mm mn mo mp b">input_values</code>的新特征—(除了<code class="fe mm mn mo mp b">file</code>、<code class="fe mm mn mo mp b">labels</code>和<code class="fe mm mn mo mp b">array</code>)以及(2)创建名为<code class="fe mm mn mo mp b">labels</code>的新特征，其值与<code class="fe mm mn mo mp b">batch["label"]</code>相同。</p><p id="e322" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">问题:你可能想知道对于每个例子都有</em> <code class="fe mm mn mo mp b"><em class="mf">label</em></code> <em class="mf">和</em> <code class="fe mm mn mo mp b"><em class="mf">labels</em></code> <em class="mf">有什么意义，尤其是当它们有相同的值时。</em> <br/> <em class="mf">原因:训练器API会寻找</em> <code class="fe mm mn mo mp b"><em class="mf">labels</em></code> <em class="mf">的列名，默认情况下，所以我们只是乐于助人。如果你愿意，你甚至可以在这一步删除</em><a class="ae ml" href="https://huggingface.co/docs/datasets/process#remove" rel="noopener ugc nofollow" target="_blank"><em class="mf"/></a><em class="mf">和其他</em> <code class="fe mm mn mo mp b"><em class="mf">label</em></code> <em class="mf">列，或者更好，在创建加载脚本时将特征命名为“标签”。</em></p><p id="ac78" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">如果你仔细观察，就会发现不像前面的<code class="fe mm mn mo mp b">map</code>用例，lambda函数只需要<em class="mf">一个</em>输入参数，<code class="fe mm mn mo mp b">prepare_dataset()</code>需要<em class="mf">两个</em>参数。</p><blockquote class="ov ow ox"><p id="83c4" class="lj lk mf ll b lm mg kd lo lp mh kg lr oy mi lu lv oz mj ly lz pa mk mc md me im bi translated">记住:每当我们需要向<code class="fe mm mn mo mp b">map</code>内部的函数传递多个参数时，我们必须将<code class="fe mm mn mo mp b">fn_kwargs</code>参数传递给<code class="fe mm mn mo mp b">map</code>。这个参数是一个字典，包含所有要传递给函数的参数。</p></blockquote><p id="5d5f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">基于其函数定义，我们需要两个参数用于<code class="fe mm mn mo mp b">prepare_dataset()</code> — (a)数据集中的行和(b)特征提取器——因此我们必须如下使用<code class="fe mm mn mo mp b">fn_kwargs</code>:</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="07c7" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">接下来，我们将使用<code class="fe mm mn mo mp b">class_encode_column()</code>将所有字符串标签转换成ids，1，2，3，4，5，6)。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="cd1f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">最后，我们通过使用<code class="fe mm mn mo mp b">train_test_split()</code>引入训练-测试-验证分割。我们需要以这种方式分割两次，以获得三个不重叠的数据集，所有这些数据集在下面的步骤8中合并成一个单独的<code class="fe mm mn mo mp b">DatasetDict</code>。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h1 id="e2d1" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">让训练开始吧…</h1><p id="b512" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">所有的细节都准备好了，我们现在准备开始使用训练者职业进行训练。</p><p id="fb7f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">首先，我们需要指定训练参数——这包括时期的数量、批量大小、存储训练模型的目录、实验记录等。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="cbeb" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">需要考虑的事情很少:</p><ul class=""><li id="68a6" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated"><a class="ae ml" href="https://huggingface.co/docs/transformers/perf_train_gpu_one#gradient-accumulation" rel="noopener ugc nofollow" target="_blank">梯度累积步骤</a>在你想在训练期间推动更大批量但你的记忆有限的情况下非常有用。设置<code class="fe mm mn mo mp b">gradient_accumulation_steps=4</code>允许我们在每4步后更新权重—在每一步中，<code class="fe mm mn mo mp b">batch_size=32</code>个样本被处理，它们的梯度被累加。只有在4个步骤之后，当积累了足够的梯度时，权重才会得到更新。</li></ul><p id="8b5e" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其次，除了指定训练和评估数据集之外，我们用这些训练参数实例化训练器类。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="608e" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">需要考虑的事情很少:</p><ul class=""><li id="dd8f" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">在第5行，我们使用了一个<code class="fe mm mn mo mp b">data_collator</code>。我们在本教程开始时简要讨论了这种动态填充输入音频数组的方法。数据排序器初始化如下:</li></ul><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="bff2" class="oc ks it mp b gy od oe l of og"># DEFINE DATA COLLATOR - TO PAD TRAINING BATCHES DYNAMICALLY</span><span id="7eac" class="oc ks it mp b gy or oe l of og">data_collator = DataCollatorCTCWithPadding(<br/>            processor=feature_extractor,<br/>            padding=True<br/>)</span></pre><p id="925c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><code class="fe mm mn mo mp b">DataCollatorCTCWithPadding</code>是改编自本<a class="ae ml" href="https://huggingface.co/blog/fine-tune-wav2vec2-english" rel="noopener ugc nofollow" target="_blank">教程</a>的dataclass。我强烈推荐快速阅读教程中的<a class="ae ml" href="https://huggingface.co/blog/fine-tune-wav2vec2-english#set-up-trainer" rel="noopener ugc nofollow" target="_blank">设置教练部分</a>,以了解这个课程发生了什么。</p><p id="cde3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">不涉及太多细节，这个类中的<code class="fe mm mn mo mp b">__call__</code>方法负责准备接收到的输入。它从数据集中获取一批示例(记住每个示例有4个特征— <code class="fe mm mn mo mp b">file</code>、<code class="fe mm mn mo mp b">labels</code>、<code class="fe mm mn mo mp b">label</code>、<code class="fe mm mn mo mp b">array</code>、<code class="fe mm mn mo mp b">input_values</code>)，并返回同一批示例，但使用<code class="fe mm mn mo mp b">processor.pad</code>将填充应用到<code class="fe mm mn mo mp b">input_values</code>。还有，批量中的<code class="fe mm mn mo mp b">labels</code>都转换成Pytorch张量。</p><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><ul class=""><li id="c731" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">在第8行，我们定义了<code class="fe mm mn mo mp b">compute_metrics()</code>，这是一种告诉培训师在评估期间必须计算哪些指标(准确度、精确度、f1、召回率等)的方式。它将评估预测(<code class="fe mm mn mo mp b">eval_pred</code>)作为输入，并使用<code class="fe mm mn mo mp b">metric.compute(predictions=.., references=...)</code>比较实际标签和预测标签。同样，<code class="fe mm mn mo mp b">compute_metrics()</code>的样板代码是从<a class="ae ml" href="https://huggingface.co/course/chapter3/3?fw=pt" rel="noopener ugc nofollow" target="_blank">改编而来的。</a></li></ul><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="b100" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注:如果您想发挥创意并显示自定义指标(例如，绝对差异btw实际值和预测值的日志)，您可以修改</em> <code class="fe mm mn mo mp b"><em class="mf">compute_metrics()</em></code> <em class="mf">。在这样做之前你只需要知道</em> <code class="fe mm mn mo mp b"><em class="mf">eval_pred</em></code> <em class="mf">返回的是什么。这可以通过在实际训练模型之前在您的评估/测试数据集上运行</em> <code class="fe mm mn mo mp b"><em class="mf">trainer.predict</em></code> <em class="mf">来提前发现。在我们的例子中，它返回实际的标签和预测(即logits在其上应用</em> <code class="fe mm mn mo mp b"><a class="ae ml" href="https://en.wikipedia.org/wiki/Arg_max" rel="noopener ugc nofollow" target="_blank"><em class="mf">argmax</em></a></code> <em class="mf">函数以获得预测的类):</em></p><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="3a42" class="oc ks it mp b gy od oe l of og">trainer = Trainer(model=..., args=...,...)<br/>output = trainer.predict(ds["test"])</span><span id="6317" class="oc ks it mp b gy or oe l of og">print(output)</span><span id="9f56" class="oc ks it mp b gy or oe l of og">**** OUTPUT *****<br/>PredictionOutput(<br/>predictions=array([<br/>       [ 0.0331, -0.0193, -0.98767, 0.0229, 0.01693, -0.0745],<br/>       [-0.0445,  0.0020, 0.13196, 0.2219, 0.94693, -0.0614],<br/>        .<br/>        .<br/>        .<br/>], dtype=float32),<br/>label_ids=array([0, 5, ......]),<br/>metrics={'test_loss': 1.780486822128296, 'test_accuracy': 0.0, 'test_runtime': 1.6074, 'test_samples_per_second': 1.244, 'test_steps_per_second': 0.622}<br/>)</span></pre><h2 id="576a" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">现在实际的训练只有一行代码</h2><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="e833" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">快速迂回:第4行包含从一个检查点继续训练的命令。但是首先，什么是检查点？</p><p id="6a56" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在训练过程中，教练将创建模型重量的快照，并将它们存储在<code class="fe mm mn mo mp b">TrainingArguments(output_dir="results")</code>中定义的<code class="fe mm mn mo mp b">output_dir</code>中。这些文件夹通常被命名为<code class="fe mm mn mo mp b">checkpoint-XXXX</code>，包含模型权重、训练参数等。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pq"><img src="../Images/6eda9f9cfaf533809152fcbf8a23dfe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l12UxQGPrchUm9t6mairNA.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">检查站</p></figure><p id="c420" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">您可以分别使用<code class="fe mm mn mo mp b"><a class="ae ml" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_strategy" rel="noopener ugc nofollow" target="_blank">save_strategy</a></code>和<code class="fe mm mn mo mp b"><a class="ae ml" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_steps" rel="noopener ugc nofollow" target="_blank">save_steps</a></code>指定<strong class="ll jd">何时</strong>和<strong class="ll jd">多久</strong>创建这些检查点。默认情况下，每500步后会保存检查点(<code class="fe mm mn mo mp b">save_steps=500</code>)。我提到这一点的原因是因为我不知道这些默认值，并且在一次培训会议(运行了7个小时)中，我看到在输出目录中没有创建任何检查点。这是我正在使用的配置:</p><ul class=""><li id="7318" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">训练样本:6697</li><li id="d984" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">次数= 5</li><li id="aaa2" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">批量= 32</li><li id="fba4" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">梯度累积步长= 4</li></ul><p id="4c3d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">经过一小时又一小时的调试，我发现在我的例子中总共只有260步，而默认的保存只发生在第500步之后。🤦作为<code class="fe mm mn mo mp b">TrainingArguments()</code>的一部分，‍♀设置<code class="fe mm mn mo mp b">save_steps = 100</code>为我解决了这个问题。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pr"><img src="../Images/daf313996e7c2ac201e226007bc28ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WStEw9RGTv3dL91OSbE0vw.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">在底部，您可以找到优化步骤的总数</p></figure><p id="c767" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><em class="mf">注意:如果您想知道如何计算总步数(即本例中的260步):</em></p><p id="231e" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><strong class="ll jd"> <em class="mf">总训练批量</em> </strong> <em class="mf"> =批量*梯度累积步长= 32*4 = 128 <br/> </em> <strong class="ll jd"> <em class="mf">总优化步长</em> </strong> <em class="mf"> =(训练样本/总训练批量)*历数=(6697/128)*5 ≈ 260。</em></p><h2 id="7c66" class="oc ks it bd kt oh oi dn kx oj ok dp lb ls ol om ld lw on oo lf ma op oq lh iz bi translated">对测试集进行预测并记录结果</h2><figure class="nk nl nm nn gt no"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="e1cf" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">需要考虑的事情很少:</p><ul class=""><li id="b61d" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">要记录权重和偏差的任何额外指标/变量，我们可以使用<code class="fe mm mn mo mp b">wandb.log()</code>。例如，在第4行，我们记录了测试集的准确性。</li><li id="282f" class="pb pc it ll b lm pk lp pl ls pm lw pn ma po me pg ph pi pj bi translated">默认情况下，<code class="fe mm mn mo mp b">wandb</code>不记录训练过的模型，所以它只在训练完成后在本地机器上可用。为了显式地存储模型工件，我们需要使用带有<code class="fe mm mn mo mp b">policy="end"</code>的<code class="fe mm mn mo mp b">wandb.save()</code>，这意味着只有当运行结束时才同步文件。</li></ul><h1 id="27a7" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">结果和反思</h1><p id="66e2" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">使用不同超参数组合的所有不同模型运行的结果都记录在我的重量和偏差仪表板上<a class="ae ml" href="https://wandb.ai/vsher/audio-classifier" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ps"><img src="../Images/adc6821755463f2506bd1734b194f022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHSlX8NfCcDPac_GknmxfA.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">WandB仪表板</p></figure><p id="a6d6" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">查看学习曲线，看起来我们最近的运行(<strong class="ll jd"> faithful-planet-28 </strong>测试精度= 68%——考虑到只花了4个小时的训练，还不算太坏)可能会受益于额外的时期，因为train和eval损失仍在减少，尚未稳定下来(或者更糟，开始发散)。根据这是否可行，可能需要解冻更多的编码器层。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pt"><img src="../Images/c170005b1f9ee1351388a209d37f93ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NW95mlvXWXA196oWnn5NQ.png"/></div></div><p class="nu nv gj gh gi nw nx bd b be z dk translated">学习曲线</p></figure><p id="3bc3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">一些反思和学习:</p><ul class=""><li id="a47a" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">如果我们正在增加纪元，可能值得考虑提前通过回调停止训练。详见<a class="ae ml" href="https://stackoverflow.com/questions/69087044/early-stopping-in-bert-trainer-instances" rel="noopener ugc nofollow" target="_blank">本</a>栈溢出讨论。</li></ul><pre class="nk nl nm nn gt ny mp nz oa aw ob bi"><span id="e3d3" class="oc ks it mp b gy od oe l of og"># TO IMPLEMENT EARLY STOPPING</span><span id="6b6e" class="oc ks it mp b gy or oe l of og">trainer = Trainer(<br/>  callbacks=[EarlyStoppingCallback(early_stopping_patience = 10)]<br/>)</span></pre><ul class=""><li id="b461" class="pb pc it ll b lm mg lp mh ls pd lw pe ma pf me pg ph pi pj bi translated">除了Cuda之外，Trainer最近还增加了对新Mac M1 GPU的支持(只需设置<code class="fe mm mn mo mp b">args = TrainingArguments(use_mps_device=True)</code>)。如果您正在与他们一起工作，请注意，有些人报告了指标的下降(这是一个已知的错误—参见<a class="ae ml" href="https://github.com/huggingface/transformers/issues/17971" rel="noopener ugc nofollow" target="_blank">这个问题</a>)。</li></ul><h1 id="a05b" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">结论</h1><p id="7462" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">希望你现在对使用变形金刚库微调深度学习模型更有信心了。如果你真的推进这个项目，请与我和更广泛的社区分享你的结果(和提高准确性的步骤)。</p><p id="df15" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">与任何ML项目一样，关注负责任的AI开发对于评估未来工作的影响至关重要。考虑到最近的工作表明情绪检测方法可能有内在的性别/种族偏见，并可能造成现实世界的伤害，这变得更加重要。此外，如果您正在处理敏感的音频数据(比如包含信用卡详细信息的客户支持电话)，请应用脱敏技术来保护个人身份信息、敏感的个人数据或业务数据。</p><p id="6dc0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">一如既往，如果有更简单的方法来做/解释本文中提到的一些事情，一定要让我知道。一般来说，避免不请自来的破坏性/垃圾/敌意评论！</p><p id="4c60" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">直到下一次✨</p></div><div class="ab cl pu pv hx pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="im in io ip iq"><p id="b660" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我喜欢写循序渐进的初学者指南、操作指南、ML/AI中使用的解码术语等。如果你想完全访问我的所有文章(以及其他媒体上的文章)，那么你可以使用 <a class="ae ml" href="https://varshitasher.medium.com/membership" rel="noopener"> <strong class="ll jd"> <em class="mf">我的链接</em></strong></a><strong class="ll jd"><em class="mf"/></strong><em class="mf">这里</em> <strong class="ll jd"> <em class="mf">注册。</em> </strong></p></div><div class="ab cl pu pv hx pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="im in io ip iq"><div class="nk nl nm nn gt mt"><a rel="noopener follow" target="_blank" href="/step-by-step-guide-to-explaining-your-ml-project-during-a-data-science-interview-81dfaaa408bf"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jd gy z fp my fr fs mz fu fw jc bi translated">在数据科学面试中解释你的ML项目的逐步指南。</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">在结尾有一个额外的样本脚本，让你谨慎地展示你的技术技能！</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="qb l ne nf ng nc nh ni mt"/></div></div></a></div><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jd gy z fp my fr fs mz fu fw jc bi translated">使用Scikit、Pandas和Numpy进行时间序列建模</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">直观地利用季节性来提高模型准确性。</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="qc l ne nf ng nc nh ni mt"/></div></div></a></div><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/hands-on-introduction-to-github-actions-for-data-scientists-f422631c9ea7"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jd gy z fp my fr fs mz fu fw jc bi translated">面向数据科学家的Github操作实践介绍</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">了解如何通过权重和偏差、单元测试、工件创建等实现自动化实验跟踪…</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="qd l ne nf ng nc nh ni mt"/></div></div></a></div><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/deploying-an-end-to-end-deep-learning-project-with-few-clicks-part-2-89009cff6f16"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd jd gy z fp my fr fs mz fu fw jc bi translated">通过几次点击部署端到端深度学习项目:第2部分</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">从Jupyter笔记本到Flask应用程序采用模型，使用Postman和Heroku部署测试API端点</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="qe l ne nf ng nc nh ni mt"/></div></div></a></div></div></div>    
</body>
</html>