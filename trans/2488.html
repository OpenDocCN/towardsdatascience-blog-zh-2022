<html>
<head>
<title>GPU-Acceleration Comes to PyTorch on M1 Macs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPU加速来到M1 MAC电脑上的PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1#2022-05-31">https://towardsdatascience.com/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1#2022-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bd26" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">新的M1芯片在新的PyTorch更新中表现如何？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/007b01f81d848b6d48dc63bf87d6b33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gQG3QVCRzqczt6_r"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@contentpixie?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内容小精灵</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3334" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2020年11月M1苹果机的发布标志着苹果机器处理能力的显著提升[1]。不幸的是，这些新功能直到现在才被集成到PyTorch <em class="lv">中。</em></p><p id="88ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的深度学习模型在很大程度上归功于不断增加的模型规模。那些更大的模型需要更多的计算来训练和运行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">本文的视频版本🙌🏼</p></figure><p id="b8d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些模型太大了，无法在CPU硬件上运行，因为CPU硬件执行大量的分步计算。相反，他们需要大规模的并行计算，就像GPU所执行的那样。</p><p id="9aee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPU使用高度并行的结构，最初设计用于处理视觉繁重过程的图像。它们成为游戏中渲染实时3D图像的重要组件。</p><p id="5442" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种渲染3D图像的能力可以很好地处理深度学习模型中所需的多维计算。自然地，GPU成为模型训练和推理的架构。</p><p id="9564" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GPU对于当今模型的规模至关重要。使用CPU使得许多这些模型太慢而无用，这可能使M1机器上的深度学习相当令人失望。</p><p id="ca3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow从一开始就支持GPU加速[2]，但TensorFlow只是深度学习的两个主流库之一。PyTorch失去了M1的支持。幸运的是，他们刚好赶上。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="2705" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">好吃吗？</h1><p id="a24b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">PyTorch v1.12引入了基于苹果芯片的GPU加速训练。这是PyTorch和苹果金属工程团队的合作成果。</p><p id="b1a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它使用苹果的<strong class="lb iu">M</strong>et al<strong class="lb iu">P</strong>performance<strong class="lb iu">S</strong>haders(MPS)作为PyTorch操作的后端。MPS针对每个M1芯片家族进行了微调。简而言之，这意味着整合速度很快。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/1024b3fa97464fef17c9701920665ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4e3s-kFFvoRUha--JLHz3Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用新的MPS后端进行训练和推理/评估。<a class="ae ky" href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="49bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看一看基线(使用M1超芯片)显示，对于流行的BERT模型，训练加速约7倍，推理加速约14倍。</p><p id="8f87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，在使用我自己的第一代M1 MacBook Pro时，我没有看到同样的加速，特别是当使用如上所示的64批次时。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/99726008ccf3c15b920374541ba0bbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lohPKXih3Z4peAGf0KZ6IQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用基本规格的M1 MacBook Pro在不同批量下的BERT推断时间。</p></figure><p id="e5d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也许这是由于低效的代码或我相对弱小的基本规格MacBook Pro，但我会采取200%的加速任何一天。现在，让我们来看看如何使用这个新的支持MPS的PyTorch，而不是看图表和数字。</p><h1 id="c889" class="mf mg it bd mh mi ne mk ml mm nf mo mp jz ng ka mr kc nh kd mt kf ni kg mv mw bi translated">M1上的GPU加速PyTorch</h1><h2 id="3cb4" class="nj mg it bd mh nk nl dn ml nm nn dp mp li no np mr lm nq nr mt lq ns nt mv nu bi translated">操作系统和Python先决条件</h2><p id="6c55" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">甚至在开始之前，有一些事情可能会让你犯错。首先是先决条件。支持MPS的PyTorch需要MacOS 12.3+ <em class="lv">和</em>一个ARM Python安装。我们可以通过以下方式检查这两种情况:</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="8c50" class="nj mg it nw b gy oa ob l oc od">import platform</span><span id="c1aa" class="nj mg it nw b gy oe ob l oc od">platform.platform()</span><span id="c301" class="nj mg it nw b gy oe ob l oc od">[GOOD] &gt;&gt; macOS-<strong class="nw iu">12.4</strong>-<strong class="nw iu">arm64</strong>-arm-64bit<br/>[BAD]  &gt;&gt; macOS-<strong class="nw iu">11.8</strong>-<strong class="nw iu">x86</strong>_64-i386-64bit</span></pre><p id="5f5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这向我们展示了两件事，<code class="fe of og oh nw b">[ 12.4 | 11.8 ]</code>指的是MacOS版本，这肯定是<code class="fe of og oh nw b">12.3</code>或更高版本。如果不是，更新你的MacOS！另一个是<code class="fe of og oh nw b">[ arm64 | x86 ]</code>。我们想要<code class="fe of og oh nw b">arm64</code>，如果你看到<code class="fe of og oh nw b">x86</code>，那么我们需要为Python创建一个新的ARM环境。</p><p id="5726" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果使用Anaconda，我们切换到终端窗口，创建一个新的ARM环境，如下所示:</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="9465" class="nj mg it nw b gy oa ob l oc od">CONDA_SUBDIR=osx-arm64 conda create -n ml python=3.9 -c conda-forge</span></pre><p id="d9c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们设置conda版本变量来使用ARM环境。然后我们<code class="fe of og oh nw b">create</code>一个新的<code class="fe of og oh nw b">conda</code>环境，命名为(<code class="fe of og oh nw b">-n</code> ) <code class="fe of og oh nw b">ml</code>。接下来，我们将环境设置为使用Python <code class="fe of og oh nw b">3.9</code>，并确保<code class="fe of og oh nw b">conda-forge</code>包存储库包含在我们的通道中(<code class="fe of og oh nw b">-c</code>)。</p><p id="642e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">(如果使用的是Python</em><a class="ae ky" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank"><em class="lv">的另一个版本</em> </a> <em class="lv">，检查是从哪里安装的ARM版本)。</em></p><p id="b8b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">初始化我们的环境后，我们用<code class="fe of og oh nw b">conda activate ml</code>激活它，并修改<code class="fe of og oh nw b">CONDA_SUBDIR</code>变量以永久使用<code class="fe of og oh nw b">osx-arm64</code>。否则，对于将来的pip安装，我们可能会默认回到不正确的<em class="lv"> x84 </em>环境。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="219c" class="nj mg it nw b gy oa ob l oc od">conda env config vars set CONDA_SUBDIR=osx-arm64</span></pre><p id="b894" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会看到一条消息，要求您重新激活环境以使这些更改生效。如果是，使用以下命令切换出并回到<code class="fe of og oh nw b">ml</code>环境:</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="caca" class="nj mg it nw b gy oa ob l oc od">conda activate<br/>conda activate ml</span></pre><h2 id="cf6f" class="nj mg it bd mh nk nl dn ml nm nn dp mp li no np mr lm nq nr mt lq ns nt mv nu bi translated">PyTorch装置</h2><p id="ee81" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">要开始，我们需要安装PyTorch v1.12版。目前，它只在夜间发布。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="011b" class="nj mg it nw b gy oa ob l oc od">pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/d70abc6ac8700a5390608c793e6a15a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4Md2L6fOXNNbByaMKsGyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在下载过程中，您应该能够看到类似于<strong class="bd oj">下载torch-1.1x.x. — arm64.whl </strong>的内容。最后的<strong class="bd oj"> arm64.whl </strong>部分很重要，它告诉我们正在下载正确的版本。</p></figure><p id="433d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用<code class="fe of og oh nw b">transformers</code>和<code class="fe of og oh nw b">datasets</code>库，它们是用<code class="fe of og oh nw b">pip install transformers datasets</code>安装的。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="a9e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">旁注:</em> </strong> <em class="lv">变形金刚库使用Rust内置的记号化器(它让它们更快)。因为我们正在使用这个新的ARM64环境，我们可能会得到</em> <strong class="lb iu"> <em class="lv">错误:为记号赋予器</em> </strong> <em class="lv">构建轮子失败。如果是这样，我们安装</em><a class="ae ky" href="https://huggingface.co/docs/tokenizers/python/v0.9.4/installation/main.html#installation-from-sources" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a><em class="lv">(在相同的环境下)与:</em></p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="2603" class="nj mg it nw b gy oa ob l oc od">curl — proto ‘=https’ — tlsv1.2 -sSf <a class="ae ky" href="https://sh.rustup.rs" rel="noopener ugc nofollow" target="_blank">https://sh.rustup.rs</a> | sh</span></pre><p id="c5db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">然后</em> ` <em class="lv"> pip再次安装变压器数据集</em> ` <em class="lv">。</em></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="1ef3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Python中，我们可以使用<code class="fe of og oh nw b">torch.has_mps</code>来确认MPS正在工作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><h1 id="cd3a" class="mf mg it bd mh mi ne mk ml mm nf mo mp jz ng ka mr kc nh kd mt kf ni kg mv mw bi translated">测试MPS</h1><h2 id="0208" class="nj mg it bd mh nk nl dn ml nm nn dp mp li no np mr lm nq nr mt lq ns nt mv nu bi translated">数据准备</h2><p id="9809" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为了测试新的支持MPS的PyTorch，我们需要一些数据。我们将提取TREC数据集的前1000行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><p id="8fd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用BERT模型测试MPS的性能，为此我们必须使用BERT记号化器来记号化我们的数据。对于前几个测试，我们将只使用TREC数据集的64行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><h2 id="39f1" class="nj mg it bd mh nk nl dn ml nm nn dp mp li no np mr lm nq nr mt lq ns nt mv nu bi translated">推理测验</h2><p id="01e4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在CPU上处理这些令牌时，我们得到的平均处理时间为547毫秒。我们可以通过将<code class="fe of og oh nw b">tokens</code>张量和<code class="fe of og oh nw b">model</code>移动到MPS设备来切换到MPS。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><p id="32f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用相同的方法，在不同的批量下，我们可以看到MPS设备肯定优于CPU，但是对于较小的批量，这不是很明显。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/99726008ccf3c15b920374541ba0bbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lohPKXih3Z4peAGf0KZ6IQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用基本规格的M1 MacBook Pro在不同批量下的BERT推断时间。</p></figure><h2 id="847b" class="nj mg it bd mh nk nl dn ml nm nn dp mp li no np mr lm nq nr mt lq ns nt mv nu bi translated">培训测试</h2><p id="94d7" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在Mac上训练大型模型并不是真正的预期用例，特别是对于低端M1芯片(如这些测试正在运行的第一代M1 MacBook Pro)。不过，<em class="lv">有</em>可能。</p><p id="cf4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在TREC数据集上微调BERT，我们将使用<em class="lv">文本</em>特征作为输入，使用<em class="lv">标签-粗略</em>特征作为目标标签。我们的标签特性包含<em class="lv">六个</em>唯一类，因此我们必须用<em class="lv">六个</em>输出类初始化BERT。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><p id="1979" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们将BERT <code class="fe of og oh nw b">model</code>移动到了MPS设备。在训练循环中遵循类似的步骤，其中每个张量被移动到MPS设备。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok lx l"/></div></figure><p id="4278" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于1000行TREC数据的训练，CPU需要大约25分钟，MPS需要大约18分钟。但是，这不是直接的比较。当微调完整的BERT模型时，MPS设备(GPU)可以处理不超过一个<em class="lv">一个</em>的批处理大小，而对于CPU，我们使用一个<code class="fe of og oh nw b">32</code>的批处理大小。</p><p id="1eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些测试正在优化整个BERT模型，这个模型非常庞大。对于大型语言模型(LLM)的微调，我们通常不需要微调整个模型。相反，我们可以冻结已经预先训练好的模型<em class="lv">核心</em>，并微调组成特定任务<em class="lv">头部</em>的最后几层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/1b44bdd01659d0317343489b94821d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjvEAgLObP7o_nPEabBjGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变压器通常由一个经过预训练的大型<strong class="bd oj">核心</strong>和一个<strong class="bd oj">头部</strong>组成，后者针对分类等特定任务进行了微调。</p></figure><p id="c0fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这种方法，我们获得了更好的训练时间，大约为<em class="lv">四分钟</em>，最大MPS批量大约为64。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="5241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是对新的支持MPS的PyTorch的介绍，以及如何在执行推理时使用它，甚至是用像BERT这样的流行模型进行训练。</p><p id="7a01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解我正在做的事情，我每周都会在YouTube上发帖子，你可以通过Discord直接联系我。我希望能见到你！</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="5dbe" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">资源</h1><p id="8ea4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">[1] <a class="ae ky" href="https://www.nytimes.com/wirecutter/reviews/best-macbooks/" rel="noopener ugc nofollow" target="_blank">纽约时报MacBook评论</a> (2021)</p><p id="a829" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] P. Kanwar，F. Alcober，<a class="ae ky" href="https://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html" rel="noopener ugc nofollow" target="_blank">在Mac上加速TensorFlow性能</a> (2020)，TensorFlow博客</p><p id="6c63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" href="https://github.com/jamescalam/pytorch-mps" rel="noopener ugc nofollow" target="_blank">文章笔记本</a></p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="de23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">*除非另有说明，所有图片均出自作者之手* </em></p></div></div>    
</body>
</html>