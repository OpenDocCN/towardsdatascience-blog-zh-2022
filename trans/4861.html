<html>
<head>
<title>How Deep Learning Enables Large-Scale Scientific Visualization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习如何实现大规模科学可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-deep-learning-enables-large-scale-scientific-visualization-f574408a6b43#2022-10-28">https://towardsdatascience.com/how-deep-learning-enables-large-scale-scientific-visualization-f574408a6b43#2022-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e666" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用神经网络压缩信息用于交互式渲染</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1ba815c647058bff3abee16108ae9a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eK7E5A3uDp1PoLO9mth8Yg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">加州大学戴维斯分校VIDI小组以前的工作样本。图片由加州大学戴维斯分校VIDI小组提供</p></figure><h1 id="46b8" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="fb41" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">作为英特尔的软件架构师和技术传播者，最大的乐趣之一就是有机会看到oneAPI卓越中心(CoE)在全球范围内所做的出色工作。最近，加州大学戴维斯分校可视化和界面设计创新(VIDI)实验室分享了他们如何使用深度学习进行高性能、交互式科学可视化。</p><h2 id="b5bf" class="mj kw iq bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">为什么这很重要？</h2><p id="a58f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">近年来，科学模拟和建模变得越来越详细。模拟可以轻松生成数百万亿字节的数据。对于生成三维数据表示的仿真模型，理解仿真输出的最佳方式是使用实时渲染器对其进行视觉检查。</p><p id="c69f" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">可以想象，使用高保真方法渲染万亿字节的数据，如使用最强大的GPU的<a class="ae na" href="https://en.wikipedia.org/wiki/Ray_marching" rel="noopener ugc nofollow" target="_blank">光线行进</a>(想想更高级的光线跟踪)和<a class="ae na" href="https://en.wikipedia.org/wiki/Path_tracing" rel="noopener ugc nofollow" target="_blank">路径跟踪</a>，是一项挑战。纯粹的强力渲染算法和标准压缩算法无法提供支持这种规模的实时交互所需的性能或压缩率。虽然以每秒1帧(fps)的速度查看模拟输出的渲染非常有价值，但以30fps的速度提供实际的交互式渲染可以为研究人员提供更有用的体验。</p><p id="b008" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">这就是关-刘妈教授和他的团队大卫·鲍尔和吴起所做的工作。他们结合了深度学习和渲染方面的最新研究，提供了一些真正了不起的东西。</p><h1 id="c22b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">用于压缩的即时神经表示</h1><p id="5502" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">上面提到的最大挑战之一是压缩数据，然后快速解压缩大型数据集，速度足够快，以实现实时、交互式渲染。如果我们以30fps的速度渲染，模拟数据的解压缩时间必须少于33.3毫秒。</p><h2 id="96f9" class="mj kw iq bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">压缩类型和速率</h2><p id="3eb9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">第二个难题是压缩率。压缩算法可能是无损的，这意味着可以完全恢复原始输入数据。它们也可能是有损的，这意味着不可能从压缩的表示中重建原始数据。正如人们所料，一般来说，无损算法往往会导致较小的压缩率。</p><p id="aa43" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">人们每天使用的典型压缩文件是zip、mp3和JPEG。前者是无损的，可以实现大约3:1的压缩率。后者是有损的，通常实现大约20:1和10:1的压缩率。这意味着这些典型算法的最大压缩比大约是原始数据大小的1/20。</p><p id="6888" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">出于实时渲染的目的，人们将寻求的主要特征是快速解压缩和相对最小的数据丢失。然而，20:1的压缩率对于许多使用案例来说可能是不够的。</p><h2 id="dc66" class="mj kw iq bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">救援的即时神经表现</h2><p id="23cd" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">正如我们在过去10年中所看到的，现代神经网络非常擅长编码信息和快速解码信息。这两个特征使得神经网络非常适合作为大规模数据集的压缩方法。</p><p id="489f" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">使用神经网络进行压缩的另一个有用特征是，与大多数压缩算法不同，表示的大小不会随着数据集大小的增加而增加。人们可能会认为，随着数据复杂性的增加，静态表示会变得更容易丢失。</p><p id="5cf6" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">然而，给定一个足够大的网络，这种退化不是线性的。由于巨大的模拟模型大小，可以使用大型神经网络，这导致良好的压缩比和随着数据集大小增长的最小信息损失。VIDI团队报告的结果是“比原始数据小10-1000倍，并且使用MLP和散列网格编码几乎是即时训练的”</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/095c6aec20d5db002f9f0e845d0d3cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1nd2zw2HL3FkHeeBrdG0Nw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:在相似的压缩率下，即时神经表示(VNR)质量与朴素方法的比较。图片由加州大学戴维斯分校VIDI小组提供</p></figure><p id="fbeb" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">在图1中，我们可以清楚地看到这种压缩算法相对于传统算法的价值。</p><h2 id="9a94" class="mj kw iq bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">创建表示</h2><p id="6c98" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为了训练模型进行压缩，VIDI团队必须获取3D模拟信息，并将其转换为适合标准机器学习工作流程的东西。从逻辑上讲，这是一个相对标准的机器学习流程:</p><ol class=""><li id="c118" class="nc nd iq lp b lq mv lt mw lw ne ma nf me ng mi nh ni nj nk bi translated">获取3D空间中的随机数据样本</li><li id="9842" class="nc nd iq lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated">使用散列网格编码对每个样本进行编码，以创建传递给DL模型的张量</li><li id="8cff" class="nc nd iq lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated">针对实际样本输出训练样本点的模型</li></ol><p id="17a9" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">为了尽可能提高性能，VIDI团队使用了英特尔开放卷内核库(oneAPI OpenVKL规范的一种实现)，从基础事实中为训练阶段生成训练样本。OpenVKL在这里很有帮助，因为它支持对体数据的高效遍历和采样。</p><p id="1336" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">使用正确的配置，训练模型大约20k步只需要不到一分钟的时间。这样做的结果是合理的峰值信噪比 (PSNR)在大多数情况下约为30dB，在一些数据集中约为49dB以上。对于那些不熟悉PSNR的人来说，它是用分贝(dB)来衡量的。该值越高，图像越接近原始图像。</p><h2 id="421d" class="mj kw iq bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">弥合渲染差距</h2><p id="86ee" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">虽然即时神经表示令人印象深刻，但它本身可能不足以提供高性能的实时交互体验。为了弥合这一差距，加州大学戴维斯分校VIDI团队还开发了几种用于即时神经表示数据的采样和渲染算法。</p><p id="f5bd" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">不幸的是，这里的技术超出了本文的范围，但是你可以查看一下<a class="ae na" href="https://arxiv.org/abs/2207.11620v2" rel="noopener ugc nofollow" target="_blank">即时神经表示和相关渲染优化</a>的细节。</p><p id="790a" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">当然，更有趣的事情是看所有这些如何组合在一起，以实现真正的交互体验。你可以在下面的视频中看到加州大学戴维斯分校VIDI团队训练即时神经表达的速度和取得的令人印象深刻的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><h1 id="0bcd" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="1abd" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我一直认为DL是一种可以解决极其复杂的问题的技术，这些问题需要很大的解决空间；我还没有考虑过如何用“众所周知”的解决方案为问题提供启发性的解决方案。即时神经表示是一个很好的例子，它利用了DL模型的力量，并将其用作一些数据的启发式优化，这些数据可以容忍某种程度的有损转换。</p><p id="ea1f" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">像即时神经表示这样的技术能够与日益复杂的模拟数据进行可用的实时交互。为了强调1fps交互和60fps交互的区别，请看这个有趣的视频。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="ac92" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">我很兴奋也很乐观，像这样的新解决方案将继续推动科学发现。支持与仿真模型数据进行30+ fps的交互对于科学家来说可能是一个游戏改变者，他们将不再需要与他们的数据进行幻灯片式的交互。</p><p id="6246" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated"><em class="ns">如果你想看看我在看什么科技新闻，你可以在Twitter上关注我。此外，一起查看</em> <a class="ae na" href="https://connectedsocialmedia.com/category/code-together/" rel="noopener ugc nofollow" target="_blank"> <em class="ns">代码</em> </a> <em class="ns">，这是我主持的面向开发者的英特尔播客，我们在这里讨论技术。</em></p><p id="0ce9" class="pw-post-body-paragraph ln lo iq lp b lq mv jr ls lt mw ju lv lw mx ly lz ma my mc md me mz mg mh mi ij bi translated">Tony是英特尔的一名软件架构师和技术宣传员。他开发过多种软件开发工具，最近领导软件工程团队构建了数据中心平台，实现了Habana的可扩展MLPerf解决方案。</p><h1 id="0c71" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><p id="40e5" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1]吴起，大卫·鲍尔，迈克尔·j·道尔，关-刘妈，<a class="ae na" href="https://arxiv.org/abs/2207.11620v2" rel="noopener ugc nofollow" target="_blank">用于交互式体绘制的即时神经表示</a>，2022，<a class="ae na" href="https://arxiv.org/abs/2207.11620v2" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2207.11620v2</a></p></div></div>    
</body>
</html>