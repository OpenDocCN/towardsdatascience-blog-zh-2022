<html>
<head>
<title>HyperOpt Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">远视去神秘化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperopt-demystified-3e14006eb6fa#2022-11-08">https://towardsdatascience.com/hyperopt-demystified-3e14006eb6fa#2022-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b873" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用HyperOpt自动调整模型</h2></div><p id="c652" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你喜欢调模型吗？如果你的答案是“是”，那么这篇文章对你来说是<strong class="kh ir">而不是</strong>。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/26eae3b85d06e136fc97de5cba81c238.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*2nxaXLy2bO0bUrq9n17K4g.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">我爷爷的一幅漫画——<a class="ae ln" href="https://cartoonsbydale.com/gallery-1" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></figure><p id="290d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我们将介绍非常流行的自动超参数调整算法，称为<a class="ae ln" href="https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf" rel="noopener ugc nofollow" target="_blank">基于树的Parzen估计器</a> (TPE)。开源包HyperOpt支持TPE。通过利用HyperOpt和TPE，机器学习工程师可以<strong class="kh ir">快速开发高度优化的模型，而无需任何手动调整</strong>。</p><p id="4525" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，我们开始吧！</p><h1 id="764f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">技术TLDR</h1><p id="af78" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">HyperOpt是一个开源python包，它使用一种称为基于树的Parzen Esimtors (TPE)的算法来选择优化用户定义的目标函数的模型超参数。通过简单地定义每个超参数的函数形式和边界，TPE彻底而有效地搜索复杂的超空间以达到最优。</p><p id="ac62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TPE是一种顺序算法，它利用贝叶斯更新并遵循以下顺序。</p><ol class=""><li id="b9d6" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">用几组随机选择的超参数训练模型，返回目标函数值。</li><li id="7df2" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">根据某个阈值γ，将我们观察到的目标函数值分成“好”和“坏”组。</li><li id="207a" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">计算“承诺度”得分，正好是<em class="mz">P(x |好)/P(x |差)</em>。</li><li id="52f9" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">通过混合模型确定最大化承诺的超参数。</li><li id="f647" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">使用步骤4中的超参数拟合我们的模型。</li><li id="0cc2" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">重复步骤2-5，直到达到停止标准。</li></ol><p id="3a61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有一个<a class="ae ln" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank">快速代码示例</a>。</p><h1 id="47d9" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">1 —什么是超参数调谐？</h1><p id="8b49" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">好吧，这是很多大词。让我们慢下来，真正明白发生了什么。</p><h2 id="805c" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">1.1 —我们的目标</h2><p id="604b" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">数据科学家很忙。我们希望生产出真正好的模型，但是要以一种有效的、理想的不干涉的方式来实现。</p><p id="1097" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，ML建模生命周期中的某些步骤很难自动化。例如，探索性数据分析(EDA)和特征工程通常是特定于主题的，并且需要人类的直觉。另一方面，模型调整是一个迭代过程，计算机在这个过程中表现出色。</p><p id="8787" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">我们在这篇文章中的目标是理解如何利用算法来自动化模型调整过程。</strong></p><p id="f903" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了帮助我们思考这个目标，让我们打个比方:我们是寻找宝藏的海盗。同样重要的是要注意到我们是非常高效的海盗，他们希望尽量减少我们寻找宝藏的时间。那么，我们应该如何最大限度地减少搜索时间呢？答案是<strong class="kh ir">用地图！</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/8af9d57d4940abcfdfe4161eeeab0456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qmPsp_N8pcACpy9bHJUAnA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图<code class="fe nr ns nt nu b">1</code>:3D超参数搜索空间示例。藏宝箱的位置是全局最优的。图片作者。</p></figure><p id="265f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图1中，我们有一个虚拟的地图，显示了我们的宝藏所在的位置。经过大量的攀爬和挖掘，找到宝藏并不困难，因为我们知道它的确切位置。</p><p id="f814" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是当我们没有地图时会发生什么呢？</p><p id="4b57" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们负责调优一个模型时，不幸的是没有给我们一张地图。我们的地形，对应于超参数搜索空间，是未知的。此外，对应于最优超参数集的我们的宝藏的位置也是未知的。</p><p id="5c60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这个设置，让我们来讨论一些有效探索这个空间和找到一些宝藏的潜在方法！</p><h2 id="2934" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">1.2 —潜在解决方案</h2><p id="92a9" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">模型调整的原始方法是“手动的”——工程师实际上会手动测试许多不同的配置，并查看哪个超参数组合产生最佳模型。虽然信息丰富，但这一过程效率低下。一定有更好的方法…</p><p id="cf76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1.2.1 —网格搜索(最差)</strong></p><p id="b9f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的第一个优化算法是网格搜索。网格搜索迭代地测试用户指定网格内超参数的所有可能组合。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nv"><img src="../Images/43cdf35987a16309fed2c9228cb03ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-PMVoT6mlD2i02VBGlfoA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图2:网格搜索布局的例子。作者图片</p></figure><p id="6aaf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，在图2中，你看到红点的地方就是我们重新训练和评估模型的地方。这个框架是低效的，因为它重用了坏的超参数T4。例如，如果超参数2对我们的目标函数几乎没有影响，我们仍然会测试它的值的所有组合，从而将所需的迭代次数增加10倍(在本例中)。</p><p id="a40e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但在继续之前，重要的是要注意网格搜索仍然相当流行，因为它保证在给定正确指定的网格的情况下找到最优解。如果你决定使用这种方法，确保你转换你的网格来反映你的超参数的函数形式。例如，<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林分类器</a>的max_depth是一个整数——不要让它搜索连续的空间。它也不太可能具有均匀分布，如果您知道超参数的函数形式，请变换网格以反映它。</p><p id="3fb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，<strong class="kh ir">网格搜索受到维数灾难的影响，并且在评估之间重新计算信息，但是仍然被广泛使用。</strong></p><p id="0408" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1.2.2 —随机搜索(好)</strong></p><p id="1691" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的第二个算法是随机搜索。随机搜索在用户指定的网格中尝试随机值。与网格搜索不同，我们不需要测试每个可能的超参数组合，这提高了效率。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nw"><img src="../Images/8d2530bc8f9f7b334ec444465bd9a5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIIIn3E0_hsJb5XtJJiuHw.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图3:随机搜索的例子。图片作者。</p></figure><p id="24ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有一个很酷的事实:随机搜索将在<a class="ae ln" href="https://web.archive.org/web/20160701182750/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning" rel="noopener ugc nofollow" target="_blank"> 60次迭代</a>中找到(平均)前5%的超参数配置。也就是说，和网格搜索一样，你必须转换你的搜索空间来反映每个超参数的函数形式。</p><p id="dd77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">随机搜索是超参数优化的良好基础。</strong></p><p id="ead3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1.2.3 —贝叶斯优化(更好)</strong></p><p id="e55e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的第三个候选算法是我们的第一个基于序列模型的优化(SMBO)算法。与现有技术的关键概念差异是我们<strong class="kh ir">迭代地使用先前的运行来确定未来的勘探点。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/8b0ac24b38972392cdb20803955cbcc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/1*yVYgXhaWbtMi6CwGRQGk4w.gif"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图4:贝叶斯优化示例— <a class="ae ln" href="https://distill.pub/2020/bayesian-optimization/" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="f1b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贝叶斯超参数优化试图开发我们的超参数搜索空间的概率分布。从那里，它使用一个获取功能，如预期改进，来转换我们的多维空间，使其更“可搜索”最后，它使用优化算法，例如随机梯度下降，来寻找使我们的获取函数最大化的超参数。这些超参数用于拟合我们的模型，并且重复该过程直到收敛。</p><p id="22cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">贝叶斯优化通常优于随机搜索，但是它有一些核心限制，例如需要数字超参数。</strong></p><p id="4b1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1.2.4 —基于树的Parzen估值器(最佳)</strong></p><p id="a912" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们来谈谈这个节目的明星:基于树的Parzen估计器(TPE)。TPE是另一种SMBO算法，通常优于基本贝叶斯优化，但主要卖点是它通过树结构处理复杂的超参数关系。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ny"><img src="../Images/860243ccd36e15103a2f8fb167481545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQrm-y_W2VMB4xvvOaKsRw.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图TPE的层次结构示例— <a class="ae ln" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="b259" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们用图5来理解这个<strong class="kh ir">树形结构</strong>。这里我们正在训练一个支持向量机(SVM)分类器。我们将测试两个内核:<code class="fe nr ns nt nu b">linear</code>和<code class="fe nr ns nt nu b">RBF</code>。一个<code class="fe nr ns nt nu b">linear</code>内核没有宽度参数，但是<code class="fe nr ns nt nu b">RBF</code>有，所以通过使用嵌套字典，我们能够编码这个结构，从而限制搜索空间。</p><p id="9e2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TPE还支持传统贝叶斯优化所不支持的分类变量。</p><p id="0d96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快速免责声明在继续之前，有<a class="ae ln" href="https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization" rel="noopener ugc nofollow" target="_blank">许多其他超参数调整包</a>。每个都支持各种算法，其中一些包括随机森林、高斯过程和遗传算法。TPE是一种非常流行的通用算法，但不一定是最好的。</p><p id="5cc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">总的来说，TPE是一个真正强大而高效的超参数优化解决方案。</strong></p><h1 id="905c" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">2 —基于树的Parzen估计器(TPE)是如何工作的？</h1><p id="6a48" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">现在，我们已经对一些流行的超参数优化算法有了大致的了解，让我们来深入了解一下TPE是如何工作的。</p><p id="7743" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回到我们的类比，我们是寻找宝藏的海盗<strong class="kh ir">，但是没有地图</strong>。我们的船长需要尽快找到宝藏，所以我们需要在有很高可能性找到宝藏的战略地点挖掘，利用之前的挖掘来确定未来挖掘的位置。</p><h2 id="b39f" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">2.1 —初始化</h2><p id="bc95" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">首先，我们<strong class="kh ir">定义空间</strong>的约束。如上所述，我们的超参数通常具有函数形式、最大/最小值以及与其他超参数的层级关系。利用我们对最大似然算法和数据的了解，我们可以定义我们的搜索空间。</p><p id="d120" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们必须<strong class="kh ir">定义我们的目标函数</strong>，它用于评估我们的超参数组合有多“好”。一些例子包括经典的ML损失函数，如RMSE或AUC。</p><p id="7a50" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！现在我们有了一个有限的搜索空间和一个衡量成功的方法，我们准备开始搜索…</p><h2 id="970b" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">2.2 —迭代贝叶斯优化</h2><p id="cb9d" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">贝叶斯优化是一种顺序算法，根据目标函数在超空间中找到“成功”概率高的点。TPE利用贝叶斯优化，但使用一些聪明的技巧来提高性能和处理搜索空间的复杂性…</p><p id="9921" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.2.0 —概念设置</strong></p><p id="3b10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个技巧是建模<em class="mz"> P(x|y) </em>而不是<em class="mz"> P(y|x)… </em></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/568fbbaf00673eda268531b8d4cdbbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*iey54qlRq5FUBuQSK0xsAw.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图TPE寻求解决的条件概率。图片作者。</p></figure><p id="247b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贝叶斯优化通常着眼于模型<em class="mz">P(y | x)</em>，这是给定超参数(<em class="mz"> x </em>)的目标函数值(<em class="mz"> y </em>)的概率。TPE做相反的事情——它观察模型<em class="mz"> P(x|y)，</em>，这是给定目标函数值(<em class="mz"> y </em>)的超参数(<em class="mz"> x </em> ) <em class="mz">，</em>的概率。</p><p id="c7b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">简而言之，TPE试图找到最佳的目标函数值，然后确定相关的超参数。</strong></p><p id="bbce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这个非常重要的设置，让我们进入实际的算法。</p><p id="4c7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.2.1 —将我们的数据分为“好”和“坏”组</strong></p><p id="06be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住，我们的目标是根据某个目标函数找到最佳的超参数值。那么，我们如何利用<em class="mz"> P(x|y) </em>来做到这一点呢？</p><p id="d07e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，TPE将我们观察到的数据点分成两组:<strong class="kh ir">好的</strong>，记为<em class="mz">g(x)</em>和<strong class="kh ir">差的</strong>，记为<em class="mz"> l(x) </em>。好与坏之间的界限由用户定义的参数gamma (γ)确定，该参数对应于分割我们的观察结果的目标函数百分点(<em class="mz"> y* </em>)。</p><p id="5736" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，当γ = 0.5时，分割我们观察值的目标函数值(<em class="mz"> y* </em>)将是我们观察点的中值。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oa"><img src="../Images/f4884f2b614a9660a2e1b4ab0bbf0888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-o9ePt_NbzO7f4OrhTD4kw.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图7:将p(x|y)分解成两组。图片作者。</p></figure><p id="d224" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图7所示，我们可以使用上面的框架形式化<em class="mz"> p(x|y) </em>。用海盗的比喻来说…</p><blockquote class="ob oc od"><p id="718c" class="kf kg mz kh b ki kj jr kk kl km ju kn oe kp kq kr of kt ku kv og kx ky kz la ij bi translated">海盗视角:看看我们已经探索过的地方，l(x)列出了宝藏很少的地方，g(x)列出了宝藏很多的地方。</p></blockquote><p id="6e5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.2.32—计算“承诺性”得分</strong></p><p id="eb8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二，TPE定义了我们应该如何通过<strong class="kh ir">“承诺度”得分</strong>评估未观察到的超参数组合。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/17df42f111dad0176f6c32faea055a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*pBX6zpj7K207Vt0YASLQpg.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图8:承诺分数定义。图片作者。</p></figure><p id="59df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图8定义了我们的承诺分数(<em class="mz"> P </em>)，这只是以下几个部分的比率…</p><ul class=""><li id="4b76" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la oi mr ms mt bi translated"><strong class="kh ir">分子</strong>:观察到一组超参数的概率(<em class="mz"> x </em>，给定对应的目标函数值为“<strong class="kh ir">好</strong>”</li><li id="7b57" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated"><strong class="kh ir">分母</strong>:观察一组超参数的概率(<em class="mz"> x </em>，给定对应的目标函数值为“<strong class="kh ir">坏</strong>”</li></ul><p id="79d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">“承诺”值越大，我们的超参数<em class="mz"> x </em>产生“良好”目标函数的可能性就越大。</strong></p><blockquote class="ob oc od"><p id="f457" class="kf kg mz kh b ki kj jr kk kl km ju kn oe kp kq kr of kt ku kv og kx ky kz la ij bi translated">海盗视角:许诺性显示了在我们的地形中某个给定的位置有多少可能拥有大量的宝藏。</p></blockquote><p id="8581" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，先说一句，如果你熟悉贝叶斯优化，这个等式就像一个获取函数，与<a class="ae ln" href="http://krasserm.github.io/2018/03/21/bayesian-optimization/" rel="noopener ugc nofollow" target="_blank">预期改善(EI) </a>成比例。</p><p id="ac25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2.2.3—创建概率密度估计值</strong></p><p id="d281" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三，TPE希望通过<a class="ae ln" href="https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/readings/L10%20Mixture%20Modeling.pdf" rel="noopener ugc nofollow" target="_blank">混合模型</a>评估“承诺度”得分。<strong class="kh ir">混合模型的思想是取多个概率分布，用一个线性组合——</strong><a class="ae ln" href="https://www.coursera.org/lecture/mixture-models/basic-definitions-Eh35h#:~:text=Mixture%20models%20are%20probability%20distributions,sense%20using%20a%20linear%20combination." rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">src</strong></a><strong class="kh ir">把它们放在一起。然后，这些组合的概率分布被用于开发概率密度估计。</strong></p><p id="f0b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，混合物建模过程是…</p><ol class=""><li id="4245" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated"><strong class="kh ir">定义我们积分的分布类型。</strong>在我们的案例中，如果我们的变量是分类的，我们使用重新加权的分类分布，如果是数值的，我们使用高斯(即正态)或均匀分布。</li><li id="b169" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kh ir">迭代每个点，并在该点插入一个分布。</strong></li><li id="456b" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kh ir">对所有分布的质量求和，以获得概率密度估计值。</strong></li></ol><p id="acf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，该过程是针对两组<em class="mz"> l(x) </em>和<em class="mz"> g(x)单独运行的。</em></p><p id="d48e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看一下图9中的例子…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi oj"><img src="../Images/0ee66922f5a32d9bce28ac3ee1e16df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCLV9mAoFIvMjFDOSKVF-Q.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图9:适合3个超参数观测值的截断高斯分布示例。图片作者。</p></figure><p id="fdb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每个观察值(x轴上的蓝点)，我们创建一个正态分布~N(μ，σ)，其中…</p><ul class=""><li id="e41e" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la oi mr ms mt bi translated"><strong class="kh ir"> μ (mu) </strong>是我们正态分布的均值。它的值是我们点在x轴上的位置。</li><li id="d42d" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated"><strong class="kh ir"> σ (sigma) </strong>是我们正态分布的标准差。其值是到最近邻点的距离<strong class="kh ir">。</strong></li></ul><p id="0994" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果点靠得很近，标准差会很小，因此分布会很高，反之，如果点分散，分布会很平坦(图10)</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ok"><img src="../Images/f9f838726a1a5ff6a82f19c061c2a71d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3HwLx8nqi37EJT8qeUtayg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图10:标准偏差对正态分布形状影响的例子。图片作者。</p></figure><blockquote class="ob oc od"><p id="3221" class="kf kg mz kh b ki kj jr kk kl km ju kn oe kp kq kr of kt ku kv og kx ky kz la ij bi translated">海盗视角:不——海盗不擅长混合模型。</p></blockquote><p id="381b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，另一个快速旁白:如果您正在阅读文献，您会注意到TPE使用“截断”高斯，这仅仅意味着高斯受我们在超参数配置中指定的范围限制，而不是扩展到+/-无穷大。</p><p id="5bd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确定下一个探索点！</p><p id="6834" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们把这些碎片放在一起。到目前为止，我们已经1)获得了目标函数观察值，2)定义了我们的“前景”公式，以及3)通过基于先前值的混合模型创建了概率密度估计。我们有所有的片段来评估给定的点！</p><p id="a1be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的第一步是为<em class="mz"> g(x) </em>和<em class="mz"> l(x) </em>创建一个平均概率密度函数(PDF)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi ol"><img src="../Images/ee3cf1937f4a1eb54cec91a1870827fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9r0WdKi30mEq2SV-zhZhg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图11:给定3个观察点的平均概率密度的叠加。图片作者。</p></figure><p id="4769" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图11显示了一个示例流程，红线是我们的平均PDF，是所有PDF的总和除以PDF的数量。</p><p id="cd29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">利用平均PDF，我们可以得到任意超参数值(<em class="mz"> x </em>)在<em class="mz"> g(x) </em>或<em class="mz"> l(x) </em>中的概率。</strong></p><p id="0ac1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，假设图11中的观察值属于“好的”集合，<em class="mz"> g(x) </em>。基于我们的平均PDF，3.9或0.05的超参数值不太可能属于“好”集。相反，一个~1.2的超参数值似乎很可能属于“好”集。</p><p id="5acc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这只是图片的一半。我们对“坏”集合应用相同的方法，<em class="mz"> l(x)。</em>因为我们希望最大化<em class="mz"> g(x) / l(x) </em>，<strong class="kh ir">有希望的点应该位于<em class="mz"> g(x) </em>高而<em class="mz"> l(x) </em>低<em class="mz">的地方。</em> </strong></p><p id="f88b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很酷，对吧？</p><p id="6929" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了这些概率分布，我们可以从我们的树形结构超参数中进行采样，并找到最大化“承诺性”的超参数集，从而值得探索。</p><blockquote class="ob oc od"><p id="a44e" class="kf kg mz kh b ki kj jr kk kl km ju kn oe kp kq kr of kt ku kv og kx ky kz la ij bi translated">海盗视角:我们挖掘的下一个地点是最大化(拥有大量宝藏的概率)/(拥有少量宝藏的概率)的地点。</p></blockquote><h1 id="5f38" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">3-代码</h1><p id="e1d3" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">现在您已经知道了它是如何工作的，下面是一些通过开源包HyperOpt实现TPE的实用技巧。</p><h2 id="4d1c" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">3.1 —超视App的结构</h2><p id="7e9a" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">通常，利用HyperOpt时有三个主要步骤…</p><ol class=""><li id="5164" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated"><strong class="kh ir">定义搜索空间，</strong>搜索空间就是你要优化的超参数的范围和函数形式。</li><li id="0e19" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kh ir">定义拟合函数</strong>，该函数调用给定训练/测试分割的<code class="fe nr ns nt nu b">model.fit()</code>函数。</li><li id="eaff" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kh ir">定义目标函数</strong>，它是任何经典的准确性度量，如RMSE或AUC。</li></ol><p id="dca6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，这些自动调优方法仍然需要数据科学家的设计输入——这不是完全免费的午餐。然而，有趣的是，TPE对超参数错误设定相当稳健(在合理范围内)。</p><h2 id="6ef9" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">3.2—提示和技巧</h2><ul class=""><li id="9a84" class="ml mm iq kh b ki mg kl mh ko om ks on kw oo la oi mr ms mt bi translated">HyperOpt可以通过<a class="ae ln" href="https://docs.databricks.com/machine-learning/automl-hyperparam-tuning/hyperopt-concepts.html#:~:text=A%20Trials%20or%20SparkTrials%20object,Horovod%20in%20the%20objective%20function." rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>和<a class="ae ln" href="http://hyperopt.github.io/hyperopt/scaleout/mongodb/" rel="noopener ugc nofollow" target="_blank"> MongoDB </a>并行化。如果您正在使用多个内核，无论是在云中还是在本地机器上，这都会大大减少运行时间。</li><li id="af99" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated">如果您正在通过Apache Spark并行化调优过程，那么对于单节点ML模型(sklearn)使用一个<code class="fe nr ns nt nu b">SparkTrials</code>对象，对于并行化ML模型(MLlib)使用一个<code class="fe nr ns nt nu b">Trails</code>对象。代码如下。</li><li id="2463" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated">MLflow 是一个追踪模型运行的开源方法。它很容易与远视集成。</li><li id="1c6a" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated">不要过早缩小搜索空间。超参数的一些组合可能会有惊人的效果。</li><li id="9c96" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated">定义搜索空间可能很棘手，尤其是如果您不知道超参数的<a class="ae ln" href="https://hyperopt.github.io/hyperopt/getting-started/search_spaces/" rel="noopener ugc nofollow" target="_blank">函数形式。然而，从个人经验来看，TPE对于那些函数形式的错误指定是相当健壮的。</a></li><li id="0356" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la oi mr ms mt bi translated">选择一个好的目标函数大有帮助。在大多数情况下，误差是不相等的。如果某种类型的错误更有问题，请确保将该逻辑构建到您的函数中。</li></ul><h2 id="9245" class="na lp iq bd lq nb nc dn lu nd ne dp ly ko nf ng ma ks nh ni mc kw nj nk me nl bi translated">3.3—代码示例</h2><p id="447e" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">下面是以分布式方式运行HyperOpt的一些代码。它改编自书中的代码，<a class="ae ln" href="https://www.manning.com/books/machine-learning-engineering-in-action" rel="noopener ugc nofollow" target="_blank">机器学习工程在行动</a>——这里是<a class="ae ln" href="https://github.com/BenWilson2/ML-Engineering/tree/main/notebooks/ch08" rel="noopener ugc nofollow" target="_blank"> git repo </a>。</p><p id="08a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个代码片段的一些不错的特性包括通过<a class="ae ln" href="http://hyperopt.github.io/hyperopt/scaleout/spark/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>的并行化和通过<a class="ae ln" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank"> MLflow </a>的模型日志。还要注意，这个代码片段优化了一个sk learn RandomForestRegressor——您必须更改模型和拟合函数以适应您的需要。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="90b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，<a class="ae ln" href="https://github.com/BenWilson2/ML-Engineering/tree/main/notebooks/ch08" rel="noopener ugc nofollow" target="_blank">原始代码</a>非常有价值，并且没有被广泛利用。它有从<code class="fe nr ns nt nu b">SparkTrials</code>对象中提取运行信息的方法，这些信息可以用来显示精度随时间的提高。<strong class="kh ir">真正高ROI的活动是为您的数据科学团队调整和模块化原始代码的功能，这大约需要一天时间。</strong>如果我错了，留下评论，我会删除这一段。</p><h1 id="6ef6" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">4 —摘要</h1><p id="1373" class="pw-post-body-paragraph kf kg iq kh b ki mg jr kk kl mh ju kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">这就是你要的——远视的荣耀！</p><p id="db68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了强调关键点，让我们快速回顾一下。</p><p id="adf0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超参数调整是ML模型生命周期的必要部分，但是非常耗时。基于序列模型的优化(SMBO)算法擅长于在复杂的超空间中搜索最优解，并且它们可以应用于超参数调整。基于树的Parzen估计器(TPE)是一种非常有效的SMBO，其性能优于贝叶斯优化和随机搜索。</p><p id="2e44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TPE重复以下步骤，直到达到停止标准:</p><ol class=""><li id="3804" class="ml mm iq kh b ki kj kl km ko mn ks mo kw mp la mq mr ms mt bi translated">根据一些超参数γ，将观察点分为“好”和“坏”组。</li><li id="7db1" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">将混合模型拟合到“好”和“坏”集，以开发平均概率密度估计。</li><li id="180b" class="ml mm iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">选择最大化“承诺”分数的点，这利用步骤2来估计在“好”和“坏”集合中的概率。</li></ol><p id="0a3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们有一个非常酷的代码片段，展示了如何通过SparkTrials并行化HyperOpt。它还将我们的所有迭代记录到MLflow中。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="1022" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mz">感谢阅读！我会再写9篇文章，把学术研究带到DS行业。如果你有兴趣，可以随时</em> <a class="ae ln" href="https://michaelberk.medium.com/" rel="noopener"> <em class="mz">关注我</em> </a> <em class="mz"> </em>🙂。</p></div></div>    
</body>
</html>