<html>
<head>
<title>What Makes Bagging Algorithms Superior?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是什么让Bagging算法优越？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-makes-bagging-algorithms-superior-4a3882dab59f#2022-05-02">https://towardsdatascience.com/what-makes-bagging-algorithms-superior-4a3882dab59f#2022-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c09" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">证明随机森林等装袋算法性能优于决策树的证据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6298921fcf1dc146a7eb0da2b4517a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iNfqqoqNN1kzvs83"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">维多利亚·帕拉西奥斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8a01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我之前的<a class="ae ky" rel="noopener" target="_blank" href="/battle-of-the-ensemble-random-forest-vs-gradient-boosting-6fbfed14cb7">文章</a>中，我比较了随机森林和梯度推进，我简要介绍了集成方法的概念，并解释了为什么它们比单独的机器学习模型表现得更好。然而，在那篇文章中，我们仅仅认为这是事实，而没有真正讨论为什么会这样。</p><p id="57a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，我们将深入研究集成方法的概念，特别是bagging算法，并提供证据说明它们为什么优于单个决策树模型。</p><p id="3f01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相关代码的链接可以在我的GitHub <a class="ae ky" href="https://github.com/chongjason914/bagging-vs-boosting/blob/main/bagging-algorithm.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="24d6" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">定义</h1><p id="e280" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在我们开始之前，让我们快速回顾一下决策树、集成方法和bagging的定义。</p><blockquote class="mz na nb"><p id="f91a" class="kz la nc lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">决策树是一种监督学习算法，可用于分类和回归问题。树的每个节点代表一个变量和分裂点，分裂点将数据分成单独的预测。</p><p id="98e0" class="kz la nc lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">集成方法涉及聚合多个机器学习模型，目的是减少偏差和方差。</p><p id="ae08" class="kz la nc lb b lc ld ju le lf lg jx lh nd lj lk ll ne ln lo lp nf lr ls lt lu im bi translated">Bagging也称为bootstrap aggregating，是指使用称为bootstrapped数据集的训练数据子集创建和合并独立、并行决策树集合的过程。</p></blockquote><h1 id="55f6" class="mc md it bd me mf ng mh mi mj nh ml mm jz ni ka mo kc nj kd mq kf nk kg ms mt bi translated">单一决策树</h1><p id="61aa" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了理解集成方法产生的改进，我们将首先检查一个决策树模型是如何建立的，并在基础模型上进一步重申。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="30f9" class="nq md it nm b gy nr ns l nt nu">head(swiss)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b44eea53df6d96a727add13a7e0d51f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*-QqlZgDkIvV6T9qVZNDn_w.png"/></div></figure><p id="6e89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Swiss是一个包含6个变量的47个观察值的数据框架(每个变量都以百分比表示),它显示了大约在1888年瑞士47个法语省份的标准化生育率和社会经济指标。</p><p id="7907" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们将生育率设置为我们的目标变量，即我们的模型试图预测的变量和作为预测变量的其他变量。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="1fe7" class="nq md it nm b gy nr ns l nt nu"># Seed for reproducibility <br/>set.seed(20)</span><span id="2ee7" class="nq md it nm b gy nw ns l nt nu"># Assign half of the data set as training data <br/>training.rows = sample(1:nrow(swiss), 0.5*nrow(swiss))<br/>train = swiss[training.rows, ]<br/>head(train)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/99c56ba6b5e0212d07eacc3b96205c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*6qEvaMwJZlViwo0lv8tLxQ.png"/></div></figure><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="0057" class="nq md it nm b gy nr ns l nt nu"># Tree package <br/>install.packages(tree)<br/>library(tree)</span><span id="5f00" class="nq md it nm b gy nw ns l nt nu"># Fit single decision tree model to training data <br/>s_tree = tree(Fertility ~ ., data = train)</span><span id="cba2" class="nq md it nm b gy nw ns l nt nu"># Plot model<br/>plot(s_tree)<br/>text(s_tree, pos = 1, offset = 1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/caf87d93d5aeb803f012748709ec0134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*gUg83U7Vr6IQNRtFoIAf3w.png"/></div></figure><p id="ba7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们想预测这个地区的生育率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f8f9bd03f0c3e72061e85d72ba8d281f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*2HKnlGJu3MPxF925QQGSrg.png"/></div></figure><p id="040d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从树的顶部到底部手动跟踪数据，或者使用r。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="ff53" class="nq md it nm b gy nr ns l nt nu">predict(s_tree, swiss["Courtelary", ])</span></pre><p id="28e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两种方法得出的值都是55.22。</p><p id="8d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们可能还想看看我们的模型在维持集(有时称为测试集)上的表现，这是我们的模型最初没有训练的数据。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="3bc1" class="nq md it nm b gy nr ns l nt nu"># Get test set <br/>test = swiss[-training.rows, ]<br/>y_tree = predict(s_tree, test)</span><span id="7030" class="nq md it nm b gy nw ns l nt nu"># Plot predicted vs observed values <br/>plot(test$Fertility, y_tree, main = "Predicted vs observed fertility", ylab = "Predicted values", xlab = "Actual values")<br/>abline(0, 1, col = "red")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/f7edb48ff83257256b17cd300fe25f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iX2KK1_mElPaCaGRfEiK7g.png"/></div></div></figure><h1 id="c40c" class="mc md it bd me mf ng mh mi mj nh ml mm jz ni ka mo kc nj kd mq kf nk kg ms mt bi translated">自举样本</h1><p id="6e8e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在我们讨论单决策树模型的结果之前，让我们使用一个引导示例重复这个过程。这可以通过将示例函数中的replace参数设置为true来实现。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="460e" class="nq md it nm b gy nr ns l nt nu"># Use bootstrapped sample <br/>set.seed(8499)<br/>bag.rows = sample(1:nrow(train), replace = TRUE)<br/>bag = train[bag.rows, ]</span><span id="7c37" class="nq md it nm b gy nw ns l nt nu"># Fit decision tree model <br/>s_bag = tree(Fertility ~ ., data = bag)</span><span id="5aa4" class="nq md it nm b gy nw ns l nt nu"># Predict <br/>predict(s_bag, test["Courtelary", ])</span></pre><p id="a250" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用自举数据集，我们得到的预测值为75.68。</p><p id="e196" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，55.22和75.68的预测值相差甚远。这反映了决策树是一个典型的高方差模型。换句话说，即使使用稍微不同的数据集来训练模型，也会导致非常不同的决策树，从而导致不同的预测。</p><p id="76be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决这个问题的一种方法是采用袋装决策树的思想，其中构建了许多树，并对预测进行平均以获得整体的单个预测。这就是所谓的随机森林。</p><p id="3013" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">库特拉里生育率的真实值是80.2，所以第二棵树预测的75.68更接近。然而，我们只是直接比较了一个特定省份的预测。尽管第二个树的预测更接近于Courtelary，但我们并不确定它是否真的整体表现更好。</p><h1 id="48f6" class="mc md it bd me mf ng mh mi mj nh ml mm jz ni ka mo kc nj kd mq kf nk kg ms mt bi translated">随机森林</h1><p id="f6fc" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了解决决策树模型中的高方差问题，我们可以部署随机森林，它依赖于一组决策树来生成预测。本质上，随机森林模型将对大量决策树返回的值进行平均。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="b024" class="nq md it nm b gy nr ns l nt nu"># Set the number of decision trees <br/>trees = 1000</span><span id="0f89" class="nq md it nm b gy nw ns l nt nu"># Construct matrix to store predictions <br/>preds = matrix(NA, nrow = nrow(test), ncol = trees)</span><span id="582c" class="nq md it nm b gy nw ns l nt nu"># Fit decision tree model and generate predictions <br/>set.seed(8499)<br/>for (i in 1:trees) {<br/>    bag.rows = sample(1:nrow(train), replace = TRUE)<br/>    bag = train[bag.rows, ]<br/>    s_bag = tree(Fertility ~ ., data = bag)<br/>    preds[, i] = predict(s_bag, test)<br/>}</span><span id="75c2" class="nq md it nm b gy nw ns l nt nu"># Take the average from predictions <br/>y_bag = rowMeans(preds)</span><span id="ec3e" class="nq md it nm b gy nw ns l nt nu"># Plot predicted vs observed values <br/>plot(test<strong class="nm iu">$</strong>Fertility, y_bag, main <strong class="nm iu">=</strong> "Predicted vs observed fertility", ylab <strong class="nm iu">=</strong> "Predicted values", xlab <strong class="nm iu">=</strong> "Actual values")<br/>abline(0, 1, col <strong class="nm iu">=</strong> "red")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/eb81b6ffa76bd5f3fc7a2b0967a72aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0skMXJJoaH68nZodxzZGIg.png"/></div></div></figure><p id="11d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图表明，使用训练数据构建的单一决策树在预测测试数据时表现不佳。这些点离参考线相当远，并且对于许多外部节点来说，这些点看起来并没有广泛地分散在参考线上。</p><p id="cc10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，袋装决策树方法似乎表现得更好。这些点更加紧密地分布在参考线周围。然而，一些点似乎仍然在实际和预测之间有很大的差异。</p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="9e75" class="nq md it nm b gy nr ns l nt nu">round(mean((test<strong class="nm iu">$</strong>Fertility <strong class="nm iu">-</strong> y_tree)<strong class="nm iu">^</strong>2), 2)<br/>round(mean((test<strong class="nm iu">$</strong>Fertility <strong class="nm iu">-</strong> y_bag)<strong class="nm iu">^</strong>2), 2)</span></pre><p id="a63b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树的MSE是128.03，袋装方法的MSE是53.85。</p><p id="5940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如所料，通过取1000个预测的平均值，袋装预测的MSE要低得多。这告诉我们，平均而言，袋装预测更接近真实值。这与我们对图表的评论一致，袋装预测显示出与真实值更好的一致性。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="205c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们回顾了集成方法的概念，特别是bagging算法。我们不仅展示了bagging算法是如何工作的，更重要的是，为什么它优于单个决策树模型。</p><p id="045e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过对多个决策树取平均值，随机森林模型能够解决单个决策树模型中存在的高方差问题，从而生成更准确的整体预测。</p><p id="25e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你从这篇文章中发现了任何价值，并且还不是一个媒体会员，如果你使用下面的链接注册会员，这对我和这个平台上的其他作者来说意义重大。它鼓励我们继续推出像这样的高质量和信息丰富的内容——提前感谢您！</p><div class="ob oc gp gr od oe"><a href="https://chongjason.medium.com/membership" rel="noopener follow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">通过我的推荐链接-杰森·庄加入媒体</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">chongjason.medium.com</p></div></div><div class="on l"><div class="oo l op oq or on os ks oe"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9b98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不知道接下来要读什么？这里有一些建议。</p><div class="ob oc gp gr od oe"><a rel="noopener follow" target="_blank" href="/battle-of-the-ensemble-random-forest-vs-gradient-boosting-6fbfed14cb7"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">整体之战——随机森林vs梯度推进</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">机器学习领域最流行的两种算法，谁会赢？</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="ot l op oq or on os ks oe"/></div></div></a></div><div class="ob oc gp gr od oe"><a rel="noopener follow" target="_blank" href="/what-does-the-career-progression-look-like-for-a-data-scientist-70bdd27f3fab"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">数据科学家的职业发展是什么样的？</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">理解大公司中初级与高级数据科学家角色的指南</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="ou l op oq or on os ks oe"/></div></div></a></div><div class="ob oc gp gr od oe"><a rel="noopener follow" target="_blank" href="/feature-selection-dimensionality-reduction-techniques-to-improve-model-accuracy-d9cb3e008624"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">提高模型精度的特征选择和降维技术</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">选择任何数据集中最重要要素的综合指南</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="ov l op oq or on os ks oe"/></div></div></a></div></div></div>    
</body>
</html>