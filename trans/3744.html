<html>
<head>
<title>What Is CLIP and Why Is It Becoming Viral?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是CLIP，为什么它会成为病毒？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-clip-and-why-is-it-becoming-viral-d3ba20c0068#2022-08-20">https://towardsdatascience.com/what-is-clip-and-why-is-it-becoming-viral-d3ba20c0068#2022-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5e37" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当神经网络使用如此多的数据时，它就变得“通用”了</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6b31e8df70d8e3ee1c1adb69563c5076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxBRUnHoGIwt6jAHc4-lqw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。数十亿张图片存储在互联网上的云端。将它们用于机器学习可能会非常有帮助。从<a class="ae ky" href="https://unsplash.com/photos/M5tzZtFCOfs" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/M5tzZtFCOfs</a>检索的图像。</p></figure><p id="3598" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预定义的类和类别:这是新的类在重新训练后只能由机器学习和神经网络进行分类的限制。一段时间以来，这种再培训和微调程序几乎已经成为“标准”——这是一种如此普遍的做法，以至于人们忘记了这仍然是一个有待解决的问题……至少在引入CLIP之前是如此。</p><h1 id="61e3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">那么，到底什么是剪辑？</h1><p id="4663" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">CLIP(对比语言-图像预训练)是一种不同于视觉社区中常见实践的训练过程。在一段时间内，模型/训练方法的能力在跨越1000个类的ImageNet数据集上进行了基准测试。我们在ImageNet的子集上进行训练，并在不同的子集上进行测试，以测量模型的泛化能力。虽然简单明了，但这种约定忽略了互联网上的指数级图像集合及其可能带来的潜在好处；剪辑，确实表明，这是我们错过了很多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e0430a033e9ceeed2d4f2431363723a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tz_yyNvvna59tYDoqD4CUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。剪辑预训练方法。图片来自https://arxiv.org/abs/2103.00020<a class="ae ky" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank"/>。</p></figure><p id="2702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了网上检索的大量图像，CLIP还利用了文本，这在它提出的时候是一种有点非正统的设置。与使用图像编码器(例如，CNN)和分类器(例如，全连接网络)的传统模型相反，CLIP联合训练图像编码器和文本编码器，以促进通过对比学习形成一对的编码器之间的紧密嵌入空间，如图2所示。如果你对对比学习不熟悉，请参考这篇文章<a class="ae ky" rel="noopener" target="_blank" href="/contrastive-learning-in-3-minutes-89d9a7db5a28">这里</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/c2ec50b560d67e527c98f6eba681e3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUsCzZB8TOW9Lx5eBLwNkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。剪辑分类管道。从图1中检索的图像。剪辑预训练方法。从<a class="ae ky" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.00020</a>检索的图像。</p></figure><p id="b60a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成功的训练将导致文本和图像指向非常接近的同一物体。例如，句子“一张小狗的照片”的嵌入将在实际的狗图像附近。在对一组有限的类进行推理时(图3。)，我们可以简单的把类名放到一个句子里，找到最接近查询图像的文本嵌入。</p><p id="7446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为CLIP是在极其庞大的4亿图文对上训练的，所以几乎成了“零拍”。你能想到的任何类几乎肯定是这个庞大数据集的一部分；你不需要任何再培训，任何课程，任何额外的数据。</p><h1 id="6b3b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">我们如何使用剪辑？</h1><p id="3cbf" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">尽管它很简单，但我们大多数人都无法访问硬件来处理4亿个图像-文本数据集，也无法访问数据集本身。但是由于其零触发能力，预训练检查点对于所有后续分类任务是通用的。<a class="ae ky" href="https://openai.com" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>在他们的GitHub上公布了训练和检查站的代码:</p><div class="mv mw gp gr mx my"><a href="https://github.com/openai/CLIP" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">GitHub - openai/CLIP:对比语言图像预处理</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">CLIP(对比语言-图像预训练)是在各种(图像、文本)对上训练的神经网络。它可以…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">github.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ks my"/></div></div></a></div><p id="973c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需遵循安装程序并运行<em class="mu">图像编码器</em>和<em class="mu">文本编码器</em>即可获得它们各自的功能。</p><h1 id="744a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">超越简单的分类</h1><p id="2a2b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">图像之外的许多数据集也包含文本标签。因此，文本和图像的精确匹配可以用于其他领域。下面我们列出了CLIP的几个有趣的扩展:</p><h2 id="da6e" class="nn lw it bd lx no np dn mb nq nr dp mf li ns nt mh lm nu nv mj lq nw nx ml ny bi translated">音响化</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/82a55716b54201f68df330d2a21bc028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ih1PwClSga61Kefk0t7kg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。将结果声音化。从<a class="ae ky" href="https://arxiv.org/abs/2112.09726" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2112.09726</a>T12】中检索到的图像。</p></figure><p id="c08d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Lin等人试图通过CLIP将最接近的声音标签与视频本身进行匹配，来缓解视频编辑过程中在视频上叠加声音所耗费的时间。有关该方法的详细信息，请参见此处的<a class="ae ky" href="https://chuanenlin.com/papers/soundify-neurips2021.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="7b55" class="nn lw it bd lx no np dn mb nq nr dp mf li ns nt mh lm nu nv mj lq nw nx ml ny bi translated">夹子锻造</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/c2414b8732a959ef6c86a0f28544a715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vZne4e40ZGdUjqSQBfJa0A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。剪辑伪造结果。从<a class="ae ky" href="https://arxiv.org/abs/2110.02624" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2110.02624</a>检索到的图像。</p></figure><p id="f428" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CLIP-Forge将文本与形状渲染进行匹配，以使用未标记的3D对象数据集实现零射击3D形状生成。有关该方法的详细信息，请参见此处的<a class="ae ky" href="https://arxiv.org/abs/2110.02624" rel="noopener ugc nofollow" target="_blank">和</a>。</p><h1 id="252a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结束注释</h1><p id="88f3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">CLIP是vision社区中的一股清新之风，它鼓励我们重新思考互联网上越来越多的可用资源。毫无疑问，在不久的将来，更多新奇有趣的应用将会从这个优雅的想法中衍生出来。</p><p id="bd59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mu">感谢您坚持到现在</em>🙏！<em class="mu">我不断写关于计算机视觉和深度学习的文章，所以</em> <a class="ae ky" href="https://taying-cheng.medium.com/membership" rel="noopener"> <em class="mu">订阅</em> </a> <em class="mu">到Medium通过我了解更多:)</em></p></div></div>    
</body>
</html>