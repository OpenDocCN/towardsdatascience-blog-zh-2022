<html>
<head>
<title>Learning product similarity in e-commerce using a supervised approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用监督方法学习电子商务中的产品相似性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-product-similarity-in-e-commerce-using-a-supervised-approach-525d734afd99#2022-04-27">https://towardsdatascience.com/learning-product-similarity-in-e-commerce-using-a-supervised-approach-525d734afd99#2022-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="718f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用深度学习寻找相似产品的实用解决方案。以产品为中心的方法。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/08cc6a1fdcf2bb0bebfe453d85d32432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1g2R0p9ML0EYkTMMw6Zl2Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯科特·韦伯在<a class="ae kv" href="https://unsplash.com/s/photos/deep-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="08eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> T </span>每个电子商务的主要组成部分是产品和与产品互动的消费者。在电子商务处理的许多用例中，有一些是以消费者为中心的，它们试图通过分析购买或查看产品的历史来了解消费者的行为，并向具有类似购买行为的消费者推荐类似的产品，以及以产品为中心的，如新产品的价格估计、需求预测、找到类似的产品等。本文的范围是研究以产品为中心的方法来学习产品相似性，目标是获得每个产品的低维向量表示。有了这样的数字表示，我们就可以应用某种相似性度量，如余弦相似性，并在不同产品之间建立相似性。我展示了两种构建低维产品表示的实用方法，首先使用PyTorch训练回归深度学习模型，然后通过提取包含产品标题的每个词的嵌入权重或提取包含所有产品属性的完整向量来构建这样的表示。每种方法都有优点和缺点，本文也将讨论这些优点和缺点。</p><p id="f91f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">电商公司发布的文章数量显示，构建产品相似度解决方案确实是一项非常重要的工作:<a class="ae kv" href="https://medium.com/gobeyond-ai/product-matching-via-machine-learning-abstract-8b5de637114b" rel="noopener"> Price2Spy </a>，<a class="ae kv" rel="noopener" target="_blank" href="/unravelling-product-matching-with-ai-1a6ef7bd8614"> EDITED </a>，<a class="ae kv" href="https://medium.com/walmartglobaltech/product-matching-in-ecommerce-4f19b6aebaca" rel="noopener"> Walmart </a>，<a class="ae kv" href="https://tech.webinterpret.com/approach-and-techniques-used-to-build-a-product-similarity-catalog/" rel="noopener ugc nofollow" target="_blank"> Webinterpret </a>。一些科学论文也讨论了这个主题:</p><ul class=""><li id="94c6" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><a class="ae kv" href="http://ceur-ws.org/Vol-1378/AMW_2015_paper_10.pdf" rel="noopener ugc nofollow" target="_blank">根据属性在电子商务网站中查找相似产品</a></li><li id="37a7" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><a class="ae kv" href="http://www.heikopaulheim.com/docs/swj2018_product_data.pdf" rel="noopener ugc nofollow" target="_blank">产品匹配和分类的机器学习方法</a></li><li id="be98" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><a class="ae kv" href="https://aclanthology.org/2020.ecomnlp-1.7.pdf" rel="noopener ugc nofollow" target="_blank">基于BERT的产品匹配相似性学习</a></li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="8f8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">产品之间相似性的定义是领域和用例特定的，例如:给定两只阿迪达斯的鞋子。在0-1的范围内，相似度是多少？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/096a646c0ac6b30d5e4d84767851636c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0L8HSCvFYKNK2LAyC3Fp0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">截图自Amazon.com</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/85ae89bf6b8bef4cad2c7725f23f5be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q3DvPrNojU-lcPhRdYhIRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">截图自Amazon.com</p></figure><p id="f453" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">答案是:视情况而定。如果只比较产品名称，产品完全相似。如果我们引入颜色属性，相似性仍然很高，但不是1，可能是0.95或0.9，我们不知道确切的数字。比较同色不同码的鞋子怎么样？我们看到价格现在发生了变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/2a2613c4133000bbf1742a9743227813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3wO5vVyzYalxgJ7ixjpzg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">截图自Amazon.com</p></figure><p id="560d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个例子表明相似性取决于所选择的产品属性。基于产品属性估计产品相似度的可能解决方案有哪些？</p><ul class=""><li id="d355" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">我们可以训练一个关于标题的<a class="ae kv" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> word2vec </a>或<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>模型，或者使用预训练的嵌入来学习包含标题的单词的矢量表示，或者使用<a class="ae kv" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> doc2vec </a>模型学习完整标题的矢量表示——然而，这些模型可能无法很好地表示简短的产品标题、拼写错误的标题或非常专业的单词、特定领域的缩写，或者用英语以外的语言编写的标题。</li><li id="0224" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我们可以应用产品的图像相似性，并将它们与标题的矢量表示相结合。但是产品的图像可能丢失或者质量不足以训练图像相似性模型。</li><li id="9fa5" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">我们可以对由不同属性、图像嵌入和标题嵌入构建的产品表示进行聚类——缺点包括可伸缩性问题、难以建立最佳数量的聚类，以及需要在一个聚类内执行额外的相似性比较。</li><li id="bafb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">最后但同样重要的是，我们可以使用基于深度学习的监督方法(分类或回归),其中在损失最小化期间学习产品向量表示。我们必须回答的唯一问题是我们的最小化任务是什么。其中一个候选可以是价格估计或需求预测。在这种情况下，模型可以了解哪些产品在价格方面彼此更相似(在价格估计的情况下)，或者哪些产品在销售方面彼此更相似(在需求预测的情况下)。这种方法的优点很多:( 1)它不需要对包括产品标题和其他文本信息在内的产品属性进行特殊处理和复杂的预处理;( 2)它解决了主监督任务并并行生成产品向量表示;( 3)然后可以通过更新关于类似产品的知识来改进主监督任务。</li></ul></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="36ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我遵循监督方法，并在来自<a class="ae kv" href="https://www.kaggle.com/c/mercari-price-suggestion-challenge" rel="noopener ugc nofollow" target="_blank"> Kaggle Mercari价格建议挑战赛</a>的数据上进行演示，该挑战赛的任务是自动向在线卖家建议产品价格:</p><blockquote class="mz na nb"><p id="9d2b" class="kw kx nc ky b kz la jr lb lc ld ju le nd lg lh li ne lk ll lm nf lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.mercari.com/" rel="noopener ugc nofollow" target="_blank">日本最大的社区购物应用Mercari </a>，想要为卖家提供定价建议，但这很难，因为他们的卖家可以在Mercari的市场上出售任何东西，或任何一捆东西。</p></blockquote><p id="d76b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Mercari数据集包含大约150万种产品，并包含以下各列:</p><ul class=""><li id="5134" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir"> <em class="nc">名称</em> </strong> —卖方提供的产品名称。</li><li id="c1ea" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"><em class="nc">item _ condition _ id</em></strong>—卖方提供的物品的五个条件(1，2…5)</li><li id="1f4f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"><em class="nc">category _ name</em></strong>—每个产品的类别列表的三个级别，如男士/上衣/t恤、女士/珠宝/项链</li><li id="fe68" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> <em class="nc"> brand_name </em> </strong> —每个产品可能所属的对应品牌</li><li id="4de2" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> <em class="nc">运费</em> </strong> — 1，如果运费由卖家支付，则为0，否则为</li><li id="2b71" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"><em class="nc">item _ description</em></strong>—产品描述(卖方可自由提供任何描述)</li><li id="9ed0" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> <em class="nc">价格</em> </strong> —目标变量，以美元表示</li></ul><p id="6316" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们可以看到的，Mercari数据包含所有类型的特征:数字、分类和文本，它们通常出现在电子商务数据中。我们在这里也有几个挑战:由于卖家可以自由提供关于产品的任何信息，产品描述可能完全是空的或拼错的。产品名称也可以用错别字书写，如下例所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/ca985ce765759a7c28ae6c9ef6bd8fde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZRVyH3KDEFNlhgNzBO10w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b596" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习解决方案具有以下架构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/0c0cfaa87700f624e832698882801cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRkhV7Ac0Hl3QcPwcgD9vQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9d11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输入由各种特征组成:数字、分类和文本。以PoC 形式提供的代码在编写时考虑到了最大的灵活性。它允许定义属于上述三种类型之一的任何特征集。分类输入通过嵌入层转换到低维空间。文本输入(产品标题、产品描述)首先被令牌化为文字和字符，并通过嵌入层转换到低维空间。在字符级别上学习产品标题和描述可以增加拼写错误的产品或在文本输入中有轻微差异的产品的匹配。GRU/LSTM层返回序列中最后一个单词或字符的隐藏状态。最后，所有图层输出被连接成一个密集图层，并且可以在顶层定义附加的密集或跳过连接图层。绿色突出显示的两个层在我们的下游任务中起着重要作用。</p><p id="4921" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型定型后，有两种方法可以提取产品表示:</p><p id="df56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">单词嵌入层</strong>包含每个单词的嵌入。我们提取属于产品标题的每个单词表示，并对所有单词向量进行平均。得到的平均向量是产品名称的数字表示。构建了所有产品的向量表示后，我们可以应用类似余弦相似性的相似性度量来比较产品之间的相似程度。这种方法的优点是，在我们提取单词嵌入后，可以丢弃训练好的模型。这种方法的缺点是在训练中没有看到的标题词不会被表示出来。因此，没有代表性的产品标题的相似性是不可能测量的。另一个不利方面是对名称几乎相似但属于不同类别的产品的敏感性。由于产品名称是由单词嵌入层构成的，所以一个单词只有一种表示。</p><p id="aad4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch示例:假设我们已经将产品标题符号化为单词。字典中有10个单词，我们的单词向量表示有4个维度。</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="e8ed" class="nn no iq nj b gy np nq l nr ns">import torch<br/>import torch.nn as nn<br/>from sklearn.metrics.pairwise import cosine_similarity</span><span id="7f1f" class="nn no iq nj b gy nt nq l nr ns">#size of the dictionary + 1 out of vocabulary token<br/>word_embedding = <strong class="nj ir">nn.Embedding</strong>(10 + 1, 4, padding_idx = 0)</span><span id="2444" class="nn no iq nj b gy nt nq l nr ns">#here we should have trained our model<br/>#extract word representations</span><span id="5f23" class="nn no iq nj b gy nt nq l nr ns">#there are 4 products each having 3 words indexed by the vocabulary<br/>#index 0 reserved for any out of vocabulary token</span><span id="fd90" class="nn no iq nj b gy nt nq l nr ns">product_words = torch.LongTensor([[1, 2, 3], <br/>                                  [3, 9, 8], <br/>                                  [4, 5, 7], <br/>                                  [1, 2, 4]<br/>                                 ])<br/>#average words per product<br/>title_vectors = word_embedding(product_words).mean(axis = 1).detach().numpy()</span><span id="f49b" class="nn no iq nj b gy nt nq l nr ns">#compare similarity of the first product to all products<br/><strong class="nj ir">cosine_similarity</strong>(title_vectors[0].reshape(1,-1), title_vectors)</span></pre><p id="5cab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">串联层</strong>是第一个密集层，它将各个数值、分类和文本层的输出串联起来。我们可以将这一层视为由所有类型的输入组成的产品表示。这样做的好处是，如果测试产品由单词嵌入层中不存在的单词组成，仍然有可能使用其他产品属性找到相似性。这种方法需要改变正向传播函数。我们返回最后一个输出图层和连接图层的结果，而不是返回回归结果(最后一个输出图层)。通过在训练和测试产品上应用该模型来执行单词表示的提取。</p><p id="0ca6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch示例:假设我们已经将产品标题符号化为单词。字典中有10个单词，我们的单词向量表示有4个维度。此外，我们有一个分类输入，它有4个唯一值，我们希望将它们表示为一个三维嵌入向量。</p><p id="20fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们通过使用嵌入层和LSTM层来处理产品标题。</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="949e" class="nn no iq nj b gy np nq l nr ns">import torch<br/>import torch.nn as nn</span><span id="33af" class="nn no iq nj b gy nt nq l nr ns"><strong class="nj ir">EMBEDDING_DIM </strong>= 4</span><span id="4322" class="nn no iq nj b gy nt nq l nr ns">#size of the dictionary + 1 out of vocabulary token<br/>word_embedding = <strong class="nj ir">nn.Embedding</strong>(10+1, EMBEDDING_DIM, padding_idx = 0)</span><span id="f4fe" class="nn no iq nj b gy nt nq l nr ns">#do we want to process text from left to right only or in both directions?<br/><strong class="nj ir">BIDIRECTIONAL </strong>= True<br/><strong class="nj ir">HIDDEN_SIZE </strong>= 5<br/><strong class="nj ir">NUM_LAYERS </strong>= 2<br/>lstm = <strong class="nj ir">nn.LSTM</strong>(input_size=<strong class="nj ir">EMBEDDING_DIM</strong>, hidden_size=<strong class="nj ir">HIDDEN_SIZE</strong>, batch_first=True, num_layers = <strong class="nj ir">NUM_LAYERS</strong>, bidirectional = <strong class="nj ir">BIDIRECTIONAL</strong>)</span><span id="83b3" class="nn no iq nj b gy nt nq l nr ns">product_words = torch.LongTensor([[1, 2, 3], <br/>                                  [3, 9, 8]<br/>                                 ])</span><span id="dafb" class="nn no iq nj b gy nt nq l nr ns">#apply lstm on embedded words<br/>packed_output, (<strong class="nj ir">hidden_state</strong>, cell_state) = <strong class="nj ir">lstm(word_embedding(product_words))</strong></span><span id="3735" class="nn no iq nj b gy nt nq l nr ns">batch_size = hidden_state.shape[1]</span><span id="407e" class="nn no iq nj b gy nt nq l nr ns">#extract last step from the hidden state<br/>last_step = hidden_state.view(<strong class="nj ir">NUM_LAYERS</strong>, 2 if <strong class="nj ir">BIDIRECTIONAL </strong>else 1, batch_size, -1)<br/>last_step = last_step[num_layers-1]<br/>#reorder batch to come first<br/>last_step = last_step.permute(1, 0, 2)<br/>#flatten dimensions (B, HIDDEN_SIZE * 2 if BIDIRECTIONAL else 1)</span><span id="9123" class="nn no iq nj b gy nt nq l nr ns">last_step = last_step.reshape(batch_size, -1)<br/>last_step.shape</span></pre><p id="63d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二，我们通过嵌入层来处理分类输入</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="8b80" class="nn no iq nj b gy np nq l nr ns"><strong class="nj ir">EMBEDDING_DIM </strong>= 3</span><span id="f804" class="nn no iq nj b gy nt nq l nr ns">cat_embedding = <strong class="nj ir">nn.Embedding</strong>(4 + 1, <strong class="nj ir">EMBEDDING_DIM</strong>, padding_idx = 0)</span><span id="abe3" class="nn no iq nj b gy nt nq l nr ns">#indices of categorical values<br/>batch_categories = torch.LongTensor([[1],<br/>                                     [3]<br/>                                 ])</span><span id="45e5" class="nn no iq nj b gy nt nq l nr ns">batch_size = batch_categories.shape[0]<br/>cat_result = <strong class="nj ir">cat_embedding(batch_categories)</strong><br/>cat_result = cat_result.reshape(batch_size, -1)<br/>cat_result.shape</span></pre><p id="74e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们连接两个向量，形成我们的乘积表示向量:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="f19c" class="nn no iq nj b gy np nq l nr ns">#tensor dimensionality [batch_size, last_step.shape[1] + cat_result.shape[1]</span><span id="0fd4" class="nn no iq nj b gy nt nq l nr ns">#this is our product representation<br/>concatenated_layer = <strong class="nj ir">torch.cat</strong>([last_step, cat_result], dim = 1)<br/>concatenated_layer.shape</span></pre><h2 id="99e9" class="nn no iq bd nu nv nw dn nx ny nz dp oa lf ob oc od lj oe of og ln oh oi oj ok bi translated">关于Kaggle Mercari数据的培训</h2><p id="c4e1" class="pw-post-body-paragraph kw kx iq ky b kz ol jr lb lc om ju le lf on lh li lj oo ll lm ln op lp lq lr ij bi translated">我从下面的<a class="ae kv" href="https://www.kaggle.com/code/tunguz/wordbatch-ftrl-fm-lgb-lbl-0-42506" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本</a>中使用了一些方法来应用数据清理和特征工程。</p><p id="9163" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在模型中使用了以下数字特征:</p><ul class=""><li id="2f88" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">运费</strong> — 1，运费由卖家支付，否则为0</li><li id="f4cc" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">desc _伦</strong> —产品描述的字数</li><li id="832d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> name_len </strong> —产品标题的字数</li><li id="359f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> is_brand_missing </strong> —如果缺少品牌信息，则为1，否则为0</li><li id="c4a4" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">is _ item _ description _ missing</strong>—如果缺少产品描述，则为1，否则为0</li></ul><p id="aa5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">分类特征:</p><ul class=""><li id="85fb" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir"> item_condition_id </strong> —卖方提供的项目的五个条件(1，2…5)</li><li id="e557" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> brand_name </strong> —每个产品可能所属的相应品牌</li><li id="a825" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">子类别_1 </strong>、<strong class="ky ir">子类别_2 </strong>、<strong class="ky ir">子类别_3 </strong> —每种产品的三级类别列表</li></ul><p id="b833" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文本特征(单词和字符级别)</p><ul class=""><li id="8461" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">名称</strong> —产品名称</li><li id="7f2d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">项目_描述</strong> —产品描述</li></ul><p id="90aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目标变量<strong class="ky ir">价格</strong>首先进行对数转换，然后使用scikit-learn<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html" rel="noopener ugc nofollow" target="_blank">power transformer</a>进行转换</p><p id="bd79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将Kaggle提供的<a class="ae kv" href="https://www.kaggle.com/competitions/mercari-price-suggestion-challenge/data?select=train.tsv.7z" rel="noopener ugc nofollow" target="_blank">训练集</a>拆分为训练和验证，分配20%的产品进行验证。最后，我对1，185，328个产品进行了10个时期的模型训练，并提前停止</p><p id="bba5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型参数总结如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/cbe35a9ece8dc11c7737ab3a6f2dbed7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*GvAPHNKVTjMHf9eGarYAjQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由单词嵌入层构造的产品表示向量的维数为50(<em class="nc">text _ Embedding _ dimension</em>)。从级联层构建的乘积表示向量具有787的维度，并且计算如下:</p><ul class=""><li id="36cc" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">具有4，178，6，23，71 → 282的不同嵌入维数的5个分类特征(<em class="nc">分类嵌入大小</em></li><li id="cadd" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">5个数字特征(<em class="nc">数字输入_大小</em>)</li><li id="be5a" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">2基于单词的<strong class="ky ir">双向</strong>文本特征(<em class="nc">text _ recurrent _ hidden _ size</em>):<br/>2 *(100 * 2)→400</li><li id="5f50" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">2基于字符的<strong class="ky ir">单向</strong>文本特征(<em class="nc">char _ recurrent _ hidden _ size</em>):2 * 50→100</li></ul><h2 id="e03c" class="nn no iq bd nu nv nw dn nx ny nz dp oa lf ob oc od lj oe of og ln oh oi oj ok bi translated">结果</h2><p id="2e1c" class="pw-post-body-paragraph kw kx iq ky b kz ol jr lb lc om ju le lf on lh li lj oo ll lm ln op lp lq lr ij bi translated"><strong class="ky ir">第一次比较:不存在于训练集中的产品</strong></p><p id="0957" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该产品为“<strong class="ky ir">speacker”</strong>，卖方将其拼错。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/abfc83882666a6c38cffc0700dc8b5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3D7XHTo8ls6pqaSMK1jeQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，单词嵌入方法不起作用，因为模型没有将这个单词映射到词汇表中。级联层方法是可行的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/f2e0abb3a3073e88293af687c4cb7717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s_oszWISCb2hJtacGXnaCQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e718" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">前4个最相似的产品是与被搜索产品具有可比价格的扬声器。类别、品牌和运输具有相同的值。我猜第一个产品的相似度更高，因为它的产品描述比其他产品更短。如果像产品描述长度这样的特征在相似性比较过程中不太重要，我们可以创建两个连接层:一个用于相似性比较，另一个用于保存对建模重要但对下游任务中的相似性比较不重要的特征的输入。</p><p id="555d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第二次比较:<em class="nc">快递波托菲诺上衣m码</em>T24】</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/a5d0ba7febe3afbf2e9a3a05337fee8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZBYUftSs9vm6E7KZeSVzQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="103c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word嵌入层的前5名同类产品:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/4fb1676eb115ffedbcada653b23457a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ca1MjCYjUuDiUjYuU8WVSQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="22b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最上面的产品与搜索的产品有相似的品牌。标题中也有三个常用词。</p><p id="3d21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">串联层的前5个类似产品:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/2d849de6efa3c073f609bd7134b23c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NVx0h_XpSPHHdFXY_vSrQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f2d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然顶级产品在被搜索产品的标题内具有不同数量的常用词，但是所有产品都属于同一品牌。而且这些商品的价格也和搜索到的商品价格相当。</p><p id="8661" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第三次对比:<em class="nc">iphone 5c 16gb att GSM</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/9ff113a8b2fefb9ef3ac95ba143e5464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B1JOMTzhNkV4s4kju0wpWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dbb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word嵌入层的前5名同类产品:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/95bcb3f0129ede41d5038130bcab002d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28dLOSOWhWAIxrLZqpTr2w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="225b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里的顶级相似产品在标题中有更多与搜索产品相同的词</p><p id="f905" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">串联层的前5个类似产品:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/9f3b57636fa6228a818d23a83ff12a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9pe8b-4666bZJi0ZxQFkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0e3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">搜索到的产品和第四个产品都提到了美国电话电报公司(att vs at&amp;t)。他们两个价格一样。很难决定第四个产品是否与第一个产品更相似，但相似性差异不大(0.808比0.805)</p><h2 id="3e98" class="nn no iq bd nu nv nw dn nx ny nz dp oa lf ob oc od lj oe of og ln oh oi oj ok bi translated">结论</h2><p id="e24a" class="pw-post-body-paragraph kw kx iq ky b kz ol jr lb lc om ju le lf on lh li lj oo ll lm ln op lp lq lr ij bi translated">我在本文中提到的几个例子表明，两种产品相似性的方法非常有效。单词嵌入层有一个缺点，那就是由训练时不知道的全新单词组成的产品标题将不可能进行比较。另一方面，一旦从单词表示中重建了产品标题，就可以丢弃训练好的模型。在连接层方法中，需要模型来预测新产品和提取产品表示。无论采用何种方法，都需要对模型参数和领域专业知识进行适当的调整。在级联层方法中，需要决定哪些特性对于产品表示是重要的。附加的文本预处理或/和包含图像嵌入也可以提高相似性。</p><p id="53a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所提供的代码非常灵活，可以适应许多具有数值、分类和文本特征的表格数据集，用于回归任务。</p><p id="9898" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码可以从我的<a class="ae kv" href="https://github.com/slavakx/medium_posts/tree/master/product_similarity_pytorch" rel="noopener ugc nofollow" target="_blank"> Github repo </a>下载</p><p id="192d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>