<html>
<head>
<title>Differences between LDA, QDA and Gaussian Naive Bayes classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LDA、QDA和高斯朴素贝叶斯分类器的区别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6#2022-08-28">https://towardsdatascience.com/differences-of-lda-qda-and-gaussian-naive-bayes-classifiers-eaa4d1e999f6#2022-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4e3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">深入研究建模假设及其含义</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/5febd25d7b039e035c6e8cb7ad6aae88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGR-02OLEOWv_Wons8SG0w.png"/></div></div></figure><p id="9777" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在挖掘经典分类方法的细节时，我发现了关于高斯朴素贝叶斯(GNB)、线性判别分析(LDA)和二次判别分析(QDA)的异同的稀疏信息。这篇文章集中了我为下一个学习者找到的信息。</p><p id="f452" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">总结:所有这三种方法都是贝叶斯分类器的一个具体实例，它们都处理连续高斯预测器，它们在预测器之间和类之间关系的假设上有所不同(即，它们指定协方差矩阵的方式)。</p><h2 id="1556" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">贝叶斯分类器</h2><p id="49fe" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">我们有一组X个<strong class="ku ir"> <em class="mm"> p个</em> </strong>预测值，以及一个离散响应变量Y(类)，取值k = {1，…，K}，用于样本的<strong class="ku ir"> <em class="mm"> n个</em> </strong>观察值。</p><p id="b9de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们遇到了一个新的观察值，我们知道预测值X的值，但不知道Y类的值，所以我们想根据我们拥有的信息(我们的样本)对Y进行猜测。</p><p id="9433" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">贝叶斯分类器将测试观测值分配给具有最高条件概率的类，由下式给出:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mn"><img src="../Images/d27b7720c52a063dee5e2c410443ed56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YIvB-oYj_dV8g3rAxHMZ_w.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">贝叶斯定理</p></figure><p id="4778" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中:pi_k是先验估计，f_k (x)是我们的似然。为了获得类k的概率，我们需要定义先验和似然的公式。</p><p id="d7ed" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">先验。</strong>观察到类别k的概率，即我们的测试观察值属于类别k，但没有关于预测值的进一步信息。查看我们的示例，我们可以将类k中的情况视为具有二项式分布的随机变量的实现:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ms"><img src="../Images/0da3dc3a267efb3784589ecc3a9679a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3F9dTLJ2VytZBZEqv3U4ng.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">在一组试验次数n(样本量)中成功次数(k类中的观察值)的分布。</p></figure><p id="4cd8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其中，对于<strong class="ku ir"><em class="mm"/></strong>n次试验，在每次试验中，观察值属于(成功)或不属于(失败)类别k。可以看出，相对成功频率——试验总数中的成功次数— <strong class="ku ir"> <em class="mm"> </em> </strong>是pi_k的无偏估计量。因此，我们使用相对频率作为观察值属于类别k的概率的先验。</p><p id="da4d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">可能性:</strong>可能性是看到X的这些值的概率，假设观测值实际上属于类别k。因此，我们需要找到类别k中预测值X的分布。我们不知道“真实”的分布是什么，所以我们无法“找到”它，我们宁愿对它的样子做一些合理的假设，然后使用我们的样本来估计它的参数。</p><p id="aa1b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如何选择合理的配送？离散的和连续的预测值之间有一个明显的区别。这三种方法都假设在每个类中，</p><blockquote class="mt mu mv"><p id="b233" class="ks kt mm ku b kv kw jr kx ky kz ju la mw lc ld le mx lg lh li my lk ll lm ln ij bi translated">预测值具有高斯分布(p=1)或多元高斯分布(p&gt;1)。</p></blockquote><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mz"><img src="../Images/aa8aa31f6d87f079a062df553b74ab6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRFm_8mC8u672QIA39Itbg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">k类条件下多元高斯分布的一般形式。</p></figure><p id="1599" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因此，只有当我们有连续的预测器时，才能使用这些算法。事实上，高斯朴素贝叶斯是一般朴素贝叶斯的一个特例，具有高斯似然性，这就是为什么我在这篇文章中将它与LDA和QDA进行比较。</p><p id="a268" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">从现在开始，我们将考虑能够展示三种方法之间差异的最简单的情况:两个预测器(p=2)和两个类(K=2)。</p><h2 id="398c" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">线性判别分析</h2><blockquote class="mt mu mv"><p id="472d" class="ks kt mm ku b kv kw jr kx ky kz ju la mw lc ld le mx lg lh li my lk ll lm ln ij bi translated">LDA假设跨类的协方差矩阵是相同的。</p></blockquote><p id="575a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这意味着类别1和类别2中的预测值可能具有不同的均值，但是它们的方差和协方差是相同的。这意味着预测值之间的“分布”和关系在各个类别中是相同的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi na"><img src="../Images/05455b3cdd6514c767875294b07f1c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfDJSNaQYWFpQsya5N37Tg.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">基于LDA假设的类别分布可视化</p></figure><p id="89d8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">上面的图是从每种形式的分布中生成的:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nb"><img src="../Images/cfe1ddf67961ef6c241d203bfeac408f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BtoPB_V4rfqaFu92yn7BNw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">LDA第1类和第2类预测因子的分布</p></figure><p id="9800" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们观察到协方差矩阵是相同的。如果我们预期预测值之间的关系在不同类别之间不会发生变化，并且如果我们只是观察到分布均值的变化，那么这种假设是合理的。</p><h2 id="a4f3" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">二次判别分析</h2><p id="ead6" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">如果我们放松LDA的恒定协方差矩阵假设，我们有QDA。</p><blockquote class="mt mu mv"><p id="0435" class="ks kt mm ku b kv kw jr kx ky kz ju la mw lc ld le mx lg lh li my lk ll lm ln ij bi translated">QDA没有假设跨类的协方差矩阵不变。</p></blockquote><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi na"><img src="../Images/d3cbc05e7ed7cf3db3715031b54b6d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKzmWEJtFjBqaQkFYUIqag.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">基于QDA假设的类别分布可视化</p></figure><p id="fbda" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">上面的图是从每种形式的分布中生成的:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nb"><img src="../Images/93f4c2f0d4f8b7315243f17c07a8ccc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*anPQYOLfUb_XDtGkyDkSMw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">QDA第一类和第二类预测因子的分布</p></figure><p id="f0f4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们观察到这两个分布的所有参数都可以变化。如果我们预期不同类别的预测者之间的行为和关系非常不同，这是一个合理的假设。</p><p id="42ff" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这个例子中，甚至两个预测值之间的关系的方向也从类别1到类别2变化，从正协方差4到负协方差-3。</p><h2 id="26f0" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">高斯朴素贝叶斯</h2><p id="a48d" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">GNB是朴素贝叶斯的一个特例，其中预测值是连续的，并且正态分布在每个k类中。一般的朴素贝叶斯(因此，GNB也是)假设:</p><blockquote class="mt mu mv"><p id="198f" class="ks kt mm ku b kv kw jr kx ky kz ju la mw lc ld le mx lg lh li my lk ll lm ln ij bi translated">给定Y，预测因子X是条件独立的。</p></blockquote><p id="8134" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">独立意味着不相关，即协方差等于零。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi na"><img src="../Images/a6a4ebb03f42dbc7c4f93b362a66190b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D4dVuy3b0qZM7jA0fNbqkw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">基于GNB假设的类别分布可视化</p></figure><p id="3626" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">上面的图是从每种形式的分布中生成的:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nb"><img src="../Images/9b7ad4a4ca23d9c9037de1e26618343c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_xaLnnWpoWh8F-lPVohjA.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">GNB第一类和第二类预测因子的分布</p></figure><p id="e475" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">使用朴素贝叶斯，我们假设预测值之间没有关系。在实际问题中很少出现这种情况，然而，正如我们将在下一节中看到的那样，这大大简化了问题。</p><h2 id="be2c" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">假设的含义</h2><p id="829e" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">选择模型后，我们估计类内分布的参数，以确定我们的测试观察的可能性，并获得我们用来分类它的最终条件概率。</p><p id="5b0a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">不同的模型导致不同数量的参数被估计。提醒:我们有<strong class="ku ir"> <em class="mm"> p </em> </strong>预测器和<strong class="ku ir"> <em class="mm"> K </em> </strong>总类。对于所有的模型，我们需要估计预测值的高斯分布的平均值，这在每一类中都是不同的。这就产生了一个基数，<strong class="ku ir"><em class="mm">p</em></strong>*<strong class="ku ir"><em class="mm">K</em></strong>为所有方法估计的参数。</p><p id="de28" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">此外，如果我们选择LDA，我们将估计所有p个预测值的方差和每对预测值的协方差，从而得出</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nc"><img src="../Images/942cfaba82832b3534ca6104f341b380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*180nj0Fj8xaZZYburpKWgA.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">用LDA估计的参数数量</p></figure><p id="3fe8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">参数。这些是跨类的常数。</p><p id="4b78" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">对于QDA，由于它们在每一类中都不同，我们将LDA的参数数量乘以K，得到下面的估计参数数量的等式:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nc"><img src="../Images/c3a53dfa46df3c38df73a0c6fac61fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZm-4Hl0mluMRs9x375M5A.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">与QDA一起估计的参数数量</p></figure><p id="ab9d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">对于GNB，我们只有每一类中所有预测因子的方差:<strong class="ku ir"> p*K </strong>。</p><p id="e94b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">很容易看出对于p和/或K的大值使用GNB的优势。对于经常出现的二进制分类问题，即当K=2时，这是三种算法的模型复杂度如何随着p的增加而发展。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nd"><img src="../Images/ffaad9c32219a1cac1cb427674b08387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9PlNN2kTBXEoGYtenSRZg.png"/></div></div></figure><h2 id="7cf6" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">那又怎样？</h2><p id="bbf7" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">从建模的角度来看，当应用一种方法时，知道你正在处理的假设是很重要的。需要估计的参数越多，最终分类对样本变化越敏感。同时，如果参数的数量太少，我们将无法捕捉到类之间的重要差异。</p><p id="693b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">谢谢你的时间，我希望它是有趣的。</p><p id="04cf" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><em class="mm">除特别注明外，所有图片均为作者所有。</em></p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="1791" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">来源:</p><ul class=""><li id="968a" class="nl nm iq ku b kv kw ky kz lb nn lf no lj np ln nq nr ns nt bi translated">机器学习第三章——汤姆·m·米切尔，2017</li><li id="921f" class="nl nm iq ku b kv nu ky nv lb nw lf nx lj ny ln nq nr ns nt bi translated">《统计学习导论》第4章—James，Witten，Hastie，Tibshirani，2013</li></ul></div></div>    
</body>
</html>