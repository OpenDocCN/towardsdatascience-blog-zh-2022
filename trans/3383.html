<html>
<head>
<title>Transformers: An Overview of the Most Novel AI Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚:最新颖的人工智能架构概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-an-overview-of-the-most-novel-ai-architecture-cdd7961eef84#2022-07-27">https://towardsdatascience.com/transformers-an-overview-of-the-most-novel-ai-architecture-cdd7961eef84#2022-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="dcd0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习理论</h2><div class=""/><div class=""><h2 id="e161" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">负责Siri、Alexa和Google Home的模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/89560e8a17e5b4980d70ae136fe1ee3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAQrbFIV-G5cT3-OchMEHg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="ba95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi md translated">转换器彻底改变了自然语言处理、计算机视觉和图像生成领域。在这篇文章中，我将分解这个新颖的架构，并解释它是如何工作的。</p><h2 id="c98c" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">介绍</h2><p id="19fb" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">在过去的几年里，最新一代的大规模人工智能模型产生了令人印象深刻的结果。就像过去十年深度学习已经彻底改变了各种各样的行业一样，这种新一代的机器学习模型具有巨大的潜力。像GPT-3和DALL-E这样依靠变压器运行的模型，已经引发了新的产品、服务和业务，将为社会增加巨大的价值。</p><p id="4543" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">变压器是这些模型的组成部分。GPT-3，可以说是迄今为止最先进的自然语言处理(NLP)模型，能够以类似人类的方式完成各种各样的NLP任务。它的架构是一个标准的变压器，使它与众不同的是它前所未有的1750亿个参数。变压器的效率和简单性使得这种大型模型变得可行。在这篇文章中，我想讨论这些是如何工作的，以及为什么它们如此重要。</p><h2 id="45a9" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">是什么让变形金刚与众不同</h2><p id="451a" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">变压器是一种机器学习模型架构，像长短期记忆神经网络(LSTMs)和卷积神经网络(CNN)。这种新的架构有一些优点，使变压器成为最新的艺术模型的基础。</p><p id="8897" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我已经提到的，变形金刚可以非常巨大。更大的模型产生更好的结果，但是训练更昂贵。然而，现代变压器模型尺寸无法通过任何其他架构实现。变压器和其他架构之间的一个关键区别是变压器是高度并行化的，这使它们非常适合计算，并允许我们训练非常大的模型。</p><p id="5968" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让变形金刚与众不同的第二点是它们对注意力的巧妙利用。为了描述他们是如何做到这一点的，我将把重点放在介绍变形金刚的原始论文上:注意力是你所需要的一切。</p><h1 id="b42b" class="nj mn it bd mo nk nl nm mr nn no np mu ki nq kj mx kl nr km na ko ns kp nd nt bi translated">你需要的只是关注</h1><blockquote class="nu nv nw"><p id="a787" class="lh li nx lj b lk ll kd lm ln lo kg lp ny lr ls lt nz lv lw lx oa lz ma mb mc im bi translated">Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser、Illia Polosukhin于2017年6月提交的论文</p></blockquote><p id="d647" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">变形金刚由一个简单的架构组成，巧妙地利用了注意力。转换器由编码器和解码器组成。在编码器中，模型首先获取要翻译的句子，对其进行矢量化，并使用注意力对其进行转换。解码器反其道而行之，从矢量化转换到句子。例如，解码可以用不同的语言输出结果，使它们在翻译任务中更强大。</p><p id="1612" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我现在将描述注意力是如何工作的，然后是多头注意力是如何工作的，最后我将谈论一个变压器如何使用它们。</p><h2 id="4e7e" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">注意力</h2><p id="cecb" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">注意力是变形金刚的关键，也是为什么它们是如此强大的架构。注意力层非常有效，比其替代品的复杂性更低:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/53c0aa6b5bfa976bc0d89462bbd05bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iA0dippGAith4W2p8R_cdA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">表I:各种层的复杂度和最大路径长度(图片来自<a class="ae oc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> [1])</p></figure><p id="7447" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">自我注意比它的对应部分具有更低的复杂性和更短的路径长度。<em class="nx"> n </em>代表序列长度，<em class="nx"> d </em>为表示维数(<em class="nx"> d </em>往往大于<em class="nx"> n </em>，文中d取512)。</p><h2 id="f742" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">注意力层是如何工作的</h2><p id="a8da" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">注意层位于转换器的编码器和解码器部分。稍后，我将描述这些层在Transformer架构中的位置。</p><p id="12ba" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在到达注意层之前，输入文本首先被标记为代表性嵌入。从每一次嵌入，编码器将产生代表性的编码使用注意，和解码器将做相反的事情，输出文本。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8e411645eaf4d1f0ce8dfb7127d28535.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*b98-wvhYbmUhaq2jnX1gcw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="90eb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意力层的第一级<strong class="lj jd">由3个神经网络组成。这些将句子中嵌入的单词作为输入，对于每个单词，产生3个输出向量:<em class="nx">查询、键和值。</em>对句子中的每个单词都这样做，会产生<em class="nx"> n个</em>查询、键和值。</strong></p><p id="b957" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">第二阶段</strong>包括计算每个单词的分数。每个单词都会有<em class="nx"> n </em>个分数，然后巧妙组合。这允许模型考虑句子中的所有其他单词，以理解每个单词的意思。</p><p id="0e4c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">看一个句子中的第一个单词，它会被分配一个<em class="nx">查询</em>，一个<em class="nx">键</em>和一个<em class="nx">值</em>。我们要计算所有其他单词在第一个单词上的分数。为此，我们首先获取第一个单词的查询向量和句子中所有关键向量的点积。这里的点积可以被认为是相似性的度量。如果查询和键向量对齐，它们的点积将产生一个大值。如果查询和关键向量接近正交，点积的结果将接近零。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/660c3882a13ac2319608109472dffc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*mA3pLvWVxmXIBpGwrg3LNA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">单词1的得分权重(图片由作者提供)</p></figure><p id="6b9a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上图中，第一个单词的查询和句子中所有关键字之间的点积用于产生分数。</p><p id="0471" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">第三步</strong>是将SoftMax标准化并应用于这些值(使它们的总和等于1)</p><p id="ecd7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">简单回顾一下:注意力从3个神经网络开始，输出每个单词的<em class="nx">查询</em>、<em class="nx">键</em>和<em class="nx">值</em>。到目前为止，我们已经使用了<em class="nx">查询</em>和<em class="nx">键</em>向量来为每个单词产生一组权重，该权重对我们在查看特定单词时应该对每个单词给予多少关注进行排序。</p><p id="ae5d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">第四步</strong>是使用<em class="nx">值</em>向量，并使用在前面步骤中计算的分数缩放它们。最后，我们将得到的缩放值向量相加，给出每个单词的注意力向量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/3486b25606cd6d7d245aebbb7c97d806.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*8JzKx1t4K9kf8ICTyRqTRg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">单词1的注意力向量(图片由作者提供)</p></figure><p id="80d8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上图中，基于查询1的每个单词的分数用于衡量这些值。然后将这些相加以产生最终矢量。</p><p id="c4c5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个结果向量是查询1的结果注意力向量(来自第一个单词)。您将对每个查询都这样做，从而产生<em class="nx"> n </em>个向量，这些是注意力层的输出。注意，每个结果向量只取决于对那个单词的<em class="nx">查询</em>，还取决于句子中所有单词的<em class="nx">键</em>和<em class="nx">值</em>。这就是注意力在连续任务中的强大之处，因为你能够将整个输入的上下文嵌入到每个表示中。</p><p id="643c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我想展示论文中说明自我关注的图表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1f0dad6c7257c72846ed3e545fe12442.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*1FADWlKu4ltTL_1upKxtfg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae oc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a> [1]</p></figure><p id="1ef9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如您所见，<em class="nx">查询</em>和<em class="nx">键</em>用于产生分数，然后将分数应用于<em class="nx">值</em>向量以产生最终的输出向量。</p><h2 id="3dab" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">多头注意力</h2><p id="d7b3" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">到目前为止，我们已经看到了单头注意力层。多头注意力层是允许更快计算的单头注意力的扩展。</p><p id="db06" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于单头注意力，每个单词的嵌入向量有512维。在多头关注中，<em class="nx">查询</em>、<em class="nx">键</em>和<em class="nx">值</em>向量被分割成8个头(64维向量)，关注层被应用到每个头上的方式与我之前展示的方式相同。然后，将每个头部的结果注意力向量连接在一起，以形成512维注意力向量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/35c2729a6720120d7dd4563629cbf663.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*8CqD9UUILWO2w1MwvxvErQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae oc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a> [1]</p></figure><p id="56f2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">多头注意力仅仅是我之前详细描述的注意力层的扩展，你不需要完全理解它来理解注意力在更大的变形金刚环境中的作用。</p><h2 id="b02c" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">变形金刚</h2><p id="75d6" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">现在您已经了解了注意力层的样子，我将通过描述transformer架构来结束本文:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/da07f16177f11bb94c96f779219fcd45.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*SP4vE3KsGOTq3tLaKJbp4g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1:变压器模型架构(图片来自<a class="ae oc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> [1])</p></figure><p id="9810" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">转换器由编码器(左块)和解码器(右块)组成</p><p id="6a64" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">编码器:</strong>编码器是左边的那块。这个块由一堆<em class="nx"> N </em>个相同的层组成(在文中<em class="nx"> N </em> = 6)。每层包含一个多头注意力层，后面是一个全连接的前馈神经网络。每个编码器层的输出被用作后续编码器层的输入。所有层输出具有相同的维度(512，就像嵌入一样)。</p><p id="81d4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">解码器:</strong>解码器堆栈也由6个相同的层组成。每个解码层有2个多头注意力层，后面是一个前馈神经网络。在每个解码器层中，第一关注层的输入是来自前一解码器的输出。第二关注层的输入是来自编码器堆栈的输出。</p><h2 id="4198" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">培训和结果</h2><p id="78a7" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">变压器接受了英语到德语和英语到法语翻译任务的训练，并且在这两个问题上都达到了新的最先进水平。该模型使用8个GPU训练了数天。这些模型比以前发表的任何结果都取得了更好的结果，而训练成本只是其一小部分。</p><p id="dd2d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">结果是突破性的，从那以后，变压器被用于该行业生产的许多最先进的模型中。</p><h1 id="10b2" class="nj mn it bd mo nk nl nm mr nn no np mu ki nq kj mx kl nr km na ko ns kp nd nt bi translated">结论</h1><p id="98b1" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">在这篇文章中，我解释了变压器的工作原理。我拿起原始的Transformer文件(你所需要的只是注意力)并把它分解成简单易懂的步骤。变形金刚改变了自然语言处理任务的游戏。它与其他模型的不同之处在于它创造性地使用了注意力，并且它是高度并行的，这使得它的训练非常有效。在未来的文章中，我期待着讨论transformer如何集成到其他复杂的模型架构中，以及它们如何彻底改变图像和声音生成等行业。</p><h2 id="2457" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">支持我</h2><p id="9192" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">希望这对你有所帮助，如果你喜欢，你可以跟我来！T13】</p><p id="f529" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您也可以使用我的推荐链接成为<a class="ae oc" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lj jd">中级会员</strong> </a> <strong class="lj jd"> </strong>，并访问我的所有文章及更多:<a class="ae oc" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="71c2" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">你可能喜欢的其他文章</h2><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/cancer-prediction-vs-telecom-churn-modelling-9e4ec49095f6"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">癌症预测与电信流失建模</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">癌症预测和流失建模之间的相似性</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa lb om"/></div></div></a></div><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svms-important-derivations-4f50d1e3d4d2"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">支持向量机:重要的推导</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">SVM理论的全面阐释和形象化</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa lb om"/></div></div></a></div><h2 id="ab0c" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">参考</h2><p id="b9e5" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">[1]阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马、雅各布·乌兹科雷特、利永·琼斯、艾丹·戈麦斯、卢卡兹·凯泽、伊利亚·波洛苏欣，《注意力是你所需要的一切》，NIPS，2017年。可用:<a class="ae oc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a>。</p><h2 id="14d8" class="mm mn it bd mo mp mq dn mr ms mt dp mu lq mv mw mx lu my mz na ly nb nc nd iz bi translated">其他有用的链接</h2><p id="da73" class="pw-post-body-paragraph lh li it lj b lk ne kd lm ln nf kg lp lq ng ls lt lu nh lw lx ly ni ma mb mc im bi translated">变形金刚的惊人插图:</p><div class="oj ok gp gr ol om"><a href="http://jalammar.github.io/illustrated-transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jd gy z fp or fr fs os fu fw jc bi translated">图示的变压器</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">讨论:黑客新闻(65分，4条评论)，Reddit r/MachineLearning (29分，3条评论)翻译…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">jalammar.github.io</p></div></div><div class="ov l"><div class="pc l ox oy oz ov pa lb om"/></div></div></a></div><p id="5232" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">AI Epiphany的这个视频很棒:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pd pe l"/></div></figure></div></div>    
</body>
</html>