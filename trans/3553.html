<html>
<head>
<title>Understanding Ensemble Methods: Random Forest, AdaBoost, and Gradient Boosting in 10 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解集合方法:10分钟内随机森林、AdaBoost和梯度增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-ensemble-methods-random-forest-adaboost-and-gradient-boosting-in-10-minutes-ca5a1e305af2#2022-08-08">https://towardsdatascience.com/understanding-ensemble-methods-random-forest-adaboost-and-gradient-boosting-in-10-minutes-ca5a1e305af2#2022-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fcd4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="a262" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个数据，三个模型，详细的计算变得简单</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1711a2c117c5f1cdfc85242782fe2ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D9D42RJPsJ5t3meV"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">尼尔斯·维斯在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2ff9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/k-nearest-neighbors-naive-bayes-and-decision-tree-in-10-minutes-f8620b25e89b"> <strong class="lk jd"> <em class="me">决策树</em> </strong> </a>是一个简单且易于解释的模型。然而，它是不稳定的，因为数据中的微小变化可能导致生成完全不同的树。它倾向于过度拟合训练数据，因此具有较高的方差，并且对看不见的测试数据的概括较差。</p><p id="ebf1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个问题通过在<strong class="lk jd">集合</strong>中使用决策树得以缓解。与一只蚂蚁做不了多少事情相比，一大群蚂蚁可以创造建筑奇迹。那么，如何集成决策树呢？有两种方法:<strong class="lk jd">装袋</strong>和<strong class="lk jd">增压。</strong></p><blockquote class="mf mg mh"><p id="79be" class="li lj me lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">注意:因为我们将在这个故事中创建许多决策树，所以最好刷新一些关于决策树以及如何构建决策树的基本概念。这里有一个故事可能对你有所帮助:</p></blockquote><div class="ml mm gp gr mn mo"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbors-naive-bayes-and-decision-tree-in-10-minutes-f8620b25e89b"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd jd gy z fp mt fr fs mu fu fw jc bi translated">10分钟内k近邻、朴素贝叶斯和决策树</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">通过实践案例研究演练</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">towardsdatascience.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc lb mo"/></div></div></a></div><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="a8df" class="ni nj it ne b gy nk nl l nm nn"><strong class="ne jd">Table of Contents</strong></span><span id="903e" class="ni nj it ne b gy no nl l nm nn"><strong class="ne jd">· </strong><a class="ae lh" href="#e8d7" rel="noopener ugc nofollow"><strong class="ne jd">Random Forest</strong></a><br/>  ∘ <a class="ae lh" href="#f4b9" rel="noopener ugc nofollow">Step 1. Create a bootstrapped data</a><br/>  ∘ <a class="ae lh" href="#b0e6" rel="noopener ugc nofollow">Step 2. Build a Decision Tree</a><br/>  ∘ <a class="ae lh" href="#a2cf" rel="noopener ugc nofollow">Step 3. Repeat</a><br/>  ∘ <a class="ae lh" href="#ea95" rel="noopener ugc nofollow">Predict, aggregate, and evaluate</a><br/><strong class="ne jd">· </strong><a class="ae lh" href="#d98d" rel="noopener ugc nofollow"><strong class="ne jd">AdaBoost</strong></a><br/>  ∘ <a class="ae lh" href="#ca9f" rel="noopener ugc nofollow">Step 1. Initialize sample weight and build a Decision Stump</a><br/>  ∘ <a class="ae lh" href="#e7fc" rel="noopener ugc nofollow">Step 2. Calculate the amount of say for the Decision Stump</a><br/>  ∘ <a class="ae lh" href="#354b" rel="noopener ugc nofollow">Step 3. Update sample weight</a><br/>  ∘ <a class="ae lh" href="#4074" rel="noopener ugc nofollow">Step 4. Repeat</a><br/>  ∘ <a class="ae lh" href="#965b" rel="noopener ugc nofollow">Prediction</a><br/><strong class="ne jd">· </strong><a class="ae lh" href="#42e9" rel="noopener ugc nofollow"><strong class="ne jd">Gradient Boosting</strong></a><br/>  ∘ <a class="ae lh" href="#ffdc" rel="noopener ugc nofollow">Step 1. Initialize a root</a><br/>  ∘ <a class="ae lh" href="#b6f4" rel="noopener ugc nofollow">Step 2. Calculate the residual to fit a Decision Tree into</a><br/>  ∘ <a class="ae lh" href="#ed09" rel="noopener ugc nofollow">Step 3. Update log(odds)</a><br/>  ∘ <a class="ae lh" href="#8939" rel="noopener ugc nofollow">Step 4. Repeat</a><br/>  ∘ <a class="ae lh" href="#b2db" rel="noopener ugc nofollow">Prediction</a><br/><strong class="ne jd">· </strong><a class="ae lh" href="#2d8b" rel="noopener ugc nofollow"><strong class="ne jd">Conclusion</strong></a></span></pre></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="e8d7" class="nw nj it bd nx ny nz oa ob oc od oe of ki og kj oh kl oi km oj ko ok kp ol om bi translated">随机森林</h1><p id="8dc3" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">随机森林是最流行的装袋方法之一。你问的是什么？是<strong class="lk jd">自举+聚合</strong>的缩写。bagging的目标是减少单个估计量的方差，即随机森林情况下单个决策树的方差。</p><p id="2108" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具体来说，让我们在整个故事中使用一个虚拟数据集。假设您有以下逃税数据集。您的任务是根据应纳税收入(以千美元计)、婚姻状况以及是否实施退税等特征来预测一个人是否会遵从纳税(逃避列)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/b065b33fa3808afa9ee10a209f0f4295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDl2g0nYwM4W3SMpTXetug.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">本文使用的数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="6762" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随机森林由这三个步骤组成:</p><h2 id="f4b9" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第一步。创建引导数据</h2><p id="91be" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">给定一个具有<em class="me"> m </em>观测值和<em class="me"> n </em>特征的原始列车数据，随机抽取<em class="me"> m </em>观测值<strong class="lk jd">并重复</strong>。这里的关键词是“有重复”。这意味着某些观测值很可能被多次选取。</p><p id="883f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是从上面的原始训练数据引导的数据的例子。由于随机性，您可能会有不同的引导数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/00ad67978ef19f9e5b1c014096b09060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3hsNXWWfXff3sqPxYFeJA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>自举数据|图片</p></figure><h2 id="b0e6" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第二步。构建决策树</h2><p id="4115" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">决策树是这样构建的:</p><ol class=""><li id="ab05" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated">使用引导数据，以及</li><li id="74ae" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated">考虑每个节点的随机特征子集(考虑的特征数量通常是<em class="me"> n </em>的平方根)。</li></ol><h2 id="a2cf" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第三步。重复</h2><p id="68a3" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">步骤1和步骤2迭代多次。例如，使用六次迭代的逃税数据集，您将获得以下随机森林。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ps"><img src="../Images/969b78a0aec28431cdd600b60e2c7e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YpyIs3wQthFExv7y4oiZOA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用逃税数据集的具有六个决策树的随机森林。注意，决策树6仅由一个节点组成，因为自举数据对于所有十个观察值都有<strong class="bd pt">逃避=无</strong>目标|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="1124" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，随机森林不是确定性的，因为有随机选择的观察值和要素在起作用。这些随机选择就是为什么随机森林减少了决策树的方差。</p><h2 id="ea95" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">预测、汇总和评估</h2><p id="4238" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">若要使用随机森林进行预测，请使用测试数据遍历每个决策树。对于我们的例子，只有一个测试观察，六个决策树分别对“规避”目标给出预测<strong class="lk jd">否</strong>、<strong class="lk jd">否</strong>、<strong class="lk jd">否</strong>、<strong class="lk jd">是</strong>、<strong class="lk jd">是</strong>和<strong class="lk jd">否</strong>。从这六个预测中进行多数投票，以做出单个随机森林预测:<strong class="lk jd">否</strong>。</p><p id="451e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将决策树的预测组合成随机森林的单个预测称为<strong class="lk jd">聚合</strong>。虽然多数表决用于分类问题，但回归问题的聚合方法使用所有决策树预测的平均值或中值作为单个随机森林预测。</p><p id="5581" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自举数据的创建允许一些训练观察不被决策树的子集看到。这些观察结果被称为<strong class="lk jd">随机样本</strong>，对于评估随机森林的性能非常有用。要获得随机森林的验证准确性(或您选择的任何指标),请执行以下操作:</p><ol class=""><li id="a0c4" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated">在没有样本的情况下运行<em class="me">构建的所有决策树，</em></li><li id="7f5a" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated">汇总预测，以及</li><li id="d22e" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated">将聚合预测与真实标签进行比较。</li></ol></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="d98d" class="nw nj it bd nx ny nz oa ob oc od oe of ki og kj oh kl oi km oj ko ok kp ol om bi translated">adaboost算法</h1><p id="6e6d" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">与随机森林不同，AdaBoost(自适应增强)是一种增强集成方法，其中简单的<em class="me">决策树是按顺序构建的。有多简单？这些树只有一根根和两片叶子，简单到它们有自己的名字:<strong class="lk jd">决策树桩</strong>。随机森林和AdaBoost有两个主要区别:</em></p><ol class=""><li id="7d57" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated">随机森林中的决策树对最终预测具有相同的贡献，而AdaBoost中的决策树桩具有不同的贡献，即一些树桩比其他树桩更有发言权。</li><li id="46df" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated">随机森林中的决策树是独立构建的，而AdaBoost中的每个决策树桩都是通过考虑前一个树桩的错误来做出的。</li></ol><p id="49f5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策树桩过于简单；它们比随机猜测稍微好一点，并且有很高的偏差。boosting的目标是减少单个估计器的偏差，即AdaBoost情况下的单个决策树桩的偏差。</p><p id="bd38" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想一下逃税数据集。您将使用AdaBoost根据三个特征来预测一个人是否会遵从纳税:应纳税收入、婚姻状况和退税。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/b065b33fa3808afa9ee10a209f0f4295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDl2g0nYwM4W3SMpTXetug.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">逃税数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="ca9f" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第一步。初始化样本权重并构建决策树桩</h2><p id="12bd" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">请记住，AdaBoost中的决策树桩是通过考虑前一个树桩的错误而形成的。为此，AdaBoost为每个训练观测值分配一个样本权重。当前树桩将较大的权重分配给错误分类的观察值，通知下一个树桩更加注意那些错误分类的观察值。</p><p id="84c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最初，因为还没有做出预测，所以所有的观察都被赋予相同的权重1/ <em class="me"> m </em>。然后，构建一个决策树桩，就像创建深度为1的决策树一样。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/ae62c086a75be6f885f1deb1dfc9e000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02XoXut-CHGXl2Xm1G3Yzg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">初始化样本权重并构建第一个决策残肢|图像由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="e7fc" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第二步。计算决策树桩的发言权</h2><p id="cf2f" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">这个决策残肢产生3个错误分类的观察。因为所有的观察值都有相同的样本重量，所以样本重量的总误差为1/10 + 1/10 + 1/10 = 0.3。</p><p id="b189" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后可以使用公式计算出的<strong class="lk jd">量</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/501e7aeac040349f5a8373111c631ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*OUbKNSNoQQnQ1ZFbcprBww.png"/></div></figure><p id="3bbf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了理解为什么这个公式有意义，让我们画出相对于总误差的量。注意，总误差仅定义在区间[0，1]上。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/5cdc2c04ec7e753333749ad52f7b16ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLyzvsYzhRKDJvAP0Gwxqw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">总误差|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="3731" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果总误差接近0，则say的量是一个大的正数，表明树桩对最终预测做出了许多贡献。另一方面，如果总误差接近1，则say的数量是一个很大的负数，这对于残肢产生的错误分类是一个很大的惩罚。如果总误差为0.5，树桩对最终预测没有任何作用。</p><h2 id="354b" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第三步。更新样品重量</h2><p id="76fa" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">AdaBoost将增加错误标记的观察值的权重，并减少正确标记的观察值的权重。新的样本权重影响下一个决策问题:权重较大的观察值更受关注。</p><p id="cd8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要更新样品重量，请使用以下公式</p><ul class=""><li id="773c" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md px pk pl pm bi translated">对于错误标记的观察值:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi py"><img src="../Images/32bd7d43595d915d0384e3363e99cbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*r2E1Sd8JMZTuF_oxSVTRMA.png"/></div></figure><ul class=""><li id="95a4" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md px pk pl pm bi translated">对于正确标记的观察结果:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/2cb983215cb21c3a13d1ba68ec42b1a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*80jZsptK2sHG1FRNZyi5yA.png"/></div></figure><p id="06f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在继续下一步之前，您需要使用以下公式对新的样本权重进行归一化，使它们的总和等于1</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/6b26d969e1144a139877c0a929756334.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*ikrhDrDt343no3X1mBFL6Q.png"/></div></figure><p id="48b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下表总结了这些计算。绿色观察值被正确标记，而红色观察值被当前树桩错误标记。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/d85562566a47593a2cbfcf195e324a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ax7gz7_itkkXuxoYTDWPeg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>更新第一个决策树桩|图像的样本权重</p></figure><h2 id="4074" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第四步。重复</h2><p id="6c79" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">使用<strong class="lk jd">加权</strong>杂质函数，即加权基尼指数，创建新的决策树桩，其中在确定每类观察值的比例时考虑归一化样本权重。或者，创建决策树桩的另一种方法是使用基于标准化样本权重的自举数据。</p><p id="99ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于归一化样本权重的总和为1，因此可以将其视为在自举时选取特定观察值的概率。因此，id为5、8和10的观察值更有可能被选中，因此决策树桩更关注它们。</p><p id="5c10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">引导数据如下所示。请注意，由于随机性，您可能会获得不同的引导数据。像以前一样建立一个决策树桩。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/bf980e51954cd44e720425194d63cc21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7fpOgC-IKnL2Np6UV7Vvg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">初始化样本权重并构建第二个决策残肢|图像作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="7a4c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你看到有一个错误分类的观察。所以，说的量是1.099。更新样本权重，并根据仅自举观测值的权重之和进行归一化。下表总结了这些计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/f4bb1aa3530b7433ddbe00264346d642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-bI_tYeOS97X3qC6-5hXQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a>更新第二个决策树桩|图像的样本权重</p></figure><p id="36bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">引导观测值返回到训练数据池，新的样本权重如下所示。然后，您可以使用第三个决策树桩的新样本权重来再次引导训练数据，该过程继续进行。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/b30f60858d4e0e7c3dcf0e44ae16c3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZiihsR9elcrvVRqcgK3-Sg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练数据及其样本权重|图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="965b" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">预言；预测；预告</h2><p id="00e2" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">假设你迭代六次这些步骤。因此，你有六个决策难题。要进行预测，请使用测试数据遍历每个树桩并对预测的类进行分组。发言权最大的一组代表AdaBoost的最终预测。</p><p id="a484" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">举例来说，您构建的前两个决策树桩的数量分别为0.424和1.099。两个树桩预测<strong class="lk jd">逃避=否</strong>因为对于应纳税所得额，77.5 &lt; 80 ≤ 97.5 (80是测试观察的应纳税所得额)。假设其他四个决策问题预测并有以下的发言权。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/ed64c2b78ae3c48065e5e00e0885070c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMsFHnpyJpqIN88aMzENVg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">决策树桩的预测和他们的发言权|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="38ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于预测<strong class="lk jd">逃避的所有树桩的发言权的总和=否</strong>是2.639，大于预测<strong class="lk jd">逃避的所有树桩的发言权的总和=是</strong>(也就是1.792)，那么最终的AdaBoost预测是<strong class="lk jd">逃避=否</strong>。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><h1 id="42e9" class="nw nj it bd nx ny nz oa ob oc od oe of ki og kj oh kl oi km oj ko ok kp ol om bi translated">梯度推进</h1><p id="fb21" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">由于两者都是增强方法，AdaBoost和梯度增强具有相似的工作流程。但是有两个主要的区别:</p><ol class=""><li id="1fc0" class="pe pf it lk b ll lm lo lp lr pg lv ph lz pi md pj pk pl pm bi translated">梯度增强使用比决策树桩大的树。在这个故事中，我们将树限制为最多有3个叶节点，这是一个可以随意更改的超参数。</li><li id="5595" class="pe pf it lk b ll pn lo po lr pp lv pq lz pr md pj pk pl pm bi translated">梯度提升不是使用样本权重来指导下一个决策树桩，而是使用决策树产生的残差来指导下一棵树。</li></ol><p id="9f5b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">回想一下逃税数据集。您将使用梯度推进基于三个特征来预测一个人是否会遵从纳税:应纳税收入、婚姻状况和退税。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/b065b33fa3808afa9ee10a209f0f4295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDl2g0nYwM4W3SMpTXetug.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">逃税数据集|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="ffdc" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第一步。初始化根目录</h2><p id="6b3c" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">根是深度为零的决策树。它不考虑任何特征来做预测:它使用回避本身来预测回避。它是怎么做到的？使用映射<strong class="lk jd">否→ 0 </strong>和<strong class="lk jd">是→ 1 </strong>对规避进行编码。然后，取平均值，就是0.3。这被称为<strong class="lk jd">预测概率</strong>(该值总是在0和1之间)，用<em class="me"> p </em>表示。</p><p id="2dae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于0.3小于阈值0.5，根将为每次列车观察预测<strong class="lk jd">否</strong>。换句话说，预测永远是规避的<em class="me">模式</em>。当然，这是一个非常糟糕的初步预测。为了提高性能，您可以使用下面的公式计算下一步要使用的<strong class="lk jd"> log(odds) </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/d1fa9d4493090f8bbfbf8c5e5e703e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*WD-81ZPZPFOKQfHkEpeZrQ.png"/></div></figure><h2 id="b6f4" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第二步。计算适合决策树的残差</h2><p id="8a90" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">为了简化表示法，让Evade的编码列表示为<em class="me"> y </em>。残差简单地定义为<em class="me">r</em>=<em class="me">y</em>-<em class="me">p</em>，其中<em class="me"> p </em>是使用其反函数从最新对数(赔率)计算的预测概率</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/e294d6343e1bb2b0a9b50bc5b0904f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*tJfYfYhD29llMNecUvXYqw.png"/></div></figure><p id="50cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用所有训练观察值构建一个决策树来预测残差。请注意，这是一个回归任务。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/10ae32b8af828408e07341ea7e10556f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J6vp8A3X2_lKYLWSYhJi2g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算残差并拟合决策树以预测残差|图像作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="ed09" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第三步。更新日志(赔率)</h2><p id="945e" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">注意，我还添加了一个节点Id列，它解释了决策树中预测每个观察的节点的索引。在上表中，每个观察都用颜色进行了编码，以便于看出哪个观察到了哪个节点。</p><p id="d397" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要组合初始根的预测和您刚刚构建的决策树，您需要转换决策树中的预测值，以便它们与初始根中的对数(几率)相匹配。</p><p id="34a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，设<em class="me"> dᵢ </em>表示节点# <em class="me"> i </em>中的观测id集合，即<em class="me"> d₂ </em> = {1，2，4，7}。那么节点#2的转换是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/0e2c632b00189f5771ff317cd7341d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*hTXrs5A9ufQoZupBWfO-2Q.png"/></div></figure><p id="ad7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对节点#3和节点#4进行相同的计算，得到所有训练观测值的<em class="me"> γ </em>，如下表所示。现在，您可以使用公式更新日志(赔率)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/b021d3ac8ebfae270592c973a30dd69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*zPU4EKyaAw-r5Nf5AfGc5A.png"/></div></figure><p id="e371" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="me"> α </em>是梯度推进的学习率，是一个超参数。我们来设置<em class="me"> α </em> = 0.4，那么更新日志(odds)的完整计算见下图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qj"><img src="../Images/0cb66116ff77ba7a2451db892c32e34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_c7ymaeBWNE9EWNxrEqFQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更新日志(赔率)|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><h2 id="8939" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">第四步。重复</h2><p id="f502" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">使用新日志(odds)，重复步骤2和步骤3。这是新的残差和用来预测它的决策树。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/1c8b8aab1d20407fd5c67b531c110e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lKLURA3R1YTw0MX8qEkkw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算残差并拟合决策树以预测残差|图像作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="2a67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">像以前一样计算<em class="me"> γ </em>。例如，考虑节点#3，然后</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/2856dfc1831565e505f24891ba025369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*TJkkyEOJdvxZh-b3woeSuQ.png"/></div></div></figure><p id="1f15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用<em class="me"> γ </em>，像以前一样计算新的对数(赔率)。这是完整的计算</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qj"><img src="../Images/ceb2bd47a491bb47bc317a4d1ffa0191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vULnLaYsRPjjkJGQqri5gg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更新日志(赔率)|图片作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="472b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以重复这个过程很多次。然而，对于这个故事，为了避免事情失控，我们现在将停止迭代。</p><h2 id="b2db" class="ni nj it bd nx ot ou dn ob ov ow dp of lr ox oy oh lv oz pa oj lz pb pc ol iz bi translated">预言；预测；预告</h2><p id="32cf" class="pw-post-body-paragraph li lj it lk b ll on kd ln lo oo kg lq lr op lt lu lv oq lx ly lz or mb mc md im bi translated">要对测试数据进行预测，首先，遍历每个决策树，直到到达一个叶节点。然后，将初始根的log(odds)与所有由学习率缩放的对应叶节点的<em class="me"> γ </em>相加。</p><p id="7d61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的例子中，测试观察落在两个决策树的节点#3上。因此，总和变成-0.847+0.4×(-1.429+1.096)=-0.980。转换为预测概率</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/078ccc20a64daa551241a82daabf5547.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*i143ljRf--YV_uY3p03a0w.png"/></div></figure><p id="37c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自0.273 &lt; 0.5, predict <strong class="lk jd">规避=否</strong>。</p><h1 id="2d8b" class="nw nj it bd nx ny qm oa ob oc qn oe of ki qo kj oh kl qp km oj ko qq kp ol om bi translated">结论</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/f2e8a0362b4499e749250b6f224a2477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zoUpgVl5vjb62C1A"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">马库斯·斯皮斯克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9962" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您已经非常详细地学习了<strong class="lk jd">随机森林</strong>、<strong class="lk jd"> AdaBoost </strong>和<strong class="lk jd">梯度增强</strong>，并将它们应用于一个简单的虚拟数据集。现在，您不仅可以使用已建立的库来构建它们，还可以自信地知道它们是如何从内到外工作的，使用它们的最佳实践，以及如何提高它们的性能。</p><p id="4309" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恭喜你。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qs"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="me">成为会员</em> </strong> </a> <em class="me">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🔖<em class="me">想了解更多关于经典机器学习模型如何工作以及如何优化其参数的信息？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="ml mm gp gr mn"><div role="button" tabindex="0" class="ab bv gv cb fp qt qu bn qv lb ex"><div class="qw l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qx qy fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qx qy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rb rc gw l"><h2 class="bd jd wc pe fp wd fr fs mu fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh su wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="ro dh rp fp ab rq fo di"><div class="di rg bv rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rg bv rj rk rl"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rm rn rl"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="ml mm gp gr mn"><div role="button" tabindex="0" class="ab bv gv cb fp qt qu bn qv lb ex"><div class="qw l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qx qy fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qx qy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rb rc gw l"><h2 class="bd jd wc pe fp wd fr fs mu fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh su wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ro dh rp fp ab rq fo di"><div class="di rg bv rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rg bv rj rk rl"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rm rn rl"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="ml mm gp gr mn"><div role="button" tabindex="0" class="ab bv gv cb fp qt qu bn qv lb ex"><div class="qw l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qx qy fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qx qy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rb rc gw l"><h2 class="bd jd wc pe fp wd fr fs mu fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh su wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="ro dh rp fp ab rq fo di"><div class="di rg bv rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rg bv rj rk rl"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rm rn rl"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="ml mm gp gr mn"><div role="button" tabindex="0" class="ab bv gv cb fp qt qu bn qv lb ex"><div class="qw l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qx qy fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qx qy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="rb rc gw l"><h2 class="bd jd wc pe fp wd fr fs mu fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh su wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="ro dh rp fp ab rq fo di"><div class="di rg bv rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rg bv rj rk rl"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rm rn rl"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="ml mm gp gr mn"><div role="button" tabindex="0" class="ab bv gv cb fp qt qu bn qv lb ex"><div class="qw l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qx qy fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qx qy fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated">艾伯斯·乌兹拉</p></div></div><div class="rb rc gw l"><h2 class="bd jd wc pe fp wd fr fs mu fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi we au wf wg wh su wi an eh ei wj wk wl el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----ca5a1e305af2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wm l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="ro dh rp fp ab rq fo di"><div class="di rg bv rh ri"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rg bv rj rk rl"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rm rn rl"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div></div></div>    
</body>
</html>