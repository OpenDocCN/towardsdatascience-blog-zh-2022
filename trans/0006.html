<html>
<head>
<title>How to Train your CLIP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练你的剪辑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-your-clip-45a451dcd303#2022-02-01">https://towardsdatascience.com/how-to-train-your-clip-45a451dcd303#2022-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3b96" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍CLIP以及我们如何针对意大利语对其进行微调</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4aa94b9a09158ccea53a2972eaf81b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eigEX5ocpoNIMS4nSbk-ig.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CLIP在向量空间中嵌入图像和文本。作者提供的图片(图片来自免费的Unsplash数据集)。</p></figure><p id="e6b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在7月份，HuggingFace和谷歌组织了一次联合社区周，感兴趣的人可以利用谷歌TPUs来试验他们喜欢的项目(也可以使用JAX图书馆)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/962616ef6e07e4a44df4679cfd192bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxcdkHWB7Vsj4IS7Zy_lVA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">jax-亚麻拥抱脸社区周标志。</p></figure><p id="7fa4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我将要描述的项目始于这个<a class="ae lr" href="https://discuss.huggingface.co/t/clip-like-contrastive-vision-language-models-for-italian-with-pre-trained-text-and-vision-models/7268" rel="noopener ugc nofollow" target="_blank">线程</a>，是一个关于多模态(图像和文本)表征学习的简单实验。然而，事实证明，就结果而言，该项目的影响比我们预期的要有趣得多。</p><blockquote class="lt lu lv"><p id="b73d" class="kv kw lw kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">我说“我们”是因为如果没有我出色的队友们，这个项目就不可能完成:<a class="ae lr" href="https://gattanasio.cc/" rel="noopener ugc nofollow" target="_blank"/><a class="ae lr" href="https://www.rpisoni.dev/" rel="noopener ugc nofollow" target="_blank">拉斐尔</a><a class="ae lr" href="https://silviatti.github.io/" rel="noopener ugc nofollow" target="_blank">西尔维亚</a><a class="ae lr" href="https://gsarti.com/" rel="noopener ugc nofollow" target="_blank">加布里埃尔</a><a class="ae lr" href="https://sites.google.com/view/srilakshmi/home" rel="noopener ugc nofollow" target="_blank">斯里</a>和达里奥。</p></blockquote><p id="7156" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们从剪辑模型开始，这是一个非常强大的模型，可以联合学习文本和图像的表示。剪辑是有用的许多任务，如图像检索和零镜头图像分类。对于后者，CLIP能够匹配并击败许多监督图像分类模型，而无需查看原始训练集的一个示例。</p><p id="b1ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">显然，如果没有HuggingFace和Google提供的资源和帮助，这是不可能的。</p><p id="6adf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一些快速链接:</p><ul class=""><li id="5142" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated"><a class="ae lr" href="https://github.com/clip-italian/clip-italian/" rel="noopener ugc nofollow" target="_blank">我们的GitHub资源库</a>，在这里你可以找到我们的培训脚本</li><li id="ce97" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated"><a class="ae lr" href="https://huggingface.co/clip-italian/clip-italian" rel="noopener ugc nofollow" target="_blank">我们的型号</a>在HuggingFace hub上</li><li id="ef7d" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated"><a class="ae lr" href="https://arxiv.org/pdf/2108.08688.pdf" rel="noopener ugc nofollow" target="_blank">我们的论文</a></li><li id="50e0" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">你可以玩我们的演示来评估意大利的能力</li></ul><h2 id="8fb1" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">今天的计划</h2><p id="5447" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">在本文中，我将首先对CLIP的工作原理进行概述(第1节)。我将努力保持高层次的抽象，但同时，我将努力分享理解这个模型如何工作所需的所有信息。之后，我将更详细地描述我们如何训练它来覆盖意大利语(第2节)。</p><p id="9833" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢视频格式，我九月份在LightOn AI做了一个演讲，你可以在这里找到视频:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h1 id="170b" class="no mp iq bd mq np nq nr mt ns nt nu mw jw nv jx mz jz nw ka nc kc nx kd nf ny bi translated"><strong class="ak">第1部分—剪辑准备工作</strong></h1><p id="e04a" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated"><a class="ae lr" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">对比语言-图像预训练</a> (CLIP)是OpenAI最近提出的一种模型，用于联合学习图像和文本的表示。在纯粹的自我监督形式中，CLIP只需要输入图像-文本对，它将学习将两者放在同一个向量空间中。剪辑需要训练图像和这些图像的标题。</p><h2 id="e4f6" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">编码</h2><p id="61f1" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">CLIP背后的假设非常简单:您需要一个图像编码器和一个文本编码器。其中的每一个都将生成一个向量(一个用于图像，一个用于文本)。</p><p id="288e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图像编码器可以使用视觉转换器或CNN构建，使用什么并不重要(是的，选择会影响性能，但想法是一样的)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/94cca51e42a09f86b049041c59ee1cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SgM2K-SY28BXxfoVveNRBg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。我们使用图像编码器从图像中生成矢量表示。</p></figure><p id="732f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要嵌入文本数据，您可以使用转换器(例如，预训练的BERT模型)或您喜欢的任何其他类型的文本编码方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/3dc12848c4d12dfd61b025191eccc427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4yKLBe3vP2jxIOC-NwxoyQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。我们使用文本编码器从一个句子中生成一个向量表示。</p></figure><h2 id="4354" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">训练剪辑模型</h2><p id="440f" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">关于CLIP最重要的事情是，它将图像和字幕的编码表示(即我们上面看到的向量)放在同一个向量空间中，并将“匹配”的字幕-图像对放在空间中。例如，像这样(我使用2D表示，但这也适用于N维):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/9d874abe6ff3e32d604e52e88afd4fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6y8uVzqhw9SX1La_OoIeZA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。剪辑目的是使图像-文本对在空间上接近。</p></figure><p id="eba1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">CLIP训练背后的直觉可以用下面的GIF简单总结一下。在训练期间，图像和描述它们的标题被放在向量空间中，而不匹配的图像(或标题)被推开。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/ed48729468489a8b8858e74b4e01e8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uyRvuUuTlYenXCQSVDgaJg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。训练过程旨在将匹配的图像-文本对放在一起，并远离其他一切。</p></figure><p id="3c86" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们可以把所有东西放在一起。在下面的GIF中，你可以看到训练是如何完成的，我们有两张图片和两个标题，我们知道每张图片的“正确”标题。我们可以为两个图像和两个标题创建嵌入，并计算各自的相似性。我们希望匹配的图像-标题对之间的相似度(点积)上升，而不匹配的图像-标题对之间的相似度下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/8b3b9043e469d8d2185186802d551b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*28h6mk54FH9zSGWIJCJgjw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。GIF更详细地展示了训练过程。我们从成批的图像和文本对开始，并且我们最大化匹配的图像-文本对之间的相似性(并且最小化不匹配的图像-文本对)。</p></figure><p id="2519" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用这个过程，我们学会了在匹配图像-文本对的向量空间中放得很近，而将其他所有东西移得很远。</p><h2 id="5074" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">为什么CLIP很酷？</h2><p id="95e4" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">CLIP非常有趣，因为在没有任何实际监督的情况下(如果我们忽略图像和标题之间的匹配)，将图像和文本放在同一个空间会带来有趣的可能性。最有趣的一个可能是零镜头分类:如果您可以访问一幅图像和一组标签(任何种类的标签)，您可以看到哪个标签与您作为模型输入的图像最相似。</p><p id="8080" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看了下面的动画，大概的思路应该更清晰了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/256e400819051cfeaee238f42551d692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gN3giPwZ4xl3WEY7bVHy2g.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。使用免费的Unsplash数据集。当投影到空间中时，猫的图像和标签“猫”将是接近的(考虑空间中的一些距离度量)。</p></figure><p id="23e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">CLIP在没有看到一个训练样本的情况下，在ImageNet等数据集上显示出令人难以置信的零拍摄性能。你可以在<a class="ae lr" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">原文</a>中读到更多这方面的内容。</p><h1 id="289d" class="no mp iq bd mq np nq nr mt ns nt nu mw jw nv jx mz jz nw ka nc kc nx kd nf ny bi translated">第2部分—剪辑—意大利语</h1><p id="37da" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">通过我们的项目，我们试图为意大利NLP和CV社区提供另一种资源。我们的模型是第一个为意大利语制作的剪辑模型。</p><p id="445e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了用意大利语训练剪辑，我们需要一样东西:数据。特别是，我们需要带有说明文字的图片。虽然我们有英语资源，但意大利语就没那么幸运了。</p><h2 id="d220" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">数据集</h2><p id="a416" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">我们考虑了三个主要的数据来源:</p><ul class=""><li id="9958" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated"><a class="ae lr" href="https://github.com/google-research-datasets/wit" rel="noopener ugc nofollow" target="_blank"> WIT </a>是从维基百科收集的图片说明数据集(参见，<a class="ae lr" href="https://arxiv.org/pdf/2103.01913.pdf" rel="noopener ugc nofollow" target="_blank"> Srinivasan等人，2021 </a>)。我们重点关注论文中描述的<em class="lw">参考描述</em>标题，因为它们是质量最高的。尽管如此，许多这些标题描述本体论的知识和百科全书的事实(例如，罗伯托·巴乔在1994年)。</li><li id="702b" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated"><a class="ae lr" href="https://github.com/crux82/mscoco-it" rel="noopener ugc nofollow" target="_blank"> MSCOCO-IT </a>。这个图像字幕数据集来自于<a class="ae lr" href="http://www.ai-lc.it/IJCoL/v5n2/IJCOL_5_2_3___scaiella_et_al.pdf" rel="noopener ugc nofollow" target="_blank"> Scaiella等人的工作，2019 </a>。字幕来自原始的MSCOCO数据集，已经用Microsoft Translator进行了翻译。2017版的MSCOCO训练集包含超过100K幅图像，每幅图像都有多个字幕。</li><li id="a4e0" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated"><a class="ae lr" href="https://ai.google.com/research/ConceptualCaptions/" rel="noopener ugc nofollow" target="_blank">概念说明</a>。这个图像字幕数据集来自于<a class="ae lr" href="https://aclanthology.org/P18-1238.pdf" rel="noopener ugc nofollow" target="_blank"> Sharma等人的工作，2018 </a>。在这个数据集中有超过3百万个图像-标题对，这些都是从网上收集的。我们用数据集提供的URL下载了图像，但我们无法全部检索到它们。最终，我们不得不将字幕翻译成意大利语。我们已经能够收集具有70万翻译字幕的数据集。</li></ul><p id="5786" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然翻译后的标题在某种程度上是一种限制，但它们看起来质量很好(我们运行了一个小型的手工评估管道来检查质量，注释者之间达成了良好的一致)。</p><h2 id="dc1d" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">培训/建筑</h2><p id="b71e" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">你可以在我们的<a class="ae lr" href="https://github.com/clip-italian/clip-italian/" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>上找到更多关于我们如何训练意大利语剪辑的细节。感谢HuggingFace脚本，这很容易做到，我们基本上只需要改变一些超级参数。</p><p id="20ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们考虑的架构使用来自CLIP的原始图像编码器，而不是作为文本编码器，我们使用意大利BERT模型(因为我们需要创建意大利嵌入)。OpenAI使用的视觉转换器已经在4亿张图像上进行了训练，它可能是我们架构中需要训练最少的元素。</p><p id="0084" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，由于我们不是从原始分量开始(BERT是新增加的)，投影不再对齐(即，BERT编码投影到向量空间的完全不同的区域)。这需要在训练中小心，不要弄乱预训练组件的重量。</p><p id="ac73" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了允许随机初始化的重投影层预热，而不干扰主干的调整权重，我们决定在我们架构的主干完全冻结的情况下进行第一次训练。只有在这些层融合后，我们才解冻模型的其余部分，以微调所有组件。这种技术允许我们达到更好的验证损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/e8298f5757ad81d8f31a196cfcc12952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y23WqvN1vBbrsM4i.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">蓝色层是编码层，我们有一个额外的层用于512维的投影，那是我们为训练热身的层。图片由作者提供。</p></figure><p id="7a36" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">只要你有数据和一些计算资源，训练CLIP并不是不可能完成的任务。运行提供的训练脚本就足够了。最近，HuggingFace开发了一个易于使用的管道来玩<a class="ae lr" href="https://huggingface.co/docs/transformers/model_doc/visionencoderdecoder" rel="noopener ugc nofollow" target="_blank"> VisionTextEncoders </a>，如果你对这一领域的研究感兴趣，你可能会想考虑一下。</p><p id="42ae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以在我们的<a class="ae lr" href="https://github.com/clip-italian/clip-italian/" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>上找到一些关于我们如何评估这个模型的细节。长话短说，对于两个不同的任务(图像检索和零镜头图像分类)来说，使用特定于语言的模型比使用多语言剪辑要好(尽管多语言剪辑由于其他原因非常酷)。</p><h2 id="31b9" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">例子</h2><p id="b96f" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">在这里，我将展示一些来自我们在线<a class="ae lr" href="https://huggingface.co/spaces/clip-italian/clip-italian-demo/" rel="noopener ugc nofollow" target="_blank">演示</a>的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/6d7e584207b4933d50f7a0c29ce9ae6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIKoamdmGvOzysfMT-vuug.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">《山上的两个人》。来自Unspash free数据集的图像。</p></figure><p id="4c94" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于“数猫”的几个例子</p><div class="kg kh ki kj gt ab cb"><figure class="oc kk od oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d5b36a32680a8098036e4156f6245d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*zJZVfR_zQd9fxHFDu2Nv7w.png"/></div></figure><figure class="oc kk oi oe of og oh paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/1c8122c82f006081ac5242dbc7517605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*CR7jdl6ywMsw8PO0cAXSKQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk oj di ok ol translated">“两只猫”和“一只猫”。来自Unspash free数据集的图像。</p></figure></div><p id="b626" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有趣的是，这个模型似乎能够计数。这显然来自训练数据，显示了其中的一些模式，但值得注意的是，该模型可以学习识别图片中的多个对象(然而，请注意，这并不能很好地扩展:对于像“四只猫”和“五只猫”这样的数字，结果就不太清楚了)。</p><h2 id="f973" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">如何使用剪辑-意大利语</h2><p id="86d4" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">你想自己玩这个吗？我们为你做好了准备，这是朱塞佩为你准备的笔记本，在PyTorch。<a class="ae lr" href="https://colab.research.google.com/drive/1SSddpjohAqRS_XxJvwz5HN1YFPevVJmy?usp=sharing" rel="noopener ugc nofollow" target="_blank">打开可乐杯</a>。</p><p id="a353" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了向您展示使用它是多么容易，我将在这里分享一些代码行，您可以使用它们来加载模型和生成嵌入。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="om nn l"/></div></figure><p id="7c8b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在可以很容易地使用它，你可以看到一个如何嵌入文本的例子，但正如我所说的，你可以在colab笔记本上找到一切:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="om nn l"/></div></figure><p id="a0ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以对图片做同样的事情，计算图片和文本之间的相似度。</p><h2 id="5604" class="mo mp iq bd mq mr ms dn mt mu mv dp mw le mx my mz li na nb nc lm nd ne nf ng bi translated">结论</h2><p id="6925" class="pw-post-body-paragraph kv kw iq kx b ky nh jr la lb ni ju ld le nj lg lh li nk lk ll lm nl lo lp lq ij bi translated">剪辑-意大利一直是一个惊人的冒险。发现和学习了许多乐趣和新事物。一个惊人的项目和一个惊人的机会，为此我们必须感谢谷歌和HuggingFace为实现这个项目所做的努力。</p></div></div>    
</body>
</html>