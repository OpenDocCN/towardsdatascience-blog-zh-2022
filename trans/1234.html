<html>
<head>
<title>Gradient Boosting in Python from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的渐变提升从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-in-python-from-scratch-788d1cf1ca7#2022-03-29">https://towardsdatascience.com/gradient-boosting-in-python-from-scratch-788d1cf1ca7#2022-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3d1f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 编码并深入解释非常流行且在竞赛中获奖的梯度推进算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd84a5b373b5310efa86405703b4a880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQ69MSfNojO9i_asf432_A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">喙的进化；图像许可<a class="ae ky" href="https://creativecommons.org/licenses/by/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/licenses/by/4.0/deed.en</a>；原文链接<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Charles_Darwin,_Journal_of_Researches..._Wellcome_L0026712.jpg" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Charles _ Darwin，_ Journal _ of _ Researches..._Wellcome_L0026712.jpg </a></p></figure><p id="8373" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的目的是使用 Python 代码和可视化来解释流行且时常神秘的<strong class="lb iu"> <em class="lv">梯度增强算法</em> </strong>。梯度提升是 CAT boost、ADA boost 或 XGBOOST 等竞赛获奖算法的关键部分，因此了解什么是提升、什么是梯度以及这两者在创建算法中的联系是任何现代机器学习从业者的必备知识。</p><p id="ec4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python 中回归的梯度推进的实现和动画可以在我的报告中访问:<a class="ae ky" href="https://github.com/Eligijus112/gradient-boosting" rel="noopener ugc nofollow" target="_blank">https://github.com/Eligijus112/gradient-boosting</a></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="bf72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文章的主要图片描述了进化的过程，以及在很长一段时间内，一个鸟类物种的喙大小是如何适应周围环境的。<a class="ae ky" href="https://en.wikipedia.org/wiki/Darwin%27s_finches" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Darwin%27s_finches</a></p><p id="580a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如动物在其栖息地以各种方式适应新的事实一样，机器学习算法也适应我们将它们置于其中的数据环境。梯度推进算法背后的主要思想是，其主要引擎是从其自身先前的错误中学习的低精度和简单的算法。</p><p id="570a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每次迭代中，不仅错误被用于调整模型，前一次迭代的模型也被调用。因此，随着数据的每次传递，梯度推进模型变得越来越复杂，因为它将越来越多的简单模型加在一起。</p><p id="e521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，许多梯度增强算法的简化方程是一种递归:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi md"><img src="../Images/e1561b16ba3ecd1abd65c0480895c9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/0*b1eMVa3oAsLBBrAj"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">直觉方程式</p></figure><p id="ca27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当前值<strong class="lb iu"> m </strong>(把它想象成现在)使用过去的信息(<strong class="lb iu"> m -1 </strong>)，并通过新的具有一定权重的当前证据(<strong class="lb iu"> G </strong>)进行调整。</p><p id="4d5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的文章中，我们将更深入地探讨梯度增强的本质细节，我希望在阅读完所有代码和解释后，读者会发现梯度增强虽然听起来吓人，但并不复杂。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="bfa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们想要一个模型，根据汽车的重量来预测汽车可以行驶多少英里。可以从这里访问数据:</p><p id="4717" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/auto+mpg" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/auto+mpg</a></p><p id="d578" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有所有特征的数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi me"><img src="../Images/ce695c7fb94fc068633d99d3511c2f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uvc-O_fnX5pXCq0Mzpb-9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">汽车数据；按作者分类的表格</p></figure><p id="3673" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题中的关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/fb1fd450996ca6e1470d3ea2f906e824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPekoKlRkgLdkT2b3aGGAw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">mpg ~体重；按作者分类的图表</p></figure><p id="1d83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一个明显的关系——车越重，每加仑能跑的里程越少。让我们试着让一个基础学习者适应这些数据，看看它的表现如何。</p><p id="9404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们开始构建一个梯度推进的机器学习算法来模拟这种关系。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="ec90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归梯度推进算法的标题中有三个非常宽泛的机器学习术语:</p><ul class=""><li id="108e" class="mg mh it lb b lc ld lf lg li mi lm mj lq mk lu ml mm mn mo bi translated"><strong class="lb iu">回归</strong></li><li id="6fa3" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated"><strong class="lb iu">渐变</strong></li><li id="3746" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated"><strong class="lb iu">增压</strong></li></ul><p id="3073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在试图将它们合并在一起之前，至少对独立的定义有一个直观的理解是很重要的。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="e841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">机器学习中的回归</em> </strong>就是寻找一个连续变量的<strong class="lb iu"> Y </strong>平均值和特征<strong class="lb iu"> X </strong>之间的关系<strong class="lb iu"> <em class="lv"> f </em> </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/619c95e7c2371f88e73652aa366af066.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/0*_8ZM1LiscjHi8S2E"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">简而言之就是回归</p></figure><p id="1537" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在回归中，我们试图预测的变量是连续的，也就是说，它可以有无穷多个值。比如人的体重，一个人在奥运会上完成一场比赛的速度，一个人的工资等等。</p><p id="52ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们试图用特性<strong class="lb iu"> X </strong>来解释<strong class="lb iu"> Y </strong>变量的方式。例如，我们可以说一个人的工资(<strong class="lb iu"> Y </strong>变量)是由他或她的工作经验、学术成就、证书数量等(<strong class="lb iu"> X </strong>变量)决定的。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="0e82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数学中函数<strong class="lb iu"> </strong>的梯度</strong>是一个向量，它的每个坐标都是给定函数自变量的偏导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/3cc90826094237cbf4ce7b18fb2fec26.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*rv1oNfz6iOs-yr7T"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度定义</p></figure><p id="c3ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度在寻找使函数最小化或最大化的函数自变量的流行算法中被广泛使用。这是因为，在任何给定点<strong class="lb iu"> x </strong>处，如果函数的梯度为负，那么原始函数在该点<strong class="lb iu"> x </strong>处递减。如果在点<strong class="lb iu"> x </strong>处梯度为正，则函数增加。这就是为什么机器学习中的许多损失函数试图尽可能简单(参数越少越好)并且可微分(能够找到梯度)。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="eb26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习中的术语<strong class="lb iu"> boosting </strong>是指将一组<strong class="lb iu"> <em class="lv">弱学习者</em> </strong>训练到训练数据中的过程，其中每个弱学习者从先前学习者的错误中迭代学习。</p><p id="11be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">弱学习器</strong>是一种机器学习算法，它可以快速拟合数据，但在准确性、均方误差或其他指标方面的性能相对较差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/238d7983d1ff4abbc1a6e0e1aa18e5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gl4S98dlRJqMh4jR4eDOJg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">增强模式；按作者分类的图表</p></figure><p id="de05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的模式中，每个 ML 模型都是弱学习者。每个误差都是用过去的预测计算出来的。每个后续模型都试图用原始特性来拟合之前的<strong class="lb iu">错误<strong class="lb iu">(这里就大胆一点，跟着文章走，我稍后会详细解释这个过程)。</strong></strong></p><p id="c0e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终预测是从得到的 ML 模型的所有输出的加权和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/d3d5e064f952ad36b34dbdd55d1092b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNNud7AEYk63-wCSk95mMA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终增强预测；按作者分类的图表</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="4d4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">回归梯度推进</strong>是一种将上述所有思想结合到一种机器学习方法中的算法。</p><p id="c35f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入研究所有这三个想法是如何相互联系的之前，我们需要选择一个基础较弱的学习者来完成提升部分。一个非常流行的选择是回归决策树。要了解关于回归决策树的更多信息，请查看我的文章:</p><div class="my mz gp gr na nb"><a rel="noopener follow" target="_blank" href="/regression-tree-in-python-from-scratch-9b7b64c815e3"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">Python 中的回归树从头开始</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">用 Python 编写流行的回归树算法，并解释其本质</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ks nb"/></div></div></a></div><p id="0518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快速回顾一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ae075aa1dfe6e31f3c282dca528a40f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*e_e1-w8AofrHUt2WuJIzig.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归树模式；按作者分类的图表</p></figure><p id="d214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">树中的每个节点都保存有 Y 和 X 特征。此外，每个节点都具有:</p><ul class=""><li id="2347" class="mg mh it lb b lc ld lf lg li mi lm mj lq mk lu ml mm mn mo bi translated">从每个 Y 值中减去 Y 平均值的特殊残差。</li><li id="5744" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated">残差的均方误差。</li><li id="1d17" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated">用于进一步创建节点的最佳分割特征和最佳分割值。</li><li id="3960" class="mg mh it lb b lc mp lf mq li mr lm ms lq mt lu ml mm mn mo bi translated">所有的初始超参数。</li></ul><p id="927f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归树的 Python 实现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nr ns l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归树；作者代码</p></figure><p id="e6cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使一个弱学习者适应数据，运行下面的代码:</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="f1c5" class="ny nz it nu b gy oa ob l oc od"># Initiating the tree<br/>reg = Tree(d, 'mpg', ['weight'], max_depth=2)</span><span id="0423" class="ny nz it nu b gy oe ob l oc od"># Fitting on data<br/>reg.fit()# Initiating the tree<br/><br/># Printing out the tree<br/>reg.print_tree()</span></pre><p id="aa69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a9b013800ed83e00bbd674e88666873c.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*hvaoHiszcNW_mMIUgHme7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">树信息；作者图片</p></figure><p id="35ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">弱学习者建议首先在 2764.5 权重点分割数据，因为在该点，与根节点中的原始均方误差相比，一般均方误差将减少最多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/4463228dbb11d4b4dfe29c36dc1db7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vL1Du6TeFphEA9vhNTxleg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归树预测；按作者分类的图表</p></figure><p id="780f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们从上面的预测中可以看到的，回归树将数据分为 4 部分:直到 2217 重量的所有数据，然后从 2217 到 2764.5，然后从 2764.5 到 3657.5，以及从 3657.5 开始的所有数据。预测值是属于该权重类别的 Y 变量的平均值。比较</p><p id="291d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，图表显示最大深度为 2 的回归树是弱学习者。让我们弄清楚如何把它们结合起来创造一个强大的学习者。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="5274" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来自维基，<strong class="lb iu">全梯度提升算法:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/e90589335103510fa56c82176ee582da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCP1eySY20B1aclq3FtuzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_boosting</a></p></figure><p id="5d40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乍一看，上面的算法很吓人。但是让我们一行一行地检查它，并通过编码找到一个完整的解决方案。</p><p id="ec24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在编码解决方案中，让我们坚持以前的例子，并尝试使用汽车的重量作为解释来创建 mpg 的预测值。</p><p id="23fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们的数据</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4d56ca58c304cdb34c7da0b0b111307a.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/0*HExyRDhMF-z8s_go"/></div></figure><p id="05f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正在实践中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/1e0fe59d68f8ae63a136cdfcfd07a075.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/0*vufsDIw2HEydOBye"/></div></figure><p id="bec9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">下一个非常重要的部分是选择损失函数</strong>。梯度增强树的一个非常流行的损失函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/02604fd78d5db6f55937c1577cb062d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/0*nzxXNrzY3jwNowXf"/></div></figure><p id="ac6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它之所以受欢迎是因为对其求导会产生:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ec8001ce7ac07621d4fe553d154db0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*PXuaEhtiBjmd0zxf"/></div></figure><p id="1b48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当简化算法中的各种公式时，这将非常方便。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="fbe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用我们选择的损失函数初始化算法(步骤 1 ):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e565fa23ecc66d468b3218c4f770a69e.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/0*fIznmsrx_cvui_Wp"/></div></figure><p id="87ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，为了找到使该函数最小化的自变量，我们用 gamma 进行微分，并使其等于 0:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/8ba1df8c9544fdbbca4c7420e6b259c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/0*kjr0APrOofBxwPdv"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2407f0ae9f9d29da7485ad50364e2d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*u4p60iYrPjXa7Qz7"/></div></figure><p id="af9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上式的右边只是初始 y 的平均值。<strong class="lb iu">因此，我们总是以 y 的平均值作为初始预测来开始算法。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a1fcfba3c1c2a04830a9e577c0e6c1cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/0*EFpwXEw9I9MdfKgh"/></div></figure><p id="b045" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的实际例子中，mpg 因变量的平均值为 23.51。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="574a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法的第二部分<strong class="lb iu">是</strong> <strong class="lb iu">提升部分</strong>，在这里我们使用来自前一次迭代的信息来调整每次迭代。</p><p id="8ff2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法第二步的第一部分涉及所谓的<strong class="lb iu">伪残差</strong>的计算。这个术语很容易混淆，听起来可能比实际情况更复杂。我喜欢称它们为残差，在本文中，我会交替使用这些术语。让我们回顾前几节，看看我们选择的损失函数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ec8001ce7ac07621d4fe553d154db0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*PXuaEhtiBjmd0zxf"/></div></figure><p id="d688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当<strong class="lb iu"> m = 1 </strong>时，算法定义的公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/9b2bcc7ddf0f12ddec6bb699a41d625e.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/0*XQ7aeTeK6zYvr_Nr"/></div></div></figure><p id="fb06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一次迭代的残差将通过从 y 的所有单个值中减去 y 的平均值来计算。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/fe7d74a708353065ab8502185ee3cf31.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*r-fI7sIUAfAPMJpgJfbcTQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一批残差(R1)；平均每加仑=23.51</p></figure><p id="df0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">从上一部分，术语梯度出现了，因为我们在进一步的步骤中使用损失函数的梯度。</strong></p><p id="e319" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法第二步的第二部分是用相同的特征 X 拟合回归树，但是现在 Y 值是从第一部分得到的残差。换句话说，使用回归树找到关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e0b2890b49556fb0679a67de00d65592.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/0*525esbscIrJcMyVe"/></div></figure><p id="e467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二步的第三部分涉及一个相当长的公式。让我们扩展一下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/cc58e4775a44ccb877833eb3f548079a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*QOLaaGMzd2NlxDWYrd6IVw.png"/></div></figure><p id="2eef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化伽马射线产量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/8f39d6f69492d56eed622557be806447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/0*3awCoGLs9zX1BZTJ"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/36aa440d47d2a6b40f52e7ce6c88d52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/0*5tjco7Rn5_NXJ1P3"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/53cb73ea7907513d56ab59b791c6be0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*GIDC2rlvZa0PPgJy"/></div></figure><p id="8ede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此使用以下公式找到迭代 m 的最佳伽马值<strong class="lb iu">:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5b66f38a7cad5e6bd5e1bd1df1d07b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/0*fl5B1ndMDnmNwliP"/></div></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="4ab9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的等式是原始算法的一般解。在他的原始论文中，<a class="ae ky" href="https://jerryfriedman.su.domains/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">https://jerryfriedman.su.domains/ftp/trebst.pdf</a>杰罗姆·h·弗里德曼提出，当使用决策树作为弱学习器时，我们不仅应该为每一层<strong class="lb iu"> m </strong>使用一个伽马值，还应该为每一片叶子<strong class="lb iu"> j </strong>使用一个伽马值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/52dee7ff1c28010e26edeef0d527fa6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/0*PQ6C3-YpAlKfn00e"/></div></figure><p id="614b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设区域(回归树中的叶子)R 有<strong class="lb iu"> k </strong>个条目。然后我们可以解析地解决优化问题:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2ef4776c6824ee0976634ab45b966c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O9Rof5-BUgrZcgYi"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/d40978eb3a06a44daee23cbd7e314827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p2fgL23et5WR7hvu"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/60bea9d03e664f046e8e062451c69f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/0*CeInpTonH9SBj1lV"/></div></figure><p id="1fca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的等式最终确定了一个非常方便的特性，即每次迭代 m 和叶 j 处的最佳伽马值是落入该叶的残差的平均值。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="131a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法的第二步的第四部分就是在步骤<strong class="lb iu"> m </strong>中对预测的定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/8b1322af78403dbd04252dbc329b1254.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/0*mAA3BkuokY_mnDwx"/></div></figure><p id="482d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">右边的新术语 alpha 是在迭代之前定义的超参数。它被称为<strong class="lb iu">学习率</strong>，控制着每一步预测更新的程度。</p><p id="eb56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">弗里德曼提出的扩展是使用下面的等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/f66855d3cb20c8c03d72e9a573574911.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/0*yUdFGxY8H3GgVzyY"/></div></figure><p id="72f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在 Python 实现中，我将使用 Friedman 的方法。</strong></p><p id="ca37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以进一步简化弗里德曼的方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/39f621165a2aeaab0cc1020cf72b199f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qKPV7RFycYjPHAikbNaJ4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弱学习者图；按作者分类的图表</p></figure><p id="15bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一些特征集合，每个弱学习器预测前一次迭代的误差。在我们的示例中，X 特征是汽车重量。在每个叶节点，预测值是落在该叶中的平均残值。</p><p id="cbb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们可以简单地将等式改写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/674de235c11dd95f146ac81004a905c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*9KMvCXx1pZKWy1A7"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度推进最终公式</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="3ef0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法的最后第三步就是最终预测的定义。如果我们训练 3 次迭代，那么预测是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/b4cfba17f9ab6c86b7989c239302fa23.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/0*ErvmYHGc0iqDZSb9"/></div></figure><p id="2a6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从等式的右侧看到，我们每次迭代的基本操作是更新 y 的平均值。从某种意义上说，我们以这样一种方式组合弱预测值，即我们从每个数据点的平均值获得最精确的偏差。这一点以及预测值持续的事实让我们称上面的算法<strong class="lb iu">为回归问题。</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="463f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归梯度提升在 Python 中的实现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nr ns l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度推进；作者代码</p></figure><p id="ff12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成梯度增强训练可视化的代码可在此处找到:</p><div class="my mz gp gr na nb"><a href="https://github.com/Eligijus112/gradient-boosting/blob/master/regression/boosting.py" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">主 Eligijus112 的梯度增强/boosting . py/梯度增强</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">此文件包含双向 Unicode 文本，其解释或编译可能与下面显示的不同…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">github.com</p></div></div><div class="nk l"><div class="pd l nm nn no nk np ks nb"/></div></div></a></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/23970511718c808b6fad7dd7baedc92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7GjeGvsJw-I4ek1FDocX_w.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习率= 0.1，最大深度= 2；作者 GIF</p></figure><p id="615e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，在没有任何训练的情况下，算法在开始时预测了 mpg 变量的平均值，而不管汽车的重量值。</p><p id="7958" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一次迭代(橙色点)之后，就预测而言，该算法只是勉强向正确的方向移动。</p><p id="6a75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过 30 次迭代后，该算法非常好地模拟了潜在的关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/db199a3a9900cae61d009efd242b5cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KgL4w8IGOSc_xBS1KEIPaQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习率= 0.3，最大深度= 2；作者 GIF</p></figure><p id="fb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图显示，将学习率从 0.1 增加到 0.3 会减少精确逼近该关系所需的迭代次数。但是，进一步增加该值会导致过度拟合。</p><p id="7d89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有两个输入特征重量和加速度的可视化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/abd4b3cedd1cf961cc2df17acb6d06a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0huCeMvprkQYtRrq9QJ5FQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者 GIF</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="f3b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将回归、提升和梯度部分结合起来，创建了一个回归梯度提升算法。</p><p id="dcc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我提供的所有代码都是开源的，是我写的，所以你想用多少就用多少。</p><p id="d716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快乐学习编码！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="d9b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]作者:<strong class="lb iu"> Dua，Dheeru 和 Graff，Casey </strong>，<br/>年份:<strong class="lb iu"> 2017 </strong> <br/>标题:<strong class="lb iu"> {UCI}机器学习资源库</strong> <br/>网址:<strong class="lb iu"/><a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/auto+mpg" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">http://archive.ics.uci.edu/ml</strong></a><br/>院校:<strong class="lb iu">加州大学欧文分校信息与计算机科学学院</strong></p></div></div>    
</body>
</html>