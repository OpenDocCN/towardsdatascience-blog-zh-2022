<html>
<head>
<title>Hardware Accelerators for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于神经网络的硬件加速器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-neural-networks-on-hardware-baa3c14cd5ba#2022-10-06">https://towardsdatascience.com/accelerating-neural-networks-on-hardware-baa3c14cd5ba#2022-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="81dd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">脉动阵列如何工作</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0193cd334799372fad1cd65355466267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5LrHTXQWVDfsr6Ay"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Vishnu Mohanan 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="2c4b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">介绍</h2><p id="0566" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">在一个神经网络被用来处理几乎任何类型的数据(从图像到音频或心脏活动)的世界，人们越来越有兴趣将这些模型的执行从云转移到边缘(嵌入式)系统。为什么这是一个有趣的趋势？这是这种范式转变的一些原因:</p><ul class=""><li id="2f91" class="mo mp it lx b ly mq mb mr li ms lm mt lq mu mn mv mw mx my bi translated"><strong class="lx iu">能耗:</strong>与其在GPU这样昂贵且耗能的硬件上运行人工智能算法，为什么不构建一个专门定制的嵌入式设备来更有效地执行算法呢？</li><li id="a103" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><strong class="lx iu">隐私问题:</strong>与其将原始的传感器/摄像头数据发送到云端，从隐私的角度来看，将原始数据保留在设备中，只将已经处理过的结果共享到云端不是更好吗？</li></ul><p id="4f8f" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">虽然有多种硬件架构和解决方案可以在嵌入式设备上加速这些算法，但最有吸引力的是基于脉动阵列的加速器。H. T. Kung在其1982年<a class="ae ky" href="https://www.cse.wustl.edu/~roger/560M.f17/01653825.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中首次介绍了这些架构，这些架构基本上是通过以级联方式重复连接基本计算单元(称为<em class="nh">处理单元，此后称为</em>PE)来构建的。这些数组的一个重要方面是PE都等于。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/c27e09cc5d952dc07104994851cb7fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtE3LJpECkGWRFf8_Rd5og.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Kung提出的基本思想(实际例子见论文)。图片作者。</p></figure><p id="fbd0" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">为什么这很重要？简单和常规的设计更容易也更便宜。此外，因为数据是在PE之间传播的，所以这种架构重用了大量数据，这在内存带宽和能耗方面是有吸引力的(数据只从内存中读取一次，可以多次重用)。</p><p id="4620" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">在本文中，您将:</p><ul class=""><li id="be96" class="mo mp it lx b ly mq mb mr li ms lm mt lq mu mn mv mw mx my bi translated">查看神经网络的基本数学运算是如何表示的</li><li id="5cec" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated">了解脉动阵列如何能够执行矩阵乘法运算(配有漂亮且易于理解的动画！)</li><li id="b435" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated">查看如何围绕脉动阵列构建完整硬件加速器架构的示例</li><li id="f178" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated">获取关于该主题的实践项目、相关论文和教育视频的摘要</li></ul><p id="6649" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated"><strong class="lx iu">免责声明:</strong>要完全理解这篇文章，你应该对线性代数(知道什么是矩阵乘法)和数字电路(知道什么是寄存器和时钟)有一个基本的了解。如果你做到了，你就可以走了！如果没有，不要让这阻止你！我相信你会发现这篇文章很有趣。</p><h2 id="c2dd" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">将复杂的神经网络简化为基础</h2><p id="9865" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">如今，神经网络正在成为极其复杂的算法，有数百层，数百万(有时数十亿)的参数。但是在这篇文章中，我们将关注当今流行的神经网络模型中使用较多的两层:卷积层和全连接层。</p><p id="86e3" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">完全连接的层可以直接描述为矩阵乘法运算。卷积层稍微复杂一点，但是存在聪明的变换，以便将它们也变换成矩阵乘法运算(参见<a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/9650846" rel="noopener ugc nofollow" target="_blank">这篇文章</a>或<a class="ae ky" href="https://ieeexplore.ieee.org/document/7995254" rel="noopener ugc nofollow" target="_blank">这篇文章</a>关于这些变换如何工作的例子)。这就是为什么我们将只关注如何设计一个脉动阵列来执行这个操作。</p><p id="9517" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">为了更新您的线性代数知识，让我们记住基本的通用矩阵乘法(GEMM)运算是如何计算的，我们将在整篇文章中继续使用这个小例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本的2x2矩阵乘法。作者视频。</p></figure><p id="ac1b" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">这里的基本操作是什么？再看一遍动画。你看到我们总是在两个元素之间做乘法，然后累加上一次乘法的结果了吗？这种操作被称为乘累加(MACC)，在硬件中实现起来相当容易。</p><h2 id="61a4" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">基础体育</h2><p id="233c" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">因此，我们需要设计一些能够执行MACC操作的PE。</p><p id="2e5e" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">我之前说过，这个运算是一个乘法后面跟着一个累加。现在，在神经网络中，A矩阵通常是该层的输入，B矩阵表示该层的参数(其<em class="nh">权重</em>)。因为这些参数是恒定的，并且在我们使用神经网络预测某些东西时不会(通常)改变，所以我们实际上可以将矩阵B的值预加载到PE中，并在多次乘法中重用它们！我向你保证，这以后会更有意义。这可以通过下图很容易地描述出来(寄存器<em class="nh"> b </em>将预加载相应的<em class="nh"> b </em>参数):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/898c6b0971ebc0d36335d652d771f62d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYbZtatYmmB60S1jDo2YDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基础体育设计。图片作者。</p></figure><p id="58a0" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">但还记得我们讨论过这些脉动架构如何一个接一个地级联PE吗？为了做到这一点，我们只需添加几条线路，让我们能够从一个PE到另一个PE共享数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/06e667f6582d048a13c613bf3e1a5b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zym9RnJKs6BEKVqHhvShEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有传播信号的基本PE。图片作者。</p></figure><p id="da49" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">这是我们的基本PE！首先，注意我们如何从顶部插入<em class="nh"> b </em>参数，并在每个时钟周期将其值传播到将连接到底部的PE(这在用B矩阵预加载脉动数组时会很方便)。第二，注意在每个时钟周期，新的输入数据<em class="nh"> a </em>来自左边的PE，与存储的权重<em class="nh">b</em>相乘，然后与来自上面的PE的部分和累加。输出部分和然后被传播到下面的PE，而原始输入数据<em class="nh"> a </em>被传播到右边的PE。最后，为了使图尽可能简单，我没有画出控制、复位和时钟信号。</p><p id="aa1a" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">接下来，我们将看到如何使用级联来执行整个矩阵乘法！</p><p id="e781" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated"><strong class="lx iu">免责声明:</strong>请记住，这只是<strong class="lx iu">一种</strong>可能的处理元件配置，它使用<em class="nh">权重固定</em>方法(权重首先预加载到PE中，然后来自矩阵A的输入数据和部分和从PE流向PE)。存在用于矩阵乘法的其他PE设计，例如，<em class="nh">输出稳定</em>方法。也可以使用脉动阵列加速的不同算法可以具有更简单或复杂的PE设计。有关所有这些其他方法和PE设计的示例，请查看“参考资料”部分和文章末尾。</p><h2 id="c7ad" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">构建脉动阵列</h2><p id="54d2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">让我们建立一个PE的2D数组:一个<strong class="lx iu">脉动数组</strong>！如前一节所述，每个PE将从其左侧和顶部的PE接收数据，并将数据传播到其右侧和底部的PE。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/f781452e95b125ae803491d4b9c71307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpbbIn16Hgmr0xFjE8OPvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">2D脉动阵列。图片作者。</p></figure><p id="18ae" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">为了执行整个矩阵乘法，我们将首先将整个B矩阵预加载到脉动阵列中(记住，我们使用的是<em class="nh">权重固定</em>方法)。然后，我们将从左到右传播输入矩阵A，并且从上到下传播部分和。C结果矩阵将从底部PEs的输出中获得。请看下面的动画，它通过在2x2脉动阵列中执行[2，2]x[2，2]矩阵乘法来说明这一过程(请注意我们实际上是如何插入A矩阵的转置版本的):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在加权静态脉动阵列上执行2×2矩阵乘法。图片作者。</p></figure><p id="a8ea" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">注意数据如何需要以这种“步进”或移位的方式进入脉动阵列，以及如何以相同的方式获得结果。如果数据在脉动阵列中一次移动一个时钟，那么计算特定配置获得完整结果需要多少个时钟是相当容易的。如果你感兴趣，可以看看阿斯加里等人的工作，其中对方程有更详细的解释。</p><p id="c736" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">如果不是执行C=A*B，我们还想添加一个矩阵(C=A*B+D，有时被称为扩展GEMM)会怎样？嗯，我们只是将D矩阵值插入到数组顶部的PEs的部分和输入中！</p><p id="3ab6" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">但是如果我们的输入矩阵大于脉动数组的大小，我们该怎么办呢？这就是我们使用<strong class="lx iu">平铺</strong>方法的时候。平铺意味着划分我们的A和B矩阵，以便我们得到可以在脉动阵列中执行的补丁。让我们来看一个8×8脉动阵列(Ax、Bx和Cx都是8×8面片)上的[16，16]x[16，16]矩阵乘法的例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">脉动阵列上的平铺示例。作者视频。</p></figure><p id="7dc9" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated"><strong class="lx iu">注意:</strong>如果平铺方法生成的分区小于脉动阵列的大小(例如，因为矩阵大小和脉动阵列大小之间的除法不是偶数)，则PE将在计算期间保持空闲。当然，我们希望在一次计算中尽可能多地使用PE。寻找跨不同工作负载最大化PE使用的操作符映射技术是一个活跃的研究领域！</p><h2 id="a5b6" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">将其集成到完整的硬件加速器架构中</h2><p id="4138" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">仅举一个例子，为了了解所有这些在真实用例场景中是如何协同工作的，让我们向硬件加速器添加更多的模块。</p><p id="4cf0" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">首先，我们将添加内存来存储矩阵A(称为<em class="nh">输入特征映射</em>)、矩阵B(正如我们已经说过的，<em class="nh">权重</em>)和矩阵C(称为<em class="nh">输出特征映射</em>)。然后，我们添加一个模块(例如，一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Direct_memory_access" rel="noopener ugc nofollow" target="_blank"> DMA </a>)来在加速器的内部存储器和外部DRAM之间移动数据(以防矩阵太大而不适合加速器的内部存储器，如果您试图运行一个当今流行的神经网络，如<a class="ae ky" href="https://arxiv.org/abs/2207.02696" rel="noopener ugc nofollow" target="_blank"> YoloV7 </a>或<a class="ae ky" href="https://arxiv.org/abs/1911.09070v7" rel="noopener ugc nofollow" target="_blank"> EfficientDet-D7 </a>，这几乎在所有情况下都会发生)。最后，我们添加了一个控制器模块来协调数据进出加速器，以及矩阵必须如何进入脉动阵列。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/1a502572c2c5939cb011a5171b9e0f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZoZEazVnhM9UzrkgLuTMxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本加速器图。图片作者。</p></figure><h1 id="9584" class="nm la it bd lb nn no np le nq nr ns lh jz nt ka ll kc nu kd lp kf nv kg lt nw bi translated">动手！</h1><p id="6330" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">您想尝试脉动阵列模拟器，尝试不同大小的暂存区和处理元件阵列尺寸，并分析整个神经网络架构的执行情况吗？查看<a class="ae ky" href="https://github.com/scalesim-project/scale-sim-v2" rel="noopener ugc nofollow" target="_blank"> Scale-SIM </a> [2】，这是一个用于脉动阵列的Python模拟器，您可以在其中探索卷积或全连接层等工作负载的内部内存大小和脉动阵列尺寸之间的权衡。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="98b2" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">欢迎在<a class="ae ky" href="https://twitter.com/PecciaF" rel="noopener ugc nofollow" target="_blank">推特</a>或<a class="ae ky" href="https://www.linkedin.com/in/fpecc/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，让我知道你对这篇文章的看法，或者<a class="ae ky" href="https://www.buymeacoffee.com/pecciaf" rel="noopener ugc nofollow" target="_blank">给我买杯咖啡</a>如果你真的喜欢它！</p><p id="cf50" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">感谢阅读！</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="b15a" class="nm la it bd lb nn oe np le nq of ns lh jz og ka ll kc oh kd lp kf oi kg lt nw bi translated">参考</h1><p id="7dbe" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">如果您对本文中使用的论文和工具感到好奇，或者您想了解更多关于脉动阵列的信息，我建议您看看这些论文和视频:</p><p id="c59b" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">脉动阵列的设计(手动或自动):</p><ul class=""><li id="f966" class="mo mp it lx b ly mq mb mr li ms lm mt lq mu mn mv mw mx my bi translated"><a class="ae ky" href="https://www.cse.wustl.edu/~roger/560M.f17/01653825.pdf" rel="noopener ugc nofollow" target="_blank"> Kung，“为什么选择收缩期架构？”1982.</a></li><li id="5c2f" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="http://ceca.pku.edu.cn/media/lw/6c22198b68248a761d8d8469080b48f1.pdf" rel="noopener ugc nofollow" target="_blank"> X .魏，“fpgas上面向高通量cnn推理的自动化脉动阵列架构综合”2017。</a></li><li id="3cee" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="https://www.researchgate.net/publication/2280595_Advanced_Systolic_Design" rel="noopener ugc nofollow" target="_blank"> D. Lavenier，“高级心脏收缩设计”，2018年。</a></li><li id="99a9" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="https://hparch.gatech.edu/papers/bahar_2020_meissa.pdf" rel="noopener ugc nofollow" target="_blank"> B. Asgari，“Meissa:在可扩展的心脏收缩架构中高效乘法矩阵”，2020年。</a></li><li id="9a4e" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="https://cadlab.cs.ucla.edu/~jaywang/papers/fpga21-autosa.pdf" rel="noopener ugc nofollow" target="_blank">王，“Autosa:一种基于fpga的高性能脉动阵列多面体编译器”，2021。</a></li></ul><p id="aa24" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">实施:</p><ul class=""><li id="03ec" class="mo mp it lx b ly mq mb mr li ms lm mt lq mu mn mv mw mx my bi translated"><a class="ae ky" href="https://arxiv.org/abs/1704.04760v1" rel="noopener ugc nofollow" target="_blank"> N. P. Jouppi，“张量<br/>处理单元的数据中心内性能分析”2017。</a></li><li id="4753" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="https://arxiv.org/abs/1911.09925" rel="noopener ugc nofollow" target="_blank"> H .根茨，“Gemmini:通过全栈集成实现系统化深度学习架构评估”，2019年。</a></li><li id="1cf3" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated">刘志国，“收缩张量阵列:一种用于移动cnn推理的高效结构稀疏gemm加速器”，2020。</li></ul><p id="562d" class="pw-post-body-paragraph lv lw it lx b ly mq ju ma mb mr jx md li ne mf mg lm nf mi mj lq ng ml mm mn im bi translated">视频:</p><ul class=""><li id="5b30" class="mo mp it lx b ly mq mb mr li ms lm mt lq mu mn mv mw mx my bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=eQeU9R8_qGQ" rel="noopener ugc nofollow" target="_blank"> A. Samajdar，“使用SCALE-Sim表征DNN加速器可扩展性的系统方法”，2020年。</a></li><li id="a6ad" class="mo mp it lx b ly mz mb na li nb lm nc lq nd mn mv mw mx my bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=1SSqV7Y75oU" rel="noopener ugc nofollow" target="_blank"> O .穆特鲁，“数字设计&amp;计算机拱门。—第19讲:VLIW和脉动阵列架构”2022。</a></li></ul></div></div>    
</body>
</html>