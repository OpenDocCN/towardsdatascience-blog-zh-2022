<html>
<head>
<title>How to Fine-tune Stable Diffusion using Dreambooth</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 Dreambooth 微调稳定扩散</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-stable-diffusion-using-dreambooth-dfa6694524ae#2022-11-15">https://towardsdatascience.com/how-to-fine-tune-stable-diffusion-using-dreambooth-dfa6694524ae#2022-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0873" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">带有自定义样式或对象的个性化生成图像</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a70096d90d81ceb6688ea465101c7170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hk7CmXXBxIGyZwNrzTanmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d5f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之前，我已经写过一篇关于使用文本反转对稳定扩散进行微调的文章。本教程重点介绍如何使用另一种叫做<a class="ae lu" href="https://dreambooth.github.io/" rel="noopener ugc nofollow" target="_blank"> Dreambooth </a>的方法来微调稳定扩散。与只训练嵌入而不修改基本模型的文本反转方法不同，Dreambooth 微调整个文本到图像模型，以便它学习将唯一标识符与特定概念(对象或样式)绑定。因此，与文本反转相比，生成的图像对于对象或风格来说更加个性化。</p><p id="7c03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程基于 HuggingFace 的 Dreambooth 实现的<a class="ae lu" href="https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth" rel="noopener ugc nofollow" target="_blank">分叉版本</a>。最初的实现需要大约 16GB 到 24GB 来微调模型。维护者<a class="ae lu" href="https://github.com/ShivamShrirao" rel="noopener ugc nofollow" target="_blank"> ShivamShrirao </a>优化了代码，将 VRAM 的使用减少到 16GB 以下。根据您的需求和设置，您可以使用 10GB 至 16GB 的 GPU 对模型进行微调。我亲自测试了在特斯拉 T4 GPU 上的训练是可行的。</p><blockquote class="lv lw lx"><p id="20e7" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">请注意，所有现有的实现都不是由 Dreambooth 的原作者实现的。因此，在再现性方面可能会有细微的差别。</p></blockquote><p id="bc11" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们继续下一部分来设置所有必要的模块。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="6038" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">设置</h1><p id="130d" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">建议在继续安装之前创建一个新的虚拟环境。</p><h2 id="aba8" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">Python 包</h2><p id="a827" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在您的工作目录中，使用以下代码创建一个名为<code class="fe ns nt nu nv b">requirements.txt</code>的新文件:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="d6a6" class="oa mk it nv b be ob oc l od oe">accelerate==0.12.0<br/>torchvision<br/>transformers&gt;=4.21.0<br/>ftfy<br/>tensorboard<br/>modelcards</span></pre><p id="e84f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">激活您的虚拟环境，并逐一运行以下命令来安装所有必需的模块:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="035d" class="oa mk it nv b be ob oc l of oe">pip install git+https://github.com/ShivamShrirao/diffusers.git<br/>pip install -r requirements.txt</span></pre><blockquote class="lv lw lx"><p id="7e10" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">注意:你需要使用上面的网址安装<code class="fe ns nt nu nv b"><em class="it">diffusers</em></code>，而不是直接从<code class="fe ns nt nu nv b"><em class="it">pypi</em></code>安装。</p></blockquote><h2 id="33d9" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">bitsandbytes 包</h2><p id="40a9" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">有一个名为<code class="fe ns nt nu nv b">bitsandbytes</code>的可选包，可以进一步减少 VRAM 的使用。但是，它仅支持 CUDA 版本 10.2–11.7，并且您的机器必须满足以下要求:</p><ul class=""><li id="a7a5" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><code class="fe ns nt nu nv b">LLM.int8()</code>:英伟达图灵(RTX 20xx；T4)或安培 GPU(RTX 30xx；a4-A100)；(2018 年或更老的一款 GPU)。</li><li id="4d59" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">8-bit optimizers and quantization</code>:英伟达 Maxwell GPU 或更新(&gt; =GTX 9XX)。</li></ul><p id="b784" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以按如下方式安装它:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="14d6" class="oa mk it nv b be ob oc l of oe">pip install bitsandbytes</span></pre><h2 id="84b2" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">变压器包</h2><p id="97f4" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">对于 GPU 小于 24GB 的用户，您需要安装<code class="fe ns nt nu nv b">xformers</code>包来进一步减少 VRAM 的使用。在撰写本文时，安装<code class="fe ns nt nu nv b">xformers</code>并不是那么简单，因为缺乏开发者的支持。</p><p id="c43c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以按如下方式安装软件包:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="3b7d" class="oa mk it nv b be ob oc l of oe">pip install xformers</span></pre><p id="907d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您在使用上面的命令时遇到错误，请运行以下命令直接从存储库构建软件包:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="4ff5" class="oa mk it nv b be ob oc l of oe">pip install git+https://github.com/facebookresearch/xformers.git@main#egg=xformers</span></pre><blockquote class="lv lw lx"><p id="c9c5" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">如果你有 CUDA 版本的问题，确保你安装了与你的机器兼容的 CUDA 的最新版本。遵循<a class="ae lu" href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=deb_local" rel="noopener ugc nofollow" target="_blank">跟随链接</a>的指示。</p></blockquote><h2 id="ab37" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">加速设置</h2><p id="920c" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">下一步是初始化一个<code class="fe ns nt nu nv b">Accelerate</code>环境。运行以下命令:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="69ff" class="oa mk it nv b be ob oc l of oe">accelerate config</span></pre><p id="670a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">终端会有多个提示。结合自己的用例来回答。看看下面的例子作为参考:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="f249" class="oa mk it nv b be ob oc l of oe">In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0<br/>Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): 0<br/>Do you want to run your training on CPU only (even if a GPU is available)? [yes/NO]:no<br/>Do you want to use DeepSpeed? [yes/NO]: no<br/>Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: fp16</span></pre><h2 id="79bc" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">拥抱脸的模型</h2><p id="e064" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">如果你已经有了稳定扩散的<code class="fe ns nt nu nv b">diffusers</code>模型(v1.4/v1.5)，可以跳过这一节。对于那些</p><blockquote class="lv lw lx"><p id="1193" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">你必须使用<code class="fe ns nt nu nv b">diffusers</code>模型而不是<code class="fe ns nt nu nv b">ckpt</code>文件进行微调。您可以使用<a class="ae lu" href="https://github.com/huggingface/diffusers/blob/main/scripts/convert_original_stable_diffusion_to_diffusers.py" rel="noopener ugc nofollow" target="_blank">下面的脚本</a>将您的<code class="fe ns nt nu nv b">ckpt</code>文件转换为<code class="fe ns nt nu nv b">diffusers</code>模型。</p></blockquote><p id="13d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 HuggingFace 中注册一个新帐户，并在下载或使用重量之前接受模型许可。</p><ul class=""><li id="550c" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><a class="ae lu" href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="noopener ugc nofollow" target="_blank"> v1.4 </a></li><li id="734f" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><a class="ae lu" href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="noopener ugc nofollow" target="_blank"> v1.5 </a></li></ul><p id="118b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦完成，请参考文档的<a class="ae lu" href="https://huggingface.co/docs/hub/security-tokens" rel="noopener ugc nofollow" target="_blank">这一部分来启用访问令牌。</a></p><p id="6ede" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">运行以下命令并传递您的令牌进行身份验证:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="5175" class="oa mk it nv b be ob oc l of oe">huggingface-cli login</span></pre><p id="6ff7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它会在初次运行时将权重下载到缓存文件夹中。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="68ad" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据集</h1><p id="a1f1" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">您需要收集高质量的数据集，以获得一致和良好的结果。训练图像应该与预期的输出相匹配，并且分辨率调整为 512 x 512。</p><p id="ad56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，运动模糊或低分辨率等伪像会影响生成的图像。这适用于训练数据集中任何不需要的文本、水印或图标。请务必注意您用于培训的数据集。</p><p id="e56f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据您的使用情况，您可以使用以下准则:</p><h2 id="32a0" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">目标</h2><p id="422d" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">使用带有普通背景的物体图像。透明背景可能会在对象周围留下边缘或边框。所有训练图像应仅聚焦于物体，并在以下方面有所变化:</p><ul class=""><li id="b109" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated">照像镜头视角</li><li id="e4d6" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">姿态</li><li id="a4a3" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">道具(服装、发型等。)</li><li id="e7f7" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">背景(在不同地点拍摄)</li></ul><p id="098e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练图像的数量应该在 5 到 20 左右。您可能需要裁剪图像，以便只关注对象。</p><h2 id="fa3d" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">风格</h2><p id="9241" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">使用你喜欢的风格的图片。可以是你自己的艺术收藏，也可以是风格一致的公共电影/动画/电视剧。所有的训练图像应该集中在风格上，而不是一个特定的对象。</p><p id="a402" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更好地概括它，您应该确保同一个对象不会在训练图像中出现超过一次(每个角色一个)。如果你的目标是生成同一风格的不同角色。包括带有字符的训练图像。否则，在训练数据集中包括风景、物体和其他相关图像。</p><h2 id="aec8" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">训练图像</h2><p id="13b5" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在本教程中，我将使用以下训练图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/96fd9a47748388509d5e54a8822e7ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnzfiFvLhD_WjqRpjhfCTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a76e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程对训练数据集使用以下术语。</p><ul class=""><li id="5fcd" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><code class="fe ns nt nu nv b">Instance images </code> —代表 dreambooth 培训特定概念的自定义图像。您应该根据您的用例收集高质量的图像。</li><li id="b405" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">Class images </code> —正则化先前保存损失的图像，以防止过拟合。您应该直接从基础预训练模型生成这些图像。您可以选择自己生成它们，或者在运行培训脚本时动态生成它们。</li></ul></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="9142" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">培养</h1><p id="8be8" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">前往下面的 Github 库中的<a class="ae lu" href="https://github.com/ShivamShrirao/diffusers/tree/main/examples" rel="noopener ugc nofollow" target="_blank">并将<code class="fe ns nt nu nv b"><a class="ae lu" href="https://github.com/ShivamShrirao/diffusers/blob/main/examples/dreambooth/train_dreambooth.py" rel="noopener ugc nofollow" target="_blank">train_dreambooth.py</a></code>文件下载到您的工作目录中。</a></p><h2 id="7f4a" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">训练命令</h2><p id="6189" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">以下是应根据您的使用情况进行修改的常见参数列表:</p><ul class=""><li id="cb49" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><code class="fe ns nt nu nv b">pretrained_model_name_or_path</code> —从 huggingface.co/models 到预训练模型或模型标识符的路径</li><li id="9447" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">pretrained_vae_name_or_path</code> —从 huggingface.co/models.到预训练 vae 或 vae 标识符的路径您可以微调带或不带 vae 的模型</li><li id="c4e1" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">instance_data_dir</code> —包含实例图像训练数据的文件夹</li><li id="02ff" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">class_data_dir</code> —包含类别图像的训练数据的文件夹</li><li id="2048" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">instance_prompt </code> —带有指定实例的标识符的提示</li><li id="bf8b" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">class_prompt</code> —提示指定与提供的实例图像在同一类中的图像</li><li id="eb15" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">num_class_images</code> —先前保存损失的最小类别图像</li><li id="03fd" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">output_dir</code>-将写入模型预测和检查点的输出目录</li><li id="dc87" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">max_train_steps</code> —要执行的训练步骤总数。建议设置为<code class="fe ns nt nu nv b">N * 100</code>，其中<code class="fe ns nt nu nv b">N</code>代表实例图像的数量。</li><li id="a962" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">learning_rate </code> —要使用的初始学习率(在潜在的预热期之后)</li><li id="5adf" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">lr_scheduler</code> —要使用的调度程序类型。在[ <code class="fe ns nt nu nv b">linear</code>、<code class="fe ns nt nu nv b">cosine</code>、<code class="fe ns nt nu nv b">cosine_with_restarts</code>、<code class="fe ns nt nu nv b">polynomial</code>、<code class="fe ns nt nu nv b">constant</code>、<code class="fe ns nt nu nv b">constant_with_warmup</code>之间选择</li><li id="5c82" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">lr_warmup_steps</code>—lr 调度程序中预热的步骤数。使用<code class="fe ns nt nu nv b">polynomial</code>时使用<code class="fe ns nt nu nv b">max_train_steps / 10</code>或使用<code class="fe ns nt nu nv b">constant</code>时使用<code class="fe ns nt nu nv b">0</code>。</li><li id="29b1" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><code class="fe ns nt nu nv b">save_interval </code> —每 N 步保存一次重量。确保你有足够的存储空间。每个重量在 4GB 左右。</li></ul><p id="9014" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以使用以下标志设置自定义值:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="4f6e" class="oa mk it nv b be ob oc l of oe">--pretrained_vae_name_or_path="stabilityai/sd-vae-ft-mse"</span></pre><p id="3e40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用先前保存损失进行训练有助于防止过度拟合。按如下方式启用它:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="45f5" class="oa mk it nv b be ob oc l of oe">--with_prior_preservation --prior_loss_weight=1.0</span></pre><p id="c690" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，您可以随<code class="fe ns nt nu nv b">unet</code>一起微调<code class="fe ns nt nu nv b">text_encoder</code>。然而，这将大大增加 VRAM 的使用。用以下标志设置它:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="1993" class="oa mk it nv b be ob oc l of oe">--train_text_encoder</span></pre><p id="7c27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于对象训练，可以用下面的例子作为<code class="fe ns nt nu nv b">instance_prompt</code>和<code class="fe ns nt nu nv b">class_prompt</code>的参考。请根据您的用例随意试验不同的字符串。</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="7737" class="oa mk it nv b be ob oc l of oe"># Woman<br/>--instance_prompt="photo of zwx woman" \<br/>--class_prompt="photo of a woman" \<br/><br/># Black man<br/>--instance_prompt="photo of zwx black man" \<br/>--class_prompt="photo of a black man" \<br/><br/># Dog<br/>--instance_prompt="photo of zwx dog" \<br/>--class_prompt="photo of a dog" \</span></pre><blockquote class="lv lw lx"><p id="1e41" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">您可以使用自己的自定义字符串作为唯一标识符。在早期的实现中，大多数例子使用<code class="fe ns nt nu nv b">sks</code>作为惟一标识符。然而，<code class="fe ns nt nu nv b">sks</code>是一种半自动步枪的已知代币。强烈建议使用不同的唯一标识符，该标识符不是原始稳定扩散数据集中使用的令牌的一部分。</p></blockquote><p id="1eae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，唯一标识符不限于单个字符串。一些用户报告了良好的表现，并提示进行以下风格训练:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="9798" class="oa mk it nv b be ob oc l of oe"># Style 1<br/>--instance_prompt="modern disney style" \<br/>--class_prompt="artwork style" \<br/><br/># Style 2<br/>--instance_prompt="classic animation style" \<br/>--class_prompt="illustration style" \</span></pre><p id="cec5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于学习率和调度器，敬请参考 HuggingFace 的<a class="ae lu" href="https://huggingface.co/blog/dreambooth" rel="noopener ugc nofollow" target="_blank">以下博客</a>。</p><h2 id="097f" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">培训示例</h2><p id="e6a6" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">看一下上面训练图像的示例训练命令(在 16GB 内存的特斯拉 T4 上测试):</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="26ba" class="oa mk it nv b be ob oc l of oe">accelerate launch train_dreambooth.py \<br/>  --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \<br/>  --pretrained_vae_name_or_path="stabilityai/sd-vae-ft-mse" \<br/>  --instance_data_dir="./instance-images/" \<br/>  --class_data_dir="./class-images/" \<br/>  --output_dir="./output-models/" \<br/>  --with_prior_preservation --prior_loss_weight=1.0 \<br/>  --instance_prompt="photo of zwx bear toy" \<br/>  --class_prompt="photo of bear toy" \<br/>  --resolution=512 \<br/>  --train_batch_size=1 \<br/>  --train_text_encoder \<br/>  --mixed_precision="fp16" \<br/>  --use_8bit_adam \<br/>  --gradient_accumulation_steps=1 \<br/>  --gradient_checkpointing \<br/>  --learning_rate=1e-6 \<br/>  --lr_scheduler="constant" \<br/>  --lr_warmup_steps=200 \<br/>  --num_class_images=300 \<br/>  --max_train_steps=2000 \<br/>  --save_interval=500</span></pre><blockquote class="lv lw lx"><p id="6c75" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">根据您的工作目录相应地修改数据目录。</p></blockquote><p id="5a98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您的内存不足，请关闭<code class="fe ns nt nu nv b">text_encoder</code>训练:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="ac58" class="oa mk it nv b be ob oc l of oe">accelerate launch train_dreambooth.py \<br/>  --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \<br/>  --instance_data_dir="./instance-images/" \<br/>  --class_data_dir="./class-images/" \<br/>  --output_dir="./output-models/" \<br/>  --with_prior_preservation --prior_loss_weight=1.0 \<br/>  --instance_prompt="photo of zwx bear toy" \<br/>  --class_prompt="photo of bear toy" \<br/>  --resolution=512 \<br/>  --train_batch_size=1 \<br/>  --mixed_precision="fp16" \<br/>  --use_8bit_adam \<br/>  --gradient_accumulation_steps=1 \<br/>  --gradient_checkpointing \<br/>  --learning_rate=1e-6 \<br/>  --lr_scheduler="constant" \<br/>  --lr_warmup_steps=0 \<br/>  --num_class_images=300 \<br/>  --max_train_steps=2000 \<br/>  --save_interval=500</span></pre><blockquote class="lv lw lx"><p id="7169" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">请确保不要过度训练您的模型，因为 Dreambooth 方法往往会很快过度拟合。如果您的模型不能很好地概括您的提示，或者上面有工件，这很可能意味着您过度训练了您的模型。请减少训练步数或使用较低的学习率进行较高的步数训练。</p></blockquote><p id="f0ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而且，最新的训练脚本接受了一个名为<code class="fe ns nt nu nv b">concept_list</code>的新参数。它表示包含字典列表的 JSON 文件的路径。它将覆盖<code class="fe ns nt nu nv b">instance_prompt</code>、<code class="fe ns nt nu nv b">class_prompt</code>等参数。您可以使用它将多个概念同时训练到一个模型中。例如，给出下面的<code class="fe ns nt nu nv b">concept_list.json</code>文件:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="1cea" class="oa mk it nv b be ob oc l of oe">[<br/>    {<br/>        "instance_prompt":      "photo of zwx dog",<br/>        "class_prompt":         "photo of a dog",<br/>        "instance_data_dir":    "./instance-images/",<br/>        "class_data_dir":       "./class-images/"<br/>    }<br/>]</span></pre><p id="99bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在 training 命令中使用以下参数:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="8db7" class="oa mk it nv b be ob oc l of oe">--concepts_list ./concepts_list.json</span></pre><p id="4f7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只需在列表中添加一个新词典，同时训练另一个概念。</p><p id="6b56" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您第一次运行它时，它将生成类图像。您可以在后续培训中重复使用相同的课程图像，只要您引用的是相同的概念。只需将<code class="fe ns nt nu nv b">class_data_dir</code>设置为与您之前的培训相同的目录。因此，在为特定风格进行训练时，您可以重用大多数生成的类图像。</p><p id="9dd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是为本教程生成的一些类图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/8ce0e607b44e5ea6e2225340ea9c694d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OT3H65K7Bwl4QbB4yXpMBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="a9d5" class="ng mk it bd ml nh ni dn mp nj nk dp mt lh nl nm mv ll nn no mx lp np nq mz nr bi translated">培训产出</h2><p id="5cd1" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">该脚本将根据<code class="fe ns nt nu nv b"> save_interval</code>的值在每个间隔上保存一个新的权重。在每个新生成的权重文件夹中，应该有以下文件和文件夹:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="5bf3" class="oa mk it nv b be ob oc l of oe">|- feature_extractor<br/>|  |- preprocessor_config.json<br/>|- scheduler<br/>|  |- scheduler_config.json<br/>|- text_encoder<br/>|  |- config.json<br/>|  |- pytorch_model.bin<br/>|- tokenizer<br/>|  |- merges.txt<br/>|  |- special_tokens_map.json<br/>|  |- tokenizer_config.json<br/>|  |- vocab.json<br/>|- unet<br/>|  |- config.json<br/>|  |- diffusion_pytorch_model.bin<br/>|- vae<br/>|  |- config.json<br/>|  |- diffusion_pytorch_model.bin<br/>|- args.json<br/>|- model_index.json</span></pre></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="f7fb" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">推理</h1><p id="1377" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">现在，在您的工作目录中创建一个名为<code class="fe ns nt nu nv b">inference.py</code>的新 Python 文件。在其中追加以下代码:</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="2050" class="oa mk it nv b be ob oc l of oe">from diffusers import StableDiffusionPipeline, DDIMScheduler<br/>import torch<br/><br/>device = "cuda"<br/># use DDIM scheduler, you can modify it to use other scheduler<br/>scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=True)<br/><br/># modify the model path<br/>pipe = StableDiffusionPipeline.from_pretrained(<br/>    f"./output-models/1500/",<br/>    scheduler=scheduler,<br/>    safety_checker=None,<br/>    torch_dtype=torch.float16,<br/>).to(device)<br/><br/># enable xformers memory attention<br/>pipe.enable_xformers_memory_efficient_attention()<br/><br/>prompt = "photo of zwx bear toy"<br/>negative_prompt = ""<br/>num_samples = 4<br/>guidance_scale = 7.5<br/>num_inference_steps = 50<br/>height = 512<br/>width = 512<br/><br/>images = pipe(<br/>    prompt,<br/>    height=height,<br/>    width=width,<br/>    negative_prompt=negative_prompt,<br/>    num_images_per_prompt=num_samples,<br/>    num_inference_steps=num_inference_steps,<br/>    guidance_scale=guidance_scale<br/>).images<br/><br/>count = 1<br/>for image in images:<br/>    # save image to local directory<br/>    image.save(f"img-{count}.png")<br/>    count += 1</span></pre><blockquote class="lv lw lx"><p id="47a1" class="ky kz ly la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">一些用户报告称，使用<code class="fe ns nt nu nv b">xformers</code>生成图像会导致不确定的结果。这意味着您不能使用相同的设置(种子、图像大小等)复制相同的图像。).请自行试验，并根据您的用例相应地修改代码。</p></blockquote><p id="e595" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成后，运行下面的命令，使用新调整的模型生成图像。</p><pre class="kj kk kl km gt nw nv nx bn ny nz bi"><span id="44b8" class="oa mk it nv b be ob oc l of oe">python inference.py</span></pre><p id="ebab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是一些示例输出(没有包括提示，因为我犯了一个错误，最初没有跟踪它们):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/194b1a649adf5d3f58316272ef9da8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRjqPcehZH_MNDUQmvtfgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="688c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑一下我关于训练有条件/无条件图像生成模型的其他文章:</p><ul class=""><li id="83ea" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><a class="ae lu" href="https://medium.com/towards-data-science/how-to-fine-tune-stable-diffusion-using-textual-inversion-b995d7ecc095" rel="noopener">如何使用文本反转微调稳定扩散</a></li><li id="8702" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><a class="ae lu" href="https://betterprogramming.pub/beginners-guide-to-unconditional-image-generation-using-diffusers-c703e675bda8" rel="noopener ugc nofollow" target="_blank">使用扩散器无条件生成图像的初学者指南</a></li><li id="f3d4" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><a class="ae lu" href="https://medium.com/p/85690292c6a8" rel="noopener">如何使用 LoRA 微调稳定扩散</a></li></ul></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="131c" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="020a" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">Dreambooth 是一种用特定概念(对象或样式)微调稳定扩散模型的好技术。</p><p id="6c86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着人工智能研究和开发的进步，现在普通人可以微调他们自己的定制模型。然而，对于艺术行业来说，这可能是一把机遇和挑战并存的双刃剑。</p><p id="9a9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你是艺术家行业的一员，建议你接受并使用这项技术来简化你的工作流程。此外，如果每个人都能明智地使用这项技术来造福人类，那就太好了。</p><p id="a1ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，目前我没有足够的内存来使用图像和标题对整个模型进行微调。</p><p id="6d93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢你阅读这篇文章。祝你有美好的一天！</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="ffa3" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><ol class=""><li id="3a1b" class="og oh it la b lb nb le nc lh ox ll oy lp oz lt pa om on oo bi translated"><a class="ae lu" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">抱紧面部扩散器 Github </a></li><li id="347a" class="og oh it la b lb op le oq lh or ll os lp ot lt pa om on oo bi translated"><a class="ae lu" href="https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth" rel="noopener ugc nofollow" target="_blank">shivamshriao 的 Dreambooth Github </a></li></ol></div></div>    
</body>
</html>