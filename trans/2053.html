<html>
<head>
<title>Support Vector Machines (SVMs): Important Derivations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机:重要的推导</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-svms-important-derivations-4f50d1e3d4d2#2022-05-09">https://towardsdatascience.com/support-vector-machines-svms-important-derivations-4f50d1e3d4d2#2022-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="06ad" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习理论</h2><div class=""/><div class=""><h2 id="019f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">SVM理论的全面阐释和形象化</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/3d4acaa2589e2400e90d912028572da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*Wf4DxRVv3P0hPexw2cjWQA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="134c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi lz translated">这里是每个数据科学家在应用特定的机器学习模型之前应该知道的一些推导。在这篇文章中，我将介绍所有你需要了解的关于支持向量机的知识。</p><h1 id="e8b7" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">介绍</h1><p id="c0df" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">由于内核方法的易用性、可解释性以及在各种应用中的强大性能，它们在机器学习世界中非常流行。支持向量机(SVMs)在处理高维数据时尤其强大，因为它们使用了核，并且它们能够很好地防止过拟合，因为它们通过选择最大可能的余量来调整自己。在本文中，我将向您展示支持向量机背后的理论，以及您需要理解的重要推导，以便有效地应用它们。</p><h1 id="5ce9" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">支持向量机和机器学习</h1><p id="6ea0" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">机器学习分类器可以分为许多类别。两种流行的分类器是:</p><ul class=""><li id="a313" class="nf ng it lf b lg lh lj lk lm nh lq ni lu nj ly nk nl nm nn bi translated">后验概率的估计量</li><li id="1ce4" class="nf ng it lf b lg no lj np lm nq lq nr lu ns ly nk nl nm nn bi translated">决策边界的直接估计量</li></ul><p id="27d9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">估计后验概率的模型例如是朴素贝叶斯分类器、逻辑回归或者甚至是神经网络。这些模型试图重建一个后验概率函数。然后可以评估后验概率的近似函数，以确定样本属于每个类别的概率，然后做出决定。</p><p id="1a54" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">决策边界的直接估计器，例如感知器和支持向量机(SVMs ),并不试图学习概率函数，相反，它们学习“线”或高维超平面，这可用于确定每个样本的类别。如果一个样本在超平面的一边，它属于一个类，否则，它属于另一边。</p><p id="0ed8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这两种方法是根本不同的，它们会影响分类器的结果。</p><h1 id="f5bf" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">核心方法</h1><p id="1bb1" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">支持向量机是一种核心方法。核方法使用核将输入数据空间映射到更高维度的空间，在该空间中数据被假设为线性可分的。在这个新的空间中，线性分类器被训练，数据被标记。</p><h2 id="ec99" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">但是内核是什么呢？</h2><p id="3e93" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">同样，矩阵是向量空间的线性映射，核是函数的线性映射。它们允许我们将输入数据映射到一个不同的空间，在这个空间中对数据进行分类可能会更容易。</p><p id="955e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">对于支持向量机，核是半正定的是很重要的。这与内核技巧有关，你可能以前听说过。如果我们选择一个半正定核，那么将存在一个函数使得核等于扩展特征空间的内积。</p><p id="e8ca" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这意味着我们可以评估扩展特征空间的内积，而无需直接计算它，从而节省大量时间，并允许我们在数据分类中使用非常高维的空间。</p><h1 id="a105" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">支持向量机如何学习？</h1><p id="0f53" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">在这一部分，我将讨论支持向量机通常是如何学习的。稍后，我将通过数学来了解他们为什么以这种方式学习。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi oe"><img src="../Images/d4164922f532cfe0f3f48776e374426d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*_vdQGC2L6sJWfNVtHxyHWQ.gif"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">SVM的Gif在二元分类中寻找最优决策边界(GIF由作者提供)</p></figure><p id="35e1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">支持向量机就是定义一个决策边界。决策边界是一个超平面(或者是一条线，如果是二维的话),其中这条线根据数据在哪一边来决定数据属于哪一类。</p><p id="235f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了找到最佳线，支持向量机优化以下标准:</p><p id="1509" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们希望找到一个决策边界，该边界最大化从最近点到每个类的边界的距离。</p><p id="d6b4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我来解释一下；首先取一个初始边界，从两个类中找出离它最近的点。离决策边界最近的点称为支持向量，它们是唯一影响决策边界的点。一旦我们有了支持向量，我们就能找到离这些支持向量最远的线。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/3d4acaa2589e2400e90d912028572da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*Wf4DxRVv3P0hPexw2cjWQA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="43e3" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">上面是支持向量、边界和决策边界的图示。边距是决策边界和最远的支持向量之间的空间。当数据不是线性可分时，它们特别有用。</p><h1 id="62b3" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">数学上表达上述内容</h1><p id="450f" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">可悲的是，我们不能只要求计算机找到符合上述标准的线。为了实现支持向量机，我们需要用数学方法表达优化问题，然后求解它。我会一步一步来。</p><h2 id="97a3" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">点到直线的距离</h2><p id="1da2" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">你们可能从小学就记得，给定一条线的方程，代入线以上任意一点的坐标都等于一个大于零的值，线以下任意一点都等于一个小于零的值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi ok"><img src="../Images/9406a4399e55a4a5d3a5f37dd5cf95fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vqvTk6Bjri1yip2XaIyWw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">根据直线方程的空间符号(图片由作者提供)</p></figure><p id="6d78" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以用向量的形式表示直线的相同方程，并且作为我们的核ϕ.的函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi ol"><img src="../Images/65e114bd5f657526b258f8f97f0c1e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aEbR134xW8vlyQKW8weJAA.png"/></div></div></figure><p id="bf0f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在上面的方程中，第一个是和上面图像中的直线相同的方程，第二个是矢量形式的相同方程。第三，可以使用核ϕ(x).将输入数据空间(x)映射到特征空间</p><p id="aecc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">第三个等式将我们的决策边界y(x)表示为权重和偏差(w，b)以及数据ϕ(x).的特征空间的函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi om"><img src="../Images/5c3a7b91ab248ac82894b00d0a5370d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13dKvNjHy1rGrMSCfcci9g.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">点到一条线的距离(图片由作者提供)</p></figure><p id="6d26" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">点和线之间的距离可以通过上图中的表达式来计算。我已经包括了向量形式的版本和你们在学校会遇到的版本。</p><p id="807f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于该点在线的下方，距离将为负，如果该点在线的上方，距离将为正。</p><h2 id="eaf8" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">问题设置</h2><p id="b4de" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">让我们回到支持向量机的分类问题。</p><p id="62ef" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，我们将决策边界y(x)定义为权重和偏差(w，b)、核函数(ϕ)和数据(x)的函数。</p><p id="5df2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在任何监督分类问题中，我们被给定数据(x)和数据的标签(t)。对于二元问题，标签要么是1，要么是-1。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/42ff034b92900720ae30dc43f7f20a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*dN9Y1Y8Eg6oQapq1t71QdQ.png"/></div></figure><p id="4828" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">目标是找到权重和偏差(w，b ),其中一个类的y(x) &gt; 0，另一个类的y(x) &lt; 0。还记得我之前展示的图片吗，这意味着每个类都在决策边界的两边。</p><h2 id="3111" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">目标函数</h2><p id="b525" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">这最终将我们引向目标函数。这个函数在求解时会产生我们决策边界的权重和偏差(w，b)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9d084399c093056703691af332491153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*lRwJZzb3mzkRnlkQ5tXh_g.png"/></div></figure><p id="0066" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先看看最小化的内部。我们正在寻找使数据点(x n)和决策边界y(Xn)之间的距离最小化的索引n。</p><p id="bb7d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">唯一的区别是我们乘以了类成员tn。当距离为正时，类成员为1，当距离为负时，类成员为-1。乘以类别巧妙地将距离的符号在分类正确时转换为正，在分类错误时转换为负。</p><p id="ac73" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">最大化找到其决策边界最大化决策边界和最近点之间的距离的权重和偏差(w，b)。</p><h2 id="0e31" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">解决优化问题</h2><p id="cee3" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">为了找到权重和偏差(w，b ),我们需要解决我上面展示的优化问题。</p><p id="8d29" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">直接解决上面的问题会非常困难。相反，我们可以应用一些约束，并创建一个更容易解决的等价优化问题。</p><p id="fceb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">添加的约束只是将距离决策边界最近的点的线的评估值缩放为等于1。因此，对于数据中的任何其他点，它将大于或等于1:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0ea9c1d0e1b52fd17fe163d83935ca3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*lakDOZNmClghSX_ShDT5vA.png"/></div></figure><p id="4e46" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">将约束插回到我们的目标函数中，优化问题简化为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/195c7c90d4657bf14d7e779c22f1ee63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*91vEahIhcBc39LVk1nQlKg.png"/></div></figure><p id="41dc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">n上的最小化消失了，因为w不依赖于n。</p><p id="2344" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个新的最优化问题是可解的。这是一个带有线性不等式约束的二次优化问题，可以用拉格朗日乘子法求解。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e52489dccc96f7413f6459b819381b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*oSt9VEgrjMy3bjf0FTlVTg.png"/></div></figure><p id="a74b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在拉格朗日乘子法中，我们希望最大化上面的拉格朗日。右手边的第一项是目标函数，我们试图最大化它。第二项是拉格朗日乘数，第三项是由不等式约束导出的误差项。最大化拉格朗日量相当于求解最优化问题。</p><p id="847e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了最大化，我们将表达式对b和w的导数等于0，然后将结果插回。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi os"><img src="../Images/d1110dc9d819fef62a247914d94fc4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EV-pkK-UPo-xRdA4zP1lEA.png"/></div></div></figure><p id="eafc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">把w插回去，我们得到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi ot"><img src="../Images/cc01b06b0d8d311c99cc101134e514bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DARzNaUu_vdZORBqOypQ8A.png"/></div></div></figure><p id="827f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这个拉格朗日是可以求解的。需要一些进一步的约束(拉格朗日乘数应该是非负的，并且每个类别的支持向量应该是相同的)。</p><p id="fe19" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">求解时，你总会发现所有拉格朗日乘子(a)不是0就是1。实际上，除了极少数例外，几乎所有的拉格朗日乘数都是0是可能的。拉格朗日乘数为0的点对决策边界没有影响。唯一对决策边界有影响的点是拉格朗日乘数为1的点(边缘内的点)支持向量。</p><h1 id="0d01" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">软利润SVM</h1><p id="2ca1" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">到目前为止，我们假设数据在特征空间中是线性可分的。然而，很多时候这并不是一个好的假设。为了应对这种情况，可以实施软利润SVM。</p><p id="39fb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">软边界支持向量机允许一些数据存在于边界内，同时应用一个小的惩罚。软边距SVM的推导过程类似，并引入了松弛变量作为边距内点数的惩罚。发送结果也非常相似，有相同的拉格朗日函数，但有不同的约束。</p><p id="b79f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里有一个软保证金SVM结束:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="of og di oh bf oi"><div class="gh gi oe"><img src="../Images/83831385c9b97154b892ffb439d075fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9-hJBcO42jsfy154w7SCHQ.gif"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">软边距SVM (GIF由作者提供)</p></figure><h1 id="dc99" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">结论</h1><p id="782d" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">在本文中，我将介绍支持向量机为什么如此强大，以及它们是如何学习的。支持向量机是一种强核方法，可用于处理高维问题。由于它们的余量，它们也有利于防止过度拟合。与其他核方法类似，支持向量机通过使用数据可线性分离的核将数据转换到更高维的空间。最后，通过所示的推导，我们了解到支持向量机直接估计它们的决策边界，并且仅基于数据中很少的点，其余的数据对决策边界没有任何贡献。理解这一点对于了解支持向量机是否是应用您的特定问题的最佳模型是非常重要的。</p><h2 id="dd35" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">支持我</h2><p id="853b" class="pw-post-body-paragraph ld le it lf b lg na kd li lj nb kg ll lm nc lo lp lq nd ls lt lu ne lw lx ly im bi translated">希望这对你有所帮助，如果你喜欢，你可以<a class="ae ou" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lf jd">关注我！</strong>T3】</a></p><p id="8997" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">你也可以成为<a class="ae ou" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lf jd">中级会员</strong> </a> <strong class="lf jd"> </strong>使用我的推荐链接，获得我所有的文章和更多:<a class="ae ou" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="7fef" class="nt mj it bd mk nu nv dn mo nw nx dp ms lm ny nz mu lq oa ob mw lu oc od my iz bi translated">你可能喜欢的其他文章</h2><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/kernel-methods-a-simple-introduction-4a26dcbe4ebd"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">内核方法:简单介绍</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">核方法和径向基函数的基础</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm kx oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/unsupervised-learning-k-means-clustering-6fd72393573c"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">无监督学习:K-均值聚类</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">k-均值聚类直观解释</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm kx oy"/></div></div></a></div></div></div>    
</body>
</html>