<html>
<head>
<title>Uncertainty in Deep Learning — Brief Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的不确定性——简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04#2022-02-01">https://towardsdatascience.com/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04#2022-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn"><p id="5916" class="jo jp iq bd jq jr js jt ju jv jw jx dk translated">当人类不知道某个特定问题的答案时，他们会说“我不知道”。但这也适用于深度学习模型吗？</p></blockquote><figure class="ka kb kc kd ke kf gh gi paragraph-image"><div class="gh gi jz"><img src="../Images/90a472f744375e71f0692ff095242963.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*034AiDrHemkh44X0iLlH1w.jpeg"/></div><p class="ki kj gj gh gi kk kl bd b be z dk translated"><a class="ae km" href="https://unsplash.com/@two_tees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">迈特·沃尔什</a>在<a class="ae km" href="https://unsplash.com/s/photos/question-mark?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="ca4c" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">在这篇文章中，我将尝试给出深度学习模型中不确定性的直觉，而不是深入解释这些不确定性。在接下来的部分中，我们将使用<strong class="kp ir">张量流概率</strong>对它们进行解释。</p><p id="0f0b" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">我将会有一系列的文章来解释这些术语并展示 TFP 的实现。</p><ul class=""><li id="b7d6" class="lk ll iq kp b kq kr ku kv ky lm lc ln lg lo jx lp lq lr ls bi translated"><strong class="kp ir">第一部分——简介</strong></li><li id="22d8" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><a class="ae km" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第 2 部分——随机不确定性和最大似然估计</a></li><li id="78ed" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><a class="ae km" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第 3 部分——认知不确定性和反向投影贝叶斯</a></li><li id="759d" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><a class="ae km" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6">第 4 部分——实现完全概率贝叶斯 CNN </a></li><li id="a93d" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><a class="ae km" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954">第五部分——贝叶斯 CNN 实验</a></li><li id="24ce" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><a class="ae km" rel="noopener" target="_blank" href="/bayesian-inference-and-transformers-3dc473ac1af2">第 6 部分——贝叶斯推理和变压器</a></li></ul><h1 id="1033" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">第一，什么是不确定性？</h1><p id="4ff3" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">不确定性可以定义为对某事缺乏了解或确定性。这是生活中不可避免的一部分，在自然和人工系统中无处不在。</p><p id="5566" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated"><strong class="kp ir">在深度学习的背景下，有两种主要类型的不确定性:</strong></p><ul class=""><li id="030d" class="lk ll iq kp b kq kr ku kv ky lm lc ln lg lo jx lp lq lr ls bi translated"><strong class="kp ir"> 1)随机不确定性:</strong>这是由于数据的随机性而产生的不确定性。</li><li id="9c44" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated"><strong class="kp ir"> 2)认知不确定性:</strong>这种不确定性是由于缺乏关于模型真实参数的知识。</li></ul><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/9a73875c170ffdcb34b09467ec9ddc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f32jdQyaY9MEpz24PuCqQQ.png"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">预测中不确定性的例子。作者图片</p></figure><p id="7aff" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">当我们训练我们的深度学习模型时，我们采用<strong class="kp ir">最大似然估计(MLE)。</strong></p><p id="7cab" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">简单来说，<strong class="kp ir"> MLE </strong>是从一组数据中估计统计模型参数的方法。这是一种寻找参数值的技术，可以在数据和模型之间产生最佳匹配。</p><p id="b21f" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">换句话说，我们寻找能很好解释数据的权重。或者，根据我掌握的数据，最佳权重是多少？这是一个经典的优化问题，也是一个随机过程。但问题是，是否有多组权重可以很好地解释数据？</p><h1 id="e3bf" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">基本线性回归</h1><p id="7a99" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">首先，我们创建一个带有随机噪声的线性数据。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ba789bed0a9c056bd7014b93314c48ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*3gxcyFAZ_DJiWHrFNZlV6A.png"/></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">图片作者。</p></figure><p id="ae6d" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">根据我们掌握的数据，应该有不同的模型组合。首先让我们定义一个效用函数。该函数将采用 3 个独立的模型，并使用相同的损失和优化器来训练它们。然后，它将打印 15 的预测结果，并绘制线条。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="1812" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">我们实际上不需要<code class="fe nn no np nq b">relu</code>激活，因为数据集是线性的。我在试验一些模型，它们的最终形式有<code class="fe nn no np nq b">relu</code>，所以我最终没有移除它们。</p><p id="a333" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">如你所知，<strong class="kp ir">每个模型都有不同的起点，因为初始化器是不同的</strong>。因此，学习到的权重应该不同，但不会相差太多。</p><p id="a2c6" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">当我们运行上面的脚本时，我们将得到:</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="67c0" class="nv lz iq nq b gy nw nx l ny nz">Model1: Prediction for 15: [[31.22317]]<br/>Model2: Prediction for 15: [[30.969236]]<br/>Model3: Prediction for 15: [[31.227913]]</span></pre><div class="nc nd ne nf gt ab cb"><figure class="oa kf ob oc od oe of paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/65ab698877d004d10ba1481fdc9ba0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*ty5i9Wd05hliTLf7eG4UsA.png"/></div></figure><figure class="oa kf og oc od oe of paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/5ae875d49252b978475d6e793555737c.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*kR055Sk-4EapckXqXFk_wA.png"/></div></figure><figure class="oa kf oh oc od oe of paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><img src="../Images/36b12a13a660cb2d6fcf3a390f60820c.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*LwHpRaXjTSrxZVXxI1VzXA.png"/></div><p class="ki kj gj gh gi kk kl bd b be z dk oi di oj ok translated">作者提供的图片</p></figure></div><p id="1599" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">所有的线看起来都是合理的，尽管学习到的权重是不同的。那是一个<strong class="kp ir">不确定性</strong>，根据起点的不同，有不止一个模型来解释给定的数据。<strong class="kp ir">换句话说，权重是不确定的！</strong></p><p id="adad" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated"><strong class="kp ir">估计的参数主要取决于两个因素:</strong></p><ul class=""><li id="2431" class="lk ll iq kp b kq kr ku kv ky lm lc ln lg lo jx lp lq lr ls bi translated">数据集大小</li><li id="ae9f" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated">梯度下降的起点</li></ul><p id="6fc0" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">正如你可能猜到的，当有更多的数据时，估计的参数会更准确。</p><p id="7fa8" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">如果考虑更大的模型和数据，在每次重新运行(拟合过程)中，大多数情况下结果是相似的，但权重不会完全相同。</p><h1 id="294f" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">基本图像分类和 Softmax</h1><p id="9829" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">这一次，我将展示一个不同的例子，其中的任务是一个多类分类。为此，我将使用<a class="ae km" href="https://www.tensorflow.org/datasets/catalog/fashion_mnist" rel="noopener ugc nofollow" target="_blank"> fashion_mnist </a>作为基本示例。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="2018" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">加载和处理数据后，可以编写如下简单模型:</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="505f" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">模型总结看起来是这样的，没有什么特别的:</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="6022" class="nv lz iq nq b gy nw nx l ny nz">Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> conv2d (Conv2D)             (None, 28, 28, 16)        160       <br/>                                                                 <br/> max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         <br/> )                                                               <br/>                                                                 <br/> conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      <br/>                                                                 <br/> max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         <br/> 2D)                                                             <br/>                                                                 <br/> conv2d_2 (Conv2D)           (None, 7, 7, 64)          18496     <br/>                                                                 <br/> max_pooling2d_2 (MaxPooling  (None, 3, 3, 64)         0         <br/> 2D)                                                             <br/>                                                                 <br/> conv2d_3 (Conv2D)           (None, 3, 3, 128)         73856     <br/>                                                                 <br/> global_max_pooling2d (Globa  (None, 128)              0         <br/> lMaxPooling2D)                                                  <br/>                                                                 <br/> dense_6 (Dense)             (None, 128)               16512     <br/>                                                                 <br/> dense_7 (Dense)             (None, 10)                1290      <br/>                                                                 <br/>=================================================================<br/>Total params: 114,954<br/>Trainable params: 114,954<br/>Non-trainable params: 0</span><span id="e2f1" class="nv lz iq nq b gy ol nx l ny nz"># Last Epoch<br/>Epoch 16/16<br/>469/469 [==============================] - 6s 12ms/step - loss: 0.1206 - acc: 0.9551 - val_loss: 0.3503 - val_acc: 0.9014</span></pre><p id="b8c4" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">让我们忽略这里的过度拟合部分，并绘制一些预测的<code class="fe nn no np nq b">softmax</code>输出。在此之前，我们将从数据集中抽取一些样本和标签，并对它们进行预测。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="462e" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">我们拿第一批来预测。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi om"><img src="../Images/acc580061a85053f4f9ef243a9f9309e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hFjAxNmN414BOjYTzkMF_A.jpeg"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">作者图片</p></figure><p id="f4fc" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">对于第一个图，正确的标签是 1。并且模型为该样本输出如此高的<code class="fe nn no np nq b">softmax</code>值。虽然<code class="fe nn no np nq b">softmax</code>值很高，但是这个<strong class="kp ir">并不能说明模型有多可靠。</strong></p><p id="3ac5" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">为了测试这一点，一个随机噪声向量将被给予模型。我们将讨论<code class="fe nn no np nq b">softmax</code>输出。</p><p id="3609" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">(我将在后面的部分解释第二个模型的参数<code class="fe nn no np nq b">ensemble</code></p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="9287" class="nv lz iq nq b gy nw nx l ny nz">{0: 6.912527, 1: 0.0038282804, 2: 0.9346371, 3: 0.7660487, <br/> 4: 4.7964582, 5: 3.58412e-05, 6: 80.114265, 7: 0.0002095818, <br/> 8: 6.4642153, 9: 0.007769133}</span></pre><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div class="gh gi on"><img src="../Images/532a07efb71cfd85b1501d9f5a604dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*1XTRe6D_glzO7QAndm-FxQ.png"/></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">图片作者。</p></figure><p id="fbfc" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">根据<code class="fe nn no np nq b">softmax</code>输出，有 80% <em class="oo">的概率</em>该给定向量属于类别 6。</p><p id="4cf5" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">关于<code class="fe nn no np nq b">softmax</code>的<strong class="kp ir">误解</strong>是<strong class="kp ir">的值</strong>越高，模型就越<strong class="kp ir">有信心</strong>[1]。<strong class="kp ir">情况并非总是如此</strong>。<code class="fe nn no np nq b">Softmax</code>预测错误时，值可能非常高。例如:</p><ul class=""><li id="d451" class="lk ll iq kp b kq kr ku kv ky lm lc ln lg lo jx lp lq lr ls bi translated">敌对攻击</li><li id="d092" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated">非分布数据(例如:随机噪声向量)</li></ul><p id="130a" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">此外，在我们讨论的回归示例中，根据梯度下降的起点，存在多个模型权重来解释数据。这里，我们完全忽略了它们，因为<strong class="kp ir">模型具有点估计权重</strong>，而我们只有一个模型。</p><p id="db2a" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated"><strong class="kp ir">所以现在问题来了</strong>，如果我们考虑其他可能的模型，并把它们组合起来得到一个预测，会怎么样？比方说，如果我们为这些数据集合 100 个不同的模型会怎么样？</p><h2 id="2661" class="nv lz iq bd ma op oq dn me or os dp mi ky ot ou mm lc ov ow mq lg ox oy mu oz bi translated">组装 100 个模型</h2><p id="f753" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">训练 100 个不同的深度学习模型，如果考虑现实生活中的项目，并不实用。我们可以想一个简单的方法从单点估计模型中得到不同的预测。</p><p id="4fe6" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">如你所知，辍学是深度学习中采用的一种正则化技术，以防止过度拟合。它的工作原理是在训练过程中随机从网络中删除一些单元(神经元),希望这样可以防止任何一个神经元变得太有影响力。我将解释我们如何使用它来得到不同的模型并将它们组合在一起。</p><p id="bd58" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">在此之前，让我们创建相同的模型，并添加辍学。</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="8553" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">从模型中获取预测时，丢弃层不活动。如果我们在预测输入的同时使用漏失层的效果会怎么样？</p><p id="3f62" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">如果我们使用<code class="fe nn no np nq b">__call__</code>模型的方法，我们仍然可能使它们保持活跃:</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="fa0d" class="nv lz iq nq b gy nw nx l ny nz">for _ in range(4):<br/>    predict_noise = model2(tf.expand_dims(random_vector,axis = 0),<br/>                        training = True).numpy().squeeze()<br/>    print('Softmax output for class 0:', predict_noise[0] * 100)</span></pre><p id="b354" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">使用<code class="fe nn no np nq b">training = True</code>，我们保持活跃的<code class="fe nn no np nq b">Dropout</code>层，就好像网络正在训练，但我们只应用向前传递。上面的代码将打印这些结果:</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="e538" class="nv lz iq nq b gy nw nx l ny nz">Softmax output for class 0: 3.7370428442955017<br/>Softmax output for class 0: 4.5094069093465805<br/>Softmax output for class 0: 0.9782549925148487<br/>Softmax output for class 0: 1.7607659101486206</span></pre><p id="5263" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">这是因为脱落层，一些神经元随机脱落。因此，<strong class="kp ir">作为一种天真的方法</strong>我们可以使用这种策略来获得不同的预测，就好像我们有 100 个不同的模型一样。</p><p id="1b30" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">第一个模型是完全<strong class="kp ir">确定性的</strong>，如果你预测相同的样本，你每次都会得到<strong class="kp ir">相同的输出，并且因为它没有<strong class="kp ir">层，所以没有实际可行的方法来得到不同的预测。但是现在，我们可以从一个<strong class="kp ir">单一模型得到<strong class="kp ir">不同的预测</strong>！</strong></strong></strong></p><p id="e16d" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">回想一下，我们有一个效用函数来预测给定噪声矢量的<code class="fe nn no np nq b">softmax</code>输出:</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="5903" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">现在我们将<code class="fe nn no np nq b">model2</code>作为模型参数，将<code class="fe nn no np nq b">100</code>作为集合大小。所以现在我们从模型中得到 100 次预测，同时保持脱落层活动。对于最终的预测，我们将只取预测的<strong class="kp ir">平均值</strong>。</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="d834" class="nv lz iq nq b gy nw nx l ny nz">{0: 4.0112467, 1: 0.4843149, 2: 11.618713, 3: 8.531735, <br/> 4: 8.837839, 5: 0.04070336, 6: 59.83536, 7: 0.96540254, <br/> 8: 4.8854494, 9: 0.7892267}</span></pre><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div class="gh gi on"><img src="../Images/77b576156e798d798f5756fefb290fc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*agTSLSlXjtv6m7ga4uGObA.png"/></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">作者图片</p></figure><p id="689c" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">嗯，结果比其他模型好，但不是最好的。这是因为即使我们使用了某种集合技术，模型的权重也有点估计值。</p><p id="496d" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">让我们用新模型预测第一批并检验结果。</p><pre class="nc nd ne nf gt nr nq ns nt aw nu bi"><span id="5170" class="nv lz iq nq b gy nw nx l ny nz">predictions = []<br/>for _ in range(1000):<br/>    predicted_ensemble = model2(samples,<br/>                        training = True).numpy().squeeze()<br/>    predictions.append(predicted_ensemble)<br/>predictions = np.array(predictions)<br/>print('Predictions shape:', predictions.shape) # (1000, 128, 10)<br/>predictions_median = np.median(predictions, axis = 0)</span></pre><p id="e717" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">我们不用均值，取<strong class="kp ir">中值</strong>作为最终预测。当我们绘制预测图时，它们看起来像这样:</p><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi om"><img src="../Images/37096620310feccc195b9ba4b7a897d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bo_bJ7xMr9uU1zICy4dk1w.png"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">作者图片</p></figure><p id="4aeb" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">如果我们将这些预测与另一个进行比较，我们会发现一些预测略有变化。这是因为我们在获得预测的同时保持主动退出，这使得我们能够拥有随机集合模型。</p><p id="a35b" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">现在，我们可以针对单个输入获得多个不同的预测。这意味着我们可以从一组预测中生成<strong class="kp ir">置信区间</strong>。</p><h2 id="4100" class="nv lz iq bd ma op oq dn me or os dp mi ky ot ou mm lc ov ow mq lg ox oy mu oz bi translated">退出以获得置信区间</h2><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="2675" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">使用 for 循环，我们可以获得样本的预测:</p><figure class="nc nd ne nf gt kf"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="4bd9" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">我将添加一些示例图来对它们进行评论:</p><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/3c3ef3565ac4569454ad8e9fb639f22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dS2Q_8wKD2Eirallt_EHiA.png"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">图片作者。</p></figure><p id="bfe5" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">该图向我们展示了，即使我们集合了特定输入的<code class="fe nn no np nq b">1000</code>预测，95%区间的范围与中值和正常预测相同。所以我们可以说这个样本确实属于<code class="fe nn no np nq b">class 1.</code>和<strong class="kp ir">不确定性低。</strong></p><p id="06be" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">让我们看看另一个例子:</p><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/9357b8ce001881a01bc59bc7a3806aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuLyQxTq9Srn9IT0aI4lKg.png"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">图片作者。</p></figure><p id="a591" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">刚刚发生了什么？当我们检查 95%的间隔时，我们看到绿色条比以前高了。这是因为通过辍学模型，我们能够捕捉到预测中的一些不确定性。</p><p id="bbd8" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated"><strong class="kp ir">结论是，随着这些绿条越来越高，预测越来越不确定。</strong></p><figure class="nc nd ne nf gt kf gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nb"><img src="../Images/44d0dd366542cb9ae599cb97de7d336f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IyTM3QlcdmKcu23aQp1ZgA.png"/></div></div><p class="ki kj gj gh gi kk kl bd b be z dk translated">图片作者。</p></figure><p id="5a05" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">最后，让我们来检验这个预测。当我们使用单点估计模型时，0 类的概率很小。当我们应用集成技术时，我们看到预测中有巨大的不确定性。注意绿色的条有多高！</p><h1 id="b954" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">结论</h1><p id="7241" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">我们讨论过:</p><ul class=""><li id="4e28" class="lk ll iq kp b kq kr ku kv ky lm lc ln lg lo jx lp lq lr ls bi translated">假设数据集是相同的，则存在取决于梯度下降起点的一组权重。</li><li id="5731" class="lk ll iq kp b kq lt ku lu ky lv lc lw lg lx jx lp lq lr ls bi translated">普通的辍学集合不是最好的方法，因为它有点随机。</li></ul><h1 id="cfd9" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">后续步骤</h1><p id="b1b9" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">在下面的文章中，<strong class="kp ir">我们将使用一种更系统的方式</strong>来表示任意的和认知的不确定性，使用<strong class="kp ir">张量流概率</strong>同时详细解释这些术语。</p><p id="f60e" class="pw-post-body-paragraph kn ko iq kp b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj jx ij bi translated">你可以从这里查看用过的笔记本<a class="ae km" href="https://github.com/Frightera/Medium_Notebooks_English/tree/main/Brief%20Introduction%20to%20Uncertainty" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="d654" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">参考</h1><p id="d71e" class="pw-post-body-paragraph kn ko iq kp b kq mw ks kt ku mx kw kx ky my la lb lc mz le lf lg na li lj jx ij bi translated">[1]:蒂姆·皮尔斯，亚历山德拉·布林特鲁普·朱军，<a class="ae km" href="https://arxiv.org/pdf/2106.04972.pdf" rel="noopener ugc nofollow" target="_blank">了解 Softmax 信心和不确定性</a>，2021。</p></div></div>    
</body>
</html>