<html>
<head>
<title>What’s Behind Word2vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2vec背后是什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-behind-word2vec-95e3326a833a#2022-07-26">https://towardsdatascience.com/whats-behind-word2vec-95e3326a833a#2022-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="79e1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/word-embeddings-primer" rel="noopener" target="_blank"><strong class="ak"/></a></h2><div class=""/><div class=""><h2 id="d93b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">单词嵌入的概念和方程概述</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/c24d890034ba8ed80093e7e1a88354e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moLYCzh3xbmNgiSFUUPpjA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@usinglight?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯蒂芬·斯坦鲍尔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1014" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">自然语言处理(NLP)领域需要语言学、统计学和计算机科学的知识。因此，在没有对你不太熟悉的学科进行大量背景研究的情况下，开始一项新的研究或项目可能会很有挑战性。无缝地跨越这些学科也是一个挑战，因为它们的术语和方程格式不同。</p><p id="35bf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我从统计学来到NLP，想了解文本集合(<em class="mk">语料库</em>)中的单词如何以向量(<em class="mk">单词嵌入</em>)的形式转换成可用数据，如下面两个例子所示:</p><pre class="kp kq kr ks gt ml mm mn mo aw mp bi"><span id="c681" class="mq mr iq mm b gy ms mt l mu mv">aardvark [  0.7660651 -0.9571466 -0.4889298  ...  -0.1600012 ]<br/>zulu     [ -0.4566793  0.7392789  0.5158788  ...   0.0398366 ]</span></pre><p id="c029" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我想了解什么是真正要测量的，什么是最重要的，以及在向Word2vec、fastText和现代语境化单词表示中的算法生成的矢量数据的转换中，什么会受到损害。</p><p id="73bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基本上，<em class="mk">单词嵌入值中的数字真正代表什么</em>？</p><p id="4860" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，许多单词有多重含义(<em class="mk"> senses </em>)，所以我特别想知道一个有多重含义但只有一个向量(比如100个维度)的单词如何在统计上仍然有效。然后，我希望能够基于Word2vec模型编写自己的单词嵌入训练算法，这样我就可以探索特定意义的表示。</p><p id="6967" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文是系列文章的第一篇。它介绍了单词如何通过它们在文本中的接近度相互关联的概念，以及创建单词关系数据背后的理论。在这个系列中，我还将把语言学和计算机科学中的NLP概念翻译成统计学的观点。</p><p id="cb0a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">NLP今天的大部分发展都集中在深度学习人工智能算法上，但任何进入该领域的人都应该对所有的构建模块有清晰的概念把握。</p><p id="6551" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本系列的文章包括:</p><ol class=""><li id="4944" class="mw mx iq lh b li lj ll lm lo my ls mz lw na ma nb nc nd ne bi translated"><strong class="lh ja">word 2 vec背后有什么</strong> ( <em class="mk">本文</em> ) <br/>单词嵌入的思路和方程概要<em class="mk"> (7分钟阅读)</em></li><li id="d4d6" class="mw mx iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated"><a class="ae le" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lh ja">单词转化为向量</strong> </a> <br/>单词嵌入概念<em class="mk"> (13分钟读完)</em></li><li id="e32b" class="mw mx iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated"><a class="ae le" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener"> <strong class="lh ja">统计学习理论</strong> </a> <br/>神经网络基础<em class="mk"> (14分钟读取)</em></li><li id="042e" class="mw mx iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated"><a class="ae le" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener"><strong class="lh ja">word 2 vec分类器</strong> </a> <br/>如何训练单词嵌入<em class="mk"> (15分钟读取)</em></li><li id="0fca" class="mw mx iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated"><a class="ae le" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener"><strong class="lh ja">word 2 vec超参数</strong> </a> <br/>一套创意重新加权<em class="mk"> (6 min读取)</em></li><li id="02a5" class="mw mx iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated"><a class="ae le" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener"> <strong class="lh ja">词语嵌入的特点</strong> </a> <br/>以及反义词的问题<em class="mk"> (11分钟读完)</em></li></ol><p id="d634" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们深入研究NLP和单词嵌入之前，让我们简要地看一下导致Word2vec产生的背景。我们将关注Word2vec，因为它普及了今天使用的单词嵌入类型。</p><h1 id="ce46" class="nk mr iq bd nl nm nn no np nq nr ns nt kf nu kg nv ki nw kj nx kl ny km nz oa bi translated"><strong class="ak">作为单词定义基础的单词邻近度</strong></h1><p id="27b7" class="pw-post-body-paragraph lf lg iq lh b li ob ka lk ll oc kd ln lo od lq lr ls oe lu lv lw of ly lz ma ij bi translated">计算语言模型的基本应用之一是预测句子中的单词，例如，在搜索引擎和消息应用程序的自动完成功能中:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/6a2afacfacb067b25404d7b5788f4ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBAzpKeq8V8kc7oAz3OtrA@2x.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="aae5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种模型可以是通过将大型文本语料库中的所有单词序列制成表格来概率性地预测每个单词的出现的语言模型，但是从实现的角度来看，由于数据量大，处理和存储所有这些信息是不切实际的。例如，即使是简单的数据集合的大小，例如单词对在一组文档中的每个文档中一起出现的频率的<em class="mk">同现矩阵</em>，也将是唯一单词总数的平方，可能是几十万个单词的平方。</p><p id="6b2e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">单词嵌入是使用向量来表示单词，有助于减少这些计算挑战。单词嵌入不是存储所有文档中所有单词的所有信息，而是利用创造性的数据处理和统计降维技术来近似单词之间的关系。</p><p id="ffa4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些现代的机器学习单词嵌入的一个有趣的特性是，当它们应用于语言模型时，它们不仅基于邻近频率预测单词序列，而且在某种程度上预测单词含义。</p><p id="965a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">单词嵌入是哲学家路德维希·维特斯坦根的观点的一种体现，即“单词的意义是它在语言中的使用”(维特根斯坦，1953)。1957年，语言学家约翰·鲁珀特·弗斯把这个概念更具体地表述为:</p><blockquote class="oh"><p id="09b2" class="oi oj iq bd ok ol om on oo op oq ma dk translated">"从一个人交的朋友，你就可以知道这个人说的话."</p></blockquote><figure class="os ot ou ov ow kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0b352df427bd4a174dbfe8a15d818d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*0Pg4LI3csCeUoboW4CDCgQ.jpeg"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">约翰·鲁伯特·弗斯(1890–1960)<br/>(图片来自n·斯科特的文章(1961)。<em class="oy">亚非学院公报，</em> <a class="ae le" href="https://www.cambridge.org/core/journals/bulletin-of-the-school-of-oriental-and-african-studies/issue/4E2968DB58D0225409242A3A37DCFAB7" rel="noopener ugc nofollow" target="_blank"> <em class="oy"> 24 </em> (3) </a>，412–418，经允许转载)</p></figure><p id="6031" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，一个词可以通过它通常出现的词来定义。例如，因为根据上下文,“摇滚”这个词可能会出现在“地球”和“音乐”等词旁边，所以地球和音乐都与摇滚的定义有关。今天，这个概念在语言学中被称为<em class="mk">分布假设</em>(佩罗尼，2018)。</p><blockquote class="oz pa pb"><p id="2cbf" class="lf lg mk lh b li lj ka lk ll lm kd ln pc lp lq lr pd lt lu lv pe lx ly lz ma ij bi translated">但是从统计NLP的角度来看，更自然的想法是，意义存在于使用单词和话语的上下文的分布中。……在这种概念下，许多统计NLP研究直接解决意义问题。”<em class="iq">(曼宁和许策，1999年)</em></p></blockquote><p id="9dd3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">类似于字典仅仅通过单词彼此之间的关系来定义所有单词，单词嵌入矩阵使用数值通过它们在使用中的接近度来定义它的单词。</p><h1 id="f697" class="nk mr iq bd nl nm nn no np nq nr ns nt kf nu kg nv ki nw kj nx kl ny km nz oa bi translated">为什么数据模型开始取代基于规则的模型</h1><p id="5425" class="pw-post-body-paragraph lf lg iq lh b li ob ka lk ll oc kd ln lo od lq lr ls oe lu lv lw of ly lz ma ij bi translated">自然语言处理(NLP)领域的目标是让计算机使用人类语言进行交互。已经采取了许多方法在计算机中实现人类语言，语言学的思想也在依赖复杂的计算机算法中发展。强有力地建立在语言结构、规则和逻辑上的语言模型通常过于处理密集或复杂而不实用，并且在计算上工作良好的快捷方式通常具有明显的语言弱点。</p><p id="3482" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">统计学在经验主义NLP中发挥着突出的作用，不仅仅是在书面和口头语言数据的分析中，而是在机器学习背后的统计学习理论中，机器学习越来越多地应用于分析大型语料库(<a class="ae le" rel="noopener" target="_blank" href="/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3"> Stewart </a>，2019)。尽管如此，即使在机器学习和人工智能(AI)加速之前，统计NLP的价值也很突出。</p><blockquote class="oz pa pb"><p id="48bd" class="lf lg mk lh b li lj ka lk ll lm kd ln pc lp lq lr pd lt lu lv pe lx ly lz ma ij bi translated">“统计[NLP]模型是稳健的，概括得很好，并且在存在错误和新数据的情况下表现优雅。”<em class="iq">(曼宁和许策，1999年)</em></p></blockquote><h1 id="fd87" class="nk mr iq bd nl nm nn no np nq nr ns nt kf nu kg nv ki nw kj nx kl ny km nz oa bi translated">Word2vec的首次亮相:是什么让它如此具有变革性</h1><p id="633b" class="pw-post-body-paragraph lf lg iq lh b li ob ka lk ll oc kd ln lo od lq lr ls oe lu lv lw of ly lz ma ij bi translated">2013年，随着Mikolov等人在谷歌发表的两篇介绍Word2vec的论文(<a class="ae le" href="http://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> Mikolov等人，2013 a</a>；<a class="ae le" href="http://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank">米科洛夫等人，2013年b </a>。Word2vec使用一个浅层神经网络来产生单词嵌入，这种嵌入表现得特别好，并具有计算效率大幅提高的额外好处。使用Word2vec，只需一台个人计算机，就可以从任何语言的相对较大的语料库中创建一组单词向量。Word2vec的另一个突出特性是观察到单词向量在向量空间中聚集同义词和附近的相关单词。此外，向量似乎有数学属性。例如，通过添加向量值，可以找到计算语言学中的以下著名等式:</p><p id="56ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mk">国王男人+女人≈王后</em></p><p id="dc70" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在二维情况下，该等式可能如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/9398c61af09a77080e0ce1a6797a2ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJ9yksA-xmubGhrIdAybDQ@2x.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd ox">文字向量数学概念</strong> <br/>(图片由作者提供)</p></figure><p id="7eb9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些Word2vec向量改进了NLP任务的许多应用，并且大量的研究随之而来，以研究神经网络单词嵌入的属性和含义。通过这项研究产生的想法最终导致了更强大的具有上下文敏感嵌入的人工智能模型(如AllenNLP <a class="ae le" href="http://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> ELMo </a>，OpenAI的<a class="ae le" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT </a>，以及谷歌的<a class="ae le" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。</p><h1 id="c9e4" class="nk mr iq bd nl nm nn no np nq nr ns nt kf nu kg nv ki nw kj nx kl ny km nz oa bi translated">摘要</h1><p id="8342" class="pw-post-body-paragraph lf lg iq lh b li ob ka lk ll oc kd ln lo od lq lr ls oe lu lv lw of ly lz ma ij bi translated">在本文中，我们学习了语言学理论，即使用中的单词邻近性与单词含义有关，并且自然语言的规则可能不切实际。我们还了解到，使用单词邻近度来创建单词向量可以产生具有有用属性的可管理数据集。</p><p id="9594" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下一篇文章<a class="ae le" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener"> <strong class="lh ja">单词到向量</strong> </a>中，我们将回顾创建单词嵌入背后的基本概念。</p></div><div class="ab cl pg ph hu pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="ij ik il im in"><p id="5397" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章是1ˢᵗ系列文章<strong class="lh ja">中关于单词嵌入的初级读本:<br/> </strong> 1。Word2vec | 2背后的东西。<a class="ae le" href="https://medium.com/@jongim/words-into-vectors-a7ba23acaf3d" rel="noopener">单词成向量</a> | <br/> 3。<a class="ae le" href="https://medium.com/@jongim/statistical-learning-theory-26753bdee66e" rel="noopener">统计学习理论</a> | 4。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-classifier-5656b04143da" rel="noopener">word 2 vec分类器</a> | <br/> 5。<a class="ae le" href="https://medium.com/@jongim/the-word2vec-hyperparameters-e7b3be0d0c74" rel="noopener">word 2 vec超参数</a> | 6。<a class="ae le" href="https://medium.com/@jongim/characteristics-of-word-embeddings-59d8978b5c02" rel="noopener">单词嵌入的特征</a></p><p id="8396" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于这个主题的更多信息:对于本系列的每一篇文章，我将推荐一个关键参考资料，以获得关于这个主题的更多信息。对于这篇文章，你可能会特别喜欢:佩罗尼，C. S. (2018)。<a class="ae le" href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/" rel="noopener ugc nofollow" target="_blank">自然语言处理的词语表述和维特根斯坦的语言哲学</a>。<em class="mk">未知领域</em>。</p><h1 id="7f79" class="nk mr iq bd nl nm nn no np nq nr ns nt kf nu kg nv ki nw kj nx kl ny km nz oa bi translated">参考</h1><p id="c62f" class="pw-post-body-paragraph lf lg iq lh b li ob ka lk ll oc kd ln lo od lq lr ls oe lu lv lw of ly lz ma ij bi translated">弗斯，J. R. (1957)。语言学理论概要，1930-1955。在弗斯(编辑)，<em class="mk">语言分析研究</em>，语言学会特刊，第1-32页。英国牛津:巴兹尔·布莱克威尔出版社。</p><p id="1d23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">曼宁和舒策(1999年)。<em class="mk">统计自然语言处理基础</em>。麻省剑桥:麻省理工学院出版社。</p><p id="85fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Mikolov、Corrado、G . Chen、k .和j . Dean(2013年a)。向量空间中单词表示的有效估计。可从<a class="ae le" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> arXiv:1301:3781v3 </a>获得。</p><p id="4328" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Mikolov、Corrado、G . Chen、k . Sutskever和j . Dean(2013年b)。词和短语的分布式表示及其组合性。可从<a class="ae le" href="https://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank"> arXiv:1310.4546v1 </a>获得。</p><p id="1c85" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">南卡罗来纳州佩罗尼(2018年)。<a class="ae le" href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/" rel="noopener ugc nofollow" target="_blank">自然语言处理的词语表述和维特根斯坦的语言哲学</a>。<em class="mk">未知领域</em>。</p><p id="e8bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">斯图尔特，M. (2019)。<a class="ae le" rel="noopener" target="_blank" href="/the-actual-difference-between-statistics-and-machine-learning-64b49f07ea3">统计和机器学习的实际区别</a>。<em class="mk">走向数据科学</em>。</p><p id="2b91" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">维特根斯坦，L. (1953)。<em class="mk">哲学研究</em>。英国牛津:巴兹尔·布莱克威尔出版社。</p><p id="1dac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">*除非另有说明，数字和图像均由作者提供。</p></div></div>    
</body>
</html>