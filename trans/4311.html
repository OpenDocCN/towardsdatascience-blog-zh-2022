<html>
<head>
<title>Elastic Net Regression: From Sklearn to Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">弹性网回归:从Sklearn到Tensorflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elastic-net-regression-from-sklearn-to-tensorflow-3b48eee45e91#2022-09-23">https://towardsdatascience.com/elastic-net-regression-from-sklearn-to-tensorflow-3b48eee45e91#2022-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f4e5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在Python中进行sklearn和Tensorflow之间的等价弹性网回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/919ae6a972015e29021f88392e590249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XzPuNq0sfHKA4pEc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迈克·考克斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a620" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章是为那些想要比较弹性网回归的sklearn和Keras实现的从业者准备的。主要是如何从Sklearn损失函数到Keras (Tensorflow)损失函数。</p><p id="0a13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文章的主要部分:</p><ul class=""><li id="645c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">回归中的正则化简介。</li><li id="86fe" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">弹性网的Sklearn实现。</li><li id="8942" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">弹性网的张量流实现。</li><li id="59ea" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从一个框架到另一个框架。</li><li id="5eb9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">编写自定义损失函数。</li><li id="a543" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">比较结果。</li></ul><p id="d0a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有的代码都在我的仓库里:【https://github.com/Eligijus112/regularization-python T4】</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="1ec3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回归是机器学习中确定响应变量的平均值<strong class="lb iu"> Y </strong>和特征<strong class="lb iu"> X </strong>之间关系的过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/3e6e1caf5f6031ff298d6ad788303cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*TXzuVd_i2bZ1NJao"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">简单回归方程；作者图片</p></figure><p id="fc49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型的预测用带帽的y表示，并通过从数据中估计系数β来计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/03ad08767a6384d7344a57cbf0e2e4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*ERn6dKcD051Grmv4"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测方程；作者图片</p></figure><p id="2484" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不带帽子的beta表示理论<em class="ms">模型</em>，带帽子的beta表示实际<em class="ms">模型</em>，使用可观测数据获得。我们如何计算系数β？</p><p id="58c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算从定义一个<strong class="lb iu">错误</strong>开始。误差显示了预测值与真实值的差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d74c856103caf080fd591693340ac3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/0*fH54HEs3SwmZDAc_"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">观察值I的误差；作者图片</p></figure><p id="b0d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，我们希望误差尽可能小，因为这将表明我们找到的系数接近基本因变量<strong class="lb iu"> Y </strong>给出我们的特征的最佳值。此外，我们希望对负误差和正误差(以及其他原因)给予相同的权重，因此我们将引入平方误差项:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/350c7a8ef2a891e4893c66eac20c2c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*LvmJACn0RbEyapk5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">观察值I的平方误差；作者图片</p></figure><p id="729d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在标准线性回归中，我们的目标是将所有观测值的平方误差降至最低。因此，我们可以定义一个称为均方误差的函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/442aab719113688115e53eb93ff9215a.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/0*VwKmQZMn87GaoSHk"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/dd2f055a5134768d1df04b8627945054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/0*D-jwIuxDzyRIOSQ3"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MSE函数；作者图片</p></figure><p id="1b74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">x和y数据是固定的——我们不能改变我们观察到的东西。我们拥有的唯一杠杆是尝试将不同的贝塔系数与我们的数据相匹配，以获得尽可能低的MSE值。这是线性回归的总体目标— <strong class="lb iu">仅使用β系数将定义的损失函数降至最低。</strong></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="a414" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了最小化MSE值，我们可以使用各种优化算法。最流行的算法之一是<strong class="lb iu">随机梯度下降</strong>或者简称为<strong class="lb iu"> SGD </strong>。</p><p id="93a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">梯度</strong>部分是我们根据所有系数计算偏导数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1e62d8018daf6704bdc469398fe6fd67.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/0*77gnuoGT69e13ptG"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有I的偏导数；作者照片</p></figure><p id="7fef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下降部分是我们迭代地更新每个系数β:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/e3f9d833c0ea095d2decde24e587a0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/0*INzb-HUAl_mD4pep"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">下降部分；作者照片</p></figure><p id="169f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> M </strong>是迭代次数。</p><p id="09fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">α</strong>常数是一个正值，称为学习率。</p><p id="f7d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机部分是我们不在一次迭代中使用数据集中的所有样本。当更新系数β时，我们使用数据的子集，也称为小批量。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="542a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">回归中的正则化</strong>是一种避免建立更复杂模型的方法，以避免过拟合的风险。正规化有多种类型，但最受欢迎的有三种:</p><ul class=""><li id="f117" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">套索</li><li id="bf89" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">山脉</li><li id="dacd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">弹性网</li></ul><p id="d6d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过向我们优化的损失函数添加某些项来进行正则化。在Lasso回归的情况下，传统的MSE变为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/98a5f03af9fcca6a43e2f7771482528c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4yHm751Z7jNACWJ-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通用套索公式；作者图片</p></figure><p id="5d4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在新的损失函数中加入绝对系数值的和。系数的绝对和越大，损耗越高。因此，在优化时，算法因系数大而受到惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/e99a1c582d9075052ae1403a55927392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2mQMOo6ydYl9hdx3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通用岭公式；作者图片</p></figure><p id="01a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭回归带来了更大的系数损失，因为幅度是平方的。岭回归的一个缺点是，这种类型的回归不会将任何系数设置为0，而lasso可以用于特征约简，因为它可以将一些系数设置为零。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi gj"><img src="../Images/87172a6bd67b47da42d12a78ab6a9bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XHPyKq8rwKNp3NwH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通用弹性网公式；作者照片</p></figure><p id="d3e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">弹性网结合了脊和套索回归。λ值通常介于1和0之间。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="48d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经有了一个简单的理论概述，我们可以继续讨论实际部分了。我们将使用的数据来自Kaggle关于2014年至2015年美国房屋销售的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/eac6aece070722a2bace67e0fadf1d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*EUf41NAUAfnwoxdhEuNpuw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原始数据；按作者分类的表格</p></figure><p id="07aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将尝试使用6个可用特征来解释房子的价格。</p><p id="9a2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将标准化这些功能。在本文中，我想展示TensorFlow和Sklearn之间的可比性，因此我不会花太多时间对数据集进行特征工程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/d0c9dbfc7fccaa2d5d07b06ef98e4a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*jeDUkag3cxf3OQOrTzFurw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标准化数据集；按作者分类的表格</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="bccc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试试弹性网的Sklearn实现</p><p id="e270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。ElasticNet.html</a></p><p id="8a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Sklearn中使用的损失函数为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/f39477739fae378c45d243a3ac6a7d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kUErHAp68xOwt7Lc"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit学习弹性净损失函数；作者图片</p></figure><p id="eff8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们设置alpha = 1，lambda(在Sklearn中称为l1比)为0.02。</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="f4b6" class="nj nk it nf b gy nl nm l nn no"># Fitting the model to data using sklearn<br/>el = ElasticNet(alpha=1.0, l1_ratio=0.02)<br/>el.fit(d[features], d[y_var])</span><span id="ed14" class="nj nk it nf b gy np nm l nn no"># Extracting the coefs<br/>coefs = el.coef_</span><span id="9f93" class="nj nk it nf b gy np nm l nn no"># Creating a dataframe with the coefs<br/>coefs_df = pd.DataFrame({‘feature’: features, ‘coef_sk’: coefs})</span><span id="e434" class="nj nk it nf b gy np nm l nn no"># Appending the intercept<br/>coefs_df = coefs_df.append({‘feature’: ‘intercept’, ‘coef_sk’: el.intercept_[0]}, ignore_index=True)</span></pre><p id="1dbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">得到的系数数据帧是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c14f7cb67e0659a1ae732be59c7c5909.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*a-JsKSqmiM5V17D0L8wFcQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弹性净回归结果；按作者分类的表格</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="8620" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Tensorflow是一个非常流行的深度学习网络。该框架允许我们使用弹性网络正则化，以及使用L1L2类<a class="ae ky" href="https://keras.io/api/layers/regularizers/#l1l2-class" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/layers/regularizers/#l1l2-class</a>。</p><p id="2474" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们比较两个公式sklearn one和TensorFlow。<br/> <strong class="lb iu"> Scikit学:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/cca18f9adcb241d55d855b49d451e3bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/0*zW9nQcKwL0-uK5H9"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit学习弹性网</p></figure><p id="e2bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">张量流:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/76ea2458455fbd7f5f84df6d9289b5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/0*NWELf876N_GTOtQV"/></div></figure><p id="2cf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，TensorFlow没有将等式的MSE部分除以2。为了获得可比较的结果，我们需要为TensorFlow创建自己的自定义MSE函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模仿sklearn的自定义MSE作者代码</p></figure><p id="e7a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">写完这个自定义损失函数后，我们现在有以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9b1d67102ed5b5542ad41d61f56c0c68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/0*CCoAOkh14cT9rCL2"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">张量流的自定义损失；作者方程式</p></figure><p id="616e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以均衡这两个方程，并尝试根据sklearn的alpha和lambda，在TensorFlow中创建lambda 1和lambda 2的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/51898125f27ed486d8ada92c19bfc025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qXYRlok80QhyQ3d1"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左手边:sklearn，右手边:TensorFlow作者方程式</p></figure><p id="5cdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们只剩下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4cf4acfffa6081c20c2102ba06aa1d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/0*Qlp0AQxmwU9bMUA7"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/8f2ecf7f61112378493c0a82cd64c60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/0*vlWRti91wEDZpe-p"/></div></figure><p id="2053" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将sklearn正则化转换为张量流正则化参数的函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从sklearn到TensorFlow作者代码</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="6071" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们将所有内容放在一起，通过正则化训练一个张量流模型:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="0a8d" class="nj nk it nf b gy nl nm l nn no"># Defining the original constants<br/>alpha = 1.0<br/>l1_ratio = 0.02</span><span id="fc1e" class="nj nk it nf b gy np nm l nn no"># Infering the l1 and l2 params<br/><strong class="nf iu">l1, l2 = elastic_net_to_keras(alpha, l1_ratio)</strong></span><span id="bd83" class="nj nk it nf b gy np nm l nn no"># Defining a simple regression neural net<br/>numeric_input = Input(shape=(len(features), ))<br/>output = Dense(1, activation=’linear’, kernel_regularizer=L1L2(l1, l2))(numeric_input)</span><span id="e321" class="nj nk it nf b gy np nm l nn no">model = Model(inputs=numeric_input, outputs=output)<br/>optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)</span><span id="1041" class="nj nk it nf b gy np nm l nn no"># Compiling the model<br/>model.compile(optimizer=optimizer, loss=<strong class="nf iu">NMSE()</strong>, metrics=[‘mse’])</span><span id="b28e" class="nj nk it nf b gy np nm l nn no"># Fitting the model to data using keras<br/>history = model.fit(<br/>  d[features].values, <br/>  d[y_var].values, <br/>  epochs=100, <br/>  batch_size=64<br/>)</span></pre><p id="ade1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当<strong class="lb iu">α= 1.0，l1比值为0.02 </strong>时，张量流正则化的常数为<strong class="lb iu"> 0.02和0.49。</strong></p><p id="961b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练看起来很顺利:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/17a29e1667f07114d53762bc93f667ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*66WKYtYZyhILN68-lR3qYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模特培训；作者照片</p></figure><p id="4f44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">张量流和sklearn回归系数:</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="a06a" class="nj nk it nf b gy nl nm l nn no"># Creating the coef frame for TF<br/>coefs_df_tf = pd.DataFrame({<br/>  ‘feature’: features + [‘intercept’],<br/>  ‘coef_tf’: np.append(model.get_weights()[0], model.get_weights()[1])<br/>})</span><span id="e510" class="nj nk it nf b gy np nm l nn no">## Merging the two dataframes<br/>coefs_df_merged = coefs_df.merge(coefs_df_tf, on='feature')</span><span id="d31c" class="nj nk it nf b gy np nm l nn no">coefs_df_merged['percent_diff'] = (coefs_df_merged['coef_sk'] - coefs_df_merged['coef_tf']) / coefs_df_merged['coef_sk'] * 100</span></pre><p id="b26e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生成的数据帧为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d3ea8ee4ca067bf6e17e80794d10d429.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*21dY7lnX1PMvpawT-lliAQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">系数比较；按作者分类的表格</p></figure><p id="dcff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">系数相差很小。这是因为在引擎盖下，sklearn和TensorFlow用不同的函数初始化系数，涉及到一些随机性，我们总是可以调整时期和批量大小以获得越来越接近的结果。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="e8d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们:</p><ul class=""><li id="2f9e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">简单介绍了一下正规化。</li><li id="3b1d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">创建了自定义张量流损失函数。</li><li id="d97d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在TensorFlow中创建了一个模仿sklearn正则化的函数。</li><li id="ec0e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">创建并比较了两个模型。</li></ul><p id="9530" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快乐学习编码！</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="8404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整代码:</p><p id="c9ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://gist.github.com/Eligijus112/5309d9dd037236b1a95a7b906f2406cf" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/eligijus 112/5309 d9dd 037236 B1 a 95 a7b 906 f 2406 cf</a></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="347b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] <br/>美国金县房屋销售；<br/>网址:<a class="ae ky" href="https://www.kaggle.com/datasets/harlfoxem/housesalesprediction?resource=download" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/datasets/harlfoxem/housesales prediction？资源=下载</a></p><p id="093d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执照:<a class="ae ky" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a></p></div></div>    
</body>
</html>