<html>
<head>
<title>K-Means Clustering — An Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类——简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-an-introduction-9825ea998d1e#2022-08-07">https://towardsdatascience.com/k-means-clustering-an-introduction-9825ea998d1e#2022-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a227" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种流行的无监督机器学习方法综述</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5d5d979ba4945e20a4552f477b4118b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGkt5Oll4cyBVYuYk77guQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由Karolina Grabowska 拍摄，来自Pexels.com。</p></figure><p id="30fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们在项目的探索性数据分析阶段使用未标记的数据集时，我们可能会对根据相似性将数据分组感兴趣。这使我们能够轻松地识别数据中可能存在的任何模式，这些模式对于人眼来说可能并不明显。这是通过聚类的无监督学习过程实现的。</p><p id="3abc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实现基于相似性的数据分组的最流行的聚类方法之一是K均值聚类。它是一种非常常用的无监督机器学习算法，相对容易理解，也容易在Python中实现。</p><p id="8a7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将介绍K-means聚类算法的基础知识。</p><h1 id="779f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是集群？</h1><p id="67e3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">聚类是一种无监督的机器学习过程，旨在根据邻近点的相似性将未标记的数据集分成许多组。</p><p id="2314" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具有相似特征的数据点被放在同一个聚类中，而具有不同特征的数据点被放在另一个聚类中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/b221bd1eed17fbc5e7b83d263cd75381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCBhxLjAC7YLzcfZqetrAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据聚类前后。图片由作者提供。</p></figure><p id="91e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多不同的算法可用于聚类数据，包括:</p><ul class=""><li id="f8f0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">k均值</li><li id="ef1f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">基于密度的噪声应用空间聚类</li><li id="d8ae" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">高斯混合模型</li><li id="1303" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">病房</li><li id="c234" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">凝聚聚类</li><li id="6c74" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">桦树</li></ul><p id="bbcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">流行的Scikit-Learn Python库中可用的不同方法的示例可以在<a class="ae ky" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p><h1 id="b740" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是K-means聚类？</h1><p id="14e0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">k均值聚类是一种流行的无监督算法，它将数据分组为“k”个聚类，其中k由用户定义。该算法试图最小化每个聚类内所有平方距离的总和，并且还最小化数据点和称为质心的聚类中心点之间的距离。</p><p id="452c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">质心在数据空间中的k个随机点处被初始化，并且基于到质心的距离，其周围的所有点被分配给相关的聚类。然后将质心调整到群集的中心点，并重新分配其周围的点。这种情况一直持续到质心没有变化或者这些点保持在同一个簇中，或者直到达到最大迭代次数。</p><p id="6938" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means是一种硬聚类方法，这意味着一个数据点要么属于一个聚类，要么不属于。</p><h1 id="e3f9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">K均值聚类的应用</h1><p id="cae4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">k-means聚类的应用很多。</p><ul class=""><li id="0045" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">图象分割法</li><li id="4d5f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">社会网络分析</li><li id="aeb5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">客户细分</li><li id="5a54" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">文件的分类</li><li id="14a5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">异常检测</li></ul><p id="d5a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在地球科学和岩石物理学领域:</p><ul class=""><li id="c74b" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">基于测井测量的异常值检测</li><li id="5336" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">根据测井记录和/或岩心分析数据进行相分类</li></ul><h1 id="80cb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">k-均值聚类算法——工作原理</h1><h2 id="cfb3" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">K均值聚类工作原理概述</h2><p id="c4f1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以下工作流说明了k-means算法工作的整个过程。每个步骤将在后续章节中详细介绍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/9cb3b48d68aa41cd340bdd9600cb7118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PAunIkoI_6aITYVNzfyMJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-means聚类算法概述。图片由作者提供。</p></figure><h2 id="d4ee" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">K-Means逐步聚类</h2><p id="3dfb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们仔细看看每一个步骤。</p><h2 id="e31b" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第一步。收集我们的数据并确定“k”的值</h2><p id="284c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">第一步是将我们的数据收集在一起，并确定我们想要将数据分成多少个簇。对于这个数据集，我们将把它分成3个集群。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/a4ea4aac97ed94a82a48084c333a7099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8IA_kMUysI12PvbvzGc1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤1，确定要将数据分组到的簇的数量。图片由作者提供。</p></figure><p id="56c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几种方法可以确定最佳集群数量，本节后面将介绍这些方法。</p><h2 id="6595" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第二步。在数据中选择k个随机点</h2><p id="184b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">接下来，我们从数据中选择“k”个随机点。在这种情况下，k = 3，所以我们将选择3个随机点。这些是星团的质心。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/3d49ed548c05ef0e1777687c30c6a4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUTve7Vc4Qvl_Wtm-Mh2fQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第二步:从数据集中随机选择k个点。图片由作者提供。</p></figure><h2 id="534c" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第三步。将点分配给最近的种子点</h2><p id="296e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">数据集中每个点与质心之间的距离是使用欧几里德距离计算的。</p><p id="f1da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦计算出每个点的质心，我们就将每个点分配到最近的质心。这就形成了我们最初的聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/348b098be3f0a7f040588d8b052d816f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_goPuqeHGYDZaAA8bRmTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第三步:计算点和质心之间的欧氏距离。然后将这些点分配给最近的种子点(质心)。图片由作者提供。</p></figure><h2 id="6a6f" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第四步。确定新的中心点</h2><p id="950c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">然后，我们计算每个聚类的平均点。这些成为新的质心。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/231db71dab1508363142aa0deef18d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wc3PGb_SfIeJofL9tAKRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤4:通过在每个聚类中寻找平均点来识别新的质心。图片由作者提供。</p></figure><h2 id="e2b0" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第五步。将点分配给最近的质心</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ad004389a2bf87a3cc8fb79ff5b5eade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhQcyWldCckJfaCDUYJvBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤5:将点分配给新的簇形心。图片由作者提供。</p></figure><p id="05dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们重复使用欧几里德距离将这些点分配给最近的聚类的过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/3fade99b6ecc0a6cb16e7beb0b324ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A280OWTlAFwGzRCZaYe5OA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">欧几里德距离的例子。图片由作者提供。</p></figure><h2 id="b781" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第六步。确定新的集群中心</h2><p id="0453" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">然后重新计算质心。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/8fe1ca34772cb8c5353a9457e97c73a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MW_cuaytzzNLjZuCU5-Esw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第六步:重新计算每个聚类的平均点，并调整质心。图片由作者提供。</p></figure><h2 id="00c0" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">第七步。重复步骤4–6</h2><p id="56c5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将点分配到最近的质心并重新计算平均点的过程</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/660b424a086f0d3a1ae735612107b314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oEuwC78a-IIm5iWR8xJm1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤6:将点重新分配到最近的质心。图片由作者提供。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/a821110a99e2f80a7b7eb1147ee1efbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hn8eYnwC_rxJ69qBixjJyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">步骤7:识别新的质心，重复这个过程，直到满足指定的条件。图片由作者提供。</p></figure><h1 id="538b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">聚类什么时候停止？</h1><p id="b341" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们重复聚类的过程，直到我们达到某些条件:</p><ul class=""><li id="8ff0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">已经达到最大迭代次数</li><li id="1dd2" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">质心位置或被聚类的点没有变化或变化很小的模型收敛</li></ul><h1 id="3f0e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">确定最佳聚类数</h1><p id="6a8c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们可以使用多种技术来为自己得出一个值，而不是猜测k的数字。这里讨论的两种方法是简单的手动方法，用于确定“k”的最佳值。</p><h2 id="902d" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">肘图</h2><p id="11e7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">肘方法是确定最佳聚类数的最常用方法，因为它简单且易于可视化。</p><p id="d56c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，我们使用不同数量的k多次运行k均值算法，并计算误差平方和的类内和(WSS)。这个特性也被称为<strong class="lb iu">惯性。</strong></p><p id="1939" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了结果，我们就绘制出每个集群的惯性，并确定图表中数据开始“变平”的点。在下面的例子中，我们可以选择5到10之间的一个值作为k的最佳值。</p><p id="be97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，对大量集群执行此操作会增加计算时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/489106e8887bc3c41c109e734a73a5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/0*WMPHmVNfpzIYDHSU.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择最佳聚类数的肘图。<a class="ae ky" rel="noopener" target="_blank" href="/how-to-use-unsupervised-learning-to-cluster-well-log-data-using-python-a552713748b5">作者的形象。</a></p></figure><h2 id="986c" class="nh lw it bd lx ni nj dn mb nk nl dp mf li nm nn mh lm no np mj lq nq nr ml ns bi translated">剪影法</h2><p id="e510" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">剪影方法提供了一种测量方法，用于测量数据点在自己的聚类(内聚)中与其他聚类(分离)的相似程度。</p><p id="9718" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它提供一个介于+1和-1之间的值，越接近+1的值越合适，表示数据点位于正确的聚类内，并且远离其他聚类。</p><p id="8e10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们有多个负值，那么我们可能有太多或太少的集群。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/4c063a766869301791557c3c2fccb9e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*y7_rJfnRXZ45MvR10x54-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于选择k-means聚类的最佳聚类数的剪影分析。图片由作者提供。</p></figure><p id="792e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关这两种方法的更多详细信息，请访问:</p><div class="ny nz gp gr oa ob"><a href="https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">K均值| K均值聚类|寻找K的最佳值的方法</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">对预测不感兴趣(因为我们没有目标/输出变量)。目标是发现…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ks ob"/></div></div></a></div><h1 id="d08c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">改进K均值结果</h1><p id="88b1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果我们再次重复K-means聚类过程，并根据参数，我们可能会得到不同的结果。这是由于被选作初始质心的点的不同。此外，一旦这些点被初始化，它们就很难移动很大的距离或靠近已经相对稳定的集群。</p><p id="34b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">改进结果的一种方法是重复k-means过程，并试图找到所有聚类中方差的最低和。</p><p id="35f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有许多初始化技术可用于k均值聚类，包括:</p><ul class=""><li id="b018" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">选择随机点(示例中使用的点)</li><li id="9daf" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank"> k-means ++ </a></li><li id="266f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://www.kdnuggets.com/2017/03/naive-sharding-centroid-initialization-method.html" rel="noopener ugc nofollow" target="_blank">天真的分片</a></li><li id="91f4" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">最远点启发式</li><li id="0f85" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">启发式排序</li><li id="04bc" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">基于投影</li></ul><h1 id="b831" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">K-均值聚类的优缺点</h1><p id="77fd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">优点:</strong></p><ul class=""><li id="766f" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">一种快速有效的算法</li><li id="f2ac" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">容易理解</li><li id="72f1" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">易于在Python中实现</li><li id="49a7" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">可以扩展到大型数据集</li><li id="3799" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">保证收敛</li></ul><p id="515c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点:</strong></p><ul class=""><li id="fb02" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">在运行算法之前，需要指定k的数值</li><li id="1100" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">取决于质心的初始化方式。一旦质心被初始化，它们就不能被移动很大的距离，或者如果其他集群相对稳定</li><li id="7908" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">对异常值和噪声敏感-异常值会影响质心的初始化</li><li id="c650" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">对于大型数据集，速度可能会成为一个问题</li><li id="f3d6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">随着维度数量的增加，可能会出现问题</li><li id="ef22" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://www.inovex.de/de/blog/disadvantages-of-k-means-clustering/#:~:text=The%20most%20important%20limitations%20of,roughly%20equal%20numbers%20of%20observations" rel="noopener ugc nofollow" target="_blank">假设聚类是球形的，每个聚类具有相似数量的数据点</a></li></ul><h1 id="41bc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Python实现</h1><p id="8fc4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我之前写了一篇文章，介绍了所需的步骤和代码示例。这个例子使用测井数据测量将数据分成不同的组，这些组可以解释为不同的岩性。</p><p id="840e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在下面的链接中找到它。</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/how-to-use-unsupervised-learning-to-cluster-well-log-data-using-python-a552713748b5"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">如何使用Python使用无监督学习对测井数据进行聚类</h2><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="oq l om on oo ok op ks ob"/></div></div></a></div><h1 id="6e74" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="d66d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">总之，K-means算法是一种非常流行的无监督机器学习技术，易于理解和实现。这是根据相似性将数据点分组的有效解决方案，应该在数据分析的探索阶段将其作为一种选择。</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><p id="8c43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oy">感谢阅读。在你走之前，你一定要订阅我的内容，把我的文章放到你的收件箱里。</em> <a class="ae ky" href="https://andymcdonaldgeo.medium.com/subscribe" rel="noopener"> <strong class="lb iu"> <em class="oy">你可以在这里做！</em></strong></a><strong class="lb iu"><em class="oy"/></strong><em class="oy">或者，您可以</em> <a class="ae ky" href="https://fabulous-founder-2965.ck.page/2ca286e572" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="oy">注册我的简讯</em> </strong> </a> <em class="oy">免费获取更多内容直接发送到您的收件箱。</em></p><p id="049e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其次，通过注册会员，你可以获得完整的媒介体验，并支持我和其他成千上万的作家。它每个月只花你5美元，你可以完全接触到所有令人惊叹的媒体文章，也有机会用你的写作赚钱。如果你用 <a class="ae ky" href="https://andymcdonaldgeo.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="oy">我的链接</em></strong></a><strong class="lb iu"><em class="oy"/></strong><em class="oy">报名，你直接用你的一部分费用支持我，不会多花你多少钱。如果你这样做了，非常感谢你的支持！</em></p></div></div>    
</body>
</html>