<html>
<head>
<title>All You Need to Know about Gradient Boosting Algorithm − Part 2. Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于梯度提升算法，您只需知道第 2 部分。分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e#2022-02-07">https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-2-classification-d3ed8f56541e#2022-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2d2b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用例子、数学和代码解释算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7b35bfdf429877e680123bf76bfe57fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJkZbOLK3EM1wFtrNPrl0A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5fe7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在第 1 部分文章的<a class="ae lr" href="https://medium.com/p/2520a34a502" rel="noopener">中，我们详细学习了梯度推进回归算法。正如我们在那篇文章中所评论的，只要损失函数是可微的，算法就足够灵活来处理任何损失函数。这意味着，如果我们只是用处理分类问题的损失函数替换用于回归的损失函数，特别是均方损失，我们就可以在不改变算法本身的情况下执行分类。尽管基本算法是相同的，但我们仍然想知道一些不同之处。在这篇文章中，我们将深入分类算法的所有细节。</a></p><h1 id="6ac4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">算法与实例</h1><p id="595c" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">梯度推进是集成方法的一种变体，其中您创建多个弱模型(它们通常是决策树)并组合它们以获得整体上更好的性能。在本节中，我们将使用非常简单的示例数据构建一个梯度推进分类模型，以便直观地理解其工作原理。</p><p id="1fb9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图是样本数据。它有二进制类<code class="fe mp mq mr ms b"><em class="mt">y</em></code> (0 和 1)和两个特性<code class="fe mp mq mr ms b"><em class="mt">x₁</em></code>和<code class="fe mp mq mr ms b"><em class="mt">x₂</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/6d0901219da34daef7af726fae3f4c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XH3DRC794hXM0EwduNHOSw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类问题示例(图片由作者提供)</p></figure><p id="7219" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的目标是建立一个梯度推进模型来分类这两个类。第一步是对所有数据点的第 1 类概率进行统一预测(我们称之为<code class="fe mp mq mr ms b"><em class="mt">p</em></code>)。统一预测的最合理值可能是第 1 类的比例，这只是一个<code class="fe mp mq mr ms b"><em class="mt">y</em></code>的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6023b5c6935358975aac15eba39ff389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1leQeFha5dLfMjZNG7qtkg.png"/></div></figure><p id="dce2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是数据和初始预测的 3D 表示。这时预测的只是一个在<code class="fe mp mq mr ms b"><em class="mt">y</em></code>轴上一直有统一值<code class="fe mp mq mr ms b"><em class="mt">p = mean(y)</em></code>的平面。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ee277a42c5453b709b3aba9dc69974e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QMHT83Wo0fRjuanp1HnLfQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">预测平面(图片由作者提供)</p></figure><p id="3993" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的数据中，<code class="fe mp mq mr ms b"><em class="mt">y</em></code>的均值是 0.56。因为它大于 0.5，所以所有事物都被分类到具有该初始预测的类 1 中。你们中的一些人可能会觉得这个统一值预测没有意义，但是不要担心。当我们添加更多的弱模型时，我们将改进我们的预测。</p><p id="1947" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了提高我们的预测质量，我们可能希望关注初始预测的残差(即预测误差)，因为这是我们希望最小化的。残差定义为<code class="fe mp mq mr ms b"><em class="mt">rᵢ = yᵢ − p</em></code> ( <code class="fe mp mq mr ms b"><em class="mt">i</em></code>代表每个数据点的索引)。在下图中，残差显示为棕色线，即从每个数据点到预测平面的垂直线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/bf7057531193999276b726a9bc788019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*P6IX85ExVieosRGGsCRx3A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">残差(图片由作者提供)</p></figure><p id="860e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了最小化这些残差，我们正在建立一个回归树模型，以<code class="fe mp mq mr ms b"><em class="mt">x₁</em></code>和<code class="fe mp mq mr ms b"><em class="mt">x₂</em></code>为特征，以残差<code class="fe mp mq mr ms b"><em class="mt">r</em></code> <em class="mt"> </em>为目标。如果我们可以构建一个树，找到<code class="fe mp mq mr ms b"><em class="mt">x</em></code>和<code class="fe mp mq mr ms b"><em class="mt">r</em></code>之间的一些模式，我们就可以通过利用那些找到的模式来减少初始预测<code class="fe mp mq mr ms b"><em class="mt">p</em></code>的残差。</p><p id="1c45" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了简化演示，我们正在构建非常简单的树，每个树只有一个分支和两个终端节点，称为“stump”。请注意，梯度推进树通常有一个稍微深一点的树，比如有 8 到 32 个终端节点的树。</p><p id="0ab5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我们创建第一棵树，用两个不同的值<code class="fe mp mq mr ms b"><em class="mt">r = {0.1, -0.6}</em></code>预测残差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/f80f36a94d750bfdc4a2f65fa7f51ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C15KyrZa-m9pr-miVoCXtA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建的树(作者图片)</p></figure><p id="8e5c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你已经阅读了关于回归算法的文章<a class="ae lr" rel="noopener" target="_blank" href="/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502">，你现在可能会认为我们想要将这些预测值添加到我们的初始预测<code class="fe mp mq mr ms b"><em class="mt">p</em></code>中，以减少其残差，但是分类的情况略有不同。我们添加到初始预测中的值(我们称之为<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> gamma)在以下公式中计算:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/215c42f819740b543281646ecfe2c1de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*3FWoRhi3Pa_qPjwPtM5v5g.png"/></div></figure><p id="4945" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">Σxᵢ∈Rⱼ</em></code>意味着我们正在聚合属于终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ</em></code>的所有样本<code class="fe mp mq mr ms b"><em class="mt">xᵢ</em></code>上的 sigma <code class="fe mp mq mr ms b"><em class="mt">Σ</em></code>中的值。<code class="fe mp mq mr ms b"><em class="mt">j</em></code>代表每个终端节点的索引。您可能会注意到，分数的分子是终端节点<code class="fe mp mq mr ms b"><em class="mt">j</em></code>中残差的总和。我们将在下一节讨论给出这个公式的所有计算，但现在让我们只使用它来计算<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>。下面是<code class="fe mp mq mr ms b"><em class="mt">γ₁</em></code>和<code class="fe mp mq mr ms b"><em class="mt">γ₂</em></code>的计算值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1e7f1c2c941a43adf01a77d8fa40ee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*gqvrlNgqFwNCoMhKLaIMEg.png"/></div></figure><p id="4d09" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>并不是简单的加在我们最初预测的<code class="fe mp mq mr ms b"><em class="mt">p</em></code> <em class="mt">上。</em>相反，我们将<code class="fe mp mq mr ms b"><em class="mt">p</em></code>转换成对数概率(我们将把这个对数概率转换值称为<code class="fe mp mq mr ms b"><em class="mt">F(x)</em></code>)，然后加上<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>。对于那些不熟悉对数优势的人，其定义如下。你可能在逻辑回归中见过它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/14e44f3500aa0d03739a8bd181925d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4-ChUPnJjLm0DG0ysvBVwQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对数概率</p></figure><p id="621e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测更新的另一个调整是，在添加到对数几率转换预测<code class="fe mp mq mr ms b"><em class="mt">F(x)</em></code>之前，通过<strong class="kx ir">学习率</strong> <code class="fe mp mq mr ms b"><em class="mt">ν</em></code>缩小<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>，学习率范围在 0 和 1 之间。这有助于模型不过度拟合训练数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/1044953e68c48a5b24a8d93a3bea9737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Id-ffFq6IFaZ-Gf97Ldpog.png"/></div></figure><p id="de55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个例子中，我们使用了一个相对较大的学习率<code class="fe mp mq mr ms b"><em class="mt">ν = 0.9</em></code>来使优化过程更容易理解，但它通常应该是小得多的值，如 0.1。</p><p id="2bc7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过用实际值替换上式右侧的变量，我们得到了更新的预测值<code class="fe mp mq mr ms b"><em class="mt">F₁(x)</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1fe079ef4b48b93e3b6bc230665e49f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9yIJb8UtPinpijNk8aqZA.png"/></div></figure><p id="bac9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们将 log-odds <code class="fe mp mq mr ms b"><em class="mt">F(x)</em></code>转换回预测概率<code class="fe mp mq mr ms b"><em class="mt">p(x)</em></code>(我们将在下一节介绍如何转换)，它看起来像下面的一个楼梯状物体。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fb7bd5341f1cb6d3a8e232c285259d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*zoKzKYY09WuuuahNKJXlNw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新的预测平面(图片由作者提供)</p></figure><p id="6182" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">紫色平面是初始预测<code class="fe mp mq mr ms b"><em class="mt">p₀</em></code> <em class="mt"> </em>和<em class="mt"> </em>它被更新为红色和黄色平面<code class="fe mp mq mr ms b"><em class="mt">p₁</em></code>。</p><p id="1786" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，更新后的残差<code class="fe mp mq mr ms b"><em class="mt">r</em></code>看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/3d917a4fa6b7ed07bb581bd7858a8531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*CtZmFFDSKh7pBYpI0BkpKw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新的残差(图片由作者提供)</p></figure><p id="0e97" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下一步中，我们使用相同的<code class="fe mp mq mr ms b"><em class="mt">x₁</em></code>和<code class="fe mp mq mr ms b"><em class="mt">x₂</em></code>作为特征，使用更新的残差<code class="fe mp mq mr ms b"><em class="mt">r</em></code>作为目标，再次创建回归树。下面是创建的树:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/1cc970a3a86c973429df6969a92a22b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*puVS09-sOyZvmSq6BdNDhA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建的树(作者图片)</p></figure><p id="b680" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们应用相同的公式来计算<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>。计算的<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>和更新的预测<code class="fe mp mq mr ms b"><em class="mt">F₂(x)</em></code>如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b0c8e2e27b7700b7d298dfca3cc1551a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lwNP3R0wEiSOmVdaxJr-YA.png"/></div></figure><p id="f1bb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，如果我们将对数赔率<code class="fe mp mq mr ms b"><em class="mt">F₂(x)</em></code>转换回预测概率<code class="fe mp mq mr ms b"><em class="mt">p₂(x)</em></code>，它看起来像下面这样。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/0875f1bdf9203a8de6a03091fcb0ed71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*c5yGUZFfHXw7h9jv4eAQVg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">更新的预测平面(图片由作者提供)</p></figure><p id="0da7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们重复这些步骤，直到模型预测停止改进。下图显示了从 0 到 4 次迭代的优化过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/bd8bcbee687d59461135fde37f1336b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5n3NX8MxIA8U8DJerol4lQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过迭代进行预测更新(图片由作者提供)</p></figure><p id="319d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以看到组合预测<code class="fe mp mq mr ms b"><em class="mt">p(x)</em></code>(红色和黄色平面)越来越接近我们的目标<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">y</em></code>，因为我们在组合模型<em class="mt">中添加了更多的树。这就是梯度增强如何通过组合多个弱模型来预测复杂目标。</em></p><p id="4fd2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图总结了算法的全过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/7283bcf22216436768c91bb370707ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uDdVWe5-MJyy27an4Yp8GQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">算法的过程(图片由作者提供)</p></figure><h1 id="d701" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数学</h1><p id="2921" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在本节中，我们将通过查看数学细节来学习一个更一般化的算法。这是数学公式中的整个算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/ea536371fd005d302021c49253f023f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIHrPFBT2fmXuTXMb-3_Xw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:改编自<a class="ae lr" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">维基百科</a>和<a class="ae lr" href="https://jerryfriedman.su.domains/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">弗里德曼的论文</a></p></figure><p id="3322" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们一行一行地仔细看看。</p><h2 id="14af" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">第一步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/78a392945530383aa05c848040653ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VfgT9QsLdzUYgsmuNbF_g.png"/></div></div></figure><p id="cd4a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一步是创建初始常数预测值<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>。<code class="fe mp mq mr ms b"><em class="mt">L</em></code>是损失函数，我们使用<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html" rel="noopener ugc nofollow" target="_blank">对数损失</a>(或者更一般地称为<a class="ae lr" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵损失</a>)来表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/97a71eaa7516d0b1abde73e9bf96062d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OH79_Fng_GT_VfxfmAE-ew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原木损失</p></figure><p id="055b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">yᵢ</em></code>是我们的分类目标，它不是 0 就是 1。<code class="fe mp mq mr ms b"><em class="mt">p</em></code>是 1 类的预测概率。您可能会看到<code class="fe mp mq mr ms b"><em class="mt">L</em></code>根据目标类别<code class="fe mp mq mr ms b"><em class="mt">yᵢ</em></code>采用不同的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/365a4ec8fc5b2f12406c5fb034b45510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ZPoxVyXFHDlnT4IPvprUIA.png"/></div></figure><p id="9e1a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于<code class="fe mp mq mr ms b"><em class="mt">−log(x)</em></code>是<code class="fe mp mq mr ms b"><em class="mt">x</em></code>的递减函数，预测越好(即<code class="fe mp mq mr ms b"><em class="mt">yᵢ=1</em></code>增加<code class="fe mp mq mr ms b"><em class="mt">p</em></code>，我们的损失就越小。</p><p id="f2c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">argmin</em></code> <em class="mt"> </em>是指我们在寻找使<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">ΣL(y</em>ᵢ<em class="mt">,γ)</em></code>最小的值<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> (gamma) </em>。虽然假设<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>是预测概率<code class="fe mp mq mr ms b"><em class="mt">p</em></code>更简单，但我们假设<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>是<strong class="kx ir">对数几率</strong>为<strong class="kx ir"> </strong>，这使得接下来的所有计算更容易。对于那些忘记我们在上一节中回顾的对数优势定义的人，它被定义为<code class="fe mp mq mr ms b"><em class="mt">log(odds) = log(p/(1-p))</em></code>。</p><p id="99db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了能够根据对数优势解决<code class="fe mp mq mr ms b"><em class="mt">argmin</em></code>问题，我们将损失函数转换成对数优势函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5164f90ca7489f814570def77eb2a88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*yex5NL4I2KHiNG4A65JeDw.png"/></div></figure><p id="7083" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可能想用对数概率表示的东西来代替上面等式中的<code class="fe mp mq mr ms b"><em class="mt">p</em></code>。通过转换前面所示的对数优势表达式，<code class="fe mp mq mr ms b"><em class="mt">p</em></code>可以用对数优势表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/22c9cbd01c3352c1a22d4ed2e496a8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Boni0CuZCydx6E1YSEd_Nw.png"/></div></figure><p id="a9ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们用这个值代替前面的<code class="fe mp mq mr ms b"><em class="mt">L</em></code>方程中的<code class="fe mp mq mr ms b"><em class="mt">p</em></code>，并简单地应用它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f7035e6988d0f2a819aea4d83eef892a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*eQ8Di6LdmoNg3qg0Uo0skA.png"/></div></figure><p id="1789" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们正在寻找最小化<code class="fe mp mq mr ms b"><em class="mt">ΣL</em></code>的<em class="mt"> </em> <code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>(请记住我们假设它是对数概率)<em class="mt"> </em>。我们对<code class="fe mp mq mr ms b"><em class="mt">ΣL</em></code>的对数几率求导。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/26b738fc644ec64b25fc0cd4c44cbfa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-xIv0Itb-DqfjzMgh8T1Q.png"/></div></figure><p id="8f89" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的等式中，我们用<code class="fe mp mq mr ms b"><em class="mt">p</em></code>替换了包含对数优势的分数，以简化等式。接下来，我们设置<code class="fe mp mq mr ms b"><em class="mt">∂ΣL/∂log(odds)</em></code>等于 0，并求解<code class="fe mp mq mr ms b"><em class="mt">p</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/7e115bb8a2e7211267e50a5c223ca32f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*K9iyvcBs2Ywi1-wcie6p8g.png"/></div></figure><p id="1573" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个二元分类问题中，<code class="fe mp mq mr ms b"><em class="mt">y</em></code>不是 0 就是 1。所以，<code class="fe mp mq mr ms b"><em class="mt">y</em></code>的均值其实就是 1 类的比例。你现在可能明白为什么我们在最初的预测中使用了<code class="fe mp mq mr ms b"><em class="mt">p = mean(y)</em></code>。</p><p id="0f09" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>是对数概率而不是概率<code class="fe mp mq mr ms b"><em class="mt">p</em></code>，我们将其转换成对数概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/018599a22ea75e2d060156f15c4dbfcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*E__R9qvy3NjbxMd7eOB1Fw.png"/></div></figure><h2 id="909f" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">第二步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/cb09564546370d7d0b83aa27bc1d3637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vs-B8RoktCPn2azTHO7Ew.png"/></div></div></figure><p id="9ff3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从 2–1 到 2–4 的整个步骤 2 过程重复<code class="fe mp mq mr ms b"><em class="mt">M</em></code>次。<code class="fe mp mq mr ms b"><em class="mt">M</em></code>表示我们正在创建的树的数量，小的<code class="fe mp mq mr ms b"><em class="mt">m</em></code>表示每棵树的索引。</p><h2 id="cfc7" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">第 2 步–1</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/83bca0d21d728c7ac1b63bc74f826407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8immBjXmJeArSyzo7nWow.png"/></div></div></figure><p id="9c81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们计算残差<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>的方法是对损失函数相对于之前的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>求导，然后乘以 1。正如您在下标索引中看到的，针对每个单个样本<code class="fe mp mq mr ms b"><em class="mt">i</em></code>计算<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>。你们中的一些人可能想知道为什么我们称之为<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>残差。该值实际上是<strong class="kx ir">负梯度</strong>，为我们提供方向(+/)和幅度，使损失函数最小化。你很快就会明白为什么我们称之为残差。顺便说一下，这种使用梯度来最小化模型损失的技术非常类似于通常用于优化神经网络的梯度下降技术。(其实两者略有不同。如果你有兴趣，请看看<a class="ae lr" href="https://explained.ai/gradient-boosting/descent.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>详述了那个话题。)</p><p id="bb8c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们在这里计算残差。方程中的<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>表示上一步的预测。在第一次迭代中，它是<code class="fe mp mq mr ms b"><em class="mt">F₀</em></code>。和上一步一样，我们对<code class="fe mp mq mr ms b"><em class="mt">L</em></code>的对数比进行求导，而不是 p，因为我们的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>是对数比。下面我们使用在上一步中得到的用对数几率表示的<code class="fe mp mq mr ms b"><em class="mt">L</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/4afd99b807a985439d9edb9b4f98809d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*ovrOuMuOZ4dOSSO2NauR0w.png"/></div></figure><p id="3560" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上一步中，我们也得到了这个等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/376938086e8d1b56fceb9f9e59e87c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*UcJKBaZNn6pcvQ4l7nmfMA.png"/></div></figure><p id="c3b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，我们可以用<code class="fe mp mq mr ms b"><em class="mt">p</em></code>代替<code class="fe mp mq mr ms b"><em class="mt">rᵢ𝑚</em></code>方程中的第二项。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a3d87ae962d62089a906a13716b1b804.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*qYCqpn-KhxGhlr9VWuFVkg.png"/></div></figure><p id="e71e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你现在可能明白为什么我们称之为<code class="fe mp mq mr ms b"><em class="mt">r</em></code>残差了。这也给了我们有趣的见解，即负梯度为我们提供了方向和大小，使损失最小化，实际上只是残差。</p><h2 id="0bbe" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">步骤 2–2</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/ae7167e91224d26b3de25f692aa0bcaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WRZuGsljLPAWeKZ9c3r2Q.png"/></div></div></figure><p id="557c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe mp mq mr ms b"><em class="mt">j</em></code>表示树中的末端节点(即叶子)<code class="fe mp mq mr ms b"><em class="mt">m</em></code>表示树索引，大写<code class="fe mp mq mr ms b"><em class="mt">J</em></code>表示叶子总数。</p><h2 id="ff30" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">第 2-3 步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/c9cd6dab906a4627020222d2b9bc2fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FM9KvlIC70j-8UsIvPFtoA.png"/></div></div></figure><p id="83e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们正在寻找使每个终端节点<code class="fe mp mq mr ms b"><em class="mt">j</em></code>上的损失函数最小化的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>。<code class="fe mp mq mr ms b"><em class="mt">Σxᵢ∈Rⱼ𝑚 L</em></code>表示我们正在合计属于终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ𝑚</em></code>的所有<code class="fe mp mq mr ms b"><em class="mt">xᵢ</em></code>的损失。让我们把损失函数代入方程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/37e6f766377797e05721746fc5e6eb6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*S-5_kOexAtNHLExmZe1eMA.png"/></div></figure><p id="6366" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>解这个方程将会非常困难。为了更容易解决，我们使用二阶泰勒多项式<a class="ae lr" href="https://en.wikipedia.org/wiki/Taylor_series" rel="noopener ugc nofollow" target="_blank">来近似<code class="fe mp mq mr ms b"><em class="mt">L</em></code>。泰勒多项式是一种将任何函数近似为具有无限/有限项数的多项式的方法。虽然我们在这里不研究它的细节，但是如果你感兴趣的话，你可以看看</a><a class="ae lr" href="https://activecalculus.org/single/sec-8-5-taylor.html" rel="noopener ugc nofollow" target="_blank">这篇教程</a>，它很好地解释了这个想法。</p><p id="0721" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是使用二阶泰勒多项式的<code class="fe mp mq mr ms b"><em class="mt">L</em></code>的近似值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/8dce422b3b956aa9febd2716ff3bcd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Fyye_YraB1QDyjAV_seRw.png"/></div></figure><p id="7c81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们用这个近似值代替等式<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>中的<code class="fe mp mq mr ms b"><em class="mt">L</em></code>，然后找到使σ(*)的导数等于零的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d3d32e6a1c4c95f472f27157e6c36be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*S3Dk_7h-nOA6Jx0tnewC0Q.png"/></div></figure><p id="3dc9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们在上一步中已经计算出的<code class="fe mp mq mr ms b"><em class="mt">∂L/∂F</em></code>,如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a438d8a89f8875798b1ef7edb508f2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ux9FoJIgycc3m3Hep4DvBA.png"/></div></figure><p id="1162" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们用这个代替<code class="fe mp mq mr ms b"><em class="mt">γ</em></code> <em class="mt"> </em>等式中的<code class="fe mp mq mr ms b"><em class="mt">∂L/∂F</em></code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/27e5679a7817cd88ff720bc68eed34d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AukIVteODZwog6MZ_ZEWPg.png"/></div></div></figure><p id="f74d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们得到了上一节中使用的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>值的简化公式。</p><h2 id="2c4c" class="ne lt iq bd lu nf ng dn ly nh ni dp mc le nj nk me li nl nm mg lm nn no mi np bi translated">第 2-4 步</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/d5bf745406685806542968ffa26b5199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvLO78g0uMntR3T6vVqOpA.png"/></div></div></figure><p id="6ecc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在最后一步，我们正在更新组合模型<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>的预测。<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚1(x ∈ Rⱼ𝑚)</em></code>意味着如果给定的<code class="fe mp mq mr ms b"><em class="mt">x</em></code>落在终端节点<code class="fe mp mq mr ms b"><em class="mt">Rⱼ𝑚</em></code>中，我们选择值<code class="fe mp mq mr ms b"><em class="mt">γⱼm</em></code>。由于所有的终端节点都是排他的，任何给定的单个<code class="fe mp mq mr ms b"><em class="mt">x</em></code>只落入一个终端节点，相应的<code class="fe mp mq mr ms b"><em class="mt">γⱼ𝑚</em></code>被添加到先前的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚-₁</em></code>中，然后进行更新的预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>。</p><p id="c443" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，<code class="fe mp mq mr ms b"><em class="mt">ν</em></code>是范围在 0 和 1 之间的学习率，其控制附加树预测<code class="fe mp mq mr ms b"><em class="mt">γ</em></code>对组合预测<code class="fe mp mq mr ms b"><em class="mt">F𝑚</em></code>的贡献程度。较小的学习率降低了额外的树预测的效果，但是它基本上也降低了模型过度适应训练数据的机会。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="dbcf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们已经完成了所有步骤。为了获得最佳的模型性能，我们希望迭代第 2 步<code class="fe mp mq mr ms b"><em class="mt">M</em></code>次，这意味着向组合模型添加<code class="fe mp mq mr ms b"><em class="mt">M</em></code>树。实际上，您可能经常想要添加超过 100 棵树来获得最佳的模型性能。</p><p id="1dc6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你读了我关于回归算法的文章，你可能会觉得分类算法的数学计算比回归复杂得多。虽然<code class="fe mp mq mr ms b"><em class="mt">argmin</em></code>和对数损失函数的导数计算复杂，但本节开头所示的基本数学算法完全相同。这实际上是梯度推进算法的优雅之处，因为完全相同的算法适用于任何损失函数，只要它是可微分的。事实上，流行的梯度增强实现如<a class="ae lr" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或<a class="ae lr" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>有各种各样的损失函数，因此您可以选择适合您问题的任何损失函数(参见<a class="ae lr" href="https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>或<a class="ae lr" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>中提供的各种损失函数)。</p><h1 id="c53e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">密码</h1><p id="d0cb" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在这一节中，我们将把刚刚复习过的数学知识转化为可行的 python 代码，以帮助我们进一步理解算法。我们使用 scikit-learn 的<code class="fe mp mq mr ms b">DecisionTreeRegressor</code>来构建树，这有助于我们只关注梯度推进算法本身，而不是树算法。我们正在模仿 scikit-learn 风格的实现，其中您使用<code class="fe mp mq mr ms b">fit</code>方法训练模型，并使用<code class="fe mp mq mr ms b">predict</code>方法进行预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在</p></figure><p id="591d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，所有经过训练的树都存储在<code class="fe mp mq mr ms b">self.trees</code>列表对象中，当我们使用<code class="fe mp mq mr ms b">predict_proba</code>方法进行预测时，会检索到这些树。</p><p id="9618" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们检查我们的<code class="fe mp mq mr ms b">CustomGradientBoostingClassifier</code>是否与 scikit-learn 的<code class="fe mp mq mr ms b">GradientBoostingClassifier</code>表现相同，方法是查看它们在我们数据上的日志损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/102f40dda7826e618ddeadc6fe3ea854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*r7C96KU-V3SwTssnIeCq3w.png"/></div></figure><p id="434a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如您在上面的输出中看到的，两个模型有完全相同的日志损失。</p><h1 id="04e8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">推荐资源</h1><p id="77a8" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在这篇博文中，我们回顾了梯度推进分类算法的所有细节。如果您也对回归算法感兴趣，请查看第 1 部分的文章。</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ir gy z fp on fr fs oo fu fw ip bi translated">关于梯度提升算法，您只需知道第 1 部分。回归</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">用例子、数学和代码解释算法</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow kp oi"/></div></div></a></div><p id="7a36" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您想了解该算法的更多细节，还有一些其他的好资源:</p><ul class=""><li id="e803" class="ox oy iq kx b ky kz lb lc le oz li pa lm pb lq pc pd pe pf bi translated"><strong class="kx ir"> StatQuest，Gradient boosting</strong><a class="ae lr" href="https://www.youtube.com/watch?v=jxuNLH5dXCs" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">Part 3</strong></a><strong class="kx ir">和</strong> <a class="ae lr" href="https://www.youtube.com/watch?v=StWY5QWMXCw" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir"> Part 4 </strong> </a> <br/>这些是 YouTube 上的视频，以初学者友好的方式解释了具有很好视觉效果的梯度提升分类算法。</li><li id="20cf" class="ox oy iq kx b ky pg lb ph le pi li pj lm pk lq pc pd pe pf bi translated"><strong class="kx ir">特伦斯·帕尔和杰瑞米·霍华德，</strong> <a class="ae lr" href="https://explained.ai/gradient-boosting/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">如何解释梯度推进</strong> </a> <strong class="kx ir"> <br/> </strong>虽然本文的重点是梯度推进回归而不是分类，但它很好地解释了算法的每个细节。</li><li id="9e01" class="ox oy iq kx b ky pg lb ph le pi li pj lm pk lq pc pd pe pf bi translated"><strong class="kx ir">杰罗姆弗里德曼，</strong> <a class="ae lr" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">贪婪函数逼近:一个梯度推进机</strong></a><strong class="kx ir"><em class="mt"><br/></em></strong>这是弗里德曼的论文原文。虽然有点难以理解，但它确实展示了算法的灵活性，他展示了一种通用算法，可以处理任何类型的具有可微损失函数的问题。</li></ul><p id="9530" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你也可以在 Google Colab 链接或者下面的 Github 链接中查看完整的 Python 代码。</p><div class="of og gp gr oh oi"><a href="https://colab.research.google.com/drive/13p46IFhg3h6BIdjxUcfXPco13jIOCV6I?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ir gy z fp on fr fs oo fu fw ip bi translated">谷歌联合实验室</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">梯度推进算法——第二部分。分类</h3></div></div><div class="or l"><div class="pl l ot ou ov or ow kp oi"/></div></div></a></div><div class="of og gp gr oh oi"><a href="https://github.com/tomonori-masui/gradient-boosting/blob/main/gradient_boosting_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd ir gy z fp on fr fs oo fu fw ip bi translated">梯度增强算法-在 tomo nori-masui/梯度增强下分类</h2><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">github.com</p></div></div><div class="or l"><div class="pm l ot ou ov or ow kp oi"/></div></div></a></div><h1 id="2aae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><ul class=""><li id="5a9c" class="ox oy iq kx b ky mk lb ml le pn li po lm pp lq pc pd pe pf bi translated">杰罗姆·弗里德曼，<a class="ae lr" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">贪婪函数逼近:一个梯度推进机</a></li><li id="2177" class="ox oy iq kx b ky pg lb ph le pi li pj lm pk lq pc pd pe pf bi translated">特伦斯·帕尔和杰瑞米·霍华德，<a class="ae lr" href="https://explained.ai/gradient-boosting/index.html" rel="noopener ugc nofollow" target="_blank">如何解释梯度推进</a></li><li id="986f" class="ox oy iq kx b ky pg lb ph le pi li pj lm pk lq pc pd pe pf bi translated">Matt Bowers，<a class="ae lr" href="https://blog.mattbowers.dev/gradient-boosting-machine-from-scratch" rel="noopener ugc nofollow" target="_blank">如何从零开始建立一个梯度推进机</a></li><li id="ce7d" class="ox oy iq kx b ky pg lb ph le pi li pj lm pk lq pc pd pe pf bi translated">维基百科，<a class="ae lr" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">渐变提升</a></li></ul></div></div>    
</body>
</html>