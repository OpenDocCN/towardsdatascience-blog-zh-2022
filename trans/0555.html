<html>
<head>
<title>Best Practices for Neural Network Exports to ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向ONNX导出神经网络的最佳实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/best-practices-for-neural-network-exports-to-onnx-99f23006c1d5#2022-02-22">https://towardsdatascience.com/best-practices-for-neural-network-exports-to-onnx-99f23006c1d5#2022-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f84f9675127d45290db491188fa54247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pvmTXmnxYqHKx-Zz"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Artem Sapegin 在<a class="ae kc" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><blockquote class="kd ke kf"><p id="6d40" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">ONNX是一种开放格式，用于表示机器学习模型。ONNX定义了一组通用的运算符——机器学习和深度学习模型的构建块——和一种通用的文件格式，使AI开发人员能够将模型与各种框架、工具、运行时和编译器一起使用。</p><p id="d60d" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">— <a class="ae kc" href="https://github.com/Q-AMeLiA/documents/blob/main/onnx.ai" rel="noopener ugc nofollow" target="_blank"> onnx.ai </a></p></blockquote><h1 id="282f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">为什么要导出到ONNX？</h1><p id="72d4" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">将您的模型导出到ONNX有助于您将(经过训练的)模型从项目的其余部分中分离出来。此外，导出还避免了对python解释器、框架版本、使用的库等环境的依赖。导出的ONNX-model可以存储模型的架构和参数。这意味着向您的同事发送一个文件来交换您的模型就足够了。</p><h1 id="b62f" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">出口</h1><p id="0ee9" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们的经验表明，出口PyTorch模型更容易。如果可能的话，选择PyTorch源，并使用内置的<code class="fe ml mm mn mo b">torch.onnx</code>模块进行转换。或者，您可以使用较新的独立<code class="fe ml mm mn mo b">onnx</code> python包(在下面的代码示例中，只需用<code class="fe ml mm mn mo b">onnx</code>替换<code class="fe ml mm mn mo b">torch.onnx</code>)。</p><h2 id="f6bf" class="mp lg iq bd lh mq mr dn ll ms mt dp lp mf mu mv lt mh mw mx lx mj my mz mb na bi translated">来自PyTorch</h2><p id="45f1" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">PyTorch模型只能以编程方式导出:</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="ca4e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">请注意，PyTorch在运行时计算计算图，因此转换引擎需要一批正确的形状(数据在大多数情况下可以是随机的)，它将通过网络来理解架构。<code class="fe ml mm mn mo b">torch.onnx</code>使用<code class="fe ml mm mn mo b">torch.jit.trace</code>找到您的数据通过网络的路径。</p><p id="b2f8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">最佳实践:</em></p><ul class=""><li id="0844" class="nh ni iq kj b kk kl ko kp mf nj mh nk mj nl le nm nn no np bi translated">小心<code class="fe ml mm mn mo b">TracerWarnings</code>。这可能表明追踪器无法跟踪您的批次。</li><li id="ef43" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">如果您在运行时做出路由决策，请确保使用一个批处理/配置来处理您尝试导出的所有路由。</li><li id="e15b" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">如果你需要做路线决定，你应该在张量上做。tracer找不到Pythons的默认类型。</li><li id="39aa" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">如果您正在使用torchhub模型，请检查它们是否提供了一个<code class="fe ml mm mn mo b">exportable</code>参数(或类似的)来替换不兼容的操作。</li></ul><h2 id="8733" class="mp lg iq bd lh mq mr dn ll ms mt dp lp mf mu mv lt mh mw mx lx mj my mz mb na bi translated">来自TensorFlow (1/2/lite/js/tf。Keras)</h2><p id="b8e1" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们推荐微软<code class="fe ml mm mn mo b"><a class="ae kc" href="https://github.com/onnx/tensorflow-onnx" rel="noopener ugc nofollow" target="_blank">tf2onnx</a></code>包用于TensorFlow模型的转换。在ONNX导出之前，必须将模型存储为TensorFlows支持的文件格式之一。支持的格式包括<code class="fe ml mm mn mo b">saved model</code>、<code class="fe ml mm mn mo b">checkpoint</code>、<code class="fe ml mm mn mo b">graphdef</code>或<code class="fe ml mm mn mo b">tflite</code>。</p><p id="db07" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">将<em class="ki">保存的模型</em>文件导出到ONNX:</p><pre class="nb nc nd ne gt nv mo nw nx aw ny bi"><span id="a98f" class="mp lg iq mo b gy nz oa l ob oc">python -m tf2onnx.convert --saved-model tensorflow-model-path --output model.onnx</span></pre><p id="333c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">这是为<em class="ki"> tflite </em>所做的事情(或者使用<a class="ae kc" href="https://pypi.org/project/tflite2onnx/" rel="noopener ugc nofollow" target="_blank"> <em class="ki"> tflite2onnx </em> </a>):</p><pre class="nb nc nd ne gt nv mo nw nx aw ny bi"><span id="9b38" class="mp lg iq mo b gy nz oa l ob oc">python -m tf2onnx.convert --opset 13 --tflite tflite--file --output model.onnx</span></pre><p id="fdfc" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">对于其他格式，您需要提供输入和输出张量的名称。tf2onnx将使用它们来跟踪网络。提供错误或不完整的标签列表可能会导致导出损坏。如果不知道模型的输入和输出节点名，可以使用<a class="ae kc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms" rel="noopener ugc nofollow" target="_blank">summary _ graph</a>tensor flow实用程序。以下是安装和使用它的方法:</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="50ee" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">或者，从显著位置检查源项目或询问原作者。对于下面的例子，我们假设有两个名为<code class="fe ml mm mn mo b">input0:0,input1:0</code>的张量流输入和一个名为<code class="fe ml mm mn mo b">output0:0</code>的输出。</p><p id="262d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">对于<em class="ki">检查点</em>格式:</p><pre class="nb nc nd ne gt nv mo nw nx aw ny bi"><span id="63e1" class="mp lg iq mo b gy nz oa l ob oc">python -m tf2onnx.convert --checkpoint tensorflow-model-meta-file-path --output model.onnx --inputs input0:0,input1:0 --outputs output0:0</span></pre><p id="b11d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">对于<em class="ki"> graphdef </em>格式:</p><pre class="nb nc nd ne gt nv mo nw nx aw ny bi"><span id="31c2" class="mp lg iq mo b gy nz oa l ob oc">python -m tf2onnx.convert --graphdef tensorflow-model-graphdef-file --output model.onnx --inputs input0:0,input1:0 --outputs output0:0</span></pre><p id="3704" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">注意:</em>导出供重用的模型(如TensorFlow Hub模型)不可使用<code class="fe ml mm mn mo b">summarize_graphs</code>进行分析，可能根本无法导出。</p><h1 id="40e3" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">潜在的出口障碍</h1><p id="5735" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">ONNX协议或所使用的转换器可能不支持源模型的所有操作。</p><p id="5ef1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">可能的解决方案:</em></p><ul class=""><li id="c4c4" class="nh ni iq kj b kk kl ko kp mf nj mh nk mj nl le nm nn no np bi translated">检查是否有更新的<a class="ae kc" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md" rel="noopener ugc nofollow" target="_blank">操作集版本</a> (opset)支持您的相关操作。</li><li id="8b43" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">检查不支持的操作是否可以被支持的操作替换，例如，通常用<code class="fe ml mm mn mo b">ReLU</code>替换<code class="fe ml mm mn mo b">Hardswish</code>或<code class="fe ml mm mn mo b">SiLU</code>激活(从opset 11开始)。</li><li id="a060" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">如果您的转换器没有映射该操作，但ONNX协议支持该操作，请实现映射或尝试不同的转换器。</li><li id="acf3" class="nh ni iq kj b kk nq ko nr mf ns mh nt mj nu le nm nn no np bi translated">如果ONNX协议不支持该操作，请尝试使用支持的操作重写您的操作，或者使用支持的操作实现映射。另外，考虑向ONNX 提交一份<a class="ae kc" href="https://github.com/onnx/onnx/blob/master/docs/AddNewOp.md" rel="noopener ugc nofollow" target="_blank"> PR。</a></li></ul><h1 id="a4f9" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">验证导出的模型</h1><p id="42a8" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">我们建议使用PyTorch加载模型，并使用内置的验证引擎。</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="5e14" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">该代码块将只验证模式。这并不保证您的架构是完整的，也不保证所有的参数都被(正确地)导出。因此，我们建议您在几个样本上运行推理，并将它们与您原始框架的推理进行比较。请注意，由于导出过程和潜在的不同执行框架，可能会略有不同。要使用推理，请确保安装带有<code class="fe ml mm mn mo b">pip</code>的<code class="fe ml mm mn mo b">onnxruntime</code> python包或您的python包管理器。</p><figure class="nb nc nd ne gt jr"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="aee2" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated">此外，您可以通过使用像<a class="ae kc" href="https://netron.app/" rel="noopener ugc nofollow" target="_blank"> Netron </a>这样的外部工具可视化导出的模型来运行健全性检查。Netron还允许您浏览存储的参数。请记住，转换器可能会根据需要减少或扩大操作，因此可能会触及原始架构。<em class="ki">注意:</em> Netron在加载大型模型时可能会有问题。</p><h1 id="bd3c" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">至理名言</h1><p id="a946" class="pw-post-body-paragraph kg kh iq kj b kk md km kn ko me kq kr mf mg ku kv mh mi ky kz mj mk lc ld le ij bi translated">无论您是否选择ONNX，都要考虑发布您训练好的模型，尤其是如果您正在准备一份科学出版物。这有助于他人轻松复制你的发现，也为他们的项目提供了一个良好的开端。</p></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="5e71" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr mf kt ku kv mh kx ky kz mj lb lc ld le ij bi translated"><em class="ki">这项工作得到了德国巴登-符腾堡州(MWK)科学、研究和艺术部的部分资助，资助项目为32–7545.20/45/1</em><a class="ae kc" href="https://q-amelia.in.hs-furtwangen.de/" rel="noopener ugc nofollow" target="_blank"><em class="ki">机器学习应用的质量保证(Q-AMeLiA) </em> </a> <em class="ki">。</em></p></div></div>    
</body>
</html>