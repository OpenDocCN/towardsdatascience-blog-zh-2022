<html>
<head>
<title>5-Minute Paper Explanations: Food AI Part III</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5分钟的书面解释:食品人工智能第三部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-minute-paper-explanations-food-ai-part-iii-bd7256473c4d#2022-08-22">https://towardsdatascience.com/5-minute-paper-explanations-food-ai-part-iii-bd7256473c4d#2022-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0e8d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">im2recipe相关论文“跨通道检索和合成(X-MRS):缩小共享表征学习中的通道差距”的直观深度探讨</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f8d92b5a2519e5a4a22e9c549e881cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sId4RP3Us-OcrpcD"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@luisabrimble?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Luisa Brimble </a>拍摄的照片</p></figure><h1 id="e802" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">问题简介</h1><p id="4e64" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">欢迎来到人工智能食品系列论文的第三部分！</p><p id="d18f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-i-9276b61873c1">第一部分</a>:“学习烹饪食谱和食物图像的跨模态嵌入”</p><p id="e66d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-ii-c085b2789bd1">第二部分</a>:“分而治之的跨模态配方检索:从最近邻基线到SoTA”</p><p id="139d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如在以前的文章中提到的，这些解释旨在绘制机器学习的特定领域的研究进展。因此，今天，我们将关注2020年发表的题为“跨模态检索和合成(X-MRS):弥合共享表征学习中的模态差距”的<a class="ae kv" href="https://arxiv.org/abs/2012.01345" rel="noopener ugc nofollow" target="_blank">论文</a>。本文通过使用基于文本编码器<strong class="lq ir">的</strong>而不是原文中的LSTM，以及本文<a class="ae kv" href="https://arxiv.org/abs/1911.12763" rel="noopener ugc nofollow" target="_blank">中的平均单词嵌入</a>(在<a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-ii-c085b2789bd1">第二部分</a>中解释)，对本文<a class="ae kv" href="http://pic2recipe.csail.mit.edu/im2recipe.pdf" rel="noopener ugc nofollow" target="_blank">中介绍的</a>和本系列第一部分中解释的<a class="ae kv" rel="noopener" target="_blank" href="/5-minute-paper-explanations-food-ai-part-i-9276b61873c1">中的im2recipe问题进行了进一步研究。此外，X-MRS的作者使用<strong class="lq ir">多语言翻译来规范模型</strong>，同时添加多语言功能，并通过生成食物图像的生成模型来显示学习嵌入的<strong class="lq ir">力量。</strong></a></p><h1 id="f76e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">相关工作和改进</h1><p id="522d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正如我们在本系列的前几部分中所看到的，以前在跨模态菜谱任务上的工作要么是端到端训练的模型，要么是预训练的编码器，这些编码器根据菜谱数据进行微调，在这些数据之上应用了跨模态对齐模块。在后一种情况下，也有对每个文本组件进行编码的方法(即标题、成分和说明)，然后连接各个嵌入物。</p><p id="cfb1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">以前的工作还使用了一个正则化模块，主要使用一些外部数据来建立对学习到的表示的分类或聚类任务。其他一些我们没有看到的论文使用GANs来正则化模型。你可能会问，这是如何实现的？嗯，基于GAN的性能，即它生成真实食物图像的能力，正在学习的表示将被修改。在这里，gan有效地充当解码器(从嵌入到实际图像)，并且“在高维空间中学习解码器是一项复杂的任务，这可能导致次优表示。”</p><p id="3063" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者与前述作者的方法不同之处在于:1)他们没有独立地处理和编码文本数据的不同部分；2)他们的模型是端到端训练的，不是预训练的；3)他们不使用GANs或设置分类任务来正则化所学习的表征。相反，他们使用多语言编码和反向翻译。</p><p id="e2d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这为关注将im2recipe应用程序应用于世界各地不同语言的真实用例提供了机会。回译是一个概念，例如，将一个句子从英语翻译成法语，然后再翻译回英语。然后，实际上是彼此释义的原始英语句子和“回译”英语句子可以被比较并用于调整</p><p id="84e0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">旁白:目前世界上许多著名的超大型生成式图像模型都是文本诱导的条件图像合成模型。这意味着这些模型被训练成基于给它们的一些文本描述的编码来生成图像。这些当前的模型可以接受描述图像和合成图像的长描述，但是X-MRS论文的作者参考了论文，并且参考了这种类型的模型的文本描述必须很短才能使它们工作的时代。此外，没有一个文本描述像配方成分和说明那样，具有随时间而被修改的固有顺序。这是作者在从学习到的嵌入构建他们自己的图像合成模型时所做的改进。</p><h1 id="4a7c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">体系结构</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/bf2e45bc6bb62458a60b00241b30fb35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUrFLPRpZFH0BJ8v2YkIBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型架构(作者图片，来自论文)</p></figure><h2 id="1042" class="mq kx iq bd ky mr ms dn lc mt mu dp lg lx mv mw li mb mx my lk mf mz na lm nb bi translated">编码器</h2><p id="198b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了对图像进行编码，使用ResNet-50。“ResNet-50的最后一个全连接层由2，1024维全连接层代替，最后一个层与文本编码器共享”。</p><p id="506b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">每个组件的文本数据不会单独处理。相反，标题、说明和成分被视为一个长文本描述，使用<a class="ae kv" href="https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece" rel="noopener ugc nofollow" target="_blank">单词块</a>标记器进行标记，嵌入到768的维度大小中，并通过2层2头变压器编码器(重点:只是变压器的编码器)。注意，这里记号赋予器的输出还包括一个<a class="ae kv" href="https://stackoverflow.com/questions/62705268/why-bert-transformer-uses-cls-token-for-classification-instead-of-average-over" rel="noopener ugc nofollow" target="_blank">【CLS】(分类)记号</a>，它们都被修剪成只有512个记号来控制模型的内存占用。</p><p id="b856" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那些使用过变形金刚的人会知道,[CLS]记号可以被认为是一个集合序列表示，这也是作者们所做的。[CLS]令牌的输出“在最终的1024维FC层将文本编码投影到共享表示空间之前，通过两个1024维FC层”。</p><h2 id="b5eb" class="mq kx iq bd ky mr ms dn lc mt mu dp lg lx mv mw li mb mx my lk mf mz na lm nb bi translated">GAN合成模块</h2><p id="573a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">使用GAN背后的逻辑是双重的:1)如果学习的表示足够好，则GAN应该能够生成图像，除了图像是真实的之外，该图像实际上显示图像中的配方成分；2)只要在独立应用程序中使用经过训练的GAN，如果提供了配料和说明，即使训练数据中没有，我们也能够生成给定配方的真实图像。</p><p id="54c0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">GAN模块基于StackGAN，其中“中间分辨率图像被丢弃”。GAN的鉴别器也有一个recipe分类器，类似于第1部分中看到的利用从Recipe1M和其他数据集构造的类信息的分类器。接下来，如前所述，GAN必须以文本为条件来生成图像。因此，在该模块中，配方编码通过“条件增强子网络”传递以创建条件代码，该条件代码被传递给解码器(与噪声一起)以生成图像。然后，鉴别器试图区分真假，并将图像归入正确的食谱类别。</p><h1 id="1f69" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">损失函数</h1><p id="45dc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">注意:为了更好地理解下面的内容，添加了一些符号。有关所有符号，请参考纸张。</p><h2 id="54a3" class="mq kx iq bd ky mr ms dn lc mt mu dp lg lx mv mw li mb mx my lk mf mz na lm nb bi translated">编码器</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/4cb9ea5413de4ddfc313d106af935695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EufyOnFwJH9lIN9MdL4WA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">编码损失函数(图片由作者提供，来自论文)</p></figure><p id="1316" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了学习表示，编码器使用简单的具有余量损失的三元组，其中使用余弦相似性函数计算锚、阳性和阴性之间的相似性。该锚可以是图像或文本编码，正面将是来自相同配方的相应文本或图像编码，而反面将是来自不同配方的图像或文本编码。</p><p id="3311" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">锚、阳性和阴性遵循的采样策略是硬阴性挖掘。在这种情况下，被选择的否定是不同的类别，但是它与所有其他否定中的锚具有最高的相似性。</p><h2 id="676b" class="mq kx iq bd ky mr ms dn lc mt mu dp lg lx mv mw li mb mx my lk mf mz na lm nb bi translated">开始</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/d8e1caf4a01bc5b28419439f358c05b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJ90CToekzhdRU9jqevhNQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鉴别器训练损失(图片由作者提供，来自论文)</p></figure><p id="51b3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如前所述，鉴别器试图区分真假，并将图像分类到正确的食谱类别中。将对应于这两个任务的损耗相加，得到最终的鉴频器损耗。损失是简单的交叉熵损失。</p><p id="c0fa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> <em class="ne"> x </em> ₁ </strong>是实际的图像编码，<strong class="lq ir"> <em class="ne"> G(v₂，z) </em> </strong>是生成的图像其中<strong class="lq ir"> <em class="ne"> z </em> </strong>是高斯噪声。<strong class="lq ir"><em class="ne">【e(x₁~ x₁data】</em></strong>表示数据来自图像编码器。<strong class="lq ir"><em class="ne">【e(v₂~ v₂data)</em></strong>表示数据来自文本编码器，然后传递给生成器。<strong class="lq ir"> <em class="ne"> Dᵣ </em> </strong>和<strong class="lq ir"> <em class="ne"> D𝒸 </em> </strong>是鉴别器内部的子网络，分别代表对真假和配方类别进行分类的网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/57393b2cabc991aa63398fd58bb2e73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2ZaQms9cYdtv68zOmkEzg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">发电机培训损失(图片由作者提供，来自论文)</p></figure><p id="e3c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">发电机损耗基本上等于1减去鉴别器损耗加上来自条件扩充(CA)子网络的正则项和监控项<strong class="lq ir"> <em class="ne"> ret。</em>后半部分非常简单明了。</strong></p><h1 id="d07c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">实验和结果</h1><p id="653d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">实验在Recipe1M数据集上进行。“R1M英语(EN)食谱通过反向翻译从德语(EN- DE-EN)和俄语(EN-RU-EN)扩充而来。使用来自<a class="ae kv" href="https://fairseq.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> fairseq </a>神经翻译工具箱的预训练模型获得德语、俄语和法语之间的翻译。</p><p id="0f66" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">检索结果使用与第二部分相同的指标进行报告。对于GAN合成模型，除了报告对合成食品图像的检索性能外，还计算了Fretchet初始距离(FID)分数，该分数用于测量真实和合成图像分布之间的相似性FID值越低表示性能越好。</p><p id="4a98" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在训练和评估期间，作者进行的一些增强是:</p><p id="e4b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">图像增强:</strong> 1)具有一个以上样品配方的随机输入图像。2)随机选择，填充(在零、边缘复制或图像反射之间随机选择)为正方形，并调整大小为256，或调整大小为256。3)随机旋转10♀4)随机裁剪到224。5)随机水平翻转。</p><p id="3fb6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">配方扩充:</strong> 1)在原始en表示和EN-DE-EN或EN-RU-EN中的反向翻译之间随机选择。2)在先前的en选项和KO、DE、RU或FR之间随机选择。</p><p id="ef60" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一如既往，准确的结果可以从论文中引用。在这里，我们重点分析结果。一个非常重要的结果是，带有基于transformer的文本编码器的X-MRS模型无法超越我们在本系列第二部分中走过的基线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/0f4da356c6ae53fac62193f63ac6b7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fiW58tqoQXUnKJQTjqEGbg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">受关注的图像到配方定性前3名检索结果。最左边的图像是带有相应配方的查询图像。在这两种语言中，我们可以看到文本注意力图集中在最能描述图像的事物上。(图片由作者提供，来自论文)</p></figure><p id="dc4d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作者还进行了消融研究，使用不同的文本数据成分测试检索性能。据观察，使用所有的信息导致最佳性能(咄！)，其次是说明+成分。这使得标题信息最少。还是那句话，不奇怪！很早的食物分类研究就已经开始仅仅使用标题，并没有取得很大进展。<strong class="lq ir">这里一个有趣的事情是使用数据集中的完整成分文本，而不仅仅是提取的成分名称会导致更好的检索性能。</strong></p><p id="5f5f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">还显示了不同语言的检索结果。这里应该注意的是，该论文的总体性能最好的检索模型也优于CkNN基线，它是在多语言数据上训练的，并使用英语进行检索。而且论文中也没有提到，到底是如何进行多语种培训的。</p><p id="b9af" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于合成模块，测试生成图像的真实性和给定合成图像的配方的检索性能。据观察，有一些模型在生成更真实的食物图像方面做得更好，但是在多语言数据上训练的所提出的模型在检索方面最好。另外，使用合成图像的检索比使用真实图像的检索更好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/7e654a2c7ee4756e1e10635da46f1728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ob_u5l1E1Zn_Fy3lD_HGkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像合成的例子。(图片由作者提供，来自论文)</p></figure><p id="5b93" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因为生成器被训练为基于完整的食谱嵌入来生成食物图像，这意味着它也具有相应的文本信息，所以它比使用原始图像(不包含文本信息)时更容易具有更高的召回率。因此，为了测试图像-文本学习表示的纯度，作者进行了一项实验，他们训练生成器根据图像嵌入(而不是文本嵌入)来生成图像。他们发现生成图像的真实性没有下降，但检索性能却下降了。这是有意义的，因为现在生成的图像不包含文本信息，这表明文本和图像嵌入是独立的，但是在共享空间中足够接近，不会影响生成的图像的真实性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/136e1daed893e383f7d80f6406c326ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOGBl97ADzpWTwcvFaVACA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用不同语言的文本数据进行图像合成(图片由作者提供，来自论文)</p></figure><h1 id="0759" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">个人想法</h1><p id="713f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">没有多语言培训的X-MRS模型无法超越第二部分中的CkNN基线，这一事实继续显示了强大的理论上合理的管道的力量，无论它使用传统方法还是现代“酷”的深层网络。也就是说，转换器的使用已经在管道中适当地完成了，在多语言数据和反向翻译的帮助下增加和规范化的想法是一个很酷的想法，也是可行的。</p><p id="7371" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是一篇更多依赖实验分析而不是理论直觉的论文。对生成的食谱图像的分析为如何分析学习到的嵌入提供了不同的视角，它甚至表明，如果学习得当，图像编码和文本编码可以互换，就像它应该的那样。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="d40d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是我开始的一个新系列的第三部分，关于直觉的论文解释。我正在挑选行业中的一个子域，并浏览该域中的论文。如果你喜欢我写的东西，可以考虑订阅或者关注我<a class="ae kv" href="https://www.medium.com/@kunjmehta10" rel="noopener">这里</a>或者在<a class="ae kv" href="http://www.linkedin.com/in/kunjmehta" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>或者<a class="ae kv" href="https://www.twitter.com/@kunjmehta10" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上与我联系！关于我以前文章的代码，请访问我的<a class="ae kv" href="https://github.com/kunjmehta/Medium-Article-Codes" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p><p id="feed" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">论文引用</strong></p><p id="7f5d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">[1] Ricardo Guerrero，Hai Xuan Pham和Vladimir Pavlovic，“跨模态检索和合成{(X-MRS ):在共享子空间中关闭模态<br/>间隙”。2020年更正</p></div></div>    
</body>
</html>