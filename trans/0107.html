<html>
<head>
<title>Random Forests Walkthrough — Wisdom of the Crowds and Why They Are Better than Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林漫游——群体的智慧以及为什么它们比决策树更好</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forests-walkthrough-why-are-they-better-than-decision-trees-22e02a28c6bd#2022-02-04">https://towardsdatascience.com/random-forests-walkthrough-why-are-they-better-than-decision-trees-22e02a28c6bd#2022-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="31e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">随机森林总是被称为“基于树的”模型的更强大和更稳定的版本。在本帖中，我们将证明为什么将群体的智慧应用到决策树是一个好主意。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b84a0b601fb0435a681f6450b4a0b8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DaC7urQmAhD4zUQw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@skamenar" rel="noopener ugc nofollow" target="_blank">@ skamenar</a>-<a class="kz la ep" href="https://medium.com/u/2053395ac335?source=post_page-----22e02a28c6bd--------------------------------" rel="noopener" target="_blank">Unsplash</a>拍摄。com</p></figure><p id="2304" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">决策树是非常强大的算法。当你进入数据科学和机器学习领域时，它们可能是你可能学习的第一批非线性算法之一。</p><p id="7314" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">决策树可以处理非线性模式，并理解目标和特征之间的一些最疯狂的关系。从这个意义上说，它们比线性模型如线性或逻辑回归有巨大的优势。</p><p id="7510" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">虽然它们具有捕捉更复杂的特征和目标关系的巨大能力，但它们也很容易过度拟合。它们是倾向于搜索完美叶节点的贪婪算法的子集。特别是当我们处理高维度(行或特征)时，这可能导致树最终用小样本进行概括，从而使您的算法对您的训练数据来说是完美的。</p><p id="11ce" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/classification-decision-trees-easily-explained-f1064dde175e?sk=ccd305c31950f2e8c843377186e5e75f">他们的内部工作方式</a>很难找到纯节点，并且很难建立一个在偏差和方差之间达到最佳平衡的决策树。如何避免这种情况？</p><p id="6ef8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">大多数情况下，研究人员已经找到了两种解决这个问题的方法:</p><ul class=""><li id="1af9" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">boosting——将多个弱学习者组合并迭代到一个算法中——XGBoost是最著名的实现之一。</li><li id="a576" class="lx ly it ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated">打包——将多个模型聚合成一个整体——例如随机森林(RF)。</li></ul><p id="d0f8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">RFs使用多个决策树的输出，将它们合并到一个投票系统中，这有助于缓解他们的一些紧迫问题。打个比方，如果决策树是一个独裁政府，只使用一个“头脑”的意见来做决定，随机森林就是一个民主政府，依靠群众的智慧来执行预测。</p><p id="735a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这篇文章会让你很好地理解随机森林背后的直觉。通过一些实验，我们将比较它们与单一决策树的性能，并理解为什么它们通常比单一决策树模型有更好的结果。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="709e" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">拟合多个决策树</h1><p id="e79c" class="pw-post-body-paragraph lb lc it ld b le nk ju lg lh nl jx lj lk nm lm ln lo nn lq lr ls no lu lv lw im bi translated">为了真正理解RF的工作原理，让我们做一个实验，在一个数据集上拟合几个决策树。为了便于每个人复制，我们将使用Kaggle中可用的<a class="ae ky" href="https://www.kaggle.com/c/titanic/data?select=train.csv" rel="noopener ugc nofollow" target="_blank"> <em class="np">泰坦尼克号</em> </a> <em class="np"> </em>数据框，这是大多数数据科学家都知道的数据集。</p><p id="e9db" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">对于这个特定的问题，我们希望根据一组特征(如乘客年龄、机票等级、性别等)来预测哪些乘客在泰坦尼克号失事中幸存。— <strong class="ld iu">这是一个二元分类问题，有两个互斥的结果:</strong></p><ul class=""><li id="55d5" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">乘客生还；</li><li id="d996" class="lx ly it ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated">乘客没有生还；</li></ul><p id="682f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们首先拟合3个不同的决策树。我们控制和创建完全不同的树的最常见的方法之一是给它们自己的一组<a class="ae ky" rel="noopener" target="_blank" href="/5-decision-tree-hyperparameters-to-enhance-your-tree-algorithms-aee2cebe92c8?sk=5124b0f5996634d4f0d214499906f1d5">超参数</a>。这意味着产生的不同决策树会因用于预测结果的不同分割和变量而有所不同。</p><p id="2ace" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">除了不同的超参数，我们还将为每棵树选择80%的原始训练样本，从而在构成的每棵树中引入更多的随机性来源。</strong></p><p id="0707" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了评估我们不同的决策树，我们将使用30%的维持率作为纯测试集:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e4d1" class="nv mt it nr b gy nw nx l ny nz"># Reading the titanic train dataset<br/>titanic &lt;- read.csv('./train.csv')</span><span id="cf16" class="nv mt it nr b gy oa nx l ny nz"># Obtaining the number of rows for training (70%)<br/>size &lt;- ceiling(0.7*nrow(titanic))</span><span id="fdab" class="nv mt it nr b gy oa nx l ny nz"># Use an indexer to perform train and test split<br/>set.seed(999)<br/>train_index &lt;- sample(<br/>  seq_len(nrow(titanic)), size = size<br/>  )</span><span id="9775" class="nv mt it nr b gy oa nx l ny nz">train_df &lt;- titanic[train_index, ]<br/>test_df &lt;- titanic[-train_index, ]</span></pre><p id="bdcb" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np"> train_df </em>数据帧包含624名乘客，而<em class="np"> test_df </em>包含267名乘客。</p><p id="0edc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们可以构建三个不同的决策树——由于它们对超参数和训练数据中的微小变化非常敏感，我们可以轻松地构建三个彼此完全不同的决策树，只需对代码进行非常小的更改。</p><p id="27b9" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">举个例子，让我来拟合1号决策树，我将它命名为<em class="np"> oak_tree </em>(用不同的名字来称呼不同的树将有助于我们更好地形象化我们的练习)。下面是我训练它的代码:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c7b3" class="nv mt it nr b gy nw nx l ny nz">library(rpart)</span><span id="0c43" class="nv mt it nr b gy oa nx l ny nz">set.seed(9990)<br/>oak_tree &lt;- rpart(Survived ~ Fare + Age + Sex + Pclass,<br/>                       data = sample_n(train_df, 600), <br/>                       method = 'class',<br/>                       control = list(maxdepth = 2,<br/>                                      minsplit=30))</span></pre><p id="6a8e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np">(不要忘记在运行set.seed的同时运行训练代码，这样您就可以复制这些结果)</em></p><p id="e95d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">请注意，我们只是使用4个特征来预测是否有人在泰坦尼克号失事中幸存——乘客票价、年龄、性别和机票等级。是什么让我的<em class="np">橡树</em>与众不同？我的<em class="np"> oak_tree </em>是一棵深度只有2的超级小树，允许在每个节点上进行至少30个例子的分割。如果这个超参数术语让你感到困惑，<a class="ae ky" rel="noopener" target="_blank" href="/5-decision-tree-hyperparameters-to-enhance-your-tree-algorithms-aee2cebe92c8?sk=5124b0f5996634d4f0d214499906f1d5">看看这个</a>。</p><p id="7579" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">正如我以前说过的，我也给我的<em class="np">橡树增加了更多的辣度— </em>我只在大约600名随机乘客的数据上训练算法，这将模拟不同树之间更多的随机性。</p><p id="f768" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">所以我们的<em class="np">橡树</em>看起来像下面的<em class="np"> : </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f8809ec82e17158c29de6d00f0209373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*1ktfykeHMVtKdzquiu6j-Q.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树1 —作者图片</p></figure><p id="f290" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在这个树中，用来决定结果的第一个变量(也是唯一的一个)是性别。这一变量似乎具有很高的判别能力，因为如果乘客是男性，他不幸存的可能性更高——约为82%，而如果乘客不是男性，则为26%。关于此图如何工作的一个小插图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/628d850cd34829c636cbc71abc6c876e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*Mx_9SYUoF7ubHJKZZn-w7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树图表说明—图片由作者提供</p></figure><p id="7e46" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">请记住，这种特殊的树是以这种方式构建的，因为我们定义了600个客户的特定子样本和一组超参数。</p><p id="ecb8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在让我们训练一个完全不同的树，这次我叫它<em class="np"> pine_tree。</em></p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="e73a" class="nv mt it nr b gy nw nx l ny nz">set.seed(9991)<br/>pine_tree &lt;- rpart(Survived ~ Fare + Age + Sex + Pclass,<br/>                      data = sample_n(train_df, 600), <br/>                      method = ‘class’,<br/>                      control = list(maxdepth = 3, <br/>                                     minsplit=3, <br/>                                     minbucket=4, <br/>                                     cp=0.01))</span></pre><p id="0414" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们的<em class="np">松树</em>可以比我们的<em class="np">橡树更深一点。</em>此外，我们允许我们的树用更少的例子构建节点——无论是在进行分割时还是在构建末端叶节点时。</p><p id="94b2" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们检查一下结果——注意这个树形图和<em class="np">橡树图</em>的区别:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c8d06a279bea5c6c2214393bbaea9d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*J7L9erK5oYCBB5mbwIWOLA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树2 —作者图片</p></figure><p id="3d16" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">注意一些很酷的事情——新的变量开始发挥作用。虽然性别仍然被用作第一分割点，但从那时起，票价、阶级和年龄就占据了舞台，并决定了我们的大部分概率。</p><p id="8c6e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">有趣的流程如下所示:</p><ul class=""><li id="b43f" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">支付超过16美元且不到2岁的儿童有86%的存活概率。</li><li id="628d" class="lx ly it ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated">上流社会的女性生存的可能性最高(95%)。</li></ul><p id="020e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们现在已经建立了两个不同的树！请注意，它们显示了完全不同的结果，尽管我们的<em class="np">松树</em>似乎是我们的<em class="np">橡树</em>的自然延伸。也预期<em class="np">松树</em>比<em class="np">橡树</em>有更多的过度拟合，仅仅是因为超参数的设置。</p><p id="c4a4" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">最后再来拟合另一棵树，<em class="np"> elm_tree。</em></p><p id="dec7" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np"> Elm_tree </em>与<em class="np"> pine_tree </em>相似，但有一点不同——我们只允许树的最大深度为2。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bea9" class="nv mt it nr b gy nw nx l ny nz">set.seed(9992)<br/>elm_tree &lt;- rpart(Survived ~ Fare + Age + Sex + Pclass,<br/>                data = sample_n(train_df, 600), <br/>                method = 'class',<br/>                control = list(maxdepth = 2, <br/>                               minsplit=2, <br/>                               minbucket=4, <br/>                               cp=0.01))</span></pre><p id="bc5c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np">榆树_树</em>看起来如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c87c21691bf72bb5611de6f1e9b29c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*6tEJfEr-FfXcsk867jMyWg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树3 —作者图片</p></figure><p id="efd9" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">请注意，性别仍然是定义第一次拆分的变量。接下来就是上课了。</p><p id="8b80" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这三棵树看起来很相似，只是深度不同。当然，我们现在需要评估它们！它们在性能上有多大的不同？</p><h1 id="50a6" class="ms mt it bd mu mv od mx my mz oe nb nc jz of ka ne kc og kd ng kf oh kg ni nj bi translated">评估每棵树的性能</h1><p id="0a52" class="pw-post-body-paragraph lb lc it ld b le nk ju lg lh nl jx lj lk nm lm ln lo nn lq lr ls no lu lv lw im bi translated">为了快速进行性能评估，并且不受限于任何阈值，让我们使用AUC 作为评估模型的指标。</p><p id="282d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">更高的<a class="ae ky" rel="noopener" target="_blank" href="/understanding-auc-roc-curve-68b2303cc9c5"> AUC </a>意味着我们的模型更善于区分0类和1类，使我们的模型在预测中更有效和正确。</p><p id="618a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我正在使用<em class="np"> ROCR </em>库来更快地获得AUC:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="bb1e" class="nv mt it nr b gy nw nx l ny nz">library(ROCR)</span><span id="4bb5" class="nv mt it nr b gy oa nx l ny nz">obtainauc &lt;- function(model) {<br/>  predictions &lt;- predict(model, test_df)[,2]<br/>  pred &lt;- prediction(predictions, test_df$Survived)<br/>  perf &lt;- performance(pred, measure = 'auc')<br/>  return (perf@y.values[[1]])<br/>}</span></pre><p id="4f68" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们的采油树表现如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b70127e9c34b0416493dbe292f54936a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*adHwvw1f2qsy4Cy_Z_7apQ.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">树与树之间AUC的比较—图片由作者提供</p></figure><p id="caa2" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">从上面的图来看:</p><ul class=""><li id="0794" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">橡树的AUC为0.778</li><li id="956c" class="lx ly it ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated"><em class="np">松树</em>的AUC为0.832</li><li id="783d" class="lx ly it ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated"><em class="np">榆树</em>的AUC为0.807</li></ul><p id="9482" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np">松树</em>是表现最好的树。这是意料之中的，因为这也是包含最佳细节的树。每当我们提高最大深度时，我们可能会在训练集上有更好的性能——当我们在测试集上评估时，可能会有所不同。</p><p id="01df" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">因此，随机森林集中在一个核心问题中:<strong class="ld iu">“即使我的<em class="np">松树</em>是最好的，我是否应该完全放弃<em class="np">橡树</em>和<em class="np">榆树</em>的观点？”</strong></p><p id="2071" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们建立另一个实验——我们将做一个多数投票选项——平均所有树的结果，并将这个意见“集合”与每棵树单独进行比较。由于我们正在处理概率结果，我们将只是平均3个不同树之间的概率，并认为这是我们的组合投票。</p><h1 id="bdc1" class="ms mt it bd mu mv od mx my mz oe nb nc jz of ka ne kc og kd ng kf oh kg ni nj bi translated">平均结果</h1><p id="301c" class="pw-post-body-paragraph lb lc it ld b le nk ju lg lh nl jx lj lk nm lm ln lo nn lq lr ls no lu lv lw im bi translated">为了更容易理解“平均结果”的真正含义，我们来做一点角色扮演。</p><p id="b1bc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">假设我询问橡树、松树和榆树对下面这个人的看法:</p><ul class=""><li id="da80" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">一名男性花了不到16美元登上泰坦尼克号，他会幸免于难吗？</li></ul><p id="c56c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">橡树说这位乘客有18%的可能性会活下来。松树说只有10%的机会活下来。<em class="np">榆树</em>也说这个人有18%的几率活下来。</p><p id="d5b6" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如果我们对每棵树的推荐进行平均，我们将得到<em class="np"> (18%+18%+10%)/3 </em>，这将产生<em class="np"> 15.3% </em> — <strong class="ld iu">使用投票平均值，这应该是我们最终的乘客存活概率。</strong></p><p id="f4ac" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们看看另一位乘客:</p><ul class=""><li id="8674" class="lx ly it ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">乘坐头等舱或二等舱的女乘客。</li></ul><p id="2d19" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">橡树说这位乘客有74%的可能性会活下来。松树和榆树都显示有95%的可能性。</p><p id="d68d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如果我们平均这些建议，我们得到88%的生存概率。</p><p id="9d9a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">主要问题是——你认为对测试集中的所有乘客使用这种基本原理会提高我们的性能吗？还是由于他们更糟糕的表现，我们的<em class="np">橡树</em>和<em class="np">榆树</em>的意见只会拖累我们的成绩？</strong></p><p id="9a2d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">让我们看看！</p><h1 id="f91e" class="ms mt it bd mu mv od mx my mz oe nb nc jz of ka ne kc og kd ng kf oh kg ni nj bi translated">群众的智慧</h1><p id="63da" class="pw-post-body-paragraph lb lc it ld b le nk ju lg lh nl jx lj lk nm lm ln lo nn lq lr ls no lu lv lw im bi translated">这里有一小段代码，我基于<em class="np">橡树、松树</em>和<em class="np">榆树</em>构建了这个集合模型。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c71b" class="nv mt it nr b gy nw nx l ny nz">ensemble &lt;- (<br/>  predict(oak_tree, test_df)[,2]<br/>  +<br/>  predict(pine_tree, test_df)[,2]<br/>  +<br/>  predict(elm_tree, test_df)[,2]<br/>)/3</span></pre><p id="8205" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这个简单的系综模型表现如何？让我们再次检查使用ROCR图书馆:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="2720" class="nv mt it nr b gy nw nx l ny nz"># Ensemble Performance<br/>prediction &lt;- prediction(ensemble, test_df$Survived)<br/>perf &lt;- performance(prediction, measure = 'auc')<br/>performance_ensemble &lt;- perf@y.values[[1]]</span></pre><p id="1d84" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们有以下AUC结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8a5d8c6825679bf9c4597ba54cf3b6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*QPk1YPO8N__cW8ytaseLeg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">树木和系综的AUC比较——作者提供的图像</p></figure><p id="4fb0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">哇！尽管我们的<em class="np">橡树</em>和<em class="np">榆树</em>比我们的<em class="np">松树</em>更糟糕，但在做预测时考虑他们的意见是有价值的。<strong class="ld iu">请注意，我们的集合模型的AUC略好于最佳个体树。</strong>结论是，即使通过将该树与“较弱”的树相结合，我们也能够提升性能。</p><p id="2e5c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这正是随机森林背后的基本原理！每次你适应一个随机的森林，你就适应了无数的树，当与一个单独的决策树比较时，即使是较弱的树也会有利于你的整体性能。</p><p id="46ed" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">当你增加被训练的树的数量时——有时增加到几千棵树——这种行为就更加强大了。当然，当你增加训练的树的数量时，你提高了你的“集合”比组成相同“集合”的单个决策树更好的可能性。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="84e6" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">就是这样！感谢你花时间阅读这篇文章，希望你喜欢。我的目标是解释为什么随机森林通常比单一决策树更好，以及为什么它们是您的机器学习项目中值得考虑的优秀算法。这个例子应该能让你很好地理解RF背后的“群体智慧”假设。</p><p id="16f5" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"><em class="np"/></strong><a class="ae ky" href="https://www.udemy.com/course/r-for-data-science-first-step-data-scientist/?referralCode=MEDIUMREADERS" rel="noopener ugc nofollow" target="_blank"><strong class="ld iu"><em class="np">我在Udemy上开设了一门关于学习数据科学的课程</em></strong></a><strong class="ld iu"><em class="np">——这门课程是为初学者设计的，包含50多个练习，我希望你能在我身边！您将学习基于树的模型、回归以及如何从头到尾构建数据科学项目。</em> </strong></p><p id="419f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这里有一个小的<em class="np">要点</em>和贯穿这篇文章的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="d9b4" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="np">数据集许可:本文中使用的数据集可以在</em><a class="ae ky" href="https://www.openml.org/d/40945" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/d/40945</a>公开使用</p><div class="ok ol gp gr om on"><a href="https://ivopbernardo.medium.com/membership" rel="noopener follow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">通过我的推荐链接加入Medium-Ivo Bernardo</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">ivopbernardo.medium.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ks on"/></div></div></a></div></div></div>    
</body>
</html>