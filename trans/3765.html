<html>
<head>
<title>Bayesian Inference and Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯推理和变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-inference-and-transformers-3dc473ac1af2#2022-08-22">https://towardsdatascience.com/bayesian-inference-and-transformers-3dc473ac1af2#2022-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bbb5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">变形金刚可以帮忙做贝叶斯推断吗？简单回顾一下变形金刚可以做贝叶斯推断</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d5757946286a1abcb36e690d3102daf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gx1jYodlSnPlwlZt"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">阿瑟尼·托古列夫在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="38ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是深度学习的不确定性系列的最后一篇文章:</p><ul class=""><li id="157b" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-brief-introduction-1f9a5de3ae04">第一部分——简介</a></li><li id="0d6f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-aleatoric-uncertainty-and-maximum-likelihood-estimation-c7449ee13712">第2部分——随机不确定性和最大似然估计</a></li><li id="be05" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-epistemic-uncertainty-and-bayes-by-backprop-e6353eeadebb">第3部分——认知不确定性和反向投影贝叶斯</a></li><li id="93a2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-bayesian-cnn-tensorflow-probability-758d7482bef6">第4部分——实现完全概率贝叶斯CNN </a></li><li id="f916" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/uncertainty-in-deep-learning-experiments-with-bayesian-cnn-1ca37ddb6954">第五部分——贝叶斯CNN实验</a></li><li id="2b5d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">第6部分——贝叶斯推理和变压器</strong></li></ul><p id="1832" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一部分将与该系列的其他部分略有不同。我将在下面的文章中描述一篇文章中的方法和提出的方法背后的推理(以便与变分推理进行比较):<a class="ae kv" href="https://arxiv.org/pdf/2112.10510.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">变形金刚可以进行贝叶斯推理</strong> </a> <strong class="ky ir">。</strong></p><p id="aff4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注:我将试着简要解释这篇论文，这并不意味着是官方的东西。</p><p id="9810" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文组织如下:</p><ul class=""><li id="dd8d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">重述</strong></li><li id="97ff" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">元学习和贝叶斯推理</strong></li><li id="fdc4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">后验和后验预测分布</strong></li><li id="5de6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">建议方法:预先拟合网络</strong></li><li id="2844" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">使用变压器并将零件组装在一起</strong></li><li id="4d35" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">结论</strong></li></ul><h1 id="3717" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">概述</h1><h2 id="cbd5" class="my mh iq bd mi mz na dn mm nb nc dp mq lf nd ne ms lj nf ng mu ln nh ni mw nj bi translated">传统深度学习模型</h2><p id="fc69" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">正如第2部分和第3部分所解释的，标准模型是确定性的。换句话说，一个输入应该得到相同的输出。因为用最大似然估计(MLE)，你会得到点估计权重。</p><p id="7eca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来举例说明:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/30dea13d48ba049ac74e8c3f82a5988c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5qu4UAnfSiUoDaRU4t7wgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="3365" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，<strong class="ky ir">概率贝叶斯深度学习模型</strong>可以对相同的输入输出不同的预测。通过观察输出，我们可以解释预测本身的不确定性。</p><h2 id="1c6d" class="my mh iq bd mi mz na dn mm nb nc dp mq lf nd ne ms lj nf ng mu ln nh ni mw nj bi translated">深度学习中的贝叶斯方法</h2><p id="e215" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">贝叶斯统计方法从一组数据中推断出与数据一致的潜在概率分布。</p><p id="4f84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与更传统的方法相比，这种方法提供了几个优点，例如能够将关于分布的先验信息结合到分析中，并计算各种可能分布的置信程度。</p><p id="e19e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在贝叶斯深度学习中，我们希望计算后验函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/e752a9f018d07f0e64ed5183a5ebdc65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhUMPVO3gc8hcK536ppgWQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="2d47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这是困难的，并且大多数时候不可能计算真实的后验概率，因此它是近似的。这种近似被称为变分贝叶斯，或者更具体地说，变分推理。然而，这些方法在计算方面是<strong class="ky ir">昂贵的。</strong></p><h1 id="e519" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">元学习和贝叶斯推理</h1><p id="3b4f" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">在我们开始之前，简单定义一下<strong class="ky ir">和</strong>什么是元学习会很有帮助。正如我们所知，计算机和人类的学习方式不同。举一个具体的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/2beeea11c0a81e15c5b68a251208f8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*93UOgVTAoCa5bmRB"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@fedederodt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Fede de Rodt </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="37b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们给这张图片贴上标签，它会是一杯<strong class="ky ir">咖啡。</strong>有趣的是，一个人可以很容易地从几个样本中知道这是一杯<strong class="ky ir">咖啡。嗯，这不适用于计算机。</strong></p><p id="0bc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想想这些图像:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/cf7f9c9f286406c17903955521e3ca40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71FL9oNYOqd4ByIBkIPkWQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由DALLE-2(<a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/dall-e-2/</a>)生成的图像</p></figure><p id="27e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个从少量图像中学习的人可以很容易地正确分类这些图像。用于训练的图像很少的计算机<strong class="ky ir">可能</strong>很难对它们进行分类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/0c387c3e3b06ac8e45ae5c5868058a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pv1l8o9ebYQObEGHi8z8YQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由DALLE-2(<a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/dall-e-2/</a>)生成的图像</p></figure><p id="e480" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者这些图像，它们中的每一个都代表一杯咖啡，人类应该能够正确地对它们进行分类，然而计算机可能会对它们进行错误的分类。</p><p id="c3cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果计算机也能用很少的样本来概括数据集，那就太好了。这就是元学习的要点和主要思想。它也被称为<strong class="ky ir">学会学习。</strong></p><p id="01e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以在没有任何狗的图像上训练一个模型，然后它应该能够在看到一些可爱的狗照片后告诉我们给定的图像是否包含狗。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/50e1ff3bb245cc682be342d3ee78642b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnQtuBTrzKE9UeHClR-MkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由DALLE-2(<a class="ae kv" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/dall-e-2/</a>)生成的图像</p></figure><p id="3a1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，在元学习时代有各种各样的方法。所提到的论文利用了高度面向研究的元学习技术，该技术试图允许模型<strong class="ky ir"> <em class="nv">在单次向前传递中学习</em> </strong>。</p><h1 id="4161" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">后验和后验预测分布</h1><p id="82b5" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">现在，让我们区分这两个词。</p><ul class=""><li id="c8b4" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在后面的</li><li id="98d9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">后验预测分布</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/8180e7c3fc7524ed8038fc2240d658ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nr2fyHtRk7kX7-JFBAw8Eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。(后)</p></figure><p id="32f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们想要找到模型参数的分布时，我们使用贝叶斯定理重写后验概率。证据，边际似然性，<strong class="ky ir"> P(x) </strong>充当归一化常数。</p><p id="8e48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里<strong class="ky ir">后验分布取决于θ</strong>，θ是<strong class="ky ir">未知参数。</strong></p><p id="e8c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预测分布是不同的。首先让我们看看<strong class="ky ir">先验预测分布</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/5cdf12c3637b99e8f6e9e6073c9f5852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kK2Fz63iTQyE7sXpV3R-jw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。(先验预测分布)</p></figure><p id="ab07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">右手边的积分告诉我们平均所有可能的<strong class="ky ir">θ值。</strong>真好，但是为什么呢？</p><p id="9f32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始训练之前，我们只有先验分布，这意味着测量模型参数不确定性唯一方法是使用先验分布！</p><p id="ea57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到时间<strong class="ky ir">模型看到新的数据，</strong>所以<strong class="ky ir"> </strong>我们要对<strong class="ky ir">θ的所有可能值进行平均。</strong>因为我们有兴趣了解由下式给出的数据的分布:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/358a0b8735dc781bcc42c052f5bbbdac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nKw7JvF1GLvWeGCY8w2IQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="ce02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在获取一些样本并进行训练后，我们通过使用后验概率更好地表示了模型参数的不确定性。按照上面相同的逻辑，<strong class="ky ir">后验预测分布变成:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/925a065bf9ee3e187a0e023ce99bcdac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7QGyCsURSi7eT9pOBcHZQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="6fa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Xn </strong>代表独立的数据点。有一些结论:</p><ul class=""><li id="1830" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如果我们能找到一个好的先验，我们的训练数据将密切遵循这种模式。</li><li id="6c71" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">由于新数据点的引入，预计后验预测将具有不同的方差(通常更大)。</li></ul><h2 id="87c0" class="my mh iq bd mi mz na dn mm nb nc dp mq lf nd ne ms lj nf ng mu ln nh ni mw nj bi translated">总结</h2><p id="5dd9" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">总之，<strong class="ky ir">后验</strong>处理<strong class="ky ir">模型参数</strong>的分布。另一方面<strong class="ky ir">后验预测分布</strong>解释了我们将在未来 <strong class="ky ir"> (Xn)得到的<strong class="ky ir">数据点的分布。</strong></strong></p><p id="549e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，后验预测的主要思想是，我们不确定模型参数(认知不确定性),希望通过使用似然的平均值来预测新的数据点，但同时我们考虑模型参数的所有可能值。</p><blockquote class="oa"><p id="dfc9" class="ob oc iq bd od oe of og oh oi oj lr dk translated">预测的关键分布，即后验预测分布(PPD)，可以推断为[1]:</p></blockquote><figure class="ol om on oo op kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/12ed570c974a7bc42ca22bff8fd7a375.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*lt0JJHn5ZbyFwHq7wdvEOQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">文献[1]中的后验预测公式。</p></figure><h1 id="83d8" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">建议的方法:先验拟合网络</h1><p id="8995" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">在贝叶斯深度学习中，大多数时候真实的后验概率是近似的，因为它是难以处理的。说到底，我们感兴趣的是在后验预测公式中显示的术语右侧。</p><p id="9bb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有趣的是，这篇论文提出了一种新方法，称为<strong class="ky ir">先验数据拟合网络</strong>，它训练一个模型来直接优化左边的术语。这意味着，我们不仅使用数据集，还使用查询。</p><p id="551b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该论文的作者绘制了该过程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/781b65541d4eab792b07e2299463b5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBVSvdAJeon_tFlYS716yw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“先验数据拟合网络(pfn)的可视化。我们对先前的数据集进行采样，并对这些数据集的保留样本进行PFN拟合。给定一个实际数据集，我们将它和一个测试点输入到PFN，并在一次正向传播中获得贝叶斯推断的近似值。”[1]</p></figure><p id="c3ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把它分成更小的部分:</p><ul class=""><li id="de5a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">首先，我们选择一个先验。</li><li id="f751" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">先前的样本数据集。</li><li id="9403" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">通过最大化可能性开始元学习过程。这也等于最小化负面可能性。应用对数是因为当涉及乘法时它具有良好的性质。因此负对数似然性被最小化。</li><li id="5636" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">数据集被输入到一个参数化的模型中。</li></ul><p id="68fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，它们生成大量数据(嗯，无限多是理想的)，并使用那些<strong class="ky ir">参数化的</strong>模型来标记生成的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/02de9e1c4f1ccb733636cbc9484873ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BVs7M0c7H3kYs2cXRf5xRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">解释了文献[1]中的算法。</p></figure><p id="d62b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型可以是卷积网络或任何其他类型的神经网络架构。</p><p id="18c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者选择了<strong class="ky ir">变压器</strong>作为<strong class="ky ir"> PFN </strong>，因为变压器毕竟是我们所需要的。</p><h1 id="7a3d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">使用Transformer并把零件组装在一起</h1><p id="68be" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">正如我们所知，最初的<strong class="ky ir">变压器架构</strong>【2】由一个编码器和一个解码器组成。作者稍微修改了这个架构，使其排列不变。因此这意味着不再使用<strong class="ky ir">位置编码。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/fa8dc8a2a21f84f5b44500fbc5c9a14f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*hsdPFXe6HHAuhTFyaQRFNg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">n = 3个输入对和m = 2个查询的转换器的可视化。每个条形代表一个输入，箭头显示每个代表可以关注什么。”[1]</p></figure><p id="1850" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所提出的模型返回每个给定查询<code class="fe ot ou ov ow b">x</code>的后验预测分布，这仅取决于数据集和查询本身。</p><p id="c2f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练期间，输入的总数保持固定(N个输入)。这些输入可以分成一个求和公式，即:</p><ul class=""><li id="d296" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> N = m + n </strong></li></ul><p id="d3bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> n </strong>是输入的数量，<strong class="ky ir"> m </strong>是给定查询的数量。换句话说，我们希望Transformer能够学会处理不同大小的数据集。</p><p id="6129" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于有各种各样的任务，最流行的是回归和分类，输出层是灵活的，并根据任务的变化。</p><h1 id="69b3" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="a779" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">本文提出了一种利用元学习技术同时训练一个特殊的转换器的方法。这使得我们可以比变分推断<strong class="ky ir">更快地做出后验推断(快1000倍)。</strong></p><p id="9fc2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基准测试结果证实:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/53041a814ea9c49d23d74559fd16d26e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnpm1jCtIyf1tT8Fv5DWfQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“具有30个训练样本的数据集子集的聚合性能。我们的小说《PFN-BNN》整体表现最强。有关每个数据集的结果，请参见附录中的表7。”[1]</p></figure><p id="30d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以从<a class="ae kv" href="https://github.com/automl/TransformersCanDoBayesianInference" rel="noopener ugc nofollow" target="_blank">这里</a>查看论文的官方实现。</p><h1 id="a12d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="5622" class="pw-post-body-paragraph kw kx iq ky b kz nk jr lb lc nl ju le lf nm lh li lj nn ll lm ln no lp lq lr ij bi translated">[1]:塞缪尔·穆勒，诺亚·霍尔曼，塞巴斯蒂安·皮内达，约瑟夫·格拉博卡，弗兰克·赫特，<a class="ae kv" href="https://arxiv.org/pdf/2112.10510.pdf" rel="noopener ugc nofollow" target="_blank">变形金刚可以做贝叶斯推理</a>，ICLR 2022。</p><p id="0604" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]: Ashish Vaswani等人<a class="ae kv" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，神经信息处理系统进展，2017。</p></div></div>    
</body>
</html>