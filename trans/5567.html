<html>
<head>
<title>DynamoDB Go SDK: How to Use the Scan and Batch Operations Efficiently</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DynamoDB Go SDK:如何有效地使用扫描和批处理操作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dynamodb-go-sdk-how-to-use-the-scan-and-batch-operations-efficiently-5b41988b4988#2022-12-15">https://towardsdatascience.com/dynamodb-go-sdk-how-to-use-the-scan-and-batch-operations-efficiently-5b41988b4988#2022-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b2db2e89203756da392ffbc2651046c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxYorye_W5CK60CIXAkEjg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用 DynamoDB Go SDK 进行并行扫描(图片由作者提供)</p></figure><div class=""/><div class=""><h2 id="4862" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">通过实际的代码示例学习</h2></div><p id="ea85" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><a class="ae lq" href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Scan.html" rel="noopener ugc nofollow" target="_blank"> DynamoDB 扫描 API </a>访问表中的每一项(或二级索引)。它相当于一个<code class="fe lr ls lt lu b">select * from</code>查询。我将在这篇博客中讨论的事情之一是如何将<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb#Client.Scan" rel="noopener ugc nofollow" target="_blank"> Scan </a> API 与 DynamoDB Go SDK 一起使用。</p><p id="51e4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">要扫描一个表，我们需要一些数据开始！所以在这个过程中，我还将深入研究如何使用<code class="fe lr ls lt lu b">Batch</code> API 在<code class="fe lr ls lt lu b">DynamoDB</code>中写批量数据。您可以使用<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb#Client.BatchWriteItem" rel="noopener ugc nofollow" target="_blank"> BatchWriteItem </a> API 来批量(25 个)创建或删除项目，并且您可以跨多个表组合这些操作。</p><p id="758b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将从简单开始，逐步改进我们的方法来有效地使用 API。我还将回顾我运行的一些基本测试，以展示增量改进。最后，我将强调使用这些操作时的一些注意事项。</p><blockquote class="lv lw lx"><p id="3c6d" class="ku kv ly kw b kx ky kg kz la lb kj lc lz le lf lg ma li lj lk mb lm ln lo lp ij bi translated"><em class="jf">可以参考</em> <a class="ae lq" href="https://github.com/abhirockzz/dynamodb-go-sdk-scan-batch" rel="noopener ugc nofollow" target="_blank"> <em class="jf">上的代码 GitHub </em> </a></p></blockquote></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="7198" class="mj mk jf bd ml mm mn mo mp mq mr ms mt kl mu km mv ko mw kp mx kr my ks mz na bi translated">在您继续之前…</h1><p id="259f" class="pw-post-body-paragraph ku kv jf kw b kx nb kg kz la nc kj lc ld nd lf lg lh ne lj lk ll nf ln lo lp ij bi translated">…确保创建一个名为<code class="fe lr ls lt lu b">users</code>的<code class="fe lr ls lt lu b">DynamoDB</code>表，其中包含:</p><ul class=""><li id="ffa0" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">分区键<code class="fe lr ls lt lu b">email</code>(数据类型<code class="fe lr ls lt lu b">String</code>)和</li><li id="2068" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><code class="fe lr ls lt lu b">On-Demand</code>容量模式。</li></ul><figure class="nv nw nx ny gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/1ad36a3873359dc6d08c6ce113274309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gyDR68amcvyv2949.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">DynamoDB 表(图片由作者提供)</p></figure><p id="dd55" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">此外，我还想称一些事情为背景:</p><ul class=""><li id="4b65" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">该表是在<code class="fe lr ls lt lu b">us-east-1</code>中创建的，测试也是从<code class="fe lr ls lt lu b">us-east-1</code>中的<code class="fe lr ls lt lu b">EC2</code>实例中执行的</li><li id="b848" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">因为这些是一般的测试，而不是专门的基准测试，所以我没有做任何特殊的调整(在任何级别上)。这些只是用不同的输入执行的 Go 函数，尽可能保持简单。</li><li id="a1d7" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">测试包括<code class="fe lr ls lt lu b">BatchWriteItem</code>操作的编组(将 Go <code class="fe lr ls lt lu b">struct</code>转换为<code class="fe lr ls lt lu b">DynamoDB</code>数据类型)和<code class="fe lr ls lt lu b">Scan</code>操作的解编组(从<code class="fe lr ls lt lu b">DynamoDB</code>数据类型转换回 Go <code class="fe lr ls lt lu b">struct</code>)。</li></ul><p id="8cf1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我们从探索<code class="fe lr ls lt lu b">BatchWriteItem</code> API 开始。这样我们也将有数据来处理<code class="fe lr ls lt lu b">Scan</code>操作。</p><p id="dbc5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">双赢！</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="026d" class="mj mk jf bd ml mm mn mo mp mq mr ms mt kl mu km mv ko mw kp mx kr my ks mz na bi translated">批量导入数据</h1><p id="fa7a" class="pw-post-body-paragraph ku kv jf kw b kx nb kg kz la nc kj lc ld nd lf lg lh ne lj lk ll nf ln lo lp ij bi translated">由于您可以在一次调用中组合<strong class="kw jg"> 25 </strong>项，因此与在循环中(甚至并行)调用<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb#Client.PutItem" rel="noopener ugc nofollow" target="_blank"> PutItem </a>相比，使用批处理方法进行批量数据导入要好得多。</p><p id="d347" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下面是一个如何使用<code class="fe lr ls lt lu b">BatchWriteItem</code>的基本示例:</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="20ea" class="od mk jf lu b be oe of l og oh">func basicBatchImport() {<br/><br/> startTime := time.Now()<br/> <br/> cities := []string{"NJ", "NY", "ohio"}<br/> batch := make(map[string][]types.WriteRequest)<br/> var requests []types.WriteRequest<br/><br/> for i := 1; i &lt;= 25; i++ {<br/>  user := User{Email: uuid.NewString() + "@foo.com", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]}<br/>  item, _ := attributevalue.MarshalMap(user)<br/>  requests = append(requests, types.WriteRequest{PutRequest: &amp;types.PutRequest{Item: item}})<br/> }<br/><br/> batch[table] = requests<br/><br/> op, err := client.BatchWriteItem(context.Background(), &amp;dynamodb.BatchWriteItemInput{<br/>  RequestItems: batch,<br/> })<br/> if err != nil {<br/>  log.Fatal("batch write error", err)<br/> } else {<br/>  log.Println("batch insert done")<br/> }<br/><br/> if len(op.UnprocessedItems) != 0 {<br/>  log.Println("there were", len(op.UnprocessedItems), "unprocessed records")<br/> }<br/><br/> log.Println("inserted", (25 - len(op.UnprocessedItems)), "records in", time.Since(startTime).Seconds(), "seconds")<br/>}</span></pre><p id="be41" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">使用<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb#BatchWriteItemInput" rel="noopener ugc nofollow" target="_blank"> BatchWriteItemInput </a>，我们可以定义我们想要在批处理中执行的操作——这里我们将执行<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb/types#PutRequest" rel="noopener ugc nofollow" target="_blank"> PutRequest </a> s(它封装在另一个名为<a class="ae lq" href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/service/dynamodb/types#WriteRequest" rel="noopener ugc nofollow" target="_blank"> WriteRequest </a>的类型中)。</p><p id="daf7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们将<code class="fe lr ls lt lu b">WriteRequest</code>组装到一个切片中，最后将它们放在一个<code class="fe lr ls lt lu b">map</code>中，key 是表名——这正是<code class="fe lr ls lt lu b">BatchWriteItemInput</code>中的<code class="fe lr ls lt lu b">RequestItems</code>属性所需要的。</p><blockquote class="lv lw lx"><p id="2d46" class="ku kv ly kw b kx ky kg kz la lb kj lc lz le lf lg ma li lj lk mb lm ln lo lp ij bi translated">在这种情况下，我们处理的是单个表，但是您可以在多个表上执行操作。</p></blockquote><p id="79ad" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在本例中，我们只处理了一批<strong class="kw jg"> 25 </strong>记录(最大允许批量)。如果我们想要导入更多的记录，我们需要做的就是将它们分成<strong class="kw jg"> 25 </strong>的批次，并且一次执行一个(子)批次。很简单，下面是一个例子:</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="8bfa" class="od mk jf lu b be oe of l og oh">func basicBatchImport2(total int) {<br/><br/> startTime := time.Now()<br/><br/> cities := []string{"NJ", "NY", "ohio"}<br/> batchSize := 25<br/> processed := total<br/><br/> for num := 1; num &lt;= total; num = num + batchSize {<br/><br/>  batch := make(map[string][]types.WriteRequest)<br/>  var requests []types.WriteRequest<br/><br/>  start := num<br/>  end := num + 24<br/><br/>  for i := start; i &lt;= end; i++ {<br/>   user := User{Email: uuid.NewString() + "@foo.com", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]}<br/>   item, _ := attributevalue.MarshalMap(user)<br/>   requests = append(requests, types.WriteRequest{PutRequest: &amp;types.PutRequest{Item: item}})<br/>  }<br/><br/>  batch[table] = requests<br/><br/>  op, err := client.BatchWriteItem(context.Background(), &amp;dynamodb.BatchWriteItemInput{<br/>   RequestItems: batch,<br/>  })<br/><br/>  if err != nil {<br/>   log.Fatal("batch write error", err)<br/>  }<br/><br/>  if len(op.UnprocessedItems) != 0 {<br/>   processed = processed - len(op.UnprocessedItems)<br/>  }<br/> }<br/><br/> log.Println("all batches finished. inserted", processed, "records in", time.Since(startTime).Seconds(), "seconds")<br/><br/> if processed != total {<br/>  log.Println("there were", (total - processed), "unprocessed records")<br/> }<br/>}</span></pre><p id="dcdd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我用<strong class="kw jg"> 50000 </strong>个记录(这意味着<strong class="kw jg"> 2000 </strong>个批次)进行了尝试，花费了大约<strong class="kw jg"> 15 </strong>秒。但是我们可以做得更好！</p><p id="c5e8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">平行批量导入</strong></p><p id="58a0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可以为每一批增加一个<code class="fe lr ls lt lu b">goroutine</code>，而不是顺序处理每一批:</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="79bd" class="od mk jf lu b be oe of l og oh">func parallelBatchImport(numRecords int) {<br/><br/> startTime := time.Now()<br/><br/> cities := []string{"NJ", "NY", "ohio"}<br/> batchSize := 25<br/><br/> var wg sync.WaitGroup<br/><br/> processed := numRecords<br/><br/> for num := 1; num &lt;= numRecords; num = num + batchSize {<br/>  start := num<br/>  end := num + 24<br/><br/>  wg.Add(1)<br/><br/>  go func(s, e int) {<br/>   defer wg.Done()<br/><br/>   batch := make(map[string][]types.WriteRequest)<br/>   var requests []types.WriteRequest<br/><br/>   for i := s; i &lt;= e; i++ {<br/>    user := User{Email: uuid.NewString() + "@foo.com", Age: rand.Intn(49) + 1, City: cities[rand.Intn(len(cities))]}<br/><br/>    item, err := attributevalue.MarshalMap(user)<br/>    if err != nil {<br/>     log.Fatal("marshal map failed", err)<br/>    }<br/>    requests = append(requests, types.WriteRequest{PutRequest: &amp;types.PutRequest{Item: item}})<br/>   }<br/><br/>   batch[table] = requests<br/><br/>   op, err := client.BatchWriteItem(context.Background(), &amp;dynamodb.BatchWriteItemInput{<br/>    RequestItems: batch,<br/>   })<br/><br/>   if err != nil {<br/>    log.Fatal("batch write error", err)<br/>   }<br/><br/>   if len(op.UnprocessedItems) != 0 {<br/>    processed = processed - len(op.UnprocessedItems)<br/>   }<br/><br/>  }(start, end)<br/> }<br/><br/> log.Println("waiting for all batches to finish....")<br/> wg.Wait()<br/><br/> log.Println("all batches finished. inserted", processed, "records in", time.Since(startTime).Seconds(), "seconds")<br/><br/> if processed != numRecords {<br/>  log.Println("there were", (numRecords - processed), "unprocessed records")<br/> }<br/>}</span></pre><p id="1f65" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">结果大大改善了。这是我得到的。平均而言:</p><ul class=""><li id="5980" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">插入<strong class="kw jg"> 50000 </strong>条记录花费了~ <strong class="kw jg"> 2.5 </strong>秒</li><li id="bf20" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">在~ <strong class="kw jg"> 4.5 </strong>到<strong class="kw jg"> 5 </strong>秒内插入<strong class="kw jg"> 100000 </strong>记录</li><li id="a128" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">在不到<strong class="kw jg"> 9.5 </strong>秒的时间内插入<strong class="kw jg"> 150000 </strong>记录</li><li id="1cd3" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">在不到<strong class="kw jg"> 11.5 </strong>秒的时间内插入<strong class="kw jg"> 200000 </strong>记录</li></ul><blockquote class="lv lw lx"><p id="76ef" class="ku kv ly kw b kx ky kg kz la lb kj lc lz le lf lg ma li lj lk mb lm ln lo lp ij bi translated"><em class="jf">批量中可能存在未处理的记录。这个例子检测这些记录，但是为了简单起见，已经跳过了重试逻辑。理想情况下，您还应该有一个(基于指数回退的)重试机制来处理未处理的记录。</em></p></blockquote><p id="d19e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了插入更多数据，我循环运行了<code class="fe lr ls lt lu b">parallelBatchImport</code>函数(如上)。例如:</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="7622" class="od mk jf lu b be oe of l og oh">for i := 1; i &lt;= 100; i++ {<br/>    parallelBatchImport(50000)<br/>}</span></pre><p id="1cc0" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">好吧，我们继续。现在我们有了一些数据，让我们试试…</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="7df2" class="mj mk jf bd ml mm mn mo mp mq mr ms mt kl mu km mv ko mw kp mx kr my ks mz na bi translated">…扫描 API</h1><p id="dc33" class="pw-post-body-paragraph ku kv jf kw b kx nb kg kz la nc kj lc ld nd lf lg lh ne lj lk ll nf ln lo lp ij bi translated">基本用法如下:</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="4e78" class="od mk jf lu b be oe of l og oh">func scan() {<br/> startTime := time.Now()<br/><br/> op, err := client.Scan(context.Background(), &amp;dynamodb.ScanInput{<br/>  TableName:              aws.String(table),<br/>  ReturnConsumedCapacity: types.ReturnConsumedCapacityTotal,<br/> })<br/><br/> if err != nil {<br/>  log.Fatal("scan failed", err)<br/> }<br/><br/> for _, i := range op.Items {<br/>  var u User<br/>  err := attributevalue.UnmarshalMap(i, &amp;u)<br/>  if err != nil {<br/>   log.Fatal("unmarshal failed", err)<br/>  }<br/> }<br/><br/> if op.LastEvaluatedKey != nil {<br/>  log.Println("all items have not been scanned")<br/> }<br/> log.Println("scanned", op.ScannedCount, "items in", time.Since(startTime).Seconds(), "seconds")<br/> log.Println("consumed capacity", *op.ConsumedCapacity.CapacityUnits)<br/>}</span></pre><p id="ed2b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">只需提供表(或二级索引)名称，就可以开始了！但是，由于 API 限制(每次调用相当于 1 MB 的数据)，您可能无法获得所有项目。在我的例子中，大约花费了<strong class="kw jg"> 0.5 </strong>秒来处理大约<strong class="kw jg"> 15000 </strong>条记录——其余的项目被跳过，因为超过了 1 MB 的限制。</p><p id="6a25" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">使用分页</strong></p><p id="6d24" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了处理数据的限制，<code class="fe lr ls lt lu b">Scan</code> API 在其输出中返回<code class="fe lr ls lt lu b">LastEvaluatedKey</code>,指向最后处理的记录。您需要做的就是再次调用<code class="fe lr ls lt lu b">Scan</code>，将<code class="fe lr ls lt lu b">ExclusiveStartKey</code>属性的值设置为<code class="fe lr ls lt lu b">LastEvaluatedKey</code>的值。</p><p id="f231" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">使用分页扫描方法花了我大约<strong class="kw jg"> 100 秒</strong>来扫描~<strong class="kw jg">750 万条</strong>记录。</p><p id="dcc1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">平行扫描</strong></p><p id="e920" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">分页很有帮助，但它仍然是一个连续的过程。有很大的改进余地。幸运的是，<code class="fe lr ls lt lu b">Scan</code>允许您采用并行化的方法，也就是说，您可以使用多个 workers(在本例中是<code class="fe lr ls lt lu b">goroutine</code> s)来并行处理数据！</p><pre class="nv nw nx ny gt nz lu oa bn ob oc bi"><span id="1a58" class="od mk jf lu b be oe of l og oh">func parallelScan(pageSize, totalWorkers int) {<br/> log.Println("parallel scan with page size", pageSize, "and", totalWorkers, "goroutines")<br/> startTime := time.Now()<br/><br/> var total int<br/><br/> var wg sync.WaitGroup<br/> wg.Add(totalWorkers)<br/><br/> for i := 0; i &lt; totalWorkers; i++ {<br/>  // start a goroutine for each segment<br/><br/>  go func(segId int) {<br/>   var segTotal int<br/><br/>   defer wg.Done()<br/><br/>   lastEvaluatedKey := make(map[string]types.AttributeValue)<br/><br/>   scip := &amp;dynamodb.ScanInput{<br/>    TableName:     aws.String(table),<br/>    Limit:         aws.Int32(int32(pageSize)),<br/>    Segment:       aws.Int32(int32(segId)),<br/>    TotalSegments: aws.Int32(int32(totalWorkers)),<br/>   }<br/><br/>   for {<br/>    if len(lastEvaluatedKey) != 0 {<br/>     scip.ExclusiveStartKey = lastEvaluatedKey<br/>    }<br/>    op, err := client.Scan(context.Background(), scip)<br/><br/>    if err != nil {<br/>     log.Fatal("scan failed", err)<br/>    }<br/><br/>    segTotal = segTotal + int(op.Count)<br/><br/>    for _, i := range op.Items {<br/><br/>     var u User<br/>     err := attributevalue.UnmarshalMap(i, &amp;u)<br/>     if err != nil {<br/>      log.Fatal("unmarshal failed", err)<br/>     }<br/>    }<br/><br/>    if len(op.LastEvaluatedKey) == 0 {<br/>     log.Println("[ segment", segId, "] finished")<br/>     total = total + segTotal<br/>     log.Println("total records processsed by segment", segId, "=", segTotal)<br/>     return<br/>    }<br/><br/>    lastEvaluatedKey = op.LastEvaluatedKey<br/>   }<br/>  }(i)<br/> }<br/><br/> log.Println("waiting...")<br/> wg.Wait()<br/><br/> log.Println("done...")<br/> log.Println("scanned", total, "items in", time.Since(startTime).Seconds(), "seconds")<br/>}</span></pre><p id="5748" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><code class="fe lr ls lt lu b">Segment</code>和<code class="fe lr ls lt lu b">TotalSegments</code>属性是<code class="fe lr ls lt lu b">Scan</code> API 如何实现并行的关键。<code class="fe lr ls lt lu b">TotalSegments</code>只是需要产生的线程/工作进程的数量，而<code class="fe lr ls lt lu b">Segment</code>是每个线程/工作进程的唯一标识符。</p><p id="4148" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在我的测试中，对于大约 750 万条记录(我尝试了各种页面大小和<code class="fe lr ls lt lu b">goroutine</code>组合)，性能(几乎)保持在<strong class="kw jg"> 37-40 秒</strong>(平均)不变。</p><p id="ad02" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">我需要配置多少个</strong> <code class="fe lr ls lt lu b"><strong class="kw jg">TotalSegments</strong></code> <strong class="kw jg">？？？</strong></p><p id="a81e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">要调优适当数量的并行线程/工作线程，您可能需要做一些实验。这很大程度上取决于您的客户端环境。</p><ul class=""><li id="f3d9" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">您有足够的计算资源吗？</li><li id="53a1" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">一些环境/运行时可能有托管线程池，所以您必须遵守这些规则</li></ul><p id="a79f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，您需要不断尝试，为您的找到最佳的并行性。考虑这个问题的一种方法是为每个数据单元选择一个段(单个工作线程/线程/ <code class="fe lr ls lt lu b">goroutine</code>)。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="d6a1" class="mj mk jf bd ml mm mn mo mp mq mr ms mt kl mu km mv ko mw kp mx kr my ks mz na bi translated">总结— API 考虑事项</h1><p id="ed2d" class="pw-post-body-paragraph ku kv jf kw b kx nb kg kz la nc kj lc ld nd lf lg lh ne lj lk ll nf ln lo lp ij bi translated"><code class="fe lr ls lt lu b">Batch</code>和<code class="fe lr ls lt lu b">Scan</code>API 都非常强大，但是有些细微差别你应该注意。我的建议是通读 API 文档。</p><p id="dcaa" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">同</strong><code class="fe lr ls lt lu b"><strong class="kw jg">Batch</strong></code><strong class="kw jg">API:</strong></p><ul class=""><li id="adba" class="ng nh jf kw b kx ky la lb ld ni lh nj ll nk lp nl nm nn no bi translated">一批不超过<strong class="kw jg"> 25 </strong>个请求</li><li id="0359" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">一批中的个别项目应<em class="ly">而不是</em>超过<strong class="kw jg"> 400KB </strong></li><li id="8d08" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">单个<code class="fe lr ls lt lu b">BatchWriteItem</code>中项目的总大小不能超过<strong class="kw jg"> 16MB </strong></li><li id="8507" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated"><code class="fe lr ls lt lu b">BatchWriteItem</code> <em class="ly">无法</em>更新物品</li><li id="3c70" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">您<em class="ly">不能</em>指定单个<code class="fe lr ls lt lu b">put</code>和<code class="fe lr ls lt lu b">delete</code>请求的条件</li><li id="ae59" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">它不<em class="ly">而不</em>在响应中返回已删除的项目</li><li id="9608" class="ng nh jf kw b kx np la nq ld nr lh ns ll nt lp nl nm nn no bi translated">如果有<em class="ly">失败的</em>操作，您可以通过<code class="fe lr ls lt lu b">UnprocessedItems</code>响应参数访问它们</li></ul><p id="1e51" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">明智地使用扫描</strong></p><p id="13d6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">由于一个<code class="fe lr ls lt lu b">Scan</code>操作遍历整个表(或二级索引),它很可能会消耗一大块提供的吞吐量，尤其是如果它是一个大表的话。话虽如此，<code class="fe lr ls lt lu b">Scan</code>应该是你的最后一招。检查<a class="ae lq" href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html" rel="noopener ugc nofollow" target="_blank">查询 API </a>(或<a class="ae lq" href="https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html" rel="noopener ugc nofollow" target="_blank"> BatchGetItem </a>)是否适用于您的用例。</p><blockquote class="lv lw lx"><p id="5ea1" class="ku kv ly kw b kx ky kg kz la lb kj lc lz le lf lg ma li lj lk mb lm ln lo lp ij bi translated"><em class="jf">同样适用于</em>平行<em class="jf"> </em> <code class="fe lr ls lt lu b"><em class="jf">Scan</em></code> <em class="jf">。</em></p></blockquote><p id="8801" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">通过使用一个<a class="ae lq" href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html#Query.FilterExpression" rel="noopener ugc nofollow" target="_blank">过滤表达式</a>、一个<code class="fe lr ls lt lu b">Limit</code>参数(如前所述)或一个<code class="fe lr ls lt lu b">ProjectionExpression</code>来进一步缩小结果的范围，只返回属性的一个子集。</p><p id="90c2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这个博客到此为止。希望你觉得有用。</p><p id="9d01" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">下次见，编码快乐！</p></div></div>    
</body>
</html>