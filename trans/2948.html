<html>
<head>
<title>Twitter Sentiment Analysis Using LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用LSTM的推特情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-lstm-in-twitter-sentiment-analysis-a5d9013b523b#2022-06-28">https://towardsdatascience.com/using-lstm-in-twitter-sentiment-analysis-a5d9013b523b#2022-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4f0d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">详细了解LSTM应用于情感分析的能力</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/660e1a8c44590a5e66343afc658b33bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GY2Pv_AL1BYaQSoK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">尼古拉·伯恩森在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5477" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你坐上汽车的驾驶座时，你是能够发动引擎开车离开，还是在开车离开之前，你需要考虑你将如何做以及这个过程的每一步？对于大多数人来说，这是第一选择。</p><p id="dd0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这也是递归神经网络(RNNs)的工作方式。你不需要从头开始考虑你一天中要做的每一个基本任务，因为我们有一个叫做记忆的小东西，循环神经网络也是如此。但是他们是如何学习和储存这些记忆的呢？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/873f1d2cbf27072e437a6ae18be3ccd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*MiEcJEe4cH_7TsH0TbyQsg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">rnn有环路，允许它们从信息中学习。来源:https://colah.github.io/posts/2015-08-Understanding-LSTMs/<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="7041" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，神经网络的一个单元接收某种类型的输入<em class="lt"> Xt </em>并输出一个值<em class="lt"> Ht。</em>然后，循环将信息提供给下一个单元格，依此类推。因此，rnn可以被认为是同一个网络的副本，每个都向下一个传递信息。当我们看一个完整的链时，就更容易理解了，它是上图中展开的环:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/ce9ed6a2d5d360bb512d44d26069f785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">展开的递归神经网络的完整链。来源:<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></figure><p id="b9a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过去的几年中，RNNs已经被成功地应用于多种自然语言处理项目中。这些应用包括语音识别、语言建模、翻译，当然还有我们将在本文中探讨的情感分析。如果没有一种特殊的RNN，这一切都是不可能的，那就是LSTM(长短期记忆)。出于许多目的，LSTMs比传统的递归网络工作得更好，这里我们将展示如何以及为什么。但首先，我们需要谈谈为什么我们不能只使用正常的RNNs，它有什么问题。</p><p id="3e9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">rnn令人兴奋的一点是，它们可以在预测中使用过去的信息。考虑一个试图根据前面的单词来预测一个句子的最后一个单词的模型。在句子“书在<em class="lt">桌子</em>上”中，最后一个单词的选择范围很窄，而“<em class="lt">桌子</em>将是一个很好的预测，不需要太多额外的上下文就可以正确预测。在相关单词与预测之间的距离很小的情况下，RNNs可以很好地预测。然而，如果我们需要更多的上下文来进行单词预测，会发生什么呢？</p><p id="d082" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想想这个句子“这几天我一直在学习，在桌子上，有一本书”。最近的信息表明，桌子上有一些物体，但如果我们想缩小到哪个物体，我们应该进一步回到句子。然后，如果我们在第一句话中查看单词“<em class="lt">学习</em>，我们将获得预测正确所需的额外上下文。由于句子以“<em class="lt">我一直在学习</em>”开头，所以桌子上不太可能有足球或伏特加酒瓶，而不是一支笔或一本书。这正是为什么LSTM模型现在被广泛使用，因为它们被特别设计成具有长期“记忆”，能够比受长期依赖问题影响的其他神经网络更好地理解整体背景。</p><p id="35b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理解LSTM如何工作的关键是细胞状态。细胞状态通过一些微小的线性相互作用来运行整个链，在这些相互作用中，LSTM可以通过称为门的结构来添加或删除信息。有两个门:一个sigmoid神经网络层和一个逐点乘法操作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/315b06669286a86dbab272e616b5ef94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGY8vT3iSpCgqUAR_abe_Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单元格状态是穿过顶部的粗体线条。来源:<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></figure><p id="c2e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">盖茨负责选择哪些信息将通过。例如，sigmoid层将sigmoid函数应用于原始向量，并输出0和1之间的值。那是什么意思？零值将阻挡所有信息，而值1将让所有信息通过第一门并在单元状态流中继续。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lv"><img src="../Images/1747a454def433aa4b2a5159b0561b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDX3N93xsmrdJ_T2nMwUgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">“忘记”栅极层将sigmoid函数应用于单元状态。来源:<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></figure><p id="7d97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经说过，LSTM的第一个决定来自sigmoid层门，我们也可以称之为“忘记”门。该层查看两个输入:<em class="lt"> Xt，</em>带有矢量化文本，我们希望预测情感(借助于标记器，如下图所示)，以及<em class="lt"> Ht-1 </em>带有模型已经拥有的先前信息。然后，应用sigmoid函数，并且它为每个单元状态<em class="lt"> Ct-1输出0和1之间的数字。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/a08b25fc1196b8eb1f07218820400c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*As42NZNlPLnXYQp7hVP2Xg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Tokenizer:这就是如何将原始文本矢量化为LSTM模型的输入。每个字相当于编码器定义的一个数字。来源:作者</p></figure><p id="000c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们不会详细介绍其他LSTM层，因为重点是展示如何将其应用于Twitter情感分析，但该算法的演练在这里<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">详细解释</a>。既然我们已经谈论了很多LSTM理论，让我们编码并展示如何使用它来预测推文的情绪。</p><p id="1bef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初，我们需要获得一个包含分类推文的数据集来训练和测试我们的模型。我选择了Kaggle的<a class="ae kv" href="https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview" rel="noopener ugc nofollow" target="_blank"> Tweet情感提取比赛</a>的数据集。这个数据集是大约27，500条推文的集合。</p><p id="60fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，让我们加载数据:</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="ad2c" class="mc md iq ly b gy me mf l mg mh">import pandas as pd<br/><br/>df_train = pd.read_csv('train.csv',sep=',')<br/>df_train.text=df_train.text.astype(str)<br/>df_val = pd.read_csv('test.csv',sep=',')<br/>df_val.text=df_val.text.astype(str)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/916d5797ed31ca1b919034da83b0327b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1blOZA5AMt8hKPs_8afziA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">预览我们加载的训练数据集(df_train)。来源:作者</p></figure><p id="e796" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">加载完数据后，我们的下一步是合并训练和验证数据集以创建单个数据集，我们可以对其应用预处理步骤，尤其是标记化过程，稍后我们将对此进行详细解释。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="46b7" class="mc md iq ly b gy me mf l mg mh">df = pd.concat([df_train.assign(ind="train"),df_val.assign(ind="validation")])</span></pre><p id="e825" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，下一步是去除中立情绪。这不是共识，但是<a class="ae kv" href="https://essay.utwente.nl/78791/1/Rietvelt_BA_EEMCS.pdf" rel="noopener ugc nofollow" target="_blank">一些文章</a>表明去除中性词是增强情感分析模型性能的一种方式。二元分类的结果更容易解释，模型往往表现更好。此外，因为我们要处理数组，所以有必要将“正”标签转换为1，将“负”标签转换为0。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="4a39" class="mc md iq ly b gy me mf l mg mh">df <strong class="ly ir">=</strong> df<strong class="ly ir">.</strong>loc[(df['sentiment'] <strong class="ly ir">==</strong> 'Positive') <strong class="ly ir">|</strong> (df['sentiment'] <strong class="ly ir">==</strong> 'Negative')]<br/><br/>df<strong class="ly ir">.</strong>loc[df['sentiment'] <strong class="ly ir">==</strong> 'Positive','sentiment'] <strong class="ly ir">=</strong> 1 df<strong class="ly ir">.</strong>loc[df['sentiment'] <strong class="ly ir">==</strong> 'Negative','sentiment'] <strong class="ly ir">=</strong> 0</span></pre><p id="862c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在好戏开始了！让我们开始操纵推文并对它们进行预处理，以准备我们的LSTM模型的输入。为此，我们需要加载两个库:Spacy和RE(正则表达式)。Spacy是一个令人惊叹的自然语言处理库，它有一些工具，如预训练的单词向量、词汇化、存储的停用词、实体识别，以及用于60多种语言的其他工具。我们所要做的就是在我们的Python环境中安装Spacy并下载我们想要的语言，在本文中是英语。然后我们将下载<em class="lt"> en_core_web_sm </em>这是一个英语培训的管道，它有较小的版本(也有中等和较大的版本)。然后，我们使用空间函数“load”将刚刚下载的管道应用到“<em class="lt"> nlp </em>对象。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="24b0" class="mc md iq ly b gy me mf l mg mh"><em class="lt"># pip install -U spacy<br/># python -m spacy download en_core_web_sm</em><br/><br/><strong class="ly ir">import</strong> spacy<br/><strong class="ly ir">import</strong> re</span><span id="b53e" class="mc md iq ly b gy mj mf l mg mh">nlp <strong class="ly ir">=</strong> spacy<strong class="ly ir">.</strong>load('en_core_web_sm')</span></pre><p id="c8b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然已经加载了NLP库，我们可以开始文本预处理了。首先，我们应该将我们的文本列转换为字符串，然后我们可以删除停用词，并使用上面刚刚创建的nlp加载管道中的“lemma_”和“is_stop”方法，通过一行代码应用词汇化。然后，我们必须将字符串转换成小写，并使用正则表达式库，这样我们就可以删除空格(\s)和字母数字字符(\w)，只留下对我们的模型有影响的单词。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="19a2" class="mc md iq ly b gy me mf l mg mh">df["text"] <strong class="ly ir">=</strong> df["text"]<strong class="ly ir">.</strong>astype(str)<br/>df["text"] <strong class="ly ir">=</strong> df['text']<strong class="ly ir">.</strong>apply(<strong class="ly ir">lambda</strong> x: " "<strong class="ly ir">.</strong>join([y<strong class="ly ir">.</strong>lemma_ <strong class="ly ir">for</strong> y <strong class="ly ir">in</strong> nlp(x) <strong class="ly ir">if</strong> <strong class="ly ir">not</strong> y<strong class="ly ir">.</strong>is_stop]))<br/>df['text'] <strong class="ly ir">=</strong> df['text']<strong class="ly ir">.</strong>apply(<strong class="ly ir">lambda</strong> x: x<strong class="ly ir">.</strong>lower())<br/>df['text'] <strong class="ly ir">=</strong> df['text']<strong class="ly ir">.</strong>apply((<strong class="ly ir">lambda</strong> x: re<strong class="ly ir">.</strong>sub('[^\w\s]','',x)))</span></pre><p id="7e3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是时候对我们的推文进行矢量化了，我们将使用Keras库预处理工具来完成这项工作。首先，我们必须基于词频定义要保留的最大单词数，我们将它设置为3000，以便在我们的向量中有一个稳定的单词种类。当我们将记号赋予文本时，重要的是我们在完整数据(df)上进行，否则，如果我们仅将记号赋予训练tweets，然后尝试使用相同的记号来转换验证数据集，我们可能会得到错误。有些单词只出现在验证数据集中，而对于标记器来说是未知的，因为它只适合使用训练数据集单词。因此，我们将使我们的tokenizer对象适合我们的完整数据集。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="6b68" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">from</strong> tensorflow.keras.preprocessing.text <strong class="ly ir">import</strong> Tokenizer<br/><strong class="ly ir">from</strong> tensorflow.keras.preprocessing.sequence <strong class="ly ir">import</strong> pad_sequences<br/><br/>max_features <strong class="ly ir">=</strong> 3000<br/>tokenizer <strong class="ly ir">=</strong> Tokenizer(num_words<strong class="ly ir">=</strong>max_features, split<strong class="ly ir">=</strong>' ')<br/>tokenizer<strong class="ly ir">.</strong>fit_on_texts(df['text']<strong class="ly ir">.</strong>values)</span></pre><p id="63e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将记号赋予器仅适用于训练数据集的另一个问题是输出的形状。例如，让我们想象我们的火车数据集只有一条推文，上面的例子是“<em class="lt"> le chat est noir </em>”。在标记化之后，我们将得到一个形状为(1，4)的数组。然后，如果我们试图预测一条推文“<em class="lt">我很高兴</em>”的情绪，一个形状为(1，3)的数组，这是不可能的，因为这些数组的形状不匹配。在这种情况下，我们可以使用numpy lib pad用0填充测试数组，以匹配标记器的原始形状(1，4)。这样做之后，我们仍然会遇到上一段提到的不适合单词的问题，因为单词“<em class="lt"> I </em>”、“<em class="lt"> am </em>”或“<em class="lt"> happy </em>”都不适合分词器，最终我们的数组将是[0，0，0，0]。可以使用max_features参数将tokenizer的形状设置为一个限制，但通常情况下，它与最大元素的形状相同，或者在我们的示例中，与包含最多单词的tweet的形状相同。</p><p id="fc66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然记号赋予器已经适合完整的数据框，我们可以分割回训练和验证数据集。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="8a4b" class="mc md iq ly b gy me mf l mg mh">df_train, df_val <strong class="ly ir">=</strong> df[df["ind"]<strong class="ly ir">.</strong>eq("train")], df[df["ind"]<strong class="ly ir">.</strong>eq("validation")]</span></pre><p id="5c01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在拆分回<em class="lt"> df_train </em>和<em class="lt"> df_val </em>并拟合记号化器之后，是时候对tweets进行矢量化了。这可以使用Keras '<em class="lt">texts _ to _ sequences</em>和<em class="lt"> pad_sequences来完成。</em></p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="a026" class="mc md iq ly b gy me mf l mg mh">X_train <strong class="ly ir">=</strong> tokenizer<strong class="ly ir">.</strong>texts_to_sequences(df_train['text']<strong class="ly ir">.</strong>values)<br/>X_train <strong class="ly ir">=</strong> pad_sequences(X_train)</span><span id="6fa8" class="mc md iq ly b gy mj mf l mg mh">X_val <strong class="ly ir">=</strong> tokenizer<strong class="ly ir">.</strong>texts_to_sequences(df_val['text']<strong class="ly ir">.</strong>values)<br/>X_val <strong class="ly ir">=</strong> pad_sequences(X_val)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/75d2386c036b99ffb07445c5665e3353.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*hPCerLQGx1J70qCJX6-xqw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量化的X_train数组。来源:作者</p></figure><p id="b854" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在对推文进行矢量化之后，是时候检查数据集的形状是否匹配了，否则我们需要调整其中一个。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="3954" class="mc md iq ly b gy me mf l mg mh">[X_train.shape,X_val.shape]</span></pre><p id="fe07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到的输出是[(16.363，20)，(2.104，18)]，意味着X_train数据集有16.363行(或tweets)和20个特征(或单词)，X_val数据集有2.104行(或tweets)和18个特征(或单词)。因此，我们需要用0填充X_val数据集，以匹配X_train的20个特征。但是我们怎么能对一个数组这么做呢？当然是Numpy！使用numpy lib pad，我们可以使用X_train的形状减去X_val的形状，以获得两个数组之间的特征(或单词)差异，并用常数值0填充X_val以匹配形状。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="6289" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">import</strong> numpy <strong class="ly ir">as</strong> np<br/>X_val <strong class="ly ir">=</strong> np<strong class="ly ir">.</strong>lib<strong class="ly ir">.</strong>pad(X_val, ((0,0),(X_train<strong class="ly ir">.</strong>shape[1] <strong class="ly ir">-</strong> X_val<strong class="ly ir">.</strong>shape[1],0)), 'constant', constant_values<strong class="ly ir">=</strong>(0))<br/>X_val<strong class="ly ir">.</strong>shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/b3e704bd60d17e38ebe06f8ea2f0e0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bsFmpp1ar7jvmodZXb8wg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">长度与X_train匹配的矢量化X_val数组。来源:作者</p></figure><p id="f2d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到的输出是(2.104，20)，最后，训练和验证数组具有相同的特征长度(20)，可以用于我们的预测。现在，我们预处理的最后一步是准备我们的目标值，所以我们要从这些变量中取出虚拟变量，并将它们转换成数组，以便在我们的训练和验证中使用。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="b528" class="mc md iq ly b gy me mf l mg mh">Y_train <strong class="ly ir">=</strong> np<strong class="ly ir">.</strong>array(pd<strong class="ly ir">.</strong>get_dummies((df_train['sentiment'])<strong class="ly ir">.</strong>values))<br/>Y_val <strong class="ly ir">=</strong> np<strong class="ly ir">.</strong>array(pd<strong class="ly ir">.</strong>get_dummies((df_val['sentiment'])<strong class="ly ir">.</strong>values))</span></pre><p id="e727" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练和验证数组都设置好了，是时候构建我们的LSTM模型了！但是我们从哪里开始呢？喀拉斯！最受欢迎的深度学习和神经网络库之一，它运行在机器学习库TensorFlow之上。它允许我们建立一个模型，并使用Keras上的sequential类，我们可以将一个线性堆栈的层组合到我们的模型中。这些层与我们在本文第一部分的LSTM解释中提到的层相同。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="9a10" class="mc md iq ly b gy me mf l mg mh"><strong class="ly ir">from</strong> tensorflow.keras.models <strong class="ly ir">import</strong> Sequential<br/><strong class="ly ir">from</strong> tensorflow.keras.layers <strong class="ly ir">import</strong> Dense, Embedding, LSTM, SpatialDropout1D</span></pre><p id="619b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，构建LSTM网络的第一步是创建一个嵌入层。嵌入层允许我们将每个单词转换成固定长度的向量，这是一种更好的方式来表示这些单词，同时降低维度。max_features参数是我们已经在标记化中设置的词汇表的大小，embed_dim参数是我们想要的每个单词的向量的长度，input_length是一个序列的最大长度，所以在我们的例子中，我们可以使用X_train.shape[1]，它输出20。就这样，我们的嵌入层完成了。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="dbf6" class="mc md iq ly b gy me mf l mg mh">max_features = 3000<br/>embed_dim = 128</span><span id="3822" class="mc md iq ly b gy mj mf l mg mh">model <strong class="ly ir">=</strong> Sequential()<br/>model<strong class="ly ir">.</strong>add(Embedding(max_features, embed_dim,input_length <strong class="ly ir">=</strong> X_train<strong class="ly ir">.</strong>shape[1]))</span></pre><p id="e474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继续，在我们的嵌入层之后，是时候给我们的模型添加一个空间下降1D层了。空间下降层的主要目的是避免过度拟合，这是通过概率性地移除该层的输入(或我们正在构建的网络中嵌入层的输出)来实现的。总而言之，它具有模拟许多不同网络的效果(通过丢弃随机元素，或者在我们的例子中是它们的30%——代码上的0.3个参数),并且最终，网络的节点对于未来的输入更加健壮，并且倾向于不过度拟合。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="e615" class="mc md iq ly b gy me mf l mg mh">model<strong class="ly ir">.</strong>add(SpatialDropout1D(0.3))</span></pre><p id="6e6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，是时候添加我们的明星层，一个我们谈论了这么多，唯一的LSTM！我们定义的第一个参数lstm_out为256，它是输出空间的维数，我们可以选择一个更大的数来尝试改进我们的模型，但这可能会导致许多问题，如过拟合和长训练时间。dropout参数应用于我们模型的输入和/或输出(线性变换)，而recurrent dropout应用于模型的递归状态或单元状态。换句话说，经常性的辍学影响了网络的“记忆”。对于这个LSTM网络，我选择使用更大的压差和0.5的经常压差，因为我们的数据集很小，这是避免过度拟合的重要一步。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="0ffc" class="mc md iq ly b gy me mf l mg mh">lstm_out = 256</span><span id="36a2" class="mc md iq ly b gy mj mf l mg mh">model<strong class="ly ir">.</strong>add(LSTM(lstm_out, dropout<strong class="ly ir">=</strong>0.5, recurrent_dropout<strong class="ly ir">=</strong>0.5))</span></pre><p id="53f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们的LSTM层已经完成了，是时候准备我们的激活函数了。我们通过添加一个密集连接的层来实现我们网络的激活功能，并输出我们最终阵列所需的维度。正如我们之前看到的，在LSTM，我们需要一个sigmoid神经网络层，这正是我们选择sigmoid激活的原因。sigmoid函数输出一个介于0和1之间的值，它可以让信息不流过或完全流过这些门。</p><p id="070d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们准备编译模型，这是为训练配置模型。我们将为我们的损失函数选择分类交叉熵，这是一种广泛使用的损失函数，用于量化分类问题中的深度学习模型错误。对于我们的优化器，我们将使用Adam优化器，这是Adam算法的一种实现，它是随机梯度下降的一种健壮扩展版本，是深度学习问题中最常用的优化器之一。此外，选择的指标将是准确性，因为我们有一个情绪分析问题，我们需要预测它是正面还是负面的推文。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="df9b" class="mc md iq ly b gy me mf l mg mh">model<strong class="ly ir">.</strong>add(Dense(2,activation<strong class="ly ir">=</strong>'sigmoid'))<br/>model<strong class="ly ir">.</strong>compile(loss <strong class="ly ir">=</strong> 'categorical_crossentropy', optimizer<strong class="ly ir">=</strong>'adam',metrics <strong class="ly ir">=</strong> ['accuracy'])<br/>print(model<strong class="ly ir">.</strong>summary())</span></pre><p id="738f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们的最终模型:</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="9a37" class="mc md iq ly b gy me mf l mg mh">Model: "sequential"<br/>_________________________________________________________________<br/> Layer (type)                Output Shape              Param #   <br/>=================================================================<br/> embedding (Embedding)      (None, 161, 128)          384000    <br/>                                                                 <br/> spatial_dropout1d (Spatia  (None, 161, 128)         0         <br/> lDropout1D)                                                     <br/>                                                                 <br/> lstm_3 (LSTM)               (None, 256)               394240    <br/>                                                                 <br/> dense_3 (Dense)             (None, 2)                 514       <br/>                                                                 <br/>=================================================================<br/>Total params: 778,754<br/>Trainable params: 778,754<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="5ed7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着模型的建立，是时候执行每个数据科学家都喜欢的命令了。因此，让我们将我们的模型拟合到X_train和Y_train阵列，10个历元应该足以获得预测的良好性能。批量大小是在执行重量更新(一个时期)之前，通过网络运行的样本数量，我们将保持它较低，因为它需要较少的内存。</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="4cfc" class="mc md iq ly b gy me mf l mg mh">batch_size = 32<br/>model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2, shuffle=False)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/23d819f387da5cf1817cfbc738fedcf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*nMKBpgjPxd00qOw0gNlSjw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">培训的回访历史。来源:作者</p></figure><p id="0627" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在10个时期后，我们可以看到我们的模型的性能在每个时期都有所提高，损失减少，精度增加，最终在训练数据集上达到94.4%的高精度。</p><p id="d20a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是时候使用我们训练过的模型来衡量它在测试数据集上的性能了，这是我们之前构建的<em class="lt"> X_val </em>。在深度学习问题上这样做的一个好方法是建立一个函数，这里我们有一个情感分析分类问题，在验证(或测试)数据集上检查我们模型的准确性、f1分数和混淆矩阵是一个好主意。下面是我用过的一个:</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="94ee" class="mc md iq ly b gy me mf l mg mh">import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="1a99" class="mc md iq ly b gy mj mf l mg mh">def evaluate_lstm(model, X_test,Y_test):</span><span id="74e6" class="mc md iq ly b gy mj mf l mg mh"> pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0<br/> results = []</span><span id="c4bc" class="mc md iq ly b gy mj mf l mg mh"> for x in range(len(X_test)):<br/>  result =   model.predict(X_test[x].reshape(1,X_test.shape[1]),<br/>  batch_size=1,verbose = 3)[0]</span><span id="c98e" class="mc md iq ly b gy mj mf l mg mh">  if np.argmax(result) == np.argmax(X_test[x]):<br/>   if np.argmax(X_test[x]) == 0:<br/>    neg_correct += 1<br/>  else:<br/>    pos_correct += 1<br/>  <br/>  if np.argmax(X_test[x]) == 0:<br/>   neg_cnt += 1<br/>  else:<br/>   pos_cnt += 1<br/>  results.append(np.argmax(result))</span><span id="a55b" class="mc md iq ly b gy mj mf l mg mh"> Y_test_argmax = np.argmax(Y_test,axis=1)<br/> Y_test_argmax  = Y_test_argmax.reshape(-1,1)<br/> results = np.asarray(results)<br/> results = results.reshape(-1,1)</span><span id="43c8" class="mc md iq ly b gy mj mf l mg mh"> conf_matrix = confusion_matrix(Y_test_argmax, results)<br/> fig = plt.figure(figsize=(6, 6))<br/> sns.heatmap(conf_matrix, annot=True, fmt="d", cmap = 'GnBu');<br/> plt.title("Confusion Matrix")<br/> plt.ylabel('Correct Class')<br/> plt.xlabel('Predicted class')</span></pre><p id="75a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并将该函数应用于<em class="lt"> X_val </em>数组，我们得到:</p><pre class="kg kh ki kj gt lx ly lz ma aw mb bi"><span id="5249" class="mc md iq ly b gy me mf l mg mh">accuracy,f1, fig = evaluate_lstm(model,X_val,Y_val)<br/>print(f'Accuracy:{accuracy:.3f}')<br/>print(f'F1 Score: {f1:.3f}')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/05e0674784c534eee5eecbb729e58d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*SatcACrGMB3W1lVb85xbWw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">度量和混淆矩阵。来源:作者</p></figure><p id="a690" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到了87.3%的准确率和0.878的F1分数，这两个分数都很棒！另一个积极的事情是，该模型在积极(1)和消极(0)情绪上都表现良好。当然，验证数据集推文与训练数据集推文的主题相似，数据集在正面和负面推文之间保持平衡，这一事实会影响这些指标。它仍然显示了神经网络在情感预测方面的能力。该模型可以用新的推文进行训练，以提高对特定主题的预测性能等。</p><p id="a409" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终，神经网络只是线性代数，神奇的事情发生了。</p></div></div>    
</body>
</html>