<html>
<head>
<title>Can Hybrid-ML Approaches Help When Supervised Data Isn’t Enough for LLMs?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当监督数据不足以用于LLM时，混合ML方法能有所帮助吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-hybrid-ml-approaches-help-when-supervised-data-isnt-enough-for-llms-fa2b587677a0#2022-05-17">https://towardsdatascience.com/can-hybrid-ml-approaches-help-when-supervised-data-isnt-enough-for-llms-fa2b587677a0#2022-05-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="17a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">具有符号向量的精益架构可以产生更好的结果</h2></div><p id="bfab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">塞缪尔·阿尔盖里尼</em> <a class="ae lc" href="https://medium.com/@leonardo.rigutini" rel="noopener">和<em class="lb">雷奥纳多·里古蒂尼</em>T5】</a></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/aac27d486a0e214383faf731aceee3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6PFwrTLCcU-iW7cMS7INpQ.jpeg"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">大卫和歌利亚。提香1542 -1544年。来源:<a class="ae lc" href="https://commons.wikimedia.org/wiki/File:Tycjan_David_i_Goliat.jpg" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><h1 id="3b27" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">大型语言模型时代</strong></h1><p id="ff3f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在过去的十年里，我们观察到了自然语言处理领域的一个重要的范式转变。随着<a class="ae lc" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>技术的出现，<a class="ae lc" rel="noopener" target="_blank" href="/e2e-the-every-purpose-ml-method-5d4f20dafee4">端到端方法</a>已经逐渐取代了基于分析和选择特征的严格阶段的原始机器学习方法，它们(尤其是变形金刚)目前是NLP和相关领域的SOTA。通常，这些方法由非常大的人工神经网络模型(数十亿个参数)组成，这些模型被训练成用基于统计的方法来模拟语言。当使用神经网络时，训练阶段通常需要大量的数据和更多的计算资源。因此，由于对数据、功率和成本的更高需求，它们通常使用通用任务在非常大的数据集上进行预训练，然后发布以供使用或最终集成到专有工具中。你可以在<a class="ae lc" href="https://huggingface.co/blog/large-language-models" rel="noopener ugc nofollow" target="_blank">这篇</a>帖子中找到关于这些巨头的有用讨论。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mq"><img src="../Images/1d6de21f2031b89fc5470eef42a73e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PBv21-mmPWMWXYvP80AKlw.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图1 — <em class="mr">过去几年大型语言模型(LLM) </em> <br/> <em class="mr">规模趋势。图片由作者受</em> <a class="ae lc" href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/" rel="noopener ugc nofollow" target="_blank">启发<em class="mr">微软研究院博客</em> </a></p></figure><p id="d402" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">集成阶段通常需要一个微调阶段，旨在将模型裁剪为您想要重现的数据和任务，并响应特定的业务需求。值得注意的是，这个过程需要大量的标记数据(受监督的)，反映要复制的任务的特征和要求。然而，在现实世界的场景中，这种数量的标记数据通常是不可用的，并且它的产生是耗时且相当昂贵的(<a class="ae lc" rel="noopener" target="_blank" href="/can-you-trust-your-model-when-data-shifts-981ed681f1fd">此处</a>您可以找到一篇关于类似问题的有趣文章)。因此，将机器学习技术应用于商业或行业特定用例的两个最大挑战是数据的稀缺和计算资源的缺乏。</p><p id="da03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">围绕“少量学习”<a class="ae lc" href="https://arxiv.org/pdf/1904.05046.pdf" rel="noopener ugc nofollow" target="_blank"> 11 </a>技术的研究主要集中在研究和比较从少量监督数据和大量未标记数据中学习的方法。在这个领域，我们已经看到越来越多的新混合方法，其中由通用大型语言模型(LLM)返回的密集表示与知识的符号表示相结合(通常富含语言和语义信息)，从而即使在处理较小的数据集时也能提供显著的提升。</p><h1 id="91e7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">但是AI不仅仅是深度学习</strong></h1><p id="67c2" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">Gartner将“<a class="ae lc" href="https://www.gartner.com/en/webinars/4002198/composite-ai-how-could-we-finally-get-ai-to-be-smarter-" rel="noopener ugc nofollow" target="_blank">复合人工智能</a>”定义为不同人工智能技术的组合，以实现更好的结果。没错，人工智能不再仅仅是机器学习或深度学习。例如，基于规则的系统是人工智能领域的经典，它们是解决特定任务的不同但有效的方法。本文中讨论的混合方法由一个ML算法组成，该算法由文本的符号表示提供支持。</p><p id="c9c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种文本的符号表示利用了来自<a class="ae lc" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>的前一步骤的丰富的语言信息列表(形态句法和语义数据),包括许多最常见的<a class="ae lc" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>任务，如词汇化、依存和选区解析、词性标注、形态分析、关系检测等。此外，这些技术拥有丰富的<a class="ae lc" href="https://en.wikipedia.org/wiki/Knowledge_graph" rel="noopener ugc nofollow" target="_blank">知识图</a>，这是一个巨大的知识树，由代表概念和它们之间关系的节点和连接组成。由NLP分析的这一步骤产生的不同类型的信息被表示在分离的向量空间中，这些向量空间被连接并输入到机器学习算法中。</p><p id="4f61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上，在这种混合方法中，使用大型语言模型(LLM)产生的基于密集向量(即<a class="ae lc" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">嵌入</a>)的最常见表示技术被替换为文本的符号表示，其中向量的每个维度编码文本的清晰和明确的语言特征。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/20a37c205f69b0638fcbb6f22d7c3888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*61M3NiHLzuMWx5OBPRx_4Q.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><em class="mr">图2——所用混合方法的方案。作者图片</em></p></figure><h1 id="bb1c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">挑战</strong></h1><p id="45c3" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">当监督数据稀缺时，将符号人工智能与机器学习结合起来可能会改变游戏规则。事实上，当数据对于LLM来说根本不够用时，微调任务是极其复杂和无效的。</p><p id="7767" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个实验中，我们比较了几种主流机器学习技术与expert.ai平台中使用的混合ML在数据稀缺条件下的性能——训练集(监督数据)的大小从每类几个样本到每类大约一百个样本不等。比较的重点是:</p><ul class=""><li id="13b4" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la my mz na nb bi translated">基于变压器架构的模型:<a class="ae lc" href="https://huggingface.co/bert-large-uncased" rel="noopener ugc nofollow" target="_blank">BERT</a>【1，2】及其衍生<a class="ae lc" href="https://huggingface.co/distilbert-base-uncased" rel="noopener ugc nofollow" target="_blank">DistilBERT</a>【3，4】和<a class="ae lc" href="https://huggingface.co/roberta-large" rel="noopener ugc nofollow" target="_blank">RoBERTa</a>【5，6】；</li><li id="3c28" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated"><a class="ae lc" href="https://spacy.io" rel="noopener ugc nofollow" target="_blank"> Spacy </a>提供的车型:BoW、CNN、Ensemble</li><li id="0c7d" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">关于BoW表示的标准<a class="ae lc" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">sk learn</a>【8，9】ML算法(SVM、NB、RF、LR)</li><li id="e112" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated"><a class="ae lc" href="https://www.expert.ai" rel="noopener ugc nofollow" target="_blank"> expert.ai </a>的混动车型</li></ul><p id="9333" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进行比较，我们选择了<a class="ae lc" href="https://www.consumerfinance.gov/data-research/consumer-complaints/" rel="noopener ugc nofollow" target="_blank">消费者投诉数据集</a> (CCD):在<a class="ae lc" href="https://www.kaggle.com/datasets/selener/consumer-complaint-database" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上公开的关于金融产品和服务的投诉集合。该数据集包括从现有公司收集的真实投诉，并侧重于金融产品和服务，其中每个投诉都已按产品正确标记，同时创建了一个有9个目标类别(标签)的监督文本分类任务。</p><p id="d4fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的实验，较长和较短的文本被删除，最终得到一个由80，523个监督文档组成的最终数据集:其中10%用作测试集(8052)，而其余的用作训练数据。为了测量每个模型在少量学习场景中的分类能力，我们通过随机二次抽样建立了4个维度递增的训练集:</p><ul class=""><li id="6e34" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la my mz na nb bi translated">T90:每类10个文档(总大小为90)；</li><li id="cb31" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">T450:每类50个文档(总大小为450)；</li><li id="0fcc" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">T810:每类90个文档(总大小为810)；</li><li id="7aaa" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">TFull:没有子采样(总大小为72471)。</li></ul><p id="97a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用增量过程来构建训练数据集，以便特定类别的监督文档始终包含在较大的数据集中:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8cb44f1acf9cdedd1055b945851e190b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NLZHxPuzFJ7ZbgylEQrD3w.png"/></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><em class="mr">图3-为少数镜头比较建立的4个训练集。作者图片</em></p></figure><h1 id="c3a0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">获胜者是……</strong></h1><p id="a3ac" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">表1显示了随着训练集大小的增加，所有模型的分类性能。对于expert.ai混合模型，这些值代表通过比较4种不同算法(SVM、朴素贝叶斯、随机森林和逻辑回归)获得的最佳结果，同时利用文本的符号表示。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ni"><img src="../Images/1c20e2219400057e53c6d3740a6fd785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZ1T7foKjQFQrk0f12jQag.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated"><em class="mr">表1:模型的性能随着训练集大小的增加而增加。作者图片</em></p></figure><p id="f3d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用较小的训练集(T90、T450和T810)，混合方法实现了其最佳性能，特别是与基于变压器的模型(即BERT)相比，以及更一般地与深度神经网络(如SpaCy)相比，具有显著的提升。</p><p id="2ce3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果并不出人意料，因为人工神经网络，尤其是深度神经网络，通常需要大量的监督数据来进行有效的微调。在缺乏必要数据的情况下，绩效可能会很差。</p><p id="bc95" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实验结果带来的一个有趣的发现是，与主流算法相比，来自符号人工智能的丰富文本表示通常也提供了更好的结果。当调查来自SkLearn模型的结果时，这一点很明显，这些模型毫无例外地低于expert.ai的混合方法。</p><p id="d67e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，当增加训练集的规模时，方法之间的差距和差异会减小——当使用整个训练集时，它们实际上消失了(TFull)。在这种情况下，表现最好的模型是RoBERTa-Large，尽管所有其他模型，包括expert.ai的混合ML方法，都以非常小的偏差紧随其后。</p><h1 id="be14" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">结论</strong></h1><p id="fe76" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这些实验中出现的概念证实了一个假设，即大多数深度神经网络方法在监督数据稀缺的典型现实世界场景中可能不是非常有效。</p><p id="8cd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这些情况下，使用利用文本符号表示的混合方法似乎更有效，并产生最佳结果。这并不意外，因为深度模型通常也需要大量的监督数据来进行微调，而在相反的情况下，性能往往会下降。</p><p id="4f9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当使用混合方法时，利用丰富的文本符号表示弥补了监督数据的不足，甚至优于基于BoW表示的经典方法。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="d9a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献:</strong></p><ol class=""><li id="8ba5" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la nq mz na nb bi translated"><em class="lb"> Devlin，Jacob，et al .〈伯特:语言理解的深度双向转换器的预训练〉arXiv预印本</em><a class="ae lc" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"><em class="lb">arXiv:1810.04805</em></a><em class="lb">，(2018)。</em></li><li id="80f6" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">抱脸的BERT-Large:</em><a class="ae lc" href="https://huggingface.co/bert-large-uncased" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://huggingface.co/bert-large-uncased</em></a></li><li id="ac6f" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb"> SANH，Victor等人《蒸馏伯特，伯特的蒸馏版:更小、更快、更便宜、更轻》。arXiv预印本</em><a class="ae lc" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank"><em class="lb">arXiv:1910.01108</em></a><em class="lb">，(2019)。</em></li><li id="a1d8" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">香椿面:</em><a class="ae lc" href="https://huggingface.co/distilbert-base-uncased" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://huggingface.co/distilbert-base-uncased</em></a></li><li id="49c7" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">刘，，等，“RoBERTa:一种稳健优化的BERT预训练方法”。arXiv预印本ar</em><a class="ae lc" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"><em class="lb">Xiv:1907.11692</em></a><em class="lb">，(2019)。</em></li><li id="27cd" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">RoBERTa-Large on hugging face:</em><a class="ae lc" href="https://huggingface.co/roberta-large" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://huggingface.co/roberta-large</em></a></li><li id="7584" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">SpaCy:</em><a class="ae lc" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://SpaCy . io</em>T11】</a></li><li id="dc3f" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb"> PEDREGOSA，Fabian等人，“sci kit-learn:Python中的机器学习。《机器学习研究杂志》，(2011)，12:2825–2830。</em></li><li id="cf47" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">sk learn</em><a class="ae lc" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://scikit-learn.org/stable/index.html</em></a></li><li id="9a0e" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated"><em class="lb">expert . ai:</em>T22<em class="lb">https://www . expert . ai</em>T25】</li><li id="d3cf" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la nq mz na nb bi translated">王，亚青，等，“从几个例子中概括:关于少投学习的调查”美国计算机学会计算调查(csur)53.3(2020):1–34。<a class="ae lc" href="https://arxiv.org/pdf/1904.05046.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lb">https://arxiv.org/pdf/1904.05046.pdf</em></a></li></ol></div></div>    
</body>
</html>