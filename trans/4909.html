<html>
<head>
<title>Understanding L1 Regularisation in Gradient Boosted Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解梯度增强决策树中的L1正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a#2022-11-01">https://towardsdatascience.com/understanding-l1-regularisation-in-gradient-boosted-decision-trees-af4f0ba9d32a#2022-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3541" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">LightGBM和R中的一个例子</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/89de0ddc158bde4f73f08f411785f3ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4xys13siPg9IujBgMzilw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="48c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我发现超参数调整通常是建模中最有趣的部分之一。老实说，模型本身的算法已经被其他人编写好了，所以调整和设置阶段是我们大多数人有所作为的时候。尽管如此，我认为对算法的工作原理有一个透彻的了解是很重要的。</p><p id="fd93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当谈到梯度增强模型时，我发现超参数有不同的类别:</p><ul class=""><li id="54b0" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">像最大深度这样简单的问题——这些问题通常不会让人感到意外。</li><li id="cf4a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">像黑森最小和这样复杂的问题——我认为建模者通常拥有的直觉是正确的，即使细节有些模糊。</li><li id="b6fa" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">然后还有像L1或L2正规化这样的参数，我发现这里有很多混乱和误解，几乎没有人费心去深入挖掘并找出它们真正是如何工作的。</li></ul><blockquote class="mi"><p id="60e7" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">如果你碰到一个超参数，你不确定它的最优值会是0.0001还是10000，我觉得你应该退一步理解算法，而不是粗暴地强迫所有可能的组合。</p></blockquote><p id="fbb9" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我最近对L1的正规化进行了分析，因为我对一些发现感到非常惊讶，所以我决定写一份我从未有过的指南。希望你会觉得有用！</p><h1 id="ca9e" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">行动计划</h1><h2 id="44f8" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">对于忙碌/不耐烦的读者</h2><p id="0eb0" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">如果你不想看完整本书，在文章的最后有一个总结部分，你会在其中找到一个叫做解决方案的子部分。所有重要的结果都在代码片段中。</p><h2 id="ddc4" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">该项目</h2><p id="afca" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我们将用<a class="ae og" href="https://lightgbm.readthedocs.io/en/v3.3.2/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>和r中的合成数据来演示一个例子。我的目标是使这个例子可复制。这篇文章将包括一些代码片段，我在这个项目中使用的所有其他代码都可以在我的<a class="ae og" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_l1l2_regularisation.R" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到。</p><blockquote class="mi"><p id="db20" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">我将采取我通常的方法:运行一些模型，看看会发生什么，然后不要停止，直到我完全理解所有的数字是如何得出的。</p></blockquote><p id="d7db" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我们将详细了解:</p><ul class=""><li id="2a36" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">叶子分数和分割增益的计算，</li><li id="41d1" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">不同的<code class="fe om on oo op b">lambda_l1</code>值对模型拟合和预测的影响，</li><li id="c472" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">单棵树内发生了什么，</li><li id="1ac5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">测试分数如何随着<code class="fe om on oo op b">lambda_l1</code>变化，以及我们是否设法在模型性能上得到任何改进。</li></ul><h2 id="5e38" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">关于命名</h2><p id="884d" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">为了避免混淆，<strong class="la iu">我将一致地使用</strong> <code class="fe om on oo op b">lambda_l1</code> <strong class="la iu">表达式</strong>作为L1正则化参数。我承认XGBoost和LightGBM都使用了<code class="fe om on oo op b">lambda_l1</code> = <code class="fe om on oo op b">reg_alpha</code>和<code class="fe om on oo op b">lambda_l2</code> = <code class="fe om on oo op b">reg_lambda</code>，但是，最好还是安全一点！</p><h2 id="e4ab" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">为什么是泊松？</h2><p id="c949" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">分析泊松回归是我经常性的“爱好”,原因如下:</p><ul class=""><li id="e106" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">我在保险行业工作，传统的广泛使用的方法是用泊松分布来模拟索赔频率。</li><li id="9b6e" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">我只是觉得它更令人兴奋——大多数回归例子都是关于经典的残差平方极小化。(在某种程度上，这就是LightGBM参数设置中所谓的“回归”。)</li><li id="e576" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">这更容易证明我的主要观点:你必须理解算法是如何工作的。人们不能简单地将参数从一个问题复制到另一个问题，即使是在同一个模型和包中，因为超参数可能根据目标的不同而完全不同地工作。</li></ul><h2 id="9657" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">假定的知识</h2><p id="4f76" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">这篇文章是关于梯度增强树的L1正则化，我假设读者对这两个原理都很熟悉。</p><p id="6b5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我有一些关于这两个主题的早期作品，如果你想仔细阅读的话:</p><ul class=""><li id="202a" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><a class="ae og" rel="noopener" target="_blank" href="/hyperparameter-tuning-in-lasso-and-ridge-regressions-70a4b158ae6d"> L1和L2线性回归的规范化</a></li><li id="8716" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae og" rel="noopener" target="_blank" href="/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80">一般情况下的梯度增强</a></li><li id="04f2" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae og" rel="noopener" target="_blank" href="/understanding-min-child-weight-in-gradient-boosting-decision-trees-293f2381306">min _ sum _ hessian in details</a></li></ul><h1 id="e300" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">数据</h1><p id="ad9f" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我们将生成一个非常简单的数据集——不用说，在现实生活中，你永远不会对这样的数据使用如此复杂的方法，但它非常适合演示我的观点。数据有一个(！)列，一个分类特征，在代码中称为<code class="fe om on oo op b">var1</code>，有三个可能的值:0、1和2。我们为每个组分配一个泊松λ，并基于这些λ参数生成随机变量。</p><p id="fae0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最终，我们会有1000次观察。这对于出现可见的随机偏差来说足够小了。例如，组1和组2应该是相似的，然而它们的实际值的平均值是<code class="fe om on oo op b">0.73</code>和<code class="fe om on oo op b">0.61</code>:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="ac02" class="np my it op b gy ou ov l ow ox">var1 lambda number mean_target sum_target<br/>   0    0.3    484   0.3347107        162<br/>   1    0.7    297   0.7306397        217<br/>   2    0.7    219   0.6118721        134</span></pre><p id="a033" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想复制结果，我建议手动构建表格，因为你将在<a class="ae og" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_l1l2_regularisation.R" rel="noopener ugc nofollow" target="_blank">我的脚本</a>中找到的随机种子将在不同的设置中以不同的方式工作。下一个表是复制数据所需的全部内容，N是具有相应变量1和目标的观察值的数量，它们加起来应该是1，000:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="8af6" class="np my it op b gy ou ov l ow ox">#     var1 target   N<br/># 1:     0      0 349<br/># 2:     0      1 113<br/># 3:     0      2  18<br/># 4:     0      3   3<br/># 5:     0      4   1<br/># 6:     1      0 150<br/># 7:     1      1  93<br/># 8:     1      2  39<br/># 9:     1      3  14<br/># 10:    1      4   1<br/># 11:    2      0 121<br/># 12:    2      1  69<br/># 13:    2      2  22<br/># 14:    2      3   7</span></pre><h1 id="19df" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">巨大的期望</h1><p id="35a7" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">现在，我们将对该数据运行一个梯度增强模型，使用各种级别的<code class="fe om on oo op b">lambda_l1</code>，没有来自超参数的其他限制。</p><p id="1d49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以相当有把握地猜测，无限制(即<code class="fe om on oo op b">lambda_l1 = 0</code>运行)将简单地预测每组中的<code class="fe om on oo op b">mean_target</code>。(因为泊松λ的最大似然估计是样本的平均值。)积极<code class="fe om on oo op b">lambda_l1</code>的影响更难预测。</p><blockquote class="oy oz pa"><p id="ea5c" class="ky kz pb la b lb lc ju ld le lf jx lg pc li lj lk pd lm ln lo pe lq lr ls lt im bi translated">我希望通过引入<code class="fe om on oo op b">lambda_l1</code>正则化，让模型预测更接近理论。具体来说，我希望第1组和第2组“一起工作”，并意识到他们毕竟没有什么不同。</p></blockquote><p id="2e69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看结果如何！</p><h1 id="d1c5" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">系统模型化</h1><p id="5c73" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我遍历了0到200之间的整数<code class="fe om on oo op b">lambda_l1</code>值，并收集了模型对第0、1和2组的预测值。</p><h2 id="0269" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">密码</h2><p id="34ff" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">如果您想运行代码，这里是脚本的相应部分。假设有一个<code class="fe om on oo op b">data_curr</code>表，上面列出了1000个观察值。我们还需要<code class="fe om on oo op b">data.table</code>和<code class="fe om on oo op b">lightgbm</code>包。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h2 id="0723" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">(英)可视化(= visualization)</h2><p id="6b8f" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">结果存储在适当命名的<code class="fe om on oo op b">result_dt</code>表中。它包含每个<code class="fe om on oo op b">lambda_l1</code>组的预测。那么，我们准备好展示这些结果的图表了吗？鼓声…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/8a7ac54d192c4aadb0602dd3f0696fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wHwkFfQMTxMR3cfMN45q6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">lambda_l1值对预测的影响(图片由作者提供)</p></figure><p id="6591" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">X轴显示不同的<code class="fe om on oo op b">lambda_l1</code>值，因此线条代表随着正则化参数的增加预测的变化。我把线移动了一点，这样我们可以更好地看到它们何时一起移动。最后，所有3个组将共享同一个预测:总平均值。</p><h2 id="53ff" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">结果分析</h2><p id="1570" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">让我们收集一下目前已知的信息:</p><ul class=""><li id="95d2" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">第2组预测暂时不变，只在<code class="fe om on oo op b">lambda_l1</code> = 30左右开始变化，</li><li id="4497" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">第1组预测逐渐减少到第2组的预测，同时第0组预测增加，</li><li id="329b" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">这三组在一个点上相遇，但是接下来会有奇怪的上下跳跃，</li><li id="b31a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">最后，三个小组再次相遇，再也不会偏离共同的目标。</li></ul><blockquote class="mi"><p id="4139" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">不知道你怎么样，我觉得剧情很惊喜！</p></blockquote><p id="b24f" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">如果你想知道:<code class="fe om on oo op b">lambda_l1</code>0到1之间的值实际上并没有实现任何可测量的东西。</p><h1 id="e422" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">测试结果</h1><p id="f793" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">现在，让我们做一个快速测试:正则化实际上对模型性能做了什么吗？我们知道它确实改变了预测，但是这些改变是在正确的方向上吗？</p><p id="df25" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的测试方法如下:我生成了一个巨大的表，使用相同的泊松lambdas作为我们的训练数据，并在这个表上运行预测。这个想法是，大表将代表“真正的”分布——我不想让随机性在这一点上发挥作用。模型性能用平均泊松对数似然来衡量。</p><p id="1f4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事不宜迟，我们来看看剧情:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/e6091d1f0d884dbe3c6b7a23cace0114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYWJ7HW5ScVZoDlaeA2sKA.png"/></div></div></figure><p id="f73a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">非常令人惊讶的是，这种规范化似乎确实提高了一点性能。在<code class="fe om on oo op b">lambda_l1</code> = 20时，总平均对数似然从<code class="fe om on oo op b">-0.7028107</code>上升到<code class="fe om on oo op b">-0.6996772</code>。不是很大的收获，但我们会接受的。</p><blockquote class="mi"><p id="72b3" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">我们证实，在某些情况下，lambda_l1正则化可以提高模型性能。</p></blockquote><p id="fee4" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">既然我们知道这个项目不是毫无意义的，那么是时候进行一次真正的深度潜水，调查每棵树、树枝和树叶了。</p><p id="bb89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一点会有点干…我已经警告过你了！</p><h1 id="c1a3" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">深潜</h1><h2 id="0593" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">证明文件</h2><p id="489f" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">对于那些想要理解梯度推进树背后的数学原理的人，我通常的建议是(曾经)去查阅<a class="ae og" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>。</p><blockquote class="oy oz pa"><p id="77bc" class="ky kz pb la b lb lc ju ld le lf jx lg pc li lj lk pd lm ln lo pe lq lr ls lt im bi translated">然而，现在有一个小问题。</p></blockquote><p id="0a39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看他们引入模型复杂性(也称为正则化)时出现的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/ef5efb813d165fac8f5b25bcc590f87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*qtZUQQuVk7NxnJVpMZX-ag.png"/></div></figure><p id="1654" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上式中引入的两个正则项是γ和λ。问题是:它们都不是L1正则化术语！应该有一个因子，一个<code class="fe om on oo op b">lambda_l1</code>，惩罚单个叶片得分的绝对值(公式中的<em class="pb"> wj </em>)。γ惩罚了叶子本身的数量(公式中的<em class="pb"> T </em>，我不知道该怎么称呼这种类型的正则化…也许是L0正则化？有趣的是，如果我们看一下<a class="ae og" href="https://xgboost.readthedocs.io/en/stable/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost参数</a>，没有那个奇怪的γ参数的迹象。</p><blockquote class="mi"><p id="0b9d" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">如果你知道这是怎么回事，一定要让我知道。</p></blockquote><p id="57f7" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">因此，文档中的公式没有一个能立即适用于我们的问题…</p><h2 id="f5fb" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">树形地图</h2><p id="aa3f" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">接下来我开始分析单独的树木地图。如果您不知道，您可以使用下面的命令将模型中所有单棵树的细节转换成漂亮的<code class="fe om on oo op b">data.table</code>:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="a504" class="np my it op b gy ou ov l ow ox">lgb.model.dt.tree(my_model)</span></pre><p id="20ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe om on oo op b">my_model</code>是LightGBM助推器。</p><p id="b751" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些是非正则化(<code class="fe om on oo op b">lambda_l1</code> = 0)模型中最重要的列:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="715b" class="np my it op b gy ou ov l ow ox">   depth split_gain threshold internal_value leaf_value leaf_count<br/>1:     0  28.861738         0     -0.6674794         NA         NA<br/>2:     1         NA      &lt;NA&gt;             NA -0.7537717        484<br/>3:     1   1.721168         1     -0.5865387         NA         NA<br/>4:     2         NA      &lt;NA&gt;             NA -0.5621415        297<br/>5:     2         NA      &lt;NA&gt;             NA -0.6196252        219</span></pre><p id="8771" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在接下来的几节中，我们将看到它们是如何被导出的！</p><h1 id="ea38" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">非正则树的叶分数</h1><p id="a654" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我不打算花太多时间来描述泊松叶是如何工作的(同样，请参见我以前关于这个主题的帖子<a class="ae og" rel="noopener" target="_blank" href="/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80?source=user_profile---------3----------------------------">此处</a>和<a class="ae og" rel="noopener" target="_blank" href="/understanding-min-child-weight-in-gradient-boosting-decision-trees-293f2381306">此处</a>)，但是作为一个例子，上面树中第0组的第一次分裂<code class="fe om on oo op b">-0.7537717</code>计算如下:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="9088" class="np my it op b gy ou ov l ow ox">gradient &lt;- 484 * 0.513 - 162<br/>hessian &lt;- 484 * 0.513 * exp(0.7)<br/>(-gradient / hessian) * 0.5 + (-0.6674794)</span></pre><p id="7eca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里</p><ul class=""><li id="0ebd" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><code class="fe om on oo op b">484</code>:对那片叶子的观察次数</li><li id="593d" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe om on oo op b">0.513</code>:当前预测(起始预测是泊松的整体平均值)</li><li id="764a" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe om on oo op b">162</code>:第0组实际目标值</li><li id="7388" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">那只是一件泊松的事情，再一次，看我关于这个主题的早期作品…</li><li id="cc5b" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe om on oo op b">0.5</code>:学习率</li><li id="9268" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><code class="fe om on oo op b">-0.6674794</code>:当前预测的对数，log( <code class="fe om on oo op b">0.513</code>)，这是一个管理步骤，因此第一棵树中的叶分数也将包括总体平均目标</li></ul><p id="04e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记住泊松回归的对数函数！在第一个树之后，组0的预测将从开始的exp( <code class="fe om on oo op b">-0.6674794</code>)变为exp( <code class="fe om on oo op b">-0.7537717</code>)。如果我们在一棵树后停止训练，那就是模型预测。</p><p id="8eb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们知道在非正则版本中会发生什么，现在让我们试试正则版本！</p><h1 id="cfee" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">正则化树的叶分数</h1><h2 id="b623" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">起始示例</h2><p id="cc4a" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我随机挑选了<code class="fe om on oo op b">lambda_l1</code> = 15作为另一棵树进行调查。让我们看看模型中的第一棵树！</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="b8bc" class="np my it op b gy ou ov l ow ox">   depth split_gain threshold internal_value leaf_value leaf_count<br/>1:     0   19.69985         0     -0.6674794         NA         NA<br/>2:     1         NA      &lt;NA&gt;             NA -0.7387716        484<br/>3:     1         NA      &lt;NA&gt;             NA -0.6006085        516</span></pre><blockquote class="mi"><p id="63fd" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">第一印象:现在只有一个分裂，第1组和第2组没有分开，不像在非正规版本中！</p></blockquote><p id="c182" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我基本上是从我在<a class="ae og" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>的结构分数部分找到的最佳叶分数的公式开始的(为了避免混淆，我没有在这里复制它)，并使用了一些试错法，其方式似乎符合叶分数绝对值的逻辑。</p><p id="1a06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在叶计算中，我们必须在梯度和中减去或加上λ参数。如果梯度和为正，我们必须减去<code class="fe om on oo op b">lambda_l1</code>，如果梯度和为负，我们加上<code class="fe om on oo op b">lambda_l1</code>。嗯，我们把它加到梯度上，但是我们取和的负值…用数字来解释更简单，我给你演示一下！</p><p id="c413" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，<code class="fe om on oo op b">-0.7387716</code>在上面的树中被计算为:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="6522" class="np my it op b gy ou ov l ow ox">gradient_l &lt;- (484 * 0.513) - 162<br/>hessian_l &lt;-  (484 * 0.513) * exp(0.7)<br/>(-(gradient_l - 15) / hessian_l) * 0.5  + log(0.513)</span></pre><p id="442e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而另一个分支，<code class="fe om on oo op b">-0.6006085</code>计算为:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="761a" class="np my it op b gy ou ov l ow ox">gradient_r &lt;- (516 * 0.513) - 351<br/>hessian_r &lt;-  (516 * 0.513) * exp(0.7)<br/>(-(gradient_r + 15) / hessian_r) * 0.5 + log(0.513)</span></pre><p id="416f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以它几乎和非正则化树的公式一样，不同的是加上或减去15。</p><h2 id="ef30" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">并发症</h2><blockquote class="mi"><p id="4a13" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">在这一点上，你可能会有一个困扰的问题:如果lambda_l1大于当前梯度和的绝对值，会发生什么？</p></blockquote><p id="824c" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">在我们的示例中，如果某个分割中的梯度之和小于15，会发生什么情况？我们会交换标志吗？好吧，如果我们看看模型中的第三棵树，<code class="fe om on oo op b">tree_index</code> = 2，我们可以看到:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="c81f" class="np my it op b gy ou ov l ow ox">   depth split_gain threshold internal_value  leaf_value leaf_count<br/>1:     0  6.4673676         0     0.00000000          NA         NA<br/>2:     1         NA      &lt;NA&gt;             NA -0.04681926        484<br/>3:     1  0.2443614         1     0.03309579          NA         NA<br/>4:     2         NA      &lt;NA&gt;             NA  0.04561548        297<br/>5:     2         NA      &lt;NA&gt;             NA  0.00000000        219</span></pre><p id="3794" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此在两棵树(索引0和1)之后，模型将停止向组2分配额外的分数。在树0和1中，它为组0分配了一个预测，并为组合的组1和2分配了一个预测。然而，在树索引2中，它决定拆分组1和组2，但实际上保持组2不变。</p><p id="1588" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目前，我们不会关注它为什么突然决定拆分这两个组。让我们暂时接受模型分裂，并尝试理解为什么组2的新叶分数是0。</p><p id="bdb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，这很简单。当我们到达树索引2时，组2的预测是<code class="fe om on oo op b">0.5745756</code>，这意味着它的梯度和将是:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="2e18" class="np my it op b gy ou ov l ow ox">219 * 0.5745756 - 134 = -8.167944</span></pre><p id="dff7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其绝对值低于我们的<code class="fe om on oo op b">lambda_l1</code>。让我们看看第一组在这一点上的梯度和:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="a4df" class="np my it op b gy ou ov l ow ox">297 * 0.5745756 - 217 = -46.35105</span></pre><p id="33ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是为什么模型将新的叶子分数分配给组1，而不是组2。</p><p id="2254" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总之，我们必须将gradient — <code class="fe om on oo op b">lambda_l1</code>表达式限制在0，以获得正确的叶子分数。</p><blockquote class="oy oz pa"><p id="5b46" class="ky kz pb la b lb lc ju ld le lf jx lg pc li lj lk pd lm ln lo pe lq lr ls lt im bi translated">不要担心现在是否有些混乱，我们将在本文后面的精确代码函数中阐明这种关系。</p></blockquote><h1 id="2de3" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">分割收益</h1><h2 id="21bf" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">大局</h2><p id="b0e3" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">在树中，我们还没有重新计算最后一个值:分割收益。这一列非常重要，例如，这是许多特征重要性分析的基础，在这些分析中，特征是按照它们的分裂所附带的增益来排序的。</p><p id="f7af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将再次链接<a class="ae og" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>,因为那里的基本思想非常容易理解，但我没有复制公式，因为它缺少<code class="fe om on oo op b">lambda_l1</code>参数。在我们的例子中，目标包括两个项目:最大化对数似然，以及L1正则化形式的惩罚。</p><blockquote class="mi"><p id="54dd" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">当树考虑拆分时，它基本上在目标函数中进行前后比较。</p></blockquote><p id="55cc" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">具体地，我们将我们将实现的左右分支/叶的增益与相同观察的未分裂增益进行比较。"这组观察值值得进一步拆分吗？"是模型所要求的。</p><h2 id="538c" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">一个例子</h2><p id="f943" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">让我们再看一遍上面的例子(这是第一棵树，从<code class="fe om on oo op b">lambda_l1</code> = 15，我们已经重新计算了它的叶子):</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="e18d" class="np my it op b gy ou ov l ow ox">   depth split_gain threshold internal_value leaf_value leaf_count<br/>1:     0   19.69985         0     -0.6674794         NA         NA<br/>2:     1         NA      &lt;NA&gt;             NA -0.7387716        484<br/>3:     1         NA      &lt;NA&gt;             NA -0.6006085        516</span></pre><p id="bfb1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再说一次，我没有详细说明我是如何得到这些公式的，以及我使用了哪些资源，下面几节有一些有用的提示。现在，让我向你展示一下上面的<code class="fe om on oo op b">19.69985</code>是如何计算的。</p><p id="24dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将需要左右分割的梯度和黑森，我们已经在<code class="fe om on oo op b">leaf_value</code>计算中使用了这些:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="d380" class="np my it op b gy ou ov l ow ox">gradient_l &lt;- (484 * 0.513) - 162<br/>hessian_l &lt;-  (484 * 0.513) * exp(0.7)<br/>gradient_r &lt;- (516 * 0.513) - 351<br/>hessian_r &lt;-  (516 * 0.513) * exp(0.7)</span></pre><p id="bdf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分割增益将由3项组成:</p><ul class=""><li id="3007" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">未分割的增益，</li><li id="3440" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">左侧增益，</li><li id="9adc" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">正确的收益。</li></ul><p id="f9cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先是<strong class="la iu">未分割增益</strong>——这在我们的例子中非常简单，这是树中的第一个分割，如果我们不这样做，我们的增益正好是0。</p><p id="29f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来自<strong class="la iu">左分离</strong>的增益可计算如下:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="1efa" class="np my it op b gy ou ov l ow ox">(gradient_l - 15) ^ 2 / hessian_l = 10.16513</span></pre><ul class=""><li id="fba7" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><strong class="la iu">右分</strong>的增益:</li></ul><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="97fb" class="np my it op b gy ou ov l ow ox">(gradient_r + 15) ^ 2 / hessian_r = 9.53473</span></pre><p id="7ff2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总分离增益计算如下:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="20e8" class="np my it op b gy ou ov l ow ox">10.16513 + 9.53473 - 0 = 19.69985</span></pre><h2 id="cc41" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">回到前面的问题</h2><p id="374f" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">还记得我说过不要担心为什么模型突然决定在树索引2中拆分组1和组2吗？好吧，现在让我们担心一下！</p><p id="3f8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们计算一下，与我们现在拥有的相比，这种拆分会带来什么样的收益！假设我们要分裂，新的梯度和黑森是:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="7fa4" class="np my it op b gy ou ov l ow ox">gradient_l &lt;- (297 * 0.513) - 217<br/>hessian_l &lt;-  (297 * 0.513) * exp(0.7)<br/>gradient_r &lt;- (219 * 0.513) - 134<br/>hessian_r &lt;-  (219 * 0.513) * exp(0.7)</span></pre><p id="0655" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，未分割的增益等于上面右侧分割的增益9.53473。这是我们最初的收获。如果我们进一步分割会发生什么？</p><p id="11fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从左边的叶子，我们会得到:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="db54" class="np my it op b gy ou ov l ow ox">(gradient_l + 15) ^ 2 / hessian_l = 8.030935</span></pre><p id="15e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从右边的叶子看:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="2b08" class="np my it op b gy ou ov l ow ox">(gradient_r + 15) ^ 2 / hessian_r = 0.1956444</span></pre><p id="8855" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(注意<code class="fe om on oo op b">lambda_l1</code>的标志，这完全取决于梯度的标志！)</p><p id="0f3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这三项的整体分割值为:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="3fc4" class="np my it op b gy ou ov l ow ox">8.030935 + 0.1956444 - 9.53473 = - 1.308151</span></pre><p id="a203" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个负数，所以我们不做分割。</p><blockquote class="oy oz pa"><p id="90ff" class="ky kz pb la b lb lc ju ld le lf jx lg pc li lj lk pd lm ln lo pe lq lr ls lt im bi translated">再一次，这只是一个例子，不要担心它是否令人困惑，你会觉得你不能复制，你会在下面的代码片段中找到确切的公式。</p></blockquote><h2 id="d61e" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">收益与现实的分割</h2><p id="23f9" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我看到很多人忽略了一个非常重要的注意事项:这里的收益并不代表基于树前后预测的客观值之间的差异。收益仅仅是衡量分裂点的好坏，而不是模型决定如何处理它们。</p><p id="734f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，作为预测基础的<code class="fe om on oo op b">leaf_values</code>,有一个分割增益没有考虑的关键因素。你能猜到吗？</p><p id="dde0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是的，这是学习率。</p><blockquote class="mi"><p id="1e30" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">学习率参数对leaf_values有影响，但对split_gains没有影响。</p></blockquote><p id="92f1" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">这意味着，根据来自树的实际预测，将一个意义附加到分割收益上是毫无意义的。如果我们有一个<code class="fe om on oo op b">learning_rate</code> = 1，这些就是被罚目标函数的估计收益。</p><h1 id="89cd" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">一个有趣的经验笔记</h1><p id="6d29" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">根据我们目前所知，如果说模型不关心调整梯度和的绝对值低于<code class="fe om on oo op b">lambda_l1</code>的组的预测，这不公平吗？在<code class="fe om on oo op b">lambda_l1</code> = 15的情况下，这些是实际值(来自单个泊松随机变量)和分组预测的总和:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="9780" class="np my it op b gy ou ov l ow ox">   var1   actual  predicted<br/>1:    0      162        177<br/>2:    1      217        202<br/>3:    2      134        134</span></pre><p id="781b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二组(上面第一张图的中间线)得到了准确的预测，其他两组相差15%。</p><blockquote class="mi"><p id="cac8" class="mj mk it bd ml mm oh oi oj ok ol lt dk translated">对我来说，这绝对是一个激动人心的时刻。</p></blockquote><p id="9450" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我认为它从一个不同的角度看待正则化的影响，而不是简单地接受目标函数会对单个叶子分数进行惩罚。</p><h1 id="004b" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">摘要</h1><h2 id="8d7d" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">解决方案</h2><p id="ea4c" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">让我们总结一下到目前为止我们所学的内容！</p><p id="ec5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在代码格式中，这是计算新叶分数、叶收益和L1正则化拆分收益所需的全部内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pf pg l"/></div></figure><h2 id="62ae" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">解释</h2><p id="9424" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">如上所述，我认为思考L1正则化实际上在做什么是非常有用的，而不是简单地根据改变的目标函数和对单个叶子分数的惩罚来观察它。</p><p id="40a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当你使用梯度增强树的L1正则化时，你基本上是在设置一个阈值，在这个阈值之下，模型将不会试图进一步优化。具体来说，如果观测值的梯度之和的绝对值低于该正则化参数，则模型不会进一步分割观测值。</p><p id="e46d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在泊松回归的情况下，梯度是实际值和预测值之间的差值。在尝试L1参数时，这是一个很好的起点，可以为自己建立一个经验法则。</p><p id="a25a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一般而言，该模型将更接近接近总体平均值的观察值。使用高L1参数将使预测更加接近。</p><h2 id="370f" class="np my it bd mz nq nr dn nd ns nt dp nh lh nu nv nj ll nw nx nl lp ny nz nn oa bi translated">结束语</h2><p id="cd0d" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">这是我对梯度增强树的L1正则化的分析。一如既往，如果我错过了什么，一定要让我知道！</p><p id="122c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个令人满意的项目，我希望你发现结果有用！</p><div class="pj pk gp gr pl pm"><a href="https://matepocs.medium.com/membership" rel="noopener follow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">加入我的推荐链接-伴侣概念</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">阅读Mate Pocs(以及媒体上成千上万的其他作者)的每一个故事。你的会员费直接支持Mate…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">matepocs.medium.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ks pm"/></div></div></a></div><h1 id="c273" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">来源</h1><p id="f3c8" class="pw-post-body-paragraph ky kz it la b lb ob ju ld le oc jx lg lh od lj lk ll oe ln lo lp of lr ls lt im bi translated">我通常认为XGBoost文档是一个非常好的起点，尽管它没有考虑L1规范:</p><div class="pj pk gp gr pl pm"><a href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">升压树简介- xgboost 1.6.2文档</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">XGBoost代表“极端梯度增强”，其中术语“梯度增强”源于论文Greedy…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">xgboost.readthedocs.io</p></div></div><div class="pv l"><div class="qb l px py pz pv qa ks pm"/></div></div></a></div><p id="b01f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除此之外，我还使用了LightGBM包中的源代码。我认为值得看一看幕后发生了什么。这个脚本的<code class="fe om on oo op b">GetLeafGain</code>功能非常关键:</p><p id="34de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae og" href="https://github.com/microsoft/LightGBM/blob/346f88398282c5677dcaa880e147026eadba29e4/src/treelearner/feature_histogram.hpp" rel="noopener ugc nofollow" target="_blank">https://github . com/Microsoft/light GBM/blob/346 f 88398282 c 5677 dcaa 880 e 147026 EAD ba 29 e 4/src/tree learner/feature _ histogram . HPP</a></p></div></div>    
</body>
</html>