<html>
<head>
<title>MLP Mixer in a Nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简而言之，MLP搅拌机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mlp-mixer-in-a-nutshell-eccffb68e4fc#2022-04-28">https://towardsdatascience.com/mlp-mixer-in-a-nutshell-eccffb68e4fc#2022-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f9cc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">视觉变压器的资源节约和性能竞争替代方案</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c7cf5d8647d22fe92f01a7340e829f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*k9lD1aN-o7rGQ9Gh"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">里卡多·戈麦斯·安吉尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="23a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> T </span>他的文章旨在对I .托尔斯泰欣、n .霍尔斯比、a .科列斯尼科夫、l .拜尔等人的论文<em class="mb"> MLP混频器:一个全MLP架构的愿景</em>中介绍的MLP混频器进行简要概述<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>。此外，我想根据我的经验提供一些进一步的背景知识，以便快速理解MLP混频器的主要特性。</p><h1 id="781d" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">概述</h1><ol class=""><li id="6ea4" class="mu mv iq ky b kz mw lc mx lf my lj mz ln na lr nb nc nd ne bi translated"><a class="ae kv" href="#7b75" rel="noopener ugc nofollow">简介</a></li><li id="11a8" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><a class="ae kv" href="#033b" rel="noopener ugc nofollow">MLP混频器的动机及其贡献</a></li><li id="9e86" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><a class="ae kv" href="#81a8" rel="noopener ugc nofollow"> MLP混频器架构及其与之前型号的比较</a></li><li id="2dc5" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><a class="ae kv" href="#0b1a" rel="noopener ugc nofollow">实验</a></li><li id="1e94" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated"><a class="ae kv" href="#b9a7" rel="noopener ugc nofollow">结论</a></li></ol><h1 id="7b75" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">介绍</h1><p id="ee6b" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">由<em class="mb"> Vaswani等人</em>在论文<em class="mb">“注意力是你所需要的一切】</em><a class="ae kv" href="#3004" rel="noopener ugc nofollow">【2】</a>中引入的transformer架构已经彻底改变了机器学习领域。虽然它最初应用于NLP(自然语言处理)任务，如将句子从英语翻译成德语，但它的架构很快被其他学科采用和改编，如计算机视觉，产生了ViT(视觉转换器)<a class="ae kv" href="#d6c9" rel="noopener ugc nofollow">【3】</a>等模型。其中，变换器的主要优势是(1)其通过注意机制捕获全局上下文的能力，(2)其用于训练的高并行化能力，以及(3)与例如旨在复制人类感知图像的行为的卷积神经元网络相比，它是具有较少归纳偏差的更一般化的操作。</p><p id="de43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们深入MLP混合器之前，让我们首先通过回顾变压器的弱点来理解新架构的动机。然后让我们总结一下MLP密炼机论文的贡献，最后换个话题来回顾一下MLP密炼机。</p><h1 id="033b" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">MLP混频器的动机及其贡献</h1><p id="5200" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">不管transformer架构的强度及其变化如何，有一个主要问题:计算和内存需求与输入序列长度成二次方关系。换句话说，在NLP问题中，词汇表中的单词越多，或者计算机视觉中图像的分辨率越大，就需要更多的资源来训练和部署模型。这种约束是注意机制的结果，其中一个集合(或序列)中的每个元素都注意第二个集合中的其他元素，而第二个集合可能与初始集合相同。</p><p id="011f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MLP混合器通过替换注意力机制来解决这个问题。根据<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>，MLP混频器的主要贡献是:</p><ul class=""><li id="59f7" class="mu mv iq ky b kz la lc ld lf nn lj no ln np lr nq nc nd ne bi translated">在ViT中引入一种有竞争力的(但不是更好的)简单架构，它不使用卷积或自我关注，而只使用多层感知器(MLP)。</li><li id="9c9e" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nq nc nd ne bi translated">仅依赖于基本的矩阵乘法、整形和转置以及标量非线性。</li><li id="0eed" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nq nc nd ne bi translated">计算复杂度与输入面片数量的线性比例关系(与ViT不同，ViT是二次比例关系)</li><li id="61d9" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nq nc nd ne bi translated">位置嵌入的去除</li></ul><h1 id="81a8" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">MLP混频器架构及其与以前模型的比较</h1><p id="fc76" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">在本节中，我们将讨论MLP混频器的模型架构，并将其与以前的模型进行比较。我们从高层架构开始，逐渐揭示细节。</p><p id="2610" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如论文的标题所示，MLP混合器是一个视觉模型的架构。这是一个高层次的观点，非常类似于ViT模型，如图1所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/bd6e15e647d5856b173c0b4712953e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QA6Y4h_M4z94b5XcaiiScA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:(左)视觉变换器<a class="ae kv" href="#d6c9" rel="noopener ugc nofollow">【3】</a>和(右)MLP混频器<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>的高层架构对比。</p></figure><p id="fe33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ViT和MLP混合器都是分类网络。他们输入一幅图像，输出一个类别概率。在高层次上，两个模型都将图像块线性投影到嵌入空间中。当ViT用步进卷积执行这个步骤时，MLP混合器使用全连接层，因为它的目标之一是表明既不需要卷积也不需要注意层。嵌入，也称为令牌，在两个模型中都被输入到它们各自的主构建块中进行计算。就ViT而言，它基于transformer编码器层，而MLP混频器引入了一种新的架构，我们将在稍后详述。值得注意的是，MLP混合器不需要额外的位置嵌入，因为它对输入嵌入的顺序很敏感，这与注意力层不同。在几个主要的计算层之后，信号被馈送到分类头，在那里模型预测给定输入图像的类别。</p><p id="ef41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们仔细看看图2中描绘的主要构建模块。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/282c8bb1a3073860a1f92432ff65b07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnhAYG_mVHTy0cX6PlrV1g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:ViT<a class="ae kv" href="#d6c9" rel="noopener ugc nofollow">【3】</a>(左)和MLP混频器层(右)的变压器编码器模块对比。作者插图，灵感来自<a class="ae kv" href="#3004" rel="noopener ugc nofollow">【2】</a>。</p></figure><p id="bf53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">左侧描述了ViT中使用的变压器编码器，右侧说明了由<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>提出的MLP混频器层。两层都重复几次(分别是L次或N次),并且它们遵循各向同性设计，这意味着其输入和输出具有相同的形状。在这个抽象层次上，唯一的区别在于注意力机制。ViT依赖于多头自我注意机制，这需要三个线性投影嵌入；键、值和查询。这一层为查询的每个值分配一个重要性因子给关键字的每个值，反之亦然，从而产生一个注意力图。注意力图捕捉嵌入的全局依赖性，而不像卷积那样只考虑局部邻域(全局上下文通常由几个卷积层捕捉，这通常会减小空间宽度并增加通道的数量)。然后，注意力图乘以嵌入的值，以加强重要的值，而不重要的值被抑制。另一方面，MLP混合器通过封装在两个矩阵转置操作之间的MLP块来取代自我注意机制，以捕捉全局上下文。为了理解这是如何工作的，我们进一步深入图3所示的混合器层的详细架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/a8a7ad0d37c49e1ab113d51ae4f8161d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2ggtlpNR0u7nwIZFdPyLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:MLP混频器<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>的混频器层。</p></figure><p id="02a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种架构是建立在<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>作者的简单观察之上的:现代视觉架构混合了它们的特性<strong class="ky ir"> (1)在给定的空间位置跨越通道</strong>和<strong class="ky ir"> (2)在不同的空间位置</strong>。CNN在一层内实现(1 ),但通常通过连续的卷积层实现(2 ),卷积层通过应用越来越多的滤波器来减小空间宽度并增加频道数量。基于注意力的架构在每一层中都允许(1)和(2)。MLP混合器的目的是清楚地区分(1)和(2 ),作者分别称之为信道混合和令牌混合。</p><p id="147a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们考虑由第一MLP，即MLP1执行的令牌混合。MLP1将作用于输入矩阵的每一行，利用权重共享。在第一标准化层之后，数据以[通道，面片]形式的矩阵表示。通道(或嵌入的隐藏维度)是一个可以变化的超参数。补丁指的是输入图像被分成的补丁的数量。为了混合来自每个标记化面片的数据，在将MLP1应用于每一行之前，对输入进行转置。MLP1的输出再次被转置，以获得其初始形式。通过将每个斑块的信息输入MLP，可以感知全球背景。</p><p id="b9bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二MLP，即MLP2，执行信道混合。MLP2的权重不同于MLP1，但也使用权重共享。MLP2从单个补丁的所有通道接收数据，允许来自每个通道的信息彼此交互。</p><p id="4e31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个MLP块由一个完全连接的层组成，接着是一个GELU激活，接着是另一个完全连接的层。</p><h1 id="0b1a" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">实验</h1><p id="5da3" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">一个重要的问题还没有答案:<em class="mb">它的实际表现如何？</em>回答问题。<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>的作者对不同规模和不同数据集的模型进行了多次实验。要了解更多的细节，我建议你阅读报纸。我只讲主要结果。</p><p id="3ee3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了进行实验，首先在不同的数据集上对几个模型进行预训练，然后在不同的下游任务上进行微调。已经分析了三个参数:</p><ol class=""><li id="af27" class="mu mv iq ky b kz la lc ld lf nn lj no ln np lr nb nc nd ne bi translated">下游任务的准确性</li><li id="d8a9" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">预培训的总计算成本</li><li id="00e5" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">测试时吞吐量</li></ol><p id="eebc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">表1显示了主要结果，并对各列进行了如下解释:</p><p id="b483" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第1列</strong>:测试模型<br/> <strong class="ky ir">第2列</strong>:带有原始标签的ImageNet下游任务的前1名准确率<br/> <strong class="ky ir">第3列</strong>:带有已清理真实标签的ImageNet下游任务的前1名准确率<br/> <strong class="ky ir">第4列</strong>:所有五个下游任务(ImageNet、CIFAR-10、CIFAR-100、Pets、Flowers)的平均性能的前1名准确率。<br/> <strong class="ky ir">第5列</strong>:视觉任务适配基准<br/> <strong class="ky ir">第6列</strong>:TPU-v3<br/><strong class="ky ir">上的图像/秒/核吞吐量第7列</strong>:TPU-v3加速器上的预训练总时间</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/211d5e5e3e8231c8f6387e330c87c717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I09c434St77yC6lbjLoq4w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1:与来自文献<a class="ae kv" href="#4bce" rel="noopener ugc nofollow">【1】</a>的最新模型相比，MLP混合器的传输性能、推理吞吐量和训练成本。</p></figure><p id="6a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就精度而言，混合器模型在所有执行的下游任务中与所有其他测试模型具有竞争力。就测试期间的吞吐量而言，混频器优于ViT和BiT。</p><p id="29bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以观察到，MLP混合器在大型JFT-300M (300M图像，18k类)数据集上的预训练时间明显优于其竞争对手，而对于较小的ImageNet-21k (14M图像，21k类)数据集，其训练实际上比其竞争对手的模型更慢。</p><p id="e256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎特别是对于大型数据集，MLP混合器是其他最先进模型的有竞争力的替代方案，因为它几乎达到了最先进(SOTA)的性能，同时在训练和测试期间更有效。人们必须决定在各自的应用中哪种度量更重要。</p><h1 id="b9a7" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论</h1><p id="d0b9" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">MLP混合器通过引入由MLPs和转置组成的简化架构来解决注意力层的计算资源的二次缩放问题。它实现了接近SOTA的性能，同时降低了对计算资源的需求。这是一种权衡，对于给定的应用程序必须做出决定。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="4bce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]托尔斯泰欣，伊利亚，尼尔·霍尔斯比，亚历山大·科列斯尼科夫，卢卡斯·拜尔，翟晓华，托马斯·安特辛纳，杰西卡·容等.《混音器:一个全的建筑视觉》<em class="mb">ArXiv:2105.01601【Cs】</em>，2021年6月11日。http://arxiv.org/abs/2105.01601<a class="ae kv" href="http://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="3004" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Vaswani、Ashish、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。“你需要的只是关注。”<em class="mb">ArXiv:1706.03762【Cs】</em>，2017年12月5日。<a class="ae kv" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1706.03762</a>。</p><p id="d6c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] Dosovitskiy，Alexey，Lucas Beyer，Alexander，Dirk Weissenborn，Xiaohua Zhai，Thomas Unterthiner，Mostafa Dehghani，等人《一幅图像抵得上16x16个字:大规模图像识别的变形金刚》<em class="mb">ArXiv:2010.11929【Cs】</em>，2021年6月3日。<a class="ae kv" href="http://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2010.11929</a>。</p></div></div>    
</body>
</html>