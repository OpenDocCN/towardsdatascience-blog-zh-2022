<html>
<head>
<title>How Neural Networks Actually Work — Python Implementation Part 2 (Simplified)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络实际上是如何工作的——Python实现第2部分(简化)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45#2022-06-09">https://towardsdatascience.com/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45#2022-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8246" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我们继续揭穿神经网络是一个我们不太明白其工作原理的黑箱的理论。我们的目标是以一种容易理解的方式实现神经网络。万一有些概念不能马上讲清楚，请查阅参考资料部分的前几篇文章。</p><p id="577c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文结束时，您应该能够在给定输入数据的情况下，通过一个具有许多功能和训练示例的网络实现数据的单次转发。</p><p id="05cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们从数据和我们将使用的神经网络架构开始。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="a684" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">数据和神经网络架构</h1><p id="c88b" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">我们将使用包含3个特征和<code class="fe ly lz ma mb b">395</code>训练示例的数据，如下图<code class="fe ly lz ma mb b">Figure 1</code>所示。神经网络是浅层的-它在输入层有3个节点(因为我们有3个特征)，一个4个神经元的隐藏层，和一个输出神经元。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/89ff0c51e1ed8b99b68a969510aa1a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PS8uGV-0fS9QSijof94SPA.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><strong class="bd ms">图1: </strong>数据和神经网络。该数据包含3个特征(x1、x2和x3)和395个以y为因变量的示例。该网络是一个3–4–1 NN，即具有3层的NN——3个输入值，4个隐藏层节点，1个输出值(来源:作者)。</p></figure></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="f341" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">参数初始化</h1><p id="1357" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">为了执行向前传递，我们需要定义参数(权重和偏差)的初始值。我们将从标准正态分布中随机生成所需的权重，并将偏差设置为零。进行参数初始化时，最重要的概念是确保权重和偏差矩阵具有所需的维数。在执行矩阵乘法时，尺寸错误会给我们带来问题。</p><h2 id="f62e" class="mt kw it bd kx mu mv dn lb mw mx dp lf kb my mz lj kf na nb ln kj nc nd lr ne bi translated">概述重量和偏差矩阵的尺寸(参见<a class="ae nf" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">链接</a>)</h2><p id="30f2" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">初始化参数时，有两条规则需要考虑:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d03150a3bfda2e9e2c4f5030782959a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*26kekwGdstqXxglSm0fBFg.png"/></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">等式1:权重和偏差矩阵的维数(来源:<a class="ae nf" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe"> Post </a>)。</p></figure><ul class=""><li id="8f45" class="nh ni it js b jt ju jx jy kb nj kf nk kj nl kn nm nn no np bi translated">层 <code class="fe ly lz ma mb b"><strong class="js iu">l</strong></code> <strong class="js iu"> ( </strong> <code class="fe ly lz ma mb b"><strong class="js iu">wˡ</strong></code> <strong class="js iu">)的<strong class="js iu">权重矩阵将具有</strong> <code class="fe ly lz ma mb b"><strong class="js iu">(nˡ, n^(l-1))</strong></code>的维度，即<code class="fe ly lz ma mb b">wˡ</code>的行数将等于当前层<code class="fe ly lz ma mb b">l</code>中神经元的数量，列数将等于前一层<code class="fe ly lz ma mb b">l-1 </code>中神经元的数量。</strong></li><li id="5e38" class="nh ni it js b jt nq jx nr kb ns kf nt kj nu kn nm nn no np bi translated"><strong class="js iu">偏置矩阵将有</strong> <code class="fe ly lz ma mb b"><strong class="js iu">nˡ</strong></code> <strong class="js iu">行和1列</strong>。这只是矢量引用:<a class="ae nf" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">上一篇</a>。</li><li id="82d2" class="nh ni it js b jt nq jx nr kb ns kf nt kj nu kn nm nn no np bi translated">例如，在我们的例子中，我们在隐藏层需要4乘3的权重矩阵。请记住，我们在输入层不需要权重，因为那里不发生任何计算，我们只是传递输入值。</li></ul><blockquote class="nv nw nx"><p id="0754" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated"><strong class="js iu">注1: </strong>输入数据必须为(n，m)维，其中n为特征数，m为训练样本数。也就是说，每个训练示例将作为输入矩阵x中的一列出现。</p><p id="bd40" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated"><strong class="js iu">符号:</strong>本文中，<code class="fe ly lz ma mb b">dim(A)=(r, c)</code>表示矩阵<code class="fe ly lz ma mb b">A</code>的维数为r乘c，其中r为行数，c为列数。nˡ是层l中神经元的数量。在上面所示的架构中，<code class="fe ly lz ma mb b">n⁰=3</code>、<code class="fe ly lz ma mb b">n¹=4</code>和<code class="fe ly lz ma mb b">n²=1</code>。</p></blockquote><p id="80ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参数初始化之后，接下来要做的事情是执行实际的计算。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="997d" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">实际计算</h1><p id="53e0" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">在任何给定的层l，进行以下两个计算:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi oc"><img src="../Images/a6ae3128242a81352d284cdf5a00b221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvF52MEEVcJW18DFg5ISpw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><strong class="bd ms">等式2: </strong>在第1层完成的计算。在等式2–1中，权重矩阵与输入矩阵相乘并添加偏差，而在第二个等式中应用激活。</p></figure><p id="4c10" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">等式2–1:</strong>权重矩阵与输入相乘，并添加偏差。注意，这里的输入可以是实际的输入值，也可以是前一层<code class="fe ly lz ma mb b">l-1</code>的值。这些方程的计算需要我们理解矩阵乘法和矩阵加法。如果<code class="fe ly lz ma mb b">A</code>中的列数等于<code class="fe ly lz ma mb b">B</code>中的行数，则两个矩阵<code class="fe ly lz ma mb b">A</code>和<code class="fe ly lz ma mb b">B</code>可以相乘为<code class="fe ly lz ma mb b">A·B</code>。记住<code class="fe ly lz ma mb b">A·B</code>和<code class="fe ly lz ma mb b"> B·A</code>不一样(矩阵乘法不可交换)。此外，相加的两个矩阵必须具有相同的维数。</p><p id="3d15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">等式2–2</strong>:激活函数<code class="fe ly lz ma mb b">gˡ</code>应用于<code class="fe ly lz ma mb b">Equation 2–1</code>的结果，成为<code class="fe ly lz ma mb b">l</code>层的输出。请注意，在生成的矩阵/向量上使用激活不会影响其维度。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><h1 id="1edf" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">现在让我们将所有这些放入Python代码中</h1><p id="3c2d" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">对于每一层，我们将初始化参数，然后执行所需的计算。在输入层，即层<code class="fe ly lz ma mb b">0</code>没有计算发生，因此我们直接进入隐藏层，即层<code class="fe ly lz ma mb b">1</code>。</p><h2 id="21bc" class="mt kw it bd kx mu mv dn lb mw mx dp lf kb my mz lj kf na nb ln kj nc nd lr ne bi translated">第1层的计算</h2><p id="e76e" class="pw-post-body-paragraph jq jr it js b jt lt jv jw jx lu jz ka kb lv kd ke kf lw kh ki kj lx kl km kn im bi translated">(参考<code class="fe ly lz ma mb b">Equation 1</code>中的规则)对于隐藏层，layer <code class="fe ly lz ma mb b">1</code>，我们需要一个维度为<code class="fe ly lz ma mb b">(n¹,n⁰)</code>的权重矩阵(即当前层的神经元数乘以上一层的神经元数。这是4乘3。我们需要(<code class="fe ly lz ma mb b">n¹, 1)</code>作为偏差，也就是4乘1。为了便于打印出数据，我们现在不使用本文开头定义的数据(我们将在本文后面这样做)。我们将使用下面的子集。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi od"><img src="../Images/2cd56e2d0e1ccb6efb554bb6f3fc4c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*82rQ-Ai680SRfEE7vvZWTw.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated"><strong class="bd ms">图2: </strong>将输入数据转换成输入矩阵。输入矩阵的每一行都包含特征数据。如果每一行都包含训练示例，我们需要转置<strong class="bd ms"> x </strong>，然后乘以权重矩阵(来源:作者)。</p></figure><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="0b50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="md me mf mg gt og mb oh oi aw oj bi"><span id="3a29" class="mt kw it mb b gy ok ol l om on">X:  [[ 9 14 10 11  8]<br/> [ 9 16  8 12  9]<br/> [ 9 16  7 10  9]]<br/>X shape: (3, 5)<br/>w1:  [[-0.14441 -0.05045  0.016  ]<br/> [ 0.08762  0.03156 -0.20222]<br/> [-0.03062  0.0828   0.02301]<br/> [ 0.0762  -0.02223 -0.02008]]<br/>w1 shape:  (4, 3)<br/>b1:  [[0.]<br/> [0.]<br/> [0.]<br/> [0.]]</span></pre><p id="c780" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第1–5行:</strong>导入所需的包，并定义输入数据。输入数据<code class="fe ly lz ma mb b">X</code>具有维度<code class="fe ly lz ma mb b">(3, 5)</code>，3个特征和5个训练样本。</p><blockquote class="nv nw nx"><p id="ac1f" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated"><strong class="js iu">回忆(特征数量影响输入层神经元数量):</strong>输入层神经元数量始终等于特征数量，因此，我们在输入层有3个神经元。</p></blockquote><p id="b638" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第7–12行:</strong>根据<code class="fe ly lz ma mb b">Equation 1</code>中的两条规则初始化参数。<code class="fe ly lz ma mb b">dim(w¹) = (n¹, n⁰) = (4,3)</code>。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="565b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出:</p><pre class="md me mf mg gt og mb oh oi aw oj bi"><span id="f4a7" class="mt kw it mb b gy ok ol l om on">f11 shape:  (4, 5)<br/>w2:  [[0.01866 0.04101 0.01983 0.0119 ]]<br/>w2 weights:  [[0.01866 0.04101 0.01983 0.0119 ]]<br/>b2: [[0.]]<br/>y0:  [[0.03629 0.03134 0.04005 0.03704 0.03582]]<br/>y0 shape:  (1, 5)<br/>y_hat:  [[0.50907 0.50783 0.51001 0.50926 0.50895]]<br/>y_hat shape:  (1, 5)</span></pre><p id="aa24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第1–2行:</strong> w1乘以输入矩阵<code class="fe ly lz ma mb b">x</code>。<code class="fe ly lz ma mb b">dim(w1)=(4, 3)</code>和<code class="fe ly lz ma mb b">dim(x)=(3, 5)</code>，因此<code class="fe ly lz ma mb b">dim(w1·x)=(4,5)</code>基于矩阵乘法的规则。</p><blockquote class="nv nw nx"><p id="b8bd" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated"><strong class="js iu">注2: </strong>最初，我们说(而且是矩阵加法的一个规则)两个矩阵A和B只有维数相同才能相加。但是<code class="fe ly lz ma mb b">dim(w¹·x)=(4, 5)</code>但是<code class="fe ly lz ma mb b">dim(b¹)=(n¹, 1) =(4, 1)</code>然而我们在第1行添加了它们。这是为什么呢？Numpy包裹来救我们了。</p><p id="c84f" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated">“广播一词描述了NumPy在算术运算过程中如何处理不同形状的数组。在某些约束条件下，较小的阵列在较大的阵列中“广播”,以便它们具有兼容的形状。来源:<a class="ae nf" href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank"> Numpy文件</a>。</p></blockquote><p id="44dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在阵列广播之后，<code class="fe ly lz ma mb b">dim(b¹)</code>变成了<code class="fe ly lz ma mb b">(4,5)</code>，因此，<code class="fe ly lz ma mb b">z11=w¹·x+b¹</code>也变成了4乘5的矩阵。</p><p id="9fc4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第4行到第10行:</strong>应用Sigmoid激活来获得第1层f11的输出。这是该层中所有神经元的输出。</p><p id="14b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第12–24行:</strong>显示了输出层的计算。如预期的<code class="fe ly lz ma mb b">dim(w²)=(n², n¹)=(1, 4)</code>和<code class="fe ly lz ma mb b">dim(b²)=(n², 1)=(1, 1)</code>。这里我们还需要应用数组广播的概念。</p><blockquote class="nv nw nx"><p id="09e4" class="jq jr ny js b jt ju jv jw jx jy jz ka nz kc kd ke oa kg kh ki ob kk kl km kn im bi translated"><strong class="js iu">注3: </strong>输出为1乘5矩阵。你期望你期望的是一个单一的数字吗？请记住，此输出是我们数据中所有5个训练示例的预测向量。换句话说，使用Numpy允许我们以如此高效的方式通过网络传递所有的训练示例。不要担心输出向量上的值，因为这只是数据基于随机初始化的权重通过网络的一次传递。</p></blockquote><p id="bb40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要在模型学习的训练期间优化参数值。训练网络需要前向传递/传播(如本文所讨论的)和反向传播。后者允许我们使用一些损失函数来评估模型，计算函数相对于参数的偏导数，并相应地调整参数值。这个过程是迭代的，通过网络传递整个数据集(正向传递)和反向传递(<a class="ae nf" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">反向传播</a>)的过程构成一个完整的迭代。这就是所谓的新纪元。我们将在接下来的文章中讨论所有这些。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><p id="a4b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们结束之前，让我们写一些代码，对开始提到的数据集进行一次完整的前向传递(我们将直接从<a class="ae nf" href="https://kipronokoech.github.io/assets/datasets/marks.csv" rel="noopener ugc nofollow" target="_blank">https://kipronokoech.github.io/assets/datasets/marks.csv</a>加载数据)。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="9f70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="md me mf mg gt og mb oh oi aw oj bi"><span id="3f36" class="mt kw it mb b gy ok ol l om on">w1 shape:  (4, 3)<br/>w2 shape:  (1, 4)<br/>X shape:  (3, 395)<br/>w1 shape:  (4, 3)<br/>b1 shape (4, 1)<br/>w2 shape:  (1, 4)<br/>b2 shape (1, 1)<br/>f1 shape (4, 395)<br/>z2.shape (1, 395)<br/>yhat shape (1, 395)</span></pre><p id="96d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于上面的代码，有几点需要注意:</p><ul class=""><li id="c789" class="nh ni it js b jt ju jx jy kb nj kf nk kj nl kn nm nn no np bi translated">我们需要传递维度为<code class="fe ly lz ma mb b">(# features, # training examples)</code>的输入特征数据。通常，在熊猫数据帧(<code class="fe ly lz ma mb b">df</code>)中，我们在行中有训练示例，在列中有特征，但我们需要输入矩阵中的数据，因此我们转置了第107行的<code class="fe ly lz ma mb b">X</code>。</li><li id="90c3" class="nh ni it js b jt nq jx nr kb ns kf nt kj nu kn nm nn no np bi translated">正向传递的输出<code class="fe ly lz ma mb b">yhat</code>是所有训练示例的预测向量。出于这个原因，我们对数据上的395个训练例子进行了<code class="fe ly lz ma mb b">yhat(1, 395)</code>。</li><li id="f1dd" class="nh ni it js b jt nq jx nr kb ns kf nt kj nu kn nm nn no np bi translated">此时，我们没有使用目标变量(<code class="fe ly lz ma mb b">y</code>)。我们将在反向传播和模型评估中使用它。</li></ul><p id="118a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><ul class=""><li id="5ade" class="nh ni it js b jt ju jx jy kb nj kf nk kj nl kn nm nn no np bi translated"><a class="ae nf" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">如何用Python从零开始实现神经网络——单个神经元上的前向传递</a>。</li><li id="c24f" class="nh ni it js b jt nq jx nr kb ns kf nt kj nu kn nm nn no np bi translated"><a class="ae nf" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">反向传播如何在神经网络中工作</a></li></ul></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><p id="57c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们已经在<a class="ae nf" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe">前一节</a>中讨论了单个神经元的计算。本文讨论了一个完整的浅3-4-1nn的单前锋传球。接下来，我们的重点将是<a class="ae nf" rel="noopener" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48">理解反向传播</a>。</p><p id="83af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请<a class="ae nf" href="https://medium.com/@kiprono_65591/membership" rel="noopener">以每月5美元的价格注册medium会员资格</a>以便能够阅读我和其他作者在medium上的所有文章。</p><p id="3bc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你也可以<a class="ae nf" href="https://medium.com/subscribe/@kiprono_65591" rel="noopener">订阅，当我发表文章时，你可以把我的文章发到你的邮箱里</a>。</p><p id="ccd3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读，欢迎下次光临！！！</p><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/how-does-back-propagation-work-in-neural-networks-with-worked-example-bc59dfb97f48"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">反向传播在神经网络中是如何工作的？</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">用一个例子演示背景如何在神经网络中工作。</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf mm or"/></div></div></a></div></div></div>    
</body>
</html>