<html>
<head>
<title>AI scaling with mixture of expert models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">混合专家模型的人工智能缩放</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-scaling-with-mixture-of-expert-models-1aef477c4516#2022-04-20">https://towardsdatascience.com/ai-scaling-with-mixture-of-expert-models-1aef477c4516#2022-04-20</a></blockquote><div><div class="fc ig ih ii ij ik"/><div class="il im in io ip"><h2 id="3703" class="iq ir is bd b dl it iu iv iw ix iy dk iz translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tds-podcast" rel="noopener" target="_blank">播客</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph jy jb is bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">Liam Fedus和Barret Zoph关于计算高效、稀疏和缩放模型</h2></div><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="kv kw l"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae lb" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae lb" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae lb" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae lb" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></p></figure><p id="a1ba" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated"><em class="ly">编者按:TDS播客由杰雷米·哈里斯主持，他是人工智能安全初创公司墨丘利的联合创始人。每周，Jeremie都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。</em></p></div><div class="ab cl lz ma hw mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="il im in io ip"><p id="7060" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">人工智能的发展真的很快。自从GPT-3问世以来，很明显，我们需要做的事情之一是超越狭隘的人工智能，走向更普遍的智能系统，这将是大规模扩大我们模型的规模，它们消耗的处理能力和它们接受训练的数据量，同时进行。</p><p id="0631" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">这导致了一波大规模模型的浪潮，训练这些模型非常昂贵，很大程度上是因为它们庞大的计算预算。但是，如果有一种更灵活的方式来扩展人工智能——一种允许我们从计算预算中分离模型大小的方式，以便我们可以跟踪更有效的计算过程来扩展，会怎么样？</p><p id="4dea" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">这就是所谓的混合专家模型(MoEs)的前景。与更传统的变形金刚不同，moe不会在每次训练时更新所有的参数。相反，他们将输入智能地路由到称为专家的子模型，每个子模型可以专门从事不同的任务。在给定的训练过程中，只有那些专家的参数被更新。结果是一个稀疏的模型，一个计算效率更高的训练过程，以及一个新的潜在的扩展途径。</p><p id="43ed" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">谷歌一直在推进MoEs研究的前沿，特别是我今天的两位客人，他们参与了这一战略的开拓性工作(还有许多其他工作！).Liam Fedus和Barret Zoph是谷歌大脑的研究科学家，在这一集的TDS播客中，他们和我一起谈论了人工智能缩放、稀疏性以及MoE模型的现在和未来。</p><p id="c794" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">以下是我在对话中最喜欢的一些观点:</p><ul class=""><li id="e159" class="mg mh is le b lf lg li lj ll mi lp mj lt mk lx ml mm mn mo bi translated">专家模型混合通常包含一个路由模型或开关，它决定特定输入将被发送给哪个专家。Liam和Barret仍然不确定对于一个给定的问题有多少专家是最佳的，他们已经测试了有2到1000个专家的系统。</li><li id="b736" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">人们常说，人工智能的历史涉及研究人员逐渐了解到，他们应该将越来越少的假设硬编码到他们的模型中(这些假设被称为“归纳先验”)。例如，在从2010年之前的时代到深度学习时代的飞跃中，我们了解到决策树和支持向量机等固执己见的结构不如神经网络那样可扩展，神经网络似乎对数据“应该”如何处理做出更少的假设。同样，卷积网络和递归网络对图像中的平移不变性和文本中的标记排序做出了假设，现在这些假设看起来越来越没有必要，因为变压器在各自的专业领域优于这两种模型类型。因此，一个问题出现了:MoEs是放松关于模型架构的假设的另一个步骤吗？如果是，它们到底放松了什么假设？</li><li id="0275" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">在某种意义上，MoEs可以被认为是密集变压器的更一般的版本。毕竟，密集变压器实际上只是一个专家的MoE。出于这个原因，Barret和Liam倾向于认为MoEs是从机器学习中去除归纳先验的进一步发展。</li><li id="fd70" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">历史上，MoEs比密集模型遭受更多的训练不稳定性。其中一个原因与路由机制有关，该机制决定了哪个专家将看到给定的输入。因为该决定是离散的(必须选择专家的特定子集，并且您不能半选一个专家)，所以样本通过MoE的路径可能会有很大的不同，根据路由器的输出，将它暴露给完全不同的专家。因此，反向传播变得更具挑战性。从工程的角度来看，由于与密集模型相比，moe的参数数量较多，因此更难训练和部署。Barret指出，一个常见的策略是按照专家的方式划分模型，在每个可用的GPU上训练一个专家。因此，当决定在您的模型中包含多少专家时，硬件的可用性成为一个关键的考虑因素。</li><li id="b44b" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">moe仍然在一般化上挣扎，当在新任务上进行微调时，表现往往比密集模型差一些。Liam和Barret认为这是因为过度拟合，由于MoE模型的高参数计数，这成为一个特别重要的问题。但他们对可能克服这个问题的潜在正规化策略保持乐观，并认为MoEs可以在通用人工智能的未来发挥重要作用。</li></ul><p id="9e7e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">你可以在这里<a class="ae lb" href="https://twitter.com/liamfedus" rel="noopener ugc nofollow" target="_blank">在Twitter上关注利亚姆</a>，在这里<a class="ae lb" href="https://twitter.com/barret_zoph?lang=en" rel="noopener ugc nofollow" target="_blank">在Twitter上关注巴雷特</a>，或者在这里<a class="ae lb" href="https://twitter.com/jeremiecharris" rel="noopener ugc nofollow" target="_blank">关注我</a>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/6ad0747c734c8c8d0562ad0a4058e806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLG770yaU7gYQFICpFS60w.png"/></div></div></figure><h2 id="31de" class="nb nc is bd nd ne nf dn ng nh ni dp nj ll nk nl nm lp nn no np lt nq nr ns iy bi translated">章节:</h2><ul class=""><li id="0b8f" class="mg mh is le b lf nt li nu ll nv lp nw lt nx lx ml mm mn mo bi translated">0:00介绍</li><li id="cb5c" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">2:15嘉宾背景</li><li id="a932" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">8:00了解专业化</li><li id="dce1" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">13:45对未来的推测</li><li id="4d0a" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">21:45开关变压器与密集网络</li><li id="a9b8" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">27:30更多可解释的模型</li><li id="c90a" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">33:30假设和生物学</li><li id="e52a" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">39:15总结</li></ul></div></div>    
</body>
</html>