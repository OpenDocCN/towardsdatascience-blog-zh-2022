<html>
<head>
<title>Visualizing Your Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化您的嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-your-embeddings-4c79332581a9#2022-07-15">https://towardsdatascience.com/visualizing-your-embeddings-4c79332581a9#2022-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d8cd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从SNE到SNE霸王龙和UMAP的进化指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6ceae11e95c650796bc72c01036a185b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-8FJWzqLfAePcNxgYPKLw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="acf2" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h2><p id="34d8" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">根据<a class="ae mk" href="https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data" rel="noopener ugc nofollow" target="_blank">多项估计</a>，当今企业生成的80%的数据是非结构化数据，如文本、图像或音频。这些数据在机器学习应用方面有着巨大的潜力，但在直接使用之前还有一些工作要做。<em class="ml">特征提取</em>帮助从原始数据中提取信息嵌入。嵌入，我和我的合著者和Arize的同事Aparna Dhinakaran在<a class="ae mk" rel="noopener" target="_blank" href="/getting-started-with-embeddings-is-easier-than-you-think-e88b7b10bed1">的上一篇文章</a>中提到过，是许多深度学习模型的支柱；它们被用于GPT 3，DALL E 2，语言模型，语音识别，推荐系统和其他领域。然而，一个长期存在的问题是，现在很难对嵌入和非结构化数据进行故障诊断。理解这种类型的数据具有挑战性，更不用说识别新的模式或变化了。为了有所帮助，有几种使用降维技术可视化数据集嵌入表示的突出方法。在<a class="ae mk" href="https://arize.com/blog-course/sne-t-sne-umap/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中，我们将介绍三种流行的降维技术及其演变。</p><p id="2149" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">首先值得注意的是，介绍这些技术的学术论文——SNE(<a class="ae mk" href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf" rel="noopener ugc nofollow" target="_blank">“随机邻居嵌入</a>，作者杰弗里·辛顿和萨姆·罗韦斯)，t-SNE(<a class="ae mk" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl" rel="noopener ugc nofollow" target="_blank">“使用t-SNE </a>可视化数据”，作者劳伦斯·范德马腾和杰弗里·辛顿)，以及UMAP(<a class="ae mk" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank">乌玛普:统一流形近似和投影降维</a>)，作者利兰·麦金尼斯、约翰·希利和詹姆斯·梅尔维尔——都值得完整阅读。也就是说，这些都是密密麻麻的文件。这篇文章旨在成为一个易于理解的指南——也许是阅读完整论文之前的第一步——帮助时间紧迫的ML从业者理解从SNE到SNE t到UMAP的潜在逻辑和演变。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/9e1e1ad09fffa5c7f71f63552de8af97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eI3r9sNgDgoes5tQxjc7NQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="6c4b" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是降维？</h2><p id="a4e7" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">降维作为机器学习可视化和预处理的一项基本技术，在数据科学中扮演着重要的角色。降维方法将高维数据<em class="ml"> X={x0，x1，…，xN} </em>映射到低维数据<em class="ml"> Y = {y0，y1，…，yN} </em>，其中<em class="ml"> N </em>是数据点的数量。</p><p id="b015" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">本质上，计算嵌入是一种降维形式。当处理非结构化数据时，输入空间可以包含大小为WHC(宽度、高度、通道)、<a class="ae mk" href="https://arize.com/blog-course/tokenization/" rel="noopener ugc nofollow" target="_blank">标记化语言</a>、音频信号等的图像。例如，让输入空间由分辨率为1024×1024的图像组成。在这种情况下，输入维数大于一百万。让我们假设您使用ReNet或EfficientNet之类的模型，并提取1000维的嵌入。在这种情况下，你的输出空间维数是1000，低了三个数量级！这种转换将非常大的、通常很稀疏的输入向量变成更小的(仍然具有相当大的大小)、<strong class="lt ir">密集的</strong>特征向量，或者<em class="ml">嵌入</em>。因此，我们将这个子空间称为<em class="ml">特征空间</em>或<em class="ml">嵌入空间</em>。</p><p id="513f" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">一旦我们有了与输入相关联的特征向量，我们该怎么处理它们呢？我们如何从它们身上获取人类可解读的信息？我们无法想象比三维更高的物体。因此，我们需要工具来进一步将嵌入空间的维度减少到两个或三个，理想情况下尽可能多地保留数据集的相关结构信息。</p><p id="b5f0" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">有大量的降维技术。它们可以分为三类:特征选择、矩阵分解和邻居图。我们将专注于后一类，其中包括SNE(随机邻居嵌入)，t-SNE(t-分布式随机邻居嵌入)和UMAP(一致流形逼近和投影)。</p><h2 id="e8eb" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">进化:SNE → t-SNE → UMAP</h2><p id="4505" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在这一节中，我们将从SNE开始讨论邻居图方法的发展。然后，我们将在此基础上，解释导致SNE霸王龙和后来的UMAP霸王龙的修改。</p><p id="fd2e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这三种算法的操作方式大致相同:</p><ul class=""><li id="bd6a" class="ms mt iq lt b lu mm lx mn le mu li mv lm mw mj mx my mz na bi translated">计算高维概率<em class="ml"> p </em>。</li><li id="ceab" class="ms mt iq lt b lu nb lx nc le nd li ne lm nf mj mx my mz na bi translated">计算低维概率<em class="ml">问</em></li><li id="2291" class="ms mt iq lt b lu nb lx nc le nd li ne lm nf mj mx my mz na bi translated">通过给定的成本函数<em class="ml"> C(p，q) </em>计算概率之间的差异。</li><li id="5511" class="ms mt iq lt b lu nb lx nc le nd li ne lm nf mj mx my mz na bi translated">最小化成本函数。</li></ul><h1 id="ed3c" class="ng kw iq bd kx nh ni nj la nk nl nm ld jw nn jx lh jz no ka ll kc np kd lp nq bi translated">窦房结电图</h1><p id="c4a4" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">本节涵盖了<a class="ae mk" href="https://www.cs.toronto.edu/~fritz/absps/sne.pdf" rel="noopener ugc nofollow" target="_blank">随机邻居嵌入(SNE)算法</a>。这将是我们进一步了解SNE霸王龙和UMAP的基础。</p><h2 id="0ac0" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">第一步:计算高维概率</h2><p id="a7a4" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我们从计算概率开始</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/98cfdeb02615b5318a563ddcda4b7465.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*xQhYb8aRjZSL_KsWfcrt9Q.png"/></div></figure><p id="f51c" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">数据点<em class="ml"> i </em>将选择另一个点<em class="ml"> j </em>作为其邻居，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6efb5effdc72cf042d80bf1de8ac394a.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*cgk7iaqTZQLyLqTJEkd8jA.png"/></div></figure><p id="8e62" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/83b1b9b0a645b8ce7bb908dd8bd534e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/format:webp/1*F4JX4Ls5It3w7yFpjIqshA.png"/></div></figure><p id="9f28" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">代表高维点<em class="ml"> Xi </em>，<em class="ml"> Xj </em>之间的相异。它被定义为缩放的欧几里德距离，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9ff545eba2abe148c7a4f7f929d9bb3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*JNPizKYnzMmIQBrhwTfqPg.png"/></div></figure><p id="1f5e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir"> <em class="ml">数学直觉:</em> </strong> <em class="ml">给定两点</em> X <em class="ml"> i，</em> X <em class="ml"> j，它们越远，它们的距离dj|i越高，它们的相异度越高，它们将彼此视为邻居的概率越低。</em></p><p id="cb74" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir"> <em class="ml">关键概念:</em> </strong> <em class="ml">两个嵌入在空间中越远，它们越不相似。</em></p><p id="ebdc" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">请注意，由于参数<em class="ml"> σi </em>的原因，这种差异是不对称的。这是什么意思？在实践中，用户设置本地邻居的有效数量或<em class="ml">困惑度</em>。一旦选择了<em class="ml">困惑</em>、<em class="ml">k</em>；该算法通过二分搜索法找到<em class="ml"> σi </em>,以使邻居上分布的熵相等</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1d766e0b0bb3b62dab8b26ae1ef2a0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/1*hSKRqmGXZ9QxZpc3nKstuA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8dd443d3a881676687d557aba764fd2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*T3oeibUWoH7eeEtiEVUM1Q.png"/></div></figure><p id="b57c" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">其中<em class="ml"> H </em>是香农熵，以比特为单位。从前面的等式我们可以得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/710fb6a0463a58982e6104556cbf552e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*8AxRp18mtZfK0kvhzKERlQ.png"/></div></figure><p id="d6e8" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">因此，通过调整<em class="ml"> σi，</em>我们可以调整右侧，直到它与用户设置的困惑度相匹配。局部邻居的有效数量(困惑度)越高，<em class="ml"> σi </em>越高，并且在相异度中使用的高斯函数越宽。</p><p id="0b7e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir"> <em class="ml">数学直觉:</em> </strong> <em class="ml">困惑度越高，越有可能将相距较远的点视为邻居。</em></p><p id="4659" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">可能会出现一个问题:如果困惑是用户决定的，我们如何知道哪个是正确的？这是艺术与科学相遇的地方。对于我们可以使用散点图可视化的低维空间的有用投影来说，困惑的选择是至关重要的。如果我们选择的困惑度太低，那么我们期望由于相似性而在一起的数据点的聚类将不会出现在一起，而我们将会看到子聚类。另一方面，如果我们选择一个对于我们的数据集来说太大的困惑，我们将看不到正确的聚类，因为来自其他聚类的点将被认为是邻居。困惑没有一个确定的全好值。然而，有一些好的经验法则。</p><p id="ea06" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir"> <em class="ml">建议:</em></strong><em class="ml"/><a class="ae mk" href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ml">SNE</em></a><em class="ml">和</em><a class="ae mk" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl" rel="noopener ugc nofollow" target="_blank"><em class="ml">t-SNE</em></a><em class="ml">(是的，t-SNE也有不惑力)使用5到50之间的不惑力值。</em></p><p id="1e68" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">因为在许多情况下，没有办法知道正确的困惑是什么，从SNE(和t-SNE)那里得到最多可能意味着分析具有不同困惑的多个情节。</p><h2 id="4297" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">第二步:计算低维概率</h2><p id="caa9" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">现在我们有了高维概率，我们继续计算低维概率，这取决于数据点在低维空间中的映射位置。幸运的是，这些更容易计算，因为SNE也使用高斯邻域，但是具有固定的方差(没有困惑参数)，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/852fbddab7c376ccb71407dc81cacb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*2ZzZZV1wTWr4FAKeJ7C9zQ.png"/></div></figure><h2 id="1ffb" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">第三步:选择成本函数</h2><p id="ae34" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">如果点<em class="ml"> Yi </em>在低维空间放置正确，条件概率<em class="ml"> p </em>和<em class="ml"> q </em>会非常相似。为了测量这两种概率之间的不匹配，SNE使用了Kullback-Leibler散度作为每个点的损失函数。高维和低维空间中的每个点都有条件概率将另一个点称为其邻居。因此，我们有多少数据点就有多少损失函数。我们将成本函数定义为所有数据点上KL散度的总和，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/89b4b11ced8a21f40ed36135af01468b.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*AuyzeoJLHILwMpKinYrwzA.png"/></div></figure><p id="78e9" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">该算法首先将所有y <em class="ml"> i </em>放置在非常靠近原点的随机位置，然后使用梯度下降来训练最小化成本函数<em class="ml"> C </em>。关于成本函数微分的细节超出了本文的范围。</p><p id="138e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">关于所使用的损失函数，有必要发展一些直觉。该算法的创造者<a class="ae mk" href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf" rel="noopener ugc nofollow" target="_blank">表示</a>“虽然SNE强调局部距离，但它的成本函数干净地强制保持附近物体的图像在附近，并保持相距较远的物体的图像相对较远。”让我们看看这是否是真的，使用下一张图，我们可以分别看到高维和低维概率<em class="ml"> p </em>和<em class="ml"> q </em>的KL散度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/f0d390435ac3c48ac92da4eb33d5904f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HD5O-qWAlzQcbqz777A6zQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">库尔贝克-莱布勒分歧:作者图片</p></figure><p id="e54a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">如果在高维空间中两个点靠得很近，它们的相异度很低，概率<em class="ml"> p </em>应该很高(<em class="ml"> p ~ 1 </em>)。那么，如果它们被映射到很远的地方，低维概率就会很低(<em class="ml">Q0</em>)。在这个场景中，我们可以看到损失函数取了很高的值，严重地惩罚了这个错误。另一方面，如果两个点在高维空间中彼此远离，则它们的相异度高，并且概率p应该低(<em class="ml"> p0 </em>)。然后，如果它们被映射到彼此附近，低维概率将会很高(<em class="ml"> q1 </em>)。我们可以看到KL背离并没有像我们希望的那样惩罚这个错误。这是UMAP将解决的一个关键问题。</p><h1 id="14b4" class="ng kw iq bd kx nh ni nj la nk nl nm ld jw nn jx lh jz no ka ll kc np kd lp nq bi translated">t-SNE</h1><p id="ffa1" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">至此，我们对SNE的工作原理有了一个很好的了解。然而，该算法有两个t-SNE试图解决的问题:它的成本函数很难优化，并且它有一个拥挤问题(下面将详细介绍)。为了解决这些问题，<a class="ae mk" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank"> t-SNE </a>引入了两个主要的修改:对称化和使用低维概率的t分布。这些修改可以说使t-SNE成为多年来可视化降维的最先进技术，直到UMAP出现。</p><h2 id="e172" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">对称SNE</h2><p id="d54b" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">第一个修改是使用SNE的对称版本。一般来说，到目前为止描述的条件概率是不对称的。这意味着点<em class="ml"> xi </em>将点<em class="ml"> xj </em>视为其邻居的概率与点<em class="ml"> xj </em>将点xi视为邻居的概率不同。我们通过定义来对称化高维空间中的成对概率</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/957a60326c9c5adc44650747c1e36157.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*Ks8XmwnNf-TmgqO19uVW9A.png"/></div></figure><p id="7f6b" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这个定义</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7eebe447acc634ada186c792bfe47186.png" data-original-src="https://miro.medium.com/v2/resize:fit:92/format:webp/1*ri72pMh8r-hAVZHSZCWTgg.png"/></div></figure><p id="5d85" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">每一个数据点</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/81c6643e9716979c88db37e62724083e.png" data-original-src="https://miro.medium.com/v2/resize:fit:72/format:webp/1*QDRK-LGOGvjpHSkB2hZVUw.png"/></div></figure><p id="4f6a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">对成本函数有不可忽略的贡献。你可能会想，低维概率呢？</p><h2 id="0322" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">拥挤问题</h2><p id="6c01" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">就我们目前所知，如果我们想要正确地投射点与点之间的近距离，中等距离会被扭曲，并在低维空间中表现为巨大的距离。<a class="ae mk" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank">根据t-SNE的作者的说法，这是因为“可用于容纳距离适中的数据点的二维地图区域，与可用于容纳附近数据点的区域相比，不够大。”</a></p><p id="6eb2" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">为了解决这个问题，引入的第二个主要修改是使用学生t-分布(这是将‘t’赋予t-SNE的原因),低维概率具有一个自由度，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f47dd9676f426c7fffe25a9dfe6110f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*v83PNZ62O2xDbwT71zfeSQ.png"/></div></figure><p id="cf09" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">现在成对概率是对称的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/74ce114aefc6b4559d63bfc38951b7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*URLge2bm6RoBBLJcnJU_0w.png"/></div></figure><p id="48d9" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">主要优点是对称成本函数的梯度，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9f1246281c621c06381b1adcef4adfd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Kjx3itEyBPK8TUo0_cTIFw.png"/></div></figure><p id="8ef3" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">具有更简单的形式，计算起来更容易、更快，从而提高了性能。</p><p id="059a" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">由于t-SNE也使用KL散度作为它的损失函数，它也带来了前一节讨论的问题。这并不是说它完全被忽视了，但主要的收获是SNE霸王龙严格地优先考虑当地结构的保护。</p><p id="1d69" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated"><strong class="lt ir"> <em class="ml">直觉:</em> </strong> <em class="ml">由于KL散度函数没有对高维空间中距离较远的点在低维空间中的错位进行惩罚，所以我们可以得出结论，全局结构没有得到很好的保存。SNE霸王龙会把相似的数据点归为一类，但是类之间的距离可能没有任何意义。</em></p><h1 id="15c0" class="ng kw iq bd kx nh ni nj la nk nl nm ld jw nn jx lh jz no ka ll kc np kd lp nq bi translated">UMAP</h1><p id="4b1d" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated"><a class="ae mk" href="https://arxiv.org/pdf/1802.03426.pdf" rel="noopener ugc nofollow" target="_blank">用于降维的统一流形近似和投影</a> (UMAP)与t-SNE有许多相似之处，也有一些非常关键的差异，这使得UMAP成为我们降维的首选。</p><p id="fbe5" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">这篇介绍这项技术的文章不适合胆小的人。它详细介绍了基于流形理论和拓扑数据分析的UMAP的理论基础。我们不会进入理论的细节，但请记住这一点:UMAP的算法决策是由强大的数学理论证明的，这最终将使它成为机器学习的高质量通用降维技术。</p><p id="7a3d" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">现在我们已经了解了SNE霸王龙是如何工作的，我们可以在此基础上谈论它与UMAP的区别以及这些区别的后果。</p><h2 id="b685" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">相似之处的图表</h2><p id="0c72" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">与研究概率的t-SNE不同，UMAP直接研究相似性。</p><ul class=""><li id="06cd" class="ms mt iq lt b lu mm lx mn le mu li mv lm mw mj mx my mz na bi translated">高维相似性:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/646d9d6815a7cabd783b20c48ff37c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*fZPMhdnaI6K-uoK8IZKxGw.png"/></div></div></figure><ul class=""><li id="75e0" class="ms mt iq lt b lu mm lx mn le mu li mv lm mw mj mx my mz na bi translated">低维相似性:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/85db23fd691fe39d987acc9508681738.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*UIlRLCYjcA4At6Hy5FZvfg.png"/></div></figure><p id="6ae5" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">请注意，在两种情况下<strong class="lt ir">都没有应用归一化</strong>(无分母)，不像t-SNE，性能有相应的提高。高维相似性的对称化是使用概率t-conorm实现的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/cf3b9544829f062175b7c02b0971031f.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*hSpjWOfworpjLZonob3Wpg.png"/></div></figure><p id="9f12" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">此外，UMAP允许为高维相似性(eq)选择不同的度量函数d。我们需要解决的是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6be76c5019be22a14543625ff4c06f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*iRrcix4c7EnCszMbBqTQbw.png"/></div></figure><p id="311e" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">前者表示低维空间中接近点之间的最小期望间隔，而后者被设置为这样的值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/c1b48858bb27802f70d7c79eb1f6e615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDuaHhmtLYNmhxqMzV3dfw.png"/></div></div></figure><p id="eb27" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">其中<em class="ml"> k </em>是最近邻居的数量。两者</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6be76c5019be22a14543625ff4c06f4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*iRrcix4c7EnCszMbBqTQbw.png"/></div></figure><p id="8702" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">是超参数，其影响将在本节末尾讨论。</p><h2 id="2c1c" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">优化改进</h2><p id="2ffb" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在算法的优化阶段，UMAP的创造者做出了一些对其出色性能起着至关重要作用的设计决策，包括:</p><ol class=""><li id="d413" class="ms mt iq lt b lu mm lx mn le mu li mv lm mw mj ol my mz na bi translated"><strong class="lt ir"> UMAP使用交叉熵作为损失函数</strong>，而t-SNE使用KL散度。CE损失函数既有引力又有斥力，而KL只有引力(值得注意的是t-SNE有斥力但不在损失函数上。排斥力出现在相似矩阵的重整化过程中。然而，全局重正化是非常昂贵的，UMAP通过使用CE损失使它更简单，并且在保持全局结构上具有更好的结果)。通过损失函数的这种新选择，将高维空间中远离的对象放置在低维空间中附近是不利的。得益于损失函数的更好选择，UMAP比它的前辈们能捕捉到更多的全局结构。</li></ol><div class="kg kh ki kj gt ab cb"><figure class="om kk on oo op oq or paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/582697eabdbe9d968999e66455eef275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*HD5O-qWAlzQcbqz777A6zQ.png"/></div></figure><figure class="om kk on oo op oq or paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/8abaf7dd62a72263a84b80cf77fcc3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*H9_E2Ckh_vrb8NFkvtA-8g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk os di ot ou translated">(a)t-SNE使用的KL损失|(b)m使用的CE损失(两幅图片均由作者提供)</p></figure></div><p id="4d45" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">图:SNE和UMAP使用的损失函数比较。当高维空间中的远点在低维空间中被紧密地映射在一起时，交叉熵损失(b)是不利的。kull back-lei bler损失(a)没有做到这一点。</p><p id="6910" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">2.<strong class="lt ir"> UMAP使用随机梯度下降</strong>来最小化成本函数，而不是较慢的梯度下降。</p><h2 id="457e" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">初始化的选择</h2><p id="dd8e" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated"><strong class="lt ir"> UMAP使用光谱初始化，而不是低维点的随机初始化</strong>。一个非常方便的拉普拉斯特征映射初始化(遵循理论基础)。这种初始化相对较快，因为它是从线性代数运算中获得的。它为随机梯度下降提供了一个良好的起点。理论上，这种初始化是确定性的。然而，考虑到所涉及的大型稀疏矩阵，计算技术提供了近似的结果。因此，不保证确定性，但实现了极大的稳定性。这种初始化提供了更快的收敛以及更大的一致性，即UMAP的不同运行将产生相似的结果。</p><p id="6097" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">还有其他一些差异值得探究。此外，UMAP还有几个缺点。请继续关注关于UMAP利弊的下一篇文章。</p><h2 id="7aa5" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h2><p id="b58c" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在过去的几十年里，可视化嵌入的技术取得了长足的进步，数学、计算机科学和机器学习的成功结合使新算法成为可能。从SNE到SNE和UMAP的演变为数据科学家和机器学习工程师更好地理解他们的数据和解决模型问题开辟了新的可能性。</p><p id="0681" class="pw-post-body-paragraph lr ls iq lt b lu mm jr lw lx mn ju lz le mo mb mc li mp me mf lm mq mh mi mj ij bi translated">想了解更多？了解为什么开始使用<a class="ae mk" href="https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/" rel="noopener ugc nofollow" target="_blank">嵌入</a>比您想象的要容易。有什么问题吗？欢迎来到<a class="ae mk" href="https://arize.com/community-new/" rel="noopener ugc nofollow" target="_blank">阿里斯社区</a>。</p></div></div>    
</body>
</html>