<html>
<head>
<title>Understanding and Implementing Faster R-CNN: A Step-By-Step Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解和实现更快的R-CNN:一步一步的指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-implementing-faster-r-cnn-a-step-by-step-guide-11acfff216b0#2022-11-02">https://towardsdatascience.com/understanding-and-implementing-faster-r-cnn-a-step-by-step-guide-11acfff216b0#2022-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ddf3" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">计算机视觉和目标检测</h2><div class=""/><div class=""><h2 id="34b0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">揭开对象检测的神秘面纱</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/660ec20281e5105a70672adb1e434cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IGVYNhg7-FbgRvXXnoza4Q@2x.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="74f9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我第一次接触对象检测是通过Tensorflow对象检测API。它使用起来很简单。我传入一个海滩的图像，作为回报，API在它识别的对象上画出方框。这似乎很神奇。我变得好奇，想要剖析API，以了解它在幕后真正是如何工作的。很难，我失败了。Tensorflow对象检测API支持最先进的模型，这些模型是数十年研究的成果。它们被错综复杂地编织成代码，就像钟表匠如何将连贯运动的微小齿轮组装在一起。</p><p id="2040" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然而，大多数当前最先进的模型都建立在fast-RCNN模型奠定的基础之上，即使在今天，它仍然是计算机视觉中被引用最多的论文之一。因此理解它是至关重要的。</p><p id="e975" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本文中，我们将分解fast-RCNN文件，理解其工作原理，并在PyTorch中一部分一部分地构建它，以理解其中的细微差别。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="65aa" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">更快的R-CNN概述</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/1671648b5818aba9dd4b085db596af92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LUuCTElqx1HRGr0WqPNXEA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">更快的R-CNN整体架构</p></figure><p id="8fd7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于物体检测，我们需要建立一个模型，并教它学会识别图像中的物体。更快的R-CNN模型采取以下方法:图像首先通过主干网络得到输出特征图，图像的地面真实边界框得到<em class="na">投影</em>到特征图上。骨干网络通常是像ResNet或VGG16这样的密集卷积网络。输出特征图是表示图像的学习特征的空间密集张量。接下来，我们将这个特征图上的每个点视为一个<em class="na">锚点</em>。对于每个锚点，我们生成多个不同大小和形状的盒子。这些锚定框的目的是捕捉图像中的对象。</p><p id="f1b2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们使用一个<code class="fe nb nc nd ne b">1x1</code>卷积网络来预测所有锚盒的<em class="na">类别</em>和<em class="na">偏移</em>。在训练期间，我们对与投影的地面真实框重叠最多的锚框进行采样。这些被称为<em class="na">正向</em>或<em class="na">激活</em>锚盒。我们还对<em class="na">负</em>锚盒进行采样，这些锚盒与地面真值盒几乎没有重叠。正锚框被分配类别<code class="fe nb nc nd ne b">object</code>，而负锚框被分配<code class="fe nb nc nd ne b">background</code>。网络学习使用二进制交叉熵损失来分类锚盒。现在，正锚盒可能不会与投影的地面真相盒完全对齐。因此，我们训练一个类似的<code class="fe nb nc nd ne b">1x1</code>卷积网络来学习预测来自地面真相盒的偏移。这些偏移应用于锚盒时，使它们更接近地面真实盒。我们使用L2回归损失来学习补偿。使用预测的偏移来变换锚盒，并将其称为<em class="na">区域提议，</em>并且上述网络被称为<em class="na">区域提议网络。这是探测器的第一阶段。fast-RCNN是一个两级检测器。还有一个阶段。</em></p><p id="47e6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第二阶段的输入是第一阶段产生的区域提案。在第二阶段，我们学习使用简单的卷积网络来预测区域提议中的对象的类别。现在，原始区域建议具有不同的大小，因此我们使用一种称为ROI pooling的技术在通过网络之前调整它们的大小。这个网络学习使用交叉熵损失来预测多个类别。我们使用另一个网络来预测来自地面实况箱的区域提议的偏移。该网络还试图将地区提案与实地真相框联系起来。这使用L2回归损失。最后，我们对两种损失进行加权组合，以计算最终损失。在第二阶段，我们学习预测类别和偏移量。这就是所谓的多任务学习。</p><p id="1840" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所有这些都发生在训练中。在推断过程中，我们通过主干网络传递图像并生成锚盒——与之前相同。但是，这一次我们只选择在第1阶段获得高分类分数的前300个盒子，并使它们有资格进入第2阶段。在第二阶段，我们预测最终的类别和偏移量。此外，我们使用一种叫做<em class="na">非最大值抑制</em>的技术执行一个额外的后处理步骤来移除重复的边界框。如果一切正常，检测器会识别图像中的对象并在其上绘制方框，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/7db5b6d37edd874f9f2c4ccee55d9287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YwvAsKN-KWZ9o-qTUcWE6w.png"/></div></div></figure><p id="a2ff" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是两级快速RCNN网络的简要概述。在接下来的几节中，我们将深入探讨每一部分。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d8ef" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">设置环境</h1><p id="3797" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">所有用到的代码都可以在<a class="ae nl" href="https://github.com/wingedrasengan927/pytorch-tutorials/tree/master/Object%20Detection" rel="noopener ugc nofollow" target="_blank">这个GitHub资源库</a>中找到。我们不需要太多的依赖，因为我们将从头开始构建。仅仅安装在标准anaconda环境中的PyTorch库就足够了。</p><p id="d309" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是我们将要使用的主要笔记本。只是浏览一下。我们将在接下来的几节中逐步介绍它。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="93e5" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">准备和加载数据</h1><p id="c6b8" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">首先，我们需要一些样本图像来处理。在这里，我从<a class="ae nl" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="noopener ugc nofollow" target="_blank">这里</a>下载了两张高分辨率图像。</p><p id="bb11" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来，我们需要标记这些图像。CVAT是最受欢迎的开源标签工具之一。你可以从这里免费下载<a class="ae nl" href="https://github.com/opencv/cvat" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="84a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">您可以简单地将图像加载到工具中，在相关对象周围画出方框，并标记它们的类别，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/c665906c957a6a24216885b9e7e97af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qSZztljCKEV7_UFCVtSmeQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">CVAT的标签</p></figure><p id="75ed" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">完成后，您可以将注释导出为更好的格式。在这里，我将它们导出为<code class="fe nb nc nd ne b">CVAT for images 1.1</code> xml格式。</p><p id="9a73" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">注释文件包含有关图像、带标签的类和边界框坐标的所有信息。</p><h2 id="feb6" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">PyTorch数据集和数据加载器</h2><p id="c08e" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">在PyTorch中，创建一个从PyTorch的<code class="fe nb nc nd ne b">Dataset</code>类继承的类来加载数据被认为是最佳实践。这将使我们对数据有更多的控制，并有助于保持代码的模块化。此外，我们可以从数据集实例创建一个PyTorch <code class="fe nb nc nd ne b">DataLoader</code>,它可以自动处理数据的批处理、混排和采样。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="8246" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在上面的类中，我们定义了一个名为<code class="fe nb nc nd ne b">get_data</code>的函数，它加载注释文件并解析它以提取图像路径、标签类和边界框坐标，然后将它们转换为PyTorch的<code class="fe nb nc nd ne b">Tensor</code>对象。图像被调整到固定的大小。</p><p id="f4a9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">注意我们填充了边界框。这与<code class="fe nb nc nd ne b">resize</code>相结合，使我们能够将图像批量处理在一起。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/b9d69adc24e9e8d06d35ec5369f60675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEAkHnRKPDCM6_kVcUJ3Ug.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">填充的工作原理</p></figure><p id="e85a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果一批图像中有两个以上的图像，并且每个图像中的对象数量都不相同，那么我们会考虑任何图像中对象的最大数量，并使用-1填充其余的图像，以匹配最大长度，如上图所示。我们填充边界框坐标以及类别。</p><p id="c2be" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以从数据加载器中抓取一些图像，并将其可视化，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/7db5b6d37edd874f9f2c4ccee55d9287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YwvAsKN-KWZ9o-qTUcWE6w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我们将使用的示例图像</p></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="334a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">穿过主干网络</h1><p id="ed7b" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">这里我们将使用ResNet 50作为主干网络。记住，ResNet 50中的单个块是由瓶颈层的堆栈组成的。沿着空间维度，在每个块之后，图像缩小一半，而通道的数量增加一倍。瓶颈层由三个卷积层和一个跳跃连接组成，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8b1bb11a767814843cace6cd2132d77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*gG1xtkEALS_idBm20mW4Tw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ResNet 50中的瓶颈层。图片来自论文<a class="ae nl" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></p></figure><p id="eeb7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将使用ResNet 50的前四个块作为主干网络。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/17092e36dfa3e955d047e39cf6841b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3-eiu6zg4oQC5WfzjmlusQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用ResNet 50的前4个块作为卷积骨干网</p></figure><p id="caf8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一旦图像通过主干网络，它就会在空间维度上被向下采样。输出是图像的丰富特征表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/dd619913fd66a87ef5490afe8e65e620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0YgTRl-ESculq_77HxhKQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在将两个图像通过主干后，可视化它们的输出特征图。</p></figure><p id="823d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果我们通过主干网络传递大小为<code class="fe nb nc nd ne b">(640, 480)</code>的图像，我们会得到大小为<code class="fe nb nc nd ne b">(15, 20)</code>的输出特征图。所以图像被<code class="fe nb nc nd ne b">(32, 32)</code>缩小了。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="7f60" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">生成锚点</h1><p id="dd37" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">我们将特征图中的每个点视为一个锚点。所以定位点只是一个表示宽度和高度坐标的数组。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">生成锚点</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/501699c27f4dac263f42510875633a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yvimws9Axzv-mJqdYgiTnQ.png"/></div></div></figure><p id="e282" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了可视化这些锚点，我们可以简单地通过乘以宽度和高度比例因子将它们投影到图像空间。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/d851d29873e900bb3474aeafa66d6621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4kGOs_AJBBg7SMwaB74tQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将锚点投影到图像空间</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/5602068177b74555d58d036a13449413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4VENX3lUb5e0yyBqxtzHg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">可视化原始图像上的定位点</p></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="ce04" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">生成锚盒</h1><p id="a146" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">对于每个锚点，我们生成九个不同形状和大小的边界框。我们选择这些盒子的大小和形状，使它们包含图像中的所有对象。锚定框的选择通常取决于数据集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/8a68fad090358d88d20faf498a4c2b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pudAQyqwK0MHctJXr2vM_g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从3个比例和3个纵横比的组合中为每个锚点生成9个锚点框。</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从比例和纵横比的组合中生成锚定框</p></figure><p id="9bba" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">调整图像大小的另一个好处是锚定框可以在所有图像中重复。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/3229e9a2e1336c0fe769cf71b97208b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nJV1GFYT77FiPPSCqvZ5tQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">重复所有图像的锚定框，因为它们的大小相同</p></figure><p id="77e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">同样，为了可视化锚定框，我们通过乘以宽度和高度比例因子将它们投影到图像空间。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/6a880fa1b9b2326a8e00175dc39eca48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZE8tOuAIPRe_NvV3p3llGQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">针对两幅图像的单个定位点，在图像空间中可视化定位框</p></figure><p id="3824" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果我们将所有锚点的所有锚点框可视化，看起来会是这样:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/33bf96b2b6b77e2d34c9743c39a9edb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1t4GpLHNm72aezIdBzu3NQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">为两幅图像的图像空间中的所有定位点可视化定位框</p></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="4a6c" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">数据准备</h1><p id="18d5" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">在本节中，我们将讨论培训的数据准备。</p><h2 id="7873" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">正负锚盒</h2><p id="44f5" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">我们只需要抽样几个锚箱进行训练。我们对正反锚盒都进行了采样。正锚定框包含对象，负锚定框不包含对象。为了对正锚盒进行采样，我们选择与任何基础真值盒具有大于0.7的IoU的锚盒，或者对于每个基础真值盒具有最高IoU的锚盒。当锚盒生成不良时，条件1失败，所以条件2来拯救，因为它为每个地面真值盒选择一个正盒。为了对负锚盒进行采样，我们选择IoU小于0.3的锚盒与任何基础真值盒。通常，阴性样本的数量会远远高于阳性样本。所以我们随机抽取一些样本来匹配阳性样本的数量。IoU是度量两个边界框之间重叠的度量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/ba8413abc7ffa561801953a210712989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NxT4-qtKtBKLRJBFEoNdXw.png"/></div></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="de09" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">上面的函数计算IoU矩阵，该矩阵包含图像中每个锚框和所有地面真实框的IoU。它将形状为<code class="fe nb nc nd ne b">(B, w_amap, h_amap, n_anc_boxes, 4)</code>的锚盒和形状为<code class="fe nb nc nd ne b">(B, max_objects, 4)</code>的地面真相盒作为输入，并返回形状为<code class="fe nb nc nd ne b">(B, anc_boxes_tot, max_objects)</code>的矩阵，其中符号如下:</p><pre class="kp kq kr ks gt ol ne om on aw oo bi"><span id="0522" class="np mi iq ne b gy op oq l or os">B - Batch Size<br/>w_amap - width of the output activation map<br/>h_wmap - height of the output activation map<br/>n_anc_boxes - number of anchor boxes per an anchor point<br/>max_objects - max number of objects in a batch of images<br/>anc_boxes_tot - total number of anchor boxes in the image i.e, w_amap * h_amap * n_anc_boxes</span></pre><p id="43aa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该函数实质上展平了所有锚定框，并使用每个基础真值框计算IoU，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a8ff0c3968ceb843d290ba60f7d8c763.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*1FYdpAsDTZEcyVhr2XPmig.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">IoU矩阵</p></figure><h2 id="ab4a" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">投影地面真相盒</h2><p id="458d" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">重要的是要记住，IoU是在生成的锚框和投影的地面真值框之间的特征空间中计算的。要将地面真值框投影到特征空间上，我们只需将其坐标除以比例因子，如以下函数所示:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="02b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，当我们将坐标除以比例因子时，我们将数值四舍五入到最接近的整数。这实质上意味着我们将基础事实框“捕捉”到特征空间中最近的网格。因此，如果图像空间和特征空间的尺度差异很大，我们将无法获得准确的投影。因此，在目标检测中使用高分辨率图像是非常重要的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/18333be49319aebc4594865f4d60c320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADhQ6BOa-pes8GkUDZ567A.png"/></div></div></figure><h2 id="d46a" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">计算偏移量</h2><p id="2bcd" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">正面锚定框与地面真实框不完全对齐。因此，我们计算正面锚盒和地面真相盒之间的偏移，并训练神经网络来学习这些偏移。失调可以通过下式计算:</p><pre class="kp kq kr ks gt ol ne om on aw oo bi"><span id="7454" class="np mi iq ne b gy op oq l or os">tx_ = (gt_cx - anc_cx) / anc_w<br/>ty_ = (gt_cy - anc_cy) / anc_h<br/>tw_ = log(gt_w / anc_w)<br/>th_ = log(gt_h / anc_h)</span><span id="3999" class="np mi iq ne b gy ov oq l or os">Where:</span><span id="cebe" class="np mi iq ne b gy ov oq l or os">gt_cx, gt_cy - centers of ground truth boxes<br/>anc_cx, anc_cy - centers of anchor boxes<br/>gt_w, gt_h - width and height of ground truth boxes<br/>anc_w, anc_h - width and height of anchor boxes</span></pre><p id="75fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">以下函数可用于计算相同的值:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="de8d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果你注意到了，我们正在教导网络去了解锚箱与地面真相箱有多远。我们没有强迫它预测锚盒的确切位置和规模。因此，网络学习的偏移和变换是<em class="na">位置</em>和<em class="na">比例</em>不变的。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="4339" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">代码走查</h2><p id="8c21" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">让我们浏览一下数据准备代码。这可能是整个存储库中最重要的功能。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="fb33" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该函数的主要输入是为一批图像生成的锚盒和投影的地面真相盒。</p><p id="c4de" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">首先，我们使用上述函数计算IoU矩阵。然后，从这个矩阵中，我们为每个基础真值框取最重叠的锚框的IoU。这是对阳性锚盒取样的条件1。我们还应用条件2，并选择IoU大于图像中任何地面真值框的阈值的锚框。我们组合条件1和2，并对所有图像的阳性锚框进行采样。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/951a0f8f91754e97747ecc99790cac7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkVDVzTBq4Nz6Ttdo2LP_w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">说明如何从IoU矩阵中抽取+ve锚</p></figure><p id="bea6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每个图像将具有不同数量的阳性样本。为了避免训练过程中的这种差异，我们将整批图像展平，并将所有图像的阳性样本合并。此外，我们可以使用<code class="fe nb nc nd ne b">torch.where</code>跟踪每个阳性样本的来源。</p><p id="3555" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">接下来，我们需要计算正样本相对于地面实况的偏移量。为此，我们需要将每个正样本映射到其对应的基础真值盒。值得注意的是，一个正锚定框只能映射到一个基础真值框，而多个正锚定框可以映射到同一个基础真值框。</p><p id="1ad6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了进行这种映射，我们首先使用<code class="fe nb nc nd ne b">Tensor.expand</code>扩展基础真相框以匹配总锚框。然后，对于每个锚定框，我们选择与它重叠最多的地面真相框。为此，我们从IoU矩阵中获取所有锚盒的最大IoU指数，然后使用<code class="fe nb nc nd ne b">torch.gather</code>在这些指数处“聚集”。最后，我们将整批样品压平并过滤阳性样品。该过程如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/f36ce47834d6d1f09dfe007a69a250f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KHkiy7aMP2Zj46KJurLWKg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将每个锚框映射到它与most和filter +ve样本重叠的地面真实框</p></figure><p id="ef83" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们对类别执行相同的过程，为每个阳性样本分配一个类别。</p><p id="0b0a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，我们已经为每个正样本映射了一个地面真值框，我们可以使用上述函数来计算偏移。</p><p id="e90e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，我们通过对IoU小于给定阈值的锚盒进行采样来选择负样本，其中<em class="na">是所有</em>的基础真值盒。由于阴性样本在数量上远远超过阳性样本，我们随机选择了一些样本来匹配计数。</p><p id="cb02" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">下面是正面和负面锚定框的样子:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/f78db98657cf0e8aa3d8c21843e2ba74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pybcs1jx4WQ61mdElVY6AQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">+ve和-ve锚盒的可视化。绿色:+ve，红色:-ve，黄色:地面实况</p></figure><p id="c9e2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在可以使用采样的正锚盒和负锚盒进行训练。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="37d7" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">构建模型</h1><h2 id="4179" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">提案模块</h2><p id="1f69" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">我们先从求婚模块开始。正如我们所讨论的，特征图中的每个点都被视为一个锚点，每个锚点都会生成不同大小和形状的方框。我们希望将这些盒子分类为<code class="fe nb nc nd ne b">object</code>或<code class="fe nb nc nd ne b">background</code>。此外，我们希望预测它们相对于相应的地面真相框的偏移。我们如何做到这一点？解决方法是使用<code class="fe nb nc nd ne b">1x1</code>卷积图层。现在，<code class="fe nb nc nd ne b">1x1</code>卷积层不会增加感受野。它们的功能不是学习图像级特征。它们更适合用来改变过滤器的数量，或者作为回归或分类头。</p><p id="b71a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所以我们采用两个<code class="fe nb nc nd ne b">1x1</code>卷积层，并使用其中一个将每个锚盒分类为<code class="fe nb nc nd ne b">object</code>或<code class="fe nb nc nd ne b">background</code>。让我们称之为信心头。因此，给定一个大小为<code class="fe nb nc nd ne b">(B, C, w_amap, h_amap)</code>的特征映射，我们卷积一个大小为<code class="fe nb nc nd ne b">1x1</code>的内核来得到一个大小为<code class="fe nb nc nd ne b">(B, n_anc_boxes, w_amap, h_amap)</code>的输出。本质上，每个输出过滤器表示锚盒的分类分数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/ce768d369025ceb7249c16e3e7d68b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k30CUulVJSkQSzC2SB1OsQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">提议模块预测所有锚盒的偏移量和分数</p></figure><p id="20fe" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">以类似的方式，另一个<code class="fe nb nc nd ne b">1x1</code>卷积层采用特征图并产生大小为<code class="fe nb nc nd ne b">(B, n_anc_boxes * 4, w_amap, h_amap)</code>的输出，其中输出滤波器表示锚盒的预测偏移。这就是所谓的回归头。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="1e4c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在训练期间，我们选择正锚框并应用预测偏移来生成区域提议。区域建议可以计算如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2e69f783af9df811b681c0d733477193.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*np6Bv_CXwnhkljt8WDf3Zw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从锚盒和偏移计算区域建议</p></figure><p id="db71" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中上标<code class="fe nb nc nd ne b">p</code>表示区域提议，上标<code class="fe nb nc nd ne b">a</code>表示锚定框，<code class="fe nb nc nd ne b">t</code>表示预测偏移。</p><p id="a3a2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">以下函数实现上述变换并生成区域建议:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="17f1" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">区域提案网络</h2><p id="b8fe" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">区域建议网络是检测器的第一级，它获取特征图并产生区域建议。在这里，我们将主干网络、采样模块和建议模块组合成区域建议网络。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">区域提案网络</p></figure><p id="1c44" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在训练和推断期间，RPN为所有锚盒产生分数和偏移。然而，在训练期间，我们仅选择正锚盒和负锚盒来计算分类损失。为了计算L2回归损失，我们只考虑正样本的偏移量。最终损失是这两种损失的加权组合。</p><p id="dc57" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在推断过程中，我们选择得分高于给定阈值的锚框，并使用预测的偏移生成建议。我们使用sigmoid函数将原始模型逻辑转换为概率得分。</p><p id="4e26" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在两种情况下产生的建议被传递到检测器的第二级。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="e621" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">分类模块</h2><p id="b164" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">在第二阶段，我们接收区域建议并预测建议中对象的类别。这可以通过一个简单的卷积网络来实现，但是有一个问题:所有的提议并不具有相同的大小。现在，你可能会想到在输入模型之前调整建议的大小，就像我们通常在图像分类任务中调整图像大小一样，但问题是调整大小是<em class="na">而不是</em>一个可微分的操作，因此<a class="ae nl" rel="noopener" target="_blank" href="/backpropagation-in-rnn-explained-bdf853b4e1c2">反向传播</a>不能通过这个操作发生。</p><p id="9656" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里有一个更聪明的调整大小的方法:我们将提议分成大致相等的子区域，并对每个子区域应用最大池操作，以产生相同大小的输出。这称为投资回报池，如下图所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/3ca6650a89f50c64fd0d080ea327ceb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tv8gM16RbzJQEVNNSx7WVA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">投资回报池</p></figure><p id="b857" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最大池是一个可微分的操作，我们一直在卷积神经网络中使用它们。</p><p id="fd91" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们不需要从头开始实现ROI pooling，<code class="fe nb nc nd ne b">torchvision.ops</code>库为我们提供了它。</p><p id="c237" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一旦使用ROI池调整了建议的大小，我们就让它们通过一个卷积神经网络，该网络由一个卷积层、一个平均池层和一个产生类别分数的线性层组成。</p><p id="dbe4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在推理过程中，我们通过对原始模型逻辑应用softmax函数并选择具有最高概率得分的类别来预测对象类别。在训练过程中，我们使用交叉熵计算分类损失。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="60dd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在一个全面的实现中，我们还在第二阶段包括了<code class="fe nb nc nd ne b">background</code>类别，但是让我们把它留在本教程中。</p><p id="5c07" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在第二阶段，我们还添加了一个回归网络，进一步为区域建议生成偏移。然而，由于这需要额外的簿记，我没有把它包括在本教程中。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="78f4" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">非最大抑制</h2><p id="1c90" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">在推理的最后一步，我们使用一种叫做非最大抑制的技术来移除重复的包围盒。在这种技术中，我们首先考虑具有最高分类分数的包围盒。然后，我们用这个盒子计算所有其他盒子的IoU，并移除具有高IoU分数的盒子。这些是与“原始”边界框重叠的复制边界框。我们对剩余的盒子也重复这个过程，直到所有重复的都被删除。</p><p id="aa75" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">同样，我们不必从头开始实现它。<code class="fe nb nc nd ne b">torchvision.ops</code>图书馆提供给我们。NMS处理步骤在上述阶段1回归网络中实现。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="7a82" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">快速RCNN模型</h1><p id="3092" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">我们将区域建议网络和分类模块放在一起，以构建最终的端到端fast-RCNN模型。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="339e" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">训练模型</h1><p id="4217" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">首先，让我们在一个小的数据样本上过度拟合网络，以确保一切都按预期工作。我们正在使用一个带有Adam optimizer的标准训练循环，学习率为<code class="fe nb nc nd ne b">1e-3</code>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/732367b34bb186b41b142f57a97b4399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a0rlajonAvUzIuEg_GXWVg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">标准训练循环</p></figure><p id="5461" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">结果如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/b08200419cc4f3dfebe204e079d75f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GUAACR8TLjFlZpA8FOOHIg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">推理结果</p></figure><p id="667e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于我们只对一小部分数据进行了训练，该模型还没有学习图像级别的特征，因此结果并不准确。这可以通过在大数据集上训练来改善。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="a364" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">结论</h1><p id="b222" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">在全面实施中，我们在MS-COCO或PASCAL VOC等标准数据集上训练网络，并使用平均精度或ROC曲线下面积等指标评估结果。然而，本教程的目的是理解更快的RCNN模型，所以我们将离开评估部分。</p><p id="e2a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">多年来，该领域取得了显著的进步，并且开发了许多新的网络。例子包括YOLO、EfficientDet、DETR和Mask-RCNN。然而，它们中的大多数都是建立在我们在本教程中讨论过的更快的RCNN模型的基础之上的。</p><p id="baef" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我希望你喜欢这篇文章。该代码可在<a class="ae nl" href="https://github.com/wingedrasengan927/pytorch-tutorials/tree/master/Object%20Detection" rel="noopener ugc nofollow" target="_blank"> GitHub </a>获得。我们来连线。你也可以通过<a class="ae nl" href="https://www.linkedin.com/in/neerajkrishnadev/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae nl" href="https://twitter.com/WingedRasengan" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="cf08" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">数据集确认</h2><p id="5e5f" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">本文中使用的两幅图像来自于<a class="ae nl" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="noopener ugc nofollow" target="_blank"> DIV2K数据集</a>。数据集在<a class="ae nl" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"> CC0:公共领域</a>下获得许可。</p><pre class="kp kq kr ks gt ol ne om on aw oo bi"><span id="dd2e" class="np mi iq ne b gy op oq l or os">@InProceedings{Agustsson_2017_CVPR_Workshops,<br/>	author = {Agustsson, Eirikur and Timofte, Radu},<br/>	title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},<br/>	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},<br/>	month = {July},<br/>	year = {2017}<br/>}</span></pre><h2 id="d1a6" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">图像制作者名单</h2><p id="c2d8" class="pw-post-body-paragraph le lf iq lg b lh ng ka lj lk nh kd lm ln ni lp lq lr nj lt lu lv nk lx ly lz ij bi translated">除非标题中明确注明出处，否则本教程中的所有图片均出自作者之手。</p><h2 id="8701" class="np mi iq bd mj nq nr dn mn ns nt dp mr ln nu nv mt lr nw nx mv lv ny nz mx iw bi translated">参考</h2><ul class=""><li id="f0a1" class="pd pe iq lg b lh ng lk nh ln pf lr pg lv ph lz pi pj pk pl bi translated"><a class="ae nl" href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/" rel="noopener ugc nofollow" target="_blank">用于计算机视觉的深度学习，UMich </a></li><li id="e0f4" class="pd pe iq lg b lh pm lk pn ln po lr pp lv pq lz pi pj pk pl bi translated"><a class="ae nl" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快-RCNN论文</a></li></ul></div></div>    
</body>
</html>