<html>
<head>
<title>How DALL-E Mini Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DALL-E Mini的工作原理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-how-dall-e-mini-works-114048912b3b#2022-06-26">https://towardsdatascience.com/understanding-how-dall-e-mini-works-114048912b3b#2022-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3ed8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用DALLE-E模型从文本生成图像，从文本提示，通过VQGAN、BART和CLIP到最终图像。</h2></div><p id="df12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你一定听说过DALL E mini，最热门的图像生成模型。但是你想知道它是如何工作的吗？在这篇文章中，我们将解密最近最著名的人工智能模型之一。为了更深入地理解所有组件，我还参考了其他资源。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/082c1d16477a2d9dae491e387911b8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vKR66xelakh_NWyl"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lu" href="https://unsplash.com/@russn_fckr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> russn_fckr </a>拍摄的照片</p></figure><p id="cd08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DALL E mini是一款能够在给出简短文本提示的情况下生成图像的模型。它在互联网上引起了很多关注，主要是因为非常搞笑的结果和易于使用的演示。如果你还没有做过，我鼓励你在这里玩DALL E mini:【https://www.craiyon.com/<a class="ae lu" href="https://www.craiyon.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="d8d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DALL E mini是OpenAI的一个更大模型的开源版本，名为DALLE [1]。该模型由几个已知的构件组成，以一种非常巧妙的方式与一些需要解决的有趣的工程问题联系在一起。如果你对DALL E mini的起源更感兴趣，可以参考[2]。那些块是<strong class="kk iu"> VQGAN </strong>、<strong class="kk iu">变压器</strong>、<strong class="kk iu"> BART </strong>和<strong class="kk iu">夹子</strong>。在本文中，我们自下而上，首先解释各个组件以及它们是如何连接的。</p><p id="9f15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始之前，先简要说明一下:为了训练DALL E，使用了三个带有图像标题的数据集。你可以在[2]阅读更多关于他们的内容。</p><h1 id="c49d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">VQGAN</h1><p id="bc66" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">第一个重要的构建模块是VQGAN生成图像模型[3]。生成意味着，模型应该生成新的图像。传统上，卷积网络是计算机视觉任务的首选方式。然而，CNN有一个重要的缺陷:它们强烈偏向于局部交互(在窗口内)。另一方面，当建模长期关系很重要时，transformer模型显示了它们在NLP领域中的性能。一个典型的transformer模型构建为在一个序列中支持多达512个令牌，因为它们的计算需求随着输入大小的平方增长。这对于高质量的图像合成来说肯定太小了——像素的数量要大几个数量级。</p><p id="f181" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">VQGAN试图结合CNN和transformer方法的优点进行图像合成。主要的想法是让CNN了解当地的互动，并把长期关系委托给transformer。这样，我们可以利用两种架构的优势。让我们更深入地了解他们是如何做到的。</p><h2 id="fc4b" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">电报密码本</h2><p id="70cb" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">VQGAN的一个重要概念是码本。可以理解为有向量的表格。每个矢量被认为是编码一个“感知丰富的图像成分”。编码器试图将输入图像表示为来自码本的代码序列。稍后，解码器使用该序列(和码本)来恢复原始图像。码本的大小和维数是固定的，并且模型学习实际的码向量。以这种方式选择参数，迫使码本学习有意义的表示。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ne"><img src="../Images/1c30c1b7df1ed5d06ab3c76c0fb9b017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*79-aWyvo4MhAqyBk"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">码本(用Z表示)由N个条目组成，每个条目是n_z维向量。条目的数量及其大小是固定的。在这个例子中，它是作者的Nx10图像</p></figure><h2 id="8dca" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">编码器</h2><p id="e7e6" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">过程如下:原始图像(<strong class="kk iu"> HxWx3 </strong> —高乘宽乘RGB)被转换成来自码本的索引序列。这是由CNN编码器完成的。使用n_z个通道将图像尺寸缩小到<strong class="kk iu">高x宽</strong>。注意，n_z是码本的维数。因此，来自编码器的每个输出“像素”具有与来自码本的向量相同的维度。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nf"><img src="../Images/07e9d58b34ae40d52a8aa6b257e82271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PitQ20hZW7-HjPWr"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">点击放大！作者图片</p></figure><p id="b5cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图中，我用红色标出了右下部分。那些红色值将被转换成来自码本的单个值。你可以看到，在这个例子中，CNN后的原始图像被转换为3x3x10的地图。3x3是任意大小，10来自码本条目大小(它们需要匹配)</p><p id="3de9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等等，但是CNN的值与码本中的向量不匹配。没错。因此，该模型使用L2范数从码本中寻找最接近的向量。</p><p id="a912" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们从码本中获得索引时，我们可以将它们展平为大小为h x w的序列S。这样，我们就创建了一个整数序列的图像表示！由于图像被转换为3x3x10，现在我们在序列s中有9个项目。</p><h2 id="6b24" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">解码器</h2><p id="949f" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">解码器做相反的事情。根据整数序列(码本索引)，它重新创建图像。由于整个模型是按照GAN风格建立的，所以在最后还有一个鉴别器。然而，在本文中，我假设您或多或少知道GAN是什么，因为这里没有空间解释它。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ng"><img src="../Images/ee88bfdc1fca0bef974a5f398f782825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Uv9K77hnuyYplsxw"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="3269" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器CNN的输入与编码器CNN的输出“几乎”相同。差不多，因为同时被编码成了码字。并且在这个过程中，我们丢失了一些信息(因为我们只依赖码本表示)。维度分析很简单，如果我们有9个元素序列，并且我们知道图像是一个正方形，我们将它排列为3x3。而解码器CNN知道要扩展多少。</p><h2 id="5049" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">变压器</h2><p id="f65d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">你可能会想，把图像编码成一个序列有什么意义，因为我们会在它之后马上回到CNN公式？答案是，我们还将转换器模型应用于顺序表示。回想一下，CNN在本地互动中表现出色，但在远程互动中表现出色。相比之下，转换器对长距离依赖性执行grat，但是对每个像素运行太昂贵。</p><p id="43c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顺序表示比原始像素更简洁。由于它是一个整数序列，我们可以很容易地将这个问题转化为语言建模，这正是转换器的优势所在。</p><p id="17bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们将中间的序列表示用于transformer模型，并在语言建模任务中训练它。如果我们将图像的顺序表示表示为S = S0，S1，S2，..Sn(记住S的长度是固定的)我们可以训练转换器预测这个序列中的下一个值。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nh"><img src="../Images/09d5c56a430159e72155b79afb5914b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YhKKu0sDIEfe_vI3"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">假设我们知道S0到S5码字，那么S6应该是什么？作者图片</p></figure><p id="dff5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这种方式，转换器了解到图像的分布片段彼此之间的相关程度。使用这种方法还有一个额外的好处。假设我们希望对图像生成进行调节(调节—提供“启动”信息，如“给我一张狗的图像”)。有了一个转换器，这是非常容易的，我们只需要预先考虑图像序列的条件序列。稍后我们将看到这一点。</p><p id="0960" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有一个话题——码本中的值究竟是如何更新的。但是，对于本文来说，这是一个太高级的主题。如果你对此感兴趣，请看原文[3]。</p><p id="3645" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，我们已经完成了DALL E mini的第一大块。让我们继续巴特</p><h1 id="22e6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">巴特</h1><p id="114e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">与VQGAN相比，这部分相对简单。Bart是一种基于(令人惊讶的)变压器架构的序列到序列自动编码器。目标是重建被噪声破坏的输入文本。让我们看一个例子。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ni"><img src="../Images/a4eb827a982d844a2ec138e62f3175d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9vFruc8gPR8g2aU-"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="0b73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以看到这个模型修复了噪音输入。而只是BART预训练的辅助任务。对于我们的例子，我们更感兴趣的是将输入文本翻译成与码本相同的词汇。它将允许我们将它们两者结合起来！你可以认为BART的作用是把字幕“翻译”到码本上。</p><h1 id="a827" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">达勒和米尼</h1><p id="b937" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们已经准备好了建造DALL E mini的最重要的部分。让我们从培训开始。程序如下:</p><ol class=""><li id="e10a" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">图像由VQGAN编码器编码。</li><li id="225f" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">字幕由BART编码器编码。</li><li id="3726" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">两个编码器的输出被组合并传递给BART解码器。</li><li id="7bb6" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">VQGAN编码器输出和BART编码器输出用于计算交叉熵损失。</li></ol><p id="fb62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主要思想是VQGAN编码器和BART解码器应该为图像和字幕对产生完全相同的序列。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nx"><img src="../Images/d19968b665f60516259316e84dac6205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SIaxHyTj-uaLHcEH"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">DALL-E mini的主要目标是匹配VQGAN和BART的输出。图片由作者提供</p></figure><blockquote class="ny nz oa"><p id="86b3" class="ki kj ob kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated">据我所知，VQGAN在DALL E mini之前是单独训练的。BART编码器也经过预训练，但解码器是从头开始训练的。</p></blockquote><p id="c8e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这看起来出奇的简单！</p><h2 id="79ba" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">模型推理</h2><p id="b277" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们来看一下DALL E mini的推论。这一次，我们只有标题，我们应该产生一个图像。第一步是向BART编码器提供描述。接下来，我们对BART解码器进行多次采样，以生成候选。每个候选图像被传递给VQGAN解码器，并生成候选图像。</p><p id="8e90" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以停在这里—生成了多个图像，我们可以将它们呈现给用户。然而，我们可以做得更好。有一种方法可以自动对候选人进行排名，并选择其中的前k名。这个模型叫做剪辑。让我们看看它是如何工作的。</p><h1 id="c5de" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">夹子</h1><p id="bbb8" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">CLIP是一个模型，能够嵌入文本和图像，并判断它们的匹配程度。为了再次训练剪辑模型，我们需要一组(图像、文本)对。图像和文本由专用编码器编码到同一个向量空间中。对于每一对，计算它们之间的点积。目标是使匹配对的文本和图像表示接近，而非匹配对的文本和图像表示远离。使用对称交叉熵训练该模型，这意味着该模型同时以两种方式学习该关系(文本→图像和图像→文本)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nx"><img src="../Images/034c93c9570732ab0d47cf0d0c8d70da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lNlVR7G0DbcHHtt-"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">CLIP将所有图像与所有标题进行比较，并确保它们匹配。作者图片</p></figure><p id="f0a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在模型被训练之后，我们可以为它提供一个单独的标题(源标题)和一堆候选图片。在输出端，我们将得到每幅图像的分数，显示标题和图像的对齐程度。假设我们只取具有最大相似性的K个图像候选。</p><h1 id="f742" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">回到DALL E mini推论</h1><p id="b2ed" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们已经准备好看到完整的DALL E mini推理管道</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi of"><img src="../Images/9253c29e840196ce9fdaf89e36453454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_d9hnXERoxlXEY6I"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">所有部件都在工作。作者图片</p></figure><p id="2990" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">提供的提示被翻译成VQGAN码本中的码字序列。我们对BART进行多次采样，以获得一个以上的可能序列。这样我们就产生了多个候选人。每个候选者被单独放入VQGAN解码器。结果，我们有一堆生成的图像。生成的图像连同提示文本一起被输入到CLIP中，以便对它们进行排序，从而允许我们选择最终返回的前K个图像。</p><p id="3bb8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">瞧，我们完成了！</p><h1 id="fcb9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">问题和考虑</h1><blockquote class="ny nz oa"><p id="1f65" class="ki kj ob kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated">注意这部分大部分是我的私人意见</p></blockquote><p id="eb6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">问:我们能解释一下密码本吗？ <br/>答:虽然在技术上我们可以将码本索引解码为BART词汇，但在我的实验中，我发现它们不提供任何类型的模型解释</p><p id="5d3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">问:训练这样的模型</strong> <br/>有什么困难？答:其中最重要的一个是argmin操作不是反向传播友好的</p><p id="e2ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">考虑因素1: </strong> <br/> DALL-E Mini没有引入任何新的、卓越的架构，而是很好地利用了现有的部件。</p><p id="6bae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑2:  <br/>我喜欢DALL E mini比OpenAI的DALL-E小得多的事实。作者声称使用spot TPUs只需200美元就可以训练。对我来说，同样重要的是小型车产生的二氧化碳要少得多。</p><h1 id="36f8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><p id="074e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">[1]—DALL-E by open ai—<a class="ae lu" href="https://openai.com/dall-e-2/" rel="noopener ugc nofollow" target="_blank">https://openai.com/dall-e-2/</a></p><p id="33e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] — <a class="ae lu" href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA" rel="noopener ugc nofollow" target="_blank"> DALL-E mini讲解</a></p><p id="aede" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3] — VQGAN纸— <a class="ae lu" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank">用于高分辨率图像合成的驯服变压器</a></p><p id="4659" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] —回形针— <a class="ae lu" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移的视觉模型</a></p><p id="6852" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5]-<a class="ae lu" href="https://github.com/openai/CLIP" rel="noopener ugc nofollow" target="_blank">剪辑Github库</a></p><p id="1cfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[6] — <a class="ae lu" href="https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb#scrollTo=C5zvMxh8cU6m" rel="noopener ugc nofollow" target="_blank">与CLIP Colab笔记本互动</a></p><p id="c160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[7] — <a class="ae lu" href="https://colab.research.google.com/github/borisdayma/dalle-mini/blob/main/tools/inference/inference_pipeline.ipynb" rel="noopener ugc nofollow" target="_blank"> DALL E mini —推理流水线Colab笔记本</a></p><p id="34ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[8] — BART论文— <a class="ae lu" href="https://arxiv.org/abs/1910.13461" rel="noopener ugc nofollow" target="_blank"> BART:用于自然语言生成、翻译和理解的去噪序列间预训练</a></p><p id="9853" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[9] — <a class="ae lu" href="https://colab.research.google.com/github/CompVis/taming-transformers/blob/master/scripts/taming-transformers.ipynb" rel="noopener ugc nofollow" target="_blank">驯服变形金刚Colab笔记本</a></p><h2 id="ef33" class="ms lw it bd lx mt mu dn mb mv mw dp mf kr mx my mh kv mz na mj kz nb nc ml nd bi translated">图像来源:</h2><p id="159a" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae lu" href="https://unsplash.com/photos/IzoQu5vH47o" rel="noopener ugc nofollow" target="_blank">https://un flash . com/photos/izoque 5 vh47 o</a><br/><a class="ae lu" href="https://unsplash.com/photos/h-ACUrBngrw" rel="noopener ugc nofollow" target="_blank">https://un flash . com/photos/h-acurbngw</a><a class="ae lu" href="https://unsplash.com/photos/KRttQCXUjNI" rel="noopener ugc nofollow" target="_blank">【https://un flash . com/photos/krttcxujni</a><br/></p></div></div>    
</body>
</html>