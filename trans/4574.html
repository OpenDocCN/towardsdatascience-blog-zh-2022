<html>
<head>
<title>Introduction to Reinforcement Learning: Temporal Difference, SARSA, Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习介绍:时间差异，SARSA，Q-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-temporal-difference-sarsa-q-learning-e8f22669c366#2022-10-11">https://towardsdatascience.com/introduction-to-reinforcement-learning-temporal-difference-sarsa-q-learning-e8f22669c366#2022-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/8d015c0a6b9255c808816565a6631689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ajg06GqpoH2aaRBe"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">菲利普·格利克曼在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="59c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习是机器学习中最复杂的领域之一，因为它的数学复杂性，以及它试图实现的雄心勃勃的任务。</p><blockquote class="lb lc ld"><p id="ee7e" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">简单地说，RL算法的最终目标是使智能体能够在一个环境中行动，以最大化总回报。</p></blockquote><p id="4f1e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">说起来容易做起来难:这一句话隐藏了许多问题，例如:</p><ol class=""><li id="7ff2" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la ln lo lp lq bi translated">我如何定义“目标”和“奖励”？</li><li id="0062" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">我如何让一个代理“行动”？</li><li id="fdae" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">我如何对“环境”建模？</li></ol><p id="4478" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题比看起来要困难得多，但迄今为止已经取得了很多进展，最著名的例子是科学突破，如DeepMind的<a class="ae kc" href="https://www.youtube.com/watch?v=x5Q79XCxMVc" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>和<a class="ae kc" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor" rel="noopener ugc nofollow" target="_blank"> AlphaTensor </a>。</p><p id="21a8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果你想深入了解一个领域，你必须从基础开始。这篇文章的范围将是给出RL的高层次概述，以及介绍代表许多其他更高级算法的基础的三种算法:时间差异、SARSA和Q-Learning。</p><h1 id="b1cf" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">强化学习:构建模块</h1><p id="2192" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在RL中，我们利用一个简单的模型来描述代理如何与空间交互，这就是<strong class="kf ir">马尔可夫决策过程</strong>。</p><p id="dce3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MDPs代表了一种数学框架，用于对动态部分由代理控制，部分由环境特征引起的情况进行建模。在MDP中，代理人不断地与环境互动，环境以影响后续代理人行动的回报作为回应。我们来看看下面这张图:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a8b42cfd3f467fc3d00636e2d5eaf34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*RUIhEZpLQDh15ZaEXQSzTg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">一个简单的MDP，作者的形象</p></figure><p id="e2cf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个时间步<em class="le"> t </em>，代理接收环境的<em class="le">状态S </em> ₜ的表示，这决定了对<em class="le">动作A </em> ₜ.的选择在下一个时间步<em class="le"> t+1 </em>，代理收到一个<em class="le">奖励</em> Rₜ₊₁，告知它过去的行为有多好，以及一个新的环境表示。</p><p id="1762" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在有限的MDP中，州Sₜ、行动Aₜ和奖励Rₜ具有有限数量的元素，Sₜ和Rₜ具有明确定义的离散概率分布，仅取决于前面的州和行动。这些特点使得问题通常更容易处理。然而，并不是所有的问题都可以用有限的状态、行动或奖励来建模，然而我们将做出的许多假设和结论很容易推广到连续的设置中。</p><p id="f24e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代理和环境之间的持续交互产生了一个轨迹:</p><blockquote class="ne"><p id="f264" class="nf ng iq bd nh ni nj nk nl nm nn la dk translated"><em class="no"> S₀、A₀、R₀、S₁、A₁、R₁、S₂、A₂、巴西……</em></p></blockquote><p id="a2fa" class="pw-post-body-paragraph kd ke iq kf b kg np ki kj kk nq km kn ko nr kq kr ks ns ku kv kw nt ky kz la ij bi translated">从状态<em class="le"> S </em> ₜ转换到<em class="le"> S </em> ₜ₊₁的概率因此受到动作<em class="le"> A </em> ₜ的选择的影响，但是，给定<em class="le"> S </em> ₜ和<em class="le"> A </em> ₜ，它有条件地独立于所有先前的状态和动作。因此，我们说MDP的状态转移满足<strong class="kf ir">马尔可夫性质</strong>。</p><p id="02a4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代理人的最终目标是在长期内最大化总<strong class="kf ir">回报</strong>，这通常考虑到一个<strong class="kf ir">贴现因子γ </strong>，计算如下:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/7874ca4c960791eb7a891c5995d7db5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lC1Xr-A-wnEl_XKcsITwJw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">贴现回报，等式来自[1]</p></figure><p id="a63f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">折现因子γ决定了代理人有多渴望眼前的回报(γ = 0)或有多长远的眼光并以未来的高回报为目标(γ = 1)。</p><p id="c313" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代理人的行为应该是为了转换到产生高回报的状态。代理的行为由策略<em class="le"> π(a|s) </em>决定，策略是一个输出执行动作<em class="le"> a </em>的概率的函数，假设代理处于状态<em class="le"> s </em>。在给定的策略下，我们可以定义一个<strong class="kf ir">状态-值函数</strong>(和一个<strong class="kf ir">状态-动作值函数</strong>)，它测量代理从<em class="le"> s </em>(并执行<em class="le"> a </em>)开始并随后跟随<em class="le"> π </em>所获得的期望回报。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/7785008a704685abe96186d413360b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pU0poFrCec6TcJh0CFXwgg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">状态值函数和状态-动作值函数，方程来自[1]</p></figure><p id="2dd7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL算法的目标通常有两个:</p><ol class=""><li id="c5f5" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la ln lo lp lq bi translated"><strong class="kf ir">预测</strong>:估计给定策略的价值函数<em class="le"> π </em></li></ol><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/7384acb6252717e5d2f90b0d610c6d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgpSJ9WGeYY5seu1qpdNow.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">策略的状态值函数和状态-动作值函数的估计值<em class="no"> π </em></p></figure><p id="e7f7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.<strong class="kf ir">控制</strong>:寻找最优策略<em class="le"> π* </em>，即长期收益最大化的策略。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/75bdfca56a7ed751d410aefea981abed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Hf56vdpOSyu2ck1gv4Xew.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最优策略的状态值函数和状态-动作值函数的估计值<em class="no"> π </em></p></figure><figure class="na nb nc nd gt jr gh gi paragraph-image"><div class="ab gu cl ny"><img src="../Images/55f15fee4e83e4bb15e13e2e8741a65b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AO2LOleYKtAIGY-DIuUyYg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最优贪婪策略，来自[1]的等式，通过选择产生最高Q值的动作获得</p></figure><p id="ea92" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这两个任务是内在相关的:如果一个代理遵循一个策略<em class="le"> π </em>，我的第一个目标是评估这个策略有多好(<strong class="kf ir">预测</strong>)。一旦我估计了策略的价值函数，我就可以决定如何改变它，以便获得更好的价值函数(<strong class="kf ir">控制</strong>)。</p><p id="6d4f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">了解环境的<strong class="kf ir">模型</strong>，也就是了解行动和回报的概率分布，在解决这两个任务时会有很大的优势。尽管如此，该模型往往是隐藏的或过于复杂而难以处理，因此算法最好从<strong class="kf ir">经验</strong>(或<strong class="kf ir">集</strong>)中学习，这些经验是状态、动作和奖励的序列，以最终状态结束。例如，玩完整的井字游戏代表一个情节，当其中一个玩家赢了(或者两个玩家都和棋)时就达到了结束状态。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nz"><img src="../Images/c6cd3e0ba93b754b8ff948ec4699e22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBFqLg3g0gK47xszfTJIUQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">一个简单的井字游戏插曲，图片由作者提供</p></figure><p id="c5d5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可用于解决预测和控制任务的算法列表很长，并且它们中的每一个都能够在某些假设下获得最佳结果。在这里，我决定介绍三个支柱算法，它们被证明总体上运行良好，但也代表了许多更复杂算法的基础:</p><ul class=""><li id="64f7" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la oa lo lp lq bi translated">时间差异</li><li id="9452" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la oa lo lp lq bi translated">萨尔萨</li><li id="fcd8" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la oa lo lp lq bi translated">q学习</li></ul><h1 id="91f1" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated"><strong class="ak">时间差</strong></h1><p id="07ab" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">时间差异被认为是强化学习的中心思想，因为它在没有环境模型的情况下从原始经验中学习。它解决了预测问题，即估计MDP状态的值函数。</p><p id="de73" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从经验中学习的问题是，人们必须等待一集的结束(从而等待最后的回报)来更新价值函数的估计。TD克服了这个障碍，只需等待一步就能完成。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/5997f5c41efd81996a5c20b7a0216683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQR5FOjBmZcWEmjzBUKC6A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">最简单版本的TD，α的更新规则是更新速率。来自[1]的方程</p></figure><p id="491e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个时间步<em class="le"> t </em>处，状态<em class="le"> Sₜ </em>的价值函数基于立即接收到的奖励<em class="le"> Rₜ₊₁ </em>以及新状态<em class="le"> Sₜ₊₁ </em>的估计而被立即更新。这是TD的最简单版本，称为TD(0)，其中零表示必须等待一个单独的步骤来执行更新，但这一思想可以扩展到更复杂的算法，如<em class="le"> n步TD </em>和<em class="le"> TD(λ) </em>。正如您所注意到的，一个状态的更新规则利用了另一个状态的估计:这种方法被称为<strong class="kf ir"> bootstrapping </strong>。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/c0d40bd094eda499bdc8f48eba30e7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9AXw5_2-vSZk_ee6.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">TD(0)的过程，来自[1]</p></figure><p id="0cf2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们提到了使用TD的好处，下面是一个总结:</p><ol class=""><li id="1b20" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la ln lo lp lq bi translated">它不需要一个环境模型</li><li id="bb32" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">它的估计收敛(快速)到真实状态值函数</li><li id="eedf" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la ln lo lp lq bi translated">它执行在线增量更新，即它不需要等待剧集结束来更新价值函数的估计。</li></ol><h1 id="8c5f" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">萨尔萨</h1><p id="544e" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我知道你在想什么:这么奇怪的算法名字。事实上，SARSA的名字来自于从状态S到S’的转换:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi od"><img src="../Images/064ec288b61d3694daed94cdde966187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lz0hcYpEfDxny8rKOYBRvg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">从状态S到S '的轨迹，这就是SARSA算法的名称</p></figure><p id="fd3c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SARSA是一种基于策略的TD方法，用于解决<strong class="kf ir">控制</strong>问题，这意味着它试图为给定的任务找到最佳策略。为什么是“政策上”？</p><blockquote class="lb lc ld"><p id="772b" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">在<strong class="kf ir"> on-policy </strong>学习中，使用当前策略𝜋(𝑎|𝑠).从所采取的行动中学习最优值函数</p></blockquote><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oe"><img src="../Images/73a6c33809642ff8f9fc295253d6e107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLrrLuBZWp8srV86MQ6M7A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">SARSA的更新规则，等式来自[1]。注意，我们知道估计Q(s，a)，而不是V(s)</p></figure><p id="778f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，SARSA被定义为“按策略”的原因是我们利用当前策略来更新<em class="le"> Q(s，a) </em>的事实。如果这还不清楚，等着看Q-Learning做了什么，并检查区别。</p><p id="c5c7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，萨莎是如何学习最优策略的呢？我们提到它是一种TD方法，这意味着它利用TD更新规则来估计值函数。但是它如何改进策略以获得一个更大的值函数呢？</p><p id="5bd9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SARSA通过以ε贪婪的方式改变策略<em class="le"> π </em>来实现这一点:在我们执行的每一步:</p><ul class=""><li id="6c9b" class="li lj iq kf b kg kh kk kl ko lk ks ll kw lm la oa lo lp lq bi translated">具有最大<em class="le"> Q(a，s) </em>的动作，概率为1 - ε</li><li id="a212" class="li lj iq kf b kg lr kk ls ko lt ks lu kw lv la oa lo lp lq bi translated">另一个随机动作，每个都有被选中的概率ε / <em class="le"> N(a) </em>，其中<em class="le"> N(a) </em>是可能动作的数量</li></ul><p id="cfe0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参数ε平衡了<em class="le">利用与探索</em>的权衡:同时你想要利用你已经知道的，即执行最佳动作，但是你也想要探索其他可能性，即发现可能的更好的动作。</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi of"><img src="../Images/c3a16419660e1f083f1c5a791d54a7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q4ehNW45UlSC6wUp.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">SARSA程序，来自[1]</p></figure><p id="33e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦过程结束，如果我们广泛地访问每个状态-动作对，我们将有一个<strong class="kf ir">最优ε-贪婪<em class="le"> Q*(a，s) </em> </strong>，这很容易导致最优ε-贪婪策略:</p><h1 id="b5df" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated"><strong class="ak">Q-学习</strong></h1><p id="5b23" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在fundo，Q-Learning中的Dulcis是一种解决<strong class="kf ir">控制</strong>问题的非策略TD方法。</p><blockquote class="lb lc ld"><p id="7ecc" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">在<strong class="kf ir">非策略</strong>学习中，从独立于当前策略𝜋(𝑎|𝑠).采取的行动中学习最优值函数</p></blockquote><p id="6f5d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Q-Learning其实和SARSA很像。唯一(但有意义)的区别在于<em class="le"> Q(a，s) </em>的更新规则，它是:</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/c5dd5af822f2b7478c6f256cf0768fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGteUCMbO1NVPuSiD3On2A.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Q学习的更新规则，等式来自[1]</p></figure><p id="3548" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，q值没有根据我们遵循的ε-贪婪策略进行更新，但是它是基于我们可以从下一个状态Sₜ₊₁采取的最佳行动(这与SARSA非常不同！).</p><figure class="na nb nc nd gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/f91a897a2a8a41432520cdb7a7b619f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h4rDr7lRVEzbkDu6.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">q-学习程序，来自[1]</p></figure><p id="4107" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，只要每个状态-动作值对被访问无限长的时间，Q-学习就收敛到<strong class="kf ir">最优Q*(a，s) </strong>和最优策略。</p><h1 id="a493" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">那么… SARSA还是Q-Learning？</h1><p id="c620" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们看到了解决控制问题的两种算法，然而，它们学习的方式有所不同。在学习过程中，SARSA由选择的ε-贪婪动作引导，这可能是(并且经常是)次优的。另一方面，在<em class="le"> Q(s，a) </em>的更新中，Q-Learning总是跟随贪婪动作。</p><blockquote class="lb lc ld"><p id="f90e" class="kd ke le kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated">这种(大)差异使得SARSA学习的策略只是接近最优。</p></blockquote><p id="c98e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以不应该一直选择Q-Learning吗？技术上来说你可以。但是有些情况下，选择贪婪的操作可能意味着一些风险和代价高昂的错误，您可以通过选择更保守的算法(如SARSA)来避免或限制这些错误。</p><h1 id="ff5e" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">结论</h1><p id="bd55" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">在这篇文章中，我总结了RL的关键概念，从基本元素开始，如状态、行动和奖励的概念，到复杂的方法，以找到达到特定目标的最佳策略。</p><p id="9599" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习是一套复杂的方法，有时很难理解和实现，然而这些方法通常基于这里所展示的概念，因此我建议你在进一步学习之前掌握它们。</p><p id="fb68" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这篇文章对你有用！如果是这样的话，你为什么不……<br/>1。拍手声👏<br/> 2。跟我来❤️ <br/> 3。查看我的<a class="ae kc" href="https://alessandropaticchio.github.io/" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a>💻<br/> 4。在<a class="ae kc" href="https://www.linkedin.com/in/alessandro-paticchio/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上加我！🤝(我随时欢迎聊天！！！)</p><h1 id="40ce" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">信用</h1><p id="1b06" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">[1]“强化学习:导论”，作者安德鲁·巴尔托和理查德·萨顿</p></div></div>    
</body>
</html>