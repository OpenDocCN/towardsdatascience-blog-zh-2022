<html>
<head>
<title>Ace your Machine Learning Interview — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第三部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-3-af432f922aa7#2022-10-27">https://towardsdatascience.com/ace-your-machine-learning-interview-part-3-af432f922aa7#2022-10-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/08191686cefa05ba576b8c7fead1e5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tuADqIYcq2WcxYyt"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">肯尼·埃利亚松在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="83b3" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Python深入研究朴素贝叶斯分类器</h2></div><p id="5d62" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我称为“Ace your Machine Learning访谈”系列的第三篇文章，在这篇文章中，我回顾了机器学习的基础。如果你错过了前两篇文章，你可以在这里找到它们:</p><ul class=""><li id="5282" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-1-e6a5897e6844"> <strong class="kx jh"> <em class="ma">王牌你的机器学习面试-第一部分:</em> </strong> <em class="ma">潜线性、套索和岭回归及其假设</em> </a></li><li id="8f50" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-2-c58526b5faba"><strong class="kx jh"><em class="ma">Ace your Machine Learning访谈-第二部分:</em> </strong> <em class="ma">使用Python对分类问题进行逻辑回归</em> </a></li></ul><h2 id="d099" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated"><strong class="ak">简介</strong></h2><p id="a4ea" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">朴素贝叶斯(Naive Bayes)是一种用于解决分类问题的机器学习算法，之所以这么说，是因为它基于贝叶斯定理。</p><p id="2056" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">称为分类器的算法为每个数据实例分配一个类别。例如，分类电子邮件是垃圾邮件还是非垃圾邮件。</p><h2 id="872b" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">贝叶斯定理</h2><p id="58a5" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">贝叶斯定理用于计算导致已验证事件的原因的概率。我们在概率课程中都学过的公式如下。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/523d1c51a7fe50630445818e0e7ecbb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*H9xLWje4KJCMqNzQQqD3Kw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">贝叶斯定理(作者图片)</p></figure><p id="f67d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以这个定理回答了这个问题:<strong class="kx jh"><em class="ma">‘给定事件B已经发生，事件A发生的概率是多少？</em></strong><em class="ma">’</em>有趣的是这个公式把问题反过来了。也就是说，我们可以通过查看每次事件A发生时B实际发生了多少次来计算这个概率。也就是<strong class="kx jh">我们可以通过去看过去(资料)</strong>来回答原问题。</p><h2 id="799c" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">朴素贝叶斯分类器</h2><p id="a06a" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">但是我们如何应用这个定理来创建一个机器学习分类器呢？假设我们有一个由<em class="ma">个特征</em>和<em class="ma">个目标</em>组成的数据集。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5fce326961a010ef9149e6e33cc6ef52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*eIe0igNalkjKTa3bCYsABg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据集(作者提供的图片)</p></figure><p id="d15c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们现在的问题是<em class="ma">‘给定这些特征发生的情况下，具有某个标签y的概率是多少？’</em></p><p id="9d9e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，如果<em class="ma"> y =垃圾邮件/非垃圾邮件</em>，<em class="ma"> x1 = len(email) </em>，<em class="ma">x2 = number _ of _ attachments</em>，我们可能会问:</p><p id="ccfa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ma">‘给定x1 = 100个字符和x2 = 2个附件，y是垃圾邮件的概率是多少？’</em></p><p id="fb90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要回答这个问题，我们只需要简单地应用贝叶斯定理，其中A = {x1，x2，…，xn}和B = {y}。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/7cfc0b7a326b1b1bb96febb08576897c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9wCvUyjTAsEjWGv_E-x64Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">应用贝叶斯定理(图片由作者提供)</p></figure><p id="d966" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但这种分类器不叫贝叶斯分类器，而是叫朴素贝叶斯分类器。这是因为做了一个<strong class="kx jh">天真的假设</strong>来简化计算，即<strong class="kx jh">特征被假设为彼此独立</strong>。这使我们能够简化公式。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/e6a3054389987539d4385ea39fcb1f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHV1fke8hu2rrdwBDuZFmw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">天真的假设(图片由作者提供)</p></figure><p id="6020" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这样，我们就可以计算出<em class="ma"> y =垃圾邮件</em>的概率。接下来我们就来计算一下<em class="ma"> y = not_spam </em>的概率，看看哪个可能性更大。但是如果你想一想，在两个标签之间，具有较高概率的那个将是具有较大分子的那个，因为分母总是相同的:<em class="ma"> P(x1) * P(x2)*… </em></p><p id="1621" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么为了简单起见，我们也可以去掉分母，因为为了比较，我们不关心它。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/738368232741da1d5086a881506642f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_ehiDglIca5prcuKgfjMQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">简化公式(图片作者)</p></figure><p id="92e5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们要<strong class="kx jh">选择最大化这个概率</strong>的类，我们只需要使用<strong class="kx jh"> argmax </strong>。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/14dd0d855783e97dc385ee0a5580c278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZIgvWGl1SHH8iksVYNLIQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Argmax用于分类(图片由作者提供)</p></figure><h2 id="9046" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">文本数据的朴素贝叶斯分类器</h2><p id="1cd2" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">这种<strong class="kx jh">算法通常用于文本数据的NLP领域</strong>。这是因为我们可以将文本中出现的单个单词视为特征，并且天真地假设这些<strong class="kx jh">单词是独立的</strong>(这当然实际上不是真的)。</p><p id="404f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设我们有一个数据集，其中每一行都有一个句子，每一列都告诉我们这个单词是否出现在句子中。我们删去了不必要的词，如冠词等。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="bf9f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们可以用下面的方法来计算一个新句子好坏的概率。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/6e52fa6a591bd917757f59333185dce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KA63VemOjlegXjVIZB4nzQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">文本示例(作者提供的图片)</p></figure><h2 id="079d" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">我们来编码吧！</h2><p id="6f5f" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">在sklearn中实现朴素贝叶斯算法非常简单，只有几行代码。我们将使用众所周知的虹膜数据集，它包含以下特征。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="no np l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">虹膜数据集(图片由作者提供)</p></figure><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="no np l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">sklearn中的朴素贝叶斯(图片由作者提供)</p></figure><h2 id="54ec" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">优势</h2><p id="8e3f" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">从好处的角度来看，朴素贝叶斯算法有其使用的简单性。虽然这是一个基本的过时的算法，但它仍然以相当的效率很好地解决了一些分类问题。然而，其应用仅限于少数特定情况。总结:</p><ul class=""><li id="b4ef" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">适用于许多功能</li><li id="dbb2" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">适用于大型训练数据集</li><li id="7486" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">训练时收敛快</li><li id="6c81" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">它在分类特征上也表现得很好</li><li id="792e" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated">对异常值稳健</li></ul><h2 id="bbb0" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">不足之处</h2><p id="b185" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">从弊端来看，下面要特别提一下。算法需要问题中所有数据的知识。尤其是简单概率和条件概率。这种信息通常很难获得，而且成本很高。该算法提供了问题的“简单”近似，因为它没有考虑实例特征之间的相关性。</p><p id="2177" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果一个概率为零，因为它从未在数据中被观察到，你必须应用<a class="ae jd" href="https://en.wikipedia.org/wiki/Laplacian_smoothing" rel="noopener ugc nofollow" target="_blank">拉普拉斯平滑</a>。</p><h2 id="31f3" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">处理缺失值</h2><p id="eb9a" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">您可以简单地跳过缺少的值。假设我们扔了3次硬币，但是我们忘记了第二次的结果。我们可以试着总结第二次投掷的所有可能性。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/9b267dd290b8ac32de9eab25d1619869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSmerkDDpciHOwVMsHL76Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">跳过缺少的值(作者图片)</p></figure><h1 id="8556" class="ns mh jg bd mi nt nu nv ml nw nx ny mo km nz kn mr kp oa kq mu ks ob kt mx oc bi translated">最后的想法</h1><p id="57a8" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">朴素贝叶斯是处理机器学习时需要了解的主要算法之一。它已经被大量使用，特别是在文本数据问题上，比如垃圾邮件识别。正如我们所看到的，它仍然有它的优点和缺点，但肯定的是，当你被问及基本的机器学习时，会有一个关于它的问题！</p><h1 id="8c39" class="ns mh jg bd mi nt nu nv ml nw nx ny mo km nz kn mr kp oa kq mu ks ob kt mx oc bi translated">结束了</h1><p id="e546" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated"><em class="ma">马赛洛·波利蒂</em></p><p id="c15d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>