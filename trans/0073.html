<html>
<head>
<title>Feature Propagation is a simple and surprisingly efficient solution for learning on graphs with missing node features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对于在缺少节点特征的图上学习来说，特征传播是一种简单且令人惊讶地高效的解决方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-on-graphs-with-missing-features-dd34be61b06#2022-02-03">https://towardsdatascience.com/learning-on-graphs-with-missing-features-dd34be61b06#2022-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d89b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><strong class="ak">数据不完整的图形ML</strong></h2><div class=""/><div class=""><h2 id="22dc" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">大多数图形神经网络通常在所有节点可用的全套特征的假设下运行。在现实场景中，功能通常只是部分可用(例如，在社交网络中，年龄和性别只能为一小部分用户所知)。我们表明，特征传播是一种有效的和可扩展的方法，用于处理图形机器学习应用中的缺失特征，尽管它很简单，但工作得非常好。</strong></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b454182851a424f21c8e6cc5ca6b5464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzgmHfVGpMCQl1RfkCfUwA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">大多数图形神经网络期望作为输入的图形具有每个节点的完整特征向量(左)。更常见的真实场景是只有部分功能可用(右图)。</p></figure><p id="c334" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">本文与</em> <a class="ae mb" href="https://emanuelerossi.co.uk/" rel="noopener ugc nofollow" target="_blank"> <em class="ma">伊曼纽·罗西</em> </a> <em class="ma">合著。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="0a57" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di"> G </span> raph神经网络(GNN)模型通常为每个节点假设一个完整的特征向量。以两层GCN模型[1]为例，它具有以下形式:</p><p id="a466" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">z</strong>=<strong class="lg ja">a</strong>σ(<strong class="lg ja">axw</strong>₁)<strong class="lg ja">w</strong>₂</p><p id="b7cd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该模型的两个输入是编码图结构的(标准化)邻接矩阵<strong class="lg ja"> A </strong>和包含作为行的节点特征向量的特征矩阵<strong class="lg ja"> X </strong>，并输出节点嵌入<strong class="lg ja"> Z </strong>。GCN的每一层执行节点式特征变换(由可学习矩阵<strong class="lg ja"> W </strong> ₁和<strong class="lg ja"> W </strong> ₂参数化),然后将变换后的特征向量传播到邻居节点。重要的是，GCN假设<em class="ma">观察到了<strong class="lg ja"> X </strong>中的所有条目</em>。</p><p id="0c4d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在现实世界的场景中，我们经常会看到一些节点特性可能缺失的情况。例如年龄、性别等人口统计信息。仅可用于一小部分社交网络用户，而内容特征通常仅呈现给最活跃的用户。在联合购买网络中，并非所有产品都有完整的描述。这种情况变得更加严重，因为随着对数字隐私的认识不断提高，只有在用户明确同意的情况下，数据才越来越可用。</p><p id="e71a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在所有上述情况下，特征矩阵具有缺失值，并且大多数现有的GNN模型不能被直接应用。最近的几项工作导出了能够处理缺失特征的GNNs模型(例如[2–3])，然而，这些模型在高缺失特征率(&gt; 90%)的情况下受到影响，并且不能扩展到具有超过几百万条边的图。</p><p id="0623" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">在与Maria Gorinova、Ben Chamberlain (Twitter)、Henry Kenlay和Xiaowen Dong (Oxford)共同撰写的一篇新论文[4]中，我们提出了特征传播(FP) [4]作为这个问题的一个简单而有效的解决方案。简而言之，FP通过传播图上的已知特征来重构缺失的特征。然后，可以将重构的特征输入任何GNN，以解决下游任务，例如节点分类或链路预测。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/d5bc848e0c899ac504bdc48dbc0a15ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxI_dpzvfxTYk_QpqXXIdg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">特征传播框架。输入是一个缺少结点要素的图表(左侧)。在初始步骤中，特征传播通过迭代扩散图中的已知特征来重建缺失的特征(中间)。随后，该图和重构的节点特征被输入到下游的GNN模型，该模型然后产生预测(右)。</p></figure><p id="f322" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">传播步骤非常简单:首先，用任意值初始化未知特征[5]。通过应用(标准化的)邻接矩阵来传播特征，然后将已知特征重置为其基础真值。我们重复这两个操作，直到特征向量收敛[6]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ms"><img src="../Images/d1d460dc5a19a9d6ddfd1d82af132649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpnS6o5PFpys68u_yVutxA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">特征传播是在缺失特征的图上学习的一种简单而惊人强大的方法。特征的每个坐标被单独处理(<strong class="bd mt"> x </strong>表示<strong class="bd mt"> X </strong>的一列)。</p></figure><p id="0e20" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">FP可以从数据<em class="ma">同质性</em>(“平滑度”)的假设中导出，即，邻居趋向于具有相似的特征向量。可以使用<em class="ma">狄利克雷能量</em>来量化同质性的水平，狄利克雷能量是一种二次形式，用于测量节点的特征与其邻居的平均值之间的平方差。狄利克雷能量的梯度流[7]是图热扩散方程，已知特征用作边界条件。FP是通过使用单位步长的显式前向欧拉方案对该扩散方程进行离散化而获得的[8]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/92c15178a3f7bae2bd99d662a969df40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vqRT-yuInnXFhfPq"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">动画展示了应用更多特征传播迭代时标量节点特征的演变示例。未知特征被初始化为零，但是迅速收敛到使给定图形上的狄利克雷能量最小化的值。</p></figure><p id="26ba" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">特征传播与标签传播(LP) [9]相似。然而，关键区别在于LP是一种<em class="ma">特性不可知的</em>方法，它通过传播图中的已知标签来直接预测每个节点的类别。另一方面，FP用于首先重建丢失的节点特征，然后将这些特征馈送到下游的GNN。这使得FP能够利用观察到的特性，并在我们实验的所有基准测试中超过LP。此外，在实践中，带有标签的节点集和带有特征的节点集不一定完全重叠，因此这两种方法并不总是直接可比的。</p><p id="d30d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">我们</span>使用七个标准节点分类基准对FP进行了广泛的实验验证，其中我们随机移除了可变分数的节点特征(独立于每个通道)。在重构特征上，FP后接2层GCN明显优于简单基线以及最新的最先进方法[2–3]。</p><p id="d44e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">FP在特征丢失率高(&gt; 90%)的情况下表现尤为突出，而在这种情况下，所有其他方法都会受到影响。例如，即使丢失了99%的特征，当所有特征都存在时，与相同的模型相比，FP平均只损失大约4%的相对精度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mv"><img src="../Images/385daff88c08a7fad00c5a06e9e711e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cl2nPXFh8ZnBKxTtERbzDg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Cora数据集上不同缺失率(从0%是大多数gnn的标准状态到99%的极端情况)下的节点分类精度。</p></figure><p id="186f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">FP的另一个关键特性是它的可伸缩性。虽然竞争方法不能扩展到几百万边的图，但是FP可以扩展到十亿边的图。我们用了不到一个小时的时间在我们的内部Twitter图表上运行它，使用一台机器有大约10亿个节点和100亿条边。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mw"><img src="../Images/6c6ec823a9629587a6fe8fb7bf3b0839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PWCygj8LDWbWRginO3By3w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">FP+GCN的运行时间(以秒为单位)以及最新的最新方法GCNMF和PAG nn[2–3]。FP+GCN比其他两种方法快3倍。GCNMF在OGBN-Arxiv上内存不足(OOM)，而GCNMF和PaGNN在OGBN-Products上内存不足(约123M边)，其中FP的重建部分(不训练下游模型)只需约10s。</p></figure><p id="9f8e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated"><span class="l mk ml mm bm mn mo mp mq mr di">FP目前的一个局限是它不能很好地处理异向图，即邻居往往具有不同特征的图。这并不奇怪，因为FP是从同伦假设(借助于扩散方程中狄利克雷能量的最小化)中导出的。此外，FP假设不同的特征通道是不相关的，这在现实生活中很少发生。用替代的更复杂的扩散机制来适应这两种限制是可能的。</span></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/81e3853f650c1b30bcdec5dc63601eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zgrz07IrHXgHyE7CpA5BFw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">当99%的特征缺失时，具有不同程度的同嗜性(0表示极端异嗜性，1表示极端同嗜性)的合成图上的节点分类精度。虽然在高同质性设置中，FP的性能几乎与具有全部特征的情况一样好，但是在低同质性设置中，两者之间的差距是显著的，并且FP的性能劣化为简单基线的性能，在简单基线中，缺失的特征用零代替。</p></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="6a37" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mj translated">尽管缺失节点特征的图在现实应用中无处不在，但在缺失节点特征的图上获取是一个几乎未被探索的研究领域。我们相信，我们的特征传播模型是提高在具有缺失节点特征的图上学习的能力的重要一步。它也提出了关于在这种环境中学习的理论能力的深刻问题，作为关于信号和图形结构的假设的函数。FP的简单性和可扩展性，以及与更复杂的方法相比惊人的好结果，即使在极度缺失特征的情况下，也使其成为大规模工业应用的良好候选。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="8e41" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[1] T. Kipf和M. Welling，<a class="ae mb" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">带图卷积网络的半监督分类</a> (2017)，ICLR。</p><p id="45cc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2] H. Taguchi等人，<a class="ae mb" href="https://arxiv.org/abs/2007.04583" rel="noopener ugc nofollow" target="_blank">包含缺失特征的图的图卷积网络</a> (2020)，未来一代计算机系统。</p><p id="2566" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] X .陈等.<a class="ae mb" href="https://arxiv.org/abs/2011.01623" rel="noopener ugc nofollow" target="_blank">属性缺失图的学习</a> (2020)，arXiv:2011.01623</p><p id="5a2d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4] E. Rossi等人，<a class="ae mb" href="https://arxiv.org/abs/2111.12128" rel="noopener ugc nofollow" target="_blank">关于在具有缺失节点特征的图上学习的特征传播的不合理有效性</a> (2021)，arXiv:2111.12128</p><p id="da19" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5]我们证明了该算法收敛于相同的解，而不管未知值的初始化。然而，不同的初始化可能导致收敛所需的不同迭代次数。在我们的实验中，我们将未知值初始化为零。</p><p id="213d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]我们发现约40次迭代足以在我们实验的所有数据集上收敛。</p><p id="20bc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">【7】一个<em class="ma">梯度流</em>可以看作是变分问题中梯度下降的连续类比。它源于泛函的最优性条件(变分法中称为<a class="ae mb" href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation" rel="noopener ugc nofollow" target="_blank">欧拉-拉格朗日方程</a>)。</p><p id="6c00" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8] B. Chamberlain等人，<a class="ae mb" href="https://arxiv.org/abs/2106.10934" rel="noopener ugc nofollow" target="_blank"> GRAND: Graph神经扩散</a> (2021)，ICML。另见<a class="ae mb" rel="noopener" target="_blank" href="/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774?sk=cf541fa43f94587bfa81454a98533e00">附带的博文</a>。</p><p id="9ed4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9] X. Zhu和Z. Ghahramani，【利用标签传播从有标签和无标签数据中学习】 (2002)，技术报告。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="70b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们非常感谢Henry Kenlay和Davide Eynard对这篇文章的校对。关于图形深度学习的其他文章，请参见《走向数据科学》中迈克尔的 <a class="ae mb" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="ma">其他帖子</em> </a> <em class="ma">，</em> <a class="ae mb" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="ma">订阅他的帖子</em> </a> <em class="ma">和</em> <a class="ae mb" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> YouTube频道</em> </a> <em class="ma">，获取</em> <a class="ae mb" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="ma">中等会员资格</em> </a> <em class="ma">，或者关注</em> <a class="ae mb" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="ma">迈克尔<em class="ma"/></em></a></p></div></div>    
</body>
</html>