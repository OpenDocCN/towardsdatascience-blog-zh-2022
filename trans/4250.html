<html>
<head>
<title>Smart Distributed Training on Amazon SageMaker with SMD: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带SMD的Amazon SageMaker上的智能分布式培训:第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-2-c833e7139b5f#2022-09-20">https://towardsdatascience.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-2-c833e7139b5f#2022-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d908" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用SageMaker分布式数据并行优化数据分布</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0c4a22438f0e974ce6e33aef94b3dfa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ds5al1KJJDCNkpY4"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@stephenally?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯蒂芬</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4b2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是关于优化分布式培训的三篇文章的第二部分。在<a class="ae kv" href="https://chaimrand.medium.com/smart-distributed-training-on-amazon-sagemaker-with-smd-part-1-cd296f87a0ee" rel="noopener">第一部分</a>中，我们提供了分布式训练算法的简要概述。我们注意到所有算法的共同点是它们依赖于多个GPU之间的高速通信。我们推测，考虑到底层实例拓扑的分布式算法，特别是GPU对之间通信链路的差异，会比不考虑的算法性能更好。</p><p id="b8df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第二部分中，我们将演示如何使用Amazon SageMaker的<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank">分布式数据并行</a> (SDP)库来执行数据分发，区分节点内和节点间的GPU到GPU通信。</p><h1 id="864d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">使用SageMaker分布式数据并行进行数据分发</h1><p id="38be" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在数据分布式训练中，每个GPU维护其自己的模型副本，副本之间的对齐通过<strong class="ky ir">梯度共享</strong>来保持。好的数据分布算法将以限制对训练吞吐量的影响的方式实现梯度共享机制。一些梯度共享算法依赖于一个或多个中央<strong class="ky ir">参数服务器</strong>，其从所有工人收集梯度更新，然后将结果广播回工人。其他则依赖于GPU之间的直接点对点通信。梯度共享的一个流行算法是<em class="mp">Ring-all reduction</em>，在这个算法中，多个消息在一个单向环中的工人之间传递。<em class="mp"> </em>见<a class="ae kv" rel="noopener" target="_blank" href="/visual-intuition-on-ring-allreduce-for-distributed-deep-learning-d1f34b4911da">此处</a>查看<em class="mp">环环相扣</em>如何工作的精彩视觉回顾。<em class="mp"> Ring-AllReduce </em>被<a class="ae kv" href="https://horovod.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> Horovod </a>使用，这是一个流行的数据分布式训练框架。</p><p id="c298" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">亚马逊SageMaker <a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank">分布式数据并行</a> (SDP)库旨在简化和加速数据分布式训练。你可以在特色<a class="ae kv" href="https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/" rel="noopener ugc nofollow" target="_blank">公告</a>、<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank">亚马逊SageMaker开发者指南</a>以及相关的<a class="ae kv" href="https://assets.amazon.science/ba/69/0a396bd3459294ad940a705ad7f5/herring-rethinking-the-parameter-server-at-scale-for-the-cloud.pdf" rel="noopener ugc nofollow" target="_blank">白皮书</a>中看到关于SDP的更多细节。(任何想知道我选择的封面图片如何与本文内容相关的人，只需看看白皮书的标题就知道了:<a class="ae kv" href="https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Herring </strong>:重新思考云的大规模参数服务器</a>。)SDP的梯度共享算法依赖于许多巧妙的技术。与本文讨论最相关的属性是节点内GPU到GPU通信和节点间GPU到GPU通信之间的区别。下图总结了这一区别，显示了一个双层流程，其中节点内GPU通过NVLink共享梯度，而不同节点的GPU之间的通信则通过所有使用中节点的CPU上的服务器进行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/f39684f83ae3d97b0a3f3694be775dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SDhXgQxq7YA--oMxcwVtgg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SDP梯度共享流程(图片来自<a class="ae kv" href="https://assets.amazon.science/ba/69/0a396bd3459294ad940a705ad7f5/herring-rethinking-the-parameter-server-at-scale-for-the-cloud.pdf" rel="noopener ugc nofollow" target="_blank"> Herring白皮书</a></p></figure><p id="fbfe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://assets.amazon.science/ba/69/0a396bd3459294ad940a705ad7f5/herring-rethinking-the-parameter-server-at-scale-for-the-cloud.pdf" rel="noopener ugc nofollow" target="_blank">白皮书</a>展示了与标准<em class="mp"> Ring-AllReduce </em>算法相比，这种方法，即一种针对底层训练环境的拓扑结构而定制的分布式算法，如何能够加速大规模分布式训练作业。</p><p id="6e6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该注意，除了SDP使用的分层梯度分布算法之外，还有几个额外的算法和库提供了解决底层实例拓扑的解决方案。例如，除了推广使用<em class="mp"> Ring-AllReduce </em>之外，Horovod还支持分层梯度共享算法。它还公开了基于项目和环境细节的<a class="ae kv" href="https://horovod.readthedocs.io/en/stable/autotune_include.html" rel="noopener ugc nofollow" target="_blank">调节</a>梯度流的控件。此外，Horovod使用的底层<a class="ae kv" href="https://developer.nvidia.com/nccl" rel="noopener ugc nofollow" target="_blank"> NCCL </a>操作也包括<a class="ae kv" href="https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/" rel="noopener ugc nofollow" target="_blank">高级分层技术</a>。</p><p id="1034" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SDP相对于其他库的一个优势在于实例拓扑<em class="mp">发现</em>。网络带宽和延迟等训练环境的细节是选择最佳梯度共享算法的重要输入。由于SDP内置于SageMaker框架中，因此它对实例拓扑结构有着即时和详细的了解。其他库不了解相同级别的细节，可能被迫猜测最佳的梯度共享策略，或者试图<em class="mp">发现</em>丢失的信息并相应地进行调整。</p><h2 id="b58e" class="mr lt iq bd lu ms mt dn ly mu mv dp mc lf mw mx me lj my mz mg ln na nb mi nc bi translated">例子</h2><p id="e007" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这里我们展示了一个将SDP集成到一个简单的TensorFlow (2.9)脚本中的例子。SageMaker文档包括许多演示SMP APIs调用的<a class="ae kv" href="https://sagemaker-examples.readthedocs.io/en/latest/training/distributed_training/index.html#id1" rel="noopener ugc nofollow" target="_blank"> TensorFlow示例</a>。在我们的示例中，我们将包括SageMaker示例中目前没有的两种有用的技术:</p><ol class=""><li id="1af8" class="nd ne iq ky b kz la lc ld lf nf lj ng ln nh lr ni nj nk nl bi translated">如何编写您的脚本，以便您可以轻松地在SDP和流行的<a class="ae kv" href="https://horovod.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> Horovod </a>库之间切换。</li><li id="5cfb" class="nd ne iq ky b kz nm lc nn lf no lj np ln nq lr ni nj nk nl bi translated">如何将SDP与TensorFlow的高层API结合起来进行模型训练— <a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit" rel="noopener ugc nofollow" target="_blank"> tf.keras.Model.fit </a>()。虽然高级API隐藏了对TensorFlow <a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank"> GradientTape </a>的访问，但SDP要求它用<a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html#smdistributed.dataparallel.tensorflow.DistributedGradientTape" rel="noopener ugc nofollow" target="_blank"> tensorflow包装。分布式GradientTape </a> API。我们通过定制model.fit()调用的训练步骤来克服这个冲突。</li></ol><p id="4894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该示例由两个脚本组成。分布式培训作业启动脚本和培训脚本。</p><p id="187c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个脚本是SageMaker培训课程启动脚本。在本例中，我们选择将输入模式设置为<a class="ae kv" rel="noopener" target="_blank" href="/amazon-sagemaker-fast-file-mode-d12829479c39">快速文件模式</a>，这是亚马逊SageMaker的一项功能，支持将输入数据直接从<a class="ae kv" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank">亚马逊S3 </a>传输到训练实例。数据存储在<a class="ae kv" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> TFRecord </em> </a>文件中。虽然典型的分布式训练将在特别大的数据集上运行，但是对于这个例子，我从<a class="ae kv" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>数据集创建了文件(使用<a class="ae kv" href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-debugger/tensorflow_profiling/demo/generate_cifar10_tfrecords.py" rel="noopener ugc nofollow" target="_blank"> this </a>脚本)。会话由四个<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/p4/" rel="noopener ugc nofollow" target="_blank"> p4d.24xlarge </a>训练实例实例化，并且<em class="mp">分布</em>设置被配置为使用SageMaker数据分布库。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="18ab" class="nw lt iq ns b be nx ny l nz oa">from sagemaker.tensorflow import TensorFlow<br/>from sagemaker.session import TrainingInput</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="61bf" class="nw lt iq ns b be nx ny l nz oa">s3_input = TrainingInput(<br/>          's3://'+S3_BUCKET_DATASET+'/cifar10-tfrecord/',<br/>          input_mode='FastFile')</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="b55f" class="nw lt iq ns b be nx ny l nz oa"><em class="mp"># Training using SMDataParallel Distributed Training Framework<br/></em>distribution = {'smdistributed':<br/>                    {'dataparallel':{'enabled': True}}<br/>               }</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="d78f" class="nw lt iq ns b be nx ny l nz oa">tensorflow = TensorFlow(entry_point='train_tf.py',<br/>                        role=&lt;role&gt;,<br/>                        instance_type='ml.p4d.24xlarge',<br/>                        instance_count=4,<br/>                        framework_version='2.9.1',<br/>                        py_version='py39',<br/>                        distribution=distribution)</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="7bc0" class="nw lt iq ns b be nx ny l nz oa">tensorflow.fit(s3_input, job_name='data-parallel-example')</span></pre><p id="3508" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，只需修改<em class="mp">分发</em>设置，就可以修改相同的脚本来启动Horovod会话:</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="47ad" class="nw lt iq ns b be nx ny l nz oa">distribution = {<br/>  'mpi': {<br/>    'enabled': True,<br/>    'processes_per_host': 8<br/>  }<br/>}</span></pre><p id="06dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二个脚本包括训练循环。我们设计这个脚本是为了展示在使用Horovod数据分发框架和Amazon SageMaker数据并行库之间进行转换是多么容易。该脚本以一个<em class="mp"> run_hvd </em>开关开始，该开关可用于在两个选项之间切换。随后的<em class="mp"> if-else </em>块包含唯一的库特定代码。如上所述，我们已经实现了一个定制的训练步骤，该步骤使用了<em class="mp">分布式梯度带</em> API。</p><pre class="kg kh ki kj gt nr ns nt bn nu nv bi"><span id="b701" class="nw lt iq ns b be nx ny l nz oa">import tensorflow as tf</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="12a9" class="nw lt iq ns b be nx ny l nz oa"><em class="mp"># toggle flag to run Horovod<br/></em><strong class="ns ir">run_hvd</strong> = False<br/>if <strong class="ns ir">run_hvd</strong>:<br/>  import horovod.tensorflow as dist<br/>  from horovod.tensorflow.keras.callbacks import \<br/>                            BroadcastGlobalVariablesCallback<br/>else:<br/>  import smdistributed.dataparallel.tensorflow as dist<br/>  from tensorflow.keras.callbacks import Callback<br/>  class BroadcastGlobalVariablesCallback(Callback):<br/>    def __init__(self, root_rank, *args):<br/>      super(BroadcastGlobalVariablesCallback, self).<br/>                                            __init__(*args)<br/>      self.root_rank = root_rank<br/>      self.broadcast_done = False</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="e49e" class="nw lt iq ns b be nx ny l nz oa">    def on_batch_end(self, batch, logs=None):<br/>      if self.broadcast_done:<br/>        return<br/>      dist.broadcast_variables(self.model.variables, <br/>                               root_rank=self.root_rank)<br/>      dist.broadcast_variables(self.model.optimizer.variables(),          <br/>                               root_rank=self.root_rank)<br/>      self.broadcast_done = True</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="8240" class="nw lt iq ns b be nx ny l nz oa"><em class="mp"># Create Custom Model that performs the train step using <br/># DistributedGradientTape</em></span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="c926" class="nw lt iq ns b be nx ny l nz oa">from keras.engine import data_adapter<br/>class CustomModel(tf.keras.Model):<br/>  def train_step(self, data):<br/>    x, y, w = data_adapter.unpack_x_y_sample_weight(data)<br/>    with tf.GradientTape() as tape:<br/>      y_pred = self(x, training=True)<br/>      loss = self.compute_loss(x, y, y_pred, w)<br/>    tape = dist.DistributedGradientTape(tape)<br/>    self._validate_target_and_loss(y, loss)<br/>    self.optimizer.minimize(loss, <br/>                            self.trainable_variables,<br/>                            tape=tape)<br/>    return self.compute_metrics(x, y, y_pred, w)</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="2db8" class="nw lt iq ns b be nx ny l nz oa">def get_dataset(batch_size, rank):<br/>  def parse_sample(example_proto):<br/>    image_feature_description = {<br/>        'image': tf.io.FixedLenFeature([], tf.string),<br/>        'label': tf.io.FixedLenFeature([], tf.int64)<br/>    }<br/>    features = tf.io.parse_single_example(example_proto, <br/>                                          image_feature_description)<br/>    image = tf.io.decode_raw(features['image'], tf.uint8)<br/>    image.set_shape([3 * 32 * 32])<br/>    image = tf.reshape(image, [32, 32, 3])<br/>    image = tf.cast(image, tf.float32)/255.<br/>    label = tf.cast(features['label'], tf.int32)<br/>    return image, label</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="0571" class="nw lt iq ns b be nx ny l nz oa">  aut = tf.data.experimental.AUTOTUNE<br/>  records = tf.data.Dataset.list_files(<br/>                     os.environ.get("SM_CHANNEL_TRAINING")+'/*',<br/>                     shuffle=True)<br/>  ds = tf.data.TFRecordDataset(records, num_parallel_reads=aut)<br/>  ds = ds.repeat()<br/>  ds = ds.map(parse_sample, num_parallel_calls=aut)<br/>  ds = ds.batch(batch_size)<br/>  ds = ds.prefetch(aut)<br/>  return ds</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="873a" class="nw lt iq ns b be nx ny l nz oa">if __name__ == "__main__":<br/>  import argparse, os<br/>  parser = argparse.ArgumentParser(description="Train resnet")<br/>  parser.add_argument("--model_dir", type=str, <br/>                      default="./model_keras_resnet")<br/>  args = parser.parse_args()<br/>  <br/>  # init distribution lib<br/>  dist.init()<br/>  gpus = tf.config.experimental.list_physical_devices('GPU')<br/>  for gpu in gpus:<br/>    tf.config.experimental.set_memory_growth(gpu, True)<br/>  if gpus:<br/>    tf.config.experimental.set_visible_devices(<br/>                     gpus[dist.local_rank()], 'GPU')<br/>  input_shape = (32, 32, 3)<br/>  classes = 10<br/>  inputs = tf.keras.Input(shape=input_shape)</span></pre><pre class="ob nr ns nt bn nu nv bi"><span id="e7c5" class="nw lt iq ns b be nx ny l nz oa">  outputs = tf.keras.applications.ResNet50(weights=None, <br/>                                          input_shape=input_shape, <br/>                                          classes=classes)(inputs)<br/>  model = CustomModel(inputs, outputs)<br/>  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),<br/>                optimizer= tf.optimizers.Adam())<br/>  dataset = get_dataset(batch_size = 1024, rank=dist.local_rank())<br/>  cbs = [BroadcastGlobalVariablesCallback(0)]<br/>  model.fit(dataset, steps_per_epoch=100, <br/>            epochs=10, callbacks=cbs, verbose=2)</span></pre><h2 id="e87e" class="mr lt iq bd lu ms mt dn ly mu mv dp mc lf mw mx me lj my mz mg ln na nb mi nc bi translated">结果</h2><p id="31c7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本节中，我们比较了使用Horovod数据分布框架和Amazon SageMaker数据并行库执行分布式训练的运行时结果。运行时性能通过每个训练步骤的平均秒数来衡量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9b5c89ac7d9b03dfc99126da00df78e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*RVvscOjZ6sz9r-NU3vIn9A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">平均步骤时间(越短越好)——(作者)</p></figure><p id="5ac9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的实验表明，SDP库的环境感知分布式训练算法比Horovod库使用的算法高出大约20%。请注意，基于项目的细节和您选择的实例配置，比较性能会有很大的不同。即使对于相同的项目和相同的实例配置，根据实例在培训工作中的精确放置，结果也可能不同。如上所述，实例可以放置在不同的<a class="ae kv" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">集群放置组</a>中，这可能会增加延迟并减慢训练。</p><p id="f332" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，在撰写本文时，并非所有实例类型都支持SDP。详见<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-use-api.html#data-parallel-tensorflow-api" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="9f42" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">后续步骤</h1><p id="a9ca" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本帖的第三部分也是最后一部分中，我们将展示<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank"> Amazon SageMaker的分布式数据并行库</a>如何以区分节点内和节点间GPU对的方式支持数据分布。</p></div></div>    
</body>
</html>