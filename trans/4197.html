<html>
<head>
<title>Distributed Parallel Training: Data Parallelism and Model Parallelism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式并行训练:数据并行和模型并行</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214#2022-09-18">https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214#2022-09-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e634" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习的分布式训练</h2><div class=""/><div class=""><h2 id="832e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何在PyTorch中扩展培训大型模型，如GPT-3和达尔-E 2</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/dd0260b156cb1464591c600182b5876b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gVP6aZNQD1r4Rdc6"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">马克·哈普尔在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3c90" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">近年来，分布式并行训练的规模和深度学习模型的规模呈指数级增长。特别是，基于Transformer的语言模型已经抢尽了风头。臭名昭著的GPT-3爆发了1750亿个参数和96个关注层，批量大小为3.2米，单词为4990亿个。整整半年后，谷歌发布了拥有1.6万亿参数的<a class="ae le" href="https://arxiv.org/abs/2101.03961" rel="noopener ugc nofollow" target="_blank">开关变压器</a>。同一天(2021年1月11日)，北京人工智能研究院(BAAI)发布了初始的<a class="ae le" href="https://en.wikipedia.org/wiki/Wu_Dao" rel="noopener ugc nofollow" target="_blank">悟道</a> 1.0。不久，悟道2.0于2021年5月31日首次亮相，成为最大的语言模型，拥有1.75万亿个参数，是GPT-3参数的十倍。</p><p id="3653" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设我们在亚马逊SageMaker训练平台的<strong class="lh ja"> 240 </strong> ml.p4d.24xlarge实例上训练GPT-3，整个模型将需要<a class="ae le" rel="noopener" target="_blank" href="/distributed-parallel-training-model-parallel-training-a768058aa02a"> 25天</a>来训练。挑战不仅仅是处理，还有记忆。吴涛2.0似乎需要超过<a class="ae le" href="https://youtu.be/tgB671SFS4w?t=418" rel="noopener ugc nofollow" target="_blank"> 1000个GPU来存储它的参数。</a></p><p id="93ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于像GPT-3和DALL-E 2这样的深度学习大型模型，采用分布式并行训练势在必行。有两种主要类型的分布式并行训练:数据并行和模型并行。我们进一步将后者分为两个子类型:流水线并行和张量并行。我们将在这里涵盖所有分布式并行培训，并演示如何在PyTorch中开发。</p><h2 id="d951" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">了解分布式并行培训</h2><p id="63c2" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">分布式并行训练有两个高级概念:并行和分布。</p><blockquote class="my"><p id="366a" class="mz na iq bd nb nc nd ne nf ng nh ma dk translated">并行是一种解决大型模型规模或提高训练效率的框架策略，分布式是一种向外扩展的基础架构。</p></blockquote><p id="4bd2" class="pw-post-body-paragraph lf lg iq lh b li ni ka lk ll nj kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">除了这两种基本类型的并行，还有更多的变体，比如<a class="ae le" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">专家并行</a>。此外，它们可以混合两种或全部，如数据和模型混合并行。对于大规模模型，混合模型和数据并行是很常见的。例如，最大的<a class="ae le" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 </a>型号和<a class="ae le" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>采用模型和数据相结合的并行方式。然而，所有这些都应该是DL建模框架策略的一部分。</p><p id="60a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一方面，分布最终会在云或集群中扩展并行性。容器化使扩展节点变得容易，Kubernetes或云解决方案可以有效地编排它们。每个节点可以有多个GPU(或TPU和其他设备)和容器群集中的各种容器。在云原生解决方案中，节点可以对用户隐藏。一个容器管理一个或多个GPU。并行性可以跨分布式GPU容器集群进行调度。所以分布是基础架构的实现。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/3fa866f49d2c7ed370b2eddbc6484aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXtkG9ABtfIW-lCBkf5Yxg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Google Switch Transformers的数据和权重划分策略(来源:<a class="ae le" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank"> Fedus等人，2021 </a></p></figure><p id="520b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以上说明了Google Switch传输中的数据和权重划分策略。每个4×4虚线网格代表16个内核，阴影方块是该内核上的数据(模型权重或一批令牌)。它演示了如何为每个策略拆分模型权重和数据张量。<strong class="lh ja">第一行</strong>展示了模型权重如何在内核间分配。该行中不同大小的形状表示前馈网络(FFN)层中较大的权重矩阵(例如，较大的dff大小)。阴影方块的每种颜色标识一个唯一的权重矩阵。每个内核的参数数量是固定的，但是更大的权重矩阵将对每个令牌应用更多的计算。<strong class="lh ja">第二行</strong>展示了如何在内核间分割数据批次。每个内核持有相同数量的令牌，在所有策略中保持固定的内存使用量。分区策略具有不同的属性，允许每个内核在不同颜色的内核之间具有相同或不同的令牌。</p><h2 id="928f" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">PyTorch中的数据并行性</h2><p id="af2e" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">数据并行性使用相同的模型在所有内核之间分割数据。PyTorch分布式数据并行、SageMaker分布式和Horovod等数据并行框架主要完成以下三项任务:</p><ol class=""><li id="fe09" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated">首先，它创建并分发模型的副本，每个加速器一个副本。</li><li id="1d38" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">它对数据进行分片，然后将其分发给相应的设备。</li><li id="b4c6" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">它最终在反向传播步骤中将所有结果聚集在一起。</li></ol><p id="1f49" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此我们可以看到，第一个任务应该在每次训练中出现一次，但最后两个任务应该在每次迭代中出现。</p><p id="3ddb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">PyTorch <a class="ae le" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank">分布式数据并行</a> (DDP)实现了模块级的数据并行，可以跨多台机器运行。它可以与PyTorch模型并行工作。DDP应用程序应该生成多个进程，并为每个进程创建一个DDP实例。DDP使用<code class="fe oc od oe of b">torch.distributed</code>包中的集体通信来同步梯度和缓冲区。此外，DDP为来自<code class="fe oc od oe of b">model.parameters()</code>的每个参数注册了一个<em class="og">自动签名的</em>钩子，当在向后传递中计算出相应的梯度时，它将触发。然后，DDP使用该信号触发过程间的梯度同步。</p><p id="b9aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，在PyTorch中设置和运行DDP有三个主要步骤:</p><ol class=""><li id="9786" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated">通过<code class="fe oc od oe of b">torch.distributed</code>建立分布式系统。</li><li id="91ea" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">通过<code class="fe oc od oe of b">torch.nn.parallel</code>定义DDP建模。</li><li id="2bf2" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">产卵贯穿<code class="fe oc od oe of b">torch.multiprocessing</code>。</li></ol><p id="dd13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请参见下面的示例代码。</p><pre class="kp kq kr ks gt oh of oi oj aw ok bi"><span id="35c4" class="mb mc iq of b gy ol om l on oo"><strong class="of ja">import</strong> <strong class="of ja">torch<br/>import</strong> <strong class="of ja">torch.nn</strong> <strong class="of ja">as</strong> <strong class="of ja">nn<br/>import</strong> <strong class="of ja">torch.distributed</strong> <strong class="of ja">as</strong> <strong class="of ja">dist</strong><br/><strong class="of ja">import</strong> <strong class="of ja">torch.multiprocessing</strong> <strong class="of ja">as</strong> <strong class="of ja">mp</strong></span><span id="b731" class="mb mc iq of b gy op om l on oo"><strong class="of ja">from</strong> <strong class="of ja">torch.nn.parallel</strong> <strong class="of ja">import</strong> <strong class="of ja">DistributedDataParallel</strong> <strong class="of ja">as</strong> <strong class="of ja">DDP</strong></span><span id="f1a6" class="mb mc iq of b gy op om l on oo"><br/>### <strong class="of ja"><em class="og">Step 1</em></strong>: <em class="og">setup and cleanup setups</em><br/><strong class="of ja">def</strong> <strong class="of ja">setup(rank,</strong> <strong class="of ja">world_size):<br/>    ...</strong></span><span id="11c4" class="mb mc iq of b gy op om l on oo">    <em class="og"># initialize the process group</em><br/>    <strong class="of ja">dist.init_process_group(</strong>"tst"<strong class="of ja">, rank=rank, world_size=world_size)</strong><br/><br/><strong class="of ja">def</strong> <strong class="of ja">cleanup():</strong><br/>    <strong class="of ja">dist.destroy_process_group()<br/></strong></span><span id="0d68" class="mb mc iq of b gy op om l on oo">### <strong class="of ja"><em class="og">Step 2</em></strong>:<em class="og"> define DDP modeling</em><br/><strong class="of ja">def</strong> <strong class="of ja">dummy_init(rank,</strong> <strong class="of ja">world_size):</strong><br/>    <strong class="of ja">setup(rank,</strong> <strong class="of ja">world_size)</strong><br/>    <strong class="of ja">model</strong> <strong class="of ja">=</strong> <strong class="of ja">DummyModel().to(rank)</strong><br/>    <strong class="of ja">ddp_model</strong> <strong class="of ja">=</strong> <strong class="of ja">DDP(model,</strong> <strong class="of ja">device_ids=[rank])</strong></span><span id="ff15" class="mb mc iq of b gy op om l on oo"><strong class="of ja">    ...</strong></span><span id="b233" class="mb mc iq of b gy op om l on oo"><strong class="of ja">    cleanup()<br/></strong></span><span id="cf7a" class="mb mc iq of b gy op om l on oo">### <strong class="of ja"><em class="og">Step 3</em></strong>: <em class="og">Spawn to run</em><br/><strong class="of ja">def</strong> <strong class="of ja">run_dummy(dummy_fn,</strong> <strong class="of ja">world_size):</strong><br/>    <strong class="of ja">mp.spawn(dummy_fn,</strong><br/>             <strong class="of ja">args=(world_size,),</strong><br/>             <strong class="of ja">nprocs=world_size,</strong><br/>             <strong class="of ja">join=True)</strong></span></pre><h2 id="3852" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">PyTorch中的模型并行性</h2><p id="b94a" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">与数据并行不同，模型并行将模型(即其层或张量)分割到多个内核，为所有训练内核复制相同的模型。PyTorch减轻了并行实现的负担，并对其进行了最小的修改。</p><p id="55f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">简而言之，在调用损失函数时，您需要通过三个相应的区域中的“<code class="fe oc od oe of b">to(device)</code>”来指定神经网络层和到所需内核的即时输出:建模定义，“<code class="fe oc od oe of b">forward</code>”方法和“<code class="fe oc od oe of b">backward</code>”方法。PyTorch将在幕后处理所有其他的事情。请在此处查看示例代码<a class="ae le" rel="noopener" target="_blank" href="/distributed-parallel-training-model-parallel-training-a768058aa02a">。</a></p><p id="fdd8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在大模型并行的现实世界中，这可能并不简单。它通常需要额外的努力来提高培训效率和资源利用率。以流水线并行为例，<a class="ae le" href="https://dl.acm.org/doi/10.1145/3341301.3359646" rel="noopener ugc nofollow" target="_blank"> PipeDream </a>通过牺牲内存来存储权重的多个副本，提高了流水线效率。<a class="ae le" href="https://arxiv.org/abs/2102.07988" rel="noopener ugc nofollow" target="_blank"> TeraPipe </a>引入了另一种特定于单变压器架构的流水线技术，这种流水线技术是跨令牌而不是微批处理进行的。此外，<a class="ae le" href="https://arxiv.org/abs/1811.02084" rel="noopener ugc nofollow" target="_blank"> Mesh-TensorFlow </a>和<a class="ae le" href="https://arxiv.org/abs/1909.08053" rel="noopener ugc nofollow" target="_blank"> Megatron-LM </a>分别基于TensorFlow和PyTorch创建了用于优化训练十亿参数模型的张量并行框架。</p><p id="da30" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Amazon sage maker<a class="ae le" href="https://arxiv.org/pdf/2111.05972.pdf" rel="noopener ugc nofollow" target="_blank">model parallelism</a>是PyTorch之上的一个软件库。它是一个通用而灵活的框架，支持流水线和张量并行，具有节省内存的特性。其流水线并行引擎支持基于<em class="og">模块-服务器</em>设计的任意模型架构的负载平衡自动分区和流水线运行时。与流水线并行一样，张量并行的基本计算单位是<code class="fe oc od oe of b">nn.Module</code>。本质上，张量并行性在于遍历模型并用它们的分布式实现替换模型的特定子模块。</p><h2 id="f85f" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">外卖</h2><p id="466d" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">分布式并行训练有并行和分布两个高级概念。<strong class="lh ja">并行是框架策略，分发是基础架构。</strong>分布式并行培训至关重要，但在行业和研究中仍处于萌芽状态。我们可以期待未来会出现三个创新领域。</p><ol class=""><li id="48a2" class="no np iq lh b li lj ll lm lo nq ls nr lw ns ma nt nu nv nw bi translated">并行性分割数据、模型或混合数据，以进行大型模型训练。随着数据和模型呈指数级增长，<strong class="lh ja">优化内存使用和处理效率</strong>变得至关重要。</li><li id="3d2a" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">训练一个大模特很贵。<strong class="lh ja">复用训练层的迁移学习</strong>将改变大规模分布式并行训练的游戏。</li><li id="6413" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">ML生命周期涉及多个分布式系统，从数据收集到处理、模型训练和服务。ML平台经常受到复杂性、数据通信成本和系统不稳定性的阻碍。统一ML的所有分布式系统意义重大。</li></ol></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><h2 id="11e4" class="mb mc iq bd md me mf dn mg mh mi dp mj lo mk ml mm ls mn mo mp lw mq mr ms iw bi translated">参考</h2><ol class=""><li id="ebcc" class="no np iq lh b li mt ll mu lo ox ls oy lw oz ma nt nu nv nw bi translated">亚马逊SageMaker模型并行性:大型模型训练的通用灵活框架:<a class="ae le" href="https://arxiv.org/abs/2111.05972" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.05972</a></li><li id="e612" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">开关变压器:用简单有效的稀疏性扩展到万亿参数模型:【https://arxiv.org/abs/2101.03961 T2】</li><li id="ec56" class="no np iq lh b li nx ll ny lo nz ls oa lw ob ma nt nu nv nw bi translated">语言模型是很少出手的学习者:【https://arxiv.org/abs/2005.14165 T4】</li></ol></div></div>    
</body>
</html>