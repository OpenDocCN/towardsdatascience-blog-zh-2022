<html>
<head>
<title>Stable diffusion using Hugging Face — DiffEdit paper implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用拥抱脸的稳定扩散— DiffEdit 纸张实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-diffedit-paper-implementation-e4c99e3e320c#2022-11-23">https://towardsdatascience.com/stable-diffusion-using-hugging-face-diffedit-paper-implementation-e4c99e3e320c#2022-11-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3473" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">DIFFEDIT 的一种实现:基于扩散的语义图像编辑，具有掩模引导🤗<a class="ae kl" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">抱紧脸扩散器库</a>。</p><p id="7735" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将实现 Meta AI 和索邦大学的研究人员最近发表的一篇名为<code class="fe km kn ko kp b"><strong class="jp ir">DIFFEDIT</strong></code>的论文。这篇博客对于那些熟悉稳定扩散过程或者正在阅读我写的关于稳定扩散的另外两篇博客的人来说更有意义。<strong class="jp ir">第一部分</strong> - <a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-501d8dbdd8">使用拥抱脸的稳定扩散|作者 Aayush agr awal | 2022 年 11 月|走向数据科学</a> <br/> 2 .<strong class="jp ir">第 2 部分</strong> - <a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">使用拥抱脸的稳定扩散-稳定扩散的变化|作者 Aayush agr awal | 2022 年 11 月|走向数据科学</a></p><p id="cf09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最初，这是我想写的博文，但是意识到没有一个地方可以理解代码的稳定扩散。这就是我最终创建其他博客作为参考或预读材料来理解本文的原因。</p></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="469c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">什么是 DiffEdit？</h1><p id="ae97" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">简单地说，你可以把<code class="fe km kn ko kp b">DiffEdit</code>方法看作是<code class="fe km kn ko kp b">Image to Image</code>管道的一个更受控制的版本。<code class="fe km kn ko kp b">DiffEdit</code>接受三个输入- <br/> 1。一个输入图像<br/> 2。<code class="fe km kn ko kp b">Caption</code> -描述输入图像<br/> 3。<code class="fe km kn ko kp b">Target Query</code> -描述您想要生成的新图像</p><p id="b40e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">并且基于查询文本产生原始图像的修改版本。如果您想对实际图像稍作调整而不完全修改它，这个过程特别好。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/1bbdd6877209ee6fc64b4cb726e47506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*joQaY9Kc6hUNZF4n.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 1:差异编辑概述。</p></figure><p id="2260" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的图片中我们可以看到，只有图片中的水果部分被替换成了梨。相当惊人的结果！</p><p id="f9cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作者解释说，他们实现这一目标的方式是通过引入一个遮罩生成模块，该模块确定图像的哪一部分应该被编辑，然后只对被遮罩的部分执行基于文本的扩散调节。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a3d03abacabe2e13067247629120e915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*Hnkz42QOGUB2Lpxj.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 2:来自论文<a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank"> DiffEdit </a>。一种通过提供标题文本和新文本来改变输入图像的方法。</p></figure><p id="59f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面取自论文的图像中我们可以看到，作者从输入图像中创建了一个遮罩，它可以准确地确定图像中存在水果的部分，并生成一个遮罩(以橙色显示),然后执行遮罩扩散以用梨替换水果。进一步阅读，作者提供了整个<code class="fe km kn ko kp b">DiffEdit</code>过程的一个很好的可视化表示。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi mr"><img src="../Images/8438306b3f4eba121c3ae5fced0370f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Da-MJK6Vq2GnZkdn.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 DiffEdit 的三个步骤。<a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="4496" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我阅读这篇论文时，似乎生成掩蔽是最重要的步骤，剩下的只是使用扩散过程的文本条件。使用蒙版对图像进行调节的想法与<a class="ae kl" href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py" rel="noopener ugc nofollow" target="_blank">拥抱面部画中画管道</a>中实现的想法类似。正如作者们所建议的，“这个<code class="fe km kn ko kp b">DiffEdit</code>过程分三步——<br/><strong class="jp ir">第一步:</strong>给输入图像加噪声，去噪:一次条件化在查询文本上，一次条件化在参考文本上(或者无条件)。我们基于去噪结果的差异来导出掩模。<br/></p><p id="8d61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在接下来的部分中，我们将开始在实际的代码中实现这些想法。</p><p id="4b82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们从导入所需的库和助手函数开始。所有这些都已经在稳定扩散系列的前<a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-501d8dbdd8">第 1 部分</a>和<a class="ae kl" rel="noopener" target="_blank" href="/stable-diffusion-using-hugging-face-variations-of-stable-diffusion-56fd2ab7a265">第 2 部分</a>中使用和解释过。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="6e15" class="mw ky iq kp b be mx my l mz na">import torch, logging<br/><br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/><br/>## Imaging  library<br/>from PIL import Image<br/>from torchvision import transforms as tfms<br/><br/><br/>## Basic libraries<br/>from fastdownload import FastDownload<br/>import numpy as np<br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from IPython.display import display<br/>import shutil<br/>import os<br/><br/>## For video display<br/>from IPython.display import HTML<br/>from base64 import b64encode<br/><br/><br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler<br/><br/>## Helper functions<br/><br/>def load_artifacts():<br/>    '''<br/>    A function to load all diffusion artifacts<br/>    '''<br/>    vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/>    unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")<br/>    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>    text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")<br/>    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", clip_sample=False, set_alpha_to_one=False)    <br/>    return vae, unet, tokenizer, text_encoder, scheduler<br/><br/>def load_image(p):<br/>    '''<br/>    Function to load images from a defined path<br/>    '''<br/>    return Image.open(p).convert('RGB').resize((512,512))<br/><br/>def pil_to_latents(image):<br/>    '''<br/>    Function to convert image to latents<br/>    '''<br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0<br/>    init_image = init_image.to(device="cuda", dtype=torch.float16) <br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215<br/>    return init_latent_dist<br/><br/>def latents_to_pil(latents):<br/>    '''<br/>    Function to convert latents to images<br/>    '''<br/>    latents = (1 / 0.18215) * latents<br/>    with torch.no_grad():<br/>        image = vae.decode(latents).sample<br/>    image = (image / 2 + 0.5).clamp(0, 1)<br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()<br/>    images = (image * 255).round().astype("uint8")<br/>    pil_images = [Image.fromarray(image) for image in images]<br/>    return pil_images<br/><br/>def text_enc(prompts, maxlen=None):<br/>    '''<br/>    A function to take a texual promt and convert it into embeddings<br/>    '''<br/>    if maxlen is None: maxlen = tokenizer.model_max_length<br/>    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") <br/>    return text_encoder(inp.input_ids.to("cuda"))[0].half()<br/><br/>vae, unet, tokenizer, text_encoder, scheduler = load_artifacts()</span></pre><p id="b815" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也下载一个图像，我们将用于代码实现过程。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="1f6b" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>init_img</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/89268ca2fbe17090e9fe9ecaa85da8a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*FeEeIlb1KycLo-9I.png"/></div></figure></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="9daf" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">2 DiffEdit:纯粹的实现</h1><p id="d166" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">让我们从按照作者的建议实现这篇论文开始，因此是纯粹的实现。</p><h1 id="1ba8" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">2.1 遮罩创建:DiffEdit 过程的第一步</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/a382e1dc0624fff1ec4639e610d165ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Bsq2nuzhMuqMx0SF.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 4:<code class="fe km kn ko kp b">DiffEdit</code>论文的第一步。信用— <a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="c212" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文中有对步骤 1 更详细的解释，下面是提到的关键部分——<br/>1。使用不同的文本条件对图像去噪，一个使用参考文本，另一个使用查询文本，并从结果中取差。这个想法是在不同的部分有更多的变化，而不是在图像的背景中。<br/> 2。重复此差分过程 10 次<br/> 3。平均这些差异，并对遮罩进行二值化</p><blockquote class="ni nj nk"><p id="fef3" class="jn jo nl jp b jq jr js jt ju jv jw jx nm jz ka kb nn kd ke kf no kh ki kj kk ij bi translated">注意——蒙版创建的第三步(平均和二值化)在文章中没有解释清楚，我做了很多实验才弄清楚。</p></blockquote><p id="a6d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们将尝试完全按照所提到的来实现这篇论文。我们将为此任务修改<a class="ae kl" href="https://aayushmnit.com/posts/2022-11-10-StableDiffusionP4/2022-11-10-StableDiffusionP4.html#variation-2-image-to-image-pipeline" rel="noopener ugc nofollow" target="_blank"> prompt_2_img_i2i </a>函数，以返回 latents，而不是重新缩放和解码的去噪图像。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="dcdc" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_i2i(prompts, init_img, neg_prompts=None, g=7.5, seed=100, strength =0.8, steps=50, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    if not neg_prompts: uncond =  text_enc([""], text.shape[1])<br/>    else: uncond =  text_enc(neg_prompt, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/><br/>        # Conditioning  the latents<br/>        #latents = scheduler.step(pred, ts, latents).pred_original_sample<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents.detach().cpu()</span></pre><p id="80d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将制作一个<code class="fe km kn ko kp b">create_mask</code>函数，该函数将获取一个初始图像、引用提示和带有我们需要重复这些步骤的次数的查询提示。在论文中，作者建议 n=10，强度为 0.5，在他们的实验中效果很好。因此，该函数的默认值被调整为。<code class="fe km kn ko kp b">create_mask</code>功能执行以下步骤- <br/> 1。创建两个去噪的潜在值，一个以参考文本为条件，另一个以查询文本为条件，取这两个潜在值的差<br/> 2。重复此步骤 n 次<br/> 3。取这些差异的平均值并标准化<br/> 4。选择阈值 0.5 进行二值化并创建一个遮罩</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="d6d2" class="mw ky iq kp b be mx my l mz na">def create_mask(init_img, rp, qp, n=10, s=0.5):<br/>    ## Initialize a dictionary to save n iterations<br/>    diff = {}<br/>    <br/>    ## Repeating the difference process n times<br/>    for idx in range(n):<br/>        ## Creating denoised sample using reference / original text<br/>        orig_noise = prompt_2_img_i2i(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Creating denoised sample using query / target text<br/>        query_noise = prompt_2_img_i2i(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Taking the difference <br/>        diff[idx] = (np.array(orig_noise)-np.array(query_noise))<br/>    <br/>    ## Creating a mask placeholder<br/>    mask = np.zeros_like(diff[0])<br/>    <br/>    ## Taking an average of 10 iterations<br/>    for idx in range(n):<br/>        ## Note np.abs is a key step<br/>        mask += np.abs(diff[idx])  <br/>        <br/>    ## Averaging multiple channels <br/>    mask = mask.mean(0)<br/>    <br/>    ## Normalizing <br/>    mask = (mask - mask.mean()) / np.std(mask)<br/>    <br/>    ## Binarizing and returning the mask object<br/>    return (mask &gt; 0).astype("uint8")<br/><br/>mask = create_mask(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=10)</span></pre><p id="bb81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们在图像上可视化生成的遮罩。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="52b8" class="mw ky iq kp b be mx my l mz na">plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/26188ac0b192988e7f6bd1305a6d7b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*uZvauoHkzAK6neLw.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 5:我们的马图像的掩蔽可视化。</p></figure><p id="c080" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在上面看到的，制作的面具很好的覆盖了马的部分，这正是我们想要的。</p><h1 id="6c2b" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">2.2 掩蔽扩散:DiffEdit 论文的步骤 2 和 3。</h1><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nq"><img src="../Images/58c77afa23523bd9269b9b9826d1ee7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oAgHGBoneVPAJLOY.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 6:<code class="fe km kn ko kp b">DiffEdit</code>论文中的第二步和第三步。信用— <a class="ae kl" href="https://arxiv.org/pdf/2210.11427.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="f0d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">步骤 2 和 3 需要在同一个循环中实现。简而言之，作者是说根据非屏蔽部分的参考文本和屏蔽部分的查询文本来调节潜在事件。<br/>使用这个简单的公式将这两个部分组合起来，以创建组合的潜在客户-</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/cf632b532fa905e82c4d36f424b2dacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*6cvj1V-AEGXDhFDrPBlgFg.png"/></div></figure><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="49a9" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_diffedit(rp, qp, init_img, mask, g=7.5, seed=100, strength =0.7, steps=70, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    rtext = text_enc(rp) <br/>    qtext = text_enc(qp)<br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    uncond =  text_enc([""], rtext.shape[1])<br/>    emb = torch.cat([uncond, rtext, qtext])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # Computing the timestep to start the diffusion loop<br/>    t_start = max(steps - init_timestep, 0)<br/>    timesteps = scheduler.timesteps[t_start:].to("cuda")<br/>    <br/>    # Converting mask to torch tensor<br/>    mask = torch.tensor(mask, dtype=unet.dtype).unsqueeze(0).unsqueeze(0).to("cuda")<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 3), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u, rt, qt = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(3)<br/>            <br/>        # Performing Guidance<br/>        rpred = u + g*(rt-u)<br/>        qpred = u + g*(qt-u)<br/><br/>        # Conditioning  the latents<br/>        rlatents = scheduler.step(rpred, ts, latents).prev_sample<br/>        qlatents = scheduler.step(qpred, ts, latents).prev_sample<br/>        latents = mask*qlatents + (1-mask)*rlatents<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents_to_pil(latents)</span></pre><p id="3405" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将生成的图像可视化</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="2c9b" class="mw ky iq kp b be mx my l mz na">output = prompt_2_img_diffedit(<br/>    rp = ["a horse image"], <br/>    qp=["a zebra image"],<br/>    init_img=init_img, <br/>    mask = mask, <br/>    g=7.5, seed=100, strength =0.5, steps=70, dim=512)<br/><br/>## Plotting side by side<br/>fig, axs = plt.subplots(1, 2, figsize=(12, 6))<br/>for c, img in enumerate([init_img, output[0]]): <br/>    axs[c].imshow(img)<br/>    if c == 0 : axs[c].set_title(f"Initial image ")<br/>    else: axs[c].set_title(f"DiffEdit output")</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ns"><img src="../Images/bfa1c3e48b34ce1aed500b8e0a66a506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8LFiik5WC6cp00cV.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 7: DiffEdit 输出可视化</p></figure><p id="c014" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们为遮罩和扩散过程创建一个简单的函数。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="7c5a" class="mw ky iq kp b be mx my l mz na">def diffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=70, dim=512):<br/>    <br/>    ## Step 1: Create mask<br/>    mask = create_mask(init_img=init_img, rp=rp, qp=qp)<br/>    <br/>    ## Step 2 and 3: Diffusion process using mask<br/>    output = prompt_2_img_diffedit(<br/>        rp = rp, <br/>        qp=qp, <br/>        init_img=init_img, <br/>        mask = mask, <br/>        g=g, <br/>        seed=seed,<br/>        strength =strength, <br/>        steps=steps, <br/>        dim=dim)<br/>    return mask , output</span></pre><p id="b21e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们也为<code class="fe km kn ko kp b">DiffEdit</code>创建一个可视化函数，显示原始输入图像、屏蔽图像和最终输出图像。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="d6da" class="mw ky iq kp b be mx my l mz na">def plot_diffEdit(init_img, output, mask):<br/>    ## Plotting side by side<br/>    fig, axs = plt.subplots(1, 3, figsize=(12, 6))<br/>    <br/>    ## Visualizing initial image<br/>    axs[0].imshow(init_img)<br/>    axs[0].set_title(f"Initial image")<br/>    <br/>    ## Visualizing initial image<br/>    axs[2].imshow(output[0])<br/>    axs[2].set_title(f"DiffEdit output")<br/>    <br/>    ## Visualizing the mask <br/>    axs[1].imshow(np.array(init_img), cmap='gray') <br/>    axs[1].imshow(<br/>        Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>        cmap='cividis', <br/>        alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>    )<br/>    axs[1].set_title(f"DiffEdit mask")</span></pre><p id="1b79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们在一些图像上测试这个函数。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="94d3" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask, output = diffEdit(<br/>  init_img, <br/>  rp = ["a horse image"], <br/>  qp=["a zebra image"]<br/>)<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/a49aaa1cc580edc4e466053831aadd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T9C2ryMjZ5FNI_oG.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 8: Purist 实现输出示例</p></figure><p id="5b98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了，让我们试试另一个。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="3ad2" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')<br/>init_img = load_image(p)<br/>mask, output = diffEdit(<br/>  init_img, <br/>  rp = ['Bowl of Strawberries'], <br/>  qp=['Bowl of Grapes']<br/>)<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/7fa0afcd31383e4919c73f64cd764d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ORLNftQ3EaYlFhBA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 9: Purist 实现输出示例</p></figure></div><div class="ab cl kq kr hu ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="ij ik il im in"><h1 id="ac22" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">3 FastDiffEdit:一个更快的 DiffEdit 实现</h1><p id="8568" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">现在我们已经看到了 purist 的实现，我建议我们可以在速度和更好的结果方面对原始的 DiffEdit 过程进行一些改进。我们姑且称这些改进为<code class="fe km kn ko kp b">FastDiffEdit</code>。</p><h1 id="b93d" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">3.1 遮罩创建:快速 DiffEdit 遮罩过程</h1><p id="c301" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">我对当前的蒙版方式最大的问题是它太花时间了(在 4500 GPU 上大约 50 秒)。我的观点是，我们不需要运行一个完整的扩散循环来对图像进行降噪，而只需在一次拍摄中使用原始样本的 U-net 预测，并将重复次数增加到 20 次。在这种情况下，我们可以将计算从 10*25 = 250 步提高到 20 步(少 12x 个循环)。让我们看看这在实践中是否行得通。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="046c" class="mw ky iq kp b be mx my l mz na">def prompt_2_img_i2i_fast(prompts, init_img, g=7.5, seed=100, strength =0.5, steps=50, dim=512):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    uncond =  text_enc([""], text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Convert the seed image to latent<br/>    init_latents = pil_to_latents(init_img)<br/>    <br/>    # Figuring initial time step based on strength<br/>    init_timestep = int(steps * strength) <br/>    timesteps = scheduler.timesteps[-init_timestep]<br/>    timesteps = torch.tensor([timesteps], device="cuda")<br/>    <br/>    # Adding noise to the latents <br/>    noise = torch.randn(init_latents.shape, generator=None, device="cuda", dtype=init_latents.dtype)<br/>    init_latents = scheduler.add_noise(init_latents, noise, timesteps)<br/>    latents = init_latents<br/>    <br/>    # We need to scale the i/p latents to match the variance<br/>    inp = scheduler.scale_model_input(torch.cat([latents] * 2), timesteps)<br/>    # Predicting noise residual using U-Net<br/>    with torch.no_grad(): u,t = unet(inp, timesteps, encoder_hidden_states=emb).sample.chunk(2)<br/>         <br/>    # Performing Guidance<br/>    pred = u + g*(t-u)<br/><br/>    # Zero shot prediction<br/>    latents = scheduler.step(pred, timesteps, latents).pred_original_sample<br/>    <br/>    # Returning the latent representation to output an array of 4x64x64<br/>    return latents.detach().cpu()</span></pre><p id="3a19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们创建一个新的屏蔽函数，它可以接受我们的<code class="fe km kn ko kp b">prompt_2_img_i2i_fast</code>函数。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="3501" class="mw ky iq kp b be mx my l mz na">def create_mask_fast(init_img, rp, qp, n=20, s=0.5):<br/>    ## Initialize a dictionary to save n iterations<br/>    diff = {}<br/>    <br/>    ## Repeating the difference process n times<br/>    for idx in range(n):<br/>        ## Creating denoised sample using reference / original text<br/>        orig_noise = prompt_2_img_i2i_fast(prompts=rp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Creating denoised sample using query / target text<br/>        query_noise = prompt_2_img_i2i_fast(prompts=qp, init_img=init_img, strength=s, seed = 100*idx)[0]<br/>        ## Taking the difference <br/>        diff[idx] = (np.array(orig_noise)-np.array(query_noise))<br/>    <br/>    ## Creating a mask placeholder<br/>    mask = np.zeros_like(diff[0])<br/>    <br/>    ## Taking an average of 10 iterations<br/>    for idx in range(n):<br/>        ## Note np.abs is a key step<br/>        mask += np.abs(diff[idx])  <br/>        <br/>    ## Averaging multiple channels <br/>    mask = mask.mean(0)<br/>    <br/>    ## Normalizing <br/>    mask = (mask - mask.mean()) / np.std(mask)<br/>    <br/>    ## Binarizing and returning the mask object<br/>    return (mask &gt; 0).astype("uint8")</span></pre><p id="305e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看这个新的蒙版函数是否能产生一个好的蒙版。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="bf9e" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask = create_mask_fast(init_img=init_img, rp=["a horse image"], qp=["a zebra image"], n=20)<br/>plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ea85fff307fe72b9c89f037d7fa00f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*IP_tgj3Sao8IUQGq.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 10: <code class="fe km kn ko kp b">FastDiffEdit</code>遮蔽我们的马的形象。</p></figure><p id="e4a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在上面所看到的，在我的机器上，屏蔽得到了改进，计算时间从大约 50 秒减少到大约 10 秒(提高了 5 倍！).</p><p id="4137" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们通过添加 cv2 技巧来改进我们的遮罩。这将只是平滑掩蔽多一点点。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="440b" class="mw ky iq kp b be mx my l mz na">import cv2<br/>def improve_mask(mask):<br/>    mask  = cv2.GaussianBlur(mask*255,(3,3),1) &gt; 0<br/>    return mask.astype('uint8')<br/><br/>mask = improve_mask(mask)<br/>plt.imshow(np.array(init_img), cmap='gray') # I would add interpolation='none'<br/>plt.imshow(<br/>    Image.fromarray(mask).resize((512,512)), ## Scaling the mask to original size<br/>    cmap='cividis', <br/>    alpha=0.5*(np.array(Image.fromarray(mask*255).resize((512,512))) &gt; 0)  <br/>)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5c8d76afefa3a42ce7e9ca58ecbf44a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*5P29CXoH9G1GbkdD.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 11:使用 cv2 高斯模糊技巧改进了我们的马图像的<code class="fe km kn ko kp b">FastDiffEdit</code>掩蔽可视化。</p></figure><p id="68f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在上面看到的，遮罩变得更加平滑，覆盖了更多的区域。</p><h1 id="a6f1" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">3.2 掩蔽扩散:替换为🤗修补管道</h1><p id="c601" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">因此，不是使用我们的函数来执行掩蔽扩散，而是有一个特殊的管道🤗<code class="fe km kn ko kp b">diffusers</code>库名为<code class="fe km kn ko kp b">inpaint</code>管道。它采用查询提示、初始图像和生成的遮罩来生成输出图像。让我们从装入<code class="fe km kn ko kp b">inpaint</code>管道开始。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="a649" class="mw ky iq kp b be mx my l mz na">from diffusers import StableDiffusionInpaintPipeline<br/>pipe = StableDiffusionInpaintPipeline.from_pretrained(<br/>    "runwayml/stable-diffusion-inpainting",<br/>    revision="fp16",<br/>    torch_dtype=torch.float16,<br/>).to("cuda")</span></pre><p id="9b0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用我们生成的蒙版和图像修复管道。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="013a" class="mw ky iq kp b be mx my l mz na">pipe(<br/>    prompt=["a zebra image"], <br/>    image=init_img, <br/>    mask_image=Image.fromarray(mask*255).resize((512,512)), <br/>    generator=torch.Generator("cuda").manual_seed(100),<br/>    num_inference_steps = 20<br/>).images[0]<br/>image</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b780ec9c6d42db7707d6febf71e5197f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*q8zzN__VxZIg54EZ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 12:油漆管道输出。</p></figure><p id="11a2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们上面看到的，修复管道创建了一个更真实的斑马图像。让我们为遮罩和扩散过程创建一个简单的函数。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="77d7" class="mw ky iq kp b be mx my l mz na">def fastDiffEdit(init_img, rp , qp, g=7.5, seed=100, strength =0.7, steps=20, dim=512):<br/>    <br/>    ## Step 1: Create mask<br/>    mask = create_mask_fast(init_img=init_img, rp=rp, qp=qp, n=20)<br/>    <br/>    ## Improve masking using CV trick<br/>    mask = improve_mask(mask)<br/>    <br/>    ## Step 2 and 3: Diffusion process using mask<br/>    output = pipe(<br/>        prompt=qp, <br/>        image=init_img, <br/>        mask_image=Image.fromarray(mask*255).resize((512,512)), <br/>        generator=torch.Generator("cuda").manual_seed(100),<br/>        num_inference_steps = steps<br/>    ).images<br/>    return mask , output</span></pre><p id="03fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们在一些图像上测试这个函数。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="90f4" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://images.pexels.com/photos/1996333/pexels-photo-1996333.jpeg?cs=srgb&amp;dl=pexels-helena-lopes-1996333.jpg&amp;fm=jpg&amp;_gl=1*1pc0nw8*_ga*OTk4MTI0MzE4LjE2NjY1NDQwMjE.*_ga_8JE65Q40S6*MTY2Njc1MjIwMC4yLjEuMTY2Njc1MjIwMS4wLjAuMA..')<br/>init_img = load_image(p)<br/>mask, output = fastDiffEdit(init_img, rp = ["a horse image"], qp=["a zebra image"])<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/306fea9b50282cc5baea2b37bce9de6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QmgCMZXnBzpQmX4j.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 13: <code class="fe km kn ko kp b">FastDiffEdit</code>输出示例</p></figure><p id="018c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">太好了，让我们试试另一个。</p><pre class="mb mc md me gt ms kp mt bn mu mv bi"><span id="56ca" class="mw ky iq kp b be mx my l mz na">p = FastDownload().download('https://raw.githubusercontent.com/johnrobinsn/diffusion_experiments/main/images/bowloberries_scaled.jpg')<br/>init_img = load_image(p)<br/>mask, output = fastDiffEdit(init_img, rp = ['Bowl of Strawberries'], qp=['Bowl of Grapes'])<br/>plot_diffEdit(init_img, output, mask)</span></pre><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nt"><img src="../Images/85e83579ff8a94530d009d407cfc83df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7-9KnjbOc7clRg63.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">图 14: <code class="fe km kn ko kp b">FastDiffEdit</code>输出示例</p></figure><h1 id="d7a6" class="kx ky iq bd kz la nc lc ld le nd lg lh li ne lk ll lm nf lo lp lq ng ls lt lu bi translated">4 结论</h1><p id="fa9b" class="pw-post-body-paragraph jn jo iq jp b jq lv js jt ju lw jw jx jy lx ka kb kc ly ke kf kg lz ki kj kk ij bi translated">在这篇文章中，我们实现了作者提到的<code class="fe km kn ko kp b">DiffEdit</code>论文，然后我们对创建<code class="fe km kn ko kp b">FastDiffEdit</code>的方法提出了改进，将计算速度提高了 5 倍。</p><p id="ec7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望你喜欢阅读它，并随时使用我的代码，并尝试生成您的图像。此外，如果对代码或博客帖子有任何反馈，请随时联系 LinkedIn 或给我发电子邮件，地址是 aayushmnit@gmail.com。你也可以在我的网站上阅读博客的早期发布【aayushmnit.com<a class="ae kl" href="https://aayushmnit.com/blog.html" rel="noopener ugc nofollow" target="_blank">Aayush agr awal-博客</a>。</p></div></div>    
</body>
</html>