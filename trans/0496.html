<html>
<head>
<title>Go Beyond Binary Classification with Multi-Class and Multi-Label Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用多类和多标签模型超越二元分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/go-beyond-binary-classification-with-multi-class-and-multi-label-models-6ce91ca08264#2022-02-19">https://towardsdatascience.com/go-beyond-binary-classification-with-multi-class-and-multi-label-models-6ce91ca08264#2022-02-19</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="95cf" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">此外，了解多类的不同策略，以提高性能</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/232a8c1764434b555fb23aadb1a5cbba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RQUY4HnhvnzdP73c"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@judithgirardmarczak?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">朱迪思·吉拉德-马克扎克</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="8f33" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是多类多标签分类？</h1><p id="438b" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">通常当你开始学习机器学习中的分类问题时，你会从<a class="ae kz" rel="noopener" target="_blank" href="/everything-you-need-to-know-to-build-an-amazing-binary-classifier-590de3482aad">二元分类</a>或者只有两种可能结果的地方开始，比如<em class="mo">垃圾</em>或者<em class="mo">非垃圾</em>、<em class="mo">欺诈</em>或者<em class="mo">非欺诈</em>等等。除此之外，我们还有<strong class="lu iv">多级</strong>和<strong class="lu iv">多标签</strong>分类。让我们从解释每一个开始。</p><p id="356c" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">多类别分类</strong>是指目标变量(<code class="fe mu mv mw mx b">y</code>)中有两个以上的类别。例如，你可以有<em class="mo">小型</em>、<em class="mo">中型</em>、<em class="mo">大型</em>和<em class="mo">大型</em>，或者你可以有一个基于一到五颗星的评级系统。这些级别中的每一个都可以被认为是一个类。该策略的目标是<strong class="lu iv">从可用类别中预测出一个类别</strong>。</p><p id="8dc4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">多标签分类</strong>略有不同。这里你也有两个以上的类别可用，但是不是只选择一个类别，这个策略的目标是<strong class="lu iv">在适用的时候预测多个类别</strong>。当您有多个相互关联的类别时，这种策略非常有用。我见过的最好的例子之一是当它被用在标签系统中时。例如，中型文章可以有多个与之相关联的标签。这篇特定的文章可能有<code class="fe mu mv mw mx b">machine learning</code>、<code class="fe mu mv mw mx b">data science</code>和<code class="fe mu mv mw mx b">python</code>作为标签。</p><p id="de53" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">在开始之前，如果你想深入了解二进制分类，可以看看我的文章:<a class="ae kz" rel="noopener" target="_blank" href="/everything-you-need-to-know-to-build-an-amazing-binary-classifier-590de3482aad">构建一个惊人的二进制分类器所需要知道的一切</a>。</p><p id="9c3f" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">现在，让我们深入研究多类策略。</p><h1 id="bd11" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">多类策略</h1><p id="76ab" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">策略是你如何指导分类器处理两个以上的类，这可能会影响性能<strong class="lu iv">泛化</strong>或计算资源。泛化指的是分类器在未来对看不见的数据工作得有多好；这与<em class="mo">过拟合</em>相反。查看 Scikit-Learn <a class="ae kz" href="https://scikit-learn.org/stable/modules/multiclass.html#ovr-classification" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多信息。</p><p id="ef61" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">一个对其余的</strong>或(OVR)，也称为<strong class="lu iv">一个对所有的</strong> (OVA)策略，适用于每个类别的单个分类器，该分类器适用于所有其他类别。OVR 本质上是把多类问题分裂成一组二元分类问题。OVR 往往表现良好，因为有<code class="fe mu mv mw mx b">n</code>类的<code class="fe mu mv mw mx b">n</code>分类器。OVR 是最常见的策略，你可以从这里开始你的旅程。这实际上是这样的:</p><ul class=""><li id="406f" class="my mz iu lu b lv mp ly mq mb na mf nb mj nc mn nd ne nf ng bi translated"><strong class="lu iv">分类器 1: </strong>小型 vs(中型、大型、xlarge)</li><li id="9fb4" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 2: </strong>中型 vs(小型、大型、xlarge)</li><li id="e4e5" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 3: </strong>大型 vs(小型、中型、xlarge)</li><li id="c8ef" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 4: </strong> xlarge vs(小、中、大)</li></ul><p id="0729" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">如果你有非常多的班级，OVR 倾向于<strong class="lu iv">不能很好地扩展。一对一可能更好。接下来我们来谈谈这个。</strong></p><p id="e2d4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">一对一 (OVO)策略适合每对类别的单个分类器。看起来是这样的:</p><ul class=""><li id="f6b5" class="my mz iu lu b lv mp ly mq mb na mf nb mj nc mn nd ne nf ng bi translated"><strong class="lu iv">分类器 1: </strong>小型与中型</li><li id="0d73" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 2: </strong>小与大</li><li id="42d4" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 3: </strong>小与大</li><li id="fa17" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 4: </strong>中型与大型</li><li id="42b0" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 5: </strong>中型与大型</li><li id="4469" class="my mz iu lu b lv nh ly ni mb nj mf nk mj nl mn nd ne nf ng bi translated"><strong class="lu iv">分类器 6: </strong>大与 x 大</li></ul><p id="5595" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">虽然<a class="ae kz" href="https://en.wikipedia.org/wiki/Big_O_notation" rel="noopener ugc nofollow" target="_blank">的计算复杂度</a>高于 OVR 策略，但当您有大量的类时，这可能是有利的，因为每个分类器适合数据的较小子集，而 OVR 策略适合每个分类器的整个数据集。</p><p id="2301" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">最后，对于<strong class="lu iv">多标签</strong>分类，还有<code class="fe mu mv mw mx b">MultiOutputClassifier</code>。类似于<strong class="lu iv"> OVR </strong>，这适合每个类的分类器。然而，与单个预测输出相反，如果适用，这可以为<strong class="lu iv">单个预测</strong>输出<strong class="lu iv">多个类别</strong>。</p><p id="527c" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">注意:</strong>专门针对 Scikit-Learn 库，所有分类器都支持多类。你可以使用这些策略来进一步完善他们的表现。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nm"><img src="../Images/2594e4f0e0828955d72e84829bdc120c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I3i2Uh-yN6bW1Lrw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><h1 id="2ab2" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">实践中的多级分类</h1><p id="1559" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们开始吧！现在，我们已经了解了一些术语，让我们来实施一些策略。对于这个例子，我们将使用 Kaggle 上的<a class="ae kz" href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews" rel="noopener ugc nofollow" target="_blank">女装电子商务服装评论</a>数据集，它可以在<a class="ae kz" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"> CC0:公共领域</a>下使用。这是一组评论文本、评级、部门名称和每个项目的类别。我们将构建一个分类器，它可以根据评论文本预测部门名称。</p><p id="4772" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">首先，我喜欢使用<a class="ae kz" href="https://www.dataknowsall.com/sklearnpipelines.html" rel="noopener ugc nofollow" target="_blank">管道</a>来确保过程的可重复性。管道允许你将数据转换成适合机器学习的格式。我们有两个特征需要改变。我们向数据集添加了<em class="mo">评论文本</em>和<em class="mo">文本长度</em>特征。审查文本将利用<a class="ae kz" href="https://www.dataknowsall.com/bowtfidf.html" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>进行矢量化，第二个将使用<code class="fe mu mv mw mx b">MinMaxScaler</code>对数字数据进行标准化。</p><p id="befe" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated"><strong class="lu iv">注意</strong>:流程中遗漏了很多步骤。像导入数据、<a class="ae kz" href="https://www.dataknowsall.com/textcleaning.html" rel="noopener ugc nofollow" target="_blank">、文本清理</a>等等。滚动到末尾，在 GitHub 上获得该笔记本的链接。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="b43e" class="nr lb iu mx b be ns nt l nu nv">def create_pipe(clf):<br/><br/>    column_trans = ColumnTransformer(<br/>            [('Text', TfidfVectorizer(), 'Text_Processed'),<br/>             ('Text Length', MinMaxScaler(), ['text_len'])],<br/>            remainder='drop') <br/><br/>    pipeline = Pipeline([('prep',column_trans),<br/>                         ('clf', clf)])<br/><br/>    return pipeline</span></pre><p id="5231" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">接下来，我们需要将我们的数据分成用于学习的<code class="fe mu mv mw mx b">X</code>数据和目标变量<code class="fe mu mv mw mx b">y</code>，这是模型将如何学习适当的类。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="0c4f" class="nr lb iu mx b be ns nt l nu nv">X = df[['Text_Processed', 'text_len']]<br/>y = df['Department Name']<br/>y.value_counts()</span></pre><pre class="nw nn mx no bn np nq bi"><span id="27f0" class="nr lb iu mx b be ns nt l nu nv">Tops        10048<br/>Dresses      6145<br/>Bottoms      3660<br/>Intimate     1651<br/>Jackets      1002<br/>Trend         118<br/>Name: Department Name, dtype: int64</span></pre><p id="ae45" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">通过检查目标的<code class="fe mu mv mw mx b">value_counts()</code>，我们可以看到我们正在处理不平衡的数据。对最低的几类，尤其是<em class="mo">趋势</em>的观察数量，比<em class="mo">最高的</em>类少得多。我们将使用一个支持不平衡数据的分类器，但是要深入研究不平衡数据，请参见我的另一篇文章:<a class="ae kz" rel="noopener" target="_blank" href="/working-with-imbalanced-data-efbd96b3e655">在构建 ML 模型时不要陷入不平衡数据的陷阱</a>。</p><p id="a6f4" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">接下来，您应该用<code class="fe mu mv mw mx b">LabelEncoder</code>对您的目标变量进行编码。虽然 Sklearn 倾向于很好地处理基于文本的类名，但最好在训练之前将所有内容都转换成数字形式。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="a262" class="nr lb iu mx b be ns nt l nu nv">le = LabelEncoder()<br/>y = le.fit_transform(y)<br/>le.classes_</span></pre><pre class="nw nn mx no bn np nq bi"><span id="eb74" class="nr lb iu mx b be ns nt l nu nv">array(['Bottoms', 'Dresses', 'Intimate', 'Jackets', 'Tops', 'Trend'], dtype=object)</span></pre><p id="cece" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">通过打印出<code class="fe mu mv mw mx b">classes_</code>，我们可以看到哪些类是根据列表的顺序编码的。</p><p id="b832" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">当然，在训练我们的模型时，我们需要将数据集分成<strong class="lu iv">训练</strong>和<strong class="lu iv">测试分区</strong>，允许我们训练我们的模型，并在测试集上验证它，看看它的表现如何。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="83de" class="nr lb iu mx b be ns nt l nu nv"># Make training and test sets <br/>X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.33, <br/>                                                    random_state=53)</span></pre><p id="8a29" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">接下来是打印分类报告和混淆矩阵的功能。查看我关于如何最好地评估分类模型的文章以获得更多信息:<a class="ae kz" rel="noopener" target="_blank" href="/evaluating-ml-models-with-a-confusion-matrix-3fd9c3ab07dd">停止使用准确性来评估你的分类模型</a>。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="d819" class="nr lb iu mx b be ns nt l nu nv">def fit_and_print(pipeline):<br/><br/>    pipeline.fit(X_train, y_train)<br/>    y_pred = pipeline.predict(X_test)<br/><br/>    print(metrics.classification_report(y_test, y_pred, digits=3))<br/><br/>    ConfusionMatrixDisplay.from_predictions(y_test, <br/>                                            y_pred, <br/>                                            cmap=plt.cm.YlGn)<br/><br/>    plt.tight_layout()<br/>    plt.ylabel('True label')<br/>    plt.xlabel('Predicted Label')<br/>    plt.tight_layout()</span></pre><p id="2797" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">在这里，我们创建分类器的实例，对其进行拟合和评估。我们使用的是<code class="fe mu mv mw mx b">LogisticRegression</code>，它本质上是一个二元分类器。Sklearn 已经直接实现了 multi-class 作为分类器的参数，但是为了演示 One-vs-Rest 如何工作，我将使用包装器。为了实现这一点，你用我上面谈到的<code class="fe mu mv mw mx b">OneVsRestClassifier()</code>策略包装你的分类器。这个包装器指示分类器如何处理多类。我们也可以以同样的方式使用<code class="fe mu mv mw mx b">OneVsOneClassifier</code>策略。继续尝试这两种方法并比较结果。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="5f9c" class="nr lb iu mx b be ns nt l nu nv">clf = OneVsRestClassifier(LogisticRegression(random_state=42, <br/>                                             class_weight='balanced'))<br/>pipeline = create_pipe(clf)<br/>fit_and_print(pipeline)</span></pre><pre class="nw nn mx no bn np nq bi"><span id="477c" class="nr lb iu mx b be ns nt l nu nv">precision    recall  f1-score   support<br/><br/>           0      0.785     0.855     0.818      1193<br/>           1      0.905     0.844     0.874      2037<br/>           2      0.439     0.581     0.500       527<br/>           3      0.551     0.794     0.650       315<br/>           4      0.909     0.818     0.861      3361<br/>           5      0.022     0.061     0.033        33<br/><br/>    accuracy                          0.810      7466<br/>   macro avg      0.602     0.659     0.623      7466<br/>weighted avg      0.836     0.810     0.820      7466</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nx"><img src="../Images/4e937e1b459dc85b092b6d73fa988432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mwu3DdykKT3QXiV3.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="6741" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">我们可以看到，该模型列出了六个类以及每个类的相对性能。人数较多的班级表现得相当好，而人数较少的班级表现较差。用这么少的观察来概括<code class="fe mu mv mw mx b">5</code>类是不可能的。在现实生活中训练模型之前，您可能需要收集更多的数据。</p><h1 id="d5ca" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">多标签分类</h1><p id="4fff" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">接下来，我们将看一个<strong class="lu iv">多标签</strong>分类问题。我们将使用与之前相同的数据集，但这次我们将使用<strong class="lu iv">类名</strong>作为目标变量，并将<strong class="lu iv">评论文本</strong>和<strong class="lu iv">部门名</strong>的组合作为我们学习的特征。我们首先创建我们的<code class="fe mu mv mw mx b">X</code>和<code class="fe mu mv mw mx b">y</code>数据。</p><p id="dd07" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">我们需要对<strong class="lu iv">类名</strong>进行记号化，以确保当我们二进制化文本时，我们不会将单词分割成单独的字母，而是将整个单词作为一个整体来维护。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="df46" class="nr lb iu mx b be ns nt l nu nv"># Tokenize the words<br/>df['Class Name'] = df['Class Name'].apply(word_tokenize)<br/>X = df[['Text_Processed', 'Department Name']]<br/>y = df['Class Name']</span></pre><p id="fbe0" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">要创建多标签分类器，需要将目标二进制化为多标签格式。在上面的例子中，我们使用了<code class="fe mu mv mw mx b">LabelEncoder</code>，它只是在这里将目标类名转换为整数；我们将使用<code class="fe mu mv mw mx b">MultiLabelBinarizer.</code>如果我们在二进制化拟合后打印<code class="fe mu mv mw mx b">y</code>，我们可以看到它产生一个 0 和 1 的矩阵。任何时候你将一个<strong class="lu iv">矩阵</strong> ( <em class="mo"> n 维数组</em> y)传递给一个分类器(相对于一个矢量，<em class="mo"> 1d 数组</em>，它将<em class="mo">自动</em>成为一个多标签问题。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="1796" class="nr lb iu mx b be ns nt l nu nv">mlb = MultiLabelBinarizer()<br/>y = mlb.fit_transform(y)<br/>print(y)</span></pre><pre class="nw nn mx no bn np nq bi"><span id="70aa" class="nr lb iu mx b be ns nt l nu nv">[[0 0 0 ... 0 0 0]<br/> [0 1 0 ... 0 0 0]<br/> [0 1 0 ... 0 0 0]<br/> ...<br/> [0 1 0 ... 0 0 0]<br/> [0 1 0 ... 0 0 0]<br/> [0 1 0 ... 0 0 0]]</span></pre><p id="beef" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">打印这些课程</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="414f" class="nr lb iu mx b be ns nt l nu nv">print(mlb.classes_)</span></pre><pre class="nw nn mx no bn np nq bi"><span id="d4af" class="nr lb iu mx b be ns nt l nu nv">array(['Blouses', 'Dresses', 'Fine', 'Intimates', 'Jackets', 'Jeans', 'Knits', 'Layering', 'Legwear', 'Lounge', 'Outerwear', 'Pants', 'Shorts', 'Skirts', 'Sleep', 'Sweaters', 'Swim', 'Trend', 'gauge'], dtype=object)</span></pre><p id="1f4f" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">我们可以看到<strong class="lu iv">类</strong>的数量比上面的六个<strong class="lu iv">部门名称</strong>要多。接下来，我们将创建一个类似于前面的管道——但是，这次我们需要以不同的方式处理部门名称。我们将使用<code class="fe mu mv mw mx b">OneHotEncoder</code>，它在 DataFrame 中为每个类名创建一个新列。这个过程用<code class="fe mu mv mw mx b">0</code>或<code class="fe mu mv mw mx b">1</code>标记每个观察值，这取决于类名是否在该行中。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="b70f" class="nr lb iu mx b be ns nt l nu nv">def create_pipe(clf):<br/><br/>    # Create the column transfomer<br/>    column_trans = ColumnTransformer(<br/>            [('Text', TfidfVectorizer(), 'Text_Processed'),<br/>             ('Categories', OneHotEncoder(handle_unknown="ignore"), <br/>              ['Department Name'])],<br/>            remainder='drop') <br/><br/>    # Build the pipeline<br/>    pipeline = Pipeline([('prep',column_trans),<br/>                         ('clf', clf)])<br/><br/>    return pipeline</span></pre><p id="d0c9" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">与上面类似，我们将包装我们的分类器。这次我们将使用<code class="fe mu mv mw mx b">MultiOutputClassifier</code>来指示分类器如何处理多个标签。剩下的过程是一样的。</p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="e7b2" class="nr lb iu mx b be ns nt l nu nv">clf = MultiOutputClassifier(LogisticRegression(max_iter=500, <br/>                                               random_state=42))<br/><br/>pipeline = create_pipe(clf)<br/><br/>pipeline.fit(X_train, y_train)<br/>y_pred = pipeline.predict(X_test)<br/>score = metrics.f1_score(y_test, <br/>                         y_pred, <br/>                         average='macro', <br/>                         zero_division=0)<br/><br/>print(metrics.classification_report(y_test, <br/>                                    y_pred, <br/>                                    digits=3, <br/>                                    zero_division=0))</span></pre><pre class="nw nn mx no bn np nq bi"><span id="0493" class="nr lb iu mx b be ns nt l nu nv">precision    recall  f1-score   support<br/><br/>           0      0.691     0.420     0.523       973<br/>           1      1.000     1.000     1.000      1996<br/>           2      0.544     0.087     0.150       355<br/>           3      1.000     0.019     0.036        54<br/>           4      0.784     0.969     0.867       229<br/>           5      0.943     0.676     0.788       340<br/>           6      0.712     0.662     0.686      1562<br/>           7      0.500     0.022     0.042        46<br/>           8      1.000     0.174     0.296        46<br/>           9      0.708     0.568     0.630       213<br/>          10      0.875     0.393     0.542       107<br/>          11      0.847     0.683     0.756       463<br/>          12      0.765     0.263     0.391        99<br/>          13      1.000     0.808     0.894       312<br/>          14      1.000     0.039     0.076        76<br/>          15      0.711     0.404     0.516       445<br/>          16      0.958     0.590     0.730       117<br/>          17      1.000     1.000     1.000        33<br/>          18      0.544     0.087     0.150       355<br/><br/>   micro avg      0.844     0.640     0.728      7821<br/>   macro avg      0.820     0.467     0.530      7821<br/>weighted avg      0.812     0.640     0.688      7821<br/> samples avg      0.658     0.666     0.661      7821</span></pre><p id="e866" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">与上面类似，每节课观察次数少的学生成绩不会很好，但是观察次数足够多的学生往往表现很好。让我们看看分类器预测了什么。</p><p id="bbfc" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">我们将通过获取预测的类名，然后使用之前我们<em class="mo">拟合的<code class="fe mu mv mw mx b">MultiLabelBinarizer</code>中的<code class="fe mu mv mw mx b">inverse_transform</code>方法，向我们的测试数据帧添加一列。我们可以直接将它添加为一个列。为了更好地展示本例的结果，我们将只过滤那些具有多个标签的观察值的数据帧。</em></p><pre class="kk kl km kn gu nn mx no bn np nq bi"><span id="7e99" class="nr lb iu mx b be ns nt l nu nv"># Retreive the text labels from the MultiLabelBinarizer<br/>pred_labels = mlb.inverse_transform(y_pred)<br/># Append them to the DataFrame<br/>X_test['Predicted Labels'] = pred_labels<br/><br/>filter = X_test['Predicted Labels'].apply(lambda x: len(x) &gt; 1)<br/>df_mo = X_test[filter]<br/>df_mo.sample(10, random_state=24)</span></pre><pre class="nw nn mx no bn np nq bi"><span id="a251" class="nr lb iu mx b be ns nt l nu nv">Text_Processed Department Name         Predicted Labels<br/>cute summer blous top...            Tops         (Blouses, Knits)<br/>awesom poncho back ev...            Tops            (Fine, gauge)<br/>great sweater true fo...            Tops  (Fine, Sweaters, gauge)<br/>love top love fabric ...            Tops         (Blouses, Knits)<br/>love alway pilcro pan...         Bottoms           (Jeans, Pants)<br/>love shirt perfect fi...            Tops         (Blouses, Knits)<br/>simpl stylish top jea...            Tops         (Blouses, Knits)<br/>tri youll love beauti...            Tops  (Fine, Sweaters, gauge)<br/>qualiti sweater beaut...            Tops  (Fine, Sweaters, gauge)<br/>cute top got top mail...            Tops         (Blouses, Knits)</span></pre><p id="db59" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">厉害！您可以看到一些文本和部门名称以及预测的标签。如果你浏览一下结果，它们似乎对什么样的职业可能属于<em class="mo">顶端</em>和<em class="mo">底端</em>很有意义！</p><h1 id="dce6" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="fbbb" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">你有它！超越二元分类需要一点额外的知识来理解幕后发生的事情。编码分类快乐！我跳过了很多步骤，但是我已经在 GitHub 上为你准备好了所有的步骤</p><p id="5d4d" class="pw-post-body-paragraph ls lt iu lu b lv mp jv lx ly mq jy ma mb mr md me mf ms mh mi mj mt ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，考虑注册成为一名媒体会员。一个月 5 美元，让你可以无限制地访问成千上万篇文章。如果您使用 <a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener"> <em class="mo">【我的链接】</em> </a> <em class="mo">注册，我会为您赚取一小笔佣金，无需额外费用。</em></p></div></div>    
</body>
</html>