<html>
<head>
<title>Closing the Gap between Machine Learning and Human Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">缩小机器学习和人类学习之间的差距</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/closing-the-gap-between-machine-learning-and-human-learning-536720af00cd#2022-05-05">https://towardsdatascience.com/closing-the-gap-between-machine-learning-and-human-learning-536720af00cd#2022-05-05</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="a2a0" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">大型语言建模的进展</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/64bdebd12827519910a8e4bfb2235be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*heGyzAlvvMg8aQYNbUVN1w.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@hauntedeyes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Lukas </a>在<a class="ae kz" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="e5f8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">人类拥有强大的推理能力。他们能理解他人提出的问题，并给出最恰当的答案。人脑可以通过快速的数学运算来回答一个微不足道的问题，比如“如果我有10个球，买了两个罐头，每个罐头有5个球，我会有多少个球？”人类可以进行常识性的推理，比如“如果一个司机在交叉路口看到一个行人，他会怎么做？”人类有智能理解某人是否在开玩笑，并可能更深刻地理解说话者真正想说什么？</p><p id="f0a0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">问题是，我们能训练机器获得我们人类拥有的这种智能吗？近年来，科学家们在这一领域进行了大量的研究。随着深度神经网络(DNN)和大型语言模型(LLM)的发明，我们在实现这一目标方面取得了良好的进展。在本文中，我将向您介绍通过使用LLM和Google最新的PaLM[ ] (Pathways语言模型)所取得的成就。</p><p id="a13e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">首先，让我们考虑一下我们正在努力完成的任务。</p><h1 id="431a" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">NLU任务</h1><p id="f758" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">在大型神经网络在自然语言处理(NLP)中取得巨大成功之后，研究人员将注意力集中在语言理解(NLU)和生成上，而不是简单的文本处理任务。那么，我们试图用这些巨大的网络来解决什么问题呢？我在下面给出了我们寻求解决的NLU任务的简短列表。尽管下面的列表并不详尽，但它会让您对我们的目标有所了解。</p><ul class=""><li id="ba41" class="mt mu iu lc b ld le lg lh lj mv ln mw lr mx lv my mz na nb bi translated">语言翻译</li><li id="9aed" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">聊天机器人(问答)</li><li id="f0b7" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">文本摘要</li><li id="7c5b" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">语言生成</li><li id="4a98" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv my mz na nb bi translated">论证</li></ul><h1 id="9b2d" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">语言翻译</h1><p id="6279" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">从英语翻译到德语或反之亦然，更确切地说，从任何语言翻译到任何语言，一直是我们的需要。今天，有几个ML模型甚至移动应用程序使用这样的预训练模型来以非常高的准确度完成这项任务。</p><h1 id="0af1" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">聊天机器人(问答)</h1><p id="a4de" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">用自动化系统取代庞大的客户服务代表队伍一直是企业的梦想。这一点现在可以通过近乎完美的聊天机器人来实现。聊天机器人需要自然语言理解和问答能力。尽管特定领域的问答系统已经非常完善，但是开发一个面向开放领域的问答系统仍然是一个挑战。人类很快理解问题的上下文(领域)来回答问题。这就需要我们所知的LLM的少量学习[ ]。GPT-3[ ]是第一个应用少镜头学习的。最近的法学硕士如GLaM[⁴]，LaMDA[⁵]，Gopher[⁶]和威震天-图灵·nlg[⁷]都采用了少击学习。</p><h1 id="8ed1" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">文本摘要</h1><p id="8ec9" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">很多时候，我们需要创建一个长文档的摘要。虽然这是一项NLP任务，但语言理解在创建有意义的摘要时也起着有效的作用。具有<em class="nh">注意力</em>的编码器-解码器架构和基于<em class="nh">转换器</em>的架构在创建抽象和提取摘要方面都表现出了突出的成功。</p><h1 id="7224" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">语言生成</h1><p id="50a1" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">像莎士比亚那样写作是许多人的梦想。从RNN(循环神经网络)、LSTM(长短期记忆)和最新的Transformer开始的神经网络架构允许创作可以模仿阿加莎·克里斯蒂和许多著名作家作品的小说。有许多工具可用，如Arria NLG PLC[⁸]，AX Semantics[⁹]，Yseop[ ⁰]，Wordsmith[ ]，SimpleNLG[ ]，NaturalOWL[ ]和其他自然语言生成(NLG)[ ⁴].</p><h1 id="a59a" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">论证</h1><p id="a90d" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">人类有很强的常识推理能力、概念理解能力、玩琐事、同义词游戏的能力，以及根据反事实做出反应的能力。</p><p id="cbe1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这些仅仅是NLP/NLU研究进展强劲的几个领域。上述目标可以通过创建大型语言模型来实现。</p><h1 id="6149" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">创建LLM</h1><p id="bd6a" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">创建LLM的主要挑战是训练一个具有数百万和数十亿参数的超大型深度神经网络。像GLaM和LaMDA这样的模型是在一个单独的TPU v3吊舱上训练的。威震天-图灵NLG使用流水线并行在GPU集群中扩展到2240 - A100个GPU。使用多个TPU v3 pod的Gopher实现了4096个TPU v3芯片的规模。他们观察到，具有更多训练参数的更大模型改善了NLG结果。PaLM是这一类别中最新的一个，它使用谷歌的<em class="nh">路径</em>系统将训练扩展到6144个芯片，并创建了5400亿参数语言模型。它实现了57.8%的硬件FLOPs利用率的训练效率，这是迄今为止LLM实现的最高效率。Google重新设计了<em class="nh"> Transformer </em>模块，允许并行计算<em class="nh">注意力</em>和<em class="nh">前馈</em>层。这有助于为训练网络创建更好的并行策略。</p><div class="ni nj gq gs nk nl"><a href="https://medium.com/@profsarang/membership" rel="noopener follow" target="_blank"><div class="nm ab fp"><div class="nn ab no cl cj np"><h2 class="bd iv gz z fq nq fs ft nr fv fx it bi translated">通过我的推荐链接加入Medium</h2><div class="ns l"><h3 class="bd b gz z fq nq fs ft nr fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="nt l"><p class="bd b dl z fq nq fs ft nr fv fx dk translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz kt nl"/></div></div></a></div><h1 id="99e1" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">手掌训练</h1><p id="eb5a" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">他们在英语和多语言数据集的组合上训练PaLM。其中包括维基百科文章、书籍、网络文档、对话甚至GitHub代码。注PaLM也能写计算机代码，那大概是因为它在GitHub上的训练。在代码生成中，空格很重要。因此，培训师创造了一个保留空白的“无损”词汇。他们还负责处理词汇表之外的Unicode字符，并将大数字拆分成单个数字。所有这些都有助于有效的代码生成。</p><p id="8857" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在，我将讨论PaLM的一些成就。</p><h1 id="b89a" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">棕榈成就</h1><p id="78b4" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">研究人员在29个广泛使用的NLP任务上评估了PaLM。在这29个任务中的28个上，它超过了先前语言模型的少数表现。它还在多语言NLP基准测试中显示了强大的性能，包括翻译。BIG[ ⁵](超越模仿游戏)是最新的基准，包含了150多个新的语言建模任务。与地鼠、龙猫和人类相比，PaLM在58/150任务的子集上表现更好。它有效地区分了原因和结果。对于特定的上下文，它会为您的问题提供合适的答案。它可以玩同义词游戏。它可以从一篇文章中推导出相反的事实。</p><p id="d601" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在我之前提出的问题中——“如果我有10个球，买了两个罐头，每个罐头有5个球，我会有多少个球？”—单纯的答案可能不容易以其准确性说服读者。你需要给出多步算术，推理出结论性的答案是如何推导出来的。使用思维链提示，PaLM将通过为所有中间步骤生成文本来推理出答案。Google博客[1]提供了一些例子来展示这些能力。</p><p id="69db" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">PaLM可以生成一个明确的解释，甚至是一个笑话。生成这样的解释需要多步逻辑推理、世界知识和深刻的语言理解。博客[ ]中提供了一个很好的例子来说明这一点。</p><p id="8dca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">除了自然语言文本生成，代码生成是我们期望LLMs执行的另一个重要任务。代码生成可能意味着文本到代码，从一种编程语言翻译到另一种，甚至修复编译错误(代码到代码)。PaLM训练数据集包括编码示例，尽管只有5%。它已经显示出与诸如Codex 12B[ ⁶].我将再次向您推荐Google博客[ ]中的优秀示例。</p><h1 id="5813" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">结论</h1><p id="8eb8" class="pw-post-body-paragraph la lb iu lc b ld mo jv lf lg mp jy li lj mq ll lm ln mr lp lq lr ms lt lu lv in bi translated">在查看了LLM的最新发展，尤其是PaLM之后，人们可以观察到机器在学习自然语言和我们人类之间的差距正在迅速缩小。最近，Meta AI也向研究社区提供了它的OPT-175[ ⁷]十亿参数模型。我们可以希望看到机器学习和人类能力之间的差距很快缩小。</p><h1 id="c833" class="lw lx iu bd ly lz ma mb mc md me mf mg ka mh kb mi kd mj ke mk kg ml kh mm mn bi translated">参考资料:</h1><ol class=""><li id="0d68" class="mt mu iu lc b ld mo lg mp lj oa ln ob lr oc lv od mz na nb bi translated"><a class="ae kz" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank">通路语言模型(PaLM) </a></li><li id="5c3e" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">少投学习</a></li><li id="ff99" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a></li><li id="743c" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html" rel="noopener ugc nofollow" target="_blank">多面手语言模型(GLaM) </a></li><li id="2605" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html" rel="noopener ugc nofollow" target="_blank"> LaMDA </a></li><li id="6429" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://arxiv.org/abs/2112.11446" rel="noopener ugc nofollow" target="_blank">地鼠</a></li><li id="241a" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://arxiv.org/abs/2201.11990" rel="noopener ugc nofollow" target="_blank">威震天-图灵NLG </a></li><li id="83bd" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://www.arria.com/" rel="noopener ugc nofollow" target="_blank">阿里亚NLG公司</a></li><li id="2c10" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://cloud.ax-semantics.com/" rel="noopener ugc nofollow" target="_blank"> AX语义</a></li><li id="8b48" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://yseop.com/" rel="noopener ugc nofollow" target="_blank"> Yseop </a></li><li id="8cd5" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated">语言大师</li><li id="873d" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated">简单明了</li><li id="6db2" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://sourceforge.net/projects/naturalowl/" rel="noopener ugc nofollow" target="_blank">自然猫头鹰</a></li><li id="3633" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://medium.com/sciforce/a-comprehensive-guide-to-natural-language-generation-dd63a4b6e548" rel="noopener">自然语言生成(NLG) </a></li><li id="dec0" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://github.com/google/BIG-bench" rel="noopener ugc nofollow" target="_blank">大板凳</a></li><li id="2948" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://arxiv.org/abs/2107.03374.pdf" rel="noopener ugc nofollow" target="_blank">抄本12B </a></li><li id="cc58" class="mt mu iu lc b ld nc lg nd lj ne ln nf lr ng lv od mz na nb bi translated"><a class="ae kz" href="https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/" rel="noopener ugc nofollow" target="_blank"> OPT-175 </a></li></ol></div></div>    
</body>
</html>