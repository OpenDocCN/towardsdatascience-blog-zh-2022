<html>
<head>
<title>Pipelining Machine Learning Libraries With PAMI: A Nice Approach to Publish Papers in Top Data Science Conferences</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PAMI 管道化机器学习库:在顶级数据科学会议上发表论文的好方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tensorflow-keras-statsmodel-sklearn-etc-pami-c7210afa62d0#2022-03-10">https://towardsdatascience.com/tensorflow-keras-statsmodel-sklearn-etc-pami-c7210afa62d0#2022-03-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="96c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章为学生/研究人员在几个顶级数据科学会议上发表论文提供了有用的提示。本文的组织结构如下:</p><ul class=""><li id="ddf5" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">关于作者</li><li id="a702" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">“为什么我的论文在顶级会议上被拒绝？”—一个普通学生的问题</li><li id="1d87" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">联合 ML 图书馆和 PAMI 加强自己的研究工作</li><li id="c1f1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">PAMI 简介</li><li id="0f6a" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">结合 TensorFlow 和 PAMI 的降雨分析示例</li></ul><h1 id="2b3f" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">关于作者:</h1><p id="c771" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我是 RAGE Uday Kiran 教授，在日本会津大学担任副教授。我还是日本东京大学 Kitsuregawa 实验室和 NICT 的客座研究员。</p><p id="9a26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">研究兴趣:</strong>大数据分析、智能交通系统、空气污染分析、推荐系统以及用于农业和环境的 ICT。</p><p id="ad8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">研究生命:</strong></p><ul class=""><li id="f9ba" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">在顶级计算机科学会议上发表了几篇论文，如 IEEE-FUZZY、IEEE-BIGDATA、EDBT、CIKM、PAKDD、SSDBM、ICONIP、DSAA、DASFAA 和 DEXA。</li><li id="48e1" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">IEEE-FUZZ、PAKDD、DEXA、DASFAA、ICONIP 和 DSAA 的评审员。</li><li id="b7f5" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">巴基斯坦 2021 年和 ICDM 2022 年宣传联合主席</li><li id="ecca" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">DASFAA 2022 出版物共同主席。</li><li id="584b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">本书:<a class="ae mc" href="https://link.springer.com/book/10.1007/978-981-16-3964-7" rel="noopener ugc nofollow" target="_blank">周期模式挖掘(斯普林格)</a></li></ul><p id="bbce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">关于我的出版物的更多细节可以在<a class="ae mc" href="https://dblp.org/pid/11/1466.html" rel="noopener ugc nofollow" target="_blank">【1】</a>找到。</p><h1 id="bae9" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">“为什么我的论文在顶级会议上被拒绝？”—一个普通学生的问题</strong></h1><p id="12fa" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">许多(数据科学)学生这样解释他们的工作:</p><blockquote class="md me mf"><p id="15cf" class="jn jo mg jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">“我正在获取真实世界的数据，将其推入机器学习库中，调整超参数，并获得比最先进的结果更好的结果(见图 1)。”</p></blockquote><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/a8932cf3a5a2b930ff6db99b897bad0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFuBA8gns29IrHhj4rB1FQ.png"/></div></div></figure><p id="da28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解释之后，他们提出的常见问题如下:</p><blockquote class="md me mf"><p id="e57f" class="jn jo mg jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">尽管我的成绩很好，为什么我的论文总是在顶级会议上被拒绝？</p></blockquote><p id="d68c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的回答很简单，就是<strong class="jp ir">“你的方法缺乏<em class="mg">新颖性</em>”</strong>特别地，简单地将真实世界的数据推入 ML 库中不再被视为核心计算机科学研究问题。它通常被视为一个应用研究问题。事实上，提交给顶级会议的很大一部分论文也是如此，我们经常因为缺乏新颖性而拒绝这些论文。(一些读者可能会认为，为真实数据集调优超参数是一项重要的任务。是的，我们接受这项索赔。然而，我们大多数人从应用计算的角度接受这种说法，而不是从核心计算机科学的角度。这是因为现实世界的数据集比天上的星星还要多。)</p><h1 id="d41c" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">结合 ML 图书馆+ PAMI 加强自己的研究工作</strong></h1><p id="0d42" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">听到我的上述回答，一些学生问</p><blockquote class="md me mf"><p id="7c02" class="jn jo mg jp b jq jr js jt ju jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj kk ij bi translated">“有没有办法加强我的工作来提高论文被接受的几率？</p></blockquote><p id="f260" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我的回答是“是的”超越聚类、分类和预测是可能的。图 2 显示了通过应用模式挖掘技术来加强传统机器学习研究的方法。例如，应用模式挖掘算法从过去和预测的数据中提取隐藏的模式。对这些模式执行各种分析，以了解概念漂移、新趋势、减弱趋势等等。</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/0d3c995fc1e9e1d87a07501dee6df310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFMP2Bjoou3EgKat1HQCsw.png"/></div></div></figure><p id="f5c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文的其余部分旨在指导学生通过将 ML 图书馆与 PAMI 结合起来加强他们的研究工作。</p><h1 id="4971" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">什么是 PAMI？</strong></h1><p id="769a" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">PAMI 代表模式挖掘。PAMI 是一个 Python 库，它包含几个模式挖掘算法来发现隐藏在海量数据集中的知识。目前，PAMI 有 50 多种算法来寻找不同类型的基于用户兴趣的模式。关于 PAMI 的安装和使用的更多信息可以在我们之前的文章<a class="ae mc" rel="noopener" target="_blank" href="/hello-i-am-pami-937439c7984d">【2】</a>中找到。</p><h1 id="2489" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">结合 TensorFlow 和 PAMI 进行降雨分析的演练示例</strong></h1><ul class=""><li id="681e" class="kl km iq jp b jq lx ju ly jy mx kc my kg mz kk kq kr ks kt bi translated">下载雨量数据<a class="ae mc" href="https://www.u-aizu.ac.jp/~udayrage/datasets/medium/rainfall/inputfile.csv" rel="noopener ugc nofollow" target="_blank">【3】</a>和像素数据<a class="ae mc" href="https://www.u-aizu.ac.jp/~udayrage/datasets/medium/rainfall/point_location.txt" rel="noopener ugc nofollow" target="_blank">【4】</a></li><li id="0aa9" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk kq kr ks kt bi translated">导入必要的库</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="902a" class="nf la iq nb b gy ng nh l ni nj"># Import the necessary libraries</span><span id="a344" class="nf la iq nb b gy nk nh l ni nj">import pandas as pd<br/>from statsmodels.tsa.ar_model import AutoReg as AR<br/>from PAMI.extras.DF2DB import DF2DB as df2db<br/>import PAMI.extras.dbStats.transactionalDatabaseStats as tds<br/>import PAMI.extras.graph.plotLineGraphFromDictionary as plt<br/>import databasePrune as dbPrune</span></pre><ul class=""><li id="f4cb" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">对降雨数据进行自回归</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="cb00" class="nf la iq nb b gy ng nh l ni nj">#reading input csv file with 20 days of global rainfall data</span><span id="8929" class="nf la iq nb b gy nk nh l ni nj">dataframe = pd.read_csv("inputfile.csv")<br/>for i in dataframe:<br/>   #training the model using auto regression</span><span id="19cb" class="nf la iq nb b gy nk nh l ni nj">   column_values = dataframe[i].values<br/>   model = AR(column_values,1)<br/>   model_fit = model.fit()</span><span id="ec81" class="nf la iq nb b gy nk nh l ni nj"># forecast start period and end period are given as parameters for the prediction model. In this example, we are learning from rainfall data from 20 days and predicting for rainfall details for the next 300 (=320-20) days.</span><span id="70f5" class="nf la iq nb b gy nk nh l ni nj">output_df = model_fit.predict(start = 20, end = 320)<br/>output_df.insert(0,'tid',output_df.index)</span></pre><ul class=""><li id="c574" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">将预测数据转换成事务数据库</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="f704" class="nf la iq nb b gy ng nh l ni nj">#Create a transactional database by considering only those pixels/points whose predicted rainfall values are greater than 500</span><span id="0b53" class="nf la iq nb b gy nk nh l ni nj">df2db_conversion = df2db.DF2DB(output_df,500, '&gt;=', 'dense','transactionalDatabase.txt')</span></pre><ul class=""><li id="fad4" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">了解数据库的统计细节。</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="cfc4" class="nf la iq nb b gy ng nh l ni nj">#statistics of a transactional database</span><span id="c4e8" class="nf la iq nb b gy nk nh l ni nj">obj = tds.transactionalDatabaseStats("transactionalDatabase.txt", sep=',') #overrride default tab seperator<br/>obj.run()<br/>print(f'Database size : {obj.getDatabaseSize()}')<br/>print(f'Total number of items : {obj.getTotalNumberOfItems()}')<br/>print(f'Database sparsity : {obj.getSparsity()}')<br/>print(f'Minimum Transaction Size : {obj.getMinimumTransactionLength()}')<br/>print(f'Average Transaction Size : {obj.getAverageTransactionLength()}')<br/>print(f'Maximum Transaction Size : {obj.getMaximumTransactionLength()}')<br/>print(f'Standard Deviation Transaction Size : {obj.getStandardDeviationTransactionLength()}')<br/>print(f'Variance in Transaction Sizes : {obj.getVarianceTransactionLength()}')</span><span id="e7cb" class="nf la iq nb b gy nk nh l ni nj">itemFrequencies = obj.getSortedListOfItemFrequencies()<br/>transactionLength = obj.getTransanctionalLengthDistribution()</span><span id="1578" class="nf la iq nb b gy nk nh l ni nj">#Storing the statistical details in a file</span><span id="e42e" class="nf la iq nb b gy nk nh l ni nj">obj.storeInFile(itemFrequencies, 'itemFrequency.csv')<br/>obj.storeInFile(transactionLength, 'transactionSize.csv')</span><span id="c47a" class="nf la iq nb b gy nk nh l ni nj"># Visualizing the distributions of items frequencies and transactional lengths.</span><span id="01d4" class="nf la iq nb b gy nk nh l ni nj">plt.plotLineGraphFromDictionary(obj.getSortedListOfItemFrequencies(),50,'item frequencies', 'item rank', 'frequency')</span><span id="9455" class="nf la iq nb b gy nk nh l ni nj">plt.plotLineGraphFromDictionary(obj.getTransanctionalLengthDistribution(),100,'distribution of transactions', 'transaction length', 'frequency')</span></pre><ul class=""><li id="a018" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">根据 Zip 定律，高频率的项目携带的信息非常少。因此，我们从事务数据库中删除高频率项目，即频率大于或等于 275 的项目，并创建新的事务数据库。下面提供了从数据库中删除高频项的代码。请用 databasePrune.py 文件名保存下面的代码。</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="12ed" class="nf la iq nb b gy ng nh l ni nj">import statistics<br/>import validators<br/>from urllib.request import urlopen<br/>import csv<br/><br/>class databasePrune:<br/>    def __init__(self, inputFile, outputFile, threshold, sep='\t'):<br/>        self.inputFile = inputFile<br/>        self.lengthList = []<br/>        self.sep = sep<br/>        self.database = {}<br/>        self.outputFile = outputFile<br/>        self.threshold = threshold<br/><br/>    def run(self):<br/>        self.readDatabase()<br/><br/>    def readDatabase(self):<br/>        numberOfTransaction = 0<br/>        with open(self.inputFile, 'r', encoding='utf-8') as f:<br/>            for line in f:<br/>                numberOfTransaction += 1<br/>                line.strip()<br/>                temp = [i.rstrip() for i in line.split(self.sep)]<br/>                temp = [x for x in temp if x]<br/>                self.database[numberOfTransaction] = temp<br/><br/>    def getItemFrequencies(self):<br/>        self.itemFrequencies = {}<br/>        for tid in self.database:<br/>            for item in self.database[tid]:<br/>                self.itemFrequencies[item] = self.itemFrequencies.get(item, 0)<br/>                self.itemFrequencies[item] += 1<br/>        myDict = {key: val for key, val in self.itemFrequencies.items() if val &gt;= self.threshold}<br/>        return myDict.keys()<br/><br/>    def dbpruning(self, itemFrequencies):<br/>        self.dict = {}<br/>        for tid in self.database:<br/>            list1 = self.database[tid]<br/>            list2 = itemFrequencies<br/>            set_diff = set(list1) - set(list2)<br/>            self.dict[tid] = list(map(int, set_diff))<br/>        with open(self.outputFile, 'w') as f:<br/>            for key, value in self.dict.items():<br/>                if value != []:<br/>                    f.write('{0}\n'.format(str(value).replace("[", "").replace("]", "")))<br/>        return self.dict<br/><br/><br/>if __name__ == '__main__':<br/>    obj = databasePrune('transactionalDatabase.txt', 'newTransactionalDatabase.txt', 275,sep=',')<br/>    obj.run()<br/>    itemFrequencies = obj.getItemFrequencies()<br/>    items = obj.dbpruning(itemFrequencies)</span></pre><ul class=""><li id="4ef9" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">对新生成的事务数据库应用频繁模式挖掘</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="4bd5" class="nf la iq nb b gy ng nh l ni nj">from PAMI.frequentPattern.basic import FPGrowth as alg<br/>obj = alg.FPGrowth('transaction_300.csv',273,sep=',')<br/>obj.startMine()<br/>obj.savePatterns('patterns.txt')<br/>df = obj.getPatternsAsDataFrame()</span><span id="8723" class="nf la iq nb b gy nk nh l ni nj">#Print the runtime and memory consumed for the mining process</span><span id="77e6" class="nf la iq nb b gy nk nh l ni nj">print('Runtime: ' + str(obj.getRuntime()))<br/>print('Memory: ' + str(obj.getMemoryRSS()))</span></pre><ul class=""><li id="b581" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">将交易数据库中的项目映射到像素/点</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="f8b4" class="nf la iq nb b gy ng nh l ni nj">import pandas as pd</span><span id="70a9" class="nf la iq nb b gy nk nh l ni nj"># reading location points from the file</span><span id="841d" class="nf la iq nb b gy nk nh l ni nj">df = pd.read_csv('point_location.txt',header=None,sep='\t',usecols=b [0])<br/>location_values = []<br/>with open('pattern_loc.txt','w') as w:<br/>    with open('patterns.txt') as f:<br/>        # mapping respective points with location in patterns file<br/>        for line in f:<br/>            freq = line.split(":")[-1]<br/>            values = list(line.split())[:-1]<br/>            for i in values:<br/>                a = "POINT" + "(" + str(df.iloc[int(i)].values[0]) + ")"<br/>                location_values.append(a)<br/>            result = re.sub(r"[',']", "", str(location_values).replace('[', '').replace(']', '')<br/>            w.write(result + " :" + freq)</span></pre><ul class=""><li id="a9fc" class="kl km iq jp b jq jr ju jv jy kn kc ko kg kp kk kq kr ks kt bi translated">可视化生成的图案</li></ul><pre class="ml mm mn mo gt na nb nc nd aw ne bi"><span id="d8f2" class="nf la iq nb b gy ng nh l ni nj">import PAMI.extras.plotPointOnMap as plt<br/>obj = plt.plotPointOnMap('pattern_loc.txt')<br/>mmap = obj.plotPointInMap()<br/>mmap</span></pre><h1 id="3e7c" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">结论</strong></h1><p id="4e9b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">许多在数据科学领域工作的学生/研究人员面临的一个基本问题是“如何在顶级会议上发表论文？”这篇博客试图通过提供一种好的方法来解决这个问题，这种方法将从现有机器学习库中导出的输出(或预测)转换成数据库，并应用模式挖掘算法来发现数据中隐藏的模式。最后，我们用一个使用真实世界全球降雨量数据的玩具例子来说明我们的方法。</p><h1 id="bb7c" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">致谢</strong></h1><p id="aea7" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我感谢我的研究生，小姐。拉希卡小姐。P. Likitha，感谢他们为玩具实验准备必要的代码和数据。</p><h1 id="1586" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated"><strong class="ak">参考文献</strong></h1><ol class=""><li id="9368" class="kl km iq jp b jq lx ju ly jy mx kc my kg mz kk nl kr ks kt bi translated">https://dblp.org/pid/11/1466.html 的 DBLP <a class="ae mc" href="https://dblp.org/pid/11/1466.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="a87b" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk nl kr ks kt bi translated">https://towardsdatascience.com/hello-i-am-pami-937439c7984d PAMI 简介<a class="ae mc" rel="noopener" target="_blank" href="/hello-i-am-pami-937439c7984d"/></li><li id="b9c2" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk nl kr ks kt bi translated">样本降雨量数据，<a class="ae mc" href="https://www.u-aizu.ac.jp/~udayrage/datasets/medium/rainfall/inputfile.csv" rel="noopener ugc nofollow" target="_blank">https://www . u-aizu . AC . jp/~ udayrage/datasets/medium/rain/input file . CSV</a></li><li id="6be4" class="kl km iq jp b jq ku ju kv jy kw kc kx kg ky kk nl kr ks kt bi translated">交易数据库中每个项目的点信息，<a class="ae mc" href="https://www.u-aizu.ac.jp/~udayrage/datasets/medium/rainfall/point_location.txt" rel="noopener ugc nofollow" target="_blank">https://www . u-aizu . AC . jp/~ udayrage/datasets/medium/rain/point _ location . txt</a></li></ol></div></div>    
</body>
</html>