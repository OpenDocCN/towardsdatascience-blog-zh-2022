<html>
<head>
<title>Stable diffusion using Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用拥抱脸的稳定扩散</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8#2022-11-09">https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8#2022-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c514" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对稳定扩散世界的全面介绍使用<a class="ae kf" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>——<a class="ae kf" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">扩散器库</a>使用文本提示创建人工智能生成的图像</h2></div><h1 id="a29a" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">1.介绍</h1><p id="2b89" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">你可能已经看到人工智能生成的图像有所上升，这是因为潜在扩散模型的兴起。稳定扩散简单地说是一种深度学习模型，它可以在给定文本提示的情况下生成图像。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/bf1adc4be4aaff3fdb63da734f64c54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HMK6Kw0smilQ9abp.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图1:稳定扩散概述</p></figure><p id="62ca" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">从上面的图像中我们可以看到，我们可以传递一个文本提示，比如“一只戴着帽子的狗”，一个稳定的扩散模型可以生成一个代表文本的图像。相当惊人！</p><h1 id="d3ac" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">2.使用🤗扩散器库</h1><p id="595e" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">与任何python库一样，在运行它之前，我们需要遵循特定的安装步骤，下面是这些步骤的概要。</p><ol class=""><li id="4f9f" class="mp mq iq la b lb mk le ml lh mr ll ms lp mt lt mu mv mw mx bi translated"><strong class="la ir">接受许可— </strong>在使用模型之前，您需要前往<a class="ae kf" href="https://huggingface.co/CompVis/stable-diffusion-v1-4" rel="noopener ugc nofollow" target="_blank">此处</a>使用您的拥抱脸帐户登录，然后接受模型许可，下载并使用砝码。</li><li id="6dcb" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><strong class="la ir">令牌生成— </strong>如果这是你第一次使用拥抱人脸库，这听起来可能有点奇怪。您需要转到这里的<a class="ae kf" href="https://huggingface.co/settings/tokens" rel="noopener ugc nofollow" target="_blank">并生成一个令牌(最好有写权限)来下载模型。</a></li></ol><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nd"><img src="../Images/7c05eb786bb748c3979b5e44f705eeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*si8PgxlEKc6hZO_s.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2:访问令牌页面</p></figure><p id="5379" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">3.<strong class="la ir">安装hugging face hub库并登录— </strong>生成令牌后，复制它。首先，我们将使用下面的代码下载hugging face hub库。</p><p id="e6dd" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated"><strong class="la ir"> <em class="ne">注意— </em> </strong> <em class="ne">为了用代码正确地渲染这些内容，我推荐你在这里阅读</em><a class="ae kf" href="https://aayushmnit.com/posts/2022-11-02-StabeDiffusionP1/2022-11-02-StableDiffusionP1.html" rel="noopener ugc nofollow" target="_blank"><em class="ne"/></a><em class="ne">。</em></p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="94bc" class="nk kh iq ng b be nl nm l nn no">!pip install huggingface-hub==0.10.1</span></pre><p id="fa99" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">然后使用下面的代码，一旦运行它，就会出现一个小部件，粘贴您新生成的令牌，然后单击登录。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="5e96" class="nk kh iq ng b be nl nm l nn no">from huggingface_hub import notebook_login<br/>notebook_login()</span></pre><p id="704b" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">4.I <strong class="la ir">安装扩散器和变压器库— </strong>一旦该过程完成，使用以下代码安装依赖项。这将下载最新版本的<a class="ae kf" href="https://github.com/huggingface/diffusers" rel="noopener ugc nofollow" target="_blank">扩散器</a>和<a class="ae kf" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="9293" class="nk kh iq ng b be nl nm l nn no">!pip install -qq -U diffusers transformers</span></pre><p id="f484" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">就这样，现在我们准备好使用扩散器库了。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="22d4" class="kg kh iq bd ki kj nw kl km kn nx kp kq jw ny jx ks jz nz ka ku kc oa kd kw kx bi translated">3.运行稳定的扩散——高层管道</h1><p id="4581" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">第一步是从扩散器库中导入<code class="fe ob oc od ng b">StableDiffusionPipeline</code>。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="f25a" class="nk kh iq ng b be nl nm l nn no">from diffusers import StableDiffusionPipeline</span></pre><p id="4cc2" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">下一步是初始化管道以生成图像。第一次运行以下命令时，它会将模型从hugging face模型中心下载到您的本地机器上。您将需要一台GPU机器来运行这段代码。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="cd95" class="nk kh iq ng b be nl nm l nn no">pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4').to('cuda')</span></pre><p id="aef3" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">现在让我们传递一个文本提示并生成一个图像。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="6c89" class="nk kh iq ng b be nl nm l nn no"># Initialize a prompt<br/>prompt = "a dog wearing hat"<br/># Pass the prompt in the pipeline<br/>pipe(prompt).images[0]</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/54f915c62e02227ce6fa3edec5540c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*udODoCD9e37_Az8I.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3:由扩散管道产生的图像的例子。</p></figure><h1 id="184d" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">4.了解稳定扩散的核心要素</h1><p id="b3c0" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如上所示的扩散模型可以生成高质量的图像。稳定扩散模型是一种特殊的扩散模型，称为<strong class="la ir">潜在扩散</strong>模型。他们在这篇论文中首次提出了<a class="ae kf" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">用潜在扩散模型进行高分辨率图像合成</a>。原始扩散模型往往会消耗更多的内存，因此创建了潜在扩散模型，它可以在称为<code class="fe ob oc od ng b">Latent</code>空间的低维空间中进行扩散过程。在高层次上，扩散模型是机器学习模型，其被逐步训练到<code class="fe ob oc od ng b">denoise</code>随机高斯噪声，以得到结果，即<code class="fe ob oc od ng b">image</code>。在<code class="fe ob oc od ng b">latent diffusion</code>中，模型被训练在一个较低的维度上做同样的过程。</p><p id="5db7" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">潜在扩散有三个主要组成部分</p><ol class=""><li id="b8b5" class="mp mq iq la b lb mk le ml lh mr ll ms lp mt lt mu mv mw mx bi translated">一个文本编码器，在这种情况下，一个<a class="ae kf" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">剪辑文本编码器</a></li><li id="eb65" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">自动编码器，在这种情况下，变型自动编码器也称为VAE</li><li id="6ecd" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">一个<a class="ae kf" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U形网</a></li></ol><p id="ede6" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们深入这些组件，了解它们在扩散过程中的用途。我将尝试通过以下三个阶段来解释这些组成部分</p><ol class=""><li id="61c0" class="mp mq iq la b lb mk le ml lh mr ll ms lp mt lt mu mv mw mx bi translated"><strong class="la ir"> <em class="ne">基础知识:什么进入组件，什么从组件中出来</em></strong>——这是理解“整个游戏”的<a class="ae kf" href="https://www.fast.ai/posts/2016-10-08-teaching-philosophy.html" rel="noopener ugc nofollow" target="_blank">自上而下学习方法</a>的一个重要且关键的部分</li><li id="276c" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><strong class="la ir"> <em class="ne">更深层次的解释运用🤗代码。</em></strong>——这一部分将提供对模型使用代码产生什么的更多理解</li><li id="03d0" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><strong class="la ir"> <em class="ne">它们在稳定扩散管道中的作用是什么</em> </strong> —这将让你对这种成分在稳定扩散过程中的作用有一个直观的认识。这将有助于你对扩散过程的直觉</li></ol><h1 id="2f53" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">5.剪辑文本编码器</h1><h2 id="e260" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">5.1基础知识—什么进出组件？</h2><p id="6428" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">CLIP(对比语言-图像预训练)文本编码器将文本作为输入，并生成潜在空间接近的文本嵌入，就像通过CLIP模型对图像进行编码一样。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi or"><img src="../Images/401593f8496db0d0acc857c31e5834c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lG2abynBvLOYGBgc.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图4:剪辑文本编码器</p></figure><h2 id="8051" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">2.2使用更深入的解释🤗密码</h2><p id="55f5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">任何机器学习模型都不理解文本数据。对于任何理解文本数据的模型，我们都需要将这个文本转换成保存文本含义的数字，称为<code class="fe ob oc od ng b">embeddings</code>。将文本转换成数字的过程可以分为两个部分。<strong class="la ir"> <em class="ne">记号化器</em></strong>——将每个单词分解成子单词，然后使用查找表将它们转换成数字<br/> 2。<strong class="la ir"> <em class="ne"> Token_To_Embedding编码器</em></strong>——将那些数字子词转换成包含该文本表示的表示</p><p id="2eca" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">我们通过代码来看一下。我们将从导入相关的工件开始。</p><p id="ee8d" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated"><strong class="la ir"> <em class="ne">注— </em> </strong> <em class="ne">要用代码正确地渲染这些内容，我推荐你在这里阅读</em><a class="ae kf" href="https://aayushmnit.com/posts/2022-11-05-StableDiffusionP2/2022-11-05-StableDiffusionP2.html" rel="noopener ugc nofollow" target="_blank"><em class="ne"/></a><em class="ne">。</em></p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="df6f" class="nk kh iq ng b be nl nm l nn no">import torch, logging<br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>## Initiating tokenizer and encoder.<br/>tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")</span></pre><p id="f65b" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们初始化一个提示符并对其进行标记。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="1634" class="nk kh iq ng b be nl nm l nn no">prompt = ["a dog wearing hat"]<br/>tok =tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt") <br/>print(tok.input_ids.shape)<br/>tok</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi os"><img src="../Images/9145b1c26602192c5ccf417a1f5370e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_oFRDav1Yq6EhL0OraUqWg.png"/></div></div></figure><p id="d2da" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">A <code class="fe ob oc od ng b">tokenizer</code>以字典的形式返回两个对象- <br/> 1。<code class="fe ob oc od ng b"><strong class="la ir"><em class="ne">input_ids</em></strong></code> -一个大小为1x77的张量作为一个提示被传递并填充到77的最大长度。<code class="fe ob oc od ng b"><em class="ne">49406</em></code>是开始标记，<code class="fe ob oc od ng b"><em class="ne">320</em></code>是给予单词“a”的标记，<code class="fe ob oc od ng b"><em class="ne">1929</em></code>是给予单词“dog”的标记，<code class="fe ob oc od ng b"><em class="ne">3309</em></code>是给予单词“wear”的标记，<code class="fe ob oc od ng b"><em class="ne">3801</em></code>是给予单词“hat”的标记，<code class="fe ob oc od ng b"><em class="ne">49407</em></code>是文本结束标记，重复直到填充长度为77。<br/> 2。<code class="fe ob oc od ng b"><strong class="la ir"><em class="ne">attention_mask</em></strong></code> - <code class="fe ob oc od ng b">1</code>表示嵌入值，<code class="fe ob oc od ng b">0</code>表示填充。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="fac7" class="nk kh iq ng b be nl nm l nn no">for token in list(tok.input_ids[0,:7]): <br/>    print(f"{token}:{tokenizer.convert_ids_to_tokens(int(token))}")</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ot"><img src="../Images/d832a1e05d4a4f46dff46305ee527382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aYze8-Ilc3lzcgIchBg2w.png"/></div></div></figure><p id="153d" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">所以，让我们看一下<code class="fe ob oc od ng b">Token_To_Embedding Encoder</code>，它接受由记号赋予器生成的<code class="fe ob oc od ng b">input_ids</code>,并将它们转换成嵌入-</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="6f42" class="nk kh iq ng b be nl nm l nn no">emb = text_encoder(tok.input_ids.to("cuda"))[0].half()<br/>print(f"Shape of embedding : {emb.shape}")<br/>emb</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ou"><img src="../Images/3eac8bddd01cdaf58bc489b95e19b9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmrK7CQ3jOwOGVp5dNyCfA.png"/></div></div></figure><p id="75a2" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">正如我们在上面看到的，每个大小为1x77的标记化输入现在已经被转换为1x77x768形状嵌入。所以，每个单词都在一个768维的空间中被表现出来。</p><h1 id="1c8e" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">5.3他们在稳定扩散管道中的作用是什么</h1><p id="4980" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">稳定扩散仅使用剪辑训练的编码器来将文本转换为嵌入。这成为U-net的输入之一。在高层次上，CLIP使用图像编码器和文本编码器来创建在潜在空间中相似的嵌入。这种相似性被更精确地定义为<a class="ae kf" href="https://arxiv.org/abs/1807.03748" rel="noopener ugc nofollow" target="_blank">对比目标</a>。关于CLIP如何训练的更多信息，请参考这个<a class="ae kf" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank">开放AI博客</a>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/e64fa9a7e5b65ba9413c9220057eaa11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*4XvUWf4NK7gvDQjP.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图5: CLIP预先训练了一个图像编码器和一个文本编码器，以预测在我们的数据集中哪些图像与哪些文本配对。信用— <a class="ae kf" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a></p></figure><h1 id="2049" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">6.VAE —变分自动编码器</h1><h2 id="e119" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">6.1基础知识—什么进出组件？</h2><p id="6bdc" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">一个自动编码器包含两部分- <br/> 1。<code class="fe ob oc od ng b">Encoder</code>将图像作为输入，并将其转换为低维潜在表示<br/> 2。<code class="fe ob oc od ng b">Decoder</code>获取潜像并将其转换回图像</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ow"><img src="../Images/291731162277ed2bacca600ba0c21169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kKahWKJ-kGJ4TAin.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图6:一个变化的自动编码器。原鸟<a class="ae kf" href="https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg" rel="noopener ugc nofollow" target="_blank"> pic功劳</a>。</p></figure><p id="9696" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">正如我们在上面看到的，编码器就像一个压缩器，将图像压缩到更低的维度，解码器从压缩版本中重新创建原始图像。</p><blockquote class="ox oy oz"><p id="a7e0" class="ky kz ne la b lb mk jr ld le ml ju lg pa mm lj lk pb mn ln lo pc mo lr ls lt ij bi translated"><strong class="la ir">注意:</strong>编解码压缩-解压缩不是无损的。</p></blockquote><h2 id="68ab" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">6.2更深入的解释使用🤗密码</h2><p id="b49c" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们通过代码来看看VAE。我们将从导入所需的库和一些辅助函数开始。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="954c" class="nk kh iq ng b be nl nm l nn no">## To import an image from a URL <br/>from fastdownload import FastDownload  <br/>## Imaging  library <br/>from PIL import Image <br/>from torchvision import transforms as tfms  <br/>## Basic libraries <br/>import numpy as np <br/>import matplotlib.pyplot as plt <br/>%matplotlib inline  <br/>## Loading a VAE model <br/>from diffusers import AutoencoderKL <br/>vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/>def load_image(p):<br/>   '''     <br/>   Function to load images from a defined path     <br/>   '''    <br/>    return Image.open(p).convert('RGB').resize((512,512))<br/>def pil_to_latents(image):<br/>    '''     <br/>    Function to convert image to latents     <br/>    '''     <br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0   <br/>    init_image = init_image.to(device="cuda", dtype=torch.float16)<br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215     <br/>    return init_latent_dist  <br/>def latents_to_pil(latents):     <br/>    '''     <br/>    Function to convert latents to images     <br/>    '''     <br/>    latents = (1 / 0.18215) * latents     <br/>    with torch.no_grad():         <br/>        image = vae.decode(latents).sample     <br/>    <br/>    image = (image / 2 + 0.5).clamp(0, 1)     <br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()      <br/>    images = (image * 255).round().astype("uint8")     <br/>    pil_images = [Image.fromarray(image) for image in images]        <br/>    return pil_images</span></pre><p id="60a6" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">我们从网上下载一张图片吧。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="53ce" class="nk kh iq ng b be nl nm l nn no">p = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')<br/>img = load_image(p)<br/>print(f"Dimension of this image: {np.array(img).shape}")<br/>img</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pd"><img src="../Images/d6cd14200b5f32360f829cc6da49f7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*FcVbFfR0Cdj4--FKMxFlNw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图7:原鸟<a class="ae kf" href="https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg" rel="noopener ugc nofollow" target="_blank"> pic信用</a>。</p></figure><p id="3ca2" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">现在让我们使用VAE编码器来压缩这个图像，我们将使用<code class="fe ob oc od ng b">pil_to_latents</code>辅助函数。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="6cc2" class="nk kh iq ng b be nl nm l nn no">latent_img = pil_to_latents(img)<br/>print(f"Dimension of this latent representation: {latent_img.shape}")</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pe"><img src="../Images/367c3f231281dff5df1ce2b5617ed374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rbD7ix_5EKVY9wScRl1x-g.png"/></div></div></figure><p id="1507" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">我们可以看到VAE是如何将一个3 x 512 x 512的图像压缩成4 x 64 x 64的图像的。这是48倍的压缩比！让我们想象这四个潜在表征的渠道。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="8160" class="nk kh iq ng b be nl nm l nn no">fig, axs = plt.subplots(1, 4, figsize=(16, 4))<br/>for c in range(4):<br/>    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pf"><img src="../Images/946541b1bc03d099b25fd83c9af7b74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gPRqgpdKRZlQfurR.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图8:来自VAE编码器的潜在表示的可视化。</p></figure><p id="5e7f" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">这种潜在的表示在理论上应该捕捉到很多关于原始图像的信息。让我们对这个表示使用解码器，看看我们得到什么。为此，我们将使用<code class="fe ob oc od ng b">latents_to_pil</code>助手函数。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="79ec" class="nk kh iq ng b be nl nm l nn no">decoded_img = latents_to_pil(latent_img)<br/>decoded_img[0]</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/c80ebe54bdd01ca807f4ba6ff5fa1aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*p2A9QXZdlXB-aHk5.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图9:来自VAE解码器的解码潜在表示的可视化。</p></figure><p id="8346" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">从上图中我们可以看到，VAE解码器能够从48x压缩的潜在图像中恢复原始图像。令人印象深刻！</p><blockquote class="ox oy oz"><p id="0923" class="ky kz ne la b lb mk jr ld le ml ju lg pa mm lj lk pb mn ln lo pc mo lr ls lt ij bi translated"><strong class="la ir">注意:</strong>如果你仔细看解码图像，它与原始图像不一样，注意眼睛周围的差异。这就是为什么VAE编码器/解码器不是无损压缩。</p></blockquote><h2 id="c65b" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">6.3他们在稳定扩散管道中的角色是什么</h2><p id="3eec" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">稳定的扩散可以在没有VAE分量的情况下完成，但是我们使用VAE的原因是为了减少生成高分辨率图像的计算时间。潜在扩散模型可以在由VAE编码器产生的这个<em class="ne">潜在空间</em>中执行扩散，并且一旦我们有了由扩散过程产生的我们期望的潜在输出，我们可以通过使用VAE解码器将它们转换回高分辨率图像。为了更直观地理解变体自动编码器以及它们是如何被训练的，请阅读Irhum Shafkat的博客。</p><h1 id="c8fa" class="kg kh iq bd ki kj kk kl km kn ko kp kq jw kr jx ks jz kt ka ku kc kv kd kw kx bi translated">7.u网模型</h1><h2 id="1163" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">7.1基础知识—什么进出组件？</h2><p id="0fc2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">U-Net模型接受两个输入- <br/> 1。<code class="fe ob oc od ng b">Noisy latent</code>或<code class="fe ob oc od ng b">Noise</code> -噪声潜伏是由VAE编码器(在提供初始图像的情况下)产生的具有附加噪声的潜伏，或者在我们想要仅基于文本描述创建随机新图像的情况下，它可以接受纯噪声输入<br/> 2。<code class="fe ob oc od ng b">Text embeddings</code> -基于剪辑的嵌入由输入的文本提示生成</p><p id="0f5b" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">U-Net模型的输出是输入噪声潜势包含的预测噪声残差。换句话说，它预测从噪声潜伏时间中减去的噪声，以返回原始的去噪声潜伏时间。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pg"><img src="../Images/55f7d11c10056f784a8a2a172dd213e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4CMZbQvwXaSjQRRX.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图10:一个U网表示。</p></figure><h2 id="e7cd" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">7.2更深入的解释使用🤗密码</h2><p id="c0f2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们通过代码开始看U-Net。我们将从导入所需的库和启动我们的U-Net模型开始。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="04b2" class="nk kh iq ng b be nl nm l nn no">from diffusers import UNet2DConditionModel, LMSDiscreteScheduler<br/>## Initializing a scheduler<br/>scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)<br/>## Setting number of sampling steps<br/>scheduler.set_timesteps(51)<br/>## Initializing the U-Net model<br/>unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")</span></pre><p id="18c5" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">您可能已经从上面的代码中注意到，我们不仅导入了<code class="fe ob oc od ng b">unet</code>，还导入了<code class="fe ob oc od ng b">scheduler</code>。<code class="fe ob oc od ng b">schedular</code>的目的是确定在扩散过程的给定步骤中有多少噪声添加到潜在噪声中。让我们来看一下schedular函数</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/852e20bb88c7491d3a15e4d41b57eada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/0*plvYlrxI8jw34z9g.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图11:采样计划可视化。</p></figure><p id="465c" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">扩散过程遵循这个采样时间表，我们从高噪声开始，并逐渐对图像去噪。让我们想象一下这个过程-</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="e5ac" class="nk kh iq ng b be nl nm l nn no">noise = torch.randn_like(latent_img) # Random noise<br/>fig, axs = plt.subplots(2, 3, figsize=(16, 12))<br/>for c, sampling_step in enumerate(range(0,51,10)):<br/>    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))<br/>    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])<br/>    axs[c//3][c%3].set_title(f"Step - {sampling_step}")</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pi"><img src="../Images/11b44c411c1f90ff486c80bfcfdd7736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KczbjUFsEQRXlA5h.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图12:通过步骤的噪声进展。</p></figure><p id="c970" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们看看U-Net是如何去除图像中的噪声的。让我们从给图像添加一些噪声开始。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="e1d3" class="nk kh iq ng b be nl nm l nn no">encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]])) latents_to_pil(encoded_and_noised)[0]</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f7c4518900d640351a388fec8c634879.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*lHl3YGQ28Ge07ToY.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图13:馈入U-Net的噪声输入。</p></figure><p id="fed4" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们浏览一下U-Net，试着去噪这个图像。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="0bbd" class="nk kh iq ng b be nl nm l nn no">## Unconditional textual prompt<br/>prompt = [""]<br/>## Using clip model to get embeddings<br/>text_input = tokenizer(prompt, padding="max_length", max_length=tokenizer.model_max_length, truncation=True, return_tensors="pt")<br/>with torch.no_grad(): <br/>    text_embeddings = text_encoder(<br/>        text_input.input_ids.to("cuda")<br/>    )[0]<br/>    <br/>## Using U-Net to predict noise    <br/>latent_model_input = torch.cat([encoded_and_noised.to("cuda").float()]).half()<br/>with torch.no_grad():<br/>    noise_pred = unet(<br/>        latent_model_input,40,encoder_hidden_states=text_embeddings<br/>    )["sample"]<br/>## Visualize after subtracting noise <br/>latents_to_pil(encoded_and_noised- noise_pred)[0]</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/059bca6fe75c5b5dd9a27eb8b3ed11b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*egUdeUd-AoR_LwyF.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图14:来自U-Net的去噪声输出</p></figure><p id="16fa" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">正如我们在上面看到的，U-Net的输出比通过的原始噪声输入更清晰。</p><h2 id="9f3c" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">7.3他们在稳定扩散管道中的角色是什么</h2><p id="213f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">潜在扩散使用U-Net通过几个步骤逐渐减去潜在空间中的噪声，以达到所需的输出。每走一步，添加到延迟中的噪声量就会减少，直到我们得到最终的去噪输出。u-网最初是由<a class="ae kf" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">本文</a>介绍的，用于生物医学图像分割。U-Net具有由ResNet块组成的编码器和解码器。稳定扩散U-Net还具有交叉注意层，使它们能够根据所提供的文本描述来调节输出。交叉注意层通常在ResNet块之间被添加到U-Net的编码器和解码器部分。你可以在这里了解更多关于这个U-Net架构<a class="ae kf" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq" rel="noopener ugc nofollow" target="_blank">的信息。</a></p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="5d4a" class="kg kh iq bd ki kj nw kl km kn nx kp kq jw ny jx ks jz nz ka ku kc oa kd kw kx bi translated">8.把所有东西放在一起，理解扩散过程</h1><p id="8c67" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">如上所述，我展示了如何安装🤗扩散器库开始生成您自己的人工智能图像和稳定扩散管道的关键组件，即剪辑文本编码器，VAE和U-Net。现在，我们将尝试把这些关键部分放在一起，并做一个产生图像的扩散过程的演练。</p><h2 id="f00c" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">8.1概述-扩散过程</h2><p id="26db" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">稳定扩散模型采用文本输入和种子。文本输入然后通过剪辑模型以生成大小为77×768的文本嵌入，并且种子用于生成大小为4×64×64的高斯噪声，该高斯噪声成为第一潜像表示。</p><blockquote class="ox oy oz"><p id="323d" class="ky kz ne la b lb mk jr ld le ml ju lg pa mm lj lk pb mn ln lo pc mo lr ls lt ij bi translated">注意—您会注意到在图像中提到了一个额外的维度(1x ),如用于文本嵌入的1x77x768，这是因为它表示批量大小为1。</p></blockquote><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi pj"><img src="../Images/362376c3b3cc955e6090b8888258a9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z5eQUBRBVtgD3Vgv.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图15:扩散过程。</p></figure><p id="7dd2" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">接下来，U-Net迭代地去除随机潜像表示的噪声，同时以文本嵌入为条件。U-Net的输出是预测的噪声残差，该噪声残差然后被用于通过调度器算法来计算条件延迟。这个去噪和文本调节的过程重复N次(我们将使用50次)以检索更好的潜像表示。一旦该过程完成，潜像表示(4x64x64)由VAE解码器解码，以检索最终的输出图像(3x512x512)。</p><blockquote class="ox oy oz"><p id="2898" class="ky kz ne la b lb mk jr ld le ml ju lg pa mm lj lk pb mn ln lo pc mo lr ls lt ij bi translated">注意——这种迭代去噪是获得良好输出图像的重要步骤。典型的步长范围是30–80。然而，有<a class="ae kf" href="https://arxiv.org/abs/2202.00512" rel="noopener ugc nofollow" target="_blank">最近的论文</a>声称通过使用蒸馏技术将其减少到4-5步。</p></blockquote><h2 id="1021" class="of kh iq bd ki og oh dn km oi oj dp kq lh ok ol ks ll om on ku lp oo op kw oq bi translated">8.2通过代码理解扩散过程</h2><p id="cd88" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们从导入所需的库和助手函数开始。上面已经解释了所有这些。</p><p id="4659" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated"><strong class="la ir"> <em class="ne">注意— </em> </strong> <em class="ne">为了用代码正确地渲染这些内容，我推荐你在这里阅读</em> <a class="ae kf" href="https://aayushmnit.com/posts/2022-11-07-StableDiffusionP3/2022-11-07-StableDiffusionP3.html" rel="noopener ugc nofollow" target="_blank"> <em class="ne">。</em> </a></p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="5cbd" class="nk kh iq ng b be nl nm l nn no">import torch, logging<br/>## disable warnings<br/>logging.disable(logging.WARNING)  <br/>## Imaging  library<br/>from PIL import Image<br/>from torchvision import transforms as tfms<br/>## Basic libraries<br/>import numpy as np<br/>from tqdm.auto import tqdm<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from IPython.display import display<br/>import shutil<br/>import os<br/>## For video display<br/>from IPython.display import HTML<br/>from base64 import b64encode<br/><br/>## Import the CLIP artifacts <br/>from transformers import CLIPTextModel, CLIPTokenizer<br/>from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler<br/>## Initiating tokenizer and encoder.<br/>tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)<br/>text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")<br/>## Initiating the VAE<br/>vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae", torch_dtype=torch.float16).to("cuda")<br/>## Initializing a scheduler and Setting number of sampling steps<br/>scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear", num_train_timesteps=1000)<br/>scheduler.set_timesteps(50)<br/>## Initializing the U-Net model<br/>unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")<br/>## Helper functions<br/>def load_image(p):<br/>    '''<br/>    Function to load images from a defined path<br/>    '''<br/>    return Image.open(p).convert('RGB').resize((512,512))<br/>def pil_to_latents(image):<br/>    '''<br/>    Function to convert image to latents<br/>    '''<br/>    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0<br/>    init_image = init_image.to(device="cuda", dtype=torch.float16) <br/>    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215<br/>    return init_latent_dist<br/>def latents_to_pil(latents):<br/>    '''<br/>    Function to convert latents to images<br/>    '''<br/>    latents = (1 / 0.18215) * latents<br/>    with torch.no_grad():<br/>        image = vae.decode(latents).sample<br/>    image = (image / 2 + 0.5).clamp(0, 1)<br/>    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()<br/>    images = (image * 255).round().astype("uint8")<br/>    pil_images = [Image.fromarray(image) for image in images]<br/>    return pil_images<br/>def text_enc(prompts, maxlen=None):<br/>    '''<br/>    A function to take a texual promt and convert it into embeddings<br/>    '''<br/>    if maxlen is None: maxlen = tokenizer.model_max_length<br/>    inp = tokenizer(prompts, padding="max_length", max_length=maxlen, truncation=True, return_tensors="pt") <br/>    return text_encoder(inp.input_ids.to("cuda"))[0].half()</span></pre><p id="c941" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">下面的代码是<code class="fe ob oc od ng b"><a class="ae kf" href="https://github.com/huggingface/diffusers/blob/269109dbfbbdbe2800535239b881e96e1828a0ef/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py" rel="noopener ugc nofollow" target="_blank">StableDiffusionPipeline.from_pretrained</a></code>函数中的精简版本，显示了扩散过程的重要部分。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="8c63" class="nk kh iq ng b be nl nm l nn no">def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):<br/>    """<br/>    Diffusion process to convert prompt to image<br/>    """<br/>    <br/>    # Defining batch size<br/>    bs = len(prompts) <br/>    <br/>    # Converting textual prompts to embedding<br/>    text = text_enc(prompts) <br/>    <br/>    # Adding an unconditional prompt , helps in the generation process<br/>    uncond =  text_enc([""] * bs, text.shape[1])<br/>    emb = torch.cat([uncond, text])<br/>    <br/>    # Setting the seed<br/>    if seed: torch.manual_seed(seed)<br/>    <br/>    # Initiating random noise<br/>    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))<br/>    <br/>    # Setting number of steps in scheduler<br/>    scheduler.set_timesteps(steps)<br/>    <br/>    # Adding noise to the latents <br/>    latents = latents.to("cuda").half() * scheduler.init_noise_sigma<br/>    <br/>    # Iterating through defined steps<br/>    for i,ts in enumerate(tqdm(scheduler.timesteps)):<br/>        # We need to scale the i/p latents to match the variance<br/>        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)<br/>        <br/>        # Predicting noise residual using U-Net<br/>        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)<br/>            <br/>        # Performing Guidance<br/>        pred = u + g*(t-u)<br/>        <br/>        # Conditioning  the latents<br/>        latents = scheduler.step(pred, ts, latents).prev_sample<br/>        <br/>        # Saving intermediate images<br/>        if save_int: <br/>            if not os.path.exists(f'./steps'):<br/>                os.mkdir(f'./steps')<br/>            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')<br/>            <br/>    # Returning the latent representation to output an image of 3x512x512<br/>    return latents_to_pil(latents)</span></pre><p id="e958" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们看看这个函数是否如预期的那样工作。</p><pre class="lv lw lx ly gt nf ng nh bn ni nj bi"><span id="3b47" class="nk kh iq ng b be nl nm l nn no">images = prompt_2_img(["A dog wearing a hat", "a photograph of an astronaut riding a horse"], save_int=False)<br/>for img in images:display(img)</span></pre><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e6a8a3f547d68fb7e7f05cf6325c728f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*mUmPjuPxYcONUhCp.png"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6484f7b8384838451dd60696dd9507cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*UOs9da2t6TV6WFfD.png"/></div></figure><p id="0343" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">看起来它正在工作！因此，让我们更深入地了解该函数的超参数。<br/> 1。<code class="fe ob oc od ng b">prompt</code> -这是我们用来生成图像的文本提示。类似于我们在第1部分<br/> 2中看到的<code class="fe ob oc od ng b">pipe(prompt)</code>函数。<code class="fe ob oc od ng b">g</code>或<code class="fe ob oc od ng b">guidance scale</code>——这是一个决定图像应该多接近文本提示的值。这与一种名为<a class="ae kf" href="https://benanne.github.io/2022/05/26/guidance.html" rel="noopener ugc nofollow" target="_blank">分类器自由引导</a>的技术有关，该技术提高了生成图像的质量。指导比例值越高，越接近文本提示<br/> 3。<code class="fe ob oc od ng b">seed</code> -设置生成初始高斯噪声潜伏时间的种子<br/> 4。<code class="fe ob oc od ng b">steps</code> -生成最终延迟所采取的去噪步骤数。<br/> 5。<code class="fe ob oc od ng b">dim</code> -图像的尺寸，为了简单起见，我们目前正在生成正方形图像，因此只需要一个值<br/> 6。<code class="fe ob oc od ng b">save_int</code> -这是可选的，一个布尔标志，如果我们想保存中间潜像，有助于可视化。</p><p id="63a9" class="pw-post-body-paragraph ky kz iq la b lb mk jr ld le ml ju lg lh mm lj lk ll mn ln lo lp mo lr ls lt ij bi translated">让我们想象一下从噪声到最终图像的生成过程。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dbbea614dfa77bacb682ed9137fe4d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*sRMNCDKvviQwRrNa.gif"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图16:去噪步骤可视化。</p></figure></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="853c" class="kg kh iq bd ki kj nw kl km kn nx kp kq jw ny jx ks jz nz ka ku kc oa kd kw kx bi translated">9.结论</h1><p id="4804" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">我希望你喜欢阅读它，并随时使用我的代码，并尝试生成您的图像。此外，如果对代码或博客帖子有任何反馈，请随时联系LinkedIn或给我发电子邮件，地址是aayushmnit@gmail.com。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="7a24" class="kg kh iq bd ki kj nw kl km kn nx kp kq jw ny jx ks jz nz ka ku kc oa kd kw kx bi translated">10.参考</h1><ul class=""><li id="5d82" class="mp mq iq la b lb lc le lf lh pk ll pl lp pm lt pn mv mw mx bi translated"><a class="ae kf" href="https://www.fast.ai/posts/part2-2022-preview.html" rel="noopener ugc nofollow" target="_blank"> Fast.ai课程——《从深度学习基础到稳定扩散》前两节</a></li><li id="9a9f" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt pn mv mw mx bi translated"><a class="ae kf" href="https://huggingface.co/blog/stable_diffusion" rel="noopener ugc nofollow" target="_blank">🧨扩散器的稳定扩散</a></li><li id="7dc8" class="mp mq iq la b lb my le mz lh na ll nb lp nc lt pn mv mw mx bi translated"><a class="ae kf" href="https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">稳定扩散世界入门</a></li></ul></div></div>    
</body>
</html>