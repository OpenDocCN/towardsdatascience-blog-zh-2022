<html>
<head>
<title>NLP Transformers pipelines with ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带 ONNX 的 NLP 变压器管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-transformers-pipelines-with-onnx-9b890d015723#2022-03-09">https://towardsdatascience.com/nlp-transformers-pipelines-with-onnx-9b890d015723#2022-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="88ae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何用 ONNX 构建真实世界的 NLP 应用，而不仅仅是对张量进行基准测试。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3201a2a89d9110e6df1f758293360182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6mCdv1OaLNYFIgSzPeA2w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@realaxer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> T K </a>在<a class="ae kv" href="https://unsplash.com/s/photos/pipe?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="6e85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ONNX 是用于神经网络的机器学习格式。它是可移植的、开源的，在不牺牲准确性的情况下提高推理速度，真的很棒。</p><p id="49cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我找到了很多关于 ONNX 基准的文章，但是没有一篇文章提供了一种将它用于实际 NLP 任务的便捷方法。我也在抱抱脸的<a class="ae kv" href="http://hf.co/join/discord" rel="noopener ugc nofollow" target="_blank"> discord 服务器</a>上回答了很多关于 ONNX 的问题，以及如何最好的使用它进行 NLP。</p><p id="a27d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么我决定写这篇博文:我想帮助你用 ONNX 和令人敬畏的变形金刚管道得到最好的结果。</p><p id="abad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本教程将向您展示如何将 Hugging Face 的 NLP 变形金刚模型导出到 ONNX，以及如何将导出的模型与适当的变形金刚管道一起使用。我使用一个<code class="fe ls lt lu lv b">Named Entity Recognition (NER)</code>模型作为例子，但是它并不局限于 NER。(在<a class="ae kv" href="https://umagunturi789.medium.com/everything-you-need-to-know-about-named-entity-recognition-2a136f38c08f" rel="noopener">这篇伟大的文章</a>中有更多关于 NER 的内容)</p><p id="c362" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有代码片段都可以在相关的<a class="ae kv" href="https://github.com/ChainYo/transformers-pipeline-onnx" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>的专用笔记本中找到。所以不要担心复制它，只需在阅读这篇博文的同时克隆存储库并运行笔记本。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="7547" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">📦作业环境</h1><p id="5ac1" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">首先，您需要安装所有必需的依赖项。建议使用隔离环境，以避免冲突。</p><p id="cee0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该项目需要 Python 3.8 或更高版本。您可以使用任何您想要的包管理器。教程我推荐用<a class="ae kv" href="https://docs.conda.io/en/latest/miniconda.html" rel="noopener ugc nofollow" target="_blank"> <em class="na">康达</em> </a>。所有需要的依赖关系都列在<code class="fe ls lt lu lv b">requirements.txt</code>文件中。要安装它们，请运行以下命令:</p><pre class="kg kh ki kj gt nb lv nc nd aw ne bi"><span id="4b97" class="nf me iq lv b gy ng nh l ni nj">$ conda create -y -n hf-onnx python=3.8<br/>$ conda activate hf-onnx </span><span id="2be7" class="nf me iq lv b gy nk nh l ni nj">$ git clone <a class="ae kv" href="https://github.com/ChainYo/transformers-pipeline-onnx.git" rel="noopener ugc nofollow" target="_blank">https://github.com/ChainYo/transformers-pipeline-onnx.git</a><br/>$ cd transformers-pipeline-onnx </span><span id="4eb4" class="nf me iq lv b gy nk nh l ni nj">$ pip install -r requirements.txt</span></pre></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="c845" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">🍿将模型导出到 ONNX</h1><p id="b412" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">对于这个例子，我们可以使用任何来自拥抱脸库的<code class="fe ls lt lu lv b">TokenClassification</code>模型，因为我们试图解决的任务是<code class="fe ls lt lu lv b">NER</code>。</p><p id="e57f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我选择了<code class="fe ls lt lu lv b"><a class="ae kv" href="https://huggingface.co/dslim/bert-base-NER" rel="noopener ugc nofollow" target="_blank">dslim/bert-base-NER</a></code>模型，因为它是一个<code class="fe ls lt lu lv b">base</code>模型，这意味着在 CPU 上的计算时间适中。另外，<code class="fe ls lt lu lv b">BERT</code>建筑对于<code class="fe ls lt lu lv b">NER</code>来说是个不错的选择。</p><p id="00cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Huggging Faces 的<code class="fe ls lt lu lv b">Transformers</code>库提供了一种将模型导出为 ONNX 格式的便捷方式。更多细节可以参考<a class="ae kv" href="https://huggingface.co/docs/transformers/serialization#exporting-transformers-models" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</p><p id="f501" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用上面提到的<code class="fe ls lt lu lv b">bert-base-NER</code>模型和<code class="fe ls lt lu lv b">token-classification</code>作为特征。<code class="fe ls lt lu lv b">token-classification</code>是我们正在努力解决的任务。您可以通过执行以下代码来查看可用功能的列表:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">检查特定模型类型的所有支持功能</p></figure><p id="fcf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过调用转换脚本，您必须从本地目录或直接从拥抱脸的中枢指定模型名称。您还需要指定如上所示的特性。输出文件将保存在<code class="fe ls lt lu lv b">output</code>目录中。</p><p id="b64d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们给<code class="fe ls lt lu lv b">onnx/</code>作为输出目录。这是 ONNX 模型将被保存的地方。</p><p id="7ce6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将<code class="fe ls lt lu lv b">opset</code>参数作为默认参数，它是在 ONNX 配置中为该模型定义的。</p><p id="63e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们还将<code class="fe ls lt lu lv b">atol</code>参数设为默认值 1e-05。这是原始 PyTorch 模型和 ONNX 模型之间的数值精度容差。</p><p id="b642" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是将模型导出为 ONNX 格式的命令:</p><pre class="kg kh ki kj gt nb lv nc nd aw ne bi"><span id="17ec" class="nf me iq lv b gy ng nh l ni nj">$ python -m transformers.onnx \<br/>    --model=dslim/bert-base-NER \<br/>    --feature=token-classification \<br/>    onnx/</span></pre></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="ba83" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">💫将 ONNX 模型用于变压器管道</h1><p id="f6b2" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">现在我们已经将模型导出为 ONNX 格式，我们可以将它用于 Transformers 管道。</p><p id="b84e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过程很简单:</p><ul class=""><li id="3409" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">使用 ONNX 模型创建一个会话，允许您将模型加载到管道中并进行推理。</li><li id="8141" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">覆盖管道的<code class="fe ls lt lu lv b">_forward</code>和<code class="fe ls lt lu lv b">preprocess</code>方法以使用 ONNX 模型。</li><li id="c71a" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">运行管道。</li></ul><p id="4933" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先导入所需的包:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">所有需要的进口</p></figure><h2 id="c736" class="nf me iq bd mf ob oc dn mj od oe dp mn lf of og mp lj oh oi mr ln oj ok mt ol bi translated">⚙️用 ONNX 模型创建一个会话</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用 onnxruntime 创建会话</p></figure><p id="6bb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们将只使用<code class="fe ls lt lu lv b">CPUExecutionProvider</code>，它是 ONNX 模型的默认执行提供者。您可以为会话提供一个或多个执行提供程序。例如，您可以使用<code class="fe ls lt lu lv b">CUDAExecutionProvider</code>在 GPU 上运行模型。默认情况下，会话将从列表中的第一个开始，使用计算机上可用的那个。</p><p id="4ff8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b">Onnxruntime</code>提供了查看所有可用执行提供者的功能:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">onnxruntime 中所有可能的执行提供程序</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/596b2c1450b47ac5e966d9c4d7f1312b.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*sHYfpZHZsn3IWClqb6C7vA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">所有执行提供程序的列表</p></figure><p id="1f83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，对于每种用例及配置，都有许多可用的提供者。</p><h2 id="4e67" class="nf me iq bd mf ob oc dn mj od oe dp mn lf of og mp lj oh oi mr ln oj ok mt ol bi translated">⚒️用 ONNX 模型创建管道</h2><p id="3a61" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">现在我们已经有了一个可以使用 ONNX 模型的会话，我们可以对原始的<code class="fe ls lt lu lv b">TokenClassificationPipeline</code>类进行重载来使用 ONNX 模型。</p><p id="a1d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要完全理解正在发生的事情，你可以参考<code class="fe ls lt lu lv b"><a class="ae kv" href="https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/pipelines/token_classification.py#L86" rel="noopener ugc nofollow" target="_blank">TokenClassificationPipeline</a></code> python 类的源代码。</p><p id="8e2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将只覆盖<code class="fe ls lt lu lv b">_forward</code>和<code class="fe ls lt lu lv b">preprocess</code>方法，因为其他方法不依赖于模型格式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">调整管道等级以适应 onnx 型号需求</p></figure><h2 id="7d23" class="nf me iq bd mf ob oc dn mj od oe dp mn lf of og mp lj oh oi mr ln oj ok mt ol bi translated">🏃运行管道</h2><p id="8e58" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们现在已经设置好了一切，所以我们可以运行管道。</p><p id="ad28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，管道需要一个标记器、一个模型和一个任务。我们将使用<code class="fe ls lt lu lv b">ner</code>任务。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用新的超额收费类别创建完整的管道</p></figure><p id="4706" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看是否可以运行管道并检查输出:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">运行 ONNX 管道</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e0602b17c443451cb376ee34caef3362.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*g5Nc59XO86CP6deNPKeB4A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ONNX 流水线输出</p></figure><blockquote class="oo op oq"><p id="588d" class="kw kx na ky b kz la jr lb lc ld ju le or lg lh li os lk ll lm ot lo lp lq lr ij bi translated"><strong class="ky ir">在这里，管道与 ONNX 模型运行良好！我们现在有一个与 ONNX 全面合作的 NER 管道。🎉</strong></p></blockquote><p id="428b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看可选的基准测试部分，了解它与原始 PyTorch 模型相比的性能，或者直接跳到结论部分，快速总结该过程。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="945b" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">🧪对整个管道进行基准测试(可选)</h1><p id="6edb" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">我们将通过使用 ONNX 模型和 PyTorch 模型测量管道的推理时间来进行基准测试。</p><p id="2141" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先需要加载 PyTorch 模型，并用它创建一个管道。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">创建 PyTorch 管道</p></figure><p id="771f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用相同的数据和 3 种不同的序列长度测试两条管道。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有三种不同长度的基准序列</p></figure><p id="8054" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们比较 3 个不同序列长度的每个管道的推理时间。对于每个序列长度，我们将重复每次迭代 300 次，以获得更准确的基准，并将所有内容放在一个表中以比较结果</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基准循环</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/4688e7847f14be79915211b69f16b22f.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*yed9-QqcKU2_vDRcnQQciw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基准测试结果</p></figure><p id="dcd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇，看起来真棒！🎉</p><p id="c11a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎对于每个序列长度，ONNX 模型都比原始 PyTorch 模型快得多。我们来计算一下 ONNX 模型和 PyTorch 模型的推理时间之比。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/364afb800cda3583856ba835d114ca6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*u2maj-laTqzQUzZFbBjQcA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基准比率</p></figure><p id="096e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们在长序列上几乎实现了 3 倍的加速！🎉</strong></p><p id="55a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们甚至没有根据模型架构和模型运行的硬件进行任何优化，而 ONNX 可以做到这一点。</p><p id="05a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">优化可能非常有用，但这是一个很深的话题，不能在这篇文章中讨论。但是很高兴知道您可以做到这一点，并且我们可以在未来的帖子中探索它。</p><p id="dc54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另外，我们的测试是在 CPU 上进行的，但我见过的所有 GPU 上的基准测试甚至比 CPU 上的基准测试更令人印象深刻。查看<a class="ae kv" href="https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333" rel="noopener">这篇伟大的文章</a>了解更多关于不同架构和推理配置的基准测试。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="312d" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">📍结论</h1><p id="4326" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf mx lh li lj my ll lm ln mz lp lq lr ij bi translated">总而言之，我们已经用 ONNX 构建了一个完全可用的 NER 管道。我们已经将 PyTorch 模型转换为 ONNX，并对原始管道类进行了超额收费，以符合 ONNX 模型的要求。最后，我们用原始 PyTorch 模型对 ONNX 模型进行了基准测试，并对结果进行了比较。</p><p id="7a38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，不幸的是 PyTorch 模型是和 ONNX 模型一起加载的。这是因为 Transformers 管道需要加载 PyTorch 模型，特别是为了模型的配置。</p><p id="35a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我正在寻找一种方法来避免 PyTorch 模型的加载，因为它可能会在某些系统上产生 RAM 问题。</p><p id="7fce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用来使它工作的过程对于拥抱脸变形金刚库中可用的每个模型和任务都是相同的。</p><p id="0025" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您唯一需要关心的是模型架构是否有针对 ONNX 实现的配置。你可以在文档中找到<a class="ae kv" href="https://huggingface.co/docs/transformers/serialization#onnx" rel="noopener ugc nofollow" target="_blank">的完整架构列表</a>。</p><p id="5fdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您正在寻找的架构还没有实现，您仍然可以创建它并向 Transformers 库发出一个<em class="na">拉请求</em>来添加它。这正是我几个月前为卡门贝干酪建筑所做的。你可以在变形金刚 GitHub 库上查看完整的 PR 。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><p id="2b79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你觉得这篇文章有用而且有趣。如果您有任何问题或面临任何问题，请告诉我。我很乐意添加更多的例子和对其他 NLP 任务的支持，所以如果你有任何想法或要求，请告诉我！🤗</p><p id="db97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如有疑问或问题，请在 GitHub 或下面的评论中打开一个<a class="ae kv" href="https://github.com/ChainYo/transformers-pipeline-onnx/issues" rel="noopener ugc nofollow" target="_blank">问题。</a></p><p id="cd6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="na"> P.S .我还计划添加另一个基准测试部分，以测试 ONNX 模型是否实现了与原始模型相同的结果(剧透:是的！).</em></p></div></div>    
</body>
</html>