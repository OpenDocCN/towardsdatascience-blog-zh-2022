<html>
<head>
<title>Bayesian Updating Simply Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯更新简单解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-updating-simply-explained-c2ed3e563588#2022-06-20">https://towardsdatascience.com/bayesian-updating-simply-explained-c2ed3e563588#2022-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8086" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用贝叶斯定理更新信念的直观解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d71e4275c872fa317cb2f528d9755d80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*huFGQZ0Ntel-O4B4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迪伦·克里夫顿的照片在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上</p></figure><h1 id="5c2d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="4036" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在我以前的文章中，我们从条件概率中推导出了贝叶斯定理。如果您不熟悉贝叶斯定理，我强烈建议您在阅读本文之前先阅读那篇文章:</p><div class="mn mo gp gr mp mq"><a href="https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">条件概率和贝叶斯定理浅释</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">条件概率和贝叶斯定理的简单直观的解释。</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">pub.towardsai.net</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="0903" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">在本文中，我们将使用贝叶斯定理来更新我们的信念，并展示如何随着更多的数据，我们变得更加确定我们的假设。</p><h1 id="3d87" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯定理</h1><p id="3677" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们可以把贝叶斯定理写成如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/18feaecf9728b45456c78d0768346a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*RKYfKnmcfWUgTlwlhoonAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><ul class=""><li id="1fa0" class="nl nm it lt b lu nf lx ng ma nn me no mi np mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu"> P(H) </em> </strong>是我们假设的概率，即<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a><strong class="lt iu">。</strong>这是在我们看到证据/数据之前，我们假设的可能性有多大。</li><li id="c0b7" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu"> P(D|H) </em> </strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">可能性</strong> </a> <strong class="lt iu">，</strong>是给定我们的假设，我们的证据/数据正确的概率。</li><li id="f391" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu"> P(H|D) </em> </strong>是从我们的数据/证据来看，我们的假设是正确的概率。这就是俗称的<a class="ae ky" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>。</li><li id="d375" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu"> P(D) </em> </strong>是我们证据/数据的概率。这被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">归一化常数</strong> </a> <strong class="lt iu">，</strong>，它是可能性和先验的乘积之和:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/94e7b2033ce34bf81d65ba638d8b364d.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*ICPrrYL6ZbbU4dXaoGnEPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><h1 id="cffe" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯更新</h1><p id="84d3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当新的证据出现时，我们可以用贝叶斯定理来更新我们的假设。</p><p id="3f2b" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">例如，给定一些数据<strong class="lt iu"><em class="nu"/></strong>其中包含了一个<strong class="lt iu"> <em class="nu"> d_1 </em> </strong>数据点，那么我们的后验就是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/cf2a5c6f3d0c7290bffecdc86ed02751.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*PLyG_5kO3uIAOUWrHDjOlg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="3b15" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">假设我们现在获得了另一个数据点<strong class="lt iu"><em class="nu">【D2】</em></strong>，因此我们有更多的证据来评估和<strong class="lt iu"> <em class="nu">更新</em> </strong>我们的信念(后验)。然而，<strong class="lt iu">我们的</strong> <a class="ae ky" href="https://www.investopedia.com/terms/p/posterior-probability.asp" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">先验现在变成了我们的旧后验</strong> </a>因为这代表了我们的<strong class="lt iu">新的先验信念</strong>我们的假设:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/eae4317cb72aab5fe634ead17eaacede.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*fQfydrrzwywnGJMDdv7P8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="8207" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">就更新而言，我们说后验与似然和先验的乘积成比例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/621962575384296a6543142db0a72f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*HBQe1n4WuC1Uvts9vgvsPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="c50d" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这种更新可能看起来有些武断，但是现在让我们通过一个例子来使它更加具体。</p><blockquote class="oe of og"><p id="1250" class="lr ls nu lt b lu nf ju lw lx ng jx lz oh nh mc md oi ni mg mh oj nj mk ml mm im bi translated">我们通常省略分母P(D ),因为它只是一个归一化常数，使概率总和为1。<a class="ae ky" href="https://stats.stackexchange.com/questions/64364/why-is-posterior-density-proportional-to-prior-density-times-likelihood-function" rel="noopener ugc nofollow" target="_blank">Stat Exchange有一个很好的帖子很好地解释了这一点。</a></p></blockquote><h1 id="8b05" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">例子</h1><p id="72d7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">假设我有三个不同的骰子，有三个不同的数字范围:</p><ul class=""><li id="7e26" class="nl nm it lt b lu nf lx ng ma nn me no mi np mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu">骰子1:1–4</em></strong></li><li id="0613" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu">骰子2:1–6</em></strong></li><li id="432c" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><strong class="lt iu"> <em class="nu">骰子3:1–8</em></strong></li></ul><p id="0c09" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">我们随机选择一个骰子，用给定的骰子做三次后续的掷骰子。使用这些掷骰子(数据)，我们可以计算在到达角色(后)后，我们选择骰子1、2或3的可能性。</p><h2 id="78ab" class="ok la it bd lb ol om dn lf on oo dp lj ma op oq ll me or os ln mi ot ou lp ov bi translated">第一卷</h2><p id="c439" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在第一次掷骰子时，我们得到数字<strong class="lt iu"> 4 </strong>。我们选择骰子1、2或3的概率是多少？</p><p id="5dce" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">我们可以使用贝叶斯定理计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/1243943b84b6a26701d7805c351ae072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-drUneakp3bkrkbDjnko3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="ce98" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">我们使用了以下值:</p><ul class=""><li id="b14d" class="nl nm it lt b lu nf lx ng ma nn me no mi np mm nq nr ns nt bi translated">之前，<strong class="lt iu"> <em class="nu"> P(骰子)</em> </strong>，简单来说就是<strong class="lt iu"> 0.33 </strong>因为每个骰子被选中的几率相等。</li><li id="ab78" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated">可能性，<strong class="lt iu"> <em class="nu"> P(掷出4 |骰子)</em> </strong>，就是每个骰子掷出一个<strong class="lt iu"> 4 </strong>的概率。</li><li id="ce8c" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated">数据的概率(归一化值)，<strong class="lt iu"> <em class="nu"> P(第4卷)</em> </strong>，就是似然性和先验乘积的<strong class="lt iu">之和。</strong></li></ul><p id="9810" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">所以第一次掷骰子后，骰子1是最有可能的。这是有意义的，因为它只有4个结果，因此1-4之间的任何一次掷骰都是最有可能的。</p><h2 id="92b3" class="ok la it bd lb ol om dn lf on oo dp lj ma op oq ll me or os ln mi ot ou lp ov bi translated">第二卷</h2><p id="1552" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用相同的骰子，我们现在掷第二次，得到一个<strong class="lt iu"> 2 </strong>。然而，我们有了新的先验，这是上面我们掷出4的计算后验。</p><p id="b3f4" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">因此，现在的概率是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/6f1fd7174c0ced4828bbf14bc4aaeace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yjv-EqFSE8eBAqQom4JHXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="2e01" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">有了这个新信息，骰子1越来越有可能是我们捡到的那个。</p><h2 id="265d" class="ok la it bd lb ol om dn lf on oo dp lj ma op oq ll me or os ln mi ot ou lp ov bi translated">第三卷</h2><p id="9aa8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们用我们选择的骰子掷第三次，得到一个<strong class="lt iu"> 5 </strong>。使用我们以前的后验概率作为我们新的先验概率，现在的概率是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/e6f8b3e5d23f74913c5fdc87a3d5c56b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KfCjZTWDH2i2odXoDkrYsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="9055" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">它是骰子1的概率现在是0，因为不可能用骰子1掷出5。因此，给定这三个数据点(掷骰子),骰子2是我们最有可能选中的一个！</p><p id="d55d" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">具有最高后验概率的值称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">【最大后验概率】</strong> </a>。这类似于<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">最大似然</strong> </a> <strong class="lt iu"> </strong>但是对于贝叶斯统计来说，是后验分布的众数。</p><blockquote class="oe of og"><p id="0697" class="lr ls nu lt b lu nf ju lw lx ng jx lz oh nh mc md oi ni mg mh oj nj mk ml mm im bi translated">骰子的三个后验值形成了一个<strong class="lt iu">后验分布</strong>，尽管非常小！</p></blockquote><h1 id="babd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="1bf8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我们展示了当你面对新数据时，如何使用贝叶斯定理来更新你的信念。这种统计方式与我们人类的思维方式非常相似，因为新信息可以强化或改变我们的信念。</p><h1 id="7c43" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系！</h1><ul class=""><li id="ca40" class="nl nm it lt b lu lv lx ly ma oz me pa mi pb mm nq nr ns nt bi translated">要在媒体上阅读无限的故事，请务必在此注册！T17<em class="nu">T19】💜</em></li><li id="9786" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><a class="ae ky" href="/subscribe/@egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="nu">要想在我发帖注册时得到更新的邮件通知就在这里！</em> </a> <em class="nu"> </em>😀</li><li id="b8dc" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><a class="ae ky" href="https://www.linkedin.com/in/egor-howell-092a721b3/" rel="noopener ugc nofollow" target="_blank"><em class="nu">LinkedIn</em></a><em class="nu"/>👔</li><li id="a7a0" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><a class="ae ky" href="https://twitter.com/EgorHowell" rel="noopener ugc nofollow" target="_blank"> <em class="nu">推特</em> </a> <em class="nu"> </em> 🖊</li><li id="fb18" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><a class="ae ky" href="https://github.com/egorhowell" rel="noopener ugc nofollow" target="_blank"><em class="nu">github</em></a><em class="nu"/>🖥</li><li id="87b0" class="nl nm it lt b lu nv lx nw ma nx me ny mi nz mm nq nr ns nt bi translated"><a class="ae ky" href="https://www.kaggle.com/egorphysics" rel="noopener ugc nofollow" target="_blank"><em class="nu"/></a><em class="nu"/>🏅</li></ul><blockquote class="oe of og"><p id="72f1" class="lr ls nu lt b lu nf ju lw lx ng jx lz oh nh mc md oi ni mg mh oj nj mk ml mm im bi translated">(所有表情符号都是由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>设计的——开源的表情符号和图标项目。许可证:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></blockquote><h1 id="13b2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外的东西！</h1><p id="a730" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我的父母住在东苏塞克斯，上周末我去拜访了他们。在旅途中，我们在一个名叫滕布里奇·韦尔斯的小镇吃了晚饭。你知道谁住在那里吗？托马斯·贝叶斯！</p><p id="4622" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">如果您在该地区，这是非常值得一试！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/d033d250f94b6a5065f3be27ca118fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7X2f1u4kukRIJkaRsOpM-w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片。</p></figure></div></div>    
</body>
</html>