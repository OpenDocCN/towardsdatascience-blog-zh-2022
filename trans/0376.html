<html>
<head>
<title>Fantastic activation functions and when to use them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">奇妙的激活功能以及何时使用它们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde#2022-02-15">https://towardsdatascience.com/fantastic-activation-functions-and-when-to-use-them-481fe2bb2bde#2022-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6a0d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">十大激活功能，它们的优缺点，何时使用它们，以及一个备忘单</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5a97051a756289b29e64863c3d3b5812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0ELkbEC7Hxs_0pZBgx0Iw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0816" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated">与权重和偏差一样，激励函数是ML模型中的一个重要组成部分。他们在使深度神经网络训练成为现实方面发挥了重要作用，并且他们是一个不断发展的研究领域。在这篇博文中，我们将它们进行对比，观察它们的优缺点，何时使用它们，以及它们的发展。</p><p id="4a11" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ma" href="https://paperswithcode.com/methods/category/activation-functions" rel="noopener ugc nofollow" target="_blank"> Paperswithcode </a>列出了48个激活函数，每年都有新的和改进的激活函数被提出。在本文中，我列出了10个最常见的激活函数，它们代表了所有其他的激活函数。</p><p id="8970" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据在ML模型中的使用方式，激活函数有两种类型。</p><ol class=""><li id="a492" class="mb mc iq kx b ky kz lb lc le md li me lm mf lq mg mh mi mj bi translated">ML模型的<strong class="kx ir"> <em class="mk">输出层</em> </strong>中用到的激活函数(想想分类问题)。这些激活函数的主要目的是将值压缩在像0到1这样的有界范围内。</li><li id="93c1" class="mb mc iq kx b ky ml lb mm le mn li mo lm mp lq mg mh mi mj bi translated">用于神经网络<strong class="kx ir"> <em class="mk">隐层</em> </strong>的激活函数。这些激活函数的主要目的是提供非线性，没有非线性，神经网络就不能模拟非线性关系。</li></ol><p id="1d06" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">隐藏层中使用的激活函数应该理想地满足以下条件，</p><ol class=""><li id="a372" class="mb mc iq kx b ky kz lb lc le md li me lm mf lq mg mh mi mj bi translated"><strong class="kx ir"> <em class="mk">非线性，</em> </strong>让神经网络学习非线性关系。<a class="ae ma" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>陈述了使用非线性激活函数的两层神经网络可以逼近任何函数。</li><li id="355d" class="mb mc iq kx b ky ml lb mm le mn li mo lm mp lq mg mh mi mj bi translated"><strong class="kx ir"> <em class="mk">无界，</em> </strong>使学习更快，避免过早饱和。当范围无限时，基于梯度的学习是高效的。</li><li id="f9ef" class="mb mc iq kx b ky ml lb mm le mn li mo lm mp lq mg mh mi mj bi translated"><strong class="kx ir"><em class="mk"/></strong>连续可微，这个性质虽然不是决定性的，但却是可取的。ReLU就是一个突出的例子，它的函数不可微≤ 0，但在实践中仍然表现很好。</li></ol><h2 id="68d4" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">备忘单</h2><p id="54be" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">为没有耐心的人准备的文章的备忘稿，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/a906664162e1d06b96bd8b77d4fc027c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7sNtf4Cmou-3eSW35Sx4g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="5766" class="np mr iq bd ms nq nr ns mv nt nu nv my jw nw jx nb jz nx ka ne kc ny kd nh nz bi translated">输出层激活功能</h1><h2 id="8bfe" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">乙状结肠的</h2><p id="b105" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">这些激活函数于20世纪90年代初推出，其优点是将输入压缩到0和1  之间的值<strong class="kx ir"> <em class="mk">，从而完美地模拟概率。函数是可微的，但是<strong class="kx ir"> <em class="mk">快速饱和</em> </strong>，因为在深度神经网络中使用时，有界性导致<strong class="kx ir"> <em class="mk">消失梯度</em> </strong>。指数计算的成本<strong class="kx ir"><em class="mk"/></strong>很高，当你必须训练一个有数百层和神经元的模型时，这种成本就会增加。</em></strong></p><p id="77e4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">函数界于0和1之间，第<strong class="kx ir"> <em class="mk"> e导数界于-3和3 </em> </strong>之间。<strong class="kx ir"> <em class="mk">输出在零附近不对称</em> </strong>，这将导致所有神经元在训练期间采用相同的符号，使其不适合训练隐藏层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bc3ba4b10ada69ba19258a6d22be0cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*JG4frdbbZIKTyz-FlEg65w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">聪明的老乙状结肠。作者图片</p></figure><p id="06c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>一般用于logistic回归，输出层的<strong class="kx ir"> <em class="mk">二元分类模型</em> </strong>。</p><h2 id="081d" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">Softmax</h2><p id="bf38" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated"><strong class="kx ir"> <em class="mk">利用0和1之间的输出范围扩展sigmoid </em> </strong>激活功能。这主要用于多类、多项分类问题的输出层，具有一个有用的属性<strong class="kx ir"> <em class="mk">输出概率的总和等于1 </em> </strong>。换句话说，这是将sigmoid应用于输出上的每个神经元，然后将其归一化为和1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/04df57d090c96e22df881cac36b68f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*z-OY3wyhM75hPuHI_wCC_g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">功利的<strong class="bd ob"> softmax。</strong>作者图片</p></figure><p id="0e69" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong> <strong class="kx ir"> <em class="mk">多项</em> </strong>和<strong class="kx ir"> <em class="mk">多类</em> </strong>分类。</p><h2 id="444b" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">双曲正切</h2><p id="296b" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">在20世纪90-2000年代广泛使用，它通过<strong class="kx ir"> <em class="mk">扩展范围以包括-1到1 </em> </strong>克服了sigmoid激活功能的缺点。这导致<strong class="kx ir"> <em class="mk">零居中</em> </strong>，这导致隐藏层的权重的平均值接近零。这导致更容易和更快的学习。这个函数是<strong class="kx ir"> <em class="mk">可微的和</em> </strong>平滑的，但是使用指数函数要付出<strong class="kx ir"> <em class="mk">的代价</em> </strong>。当用于深层神经网络的隐藏层时，它很快饱和，消失梯度渗透进来。导数比sigmoid的更陡。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/01e1651f5edbc5008c00797985cae61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Fxl8HFpp4pKOD6NhcxmEKg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">值得信赖的tanh。作者图片</p></figure><p id="df63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>可用于<strong class="kx ir"> <em class="mk"> RNN </em> </strong>的隐藏图层。但是有更好的选择，比如ReLU</p><p id="5213" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然sigmoid和tanh函数可以用在隐藏层中，但由于其正有界性，训练很快饱和，并且消失的梯度使得不可能在深度神经网络设置中使用它们。</p><h1 id="1d00" class="np mr iq bd ms nq nr ns mv nt nu nv my jw nw jx nb jz nx ka ne kc ny kd nh nz bi translated">隐藏层激活功能</h1><h2 id="ef6b" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">输入ReLU</h2><p id="1163" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">整流线性单元是激活功能的<strong class="kx ir"><em class="mk"/></strong>。这是最广泛使用的<strong class="kx ir"><em class="mk"/></strong>和大多数类型问题的goto激活功能。从2010年左右开始，一直<strong class="kx ir"> <em class="mk">广泛研究</em> </strong>。负的时候有界到0，正的时候无界。这种<strong class="kx ir"> <em class="mk">有界和无界的混合创建了一个内置的正则化</em> </strong>，这对于深度神经网络来说很方便。正则化提供了一种<strong class="kx ir"> <em class="mk">稀疏表示</em> </strong>，导致计算上高效的训练和推断。</p><p id="ced9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正无界<strong class="kx ir"> <em class="mk">加速梯度下降</em> </strong>的收敛，同时保持<strong class="kx ir"> <em class="mk">计算简单</em> </strong>的计算。ReLU唯一的主要缺点是神经元死亡。由于对0的负有界性，在训练过程中早期被关闭的一些<strong class="kx ir"> <em class="mk">死亡神经元</em> </strong>永远不会被打开。函数从x &gt;为0时的无界快速切换到x ≤ 0时的有界，使其<strong class="kx ir"> <em class="mk">连续不可微</em> </strong>。但在现实中，由于学习率低和负偏差大，这可以被克服，而不会对性能产生任何残余影响。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/0509fab5df49243906de9019da4dc95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*2rt3DlGETkXNhh_H1iGqJw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">摇滚明星<strong class="bd ob"> ReLU。</strong>图片卜作者</p></figure><p id="e106" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong> CNN的，RNN的，以及其他深度神经网络。</p><p id="38b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ReLU是隐藏层激活功能的明显赢家。自2010年以来，人们一直在研究ReLU的优缺点，并提出了新的激活函数，这些函数有助于提高ReLU的性能，同时解决其困难。</p><h2 id="4fba" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated"><strong class="ak">对ReLU </strong>的改进</h2><h2 id="a32c" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">泄漏ReLU</h2><p id="06b9" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">2011年引入，通过<strong class="kx ir"> <em class="mk">允许负值</em> </strong>存在，直接解决了ReLU的担忧。这使得负值能够反向传播，从而<strong class="kx ir"> <em class="mk">克服ReLU的死神经元问题</em> </strong>。负值被积极地缩放，这导致较慢的训练。即使有了这种改进，Leaky ReLU也不是普遍比ReLU更好。T53】</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1d4839ffeed3cc56dce1a46fec911af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*VjJFkyqEmcCtoGZqwDIUHA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ob">泄漏的ReLU </strong>。作者图片</p></figure><p id="5f7a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>涉及稀疏渐变的任务像<strong class="kx ir"> <em class="mk">甘</em> </strong>。</p><h2 id="7d08" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">参数ReLU</h2><p id="7c38" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">这改进了Leaky ReLU，其中标量倍数不是任意选择的，而是根据数据训练的。这是一把双刃剑，因为来自数据的训练导致模型对缩放参数(a)敏感，并且对于不同的a值表现不同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c804b9820cebb03f559a2f8d7ca83b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*1kgzZawz2wfdGFKgzinToQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可训练的参数ReLU。图片作者。</p></figure><p id="c506" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>可用于<strong class="kx ir"> <em class="mk">修复Leaky ReLU不起作用时的死神经元</em> </strong>问题。</p><h2 id="86e2" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">ELU</h2><p id="a48b" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">指数线性单位，2015年推出，正无界，使用<strong class="kx ir"> <em class="mk">对数曲线表示负值</em> </strong>。这是一个稍微不同的处理死神经元问题的方法，不像Leaky和Parameter ReLU。与ReLU不同的是，负值会慢慢变平滑并变得有界，从而避免死神经元。但是负斜率使用指数函数建模，这使得<strong class="kx ir"> <em class="mk">成本</em> </strong>。当采用次优初始化策略时，指数函数有时会导致<strong class="kx ir"> <em class="mk">爆炸梯度</em> </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2df410fd4df9bcef51c01842c7752d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*rr1QyUjab-x_x5Ecx6mt1A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">昂贵的<strong class="bd ob"> ELU </strong>。图片作者。</p></figure><h2 id="43b8" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">格鲁</h2><p id="8755" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">高斯误差线性单元，2018年推出，是该模块中的新成员，显然是NLP相关任务 的<strong class="kx ir"> <em class="mk">赢家。它用于SOTA算法，如GPT-3、BERT和其他基于变压器的架构。GeLU结合了<strong class="kx ir"><em class="mk"/></strong>(对稀疏网络随机清零神经元)<strong class="kx ir"> <em class="mk"> zone out </em> </strong>(保持先前值)，以及ReLU。它<strong class="kx ir"> <em class="mk">通过百分点而不是门</em> </strong>对输入进行加权，从而产生更平滑版本的ReLU。</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/80127bfd77e0cab9085a99d9820d5ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*UvTaVXWUAiOyQU7ktutEMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">新潮的<strong class="bd ob">葛鲁。</strong>图片作者。</p></figure><p id="609b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>自然语言处理、计算机视觉和语音识别</p><h2 id="35d2" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">嗖嗖</h2><p id="626b" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">2017年推出。大的负值将具有0的导数，而<strong class="kx ir"> <em class="mk">小的负值仍然与捕捉潜在模式</em> </strong>相关。可作为ReLU的替代产品。导数有一个有趣的形状。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/4bffecddb3a0b2a2dc8133516d7eeb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKTvlyU9G4ug30yCCZzQkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">谷歌<strong class="bd ob">嗖嗖。</strong>图片作者。</p></figure><p id="45d4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">用途:</strong>在图像分类和机器翻译方面匹配或优于ReLU。</p><p id="a2dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更多小抄，<br/><a class="ae ma" href="https://github.com/adiamaan92/mlnotes_handwritten" rel="noopener ugc nofollow" target="_blank">https://github.com/adiamaan92/mlnotes_handwritten</a></p><h2 id="baed" class="mq mr iq bd ms mt mu dn mv mw mx dp my le mz na nb li nc nd ne lm nf ng nh ni bi translated">关于作者:</h2><p id="d136" class="pw-post-body-paragraph kv kw iq kx b ky nj jr la lb nk ju ld le nl lg lh li nm lk ll lm nn lo lp lq ij bi translated">我是一名数据科学家，在🛢️石油和⛽.天然气公司工作如果你喜欢我的内容，请关注我👍🏽在<a class="ae ma" href="https://www.linkedin.com/in/adiamaan-keerthi/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae ma" href="https://blog.adiamaan.com/" rel="noopener ugc nofollow" target="_blank"> Medium </a>和<a class="ae ma" href="https://github.com/adiamaan92" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上。订阅,每当我在媒体上发帖时都会收到提醒。</p></div></div>    
</body>
</html>