<html>
<head>
<title>Foundational RL: Solving Markov Decision Process</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基础 RL:求解马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/foundational-rl-solving-markov-decision-process-d90b7e134c0b#2022-12-12">https://towardsdatascience.com/foundational-rl-solving-markov-decision-process-d90b7e134c0b#2022-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6b20" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习之路</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d831ef77f8991309ad3880b7e69ac93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJtQjnv6nhWkqvKDC4UxDw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者使用人工智能工具 Dreamstudio 生成的封面照片(授权为<a class="ae ky" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a></p></figure><p id="5cf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005">的第一部分</a>中，我讨论了一些基本概念来为强化学习(RL)建立基础，如马尔可夫状态、马尔可夫链和马尔可夫决策过程(MDP)。强化学习问题是建立在 MDP 之上的。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">基础 RL:马尔可夫状态、马尔可夫链和马尔可夫决策过程</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">强化学习之路</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="dd08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MDP 是一个 4 元组模型(𝓢、𝓐、𝓟、𝓡)，其中<em class="mn"> s </em> ∈ 𝓢是一个状态，<em class="mn"> a </em> ∈ 𝓐是当代理是一个状态<em class="mn"> s </em>时采取的一个动作，𝓟<em class="mn">(s“| s，a) </em>是在动作<em class="mn"> a </em>的影响下从<em class="mn"> s </em>转移到状态<em class="mn">s’</em>的转移概率矩阵(或者</p><p id="98c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">策略函数:</strong>策略函数，在 RL 文献中通常用π表示，规定了从状态空间𝓢到动作空间𝓐.的映射</p><blockquote class="mo"><p id="21f2" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">MDP 的目标是找到一个最大化长期回报的最优策略。</p></blockquote><h1 id="038c" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">贴现因素</h1><p id="d169" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">MDP 需要一个离散时间的概念，因此 MDP 被定义为一个离散时间随机控制过程。在 RL 的上下文中，每个 MDP 由折扣因子γ (gamma)来参数化，该折扣因子γ确定未来奖励相对于当前奖励的重要性。换句话说，这是一个衡量未来的回报相对于现在的回报对代理人有多大价值的指标。折扣因子是一个介于 0 和 1 之间的值，其中值 0 意味着代理人只关心眼前的奖励，并将完全忽略任何未来的奖励，而值 1 意味着代理人将把未来的奖励视为与现在获得的奖励同等重要。</p><p id="398e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在代理人在两个行动 A 和 B 之间做出决定的情况下，这两个行动将导致不同的未来奖励序列，折扣因子可以用于确定这些不同奖励序列的相对值。如果折扣因子低，那么代理人将更有可能选择导致即时奖励的行动，而如果折扣因子高，代理人将更有可能选择导致更大金额的未来奖励的行动。</p><p id="3f43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，折扣因子是 MDP 中的一个重要考虑因素，因为它允许代理人用长期奖励来换取短期奖励，并平衡即时满足的需求和延迟满足的潜在好处。</p><p id="1f05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在 RL 环境中的 MDP 是(𝓢，𝓐，𝓟，𝓡，γ)。</p><h1 id="634d" class="my mz it bd na nb nc nd ne nf ng nh ni jz nv ka nk kc nw kd nm kf nx kg no np bi translated">奖励公式</h1><p id="5657" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">考虑到贴现因子γ，在时间<em class="mn"> t </em>的回报可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/320deb8dd3918fa2b4115998637bb0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*71FSEtf4kPt26SXOmp4tkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 1。时间步长<em class="nz"> t </em>的奖励。</p></figure><p id="3558" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总奖励，即所有时间的累积奖励是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ce051be995d22792a59c63dab77cf167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*cRhekasdxFPhjdON2yA6kQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 2。累积奖励</p></figure><p id="f89a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大化累积回报的π策略是我们的 MDP 问题的解决方案。</p><h2 id="0e9a" class="ob mz it bd na oc od dn ne oe of dp ni li og oh nk lm oi oj nm lq ok ol no om bi translated">我们来看一个例子(自动驾驶代理):</h2><p id="8b0b" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在自动驾驶的情况下，可以使用马尔可夫决策过程来对车辆的决策过程进行建模，以便在长期内最小化燃料消耗。在这种情况下，系统的状态可以由车辆的当前速度和加速度来表示，目标是找到将最小化燃料消耗的最佳动作序列。</p><p id="7487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">车辆的瞬时燃料消耗可以建模为函数<em class="mn"> g(v，a) </em>，其中<em class="mn"> v </em>是当前速度，而<em class="mn"> a </em>是当前加速度。该函数可用于评估给定状态下每个可能行动的成本，马尔可夫决策过程可用于寻找将最小化车辆长期燃料消耗的最佳行动序列。</p><p id="7040" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在当前速度为<em class="mn"> v </em>且当前加速度为<em class="mn">a</em>的给定状态下，MDP 可以考虑一系列可能的动作，例如加速、减速或保持当前速度。可以使用函数<em class="mn"> g(v，a) </em>来评估每项措施的成本，并根据哪项措施将导致长期最低的燃油消耗来选择最佳措施。然后，可以在每个后续状态下重复该过程，以找到最大限度降低燃油消耗的最佳行动顺序。</p><blockquote class="on oo op"><p id="62b8" class="kz la mn lb b lc ld ju le lf lg jx lh oq lj lk ll or ln lo lp os lr ls lt lu im bi translated"><strong class="lb iu">需要强调的重要一点是，在上面的例子中，我没有考虑转移概率𝓟，这是为了让我们的例子更容易理解，也更符合实际。</strong></p></blockquote><h2 id="dc42" class="ob mz it bd na oc od dn ne oe of dp ni li og oh nk lm oi oj nm lq ok ol no om bi translated">一些 python 代码:</h2><p id="9d18" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">考虑一个 MDP，其中状态为速度和加速度，速度的最小值为 0 米/秒，最大值为 50 米/秒，加速度的最小值为-4.5 米/秒，最大值为 3.0 米/秒，它们中的每一个都以 0.1 量化。我们可以考虑以下构造函数中的 MDP 类:</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="21e7" class="oy mz it ou b be oz pa l pb pc">def __init__(self, velocity_min, velocity_max, acceleration_min, acceleration_max, velocity_step, acceleration_step, acceleration_min_accelerate, acceleration_max_accelerate):<br/>        # Define minimum and maximum values for velocity and acceleration<br/>        self.VELOCITY_MIN = velocity_min<br/>        self.VELOCITY_MAX = velocity_max<br/>        self.ACCELERATION_MIN = acceleration_min<br/>        self.ACCELERATION_MAX = acceleration_max<br/><br/>        # Define quantization step for velocity and acceleration<br/>        self.VELOCITY_STEP = velocity_step<br/>        self.ACCELERATION_STEP = acceleration_step<br/><br/>        # Define minimum and maximum values for acceleration when accelerating or decelerating<br/>        self.ACCELERATION_MIN_ACCELERATE = acceleration_min_accelerate<br/>        self.ACCELERATION_MAX_ACCELERATE = acceleration_max_accelerate<br/><br/>        # Calculate number of possible values for velocity and acceleration<br/>        self.num_velocity_values = int((self.VELOCITY_MAX - self.VELOCITY_MIN) / self.VELOCITY_STEP) + 1<br/>        self.num_acceleration_values = int((self.ACCELERATION_MAX - self.ACCELERATION_MIN) / self.ACCELERATION_STEP) + 1<br/><br/>        # Create list of possible values for velocity and acceleration<br/>        self.velocity_values = [self.VELOCITY_MIN + i * self.VELOCITY_STEP for i in range(self.num_velocity_values)]<br/>        self.acceleration_values = [self.ACCELERATION_MIN + i * self.ACCELERATION_STEP for i in range(self.num_acceleration_values)]</span></pre><p id="fa89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">期望的动作可以是加速、减速或保持车辆的恒定速度。</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="edb2" class="oy mz it ou b be oz pa l pb pc"> # Function to calculate available actions in a given state<br/>    def calculate_actions(self, v, a):<br/>        # Initialize list of available actions<br/>        actions = []<br/><br/>        # If current velocity is less than maximum, add option to accelerate<br/>        if v &lt; self.VELOCITY_MAX:<br/>            for a_new in self.acceleration_values:<br/>                if self.ACCELERATION_MIN_ACCELERATE &lt;= a_new &lt;= self.ACCELERATION_MAX_ACCELERATE:<br/>                    actions.append((v, a_new))<br/><br/>        # If current velocity is greater than minimum, add option to decelerate<br/>        if v &gt; self.VELOCITY_MIN:<br/>            for a_new in self.acceleration_values:<br/>                if -self.ACCELERATION_MAX_ACCELERATE &lt;= a_new &lt;= -self.ACCELERATION_MIN_ACCELERATE:<br/>                    actions.append((v, a_new))<br/><br/>        # Add option to maintain current velocity and acceleration<br/>        actions.append((v, a))<br/><br/>         return actions</span></pre><p id="37ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们可以定义一个函数来计算预期油耗:</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="343b" class="oy mz it ou b be oz pa l pb pc"># Function to evaluate the expected fuel consumption for a given state and action<br/>    def evaluate_fuel_consumption(self, v, a, v_new, a_new):<br/>        # Calculate expected fuel consumption for current state and action<br/>        fuel_current = self.fuel_consumption(v, a)<br/>        fuel_new = self.fuel_consumption(v_new, a_new)<br/>        return fuel_current + fuel_new</span></pre><p id="2ebf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算最优策略的一种简单方法是扫描整个状态空间:</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="831c" class="oy mz it ou b be oz pa l pb pc"># Function to find the optimal action in a given state, based on minimizing expected fuel consumption<br/>    def find_optimal_action(self, v, a):<br/>        # Calculate available actions in current state<br/>        actions = self.calculate_actions(v, a)<br/><br/>        # Initialize minimum expected fuel consumption<br/>        min_fuel = float("inf")<br/><br/>        # Initialize optimal action<br/>        optimal_action = None<br/><br/>        # Iterate over available actions and find action with minimum expected fuel consumption<br/>        for v_new, a_new in actions:<br/>            fuel = self.evaluate_fuel_consumption(v, a, v_new, a_new)<br/>            if fuel &lt; min_fuel:<br/>                min_fuel = fuel<br/>                optimal_action = (v_new, a_new)<br/><br/>        return optimal_action<br/><br/>    # Function to calculate the optimal policy for the MDP<br/>    def calculate_optimal_policy(self):<br/>        # Initialize dictionary to store optimal policy<br/>        optimal_policy = {}<br/><br/>        # Iterate over all possible states and calculate optimal action for each state<br/>        for v in self.velocity_values:<br/>            for a in self.acceleration_values:<br/>                optimal_policy[(v, a)] = self.find_optimal_action(v, a)<br/><br/>        return optimal_policy</span></pre><p id="8a9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上代码片段的测试实现如下:</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="cad7" class="oy mz it ou b be oz pa l pb pc"># Create MDP instance<br/>mdp = MDP(VELOCITY_MIN, VELOCITY_MAX, ACCELERATION_MIN, ACCELERATION_MAX, VELOCITY_STEP, ACCELERATION_STEP, ACCELERATION_MIN_ACCELERATE, ACCELERATION_MAX_ACCELERATE)<br/><br/># Calculate optimal policy for the MDP<br/>optimal_policy = mdp.calculate_optimal_policy()<br/><br/># Print optimal policy for the first few states<br/>for i in range(10):<br/>    for j in range(10):<br/>        print(optimal_policy[(mdp.velocity_values[i], mdp.acceleration_values[j])])</span></pre><p id="760a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面例子的完整代码可以从<a class="ae ky" href="https://gist.github.com/rahulbhadani/92d3be52529a64372c796ca5e7cb3770" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/rahulbhadani/92d 3 be 52529 a 64372 c 796 ca 5 e 7 CB 3770</a>下载。</p><blockquote class="mo"><p id="11df" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">现在，我们可能会问一个问题:上述实现是否高效？</p></blockquote><p id="c822" class="pw-post-body-paragraph kz la it lb b lc pd ju le lf pe jx lh li pf lk ll lm pg lo lp lq ph ls lt lu im bi translated">我们清楚地看到，上面的实现扫描了整个状态空间，效率不是很高。为了提高这种实现的效率，我们可以使用动态编程来存储每个状态的最佳动作，然后使用存储的值来计算整个 MDP 的最佳策略，而不需要迭代所有可能的状态。通过利用任何给定状态的最优策略仅取决于从当前状态可以到达的状态的最优策略的事实，这将允许更有效地计算最优策略。</p><p id="b981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种选择是使用函数逼近来逼近最优策略，这比显式计算所有可能状态的最优策略更有效。这可以通过在状态空间的代表性样本上训练诸如神经网络(深度 RL)的模型，然后使用训练的模型来预测任何给定状态的最佳动作来完成。</p><p id="6b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，我们应该可以轻松地讨论贝尔曼方程、动态规划和 Q 函数。</p><blockquote class="mo"><p id="5889" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">贝尔曼方程在控制理论或控制工程中也称为汉密尔顿-雅可比方程。</p></blockquote><h1 id="39e4" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">贝尔曼方程:价值函数和 Q 函数</h1><p id="c4fa" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在上述定义的 MDP 中，目标是长期最小化燃料消耗，贝尔曼方程在确定最优策略中起着重要作用。贝尔曼方程提供了状态值和从当前状态可达到的状态值之间的递归关系，可用于通过找到导致具有最高值的状态的动作来确定最佳策略。</p><p id="afab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种递归使用迭代计算机科学算法来解决，如动态规划和线性规划。它们的变化导致了多种 RL 训练算法。</p><p id="de8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">状态值和状态-动作值的贝尔曼方程分别称为值函数和 Q 函数。</p><h2 id="321c" class="ob mz it bd na oc od dn ne oe of dp ni li og oh nk lm oi oj nm lq ok ol no om bi translated">价值函数</h2><p id="0642" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在 MDP，状态的值被定义为从当前状态开始的长期的预期燃料消耗。我们称之为价值函数。</p><p id="0f8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上，价值函数可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/a8e3eebe43e4dbcaeac86816a7027385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcn14QD13AKKx8kkg30zzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 3。价值函数。</p></figure><p id="976e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="mn"> P( s(t)，a(t) ) </em>在动作<em class="mn"> a(t) </em>的影响下，从状态<em class="mn"> s(t) </em>到<em class="mn"> s(t +1 ) </em>的转移概率。等式 3 的定义是从<a class="ae ky" href="https://doi.org/10.1007/978-981-19-0638-1" rel="noopener ugc nofollow" target="_blank"> 1 </a>得到的值函数的修改形式。</p><p id="04fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">价值函数可用于通过将当前状态下的预期燃料消耗与下一状态下的预期燃料消耗相加来计算状态的价值，其中下一状态下的预期燃料消耗通过在当前状态下可采取的所有可能行动中取最大值来计算。这个过程可以递归地重复，以计算每个状态的值，从初始状态开始，向后工作到最终状态。</p><p id="fa04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦使用贝尔曼方程计算了所有状态的值，就可以通过找到导致每个状态具有最高值的状态的动作来确定最优策略。然后，该最佳策略可用于确定在每个状态下采取的最佳行动，以便在长期内将燃料消耗降至最低。</p><h2 id="271a" class="ob mz it bd na oc od dn ne oe of dp ni li og oh nk lm oi oj nm lq ok ol no om bi translated">q 函数</h2><p id="2066" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在上面定义的 MDP 的情况下，目标是在长期内最小化燃料消耗，Q 函数是将每个状态和动作对映射到实数的函数，表示从该状态开始并采取该动作的长期内的预期燃料消耗。</p><p id="b0bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上，Q 函数可以写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/98b8fb7cc54be928e9e36e4105f7bcc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAdajwA_NosQYooEzRmX_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式 4。q 函数</p></figure><p id="d7b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是在[ <a class="ae ky" href="https://doi.org/10.1007/978-981-19-0638-1" rel="noopener ugc nofollow" target="_blank"> 1 </a>中使用的 Q 函数的修改形式。</p><p id="6f64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用贝尔曼方程来计算 Q 函数，该方程提供了状态值和从当前状态可以达到的状态值之间的递归关系。可以通过将当前状态下的预期燃料消耗与下一状态下的预期燃料消耗相加来计算 Q 函数，其中下一状态下的预期燃料消耗是通过采取当前状态下可以采取的最大总可能行动来计算的。</p><p id="2a2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦使用贝尔曼方程计算出所有状态-动作对的值，就可以通过找到使每个状态的 Q 函数最大化的动作来确定最佳策略。然后，该最佳策略可用于确定在每个状态下采取的最佳行动，以便在长期内将燃料消耗降至最低。</p><p id="c9a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看他们如何寻找本文中的例子。</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="fa79" class="oy mz it ou b be oz pa l pb pc"><br/>class MDP:<br/>    # ...<br/><br/>    # Function to calculate the value function for the MDP<br/>    def calculate_value_function(self):<br/>        # Initialize dictionary to store values of each state<br/>        values = {}<br/><br/>        # Iterate over all possible states and calculate value of each state<br/>        for v in self.velocity_values:<br/>            for a in self.acceleration_values:<br/>                values[(v, a)] = self.evaluate_value(v, a, values)<br/><br/>        return values<br/><br/>    # Function to evaluate the value of a state using the Bellman equation<br/>    def evaluate_value(self, v, a, values):<br/>        # Check if value of current state has already been calculated<br/>        if (v, a) in values:<br/>            return values[(v, a)]<br/><br/>        # Calculate available actions in current state<br/>        actions = self.calculate_actions(v, a)<br/><br/>        # Initialize maximum expected fuel consumption<br/>        max_fuel = float("-inf")<br/><br/>        # Iterate over available actions and find action with maximum expected fuel consumption<br/>        for v_new, a_new in actions:<br/>            fuel = self.evaluate_fuel_consumption(v, a, v_new, a_new)<br/>            if fuel &gt; max_fuel:<br/>                max_fuel = fuel<br/><br/>        # Return maximum expected fuel consumption<br/>        return max_fuel</span></pre><pre class="pk ot ou ov bn ow ox bi"><span id="01b3" class="oy mz it ou b be oz pa l pb pc">class MDP:<br/>    # ...<br/><br/>    # Function to calculate the Q-function for the MDP<br/>    def calculate_q_function(self):<br/>        # Initialize dictionary to store values of each state-action pair<br/>        q_values = {}<br/><br/>        # Iterate over all possible states and actions<br/>        for v in self.velocity_values:<br/>            for a in self.acceleration_values:<br/>                for v_new, a_new in self.calculate_actions(v, a):<br/>                    q_values[((v, a), (v_new, a_new))] = self.evaluate_q_value(v, a, v_new, a_new, q_values)<br/><br/>        return q_values<br/><br/>    # Function to evaluate the Q-value of a state-action pair using the Bellman equation<br/>    def evaluate_q_value(self, v, a, v_new, a_new, q_values):<br/>        # Check if Q-value of current state-action pair has already been calculated<br/>        if ((v, a), (v_new, a_new)) in q_values:<br/>            return q_values[((v, a), (v_new, a_new))]<br/><br/>        # Calculate expected fuel consumption in current state<br/>        fuel = self.evaluate_fuel_consumption(v, a, v_new, a_new)<br/><br/>        # Calculate expected fuel consumption in next state by taking maximum over all possible actions<br/>        max_fuel = float("-inf")<br/>        for v_next, a_next in self.calculate_actions(v_new, a_new):<br/>            fuel_next = self.evaluate_q_value(v_new, a_new, v_next, a_next, q_values)<br/>            if fuel_next &gt; max_fuel:<br/>                max_fuel = fuel_next<br/><br/>        # Return expected fuel consumption in current state plus expected fuel consumption in next state<br/>        return fuel + max_fuel</span></pre><p id="029a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，该示例忽略了转移概率的概念，因此用于优化燃料消耗的示例的值函数和 Q 函数比用于一些现实世界的实际问题要简单得多。</p></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><p id="c5a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我讨论了通过求解 MDP 来获得最优策略、关联值函数和 Q 函数，这些函数可用于以最优方式求解 MDP。</p><p id="68c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章中，我将在强化学习问题的背景下讨论动态编程。在以后的文章中，我将讨论深层的 RL 概念以及在这种场景中引入神经网络，并给出一些仿真示例。加入我的<a class="ae ky" href="https://rahulbhadani.medium.com/subscribe" rel="noopener">电子邮件列表</a>，让我未来的文章直接发送到你的收件箱。</p><p id="8c65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你还没有看完基金会 RL 系列的第一篇文章，请一定要去看:<a class="ae ky" rel="noopener" target="_blank" href="/foundational-rl-markov-states-markov-chain-and-markov-decision-process-be8ccc341005">https://towards data science . com/fundamental-RL-Markov-States-Markov-chain-and-Markov-decision-process-be 8 CCC 341005</a>。</p></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><blockquote class="mo"><p id="9314" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated"><em class="nz">你喜欢这篇文章吗？</em> <a class="ae ky" href="https://www.buymeacoffee.com/rahulbhadani" rel="noopener ugc nofollow" target="_blank"> <em class="nz">给我买杯咖啡</em> </a> <em class="nz">。</em></p><p id="d746" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated"><em class="nz">爱我的文字？加入我的</em> <a class="ae ky" href="https://rahulbhadani.medium.com/subscribe" rel="noopener"> <em class="nz">邮箱列表</em> </a> <em class="nz">。</em></p><p id="ba76" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated"><em class="nz">想了解更多 STEM 相关话题？加入</em> <a class="ae ky" href="https://rahulbhadani.medium.com/membership" rel="noopener"> <em class="nz">中等</em> </a></p></blockquote><h1 id="678e" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">参考</h1><ol class=""><li id="bfdb" class="ps pt it lb b lc nq lf nr li pu lm pv lq pw lu px py pz qa bi translated">深度强化学习，Aske Plaat，<a class="ae ky" href="https://doi.org/10.1007/978-981-19-0638-1," rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-981-19-0638-1,</a>施普林格</li><li id="ac0e" class="ps pt it lb b lc qb lf qc li qd lm qe lq qf lu px py pz qa bi translated">强化学习和随机优化:连续决策的统一框架。)，威利(2022)。精装本。ISBN 9781119815051。</li><li id="a66e" class="ps pt it lb b lc qb lf qc li qd lm qe lq qf lu px py pz qa bi translated">罗纳德·霍华德(1960)。<a class="ae ky" href="http://web.mit.edu/dimitrib/www/dpchapter.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mn">动态规划与马尔可夫过程</em> </a> (PDF)。麻省理工学院出版社。</li></ol></div></div>    
</body>
</html>