<html>
<head>
<title>Learning Facial Features Using Non-Negative Matrix Factorization Through Scratch Coding In R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过R中的划痕编码使用非负矩阵分解学习面部特征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-the-facial-features-using-non-negative-matrix-factorization-from-scratch-in-r-e0a82c836775#2022-10-16">https://towardsdatascience.com/learning-the-facial-features-using-non-negative-matrix-factorization-from-scratch-in-r-e0a82c836775#2022-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8b79" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">动手香草建模第四部分</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/10844792ea1842b219006f4e91ec874e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dblCJnPayfENLFO3VktHXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="88dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">转向离你最近的窗户，试着凝视外面你的眼睛可能遇到的任何物体。现在问你自己一个问题，在识别物体的时候，你的大脑是把它看成一个整体，还是物体的某些部分或特征足以让你决定这个物体是什么？我们的大脑能够通过识别一个物体的各个部分来识别它，而不必看到整个物体，这是一个非常有趣的现象。当看到一朵抽象形状的云时，我们的大脑会自动将漂浮的形状与最相似的物体联系起来。心理学研究[ <a class="ae lr" href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295x.94.2.115" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，生理学研究[ <a class="ae lr" href="https://academic.oup.com/cercor/article-abstract/4/5/509/332250" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]，以及计算机视觉的电子神经网络架构等计算研究[ <a class="ae lr" href="https://mitpress.mit.edu/9780262710077/high-level-vision/" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]调查并支持大脑中基于部分的表示的想法。</p><p id="e78e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一种这样的计算机视觉技术被称为非负矩阵分解(NMF ),它巧妙地学习对象的特征，并且<strong class="kx ir">可能是最简单和容易实现的算法</strong>。NMF作为一种探索性因素分析于1994年由Paatero和Tapper提出，并于1999年由Daniel D . Lee&amp;<a class="ae lr" href="https://www.nature.com/articles/44565#auth-H__Sebastian-Seung" rel="noopener ugc nofollow" target="_blank">H Sebastian Seung</a><a class="ae lr" href="https://www.nature.com/articles/44565#Bib1" rel="noopener ugc nofollow" target="_blank">4</a>进一步推广。它仍然是图像和文本挖掘的流行算法。NMF的基本原理是基于非负约束的部分学习，这意味着NMF只有在感兴趣的数据矩阵没有负条目时才有用。</p><p id="7ae0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">鼓励对实现和代码感兴趣的读者快速阅读下一部分，以便理解我将在数据准备和代码中使用的维度符号。还有，随意探索一下动手香草造型系列的 <a class="ae lr" rel="noopener" target="_blank" href="/a-laymans-guide-to-building-your-first-image-classification-model-in-r-using-keras-b285deac6572?source=your_stories_page-------------------------------------"> <em class="ls">第一部分</em> </a>、<a class="ae lr" rel="noopener" target="_blank" href="/understanding-a-neural-network-through-scratch-coding-in-r-a-novice-guide-a81e5777274b?source=your_stories_page-------------------------------------"> <em class="ls">第二部分、</em> </a> <em class="ls">和</em> <a class="ae lr" rel="noopener" target="_blank" href="/logistic-regression-explained-from-scratch-visually-mathematically-and-programmatically-eb83520fdf9a"> <em class="ls">第三部分</em> </a> <em class="ls">。欢呼:)</em></p><h1 id="46b1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">了解NMF</h1><p id="c3bc" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">让我们直接开始理解NMF背后的数学。本质上，NMF通过将数据分解成权重和因子来减少数据，使得原始矩阵和新形成的矩阵的乘积之间的距离(欧几里德或弗罗贝纽斯)最小。</p><p id="df8e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">设<strong class="kx ir">V</strong>(<em class="ls">m</em>x<em class="ls">n</em>)<em class="ls"/>为我们的数据矩阵。目标是将<strong class="kx ir"> V </strong>分解为低维<em class="ls"> k </em>的<em class="ls">基</em>W(<em class="ls">m</em>x<em class="ls">k</em>)<em class="ls"/>和<em class="ls">编码矩阵</em><strong class="kx ir">H</strong>(<em class="ls">k</em>x<em class="ls">n</em>)<em class="ls"/>这样<strong class="kx ir"> <em class="ls"> H </em> </strong>的各列与<strong class="kx ir"> <em class="ls"> V </em> </strong>中的列一一对应。基本上，编码列是与基矩阵的线性组合产生v列的系数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/f1adf34fd535363acc664053d97486b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxoftoYHh8ACdTRqL1oIVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d770" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，当数据矩阵<strong class="kx ir"> V </strong>的每一列表示面部图像时，基列是除了诸如眼睛、鼻子、嘴唇等特征之外什么也不是的基图像。，而H的列指示哪个特征以什么强度出现在哪个图像中。<em class="ls"> k </em>维度表示数据的特征(部分)。通常选择<em class="ls"> k </em>使得(<em class="ls">m</em>+<em class="ls">n</em>)<em class="ls">k</em>≤<em class="ls">Mn。</em>把产品<strong class="kx ir"><em class="ls">W</em></strong>x<strong class="kx ir"><em class="ls">H</em></strong>称为数据<strong class="kx ir"> V </strong>的压缩形式不会错。</p><p id="1ca8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">潜在的NMF目标是相对于<strong class="kx ir"> W </strong>和<strong class="kx ir"> H </strong>最小化<strong class="kx ir"> V </strong>和<strong class="kx ir"> W </strong> x <strong class="kx ir"> H </strong>之间的距离，同时保持<strong class="kx ir"> W </strong>和<strong class="kx ir"> H </strong>的非负性。</p><blockquote class="mr ms mt"><p id="0a44" class="kv kw ls kx b ky kz jr la lb lc ju ld mu lf lg lh mv lj lk ll mw ln lo lp lq ij bi translated">非否定性约束与组合部分以形成整体的直观概念是相容的，这就是NMF学习基于部分的表征的方式。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f2c5895575a61c12ded4acbefe752d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*HuxssnpZ5wP5KbTjq4dqJA.png"/></div></figure><p id="18c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个最小化问题无法解析地解决，因为它是一个NP-Hard问题，这意味着它在<strong class="kx ir"> W </strong>和<strong class="kx ir"> H </strong>中不是凸的。简而言之，找到全局最小值即使不是不可能，也是很困难的。对于标量情况可以观察到这种证明，其中<em class="ls"> m </em>和<em class="ls"> n </em>是<em class="ls"> </em>等于1。那么目标就是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/24e9c9e0893e50246106a2325083d079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*RMmXz5ldqAPcOT2BmSPaFg.png"/></div></figure><p id="48ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个目标函数的梯度是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/558d7e2759eb7fbedbecbdb337d8a5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*SxlKufXj3L7AalEpcXA4NQ.png"/></div></figure><p id="aa4f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">同样，黑森人会是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/7f452e7dd21c854b1ee79ff91db6a75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*-ZCwSDJxZf_pklWufY5G0w.png"/></div></figure><p id="fe92" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，可以容易地观察到，对于所有的<em class="ls"> w </em>和<em class="ls">h</em>的值，hessian并不总是<a class="ae lr" href="https://math.stackexchange.com/questions/2533610/how-do-you-recognize-a-positive-semidefinite-matrix" rel="noopener ugc nofollow" target="_blank">正半确定的</a></p><p id="9296" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于梯度下降的知识，朝向梯度的缓慢步骤应该引导我们到达目标函数的最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f8a396920812844581e26a966efc4964.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*YOZNeegPMGX4LeXe4CxybQ.png"/></div></figure><p id="47dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里α和β是学习率。原则上，这些学习率可以估计为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/15fa24b29b18ffeb4e28e6931a1aa51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*C4C6nRwZohMmvuOvCN4mGQ.png"/></div></figure><p id="33d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从梯度矩阵中代入学习率和梯度，更新规则变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/79f3ca902e4077d1014b454911deaa0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*R3sCakEklbCknZgxzETc_Q.png"/></div></figure><p id="8ec7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从针对<em class="ls"> W </em>和<em class="ls"> H </em>的非负初始条件开始，针对非负<em class="ls"> V </em>的这些更新规则的迭代通过收敛到目标函数的局部最小化来找到<em class="ls"> V </em> ≈ <em class="ls"> WH </em>的近似因式分解。</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><h1 id="0bed" class="lt lu iq bd lv lw nl ly lz ma nm mc md jw nn jx mf jz no ka mh kc np kd mj mk bi translated">履行</h1><p id="d121" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">为了学习面部特征，我们使用了<a class="ae lr" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank"> LFW(野生标记人脸)</a>数据集，这是一个面部照片数据库，旨在研究无约束人脸识别问题。该数据集包含从网络上收集的13，000多张面部图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/4b45abcdf84d7ad82394fa585c5629b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWr8v8gSXdNmGF19daBnZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">LFW的拼贴画(作者)</p></figure><p id="d155" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NMF的代码如下。W和H矩阵由来自均匀分布的值初始化。默认情况下，迭代次数设置为500，但是50次迭代足以达到可观的收敛。</p><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="391b" class="nw lu iq ns b gy nx ny l nz oa">#---------- NMF ---------------</span><span id="5531" class="nw lu iq ns b gy ob ny l nz oa">NMF&lt;-function(X, K, ite = 500, verbose){<br/>  <br/>  N=nrow(X); M=ncol(X)<br/>  W&lt;-matrix(runif(N*K), nrow = N)<br/>  H&lt;-matrix(runif(K*M), nrow = K)<br/>  <br/>  loss&lt;-NULL</span><span id="f3a9" class="nw lu iq ns b gy ob ny l nz oa">#-------Update------------#  <br/>  for(i in 1:ite){<br/>    <br/>    H_num&lt;-t(W)%*%X<br/>    H_deno&lt;-t(W)%*%W%*%H + 1e-10<br/>    H&lt;-H*(H_num/H_deno)<br/>    <br/>    W_num&lt;-X%*%t(H)<br/>    W_deno&lt;-W%*%H%*%t(H) + 1e-10<br/>    W&lt;-W*(W_num/W_deno)<br/>    <br/>    loss[i]&lt;-sum((X-W%*%H)^2)<br/>    <br/>    if(i%%500==0 &amp; verbose == T){print(paste("iteration----------",i, sep = " "))<br/>      print(sum((X-W%*%H)^2))}<br/>    <br/>  }<br/>  <br/>  return(list(Basis = W, Encoding = H, loss = loss))<br/>}</span></pre><p id="6ee6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，我们将只使用500张随机选择的图片。第一步，面部图像被缩放到150×150像素。然后，每个图像的像素被标准化，使得平均值和标准偏差等于0.25。最后，图像像素被裁剪到范围[0，1]以允许非负性。</p><p id="b320" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在步骤2中，每个图像像素然后被展平以成为列向量。将所有展平的图像组合成列，我们得到数据矩阵v。</p><p id="8914" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在步骤3中，用随机初始化的<em class="ls"> W </em>和<em class="ls"> H </em>用上述迭代算法在V上执行NMF。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/5b43c12813ab3ccee6188e75ead6e37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvknLVuUjs7rSfZsMug2KQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">工作流程(作者图片)</p></figure><pre class="kg kh ki kj gt nr ns nt nu aw nv bi"><span id="3a5a" class="nw lu iq ns b gy nx ny l nz oa">setwd(".\\medium\\NMF\\lfw")<br/>set.seed(123)<br/>main.dir&lt;-".\\medium\\NMF\\lfw"<br/>my.list&lt;-list(NULL)<br/>index&lt;-1<br/>for(j in seq_along(dir())){ #reading all the image and storing into a list<br/>  <br/>  paste(main.dir,"\\",dir()[j], sep="")%&gt;%setwd()<br/>  <br/>  for(i in seq_along(dir())){<br/>    <br/>    my.list[[index]]&lt;-readImage(dir()[i])<br/>    index&lt;-index+1<br/>    <br/>  }<br/>  setwd(main.dir)<br/>  <br/>}<br/>#-------------------------Functions------------------------#<br/>zscale&lt;-function(x, u, sigma){ (x-mean(x, na.rm=T))*sigma/sd(x, na.rm = T) + u} #standardization function<br/>minmax&lt;-function(x){ (x-min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T)) } #min max function<br/>#---------------------------------#<br/>images=500 #number of images to be selected from the data base<br/>pixel=150<br/>sample.image&lt;-sample(size = images, c(1:length(my.list)), replace = F)<br/>my.new.list&lt;-my.list[c(sample.image)]<br/>#-------Step 1-------------------#<br/>for(i in 1:images){my.new.list[[i]]&lt;-c(channel(my.new.list[[i]], "grey")%&gt;%resize(pixel, pixel))%&gt;%zscale(u=0.25, sigma = 0.25)%&gt;%minmax()}<br/>#---------------Step 2--------------------------#<br/>V&lt;-unlist(my.new.list)%&gt;%matrix(nrow = pixel*pixel, byrow = F)<br/>#---------------Step 3------------------#<br/>a&lt;-NMF(X = V, K = 100, ite=1000, verbose = F)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/e9ec1a5ffab0b0a0be67a6dfb3a8113b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z1zv8fN4jGn3PQPEJ-Mg1A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/a20ff623a94b4081a7fd357b0459053c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7gkr2Vmole39BAx0kVI-vg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">损失(按作者)</p></figure><p id="c4c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NMF同时执行学习和推理。也就是说，它既学习一组基础图像，又从可见变量中推断隐藏变量的值。</p><p id="bf23" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">局限性</strong> <br/>虽然NMF在学习面部零件方面是成功的，但这并不意味着该方法可以从任何数据库中学习零件，例如从极其不同的角度观看的物体图像或高度铰接的物体。虽然非负约束可以帮助这种模型学习基于部分的表征，但NMF的发明者并不认为它们本身就足够了。此外，NMF不知道各部分之间的“句法”关系。NMF假设隐变量是非负的，但没有进一步假设它们的统计相关性。人们还必须认识到，一种可以在人们不知情的情况下远程识别或分类的技术从根本上来说是危险的。跟随<a class="ae lr" href="https://www.nature.com/articles/d41586-020-03187-3" rel="noopener ugc nofollow" target="_blank">这篇</a>更好地理解面部识别或特征学习模型的伦理衍生。</p><p id="8ad1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望这篇文章能让你对NMF有所了解，作为一名读者，你会喜欢它的内容。从我的<a class="ae lr" href="https://github.com/AbhibhavS/NMF/tree/main" rel="noopener ugc nofollow" target="_blank"> GitHub </a>访问代码。留下来更多的手香草造型系列。干杯！</p><blockquote class="mr ms mt"><p id="c97c" class="kv kw ls kx b ky kz jr la lb lc ju ld mu lf lg lh mv lj lk ll mw ln lo lp lq ij bi translated"><a class="ae lr" rel="noopener" target="_blank" href="/logistic-regression-explained-from-scratch-visually-mathematically-and-programmatically-eb83520fdf9a">从零开始解释的逻辑回归(可视化、数学化、程序化)</a> <br/> <a class="ae lr" rel="noopener" target="_blank" href="/understanding-a-neural-network-through-scratch-coding-in-r-a-novice-guide-a81e5777274b">通过R中的Scratch编码理解一个神经网络；新手指南</a> <br/> <a class="ae lr" rel="noopener" target="_blank" href="/a-laymans-guide-to-building-your-first-image-classification-model-in-r-using-keras-b285deac6572">使用Keras建立你的第一个R图像分类模型的外行指南</a></p></blockquote><h1 id="90b0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">资源</h1><blockquote class="mr ms mt"><p id="20cc" class="kv kw ls kx b ky kz jr la lb lc ju ld mu lf lg lh mv lj lk ll mw ln lo lp lq ij bi translated"><a class="ae lr" href="https://www.nature.com/articles/44565#Bib1" rel="noopener ugc nofollow" target="_blank">通过非负矩阵分解学习物体的部件</a></p><p id="a3b3" class="kv kw ls kx b ky kz jr la lb lc ju ld mu lf lg lh mv lj lk ll mw ln lo lp lq ij bi translated"><a class="ae lr" href="https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/" rel="noopener ugc nofollow" target="_blank">非负矩阵分解的原因和方法</a></p></blockquote></div></div>    
</body>
</html>