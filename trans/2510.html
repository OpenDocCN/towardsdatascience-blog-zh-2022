<html>
<head>
<title>Risk Prediction with EHR Data using Hierarchical Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于分层注意机制的EHR数据风险预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/risk-prediction-with-ehr-data-using-hierarchical-attention-mechanism-685028e01faa#2022-05-31">https://towardsdatascience.com/risk-prediction-with-ehr-data-using-hierarchical-attention-mechanism-685028e01faa#2022-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a09b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">LSAN基本指南:用分级注意建模长期依赖和短期关联</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e6908a176bdec207ed5550692187c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVWzDX-tPmZzgefCZO74aQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/jniittymaa0-701650/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1185076" rel="noopener ugc nofollow" target="_blank"> Jukka Niittymaa </a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1185076" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="9f76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">电子健康记录(EHR)是全面的历史健康记录，包含患者就医时的症状。EHR数据具有两级层次结构，由一组按时间顺序排列的就诊组成，在每次就诊中，有一组无序的诊断代码。诊断代码可以属于ICD-9或ICD-10格式，表示某种疾病的症状。</p><p id="8e85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">风险预测是医疗保健行业中最流行的问题陈述之一。风险预测是指对未来某一疾病高风险成员的预测。现有方法侧重于对时间访问进行建模，而忽略了对访问中的诊断代码进行建模的重要性，并且访问中大量与任务无关的信息通常导致现有方法的性能不令人满意。</p><p id="cfca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论如何通过保持长期依赖性和短期相关性来执行风险预测。</p><h1 id="f2b6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">LSAN简介:</h1><blockquote class="mn mo mp"><p id="e396" class="kz la mq lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated"><strong class="lb iu">参考论文:</strong><a class="ae ky" href="https://dl.acm.org/doi/abs/10.1145/3340531.3411864" rel="noopener ugc nofollow" target="_blank">LSAN——对风险预测的长期依赖和短期相关性进行分层关注建模</a></p></blockquote><p id="60f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EHR数据由两级层次结构组成:就诊层次结构和每次就诊的诊断代码层次结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/199f4cc2d073b04be124017ab3b84b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7dJ4SZIMaiGeiXQfoqqAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来源)，EHR的等级表示图</p></figure><p id="3543" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文提出了一个LSAN深度神经网络模型来模拟EHR数据的层次结构。</p><h1 id="d7f1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">任务:</h1><p id="1efe" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">任务是使用纵向EHR数据𝑯 ∈ R𝑚×n.计算能够预测患者‘p’在未来可能发生的某些疾病的函数f。该函数的主要关注点是从患者数据h中提取隐藏的疾病进展信息，并处理噪声信息的问题。</p><h2 id="f214" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated">输入符号:</h2><p id="b25d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">对于每个病人<code class="fe nm nn no np b">‘p’</code>，它期望历史诊断结果作为一个顺序列表𝑯=【𝒉1，𝒉2,…，𝒉𝑛】，其中𝒉𝑖是𝑖-the就诊的诊断结果，n是就诊次数。</p><p id="563e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个就诊诊断结果由ICD-9代码的子集<code class="fe nm nn no np b"><strong class="lb iu">𝑪 = {𝒄1,𝒄2,…,𝒄𝑚}</strong></code>组成，其中𝑚是数据集中唯一诊断代码的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/08ca385af272ae46d3eeefe2184acef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*KkTmnLuKvikMLGKEKC7GMg.png"/></div></figure><p id="8b84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此处<code class="fe nm nn no np b"><strong class="lb iu">hiCj=1</strong></code>如果就诊的诊断结果包含cj诊断代码，否则<code class="fe nm nn no np b"><strong class="lb iu">hiCj=0</strong></code></p><h1 id="c32d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">创意:</strong></h1><p id="8ef8" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">LSAN是一种端到端模型，</p><ol class=""><li id="edec" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated"><strong class="lb iu"> HAM </strong>(在诊断代码层次中):它用设计的诊断代码级注意力学习访问嵌入。</li><li id="31f4" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">TAM (时间聚合模块):它捕获访问之间的长期依赖性和短期相关性。</li><li id="63c4" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated"><strong class="lb iu"> HAM </strong>(在就诊层级中):TAM的输出用于通过HAM中的就诊级注意力来学习最终的综合患者表现。</li><li id="acfb" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated"><strong class="lb iu">分类器</strong>:综合表示用于预测</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/4b46c75148209c62d1d5f08741b06f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sp9LdO3RKt8Nc0SiBvjAiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://dl.acm.org/doi/10.1145/3340531.3411864" rel="noopener ugc nofollow" target="_blank">来源</a>)，LSAN架构</p></figure><p id="ead3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">HAM的功能是利用HER的分层表示，并且它具有注意机制来去除EHR数据中的噪声。</p><h2 id="66a0" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak"> HAM(在诊断代码层级中):</strong></h2><h2 id="23f9" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak">用途:</strong></h2><p id="3e73" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在诊断代码的层次结构中，我们应该减少噪声信息，以便为每次访问学习更好的嵌入。在每次就诊中，可能存在与目标类别无关的诊断代码。因此，我们需要区分每次就诊中诊断代码的重要性。</p><p id="bdd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">HAM利用分级注意机制来更加注意与我们的目标疾病相关的诊断代码，而较少注意其他代码。</p><p id="182c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施:</strong></p><ol class=""><li id="239b" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">由于𝑯 ∈ R^(m*n)是一个稀疏矩阵，不适合表示学习，所以想法是学习每个代码的密集嵌入</li><li id="862d" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">HAM首先通过一层前馈网络FFN1将每个诊断码𝒄𝑖编码成密集嵌入的𝒆𝑖 ∈ R𝑑</li><li id="7de3" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">HAM首先通过1层前馈网络FFN1 <code class="fe nm nn no np b">𝒆𝑖 = FFN1 (𝒄𝑖) = ReLU(𝑾1𝒄𝑖 + 𝒃1), where 𝑾1 ∈ R𝑑×𝑚, 𝒃1 ∈ R^d</code>将每个诊断码𝒄𝑖编码成密集嵌入的𝒆𝑖 ∈ R𝑑</li><li id="8b6b" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">𝑬 = [𝒆1,…，𝒆𝑚] ∈ R^(𝑑×m)，m个唯一诊断码的d维稠密表示</li><li id="f838" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">对于𝑖-th访问，我们获得一个密集嵌入集𝑺𝑖 = [𝒔1,…,𝒔𝑚]其中𝒔𝑗 = 𝒆𝑗如果𝒉𝑖𝑗 = 1以反映某种症状或疾病的存在，否则𝒔𝑗 = 0</li><li id="e2b7" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">尽管如此，𝑺𝑖 ∈ R𝑑×m对于学习过程来说是多余的。现在哈姆提取每次访问的潜在信息，并将其表示为<code class="fe nm nn no np b"><strong class="lb iu">𝒉¯𝑖 ∈ R𝑑</strong></code></li><li id="b34d" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">为了提取潜在的访问信息，HAM使用了一个三层前馈网络(FFN_2)。FFN_2学习每个密集嵌入𝑺𝑖的注意力权重并一起出席它们。</li><li id="d3db" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">我们得到每个诊断代码的注意力得分𝛼𝑖∈r(𝛼𝑖= ff N2(𝒔𝑖))，如果𝒔𝑖 = 0，我们设置𝛼𝑖=∞。用softmax: <code class="fe nm nn no np b"><strong class="lb iu">𝑎𝑖 = exp(𝛼𝑖)/( Σ𝑗=1tom exp(𝛼𝑗) )</strong></code>归一化关注度，这里𝑎𝑖是归一化权重</li><li id="952d" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">现在，我们为𝑖-th就诊<code class="fe nm nn no np b"><strong class="lb iu">(𝒉¯𝑖 = Σ𝑖=(1 to m) 𝑎𝑖 · 𝒔𝑖).</strong></code>获得了单个嵌入𝒉 𝑖。结果，在诊断代码的层级中，我们获得了患者‘p’的一组关注特征<code class="fe nm nn no np b"><strong class="lb iu">𝑯¯ = [𝒉¯1,…, 𝒉¯𝑛] ∈ R𝑑×𝑛</strong></code>。</li></ol><h2 id="cbcc" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak"> TAM(在诊断代码层级中):</strong></h2><h2 id="a2c3" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak">用途:</strong></h2><p id="cf06" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">TAM将访问嵌入与来自全局和时态结构的两种时态信息聚合在一起。当所有访问的特征被放入TAM时，</p><ol class=""><li id="c641" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">它通过<strong class="lb iu"> Transformer </strong>对全球结构中的长期依赖关系进行建模，例如在患者的整个医疗旅程中，每次就诊如何与其他就诊相关联。</li><li id="5259" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">通过<strong class="lb iu">卷积层</strong>在本地结构中进行短期关联，例如每个访问如何在短时间内与其他访问相关联。</li></ol><p id="eacc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施:</strong></p><ol class=""><li id="881a" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated"><strong class="lb iu"> TAM通过卷积进行短期相关性建模:</strong>它过滤掉来自无关诊断代码的噪声，提取每个阶段中相关的疾病进展信息，用于时间聚合</li></ol><p id="828e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。TAM在变压器长期依赖关系建模中:</strong></p><p id="e41c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">a.Transformer并行处理所有的访问功能，不会模糊每个功能的细节</p><p id="a8f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">b.我们在Transformer中使用多头自关注机制进行特征关注，TAM中的Transformer编码器具有𝑙层，其中每层中的计算是相同的</p><p id="919a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">c.将位置编码添加到𝑖-th输入访问中，𝒉 𝑡 𝑖 = 𝒉 𝑖 + 𝒕I，其中𝒕𝑖是位置编码</p><p id="ac19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">d.每层变压器都有'ℎ'头、</p><p id="922a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个时间信息都有利于学习特征的鲁棒性，因此我们连接𝒉 𝑔 𝑖和𝒉 𝑙 𝑖以获得用于风险预测的特征𝒉𝑖 ∈ R2𝑑，</p><blockquote class="mn mo mp"><p id="9e2b" class="kz la mq lb b lc ld ju le lf lg jx lh mr lj lk ll ms ln lo lp mt lr ls lt lu im bi translated">𝒉𝑖=𝒉𝑙𝑖concate(𝒉𝑔𝑖)</p></blockquote><p id="71aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，TAM输出一个矩阵𝑯 = [𝒉~1,…，𝒉~𝑛] ∈ R2𝑑×n</p><h2 id="53a0" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak">火腿(按拜访层次):</strong></h2><h2 id="17db" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak">用途:</strong></h2><p id="319e" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在访问层次中，我们应该注意访问之间的相关性。它捕捉了疾病的时间模式。通过提取相邻访问之间的局部时间相关性并利用长期相关性信息来滤除噪声。</p><p id="7336" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它侧重于从所有访问中提取整体语义。</p><p id="ccde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实施:</strong></p><ol class=""><li id="4110" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">类似于诊断代码层次中的HAM，它首先采用3层前馈网络FFN4来学习注意力分数𝛽𝑖 ∈ R，<code class="fe nm nn no np b"><strong class="lb iu">𝛽𝑖 = FFN4 ( 𝒉𝑖).</strong></code></li><li id="b006" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">然后，我们用softmax函数<code class="fe nm nn no np b"><strong class="lb iu">b𝑖 = exp(𝛽𝑖)/( Σ𝑗 =1m exp(𝛽𝑗)</strong></code>得到归一化的注意力权重𝑏𝑖 ∈ R</li><li id="3e65" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">风险预测的综合特征𝒙 ∈ R2𝑑通过注意机制学习，其中<code class="fe nm nn no np b"><strong class="lb iu"> 𝒙 = Σ𝑖=1n (𝑏𝑖 · 𝒉~𝑖)</strong></code></li></ol><h2 id="256a" class="na lw it bd lx nb nc dn mb nd ne dp mf li nf ng mh lm nh ni mj lq nj nk ml nl bi translated"><strong class="ak">分类器:</strong></h2><ol class=""><li id="706d" class="nr ns it lb b lc mv lf mw li og lm oh lq oi lu nw nx ny nz bi translated">最后，我们利用𝒙进行风险预测，𝑦ˆ = 𝜎(𝒘T𝒙 + 𝑏)，其中𝒘 ∈ R2d和b ∈ R</li><li id="c551" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">对于训练集t，我们使用二元交叉熵损失l来训练模型，并获得学习参数𝜽</li></ol></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h1 id="e04f" class="lv lw it bd lx ly oq ma mb mc or me mf jz os ka mh kc ot kd mj kf ou kg ml mm bi translated">结论:</h1><p id="c115" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">LSAN架构捕获访问之间的长期依赖性和短期相关性。根据论文作者进行的实验，LSAN方法的性能相对优于浅层方法(SVM、线性回归等)或RNN/RNN+注意力模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/de43fea0c835ac47e036b07472d67f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIoDcgS0bH91HAl09TbQQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://dl.acm.org/doi/10.1145/3340531.3411864" rel="noopener ugc nofollow" target="_blank">来源</a>)，不同EHR数据集上的风险预测性能比较</p></figure><h1 id="399a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><p id="9822" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">[1] LSAN论文(2020年10月19日):<a class="ae ky" href="https://dl.acm.org/doi/10.1145/3340531.3411864" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3340531.3411864</a></p><blockquote class="ow"><p id="dcbc" class="ox oy it bd oz pa pb pc pd pe pf lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>