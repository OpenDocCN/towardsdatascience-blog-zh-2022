<html>
<head>
<title>N-BEATS: Time-Series Forecasting with Neural Basis Expansion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">N-BEATS:基于神经基扩展的时间序列预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538#2022-11-25">https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538#2022-11-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f6ed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">零触发时间序列预测的深度学习模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b08c281572298b833913bb2837194b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yacgnk0WblMoYTvCTDpvJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用 DALLE [1]创建</p></figure><p id="14b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有一件事让时间序列预测变得特别。</p><p id="165a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据科学中唯一深度学习和变形金刚没有明显优于其他模型的领域。</p><p id="8e93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们使用久负盛名的<strong class="la iu"> Makridakis M 竞赛</strong>作为基准——一系列展示时间序列预测领域最新进展的大规模挑战。</p><p id="4055" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在被称为<strong class="la iu"> M4 </strong>的第四次迭代中，获胜的解决方案是<a class="ae lu" href="https://eng.uber.com/m4-forecasting-competition/" rel="noopener ugc nofollow" target="_blank">ES-RNN【2】</a>，这是优步开发的混合 LSTM &amp;指数平滑模型。有趣的是，6 个(57 个中的)纯 ML 模型表现很差，它们几乎没有超过竞争基线。</p><p id="49f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一年后，情况发生了变化。<em class="lv"> Elemental AI </em>(与<strong class="la iu"> Yoshua Bengio </strong>共同创立)发表了<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">N-BEATS【3】</strong></a><strong class="la iu"><em class="lv">，</em> </strong>一个纯深度学习模型，比 M4 获奖的 ES-RNN 模型高出 3%。但是还有更多。</p><p id="ec36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们深入描述:</p><ol class=""><li id="4308" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu"><em class="lv">N-BEATS</em>的架构，模型如何工作，为什么如此强大。</strong></li><li id="63b3" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">如何产生可解释的预测。</li><li id="ecdb" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">N 拍<em class="lv"> N 拍</em>如何实现无与伦比的零拍迁移学习。</strong></li><li id="0be3" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">为什么<em class="lv"> ARIMA </em>不能原生支持迁移学习。</strong></li></ol><p id="7510" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始吧。</p><blockquote class="mk ml mm"><p id="5988" class="ky kz lv la b lb lc ju ld le lf jx lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">如果你对时间序列预测感兴趣，可以查看我收集的最佳深度学习模型和教程。</p></blockquote><h1 id="49bf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">什么是 N-BEATS</h1><blockquote class="ni"><p id="736a" class="nj nk it bd nl nm nn no np nq nr lt dk translated">N-BEATS 是一种快速、可解释的 DL 模型，它使用完全连接层的双残差堆栈来重新创建统计模型的机制。</p></blockquote><p id="7312" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated"><strong class="la iu"> N-BEATS </strong>代表<strong class="la iu"><em class="lv">N</em></strong><em class="lv">eural</em><strong class="la iu"><em class="lv">B</em></strong><em class="lv">asis</em><strong class="la iu"><em class="lv">E</em></strong><em class="lv">x expansion</em><strong class="la iu"><em class="lv">A</em></strong><em class="lv">分析为</em><strong class="la iu"><em class="lv">T</em></strong><em class="lv">ime</em><strong class="la iu"><em class="lv">S 这家公司是由 Yoshua Bengio 联合创立的，后来被 T42 的 ServiceNow 收购。</em></strong></p><p id="a802" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv"> N-BEATS </em> <strong class="la iu"> </strong>是一个有趣的预测模型，因为:</p><ul class=""><li id="f876" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated">这是第一个超越所有成熟的统计方法的纯深度学习模型。</li><li id="c999" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">它提供了可解释的预测。</li><li id="ce3a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">它为时序上的<strong class="la iu">迁移学习</strong>奠定了基础。</li></ul><p id="411a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大约在那个时候，<strong class="la iu">亚马逊</strong>发布了其新颖的时间序列模型，被称为<a class="ae lu" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">DeepAR【4】</strong></a><strong class="la iu">。</strong>虽然<em class="lv"> DeepAR </em>包含<em class="lv"> </em>深度学习组件，但该模型也采用了一些统计概念(最大似然估计)。</p><h1 id="57fc" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">n 节拍—概述</h1><p id="3d41" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">让我们简单讨论一下<em class="lv"> N-BEATS </em>的几个关键特质:</p><ul class=""><li id="f531" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated"><strong class="la iu">多时间序列支持</strong> : <em class="lv"> N 拍</em>可以在多个时间序列上训练，每个时间序列代表一个不同的分布。</li><li id="01f4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated"><strong class="la iu">快速训练:</strong>模型不包含任何递归或自我注意层——因此，更快的训练&amp;稳定的梯度流。</li><li id="e412" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated"><strong class="la iu">多时段预测</strong>:模型产生多步预测。</li><li id="bea4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated"><strong class="la iu">可解释性:</strong>作者开发了 2 个模型版本，<strong class="la iu"> <em class="lv">通用</em> </strong>版本，以及<strong class="la iu"> <em class="lv">可解释性</em> </strong>版本。可解释版本可以输出关于趋势和季节性的可解释预测。</li><li id="0ccd" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated"><strong class="la iu">零触发迁移学习:</strong>该模型可以将其知识转移到其他时间序列数据集，取得惊人的成功。</li></ul><blockquote class="mk ml mm"><p id="0d4a" class="ky kz lv la b lb lc ju ld le lf jx lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated"><strong class="la iu">注:</strong>element ai 最初的 N-BEATS 实现仅适用于单变量时间序列。Darts 库发布了支持多元时间序列和概率输出的更新版本。在本文中，我们将重点放在原始版本上。</p></blockquote><h1 id="0d47" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">N-BEATS —通用架构</h1><p id="d62b" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">N-BEATS 架构很深，但非常简单。<strong class="la iu">图 1 </strong>显示顶层视图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/404627debcd092040056dd4c55b27d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wgLibCLYo7cSyO6moaK2Hg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 1:</strong>N-BEATS 的顶层架构(<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="4079" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意 3 件事:</p><ol class=""><li id="1586" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu"> <em class="lv">块</em> </strong>(蓝色)——基本加工单元。</li><li id="61ad" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu"> <em class="lv">栈</em> </strong>(橙色)——积木的集合。</li><li id="ce5d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu"> <em class="lv">最终型号</em> </strong>(黄色)——一叠叠的集合。</li></ol><p id="6f57" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型中的每个神经网络层只是一个密集(全连接)层。</p><p id="a2a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从第一个组件开始，即基本块:</p><h2 id="dd6d" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">1.基本块</h2><p id="c119" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">假设<code class="fe or os ot ou b">H</code>是预测范围。在<em class="lv"> N 拍</em>中，回看窗口是视界<code class="fe or os ot ou b">H</code>的倍数。</p><p id="5a91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">图 2 </strong>显示了基本块的架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0e43780b15fca3c2511a04eaec7e29b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*vFtl3dMCKaKChXY1_f9Oxw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 2: </strong>基本块架构(<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="a766" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看引擎盖下。</p><p id="577f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<strong class="la iu">图 3 中，</strong>我们<strong class="la iu"> </strong>使用来自<strong class="la iu">电力数据集【5】</strong>的纸张基准参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/f7e05ee3f69431c1dbacb29296793196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgr5wJMd5dfHLi0Fls-Zsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 3: </strong>基本块内的所有操作(图片由作者提供)</p></figure><p id="ad8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看这里会发生什么:</p><ul class=""><li id="9a80" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated">模型回顾<code class="fe or os ot ou b">3 days</code> = <code class="fe or os ot ou b">72 hours</code> = <code class="fe or os ot ou b">3 horizons</code>来预测未来 24 小时的用电量。</li><li id="c066" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">该块接收回看窗口输入。</li><li id="5d51" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">然后输入通过一个 4 层神经网络。</li><li id="f45c" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">该计算的结果被导向 2 个输出。这里密集层<strong class="la iu">密集层 5 </strong>估计θ参数(<code class="fe or os ot ou b">θ^b</code>和<code class="fe or os ot ou b">θ^f</code>)，称为<strong class="la iu">展开系数</strong>。</li><li id="95c4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">然后，使用<strong class="la iu">基</strong>层变换<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>将这些参数线性投影到新的空间，以产生<strong class="la iu"><em class="lv"/></strong>和<strong class="la iu"> <em class="lv">预测</em> </strong>信号。这个过程叫做“<strong class="la iu">神经基础扩展</strong>”。</li></ul><blockquote class="mk ml mm"><p id="789d" class="ky kz lv la b lb lc ju ld le lf jx lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated">那么<strong class="la iu">、</strong>反向预测和预测向量是如何有用的呢？</p></blockquote><p id="32f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>变换，<strong class="la iu">反向预测</strong>信号是能够最佳预测<strong class="la iu">预测</strong>信号的最佳近似向量。当<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>采取特定的形式时，反向预测和预测向量就变成了<strong class="la iu">可解释的</strong>(后面会详细说明)。</p><h2 id="c8e6" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">2.堆栈</h2><p id="c5fd" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">为了提高<em class="lv">神经扩展过程的效率，</em>作者将许多块堆叠在一起。该结构显示在<strong class="la iu">图 4: </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/452cbac49d5db55fda1791584b32f14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*yY7WxZUwKXzpf_jZVC6f7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 4: </strong>积木堆(左)和积木堆(右)——(<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="1635" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只有第一个块接收原始序列输入。下游块接收来自前一块的回播信号<code class="fe or os ot ou b"><strong class="la iu">x_l+1</strong></code> <strong class="la iu"> </strong>(其中<code class="fe or os ot ou b">l</code>是块索引，即<code class="fe or os ot ou b">l_1</code>是堆栈中的第一块)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/da2ffe363857ada2f03eaf2f496df961.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*a_CeIE0dA1VcwyGi15FxNQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 5: </strong>块内部的操作(图片由作者提供)</p></figure><p id="d46e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每个堆栈内，反向预测和预测信号被组织成两个分支:这种拓扑被称为<strong class="la iu">双重残差堆栈</strong>，并且可以由以下等式描述为<strong class="la iu"> </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b91db381568b2986f7918ee8497e84b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Itp_psm_L_KQ_Y1_FvgNiA.png"/></div></figure><p id="1558" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每个块中，模型从输入<code class="fe or os ot ou b"><strong class="la iu">x</strong>_l</code>中移除已经很好地近似的那部分反向预测信号<code class="fe or os ot ou b"><strong class="la iu">x̂</strong>_l</code>。换句话说:</p><blockquote class="ni"><p id="85ad" class="nj nk it bd nl nm nn no np nq nr lt dk translated">每个模块的模型学习最佳地逼近输入信号的一部分，并发送其余部分以由下游模块逼近。</p></blockquote><p id="7870" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">由于每个模块仅模拟输入信号的一部分，最终预测是来自所有模块的所有<em class="lv">预测</em> <code class="fe or os ot ou b">ŷ</code>信号的总和。</p><p id="2c63" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，堆叠也堆叠起来(<strong class="la iu">图 4，</strong>右)。这种架构选择进一步增加了模型的深度，并增强了其学习复杂时间序列的能力。</p><p id="d35d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经看到了通用版本的<em class="lv"> N-BEATS </em>是如何工作的。接下来，我们将描述<em class="lv">可解释的</em>版本。</p><h1 id="e5e9" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">N-BEATS 和 ARIMA 有关系吗？</h1><p id="7e81" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">如果你熟悉<strong class="la iu"> ARIMA </strong>，你可能会注意到与<em class="lv"> N 节拍</em>方法<em class="lv">的一些相似之处。</em></p><p id="0089" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv"> ARIMA </em>使用<strong class="la iu"> Box-Jenkins </strong>方法建模，这是一个迭代过程。具体来说:</p><ol class=""><li id="9f26" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">首先，我们猜测<strong class="la iu"> AR() </strong>和<strong class="la iu"> MA() </strong>函数(<code class="fe or os ot ou b">p</code>和<code class="fe or os ot ou b">q</code>参数)的顺序。</li><li id="240e" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">之后，我们使用例如最大似然估计来估计这些参数的系数。</li><li id="eee4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">然后，我们验证模型的条件是否成立。例如，模型的残差应该是正态和独立的。如果没有，我们返回到步骤 1 并重复该过程。这一次，我们在之前的基础上增加了新的<code class="fe or os ot ou b">p</code>和<code class="fe or os ot ou b">q</code>度。</li></ol><p id="6692" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，在 Box-Jenkins 的每一步中，我们都向模型中添加了更多的信息。每次迭代都会根据模型残差创建更好的输入表示。</p><p id="c147" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们可以得出结论:</p><blockquote class="ni"><p id="fb73" class="nj nk it bd nl nm nn no np nq nr lt dk translated">在 N-BEATS 中，每个连续的块仅模拟由前一个块重建的反向预测的残余误差，然后根据该误差更新预测。在拟合 ARIMA 模型时，这一过程模仿了 Box-Jenkins 方法。</p></blockquote><p id="f759" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">这两种方法的主要区别在于残差的目标函数。<em class="lv"> ARIMA </em>关注残差的质量，而<em class="lv"> N-BEATS </em>使用任意损失函数。</p><p id="09d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另外，我们不用手动调整任何带有 N 拍的方程——基变换会通过反向传播自动优化。使用<em class="lv"> ARIMA </em>然而，我们大量使用自相关和偏自相关图来猜测<code class="fe or os ot ou b">AR()</code>和<code class="fe or os ot ou b">MA()</code>函数的顺序。</p><blockquote class="mk ml mm"><p id="5971" class="ky kz lv la b lb lc ju ld le lf jx lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated"><strong class="la iu">注意:</strong>根据编程语言和库的不同，一些 ARIMA 库实现了略有不同的 Box-Jenkins 方法。在这里，我们记录了教科书的实现。</p></blockquote><h1 id="db8c" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">n-BEATS——可解释的架构</h1><p id="46e8" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">通过一些改变，<em class="lv"> N-BEATS </em>模型变得可以解释了:这些改变是:</p><ul class=""><li id="2030" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated">我们只使用 2 个堆栈，趋势堆栈<strong class="la iu">和季节性堆栈<strong class="la iu"/>。通用架构至少使用 30 个。</strong></li><li id="d0a7" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">趋势和季节性堆栈都包含 3 个块。在通用架构中，每个堆栈有一个块。</li><li id="35a6" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated"><code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>的<strong class="la iu">基础</strong>层权重在堆栈级别共享。</li></ul><p id="b022" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本思想是<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>基采取特定的形式。让我们更详细地描述它们。</p><h2 id="3fcf" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated"><strong class="ak">趋势块</strong></h2><p id="dd76" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">我们的目标是将<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>函数重构为单调的，在预测窗口中缓慢变化。</p><p id="2936" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定一个时间向量<code class="fe or os ot ou b">t=[0,1,2,…,<em class="lv">H</em>−2,<em class="lv">H</em>−1]</code> ( <code class="fe or os ot ou b">H</code>是地平线)、来自前一层的θs<code class="fe or os ot ou b">θ</code>和多项式次数<code class="fe or os ot ou b">p</code>，趋势模型被定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d3606829822b1c3438df1ad3f821f8af.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*1fEyKnCGBuw3j3wCdrjGig.png"/></div></figure><p id="0a73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，我们使用<strong class="la iu">图 3 </strong>(通用块)的架构，用上面的操作交换最后一个线性层。结果如<strong class="la iu">图 6 所示:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/b74c0156d9dd2a153fca18a184ed339b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9DBrgNZXqxwVuYvz5mDTg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 6: </strong>趋势块(图片由作者提供)</p></figure><p id="9ecc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文中没有描述逆向预测方程，但是它们可以很容易地从项目的实现中推导出来。此外，趋势和季节性块(<strong class="la iu">图 6 </strong>和<strong class="la iu">图 7 </strong>)采用来自<a class="ae lu" href="https://github.com/ServiceNow/N-BEATS/blob/master/experiments/electricity/interpretable.gin" rel="noopener ugc nofollow" target="_blank">官方项目报告</a>的参数:</p><pre class="kj kk kl km gt pc ou pd bn pe pf bi"><span id="fada" class="pg mr it ou b be ph pi l pj pk">interpretable.seasonality_layer_size = 2048<br/>interpretable.seasonality_blocks = 3<br/>interpretable.seasonality_layers = 4<br/>interpretable.trend_layer_size = 256<br/>interpretable.degree_of_polynomial = 3<br/>interpretable.trend_blocks = 3<br/>interpretable.trend_layers = 4<br/>interpretable.num_of_harmonics = 1</span></pre><h2 id="005e" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">季节性障碍</h2><p id="14ce" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">类似地，我们用适当的<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>函数替换最后一层，以捕捉季节性。傅立叶级数是一个很好的选择:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0e429654550b097e506cbbc9058445b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*v1pQbx2pQLBUffvn-4x_uQ.png"/></div></figure><p id="0926" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，架构变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/1146cf17102c5719ab2d9a4e07c5bb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNSHEkR8rQ8PWcWkpX6Flg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 7: </strong>季节性区块(图片由作者提供)</p></figure><p id="6489" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们再次强调，在所有可解释的堆栈中，堆栈中的<code class="fe or os ot ou b">g^b</code>和<code class="fe or os ot ou b">g^f</code>权重是共享的。</p><h1 id="72a3" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">实验结果</h1><p id="bc20" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">最后，作者在 3 个流行的时间序列数据集上测试了<em class="lv"> N-BEATS </em>的性能——M3[6]、M4[7]和旅游业[8]。</p><h2 id="0ee1" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">实验装置</h2><p id="d17e" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">作者将所有模型分为特定的类别，并将 N-BEATS 与每个类别的最佳模型进行比较。例如，<em class="lv"> DL/TS hybrid </em>是 M4 获奖的 ES-RNN 车型。</p><p id="5cca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于所有这些数据集都用于数据科学竞赛，因此所有参与者都依赖集成来实现最佳性能。因此，<em class="lv"> N-BEATS </em>的作者依靠集合来进行比较。他们使用了三种变体:<strong class="la iu"> N-BEATS-G </strong>(通用)<strong class="la iu">、N-BEATS-I </strong>(可解释)<strong class="la iu"> </strong>和<strong class="la iu">N-BEATS-I+G</strong>(N-BEATS-G 和 N-BEATS-I 所有模型的集合)。</p><p id="240c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最重要的是，他们创造了 6 个不同的模型，每个地平线和变化都有回顾窗口<code class="fe or os ot ou b">2H, 3H .. 7H</code>。关于组件配置的更多细节，请查看原始文件。总之，作者集合了 180 个模型来报告测试集的最终结果。</p><h2 id="9433" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">结果</h2><p id="1f09" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">所有数据集的结果如<strong class="la iu">图 8: </strong>所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/0c6bcbf4fe60ecd941a856f00c8627d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXY95UmKctKYx-Nvm_Lidg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 8: </strong>在 M3、M4 和旅游数据集上的实验结果(<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="5dc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果相当可观。</p><p id="e4e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在所有数据集上，<em class="lv"> N-BEATS </em>优于其他实现，其中<em class="lv"> N-BEATS-I+G </em>最为成功。</p><p id="4085" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，在每个数据集中，竞赛使用 MAPE、斯马普和 OWA 指标(越低越好)。这些指标在时间序列竞赛中很受欢迎。</p><blockquote class="mk ml mm"><p id="0b1e" class="ky kz lv la b lb lc ju ld le lf jx lg mn li lj lk mo lm ln lo mp lq lr ls lt im bi translated"><strong class="la iu">注意:</strong>与其他方法相反，N-BEATS 不需要任何手工制作的特征工程或输入缩放。因此，N-BEATS 在不同的时间序列任务中更容易使用。</p></blockquote><h1 id="76bf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">零射击迁移学习</h1><p id="3be2" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><em class="lv"> N-BEATS </em>的主要贡献是能够在时间序列上成功实现迁移学习。</p><h2 id="4317" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">预赛</h2><p id="bda1" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><strong class="la iu">迁移学习</strong>是一个更通用的术语——它指的是一个模型如何在不同的数据集之间转移其知识。这已经建立在计算机视觉或 NLP 任务上:我们可以下载一个预训练的模型，并通过微调将其调整到我们的数据集。</p><p id="887a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">元学习</strong>(或少量学习)是指模型只需少量训练/微调就能适应我们的数据集。最好的场景是<strong class="la iu"> <em class="lv">零触发学习</em> </strong>，其中模型不在目标数据集上训练。</p><blockquote class="ni"><p id="446b" class="nj nk it bd nl nm nn no np nq nr lt dk translated">零命中率学习是模型使用看不见的数据进行预测的能力，而无需对它们进行专门训练。这种学习方法更好地反映了人类的感知。</p></blockquote><p id="20eb" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">此外，这种向元学习的新范式转变已经被最新的人工智能研究所接受，如 open AI<strong class="la iu">—</strong><a class="ae lu" href="https://medium.com/towards-data-science/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1" rel="noopener"><strong class="la iu">CLIP【9】</strong></a><strong class="la iu"/>和<strong class="la iu"/><a class="ae lu" href="https://medium.com/towards-data-science/whisper-transcribe-translate-audio-files-with-human-level-performance-df044499877" rel="noopener"><strong class="la iu">Whisper【10】</strong></a>就是其中的几个。</p><h2 id="2bb7" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">零拍 N 拍</h2><p id="9260" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><strong class="la iu">yo shua beng io</strong>(<em class="lv">N-BEATS</em>的合著者)在之前的工作中已经建立了预测任务迁移学习的理论基础【11】。</p><p id="49c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv"> N-BEATS </em>的作者发表了一篇后续论文[12]，其中总结了大部分工作，包括时间序列预测模型应该满足哪些要求才能执行有效的迁移学习。</p><p id="6dbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来关注一下<em class="lv">的 N-BEATS。</em></p><p id="5f2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者指出<em class="lv"> N-BEATS 的</em>元学习能力取决于两个过程:I)内部学习过程和外部学习过程<strong class="la iu">。它们如<strong class="la iu">图 9 所示:</strong></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/1b051478b37ce805905990bc138e6e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VICF6tL0_v2H19FBB1-mAg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 9:</strong>N-BEATS 中的元学习过程(<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>，作者编辑)</p></figure><p id="6ca3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">内部循环</strong>发生在每个模块内部，重点是学习特定任务的特征。</p><p id="a1e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">外部循环</strong>发生在堆栈级别。在这里，模型学习所有任务的全局特征。</p><blockquote class="ni"><p id="54e0" class="nj nk it bd nl nm nn no np nq nr lt dk translated">换句话说，内环学习局部时间特性，而外环学习所有时间序列的更长相关性。</p></blockquote><p id="9967" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">然而，这回避了下面的部分:</p><h2 id="5d19" class="of mr it bd ms og oh dn mw oi oj dp na lh ok ol nc ll om on ne lp oo op ng oq bi translated">为什么 ARIMA 不适合迁移学习？</h2><p id="0b4e" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">如果一个既定的范例规定了一个预测模型应该满足哪些标准才适合迁移学习，那么为什么 ARIMA 不是呢？</p><p id="d3f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了回答这个问题，我们将再次关注[12]描述的两个学习过程</p><p id="bd97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">创建 ARIMA 模型时，有两个挑战:</p><ol class=""><li id="debc" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">参数估计:</strong>使用最大似然法等统计技术估计参数。<strong class="la iu">这是内循环。</strong></li><li id="b0f1" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">模型公式:</strong>这定义了自回归方程的形式。例如，如果我们的模型有一点趋势，没有季节性，正常残差，我们可以决定高斯 ETS 可能会做这项工作。<strong class="la iu">这是外循环。</strong></li></ol><p id="a734" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，统计模型只通过了第一个标准，即内部循环。</p><p id="f836" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们选择了模型，参数估计部分就很简单了。但是，模型制定部分需要人工干预。因此，关于统计方法，<strong class="la iu">外环的作用没有实现。</strong></p><p id="2b59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们得出结论:</p><blockquote class="ni"><p id="85ee" class="nj nk it bd nl nm nn no np nq nr lt dk translated">N-BEATS 用可学习的参数估计策略取代了经典统计模型的模型参数估计的预定义规则集。这种策略允许 N-BEATS 在多个看不见的时间序列上很好地概括。</p></blockquote><h2 id="eb6a" class="of mr it bd ms og pp dn mw oi pq dp na lh pr ol nc ll ps on ne lp pt op ng oq bi translated">N 拍的零拍学习结果</h2><p id="c5fb" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">在本次实验分析中，作者用一些新模型丰富了之前基准(<strong class="la iu">图 8 </strong>)的结果:</p><ul class=""><li id="5d3c" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated"><strong class="la iu"> N 拍-M4: </strong>作者在 M4 上建立了一个预训练的<em class="lv"> N 拍</em>模型。</li><li id="5d6b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">迪帕尔-M4:M4 上的一款经过预训练的<em class="lv">迪帕尔</em>型号也加入了型号库。</li></ul><p id="8a48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总结果如<strong class="la iu">图 10 </strong>所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/d7450c162f2b10ca168b7999724f9572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CdT_-mP12KoV-7JtpVcIvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图 10: </strong>零拍车型对比(<a class="ae lu" href="https://arxiv.org/pdf/2002.02887.pdf" rel="noopener ugc nofollow" target="_blank">来源)</a></p></figure><p id="2d45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，结果非常有趣。</p><ul class=""><li id="8e3d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated">zero-shot<strong class="la iu"><em class="lv">N-BEATS-M4</em></strong>在 M4 和旅游数据集上的表现优于所有其他模型(包括获胜者)，尽管它没有在这些数据集上进行训练。</li><li id="2d97" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">零距离射击<strong class="la iu">迪帕尔-M4 </strong>似乎表现不佳。这是意料之中的，因为<em class="lv"> DeepAR </em>不适合迁移学习。</li><li id="bac9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">在每个数据集中，与定制训练的<em class="lv"> N 拍</em>相比，零拍<em class="lv"> N 拍</em>模型表现非常好。如果能看到在 Whisper 等其他零炮模型中发现的有效稳健性与总体稳健性的关系图，那将会很有意思。</li></ul><h1 id="7909" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结束语</h1><p id="01c3" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated"><em class="lv"> N-BEATS </em>是一个突破性的深度学习预测模型，对时间序列领域产生了持久的影响。</p><p id="fe69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们描述了<em class="lv"> N-BEATS </em>的两个主要优势:首先，它是一个产生 SOTA 结果的强大模型。其次，<em class="lv"> N-BEATS </em>为实施零投迁移学习建立了一个定义明确的框架。据我所知，这是第一个成功实现这一点的模型。</p><p id="e5fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的下一篇文章中，我们将介绍一个使用<em class="lv"> N-BEATS、</em>的编程教程，并描述一篇更新的论文，名为<em class="lv"> N-HiTS </em>。</p></div><div class="ab cl pv pw hx px" role="separator"><span class="py bw bk pz qa qb"/><span class="py bw bk pz qa qb"/><span class="py bw bk pz qa"/></div><div class="im in io ip iq"><h1 id="c727" class="mq mr it bd ms mt qc mv mw mx qd mz na jz qe ka nc kc qf kd ne kf qg kg ng nh bi translated">感谢您的阅读！</h1><p id="6d28" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">我每个月写一篇有影响力的 AI 论文的深度分析。<br/> <strong class="la iu">保持连接！</strong></p><ul class=""><li id="e28b" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt nx mc md me bi translated">订阅我的<a class="ae lu" href="https://medium.com/subscribe/@nikoskafritsas" rel="noopener">简讯</a>！</li><li id="5cd4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt nx mc md me bi translated">在<a class="ae lu" href="https://www.linkedin.com/in/nikos-kafritsas-b3699180/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上关注我！</li></ul><h1 id="3a39" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">参考</h1><p id="cdc7" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">[1]根据 DALLE 创作，文字提示“通过空间传输的霓虹正弦信号，概念艺术”，给 rg</p><p id="6e6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] Slawek Smyl 等著<em class="lv"> </em> <a class="ae lu" href="https://eng.uber.com/m4-forecasting-competition/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> M4 预测竞赛:引入全新混合 ES-RNN 模型</em> </a></p><p id="f263" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] Boris O .等著<a class="ae lu" href="https://arxiv.org/pdf/1905.10437.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> N-BEATS:可解释性时间序列预测的神经基础展开分析</em> </a> <em class="lv">，</em> ICLR (2020)</p><p id="7f39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] D. Salinas 等人<a class="ae lu" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> DeepAR:用自回归递归网络进行概率预测</em> </a>《国际预测杂志》(2019)。</p><p id="8368" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[5]<a class="ae lu" href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014" rel="noopener ugc nofollow" target="_blank">electricityloaddiagrams 2011 2014</a>数据集由 UCI，CC BY 4.0。</p><p id="5f86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[6]https://forecasters.org/resources/time-series-data/ M3 数据集<a class="ae lu" href="https://forecasters.org/resources/time-series-data/" rel="noopener ugc nofollow" target="_blank"/>，公共领域</p><p id="1508" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[7]https://forecasters.org/resources/time-series-data/ M4 数据集<a class="ae lu" href="https://forecasters.org/resources/time-series-data/" rel="noopener ugc nofollow" target="_blank"/>，公共领域</p><p id="0767" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[8]旅游数据集<a class="ae lu" href="https://www.kaggle.com/c/tourism1" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/tourism1</a>，公共领域</p><p id="dc73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[9]亚历克·拉德福德等<a class="ae lu" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">从自然语言监督中学习可转移的视觉模型</em></a><em class="lv">(2021 年 2 月)</em></p><p id="8037" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[10]亚历克·拉德福德等人<a class="ae lu" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模弱监督的鲁棒语音识别</a><em class="lv">(2022 年 9 月)</em></p><p id="6e86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[11] Yoshua Bengio 等人<a class="ae lu" href="https://mila.quebec/wp-content/uploads/2019/08/bengio_1991_ijcnn.pdf" rel="noopener ugc nofollow" target="_blank">T19】学习一个突触学习规则 T21】</a></p><p id="82b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[12] Boris O .等人<a class="ae lu" href="https://arxiv.org/pdf/2002.02887.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">元学习框架及其在零炮时间序列预测中的应用</em> </a></p></div></div>    
</body>
</html>