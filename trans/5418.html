<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-ae5ea52b540d#2022-12-05">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-ae5ea52b540d#2022-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ffac1178f4b1b765ba09475bb76f7647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9g0o_RccyFazT8qI.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="9b4c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第 7 部分:聚类分析</h2></div><p id="8fcf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我将对我们的司法判决进行一些进一步的分析和视觉呈现。我将重点介绍聚类分析，这是一种无监督的学习技术，即使我们对它了解不多，它也非常有助于发现数据中的模式。在 NLP 中，它可以有多种应用:</p><ul class=""><li id="7a2b" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">识别推文中对话的共同趋势</li><li id="2060" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">对来自一个国家不同地区的文本进行分组(每个地区有不同的方言)</li><li id="e40e" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">根据电影涉及的类型对电影评论进行分组</li></ul><p id="2fc2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">诸如此类。无监督学习的好处是并不总是清楚聚类的含义:这取决于用户根据分析的结果来推导。例如，在我们的研究中，分组可能反映不同的干预领域，或者可能确定宣布判决的司法区域。</p><p id="bd30" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了进行聚类分析，我将使用<strong class="la jk"> K-means 算法。</strong></p><p id="f6e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该算法的第一步是在我们未标记的观测值中，创建随机定位的<em class="mi"> c </em>新观测值，称为“质心”。质心的数量将代表簇的数量(我们将在后面看到如何确定这个数量)。现在，将开始一个迭代过程，由两个步骤组成:</p><ul class=""><li id="d02e" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">首先，对于每个质心，该算法找到离该质心最近的点(根据通常计算为欧几里德距离的距离),并将它们分配到其类别中；</li><li id="e091" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">其次，对于每个类别(由一个质心表示)，该算法计算属于该类别的所有点的平均值。该计算的输出将是该类的新质心。</li></ul><p id="5069" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每次重复该过程时，最初与一个质心分类在一起的一些观察结果可能被重定向到另一个质心。此外，在几次重复之后，质心位置的变化应该越来越不重要，因为初始随机质心与真实质心收敛。当质心位置不再变化时，该过程结束。</p><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mj"><img src="../Images/e63f11be01a29387164a5dac7aea4829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*CFW4k0pYMZUBVznPplLOcA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="ffa4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有兴趣深入研究聚类无监督技术，你可以在这里阅读我以前的文章<a class="ae jg" rel="noopener" target="_blank" href="/unsupervised-learning-k-means-vs-hierarchical-clustering-5fe2da7c9554"/>。</p><p id="50a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们用 Python 实现它。</p><p id="b0e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始定义一个函数，稍后绘制我们的聚类结果。</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="0399" class="mt mu jj mp b be mv mw l mx my">#initializing a plotting function<br/><br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>from matplotlib import colors<br/>import seaborn as sns<br/><br/>sns.set_context('notebook')<br/><br/>def plot_vectors(vectors, title='VIZ', labels=None, dimensions=3):<br/>    """<br/>    plot the vectors in 2 or 3 dimensions. <br/>    If labels are supplied, use them to color the data accordingly<br/>    """<br/>    # set up graph<br/>    fig = plt.figure(figsize=(10,10))<br/><br/>    # create data frame<br/>    df = pd.DataFrame(data={'x':vectors[:,0], 'y': vectors[:,1]})<br/>    # add labels, if supplied<br/>    if labels is not None:<br/>        df['label'] = labels<br/>    else:<br/>        df['label'] = [''] * len(df)<br/><br/>    # assign colors to labels<br/>    cm = plt.get_cmap('afmhot') # choose the color palette<br/>    n_labels = len(df.label.unique())<br/>    label_colors = [cm(1. * i/n_labels) for i in range(n_labels)]<br/>    cMap = colors.ListedColormap(label_colors)<br/>        <br/>    # plot in 3 dimensions<br/>    if dimensions == 3:<br/>        # add z-axis information<br/>        df['z'] = vectors[:,2]<br/>        # define plot<br/>        ax = fig.add_subplot(111, projection='3d')<br/>        frame1 = plt.gca() <br/>        # remove axis ticks<br/>        frame1.axes.xaxis.set_ticklabels([])<br/>        frame1.axes.yaxis.set_ticklabels([])<br/>        frame1.axes.zaxis.set_ticklabels([])<br/><br/>        # plot each label as scatter plot in its own color<br/>        for l, label in enumerate(df.label.unique()):<br/>            df2 = df[df.label == label]<br/>            color_values = [label_colors[l]] * len(df2)<br/>            ax.scatter(df2['x'], df2['y'], df2['z'], <br/>                       c=color_values, <br/>                       cmap=cMap, <br/>                       edgecolor=None, <br/>                       label=label, <br/>                       alpha=0.4, <br/>                       s=100)<br/>      <br/>    # plot in 2 dimensions<br/>    elif dimensions == 2:<br/>        ax = fig.add_subplot(111)<br/>        frame1 = plt.gca() <br/>        frame1.axes.xaxis.set_ticklabels([])<br/>        frame1.axes.yaxis.set_ticklabels([])<br/><br/>        for l, label in enumerate(df.label.unique()):<br/>            df2 = df[df.label == label]<br/>            color_values = [label_colors[l]] * len(df2)<br/>            ax.scatter(df2['x'], df2['y'], <br/>                       c=color_values, <br/>                       cmap=cMap, <br/>                       edgecolor=None, <br/>                       label=label, <br/>                       alpha=0.4, <br/>                       s=100)<br/><br/>    else:<br/>        raise NotImplementedError()<br/><br/>    plt.title(title)<br/>#     plt.legend()<br/>    plt.show()</span></pre><p id="13bc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那我们就准备数据吧。对于我的文档的数字表示，我将使用 TF-IDF 矩阵表示(您可以在<a class="ae jg" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-102064f24372">第 3 部分</a>中阅读更多关于该技术的内容)。</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="f1a1" class="mt mu jj mp b be mv mw l mx my">#downloading data<br/>df_factor = pd.read_pickle('data/df_factor.pkl')<br/><br/>documents = df_factor.text.apply(str).tolist()<br/>tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', analyzer='word', <br/>                                   min_df=0.001, max_df=0.5, sublinear_tf=True, use_idf=True)<br/>X = tfidf_vectorizer.fit_transform(documents)</span></pre><p id="42ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们进入集群步骤。我将首先用任意数量的质心训练 K-means 模型，然后我将介绍识别它们的适当数量的技术。</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="81d0" class="mt mu jj mp b be mv mw l mx my">from sklearn.cluster import KMeans, AgglomerativeClustering<br/><br/>k = 16 #just to try, not the optimal number of clusters<br/><br/># reduce the dimensionality of the input, to speed up clustering<br/>X2 = TruncatedSVD(n_components=300).fit_transform(X)<br/><br/>agg = AgglomerativeClustering(n_clusters=k)<br/><br/>agg_ids = agg.fit_predict(X2)<br/><br/>centroids = np.array([X2[agg_ids == c].mean(axis=0) for c in range(k)])<br/>#print(centroids.shape)</span></pre><p id="4b9f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们训练模型并使其适合我们的 TF-IDF 截断矩阵:</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="65a3" class="mt mu jj mp b be mv mw l mx my">km = KMeans(n_clusters=k, <br/>            n_jobs=-1, <br/>            init=centroids)<br/><br/><br/>km.fit(X2)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d88fd69c425ad32d821936f352bdf7ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*_I7jSU4NmXz0nuqlk6Hz0w.png"/></div></figure><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="dd17" class="mt mu jj mp b be mv mw l mx my">plot_vectors(X2, labels=km.labels_)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/8f64fed04369f0f40f4d71698b52d4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*XDA98WFibH6JtROwRJzQsA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="df87" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，为了进行更有意义的分析，我想确定聚类(或质心)的最佳数量。为此，我将使用轮廓得分技术。</p><p id="2010" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每个数据点或观察值以及每个固定数量的聚类，该分数由两部分组成:</p><ul class=""><li id="a9dd" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">该观测值与同一聚类内所有其他观测值之间的平均距离(表示为<em class="mi"> a </em></li><li id="3f21" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">该观测值与下一个最近簇的所有其他观测值之间的平均距离(表示为<em class="mi"> b </em></li></ul><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f26e4d701f68081a2f8138de380bc5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*MRMTzfrDfc9kbptmLt05lQ.png"/></div></figure><p id="6c67" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如公式所示，分子越大，用该数量的质心完成的聚类越多。</p><p id="a9d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法是，我们训练一个增量 K-means 模型，其质心数量不断增加，我们为每个质心计算轮廓得分。导致具有最高轮廓分数的模型的质心的数量被假定为最佳数量。</p><p id="34b1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的分析中，我训练了 10 个质心在 15 到 25 之间的模型。</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="1951" class="mt mu jj mp b be mv mw l mx my">from sklearn.metrics import silhouette_score<br/><br/>silhouettes = []<br/><br/>K = range(15, 26)<br/><br/>for c in K:<br/>    agg_clustering = AgglomerativeClustering(n_clusters=c)<br/><br/>    agg_cluster_ids = agg_clustering.fit_predict(X2)<br/>    score = silhouette_score(X2, agg_cluster_ids)<br/>    silhouettes.append(score)<br/>    print(c, score)<br/>    <br/>fig, ax = plt.subplots(figsize=(20,10))<br/>plt.plot(K, silhouettes, 'bx-')<br/>plt.xlabel('k')<br/>plt.ylabel('Silhouette score')<br/>plt.title('Silhouette Method For Optimal k')<br/>plt.show()</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/6dcba7162ac2ecaa06ffa18e27960377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GdAh31N6QUz14MXP89L2EQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="a831" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的图中，似乎最一致的聚类数是 k=24，因为它具有最高的分数。如果添加更多的集群，它可能会更高，但是，我不想让我的数据过多，所以我将坚持这个数字。</p><pre class="mk ml mm mn gt mo mp mq bn mr ms bi"><span id="37b0" class="mt mu jj mp b be mv mw l mx my">from sklearn.cluster import KMeans, AgglomerativeClustering<br/><br/>k = 24<br/><br/># reduce the dimensionality of the input, to speed up clustering<br/>X2 = TruncatedSVD(n_components=300).fit_transform(X)<br/><br/>agg = AgglomerativeClustering(n_clusters=k)<br/>agg_ids = agg.fit_predict(X2)<br/><br/>centroids = np.array([X2[agg_ids == c].mean(axis=0) for c in range(k)])<br/>print(centroids.shape)<br/><br/>km = KMeans(n_clusters=k, <br/>            n_jobs=-1, <br/>            init=centroids)<br/><br/># fit it on the full 300-dimensional data set<br/>km.fit(X2)<br/><br/>plot_vectors(X2, labels=km.labels_)</span></pre><figure class="mk ml mm mn gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0350e7f025daa3ea2955fa9a523210cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*ciRaVej32lz_DejYtu8ZAA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9fa4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所述，没有预先向用户给出聚类的含义。我们有责任进一步研究模型发现的模式，并将它们与我们拥有的进一步信息联系起来。</p><p id="51c2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望你喜欢阅读！敬请期待下一部分:)</p><h2 id="8551" class="ne mu jj bd nf ng nh dn ni nj nk dp nl lh nm nn no ll np nq nr lp ns nt nu nv bi translated">参考</h2><ul class=""><li id="cb7d" class="lu lv jj la b lb nw le nx lh ny ll nz lp oa lt lz ma mb mc bi translated"><a class="ae jg" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="aea8" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">Python 中的 spaCy 工业级自然语言处理</li><li id="8579" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="1ac4" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated"><a class="ae jg" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009-2018 年新闻发布| Kaggle </a></li></ul></div></div>    
</body>
</html>