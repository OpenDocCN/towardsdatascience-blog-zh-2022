<html>
<head>
<title>A Practical Introduction to Support Vector Machines from scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自 scikit-learn 的支持向量机实用介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-introduction-to-support-vector-machines-from-scikit-learn-6e678cf1f228#2022-02-16">https://towardsdatascience.com/a-practical-introduction-to-support-vector-machines-from-scikit-learn-6e678cf1f228#2022-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8cb1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">UCL 数据科学学会研讨会 15:什么是支持向量机，如何实现它们，以及如何评估它们</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6eb8b07590c48a1975a1370b40fcce18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1SwsQe_So9J9rqBx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>拍摄</p></figure><p id="0b8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UCL 数据科学系列的第十五个研讨会是 Python 数据科学研讨会系列的一部分，涵盖了使用 Scikit-learn 进行分类的支持向量机。在本次研讨会中，我们将讨论什么是支持向量机，如何实现它们，以及如何评估它们。一如既往，这篇博客文章是整个研讨会的总结，可在<a class="ae ky" href="https://github.com/UCL-DSS/svm-workshop" rel="noopener ugc nofollow" target="_blank">这里</a>找到，它更详细地涵盖了这些主题，还涵盖了多类分类和非线性决策边界。</p><h2 id="1a3e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">什么是支持向量机？</h2><p id="b8f7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">支持向量机是一种算法，通常用于对数据进行分类，因此与决策树或随机森林算法属于同一类别和用例。这意味着它是受监督的机器学习算法组的一部分，借此我们有了一个可以瞄准的明确目标。</p><p id="f78a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">支持向量机算法的具体工作原理是试图找到一个界限，将您试图找到的两个或更多不同的类分开。在这样做时，该模型可用于预测，通过找到点可能位于边界的哪一侧，从而找到该点可能属于哪个组。</p><p id="e139" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用该算法的优点包括:</p><ul class=""><li id="b12a" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">它在高维空间中是有效的</li><li id="beba" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">当维数超过样本数时，它仍然可以使用</li><li id="f7a8" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">它是通用的，因为可以为决策函数指定不同的核函数(组织决策边界的方式),包括您自己的核函数</li></ul><h2 id="b57e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">支持向量机实现</h2><p id="59b2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将用来训练和测试算法的数据集是基于一个人造的红点和蓝点数据集，可以在下面看到。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6343" class="lv lw it ni b gy nm nn l no np"># Splits rows based on the colour in the "Colour" column<br/>red = df.loc[df["Colour"] == "red"]<br/>blue =  df.loc[df["Colour"] == "blue"]</span><span id="0bb9" class="lv lw it ni b gy nq nn l no np"># Plots the red data and the blue data<br/>plt.plot(red["x"],red["y"],"r+")<br/>plt.plot(blue["x"],blue["y"],"b+")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/18de259e0f6f85dc6faf0d3f2b8593d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ksvRgRywyrwglMfzkSHmCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1d2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里可以看到两组明显不同的红点和蓝点，它们在理论上可以代表各种不同的群体，例如不同的血型，项目是否成功，不同的消费群体。</p><p id="5513" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然对于这些数据点，我们可以通过肉眼指定一个新的数据点，但是对于更高维度的数据、大量的点或者即使我们有更多的组，这也会变得更加困难。在这种程度上，我们可以实现支持向量机算法，看看它在这样一个简单的数据集上如何表现，以便我们可以在更复杂的情况下理解和实现它。</p><p id="eeb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这一点，就像大多数数据科学项目一样，我们可以将数据分成测试和训练数据集，以便我们可以看到模型如何对我们知道结果的看不见的数据进行操作。这可以通过以下方式实现:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="7473" class="lv lw it ni b gy nm nn l no np">from sklearn.model_selection import train_test_split</span><span id="4a1f" class="lv lw it ni b gy nq nn l no np"># Independent variables<br/>X = df.drop("Colour", axis=1)</span><span id="9868" class="lv lw it ni b gy nq nn l no np"># Dependent variable <br/>y = df["Colour"]</span><span id="e414" class="lv lw it ni b gy nq nn l no np"># Allocates 80% of the data for training and 20% for testing<br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)</span></pre><p id="f542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以考虑在训练数据集上训练模型，然后使用该模型来查看它在测试数据集上的表现。为此，我们使用<code class="fe ns nt nu ni b">scikit-learn</code>库定义创建算法，其中我们将分类器<code class="fe ns nt nu ni b">clf</code>定义为<code class="fe ns nt nu ni b">clf = svm.SVC(kernel = “linear")</code>，其中<code class="fe ns nt nu ni b">SVC</code>代表支持向量分类器。在这里，<code class="fe ns nt nu ni b">kernel="linear"</code>参数指定我们想要一个线性的决策边界。训练该算法包括如下确定两个组之间的分离边界:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="1bc8" class="lv lw it ni b gy nm nn l no np">from sklearn import svm</span><span id="e02e" class="lv lw it ni b gy nq nn l no np"># Chooses the support vector machine algorithm for our classifier<br/>clf = svm.SVC(kernel = "linear")</span><span id="5d23" class="lv lw it ni b gy nq nn l no np"># Training the classifier<br/>clf_trained = clf.fit(X_train,y_train)</span><span id="a8d6" class="lv lw it ni b gy nq nn l no np"># Scoring the classifier<br/>clf_trained.score(X_train,y_train)</span><span id="aad2" class="lv lw it ni b gy nq nn l no np">#out:<br/>1.0</span></pre><p id="f4ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，在训练数据集上，我们有完美的准确性。</p><h2 id="3a5e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">支持向量机评估</h2><p id="e54e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">但是这种模式是如何运作的呢？我们可以尝试通过从模型中提取决策边界并将其与训练数据一起绘制来了解该模型是如何工作的，如下所示:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="dbf2" class="lv lw it ni b gy nm nn l no np">plt.scatter(X_train["x"], <br/>            X_train["y"], <br/>            c=y_train, s=30, cmap=plt.cm.Paired)</span><span id="faef" class="lv lw it ni b gy nq nn l no np"># plot the decision function<br/>ax = plt.gca()<br/>xlim = ax.get_xlim()<br/>ylim = ax.get_ylim()</span><span id="ad67" class="lv lw it ni b gy nq nn l no np"># create grid to evaluate model<br/>xx = np.linspace(xlim[0], xlim[1], 30)<br/>yy = np.linspace(ylim[0], ylim[1], 30)<br/>YY, XX = np.meshgrid(yy, xx)<br/>xy = np.vstack([XX.ravel(), YY.ravel()]).T<br/>Z = clf.decision_function(xy).reshape(XX.shape)</span><span id="d140" class="lv lw it ni b gy nq nn l no np"># plot decision boundary and margins<br/>ax.contour(<br/>    XX, YY, Z, colors="k", levels=[-1, 0, 1], alpha=0.5, linestyles=["--", "-", "--"]<br/>)<br/># plot support vectors<br/>ax.scatter(<br/>    clf.support_vectors_[:, 0],<br/>    clf.support_vectors_[:, 1],<br/>    s=100,<br/>    linewidth=1,<br/>    facecolors="none",<br/>    edgecolors="k",<br/>)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/80d4d617fc512d56395f3f2786157527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1KhTa2n1lxt6E9UIpkyAfg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1b36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从图中我们可以看到，决策边界沿着上图中的实线，虚线代表模型中的边界。</p><p id="c58d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以使用来自<code class="fe ns nt nu ni b">scikit-learn</code>的分类报告在决策树和随机森林研讨会中评估模型。这可以通过以下方式实现:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="1c1b" class="lv lw it ni b gy nm nn l no np">#import the necessary functions<br/>from sklearn import metrics</span><span id="b41e" class="lv lw it ni b gy nq nn l no np">#extract the predictions of the model<br/>test_pred_svm = clf_trained.predict(X_test)</span><span id="f2d3" class="lv lw it ni b gy nq nn l no np">#print the classification report<br/>print (metrics.classification_report(y_test, test_pred_svm))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/945ce71ed2c2f90533327febd34db9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v24QKUrEhq_nDvWmqSL9gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9e7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，由于我们有一个相对简单的数据集，边界清晰，我们可以看到，该模型在预测未知数据集方面是完美的，正如我们所预期的那样。</p><p id="ec64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，您可以相对容易地在二维上实现具有线性决策边界的支持向量机。当然，现实永远不会那么简单，因此在研讨会中，我们还将讨论如何对目标群体以外的数据进行分类，以及如何实施不同的内核来表示不同的决策界限。您可以在我们的 GitHub 资源库中找到它和问题表:</p><div class="nx ny gp gr nz oa"><a href="https://github.com/UCL-DSS/svm-workshop" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">GitHub-UCL-DSS/SVM-研讨会</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">欢迎参加我们的第七届 Python 系列数据科学研讨会！作者:塔尼亚·图尔迪安，科学执行官，UCL 数据…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">github.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="521e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了之前的任何一场研讨会，可以在这里找到后三场研讨会:</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/how-to-implement-and-evaluate-decision-tree-classifiers-from-scikit-learn-36ef7f037a78"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">如何实现和评估来自 scikit-learn 的决策树分类器</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">UCL 数据科学学会研讨会 13:什么是决策树，决策树的实现，可视化和…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ow l ol om on oj oo ks oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/non-linear-regression-with-decision-trees-and-random-forest-afae406df27d"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">决策树和随机森林的非线性回归</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">UCL 数据科学协会研讨会 12b:决策树和随机森林在 Python 中的实现及性能…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ox l ol om on oj oo ks oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/an-introduction-lasso-and-ridge-regression-using-scitkit-learn-d3427700679c"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">使用 scitkit-learn 介绍套索和岭回归</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">UCL 数据科学学会 12a 研讨会:偏差-方差权衡，套索实施，山脊实施，及其…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="oy l ol om on oj oo ks oa"/></div></div></a></div><p id="1f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解 UCL 数据科学协会和其他优秀作者的最新信息，请使用我下面的推荐代码注册 medium。</p><div class="nx ny gp gr nz oa"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="oj l"><div class="oz l ol om on oj oo ks oa"/></div></div></a></div></div></div>    
</body>
</html>