<html>
<head>
<title>Complex-Valued CNNs for Medical Image Denoising</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于医学图像去噪的复值CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complex-valued-cnns-for-medical-image-denoising-12a4262c6ef6#2022-10-14">https://towardsdatascience.com/complex-valued-cnns-for-medical-image-denoising-12a4262c6ef6#2022-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5031" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种新的医学图像去噪方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ee6da324446fb1521173c43888458859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8vOEp2DL8YAShpFhhucdg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d5a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习，特别是卷积神经网络(CNN)，正在塑造数据驱动的问题解决的未来。从文本相关的问题，如语音生成、内容编写等，到视觉任务，如图像分类、对象检测，CNN被广泛使用。在过去的几年中，已经提出了许多先进的CNN架构，如图形CNN、基于注意力的CNN、复值CNN等。在这篇文章中，我将总结我在<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">这里</a>发表的研究论文，其中提出了一种新的基于复值CNN的深度学习模型用于医学图像去噪。</p><h2 id="7136" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">介绍</h2><p id="30c4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">医学成像通过以多种方式帮助医疗专业人员，包括疾病诊断、治疗和风险预测，彻底改变了卫生部门。然而，X射线、计算机断层扫描(CT)、超声波和磁共振成像(MRI)等医学图像容易受到各种噪声的影响。例如，胸部X射线(CXR)图像经常被高斯噪声破坏，高斯噪声出现在采集、存储、传输和处理过程中。医学图像中的噪声会降低图像质量，甚至使其在诊断上无法使用。这可能会阻碍进一步的决策，导致疾病的诊断、治疗或分析不佳。因此，总是迫切需要在不损害潜在信息的情况下降低医学图像的噪声，因为由此产生的诊断直接影响人类健康和生命。</p><p id="7127" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些传统的医学图像去噪(MID)技术包括基本的数字图像滤波器、自适应滤波器、非局部均值算法和多尺度技术。然而，医学图像中复杂的噪声导致这些技术的性能不尽人意。“然后是卷积神经网络(CNN)”。毫无疑问，CNN已经成为众多数据驱动任务，尤其是图像处理的主流解决方案。对于MID，已经报道了各种基于CNN的架构，如卷积自动编码器和生成式对抗网络(GANs)。这些CNN技术涉及两种CNN架构，例如，在GANs中，我们有一个生成器和鉴别器，在自动编码器中，我们有一个编码器和解码器。然而，有许多方法只涉及一种CNN架构，如DnCNN。</p><p id="b8ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定用于MID的基于CNN的方法，可以得出结论，所有这些方法都涉及实值CNN，即处理实数的CNN架构。这促使我为MID开发CVC nn，正如你可能已经猜到的，这是为MID实现CVC nn的第<strong class="ky ir">篇研究论文</strong>(同时，这也是我的第一篇研究论文)。</p><p id="6eba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最近，复值CNN(cv CNN)越来越受欢迎，因为它们提供了比实值CNN更好的结果，并且由于硬件行业的进步，这些模型的实现已经成为可能。现在出现了以下问题:</p><ol class=""><li id="4781" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">CVCNNs如何优于它们的实值对应物</li><li id="c966" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">CVCNNs如何用于MID</li></ol><p id="ae61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我不会主要关注第一个问题，因为它超出了本文的范围。不过，我可能会写另一篇文章，说明CVCNNs相对于其实值对等物的优势，所以请继续关注:)</p><h2 id="d10f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">提议的方法</h2><p id="c9ac" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">提出的用于MID的CVCNN模型被称为CVMIDNet。模型框架如图1所示，其中Conv、BN和ReLU代表卷积层、批量归一化和校正线性单元函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/bf6bb07bdbc923598bcababb85ef35cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*amC0eMjo_1xifWmpWljGzg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nf">图一。</strong>CVMIDNet的架构。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="56c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是该模型的主要组成部分:</p><h2 id="5f5a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">复值卷积层(CVCL)</h2><p id="c871" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在CVCL中，实值卷积运算被推广到复数域。在实值卷积层中，在输入矩阵I和实值核K，即I*K之间执行卷积运算。在复值卷积层中，I和K都是复矩阵，使得:</p><blockquote class="ng"><p id="883b" class="nh ni iq bd nj nk nl nm nn no np lr dk translated">I = Ir + iIc <br/> K = Kr + iKc</p></blockquote><p id="2249" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">其中，Ir、Ic、Kr和Kc是实矩阵。现在，复值卷积运算变为:</p><blockquote class="ng"><p id="0c44" class="nh ni iq bd nj nk nl nm nn no np lr dk translated">I * K =(Ir * Kr Ic * Kc)+I(Ic * Kr+Ir * Kc)</p></blockquote><h2 id="bd98" class="ls lt iq bd lu lv nv dn lx ly nw dp ma lf nx mc md lj ny mf mg ln nz mi mj mk bi translated">复值批量规范化(CVBN)</h2><p id="0067" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">类似于CVCL，在CVBN中，实值BN运算以如下方式推广到复数域:</p><blockquote class="ng"><p id="ba14" class="nh ni iq bd nj nk nl nm nn no np lr dk translated">CNB(Z) = BN(A) + i(BN(B))</p></blockquote><p id="6513" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">其中BN()和CNB()分别表示实值和复值BN运算。Z是被认为是Z = A + iB的复值参数</p><h2 id="5488" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">复值ReLU (CVReLU)</h2><p id="2ecc" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">当涉及到一般化实值激活函数(在这个例子中是ReLU)时，我们有许多方法可以做到这一点，因此我们有不同版本的CVReLU。最常见的包括ModReLU、zReLU和CReLU。在文献中，有足够的证据表明CReLU的性能优于其他潜在的同类产品，尤其是在与图像相关的任务中，因此它被考虑用于CVMIDNet。CReLU按如下方式获得:</p><blockquote class="ng"><p id="2bf1" class="nh ni iq bd nj nk nl nm nn no np lr dk translated">CReLU(z) = ReLU(R(z)) + i(ReLU(I(z)))</p></blockquote><p id="f8d4" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">其中，z、R(z)和I(z)表示复值参数、z的实部和z的虚部。</p><h2 id="4502" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">数据集和实验细节</h2><p id="19ed" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">本研究考虑了一个公开可用的CXR图像数据集(<a class="ae kv" href="https://data.mendeley.com/datasets/rscbjbr9sj/3" rel="noopener ugc nofollow" target="_blank">链接</a>)，其中图像受到高斯噪声的影响。CXR图像被考虑用于去噪，因为由于其成本效益和非侵入性方法，它们是最重要的医学成像技术类型之一。为训练选择了400幅图像，为测试集选择了100幅(与训练集不重叠)图像。</p><p id="5447" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">整个工作的代码是使用Tensorflow在Python中开发的。必要时还开发了定制层和数据生成器。</p><h2 id="6abd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结果</h2><p id="c63e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">峰值信噪比(PSNR)和结构相似性指数(SSIM)被用来评估CVMIDNet。此外，该模型还与BM3D、DnCNN、FDCNN等其他先进的MID技术进行了比较。CVMIDNet的实值对应物(通过用其实值对应物替换CVMIDNet中的复值运算而开发的)，称为RVMIDNet，也被考虑用于性能评估。</p><p id="c644" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑了许多高斯噪声水平，即sigma = 15、25、40、50和60 (sigma代表高斯噪声中的噪声水平，更多的是这里的<a class="ae kv" href="https://en.wikipedia.org/wiki/Gaussian_noise" rel="noopener ugc nofollow" target="_blank"/>)。结果显示在下表和下图中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/b8ab1afb005cb245754655767d019449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*noY5eisYbKkqSh5yVyFwew.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nf">表1 </strong>去噪图像的PSNR结果。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/147a003a16a4268227878d328a29f3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6L2m3miziKUSH-96wda_5A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nf">表2 </strong>去噪图像的SSIM结果。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/79eda797b58ba8c9f4e082b92ab15362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TfY3nY2Q8CrwsnLGmbm1Yw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nf">图二。</strong>去噪图像的平均PSNR图。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/c282c7b4dc037e853bd6acfc41f35974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xl_xxB0H8bOn8AKlAPjLJQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nf">图3。</strong>去噪图像的平均SSIM图。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0ed1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过与其他最新技术进行比较，发现CVMIDNet在各种噪声水平下对受高斯噪声影响的CXR图像进行去噪方面优于所有其他方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/476d2f04b7b9af0661f7b83e534be932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ILNXbnVcJ4Je1rRK4NOVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图sigma = 15的去噪图像。<a class="ae kv" href="https://doi.org/10.1016/j.bspc.2021.102859" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="9782" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结论</h2><p id="411b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">首次提出并实现了一种新的基于复值CNN的深度学习模型CVMIDNet，该模型具有用于医学图像去噪的残差学习。CVMIDNet使用复值卷积层、复值批量归一化和CReLU激活来实现，以从胸部X射线图像中去除高斯噪声。CVMIDNet的去噪性能与四种潜在的最先进的去噪方法进行了比较，即块匹配和3D滤波、DnCNN、特征引导去噪卷积神经网络以及除RVMIDNet之外的具有残差学习的深度CNN模型，其中RVMIDNet具有与CVMIDNet相同的架构，但所有操作都是实值的。据观察，CVMIDNet在所有调查的五个噪音水平上以显著的优势优于所有其他模型。此外，视觉评估还清楚地表明，CVMIDNet比其他比较模型更有效地降低了噪声并恢复了图像。</p><h2 id="9b50" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">结尾注释</h2><p id="fbda" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">所以这是一个关于我的一个新方法的研究的简介，这个方法被杂志接受:<a class="ae kv" href="https://www.sciencedirect.com/journal/biomedical-signal-processing-and-control" rel="noopener ugc nofollow" target="_blank">生物医学信号处理和控制</a>。</p><p id="1e25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这篇文章对你有用，那么你可能会发现我的其他文章同样有趣。此外，如果你想或正在寻找开发这样的模式，那么你可以联系我在五月:【https://www.fiverr.com/shubhankarrawat T2】</p><p id="949a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快乐学习！！</p></div></div>    
</body>
</html>