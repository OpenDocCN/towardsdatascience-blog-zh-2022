<html>
<head>
<title>Neural-Implicit Representations for 3D Shapes and Scenes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三维形状和场景的神经隐式表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-implicit-representations-for-3d-shapes-and-scenes-c6750dff49db#2022-06-26">https://towardsdatascience.com/neural-implicit-representations-for-3d-shapes-and-scenes-c6750dff49db#2022-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8613" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">近年来，有一个帮助解决计算机图形任务的神经隐式表示的爆炸。在本文中，我将重点讨论它们在三种不同任务中的适用性——形状表示、新颖的视图合成和基于图像的3D重建。</h2></div><p id="44ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">近年来，计算机图形学领域与深度学习一起蓬勃发展，有几项贡献对该领域产生了重大影响。计算机图形学解决了用计算机创建图像的问题。虽然这听起来很简单，但创建物理上正确和真实的图像肯定是一项复杂的任务。要做到这一点，人们需要理解如何表现3D场景，3D场景通常包含光线、不同的材料、几种几何形状和拍摄照片的相机型号。幸运的是，研究人员已经找到了使用深度学习模型解决计算机图形问题的迷人方法。</p><p id="60ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单地说<strong class="kh ir">但不准确地说</strong>，这篇文章旨在提炼以下基于深度学习的计算机图形任务解决方案的进展:</p><ol class=""><li id="6005" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">2017–2019—如何用神经网络(NN)表示<strong class="kh ir">形状</strong>。我将把重点放在使用<a class="ae lk" href="https://jasmcole.com/2019/10/03/signed-distance-fields/" rel="noopener ugc nofollow" target="_blank">带符号距离函数</a>来实现这一点的作品上。</li><li id="8498" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">2019–2020—如何用NNs表现<strong class="kh ir">场景</strong>。具体来说，神经网络如何表现一个完整的场景，包括闪电、纹理&amp;形状的所有精细细节？</li><li id="f45c" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">2021 —2022 —我们如何<strong class="kh ir">理解</strong>有NNs的场景？具体来说，给定神经场景表示，我们如何重建场景中的3D表面？</li><li id="33c8" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">2022年——我们如何让它更快？</li></ol><p id="8d4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这仅仅是我将进展提炼为不同阶段的方式，而显然，这个领域并没有以如此清晰的方式进展。在2021年之前，人们试图理解具有NNs的场景，形状表示仍然是人们正在努力的事情(在形状之间进行概括仍然是一个问题)。然而，在我看来，这种顺序使得我将在这篇文章中讨论的观点和作品更容易理解。</p><h1 id="04ec" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">计算机图形学的简要背景</strong></h1><p id="d98c" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">而我假设读者熟悉深度学习(梯度下降、神经网络等。)，为了完整起见，我将在这里简单介绍一下本文中用到的一些术语的背景。</p><h2 id="2887" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak">光线跟踪与渲染光栅化</strong></h2><p id="ce5e" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">注意，渲染图像有两个类别——<a class="ae lk" href="https://en.wikipedia.org/wiki/Rasterisation" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">栅格化</strong> </a>和<a class="ae lk" href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">光线追踪</strong> </a>。查看<a class="ae lk" href="https://blogs.nvidia.com/blog/2018/03/19/whats-difference-between-ray-tracing-rasterization/#cancel" rel="noopener ugc nofollow" target="_blank">Nvidia</a>的这篇文章，了解他们的介绍，我<strong class="kh ir">强烈推荐</strong>观看迪士尼在<a class="ae lk" href="https://disneyanimation.com/technology/hyperion/" rel="noopener ugc nofollow" target="_blank"> Hyperion </a>上的视频，这是他们的(光线跟踪)渲染引擎，直观地解释了通过与物体的几次碰撞，从光源到相机的光线路径的计算建模过程。然而，如果你不太熟悉渲染，请注意<strong class="kh ir">光栅化在现代GPU上要快得多</strong>，然而它并不像基于光线跟踪的渲染那样模拟物理世界。在这篇文章中，我将重点关注利用差分光线跟踪渲染的作品，因为它们往往会产生更好的结果，并且我将描述使它们可用于实时应用程序的方法。</p><h2 id="2e24" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak">三维几何图形表示</strong></h2><p id="8b13" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">虽然很清楚3D场景包含几个3D对象，但不清楚如何表示它们的几何形状。没有一种计算和存储效率都很高的规范表示法能够表示任意拓扑的高分辨率几何图形。几种<a class="ae lk" href="https://www.antoinetlc.com/blog-summary/3d-data-representations" rel="noopener ugc nofollow" target="_blank">表示</a>进化而来，最常用的有<a class="ae lk" href="https://en.wikipedia.org/wiki/Polygon_mesh" rel="noopener ugc nofollow" target="_blank">网格</a>、<a class="ae lk" href="https://en.wikipedia.org/wiki/Voxel" rel="noopener ugc nofollow" target="_blank">基于体素的</a>或<a class="ae lk" href="https://en.wikipedia.org/wiki/Point_cloud" rel="noopener ugc nofollow" target="_blank">基于点的</a>。如果您不熟悉这些术语，我建议您在继续之前阅读它们。然而，需要注意的重要一点是，这些方法在效率<strong class="kh ir"/>(基于体素的表示内存使用相对于分辨率以立方增长)<strong class="kh ir">表现性</strong>(头发等精细几何体很难使用网格建模)或<strong class="kh ir">拓扑约束</strong>(直接从点云生成<a class="ae lk" href="https://davidstutz.de/a-formal-definition-of-watertight-meshes/" rel="noopener ugc nofollow" target="_blank">防水表面</a> —一个没有洞的闭合表面——可能不是一项简单的任务)。</p><p id="beef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面所有的表示都有一个共同的特性——它们依赖于几何的显式表达。但是，它是通过用离散对象(如三角形、栅格或简单的点)来近似3D表面来实现的。另一种表示3D对象的常见方式是使用<strong class="kh ir">连续</strong> <a class="ae lk" href="https://en.wikipedia.org/wiki/Implicit_surface" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">隐式表示法</strong>。</a>通常，隐函数将几何图形表示为在3D点上操作的函数，满足:</p><ol class=""><li id="fcff" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">F(x，y，z)&lt;0 — interior point</li><li id="b098" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">F(x,y,z)&gt;0-外部点</li><li id="9222" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">F(x，y，z)= 0-曲面点</li></ol><p id="7834" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体地说，<a class="ae lk" href="https://en.wikipedia.org/wiki/Signed_distance_function" rel="noopener ugc nofollow" target="_blank">有符号距离函数</a> (SDF)满足这些性质。SDF就是给定点到物体<a class="ae lk" href="https://en.wikipedia.org/wiki/Boundary_(topology)" rel="noopener ugc nofollow" target="_blank">边界</a>的距离，并根据上述规则设置距离的符号。如果隐式表示对你来说是新的，我推荐阅读密苏里CS8620课程的<a class="ae lk" href="http://web.missouri.edu/~duanye/course/cs8620-spring-2017/lecture-notes/3a-implicit-geometry.pdf" rel="noopener ugc nofollow" target="_blank">讲义。请记住，SDF有许多好的属性(可以用来很容易地计算相交，使光线投射快速)。然而，如何获得某种形状的SDF并不是一件小事？</a></p><p id="bebf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">视觉上，球体的SDF可以被视为:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi mz"><img src="../Images/6024625579e5ee70de871c0a589c220b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAG3rW4LR3lkmseOVrgbkA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">围绕球体的有符号距离的视觉。<a class="ae lk" href="https://www.researchgate.net/publication/319307450_Particle_Tracking_Acceleration_via_Signed_Distance_Fields_in_DAGMC" rel="noopener ugc nofollow" target="_blank">图源</a>。</p></figure><h2 id="1810" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak">计算机图形任务</strong></h2><p id="64ed" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">回想一下，计算机图形学的总体目标是使用计算机创建图像。这个目标可以分解成几个问题。今天，我将重点讨论深度学习如何帮助解决以下计算机任务:</p><ol class=""><li id="71ca" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kh ir">形状表示</strong>——正如我在上面详述的，表示3D形状并不简单。虽然存在几种经典的表示方法，但我将在这篇文章中详细介绍如何使用神经网络来有效地表示连续的3D形状(特别是SDF)。</li><li id="3dd3" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><strong class="kh ir">新颖的视图合成(NVS) </strong> —给定一个场景的几幅图像，我们如何从新的视图生成图像？需要多少幅图像，除了视图之外，我们还能控制哪些属性(例如，我们能否用不同的光照生成视图)？</li><li id="8236" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><strong class="kh ir">曲面重建— </strong>给定一个曲面的多幅图像，我们如何重建它的3D模型？</li></ol><p id="132a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人们现在可能想知道求解NVS是否隐含地解决了表面重建任务。有理由认为，如果计算机可以生成场景的新视图，它就必须(隐含地)重建场景的3D模型。因此，似乎解决NVS(即，将NeRF拟合到场景)就解决了表面重建问题。虽然这听起来是正确的，但并不完全正确，我将描述一种通过解决NVS任务来重建曲面的机制。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="9b6d" class="lq lr iq bd ls lt nw lv lw lx nx lz ma jw ny jx mc jz nz ka me kc oa kd mg mh bi translated"><strong class="ak"> 1。形状表示的神经网络</strong></h1><p id="d275" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">如何在捕捉高频局部细节的同时获得高效紧凑的表现是一项具有挑战性的任务。虽然隐式表示是表示形状的有效方式，但使用经典方法很难获得它们。DeepSDF是深度学习在这项任务中的第一个成功应用。</p><h2 id="dac8" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak">1.1</strong><a class="ae lk" href="https://arxiv.org/pdf/1901.05103.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">deep SDF</strong></a><strong class="ak">—表示SDF的神经网络</strong></h2><p id="c72b" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">给定一个网格，如何求它的SDF？经典方法通过用3D网格离散化空间来近似SDF，其中每个体素代表一个距离。这些体素在内存中很昂贵，尤其是当需要细粒度的近似时。</p><p id="fd60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DeepSDF建议使用神经网络对其进行近似，该网络简单地学习预测给定点的SDF值。为了训练网络，他们用相应的SDF采样了大量的点。他们对物体表面的更多点进行采样，以获取详细的图像。</p><p id="ff7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们可以使用神经网络来表示SDF，关于这种表示的几个问题就出现了。一个网络可以概括为代表几个形状，而不是为每个形状训练一个特定的网络吗？如果是这样的话，多种形状的连续表示是否使我们能够在它们之间进行插值？在不深入细节的情况下，我注意到DeepSDF确实显示了形状之间的某种程度的一般化，这是通过将DeepSDF网络限制在可训练的形状代码上来实现的。有关更多详细信息，请参考DeepSDF第4节。</p><h2 id="c6be" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated">1.2.<a class="ae lk" href="https://arxiv.org/pdf/2003.10983.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">DeepLS</strong></a><strong class="ak">—用计算换取内存</strong></h2><p id="5f67" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">虽然DeepSDF在逼近SDF方面取得了非常好的结果，但训练和推断的成本很高。MLP网络通常是根据～2M参数构建的，这使得高分辨率3D形状的训练和推断成本很高。回想一下，DeepSDF是一个基于<strong class="kh ir">坐标的</strong>(即一个网络一次只在一个坐标上运行)网络。因此，在训练期间，必须为每个单点计算每个～2M参数的<strong class="kh ir">梯度。</strong>用300-500k个点对特定网格的DeepSDF进行训练，这导致训练的计算开销很大。因此，使用网络进行推理也是昂贵的。</p><p id="041b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个天真的建议是通过简单地使用一个较小的网络来使DeepSDF的训练和推理更快。然而，正如DeepSDF所示，他们在考虑了速度与精度的权衡后选择了网络架构，因为他们发现较小的模型性能更差。需要一种更复杂的方法。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ob"><img src="../Images/688db75994b5db6590991bc387f0b00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dJWB3ZcduapS0vfva1hOeA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">作为网络深度函数的回归精度。图来自<a class="ae lk" href="https://arxiv.org/pdf/1901.05103.pdf" rel="noopener ugc nofollow" target="_blank"> DeepSDF </a>。</p></figure><p id="1f90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2003.10983.pdf" rel="noopener ugc nofollow" target="_blank"> DeepLS </a>建议通过结合经典计算机图形学文献中一个至关重要且众所周知的观察结果来解决这个问题——<strong class="kh ir">一个大的表面可以分解成</strong> <strong class="kh ir">个局部形状。</strong>因此，他们将空间划分为一个体素网格，每个体素在一个小的局部邻域中存储有关表面的信息。该信息被存储为<strong class="kh ir">学习潜在代码</strong>(即，为每个体素V_i存储潜在代码z_i)。这样，网格本质上<strong class="kh ir">编码关于形状的局部信息</strong>，并且网络将该局部信息映射到SDF值。本质上，他们的方法是:</p><ol class=""><li id="b860" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">将空间划分为规则的体素网格。</li><li id="2a58" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">对于给定的点，<strong class="kh ir"> x </strong>，在网格上找到对应的体素<em class="oc"> V_i </em>。</li><li id="8d0b" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">将x变换到<em class="oc"> V_i </em>局部坐标系。<em class="oc">T _ I</em>(<em class="oc">x</em>)=<em class="oc">x</em>—<em class="oc">x _ I</em></li><li id="e7fc" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">将潜在代码(z_i)和转换后的坐标(T_i)输入网络，并回归SDF。</li><li id="fdd5" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">在优化潜在代码(z_i)和神经网络参数的同时，对大量步骤重复2–5。</li></ol><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi od"><img src="../Images/729ddcb314d1ca1d5464659f1b7f70a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HIS3t4dFjl9KbiVe04E0w.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">DeepSDF和DeepLS方法的2D比较。DeepSDF使用一个点和一些形状代码(z)来表示特定的形状SDF，而DeepLS利用网格来表示局部形状代码。注意，DeepLS优化了神经网络参数和潜在向量(z_i)。该图取自<a class="ae lk" href="https://arxiv.org/pdf/2003.10983.pdf" rel="noopener ugc nofollow" target="_blank">深度</a>。</p></figure><p id="dc82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我建议用另一种方式来考虑他们的建议。分析学习问题的一个常见主题是将任务分成两个不同的子任务。首先，给定原始数据(在我们的例子中是3D点)的特征提取任务产生描述数据的提取特征向量。然后，第二个子任务是在给定该特征向量的情况下回归SDF值。传统上，特征提取部分负责将原始数据转换为良好的表示特征，从而简化回归(一般是下游任务)。虽然这种描述听起来可能很模糊，但今天的深度学习实践者已经习惯于使用大型冷冻(已经在大规模数据集上训练过)神经网络作为特征提取器，并训练一个小型模型，该模型将这些特征作为给定任务的输入。</p><p id="3cc7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑到这一点，我将DeepLS的网格视为一个<strong class="kh ir">局部特征提取器</strong>，将神经网络视为一个简单的回归模型。所以回归模型可以小很多。具体来说，他们的神经网络只有<strong class="kh ir"> 50K </strong>个参数，相比之下DeepSDF的<strong class="kh ir"> ~2M </strong>个参数。此外，虽然网格本身可能很大(甚至比DeepSDF网络本身更大)，但是在每次通过网络时，只有少量的参数影响计算<strong class="kh ir">！这使得训练和推理速度更快，同时以计算换取内存。</strong></p><p id="d31c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总的来说，使用网格，DeepLS明显比DeepSDF更有效。在下面的例子中，DeepLS的训练速度比DeepSDF快10，000倍。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oe"><img src="../Images/e3017628ae931bb5c64c7d8a1cbbb092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtbR2ftMO7vPt-L6vgRExA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">在<a class="ae lk" href="https://en.wikipedia.org/wiki/Stanford_bunny" rel="noopener ugc nofollow" target="_blank">斯坦福兔子</a>上进行DeepLS和DeepSDF训练时间对比。DeepSDF用了<strong class="bd of"> 8天</strong>才在<strong class="bd of"> 1分钟</strong>后达到DeepLS结果。大约快了10，000倍。图来自<a class="ae lk" href="https://arxiv.org/pdf/2003.10983.pdf" rel="noopener ugc nofollow" target="_blank">深处</a>。</p></figure><p id="35ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，DeepLS从几个方面衡量了他们的贡献，而我只关注效率。更多细节请参考他们的论文。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="97f2" class="lq lr iq bd ls lt nw lv lw lx nx lz ma jw ny jx mc jz nz ka me kc oa kd mg mh bi translated">2.场景表示的神经网络</h1><p id="d7bc" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">在这一部分，我将重点介绍场景表现任务。具体来说，在使用神经网络解决新颖的视图合成任务上，如<a class="ae lk" href="https://arxiv.org/pdf/1906.01618.pdf" rel="noopener ugc nofollow" target="_blank"> SRN </a>和<a class="ae lk" href="https://arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank"> NeRF </a>所建议的。起初，这可能与上一部分没有太大关系，但在下一部分，它将有希望变得有意义。</p><h2 id="144f" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak"> 2.1。</strong><a class="ae lk" href="https://arxiv.org/pdf/1906.01618.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">SRN</strong></a><strong class="ak">—用神经网络表示场景</strong></h2><p id="b719" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">到目前为止，我们讨论了形状表示。然而，为了解决新颖的视图合成任务，需要一个<strong class="kh ir">场景表示</strong>。理想情况下，这种表示将是连续的，并使人们能够从不同的视图中查询场景并控制场景属性(例如，其光源)。 <a class="ae lk" href="https://arxiv.org/pdf/1906.01618.pdf" rel="noopener ugc nofollow" target="_blank"> SRN </a>提出用神经网络来表现场景。</p><p id="772f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等等，我们如何训练一个神经网络来代表场景？为了做到这一点，我们需要以某种方式监督网络如何很好地表现场景。这意味着我们需要它从场景中的特定视图生成图像，并训练它最小化生成的图像和真实图像之间的差异。虽然近年来使用神经网络(所谓的生成模型，如著名的<a class="ae lk" href="https://arxiv.org/abs/1812.04948" rel="noopener ugc nofollow" target="_blank"> StyleGAN </a>)生成图像取得了巨大的成功，但它未能以多视图一致的方式做到这一点，并且在3D视点上精确地调节它是非常重要的。</p><p id="514d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，SRN建议使用一个<strong class="kh ir">可区分渲染器</strong>来训练一个网络来表示场景。虽然SRN训练机制有些复杂，但总体思路可以直观地简化。给定一个3D坐标，神经网络将其表示为该坐标处场景的<strong class="kh ir">特征。然后，他们使用可区分的渲染器为该坐标的特征生成相应的像素颜色。对于给定的特定像素，其<strong class="kh ir">颜色受沿到达相机并与该像素相交的光线的多个3D坐标</strong>的影响。从相机发出的光线穿过像素可以在下图中看到。</strong></p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi og"><img src="../Images/b7eb970286febc6debaf69cb916fbc74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uRBJx6I46g-oudEv.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">体光线投射的四个基本步骤。对于图像平面中的每个像素(图像底部的条)，一条射线穿过体积(1)。沿着该光线采样几个坐标(2)，并计算它们的颜色(3)。然后，它们的颜色的集合定义了像素颜色。图来自<a class="ae lk" href="https://en.wikipedia.org/wiki/Volume_ray_casting" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p></figure><p id="26d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，由于沿着那条射线有无限的3D坐标，并且场景几何在训练期间隐含地出现，SRN不得不处理一个关键问题——<strong class="kh ir">如何决定沿着射线采样什么点</strong>？天真地为每条光线采样大量的3D坐标而不知道聚焦在哪里是不切实际的。</p><p id="2195" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，SRN提出了一种有点复杂的时间射线行进层(称为射线行进LSTM)来解决采样问题。具体来说，他们使用了一个LSTM，给定相机的光线起点，迭代地寻找其与场景几何图形的交点3D坐标，并返回该3D坐标提取的特征。然后，使用像素生成器网络将3D坐标特征向量映射到单个RGB值，即对应的像素颜色。通过这样做，他们将生成的像素颜色与给定图像的原始像素颜色进行比较，从而<strong class="kh ir">仅使用图像监督场景表示网络</strong>。</p><p id="8e96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然它们的机制有些复杂，但我建议你记住神经表征学习的一般流程，因为它更容易掌握。基本上，给定一个场景的一组图像，我们使用神经网络来表示该场景，并从不同的视图渲染该场景，只需简单地管理网络来产生与原始视图相同的场景视图。</p><p id="7514" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在SRN的原始论文中，一个被认为可能是未来工作的关键限制是<strong class="kh ir">建模视图和闪电效果</strong>。人们应该如何将这些信息编码到SRN中还不清楚。</p><h2 id="c5a2" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak"> 2.2。然后就是</strong><a class="ae lk" href="https://arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">NeRF</strong></a><strong class="ak">——神经辐射场</strong></h2><p id="aac6" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">正如在研究中经常发生的那样，简单的解决方案往往在复杂的解决方案被提出后出现，以更直观的机制产生更好的结果。NeRF的作者建议使用(基于坐标的)MLP对场景建模，给定5D输入(位置和观察方向)产生颜色和密度(RGB-alpha)。NeRF没有将坐标映射到潜在特征，也没有使用SRN的复杂机制来生成相应的像素颜色，而是直接回归该坐标的RGB-alpha值，并将其提供给可区分的光线行进渲染器。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oh"><img src="../Images/a5d49dd1cc02e0979f1ad357ad84741e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPIyMDTTAfn64mcYK5Vqiw.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">NeRF通过沿相机光线采样5D坐标(位置和观察方向)来合成图像(a)，将这些位置输入MLP以产生颜色和体积密度(b)，并使用体积渲染技术将这些值合成为图像(c)。图来自<a class="ae lk" href="https://arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank"> NeRF论文</a>。</p></figure><p id="9486" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">体积渲染听起来可能很吓人。但是需要注意的重要一点是，体绘制基本上就是用<strong class="kh ir">来近似沿着到达相机的光线</strong>的总光亮度 <strong class="kh ir">。因为我们要对光线求和，所以我们可以衡量“来自光线坐标上某个位置的光线对总光线的影响有多大”是至关重要的关于场景属性(遮挡、材质等)。).直觉上，我们会给不透明的物体</strong>较大的权重<strong class="kh ir">，给透明的物体</strong>较小的权重<strong class="kh ir">，让我们能够透过玻璃而不是透过桌子看到东西。这是预测密度(sigma)值在NeRF算法中的关键规则。</strong></p><p id="3ebf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用基于5D坐标的MLP，NeRF通过计算来估计每个像素的颜色:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oi"><img src="../Images/4464d8bc17c35e343f955a46bce92e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MKrZ3Ufs9QX0MZBnUVkoWw.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">计算预测颜色(C^(r)的公式，用于从相机通过像素追踪的光线。T_i计算累积透射比(即光线传播到一定距离而不碰到不透明粒子的概率)。<strong class="bd of"> <em class="oj"> σi </em> </strong>表示给定5D坐标下的预测密度(alpha)值，<strong class="bd of"> <em class="oj"> δi </em> </strong>为沿射线相邻样本之间的距离。</p></figure><p id="a8d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我跳过了具体的部分，比如关于NeRF训练机制的细节，沿着射线的采样坐标，以及位置编码自一个<a class="ae lk" href="https://www.youtube.com/watch?v=CRlN-cYFxTk" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">lot</strong></a><strong class="kh ir"/><a class="ae lk" href="https://datagen.tech/guides/synthetic-data/neural-radiance-field-nerf/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">有</strong> </a> <strong class="kh ir"> </strong> <a class="ae lk" href="https://blog.devgenius.io/paper-explained-nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-e16567180531" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">有</strong> </a> <strong class="kh ir"> </strong> <a class="ae lk" href="https://medium.com/swlh/nerf-neural-radiance-fields-79531da37734" rel="noopener"> <strong class="kh ir">写成</strong> </a> <strong class="kh ir"> </strong> <a class="ae lk" href="https://www.analyticsvidhya.com/blog/2021/04/introduction-to-neural-radiance-field-or-nerf/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">上有</strong> </a> <strong class="kh ir"> </strong> <a class="ae lk" rel="noopener" target="_blank" href="/its-nerf-from-nothing-build-a-vanilla-nerf-with-pytorch-7846e4c45666"> <strong class="kh ir">吧</strong></a></p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="54f4" class="lq lr iq bd ls lt nw lv lw lx nx lz ma jw ny jx mc jz nz ka me kc oa kd mg mh bi translated">3.理解场景的神经网络</h1><p id="3699" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">NeRF提出了一个惊人的强大的场景表示。然而，它将所有场景属性耦合在一起，从结构到闪电。在这里，我将描述从中提取一个几何图形(表面)的方法。</p><p id="f46a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2106.12052.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">VolSDF</strong></a><strong class="kh ir">&amp;</strong><a class="ae lk" href="https://arxiv.org/pdf/2106.10689.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">NeuS</strong></a><strong class="kh ir">—从密度到有符号距离</strong></p><p id="3e8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NeRF在捕捉场景方面表现得非常成功。然而，虽然NeRF可以生成场景的新视图，但尚不清楚如何提取几何图形。起初，这可能听起来微不足道——我们已经预测了每个坐标的密度值，这应该模糊地描述了一个表面！事实上，表面周围的密度值较高，远离表面的密度值较低。然而，定义“高”密度的阈值绝非易事。此外，它依赖于视图，这不是几何图形所需的特性。因此，研究人员寻找创造性的方法，通过明确学习几何来修改NeRF架构。</p><p id="4888" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，深度学习的直观连续几何表示是SDF。希望我们可以训练一个NeRF来预测SDF值，并仍然使用体绘制来端到端地训练它(并直接从场景图像中获得监督，从而在没有任何3D监督的情况下重建几何图形！).幸运的是，在NeurIPS 2021上，两篇有趣的论文解决了这个特定的问题。他们的作品被戏称为<a class="ae lk" href="https://arxiv.org/pdf/2106.12052.pdf" rel="noopener ugc nofollow" target="_blank"> VolSDF </a>和<a class="ae lk" href="https://arxiv.org/pdf/2106.10689.pdf" rel="noopener ugc nofollow" target="_blank"> NeuS </a>(读作“新闻”)。</p><p id="6c2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他们都建议训练两个网络——一个SDF网络和一个颜色/外观网络。他们的不同之处在于他们建议<strong class="kh ir">将SDF值转换为密度值，并在体绘制中使用它们。</strong>主要思想就是这么简单——我们想要转换SDF值，这样靠近表面的点将接收高alpha值，而远离表面的点将接收低alpha值。这样，近表面点的颜色将对光线的积分产生最大的影响。如NeuS所述，SDF到密度的转换必须满足以下特性:</p><ol class=""><li id="d670" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kh ir">无偏</strong> —对于给定光线，在SDF函数产生0的点(即表面上的点)，密度值应达到<strong class="kh ir">局部最大值。</strong>注意，我们只希望局部最大，而不是全局最大，因为沿射线可能有几个表面交点。</li><li id="89cf" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><strong class="kh ir">遮挡感知— </strong>当两个点具有相同的SDF值时，更靠近相机的点对最终输出颜色的贡献更大。</li></ol><p id="6732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直观地说，该转换只是使用以零为中心的<strong class="kh ir">逻辑分布</strong>将SDF值映射到密度值。这很方便，因为它是一个以0为中心的钟形分布，因此将最大密度分配给SDF值0，并以递减方式将密度分配给其他地方的SDF值。</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ok"><img src="../Images/2b427b19cf99eb384c22f7c8ae8f638b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5zcKwE96Lt8QwUrL320bA.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">均值为0、标准差为1的逻辑密度分布的PDF。人们可以天真地使用这种分布将SDF值映射到密度值。然而，如NeuS和VolSDF所示，需要更复杂的方法来使变换不偏不倚且具有遮挡意识。</p></figure><p id="22c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我建议读者参考相关论文，以了解构建从SDF到密度值的转换的更复杂的方法。</p><p id="8dcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然他们都提出了一种直接从场景观察中学习SDF的简洁方法，但他们<strong class="kh ir"> </strong>仍然有一个关键的限制——从这些相对较大的基于坐标的MLP中进行推断的训练&amp;仍然非常<strong class="kh ir">缓慢。</strong>具体来说，针对特定场景训练他们每个人需要<strong class="kh ir">~ 12-14小时</strong>。</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="761c" class="lq lr iq bd ls lt nw lv lw lx nx lz ma jw ny jx mc jz nz ka me kc oa kd mg mh bi translated">4.把点连起来，让它更快！</h1><h2 id="0450" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated"><strong class="ak"> 4.1。</strong><a class="ae lk" href="https://arxiv.org/pdf/2201.05989.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="ak">InstantNGP</strong></a><strong class="ak">—更好的内存与计算的权衡</strong></h2><p id="09bd" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">有两个可能的方向使基于NeRF的方法的训练和推断更快:沿着每条射线采样<strong class="kh ir">更少的</strong>点，或者采样<strong class="kh ir">更快</strong>。对于采样更少的点有一些有趣的想法，但是在这里我关注的是采样更快。</p><p id="c4b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回想一下，DeepLS通过引入一个可学习的体素网格，使DeepSDF的采样速度更快。然而，随着内存相对于网格的立方增长，它以昂贵的内存成本换取了采样效率。有多贵？对于具有16维潜在向量的128大小的简单网格，需要存储<strong class="kh ir"> ~33.6M网格参数</strong>，而不考虑MLP大小(通常很小，大约100-300k参数)。不幸的是，简单地缩小网格会严重影响捕捉精细几何图形的性能和能力。需要更好的解决方案。</p><p id="49c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，今年早些时候(2022年), InstantNGP建议通过用<strong class="kh ir">多分辨率散列网格替换网格来显著减少内存使用。</strong>instant ngp背后的关键思想是通过几个不同大小的网格来捕捉粗略和精细的细节。低分辨率网格只占用少量内存，并且只捕捉粗略的细节。高分辨率的网格可能捕捉到细微的细节，但代价是占用大量内存。因此，他们建议<strong class="kh ir">用哈希表索引高分辨率网格。传统上，在计算机科学中，哈希表用于权衡计算时间和内存使用。他们的方法如下:</strong></p><ol class=""><li id="dcd3" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">将空间分成几个不同分辨率级别的网格(从粗到细的网格)。</li><li id="45e0" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">对于给定点，<strong class="kh ir"> x </strong>，在每个分辨率级别找到包含体素的角的索引。</li><li id="bc6e" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">使用每个分辨率级别的哈希表，查找角的特征向量。</li><li id="2134" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" href="https://en.wikipedia.org/wiki/Trilinear_interpolation" rel="noopener ugc nofollow" target="_blank">三线性插值</a>每个分辨率级别的特征向量。</li><li id="fc9a" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">将每个分辨率级别的向量连接起来，并将其输入神经网络。</li><li id="4a55" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">对大量步骤重复2–5，同时优化多分辨率网格的特征向量和神经网络参数。</li></ol><p id="d6ce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图总结了这一过程:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ol"><img src="../Images/dd24fdcfad1a75c1c7ddb45a968b7d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFCWK_SNyi5aDBd-5i6CPQ.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">2D的InstantNGP多分辨率散列网格演示。给定一个坐标x，找出每个分辨率下的包围体素(标为红色和蓝色)并对它们的边缘进行插值。连接多分辨率插值向量，并将它们输入到神经网络中，然后执行您想要的基于坐标的任务。图来自<a class="ae lk" href="https://arxiv.org/pdf/2201.05989.pdf" rel="noopener ugc nofollow" target="_blank"> InstantNGP </a>。</p></figure><p id="4ee4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，指出关于InstantNGP的一个不直观的问题是至关重要的。当一个人使用哈希表时，他需要解决<strong class="kh ir">冲突的可能性</strong>(即，两个不同的3D坐标被映射到相同的特征向量)以及<strong class="kh ir">如何解决</strong>这样的冲突。关于这一主题的传统计算机科学文献充满了许多复杂的机制和技巧，以减少冲突的可能性，并在冲突发生时找到快速解决冲突的方法。然而，InstantNGP仅仅依靠神经网络来学习消除哈希冲突 <strong class="kh ir">本身</strong>。</p><p id="a632" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，InstantNGP提供了有效训练基于坐标的神经网络的通用方法。具体来说，他们展示了他们的方法对于NeRF以及这里没有讨论的其他几个任务是多么有效。为了理解他们的方法有多有效，请注意NeRF网络对单个场景的训练时间从12小时减少到5秒。也就是大约2年左右的时间<strong class="kh ir">~ 8500 x</strong>。</p><h2 id="a876" class="mn lr iq bd ls mo mp dn lw mq mr dp ma ko ms mt mc ks mu mv me kw mw mx mg my bi translated">4.2.<a class="ae lk" href="https://arxiv.org/pdf/2206.00665.pdf" rel="noopener ugc nofollow" target="_blank"> MonoSDF </a> —表面重建，带体积渲染，更快！</h2><p id="d83f" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">我们学习了如何修改NeRF训练机制，以支持显式SDF表示学习。此外，我们还学习了如何通过合并多分辨率哈希表来加快NeRF的训练和推理。MonoSDF结合了这两种方法以及<strong class="kh ir">几何线索</strong>,使得从设定好的图像进行表面重建更快更准确。</p><p id="3011" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，MonoSDF不仅使它更快，而且旨在更好地执行，因为它为网络提供了几何线索。具体而言，使用单目几何预测领域的进展(即，直接从单个RGB图像预测正常或深度图像)，它监督网络为NeRF训练渲染深度和正常图像以及重建的单目图像。这些几何线索改善了重建质量和优化时间。他们的(高层)架构是:</p><figure class="na nb nc nd gt ne gh gi paragraph-image"><div class="gh gi om"><img src="../Images/493922dde0c7a01bfd4ee43655b09e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*Ul1ZLu0_qQJJphOx932tMQ.png"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">MonoSDF使用多分辨率特征格网(如InstantNGP中所示)来预测给定坐标的SDF值、法线(n)和深度(z)值。他们使用体绘制来重建场景，并通过预测深度和正常图像的预训练网络进行监督。图来自<a class="ae lk" href="https://arxiv.org/pdf/2206.00665.pdf" rel="noopener ugc nofollow" target="_blank"> MonoSDF </a>。</p></figure></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="26f2" class="lq lr iq bd ls lt nw lv lw lx nx lz ma jw ny jx mc jz nz ka me kc oa kd mg mh bi translated"><strong class="ak">结论</strong></h1><p id="35fd" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">近年来，有一个帮助解决计算机图形任务的神经隐式表示的爆炸。在这篇文章中，我重点讨论了它们在三种不同任务中的适用性——形状表示、新颖的视图合成和基于图像的3D重建。</p><p id="634d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">拥有神经表示是解决许多有趣任务的使能器，但仍有很大的改进空间。我们需要可编辑的(具有显式控制)、可扩展的(不仅适用于受控环境中的小场景)、可快速查询的和可概括的表示。这些要求中的每一项都包含许多挑战。我希望在接下来的几年里，当我回过头来看这篇文章时，会惊讶于研究界是如何再一次带领我们取得了今天看来几乎难以置信的进步。</p></div></div>    
</body>
</html>