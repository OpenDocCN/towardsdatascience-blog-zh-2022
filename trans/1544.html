<html>
<head>
<title>How to boost your GNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何提升你的GNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-boost-your-gnn-356f70086991#2022-04-14">https://towardsdatascience.com/how-to-boost-your-gnn-356f70086991#2022-04-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="891c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">改善图形神经网络的技巧和诀窍</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/2fa0e244fc4d8aa5dfe7b149dfa740af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f9OcJeBadVnLXlG6"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">由<a class="ae kw" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">彼得罗·郑</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4876" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">图形神经网络(GNN)是新的酷小子。它的名字听起来很花哨，数学很先进，GNNs在各种任务上表现出最先进的性能。</p><p id="f31d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">GNNs提供了一种在图结构数据上使用深度学习技术的方法。图结构数据无处不在:从化学(如分子图)，到社交媒体(如社交网络)，还有金融投资(如风险投资网络)。</p><p id="773a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我的硕士论文期间，我在一个由投资者、创业公司和个人组成的投资网络上，使用GNNs预测了创业公司的未来融资轮次。当我实现GNN模型(R-GCN和HGT)时，基线随机森林模型已经相当强大(AUC = 0.69)。</p><p id="20a4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我很高兴看到先进的GNN将如何提高我的成绩。令我惊讶的是，GNN模型总是落后于基线模型的表现。增加层数和隐藏尺寸没有帮助。所以我开始研究，发现我并不是唯一一个有GNN关系问题的人。</p><p id="ac96" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果这听起来对你来说很熟悉(很可能是这样，否则你为什么要费心阅读这篇枯燥的博客文章)，我可能有一些提高你的GNN模型性能的技巧。</p><h1 id="63bd" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">什么是GNNs来着？</h1><p id="b33f" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在我们讨论如何改进gnn之前，让我们快速回顾一下它们是如何工作的。关于图形神经网络的伟大而深入的介绍，请查看来自<a class="ae kw" href="https://www.youtube.com/channel/UCScjF2g0_ZNy0Yv3KbsbR7Q" rel="noopener ugc nofollow" target="_blank"> DeepFindr </a>(或一些literature⁴)的Youtube频道</p><p id="8474" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设我们有一个简单的图表，如下所示:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mq"><img src="../Images/cee312d9015a4250a9b5cefd9dc80ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d41ghszApj_zPIHYLZcMzg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">包含节点特征的图(图片由作者提供，改编自DeepFindr)</p></figure><p id="e511" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">图</strong> <br/>图包含节点(I，j，k)和连接这些节点的边(e)。此外，该图还包括每个节点(X1，…)的节点特征，可能还包括每条边(黑色)。目标节点用黄色表示，它的1跳邻居用蓝色表示，2跳邻居用绿色表示。包括不同类型节点的图被称为“异构”图，就像我们的例子一样。</p><p id="9272" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">消息传递</strong> <br/>节点在步骤0的嵌入只是它的特征向量(由特征X1，X2……组成)。为了获得我们的目标节点I(黄色)的新的(l + 1)节点嵌入h，我们从它的相邻节点j(蓝色)、它自己的表示以及潜在的边特征e(黑色)中提取所有嵌入，并且聚集这些信息。这显示在下面的公式中。但是，请记住，目前大多数著名的GNN架构都没有利用边缘features⁵.之后，我们可以将这些新的节点嵌入用于各种任务，例如节点分类、链路预测或图形分类。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mr"><img src="../Images/8f9d470d445da88e46f73a172c86d746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-6fFXpMa9s-xWNVKNPALg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">通用GNN层公式</p></figure><h1 id="0d74" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">GNNs的问题</h1><p id="82ae" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><strong class="kz ir">在实践中<br/> </strong>最近在各个领域的许多研究发现，GNN模型并没有提供预期的performance⁵ ⁶ ⁷.当研究人员将它们与更简单的基于树的基线模型进行比较时，GNNs无法超越甚至匹配基线。</p><p id="9b44" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">其他研究人员对GNNs⁸.有时表现不佳提供了理论上的解释根据他们的实验，GNNs仅执行特征去噪，而不能学习非线性流形。因此，他们主张将GNNs视为图学习模型的一种机制(例如，用于特征去噪),而不是一个完整的端到端模型。</p><h1 id="1c71" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">修理</h1><p id="40d6" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了弥补这些缺点，提高你的GNN，我找到了3个主要的技巧/想法:</p><ol class=""><li id="c999" class="ms mt iq kz b la lb ld le lg mu lk mv lo mw ls mx my mz na bi translated"><em class="nb">利用GNN中的边缘特征</em></li><li id="a5a8" class="ms mt iq kz b la nc ld nd lg ne lk nf lo ng ls mx my mz na bi translated"><em class="nb">GNNs自我监督预培训</em></li><li id="00b5" class="ms mt iq kz b la nc ld nd lg ne lk nf lo ng ls mx my mz na bi translated"><em class="nb">单独托辞&amp;下游任务</em></li></ol><p id="1d14" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们来详细了解一下。</p><h1 id="6c8e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">利用边缘特征</h1><p id="e229" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">利用边要素的想法在很大程度上取决于您正在处理的数据类型。如果您的数据包含(多维)边要素，利用边要素可以提高模型的性能。</p><p id="d8a8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然而，很少建立的GNN模型架构支持多维边缘features⁵.我使用的一个简单的工作方法是创建人工节点:</p><p id="5f05" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人工节点<br/> </strong>通过使用人工节点，你可以继续使用你之前使用的相同模型。我们唯一要改变的是图表本身，它会变得更复杂一点。不是边保存边特征，而是每条边将成为自身的一个节点(浅蓝色)连接到原始的深蓝色节点。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nh"><img src="../Images/b8d41d5d962ddb9d2f96f18fb13c8f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GDy8Ube34nWGuVYL5TpzQA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">添加人工节点前后的投资网络元图(图片由作者提供)</p></figure><p id="3124" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这样，您可以通过将边要素作为人造节点的节点要素传递给模型来间接利用边要素。如果边要素与任务相关，这可以提高模型的性能，但也会增加复杂性。您可能需要考虑向模型中添加更多的GNN图层(以允许更多的邻居跃点)。人工结节导致AUC增加约2%。</p><p id="821e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">你自己的Edge特性架构<br/> </strong>你也可以创建自己的GNN层实现。这听起来很难；但是，如果你使用一个图形学习库，比如<a class="ae kw" href="https://www.dgl.ai/" rel="noopener ugc nofollow" target="_blank"> DGL </a>，这肯定是可行的。我建议您使用现有的GNN实现(见<a class="ae kw" href="https://docs.dgl.ai/en/0.6.x/api/python/nn.pytorch.html" rel="noopener ugc nofollow" target="_blank">这里</a>)并根据您的需要进行调整。</p><p id="cf22" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">作为一个例子，我基于GAT ⁰和R-GCN的现有思想制定了自己的实现，我称之为Edge-GCN:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/219430716082d339dda98309c8c969ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KdmwugFR_xLPdDAkSp3VQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">我自己的边缘-GCN公式(v代表节点，e代表边缘，sigma是非线性激活函数[在这种情况下是RELU]，alpha是每个关系类型的学习注意力分数，c是归一化常数)</p></figure><p id="df11" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">边GCN使用注意机制来学习所有不同关系类型的边相对于节点特征的重要性。如果您对DGL的实施感兴趣，请联系我。E-GCN架构将GNN模型的AUC结果提高了约2%(人造节点也是如此)。</p><h1 id="738d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">自我监督的预培训</h1><p id="08e9" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">这可能是提高你的GNN性能的最重要的技巧。尽管预训练GNNs已经在理论上进行了探索，但在实践中的应用仍然是rare⁶ ⁷ ⁸.</p><p id="8ffa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这个想法与计算机视觉和自然语言处理领域的概念非常相似。以语言模型BERT为例，它被训练来预测句子中的屏蔽词(这是自我监督的，因为它不依赖于标记的数据)。我们通常不太关心预测屏蔽词的具体任务。然而，由于模型了解特定单词如何相互关联，因此得到的单词嵌入对于许多不同的任务非常有用。</p><p id="c30a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">自我监督</strong> <br/>我们想要使用自我监督任务来预训练GNN模型节点嵌入。在有噪声的标签的情况下，这可能是特别有益的，因为自我监督过程提供了更多的“有标签的”例子(因为我们不需要为预训练提供标签)，并且可能也不容易有噪声。</p><p id="8df1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果我们的最终目标是对节点进行分类，我们可以在整个图上使用链接预测作为自我监督的预训练任务。在链接预测中，我们试图预测两个节点之间是否存在边(=链接)。因此，我们训练GNN来区分图中的真实边和人工引入的假边(“用于链接预测的负采样”)。由于我们只是简单地添加伪边并移除图中现有节点之间的真实边，因此我们不依赖任何标记的数据。接下来，我们可以使用从链路预测GNN模型得到的节点嵌入作为另一个节点分类模型的输入。</p><p id="507d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">将自我监督的预训练管道添加到模型中，其AUC分数惊人地增加了14%，使这成为最有影响力的技巧。</p><h1 id="c2aa" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">单独的借口和下游任务</h1><p id="34d2" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">到目前为止，我们只讨论了在自我监督的任务(“借口”)上对GNN进行预训练，以及在最终(“下游”)任务上使用相同的GNN架构。然而，我们不必为这两个任务使用相同的模型架构。您还可以组合不同的GNN架构。或者更有希望的是:</p><p id="4b7f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">两全其美<br/> </strong>如前所述，GNN层可能无法在各种情况下高效学习。因此，我们可以使用GNN模型，通过自我监督的预训练来创建节点嵌入，并将这些嵌入传递给经典的机器学习算法或完全连接的神经网络层，以用于最终的下游任务。这种架构可以用于许多不同的下游任务，从图形分类到节点分类和回归。</p><p id="8008" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">该模型将受益于将访问图中包含的所有信息的能力与非线性流形学习属性相结合。此外，该模型继承了更简单的机器学习算法的一些好处，如减少训练时间和更好的可解释性。在我们的实验中，基于树的模型(例如随机森林)对于节点分类的下游任务显示出特别强的性能，因此可以从那里开始。</p><p id="c7d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">接下来，您可以看到包含所有三个提出的想法的最终管道的图形概述:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/63bcc21e656f903e798eea462dfb7780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU7Gzt_paof5oZ1zKjLr3Q.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">最终管道使用边缘特征、自我监督预训练和下游随机森林模型对图中的节点进行分类(X表示特征，E表示学习嵌入，h表示隐藏节点表示)(图片由作者提供)</p></figure><p id="7dc9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这种架构代表了我们的最终模型，AUC得分为78.1(另一个+ 1.5%)，或以AUC衡量的总性能增加17%(基础R-GCN模型:AUC = 66.6)。</p><h1 id="18cb" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="6327" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在这篇博文中，我们讨论了图形神经网络模型的缺点以及提高模型性能的三个主要技巧。结合这些技巧，我能够将我的最终GNN模型的AUC提高17%(其他指标甚至更高)。</p><p id="b66f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">首先，如果您的数据包含边特征，并且您认为它们对最终的预测任务很有帮助，那么您可以尝试利用边特征。第二，使用自我监督的目标预先训练GNN模型通常有益于最终模型的性能。它可以增加训练样本的数量，有时还可以减少固有噪声。第三，测试前文本和最终预测任务的不同架构可以提高模型的预测能力。</p><p id="6185" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这些技巧对你有用吗？如果你有任何补充或反馈，请在评论区告诉我！GNN调音快乐！</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><h1 id="305b" class="lt lu iq bd lv lw nr ly lz ma ns mc md jw nt jx mf jz nu ka mh kc nv kd mj mk bi translated">参考</h1><p id="ceff" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">[1] Michael Schlichtkrull、Thomas N. Kipf、Peter Bloem、Rianne van den Berg、Ivan Titov和Max Welling。用图卷积网络对关系数据建模，2017。</p><p id="2755" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[2]胡，董，王宽三，。异构图形转换器，2020。</p><p id="29ec" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[3]<a class="ae kw" href="https://deepfindr.com/understanding-graph-neural-networks-part-2-3/" rel="noopener ugc nofollow" target="_blank">https://deepfindr.com</a>，2020年。</p><p id="75f1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[4]达维德·巴丘、费德里科·埃里卡、阿莱西奥·米凯利亚、马尔科·波德达。图形深度学习的温和介绍，2020</p><p id="fa25" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[5]杨。NENN:在图形神经网络中结合节点和边特征，2020</p><p id="74ae" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[6]费德里科·埃里卡、马尔科·波德达、达维德·巴丘和阿莱西奥·米凯利。用于图形分类的图形神经网络的公平比较，2020。</p><p id="71d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[7]克莱门特·加斯托、西奥菲勒·卡尼尔和让-米歇尔·达勒。创业融资成功的外部因素的不同重要性:早期阶段的竞争和成长期的网络，2019。</p><p id="502f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[8]蒋德军，吴振兴，谢长玉，，陈，廖奔，王哲，，曹东升，，侯廷军.图形神经网络可以为药物发现学习更好的分子表示吗？2021.</p><p id="74a8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[9]黄NT和前原诚司。重温图形神经网络:我们所拥有的只是低通滤波器，2019。</p><p id="0cb1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[10]Petar veli kovi、Guillem Cucurull、Arantxa Casanova、Adriana Romero、Pietro Liò和Yoshua Bengio。图注意力网络，2018。</p><p id="06ac" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">11卡罗·哈普雷希特。使用图形神经网络预测未来融资轮次，2021</p><p id="24e4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[12]华伟·胡、、约瑟夫·戈麦斯、马林卡·兹特尼克、珀西·梁、维贾伊·潘德和朱尔·莱斯科维奇。预训练图神经网络的策略，2019。</p><p id="6561" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[13] Jacob Devlin，Chang Ming-Wei，Kenton Lee和Kristina Toutanova。BERT:面向语言理解的深度双向转换器预训练，2018。</p></div></div>    
</body>
</html>