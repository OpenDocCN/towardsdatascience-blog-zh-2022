<html>
<head>
<title>Topic Modeling — Intro and Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主题建模——介绍和实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-intro-and-implementation-927d6463b892#2022-12-06">https://towardsdatascience.com/topic-modeling-intro-and-implementation-927d6463b892#2022-12-06</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="5733" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用 NLTK 进行主题建模的简短教程</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/6a5c9484e957e3950d21de7638acc330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Ho16-yQ3HmjwjIBitGLBw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">成堆的书。E 2 </p></figure><p id="0849" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">企业与他们的客户互动，以更好地了解他们，也改善他们的产品和服务。这种互动可以采取电子邮件、文本社交媒体帖子(如 Twitter)、客户评论(如 Amazon)等形式。让人类代表浏览所有这些形式的文本通信，然后将这些通信路由到相关团队以对客户进行审查、采取行动和/或做出响应，这将是低效且成本高昂的。将这样的交互分组并分配给相关团队的一种廉价方法是使用主题建模。</p><p id="c78c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">自然语言处理(NLP)环境中的主题建模是一种无监督(即，数据未被标记)的机器学习任务，其中算法的任务是基于文档的内容将主题分配给文档集合。给定的文档通常包含不同比例的多个主题，例如，如果文档是关于汽车的，我们将期望汽车的名称比某些其他主题(例如，动物的名称)更显著地出现，而我们期望诸如“the”和“are”之类的某些词以几乎相等的比例出现。主题模型实现了数学方法来量化给定文档集合中这种主题的概率。</p><p id="376c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本帖中，我们将扩展我们的 NLP 知识深度，作为<a class="ae kz" href="https://medium.com/@fmnobar/data-scientist-role-requirements-bbae1f85d4d5" rel="noopener">数据科学家角色要求</a>的一部分。我们将首先围绕标记化、词性和命名实体识别的概念建立一些基础知识。然后，我们将实现一个情感分析练习，最后使用潜在的狄利克雷分配进行主题建模。</p><p id="b049" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">和我的其他帖子类似，学习会通过练习问答来实现。我将根据需要在问题中包含提示和解释，以使旅程更容易。最后，我用来创建这个练习的笔记本链接在文章的底部，你可以下载，运行和跟随。</p><p id="8a1f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们开始吧！</p><p id="a331" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><em class="lw">(所有图片，除非特别注明，均为作者所有。)</em></p><div class="lx ly gq gs lz ma"><a href="https://medium.com/@fmnobar/membership" rel="noopener follow" target="_blank"><div class="mb ab fp"><div class="mc ab md cl cj me"><h2 class="bd iv gz z fq mf fs ft mg fv fx it bi translated">加入我的介绍链接-法扎德 Mahmoodinobar 媒体</h2><div class="mh l"><h3 class="bd b gz z fq mf fs ft mg fv fx dk translated">阅读法扎德(以及媒体上的其他作家)的每一个故事。你的会员费直接支持法扎德和其他…</h3></div><div class="mi l"><p class="bd b dl z fq mf fs ft mg fv fx dk translated">medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo kt ma"/></div></div></a></div></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="f9c1" class="mw mx iu bd my mz na nb nc nd ne nf ng ka nh kb ni kd nj ke nk kg nl kh nm nn bi translated">数据集</h1><p id="c9d1" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">为了实现这篇文章中涵盖的概念，我们将使用来自<a class="ae kz" href="https://archive.ics.uci.edu/ml/datasets/sentiment+labelled+sentences" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习知识库</a>的数据集，该数据集基于论文“使用深度特征从组到个体标签”(<a class="ae kz" href="http://www.datalab.uci.edu/papers/kdd2015_dimitris.pdf" rel="noopener ugc nofollow" target="_blank">Kotzias et al .al，2015 </a>)并且可以从<a class="ae kz" href="https://gist.github.com/fmnobar/88703ec6a1f37b3eabf126ad38c392b8" rel="noopener ugc nofollow" target="_blank">这个链接</a>下载(CC BY 4.0)。</p><p id="406a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们从导入我们今天将使用的一些库开始，然后读取数据集并查看数据帧的前 10 行。每个命令前都有注释来进一步解释这些步骤。</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="063c" class="ny mx iu nu b be nz oa l ob oc"># Import libraries<br/>import pandas as pd<br/>import numpy as np<br/>import nltk<br/><br/># Making the full width of the columns viewable<br/>pd.set_option('display.max_colwidth', None)<br/><br/># Making all rows viewable<br/>pd.set_option('display.max_rows', None)<br/><br/># Read the data set and drop the df['label'] column<br/>df = pd.read_csv('imdb_labelled.csv').drop('label', axis = 1)<br/><br/># Taking out a list of strings to clean up the data<br/>df = df.replace(['\n', '\t1', '\t0'],'', regex=True)<br/><br/># Return top 10 rows of the dataframe<br/>df.head(10)</span></pre><p id="c1ef" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj od"><img src="../Images/787dae688c5fbaeb19c8f660a21f2834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hy7iu5nZ1Y1xehz0oIBWyA.png"/></div></div></figure><h1 id="311e" class="mw mx iu bd my mz oe nb nc nd of nf ng ka og kb ni kd oh ke nk kg oi kh nm nn bi translated">教程+问答</h1><h2 id="0a9d" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">标记化</h2><p id="b37c" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">标记化是指将文本字符串分解成更小的子字符串。这些子字符串可以位于不同的级别。例如，一个句子级的标记化策略将一个给定的字符串分解成句子，而其他标记化器可以将一个句子分解成更小的标记，如单词、二元模型等。在这个练习中，我们只需要将字符串分解成句子和单词，所以我不会深入研究其他标记化策略，但是如果你有兴趣了解更多，我有另一个帖子<a class="ae kz" href="https://medium.com/towards-data-science/sentiment-analysis-intro-and-implementation-ddf648f79327" rel="noopener">链接到这里</a>，在这里我将更详细地介绍标记、二元模型和 N-gram。</p><p id="2438" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题 1: </strong></p><p id="a318" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">定义一个名为“make_sentences”的函数，它接受一个系列作为它的参数，默认为我们的数据帧的“text”列的前 15 行，将每个条目分解成句子并返回这样的句子的列表。然后将该函数应用于数据帧的前 10 行。</p><p id="e64a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv"> <em class="lw">提示:</em> </strong> <em class="lw">使用</em> <code class="fe ov ow ox nu b"><em class="lw">nltk.sent_tokenize</em></code> <em class="lw">，将一个给定的字符串分成一个句子级别的子字符串列表。</em></p><p id="09da" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">答案:</strong></p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="ae07" class="ny mx iu nu b be nz oa l ob oc"># Import packages<br/>from nltk import sent_tokenize<br/><br/># Define the function<br/>def make_sentences(text = df['text'].head(15)):<br/>    <br/>    # Define a lambda function to apply the sent_tokenize to the df['text']<br/>    return text.apply(lambda x: sent_tokenize(x)).tolist()<br/><br/># Return the results for the top 10 rows of the dataframe<br/>make_sentences(df['text'].head(10))</span></pre><p id="8f35" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oy"><img src="../Images/7734d7e2a4b5dccaf282fadad93e1327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYiAljAGMdO4TA7HB3OEXQ.png"/></div></div></figure><h2 id="1519" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">词性</h2><p id="6fc2" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">到目前为止，我们可以把给定的字符串分成句子，句子由单词的集合组成。单词可以分解成词汇类别(类似于分类机器学习任务中的类)，包括名词、动词、形容词、副词等。这些词汇组在自然语言处理中被称为词类。将词性自动分配给单词的过程称为词性标注，这是 NLP 管道的常见步骤。标记在各种 NLP 任务中非常有用，例如，在机器翻译中，任务是提供输入文本(原始语言)的翻译(目标语言)。如果原始文本输入包括一个人的名字，我们不希望机器翻译模型翻译这个名字。确保这一点的一种方法是将那个人的名字标记为实体，然后当有标记的实体时，该模型将被绕过。换句话说，除了那个标记的实体之外，句子中的其他所有内容都将被翻译。然后，在最后，作为后处理步骤的一部分，标记的实体将被映射到最终翻译结果中的正确位置。</p><p id="c710" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">有各种方法来创建标记策略，例如基于正则表达式的或甚至训练过的机器学习模型。在今天的练习中，我们将依靠 NLTK 提供的现有的词性标注。让我们看一个例子来更好地理解这个概念。</p><p id="d033" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们从创建一个样本字符串开始，然后通过 NLTK 的 POS 标记器运行它并检查结果。</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="419d" class="ny mx iu nu b be nz oa l ob oc"># Create a sample sentence<br/>sample = 'I am impressed by Amazon delivering so quickly in Japan!'<br/><br/># Import required libraries<br/>from nltk import word_tokenize, pos_tag<br/><br/># Break down the sample into word tokens<br/>tokens = word_tokenize(sample)<br/><br/># Create POS tagging<br/>pos = pos_tag(tokens)<br/><br/># Return the POS tag results<br/>pos</span></pre><p id="9644" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oz"><img src="../Images/788918965bdf4981d464190162ed464c.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*rONYUjU8gMckYpXk2abTZg.png"/></div></figure><p id="e57c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在我们看看标记结果是什么样的。例如，“快速”被标记为“RB”，这意味着一个副词，或者“亚马逊”被标记为“NNP”，这意味着一个名词。NLTK 为标签提供了文档。例如，如果我们想知道“RB”指的是什么，我们可以运行以下命令:</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="a6d5" class="ny mx iu nu b be nz oa l ob oc">nltk.help.upenn_tagset('RB')</span></pre><p id="f7ec" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pa"><img src="../Images/8e4906952cc1383f920c215e8eebb14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZVlZUplUX-nqTOrX9c8qg.png"/></div></div></figure><p id="a551" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">如果您想查看所有的标签，您可以不带参数运行相同的命令。</p><h2 id="8c09" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">命名实体识别</h2><p id="80a7" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">现在我们对句子中的每个单词都进行了词性标注，但并不是所有的名词都是相同的。例如，“亚马逊”和“日本”都被标记为“NNP”，但一个是公司名称，另一个是国家名称。命名实体识别或 NER(又名命名实体分块)涉及通过将给定的文本输入分类成预定义的类别，如个人、组织、位置等，来从给定的文本输入中提取信息。让我们看一个例子来看看这是如何工作的。</p><p id="c709" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题二:</strong></p><p id="d31e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">首先将示例句子分解成标记，然后应用词性标注，接着进行命名实体识别并返回结果。</p><p id="279b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">回答:</strong></p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="5422" class="ny mx iu nu b be nz oa l ob oc"># Import required packages<br/>from nltk import word_tokenize, pos_tag, ne_chunk<br/><br/># Break down the sample into tokens<br/>tokens = word_tokenize(sample)<br/><br/># Create POS tagging<br/>part_of_speach = pos_tag(tokens)<br/><br/># Create named-entity chunks<br/>named_entity_chunks = ne_chunk(part_of_speach)<br/><br/># Return named_entity_chunks<br/>named_entity_chunks</span></pre><p id="dfc6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pb"><img src="../Images/ca793e117c953a653a82c30b0f195022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vtCxstmo1HeCyvvmp8qEkA.png"/></div></div></figure><p id="fadd" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们看看结果，特别是“亚马逊”和“日本”，因为我们知道这两个是实体。亚马逊被归类为“人”，这是我们算法的改进机会。我更喜欢“公司”或类似的类别。然后“日本”被归类为 GPE，代表一个地缘政治实体。听起来没错！因此，我们观察了 NER 是如何帮助我们将名词进一步细分为实体类的。</p><p id="ab8c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在我们已经学习了如何执行词性标注和 NER，让我们创建一个可以自动实现这些任务的函数。</p><p id="9e96" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题三:</strong></p><p id="7713" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">定义一个名为“make_chunks”的函数，它接受一个句子列表作为参数，默认为问题 1 中定义的“make_sentences”函数，并返回一个字典(将被称为外部字典)，其中外部字典的键是一个整数，表示条目的行号。外部字典的值是字典本身(将被称为内部字典)，其中内部字典的关键字是句子编号，内部字典的值是命名实体识别的结果(类似于问题 2)。例如，回顾问题 1 的结果，结果的第六行是以下句子列表:</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="e17e" class="ny mx iu nu b be nz oa l ob oc">['The rest of the movie lacks art, charm, meaning...',  <br/>"If it's about emptiness, it works I guess because it's empty."],</span></pre><p id="7ac8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">因此，使用默认参数运行问题 3 中定义的函数，预计会为第六行返回以下内容:</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="77d0" class="ny mx iu nu b be nz oa l ob oc">5: {<br/>     0: [('The', 'DT'),<br/>         ('rest', 'NN'),<br/>         ('of', 'IN'),<br/>         ('the', 'DT'),<br/>         ('movie', 'NN'),<br/>         ('lacks', 'VBZ'),<br/>         ('art', 'RB'),<br/>         (',', ','),<br/>         ('charm', 'NN'),<br/>         (',', ','),<br/>         ('meaning', 'NN'),<br/>         ('...', ':')<br/>     ],<br/>     1: [('If', 'IN'),<br/>         ('it', 'PRP'),<br/>         ("'s", 'VBZ'),<br/>         ('about', 'IN'),<br/>         ('emptiness', 'NN'),<br/>         (',', ','),<br/>         ('it', 'PRP'),<br/>         ('works', 'VBZ'),<br/>         ('I', 'PRP'),<br/>         ('guess', 'NN'),<br/>         ('because', 'IN'),<br/>         ('it', 'PRP'),<br/>         ("'s", 'VBZ'),<br/>         ('empty', 'JJ'),<br/>         ('.', '.')<br/>     ]<br/> },</span></pre><p id="4297" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">回答:</strong></p><p id="e908" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了定义这个函数，我们将遍历两个字典，其中内部字典将包括标记化、词性标注和 ner，类似于这个问题之前的例子。</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="828e" class="ny mx iu nu b be nz oa l ob oc"># Define the function<br/>def make_chunks(make_sentences = make_sentences):<br/>    <br/>    # Create the outer dictionary with row number as key<br/>    row_dict = dict()<br/>    <br/>    # Form the first iteration<br/>    for i, row in enumerate(make_sentences()):<br/>        <br/>        # Create the inner dictionary with sentence number as key<br/>        sent_dict = dict()<br/>        <br/>        # Form the second iteration<br/>        for j, sent in enumerate(row):<br/>            <br/>            # Tokenize<br/>            w = word_tokenize(sent)<br/>            <br/>            # POS tagging<br/>            pos = pos_tag(w)<br/>            <br/>            # Add named-entity chunks as the values to the inner dictionary<br/>            sent_dict[j] = list(ne_chunk(pos))<br/>        <br/>        # Add the inner dictionary as values to the outer dictionary<br/>        row_dict[i] = sent_dict<br/>    <br/>    # Return the outer dictionary<br/>    return row_dict<br/><br/># Test on the sixth row of the dataframe<br/>make_chunks()[5]</span></pre><p id="4308" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="c74e" class="ny mx iu nu b be nz oa l ob oc">{0: [('The', 'DT'),<br/>  ('rest', 'NN'),<br/>  ('of', 'IN'),<br/>  ('the', 'DT'),<br/>  ('movie', 'NN'),<br/>  ('lacks', 'VBZ'),<br/>  ('art', 'RB'),<br/>  (',', ','),<br/>  ('charm', 'NN'),<br/>  (',', ','),<br/>  ('meaning', 'NN'),<br/>  ('...', ':')],<br/> 1: [('If', 'IN'),<br/>  ('it', 'PRP'),<br/>  ("'s", 'VBZ'),<br/>  ('about', 'IN'),<br/>  ('emptiness', 'NN'),<br/>  (',', ','),<br/>  ('it', 'PRP'),<br/>  ('works', 'VBZ'),<br/>  ('I', 'PRP'),<br/>  ('guess', 'NN'),<br/>  ('because', 'IN'),<br/>  ('it', 'PRP'),<br/>  ("'s", 'VBZ'),<br/>  ('empty', 'JJ'),<br/>  ('.', '.')]}</span></pre><p id="ad5a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">正如所料，结果与问题中提供的示例相匹配。</p><h2 id="8d85" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">情感分析</h2><p id="7164" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">在自然语言处理领域，情感分析是从文本数据中识别、量化、提取和研究主观信息的工具。在这个练习中，我们将使用极性得分，这是一个在[-1.0，1.0]范围内的浮动值，旨在区分文本是积极还是消极的情绪。这种熟悉程度足以满足这篇文章的目的，但如果你有兴趣了解更多，请参考我关于情绪分析的文章<a class="ae kz" href="https://medium.com/towards-data-science/sentiment-analysis-intro-and-implementation-ddf648f79327" rel="noopener">，链接于此</a>。我们一起来看一个例子。</p><p id="d145" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题 4: </strong></p><p id="ec6f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">创建一个接受句子列表作为参数的函数，默认为问题 1 中定义的“make_sentences”函数，然后返回一个包含“句子”和“情感”两列的 dataframe。请使用 NLTK 的“SentimentIntensityAnalyzer”进行情感分析。最后，使用默认参数运行函数并返回结果。</p><p id="4bf4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">回答:</strong></p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="d692" class="ny mx iu nu b be nz oa l ob oc"># Import the package<br/>from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/><br/># Define the function<br/>def sentiment_analyzer(make_sentences = make_sentences):<br/>    <br/>    # Create an instance of SentimentIntensityAnalyzer<br/>    sia = SentimentIntensityAnalyzer()<br/>    <br/>    # Create the dataframe with two columns as described<br/>    df = {<br/>        'sentence' : []<br/>        , 'sentiment' : []<br/>    }<br/>    <br/>    # Create two loops to add the column vlaues<br/>    for i, row in enumerate(make_sentences()):<br/>        for sent in row:<br/>            <br/>            # Add the sentence to the dataframe<br/>            df['sentence'].append(sent)<br/>            <br/>            # Add the polarity score of the sentiment analysis to the sentiment column of the dataframe<br/>            df['sentiment'].append(sia.polarity_scores(sent)['compound'])<br/>    <br/>    <br/>    # Return the dataframe<br/>    return pd.DataFrame(df)<br/><br/># Run the function<br/>sentiment_analyzer()</span></pre><p id="1727" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pc"><img src="../Images/35e9481f0afab959b446f9a73cdf2c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDbQmaSz83C5jDV9bqKGgA.png"/></div></div></figure><h2 id="860f" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">主题建模——潜在的狄利克雷分配</h2><p id="b5ea" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">潜在狄利克雷分配(LDA)是用于主题建模的常用模型之一。虽然探索 LDA 的数学细节超出了本文的范围，但我们可以将它视为一个将单词与主题和文档联系起来的模型。例如，当一组文档被提供给 LDA 模型时，它将查看单词，并根据每个文档中包含的单词，为每个文档分配具有各自概率的主题。</p><p id="d8df" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">幸运的是，LDA 可以在 scikit-learn 中轻松实现。NLTK 的 LDA 类接受文档术语矩阵(DTM)作为参数，因此，让我们回顾一下什么是 DTM，然后我们将看一个使用 scikit-learn 的 LDA 模型进行主题建模的例子。</p><h2 id="a254" class="oj mx iu bd my ok ol dn nc om on dp ng lj oo op ni ln oq or nk lr os ot nm ou bi translated">文档术语矩阵</h2><p id="5d93" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">DTM 是一个矩阵，表示在文档集合中出现的术语的频率。让我们看两个句子来理解 DTM 是什么。</p><p id="c617" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">假设我们有以下两个句子(在本例中，每个句子都被视为一个“文档”):</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="75bf" class="ny mx iu nu b be nz oa l ob oc">sentence_1 = 'He is walking down the street.'<br/><br/>sentence_2 = 'She walked up then walked down the street yesterday.'</span></pre><p id="87ca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">DTM 的上述两句话将会是:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pd"><img src="../Images/5a6c7da557ff0e10c12f9371c8f58f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oUJL176aVYy0gcPG4Mpiqg.png"/></div></div></figure><p id="3091" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">DTM 可以使用 scikit-learn 的计数矢量器来实现。这将满足我们当前练习的目的，但如果你有兴趣了解更多关于 DTM 的知识，请访问我在情感分析上的帖子，<a class="ae kz" rel="noopener" target="_blank" href="/sentiment-analysis-intro-and-implementation-ddf648f79327">链接于此</a>。</p><p id="2a60" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">让我们把目前所学的东西付诸实践。我们将实施以下步骤:</p><ol class=""><li id="9621" class="pe pf iu lc b ld le lg lh lj pg ln ph lr pi lv pj pk pl pm bi translated">导入 DTM 和 LDA 所需的包并实例化它们</li><li id="43b4" class="pe pf iu lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">为我们的数据框架的“文本”列创建一个 DTM</li><li id="5c0d" class="pe pf iu lc b ld pn lg po lj pp ln pq lr pr lv pj pk pl pm bi translated">使用 LDA 为提供的 DTM 创建主题</li></ol><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="bfd3" class="ny mx iu nu b be nz oa l ob oc"># Step 1 - Import packages<br/>from sklearn.decomposition import LatentDirichletAllocation<br/>from sklearn.feature_extraction.text import CountVectorizer<br/><br/># Create an instance of the imported packages<br/>lda = LatentDirichletAllocation()<br/>cvect = CountVectorizer(stop_words='english')<br/><br/># Step 2 - Create a DTM of 50 randomly-selected rows of the dataframe<br/>dtm = cvect.fit_transform(df['text'])<br/><br/># Step 3 - Create list of topics using LDA<br/>topics = lda.fit_transform(dtm)</span></pre><p id="6466" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">既然我们已经做好了模型，让我们来看看每个话题都包括哪些单词。可以使用<code class="fe ov ow ox nu b">lda.components_</code>查看模型的结果。让我们看一个例子。</p><p id="d2d1" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题 5: </strong></p><p id="799c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">定义一个名为“top_n_words”的函数，它接受两个参数:<br/> 1。“特征名称”，这是从 DTM <br/> 2 产生的特征名称。“n”，这是将要返回的行数和字数。</p><p id="2f1e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">该函数接受上面的两个参数，并返回 n 个主题以及该主题中的前 n 个单词。例如，第一个主题的结果可能如下:</p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="b636" class="ny mx iu nu b be nz oa l ob oc">Topic 0: film ve really end movies just dialogue choked drama worthless</span></pre><p id="2232" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最后，运行函数，返回每个主题的前 10 个单词。</p><p id="08cf" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">回答:</strong></p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="b157" class="ny mx iu nu b be nz oa l ob oc"># Define the function<br/>def top_n_words(feature_names, n): <br/>    <br/>    for topic_index, topic in enumerate(lda.components_):<br/>        <br/>        # Create the "Topic {index}:" portion of the output<br/>        output = "Topic %d: " % topic_index <br/>        <br/>        # Add the top n words of that topic<br/>        output += " ".join([feature_names[i] for i in topic.argsort()[:-n - 1:-1]])<br/>        <br/>        # Print the output<br/>        print(output)<br/>    <br/>    # Print the output<br/>    print()<br/>    <br/># Create function names from the DTM<br/>feature_names = cvect.get_feature_names_out()<br/><br/># Run the function for top 10 words of each topic<br/>top_n_words(feature_names, n=10)</span></pre><p id="0032" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ps"><img src="../Images/43243baa01b0f8d3531bce4bdbf7bb59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*runumdFtFWWmj3QZzyaD3A.png"/></div></div></figure><p id="1465" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">问题 6: </strong></p><p id="6794" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">定义一个函数，它接受两个参数“search_word”和“n ”,并返回与“search_word”中提供的主题相关的前“n”个最可能的单词。结果应该是数据帧的形式，包含两列。第一列是每个单词的“概率”，第二列是“特征”或与提供的主题相关的单词(即“search_word”)。最后，使用“action”作为“search_word”运行该函数，并返回与该主题相关的前 10 个单词。</p><p id="bf5a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">回答:</strong></p><pre class="kk kl km kn gu nt nu nv bn nw nx bi"><span id="b161" class="ny mx iu nu b be nz oa l ob oc"># Define the function<br/>def word_feature(search_word, n=10):<br/>    search_word_cvect = cvect.transform(np.array([search_word])) <br/>    probabilities = lda.transform(search_word_cvect)<br/>    topics = lda.components_[np.argmax(probabilities)]<br/>    features = cvect.get_feature_names_out()<br/>    return pd.DataFrame({'probability': topics, 'feature': features}).nlargest(n, 'probability')<br/><br/># Run the function for the given search_word and return top 10 results<br/>word_feature('action', n=10)</span></pre><p id="1edf" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pt"><img src="../Images/5d8aca3a0777e21224747a97db705100.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*osfu1ejjlujqiTKfPDJTpw.png"/></div></figure></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="67c8" class="mw mx iu bd my mz na nb nc nd ne nf ng ka nh kb ni kd nj ke nk kg nl kh nm nn bi translated">带练习题的笔记本</h1><p id="373c" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">下面是带问题和答案的笔记本，您可以下载并练习。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu pv l"/></div></figure></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="ff32" class="mw mx iu bd my mz na nb nc nd ne nf ng ka nh kb ni kd nj ke nk kg nl kh nm nn bi translated">结论</h1><p id="f008" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">在这篇文章中，我们讨论了如何使用机器学习，特别是主题建模来将一组文档分组在一起，这可以促进企业的下游任务，处理大量以电子邮件、社交媒体帖子、客户评论等形式传入的文本数据。我们从建立所需的直觉和基础开始，涵盖了标记化、词性和命名实体识别，然后是情感分析和主题建模的实现，使用潜在的狄利克雷分配。</p></div><div class="ab cl mp mq hy mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="in io ip iq ir"><h1 id="bfbe" class="mw mx iu bd my mz na nb nc nd ne nf ng ka nh kb ni kd nj ke nk kg nl kh nm nn bi translated">感谢阅读！</h1><p id="a003" class="pw-post-body-paragraph la lb iu lc b ld no jv lf lg np jy li lj nq ll lm ln nr lp lq lr ns lt lu lv in bi translated">如果你觉得这篇文章有帮助，请<a class="ae kz" href="https://medium.com/@fmnobar" rel="noopener">在媒体</a>上关注我，订阅接收我的最新文章！</p></div></div>    
</body>
</html>