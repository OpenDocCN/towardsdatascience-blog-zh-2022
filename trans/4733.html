<html>
<head>
<title>Fine-Tuning OCR-Free Donut Model for Invoice Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于发票识别的微调无OCR甜甜圈模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-ocr-free-donut-model-for-invoice-recognition-46e22dc5cff1#2022-10-21">https://towardsdatascience.com/fine-tuning-ocr-free-donut-model-for-invoice-recognition-46e22dc5cff1#2022-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="564d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并将它的性能与layoutLM进行比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e4d3fb165d2bda03e96837145f842fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iqJIpJ5dIF7O1TA8T3q9A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://elements.envato.com/home-office-desk-workspace-laptop-invoice-and-eyeg-3NKTRFG" rel="noopener ugc nofollow" target="_blank"> Envanto </a></p></figure><h1 id="2049" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8bec" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">智能文档处理(IDP)是自动理解文档内容和结构的能力。对于任何需要处理大量文档的组织来说，这都是一项至关重要的功能，例如客户服务、索赔处理或法规遵从性。然而，国内流离失所者不是一项微不足道的任务。即使对于最常见的文档类型，如发票或简历，现有的各种格式和布局也会使IDP软件很难准确解释内容。</p><p id="f743" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当前的文档理解模型(如layoutLM)通常需要进行OCR处理，以便在处理文档之前从文档中提取文本。虽然OCR是一种从文档中提取文本的有效方法，但它也不是没有挑战。OCR准确性会受到一些因素的影响，如原始文档的质量、使用的字体和文本的清晰度。此外，OCR速度慢且计算量大，这又增加了一层复杂性。这使得很难实现IDP所需的高精度。为了克服这些挑战，需要新的IDP方法来准确地解释文档，而不需要OCR。</p><p id="7310" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">进入Donut，它代表<strong class="lt iu">Do</strong>cume<strong class="lt iu">n</strong>T<strong class="lt iu">U</strong>understanding<strong class="lt iu">T</strong>transformer，这是一款无OCR的变压器模型，根据<a class="ae ky" href="https://arxiv.org/pdf/2111.15664.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>的说法，它在精度方面甚至击败了layoutLM模型。</p><p id="c335" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本教程中，我们将微调新的发票提取Donut模型，并将其性能与最新的layoutLM v3进行比较。我们开始吧！</p><p id="aee0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为参考，下面是用于微调甜甜圈模型的google colab脚本:</p><div class="ms mt gp gr mu mv"><a href="https://colab.research.google.com/drive/16iPnVD68oMnCqxHcLaq9qn9zkRaIGeab?usp=sharing#scrollTo=h072rVoFMNYb" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">谷歌联合实验室</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">编辑描述</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">colab.research.google.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><h1 id="50a0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">环形建筑</h1><p id="2e64" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">那么，该模型如何能够提取文本并理解图像，而不需要任何OCR处理呢？甜甜圈架构是基于一个视觉编码器和一个文本解码器。视觉编码器将视觉特征x∈R H×W×C作为输入输入到一组嵌入{zi |zi∈R d，1≤i≤n}中，其中n是特征图大小或图像块的数量，d是编码器的潜在向量的维数。作者使用Swin变压器作为编码器，因为根据他们的初步研究，它显示出最佳性能。文本解码器是一个BART transformer模型，它将输入特征映射到一系列子词标记中。</p><p id="8f21" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Donut使用<a class="ae ky" rel="noopener" target="_blank" href="/what-is-teacher-forcing-3da6217fed1c">教师强制策略</a>模型，该模型在输入中使用基本事实，而不是输出。该模型基于提示生成一系列令牌，提示取决于我们希望实现的任务类型，如分类、问答和解析。例如，如果我们希望提取文档的类别，我们将把图像嵌入和任务类型一起提供给解码器，模型将输出对应于文档类型的文本序列。如果我们对问答感兴趣，我们将输入问题“choco mochi的价格是多少”，模型将输出答案。输出序列然后被转换成一个JSON文件。更多信息，请参考<a class="ae ky" href="https://arxiv.org/pdf/2111.15664.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/90593a71d965b7fd109d7253d4ddb265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frMAFkpK3zFy-kmB1KJb6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">甜甜圈建筑。<a class="ae ky" href="https://arxiv.org/pdf/2111.15664.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="5a9a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">发票标签</h1><p id="b093" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本教程中，我们将对使用UBIAI文本注释工具标记的220张发票的模型进行微调，类似于我以前关于微调<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf"> layoutLM模型</a>的文章。下面是一个<a class="ae ky" href="https://github.com/walidamamou/example_labelled_invoices.git" rel="noopener ugc nofollow" target="_blank">示例</a>，展示了从UBIAI导出的带标签数据集的格式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/55eff988c3cd9c3a387ced6e4ea20e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SuvyuFevyY94-kWbgrZKoQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:UBIAI OCR注释功能</p></figure><p id="9fea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">UBIAI支持OCR解析、原生PDF/图像注释和以正确格式导出。您可以<a class="ae ky" href="https://ubiai.tools/Docs#modelcreation" rel="noopener ugc nofollow" target="_blank">在UBIAI平台中对layouLM模型</a>进行微调，并使用它自动标记您的数据，这可以节省大量手动注释时间。</p><h1 id="6978" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">微调甜甜圈</h1><p id="004b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一步是导入所需的包，并从Github克隆Donut repo。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="b0ae" class="nr la it nn b gy ns nt l nu nv">from PIL import Image</span><span id="de3a" class="nr la it nn b gy nw nt l nu nv">import torch</span><span id="9f08" class="nr la it nn b gy nw nt l nu nv">!git clone https://github.com/clovaai/donut.git</span><span id="eaad" class="nr la it nn b gy nw nt l nu nv">!cd donut &amp;&amp; pip install .</span><span id="a1dc" class="nr la it nn b gy nw nt l nu nv">from donut import DonutModel</span><span id="d228" class="nr la it nn b gy nw nt l nu nv">import json</span><span id="45e7" class="nr la it nn b gy nw nt l nu nv">import shutil</span></pre><p id="012b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们需要从UBIAI导出的JSON文件中提取标签并解析图像名称。我们将标注数据集的路径和处理过的文件夹(替换为您自己的路径)。</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="04d4" class="nr la it nn b gy ns nt l nu nv">ubiai_data_folder = "/content/drive/MyDrive/Colab Notebooks/UBIAI_dataset"</span><span id="1f0b" class="nr la it nn b gy nw nt l nu nv">ubiai_ocr_results = "/content/drive/MyDrive/Colab Notebooks/UBIAI_dataset/ocr.json"</span><span id="1402" class="nr la it nn b gy nw nt l nu nv">processed_dataset_folder = "/content/drive/MyDrive/Colab Notebooks/UBIAI_dataset/processed_dataset"</span><span id="7916" class="nr la it nn b gy nw nt l nu nv">with open(ubiai_ocr_results) as f:</span><span id="2f4f" class="nr la it nn b gy nw nt l nu nv">  data = json.load(f)</span><span id="fcad" class="nr la it nn b gy nw nt l nu nv">#Extract labels from the JSON file<br/>all_labels = list()<br/>for j in data:<br/>  all_labels += list(j['annotation'][cc]['label'] for cc in range(len(j['annotation'])))</span><span id="8955" class="nr la it nn b gy nw nt l nu nv">all_labels = set(all_labels)<br/>all_labels</span><span id="b29a" class="nr la it nn b gy nw nt l nu nv">#Setup image path<br/>images_metadata = list()<br/>images_path = list()</span><span id="52f2" class="nr la it nn b gy nw nt l nu nv">for obs in data:</span><span id="604d" class="nr la it nn b gy nw nt l nu nv">ground_truth = dict()<br/>  for ann in obs['annotation']:<br/>    if ann['label'].strip() in ['SELLER', 'DATE', 'TTC', 'INVOICE_NUMBERS', 'TVA']:<br/>      ground_truth[ann['label'].strip()] = ann['text'].strip()</span><span id="e48b" class="nr la it nn b gy nw nt l nu nv">try:<br/>    ground_truth = {key : ground_truth[key] for key in ['SELLER', 'DATE', 'TTC', 'INVOICE_NUMBERS', 'TVA']}<br/>  except:<br/>    continue<br/>  <br/>  images_metadata.append({"gt_parse": ground_truth})<br/>  images_path.append(obs['images'][0]['name'].replace(':',''))</span><span id="f75e" class="nr la it nn b gy nw nt l nu nv">dataset_len = len(images_metadata)</span></pre><p id="53bf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将数据分为训练集、测试集和验证集。为此，只需创建三个文件夹:培训、测试和验证。在每个文件夹中创建一个空的metadata.jsonl文件，并运行下面的脚本:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="41ff" class="nr la it nn b gy ns nt l nu nv">for i, gt_parse in enumerate(images_metadata):<br/>  # train<br/>  if i &lt; round(dataset_len*0.8) :<br/>    with open(processed_dataset_folder+"/train/metadata.jsonl", 'a') as f:<br/>      line = {"file_name": images_path[i], "ground_truth": json.dumps(gt_parse, ensure_ascii=False)}<br/>      f.write(json.dumps(line, ensure_ascii=False) + "\n")<br/>      shutil.copyfile(ubiai_data_folder + '/' + images_path[i], processed_dataset_folder + "/train/" + images_path[i])<br/>      if images_path[i] == "050320sasdoodahfev20_2021-09-24_0722.txt_image_0.jpg":<br/>        print('train')<br/>  <br/>  # test<br/>  if round(dataset_len*0.8) &lt;= i &lt; round(dataset_len*0.8) + round(dataset_len*0.1):<br/>    with open(processed_dataset_folder+"/test/metadata.jsonl", 'a') as f:<br/>        line = {"file_name": images_path[i], "ground_truth": json.dumps(gt_parse, ensure_ascii=False)}<br/>        f.write(json.dumps(line, ensure_ascii=False) + "\n")<br/>        shutil.copyfile(ubiai_data_folder + '/' + images_path[i], processed_dataset_folder + "/test/" + images_path[i])<br/>        if images_path[i] == "050320sasdoodahfev20_2021-09-24_0722.txt_image_0.jpg":<br/>          print('test')</span><span id="1413" class="nr la it nn b gy nw nt l nu nv"># validation<br/>  if round(dataset_len*0.8) + round(dataset_len*0.1) &lt;= i &lt; dataset_len:<br/>    with open(processed_dataset_folder+"/validation/metadata.jsonl", 'a') as f:<br/>        line = {"file_name": images_path[i], "ground_truth": json.dumps(gt_parse, ensure_ascii=False)}<br/>        f.write(json.dumps(line, ensure_ascii=False) + "\n")<br/>        shutil.copyfile(ubiai_data_folder + '/' + images_path[i], processed_dataset_folder + "/validation/" + images_path[i])</span></pre><p id="8d4e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该脚本会将我们的原始注释转换成包含图像路径和基本事实的JSON格式:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="b128" class="nr la it nn b gy ns nt l nu nv">{"file_name": "156260522812_2021-10-26_195802.2.txt_image_0.jpg", "ground_truth": "{\"gt_parse\": {\"SELLER\": \"TJF\", \"DATE\": \"création-09/05/2019\", \"TTC\": \"73,50 €\", \"INVOICE_NUMBERS\": \"N° 2019/068\", \"TVA\": \"12,25 €\"}}"}</span><span id="2202" class="nr la it nn b gy nw nt l nu nv">{"file_name": "156275474651_2021-10-26_195807.3.txt_image_0.jpg", "ground_truth": "{\"gt_parse\": {\"SELLER\": \"SAS CALIFRAIS\", \"DATE\": \"20/05/2019\", \"TTC\": \"108.62\", \"INVOICE_NUMBERS\": \"7133\", \"TVA\": \"5.66\"}}"}</span></pre><p id="a228" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，转到“/content/donut/config”文件夹，创建一个名为“train.yaml”的新文件，并复制以下配置内容(确保用您自己的路径替换数据集路径):</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="4dd2" class="nr la it nn b gy ns nt l nu nv">result_path: "/content/drive/MyDrive/Colab Notebooks/UBIAI_dataset/processed_dataset/result"<br/>pretrained_model_name_or_path: "naver-clova-ix/donut-base" # loading a pre-trained model (from moldehub or path)<br/>dataset_name_or_paths: ["/content/drive/MyDrive/Colab Notebooks/UBIAI_dataset/processed_dataset"] # loading datasets (from moldehub or path)<br/>sort_json_key: False # cord dataset is preprocessed, and publicly available at <a class="ae ky" href="https://huggingface.co/datasets/naver-clova-ix/cord-v2" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/datasets/naver-clova-ix/cord-v2</a><br/>train_batch_sizes: [1]<br/>val_batch_sizes: [1]<br/>input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)<br/>max_length: 768<br/>align_long_axis: False<br/>num_nodes: 1<br/>seed: 2022<br/>lr: 3e-5<br/>warmup_steps: 300 # 800/8*30/10, 10%<br/>num_training_samples_per_epoch: 800<br/>max_epochs: 50<br/>max_steps: -1<br/>num_workers: 8<br/>val_check_interval: 1.0<br/>check_val_every_n_epoch: 3<br/>gradient_clip_val: 1.0<br/>verbose: True</span></pre><p id="b94d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">请注意，您可以根据自己的用例更新超参数。</p><p id="9ba9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们终于准备好训练模型了，只需运行下面的命令:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="6d4d" class="nr la it nn b gy ns nt l nu nv">!cd donut &amp;&amp; python train.py --config config/train.yaml</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/e661416cb98fe062ecb111336a6d396b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZ18omSsGOAU27CEtZxLfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:甜甜圈模型训练</p></figure><p id="5e4f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用启用了GPU的google colab进行模型培训大约需要1.5小时。</p><p id="9bca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了获得模型性能，我们在测试数据集上测试模型，并将其预测与实际情况进行比较:</p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="918a" class="nr la it nn b gy ns nt l nu nv">import glob<br/>with open('/content/drive/MyDrive/Invoice dataset/UBIAI_dataset/processed_dataset/test/metadata.jsonl') as f:<br/>  result = [json.loads(jline) for jline in f.read().splitlines()]<br/>test_images = glob.glob(processed_dataset_folder+'/test/*.jpg')</span><span id="35bb" class="nr la it nn b gy nw nt l nu nv">acc_dict = {'SELLER' : 0, 'DATE' : 0, 'TTC' : 0, 'INVOICE_NUMBERS' : 0, 'TVA' : 0}</span><span id="b9f4" class="nr la it nn b gy nw nt l nu nv">for path in test_images:<br/>  image = Image.open(path).convert("RGB")<br/>  <br/>  donut_result = my_model.inference(image=image, prompt="&lt;s_ubiai-donut&gt;")<br/>  returned_labels = donut_result['predictions'][0].keys()</span><span id="e1e2" class="nr la it nn b gy nw nt l nu nv">for i in result:<br/>    if i['file_name'] == path[path.index('/test/')+6:]:<br/>      truth = json.loads(i['ground_truth'])['gt_parse']<br/>      break</span><span id="c5f6" class="nr la it nn b gy nw nt l nu nv">for l in [x for x in returned_labels if x in ['SELLER', 'DATE', 'TTC', 'INVOICE_NUMBERS', 'TVA']]:<br/>    if donut_result['predictions'][0][l] == truth[l]:<br/>      acc_dict[l] +=1</span></pre><p id="5988" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是每个实体的得分:</p><p id="b261" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">卖家:0%，日期:47%，TTC: 74%，发票号码:53%，电视广告:63%</p><p id="c9e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管有足够多的例子(274)，卖方实体的得分为0。其余实体得分较高，但仍在较低范围内。现在，让我们尝试对不属于训练数据集的新发票运行该模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/8005bca43defff92c4c8e407a35c3702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7b6AT1_XPT6cipkmLpSJ7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:测试发票</p></figure><p id="f11f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">模型预测是:</p><p id="fd94" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">日期:“2017年1月31日”，</p><p id="c431" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TTC ':' 1455.00美元'，</p><p id="2e7f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">INVOICE_NUMBERS ':'发票'，</p><p id="023e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TVA ':' 35.00美元'</p><p id="4dc3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该模型在提取卖家名称和发票号码时遇到了问题，但它正确地识别出了总价(TTC)、日期，并错误地标注了税款(TVA)。虽然模型的性能相对较低，但我们可以尝试一些超参数调整来增强它和/或标记更多的数据。</p><h1 id="a3da" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">甜甜圈vs布局LM</h1><p id="6c63" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">圆环模型相对于其对应部分布局有几个优点，例如较低的计算成本、较低的处理时间和较少的由OCR引起的错误。但是性能对比如何？根据原始论文，Donut模型在CORD数据集上的性能优于layoutLM。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/c22f3e5c5d4c2abef5431d226b13f714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJScdn2Qz4uLViGYCCNyxw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型性能得分比较</p></figure><p id="8e6f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，当使用我们自己的标记数据集时，我们没有注意到性能的提高。如果说有什么不同的话，LayoutLM已经能够<a class="ae ky" rel="noopener" target="_blank" href="/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf">捕获更多的实体</a>，比如卖家姓名和发票号码。这种差异可能是因为我们没有进行任何超参数调整。或者，Donut可能需要更多的标记数据来实现良好的性能。</p><h1 id="abf2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="2c18" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本教程中，我们关注的是数据提取，但是Donut模型能够进行文档分类、文档问题回答和综合数据生成，所以我们只是触及了表面。无OCR模型具有许多优点，例如更高的处理速度、更低的复杂性、更少的由低质量OCR引起的错误传播。</p><p id="c748" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一步，我们可以通过执行超参数调整和标注更多数据来提高模型性能。</p><p id="418c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您有兴趣标记自己的训练数据集，请不要犹豫，免费尝试UBIAI OCR注释功能<a class="ae ky" href="https://ubiai.tools/Signup" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="d403" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在推特上关注我们<a class="ae ky" href="https://twitter.com/UBIAI5" rel="noopener ugc nofollow" target="_blank"> @UBIAI5 </a>或<a class="ae ky" href="https://walidamamou.medium.com/subscribe" rel="noopener">订阅这里</a>！</p></div></div>    
</body>
</html>