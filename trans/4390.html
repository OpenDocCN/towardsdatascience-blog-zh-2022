<html>
<head>
<title>CUDA by Numba Examples: Atomics and Mutexes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Numba的CUDA示例:原子和互斥</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cuda-by-numba-examples-c583474124b0#2022-09-28">https://towardsdatascience.com/cuda-by-numba-examples-c583474124b0#2022-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7042" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">本系列的第4部分总结了使用Python从头开始学习CUDA编程的旅程</h2></div><h1 id="8f29" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="c278" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的前三期中(<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-1-4-e0d06651612f">第1部分这里</a>、<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-215c0d285088">第2部分这里</a>和第3部分)，我们已经了解了CUDA开发的大部分基础知识，例如启动内核来执行令人尴尬的并行任务、利用共享内存来执行快速缩减、将可重用逻辑封装为设备功能，以及如何使用事件和流来组织和控制内核执行。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/921c1853ed005792e29f6555d3dfd71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKaQL6AiJxOPe9Cgmt6-gg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3.0。运行<a class="ae lt" href="https://replicate.com/stability-ai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>以“大历风格的原子心妈妈专辑封面”。学分:在CreativeML Open RAIL-M许可下拥有作品。</p></figure><h1 id="8ad7" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">在本教程中</h1><p id="ce26" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的最后一部分中，我们将讨论原子指令，它将允许我们从多个线程安全地操作同一个内存。我们还将学习如何利用这些操作来创建一个<a class="ae lt" href="https://en.wikipedia.org/wiki/Lock_(computer_science)" rel="noopener ugc nofollow" target="_blank"> <em class="mk">互斥体</em> </a>，这是一种允许我们“锁定”某个资源的编码模式，因此它一次只能被一个线程使用。</p><p id="322e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><a class="ae lt" href="https://colab.research.google.com/drive/1Eq1Xyuq8hJ440ma_9OBrEVdGm3bIyigt?usp=sharing" rel="noopener ugc nofollow" target="_blank">点击此处获取Google colab中的代码。</a></p><h1 id="731a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">入门指南</h1><p id="6d00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">导入和加载库，确保你有一个GPU。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="6694" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">原子学</h1><p id="f9bc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">GPU编程完全基于尽可能并行化相同的指令。对于许多“令人尴尬的并行”任务，线程不需要协作或使用其他线程使用的资源。其他模式，比如reductions，通过算法的设计来确保相同的资源只被线程的子集使用。在这些情况下，我们通过使用<code class="fe ms mt mu mv b">syncthreads</code>来确保所有其他线程保持最新。</p><p id="c158" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在某些情况下，许多线程必须读写同一个数组。当试图同时执行读取或写入时，这可能会导致问题。假设我们有一个将单个值递增1的内核。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="88ae" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">当我们用一个线程块启动这个内核时，我们将在输入数组中获得一个值1。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="f30e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">现在，当我们启动10个16线程的块时会发生什么？我们将10 × 16 × 1的总数加到同一个存储元素上，因此我们应该希望得到存储在<code class="fe ms mt mu mv b">dev_val</code>中的值160。对吗？</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="4713" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">事实上，我们不太可能在<code class="fe ms mt mu mv b">dev_val</code>达到160。为什么？因为线程是同时读写同一个内存变量的！</p><p id="9775" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">下面是当四个线程试图从同一个全局内存中读取和写入时可能发生的情况的示意图。线程1–3在不同的时间(分别为t=0、2、2)从全局寄存器读取相同的值0。它们都递增1，并在t=4、7和8时写回全局内存。在t=5时，线程4的启动比其他线程稍晚一些。此时，线程1已经写入全局内存，因此线程4读取1的值。它最终会在t=12时将全局变量改写为2。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mw"><img src="../Images/8c47df088319fd6ce3b2b0ba5ad55ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhgO-McHWkorkrDGELHF0g.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图4.1。几个线程试图从同一个全局内存中读写可能会导致<a class="ae lt" href="https://en.wikipedia.org/wiki/Race_condition#Computing" rel="noopener ugc nofollow" target="_blank">竞争情况</a>。学分:自己的工作。</p></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mx"><img src="../Images/736bcd375312e841320a253b7b396e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkE70sAHWKKcX08zc41c-g.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图4.2。当线程对内容进行操作时，资源被锁定读/写，我们确保每个线程在读取时获得更新的值，并且它的写入被其他线程看到。原子操作通常较慢。学分:自己的作品。</p></figure><p id="94af" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">如果我们想得到我们最初期望的结果(如图4.2所示)，我们应该用非原子加法运算代替原子运算。原子操作将确保一次由一个单独的线程完成对任何内存的读/写。让我们多谈谈他们。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><h1 id="7b26" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">原子加法:计算直方图</h1><p id="6079" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了更好地理解在哪里以及如何使用原子，我们将使用直方图计算。假设一个人想计算字母表中的每个字母在某个文本中有多少个。实现这一点的简单算法是创建26个“桶”，每个桶对应于英语字母表中的一个字母。然后，我们将遍历文本中的字母，每当我们遇到“a”时，我们将第一个桶递增1，每当我们遇到“b”时，我们将第二个桶递增1，以此类推。</p><p id="89c3" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在标准Python中，这些“桶”可以是字典，每个都将一个字母链接到一个数字。由于我们喜欢在数组上操作GPU编程，我们就用数组来代替。我们将存储所有128个<a class="ae lt" href="https://en.wikipedia.org/wiki/ASCII" rel="noopener ugc nofollow" target="_blank"> ASCII </a>字符，而不是存储26个字母。</p><p id="9f6b" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在此之前，我们需要将字符串转换为“数字”数组。在这种情况下，将UTF-8字符串转换为<code class="fe ms mt mu mv b">uint8</code>数据类型是有意义的。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="8e4e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">请注意，小写和大写字母有不同的代码。因此，我们将使用几个实用函数来只选择小写字母或大写字母。</p><p id="2f56" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">此外，Numpy已经提供了一个直方图函数，我们将使用它来验证我们的结果并比较运行时间。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi my"><img src="../Images/557e799af161a3d3092e07a09365b967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*19HwTlYrspUELml37MJeSw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">“Numba示例”的CUDA直方图。学分:自己的工作。</p></figure><p id="a69c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">让我们编写自己的CPU版本的函数来理解其中的机制。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="06a1" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">由于每个ASCII字符都映射到128元素数组中的一个bin，所以我们需要做的就是找到它的bin并递增1，只要该bin在0和127(包括)之间。</p><p id="c5f8" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们已经准备好了我们的第一个GPU版本。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="3a7e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">酷！所以至少我们的功能在起作用。内核非常简单，与串行版本具有相同的结构。它从标准的1D网格步长循环结构开始，与串行版本不同，它使用原子加法。Numba中的原子add有三个参数:将被递增的数组(<code class="fe ms mt mu mv b">histo</code>)、将看到incremenet的数组位置(<code class="fe ms mt mu mv b">arr[iarr]</code>，它相当于串行版本中的<code class="fe ms mt mu mv b">char</code>)，最后是<code class="fe ms mt mu mv b">histo[arr[iarr]]</code>将被递增的值(即本例中的1)。</p><p id="28be" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">现在，让我们加大赌注，将其应用于更大的数据集。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="3843" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们将处理大约570万个字符。让我们运行并记录到目前为止的三个版本。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7e26ed28891637758e4affc9060919e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*4wRZhluoKZkO6aF_HTNtXg.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">学分:自己的工作。</p></figure><p id="67f5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">以我们的GPU版本为基准，我们看到NumPy版本至少慢40倍，而我们的CPU版本慢数千倍。我们可以在几毫秒内处理这个570万字符的数据集，而我们简单的CPU解决方案需要10秒以上。这意味着我们有可能在几秒钟内处理200亿个字符的数据集(如果我们有一个超过20 Gb RAM的GPU)，而在我们最慢的版本中，这将需要一个多小时。所以我们已经做得很好了！</p><p id="cf28" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们能改进它吗？好了，让我们再来看看这个内核的内存访问模式。</p><pre class="lv lw lx ly gt na mv nb nc aw nd bi"><span id="fc7f" class="ne kg iq mv b gy nf ng l nh ni">...<br/>for iarr in range(i, arr.size, threads_per_grid):<br/>    if arr[iarr] &lt; 128:<br/>        cuda.atomic.add(histo, arr[iarr], 1)</span></pre><p id="d3d5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><code class="fe ms mt mu mv b">histo</code>是一个位于GPU全局内存中的128元素数组。在任一点启动的每个线程都试图访问该数组的某个元素(即元素<code class="fe ms mt mu mv b">arr[iarr]</code>)。因此，在任何时候，我们都有大约<code class="fe ms mt mu mv b">threads_per_block * blocks_per_grid</code> = 128 × 32 × 80 = 327，680个线程试图访问128个元素。因此，我们平均有大约32 × 80 = 2，560个线程竞争同一个全局内存地址。</p><p id="2238" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了减轻这种情况，我们在共享存储器阵列中计算局部直方图。这是因为</p><ol class=""><li id="38c0" class="nj nk iq kz b la ml ld mm lg nl lk nm lo nn ls no np nq nr bi translated">共享阵列位于芯片上，因此读/写速度更快</li><li id="f59c" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls no np nq nr bi translated">共享数组对于每个线程块来说都是本地的，因此较少的线程可以访问并因此竞争它的资源。</li></ol><p id="3c77" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated"><strong class="kz ir"> <em class="mk"> INFO: </em> </strong> <em class="mk">我们的计算假设字符是均匀分布的。要小心，像自然数据集这样的假设可能不符合它们。例如，自然语言文本中的大多数字符都是小写字母，而不是平均2560个线程竞争，我们将有128×32×80÷26≈12603个线程竞争，这就有更多的问题了！</em></p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="2b54" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">之前我们有2，560个线程争用同一个内存，现在我们有2，560 ÷ 128 = 20个线程。在核的末尾，我们需要对所有的局部结果求和。由于有32 × 80 = 2，560个块，这意味着有2，560个块在争用全局内存。然而，我们确保每个线程只做一次，而以前我们必须这样做，直到我们用尽了输入数组的所有元素。</p><p id="ce5e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">让我们看看这个新版本与以前的版本相比如何！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/796fc150215267bdd9d75d67253c7907.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*oLjKcCCFejnBCXB6LIjl2Q.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">学分:自己的工作。</p></figure><p id="6ae5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">因此，这比原始版本提高了约3倍！</p><p id="e4ef" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们将块的数量设置为32×SMs数量的倍数，就像上一个教程中建议的那样。但是哪个倍数呢？让我们计时吧！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8ce2a33290aae329509a5fcf03d85c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*96CZMHi53R-EPsKk8S1ZOw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">学分:自己的工作。</p></figure><p id="b3d4" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">两件事:首先，我们需要两个轴来显示数据，因为原始版本(蓝色)要慢得多。第二，竖线显示对于某个功能有多少条短信是最佳的。最后，尽管简单版本不会因为添加了更多的块而变得更差，但共享版本却不是这样。要理解这是为什么，请记住共享数组版本有两个部分</p><ul class=""><li id="f810" class="nj nk iq kz b la ml ld mm lg nl lk nm lo nn ls nz np nq nr bi translated">第一部分很少有线程竞争相同的(快速)内存(共享数组部分)。</li><li id="23cf" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls nz np nq nr bi translated">第二部分是许多线程竞争同一个(慢)内存(最后的原子加法)。</li></ul><p id="e861" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">随着更多块的添加，在简单版本中，它很快就遇到了瓶颈，而在共享阵列版本中，竞争在第一部分保持不变，但在第二部分<em class="mk">增加了</em>。另一方面，太少的块不会产生足够的并行化(对于任一版本)。上图找到了这两个极端之间的“最佳点”。</p><h1 id="5940" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">用互斥锁锁定资源</h1><p id="57ae" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在前面的例子中，我们使用了带有整数值的原子加法操作来锁定某些资源，并确保一次只有一个线程控制它们。加法不是唯一的原子操作，它不需要应用于整数值。Numba CUDA支持对整数和浮点数的各种原子操作。但是曾几何时(CUDA compute 1.x)，浮点原子是不存在的。因此，如果我们想用原子为浮点写一个归约，我们就需要另一个结构。</p><p id="860d" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">虽然现在原子确实支持浮点，但允许我们应用任意原子操作的“互斥”代码模式在某些情况下仍然有用。</p><p id="cd17" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">互斥，也就是互斥，是一种向试图访问它的其他线程发出某种资源可用或不可用的信号的方式。互斥体可以用一个变量来创建，这个变量有两个值:</p><ul class=""><li id="8d7d" class="nj nk iq kz b la ml ld mm lg nl lk nm lo nn ls nz np nq nr bi translated"><strong class="kz ir"> 0 </strong> : 🟢绿灯，继续使用某个内存/资源</li><li id="d6a0" class="nj nk iq kz b la ns ld nt lg nu lk nv lo nw ls nz np nq nr bi translated"><strong class="kz ir"> 1 </strong>:🔴红灯，停止，不要试图使用/访问某个内存/资源</li></ul><p id="ce88" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">要锁定内存，应该向互斥体写入1，要解锁，应该写入0。但是需要注意的是，如果有人(自动地)写互斥体，其他线程可能正在访问该资源，至少会产生错误的值，甚至更糟，会产生死锁。另一个问题是互斥体只有在之前没有被锁定的情况下才能被锁定。因此，在写入1(锁定)之前，我们需要读取互斥体并确保它是0(未锁定)。CUDA提供了一种特殊的操作来以原子方式完成这两件事:atomicCAS。在Numba CUDA中，它的名称更加明确:</p><pre class="lv lw lx ly gt na mv nb nc aw nd bi"><span id="20f0" class="ne kg iq mv b gy nf ng l nh ni">cuda.atomic.compare_and_swap(array, old, val)</span></pre><p id="bd75" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">如果<code class="fe ms mt mu mv b">array[0]</code>处的当前值等于<code class="fe ms mt mu mv b">old</code>(这是“比较”部分)，该函数只会自动将<code class="fe ms mt mu mv b">val</code>分配给<code class="fe ms mt mu mv b">array[0]</code>(这是“交换”部分)；否则它现在将交换。此外，它自动返回<code class="fe ms mt mu mv b">array[0]</code>的当前值。为了锁定一个互斥体，我们可以用</p><pre class="lv lw lx ly gt na mv nb nc aw nd bi"><span id="ea28" class="ne kg iq mv b gy nf ng l nh ni">cuda.atomic.compare_and_swap(mutex, 0, 1)</span></pre><p id="b737" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">由此我们将仅在锁被解锁(0)时分配锁(1)。上面这行代码的一个问题是，如果线程到达它并读取1 (locked ),它就会继续执行，这可能不是我们想要的。理想情况下，我们希望线程停止前进，直到我们可以锁定互斥体。因此，我们采取以下措施:</p><pre class="lv lw lx ly gt na mv nb nc aw nd bi"><span id="7aa5" class="ne kg iq mv b gy nf ng l nh ni">while cuda.atomic.compare_and_swap(mutex, 0, 1) != 0:<br/>    pass</span></pre><p id="0e14" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在这种情况下，线程将一直存在，直到它可以正确地锁定线程。假设线程到达了一个先前锁定的互斥体。它的当前值是1。所以我们首先注意到从<code class="fe ms mt mu mv b">curr = 1 != old = 0</code>开始<code class="fe ms mt mu mv b">compare_and_swap</code>将<em class="mk">而不是</em>能够锁定它。它也不会退出<code class="fe ms mt mu mv b">while</code>，因为当前值1不同于0(<code class="fe ms mt mu mv b">while</code>条件)。它将保持在这个循环中，直到最终能够读取当前值为0的未锁定互斥体。在这种情况下，从<code class="fe ms mt mu mv b">curr = 0 == old = 0</code>开始，它也能够将1分配给互斥体。</p><p id="0b72" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">要解锁，我们只需要自动给互斥体赋值一个0。我们将使用</p><pre class="lv lw lx ly gt na mv nb nc aw nd bi"><span id="9ee3" class="ne kg iq mv b gy nf ng l nh ni">cuda.atomic.exch(array, idx, val)</span></pre><p id="5aef" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">它简单地自动赋值<code class="fe ms mt mu mv b">array[idx] = val</code>，返回旧值<code class="fe ms mt mu mv b">array[idx]</code>(自动加载)。因为我们不会使用这个函数的返回值，在这种情况下，你可以把它看作一个原子赋值(也就是说，鉴于<code class="fe ms mt mu mv b">atomic_add(array, idx, val)</code>对<code class="fe ms mt mu mv b">array[idx] += val</code>的赋值就像<code class="fe ms mt mu mv b">exch(array, idx, val)</code>对<code class="fe ms mt mu mv b">array[idx] = val</code>的赋值)。</p><p id="78c6" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">现在我们有了锁定和解锁机制，让我们重试原子的“添加一个”,但是使用互斥体代替。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="d1ca" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">上面的代码相当简单，我们有一个内核，它锁定线程的执行，直到它们自己可以得到一个解锁的互斥体。此时，他们将更新<code class="fe ms mt mu mv b">x[0]</code>的值并解锁互斥体。在任何时候<code class="fe ms mt mu mv b">x[0]</code>都不会被一个以上的线程读取或写入，这实现了原子性！</p><p id="2f19" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">上面的代码中只有一个细节我们没有涉及，那就是<code class="fe ms mt mu mv b">cuda.threadfence()</code>的使用。这个例子并不要求这样，但是要确保锁定和解锁机制的正确性。我们很快就会知道为什么了！</p><h1 id="fe7d" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">互斥点积</h1><p id="4947" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的第2部分<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-215c0d285088">中，我们学习了如何在GPU中应用缩减。我们用它们来计算一个数组的和。我们代码的一个不优雅的方面是我们把一些求和留给了CPU。我们当时缺乏的是应用原子操作的能力。</a></p><p id="abb7" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们将这个例子重新解释为点积，但是这一次将求和进行到底。这意味着我们不会返回“部分”点积，而是通过使用互斥体在GPU中使用原子求和。让我们首先将reduce重新解释为一个点积:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="644f" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">一切都检查过了！</p><p id="4b07" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在我们结束之前，我答应我们会再来一次。来自CUDA“圣经”(<a class="ae lt" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions" rel="noopener ugc nofollow" target="_blank"> B.5 .内存栅栏函数</a> ): <code class="fe ms mt mu mv b">__threadfence()</code> <em class="mk">确保在调用</em> <code class="fe ms mt mu mv b">__threadfence()</code> <em class="mk">之后，调用线程对所有内存的写操作不会被设备中的任何线程观察到，因为在调用</em> <code class="fe ms mt mu mv b">__threadfence()</code> <em class="mk">之前，调用线程对所有内存的任何写操作都会发生。</em></p><p id="6f57" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">如果我们在解锁互斥锁之前忽略线程防护，即使使用原子操作，我们也可能会读取陈旧的信息<em class="mk">，因为内存可能还没有被其他线程写入。同样，在解锁之前，我们必须确保更新内存引用。所有这些都不是显而易见的，而且是在许多年前在<a class="ae lt" href="https://johnwickerson.github.io/papers/gpuconcurrency.pdf" rel="noopener ugc nofollow" target="_blank">阿尔格拉夫等人2015 </a>首次提出的。最终，这个补丁在CUDA的勘误表中发布了，这个例子启发了这一系列的教程。</em></p><h1 id="9046" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="2ad3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的最后一个教程中，您学习了如何使用<em class="mk">原子</em>操作，这是协调线程的一个基本要素。您还学习了<em class="mk">互斥</em>模式，它利用原子来创建自定义区域，一次只有一个线程可以访问这些区域。</p><h1 id="6452" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">收场白</h1><p id="0caa" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的四个部分中，我们已经介绍了足够多的内容，可以让您在各种常见的情况下使用Numba CUDA。这些教程并不详尽，它们旨在介绍并激发读者对CUDA编程的兴趣。</p><p id="5327" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们还没有涉及的一些主题有:动态并行性(让内核启动内核)、复杂同步(例如warp-level、协作组)、复杂内存防护(我们在上面提到过)、多GPU、纹理和许多其他主题。其中一些目前还不被Numba CUDA支持(从0.56版本开始)，其中一些对于入门教程来说被认为是太高级的技术。</p><p id="096e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">为了进一步提高你的CUDA技能，强烈推荐<a class="ae lt" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" rel="noopener ugc nofollow" target="_blank"> CUDA C++编程指南</a>，以及<a class="ae lt" href="https://developer.nvidia.com/blog/?tags=cuda&amp;categories=" rel="noopener ugc nofollow" target="_blank"> Nvidia博客文章</a>。</p><p id="20eb" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">在Python生态系统中，重要的是要强调Numba之外的许多解决方案可以提升GPU。而且它们大多是互操作的，所以不需要只选择一个。<a class="ae lt" href="https://documen.tician.de/pycuda/" rel="noopener ugc nofollow" target="_blank"> PyCUDA </a>、<a class="ae lt" href="https://nvidia.github.io/cuda-python/" rel="noopener ugc nofollow" target="_blank"> CUDA Python </a>、<a class="ae lt" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> RAPIDS </a>、<a class="ae lt" href="https://github.com/ozen/PyOptiX" rel="noopener ugc nofollow" target="_blank"> PyOptix </a>、<a class="ae lt" href="https://cupy.dev/" rel="noopener ugc nofollow" target="_blank"> CuPy </a>和<a class="ae lt" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>是正在积极开发中的库的例子。</p></div></div>    
</body>
</html>