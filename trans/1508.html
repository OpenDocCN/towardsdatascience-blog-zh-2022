<html>
<head>
<title>Neural Feature Importance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经特征重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-feature-importance-1c1868a4bf53#2022-04-13">https://towardsdatascience.com/neural-feature-importance-1c1868a4bf53#2022-04-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d24a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于人工神经网络权重的特征重要性(加上实际代码)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4f9b16fd53c5b0d7e6480fbba791d091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KIpszJYkho5lYrkm"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@6690img?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> J K </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8436" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">你有什么好处？<br/> </strong>在本文中，我们将探索一种基于人工神经网络(ANN)的权重以及实际操作的python代码来计算相对特征重要性的方法。我们开始吧</p><h1 id="e01e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">安</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/52ba8bfa89fe6fb4a93de884748dc5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yX0fzCeYVezqXgvM4k5yfA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e5ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图显示了一个具有单个隐藏层的人工神经网络，该隐藏层中只有一个神经元。此外，为了更好地理解，我将隐藏的神经元分解成多个部分。如您所见，我们有3个功能(I1、I2和I3)。神经特征重要性完全是从神经元的权重中推导出特征重要性，所以让我们深入到上述超级简单神经网络的矩阵乘法形式中来看看实际的权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/fe77bec113a11f8d8a89db090f67b5c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*YQ-Pg5W2HKPGyNISoSr7xw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0cc2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将对数据集中的每一行计算上述等式。这里alpha(图中A1)代表我们的激活函数(可以是relu、tanh、softplus等)。<br/>如你所见，W11、W12和W13分别代表神经元H1中与特征I1、I2和I3相关联的权重。将这两个形状矩阵(1×3)和(3×1)相乘，我们将得到一个形状矩阵(1×1 ),我们的偏差(B1)将被添加到该矩阵中。结果将作为输入传递给我们的激活函数，其结果将被转发给后续层。</p><p id="892d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们在隐藏层中有两个神经元，矩阵乘法将如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/765fcef937b1e9fd512db067d00ecea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*iA7s82p5fSHNZXjmU8osew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e01b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，输入X权重的结果将产生一个形状为<br/> (2 x 1)的矩阵，可以向其中添加偏差。应当注意，激活函数的结果也将是形状矩阵(2×1)，即每个神经元一个值</p><h1 id="6e81" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征重要性</h1><p id="b2fb" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">相信我，我们越来越重视了，耐心点。让我们评估一下我们的单神经元案例，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1045bf279316ead51069fde0f5588f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*9aKZ953PzhJAUS00y1wINw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><blockquote class="mt mu mv"><p id="3e22" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated">如果W11是0.6，W12是0.3，那么这意味着，I1的变化将比I2的变化产生两倍的影响(因为0.6 = 0.3 x 2)</p></blockquote><p id="51d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重可以是正的，也可以是负的，但这只会影响输出变化的方向，而不会影响大小。因此我们可以说，如果abs(W11) &gt; abs(W12)，那么根据神经网络权重，输入1 (I1)比输入2 (I2)更重要。有意思？让我们通过编码来尝试一下。</p><h1 id="3e47" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">把这个编码出来</h1><p id="e5c0" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf mp lh li lj mq ll lm ln mr lp lq lr ij bi translated">注意:我将用斜体提到<em class="mw">列名</em></p><h2 id="e597" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">导入和加载数据</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/5d98b2f09ace596e3561b71526c0f913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NrophKpSADewz6En1Nt_cA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从导入所需的库开始。除了标准的<code class="fe nn no np nq b"><a class="ae kv" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank">numpy</a></code>和<code class="fe nn no np nq b"><a class="ae kv" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">pandas</a></code>，我们还导入了<code class="fe nn no np nq b"><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code>，它将帮助我们找到模型的最佳超参数，<code class="fe nn no np nq b"><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" rel="noopener ugc nofollow" target="_blank">MLPClassifier</a></code>(多层感知器分类器)，用于定义我们的ANN和<code class="fe nn no np nq b"><a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html" rel="noopener ugc nofollow" target="_blank">MinMaxScaler</a></code>，用于将我们的数据缩放到[0，1]范围。<code class="fe nn no np nq b">MLP<strong class="ky ir">Classifier</strong></code>之所以被导入，是因为这个例子围绕着在分类问题的上下文中寻找特征重要性。</p><p id="4d43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，我们不需要<code class="fe nn no np nq b"><a class="ae kv" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">tensorflow</a></code> / <code class="fe nn no np nq b"><a class="ae kv" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">keras</a></code>或<code class="fe nn no np nq b"><a class="ae kv" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">pytorch</a></code>，因为<code class="fe nn no np nq b">MLPClassifier</code>已经有了我们所需要的。我们可以指定隐藏层的大小、隐藏层的一个通用激活函数以及优化器和学习速率。对于二进制分类任务，它将使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>激活功能，而对于多分类任务，它将使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax </a>。</p><p id="7a0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">指定导入后，我们定义一个简单的加载数据函数。正如您将在剩余代码中看到的，我非常喜欢在python中使用<a class="ae kv" href="https://docs.python.org/3/library/typing.html" rel="noopener ugc nofollow" target="_blank">类型提示</a>。</p><h2 id="964c" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">定义常数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/65af8c530b7468ef9710b6a71d52f755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e16dr1PNyEg4dAHblb9HBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7f5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们跳到一些数据处理，定义变量和常量以避免重复和给文字命名总是有帮助的。我们指定分类列、数字列、要删除的列(<em class="mw"> PassengerId </em>因为它是一个Id，而<em class="mw"> Name </em>因为对名称进行编码没有意义)、目标列以及一个常量<code class="fe nn no np nq b">OHE_PREFIX</code>，当我们执行一次性编码时将使用这个常量。</p><p id="1098" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想在这里澄清两件事，第一件是我没有决定放弃<em class="mw">票</em>号的原因。做一些简单的分析，就可以发现<em class="mw">票</em>号首先不是唯一的。此外，如果您找出每张票的频率(出现次数)以及目标列中1的相应百分比，您将会看到<em class="mw">票</em>号确实在目标列的预测中发挥了作用</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4554f14df186b562a5279e619642568c.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*8k6oMYxN-W6oHOk3WbTlPg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c4d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要澄清的第二点是，尽管<em class="mw"> Pclass </em>被表示为整数(1，2，3 ),但是因为它表示第一、第二和第三类，所以它实际上是一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Ordinal_data" rel="noopener ugc nofollow" target="_blank">顺序分类</a>列，因此我将其视为分类列，而不是数字列。</p><h2 id="7448" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">输入空值</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/ba58bea5ff4d67a026de8bc63d5ff0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVJ9hRmj14euw8cBs5xIkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4abe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们定义一个函数来估算缺失值。正如你所看到的，这个函数获取了一个pandas数据帧，并返回一个pandas数据帧，只需查看函数签名(不需要查看函数体)就可以推断出来，这就是键入提示的妙处。关于它们，我们用中值填充数字列中的空值，对于分类列，我们用最频繁(非空)的值填充它们。</p><h2 id="2eff" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">缩放数字</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/6b68bdbc791cad663eae84185d68cec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-8eFe7IkBMWB9ezESCY2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ea6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了根据我们的数据训练神经网络，确保所有特征都在相同的尺度上是很重要的。此外，较小的值有助于更快地达到全局最小值。通常，数据被缩放到[0，1]范围或[-1，1]范围。这里我们使用sklearn的<code class="fe nn no np nq b">MinMaxScaler</code>,它默认将输入数据缩放到[0，1]范围。因为我们不能缩放类别，所以我们传递数字列并只缩放它们。这里的num_cols听起来像“列数”,但是提供的输入提示表明它是一个字符串列表。</p><h2 id="7043" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">编码类别</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/a96522a31b1bfc3ea62768e865f29489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTa3gMiUKO3eMRAMujQMNw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="654c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把注意力转向范畴。我们知道，我们不能直接用它们来训练我们的神经网络，我们必须以某种方式对它们进行编码(将它们转换成数字)。让我们探索几个选项。我们可以顺序编码(标签编码是针对标签(目标列)的)！)通过给列中的每个唯一值分配一个整数来对它们进行赋值。但是由于我们所有的数据都需要在范围[0，1]内，我们必须调整编码值。请想一想。</p><p id="173a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如:假设值“Susmit”被编码为3，然后缩放为0.21，值“Python”被编码并缩放为0.22，这没有任何意义，对吗？</p><p id="be36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用<code class="fe nn no np nq b"><a class="ae kv" href="https://contrib.scikit-learn.org/category_encoders/targetencoder.html" rel="noopener ugc nofollow" target="_blank">TargetEncoder</a></code>，但是我们需要缩放。同样，在我们必须执行逆变换操作的情况下，我们将必须存储映射，但是由于可能存在编码值对于两个或更多不同类别值是相同的情况，所以不可能执行精确的逆变换。</p><p id="1c89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里使用pandas的<code class="fe nn no np nq b">get_dummies</code>方法执行一个热编码。我们还指定了<code class="fe nn no np nq b">drop_first=True</code>，这将确保没有重复的信息。这就是我们的常量<code class="fe nn no np nq b">OHE_PREFIX</code>也被用来保持控制的地方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/018c919838a6f154bd13a70f66274992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_h1-BGY5Lj2h9K6WTr2EZQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3152" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上图所示，它有助于控制编码列的名称和为其生成列的值之间的分隔符。通常分隔符值应该是数据中不会出现的值(在我们的例子中是列名)。当我们想要找到原始列的特性重要性(而不是作为一次性编码的结果生成的列)时，将需要分隔符值。</p><h2 id="81ae" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">定义模型架构</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/92dc6d83681843d6a024461562ce2cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_Lyj_b_4jCHk6FSVrkIUw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="42c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们定义一个简单的函数，它定义了我们的模型，没有需要调整的超参数。我们将对我们的隐藏层使用<code class="fe nn no np nq b"><a class="ae kv" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">Rectified Linear Unit</a> </code> (ReLU)激活函数，它只不过是输入x的<code class="fe nn no np nq b">max(0, x)</code>。连同其他一些超参数，我们指定我们想要一个有512个神经元的隐藏层。如果对于一些其他任务，我们想要两个隐藏层，第一个具有256个神经元，第二个具有512个神经元，我们可以将其定义为，</p><pre class="kg kh ki kj gt oa nq ob oc aw od bi"><span id="246f" class="na lt iq nq b gy oe of l og oh">hidden_layer_sizes = (256, 512,)</span></pre><h2 id="8e8a" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">定义网格搜索CV</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/285b5bed957a566fb5f64c9e1c65ab96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJ9A7eRgkeOp-RiIG1_OOA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7e06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是时候进行一些超参数优化了！让我们理解我们定义的是什么。我们希望使用我们的<code class="fe nn no np nq b">model</code>，对于<code class="fe nn no np nq b">learning_rate_init</code>和<code class="fe nn no np nq b">alpha</code>的每个组合，我们希望使用<code class="fe nn no np nq b">(Stratified)KFold</code>策略和<code class="fe nn no np nq b">k=5</code>进行交叉验证，以确定最佳表现模型，从而确定最佳超参数，将使用<code class="fe nn no np nq b">f1_macro</code>评分。此外，我们希望拟合/训练在整个数据集上找到的最佳模型，我们通过<code class="fe nn no np nq b">refit=True</code>指定该数据集，最后但并非最不重要的是，我们希望使用我们机器的所有可用内核(<code class="fe nn no np nq b">n_jobs=-1</code>)。</p><h1 id="0bda" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后，对于特性的重要性，</h1><h2 id="5841" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">计算绝对平均重量</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/882a7f8dfa8bcf661bed591e537d20b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A7LtGeyiHuFOk6879kIm8Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4ae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开头显示的神经网络的超级简单的例子中，在隐藏层中只有单个神经元。因此，我们能够直接使用对应于输入特征的权重。但是在我们的例子中，我们的隐藏层有512个神经元。所以我们取每个特征的所有神经元系数的平均值。因为我们只关心大小，而不是方向，所以我们取绝对值。</p><p id="731c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解第一行中发生了什么，让我们探索一下<code class="fe nn no np nq b">model.coefs_</code>。首先，顾名思义，它存储了神经元之间的系数/权重。</p><pre class="kg kh ki kj gt oa nq ob oc aw od bi"><span id="9960" class="na lt iq nq b gy oe of l og oh">print([coef.shape for coef in model.coefs_]) <br/># Output: [(835, 512), (512, 2)]</span></pre><p id="dbe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所见，这里有两个元素，为什么？第一个元素存储输入层和隐藏层之间的权重，第二个元素存储隐藏层和输出层之间的权重。第一个元素中有835行和512列，因为在删除两个特征并添加一个热编码特征而不是没有目标列的分类特征后，我们的数据中有835个特征。512因为我们的隐藏层有512个神经元。</p><blockquote class="mt mu mv"><p id="8161" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated"><code class="fe nn no np nq b">(835 , 512)</code>隐藏层中每个神经元一列，每个特征一行</p></blockquote><p id="5f86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，当我们执行时，<code class="fe nn no np nq b">[i.mean() for i in model.coefs_[0]]</code>我们取所有512个系数的平均值，835个特征/(矩阵中的行)中的每一个都有一个平均值。</p><h2 id="a929" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">一个热编码列的分组权重</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/d05d23819db91cbcb740700e47591eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayc-4dEiGdA4VyIl8mHcfg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="990d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要执行这个步骤，因为我们使用了一个热编码，它将一个列分割成多个列，用0或1表示值的存在。例如，列<em class="mw">abowed</em>有三个唯一的值S、C和q。因为我们已经指定了<code class="fe nn no np nq b">drop_first=True</code>，所以生成了两个列。一个用于表示值Q的存在，一个用于表示值s。如果两者都不存在，则值为c。现在,<em class="mw">登上</em>的系数是<em class="mw">登上__OHE__Q </em>和<em class="mw">登上__OHE__S. </em>的系数之和。因此我们<em class="mw"> </em>提取原始列名，按其分组，并取和。</p><h2 id="12ab" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">称量重量</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/775610cd22d93d6f7a5914bd8464cc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vtk_eZ81XUoKVj4pihq9Sw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了确保所有的权重都在同一标度上，并且表示最重要特征的最大值是值1，表示最不重要特征的最小值是值0，我们执行最小-最大标度。</p><h2 id="0988" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">特征重要性函数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/2bc98c35e1d327f3398ff9bb7d055830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SbVOdjoasuTP4fZhaAmkOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0465" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们调用与计算特征重要性相关的函数，并将结果转换成一个字典。</p><h2 id="f5b2" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">缝合在一起</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/8b069098ad7639fac3a8357575bf9a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICJdt8yXVQEt9kRayodTcg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4736" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能已经猜到了，我是一个超级粉丝，</p><blockquote class="mt mu mv"><p id="3f6d" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated">函数的第一条规则是它们应该很小。函数的第二个规则是它们应该比那个小。罗伯特·马丁</p></blockquote><p id="2ad0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，</p><blockquote class="mt mu mv"><p id="f666" class="kw kx mw ky b kz la jr lb lc ld ju le mx lg lh li my lk ll lm mz lo lp lq lr ij bi translated">函数应该做一件事。他们应该做好这件事。他们应该只做这件事。罗伯特·马丁</p></blockquote><p id="3e21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是完整的代码供您参考</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure></div><div class="ab cl oo op hu oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="ij ik il im in"><p id="b0c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">乡亲们，就这样吧！</strong></p><p id="4ce8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢阅读，一定要读一读这篇关于云文件浏览器(一种用于云的文件浏览器和查看器)的发展故事的博客:<a class="ae kv" href="https://medium.com/@susmit.py/file-viewer-for-the-cloud-65dea0455dc7" rel="noopener">https://medium . com/@ susmit . py/File-Viewer-for-the-Cloud-65de a 0455 DC 7</a>(<a class="ae kv" href="https://cloudfileviewer.web.app/" rel="noopener ugc nofollow" target="_blank">https://cloudfileviewer.web.app/</a></p><p id="00ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想讨论一些很棒的东西，你可以在LinkedIn<a class="ae kv" href="https://www.linkedin.com/in/susmit-vengurlekar/" rel="noopener ugc nofollow" target="_blank">susmit-vengurlekar</a>上联系我！</p></div></div>    
</body>
</html>