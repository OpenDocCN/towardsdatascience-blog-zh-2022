<html>
<head>
<title>Training a Neural Network With a Few, Clean Lines of Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用几行简洁的代码训练一个神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-neural-network-with-a-few-clean-lines-of-code-b688da064bd5#2022-06-14">https://towardsdatascience.com/training-a-neural-network-with-a-few-clean-lines-of-code-b688da064bd5#2022-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6b3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可重用、可维护且易于理解的机器学习代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d5f22153d8e5413c36479baac4171c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GimV7Taz8hAp7ekbsTruPw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@fakurian?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">米拉德·法库里安</a>在<a class="ae ky" href="https://unsplash.com/s/photos/white?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄——由作者编辑</p></figure><p id="fb3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较少的代码通常会产生易于理解和维护的可读代码。Python编程语言已经在机器学习社区中非常流行，与其他编程语言相比，它允许您用更少的代码获得更好的结果。</p><p id="d5e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PyTorch是一个流行的Python深度学习框架，它有一个干净的API，并允许您编写真正像Python一样的代码。因此，用Python中的PyTorch创建模型和进行机器学习实验真的很有趣。</p><p id="7490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将向您展示训练一个识别手写数字的简单分类器所需的基本步骤。您将看到如何</p><ul class=""><li id="1292" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">用PyTorch的数据加载器加载MNIST数据集(用于机器学习的“Hello World”数据集)</li><li id="ed60" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">声明我们模型的架构</li><li id="2240" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">选择优化程序</li><li id="39bd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实施培训循环</li><li id="5658" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">确定训练模型的准确性</li></ul><p id="7f85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想让一切尽可能简单。因此，我不涉及过拟合、数据预处理或不同的度量来评估我们的分类器的性能。我们将只实现训练分类器所需的基本构件，这些构件可以很容易地在其他机器学习实验中重用。</p><p id="f545" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以让我们开始写一些代码。</p><p id="214d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要做的第一件事是导入必要的包。因为我们使用PyTorch，所以我们需要导入包<code class="fe mj mk ml mm b">torch</code>和<code class="fe mj mk ml mm b">torchvision</code>。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="d52e" class="mr ms it mm b gy mt mu l mv mw">import torch<br/>import torchvision as tv</span></pre><h1 id="462d" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">加载数据</h1><p id="b0a6" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在，我们可以通过torchvision加载我们的训练和验证数据集。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="5b54" class="mr ms it mm b gy mt mu l mv mw">t = tv.transforms.ToTensor()</span><span id="3767" class="mr ms it mm b gy nt mu l mv mw">mnist_training = tv.datasets.MNIST(<br/>    root='/tmp/mnist',<br/>    train=True,<br/>    download=True,<br/>    transform=t<br/>)</span><span id="bb37" class="mr ms it mm b gy nt mu l mv mw">mnist_val = tv.datasets.MNIST(<br/>    root='/tmp/mnist', <br/>    train=False, <br/>    download=True, <br/>    transform=t<br/>)</span></pre><p id="6cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们创建一个<code class="fe mj mk ml mm b">ToTensor()</code>的实例，用于将从<code class="fe mj mk ml mm b">datasets</code>包中获得的图像转换成张量。我们需要这一步，因为所有PyTorch函数都在张量上运行。如果你不了解张量，这些基本上只是多维数组的一个花哨名字。张量有秩。例如，秩为0的张量是标量，秩为1的张量是向量，秩为2的张量是矩阵，等等。</p><p id="3166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们加载我们的训练和验证数据集。使用<code class="fe mj mk ml mm b">root</code>,我们可以指定用于在光盘上存储数据集的目录。如果我们将<code class="fe mj mk ml mm b">train</code>设置为真，则训练集被加载。否则，加载验证集。如果我们将<code class="fe mj mk ml mm b">download</code>设置为true，PyTorch将下载数据集并将它们存储到通过<code class="fe mj mk ml mm b">root</code>指定的目录中。最后，我们可以指定应该应用于训练和验证数据集的每个示例的转换。在我们的情况下，它只是<code class="fe mj mk ml mm b">ToTensor()</code>。</p><h1 id="b85f" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">指定我们模型的架构</h1><p id="7953" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">接下来，我们指定模型的架构。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="e133" class="mr ms it mm b gy mt mu l mv mw">model = torch.nn.Sequential(<br/>    torch.nn.Linear(28*28, 128),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(128, 10)<br/>)</span></pre><h1 id="42e7" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">选择优化器和损失函数</h1><p id="a072" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">接下来，我们指定优化器和损失函数。</p><p id="192e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们正在使用<a class="ae ky" href="https://optimization.cbe.cornell.edu/index.php?title=Adam" rel="noopener ugc nofollow" target="_blank"> Adam优化器</a>。通过第一个参数，我们指定了优化器需要优化的模型参数。通过第二个参数<code class="fe mj mk ml mm b">lr</code>，我们指定了学习率。</p><p id="6690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第二行中，我们选择<code class="fe mj mk ml mm b">CrossEntropyLoss</code>作为损失函数(常用的损失函数的另一个词是<em class="nu">标准</em>)。此函数获取输出层的非标准化(N x 10)维输出(N是我们批次的样本数),并计算网络输出和目标标签之间的损失。目标标签被表示为包含输入样本的类别索引的N维向量(或者更具体地，秩为1的张量)。如您所见，CrossEntropyLoss是一个非常方便的函数。首先，在我们的网络末端，我们不需要像softmax这样的标准化层。第二，我们不必在不同的标签表示之间转换。我们的网络输出分数的10维向量，并且目标标签被提供为类别索引的向量(0到9之间的整数)。</p><p id="1638" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们为训练数据集创建一个数据加载器。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="7de5" class="mr ms it mm b gy mt mu l mv mw">loader = torch.utils.data.DataLoader(<br/>    mnist_training, <br/>    batch_size=500, <br/>    shuffle=True<br/>)</span></pre><p id="cbdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据加载器用于从数据集中检索样本。我们可以使用数据加载器来轻松地迭代一批样本。这里，我们创建一个加载器，它在每次迭代中从训练数据集中返回500个样本。如果我们设置<code class="fe mj mk ml mm b">shuffle</code>为真，样本将在批次中被混洗。</p><h1 id="37af" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">训练机器学习模型</h1><p id="274f" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在，我们有了训练模型所需的一切。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="235b" class="mr ms it mm b gy mt mu l mv mw">for epoch in range(10):<br/>    for imgs, labels in loader:<br/>        n = len(imgs)<br/>        imgs = imgs.view(n, -1)<br/>        predictions = model(imgs)  <br/>        loss = loss_fn(predictions, labels) <br/>        opt.zero_grad()<br/>        loss.backward()<br/>        opt.step()<br/>    print(f"Epoch: {epoch}, Loss: {float(loss)}")</span></pre><p id="2fdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用10个历元来训练我们的网络(第1行)。在每个时期，我们对加载器进行迭代，以在每次迭代中获得500张图像及其标签(第2行)。变量<code class="fe mj mk ml mm b">imgs</code>是形状的张量(500，1，28，28)。变量<code class="fe mj mk ml mm b">labels</code>是具有500个类别索引的秩为1的张量。</p><p id="675a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第3行，我们将当前批次的图像数量保存在变量<code class="fe mj mk ml mm b">n</code>中。在第4行中，我们将形状(n，1，28，28)的张量<code class="fe mj mk ml mm b">imgs</code>整形为形状(n，784)的张量。在第5行中，我们使用我们的模型来预测当前批次的所有图像的标签。然后，在第6行，我们计算这些预测和事实之间的损失。张量<code class="fe mj mk ml mm b">predictions</code>是形状(n，10)的张量，而<code class="fe mj mk ml mm b">labels</code>是包含类别索引的秩为1的张量。在第7行到第9行中，我们重置了网络所有参数的梯度，计算了梯度并更新了模型的参数。</p><p id="ae2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还在每个时期之后打印损耗，以便我们可以验证网络在每个时期之后变得更好(即损耗减少)。</p><h1 id="6135" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">确定准确度</h1><p id="a72b" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">现在，我们已经训练了网络，我们可以确定模型识别手写数字的准确性。</p><p id="bb92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要从验证数据集中获取数据。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="4fe8" class="mr ms it mm b gy mt mu l mv mw">n = 10000<br/>loader = torch.utils.data.DataLoader(mnist_val, batch_size=n)<br/>images, labels = iter(loader).next()</span></pre><p id="44b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的验证数据集<code class="fe mj mk ml mm b">mnist_val</code>包含10000张图片。为了得到所有这些图像，我们使用一个数据加载器，并将<code class="fe mj mk ml mm b">batch_size</code>设置为10000。然后，我们可以通过从数据加载器创建一个迭代器，并在迭代器上调用<code class="fe mj mk ml mm b">next()</code>来获取第一个元素，从而获得数据。</p><p id="a6d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是一个元组。这个元组的第一个元素是形状张量(10000，1，28，28)。第二个元素是秩为1的张量，它包含图像的类别索引。</p><p id="285f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以使用我们的模型来预测所有图像的标签。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="41c5" class="mr ms it mm b gy mt mu l mv mw">predictions = model(images.view(n, -1))</span></pre><p id="7d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们能够向我们的模型提供数据之前，我们需要重塑它(类似于我们已经在训练循环中所做的)。我们模型的输入张量需要具有形状(n，784)。当数据具有正确的形状时，我们可以将其用作模型的输入。</p><p id="1f32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是形状的张量(10000，10)。对于我们的验证集的每个图像，这个张量存储了十个可能标签中每个标签的分数。标签的分数越高，样品越有可能属于相应的标签。</p><p id="8e5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，样本被分配给具有最高分数的标签。我们可以通过张量的<code class="fe mj mk ml mm b">argmax()</code>方法很容易地确定标签，因为这个方法返回最大值的位置。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="7029" class="mr ms it mm b gy mt mu l mv mw">predicted_labels = predictions.argmax(dim=1)</span></pre><p id="b7e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对维度1的最大值感兴趣，因为每个样本的分数都存储在这个维度上。结果是秩为1的张量，它现在存储预测的类索引而不是分数。</p><p id="112b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以将预测的类索引与基础事实标签进行比较，以计算验证数据集的准确性。准确度被定义为已经被正确预测的样本的分数，即正确预测的样本数除以样本总数。</p><p id="88e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获得这个度量的技巧是对预测标签和实际标签进行元素比较。</p><pre class="kj kk kl km gt mn mm mo mp aw mq bi"><span id="3256" class="mr ms it mm b gy mt mu l mv mw">torch.sum(predicted_labels == labels) / n</span></pre><p id="8cff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们比较<code class="fe mj mk ml mm b">predicted_labels</code>和<code class="fe mj mk ml mm b">labels</code>是否相等，得到一个布尔向量。如果同一位置的两个元素相等，则该向量中的一个元素为真。否则，元素为false。然后，我们使用<code class="fe mj mk ml mm b">sum</code>来计算为真的元素的数量，并将该数量除以n</p><p id="bd1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们执行所有这些步骤，我们应该可以达到大约97%的准确率。</p><p id="ab1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GitHub上也有完整的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="36fb" class="mx ms it bd my mz na nb nc nd ne nf ng jz nh ka ni kc nj kd nk kf nl kg nm nn bi translated">结论</h1><p id="0bd8" class="pw-post-body-paragraph kz la it lb b lc no ju le lf np jx lh li nq lk ll lm nr lo lp lq ns ls lt lu im bi translated">作为一名软件工程师，我喜欢干净的代码。有时候，我会在一些小代码前坐半个小时甚至更长时间，只是为了让它更优雅、更漂亮。</p><p id="1670" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PyTorch是一个非常棒的深度学习框架，它允许您编写易于理解并且感觉像Python的干净代码。我们已经看到，只用几行代码就可以生成表达性代码来创建和训练最先进的机器学习模型。</p><p id="83be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这个简单演示中使用的构建模块可以在许多机器学习项目中重用，我希望它们也能在您的机器学习之旅中对您有所帮助。</p></div></div>    
</body>
</html>