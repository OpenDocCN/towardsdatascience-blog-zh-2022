<html>
<head>
<title>Serving TensorRT Models with NVIDIA Triton Inference Server</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NVIDIA Triton 推理服务器为 TensorRT 模型提供服务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serving-tensorrt-models-with-nvidia-triton-inference-server-5b68cc141d19#2022-12-15">https://towardsdatascience.com/serving-tensorrt-models-with-nvidia-triton-inference-server-5b68cc141d19#2022-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f9cb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过对高客户端-服务器流量的模型推断实现最佳吞吐量和延迟</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6ea970a4e34c4ba7352dab9249fd2f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoIds7b83MgSohjKqtCWsQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@floriankrumm" rel="noopener ugc nofollow" target="_blank">弗洛里安·克拉姆</a>在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="27c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在实时 AI 模型整体部署中，模型推理和硬件/GPU 使用的效率至关重要。单个客户机-服务器推理请求的速度取决于服务器的延迟和吞吐量。这是因为深度学习模型通常安装在从公共客户端设备接收多个传入请求或数据的服务器或服务器集群上。让我在这里描述一些定义:</p><p id="fc63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延迟</strong>:通过客户端-服务器连接的单个请求-响应循环所花费的时间。假设稳定的互联网连接，延迟将取决于模型推断的速度、数据包的传输和一些其他因素。</p><p id="97cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">吞吐量</strong>:服务器在单个时间实例中可以处理的传入请求的数量。当传入流量超过吞吐量时，多余的请求会被排队，从而减慢请求-响应过程。</p><p id="888e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的上一篇文章中，我们已经讨论了在 edge (一个独立的 GPU Linux 设备)上用 TensorRT 设置和运行模型推理<strong class="lb iu">。正如我们所见，这非常简单:</strong></p><div class="lv lw gp gr lx ly"><a href="https://medium.com/mlearning-ai/running-tensorflow-model-on-edge-with-tensorrt-for-fast-inference-4dcad300a523" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">使用 TensorRT 在 Edge 上运行 Tensorflow 模型进行快速推理</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">关于 Linux 环境下 TensorRT 的设置、量化和运行推理</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="0471" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，在本文中，我们将讨论在单个服务器(一个将通过网络接收来自其他设备的请求的 GPU 工作站)上使用 tensort 设置和运行模型推理<strong class="lb iu">，即<a class="ae ky" href="https://developer.nvidia.com/nvidia-triton-inference-server" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> NVIDIA Triton 推理服务器</strong> </a> <strong class="lb iu">。</strong></strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="70e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 Triton 服务器上运行深度学习模型有几个优点，据报道，它优于其他框架，如 TFServing 和 TorchServe。例如，它能够通过动态批量推理和多个请求的模型推理中的并发性来优化吞吐量。结合使用 TensorRT 来优化延迟，Triton server 可以同时提供极快的推理速度。</p><p id="852b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于 Triton server 性能的更多信息可以在以下文章中找到:</p><div class="lv lw gp gr lx ly"><a href="https://blog.advance.ai/blog/accelerating-ai-deep-learning-models" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">使用 tensorRT &amp; triton 推理加速 AI/深度学习模型</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">模型服务问题人工智能/深度学习模型已经接管了许多以前需要人类的服务…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">blog.advance.ai</p></div></div></div></a></div><div class="lv lw gp gr lx ly"><a href="https://developer.nvidia.com/blog/nvidia-serves-deep-learning-inference/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">NVIDIA Triton 推理服务器提升深度学习推理| NVIDIA 技术博客</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">你已经建立、训练、调整和调整了你的模型。您最终创建了一个 TensorRT、TensorFlow 或 ONNX 模型…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">developer.nvidia.com</p></div></div><div class="mh l"><div class="mp l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="87b8" class="mx my it bd mz na nb dn nc nd ne dp nf li ng nh ni lm nj nk nl lq nm nn no np bi translated">先决条件</h2><ul class=""><li id="f950" class="nq nr it lb b lc ns lf nt li nu lm nv lq nw lu nx ny nz oa bi translated">配备 NVIDIA GPU 的本地工作站/笔记本电脑</li><li id="d277" class="nq nr it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">Docker 容器和 Linux 终端的基础知识</li><li id="8f61" class="nq nr it lb b lc ob lf oc li od lm oe lq of lu nx ny nz oa bi translated">Python 基础和深度学习库的知识</li></ul><p id="c531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事不宜迟，让我们开始吧！</p><h1 id="aa85" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">1.张量流模型的一个例子</h1><p id="a381" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">在我们在 Triton 服务器上测试的深度学习模型示例中，我们选择了一个经典的 CNN 模型——resnet 50——在 ImageNet 数据集上进行了预训练，如下面的代码片段所示。接下来，我们将这个 Tensorflow 模型优化为 TensorRT 模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou mo l"/></div></figure><h1 id="7539" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">2.转换到 ONNX 模型</h1><p id="e3bf" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">虽然有不同的 tensort 框架，如 tensor flow-tensort 和 ONNX tensort，但 NVIDIA Triton server 采用的框架只有 ONNX tensort。因此，我们需要首先将任何 Keras 或 Tensorflow 模型转换为 ONNX 格式，如下面的代码片段所示。首先，您可能希望将<code class="fe ov ow ox oy b">model.onnx</code>模型保存在以下目录中:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="1b1f" class="pd my it oy b be pe pf l pg ph">${PWD}/models/tensorrt_fp16_model/1/model.onnx</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou mo l"/></div></figure><h1 id="e497" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">3.使用 Docker 容器转换到 TensorRT 模型</h1><p id="170f" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">接下来，我们要将 ONNX 型号<code class="fe ov ow ox oy b">model.onnx</code>转换为 TensorRT 型号<code class="fe ov ow ox oy b">model.plan</code>。如果您在本地安装了 TensorRT，您可能会尝试进行本地转换。然而，这是有问题的，因为 Triton 服务器容器中的 TensorRT 和 CUDA 软件——我们将在后面看到——在运行<code class="fe ov ow ox oy b">model.plan</code>文件时可能表现不同。即使本地 tensort 的版本与 Triton TensorRT 的版本相似，也是如此。</p><p id="cf5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，NVIDIA 为 TensorRT 提供了 Docker 映像，其版本标签与 Triton server 的版本标签互补。假设您在本地安装了 Docker，在终端上运行:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="7655" class="pd my it oy b be pe pf l pg ph">docker pull nvcr.io/nvidia/tensorrt:22.11-py3</span></pre><p id="8405" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦提取了 Docker 映像，我们将在运行容器时进行卷绑定挂载。请注意，卷绑定装载的参数必须是绝对路径。</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="e138" class="pd my it oy b be pe pf l pg ph">docker run -it --gpus all --rm -v ${PWD}/models/tensorrt_fp16_model/1:/trt_optimize nvcr.io/nvidia/tensorrt:22.11-py3</span></pre><p id="8eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将在容器中启动一个终端，随后我们在容器中进行 TensorRT 转换以创建<code class="fe ov ow ox oy b">model.plan</code>，由于绑定挂载，它也将在本地可用。</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="a5e8" class="pd my it oy b be pe pf l pg ph"># cd /trt_optimize<br/># /workspace/tensorrt/bin/trtexec --onnx=model.onnx --saveEngine=model.plan  --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16</span></pre><h1 id="ff2f" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">4.设置本地目录以镜像 Triton 服务器</h1><p id="76d1" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">Triton 服务器软件同样是一个 Docker 映像，我们将下载该映像，之后我们还将进行卷绑定挂载。然而，在我们进行 Docker 拉取之前，我们会想要坚持并在本地初始化某个目录结构以及一个必要的<code class="fe ov ow ox oy b">config.pbtxt</code>模型配置文件。</p><p id="7f23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">目录结构应该如下所示:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="e57e" class="pd my it oy b be pe pf l pg ph">${PWD}/models<br/>         |<br/>         |-- tensorrt_fp16_model<br/>         |          |<br/>         |          |-- config.pbtxt<br/>         |          |-- 1<br/>         |          |   |<br/>         |          |   |--model.plan</span></pre><p id="2e59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用例的<code class="fe ov ow ox oy b">config.pbtxt</code>文件示例如下:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="c4e6" class="pd my it oy b be pe pf l pg ph">name: "tensorrt_fp16_model"<br/>platform: "tensorrt_plan"<br/>max_batch_size: 32<br/><br/>input [ <br/>    {<br/>        name: "input_1"<br/>        data_type: TYPE_FP16<br/>        dims: [ 224, 224, 3 ]<br/><br/>    }<br/>]<br/><br/>output [<br/>    {<br/>        name: "predictions"<br/>        data_type: TYPE_FP16<br/>        dims: [ 1000 ]<br/>    }<br/>]</span></pre><h1 id="cbea" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">5.设置 Triton 服务器容器</h1><p id="352b" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">在确保 Triton 服务器的 Docker 镜像版本标签与 TensorRT 的相似之后，我们就可以开始下载了:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="0f98" class="pd my it oy b be pe pf l pg ph">docker pull nvcr.io/nvidia/tritonserver:22.11-py3</span></pre><p id="cd32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后以附加模式运行 Docker 容器:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="867f" class="pd my it oy b be pe pf l pg ph">docker run --gpus=all --rm  --name triton_server -p 8000:8000 -p 8001:8001 -p 8002:8002 -v ${PWD}/models:/models nvcr.io/nvidia/tritonserver:22.11-py3 tritonserver --model-repository=/models --model-control-mode=poll --repository-poll-secs 30</span></pre><p id="06f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果 Triton 服务器容器设置正确并准备好进行推理，您应该在终端中看到以下输出，并且“状态”中没有任何错误消息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/8e22716aca8344308debafcbe5c3861d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPK7a71UUDyvdqGUN88jMQ.png"/></div></div></figure><h1 id="b78b" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">6.来自客户端设备的推断</h1><p id="3843" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">在这里，我将演示在客户机工作站上通过 Triton 服务器运行推理的示例代码。当然，我们还需要在客户端工作站上安装<code class="fe ov ow ox oy b">tritonclient</code>库，包括:</p><pre class="kj kk kl km gt oz oy pa bn pb pc bi"><span id="2eb1" class="pd my it oy b be pe pf l pg ph">pip install tritonclient[all]</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou mo l"/></div></figure><h1 id="86f3" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">7.Triton TensorRT 比本地 tensort 慢</h1><p id="f07a" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">在我们结束本文之前，我必须提到的一个警告是，由于优化的 GPU 使用和批量推断等优势，Triton server 在处理大量客户端-服务器流量时确实表现出色。</p><p id="15ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当我们考虑在 Triton TensorRT 和 local TensorRT 之间运行单个本地推理时，local TensorRT 仍然具有更快的推理速度，因为 Triton TensorRT 在网络上运行推理时具有额外的开销。在下面的线程中可以看到一些参数:</p><div class="lv lw gp gr lx ly"><a href="https://github.com/triton-inference-server/server/issues/4812" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">Tritonserver 推理中不可理解的开销问题# 4812 triton-推理-服务器/服务器</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">描述关于 Tritonserver 的性能有两个奇怪的实验结果，我不能…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">github.com</p></div></div><div class="mh l"><div class="pj l mj mk ml mh mm ks ly"/></div></div></a></div><h1 id="6a06" class="og my it bd mz oh oi oj nc ok ol om nf jz on ka ni kc oo kd nl kf op kg no oq bi translated">8.结论</h1><p id="f5e4" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li or lk ll lm os lo lp lq ot ls lt lu im bi translated">感谢阅读这篇文章！部署具有可扩展性的人工智能模型是任何有抱负的人工智能工程师或机器学习工程师的一项重要技能，了解 NVIDIA Triton 服务器无疑会在竞争激烈的数据科学领域中占据优势。否则，强大而精确的模型只能在 Jupyter 笔记本和 VS 代码脚本后面枯萎。</p><p id="faf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的<a class="ae ky" href="https://github.com/tanpengshi/NVIDIA_Triton_Server_TensorRT" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>上查看这篇文章的代码。</p><p id="7fc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，感谢你和我一起学习人工智能和数据科学。如果你喜欢这些内容，可以在<a class="ae ky" href="https://tanpengshi.medium.com/" rel="noopener">媒体</a>上阅读我的其他文章，并在<a class="ae ky" href="https://www.linkedin.com/in/tanpengshi/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我。</p><p id="f46f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，如果你想知道如何快速深入地学习数据科学和人工智能，请查看我在《学习》中的一篇顶级文章:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/a-brief-guide-to-effective-learning-in-data-science-637de316da0e"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">数据科学有效学习完全指南</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">基础和详细的指南，让你在数据科学(或任何学科)上突飞猛进</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="pk l mj mk ml mh mm ks ly"/></div></div></a></div><blockquote class="pl pm pn"><p id="9cd6" class="kz la po lb b lc ld ju le lf lg jx lh pp lj lk ll pq ln lo lp pr lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">支持我！</em> </strong> <em class="it"> — </em>如果你没有订阅 Medium，但喜欢我的内容，请考虑通过我的<a class="ae ky" href="https://tanpengshi.medium.com/membership" rel="noopener">推荐链接</a>加入 Medium 来支持我。</p></blockquote><div class="lv lw gp gr lx ly"><a href="https://tanpengshi.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">加入我的介绍链接媒体-谭师鹏阿尔文</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">阅读谭·师鹏·阿尔文(以及媒体上成千上万其他作家)的每一个故事。您的会员费直接…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">tanpengshi.medium.com</p></div></div><div class="mh l"><div class="ps l mj mk ml mh mm ks ly"/></div></div></a></div></div></div>    
</body>
</html>