<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-102064f24372#2022-11-06">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-102064f24372#2022-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c131d76bc2a15c3b36a99920fe9466ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*srkcTOQVVsaAaEaZ.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="958e" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第3部分:TF-IDF分析</h2></div><p id="e907" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本系列的<a class="ae jg" href="https://valentinaalto.medium.com/natural-language-process-for-judicial-sentences-with-python-part-2-964b0e12dd4a" rel="noopener">上一篇文章</a>中，我们使用简单的描述性统计数据更深入地研究了我们的数据集。在本文中，我们将使用术语频率(TF) —逆密集频率(IDF)技术对术语的重要性进行更深入的分析。这种技术用于度量一个单词对于文档集合中的一个文档的重要性。</p><h2 id="0625" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">TF-IDF</h2><p id="3e0c" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这种方法背后的思想是找到一种方法来确定一个单词对一个文档的重要性，不仅仅是通过计算它的出现次数，还要考虑集合中其他文档的上下文。</p><p id="b0b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，假设我们有100篇关于计算机视觉的科学论文，让我们考虑一个专注于特定卷积神经网络ResNet的文档A。在这个文档中，ResNet和相关术语的出现频率会很高，这些术语对于这个特定的文档非常重要。然而，也有高频的其他术语，如“图像”、“神经网络”、“像素”，所有术语都与计算机视觉领域相关。然而，这些最后的术语将出现在所有的文档中，所以它们不会出现在我们关注的文档A中。</p><p id="0f62" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们说，对于文档A，术语“ResNet”和“image”将具有高频率。对于文档A，我们如何告诉模型“ResNet”比“image”更重要？答案在IDF分量中，它只不过是一个用来补偿TF的权重，计算如下:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e4f0ee3d5031cdbb310fda2e661a1e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*gCJmaZEHrTl_O6OFX3mKjQ.png"/></div></figure><p id="76c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以它是这个词出现的文档数的对数。如果该术语出现在所有文档中(如术语“图像”)，IDF权重将等于0，因此该术语将被标记为不重要。每当文档中的术语频繁出现(高TF)但在集合中的许多文档中不出现(IDF低)时，TF-IDF得分增加。</p><p id="a057" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以最后，TF-IDF的分数是:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f46ffe716821353db5aea39a1dc8b1b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*Yd9XpFT-KMjEIh5XDfYrTQ.png"/></div></figure><p id="8661" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">产生加权的术语频率，其能够测量术语对于文档集合中的文档的重要性。</p><p id="80a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了继续计算这个度量，我将对我的文档进行矢量化处理，使文档中的每个术语都根据其TF-IDF得分得到一个权重。具有较高权重分数的术语被认为更重要。</p><h2 id="fc38" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">文档矢量化</h2><p id="9303" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">文档矢量化是自然语言处理任务中文本数据预处理的重要步骤。事实上，通过这种实践，原始文本可以被我们的模型解释为数字表示。出于TF-IDF分析的目的，我们将使用术语-文档矩阵的表示。</p><p id="04e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其想法是创建一个矩阵，其维度分别是文档和单个术语。每个单元格都填充了相应文档中特定术语的频率。</p><p id="7bdb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来举个例子:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/472308382734995c515b562ef87222f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwhMN5yodx6ei6fimyH6dQ.png"/></div></div></figure><p id="d4f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们进入代码。</p><h2 id="06e9" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">用Python计算TF-IDF</h2><p id="4680" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我将首先尝试对词干进行矢量化，然后尝试一种使用引理的不太激进的方法(在本系列的第1部分中计算)。此外，由于标记为的<strong class="la jk">条目少于5000个，我将首先对整个数据集进行分析(以便遵守最少5000个单词的限制)。然后，我将对较小的标签文章数据集执行相同的分析，这样我也可以显示每个术语的类别。</strong></p><p id="4ec0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以让我们从最重要的100个术语开始(用词干和词条表示):</p><pre class="mt mu mv mw gt mz na nb nc aw nd bi"><span id="64a4" class="lu lv jj na b gy ne nf l ng nh">#first thing first, let's create a df_factor where the labels are one-hot encoded. </span><span id="050b" class="lu lv jj na b gy ni nf l ng nh">from sklearn.preprocessing import MultiLabelBinarizer</span><span id="9dd8" class="lu lv jj na b gy ni nf l ng nh">mlb = MultiLabelBinarizer()</span><span id="e0c2" class="lu lv jj na b gy ni nf l ng nh">df_factor = df.join(pd.DataFrame(mlb.fit_transform(df['category']),<br/>                          columns=mlb.classes_,<br/>                          index=df.index))</span><span id="927f" class="lu lv jj na b gy ni nf l ng nh">#representing Stems: analyzing the most important 100 terms</span><span id="30a2" class="lu lv jj na b gy ni nf l ng nh">sns.set_context("paper")<br/>sns.set()<br/>fig, ax = plt.subplots(figsize = (16, 9))<br/>ax.set_title("Stems TF-IDF (size scaling on TF)", fontsize = 15)<br/>ax.set_ylabel("idf")<br/>ax.set_xlabel("tfidf")<br/>ax.set_xbound(lower = 0, upper = 1000)</span><span id="f7b6" class="lu lv jj na b gy ni nf l ng nh">tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', min_df = 0.2, max_df = 0.6, <br/>                                       stop_words = 'english', sublinear_tf = True)</span><span id="fd4d" class="lu lv jj na b gy ni nf l ng nh">X = tfidf_vectorizer.fit_transform(df_factor.Stems)</span><span id="7045" class="lu lv jj na b gy ni nf l ng nh">count_vectorizer = CountVectorizer(analyzer='word', min_df = 0.2, <br/>                                       max_df = 0.6, stop_words = 'english')</span><span id="5294" class="lu lv jj na b gy ni nf l ng nh">X2 = count_vectorizer.fit_transform(df_factor.Stems)</span><span id="337d" class="lu lv jj na b gy ni nf l ng nh">word_counts = X2.toarray()</span><span id="a2bf" class="lu lv jj na b gy ni nf l ng nh">word_tfidf = X.toarray()</span><span id="05ac" class="lu lv jj na b gy ni nf l ng nh">word_tfidf[word_tfidf &lt; 0.2] = 0</span><span id="23df" class="lu lv jj na b gy ni nf l ng nh">df_tfidf = pd.DataFrame(data = {'word': count_vectorizer.get_feature_names(), <br/>                            'tf': word_counts.sum(axis = 0), <br/>                            'idf': tfidf_vectorizer.idf_,<br/>                            'tfidf': word_tfidf.sum(axis = 0)})</span><span id="5632" class="lu lv jj na b gy ni nf l ng nh">df_tfidf.sort_values(["tfidf", 'tf', 'idf'], ascending = False, inplace = True)<br/>df_tfidf.reset_index(drop = True, inplace = True)<br/>    <br/>    <br/>    <br/>    <br/>    <br/>ax.scatter(df_tfidf.tfidf[:100], df_tfidf.idf[:100], s = df_tfidf.tf * 0.14, <br/>            cmap = "Blues", alpha = 0.7, edgecolors = "black", linewidths = 1.2) #picking the most important 100 terms</span><span id="4f98" class="lu lv jj na b gy ni nf l ng nh">for index, text in enumerate(df_tfidf.word[:100].tolist()):<br/>        ax.annotate(text, (df_tfidf.tfidf.tolist()[index], df_tfidf.idf.tolist()[index]), <br/>                    (df_tfidf.tfidf.tolist()[index] + 0, df_tfidf.idf.tolist()[index] + 0.11))<br/>        <br/>box = ax.get_position()<br/>ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9c118e83ef2235f4eb0a7d9678af4347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*9z45yDQK52kz2XZpzUOGBg.png"/></div></figure><pre class="mt mu mv mw gt mz na nb nc aw nd bi"><span id="ff5d" class="lu lv jj na b gy ne nf l ng nh">#representing Lemmas: analyzing the most important 100 terms</span><span id="0b92" class="lu lv jj na b gy ni nf l ng nh">sns.set_context("paper")<br/>sns.set()<br/>fig, ax = plt.subplots(figsize = (16, 9))<br/>ax.set_title("Lemmas TF-IDF (size scaling on TF)", fontsize = 15)<br/>ax.set_ylabel("idf")<br/>ax.set_xlabel("tfidf")<br/>ax.set_xbound(lower = 0, upper = 1000)</span><span id="8526" class="lu lv jj na b gy ni nf l ng nh">tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', min_df = 0.2, max_df = 0.6, <br/>                                       stop_words = 'english', sublinear_tf = True)</span><span id="fa4b" class="lu lv jj na b gy ni nf l ng nh">X = tfidf_vectorizer.fit_transform(df_factor.Lemmas)</span><span id="aaac" class="lu lv jj na b gy ni nf l ng nh">count_vectorizer = CountVectorizer(analyzer='word', min_df = 0.2, <br/>                                       max_df = 0.6, stop_words = 'english')</span><span id="0bf1" class="lu lv jj na b gy ni nf l ng nh">X2 = count_vectorizer.fit_transform(df_factor.Lemmas)</span><span id="686a" class="lu lv jj na b gy ni nf l ng nh">word_counts = X2.toarray()</span><span id="1af8" class="lu lv jj na b gy ni nf l ng nh">word_tfidf = X.toarray()</span><span id="63f8" class="lu lv jj na b gy ni nf l ng nh">word_tfidf[word_tfidf &lt; 0.2] = 0</span><span id="8edc" class="lu lv jj na b gy ni nf l ng nh">df_tfidf = pd.DataFrame(data = {'word': count_vectorizer.get_feature_names(), <br/>                            'tf': word_counts.sum(axis = 0), <br/>                            'idf': tfidf_vectorizer.idf_,<br/>                            'tfidf': word_tfidf.sum(axis = 0)})</span><span id="9ea5" class="lu lv jj na b gy ni nf l ng nh">df_tfidf.sort_values(["tfidf", 'tf', 'idf'], ascending = False, inplace = True)<br/>df_tfidf.reset_index(drop = True, inplace = True)<br/>    <br/>    <br/>    <br/>    <br/>    <br/>ax.scatter(df_tfidf.tfidf[:100], df_tfidf.idf[:100], s = df_tfidf.tf * 0.14, <br/>            cmap = "Blues", alpha = 0.7, edgecolors = "black", linewidths = 1.2)</span><span id="bdd4" class="lu lv jj na b gy ni nf l ng nh">for index, text in enumerate(df_tfidf.word[:100].tolist()):<br/>        ax.annotate(text, (df_tfidf.tfidf.tolist()[index], df_tfidf.idf.tolist()[index]), <br/>                    (df_tfidf.tfidf.tolist()[index] + 0, df_tfidf.idf.tolist()[index] + 0.11))<br/>        <br/>box = ax.get_position()<br/>ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/15a496205e5b7274d26811b49c8f72f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*jWdHIb2YbavOk_5OBto1ng.png"/></div></figure><p id="2304" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们只使用带标签的文章，并使用类别作为进一步的变量(颜色=类别)。更具体地说，我们为每个类别选择了前两个最重要的词:</p><pre class="mt mu mv mw gt mz na nb nc aw nd bi"><span id="0bd8" class="lu lv jj na b gy ne nf l ng nh">sns.set_context("talk")<br/>sns.set()<br/>fig, ax = plt.subplots(figsize = (16, 9))<br/>ax.set_title("Stem TF-IDF (size scaling on TF)", fontsize = 15)<br/>ax.set_ylabel("idf")<br/>ax.set_xlabel("tfidf")<br/>ax.set_xbound(lower = 0, upper = 1000)<br/>z=categories_list</span><span id="ceb4" class="lu lv jj na b gy ni nf l ng nh">cmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i) for i in np.linspace(0, 1, len(z))]<br/>for category, color in zip(z, colors):<br/>    tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', min_df = 0.2, max_df = 0.6, <br/>                                       stop_words = 'english', sublinear_tf = True)</span><span id="90ef" class="lu lv jj na b gy ni nf l ng nh">X = tfidf_vectorizer.fit_transform(df_factor[df_factor[category] == 1].Stems)</span><span id="e5ed" class="lu lv jj na b gy ni nf l ng nh">count_vectorizer = CountVectorizer(analyzer='word', min_df = 0.2, <br/>                                       max_df = 0.6, stop_words = 'english')</span><span id="fc30" class="lu lv jj na b gy ni nf l ng nh">X2 = count_vectorizer.fit_transform(df_factor[df_factor[category] == 1].Stems)</span><span id="4ccd" class="lu lv jj na b gy ni nf l ng nh">word_counts = X2.toarray()</span><span id="7efa" class="lu lv jj na b gy ni nf l ng nh">word_tfidf = X.toarray()</span><span id="bdb9" class="lu lv jj na b gy ni nf l ng nh">word_tfidf[word_tfidf &lt; 0.2] = 0</span><span id="5e4a" class="lu lv jj na b gy ni nf l ng nh">df_tfidf = pd.DataFrame(data = {'word': count_vectorizer.get_feature_names(), <br/>                            'tf': word_counts.sum(axis = 0), <br/>                            'idf': tfidf_vectorizer.idf_,<br/>                            'tfidf': word_tfidf.sum(axis = 0)})</span><span id="9059" class="lu lv jj na b gy ni nf l ng nh">df_tfidf.sort_values(["tfidf", 'tf', 'idf'], ascending = False, inplace = True)<br/>    df_tfidf.reset_index(drop = True, inplace = True)<br/>    <br/>    <br/>    <br/>    <br/>    ax.scatter(df_tfidf.tfidf[:2], df_tfidf.idf[:2], s = df_tfidf.tf * 0.14, <br/>            cmap = "Blues", alpha = 0.7, edgecolors = "black", linewidths = 1.2, color = color, label = category)</span><span id="6681" class="lu lv jj na b gy ni nf l ng nh">for index, text in enumerate(df_tfidf.word[:2].tolist()):<br/>        ax.annotate(text, (df_tfidf.tfidf.tolist()[index], df_tfidf.idf.tolist()[index]), <br/>                    (df_tfidf.tfidf.tolist()[index] + 0, df_tfidf.idf.tolist()[index] + 0.11))<br/>        <br/>box = ax.get_position()<br/>ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])<br/>ax.legend(title = "Category", frameon = True, ncol = 2, fancybox = True, title_fontsize = 15,<br/>          loc = 'center left', bbox_to_anchor = (1, 0.5), labelspacing = 2.5, borderpad = 2);</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/837c61b46ad7a695b8be2d2e64be3302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6r0x5PWkF8eaWpDxjCB7lg.png"/></div></div></figure><pre class="mt mu mv mw gt mz na nb nc aw nd bi"><span id="1751" class="lu lv jj na b gy ne nf l ng nh">#let's do the same also for Lemmas</span><span id="044f" class="lu lv jj na b gy ni nf l ng nh">sns.set_context("talk")<br/>sns.set()<br/>fig, ax = plt.subplots(figsize = (16, 9))<br/>ax.set_title("Lemmas TF-IDF (size scaling on TF)", fontsize = 15)<br/>ax.set_ylabel("idf")<br/>ax.set_xlabel("tfidf")<br/>ax.set_xbound(lower = 0, upper = 1000)<br/>z=categories_list</span><span id="9154" class="lu lv jj na b gy ni nf l ng nh">cmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i) for i in np.linspace(0, 1, len(z))]</span><span id="ae6f" class="lu lv jj na b gy ni nf l ng nh">for category, color in zip(z, colors):<br/>    tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', min_df = 0.2, max_df = 0.6, <br/>                                       stop_words = 'english', sublinear_tf = True)</span><span id="8a83" class="lu lv jj na b gy ni nf l ng nh">X = tfidf_vectorizer.fit_transform(df_factor[df_factor[category] == 1].Lemmas)</span><span id="0a5d" class="lu lv jj na b gy ni nf l ng nh">count_vectorizer = CountVectorizer(analyzer='word', min_df = 0.2, <br/>                                       max_df = 0.6, stop_words = 'english')</span><span id="f22d" class="lu lv jj na b gy ni nf l ng nh">X2 = count_vectorizer.fit_transform(df_factor[df_factor[category] == 1].Lemmas)</span><span id="02f2" class="lu lv jj na b gy ni nf l ng nh">word_counts = X2.toarray()</span><span id="062e" class="lu lv jj na b gy ni nf l ng nh">word_tfidf = X.toarray()</span><span id="b9e4" class="lu lv jj na b gy ni nf l ng nh">word_tfidf[word_tfidf &lt; 0.2] = 0</span><span id="834f" class="lu lv jj na b gy ni nf l ng nh">df_tfidf = pd.DataFrame(data = {'word': count_vectorizer.get_feature_names(), <br/>                            'tf': word_counts.sum(axis = 0), <br/>                            'idf': tfidf_vectorizer.idf_,<br/>                            'tfidf': word_tfidf.sum(axis = 0)})</span><span id="57cc" class="lu lv jj na b gy ni nf l ng nh">df_tfidf.sort_values(["tfidf", 'tf', 'idf'], ascending = False, inplace = True)<br/>    df_tfidf.reset_index(drop = True, inplace = True)<br/>    <br/>    <br/>    <br/>    <br/>    ax.scatter(df_tfidf.tfidf[:3], df_tfidf.idf[:3], s = df_tfidf.tf * 0.14, <br/>            cmap = "Blues", alpha = 0.7, edgecolors = "black", linewidths = 1.2, color = color, label = category)</span><span id="3d01" class="lu lv jj na b gy ni nf l ng nh">for index, text in enumerate(df_tfidf.word[:3].tolist()):<br/>        ax.annotate(text, (df_tfidf.tfidf.tolist()[index], df_tfidf.idf.tolist()[index]), <br/>                    (df_tfidf.tfidf.tolist()[index] + 0, df_tfidf.idf.tolist()[index] + 0.11))<br/>        <br/>box = ax.get_position()<br/>ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])<br/>ax.legend(title = "Category", frameon = True, ncol = 2, fancybox = True, title_fontsize = 15,<br/>          loc = 'center left', bbox_to_anchor = (1, 0.5), labelspacing = 2.5, borderpad = 2);</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/e637e9f0764976c0e6bdbd76bda9c363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcEv2Nl-VXG-I9yapMdaTA.png"/></div></div></figure><p id="b122" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">TF-IDF分析是NLP分析中的一个强大工具，并且在我们的场景中是有用的，例如，对于司法判决中的文本挖掘和搜索引擎，它是给定用户查询对文档的相关性进行评分和排序的中心工具</p><p id="39ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，我们将继续进行文本分析和潜在主题分析，敬请关注！:)</p><h1 id="bf8c" class="nn lv jj bd lw no np nq lz nr ns nt mc kp nu kq mf ks nv kt mi kv nw kw ml nx bi translated">参考</h1><ul class=""><li id="671b" class="ny nz jj la b lb mn le mo lh oa ll ob lp oc lt od oe of og bi translated"><a class="ae jg" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK::自然语言工具包</a></li><li id="16bc" class="ny nz jj la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">Python中的spaCy工业级自然语言处理</li><li id="9e7c" class="ny nz jj la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated"><a class="ae jg" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="0650" class="ny nz jj la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated"><a class="ae jg" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部2009-2018年新闻发布| Kaggle </a></li></ul></div></div>    
</body>
</html>