<html>
<head>
<title>Hosting Models with TF Serving on Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Docker上提供TF服务的托管模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hosting-models-with-tf-serving-on-docker-aceff9fbf533#2022-06-08">https://towardsdatascience.com/hosting-models-with-tf-serving-on-docker-aceff9fbf533#2022-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a594" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将张量流模型部署为REST端点</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/161fc0fb0491ff8e174cc554a8aa9835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kauT3Kt3TGt-U8P3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/6Dv3pe-JnSg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="8bc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练机器学习(ML)模型只是ML生命周期中的一步。如果你不能从你的模型中得到回应，那么ML就没有任何意义。你必须能够主持你的训练模型进行<a class="ae ky" href="https://hazelcast.com/glossary/machine-learning-inference/" rel="noopener ugc nofollow" target="_blank">推理</a>。有多种托管/部署选项可用于ML，其中最受欢迎的是<a class="ae ky" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"> TensorFlow Serving </a>。</p><p id="456a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow服务有助于获取您的已训练模型的工件，并托管它以进行推理。使用TensorFlow Serving进行推理的最简单方法是<a class="ae ky" href="https://www.docker.com/" rel="noopener ugc nofollow" target="_blank"> Docker </a>。在本文中，我们将举一个例子，训练一个模型，然后使用TensorFlow Serving结合Docker托管它，以提供一个用于推理的REST API。如果你想直接跳到代码，看看这个<a class="ae ky" href="https://github.com/RamVegiraju/TF-Serving-Demo" rel="noopener ugc nofollow" target="_blank">库</a>。</p><h2 id="c495" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">目录</h2><ol class=""><li id="03bf" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated">先决条件/设置</li><li id="f873" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">张量流服务架构</li><li id="d5b9" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">培训和保存您的模型</li><li id="65e3" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">使用TensorFlow Serving &amp; Docker托管您的模型</li><li id="9432" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">学分+额外资源和结论</li></ol><h2 id="5351" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">先决条件/设置</h2><p id="5535" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在这篇文章中，我们不会深入任何实际的模型构建。我们将采用流行的<a class="ae ky" href="https://www.kaggle.com/code/prasadperera/the-boston-housing-dataset/data" rel="noopener ugc nofollow" target="_blank">波士顿数据集</a>并训练简单的<a class="ae ky" href="https://www.sciencedirect.com/topics/earth-and-planetary-sciences/artificial-neural-network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。本文更多的是围绕托管ML模型的基础设施，而不是优化模型性能本身。</p><p id="6581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确保<a class="ae ky" href="https://docs.docker.com/get-docker/" rel="noopener ugc nofollow" target="_blank"> Docker已安装</a>并正常运行。要完全理解这篇文章，请确保对<a class="ae ky" href="https://docs.docker.com/engine/reference/commandline/docker/" rel="noopener ugc nofollow" target="_blank"> Docker命令</a>以及Python有一个基本的了解。</p><p id="5edf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>:该数据集最初由Harrison，d .和Rubinfeld，D.L .出版并授权<a class="ae ky" href="https://www.sciencedirect.com/science/article/abs/pii/0095069678900062" rel="noopener ugc nofollow" target="_blank">《享乐价格和对清洁空气的需求》，J. Environ。经济学&amp;管理学</a>。</p><h2 id="6a09" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">张量流服务架构</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/c1f72263cfccc2c35208e18566ebd541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ITqND4lJsZxHqs7RhMbQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图(来源:<a class="ae ky" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture.md#:~:text=following%20key%20concepts%3A-,Servables,of%20a%20Servable%20is%20flexible." rel="noopener ugc nofollow" target="_blank"> TensorFlow Github </a>)</p></figure><p id="66b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解TensorFlow Serving背后的要点是客户端用来执行推理的<a class="ae ky" href="https://www.tensorflow.org/tfx/serving/architecture" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">可服务的</strong> </a>对象。通常，这个Servable是您将在训练您的模型之后创建的<a class="ae ky" href="https://www.tensorflow.org/guide/saved_model" rel="noopener ugc nofollow" target="_blank"> SavedModelBundle </a>。每个Servable都有一个相关的版本，这可以在保存模型时指定。如果你愿意的话，你还可以用它同时为多个不同的型号服务。</p><p id="ca37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">流程大致如下-</p><ul class=""><li id="0731" class="mo mp it lb b lc ld lf lg li ni lm nj lq nk lu nl mw mx my bi translated"><a class="ae ky" href="https://www.tensorflow.org/tfx/serving/architecture#sources" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Source </strong> </a>:从提供的文件路径加载模型，这是可服务对象。我们将在启动容器时指定这个路径。</li><li id="4b71" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nl mw mx my bi translated"><a class="ae ky" href="https://www.tensorflow.org/tfx/serving/architecture#loaders" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Loader </strong> </a>:从源代码中获取一个新模型，包含加载和卸载servable的功能，根据新模型创建一个期望的版本。</li><li id="3fc4" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nl mw mx my bi translated"><a class="ae ky" href="https://www.tensorflow.org/tfx/serving/architecture#managers" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">模型管理器</strong> </a>:维护模型的生命周期，这意味着对模型版本/元数据的任何更新。如果必要的资源可用(例如:特定版本的元数据存在或不存在)，管理器实质上监听源并满足请求。</li><li id="b21e" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu nl mw mx my bi translated"><a class="ae ky" href="https://www.tensorflow.org/tfx/serving/architecture#servables" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">servable handle</strong></a>:这是与客户端代码的对话点，这里公开了合适的API(<a class="ae ky" rel="noopener" target="_blank" href="/serving-ml-models-with-grpc-2116cf8374dd#:~:text=gRPC%20takes%20full%20advantage%20of,10%20times%20faster%20than%20REST.">REST或者gRPC </a>)。</li></ul><p id="3d95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要全面深入了解TensorFlow服务架构，请务必阅读官方文档<a class="ae ky" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture.md#:~:text=following%20key%20concepts%3A-,Servables,of%20a%20Servable%20is%20flexible." rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="f396" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">培训和保存您的模型</h2><p id="023c" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们将首先在波士顿住房数据集上训练一个简单的模型。我们在一个train.py脚本中提供了所有这些，确保安装了以下导入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训导入</p></figure><p id="f98b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以直接从Sklearn加载波士顿数据集，拆分数据集进行训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加载波士顿数据集</p></figure><p id="7666" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们创建我们的神经网络并训练我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模特培训</p></figure><p id="a031" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">脚本的最后一行是正确服务您的模型的关键。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">保存TF模型</p></figure><p id="3d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“波士顿模型”将是<strong class="lb iu">模型名称</strong>和目录，我们将在其中指向我们训练过的模型工件。创建的子目录‘0000001’是包含您的<strong class="lb iu">模型元数据</strong>的目录:assets、variables、keras_metadata.pb和saved_model.pb。确保运行该脚本，并确保在执行后创建以下目录，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/1f20836de103a62da7f2872be51ed416.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*mKHyl6K3poecP0SWgy_atA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型数据(作者截图)</p></figure><h2 id="2a87" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用TensorFlow Serving &amp; Docker托管您的模型</h2><p id="9e07" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">确保Docker启动并运行。我们要做的第一件事是使用以下命令提取TensorFlow服务Docker映像。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拉张量流服务图像</p></figure><p id="cd08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在您提取图像之后，我们需要启动容器，对于REST API，我们需要公开端口8501。请注意，<a class="ae ky" rel="noopener" target="_blank" href="/image-classification-on-tensorflow-serving-with-grpc-or-rest-call-for-inference-fd3216ebd4f3"> gRPC </a>也是受支持的，对于这个expose端口8500。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">启动容器</p></figure><p id="2d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您运行下面的Docker命令，您应该会看到您添加的容器名称。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/93315465f17d378a988149f2f303c095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXgSHV_lBU586BrO_ImY0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">tfserving_boston Docker容器正在运行</p></figure><p id="acc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确保在源命令中添加项目所在位置的路径。“MODEL_NAME”环境变量是保存模型的目录，这是保存模型最新版本的地方。请注意，在运行容器时，您还可以在这里注入容器可能使用的其他环境变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/df26a9a6872f0ed61b650eaada5e34e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpBDeo_rnNsQlAhmhySqmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">剩余端点暴露</p></figure><p id="a15d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了一个REST端点，我们可以使用一个<a class="ae ky" href="https://phoenixnap.com/kb/curl-command#:~:text=apt%20install%20curl-,What%20Is%20the%20curl%20Command%3F,to%20be%20sent%20or%20received." rel="noopener ugc nofollow" target="_blank"> Curl命令</a>发送一个示例调用。对于单个数据点，运行以下命令。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单次调用</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8d72d8837168e8a533002f7797939cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*ZBeFeR9Ih0Cvxii-G6wJ6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果(作者截图)</p></figure><p id="e806" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想要发送多次调用，您也可以创建一个示例shell脚本来迭代有效负载。我们可以将样本数据点打包到一个JSON中，并将其提供给我们的端点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">样本数据点</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迭代有效载荷</p></figure><p id="8363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下命令执行shell脚本，并看到10个返回的结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="2fd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更有创造性或者更大规模地加载测试，你也可以使用像Python <a class="ae ky" href="https://github.com/RamVegiraju/TF-Serving-Demo/blob/master/predict.py" rel="noopener ugc nofollow" target="_blank"> requests </a>库这样的包。</p><h2 id="7df2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论</h2><div class="ns nt gp gr nu nv"><a href="https://github.com/RamVegiraju/TF-Serving-Demo" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">GitHub-RamVegiraju/TF-Serving-Demo:用Docker进行TF服务的推理示例</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">docker pull tensor flow/serving docker run-p 8501:8501-name TF serving _ Boston \-mount type = bind，source=/{path to…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">github.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj ks nv"/></div></div></a></div><p id="1f21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关本示例的完整代码，请访问上面的链接。我希望这篇文章是对TensorFlow服务的良好介绍。对于数据科学家来说，理解机器学习的基础设施和托管方面变得比以往任何时候都更加重要。有很多主机选项，我在下面附上了我一直在做的SageMaker系列，以及一些其他资源，当我开始使用模型主机和推理时，这些资源也帮助了我。</p><h2 id="5442" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">信用/额外资源</h2><p id="a47c" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><a class="ae ky" href="https://neptune.ai/blog/how-to-serve-machine-learning-models-with-tensorflow-serving-and-docker" rel="noopener ugc nofollow" target="_blank">带TF上菜和Docker的上菜模特</a></p><p id="aa6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://ram-vegiraju.medium.com/list/amazon-sagemaker-f1b06f720fba" rel="noopener"> SageMaker系列</a></p><p id="b70b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html" rel="noopener ugc nofollow" target="_blank">与亚马逊SageMaker合作的TF</a></p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="0e86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="or">如果你喜欢这篇文章，请在</em><a class="ae ky" href="https://www.linkedin.com/in/ram-vegiraju-81272b162/" rel="noopener ugc nofollow" target="_blank"><em class="or">LinkedIn</em></a><em class="or">上与我联系，并订阅我的媒体</em> <a class="ae ky" href="https://ram-vegiraju.medium.com/subscribe" rel="noopener"> <em class="or">简讯</em> </a> <em class="or">。如果你是新来的中号，用我的</em> <a class="ae ky" href="https://ram-vegiraju.medium.com/membership" rel="noopener"> <em class="or">会员推荐</em> </a> <em class="or">报名吧。</em></p></div></div>    
</body>
</html>