<html>
<head>
<title>Keyphrase Extraction with BERT Transformers and Noun Phrases</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT变换器和名词短语提取关键短语</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db#2022-02-07">https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db#2022-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="70ec" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用名词短语预处理增强基于BERT的关键词抽取</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b3ce34e064c854042ca1e1acf04f1ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SevZ61QnLIxrvtYNZ4DenQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://unsplash.com/@amadorloureiro" rel="noopener ugc nofollow" target="_blank">阿玛多·洛雷罗</a>在<a class="ae kv" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="4708" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">这篇文章基于我们的论文</em> <a class="ae kv" href="https://arxiv.org/abs/2210.05245" rel="noopener ugc nofollow" target="_blank"> <em class="ls">“模式排序:利用预训练的语言模型和词性进行无监督的关键短语提取(2022)”</em></a><em class="ls">。你可以在那里或者在我们的</em> <a class="ae kv" rel="noopener" target="_blank" href="/unsupervised-keyphrase-extraction-with-patternrank-28ec3ca737f0"> <em class="ls"> PatternRank博客文章</em> </a> <em class="ls">中阅读关于我们方法的更多细节。</em></p><p id="a2b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要快速浏览文本内容，提取能简明反映其语义上下文的关键词会很有帮助。虽然常用的术语是关键字，但我们通常实际上想要<strong class="ky ir">关键短语</strong>来实现这个目的。</p><blockquote class="lt lu lv"><p id="7623" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">关键词或关键短语都应该描述文章的本质。两者的区别在于，关键词是单个单词，而关键短语是由几个单词组成的。例如“小狗”vs“小狗服从训练”。——<a class="ae kv" href="https://yoast.com/difference-between-keyword-and-keyphrase/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">艾里斯·盖伦</strong> </a></p></blockquote><p id="c7ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关键短语比简单的关键字提供了更准确的描述，因此通常是首选。幸运的是，许多开源解决方案允许我们从文本中自动提取关键短语。最近非常流行的解决方案之一是<a class="ae kv" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> KeyBERT </strong> </a>。这是一个易于使用的Python包，通过<a class="ae kv" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> BERT语言模型</a>提取关键短语。简单地说，KeyBERT首先创建文档文本的BERT嵌入。然后，创建具有预定义长度的单词n元文法的BERT关键短语嵌入。最后，计算文档和关键短语嵌入之间的余弦相似度，以提取最佳描述整个文档的关键短语。关于KeyBERT更详细的介绍可以在<a class="ae kv" rel="noopener" target="_blank" href="/how-to-extract-relevant-keywords-with-keybert-6e7b3cf889ae">这里</a>找到。</p><h1 id="f623" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">为什么需要增强KeyBERT结果</h1><p id="638c" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">尽管KeyBERT能够自己提取好的关键短语，但实际上仍然存在两个问题。这是由KeyBERT在嵌入步骤之前从文档中提取关键短语的方式造成的。用户需要预定义一个<em class="ls">单词n元语法范围</em>来指定提取的关键短语的长度。然后，KeyBERT从文档中提取定义长度的简单单词n-grams，并将它们用作嵌入创建和相似性计算的候选关键短语。</p><blockquote class="lt lu lv"><p id="d282" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">单词n元语法范围让用户决定应该从给定文本中提取的连续单词序列的长度。假设我们定义了一个<code class="fe mw mx my mz b">word n-gram range = (1,3)</code> <em class="iq">。然后，我们将选择从文本中提取一元词(只有一个单词)、二元词(两个连续单词的组合)和三元词(三个连续单词的组合)。将单词n-gram range应用于<code class="fe mw mx my mz b">"an apple a day keeps the doctor away"</code>将导致<code class="fe mw mx my mz b">["an", "apple", "a","day", "keeps", "the", "doctor", "away", "an apple", "apple a", "a day", "day keeps", "keeps the", "the doctor", "doctor away", "an apple", "apple a day", "a day keeps", "day keeps the", "keeps the doctor", "the doctor away"]</code>。- <a class="ae kv" href="https://www.quora.com/What-is-the-n-gram-range" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">德维什·帕尔马</strong> </a></em></p></blockquote><p id="30d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，用户通常不知道最佳的n-gram范围，因此必须花费一些时间进行试验，直到他们找到合适的n-gram范围。此外，这意味着根本不考虑语法句子结构。这导致这样的效果，即使在找到一个好的n元语法范围之后，返回的关键短语有时仍然在语法上不太正确或者稍微跑调。继续上面的例子，如果KeyBERT从候选关键短语集合中识别出最重要的关键短语<em class="ls">、</em>或<em class="ls">、</em>。</p><h1 id="fe47" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">如何用关键短语向量增强KeyBERT结果</h1><p id="b68a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">为了解决上面提到的问题，我们可以将<a class="ae kv" href="https://github.com/TimSchopf/KeyphraseVectorizers" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">keyphrasvectors</strong></a>包与KeyBERT一起使用。KeyphraseVectorizers包从一组文本文档中提取具有词性模式的关键短语，并将它们转换成文档-关键短语矩阵。文档关键短语矩阵是描述关键短语在文档集合中出现的频率的数学矩阵。</p><h2 id="14cf" class="na ma iq bd mb nb nc dn mf nd ne dp mj lf nf ng ml lj nh ni mn ln nj nk mp nl bi translated">KeyphraseVectorizer包是如何工作的？</h2><p id="4fde" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，文档文本用<a class="ae kv" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">空间</a>词性标签标注。其次，从词性标签与预定义的正则表达式模式匹配的文档文本中提取关键短语。默认情况下，矢量器提取包含零个或多个形容词的关键短语，后跟一个或多个使用英语空间词性标签的名词。最后，矢量器计算文档关键短语矩阵。除了矩阵之外，这个软件包还可以为我们提供通过词性提取的关键短语。</p><p id="801f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">举例:</strong></p><p id="763a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用下面的命令来安装KeyphraseVectorizers包:<code class="fe mw mx my mz b">pip install keyphrase-vectorizers</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="852e" class="na ma iq mz b gy ns nt l nu nv">{'binary': False, 'dtype': &lt;class 'numpy.int64'&gt;, 'lowercase': True, 'max_df': None, 'min_df': None, 'pos_pattern': '&lt;J.*&gt;*&lt;N.*&gt;+', 'spacy_pipeline': 'en_core_web_sm', 'stop_words': None, 'workers': 1}</span></pre><p id="58f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认情况下，矢量器针对英语进行初始化。这意味着，指定了一个英语<code class="fe mw mx my mz b">spacy_pipeline</code>，没有删除任何<code class="fe mw mx my mz b">stop_words</code>，并且<code class="fe mw mx my mz b">pos_pattern</code>提取具有零个或多个形容词的关键字，后跟一个或多个使用英语spaCy词性标签的名词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="cdef" class="na ma iq mz b gy ns nt l nu nv"><br/>[[0 0 0 0 1 3 2 1 1 0 1 1 3 1 0 0 0 0 1 0 1 1 1 0 1 0 2 0 1 1 1 0 1 1 0 0 0 1 1 3 3 0 1 3 3]<br/> [1 1 5 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 0 0 2 0 1 0 1 0 0 0 2 0 0 1 1 1 0 0 0 0 5 0 0 0]]</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="4085" class="na ma iq mz b gy ns nt l nu nv">['users' 'main topics' 'learning algorithm' 'overlap' 'documents' 'output' 'keywords' 'precise summary' 'new examples' 'training data' 'input' 'document content' 'training examples' 'unseen instances' 'optimal scenario' 'document' 'task' 'supervised learning algorithm' 'example' 'interest' 'function' 'example input' 'various applications' 'unseen situations' 'phrases' 'indication' 'inductive bias' 'supervisory signal' 'document relevance' 'information retrieval' 'set' 'input object' 'groups' 'output value' 'list' 'learning' 'output pairs' 'pair' 'class labels' 'supervised learning' 'machine' 'information retrieval environment' 'algorithm' 'vector' 'way']</span></pre><p id="44d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矢量器的输出显示，与简单的n元语法不同，提取的单词语法正确，有意义。这是矢量器提取名词短语和扩展名词短语的结果。</p><blockquote class="lt lu lv"><p id="4d14" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">名词短语是围绕一个名词构建的简单短语。它包含一个限定词和一个名词。例如:一棵树，一些糖果，城堡。一个<strong class="ky ir">扩展名词短语</strong>通过添加一个或多个<strong class="ky ir">形容词来为名词添加更多细节。形容词是描述名词的词。例如:一棵<em class="iq">巨大的</em>树，一些<em class="iq">五彩缤纷的</em>糖果，那些<em class="iq">巨大的、皇家的</em>城堡。英国广播公司</strong></p></blockquote><h2 id="4415" class="na ma iq bd mb nb nc dn mf nd ne dp mj lf nf ng ml lj nh ni mn ln nj nk mp nl bi translated">KeyBERT如何使用KeyphraseVectorizers？</h2><p id="85a2" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">关键短语矢量器可以与KeyBERT一起使用，以提取与文档最相似的语法正确的关键短语。因此，矢量器首先从文本文档中提取候选关键短语，随后由KeyBERT基于它们的文档相似性对其进行排序。然后，前n个最相似的关键短语可以被认为是文档关键词。</p><blockquote class="nw"><p id="d345" class="nx ny iq bd nz oa ob oc od oe of lr dk translated">除了KeyBERT之外，使用KeyphraseVectorizers的优点是，它允许用户获得语法正确的关键短语，而不是简单的预定义长度的n元语法。</p></blockquote><p id="b697" class="pw-post-body-paragraph kw kx iq ky b kz og jr lb lc oh ju le lf oi lh li lj oj ll lm ln ok lp lq lr ij bi translated">关键短语分类器首先提取由零个或多个形容词组成的候选关键短语，然后在预处理步骤中提取一个或多个名词，而不是简单的n元语法。<a class="ae kv" href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" rel="noopener ugc nofollow" target="_blank"> TextRank </a>、<a class="ae kv" href="https://aclanthology.org/C08-1122.pdf" rel="noopener ugc nofollow" target="_blank"> SingleRank </a>和<a class="ae kv" href="https://aclanthology.org/K18-1022.pdf" rel="noopener ugc nofollow" target="_blank">embe beed</a>已经成功使用这种名词短语方法进行关键短语提取。提取的候选关键短语随后被传递给KeyBERT用于嵌入生成和相似性计算。为了使用这两个包来提取关键短语，我们需要传递给KeyBERT一个带有<code class="fe mw mx my mz b">vectorizer</code>参数的关键短语矢量器。由于关键短语的长度现在取决于词性标签，因此不再需要定义n元语法长度。</p><h2 id="1198" class="na ma iq bd mb nb nc dn mf nd ne dp mj lf nf ng ml lj nh ni mn ln nj nk mp nl bi translated">示例:</h2><p id="a7be" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">KeyBERT可以通过<code class="fe mw mx my mz b">pip install keybert</code>安装。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="5d4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不是决定合适的n元语法范围，例如(1，2)…</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="3b66" class="na ma iq mz b gy ns nt l nu nv">[[('labeled training', 0.6013),<br/>  ('examples supervised', 0.6112),<br/>  ('signal supervised', 0.6152),<br/>  ('supervised', 0.6676),<br/>  ('supervised learning', 0.6779)],<br/> [('keywords assigned', 0.6354),<br/>  ('keywords used', 0.6373),<br/>  ('list keywords', 0.6375),<br/>  ('keywords quickly', 0.6376),<br/>  ('keywords defined', 0.6997)]]</span></pre><p id="72c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以让关键短语向量器决定合适的关键短语，而没有最大或最小n元语法范围的限制。我们只需将一个关键短语矢量器作为参数传递给KeyBERT:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="6f53" class="na ma iq mz b gy ns nt l nu nv">[[('learning', 0.4813), <br/>  ('training data', 0.5271), <br/>  ('learning algorithm', 0.5632), <br/>  ('supervised learning', 0.6779), <br/>  ('supervised learning algorithm', 0.6992)], <br/> [('document content', 0.3988), <br/>  ('information retrieval environment', 0.5166), <br/>  ('information retrieval', 0.5792), <br/>  ('keywords', 0.6046), <br/>  ('document relevance', 0.633)]]</span></pre><p id="755b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这使我们能够确保我们不会因为将n元语法范围定义得太短而删除重要的单词。例如，我们可能找不到带有<code class="fe mw mx my mz b">keyphrase_ngram_range=(1,2)</code>的关键词<em class="ls">“监督学习算法”</em>。此外，我们避免获得稍微跑调的关键短语，如<em class="ls">、</em>、<em class="ls">、【信号监控】、</em>或<em class="ls">、【快速关键词】、</em>。</p><p id="a0cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">提取英语以外的语言中的关键短语:</strong></p><p id="0e0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们还可以将这种方法应用于其他语言，如德语。这只需要对<a class="ae kv" href="https://github.com/TimSchopf/KeyphraseVectorizers" rel="noopener ugc nofollow" target="_blank">键相器</a>和<a class="ae kv" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank">键齿</a>的一些参数进行修改。</p><p id="0dac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于关键相分离器，<code class="fe mw mx my mz b">spacy_pipeline</code>和<code class="fe mw mx my mz b">stop_words</code>参数需要修改为<code class="fe mw mx my mz b">spacy_pipeline=’de_core_new_sm’</code>和<code class="fe mw mx my mz b">stop_words=’german’</code>。因为德语spaCy词性标签与英语不同，<code class="fe mw mx my mz b">pos_pattern</code>参数也需要修改。regex模式<code class="fe mw mx my mz b">&lt;ADJ.*&gt;*&lt;N.*&gt;+</code>提取包含零个或多个形容词的关键字，后跟一个或多个使用德语spaCy词性标签的名词。</p><p id="bb3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于KeyBERT，需要通过<code class="fe mw mx my mz b">pip install flair</code>安装Flair包，并且必须选择德国BERT型号。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div></figure><pre class="kg kh ki kj gt no mz np nq aw nr bi"><span id="957b" class="na ma iq mz b gy ns nt l nu nv">[[('schwester cornelia', 0.2491),<br/>  ('neigung', 0.2996),<br/>  ('angesehenen bürgerlichen familie', 0.3131),<br/>  ('ausbildung', 0.3651),<br/>  ('straßburg', 0.4022)],<br/> [('tochter', 0.0821),<br/>  ('friedrich schiller', 0.0912),<br/>  ('ehefrau elisabetha dorothea schiller', 0.0919),<br/>  ('neckar johann kaspar schiller', 0.092),<br/>  ('wundarztes', 0.1334)]]</span></pre><h1 id="678b" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">摘要</h1><p id="6d06" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/TimSchopf/KeyphraseVectorizers" rel="noopener ugc nofollow" target="_blank">keyphrasvectors</a>是最近发布的一个包，除了<a class="ae kv" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank"> KeyBERT </a>之外，它还可以用来从文本文档中提取增强的关键短语。这种方法消除了对用户定义的单词n元语法范围的需要，并提取语法正确的关键短语。此外，该方法可以应用于许多不同的语言。这两个开源包都易于使用，只需几行代码就可以精确提取关键短语。</p><p id="7f0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也非常感谢<a class="ae kv" href="https://www.maartengrootendorst.com" rel="noopener ugc nofollow" target="_blank"> Maarten Grootendorst </a>，他在我编写<a class="ae kv" href="https://github.com/TimSchopf/KeyphraseVectorizers" rel="noopener ugc nofollow" target="_blank">keyphrasevectors</a>包时给了我输入和灵感。</p><h1 id="26be" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">来源</h1><div class="ol om gp gr on oo"><a href="https://github.com/TimSchopf/KeyphraseVectorizers" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">GitHub-TimSchopf/keyphrasevectors:一组向量器，用…</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一组向量器，从一组文本文档中提取带有词性模式的关键短语，并转换…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://github.com/MaartenGr/KeyBERT" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">GitHub - MaartenGr/KeyBERT:用BERT提取最少的关键字</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">KeyBERT是一种简单易用的关键字提取技术，它利用BERT嵌入来创建关键字和…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc kp oo"/></div></div></a></div><div class="ol om gp gr on oo"><a href="https://arxiv.org/abs/2210.05245" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">PatternRank:利用预训练的语言模型和无监督关键短语的词性…</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">关键短语提取是从给定文本中自动选择一小组最相关短语的过程…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">arxiv.org</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc kp oo"/></div></div></a></div><p id="afe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">米哈尔恰和塔劳(2004年)。<a class="ae kv" href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" rel="noopener ugc nofollow" target="_blank"> TextRank:将or- <br/> der带入文本。</a>2004年自然语言处理经验方法会议论文集- <br/> ing，第404–411页，西班牙巴塞罗那。计算语言学协会。</p><p id="6cbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">万，谢，肖(2008)。<a class="ae kv" href="https://aclanthology.org/C08-1122.pdf" rel="noopener ugc nofollow" target="_blank"> CollabRank:实现一种<br/>协作方法来提取单个文档的关键短语<br/>。</a>《第22届国际计算语言学会议(T4)论文集》(2008年出版)，第969-976页，英国曼彻斯特。科林2008组委会。</p><p id="3b7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本纳尼-斯米尔，k .，穆萨特，c .，霍斯曼，a .，贝里斯维尔，<br/> M .，贾吉，M. (2018)。<a class="ae kv" href="https://aclanthology.org/K18-1022.pdf" rel="noopener ugc nofollow" target="_blank">使用句子嵌入的简单无监督<br/>关键短语提取。</a>第22届计算机自然语言学习会议论文集，第221-229页，<br/>比利时布鲁塞尔。计算语言学协会。</p></div></div>    
</body>
</html>