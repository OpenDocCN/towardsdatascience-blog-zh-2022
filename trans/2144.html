<html>
<head>
<title>Random Seeds and Reproducibility</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机种子和再现性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-seeds-and-reproducibility-933da79446e3#2022-05-13">https://towardsdatascience.com/random-seeds-and-reproducibility-933da79446e3#2022-05-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Python、Numpy和PyTorch中设置您的实验</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cb8357bf4d86c93456bc6a4c3ecc7813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UpBgdkaz7SOwi7Z4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯蒂安·朱德雷在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="94ec" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">动机</h1><blockquote class="lr ls lt"><p id="8cc4" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“程序员最可怕的噩梦是什么？”</p></blockquote><p id="be8e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">就我而言，我可以有把握地说，作为一名程序员，我最糟糕的噩梦是一段<strong class="lx iu">代码，它的行为就好像是随机的</strong>，每次运行它都会产生<strong class="lx iu">不同的结果，即使我给它<strong class="lx iu">完全相同的输入</strong>！</strong></p><p id="4e55" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">实际上有一个著名的关于精神错乱的定义:</p><blockquote class="mu"><p id="1c7d" class="mv mw it bd mx my mz na nb nc nd mq dk translated"><strong class="ak">“疯狂就是一遍又一遍地做同样的事情，却期待不同的结果。”</strong></p></blockquote><p id="8606" class="pw-post-body-paragraph lu lv it lx b ly ne ju ma mb nf jx md mr ng mg mh ms nh mk ml mt ni mo mp mq im bi translated">尽管人们通常认为这是阿尔伯特·爱因斯坦的功劳，但研究表明事实并非如此。但是，撇开引用的作者不谈，事实仍然是:一遍又一遍地给一段代码输入相同的输入，每次都得到不同的结果，会让你发疯的:-)</p><blockquote class="lr ls lt"><p id="02a4" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">本帖包含部分转载自我的书内容:<strong class="lx iu"/><a class="ae ky" href="https://pytorchstepbystep.com" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">深度学习用PyTorch循序渐进:初学者指南</strong> </a> <strong class="lx iu">。</strong></p></blockquote><h1 id="c37d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">(伪)随机数</h1><blockquote class="lr ls lt"><p id="8ad2" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">"一个人怎么可能调试和修复这样的东西？"</p></blockquote><p id="540f" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">幸运的是，对于我们程序员来说，我们不必处理真正的随机性，而是处理伪随机性。</p><blockquote class="lr ls lt"><p id="f961" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“什么意思？”</p></blockquote><p id="afa8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">嗯，你知道，随机数并不完全是随机的……它们实际上是伪随机的，也就是说，一个数字生成器抛出一系列数字，看起来像是随机的。但是是<strong class="lx iu">不是</strong>，真的。</p><p id="4cf1" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这种行为的好处是我们可以<strong class="lx iu">告诉生成器启动一个特定的伪随机数序列</strong>。在某种程度上，它的工作原理就好像我们告诉生成器:“<em class="lw">请生成序列#42，</em>”，它就会溢出一个数字序列。</p><p id="016a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这个数字42的作用类似于序列的<strong class="lx iu">索引，被称为<strong class="lx iu">种子</strong>。每次我们给它<strong class="lx iu">相同的种子</strong>，它都会生成<strong class="lx iu">相同的数字</strong>。</strong></p><blockquote class="mu"><p id="f6bf" class="mv mw it bd mx my mz na nb nc nd mq dk translated">"同样的老种子，同样的老号码。"</p></blockquote><p id="f3ff" class="pw-post-body-paragraph lu lv it lx b ly ne ju ma mb nf jx md mr ng mg mh ms nh mk ml mt ni mo mp mq im bi translated">这意味着我们有两个世界最好的<strong class="lx iu"/>:一方面，我们做<strong class="lx iu">产生</strong>一个数字序列，对于所有意图和目的来说，<strong class="lx iu">被认为是随机的</strong>；另一方面，我们有能力复制任何给定的序列。我确信您能够理解这对于调试目的和避免精神错乱是多么方便:-)。</p><p id="76fb" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">此外，你可以保证其他人能够复制你的结果。想象一下，运行从博客帖子或书籍中获得的代码，每次都得到不同的输出，不得不怀疑它是否有问题，这是多么令人讨厌的事情。</p><p id="c476" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">在学习一个新主题时，您最不需要的就是失去平衡，因为每次运行一些代码时，您都会得到不同的结果(除了有一个种子集之外，代码很可能完全正确)。但是，通过正确设置随机种子，您和我，以及运行代码的每个人，都可以获得完全相同的输出，即使这涉及到生成随机数据！</p><h1 id="d280" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">生成随机数</h1><p id="43ee" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">虽然种子叫做<em class="lw">随机</em>，但是它的<em class="lw">选择</em>肯定不是！通常，你会看到选择的随机种子是<a class="ae ky" href="https://bit.ly/2XZXjnk" rel="noopener ugc nofollow" target="_blank"> 42 </a>，这是一个人可能选择的所有随机种子中<a class="ae ky" href="https://bit.ly/3fjCSHR" rel="noopener ugc nofollow" target="_blank">(第二个)最不随机的</a>。</p><p id="0932" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">因此，我们也在本帖中向将种子设定为42的悠久传统致敬。在纯Python中，您使用<code class="fe no np nq nr b"><a class="ae ky" href="https://docs.python.org/3/library/random.html#random.seed" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">random.seed()</strong></a></code>来设置种子，然后您可以使用<code class="fe no np nq nr b"><a class="ae ky" href="https://docs.python.org/3/library/random.html#random.randint" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">random.randint()</strong></a></code>来绘制一个随机整数，例如:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c68bda5175257773cac6146e6bc033b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*UlOmpAB_wBzUTj4W_arZnQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">设定种子后抽取四个随机整数。图片作者。</p></figure><p id="e474" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">看到了吗？<em class="lw">完全决定论</em>！一旦你将随机种子设置为42(显然！)，生成的前四个整数依次是10、1、0和4，无论您是一个接一个地生成它们，还是在一个列表中理解它们。</p><blockquote class="lr ls lt"><p id="5c2a" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">如果你对生成本身感到好奇，Python的random模块使用了<strong class="lx iu"> Mersenne Twister随机数生成器</strong>，这是一个<strong class="lx iu">完全确定性算法</strong>。这意味着该算法对于解决再现性问题来说很棒，但是完全不适合用于加密目的。</p></blockquote><p id="3c69" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">数字<strong class="lx iu">发生器</strong>有一个<strong class="lx iu">内部状态</strong>，它跟踪从特定序列中提取的最后一个元素(每个序列由其对应的种子标识)，因此它知道从哪里挑选下一个元素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7ad8603f500e07de485eaa3bd7f68891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*Ij7zs9XlZOSQcDKC9N5bTg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">种子#42的状态序列和相应的生成号。图片作者。</p></figure><p id="5f7e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">如果您愿意，可以使用<code class="fe no np nq nr b"><a class="ae ky" href="https://docs.python.org/3/library/random.html#random.getstate" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">random.getstate()</strong></a></code>和<code class="fe no np nq nr b"><a class="ae ky" href="https://docs.python.org/3/library/random.html#random.setstate" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">random.setstate()</strong></a></code>来检索(和设置)该状态:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1c5ae7a4435000cbda1ae60c717334d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*p6xFfAv-47egcaKUInrwaA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">让生成器“忘记”抽取了一个数字！图片作者。</p></figure><p id="497f" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">不出所料，第一个数字还是10(因为我们使用了同一个种子)。此时，发生器的内部状态记录了从序列中只抽取了一个数字。我们将这个州命名为<code class="fe no np nq nr b"><strong class="lx iu">first_state</strong></code>。</p><p id="28c8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">所以我们又画了一个，我们得到了，不出所料，数字1。内部状态会相应地更新，但我们会将其设置回绘制第二个数字之前的状态。</p><p id="1c7b" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">现在，如果我们绘制另一个数字，我们将再次得到数字1，因为我们通过更新它的内部状态，迫使生成器“<em class="lw">忘记</em>”最后绘制的数字。</p><p id="1af0" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">这些数字看起来不再是随机的了，嗯？</p><blockquote class="lr ls lt"><p id="6988" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">"是的，但是我必须问…那个州有什么？"</p></blockquote><p id="702e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">很高兴你问了。它只是一个元组！第一个元素是version (3)，第二个元素是625个整数的长列表(内部状态)，最后一个元素通常是<code class="fe no np nq nr b"><strong class="lx iu">None</strong></code>(你可以暂时放心地忽略它)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/726ab78c1b517bcdd9134925541b1116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*NSlcPLe3VgWCEx3iYqq9MQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成器的内部状态(仅显示第一行和最后一行)。图片作者。</p></figure><p id="7779" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">看到最后那个“1”了吗？那是列表的第<em class="lw">625个元素，它作为其他元素</em>的索引<em class="lw">—<strong class="lx iu">实际内部状态由前624个元素</strong>表示。记住这一点，我们将很快回到它！</em></p><blockquote class="lr ls lt"><p id="a140" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">"好吧，那么我们很好，现在一切都完全可以复制了？"</p></blockquote><p id="a080" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们还没有到那一步…如果你查看Python的“<a class="ae ky" href="https://docs.python.org/3/library/random.html#notes-on-reproducibility" rel="noopener ugc nofollow" target="_blank"> <em class="lw">关于再现性的注释</em> </a>”，你会看到:</p><blockquote class="lr ls lt"><p id="e81a" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“通过重用种子值，只要多个线程没有运行，相同的序列就应该可以在不同的运行<strong class="lx iu">之间重现。”</strong></p></blockquote><p id="e4aa" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">所以，如果你是多线程的，再现性就拜拜了！从好的一面来看，Python的(伪)随机数生成器(从现在开始姑且称之为RNG)有两个保证(从“<em class="lw">注释</em>”转录而来):</p><ul class=""><li id="b661" class="nt nu it lx b ly lz mb mc mr nv ms nw mt nx mq ny nz oa ob bi translated">如果添加了新的播种方法，那么将提供向后兼容的播种机。</li><li id="30ed" class="nt nu it lx b ly oc mb od mr oe ms of mt og mq ny nz oa ob bi translated">当兼容的播种机被给予相同的种子时，生成器的<code class="fe no np nq nr b">random()</code>方法将继续产生相同的序列。</li></ul><blockquote class="lr ls lt"><p id="e5e2" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“好了，现在我们好了吗？”</p></blockquote><p id="feb7" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">抱歉，但是不行！Python自己的RNG是<strong class="lx iu">而不是唯一一个</strong>你可能需要设置种子的。</p><h2 id="e572" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">Numpy</h2><p id="ad03" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">如果你也使用Numpy，你需要为它自己的RNG设置一个种子。为此，您可以使用<code class="fe no np nq nr b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">np.random.seed()</strong></a></code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ab6e41adc37beebd5777e9024e22ed9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*SNeL9ZmDxl_Nn62vNI6xIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Numpy中的随机数(传统)。图片作者。</p></figure><p id="6f24" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">从上面的例子可以看出，Numpy的RNG的行为方式与Python的RNG相同:一旦设置了种子，生成器就输出完全相同的数字序列，6、3、7和4。</p><p id="8788" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">虽然上面的代码是最常见的“T7”，而且许多人一直这样使用它(包括我自己，被指控有罪)，但它已经被认为是<strong class="lx iu">遗留的</strong>代码。</p><p id="539c" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">从1.17开始，更近的Numpy版本使用不同的方式生成(伪)随机数:<strong class="lx iu">首先创建一个生成器</strong>，然后<strong class="lx iu">从中抽取数字</strong>。用户可以使用<code class="fe no np nq nr b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.default_rng" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">np.random.default_rng()</strong></a></code>创建默认生成器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6d58a110b4110dd03bd0f05eb13728f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*eVSON0Yh-j89b1xgLI_H7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Numpy中的随机数(最新)。图片作者。</p></figure><blockquote class="lr ls lt"><p id="df80" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“等等，现在数字不一样了吗？”</p></blockquote><p id="62e8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">是的，它们<strong class="lx iu">不同</strong>，尽管我们使用的是<strong class="lx iu">相同的种子</strong>，42。</p><blockquote class="lr ls lt"><p id="47a5" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“那是为什么？”</p></blockquote><p id="86d8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">数字不同是因为<strong class="lx iu">发生器与</strong>不同，也就是说，它使用了<strong class="lx iu">不同的算法</strong>。Numpy的遗留代码使用<em class="lw"> Mersenne Twister (MT) </em>算法，就像Python的random模块一样，而<strong class="lx iu"> Numpy的新默认生成器</strong>使用<em class="lw"> Permute同余生成器(PCG) </em>算法。</p><p id="28ca" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">但是，事实证明，即使Numpy的遗留代码和Python的随机模块使用相同的算法，并且我们在它们两者中使用相同的种子，生成的数字仍然<em class="lw">不同</em>！</p><blockquote class="lr ls lt"><p id="716b" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“你一定是在和我开玩笑！为什么？!"</p></blockquote><p id="c48c" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我理解你可能会感到沮丧，这种差异归结为<em class="lw">Python的随机模块和Numpy处理生成器内部状态</em>中讨厌的“索引”的方式。如果你对这方面的更多细节感兴趣，请查看下面的旁白——否则，请随意跳过它。</p></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h2 id="3845" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">匹配内部状态</h2><p id="7629" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">如果我们使用相同的624个数字的列表来更新两个生成器的状态，同时将“索引”设置为624(就像Numpy默认设置的那样)，这就是我们得到的结果:匹配序列！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/91e5d0e7df6aea4ba282f9da0f8a8e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*SFfbscuAv9rAECsGyYNicQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">匹配Python的random和Numpy的内部状态。图片作者。</p></figure><p id="9112" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">正如您在上面的代码中看到的，还可以分别使用</em> <code class="fe no np nq nr b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.set_state.html" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu"><em class="lw">set_state()</em></strong></a></code> <em class="lw">和</em> <code class="fe no np nq nr b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.get_state.html" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu"><em class="lw">get_state()</em></strong></a></code> <em class="lw">来检索或设置Numpy的生成器的内部状态，并且状态本身在其元组中有更多的元素(‘mt 19937’代表Mersenne Twister (MT)及其范围(顺便说一下，2 ⁹⁹ ⁷-1))，但是我们不会对此进行任何深入的研究。毕竟，在Numpy中，您不太可能需要修改生成器的内部状态… </em></p></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><p id="89bb" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">还有一点需要指出，转自Numpy的<code class="fe no np nq nr b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/random/generator.html#numpy.random.Generator" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">Generator</strong></a></code>文档，一个标题为“<strong class="lx iu"> <em class="lw">无兼容性保证</em> </strong>”的章节:</p><blockquote class="lr ls lt"><p id="0bda" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><code class="fe no np nq nr b">Generator</code>不提供版本兼容性保证。特别是，随着更好的算法的发展，比特流可能会改变。</p></blockquote><p id="4764" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">谁说确保再现性很容易？不是我！</p><blockquote class="lr ls lt"><p id="7996" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">请记住:为了真正的再现性，您需要使用相同的随机种子、相同的模块/包和相同的版本！</p></blockquote><p id="afe8" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">是时候换一个不同的包了！</p><h2 id="db14" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">PyTorch</h2><p id="a5d3" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">就像Numpy一样，PyTorch也有自己设置种子的方法，<code class="fe no np nq nr b"><a class="ae ky" href="https://bit.ly/3hOwklL" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">torch.manual_seed()</strong></a></code>，为所有设备(CPU和GPU/CUDA)设置一个种子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c3a39e8cbba812ff98dc3a40e2260568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*kNlwMp2jsMlOZsl8ev0JRQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch中的随机数(CPU)。图片作者。</p></figure><p id="2694" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">正如您可能已经预料到的，生成的序列又是不同的。<strong class="lx iu">新包装，新顺序</strong>。</p><p id="7fd1" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">但还有更多！如果你在一个<strong class="lx iu">不同的设备</strong>中生成一个序列，比如你的GPU ( <code class="fe no np nq nr b"><strong class="lx iu">'cuda'</strong></code>)，你会得到<strong class="lx iu">又一个序列</strong>！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f2f8ab952708ed838d37f7c8864c4d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*5M3LMz7Jfi6N0rcaODEUgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch中的随机数(CUDA/GPU)。图片作者。</p></figure><p id="8926" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">在这一点上，这不应该是一个惊喜，对不对？此外，PyTorch关于<a class="ae ky" href="https://pytorch.org/docs/stable/notes/randomness.html" rel="noopener ugc nofollow" target="_blank">再现性</a>的文档非常简单明了:</p><blockquote class="lr ls lt"><p id="ae76" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“不能保证PyTorch版本、单个提交或不同平台的结果完全可重复。此外，即使使用相同的种子，CPU和GPU执行之间的结果也可能不可重复。”</p></blockquote><p id="8fb7" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">因此，我相应地更新了上一节的建议:</p><blockquote class="lr ls lt"><p id="ed6a" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">请记住:为了真正的再现性，您需要使用相同的随机种子、相同的模块/包、相同的版本、相同的平台、相同的设备，甚至可能是相同的驱动程序(例如，您的GPU的CUDA版本)！</p></blockquote><p id="6c81" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">也许你注意到了上面输出中的一个<code class="fe no np nq nr b"><strong class="lx iu">Generator </strong></code>…不出所料，PyTorch也使用生成器，就像Numpy一样，并且那个生成器是<strong class="lx iu"> PyTorch的默认生成器</strong>。我们可以使用<code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">torch.default_generator</strong></a><strong class="lx iu"> </strong></code>检索它，并使用<code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">manual_seed()</strong></a></code>方法设置它的种子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/57f20bdf9e35467d566afd1a4d50245f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*YmaPwPnOOO4jBbklk6W00Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PyTorch的默认生成器。图片作者。</p></figure><p id="4216" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">您也可以创建另一个生成器，并将其用作其他函数或对象的参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/296aa43a24679f9357afc1859edf59a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*9PTe8JH5ArJRLtAi-bSy7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在PyTorch中创建和使用生成器。图片作者。</p></figure><p id="d90d" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">有一种情况下使用自己的生成器特别有用:数据加载器中的采样。</p><h2 id="ce75" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">数据加载器</h2><p id="6cf6" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">在为训练集创建数据加载器时，我们通常将其参数<code class="fe no np nq nr b"><strong class="lx iu">shuffle</strong></code>设置为<code class="fe no np nq nr b"><strong class="lx iu">True</strong></code>(因为在大多数情况下，混合数据点可以提高梯度下降的性能)。这是一种非常方便的<strong class="lx iu">混洗数据</strong>的方式，它是使用引擎盖下的<code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">RandomSampler</strong></a></code>实现的。每次请求一个新的小批量时，它会随机采样一些索引，并返回对应于这些索引的数据点。</p><p id="5517" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">即使不涉及混洗，也要使用<code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">SequentialSampler</strong></a></code>，这在用于验证集的数据加载器中是很典型的。在这种情况下，每当请求新的小批量时，该采样器简单地按顺序返回一系列索引，并且返回对应于那些索引的数据点。</p><p id="3384" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">从PyTorch 1.7开始，为了保证再现性，<strong class="lx iu">我们需要给</strong> <code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">DataLoader</strong></a></code>分配一个生成器，所以在相应的采样器中使用它(当然前提是它使用生成器)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/52b6241466d6d474e846e60b3cb9cf36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*ZXrzcHo6gImkBu0x-sa84Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将生成器分配给数据加载器。图片作者。</p></figure><p id="4429" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">实际上，我们可以从加载器中检索采样器，检查其初始种子，并根据需要手动设置不同的种子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/584dbc6843b1968f0070302ddb9170b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*LUUwohiKfezhIyCOcWjT8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">手动设置装载机发电机的种子。图片作者。</p></figure><p id="9bc7" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们将在几个部分中这样做，同时编写一个函数来使用"<em class="lw">一个种子来统治所有的</em> " :-)</p><p id="1490" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">为数据加载器分配一个生成器将会覆盖您，但是只有当您在主进程 ( <code class="fe no np nq nr b"><strong class="lx iu">num_workers=0</strong></code>，默认)中加载数据时<strong class="lx iu">。如果您想要使用<strong class="lx iu">多重处理来加载数据</strong>，也就是说，指定更大数量的工人，您还需要<strong class="lx iu">为您的数据加载器</strong>分配一个</strong> <code class="fe no np nq nr b"><strong class="lx iu">worker_init_fn()</strong></code> <strong class="lx iu">，以避免您的所有工人绘制完全相同的数字序列。让我们看看为什么会出现这种情况！</strong></p><p id="4a0a" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">PyTorch实际上可以在上面的情况下照顾自己——<a class="ae ky" href="https://pytorch.org/docs/stable/data.html#randomness-in-multi-process-data-loading" rel="noopener ugc nofollow" target="_blank">它用不同的编号</a>播种每个worker，也就是<code class="fe no np nq nr b"><strong class="lx iu">base_seed + worker_id</strong></code>，但是它不能照顾其他的包(比如Numpy或者Python的random模块)。</p><p id="c43b" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们可以使用作为参数传递给数据加载器的<code class="fe no np nq nr b"><strong class="lx iu">seed_worker()</strong></code>函数中的一些打印语句来看看发生了什么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9077abc3c270fd19fea5b2646fe273cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*j1tic60RsWS_WYv3_lJcFA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">看一眼每个工人使用的种子。图片作者。</p></figure><p id="50b0" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">有两个工人，(0)和(1)，每次调用一个工人执行任务时，<code class="fe no np nq nr b"><strong class="lx iu">seed_worker()</strong></code>函数打印PyTorch、Numpy和Python的random模块使用的种子。</p><p id="5b1e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">你可以看到PyTorch使用的种子刚刚好——第一个工人使用一个以55结尾的数字；第二个工人的，一个以56结尾的号码，不出所料。</p><blockquote class="lr ls lt"><p id="5c8c" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">但是Numpy和Python的random模块使用的<strong class="lx iu">种子</strong>是跨worker的<strong class="lx iu">相同，这也是我们要<strong class="lx iu">避免</strong>的。不过，不同模块之间的种子可以相同。</strong></p></blockquote><p id="f945" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">幸运的是，有一个简单的解决方法:我们<strong class="lx iu">在</strong> <code class="fe no np nq nr b"><strong class="lx iu">seed_worker()</strong></code> <strong class="lx iu">函数</strong>中包含一些种子设置语句，使用PyTorch的初始种子(并将其调整为32位整数)，而不是打印语句:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/971600dd8f8c644f9a4c51f4ce84ceaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*QNyNvHWBftLRUNFMjSdSyw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">适当地播种工人。图片作者。</p></figure><p id="fd83" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">现在每个工人将为PyTorch、Numpy和Python的random模块使用不同的种子。</p><blockquote class="lr ls lt"><p id="ab31" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“好的，我明白，但是如果我只使用PyTorch，为什么我需要播种其他的包呢？”</p></blockquote><h2 id="7631" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">播种PyTorch是不够的！</h2><p id="fdf4" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">您可能认为，如果您没有在代码中显式使用Numpy或Python的random模块，您就不需要关心为它们设置种子，对吗？</p><p id="e059" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">也许你没有，但是最好<strong class="lx iu">稳妥一点，为一切设置种子:PyTorch，Numpy，甚至Python的random模块</strong>，这就是我们在上一节中所做的。</p><blockquote class="lr ls lt"><p id="2a90" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“那是为什么？”</p></blockquote><p id="cefb" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">原来，PyTorch <em class="lw">可能</em>用的是不属于自己的发电机！老实说，当我发现这件事的时候，我也很惊讶！听起来可能很奇怪，在0.8 之前的<strong class="lx iu"> Torchvision版本中，仍然有一些<strong class="lx iu">代码依赖于Python的随机模块，而不是PyTorch自己的随机生成器</strong>。当使用一些用于数据扩充的随机转换时，问题就出现了，比如<code class="fe no np nq nr b"><strong class="lx iu">RandomRotation()</strong></code>、<code class="fe no np nq nr b"><strong class="lx iu">RandomAffine()</strong></code>等等。</strong></p><h2 id="0e15" class="oh la it bd lb oi oj dn lf ok ol dp lj mr om on ll ms oo op ln mt oq or lp os bi translated">库达</h2><p id="6d5c" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated">手动设置PyTorch的种子对CPU和CUDA/GPU都有效，正如我们在前面几节中看到的那样。但是由<strong class="lx iu"> CUDA卷积运算</strong>使用的cuDNN库仍然可能是<strong class="lx iu">非确定性</strong>行为的来源。</p><p id="b80f" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">事实证明，根据所提供的参数以及底层硬件和环境，库试图使用最快的算法。但是我们可以通过禁用这个所谓的基准特性，将<code class="fe no np nq nr b"><strong class="lx iu">torch.backends.cudnn.benchmark</strong> </code>设置为<code class="fe no np nq nr b"><strong class="lx iu">False</strong>.</code>，来强制<strong class="lx iu">确定性地选择一个算法</strong></p><p id="3adb" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">虽然使用上述配置可以使算法的<strong class="lx iu"> <em class="lw">选择</em>具有确定性</strong>，但是<strong class="lx iu">算法本身<em class="lw">可能不是</em>的</strong>！</p><blockquote class="lr ls lt"><p id="dbca" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“哦，来吧！”</p></blockquote><p id="1f28" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我听到了。为了解决这个问题，我们还需要做另一个配置:将<code class="fe no np nq nr b"><strong class="lx iu">torch.backends.cudnn.deterministic </strong></code>设置为<code class="fe no np nq nr b"><strong class="lx iu">True</strong>.</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b2aee2ec602faf06cc3ea2b0f5ab88e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*MphqsLzK0w0eCksVJQP9cg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使CUDA卷积运算具有确定性。图片作者。</p></figure><blockquote class="lr ls lt"><p id="93ae" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">使用CUDA的可再现性还有其他含义:由于CUDA版本10.2中引入的变化，RNN和LSTM层也可能表现出不确定性行为(详细信息参见<a class="ae ky" href="https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-840__section_nbs_qgh_fsb" rel="noopener ugc nofollow" target="_blank">文档</a>)。</p><p id="0171" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">PyTorch的文档建议将环境变量<code class="fe no np nq nr b"><strong class="lx iu">CUBLAS_WORKSPACE_CONFIG</strong> </code>设置为<code class="fe no np nq nr b"><strong class="lx iu">:16:8</strong> </code>或<code class="fe no np nq nr b"><strong class="lx iu">:4096:2</strong></code>来实施确定性行为。</p></blockquote><h1 id="76dd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">合唱:老麦克托奇有一个模型</h1><blockquote class="mu"><p id="d043" class="mv mw it bd mx my mz na nb nc nd mq dk translated">“老麦克托奇有一个模型，咿呀咿呀</p><p id="714f" class="mv mw it bd mx my mz na nb nc nd mq dk translated">在它的模型上，有一些种子，咿呀咿呀</p><p id="8579" class="mv mw it bd mx my mz na nb nc nd mq dk translated">这里有一粒种子，那里有一粒种子</p><p id="38db" class="mv mw it bd mx my mz na nb nc nd mq dk translated">这里一粒种子，那里一粒种子，到处都是一粒种子</p><p id="5aac" class="mv mw it bd mx my mz na nb nc nd mq dk translated">老麦克托奇有一个模型，咿呀咿呀哟"</p></blockquote><p id="2a0d" class="pw-post-body-paragraph lu lv it lx b ly ne ju ma mb nf jx md mr ng mg mh ms nh mk ml mt ni mo mp mq im bi translated">你觉得上面这首歌怎么样，来自“<em class="lw">程序员童谣</em>”？对了，我开玩笑，<em class="lw">那不是真书</em>，我编的！也许我应该写这样一本书…但是我跑题了！</p><p id="8490" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">回到我们的主要话题，可能感觉和那首歌一模一样——种子和更多的种子——到处都是种子！</p><p id="053e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">要是有就好了…</p><blockquote class="mu"><p id="0e41" class="mv mw it bd mx my mz na nb nc nd mq dk translated">“一粒种子统治他们所有人！”</p></blockquote><p id="4d96" class="pw-post-body-paragraph lu lv it lx b ly ne ju ma mb nf jx md mr ng mg mh ms nh mk ml mt ni mo mp mq im bi translated">没有这种事，但是我们可以试试退而求其次:<strong class="lx iu">我们自己的函数设置尽可能多的种子</strong>！下面的代码为PyTorch、Numpy、Python的random模块、采样器的生成器设置种子；除了配置PyTorch的后端使CUDA卷积操作具有确定性之外。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/78942d221229cf76734ee97ca22b3e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*clrnKdEQZrkF31IB2Z1UPQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一粒种子统治所有人。图片作者。</p></figure><blockquote class="lr ls lt"><p id="6852" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“这样够吗？”</p></blockquote><p id="6047" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">不一定，不。一些操作<em class="lw">可能</em>仍然具有不确定性，从而使您的结果不完全可再现。虽然有可能<strong class="lx iu">强制PyTorch只使用确定性算法</strong>设置<code class="fe no np nq nr b"><a class="ae ky" href="https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu">torch.use_deterministic_algorithms(True)</strong></a></code>，但是有一个问题…</p><blockquote class="lr ls lt"><p id="d274" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“我就知道！”</p></blockquote><p id="cc82" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">可能你正在执行的一些操作只有<strong class="lx iu">非确定性</strong>算法可用，然后你的代码在被调用时会抛出一个<code class="fe no np nq nr b"><strong class="lx iu">RuntimeError</strong></code>。出于这个原因，我没有将它包含在上面的set_seed函数中——我们没有破坏代码，以确保它的可再现性。</p><blockquote class="lr ls lt"><p id="f63e" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">此外，如果您使用的是CUDA(10.2或更高版本)，除了设置<code class="fe no np nq nr b"><strong class="lx iu">torch.use_deterministic_algorithms(True)</strong></code>，您还需要设置环境变量<code class="fe no np nq nr b"><strong class="lx iu">CUBLAS_WORKSPACE_CONFIG</strong></code>，如前一节所述。</p></blockquote><p id="3b20" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">那些<strong class="lx iu">非确定性算法</strong>可能来自<strong class="lx iu">最意想不到的地方</strong>。例如，在PyTorch的文档中，有一条关于在图像中使用<strong class="lx iu">填充</strong>时可能出现再现性问题的警告:</p><blockquote class="lr ls lt"><p id="0fbb" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“使用CUDA后端时，此操作可能会在其反向传递中引发不确定性行为，这种行为不容易被关闭。请参阅关于再现性的注释以了解背景信息。”</p></blockquote><p id="9ac3" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">让我觉得有点奇怪的是，这么简单的操作会危及可重复性。真不敢相信</p><h1 id="9105" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">随机种子调谐</h1><blockquote class="lr ls lt"><p id="2208" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">"(正确的)随机种子是你所需要的！"</p></blockquote><p id="98fd" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">看起来像个笑话，但是随机种子的选择可能会对模型训练产生影响。一些种子比其他种子“更幸运”，因为它们允许模型训练得更快，或者实现更低的损失。当然，没有办法事先说清楚，也没有，42不是“<em class="lw">什么是正确的随机种子</em>”问题的答案:-)</p><p id="3480" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">如果你对这个话题很好奇，可以查看大卫·皮卡德的论文:“<a class="ae ky" href="https://arxiv.org/abs/2109.08203" rel="noopener ugc nofollow" target="_blank"> <em class="lw"> Torch.manual_seed(3407)是你所需要的全部:论深度学习架构中随机种子对计算机视觉</em> </a>的影响”。摘要如下:</p><blockquote class="lr ls lt"><p id="f230" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">“在本文中，我研究了在将流行的深度学习架构用于计算机视觉时，随机种子选择对准确性的影响。我在CIFAR 10上扫描了大量种子(多达104个),在Imagenet上也扫描了较少的种子，使用预先训练的模型来调查大规模数据集。结论是，即使方差不是很大，也很容易找到表现比平均值好得多或差得多的异常值。”</p></blockquote><h1 id="f9e7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的想法</h1><p id="5dd9" class="pw-post-body-paragraph lu lv it lx b ly nj ju ma mb nk jx md mr nl mg mh ms nm mk ml mt nn mo mp mq im bi translated"><strong class="lx iu">再现性很难！</strong></p><p id="652e" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们甚至没有谈论更基本的问题，例如确保你正确使用数据，以避免多年后当别人试图复制你发表的结果时的尴尬(见<a class="ae ky" href="https://www.businessinsider.com/reinhart-and-rogoff-admit-excel-blunder-2013-4" rel="noopener ugc nofollow" target="_blank">莱因哈特和罗格夫的Excel大错</a>，也被称为“<a class="ae ky" href="https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646" rel="noopener ugc nofollow" target="_blank">如何不擅长经济学</a>”)！</p><p id="e4fd" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated">我们只关注(伪)随机数生成器，即使如此，我们也需要考虑<strong class="lx iu"> <em class="lw">许多不同来源的(伪)随机性</em> </strong>以确保可再现性。那是大量的工作，但是它值得麻烦。</p><blockquote class="lr ls lt"><p id="c086" class="lu lv lw lx b ly lz ju ma mb mc jx md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated"><strong class="lx iu">确保总是在代码的最开始初始化你的随机种子，以确保(或者尝试！)您的结果的可重复性。</strong></p></blockquote><blockquote class="mu"><p id="29b1" class="mv mw it bd mx my pa pb pc pd pe mq dk translated">愿你未来的实验完全可复制！</p></blockquote></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><p id="4358" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">如果您有任何想法、意见或问题，请在下方留下评论或通过我的</em> <a class="ae ky" href="https://bio.link/dvgodoy" rel="noopener ugc nofollow" target="_blank"> <em class="lw">简历链接</em> </a> <em class="lw">页面联系。</em></p><p id="8dbe" class="pw-post-body-paragraph lu lv it lx b ly lz ju ma mb mc jx md mr mf mg mh ms mj mk ml mt mn mo mp mq im bi translated"><em class="lw">如果你喜欢我的帖子，请考虑使用我的推荐页面通过</em> <a class="ae ky" href="https://dvgodoy.medium.com/membership" rel="noopener"> <em class="lw">注册一个中级会员</em> </a> <em class="lw">来直接支持我的工作。对于每一个新用户，我从中获得一小笔佣金:-) </em></p></div></div>    
</body>
</html>