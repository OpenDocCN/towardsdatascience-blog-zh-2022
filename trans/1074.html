<html>
<head>
<title>Introduction to Text Summarization with ROUGE Scores</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 ROUGE 评分的文本摘要介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471#2022-03-21">https://towardsdatascience.com/introduction-to-text-summarization-with-rouge-scores-84140c64b471#2022-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0911" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自然语言处理中文本摘要的信息指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c1279255c3601872c38cd3f97daf97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMr3l9__a_nfIJ7T3PG_ZQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的<a class="ae ky" href="https://unsplash.com/@mrthetrain" rel="noopener ugc nofollow" target="_blank"> Joshua Hoehne </a>拍摄</p></figure><blockquote class="kz"><p id="b5a4" class="la lb it bd lc ld le lf lg lh li lj dk translated">本教程简要介绍了自动文本摘要这一有监督的自然语言处理任务，并详细介绍了量化摘要性能的标准。</p></blockquote><p id="b417" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me lj im bi mf translated"><span class="l mg mh mi bm mj mk ml mm mn di">在信息爆炸的时代，专业人士在工作或日常生活中可能会消耗大量的文本，通常是报告、文章或科学论文。通常，这些文本包含围绕一些中心主题或思想的细节或阐述。然而，在这些过多的文本文档中，有时我们要么只想捕捉某些文档的要点，要么搜索与主要思想相关的内容。一个类似的类比是浏览报纸的标题，尽管一篇文章的标题，作为一句俏皮话，并不是最好的总结形式。</span></p><p id="927d" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">这也是机器学习和人工智能可以介入的地方，因为<strong class="lm iu">自动文本摘要</strong>是自然语言处理(NLP)任务中的一个重要领域。</p><h1 id="4d72" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">1.两种类型的文本摘要</h1><p id="d7cc" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">自动文本摘要训练通常是受监督的学习过程，其中每个文本段落的<em class="nq">目标</em>是相应的<strong class="lm iu">金色注释摘要</strong>(人类专家指导的摘要)。另一方面，模型生成的摘要或训练后的<em class="nq">预测</em>有两种类型:</p><h2 id="dddf" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">1.1 抽象概括</h2><p id="a01a" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">摘要是文本段落的主旨，是完全转述的，有时会用原文中没有的词和内容。因此，抽象摘要是一项困难的任务，可能会遇到生成的摘要在语法或逻辑上不一致的情况。通常，它需要最先进的变压器架构来实现稳定的结果。</p><h2 id="b2b8" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">1.2 摘要概述</h2><p id="d96f" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">摘要，顾名思义，包含从原文段落中提取的单词和短语。虽然有不同的提取技术，但最常见和最简单的一种是按照正确的顺序从文本段落中提取句子。一般来说，前几个句子对文章的意义起着更重要的作用，并且经常被提取出来，尽管文章后面的句子也可以被提取出来。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="e13b" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">值得注意的是，在研究社区中，金色注释摘要通常在本质上是抽象的，因为文本段落经常被人类解释成摘要。因此，用于抽象概括的监督学习过程是直接的，因为目标是明确定义的。但是对于逐句提取摘要，有时需要一个额外的步骤。</p><p id="d173" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">基于金色注释摘要及其句子数量，从原始文本段落中提取具有最高相似度的相应句子子集。相似度由一个叫做<strong class="lm iu">胭脂分数</strong>的度量来衡量，我们将在后面讨论。这些提取的句子集，基于金色注释的抽象摘要，在研究界被称为<strong class="lm iu">甲骨文摘要</strong>。然后，用于提取摘要的监督学习过程基于甲骨文摘要的标记句子。最后，任何提取模型的性能都是生成的提取概要的 ROUGE 分数和 oracle 概要的 ROUGE 分数之间的相对比较。</p><p id="2ae3" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated"><em class="nq"> P.S .由于排列的数量，获得句子的全局最优集合(具有最大 ROUGE 得分)的过程有时在计算上是昂贵的，并且通常用某些算法来近似，例如在 2016 年论文</em><a class="ae ky" href="https://arxiv.org/abs/1611.04230" rel="noopener ugc nofollow" target="_blank"><em class="nq">summary runner:用于文档的提取摘要的基于递归神经网络的序列模型</em> </a> <em class="nq">中介绍的贪婪方法。</em></p><h1 id="da21" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">2.总结模型—进一步阅读</h1><p id="a551" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">总结模型架构通常更加复杂和费解，因此我们不会在这个简单的教程中讨论它们。但是，在下面的段落中，我们将介绍一些研究论文，以及它们相应的代码库，以供进一步阅读和修改。</p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="b5da" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">在变形金刚时代之前，正如 2017 年的论文<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">所介绍的那样</a>，无论有无注意机制，由 LSTMs 管理的 NLP 模型都没有那么强大。因此，提取摘要模型更有效。一个这样的模型如下:</p><h2 id="dcd6" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">纽阿尔斯姆</h2><ul class=""><li id="44b1" class="ok ol it lm b ln nl lq nm lt om lx on mb oo lj op oq or os bi translated"><em class="nq">描述</em>:关注基于 LSTM 编解码模型的摘要提取模型</li><li id="bb6e" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated"><em class="nq"> 2016 论文</em> : <a class="ae ky" href="https://arxiv.org/abs/1603.07252" rel="noopener ugc nofollow" target="_blank">通过提取句子和单词的神经摘要</a></li><li id="aea5" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated">代码库:<a class="ae ky" href="https://github.com/cheng6076/NeuralSum" rel="noopener ugc nofollow" target="_blank">https://github.com/cheng6076/NeuralSum</a></li></ul><p id="e84c" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">在变形金刚时代之后，尤其是随着 BERT 架构的出现(2018 年论文，<a class="ae ky" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:深度双向变形金刚语言理解的预训练</a>)。这样，抽象摘要和提取摘要的性能都变得健壮。一些模型如下:</p><h2 id="baba" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">伯特森(伯特文本和伯特布斯)</h2><ul class=""><li id="ae08" class="ok ol it lm b ln nl lq nm lt om lx on mb oo lj op oq or os bi translated"><em class="nq">描述</em>:用于抽象和提取总结的类似 BERT 的架构。</li><li id="2a90" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated">2019 年论文:<a class="ae ky" href="https://arxiv.org/abs/1908.08345" rel="noopener ugc nofollow" target="_blank">带预训练编码器的文本摘要</a></li><li id="2455" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated">代码库:<a class="ae ky" href="https://github.com/nlpyang/PreSumm" rel="noopener ugc nofollow" target="_blank">https://github.com/nlpyang/PreSumm</a></li></ul><h2 id="85cb" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">飞马座</h2><ul class=""><li id="8571" class="ok ol it lm b ln nl lq nm lt om lx on mb oo lj op oq or os bi translated"><em class="nq">描述</em>:基于变压器的编码器-解码器模型，具有用于抽象概括的自我监督目标</li><li id="4b78" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated">2019 论文:<a class="ae ky" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank"> PEGASUS:用提取的 Gap-sentences 进行抽象概括的预训练</a></li><li id="8d9d" class="ok ol it lm b ln ot lq ou lt ov lx ow mb ox lj op oq or os bi translated">代码库:<a class="ae ky" href="https://github.com/google-research/pegasus" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/pegasus</a></li></ul><h1 id="61f8" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">3.胭脂分数简介</h1><p id="4f6d" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">接下来，我们继续评估指标，比较模型生成的摘要与黄金注释摘要的接近程度。值得注意的是，在研究社区中，ROUGE score 已经发展成为摘要任务的标准评估标准。ROUGE 代表面向回忆的 Gisting 评价替角，在论文<a class="ae ky" href="https://aclanthology.org/W04-1013/" rel="noopener ugc nofollow" target="_blank"> ROUGE:一个自动评价概要的包</a>中有介绍。虽然 ROUGE 顾名思义是“面向召回的”，但本质上，它同时考虑了<strong class="lm iu">候选</strong>(模型生成的或<em class="nq">预测的</em>)和<strong class="lm iu">参考</strong>(金色注释的或<em class="nq">目标</em>)摘要之间的召回率和精确度。此外，胭脂分数分为胭脂-1、胭脂-2 和胭脂-1 分数。例如，让我们考虑下面这段话:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="802c" class="nr mu it oz b gy pd pe l pf pg">John loves data science. He studies data science concepts day and night, forgetting to eat at times.</span></pre><p id="0f77" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">参考摘要:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="d3e2" class="nr mu it oz b gy pd pe l pf pg">John really loves data science.</span></pre><p id="7472" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">和候选摘要(考虑提取的情况):</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="445c" class="nr mu it oz b gy pd pe l pf pg">John loves data science.</span></pre><h2 id="014f" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">胭脂-1</h2><p id="608d" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">ROUGE-1 Precision 和 Recall 比较<strong class="lm iu"> </strong>参考摘要和候选摘要之间<strong class="lm iu">单字</strong>的相似性。对于单字，我们简单的意思是每个比较的记号都是一个单词。例如，我们可以将“John 热爱数据科学”的单词表达为一个 Python 标记列表:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="5ad7" class="nr mu it oz b gy pd pe l pf pg">['John','loves','data','science']</span></pre><p id="2546" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">通过<strong class="lm iu">回忆</strong>，我们简单的参考候选摘要所抓取的参考摘要中的<em class="nq">单词比例。</em></p><p id="2b8d" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">通过<strong class="lm iu">精度，</strong>我们指的是由候选摘要建议的、实际出现在参考摘要中的单词的<em class="nq">比例。</em></p><p id="dc33" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">例如:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="b719" class="nr mu it oz b gy pd pe l pf pg">Reference Tokens: ['John','<em class="nq">really</em>','loves','data','science'](<strong class="oz iu">n_r=5</strong>)<br/>Candidate Tokens: ['John','loves','data','science'] (<strong class="oz iu">n_can=4</strong>)<br/><strong class="oz iu">Captured Tokens</strong>: ['John','loves','data','science'] (<strong class="oz iu">n_cap=4</strong>)</span><span id="b8d3" class="nr mu it oz b gy ph pe l pf pg"><strong class="oz iu">Rouge-1 Recall = n_cap/n_r = 4/5 <br/>Rouge-1 Precision = n_cap/n_can = 4/4</strong></span></pre><p id="a6c3" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">同样地，我们可以看到，对于由候选摘要准确预测单词的数量，召回奖励，而对于候选摘要中出现的不必要的冗长单词，精确惩罚。精确度差的一个例子是下面的候选摘要:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="555e" class="nr mu it oz b gy pd pe l pf pg">John really really really loves data science. (<strong class="oz iu">n_can=</strong>7)</span></pre><p id="19e9" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">在这种情况下，所有的引用标记都被捕获，因此它应该有一个完美的召回分数。但是，由于过于冗长，它的 Rouge-1 精度只有 5/7。</p><h2 id="6e96" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">胭脂-2</h2><p id="138d" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">ROUGE-2 Precision 和 Recall 比较<strong class="lm iu"> </strong>参考和候选摘要之间的<strong class="lm iu">二元模型</strong>的相似性。对于二元语法，我们的意思是每个比较标记是来自参考和候选摘要的两个连续单词。例如,“John 热爱数据科学”的双字母组合可以表示为以下符号:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="e38a" class="nr mu it oz b gy pd pe l pf pg">['John loves','loves data','data science']</span></pre><p id="2d22" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">因此，可以计算召回率和精确度:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="7fd3" class="nr mu it oz b gy pd pe l pf pg">Reference Tokens: ['John really','really loves','loves data','data science'](<strong class="oz iu">n_r=</strong>4)</span><span id="7519" class="nr mu it oz b gy ph pe l pf pg">Candidate Tokens: ['John loves','loves data','data science'] (<strong class="oz iu">n_can=</strong>3)</span><span id="c096" class="nr mu it oz b gy ph pe l pf pg"><strong class="oz iu">Captured Tokens</strong>: ['loves data','data science'] (<strong class="oz iu">n_cap=</strong>2)</span><span id="f44a" class="nr mu it oz b gy ph pe l pf pg"><strong class="oz iu">Rouge-2 Recall = n_cap/n_r = 2/4 <br/>Rouge-2 Precision = n_cap/n_can = 2/3</strong></span></pre><h2 id="3fe5" class="nr mu it bd mv ns nt dn mz nu nv dp nd lt nw nx nf lx ny nz nh mb oa ob nj oc bi translated">胭脂-L</h2><p id="471b" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">ROUGE-L 精度和召回测量参考和候选摘要之间的<strong class="lm iu">最长公共子序列(LCS) </strong>单词。对于 LCS，我们指的是在序列中的<strong class="lm iu">，但是<strong class="lm iu">不一定是连续的</strong>。为了理解这一点，让我们看一个带有参考摘要的复杂示例:</strong></p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="76fa" class="nr mu it oz b gy pd pe l pf pg">John really loves data science very much and studies it a lot.</span></pre><p id="03b6" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">以及候选人总结(斜体字代表 LCS):</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="5ff6" class="nr mu it oz b gy pd pe l pf pg"><em class="nq">John</em> very much <em class="nq">loves data science and</em> enjoys <em class="nq">it a lot</em>.</span></pre><p id="1852" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">参考标记、候选标记和捕获标记以及相应的精度和召回看起来如下:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="298e" class="nr mu it oz b gy pd pe l pf pg">Reference Tokens: ['John','really','loves','data','science','very','much','and','studies','it','a','lot'](<strong class="oz iu">n_r=</strong>12)</span><span id="1b22" class="nr mu it oz b gy ph pe l pf pg">Candidate Tokens: ['John','very','much','loves','data','science','and','enjoys','it','a','lot'] (<strong class="oz iu">n_can=</strong>11)</span><span id="d168" class="nr mu it oz b gy ph pe l pf pg"><strong class="oz iu">Captured Tokens</strong>: ['John','loves','data','science','and','it','a','lot'] (<strong class="oz iu">n_cap=</strong>8)</span><span id="2230" class="nr mu it oz b gy ph pe l pf pg"><strong class="oz iu">Rouge-2 Recall = n_cap/n_r = 8/12 <br/>Rouge-2 Precision = n_cap/n_can = 8/11</strong></span></pre><p id="7618" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">ROUGE-1、ROUGE-2 和 ROUGE-L Precision/Recall 一起很好地表示了模型生成的摘要如何很好地表示黄金注释的摘要。为了使分数更简洁，通常 F1 分数，即精确度和召回率之间的调和平均值，是为所有 ROUGE 分数计算的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/16be53048758e29314213c6b9167da91.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Ygg8wKYgCAZZU6xn45gnHQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">F1 方程式。图片作者。</p></figure><p id="7beb" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">此外，可以使用 Python 包轻松计算这些 ROUGE 分数。在终端或命令提示符下:</p><pre class="kj kk kl km gt oy oz pa pb aw pc bi"><span id="d109" class="nr mu it oz b gy pd pe l pf pg">pip install rouge</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pj pk l"/></div></figure></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><h1 id="8d84" class="mt mu it bd mv mw pl my mz na pm nc nd jz pn ka nf kc po kd nh kf pp kg nj nk bi translated">4.最后的想法</h1><p id="2866" class="pw-post-body-paragraph lk ll it lm b ln nl ju lp lq nm jx ls lt nn lv lw lx no lz ma mb np md me lj im bi translated">感谢阅读！我希望这篇文章对自动文本摘要的工作原理有所帮助。摘要是自然语言处理中一个令人兴奋的研究领域，有许多潜在的下游应用，如自动标题生成、报告摘要等，也可能产生商业价值。为了进行实验，可以在这里看到一个有趣的总结一段文字的 web 应用程序:</p><div class="pq pr gp gr ps pt"><a href="https://www.paraphraser.io/text-summarizer" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">文本摘要生成器</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">文本摘要器是一个在线工具，它将文本打包成指定的短长度。它把一篇长文浓缩成…</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">www.paraphraser.io</p></div></div></div></a></div><p id="7369" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">我会写一些关于 NLP 的其他话题。在接下来的相关文章中，我将讨论其他的自然语言处理任务，比如共指消解(即将文章中相互共指的词实体进行聚类)。一定要保持警惕！</p><p id="211c" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">干杯！_/\_</p><p id="53dd" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated">我还介绍了 NLP 中的一些其他主题。如果你有兴趣，可以看看我下面的一些文章:</p><div class="pq pr gp gr ps pt"><a href="https://medium.com/mlearning-ai/introduction-to-hidden-markov-model-hmm-with-simple-ner-d1353ff35842" rel="noopener follow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">简单 NER 隐马尔可夫模型简介</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">隐马尔可夫模型在命名实体识别中的简单应用</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">medium.com</p></div></div><div class="qc l"><div class="qd l qe qf qg qc qh ks pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a rel="noopener follow" target="_blank" href="/dynamic-word-tokenization-with-regex-tokenizer-801ae839d1cd"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">使用正则表达式分词器进行动态单词分词</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">关于用正则表达式单步预处理文本的简短教程</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">towardsdatascience.com</p></div></div><div class="qc l"><div class="qi l qe qf qg qc qh ks pt"/></div></div></a></div><p id="5695" class="pw-post-body-paragraph lk ll it lm b ln mo ju lp lq mp jx ls lt mq lv lw lx mr lz ma mb ms md me lj im bi translated"><em class="nq">附:如果你对</em> <strong class="lm iu"> <em class="nq">加速你的数据科学学习</em> </strong> <em class="nq">感兴趣，这里还有一篇关于养成良好学习习惯的极其有用的文章:</em></p><div class="pq pr gp gr ps pt"><a rel="noopener follow" target="_blank" href="/a-brief-guide-to-effective-learning-in-data-science-637de316da0e"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">数据科学有效学习完全指南</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">让你在数据科学(或任何学科)领域飞速发展的基本指南</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">towardsdatascience.com</p></div></div><div class="qc l"><div class="qj l qe qf qg qc qh ks pt"/></div></div></a></div><blockquote class="qk ql qm"><p id="237e" class="lk ll nq lm b ln mo ju lp lq mp jx ls qn mq lv lw qo mr lz ma qp ms md me lj im bi translated"><em class="it">感谢阅读！如果您喜欢这些内容，请在</em> <a class="ae ky" href="https://tanpengshi.medium.com/" rel="noopener"> <em class="it">中的</em> </a> <em class="it">上阅读我的其他文章，并在</em><a class="ae ky" href="https://www.linkedin.com/in/tanpengshi/" rel="noopener ugc nofollow" target="_blank"><em class="it">LinkedIn</em></a><em class="it">上关注我。</em></p><p id="5fb9" class="lk ll nq lm b ln mo ju lp lq mp jx ls qn mq lv lw qo mr lz ma qp ms md me lj im bi translated"><strong class="lm iu"> <em class="it">支持我！</em> </strong> —如果你没有<em class="it">订阅 Medium，并且喜欢我的内容，请考虑通过我的<a class="ae ky" href="https://tanpengshi.medium.com/membership" rel="noopener">推荐链接</a>加入 Medium 来支持我。</em></p></blockquote><div class="pq pr gp gr ps pt"><a href="https://tanpengshi.medium.com/membership" rel="noopener follow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">加入我的介绍链接媒体-谭师鹏阿尔文</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">阅读谭·师鹏·阿尔文(以及媒体上成千上万其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">tanpengshi.medium.com</p></div></div><div class="qc l"><div class="qq l qe qf qg qc qh ks pt"/></div></div></a></div></div></div>    
</body>
</html>