<html>
<head>
<title>How to Read Kafka Clickstream Event Data in Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何阅读熊猫中的卡夫卡点击流事件数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-read-kafka-clickstream-event-data-in-pandas-96f50e88f7eb#2022-10-06">https://towardsdatascience.com/how-to-read-kafka-clickstream-event-data-in-pandas-96f50e88f7eb#2022-10-06</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="3910" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">构建Kafka消费程序以读取Jupyter笔记本中特定日期范围内的事件数据</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/3e749f62ab25e9e3d4b6af825757c53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H6ehqPc781N4Zst_"/></div></div><p class="ks kt gk gi gj ku kv bd b be z dk translated">照片由<a class="ae kw" href="https://unsplash.com/@r3dmax?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔纳森派</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="06a6" class="kx ky ir bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">背景</h1><p id="57db" class="pw-post-body-paragraph lp lq ir lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ik bi translated">最近，我被分配了一个看似简单的任务→</p><blockquote class="ml mm mn"><p id="5d26" class="lp lq mo lr b ls mp js lu lv mq jv lx mr ms ma mb mt mu me mf mv mw mi mj mk ik bi translated">“我们开始从一个新的应用程序中捕捉事件。你能证明这样的测试事件在卡夫卡那里是正确的吗？”</p></blockquote><p id="6e9c" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">我对我们系统中的数据流有一个粗略的想法:从Web和移动应用程序收集的点击流数据将首先流向MetaRouter，然后MetaRouter作为Kafka生产者，为某个Kafka主题生成事件日志。我们的新应用程序生成的事件有一个共享的<code class="fe mx my mz na b">writeKey</code>。因此，为了阅读这些事件，我需要:</p><ol class=""><li id="a9c1" class="nb nc ir lr b ls mp lv mq ly nd mc ne mg nf mk ng nh ni nj bi translated">创建一个卡夫卡消费者来听这个卡夫卡话题</li><li id="d588" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk ng nh ni nj bi translated">因为我知道这样的测试事件是在特定的日期范围内产生的，所以我想为指定的日期构建Kafka消费者只读事件。</li><li id="8b24" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk ng nh ni nj bi translated">以一种我可以过滤和分析的方式存储数据，最好是在熊猫的数据框架中。</li></ol><p id="74c6" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">所以我的目标是实现卡夫卡-&gt;熊猫的数据流！</p><p id="cc06" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">在谷歌搜索了StackOverflow、GitHub和各种网站很多天后，我终于让它工作了！下面是我关于如何在Kafka的两个最流行的python库中实现的代码片段。</p></div><div class="ab cl np nq hv nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ik il im in io"><h1 id="33e9" class="kx ky ir bd kz la nw lc ld le nx lg lh jx ny jy lj ka nz kb ll kd oa ke ln lo bi translated">解决方案1:使用“kafka-python”库</h1><p id="6eba" class="pw-post-body-paragraph lp lq ir lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ik bi translated"><strong class="lr is">必备:</strong> <code class="fe mx my mz na b">pip install kafka-python</code>(我的笔记本用的是最新版本2.0.2)</p><p id="744d" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">首先，我们需要导入下面的库和Kafka环境变量。下面的设置脚本可以在解决方案2中重复使用，只需稍作修改。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ob oc l"/></div></figure><p id="da43" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">接下来，构建一个Kafka消费程序来读取特定日期时间范围内的事件。有5个步骤:</p><ul class=""><li id="3510" class="nb nc ir lr b ls mp lv mq ly nd mc ne mg nf mk od nh ni nj bi translated"><strong class="lr is">步骤1 </strong>:因为我知道测试事件是在2022–09–22之间的12点至14点(UTC)时间发送的，所以我使用<code class="fe mx my mz na b">datetime</code>函数创建<code class="fe mx my mz na b">dt_start</code>和<code class="fe mx my mz na b">dt_end</code>来限定时间范围。</li><li id="cb87" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk od nh ni nj bi translated"><strong class="lr is">第二步</strong>:在Kafka中，只有来自同一个分区的事件才是有序的，所以我们需要从指定的分区读取事件。(假设您有6个主题分区，您可以从0-5中选择任意一个数字作为分区)。</li><li id="d41e" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk od nh ni nj bi translated"><strong class="lr is">第三步</strong>:基础消费者要求<code class="fe mx my mz na b">topic</code>、<code class="fe mx my mz na b">bootstrap_servers</code>和<code class="fe mx my mz na b">group_id</code>。我发现在Jupyter笔记本中，如果我不提供<code class="fe mx my mz na b">security_protocol</code>它就会抛出错误。</li><li id="3049" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk od nh ni nj bi translated"><strong class="lr is">第四步:</strong>这是关键！它的工作方式是这样的:<br/> - datetime对象→转换为UTC时间戳(单位为毫秒)→转换为主题分区中的相关偏移量<br/> -基本函数是<code class="fe mx my mz na b">consumer.offsets_for_times({tp:dt_start.timestamp()*1000})</code></li><li id="6a89" class="nb nc ir lr b ls nk lv nl ly nm mc nn mg no mk od nh ni nj bi translated"><strong class="lr is">步骤5: </strong>使用<code class="fe mx my mz na b">seek</code>获取从期望的开始时间<br/>开始的事件——每个消息都有一个属性<code class="fe mx my mz na b">offset</code>，我们将它与期望的结束时间偏移进行比较，以决定是继续还是中断</li></ul><p id="157a" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">说够了，下面是完整的代码→</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ob oc l"/></div></figure><p id="371d" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">之后，我可以在熊猫里查询我们新应用的<code class="fe mx my mz na b">writeKey</code>！🐼</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj oe"><img src="../Images/f030236b25cfde1eef40ebf6ac7e1ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D0CpW84DG0ziXUFx65946Q.png"/></div></div></figure><p id="dad8" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">上述解决方案受到StackOverflow 的<a class="ae kw" href="https://stackoverflow.com/questions/50405509/kafka-how-to-consume-data-based-on-timestamp" rel="noopener ugc nofollow" target="_blank">类似问题的启发。实际上，这是我开始做大量搜索的地方，发现没有使用<code class="fe mx my mz na b">confluent-kafka</code>的等价解决方案。由于我的原始代码是基于<code class="fe mx my mz na b">confluent-kafka</code>而不是<code class="fe mx my mz na b">kafka-python</code>，我对它们表面上的相似性和细微差别感到困惑。</a></p><p id="7dca" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">现在我很高兴使用<code class="fe mx my mz na b">confluent-kafka</code>介绍我自己的解决方案😃~~~</p></div><div class="ab cl np nq hv nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ik il im in io"><h1 id="b848" class="kx ky ir bd kz la nw lc ld le nx lg lh jx ny jy lj ka nz kb ll kd oa ke ln lo bi translated">解决方案2:使用“汇合-卡夫卡”库</h1><p id="caf6" class="pw-post-body-paragraph lp lq ir lr b ls lt js lu lv lw jv lx ly lz ma mb mc md me mf mg mh mi mj mk ik bi translated"><strong class="lr is">先决条件:</strong> <code class="fe mx my mz na b">pip install confluent-kafka</code>(我的笔记本用的是最新版本1.9.2)</p><p id="c623" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">在这里，您可以使用解决方案1中的相同设置脚本，只需稍作修改:</p><ul class=""><li id="aff5" class="nb nc ir lr b ls mp lv mq ly nd mc ne mg nf mk od nh ni nj bi translated">将第10行改为<code class="fe mx my mz na b">from confluent_kafka import Consumer, TopicPartition</code></li></ul><p id="60c9" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">接下来，我们需要构建一个Kafka消费者来读取特定日期时间范围内的事件。在高层次上，我们仍然需要同样的5个步骤，但是主要的区别是我们需要使用<code class="fe mx my mz na b">on_assign</code>来实现<code class="fe mx my mz na b">seek</code>所做的——从主题分区中获取特定的偏移量。</p><p id="c4d8" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated"><strong class="lr is">步骤1: </strong>和解决方案1一样，我们需要datetime对象来限定搜索范围。</p><p id="4d94" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated"><strong class="lr is">第二步</strong>:同方案一。<strong class="lr is">一个比较棘手的事情</strong>就是大部分时候你可以用string作为topic比如<code class="fe mx my mz na b">topic = 'analytics__pageview'</code>，但是当你要<code class="fe mx my mz na b">subscribe</code>的时候，它只接受一个list比如<code class="fe mx my mz na b">consumer.subscribe(['analytics__pageview'])</code>！(就像邓布利多可能会说的:“多么希望如此~ ~”🧙</p><p id="d956" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated"><strong class="lr is">步骤3: </strong>除了将变量赋值中的<code class="fe mx my mz na b">=</code>替换为<code class="fe mx my mz na b">:</code>之外，几乎与解决方案1相同。</p><p id="a4e4" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated"><strong class="lr is">第四步:</strong>下面是你将看到的细微差别！我们需要一个额外的步骤<code class="fe mx my mz na b">4c</code>来建造<code class="fe mx my mz na b">on_assign</code>。而这个功能最初是来自<a class="ae kw" href="https://github.com/edenhill" rel="noopener ugc nofollow" target="_blank"> Magnus Edenhill </a>提供的<a class="ae kw" href="https://github.com/confluentinc/confluent-kafka-python/issues/373" rel="noopener ugc nofollow" target="_blank">github confluent-Kafka-python issue</a>。</p><p id="f8a4" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated"><strong class="lr is">步骤5: </strong>这里我们不使用<code class="fe mx my mz na b">seek</code>，而是使用<code class="fe mx my mz na b">subscribe</code>和主题(列表形式)以及<code class="fe mx my mz na b">on_assign</code>来获取所需开始时间的偏移量。而且取数后我们需要调用<code class="fe mx my mz na b">close()</code>。</p><p id="0af1" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">还有一个值得注意的细节是，如何得到偏移量。</p><p id="ca65" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">在<code class="fe mx my mz na b">kafka-python</code>中，你用的是<code class="fe mx my mz na b">offset_start[tp].offset</code>，<code class="fe mx my mz na b">offset_start</code>是字典。</p><pre class="kh ki kj kk gu of na og oh aw oi bi"><span id="5069" class="oj ky ir na b gz ok ol l om on">offset_start = consumer.offsets_for_times({tp:dt_start.timestamp() * 1000})</span><span id="c470" class="oj ky ir na b gz oo ol l om on"># to print out the offset number<br/>offset_start[tp].offset</span></pre><p id="2c86" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">输出:(注意表示类型的{ }是dict)</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj op"><img src="../Images/01bf3c0772fba080cd1fbb7f23263537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBqYtzkxuJqpjW9trZdhkw.png"/></div></div></figure><p id="4f4e" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">在<code class="fe mx my mz na b">confluent-kafka</code>中，<code class="fe mx my mz na b">offset_start</code>是一个列表，所以需要使用<code class="fe mx my mz na b">offset_start[0].offset</code></p><pre class="kh ki kj kk gu of na og oh aw oi bi"><span id="5823" class="oj ky ir na b gz ok ol l om on">tp_in = TopicPartition(topic=topic, partition=partition, <br/>                       offset=int(dt_start.timestamp() * 1000))</span><span id="3fef" class="oj ky ir na b gz oo ol l om on">offset_start = c.offsets_for_times([tp_in])</span><span id="076f" class="oj ky ir na b gz oo ol l om on"># to print out the offset number<br/>offset_start[0].offset</span></pre><p id="2b15" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">输出:(注意[ ]表示类型是list)</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj oq"><img src="../Images/345dff390196c5ea50313e1392098941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D19-YgkU2zlIGYZOP50P6A.png"/></div></div></figure><p id="ba9e" class="pw-post-body-paragraph lp lq ir lr b ls mp js lu lv mq jv lx ly ms ma mb mc mu me mf mg mw mi mj mk ik bi translated">好的，这是实现<code class="fe mx my mz na b">confluent-kafka</code>的完整代码</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="ob oc l"/></div></figure><h1 id="aa06" class="kx ky ir bd kz la lb lc ld le lf lg lh jx li jy lj ka lk kb ll kd lm ke ln lo bi translated">摘要</h1><ul class=""><li id="0ab4" class="nb nc ir lr b ls lt lv lw ly or mc os mg ot mk od nh ni nj bi translated">构建事件驱动的应用程序是一种趋势，我预见到对数据科学家能够快速处理和对事件数据进行简单探索分析的需求会不断增长。这可以帮助通知哪些数据字段应该被进一步转换并引入ETL管道，这可能应该涉及<code class="fe mx my mz na b">Faust</code>和<code class="fe mx my mz na b">ksql</code>而不是<code class="fe mx my mz na b">pandas.</code></li></ul></div></div>    
</body>
</html>