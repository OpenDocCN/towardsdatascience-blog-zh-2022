<html>
<head>
<title>How to Choose the Optimal Learning Rate for Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何选择神经网络的最佳学习速率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-choose-the-optimal-learning-rate-for-neural-networks-362111c5c783#2022-09-21">https://towardsdatascience.com/how-to-choose-the-optimal-learning-rate-for-neural-networks-362111c5c783#2022-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="df57" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过示例调整最重要的神经网络超参数的指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/01cb280c215c3c0e53d1c5f1490280c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jElJxtyw3MMwUPzLJbbow.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7416308" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/10genie-7861852/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=7416308" rel="noopener ugc nofollow" target="_blank">大卫主人</a></p></figure><p id="c4df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用ML和DL算法时，超参数调整或优化是一个主要挑战。超参数几乎控制了这些算法中的一切。</p><p id="2a31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的<strong class="lb iu"><em class="lv"/></strong><a class="ae ky" rel="noopener" target="_blank" href="/classification-of-neural-network-hyperparameters-c7991b6937c3"><strong class="lb iu"><em class="lv">神经网络超参数分类</em></strong></a><strong class="lb iu"><em class="lv"/></strong>帖子中，我已经用一个合适的<a class="ae ky" rel="noopener" target="_blank" href="/classification-of-neural-network-hyperparameters-c7991b6937c3#a105">分类图</a>讨论了12种神经网络超参数。</p><p id="0ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些超参数决定了运行神经网络模型的时间和计算成本。它们甚至可以决定网络的结构，最终直接影响网络的预测精度和泛化能力。</p><p id="9fde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这些超参数中，最重要的神经网络超参数是用α(<strong class="lb iu"><em class="lv">α</em></strong>)表示的<strong class="lb iu"> <em class="lv">学习率</em> </strong>。</p><p id="a942" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DL初学者总会问学习率有多重要。我可以这样回答这个问题。</p><blockquote class="lw"><p id="55fb" class="lx ly it bd lz ma mb mc md me mf lu dk translated">在训练神经网络时，如果只允许我调整一个超参数，我会毫不犹豫地选择调整学习率。</p></blockquote><p id="4db5" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">让我们深入了解更多细节，找出什么是学习率，以及它如何影响神经网络的训练过程。</p><h1 id="daa7" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">神经网络的学习率是多少？</h1><p id="1e71" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">学习率是神经网络中最重要的超参数。可以在任何优化算法中找到，如RMSprop、Adam、梯度下降等。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ed51" class="nn mm it nj b gy no np l nq nr">from tensorflow.keras.optimizers import RMSprop<br/>RMSprop(<strong class="nj iu">learning_rate=0.001</strong>, ...<!-- -->)</span></pre><p id="ead0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练过程的开始，网络的参数(权重和偏差)用随机值初始化，但是这些不是给出最小误差或损失的最佳值。这就是我们继续培训的原因。</p><p id="5868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在网络的每个训练步骤(迭代)中，将向前传播步骤中计算的结果与地面真实(实际)值进行比较，以计算误差或损失分数。然后，在反向传播步骤中反向传播该误差，以调整权重和偏差的初始值，从而在接下来的训练步骤中最小化该误差。我们继续训练，直到误差最小。</p><p id="4ba5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算每个训练步骤的预测值和实际值之间的误差，需要一个<strong class="lb iu"> <em class="lv">损失函数</em> </strong>。在训练过程中，我们的目标是尽可能将损失函数最小化。为了最小化损失函数，我们需要一个<strong class="lb iu"> <em class="lv">优化器</em> </strong>(优化算法)。</p><p id="284f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Keras中，优化器可以定义为一个函数，如<code class="fe ns nt nu nj b">RMSprop()</code>、<code class="fe ns nt nu nj b">Adam()</code>、<code class="fe ns nt nu nj b">SGD()</code>等。学习率可以被指定为函数中的一个关键字参数，就像我之前展示的那样。</p><p id="6042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们绘制相对于单个重量值(x轴)的损失函数(y轴)时，我们得到2D平面中的误差曲线。</p><p id="0706" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们绘制相对于两个权重值(x和y轴)的损失函数(z轴)时，我们得到3D平面中的误差曲线。这种类型的3D曲线通常被称为<strong class="lb iu"> <em class="lv">误差山</em> </strong>，通常用于描述与学习率和DL算法优化相关的事情！</p><p id="27fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现实世界的神经网络最终会有成千上万的权重值。当神经网络有那么多的权重时，我们无法想象误差曲线的可视化！</p><p id="e8d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">误差山就像一座有山峰和山谷的真正的山。峰值代表高误差值，而谷值代表低误差值。想象权重的随机初始值在那座山的一个顶峰。</p><p id="d7a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在优化过程中，算法需要采取一系列微小的步骤来降低误差山，以最小化误差。每一小步都有两个属性:方向和大小。</p><p id="ab00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步长的方向由梯度(导数)决定。为了得到方向，我们应该计算误差对重量值的导数。</p><p id="cd6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步长由学习速率决定。它决定了优化器下降误差曲线的快慢。有了较大的学习率，优化器会采取较大的步骤来降低错误率。在学习率较低的情况下，优化器会采取较小的步骤来降低错误率。</p><h1 id="8814" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">学习率如何影响神经网络的训练过程</h1><p id="bbe4" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">接下来我们想了解的是学习率如何影响神经网络的训练过程。确定神经网络模型的最佳学习速率非常重要。</p><p id="5315" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里将讨论四种不同的场景:</p><ul class=""><li id="7513" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">以较小的学习速率(α1)缓慢收敛</strong></li><li id="de2b" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">以较大的学习率(α2)在最小值附近振荡</strong></li><li id="777a" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">学习率非常大的振荡和发散(α3) </strong></li><li id="215d" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">以最优学习率(α4)适当收敛</strong></li></ul><p id="2395" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们详细讨论每个场景。</p><h2 id="b8c3" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">场景1:以较小的学习速率缓慢收敛</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/ee058090954b6773bd475793c80e6b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*uvSC2oHzCir8CyJD97umpA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">收敛慢</strong>(图片由作者提供，用draw.io制作)</p></figure><p id="6942" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当学习率远小于最佳值时会发生这种情况(α1 &lt;&lt; α4). The algorithm takes very small steps to descend the error mountain in order to reach the minimum. So, the convergence happens very slowly and the algorithm takes a lot more time to converge. But, the convergence is guaranteed if we allow the algorithm to run for a long period of time with a high number of epochs! So, it is time-consuming even if we get accurate results.</p><h2 id="237e" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">Scenario 2: Oscillating around the minimum with a large learning rate</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/58ad40b5be90ff0562887851ca0363d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*_VQJcVU3sXW3UC_kQBkjEw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">过冲并在最小值</strong>附近振荡)(图片由作者提供，使用draw.io制作)</p></figure><p id="47b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当学习率大于最佳值(α2 &gt; α4)时会出现这种情况。如图所示，当算法采取大的步骤来降低误差山时，优化器在第一步中超过了最佳权重，并且在下一步中再次超过。该算法试图达到最小值，但它仍然远离最小值。在最后的步骤中(接近最小值)，优化器在最小值附近振荡，并且永远不会达到最小值，即使我们在很长一段时间内以大量的时期运行该算法。无论如何，这允许我们在早期获得一个较低的(但不是最佳的)误差值，比前一种情况更快！</p><h2 id="feb8" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">场景3:学习率非常大的振荡和发散</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/7602d132fc6eb2a4062e6e69bc2efbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*1PiVgYGaF_1Sl0rFN-gfEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">超调振荡发散</strong>(图片由作者提供，用draw.io制作)</p></figure><p id="dbc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当学习率远大于最优值(α3 &gt;&gt; α4)时会出现这种情况。因为学习率<strong class="lb iu"> <em class="lv">非常</em> </strong>大，所以算法从不试图下误差山。相反，它会在第一步中超出最佳权重，并在接下来的步骤中继续超出。在最初的几个步骤之后，随着优化器偏离最小值，错误开始增加。在最后的步骤中，优化器在一个比开始的错误值更高的错误附近振荡。这个过程叫做<strong class="lb iu"> <em class="lv">振荡发散</em> </strong>。这是最坏的情况，因为算法结束时的误差比开始时更大！</p><h2 id="6959" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">场景4:以最优学习速率适当收敛</h2><p id="22c1" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们应该避免以上三种情况，尤其是第三种。通过改变Adam优化器的学习速率，最佳学习速率在α1和α2 (α1 &lt; α4 &lt; α2). With the optimal learning rate, the algorithm reaches the minimum in a short period of time with a considerably fewer number of epochs!</p><h1 id="0cfb" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">Guidelines for choosing the optimal learning rate with examples</h1><p id="251a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">This is the most important section of this article. I’ll use an example to show you how the above four scenarios actually happen during the training process.</p><h2 id="ce2e" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">Approach</h2><p id="3e93" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">I’ll train a <a class="ae ky" rel="noopener" target="_blank" href="/generate-mnist-digits-using-shallow-and-deep-autoencoders-in-keras-fb011dd3fec3">浅层自动编码器模型</a>在<a class="ae ky" href="https://rukshanpramoditha.medium.com/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7" rel="noopener"> <strong class="lb iu"> MNIST数据集</strong> </a>(见最后的<a class="ae ky" href="#f7cb" rel="noopener ugc nofollow">引文</a>)之间出现几次。我将从默认的学习率值0.001开始，然后按照下面的指导方针，为您提供一个找到最佳学习率的<strong class="lb iu"> <em class="lv">系统方法</em> </strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox oy l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><h2 id="a771" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">指导方针</h2><ul class=""><li id="3616" class="nv nw it lb b lc nd lf ne li oz lm pa lq pb lu oa ob oc od bi translated"><strong class="lb iu">最好从优化器的默认学习率值开始。</strong>这里，我使用Adam优化器，其默认学习率值为0.001。当训练开始时，监视模型在最初几个时期的表现。我得到了以下训练和验证损失值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/b5571fa45ef359265bd5d3b7a9a6e33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*_j1yYrL2J_Reiok98eLrog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出1 </strong>(图片作者提供)</p></figure><p id="5e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，在每个时期之后，训练和验证损失都减少了。因此，您应该继续运行模型，直到所有的时期(这里是70个)都完成。完成训练过程后我得到了以下情节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/a3ba65b890e3c96a2915098e1ef4e7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*mrBKXZF1rU6E0H2zXQtHBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出2 </strong>(图片作者提供)</p></figure><p id="a3d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最好也在最后几个时期监控模型的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/839a6f46f519059be8d6d253b205632c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*XzeGwb-Hg6krgIAmuEhgfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出3 </strong>(图片作者提供)</p></figure><p id="592b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看图(输出2)，我们可以确定初始学习率太小，收敛发生得非常慢。这对应于场景1。所以，<strong class="lb iu"> α1 = 0.001 </strong>(默认值)。当事情发生这样的时候，我们有两个选择。</p><ul class=""><li id="251c" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">显著增加历元数，以相同的学习速率再次训练模型。</strong>注意，这很费时间。通过增加历元的数量(这里，我们从70增加到130)，我们给模型更多的时间来进一步收敛。我在完成全部130个纪元后得到了下面的情节。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/2a45632d27b561aa3f7ed82fd08fbe25.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*Lslge3CHTV4_WB_-wwVjEA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出4 </strong>(图片作者提供)</p></figure><p id="77d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还监控了模型在过去几个时期的表现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/35860fc4b4358870d9fbc3378f3d4c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*an8cSlzb6EsoRYZh0zwuLA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出5 </strong>(图片作者提供)</p></figure><p id="91d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">增加历元数后，模型的性能提高了(主持输出3和输出5的最后几行)。该模型消耗了更多的时间来获得这种性能改进。这是这个选项的缺点。所以，不如试试第二种选择。</p><ul class=""><li id="e84d" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">增加学习率的值。</strong>注意，在这里，我们不增加历元的数量。这里设置为70。新的学习率是0.01。完成训练后我得到了以下情节。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c5d273872134a065724c63ec9c8120f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*TALBzGGihmwoEgI1EyMeVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出6 </strong>(图片作者提供)</p></figure><p id="e5cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还监控了模型在最初几个时期的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/eddffb6dfa36b6f340c8263cc04d1cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*hVb_BMXjr1-BVpjSzpEm8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出7 </strong>(图片作者提供)</p></figure><p id="f195" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过增加学习率的值，该算法在早期和中期很快降低了误差山。因此，我们在早期获得了较低的误差值。您可以通过比较输出1和输出7来确认这一点。另一个证实是，学习率为0.01的模型仅用了大约20个历元就达到了0.16的误差，但是具有默认学习率值(0.001)的模型用了大约40个历元才达到相同的误差。</p><p id="a158" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看模型在最近几个时代的表现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b2b61ee2fd0797d632a6dcc6fe1263ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*DceArgkGpHd06S7BybQXuA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出8 </strong>(图片作者提供)</p></figure><p id="44bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将这个输出与输出3进行比较。查看两个输出的最后一行。在输出8中，训练和验证损失都显著高于输出3中的相应值。即使算法在学习率较大的早期和中期给出了较低的误差值，但仍远未达到最小值。这对应于场景2。所以，<strong class="lb iu"> α2 = 0.01 </strong>。</p><p id="f4e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，很明显，最佳学习率(α4)应该在α1和α2之间的某个位置(0.001 &lt; α4 &lt; 0.01).</p><ul class=""><li id="24fe" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">尝试α1和α2之间的不同学习率值。</strong>我推荐你从下侧开始。因此，我们可以将这些值增加为0.0015、0.002和0.0025。这是结果。</li></ul><ol class=""><li id="9149" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0010</code> : Val — 0.1278，Train — 0.1292，第70个历元</li><li id="f7a0" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0015</code> : Val — 0.1264，Train — 0.1280，第70个历元</li><li id="04d6" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0020</code> : Val — 0.1265，Train — 0.1281，第70个历元</li><li id="35fa" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0025</code> : Val — 0.1286，Train — 0.1300在第70个历元</li></ol><p id="1c35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过观察上述结果，我们可以得出结论，最佳学习率出现在0.0015和0.0020之间的某处。所以，我们可以尝试以下数值。</p><ol class=""><li id="e8b2" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0016</code> : Val — 0.1259，Train — 0.1276，第70个历元</li><li id="85c3" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0017</code> : Val — 0.1258，Train — 0.1275，第70个历元</li><li id="c5ab" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu pj ob oc od bi translated"><code class="fe ns nt nu nj b">learning_rate=0.0018</code> : Val — 0.1267，Train — 0.1280，第70个历元</li></ol><blockquote class="lw"><p id="df35" class="lx ly it bd lz ma pk pl pm pn po lu dk translated">通过查看上述结果，我们可以得出结论，最佳学习率为0.0017，这可能因<a class="ae ky" href="#1821" rel="noopener ugc nofollow">这些因素</a>而异。</p></blockquote><blockquote class="pp pq pr"><p id="6705" class="kz la lv lb b lc mg ju le lf mh jx lh ps mi lk ll pt mj lo lp pu mk ls lt lu im bi translated"><strong class="lb iu">注意:</strong>这个最佳学习率仅用70个时期就给出了比我们用130个时期以0.001的学习率获得的结果更好的结果！</p></blockquote><p id="9794" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不是以默认的学习率值或任何其他小的学习率值开始，如果你以<em class="lv">非常大的</em>学习率开始，你可以通过查看前几个时期的训练和验证损失来知道它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/d20db981a8288a393bc0aa4368dfb987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*XihQQYiFabm8o9L0VBdpmA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出9 </strong>(图片作者提供)</p></figure><p id="8d19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">启动误差在第5个时期增加！该模型将在下一个纪元开始振荡发散！这种情况发生在学习率很大的时候(这里我用了10！).这对应于场景3。所以，<strong class="lb iu"> α3 = 10 </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/065aade382fa05988692a1a18aca096b.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*BpGc48ezP6InUEeIveVEZw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ov">输出10 </strong>(图片作者提供)</p></figure><p id="b597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你得到一个类似上面的图时，考虑显著降低学习率，然后用你新的学习率再次训练模型。</p><h1 id="1821" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">决定学习率值的因素</h1><p id="c8ea" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">有各种因素影响学习率的值。它们包括:</p><ul class=""><li id="872e" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">网络结构(宽度和深度)</li><li id="bb87" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">神经网络架构的类型(例如MLP、CNN、AE)</li><li id="9af4" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">数据集的性质</li><li id="5dba" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">优化器的类型(例如，RMSprop优化器的最佳学习率不会是Adam、SGD等其他优化器的最佳值。)</li><li id="cc46" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431">随机状态</a>涉及一些函数，如<strong class="lb iu"> train_test_split() </strong>和某些DL算法</li></ul><h1 id="db4c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">在Keras优化器中更改默认学习率</h1><p id="7362" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">默认情况下，Keras为其优化器的学习率提供了一个默认值。在大多数情况下，该值为0.001。从默认的学习率值开始比较好。但是，在大多数情况下，我们需要改变这个值。</p><p id="8428" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，Keras将优化器的类型定义为模型的compile()方法中的字符串标识符。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="22d3" class="nn mm it nj b gy no np l nq nr"># As a string identifier<br/>model.compile(<strong class="nj iu">optimizer="rmsprop"</strong>,...)</span></pre><p id="97e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当以这种方式定义优化器时，没有调整学习率值的选项。默认学习率值将应用于优化器。</p><p id="d517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要更改默认值，我们需要避免使用优化器的字符串标识符。相反，我们应该为优化器使用正确的函数。在本例中，它是RMSprop()函数。新的学习率可以在该函数的<code class="fe ns nt nu nj b">learning_rate</code>参数中定义。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="1f93" class="nn mm it nj b gy no np l nq nr">from tensorflow.keras.optimizers import RMSprop<br/>model.compile(<strong class="nj iu">optimizer=RMSprop(learning_rate=0.005)</strong>,...)</span></pre><h1 id="1d1d" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">静态与动态学习率</h1><p id="6d94" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">到目前为止，我们使用静态学习率。我们仅在培训后<em class="lv">更改了学习率的值。所以，值是静态的！相反，动态学习率在</em>训练期间改变其值<em class="lv">。在大多数情况下，动态学习率比静态学习率表现得更好。</em></p><p id="71e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于动态学习率的信息，请阅读我的文章，<em class="lv"/><a class="ae ky" href="https://rukshanpramoditha.medium.com/learning-rate-schedules-and-decay-in-keras-optimizers-f68bf91de57d" rel="noopener"><em class="lv">Keras优化器中的学习率计划和衰减</em></a><em class="lv"/>。</p><h1 id="5ef6" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">结论</h1><p id="de4a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">学习率是最重要的神经网络超参数。它在训练网络的时候可以决定很多事情。</p><p id="4e83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Keras的大多数优化器中，默认的学习率值是0.001。这是培训入门的推荐值。</p><p id="3bbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当调整任何超参数时，不仅仅是学习率，我们应该总是试图缩小超参数的最佳值所在的范围。</p><p id="5b20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">调整学习率时，有必要在每个时期结束时监控训练和验证损失。您可以使用由<code class="fe ns nt nu nj b">model.fit()</code>方法返回的<strong class="lb iu"> <em class="lv">历史</em> </strong>对象中存储的信息绘制一个图。您还可以在训练过程中实时查看这些值。确保在<code class="fe ns nt nu nj b">model.fit()</code>方法中设置了<code class="fe ns nt nu nj b">verbose=2</code>来打印每个时期的值。</p><p id="1d8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我可以说，</p><blockquote class="lw"><p id="900c" class="lx ly it bd lz ma mb mc md me mf lu dk translated">调整神经网络的学习速率是很棘手的。这只能通过在每个时期后监控训练和验证损失并遵循系统方法来实现。这不是一次性的、容易的过程。这是一个迭代的任务！一旦获得，最佳值会随着这些因素而变化<a class="ae ky" href="#1821" rel="noopener ugc nofollow">。</a></p></blockquote></div><div class="ab cl pw px hx py" role="separator"><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb"/></div><div class="im in io ip iq"><p id="5c64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="5c5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果您有任何问题或反馈，请告诉我。</strong></p><h2 id="8762" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">阅读下一篇(推荐)</h2><ul class=""><li id="4984" class="nv nw it lb b lc nd lf ne li oz lm pa lq pb lu oa ob oc od bi translated"><strong class="lb iu">Keras优化器中的学习率计划和衰减</strong></li></ul><div class="qd qe gp gr qf qg"><a href="https://rukshanpramoditha.medium.com/learning-rate-schedules-and-decay-in-keras-optimizers-f68bf91de57d" rel="noopener follow" target="_blank"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">Keras优化器中的学习率计划和衰减</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">培训期间更改学习率的选项</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qp l"><div class="qq l qr qs qt qp qu ks qg"/></div></div></a></div><ul class=""><li id="2ffa" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">神经网络超参数分类</strong></li></ul><div class="qd qe gp gr qf qg"><a rel="noopener follow" target="_blank" href="/classification-of-neural-network-hyperparameters-c7991b6937c3"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">神经网络超参数的分类</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">受网络结构、学习和优化以及正则化效应的影响</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">towardsdatascience.com</p></div></div><div class="qp l"><div class="qv l qr qs qt qp qu ks qg"/></div></div></a></div><ul class=""><li id="653e" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">参数Vs超参数:有什么区别？</strong></li></ul><div class="qd qe gp gr qf qg"><a href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener follow" target="_blank"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">参数Vs超参数:区别是什么？</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">用4个不同的例子进行讨论</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qp l"><div class="qw l qr qs qt qp qu ks qg"/></div></div></a></div><ul class=""><li id="650a" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><strong class="lb iu">全集我的</strong> <a class="ae ky" href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75" rel="noopener"> <strong class="lb iu">神经网络与深度学习教程</strong></a><strong class="lb iu"/></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qx"><img src="../Images/92ef08f12975c0f02852a805b8f15f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*q1LeTIErTb8hRKQ3e4glgA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure></div><div class="ab cl pw px hx py" role="separator"><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb"/></div><div class="im in io ip iq"><h2 id="7d38" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">支持我当作家</h2><p id="655a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">注册会员</em> </strong> </a> <em class="lv">以获得无限制的媒体访问权限。它只需要每月5美元，我会收到你的会员费的一部分。</em></p><div class="qd qe gp gr qf qg"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="qh ab fo"><div class="qi ab qj cl cj qk"><h2 class="bd iu gy z fp ql fr fs qm fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="qn l"><h3 class="bd b gy z fp ql fr fs qm fu fw dk translated">阅读Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qo l"><p class="bd b dl z fp ql fr fs qm fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qp l"><div class="qy l qr qs qt qp qu ks qg"/></div></div></a></div><p id="cbae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p></div><div class="ab cl pw px hx py" role="separator"><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb"/></div><div class="im in io ip iq"><h2 id="f7cb" class="nn mm it bd mn oj ok dn mr ol om dp mv li on oo mx lm op oq mz lq or os nb ot bi translated">MNIST数据集信息</h2><ul class=""><li id="f1cf" class="nv nw it lb b lc nd lf ne li oz lm pa lq pb lu oa ob oc od bi translated"><strong class="lb iu">引用:</strong>邓，l，2012。用于机器学习研究的手写数字图像mnist数据库。<strong class="lb iu"> IEEE信号处理杂志</strong>，第29卷第6期，第141–142页。</li><li id="532e" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">来源:</strong><a class="ae ky" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/mnist/</a></li><li id="b110" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><strong class="lb iu">许可:</strong><em class="lv">Yann le Cun</em>(NYU库朗研究所)和<em class="lv"> Corinna Cortes </em>(纽约谷歌实验室)持有MNIST数据集的版权，该数据集在<em class="lv">知识共享署名-共享4.0国际许可</em>(<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">CC BY-SA</strong></a>)下可用。您可以在此了解有关不同数据集许可类型<a class="ae ky" href="https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade" rel="noopener">的更多信息。</a></li></ul></div><div class="ab cl pw px hx py" role="separator"><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb"/></div><div class="im in io ip iq"><p id="3a02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="qz ra ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----362111c5c783--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/>T43】2022–09–21</p></div></div>    
</body>
</html>