<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-a0ec2792b70#2022-11-15">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-a0ec2792b70#2022-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/89b1276a7b8a060cce142520835c5b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g0HJqbQTtFkPy4nh.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="3da8" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第四部分:矩阵分解的潜在语义</h2></div><p id="b284" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将执行潜在语义(或主题)分析，这是一种通过产生与文档和术语相关的新“概念”(也称为语义或主题)来分析文档和相关术语之间关系的技术。这种方法建立在<em class="lu">分布假设</em>的基础上，根据该假设“具有相似分布的语言术语具有相似的含义”。因此，意义相近的术语很可能出现在相似的文本中。</p><p id="4621" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">潜在语义分析包括两个主要步骤:</p><ol class=""><li id="1e69" class="lv lw jj la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">创建一个 Word 文档矩阵(你可以在这里阅读更多关于文档矢量化的内容<a class="ae jg" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-102064f24372"/></li><li id="c701" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">降低矩阵的维度，以产生新的变量(这将是我们的语义或主题)。</li></ol><p id="0265" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我会进行两种矩阵分解技术:<strong class="la jk">奇异值分解</strong>和<strong class="la jk">非负矩阵分解</strong>。他们都依赖于一个假设，即在高维特征空间中，实际上可能需要更少的维度来解释数据中的变化(你可以在这里阅读更多关于维度减少的概念→<a class="ae jg" rel="noopener" target="_blank" href="/pca-eigenvectors-and-eigenvalues-1f968bc6777a">https://towardsdatascience . com/PCA-intrinsic vectors-and-enforcern-1 f 968 BC 6777 a</a>)。</p><p id="e7cc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种方法的主要区别在于，SVD 将文档 X 的矢量化矩阵分解成 3 个低维矩阵，而 NMF 只用 2 个矩阵就完成了分解。</p><p id="17e3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用 Python 来实现它们。</p><h2 id="6c19" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">二维分析</h2><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="5b70" class="nl mk jj nh b be nm nn l no np">#create the original matrix X (term-document matrix), vectorized with tf-idf weights.<br/><br/>documents = df_factor.Tokens.apply(str).tolist()<br/>tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', analyzer='word', <br/>                                   min_df=0.001, max_df=0.5, sublinear_tf=True, use_idf=True)<br/>X = tfidf_vectorizer.fit_transform(documents)<br/><br/>from sklearn.decomposition import TruncatedSVD #svd<br/><br/># set number of latent components<br/>k = 10<br/><br/>svd = TruncatedSVD(n_components=k)<br/>%time U = svd.fit_transform(X)<br/>S = svd.singular_values_<br/>V = svd.components_<br/><br/>from sklearn.decomposition import NMF #nmf<br/><br/>nmf = NMF(n_components=k, init='nndsvd', random_state=0)<br/><br/>%time W = nmf.fit_transform(X)<br/>H = nmf.components_<br/><br/>print("SVD matrices shapes: ", U.shape, S.shape, V.shape)<br/>print("NMF matrices shapes: ",W.shape, H.shape)<br/><br/>Wall time: 2.5 s<br/>Wall time: 11.4 s<br/>SVD matrices shapes:  (13087, 10) (10,) (10, 40808)<br/>NMF matrices shapes:  (13087, 10) (10, 40808)<br/><br/>import numpy as np<br/>def show_topics(A, vocabulary, topn=5):<br/>    """<br/>    find the top N words for each of the latent dimensions (=rows) in a<br/>    """<br/>    topic_words = ([[vocabulary[i] for i in np.argsort(t)[:-topn-1:-1]]<br/>                    for t in A])<br/>    return [', '.join(t) for t in topic_words]<br/></span></pre><p id="b523" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们打印矢量化文档矩阵中的顶级术语，用 TF-IDF 分数进行加权(您可以在前面的部分<a class="ae jg" rel="noopener" target="_blank" href="/natural-language-process-for-judicial-sentences-with-python-102064f24372">这里</a>中阅读关于该分数的更多信息)。</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="5907" class="nl mk jj nh b be nm nn l no np">#SVD<br/>terms = tfidf_vectorizer.get_feature_names()<br/><br/>sorted(show_topics(V, terms))</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="a286" class="nl mk jj nh b be nm nn l nr np">['antitrust, antitrust division, bid, rigging, bid rigging',<br/> 'child, criminal division, safe childhood, project safe, childhood',<br/> 'child, safe childhood, project safe, childhood, exploitation',<br/> 'epa, environmental, clean, environment, natural',<br/> 'injunction, customers, complaint, preparing, preparers',<br/> 'medicare, hhs, health, health care, care',<br/> 'osc, ina, immigration, citizenship, discrimination provision',<br/> 'rights, civil rights, rights division, discrimination, employment',<br/> 'tax, fraud, false, prison, irs',<br/> 'tax, returns, irs, tax returns, tax division']</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="b9e9" class="nl mk jj nh b be nm nn l no np">#NMF<br/>sorted(show_topics(H, terms))</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="288b" class="nl mk jj nh b be nm nn l nr np">['antitrust, antitrust division, bid, rigging, bid rigging',<br/> 'child, safe childhood, project safe, childhood, exploitation',<br/> 'epa, environmental, clean, environment, natural',<br/> 'false claims, claims act, claims, civil division, health',<br/> 'fbi, indictment, police, security, law',<br/> 'medicare, hhs, health, health care, care',<br/> 'osc, ina, employment, citizenship, anti discrimination',<br/> 'rights, civil rights, rights division, civil, discrimination',<br/> 'tax, irs, tax division, returns, tax returns',<br/> 'tax, returns, customers, injunction, tax returns']</span></pre><p id="e702" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们画出低维项矩阵。</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="0d5c" class="nl mk jj nh b be nm nn l no np">#Initializing a plotting function<br/><br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>from matplotlib import colors<br/>import seaborn as sns<br/><br/>sns.set_context('notebook')<br/><br/>def plot_vectors(vectors, V, title='VIZ', labels=None, dimensions=3):<br/>    """<br/>    plot the vectors in 2 or 3 dimensions. <br/>    If labels are supplied, use them to color the data accordingly<br/>    """<br/>    # set up graph<br/>    fig = plt.figure(figsize=(10,10))<br/><br/>    # create data frame<br/>    df = pd.DataFrame(data={'x':vectors[:,0], 'y': vectors[:,1]})<br/>    # add labels, if supplied<br/>    if labels is not None:<br/>        df['label'] = labels<br/>    else:<br/>        df['label'] = [''] * len(df)<br/><br/>    # assign colors to labels<br/>    cm = plt.get_cmap('tab20b') # choose the color palette<br/>    n_labels = len(df.label.unique())<br/>    label_colors = [cm(1. * i/n_labels) for i in range(n_labels)]<br/>    cMap = colors.ListedColormap(label_colors)<br/>        <br/>    # plot in 3 dimensions<br/>    if dimensions == 3:<br/>        # add z-axis information<br/>        df['z'] = vectors[:,2]<br/>        # define plot<br/>        ax = fig.add_subplot(111, projection='3d')<br/>        frame1 = plt.gca() <br/>        # remove axis ticks<br/>        frame1.axes.xaxis.set_ticklabels([])<br/>        frame1.axes.yaxis.set_ticklabels([])<br/>        frame1.axes.zaxis.set_ticklabels([])<br/><br/>        # plot each label as scatter plot in its own color<br/>        for l, label in enumerate(df.label.unique()):<br/>            df2 = df[df.label == label]<br/>            color_values = [label_colors[l]] * len(df2)<br/>            ax.scatter(df2['x'], df2['y'], df2['z'], <br/>                       c=color_values, <br/>                       cmap=cMap, <br/>                       edgecolor=None, <br/>                       label=label, <br/>                       alpha=0.4, <br/>                       s=100)<br/>            <br/>        topics = sorted(show_topics(V.components_, tfidf_vectorizer.get_feature_names()))<br/>        print(topics)<br/>        frame1.axes.set_xlabel(topics[0])<br/>        frame1.axes.set_ylabel(topics[1])<br/>        frame1.axes.set_zlabel(topics[2])<br/>      <br/>    # plot in 2 dimensions<br/>    elif dimensions == 2:<br/>        ax = fig.add_subplot(111)<br/>        frame1 = plt.gca() <br/>        frame1.axes.xaxis.set_ticklabels([])<br/>        frame1.axes.yaxis.set_ticklabels([])<br/><br/>        for l, label in enumerate(df.label.unique()):<br/>            df2 = df[df.label == label]<br/>            color_values = [label_colors[l]] * len(df2)<br/>            ax.scatter(df2['x'], df2['y'], <br/>                       c=color_values, <br/>                       cmap=cMap, <br/>                       edgecolor=None, <br/>                       label=label, <br/>                       alpha=0.4, <br/>                       s=100)<br/>        topics = sorted(show_topics(V.components_, tfidf_vectorizer.get_feature_names()))<br/>        print(topics)<br/>        frame1.axes.set_xlabel(topics[0])<br/>        frame1.axes.set_ylabel(topics[1])<br/><br/>    else:<br/>        raise NotImplementedError()<br/>    plt.legend(ncol = 5, loc = "upper left", frameon = True, fancybox = True)<br/>    ax.legend(frameon = True, ncol = 2, fancybox = True, title_fontsize = 15,<br/>              loc = 'center left', bbox_to_anchor = (1, 0.5), labelspacing = 2.5, borderpad = 2)<br/>    plt.title(title)<br/>#     plt.legend()<br/>    plt.show()<br/><br/></span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="008c" class="nl mk jj nh b be nm nn l no np"># now let's perform the same computations with 2 and 3 dimensions, so that we can visualize them. Let's start with 2 dims.<br/><br/>low_dim_svd = TruncatedSVD(n_components = 2)<br/>low_dim_U = low_dim_svd.fit_transform(X)<br/>sorted(show_topics(low_dim_svd.components_, tfidf_vectorizer.get_feature_names()))</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="12fc" class="nl mk jj nh b be nm nn l nr np">['medicare, hhs, health, health care, care', 'tax, fraud, false, prison, irs']</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="510b" class="nl mk jj nh b be nm nn l no np">low_dim_nmf = NMF(n_components=2, init='nndsvd')<br/>low_dim_W = low_dim_nmf.fit_transform(X)<br/>sorted(show_topics(low_dim_nmf.components_, tfidf_vectorizer.get_feature_names()))</span></pre><pre class="nq ng nh ni bn nj nk bi"><span id="df7e" class="nl mk jj nh b be nm nn l nr np">['medicare, health, health care, care, hhs',<br/> 'tax, irs, returns, prison, tax division']</span></pre><p id="a652" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种方法的结果似乎是一致的。让我们来看看策划:</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="153e" class="nl mk jj nh b be nm nn l no np">plot_vectors(low_dim_U, low_dim_svd, title = 'SVD 2d', dimensions=2)</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8940f5243f713e298802fa4f0de4d73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*xnM2HAqK9FnumOgvVxJE4g.png"/></div></figure><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="7fc5" class="nl mk jj nh b be nm nn l no np">plot_vectors(low_dim_W, low_dim_nmf, title = 'NMF 2d', dimensions=2)</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/cfe04c876e7421fc5e38048a61252465.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*RGec8F0HDaGXQ-PEAzyiKw.png"/></div></figure><p id="75a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我想进行同样的分析，但是也要考虑我的文章的标签(或类别)。</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="2f79" class="nl mk jj nh b be nm nn l no np">#creating a df with records with only one label<br/>data_single = df_factor.copy()[df_factor[categories].sum(axis = 1) == 1]<br/><br/><br/>documents = data_single.text.apply(str).tolist()<br/>tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', analyzer='word', <br/>                                   min_df=0.001, max_df=0.5, sublinear_tf=True, use_idf=True)<br/>X = tfidf_vectorizer.fit_transform(documents)<br/><br/>from sklearn.decomposition import TruncatedSVD # this also works with sparse matrices<br/>labels = [i[0] for i in data_single.category] #each component is a list, I want a list of elements not a list of lists<br/># set number of latent components<br/>k = 10<br/><br/>svd = TruncatedSVD(n_components=k)<br/>%time U = svd.fit_transform(X)<br/>S = svd.singular_values_<br/>V = svd.components_<br/><br/>from sklearn.decomposition import NMF<br/><br/>nmf = NMF(n_components=k, init='nndsvd', random_state=0)<br/><br/>%time W = nmf.fit_transform(X)<br/>H = nmf.components_</span></pre><p id="e47d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这种分析，我将只依靠 NMF 方法。事实上，我注意到用 SVD 提取的主题非常相似:</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="1f8a" class="nl mk jj nh b be nm nn l no np">low_dim_svd = TruncatedSVD(n_components = 2)<br/>low_dim_U = low_dim_svd.fit_transform(X)<br/>sorted(show_topics(low_dim_svd.components_, tfidf_vectorizer.get_feature_names()))<br/><br/>['tax, irs, tax division, fraud, returns',<br/> 'tax, returns, tax division, irs, tax returns']</span></pre><p id="eb3e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让潜在的主题覆盖尽可能多的信息，我不希望两个或更多的组件带来相同的信息，因此是多余的。</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="2504" class="nl mk jj nh b be nm nn l no np">low_dim_nmf = NMF(n_components=2, init='nndsvd')<br/>low_dim_W = low_dim_nmf.fit_transform(X)<br/>plot_vectors(low_dim_W, low_dim_nmf, labels = labels, title = 'NMF 2d', dimensions=2)</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/c02137d53041a1a2a94a2038fdc6154d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-Sz-33cbh10slBkUrmqUA.png"/></div></div></figure><h2 id="1cd5" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">三维分析</h2><p id="4c35" class="pw-post-body-paragraph ky kz jj la b lb nv kk ld le nw kn lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">现在让我们做同样的三维。同样，在这种情况下，我将只依靠 NMF 方法。事实上，在查看 SVD 方法的第二个和第三个潜在主题(或组件)时，我注意到它们非常相似:</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="2268" class="nl mk jj nh b be nm nn l nr np">['medicare, hhs, health, health care, care',<br/> 'tax, fraud, false, prison, irs',<br/> 'tax, returns, irs, tax returns, tax division']</span></pre><p id="a1a8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，由于上面解释的同样的原因，我将不再继续 SVD 方法。</p><p id="cfb1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，让我们继续创建 3D 矩阵并绘制结果:</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="60e6" class="nl mk jj nh b be nm nn l no np">low_dim_nmf = NMF(n_components=3, init='nndsvd')<br/>low_dim_W = low_dim_nmf.fit_transform(X)<br/><br/>plot_vectors(low_dim_W, low_dim_nmf, title = 'NMF 3d', dimensions=3)</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0b1926fb20b8dc3122d16679e02f501b.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*03O677l3zttXRxbz9yeTzg.png"/></div></figure><p id="95ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样通过 3d 分析，我想通过考虑我的文章的标签(或类别)来为我的图添加信息。</p><pre class="nc nd ne nf gt ng nh ni bn nj nk bi"><span id="a212" class="nl mk jj nh b be nm nn l no np"><br/>low_dim_nmf = NMF(n_components=3, init='nndsvd')<br/>low_dim_W = low_dim_nmf.fit_transform(X)<br/>sorted(show_topics(low_dim_nmf.components_, tfidf_vectorizer.get_feature_names()))<br/>plot_vectors(low_dim_W, low_dim_nmf, labels = labels, title = 'NMF 3d', dimensions=3)</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/523fa5929204aea433ffa0304b5100e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iB1NNt2nnhKdx_ZVQ4fYLw.png"/></div></div></figure><h2 id="02ed" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">结论</h2><p id="3747" class="pw-post-body-paragraph ky kz jj la b lb nv kk ld le nw kn lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">从上面的分析来看，我们的方法似乎能够将属于具有相同标签的文档的单词聚类到相同的潜在主题中。这可能有助于搜索引擎或自动司法判决分类，并且通常有助于在司法知识库中导航。</p><p id="4037" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一篇文章中，将继续处理主题和语义，所以请继续关注第 5 部分！</p><h2 id="f7db" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">参考</h2><ul class=""><li id="3980" class="lv lw jj la b lb nv le nw lh oc ll od lp oe lt of mb mc md bi translated">自然语言工具包</li><li id="1a3f" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt of mb mc md bi translated"><a class="ae jg" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">Python 中的 spaCy 工业级自然语言处理</a></li><li id="bea5" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt of mb mc md bi translated"><a class="ae jg" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="f61b" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt of mb mc md bi translated"><a class="ae jg" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部 2009-2018 年新闻发布| Kaggle </a></li><li id="6e3b" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt of mb mc md bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Distributional_semantics" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Distributional_semantics</a></li><li id="6efd" class="lv lw jj la b lb me le mf lh mg ll mh lp mi lt of mb mc md bi translated"><a class="ae jg" href="https://aurelieherbelot.net/research/distributional-semantics-intro/" rel="noopener ugc nofollow" target="_blank">https://aurelieherbelot . net/research/distributional-semantics-intro/</a></li></ul></div></div>    
</body>
</html>