<html>
<head>
<title>Text Summarization with GPT2 and Layer AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于GPT2和人工智能层的文本摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-summarization-with-gpt2-and-layer-ai-599625085d8e#2022-05-13">https://towardsdatascience.com/text-summarization-with-gpt2-and-layer-ai-599625085d8e#2022-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="686a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用拥抱的人脸变形库和人工智能层微调GPT2进行文本摘要</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/04d37c094c5418e3a6108cf1a2461bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4-01ch8xrF5Bk0WwPrtcg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@aaronburden" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在Unsplash上拍摄</p></figure><p id="1055" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变形金刚在2017年的著名文章<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">中首次亮相后，很快成为NLP中最受欢迎的型号</a>。以非顺序方式(与RNNs相反)分析文本的能力使得大型模型能够被训练。事实证明，注意力机制的引入对于概括文本具有极大的价值。</p><p id="2c21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在深度学习出现之前，以前的NLP方法更多的是基于规则的，更简单的(纯统计)机器学习算法被教会在文本中查找单词和短语，当发现这些短语时，会创建特定的回复。</p><p id="323c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该研究发表后，涌现出无数流行的变形金刚，其中最著名的是<strong class="ky ir">GPT</strong>(<strong class="ky ir"><em class="ls">G</em></strong><em class="ls">generative</em><strong class="ky ir"><em class="ls">P</em></strong><em class="ls">re-trained</em><strong class="ky ir"><em class="ls">T</em></strong><em class="ls">transformer</em>)。<em class="ls"> OpenAI </em>，人工智能研究的先驱之一，创造并训练了GPT模型。<a class="ae kv" href="https://openai.com/blog/openai-api/" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>是最新版本，有1750亿个参数。因为这个模型非常复杂，OpenAI决定不开源它。人们可以在完成漫长的注册过程后通过API使用它。</p><p id="31d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将用一些技术背景来介绍变压器。然后，我们将使用Layer来获取预训练版本的GPT2，以对其进行微调，用于总结目的。我们将使用的数据集由亚马逊评论组成，可以通过以下链接在Kaggle中找到:</p><div class="lt lu gp gr lv lw"><a href="https://www.kaggle.com/code/currie32/summarizing-text-with-amazon-reviews/notebook" rel="noopener  ugc nofollow" target="_blank"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">用亚马逊评论总结文本</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用来自亚马逊美食评论的数据</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">www.kaggle.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kp lw"/></div></div></a></div><h2 id="f579" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">概观</h2><ol class=""><li id="570d" class="ne nf iq ky b kz ng lc nh lf ni lj nj ln nk lr nl nm nn no bi translated">变形金刚(电影名)</li><li id="4986" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">亚马逊评论数据集</li><li id="5041" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">微调GPT2</li><li id="2286" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">评估我们的微调模型</li><li id="caba" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">结论</li></ol><p id="21b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我在本文中包含了最有启发性的代码。这个项目的完整代码库和数据集可以在我的公共<a class="ae kv" href="https://colab.research.google.com/drive/1iDhg8ss-BW0ZpzL-umJhd9XAm7xUlz-g?authuser=2#scrollTo=AY5gYk6U4-l7" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>或<a class="ae kv" href="https://github.com/aymanehachcham/GPT2_Text_Summarization" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>中找到。</p><div class="lt lu gp gr lv lw"><a href="https://github.com/aymanehachcham/GPT2_Text_Summarization" rel="noopener  ugc nofollow" target="_blank"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">GitHub-aymanehachham/gp T2 _ Text _ summary</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">这个项目的重点是微调GPT2模型，在公共Amanzon评论数据集上执行文本摘要…</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">github.com</p></div></div><div class="mf l"><div class="nu l mh mi mj mf mk kp lw"/></div></div></a></div><div class="lt lu gp gr lv lw"><a href="https://colab.research.google.com/drive/1iDhg8ss-BW0ZpzL-umJhd9XAm7xUlz-g?usp=sharing#scrollTo=rinybKnGJLpY" rel="noopener  ugc nofollow" target="_blank"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">谷歌联合实验室</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">编辑描述</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">colab.research.google.com</p></div></div><div class="mf l"><div class="nv l mh mi mj mf mk kp lw"/></div></div></a></div></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h2 id="34cc" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">快速参观变形金刚图书馆</h2><h2 id="24c7" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">基于注意力的模型</h2><p id="a4bb" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">机器学习中的注意力机制是基于我们自己的大脑皮层如何工作的。当我们检查一张图片来描述它时，我们自然会将注意力集中在几个我们知道保存着关键信息的关键地方。我们不会以相同的强度检查图像的每个细节。在处理要分析的复杂数据时，这种方法有助于节省处理资源。</p><p id="c07b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，当口译员将材料从源语言翻译成目标语言时，他们根据以前的经验知道源句子中的哪些单词对应于翻译短语中的哪些术语。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/b19d125da7b28facabd8b549c60b6a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGE5mmcD__BmWUaw9E5nPA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由纳迪·博罗迪纳在Unsplash上拍摄</p></figure><h2 id="8706" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">GPT2</h2><p id="dc2e" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">GPT语言模型最初是在2018年由Alec、Jeffrey Wu、Rewon Child、David Luan 在论文“<a class="ae kv" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ls">中介绍的，语言模型是无监督的多任务学习器</em></a><em class="ls"/><em class="ls">，其目标是开发一个可以从先前产生的文本中学习的系统。它将能够以这种方式为完成一个短语提供多种选择，节省时间，并增加文本的多样性和语言深度。这一切都完成了，没有任何语法错误。</em></p><p id="9e3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GPT2架构的主要层是关注层。在不深究其技术细节的情况下，我想列出它的核心特性:</p><blockquote class="oh oi oj"><p id="fe7e" class="kw kx ls ky b kz la jr lb lc ld ju le ok lg lh li ol lk ll lm om lo lp lq lr ij bi translated">GPT2使用字节对编码在其词汇表中创建令牌。这意味着标记通常是单词的一部分。</p><p id="8f90" class="kw kx ls ky b kz la jr lb lc ld ju le ok lg lh li ol lk ll lm om lo lp lq lr ij bi translated">GPT-2以因果语言建模(CLM)为目标进行训练，因此能够预测序列中的下一个单词。GPT-2可以利用这种能力创建语法连贯的文本。</p><p id="eb7c" class="kw kx ls ky b kz la jr lb lc ld ju le ok lg lh li ol lk ll lm om lo lp lq lr ij bi translated">GPT-2生成合成文本样本，以响应用任意输入启动的模型。这个模型就像变色龙一样——它适应条件文本的风格和内容。</p></blockquote></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h2 id="c3d9" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">亚马逊评论数据集</h2><p id="4fbb" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">Kaggle中呈现的数据集的目标是开发一种算法，可以为亚马逊美食评估提供有意义的摘要。这个数据集在Kaggle上提供，有超过500，000条评论。</p><p id="467a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当顾客在亚马逊上写评论时，他们会创建一个文本评论和一个标题。综述的标题用作数据集中的摘要。</p><p id="2550" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">样品:</em></strong><em class="ls">我买过几款活力狗粮罐头，发现质量都不错。这种产品看起来更像炖肉，而不是加工过的肉，而且闻起来更香。我的拉布拉多很挑剔，她比大多数人更喜欢这个产品。</em></p><p id="f361" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">总结</em></strong>:<em class="ls">优质狗粮</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/a9faeef7bb09590e8a226e9c0679dded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pNLQWi8QGd2DF3UjiykITA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上传到图层的数据集中的样本</p></figure><p id="f5e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集中有大约71K个实例，这足以训练一个GPT-2模型。</p><p id="e3da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始处理阶段之前，我们需要建立与层的连接。我们需要登录并初始化一个项目:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="969a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以访问图层控制台，初始化我们的项目，并准备好记录我们将使用的数据集和模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/ea07a2307686151483ca8efa21af40a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvk-6sqYXN8l7GrtMyReUg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从层控制台捕获</p></figure><p id="9f7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们将数据保存到层:</p><p id="1e3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们通过将Github repo克隆到Colab中来安装它:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="b15a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以这样从层中获取数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><h2 id="a5b1" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">执行一些数据处理</h2><p id="23cd" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">GPT 2号的多任务处理能力是它最吸引人的特点之一。同时，同一个模型可能在许多任务上被训练。但是，我们必须使用组织文件中概述的相关任务指示器。</p><p id="5962" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">TL；博士</strong>符号，代表“太冗长；没读过”是总结工作的理想指示词。</p><p id="1af8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">TL；DR </strong>符号可以用作填充元素，以缩短评审文本，并向模型指示重要内容到此为止。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="fb8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还需要获得句子输入长度的平均值:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="623a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，平均长度总计为每句话73个单词。我们可以推断，最大长度100将涵盖大多数情况，因为单词的平均实例长度是73。</p><p id="2805" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用于处理数据样本的代码可能变得复杂且难以维护；为了更好的可读性和模块化，我们希望数据集代码与模型训练代码分离。因此，我们可以使用PyTorch Dataset类作为包装器，它将充当一个紧凑的模块，将文本评论转换为准备用于训练的张量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="e750" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">导入模型后，我们初始化数据集和数据加载器组件，为训练阶段做准备。我们可以从层调用我们的记号赋予器:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><h2 id="1faf" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">微调GPT2</h2><p id="90e9" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">训练过程很简单，因为GPT2能够完成多项任务，包括总结、生成和翻译。对于摘要，我们只需要包含数据集的标签作为输入。训练部分包括构建GPT2模型并上传到层。</p><p id="f34f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训将完全在层中执行，我们将使用<a class="ae kv" href="https://docs.app.layer.ai/docs/reference/fabrics" rel="noopener ugc nofollow" target="_blank"> f-gpu-small fabric </a>，这是一个具有48 GB内存的小型gpu。我们首先从训练循环开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="3648" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们用需要的参数建立模型。我们将它上传到层:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="994f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们调用“layer.run”函数在层中运行训练:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="aa16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练开始了。这可能需要很长时间，具体取决于纪元和可用的资源。</p><h2 id="a75d" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">评估我们的微调模型</h2><p id="a607" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">一旦您微调了我们的模型，我们现在就可以开始按照各自的方法处理评论:</p><ul class=""><li id="1093" class="ne nf iq ky b kz la lc ld lf or lj os ln ot lr ou nm nn no bi translated"><strong class="ky ir">步骤1 </strong>:首先对模型进行评审。</li><li id="7ede" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr ou nm nn no bi translated"><strong class="ky ir">第二步</strong>:然后从所有我们有top-k选项的评论中，选择一个。</li><li id="8bad" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr ou nm nn no bi translated"><strong class="ky ir">第三步</strong>:选择被添加到概要中，当前序列被输入到模型中。</li><li id="34b8" class="ne nf iq ky b kz np lc nq lf nr lj ns ln nt lr ou nm nn no bi translated"><strong class="ky ir">步骤4 </strong>:应重复步骤2和3，直到达到max_length或产生EOS令牌。</li></ul><p id="0c6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从模型的所有生成预测中选择前k个预测:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="b226" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们定义我们的推理方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="2e61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以用3个示例审查来测试上面的代码，并查看生成的摘要。</p><p id="47b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们调用来自层的训练模型:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="6c6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从测试数据中抽取一些样本:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="6aa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最终结果:</p><p id="0595" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">点评1 </strong> <br/>文字点评:“<em class="ls">爱这些芯片。味道很好，非常脆，非常容易清理整个3盎司。一口气搞定。没有油腻的余味。原味和烧烤口味是我的最爱，但我还没有尝过所有的口味。伟大的产品。</em></p><p id="6bd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">相关摘要</strong> : {' <em class="ls">非常好吃！，'喜欢这些薯片！'，'我最喜欢的水壶芯片</em> '}</p><p id="ae59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">回顾2 </strong> <br/>文字回顾:“<em class="ls">我们已经很多年没有咸菜了，因为不需要的成分。这个牌子很好吃，不含有害成分。这种情况下也比当地超市</em>便宜很多</p><p id="8571" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关联摘要</strong> : {' <em class="ls">好吃'，'好吃'，'好棒的产品</em>！'}</p><p id="7c47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文字评论:“<em class="ls">这是这一品种的爱好者的最佳英式早餐茶，我试过很多种，包括从英国进口的。经过20年的寻找，我找到了最美味的茶的一个非常合理的价格。</em></p><p id="be33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关联摘要</strong> : {' <em class="ls">妙茶'，'爱喝茶的人最好的茶'，'爱喝茶的人最好的茶</em> '}</p><h2 id="4266" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lf mu mv mw lj mx my mz ln na nb nc nd bi translated">总结想法</h2><p id="f644" class="pw-post-body-paragraph kw kx iq ky b kz ng jr lb lc nh ju le lf od lh li lj oe ll lm ln of lp lq lr ij bi translated">考虑到我们所做的大量训练，我们目前所取得的成绩已经相当不错了。Layer简化了导入模型和记号化器的整个过程。记录所有模型工件和数据集的选项也非常有用，因为您可以实时跟踪工作的效果。还有可能分享你的项目，在团队中协作，所以绝对值得。</p><p id="f228" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有代码都托管在上面提到的<a class="ae kv" href="https://colab.research.google.com/drive/1iDhg8ss-BW0ZpzL-umJhd9XAm7xUlz-g?authuser=2#scrollTo=4JObyccnfeZO" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>中，请随时查看并自行尝试。</p></div></div>    
</body>
</html>