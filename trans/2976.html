<html>
<head>
<title>How to Fine-Tune an NLP Regression Model with Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用转换器微调NLP回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f#2022-06-29">https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f#2022-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c09c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从数据预处理到使用的完整指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8947c6c9f8d0217916b7cb4156a44c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pLDgN56OWQedBWh9.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@deepmind?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="bb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> HuggingFace </a>这样的在线图书馆为我们提供了最先进的预训练人工智能模型，可以用于数据科学的许多不同应用。在本帖中，我们将向您展示如何使用预先训练好的模型来解决回归问题。我们将使用的预训练模型是DistilBERT，它是著名的BERT的更轻、更快的版本，其性能为95%。</p><p id="34c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有来自在线广告的文本，并且它的响应率被广告集标准化了。我们的目标是创建一个可以预测广告效果的机器学习模型。</p><p id="9ab8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过导入必要的库并导入我们的数据来开始编码:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2ff4" class="ma mb it lw b gy mc md l me mf">import numpy as np<br/>import pandas as pd</span><span id="bc14" class="ma mb it lw b gy mg md l me mf">import transformers<br/>from datasets import Dataset,load_dataset, load_from_disk<br/>from transformers import AutoTokenizer, AutoModelForSequenceClassification</span><span id="9d6a" class="ma mb it lw b gy mg md l me mf">X=pd.read_csv('ad_data.csv')<br/>X.head(3)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c66f5a0233b55803783548c278827e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*YAgEc9qO-roVChcR.png"/></div></figure><p id="9769" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">文本</strong>代表广告文本，而<strong class="lb iu">标签</strong>是标准化回复率。</p><h1 id="3635" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">熊猫到数据集</h1><p id="c08e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">为了使用我们的数据进行训练，我们需要将<strong class="lb iu"> Pandas数据帧</strong>转换为“<strong class="lb iu">数据集</strong>格式。此外，我们希望将数据分为训练和测试，以便我们可以评估模型。这些可以通过运行以下命令轻松完成:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="09c5" class="ma mb it lw b gy mc md l me mf">dataset = Dataset.from_pandas(X,preserve_index=False) <br/>dataset = dataset.train_test_split(test_size=0.3) </span><span id="c2a6" class="ma mb it lw b gy mg md l me mf">dataset</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c7914f7238dd8f3f58c35004b833dbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*ZOLISw1b6nt69e1X.png"/></div></figure><p id="fedd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，数据集对象包含训练集和测试集。您仍然可以访问如下所示的数据:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bf7f" class="ma mb it lw b gy mc md l me mf">dataset['train']['text'][:5]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3dfe25588df5a42775b966b73c4322e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*UtYADlDs-PWF9cks.png"/></div></figure><h1 id="d3b9" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">令牌化&amp;如何添加新令牌</h1><p id="3318" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">我们将使用一个预训练的模型，因此我们需要导入它的标记器并标记我们的数据。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4160" class="ma mb it lw b gy mc md l me mf">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")</span></pre><p id="6229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来标记一个句子，看看我们得到了什么:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4703" class="ma mb it lw b gy mc md l me mf">tokenizer('🚨 JUNE DROP LIVE 🚨')['input_ids']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/38d1e16658d44f97947e182ec6f68395.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/0*saouZ9uv8q-yXCd_.png"/></div></figure><p id="6028" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以解码这些id并看到实际的令牌:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a37f" class="ma mb it lw b gy mc md l me mf">[tokenizer.decode(i) for i in tokenizer('🚨 JUNE DROP LIVE 🚨')['input_ids']]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/58a3dee3bad617478798a790f15f3db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/0*uyyFVKMDUkl_DMQQ.png"/></div></figure><p id="2147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【CLS】</strong>和<strong class="lb iu">【SEP】</strong>是特殊的标记，总是出现在句子的开头和结尾。如你所见，代替表情符号的是🚨是<strong class="lb iu">【UNK】</strong>令牌，表示令牌未知。这是因为预先训练好的模型<strong class="lb iu">蒸馏器</strong>的单词袋里没有表情符号。但是，我们可以向标记器添加更多的标记，以便在我们根据数据调整模型时可以对它们进行训练。让我们给我们的符号化器添加一些表情符号。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7b27" class="ma mb it lw b gy mc md l me mf">for i in ['🚨', '🙂', '😍', '✌️' , '🤩 ']:<br/>    tokenizer.add_tokens(i)</span></pre><p id="2d81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果你将句子符号化，你会看到表情符号仍然是表情符号，而不是[UNK]符号。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="38c6" class="ma mb it lw b gy mc md l me mf">[tokenizer.decode(i) for i in tokenizer('🚨 JUNE DROP LIVE 🚨')['input_ids']]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/8df6d0529877ce10fd9d7bf7e97580e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*Q5I7X4v9lu7eAFlo.png"/></div></figure><p id="4d37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是对数据进行标记。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="216f" class="ma mb it lw b gy mc md l me mf">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True)</span><span id="9525" class="ma mb it lw b gy mg md l me mf">tokenized_datasets = dataset.map(tokenize_function, batched=True)</span></pre><h1 id="a9c3" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">微调模型</h1><p id="041a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">是时候导入预先训练好的模型了。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8130" class="ma mb it lw b gy mc md l me mf">model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=1)</span></pre><p id="6f4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据文档，对于回归问题，我们必须通过<strong class="lb iu"> num_labels=1 </strong>。</p><p id="a6d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们需要调整令牌嵌入的大小，因为我们向令牌化器添加了更多的令牌。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="02e5" class="ma mb it lw b gy mc md l me mf">model.resize_token_embeddings(len(tokenizer))</span></pre><h1 id="af31" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">度量函数</h1><p id="7783" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在回归问题中，您试图预测一个连续值。因此，您需要度量预测值和真实值之间距离的指标。最常见的指标是MSE(均方误差)和RMSE(均方根误差)。对于这个应用程序，我们将使用RMSE，我们需要创建一个函数在训练数据时使用它。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2686" class="ma mb it lw b gy mc md l me mf">from datasets import load_metric<br/></span><span id="726c" class="ma mb it lw b gy mg md l me mf">def compute_metrics(eval_pred):<br/>    predictions, labels = eval_pred<br/>    rmse = mean_squared_error(labels, predictions, squared=False)<br/>    return {"rmse": rmse}</span></pre><h1 id="9ec4" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">训练模型</h1><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4fb9" class="ma mb it lw b gy mc md l me mf">from transformers import TrainingArguments, Trainer</span><span id="515f" class="ma mb it lw b gy mg md l me mf">training_args = TrainingArguments(output_dir="test_trainer",<br/>                                  logging_strategy="epoch",<br/>                                  evaluation_strategy="epoch",<br/>                                  per_device_train_batch_size=16,<br/>                                  per_device_eval_batch_size=16,<br/>                                  num_train_epochs=3,<br/>                                  save_total_limit = 2,<br/>                                  save_strategy = 'no',<br/>                                  load_best_model_at_end=False<br/>                                  )<br/></span><span id="9682" class="ma mb it lw b gy mg md l me mf">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=tokenized_datasets["train"],<br/>    eval_dataset=tokenized_datasets["test"],<br/>    compute_metrics=compute_metrics<br/>)<br/>trainer.train()</span></pre><h1 id="d62b" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">保存并加载预训练的模型和标记器</h1><p id="429a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">要保存和加载模型，请运行以下命令:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="481c" class="ma mb it lw b gy mc md l me mf"># save the model/tokenizer</span><span id="24c3" class="ma mb it lw b gy mg md l me mf">model.save_pretrained("model")<br/>tokenizer.save_pretrained("tokenizer")</span><span id="64ca" class="ma mb it lw b gy mg md l me mf"># load the model/tokenizer</span><span id="4223" class="ma mb it lw b gy mg md l me mf">from transformers import AutoModelForTokenClassification<br/>model = AutoModelForSequenceClassification.from_pretrained("model")<br/>tokenizer = AutoTokenizer.from_pretrained("tokenizer")</span></pre><h1 id="da5e" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">如何使用模型</h1><p id="8ac4" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">一旦我们加载了记号赋予器和模型，我们就可以使用Transformer的<strong class="lb iu">训练器</strong>从文本输入中获得预测。我创建了一个函数，它将文本作为输入并返回预测。我们需要做的步骤如下:</p><ol class=""><li id="72f5" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">将数据帧中的文本添加到名为text的列中。</li><li id="ffb0" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">将数据帧转换为数据集。</li><li id="4ae5" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">将数据集标记化。</li><li id="454c" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">使用培训师进行预测。</li></ol><p id="bfe8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，你可以不用一个函数来处理多个输入。这样，它会更快，因为它使用批次做预测。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a55b" class="ma mb it lw b gy mc md l me mf">from transformers import Trainer<br/>trainer = Trainer(model=model)</span><span id="73b2" class="ma mb it lw b gy mg md l me mf">def tokenize_function(examples):<br/>    return tokenizer(examples["text"], padding="max_length", truncation=True) </span><span id="a906" class="ma mb it lw b gy mg md l me mf">def pipeline_prediction(text):<br/>    df=pd.DataFrame({'text':[text]})<br/>    dataset = Dataset.from_pandas(df,preserve_index=False) <br/>    tokenized_datasets = dataset.map(tokenize_function)<br/>    raw_pred, _, _ = trainer.predict(tokenized_datasets) <br/>    return(raw_pred[0][0])</span><span id="127e" class="ma mb it lw b gy mg md l me mf">pipeline_prediction("🚨 Get 50% now!")</span><span id="a6d7" class="ma mb it lw b gy mg md l me mf">-0.019468416</span></pre><h1 id="e0ee" class="mi mb it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">总结一下</h1><p id="a338" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在这篇文章中，我们向您展示了如何使用预先训练好的模型来解决回归问题。我们使用Huggingface的transformers库来加载预训练的模型DistilBERT，并根据我们的数据对其进行微调。我认为transformer模型非常强大，如果使用得当，可以比word2vec和TF-IDF等更经典的单词嵌入方法产生更好的结果。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="1845" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">想从我这里得到更多？:<br/></strong><a class="ae ky" href="https://medium.com/@billybonaros" rel="noopener">在媒体<br/> </a>上关注我在 <br/>中链接的<a class="ae ky" href="https://www.linkedin.com/in/billybonaros/" rel="noopener ugc nofollow" target="_blank">上添加我通过使用</a><a class="ae ky" href="https://billybonaros.medium.com/membership" rel="noopener"> <strong class="lb iu">我的推荐链接</strong> </a>注册媒体来支持我。</p></div></div>    
</body>
</html>