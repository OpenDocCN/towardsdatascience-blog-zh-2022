<html>
<head>
<title>Deep Transfer Learning on the Animals-10 Dataset in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 中 Animals-10 数据集上的深度迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-transfer-learning-on-the-animals-10-dataset-in-pytorch-53c84b33ad39#2022-02-22">https://towardsdatascience.com/deep-transfer-learning-on-the-animals-10-dataset-in-pytorch-53c84b33ad39#2022-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9283" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Resnet-18 深度学习 CNN 实现 97%验证准确率的迁移学习教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/89ccc033f6399b982392eb1965172d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5HrZXho5nhRiqSkIb3tBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://www.kaggle.com/alessiocorrado99/animals10" rel="noopener ugc nofollow" target="_blank"> Animals-10 数据集</a> |塞犍陀·维韦克上可视化深度迁移学习(使用 Resnet-18)标签</p></figure><p id="a626" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用于图像预测任务的深度卷积神经网络非常受欢迎。似乎每隔几个月就有一个新的算法挑战 CNN 架构的极限。就在几年前，这些最先进的算法以及构建这些算法背后的知识只有少数专家可以获得。然而，由于越来越容易获得和低成本的计算资源，以及世界上任何地方的任何人都可以运行的开放代码，这种情况正在发生变化。有一些了不起的人愿意为了更大的利益分享他们的知识和专长。</p><p id="d700" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我将使用脸书人工智能研究实验室开发的流行深度学习框架 PyTorch，通过一个这样的最先进的算法 Resnet-18 来预测 10 个类别的动物。我将展示迁移学习如何实现非常高的准确率(这里的<strong class="lb iu"> 97%，基于测试数据</strong>)。</p><p id="5dce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在本教程中完成以下步骤:</p><p id="3ac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="#d87f" rel="noopener ugc nofollow"> <strong class="lb iu">导入和格式化</strong> </a> <strong class="lb iu"> <br/> </strong> <a class="ae ky" href="#71a7" rel="noopener ugc nofollow"> <strong class="lb iu">加载和预处理数据<br/> </strong> </a> <a class="ae ky" href="#33dc" rel="noopener ugc nofollow"> <strong class="lb iu">模型训练和可视化<br/> </strong> </a> <a class="ae ky" href="#7bd9" rel="noopener ugc nofollow"> <strong class="lb iu">可视化结果</strong> </a></p><h1 id="d87f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">导入和格式化</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="762e" class="ms lw it mo b gy mt mu l mv mw">import os<br/>import numpy as np<br/>import torch<br/>from torch import nn<br/>from torch import optim <br/>from torch.autograd import Variable<br/>import torch.utils.data as data<br/>import torchvision<br/>from torchvision import models<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from torch.utils.data import Dataset, DataLoader<br/>from torchvision import transforms, utils<br/>from torchvision.datasets import ImageFolder<br/><br/>res_18_model = models.resnet18(pretrained=True)</span></pre><p id="89ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，最后我从 PyTorch 导入了 resnet18 模型，并选择 pretrained=True。这是因为对于迁移学习，我们通常使用深度学习算法在其他数据上学习的(一些或大部分)权重。然后，我们修改几层，以适用于我们的情况。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="cd63" class="ms lw it mo b gy mt mu l mv mw">T = transforms.Compose([<br/>     transforms.Resize((224,224)),<br/>     transforms.ToTensor(),<br/>     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<br/>])</span></pre><p id="476e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要从 kaggle 下载图片并解压。接下来，我们使用 PyTorch 内置转换框架将数据集转换成标准化格式。这允许我们将图像调整为某种标准格式(在本例中为 224x224)，并进行其他预处理。对于迁移学习模型，重要的是标准化输入数据，以匹配原始模型的训练内容(尽可能)。否则，您可能会面临糟糕的性能风险(根据经验:)。对于 Resnet-18，PyTorch 官方文档建议进行转换。规格化([0.485，0.456，0.406]，[0.229，0.224，0.225])。由 3 个元素组成的第一个阵列对应于 3 个 RGB 通道的平均值。第二个数组也由 3 个元素组成，对应于相同 3 个通道的标准偏差。</p><h1 id="71a7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">加载和预处理数据</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="1077" class="ms lw it mo b gy mt mu l mv mw">#download the dataset from here: https://www.kaggle.com/alessiocorrado99/animals10 <br/><br/>dataset = ImageFolder('./archive/raw-img/', transform=T)<br/>train_set, val_set = torch.utils.data.random_split(dataset, [int(len(dataset)*.8), len(dataset)-int(len(dataset)*.8)])<br/>train_loader = torch.utils.data.DataLoader(train_set, batch_size=64)<br/>test_loader = torch.utils.data.DataLoader(val_set, batch_size=64)</span></pre><p id="bd66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们从各自的图像文件夹中加载数据，应用转换，并随机分成 80%的训练和 20%的测试。现在我们准备训练(差不多！)但是有一个问题……如果你看 Resnet-18 架构，它看起来相当复杂——这只是最初的几层。总共有 18 层做各种事情(卷积，batchnorm，relu，maxpool 等。).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/aa9e92102b9ceb5e6f292c221aa46972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W3enYgqzF_jp4NGZ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">前几个 Resnet-18 层|塞犍陀·维韦克</p></figure><p id="de4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果你看最后一层，它有 1000 个特征。这是因为 Resnet-18 模型最初被训练为对 1000 个类进行预测。这与我们有 10 个类的动物数据集不匹配。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/701bea69cc21b214af13d304d7f52ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/0*tHhEuyQ-mn-XUfkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最后的 Resnet-18 fc 层|塞犍陀·维韦克</p></figure><p id="04ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们至少需要改变这最后一层。结果非常简单，如下所示:</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="a696" class="ms lw it mo b gy mt mu l mv mw">res_18_model.fc= nn.Linear(512, 10)</span></pre><p id="a4f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在我们可以开始训练了！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/8ceaf0bfae051eb40f0cf4640183d78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*19Fer8Nm3MfFryMv.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Resnet-18 将最后一层修改为具有 10 个对应于 10 个类别的特征|塞犍陀·维韦克</p></figure><h1 id="33dc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">模型训练和评估</h1><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6e4a" class="ms lw it mo b gy mt mu l mv mw">model=res_18_model</span><span id="44fd" class="ms lw it mo b gy na mu l mv mw">if(torch.cuda.is_available()==True):<br/>    model=res_18_model.cuda()<br/>    <br/>optimiser=optim.SGD(model.parameters(),lr=1e-2)<br/>loss=nn.CrossEntropyLoss()</span></pre><p id="19a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我有一个 NVIDIA GPU，所以我把它用于相应的培训。这里的两个重要变量是优化器和损失函数。对于优化器，我使用 PyTorch 的随机梯度下降函数。我在这里使用交叉熵，因为它是多类预测的常用损失函数。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="88dc" class="ms lw it mo b gy mt mu l mv mw"># My training and validation loops<br/>nb_epochs = 5<br/>acc_tot=np.zeros(nb_epochs)<br/>for epoch in range(nb_epochs):<br/>    losses = list()<br/>    accuracies = list()<br/>    model.train()     <br/>    for batch in train_loader: <br/><br/>        x,y = batch<br/>        if(torch.cuda.is_available()==True):<br/>            x=x.cuda()<br/>            y=y.cuda()        <br/><br/><br/>        # 1 forward<br/>        l = model(x) # l: logits<br/><br/>        #2 compute the objective function<br/>        J = loss(l,y)<br/><br/>        # 3 cleaning the gradients<br/>        model.zero_grad()<br/>        # optimiser.zero_grad()<br/>        # params.grad.zero_()<br/><br/>        # 4 accumulate the partial derivatives of J wrt params<br/>        J.backward()<br/><br/>        # 5 step in the opposite direction of the gradient<br/>        optimiser.step()<br/><br/><br/><br/>        losses.append(J.item())<br/>        accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())<br/><br/>    print(f'Epoch {epoch + 1}', end=', ')<br/>    print(f'training loss: {torch.tensor(losses).mean():.2f}', end=', ')<br/>    print(f'training accuracy: {torch.tensor(accuracies).mean():.2f}')<br/><br/><br/>    losses = list()<br/>    accuracies = list() <br/>    model.eval()<br/>    for batch in test_loader: <br/>        x,y = batch<br/>        if(torch.cuda.is_available()==True):<br/>            x=x.cuda()<br/>            y=y.cuda()<br/><br/>        with torch.no_grad(): <br/>            l = model(x)<br/><br/>        #2 compute the objective function<br/>        J = loss(l,y)<br/><br/>        losses.append(J.item())<br/>        accuracies.append(y.eq(l.detach().argmax(dim=1)).float().mean())<br/><br/>    print(f'Epoch {epoch + 1}',end=', ')<br/>    print(f'validation loss: {torch.tensor(losses).mean():.2f}', end=', ')<br/>    print(f'validation accuracy: {torch.tensor(accuracies).mean():.2f}')<br/>    acc_tot[epoch]=torch.tensor(accuracies).mean().numpy()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/5d36d0d3395625ae8636a4cf23bcb572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/0*jcHoS-fypp7SfsgF.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失和准确性指标|塞犍陀·维维克</p></figure><h1 id="7bd9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">形象化</h1><p id="d641" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">太好了！现在，我们如何将我们的神奇模型可视化呢？由于这是一个图像数据集，可视化的结果特别强大。首先，我定义了一个函数，用于在初始规范化后将图像转换为正确的格式。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="c128" class="ms lw it mo b gy mt mu l mv mw">def imformat(inp, title=None):<br/>    """Imshow for Tensor."""<br/>    inp = inp.numpy().transpose((1, 2, 0))<br/>    mean = np.array([0.485, 0.456, 0.406])<br/>    std = np.array([0.229, 0.224, 0.225])<br/>    inp = std * inp + mean<br/>    inp = np.clip(inp, 0, 1)<br/>    return(inp)</span></pre><p id="fcec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据附带了一个字典——我在下面使用它。原来这个数据集的标签不是英文的。为了便于理解标签，我们来转换一下。</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="6917" class="ms lw it mo b gy mt mu l mv mw">class_names = dataset.classes<br/>translate = {"cane": "dog", "cavallo": "horse", "elefante": "elephant", "farfalla": "butterfly", "gallina": "chicken", "gatto": "cat", "mucca": "cow", "pecora": "sheep", "scoiattolo": "squirrel", "dog": "cane", "cavallo": "horse", "elephant" : "elefante", "butterfly": "farfalla", "chicken": "gallina", "cat": "gatto", "cow": "mucca", "spider": "ragno", "squirrel": "scoiattolo"}<br/>t_inv = {v: k for k, v in translate.items()}</span></pre><p id="0d68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们把结果可视化！</p><pre class="kj kk kl km gt mn mo mp mq aw mr bi"><span id="cabe" class="ms lw it mo b gy mt mu l mv mw">train_loader2 = torch.utils.data.DataLoader(train_set, batch_size=9)<br/><br/>plt.figure(figsize=(15, 13))<br/><br/>inputs, classes = next(iter(train_loader2))<br/>preds=model(inputs.cuda()).argmax(dim=1)<br/><br/><br/>for i in range(0,9):<br/>    ax = plt.subplot(3, 3, i + 1)<br/>    img=imformat(inputs[i])<br/>    <br/>    plt.imshow((img))<br/><br/>    try:<br/>        plt.title('True:'+str(t_inv[class_names[classes[i]]])+'    Pred:'+str(t_inv[class_names[preds[i]]]))<br/>    except:<br/>        plt.title('True:'+str(translate[class_names[classes[i]]])+'    Pred:'+str(translate[class_names[preds[i]]]))<br/>    if(i==9):<br/>        plt.axis("off")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ae2c3262fe577945491041febb51340c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*_FwnvNMFBpaQ7fIo.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度迁移学习(使用 Resnet-18)标签在<a class="ae ky" href="https://www.kaggle.com/alessiocorrado99/animals10" rel="noopener ugc nofollow" target="_blank"> Animals-10 数据集</a> |塞犍陀·维韦克上的可视化</p></figure><p id="a1c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有成功！将经过适当训练的算法的结果可视化是相当强大的！</p><p id="a34f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ni">下面是</em></strong><a class="ae ky" href="https://github.com/skandavivek/PyTorch-Transfer-Learning" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ni">Github</em></strong></a><strong class="lb iu"><em class="ni">上的代码。快乐深度迁移学习！</em>T13】</strong></p><p id="3862" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ni">来源:</em></p><ol class=""><li id="36ed" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">https://www.youtube.com/watch?v=OMDn66kM9Qc&amp;ab _ channel = PyTorchLightning</li><li id="fd7f" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/vision/main/generated/torch vision . transforms . normalize . html</a></li><li id="1edd" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><a class="ae ky" href="https://www.kaggle.com/alessiocorrado99/animals10" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/alessiocorrado99/animals10</a>(<a class="ae ky" href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html" rel="noopener ugc nofollow" target="_blank">牌照 GPL2 </a>)</li></ol></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="3142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ni">如果你还不是中会员，想支持我这样的作家，可以通过我的推荐链接随意报名:</em><a class="ae ky" href="https://skanda-vivek.medium.com/membership" rel="noopener"><em class="ni">https://skanda-vivek.medium.com/membership</em></a></p><p id="8fef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@skanda.vivek" rel="noopener"> <em class="ni">关注我</em> </a> <em class="ni">如果你喜欢这篇文章——我经常在复杂系统、物理学、数据科学和社会的界面上写作。</em></p><p id="61cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ni">获取每周数据透视</em> <a class="ae ky" href="https://skandavivek.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ni">订阅此处</em> </a> <em class="ni">！</em></p></div></div>    
</body>
</html>