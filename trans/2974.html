<html>
<head>
<title>A Quiet Shift in the NLP Ecosystem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP生态系统的悄然转变</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quiet-shift-in-the-nlp-ecosystem-84672b8ec7af#2022-06-29">https://towardsdatascience.com/a-quiet-shift-in-the-nlp-ecosystem-84672b8ec7af#2022-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9cf1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">它已经在运动了…</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3ffc724ebb5502ffcadbc854a5b99a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hgFz7Xvqgf-5laC9awrDaw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图一。</strong>随着模型规模的增加，语言模型的<strong class="bd ky">能力的出现和增长的抽象说明。大型语言模型的效用超出了它们生成连贯故事的能力——它们可以用来解决几乎任何NLP任务——无论是生成性的还是鉴别性的。这方面的例子有名词短语标注、NER、关系提取、分类、共指消解、语义搜索(句子嵌入)等。此外，对于其中的一些任务，通过微调<strong class="bd ky">，其性能可能足以满足生产用例。</strong> <a class="ae kz" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">作者图片</strong>——使用来自谷歌PaLM模型博客</a>的信息</strong></p></figure><h1 id="3ec8" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">要旨</h1><p id="0d2c" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">几天前发表的一篇论文<em class="mo">(2022年6月15日)</em>提供了对最近大量十亿+参数语言模型令人印象深刻的任务解决能力的关键见解。</p><blockquote class="mp mq mr"><p id="840a" class="ls lt mo lu b lv ms ju lx ly mt jx ma mu mv md me mw mx mh mi my mz ml mm mn im bi translated">当满足两个条件时，大型语言模型表现出解决特定任务的能力，这两个条件是:模型参数大小超过1000亿，训练量超过10次FLOPS。低于这些阈值，解决这些特定任务的能力实际上是不存在的。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/d18c82f887c6e5994a0af6e75fcab0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjJVt4eB1nnEeswQXB7IRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图二。</strong> <a class="ae kz" href="https://arxiv.org/pdf/2206.07682v1.pdf" rel="noopener ugc nofollow" target="_blank">图片来自最近的一篇论文(2022年6月15日</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/742856db544dfa74e354b6171cc7b629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3sk1h4V0doGiEoJQ0q4Uw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图3。来自最近一篇论文的图片(2022年6月15日</strong></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/4398a6dffe1623eefd4593acf2475dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m7GYWiiraHzHaB2LbIGr3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图3a。</strong> <a class="ae kz" href="https://arxiv.org/pdf/2206.07682v1.pdf" rel="noopener ugc nofollow" target="_blank">图片来自最近的一篇论文(2022年6月15日</a></p></figure><p id="ec10" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">虽然今年早些时候的另一篇论文证实了我们在大型语言模型性能中观察到的情况，即<a class="ae kz" href="https://arxiv.org/pdf/2001.08361v1.pdf" rel="noopener ugc nofollow" target="_blank">语言建模性能随着我们增加模型大小、数据大小和计算预算而提高</a>，但6月15日<em class="mo">的这一最新发现(模型大小(和训练FLOPS)对于某些任务解决能力</em>的<strong class="lu iu"><em class="mo"/></strong><em class="mo">的出现是必要的)可能会对整个NLP生态系统产生影响，该生态系统已经在经历转变以利用这些大型语言模型。</em></p><ul class=""><li id="f978" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated">一个NLP从业者必须学会利用这些LLM<em class="mo">(大型语言模型)</em>来解决NLP任务。例如，<strong class="lu iu"> (1) </strong>如何设计提示以产生任务的最佳性能，<strong class="lu iu"> (2) </strong>用于训练的数据集的最佳大小是多少<em class="mo">(通常是数百个，而传统的BERT模型微调是数千个)</em>以达到期望的性能水平<em class="mo">，</em> <strong class="lu iu"> (3) </strong>微调的最佳模型选择是什么， 给定特定的任务解决能力，如句子的分类，即使在数亿参数范围内的较小模型中也可以产生期望的模型性能<em class="mo">(模型大小的选择通常是模型性能和训练和推理的成本/时间之间的工程权衡)</em><strong class="lu iu">【4】</strong><em class="mo"/>如何准备数据和微调这些模型<em class="mo">(模型权重更新可能通过API服务完成) — </em>考虑到在生产中使用零触发或少触发学习的低效率和成本影响，这是一个必要的步骤，<strong class="lu iu"> (5) </strong>如何在生产中部署这些模型<em class="mo">(同样可能通过API服务)</em>和部署成本<em class="mo"> (API提供商根据令牌计数收费。 另一种选择是托管一个用于推理的微调模型，即使可用，其本身也可能是每年数万个)。</em></li><li id="f9a7" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">NLP研究人员现在不得不面对这样一个事实，即进行NLP研究的能力可能会集中在极少数能够负担得起训练这种模型的商业组织或资金充足的教育机构中，因为训练这些大型模型的成本高达数百万美元，远远高于通常可用于研究的拨款金额。</li><li id="887f" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">以解决自然语言处理任务为主要业务的自然语言处理公司被迫整合这些大型语言模型的功能，否则将面临变得无关紧要的风险。也许这并不奇怪，即使是像HuggingFace这样相对较新的NLP参赛者，尽管它在短时间内积累了令人印象深刻的50k+语言和图像模型，也正在进入大型语言模型训练的竞争——自3月26日以来，在384个GPU上已经进行了1760亿参数<a class="ae kz" href="https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4" rel="noopener ugc nofollow" target="_blank">模型训练。此外，HuggingFace托管了几十亿+参数模型用于推理，如GPT-2、GPT-J-6B、GPT-Neox-20B<em class="mo">(</em></a><a class="ae kz" href="https://huggingface.co/spaces/Gradio-Blocks/zero-and-few-shot-reasoning" rel="noopener ugc nofollow" target="_blank"><em class="mo">GPT-Neox-20B spaces demo</em></a><em class="mo">发布在HuggingFace上，声明由于托管这样一个开放公共使用的大型模型的挑战，偶尔会出现错误输出)</em>。一些初创公司已经开始利用大型语言模型的能力，提供“<em class="mo"> NLP作为API服务</em>”。为了展示他们的新产品，他们向公众展示了<em class="mo">游乐场</em>来尝试这些大型模型，因为在大多数情况下，不可能在Google Colab或单个GPU机器上按需加载它们。例如，<a class="ae kz" href="https://docs.cohere.ai/" rel="noopener ugc nofollow" target="_blank"> Co:here </a>刚刚推出了一个类似于<a class="ae kz" href="https://beta.openai.com/playground" rel="noopener ugc nofollow" target="_blank"> OpenAI的</a>游乐场的游乐场——其中至少有一些模型看起来是经过自回归训练的大型语言模型<em class="mo">(我在这篇文章中接触了他们游乐场中模型的大小，他们的回答是定性/主观比较。他们的“Xlarge”型号堪比</em> <a class="ae kz" href="https://blog.eleuther.ai/gpt3-model-sizes/" rel="noopener ugc nofollow" target="_blank"> <em class="mo"> GPT-3达芬奇175B </em> </a> <em class="mo">而他们的“medium”型号比</em> <a class="ae kz" href="https://blog.eleuther.ai/gpt3-model-sizes/" rel="noopener ugc nofollow" target="_blank"> <em class="mo">巴贝奇1.3B </em> </a> <em class="mo">更强大)。</em> <a class="ae kz" href="https://www.ai21.com/" rel="noopener ugc nofollow" target="_blank"> AI21 labs </a>，另一家创业公司也提供NLP即服务。他们展示他们的模型尺寸<em class="mo"> (7B，17B，178B-侏罗纪1) </em>，我们可以在他们的<a class="ae kz" href="https://www.ai21.com/studio" rel="noopener ugc nofollow" target="_blank">游乐场</a>与他们互动。GooseAI 现在通过提供更便宜的API服务来与<a class="ae kz" href="https://openai.com/api/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>竞争，利用来自<a class="ae kz" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank">EleutherAI</a><em class="mo">(GPT-Neox-20B)</em>的可公开访问的预训练模型。<a class="ae kz" href="https://nlpcloud.com/#plans" rel="noopener ugc nofollow" target="_blank"> NLP Cloud </a>是另一种类似GooseAPI的服务，从价格、性能和数据隐私的角度来看，它将自己定位为Huggingface或OpenAI的更好替代者。它利用<a class="ae kz" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank"> EleutherAI </a> LLM模型<em class="mo">(以及更小的公共LMs) </em>来提供API服务。</li><li id="bb38" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">鉴于Nvidia <em class="mo"> (A100s) </em>是目前唯一可用的选择，像<a class="ae kz" href="https://www.cerebras.net/blog/training-multi-billion-parameter-models-on-a-single-cerebras-system-is-easy" rel="noopener ugc nofollow" target="_blank"> Cerebras </a>和<a class="ae kz" href="https://sambanova.ai/solutions/natural-language-processing/" rel="noopener ugc nofollow" target="_blank"> Sambanova </a>这样的硬件初创公司完全可以利用这个<em class="mo">淘金热</em>来培训大型语言模型。<a class="ae kz" href="https://www.graphcore.ai/posts/graphcore-announces-roadmap-to-ultra-intelligence-ai-supercomputer" rel="noopener ugc nofollow" target="_blank"> Graphcore刚刚宣布了一台机器</a>，目标是在2024年发布，可以训练500万亿个参数模型。作为对硬件方法的补充，像<a class="ae kz" href="https://www.hpcaitech.com/" rel="noopener ugc nofollow" target="_blank"> HPC-AI Tech </a>这样的初创公司试图提供一种软件解决方案<a class="ae kz" href="https://www.youtube.com/watch?v=XJEQ_SJIgGI" rel="noopener ugc nofollow" target="_blank"> <em class="mo">【庞氏人工智能】</em> </a> <em class="mo"> </em>来高效地训练和微调大型模型。</li><li id="6a31" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">云计算提供商也有机会利用这一优势。例如，模型权重<a class="ae kz" href="https://github.com/EleutherAI/gpt-neox" rel="noopener ugc nofollow" target="_blank">目标增加到175B </a>参数的GPT-Neox在<a class="ae kz" href="https://blog.coreweave.com/nlp-as-a-service-the-next-evolution-in-ai?fbclid=IwAR2zV1KLWv-J-2PHdZA9Poe-GUueiK_mPy7MKISCI0LaowpZmRMV6SeVY8c" rel="noopener ugc nofollow" target="_blank"> Coreweave </a> <em class="mo">提供的计算上进行训练(在100个GPU上。</em><a class="ae kz" href="https://arxiv.org/pdf/2204.06745.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mo">GPT-Neox 20B</em></a><em class="mo">参数模型</em> <a class="ae kz" href="https://blog.eleuther.ai/announcing-20b/" rel="noopener ugc nofollow" target="_blank"> <em class="mo">是在96 A100s </em> </a> <em class="mo"> ) </em>上训练出来的。像<a class="ae kz" href="https://cirrascale.com/cirrascale-cloud-pricing.php" rel="noopener ugc nofollow" target="_blank"> Cirrascale </a>这样的云计算提供商目前除了提供用于训练和推理的A100s之外，还提供Cerebras和Sambanovoa机器。</li><li id="14d8" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated"><strong class="lu iu">超越NLP生态系统</strong>。最后，大型语言模型的影响超出了NLP生态系统。多模态研究的进展在通向跨模态通用模型的道路上利用了语言模型——要么通过以冻结的方式直接使用大型语言模型来表示文本<em class="mo">(例如</em><a class="ae kz" href="https://arxiv.org/abs/2206.10789" rel="noopener ugc nofollow" target="_blank"><em class="mo">Parti 2022 June 22 with 4B</em></a><em class="mo">语言模型)。</em><a class="ae kz" href="https://arxiv.org/pdf/2204.14198v1.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mo">Flamingo 2022年4月</em> </a> <em class="mo">采用70B param语言模型，Imagen 2022年5月采用</em> <a class="ae kz" href="https://arxiv.org/pdf/2205.11487v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo"> 4.6B param T5-XXL语言模型</em> </a> <em class="mo"> ) </em>或者作为编码器，是文本/图像混合系统的一部分<em class="mo"/><a class="ae kz" href="https://arxiv.org/pdf/2206.06336.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mo">MetaLM</em></a><em class="mo">11</em></li></ul><p id="041e" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">现在，对于我们这些习惯于微调预训练语言模型的人来说，比如BERT，加上一些小的架构添加<em class="mo">(顶部加上一个线性层)</em>来解决特定的区分任务，比如文档、句子或标记分类，一个问题自然会出现…</p><h2 id="801c" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated"><em class="od">创成式模型如何成为所有NLP任务的解决方案？</em></h2><p id="9777" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">2019年的<a class="ae kz" href="https://ajitrajasekharan.medium.com/gpt-2-a-promising-but-nascent-transfer-learning-method-that-could-reduce-or-even-eliminate-in-some-48ea3370cc21" rel="noopener"> GPT-2展示了超越连贯讲故事能力的生成模型的能力</a>。如果我们将NLP任务设计成一个句子完成问题，那么这个任务就变成了模型在训练中学习做的事情——一次一个单词地完成一个句子<em class="mo">(自回归建模)</em>。换句话说，我们可以利用自回归模型的生成能力来解决区分性任务，如分类<em class="mo">(在文档、句子或标记级别)</em>，如果我们将问题作为句子完成问题来处理——这是一个简单的想法，现在有了自己的名字——<a class="ae kz" href="https://arxiv.org/pdf/2104.08691.pdf" rel="noopener ugc nofollow" target="_blank">提示设计或启动</a>。我们将在下面的下一节中研究这样的例子。</p><p id="bf59" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">这种方法在GPT-2中显示了前景，但由于有限的模型大小<em class="mo"> (1.5B) </em>，以及其他一些原因，它的任务解决能力无法与像BERT这样的微调模型的性能相提并论。GPT-3用它的175个B参数在<a class="ae kz" href="https://arxiv.org/pdf/2203.15556.pdf" rel="noopener ugc nofollow" target="_blank">3000亿代币</a> <em class="mo">(以及其他几个改进)</em>上训练，克服了那个不足。不仅模型的“上下文学习能力”<em class="mo">(在没有模型权重更新的情况下，在推理时间内从示例中识别任务的能力)</em>在零个、一个或几个示例的情况下显著提高，它甚至证明了在一些提示(如“让我们一步一步地思考”<a class="ae kz" href="https://arxiv.org/pdf/2205.11916.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo">)的推动下，在一定程度上解决推理任务的能力(2022年6月9日的论文在2022年4月发布的GPT-3和PALM 540B参数模型上测试了这一点)。这种方法的另一个变体是上下文相关的例子，以句子的形式给出解释，导致结果)</em> </a> <em class="mo">和</em> <a class="ae kz" href="https://arxiv.org/pdf/2203.11171.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo">方法通过多数表决</em> </a> <em class="mo">改进了思维链提示。</em></p><p id="aaad" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">然而，从部署的角度来看，<a class="ae kz" href="https://openai.com/blog/customized-gpt-3/" rel="noopener ugc nofollow" target="_blank">在针对特定任务的训练集上微调自回归模型</a>，就像我们对BERT微调<em class="mo">所做的一样(除了使用少于训练BERT模型所需数据一半的数据之外)，</em>是在推理时给定少量学习开销的首选方法。少量学习要求在每次输入之前都有上下文学习的例子。这增加了推理时间和成本——计量是大多数API服务中的令牌数量。</p><p id="f3a3" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">因此，微调自回归模型以解决判别性<em class="mo">(和生成性)</em>任务的一个可能的步骤序列是</p><ul class=""><li id="1434" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated">首先将判别式问题作为句子完成问题<em class="mo">(启动)</em></li><li id="a6de" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">然后通过少量学习测试快速迭代不同的提示候选项，并挑选最佳候选项进行微调。</li><li id="d5e9" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">对于某些任务，可能需要对输出进行额外的后处理，以移除模型输出的拖尾附加物。在提供给模型的上下文内学习示例中的输入示例之间添加停止序列使得这种后处理变得微不足道，因为模型也将输出停止序列(<a class="ae kz" href="https://arxiv.org/pdf/2203.11171.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo">一种通常使用的替代方案是挑选top-k个记号</em> </a>)。</li></ul><p id="edeb" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">考虑到大多数任务中需要标记以进行微调的数据量最多只有几千个(如果不是几百个的话),标记的负担通过即时设计得以减轻。因此，可以从一个小的初始训练数据集开始微调一个自回归模型，并继续用额外的数据逐步微调，直到模型性能稳定或下降。</p><p id="d46e" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">需要注意的一点是，虽然某些能力的出现只发生在100b参数之后，但实际上，这并不像论文所声称的那样是<a class="ae kz" href="https://arxiv.org/pdf/2206.07682v1.pdf" rel="noopener ugc nofollow" target="_blank">二进制的。</a></p><ul class=""><li id="fad2" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated">一些能力出现在更小的模型中——几十亿甚至更小。例如在【2022年6月22日的论文Parti 中，该模型展示了在20B参数下在图像内的生成文本中拼写单词的能力</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/f29235e9e34ddfdb5e5062c464b20e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RlcProfLjGo2huhX1-Z3kQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图四。</strong>为文本"<em class="od">生成的图像一张袋鼠的肖像照片，穿着橙色连帽衫，戴着蓝色太阳镜，站在悉尼歌剧院前的草地上，胸前举着一个牌子，上面写着欢迎朋友！"。请注意，该模型在20B处正确显示了图像“欢迎朋友”中的文本。</em> <strong class="bd ky">图像从纸张上</strong> <a class="ae kz" href="https://parti.research.google/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">文字到图像的模型Parti </strong> </a></p></figure><ul class=""><li id="fe23" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated">对于某些任务，小模型表现出一种特殊的能力，并且这种能力的性能随着模型的大小而不断提高。</li><li id="3470" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">在某些情况下，给定两个不同大小的模型，随着我们增加训练数据集大小，大型模型<em class="mo">(如gp T3–175B)</em>的性能可能会达到峰值，但较小模型<em class="mo">(如6B参数)</em>的性能可能会继续增长，使较小模型更适合我们的任务。</li></ul><p id="002b" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">因此，可以考虑的一种方法是，在我们收敛于一个或多个提示之后，选择一组不同大小的模型<em class="mo">(我们可以通过少量学习迭代来完成)</em>，然后在一个数据集上训练所有模型，增加数据集大小，直到我们达到期望的性能。然后，我们选择适合我们在推理预算成本、性能等之间权衡的模型。</p><p id="deee" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">但是，根据提示微调模型的缺点是会创建模型副本。这意味着我们在部署时需要为每个任务建立一个大的模型。相反，在上下文学习中，尽管有前面提到的缺陷，也不需要创建副本。<a class="ae kz" href="https://www.youtube.com/watch?v=TwE2m6Z991s" rel="noopener ugc nofollow" target="_blank">几种方法</a>介于0%模型更新<em class="mo">(在上下文学习中)</em>到100 %模型更新<em class="mo">(微调)</em>这两个极端之间，例如<a class="ae kz" href="https://aclanthology.org/2021.acl-long.353.pdf" rel="noopener ugc nofollow" target="_blank">前缀调整</a> (2%更新)、适配器调整(2–4%更新)、顶层k层调整、<a class="ae kz" href="https://arxiv.org/pdf/2104.08691.pdf" rel="noopener ugc nofollow" target="_blank">提示调整</a>、<a class="ae kz" href="https://arxiv.org/pdf/2103.10385.pdf" rel="noopener ugc nofollow" target="_blank"> p调整</a>、<a class="ae kz" href="https://arxiv.org/pdf/2204.10019.pdf" rel="noopener ugc nofollow" target="_blank">输入相关提示调整</a>等。这些方法旨在用于展示上下文学习的大型自回归模型。也有人试图通过使用提示模板来微调更小的模型<em class="mo"> ( &lt; 1B) </em>来模拟提示完成功能，这种方法如<a class="ae kz" href="https://arxiv.org/pdf/2001.07676.pdf" rel="noopener ugc nofollow" target="_blank"> PET </a>、<a class="ae kz" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank"> iPET </a>和<a class="ae kz" href="https://arxiv.org/pdf/2103.11955.pdf" rel="noopener ugc nofollow" target="_blank"> ADAPET </a>，在本文中没有讨论，因为在那些小模型<em class="mo">中实际上不存在上下文学习(将伯特的填充掩码预测能力称为“上下文学习”，这只是MLM损失的直接结果)</em></p><p id="4df5" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">尽管存在创建副本的缺点，但微调自回归模型可能比微调BERT模型具有优势，因为对于预训练和微调自回归模型，损失函数是相同的<em class="mo">(下一个令牌预测)</em>。相比之下，对于像BERT这样的模型，情况并非如此——它们不仅有附加层<em class="mo">(其权重在微调过程中被重新学习)</em>这样的架构附加物，在模型附属和微调过程中，损失函数也是不同的。这在一定程度上可以解释为什么MLM预训练模型尽管缺乏生成能力，却不能像自回归模型<em class="mo"> (GPT) </em>一样自然地进行上下文学习，但也可以解释MLM模型不能被微调的事实<em class="mo">(开箱即用，没有额外的方法，如</em> <a class="ae kz" href="https://arxiv.org/pdf/2001.07676.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo"> PET </em> </a> <em class="mo">、</em><a class="ae kz" href="https://arxiv.org/pdf/2009.07118.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mo">iPET</em></a><em class="mo">和</em> <a class="ae kz" href="https://arxiv.org/pdf/2103.11955.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo"> ADAPET】</em></a></p><h2 id="a2a8" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated">使用生成模型(如GPT-3)解决NLP任务的示例</h2><p id="1578" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">NLP任务可以分为三大类</p><ul class=""><li id="6d84" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated"><strong class="lu iu">序→序。</strong>这方面的例子有内容创作<em class="mo">(博客文章、电子邮件副本)</em>、抽象概括、翻译、问题回答&amp;等。</li><li id="5400" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated"><strong class="lu iu">序列→令牌</strong>。这可以是文档或句子的分类。关系提取可以作为一个分类问题来处理，其中提示输入对关系操作数标记以及要分类的句子进行编码——提示完成将是一个是/否布尔标记。也可以认为是输入序列不小于输出序列的序列→序列问题。</li><li id="a745" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated"><strong class="lu iu">利用嵌入的任务</strong>。句子相似度，语义搜索。</li></ul><p id="2b6d" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">某些传统的自然语言处理任务，如词性标注或NER，可能被认为是<em class="mo">序列→序列</em>，可能没有意义直接映射到生成任务，其中输入中的每个标记都有相应的输出标签<em class="mo">(尽管原则上可以这样做)</em>。相反，可以使用自回归模型来输出句子中的名词或实体，如下所示。</p><p id="9714" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">下图是来自<a class="ae kz" href="https://beta.openai.com/playground" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>、<a class="ae kz" href="https://studio.ai21.com/playground" rel="noopener ugc nofollow" target="_blank"> AI21 </a>、<a class="ae kz" href="https://os.cohere.ai/playground/small/generate" rel="noopener ugc nofollow" target="_blank"> Co:here </a>和<a class="ae kz" href="https://www.goose.ai/playground" rel="noopener ugc nofollow" target="_blank"> GooseAPI </a>曝光的操场的零投/少投学习<em class="mo">(无权重更新)</em>例子。这四个都可以免费测试。GooseAPI是开放使用的，不需要注册，而其他三个需要注册。此外，还有其他几个十亿参数语言模型，如<a class="ae kz" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank"> PaLM (540B) </a>、<a class="ae kz" href="https://arxiv.org/pdf/2201.11990.pdf" rel="noopener ugc nofollow" target="_blank">威震天-图灵NLG (530B </a>)、<a class="ae kz" href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" rel="noopener ugc nofollow" target="_blank"> Gopher (280B) </a>、<a class="ae kz" href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT" rel="noopener ugc nofollow" target="_blank"> OPT-175B </a>、<em class="mo">(参见下面的参考资料，获取大型模型列表的链接)。然而，他们还没有开放的游乐场。</em></p><p id="16ac" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">虽然下面显示的例子被选择来说明不同的NLP任务是如何通过精心制作合适的提示来解决的，但是在一些例子中的难度水平，特别是只有零镜头/少镜头学习，是为了说明模型如何处理这种情况。从模型的角度来看，用数百个样本进行微调可能会降低难度，并提高性能。这也可能使我们在下面的零镜头/少镜头设置中看到的模型性能的差异变得均匀。</p><p id="678d" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">序列→序列任务</strong></p><p id="e1f3" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">总结</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/a8c74b30eb798cc0531030340f0ef355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stPapUYfr6mcColbH2mOiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图5。在这种情况下，模特被要求在没有例子的情况下完成一项任务，只有任务的描述。有人可能会说，没有一个模型能以对二年级学生来说足够简单的形式来解释它。然而，在很少的例子中，模型可能会做得更好，如下例所示。<strong class="bd ky">更新Co:here模型的未知尺寸</strong> : Co:here模型看起来可与达芬奇(175B) <strong class="bd ky">作者图片</strong></strong></p></figure><p id="0acb" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">从命令输入生成电子邮件</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/f234360f02aebe8126476e3138afb3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-0yoNZRi1oKKffw-z-F6ZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图6 </strong>。在这种情况下，GPT-Neox-20和AI21搞错了命令意图。<strong class="bd ky">更新Co:here模型的未知尺寸</strong> : Co:here模型看起来可与达芬奇(175B) <strong class="bd ky">图片作者</strong></p></figure><p id="8b2a" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">名词短语识别<em class="mo"> (POS标签功能)</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/3bf5a7c0c3d9c20e200ff003f774dbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMJ0OJ-29rKrggruvZBiLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图7。</strong>GPT-尼奥克斯得到三个名词，而其余的得到两个。<strong class="bd ky">更新Co:here模特未知尺寸</strong>:Co:here XL大号模特看起来可与达芬奇(175B)媲美<strong class="bd ky">图片作者</strong></p></figure><p id="80ec" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">识别命名实体(NER功能)</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/3a24a0df2a7531f03c998b810a128d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uZpOH1k3akvhCHZgVBPNpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图8 </strong>。四个模特都喜欢这个。<strong class="bd ky">更新Co:here模特未知尺寸</strong>:Co:here XL大号模特看起来可与达芬奇(175B)媲美<strong class="bd ky">图片作者</strong></p></figure><p id="f81f" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">关系提取(带实体信息)</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d22170623b08aea62de297bd71d5e367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvKERSW6ahOokpz0xJS45w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图9 </strong>。该测试有多个实体，不同于用于少量学习的示例。<strong class="bd ky">更新Co:here模特未知尺寸</strong> : Co:here Xlarge模特看起来堪比达芬奇(175B) <strong class="bd ky">图片作者</strong></p></figure><p id="b668" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">从文档中提取主题</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/68aa3a855b5808b590daceff240ccf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgiXMJ3o1ph8VDrcda14dg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图10。</strong>模型得出的结果存在显著差异。<strong class="bd ky">更新Co:here模特的未知尺寸</strong> : Co:here Xlarge模特看起来可与达芬奇(175B) <strong class="bd ky">图片作者</strong></p></figure><p id="78d1" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">共指消解</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/bff06e58df358a21ddf8dab5a6bfe34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bj_Gmq7EBiHNfzoPDVYVjA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图11。没有一个模型做对了。AI21的结果相当有趣。<strong class="bd ky">更新Co:here模特的未知尺寸</strong>:Co:here XL大号模特看起来堪比达芬奇(175B) <strong class="bd ky">图片作者</strong></strong></p></figure><p id="e799" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">序列→令牌任务。</strong></p><p id="af6d" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">句子分类<em class="mo">(其中上下文学习示例没有列出所有分类类别——语言模型从上下文中推断出来)</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/58c8a400b7dc417b4b5ac463121bad5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XRYGQjoVmy-px5_HX68BxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图12。所有的型号都解决了这个问题，尽管其中一个型号需要对GPT-奈奥斯进行一些处理。这可以通过几个例子来避免，让模型学习预期的输出格式。<strong class="bd ky">更新Co:here模型的未知尺寸</strong> : Co:here Xlarge模型看起来与达芬奇(175B) I <strong class="bd ky"> mage作者</strong></strong></p></figure><p id="a09f" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">句子分类<em class="mo">(其中上下文学习示例列出了所有类别)</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/45991a24dafc6f53972e30e582586065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-Z2HCGNAfPPpT4Az3fulg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图13。Co:这里的输出虽然不同于其他输出，但也可以说是合理的。<strong class="bd ky">更新</strong> : Co:这里Xlarge模型看起来堪比达芬奇(175B) <strong class="bd ky">图片作者</strong></strong></p></figure><p id="80df" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><strong class="lu iu">open ai、AI21、Co:here和GooseAI展示的各种用例</strong></p><p id="6ccc" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">下表显示了所有暴露的操场的共同属性</p><ul class=""><li id="ef2d" class="nd ne it lu b lv ms ly mt mb nf mf ng mj nh mn ni nj nk nl bi translated">大多数任务是终端用户用例，而不是一般的NLP任务。例如，OpenAI在不同的用例中公开了相同的底层NLP任务——机场代码提取，提取联系信息传统上被认为是信息提取。菜谱创建者、论文提纲等。传统上被称为生成任务。</li><li id="c98e" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn ni nj nk nl bi translated">传统NLP公司会展示的NLP任务，例如命名实体识别、关系提取、依赖解析器、分块器等。都缺席了。相反，它们被隐式地公开为特定的用例任务。这部分是因为，如前所述，以一对一的方式映射这些NLP任务可能不是利用这些语言模型的最佳方式，尽管原则上可以<em class="mo">(例如，发出BIO或POS标签)。</em>此外，LLM能够在传统的NLP管道中折叠多个步骤，并直接交付输出，这是解决任务的传统方法的一个明显优势。</li></ul><p id="6c65" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">这重申了一个事实，即当利用LLM时，从业者必须以新的思路为任务设计解决方案——传统的多步骤管道可能通过LLM的一个步骤就可以解决。然后，我们只需为任务设计正确的启动和完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/01c22baf3c325061c6be14f5c94a6bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0KKGpM2Hz0TwnBsLmknKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图14。</strong>这些用例涵盖了广泛的NLP任务。<strong class="bd ky">作者图片</strong></p></figure><h2 id="8a0f" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated">利用嵌入的任务</h2><p id="b473" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">利用嵌入的任务可以是语义搜索或句子相似性。这里不做嵌入的比较，因为此时只有OpenAI和Co:here公开了嵌入接口。今年早些时候<a class="ae kz" href="https://medium.com/@nils_reimers/openai-gpt-3-text-embeddings-really-a-new-state-of-the-art-in-dense-text-embeddings-6571fe3ec9d9" rel="noopener">对OpenAI嵌入进行了一项分析</a>，当时它处于测试模式，嵌入的性能不如使用较小模型的标准已知嵌入方法。句子的嵌入本质上是添加到句子末尾的特殊标记的嵌入。在学习句子嵌入的对比训练方法中，文档中的连续句子对被认为是正对的。</p><h2 id="5561" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated">使用LLM减轻LLM的一个关键缺陷——一种新方法</h2><p id="c8f9" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">尽管LLM解决各种任务的能力令人印象深刻，但考虑到错误率，对于某些用例来说，它不是一个可靠的生产部署选项。例如，虽然我们已经在网上看到了LLM解决简单和复杂数学问题的例子，甚至中间步骤也显示了它是如何得到答案的，但在一个简单计算<em class="mo">(如下所示)</em>上LLM的性能很差，一个优秀的老式符号计算器可以可靠地解决这个问题， 强调了使用LLM执行如此简单的算术任务的不可靠性——上面NLP用例中使用的所有LLM在下面的简单计算中都失败了<em class="mo">(提高语言模型在推理任务上的性能的方法已经在本帖发布后发布——例如</em><a class="ae kz" href="https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html" rel="noopener ugc nofollow" target="_blank"><em class="mo">Minerva——一个关于数学内容的微调PaLM (540B)模型</em> </a> <em class="mo">)。 </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/3b041b34d55cad16bbfbc41001420f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBGFHX6y4GEeQ3iu_Q80iQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图15。</strong>尽管OpenAI得到了错误的答案，但它概述了步骤，甚至出于某种原因将大操作数交换到了左边。Co:这里的延续输出只是因为模型没有看到任务的几个例子。总的来说，我们很可能会看到零触发设置的模型之间的输出差异很大。通过少量拍摄和微调，变化趋于减少。对数百个(如果不是数千个的话)进行微调可能是确定这些模型真实性能的正确方法。<strong class="bd ky">更新Co:here模特未知尺寸</strong>:Co:here XL大号模特看起来可与达芬奇(175B)媲美<strong class="bd ky">图片作者</strong></p></figure><p id="4e52" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated"><a class="ae kz" href="https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system" rel="noopener ugc nofollow" target="_blank"> A121通过结合两个世界的优点为这个问题提供了一个解决方案</a>——利用可靠的专家模型<em class="mo">(可以是神经网络或非神经网络模块)</em>的能力和使用LLM的能力。他们考虑可以回答任何问题的搜索界面的用例。系统使用LLM作为前端来接受用户输入，并将其路由到正确的专家模块(如果存在的话)。如果不存在专家模块，用户输入将作为后备路由到LLM。然后，这一步的输出再次被馈送到LLM，以句子的形式构造答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/fdbb1778f1d9843590deafd22e9edf3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGpcw1DysGRk2CQ9ku5L7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图16。</strong>AI21提供的混合解决方案。<strong class="bd ky">图片由作者利用</strong> <a class="ae kz" href="https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky"> AI21博文</strong> </a>获得</p></figure><p id="b02f" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">为了实现这样一个可能包含多个LLM的系统，他们提供了替代方案，以避免多个LLM的微调副本充当特定领域的专家。基本的想法是使用一个冻结的LLM来执行多项任务，这些任务使用了本文概述的方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/4169de52013ce7dde9e09fd008d53730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CoZ6aaKLtgay6ANjNgJDxA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">图十六。</strong>混合方法不仅有效，而且从生产部署的角度来看，人们可以看到哪个专家(如果有的话)解决了这个问题，并且知道任务何时由LLM解决，作为后备方案<strong class="bd ky">。图片由作者根据</strong> <a class="ae kz" href="https://studio.ai21.com/jurassic-x" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">侏罗纪-X游乐场</strong> </a>的输出创建</p></figure><h2 id="4ad4" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated">混合专家——解决模型规模扩大挑战的一种有前途的方法</h2><p id="b421" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">AI21利用专家模块完成任务的方法本质上类似于混合专家方法。使用专家混合物(MoE)的想法并不新鲜— <a class="ae kz" href="https://arxiv.org/pdf/1701.06538.pdf" rel="noopener ugc nofollow" target="_blank">它可以追溯到2017年的工作</a>和随后的型号发布，如<a class="ae kz" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">Switch</a>2021年1月<em class="mo">(多达一万亿个参数)</em>。与对每个输入重复使用模型的所有参数的深度学习模型相比，MoE模型基于具有路由机制的输入示例选择性地使用一部分参数。这导致了稀疏激活的模型，其潜在的总参数数以万亿计，但是具有固定的计算成本。最近的工作，如<a class="ae kz" href="https://arxiv.org/abs/2110.03742" rel="noopener ugc nofollow" target="_blank">Task-MOE</a><em class="mo">(2021年9月)</em>能够从大型稀疏模型中提取随时可用的可部署子网络。这是一项充满希望的工作，解决了部署这些大型模型的挑战。<a class="ae kz" href="https://arxiv.org/abs/2112.06905" rel="noopener ugc nofollow" target="_blank"> GLaM </a>特别地，这个万亿参数模型大约是GPT-3 (175B)参数的7倍，但是消耗了用于训练GPT-3的1/3的能量，以及用于推断的一半计算失败，总体性能优于GPT-3。混合专家方法也已应用于视觉系统<a class="ae kz" href="https://arxiv.org/pdf/2106.05974.pdf" rel="noopener ugc nofollow" target="_blank"> (V-MoE) </a>。虽然<a class="ae kz" href="https://arxiv.org/pdf/2106.05974.pdf" rel="noopener ugc nofollow" target="_blank"> V-MoE </a>是迄今为止最大的视觉模型(15B)参数，但它的识别性能与最先进的视觉模型相当，推理时的计算量只有后者的一半。最近<a class="ae kz" href="https://ai.googleblog.com/2022/06/limoe-learning-multiple-modalities-with.html" rel="noopener ugc nofollow" target="_blank">LIMoE(2022年6月</a>)的稀疏MoE模型展示了多模态学习(文本和图像)的能力——上面讨论的所有MOE模型都是单峰的。该模型包含5.6B个参数，但每个令牌仅应用6.75亿个参数。</p><h1 id="0c0e" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">最后的想法</h1><p id="9364" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">利用大型语言模型完成任务仍处于初级阶段，随着模型规模的增加，会出现哪些额外的功能还有待观察。随着新的硬件、软件和云初创公司竞相利用这一优势，整个NLP生态系统已经在经历转型。</p><p id="6883" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">然而，模型大小的增加很大程度上是由变压器的使用所驱动的，变压器已经成为几乎所有模态的公共基础架构。虽然随着模型参数的增加，新的功能不断涌现，<a class="ae kz" href="https://arxiv.org/pdf/2203.15556.pdf" rel="noopener ugc nofollow" target="_blank">最近的工作<em class="mo">(2022年3月29日)</em>也表明</a>计算高效的模型可以与更大的模型表现一样好，因为大多数大型模型由于只是缩放参数而保持训练数据不变，所以训练不足。<a class="ae kz" href="https://arxiv.org/pdf/2203.15556.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文声称一个只有70B个参数的模型</a><em class="mo">龙猫</em>在大范围的下游评估任务中胜过它更大的对手GPT-3(175B)、侏罗纪-1 (178B)、地鼠(280B)、威震天-图灵NLG (530B)。此外，混合专家方法既解决了训练大型模型的挑战<em class="mo">(通过只更新每个输入模型的一部分)</em>，也解决了通过提取较小的子网进行推理的部署挑战。这些工作以及“小模型空间”中的创新可能会产生计算高效模型，甚至是更新的架构，它们不一定像当前模型那样大，以用于任务解决能力的出现和增长。</p><p id="1538" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">最后，有一种越来越多的期望或误解<em class="mo">(基于个人观点)</em>，这主要是由最近大型语言模型的成功推动的，</p><blockquote class="mp mq mr"><p id="ef94" class="ls lt mo lu b lv ms ju lx ly mt jx ma mu mv md me mw mx mh mi my mz ml mm mn im bi translated">我们可以通过预测序列中的记号来解决一般智力问题，而不需要任何与世界互动的物理经验</p></blockquote><p id="8c5d" class="pw-post-body-paragraph ls lt it lu b lv ms ju lx ly mt jx ma mb mv md me mf mx mh mi mj mz ml mm mn im bi translated">这种观点有一个对立面，它审视了大型语言模型是解决人工一般智能的解决方案这一预期中的缺陷(AGI)。在最近的<a class="ae kz" href="https://openreview.net/pdf?id=BZ5a1r-kVsf" rel="noopener ugc nofollow" target="_blank"/><em class="mo">【2022年6月27日】</em>中捕捉到了这一点，它不仅概述了我们需要解决的问题以更接近AGI，还概述了实现这一目标所需的合理的架构元素。然而，这种替代观点并不排除让我们更接近一般智能的模型变大——只是它不太可能是我们现在拥有的那种大型语言模型，但具有更多参数。</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><h2 id="496f" class="nr lb it bd lc ns nt dn lg nu nv dp lk mb nw nx lm mf ny nz lo mj oa ob lq oc bi translated">参考</h2><ol class=""><li id="5e24" class="nd ne it lu b lv lw ly lz mb oy mf oz mj pa mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2206.07682v1.pdf" rel="noopener ugc nofollow" target="_blank">大型语言模型的涌现能力。2022年6月15日</a></li><li id="db1a" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2001.08361v1.pdf" rel="noopener ugc nofollow" target="_blank">神经语言模型的标度律。2022年1月23日</a></li><li id="cf08" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/abs/2203.15556" rel="noopener ugc nofollow" target="_blank">训练计算最佳大型语言模型2022年3月</a></li><li id="57b2" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://www.graphcore.ai/posts/graphcore-announces-roadmap-to-ultra-intelligence-ai-supercomputer" rel="noopener ugc nofollow" target="_blank">超智能人工智能超级计算机路线图2022年3月</a></li><li id="1ef5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2204.10019.pdf" rel="noopener ugc nofollow" target="_blank">站在巨型冰冻语言模型的肩膀上2022年4月</a></li><li id="86ad" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2205.00445.pdf" rel="noopener ugc nofollow" target="_blank"> MRKL系统公司2022年5月</a></li><li id="58a6" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2205.01068.pdf" rel="noopener ugc nofollow" target="_blank"> OPT-Open预训练变压器语言模型2022年6月</a></li><li id="76a9" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2204.06745.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-Neox-20B和开源自回归语言模型2022年4月</a></li><li id="a024" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2205.05638v1.pdf" rel="noopener ugc nofollow" target="_blank">少量参数高效微调优于上下文学习2022年5月</a></li><li id="8bc0" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2205.11916v2.pdf" rel="noopener ugc nofollow" target="_blank">大型语言模型是零射击推理机2022年6月</a></li><li id="8ec5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2206.06336v1.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是通用接口2022年6月</a></li><li id="7a99" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2206.08916v1.pdf" rel="noopener ugc nofollow" target="_blank">Unified-IO——视觉、语言和多模态任务的统一模型2022年6月</a></li><li id="2917" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2111.07991.pdf" rel="noopener ugc nofollow" target="_blank">点亮——锁定图像文本调谐的零拍摄传输，2021年11月</a></li><li id="6ed5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2109.01652.pdf" rel="noopener ugc nofollow" target="_blank">微调语言模型是零射击学习者2022年2月</a></li><li id="e724" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/tackling-multiple-tasks-with-a-single-visual-language-model/flamingo.pdf" rel="noopener ugc nofollow" target="_blank">Flamingo——用于少量学习的视觉语言模型2022年4月</a></li><li id="15d3" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2112.11446.pdf" rel="noopener ugc nofollow" target="_blank">扩展语言模型——方法、分析和来自训练Gopher的见解2022年1月</a></li><li id="a127" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2205.12689.pdf" rel="noopener ugc nofollow" target="_blank">大型语言模型是零射击临床信息提取器2022年5月</a></li><li id="77c5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">开关变压器—利用简单有效的稀疏性扩展至万亿参数模型2002年1月1日</a> 1</li><li id="c0cb" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="noopener ugc nofollow" target="_blank"> PaLM Pathways语言模型—扩展到550B参数以实现突破性性能2022年4月</a></li><li id="9df8" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html" rel="noopener ugc nofollow" target="_blank">GLaM 2021年12月推出更高效的情境学习</a></li><li id="ec51" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://blog.eleuther.ai/gpt3-model-sizes/" rel="noopener ugc nofollow" target="_blank">关于OpenAPI型号的尺寸</a></li><li id="94cb" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2201.08239.pdf" rel="noopener ugc nofollow" target="_blank"> LaMDA —对话应用的语言模型—2022年2月</a></li><li id="93d5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0" rel="noopener ugc nofollow" target="_blank">带有链接的大型模型列表</a></li><li id="1352" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/pdf/2203.11171.pdf" rel="noopener ugc nofollow" target="_blank">自我一致性改善语言模型中的思维链推理——2022年4月</a></li><li id="1754" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://storage.googleapis.com/minerva-paper/minerva_paper.pdf" rel="noopener ugc nofollow" target="_blank"> Minerva:解决语言模型中的定量推理，2022年6月30日</a></li><li id="a831" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://arxiv.org/abs/2110.03742" rel="noopener ugc nofollow" target="_blank">超越蒸馏:高效推理的任务级专家混合。2021年9月</a></li><li id="07a5" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated">值得注意的是，图像模型已经经历了一个性能提高的阶段，对于区分性和生成性任务，模型大小都有所增加。但是最近用于无条件和<a class="ae kz" href="https://benanne.github.io/2022/05/26/guidance.html" rel="noopener ugc nofollow" target="_blank">有条件图像生成</a> <em class="mo">(利用了</em> <a class="ae kz" href="https://arxiv.org/pdf/2205.11487v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mo">之前提到的一些多模态</em> </a> <em class="mo">模型)</em>的最新视觉模型，在参数数量上比它们更大的语言生成对应模型小两个数量级。对于辨别任务，像<a class="ae kz" href="https://arxiv.org/pdf/2106.05974.pdf" rel="noopener ugc nofollow" target="_blank"> V-MoE </a>这样的MoE模型，尽管是迄今为止最大的视觉模型(15B)，却利用了当前最先进模型一半的计算量来完成识别任务。</li><li id="aeec" class="nd ne it lu b lv nm ly nn mb no mf np mj nq mn pb nj nk nl bi translated"><a class="ae kz" href="https://twitter.com/adityakusupati/status/1541554858829815809?s=20&amp;t=KiVcg9LRSEtlYouPR6O1XQ" rel="noopener ugc nofollow" target="_blank">最近的一个“小模型空间”创新的例子</a>即使只针对图像。<a class="ae kz" href="https://arxiv.org/abs/2205.13147" rel="noopener ugc nofollow" target="_blank">2022年5月Matryoshka代表自适应部署</a></li></ol></div></div>    
</body>
</html>