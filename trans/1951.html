<html>
<head>
<title>Expand your Time Series Arsenal with These Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用这些模型扩展您的时间序列库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/expand-your-time-series-arsenal-with-these-models-10c807d37558#2022-05-04">https://towardsdatascience.com/expand-your-time-series-arsenal-with-these-models-10c807d37558#2022-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c726" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">整理、装袋、堆叠等等</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3a285deef5139c50bb79dedd1712f0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfhmlWwhgp8UPD4ecDVz4A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e16f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">时间序列数据通常有三个组成部分:</p><ul class=""><li id="2072" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">季节性</li><li id="77ad" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">趋势</li><li id="39a4" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">残留的；剩余的</li></ul><p id="6fb1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测这些组成部分，你就可以预测几乎任何时间序列。听起来很简单，对吧？</p><p id="cb24" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不完全是。围绕指定模型以使其能够正确考虑这些元素的最佳方法存在许多模糊之处，并且在过去几年中已经发布了许多研究来寻找这样做的最佳方法，其中最先进的模型(如递归和其他神经网络模型)占据了中心位置。此外，许多系列还有其他应该考虑的影响，如假期和结构性中断。</p><p id="1215" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总而言之，预测任何给定的序列通常不像使用线性回归那么容易。非线性估计和集成方法可以与线性方法相结合，找到任何序列的最佳方法。在本文中，我概述了Scitkit-learn库中这些模型的一个示例，以及如何利用它们来最大化准确性。这些方法应用于每日访客数据集中，这些数据在<a class="ae mf" href="https://www.kaggle.com/datasets/bobnau/daily-website-visitors" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>或<a class="ae mf" href="https://regressit.com/data.html" rel="noopener ugc nofollow" target="_blank"> RegressIt </a>上有超过2000次的观察。特别感谢<a class="ae mf" href="https://www.fuqua.duke.edu/faculty/robert-nau" rel="noopener ugc nofollow" target="_blank">教授Bob Nau </a>提供！</p><p id="854f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇博文的结构相当重复，每个应用的模型都有相同的图表和评估指标。如果您已经熟悉Scikit-learn模型和API，如MLR、XGBoost等，您可以跳到打包和堆叠部分来概述一些可能略有不同的内容。你可以在<a class="ae mf" href="https://github.com/mikekeith52/scalecast-examples/blob/main/sklearn/sklearn.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到完整的笔记本。</p><h1 id="7846" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">准备模型</h1><p id="6b82" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">所有模型都使用scalecast包运行，该包包含结果，并将Scikit-learn和其他模型包装在时序数据周围。默认情况下，它的所有预测都使用动态多步预测，与一步预测的平均值相比，它在整个预测范围内返回更合理的准确性/误差指标。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a85e" class="ni mh iq ne b gy nj nk l nl nm">pip install scalecast</span></pre><p id="46b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你觉得这个包有趣，就在GitHub 上给它一颗星。</p><p id="2587" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用60天的预测范围，并使用60天的设置来验证每个模型并调整其超参数。所有模型都将在20%的原始数据上进行测试:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="9e44" class="ni mh iq ne b gy nj nk l nl nm">f=Forecaster(y=data['First.Time.Visits'],current_dates=data['Date'])<br/>f.generate_future_dates(60)<br/>f.set_test_length(.2)<br/>f.set_validation_length(60)</span></pre><p id="3669" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从EDA(此处未显示)来看，似乎在过去4周内存在自相关，并且前7个因变量滞后可能是显著的。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7112" class="ni mh iq ne b gy nj nk l nl nm">f.add_ar_terms(7) # 7 auto-regressive terms<br/>f.add_AR_terms((4,7)) # 4 seasonal terms spaced 7 apart</span></pre><p id="4fc0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于这个数据集，必须考虑几个季节性因素，包括每日、每周、每月和每季度的波动。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8f6a" class="ni mh iq ne b gy nj nk l nl nm">f.add_seasonal_regressors(<br/>    'month',<br/>    'quarter',<br/>    'week',<br/>    'dayofyear',<br/>    raw=False,<br/>    sincos=True<br/>) # fourier transformation<br/>f.add_seasonal_regressors(<br/>    'dayofweek',<br/>    'is_leap_year',<br/>    'week',<br/>    raw=False,<br/>    dummy=True,<br/>    drop_first=True<br/>) # dummy vars</span></pre><p id="497e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们可以通过添加年份变量来模拟序列的趋势:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6207" class="ni mh iq ne b gy nj nk l nl nm">f.add_seasonal_regressors('year')</span></pre><p id="fd03" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于所有这些模型，您通常希望为它们提供稳定的时间序列数据。我们可以用扩展的Dickey-Fuller检验来确认该数据是稳定的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/fd77d358efbbbccf330211d6c826c140.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*znT7LdDJcOeK-daa1VDKrA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="3d25" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">最低贷款利率（minimumlendingrate）</h1><p id="4f59" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">我们将从简单开始，尝试应用多元线性回归(MLR)。该模型快速、简单，并且没有要调整的超参数。它经常获得很高的精确度，即使使用更先进的方法也很难被击败。</p><p id="8be6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">它假设模型中的所有组件可以以线性方式组合，以预测最终输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1eec8f08fb0dd10f45a1163146d325b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*77BxkqEQqTG_JXPHiFtGIA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c31d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="nr"> j </em>是添加的回归量的数量(在我们的例子中，是AR、季节和趋势分量),α是相应的系数。在我们的代码中，调用这个函数类似于:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="b248" class="ni mh iq ne b gy nj nk l nl nm">f.set_estimator('mlr')<br/>f.manual_forecast()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/93f2d30101060f06ee174483c0b848f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKZ5lmKMEUSRf5M7f1ulaw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d868a35cd67629c389a6d7518f9b7b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*14vt3b-qj1yqeoWjrY9J5w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c17b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">值得注意的是，时间序列的线性方法的一个更常见的应用是<a class="ae mf" rel="noopener" target="_blank" href="/forecast-with-arima-in-python-more-easily-with-scalecast-35125fc7dc2e"> ARIMA </a>，它也使用序列的误差作为回归变量。MLR假设序列的误差是不相关的，这在时间序列中是虚假的。也就是说，在我们的分析中，MLR获得了13%的测试集平均绝对百分比误差和76%的R2。让我们看看是否可以通过增加复杂性来解决这个问题。</p><h1 id="7c5e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">套索</h1><p id="74b8" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">接下来回顾的三个模型，Lasso、Ridge和ElasticNet，都使用MLR的相同基础函数进行预测，但是估计系数的方式不同。有L1和L2正则化参数，用于减少系数的大小，从而减少过拟合，并可以导致更好的样本外预测。在我们的案例中，这可能是一个很好的尝试技术，因为MLR的样本内R2得分为95%，明显大于样本外R2的76%，表明过度拟合。</p><p id="3459" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用lasso可以估计一个参数，即L1惩罚参数或alpha的大小。我们可以通过对验证数据集的100个alpha值进行网格搜索来做到这一点。看起来是这样的:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5ac1" class="ni mh iq ne b gy nj nk l nl nm">f.add_sklearn_estimator(Lasso,'lasso')<br/>f.set_estimator('lasso')<br/>lasso_grid = {'alpha':np.linspace(0,2,100)}<br/>f.ingest_grid(lasso_grid)<br/>f.tune()<br/>f.auto_forecast()</span></pre><p id="1872" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Lasso(以及Ridge和ElasticNet)使用缩放输入很重要，这样惩罚参数就能平衡所有系数。默认情况下，Scalecast使用最小最大缩放器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/681b528ba3ff686e445b622778d8a31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONr7Vj7axbZjAFJMcQQCeg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ccbab90770a0ac81804c8f68f303a8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*yXV41NdzpDraUiOFPjBk4w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d406" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">选择的最佳alpha值为0.081。该模型既没有提高MLR模型的样本外精度，也没有减少过拟合。我们可以用山脊模型再试一次。</p><h1 id="895a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">山脉</h1><p id="71be" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">山脊类似于套索，除了它使用L2惩罚。这里的区别在于，L1罚函数可以将某些系数降低到零，而里奇罚函数只能将系数降低到接近零。通常，两种模型产生相似的结果。我们可以使用为套索模型创建的相同网格来调整山脊模型。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0f7b" class="ni mh iq ne b gy nj nk l nl nm">f.set_estimator('ridge')<br/>f.ingest_grid(lasso_grid)<br/>f.tune()<br/>f.auto_forecast()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/cdba639d5e647c87ea53581d678d85f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSVKwaVM4Fcp_bxp49IsKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/33456a8b53c8f44fa4be53286c8b6fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*D-cgEC7jJY7A8Ssk1hn9dA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c3a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为脊模型选择的最佳α是0.384，其结果与套索模型相似，如果不比套索模型差一点的话。我们还有一个可以将正则化应用于MLR的模型:ElasticNet。</p><h1 id="e8c0" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">弹性网</h1><p id="f20c" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">Scikit-learn提供的ElasticNet模型将使用线性模型预测输出，但现在它将混合L1和L2惩罚。现在要调整的关键参数是:</p><ul class=""><li id="e3ad" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">L1/L2罚款比率(<code class="fe nw nx ny ne b">l1_ratio</code>)</li><li id="04ba" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">惩罚值(<code class="fe nw nx ny ne b">alpha</code>)</li></ul><p id="8ef2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Scalecast为ElasticNet提供了一个很好的默认验证网格，所以我们不必创建一个。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="19ad" class="ni mh iq ne b gy nj nk l nl nm">f.set_estimator('elasticnet')<br/>f.tune()<br/>f.auto_forecast()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/e692e1864724c8c79f2dc086172712f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bdGwK74zLBdgFI-t8HBLw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5be407231eca00c8fe9e9eea6a81429c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*glNJcyRITYXAOgqVaPnLUw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6e37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ElasticNet的最佳参数是1的<code class="fe nw nx ny ne b"> l1_ratio</code>,这使得它相当于套索模型，以及0.3的alpha。它的表现与套索和脊模型一样好，并且没有减少过度拟合。</p><h1 id="57b2" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">随机森林</h1><p id="bbda" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">在用尽了几种线性方法来估计这个数列之后，我们可以转向非线性方法。其中最流行的是随机森林，一种基于树的集成方法。它通过用原始数据集的自举样本(替换采样)聚集几个树估计器来起作用。每个样本可以利用原始数据集的不同行和列，最终结果是应用于每个样本的特定数量的基础决策树估计器的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/fde7ffd60a05c4f01bf3b0cfc7ad969e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6hPhW-rsAFnlx6Rt6g6aw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7be3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对使用随机森林进行时间序列预测的一些批评不时出现，例如估计值不能大于最大观测值或小于最小观测值。有时，这个问题可以通过确保只将平稳数据提供给估计器来解决。但总的来说，这个模型并不以其时间序列的威力而闻名。让我们看看它在这个例子中的表现。我们可以指定自己的验证网格。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="26a2" class="ni mh iq ne b gy nj nk l nl nm">rf_grid <strong class="ne ir">=</strong> {<br/>     'max_depth':[2,3,4,5],<br/>     'n_estimators':[100,200,500],<br/>     'max_features':['auto','sqrt','log2'],<br/>     'max_samples':[.75,.9,1], <br/>}</span></pre><p id="7985" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后运行预测。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="d9b0" class="ni mh iq ne b gy nj nk l nl nm">f<strong class="ne ir">.</strong>set_estimator('rf')<br/>f<strong class="ne ir">.</strong>ingest_grid(rf_grid) <br/>f<strong class="ne ir">.</strong>tune() <br/>f<strong class="ne ir">.</strong>auto_forecast()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/300b6dd9cb3b843ec2eab8ff3af02f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ttfzOaDMqvCgzDApwR8FHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2663" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从现在开始，我必须粘贴两个表来进行模型基准测试，否则就无法阅读了。底部表格中的行与顶部表格中列出的型号相对应。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/eccf007624107a8b1508a580f605b5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*QmSgfw1KF1jniRKsYs3X_g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/5a7754c5c40808155031b59bc315a2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*iF7Mf1P78a7-V-9OK0ugOA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="cd30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不幸的是，我们在随机森林没什么运气。它的表现明显不如之前评估的那些。然而，一旦我们用BaggingRegressor模型概述预测，这里介绍的自举抽样的概念将是完整的。</p><h1 id="2648" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">XGBoost</h1><p id="4907" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">XGBoost是一个很难简单解释的模型，所以如果读者感兴趣的话，我将在这里的文章<a class="ae mf" rel="noopener" target="_blank" href="/a-beginners-guide-to-xgboost-87f5d4c30ed7"/>(针对初学者)和这里的文章<a class="ae mf" href="https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349" rel="noopener"/>(针对深入研究)中进行解释。基本思想是，类似于随机森林，通过一系列决策树进行估计，最终结果是每个基础估计的一种加权平均值。与随机森林不同，树是按顺序构建的，其中每个后续的树都对之前的树的残差进行建模，希望尽可能地最小化最终残差。采样不是随机的，而是基于先前树的弱点。通过这种方式，通过提升样本来获得结果，而随机森林使用引导聚合，其中每个树和样本都相互独立。这是对引擎盖下真正发生的事情的过度简化，所以如果这种解释不能让你满意，请阅读链接的文章。在这种模型中有许多超参数需要调整，我们可以构建这样一个网格:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="214e" class="ni mh iq ne b gy nj nk l nl nm">xgboost_grid = {<br/>     'n_estimators':[150,200,250],<br/>     'scale_pos_weight':[5,10],<br/>     'learning_rate':[0.1,0.2],<br/>     'gamma':[0,3,5], <br/>     'subsample':[0.8,0.9]<br/>}</span></pre><p id="2911" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">评估模型:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5e7c" class="ni mh iq ne b gy nj nk l nl nm">f.set_estimator('xgboost')<br/>f.ingest_grid(xgboost_grid)<br/>f.tune()<br/>f.auto_forecast()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/6f27e21651f6a6040d244adf9080c805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyeP-LOnft75bjQ39z30Nw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/49b174a931e385c1c45d99295a4329b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*nDKG1lA5ox9xTQ3qMhnZ6Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4bdc16875ed453d3d411cb45417a05a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*Xmhy0z9mIQ3jO1Z-uQ9axw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="93fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">XGBoost表现优于MLR，测试集MAPE为12%。然而，它似乎更加过度，样本内R2得分接近100%。</p><p id="9713" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到目前为止，希望您已经了解了这些模型是如何构建和评估的。在提供的笔记本中，您还可以看到<a class="ae mf" href="https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc" rel="noopener"> LightGBM </a>(微软的boosted tree模型，类似于XGBoost)、随机梯度下降和K-最近邻的示例。为了简洁起见，我将跳到打包和堆叠模型来结束这篇文章。</p><h1 id="7b3f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">制袋材料</h1><p id="2d3b" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">Scikit-learn的BaggingRegressor使用了本文随机森林部分介绍的相同的采样概念，但不是每个底层估计器都是决策树，我们可以使用任何我们喜欢的模型。在这种情况下，我指定了10个<a class="ae mf" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html" rel="noopener ugc nofollow" target="_blank">多层感知器</a>神经网络模型，每层100个单元，以及LBFGS解算器。允许每个数据子集使用原始数据集大小的90%来对观察值进行采样，还可以随机使用原始数据集的50%的特征。在代码中，它看起来像这样:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5ac7" class="ni mh iq ne b gy nj nk l nl nm">f<strong class="ne ir">.</strong>add_sklearn_estimator(BaggingRegressor,'bagging')<br/>f<strong class="ne ir">.</strong>set_estimator('bagging')<br/>f<strong class="ne ir">.</strong>manual_forecast(<br/>     base_estimator <strong class="ne ir">=</strong> MLPRegressor(<br/>         hidden_layer_sizes<strong class="ne ir">=</strong>(100,100,100),<br/>         solver<strong class="ne ir">=</strong>'lbfgs'<br/>     ),<br/>     max_samples <strong class="ne ir">=</strong> 0.9,<br/>     max_features <strong class="ne ir">=</strong> 0.5,<br/>)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/5cd310cf2a69c9dd99adf7e9e4fd44e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KC4HAL84CyBNMKuEUjq7MA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/97a40e67b4afa68b227ac79282babb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEY0cikvdtYIjKllrguV-g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b659937d40dc80a053f1a8a7fac8c7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*p4xH5pPMGljn7eEi0BHUnQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="11ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个模型迄今为止表现最好，测试集MAPE为11%，测试集R2得分为79%。从XGBoost和MLR这两个指标来看，这比第二好的模型要好得多。</p><h1 id="30c6" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">堆垛</h1><p id="28ce" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">最后一个模型是StackingRegressor。它创建了一个新的估计器，该估计器使用来自其他指定模型的预测来创建最终预测。它使用传递给<code class="fe nw nx ny ne b">final_estimator </code>参数的估计量进行最终预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b41ca3ee02cf49d2cbf01e359b63b235.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*WgA78XrvY4Qrv-DqsFJjdg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ebf0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的示例中，我们堆叠了四个模型，这些模型评估了该数据集上的最佳样本外数据:</p><ul class=""><li id="632f" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">k-最近邻(KNN)</li><li id="9e53" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">XGBoost</li><li id="f70b" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">LightGBM</li><li id="8687" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">随机梯度下降</li></ul><p id="d16a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后一个估计量是我们在上一节定义的bagging估计量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3a285deef5139c50bb79dedd1712f0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hfhmlWwhgp8UPD4ecDVz4A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6694" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用BaggingRegressor作为最终估计器的优点是，即使KNN和XGBoost模型高度过拟合，MLP模型也应该相应地在概念上人为地将它们的预测加权，因为这个模型只使用任何给定MLP模型的一半特征输入来训练，它也应该学会信任来自两个没有过拟合的模型的预测:LightGBM和SGD。因此，该模型被评估为:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/c4a86212f17323a588b63d0b8e84a4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEJq0PIs9_ozv7q6uqh5cg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/c006664e630b84a3213ae0839fc3f08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKLiIFHbFZJr06w8zyrekA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/679eeeb9bff00960aa7a1ff174e597a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*OmVTGhuPNfVCk7v4HxH-zg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3489" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">增加模型的复杂性并不总是会改善结果，但这似乎是正在发生的事情。我们的堆叠模型明显优于其他模型，测试集MAPE为10%，测试集R2得分为83%。它还具有与评估的MLR相当的样本内指标。</p><h1 id="5adf" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">回溯测试</h1><p id="70fd" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">为了进一步验证我们的模型，我们可以对它们进行回溯测试。这是一个迭代地评估它们在最后n个预测范围内的准确性的过程，以查看如果该模型被实施、仅在每个预测范围之前的观测值上被训练，实际上会实现什么结果。默认情况下，scalecast选择10个预测时段，其长度由对象中生成的未来日期数决定(在我们的示例中为60)。下面是它在代码中的样子:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4372" class="ni mh iq ne b gy nj nk l nl nm">f<strong class="ne ir">.</strong>backtest('stacking')<br/>f<strong class="ne ir">.</strong>export_backtest_metrics('stacking')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/e510a471fe945c1c3b33844f8e9fbe58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jZYVmtyGG4Eyo_xJnojcCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f599" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这告诉我们，平均而言，使用60天的预测长度，我们的模型将获得7%的MAPE分数和76%的R2分数。这是一个很好的方法来验证模型不仅仅是幸运地得到了特定的测试集，而且实际上可以推广到看不见的数据。</p><h1 id="d27a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="2514" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">这篇文章介绍了几种用于预测的建模概念，包括线性方法(MLR、Lasso、Ridge)、树集合(Random Forest、XGBoost、LightGBM)、装袋和堆叠。通过提高应用于它的非线性方法的复杂性，并使用巧妙的采样方法，我们在一个有大约400个观察值的测试集上获得了10%的测试集MAPE分数。我希望这个例子能够激发你自己的一些想法，我很感谢你的参与！如果你觉得这个概述有帮助，在GitHub上给scalecast包打个星吧。</p><div class="ol om gp gr on oo"><a href="https://github.com/mikekeith52/scalecast" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">GitHub - mikekeith52/scalecast:使用多种工具轻松、动态地预测时间序列</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">Scalecast是一个轻量级的建模过程和模型包装器，是为那些至少有中级…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">github.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div></div></div>    
</body>
</html>