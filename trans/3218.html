<html>
<head>
<title>Towards Geometric Deep Learning III: First Geometric Architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">走向几何深度学习III:第一几何架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-geometric-deep-learning-iii-first-geometric-architectures-d1578f4ade1f#2022-07-18">https://towardsdatascience.com/towards-geometric-deep-learning-iii-first-geometric-architectures-d1578f4ade1f#2022-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b930" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">几何深度学习的起源</h2><div class=""/><div class=""><h2 id="1a05" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">几何深度学习从对称性和不变性的角度处理了一大类ML问题，为多种多样的神经网络架构(如CNN、gnn和Transformers)提供了一个通用蓝图。在一系列新的帖子中，我们研究这些想法如何将我们从古希腊带到卷积神经网络。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/036e04f8dc9457511b4c9accbb55f1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_9SFwoZWK7uM92zLpniGQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片:Shutterstock。</p></figure><p id="27a9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在“走向几何深度学习系列”的第三篇文章中，我们讨论了第一个“几何”神经网络:Neocognitron和CNN。这篇文章基于M. M .布朗斯坦、j .布鲁纳、t .科恩和p .韦利奇科维奇、 <a class="ae me" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank"> <em class="md">几何深度学习</em> </a> <em class="md">(在麻省理工学院出版社完成后出现)一书的介绍章节，并伴随</em> <a class="ae me" href="https://youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C" rel="noopener ugc nofollow" target="_blank"> <em class="md">我们的课程</em> </a> <em class="md">参加非洲机器智能大师赛(AMMI)。参见</em> <a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-i-on-the-shoulders-of-giants-726c205860f5?sk=fd04bfaab732177ba7b4d7da90d88e9e"> <em class="md">第一部分</em></a><em class="md"/><a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-ii-the-perceptron-affair-fafa61b5c40a?sk=7a8f4beb9bd4ed347ad01c05ea54fb2e"><em class="md">第二部分</em> </a> <em class="md">关于神经网络的早期历史以及第一部《艾冬》、《第四部分</em>  <em class="md">献给早期的GNNs。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="cc41" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated">新“几何”类型的第一个神经网络架构的灵感来自神经科学。在一系列经典实验中，哈佛大学神经生理学家David Hubel和Torsten Wiesel[1-2]揭示了大脑中负责模式识别的部分——视觉皮层——的结构和功能。通过向一只猫呈现变化的光模式，并测量其脑细胞(神经元)的反应，他们表明，视觉皮层中的神经元具有局部空间连接的多层结构:只有在其附近的细胞(“感受野”[3])被激活时，细胞才会产生反应。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mv"><img src="../Images/16b79c6e2c877a9b207bd25a49c744ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdcidclSc5rA-_h-dw4DIQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">获得诺贝尔奖的生理学家大卫·胡贝尔和托尔斯滕·威塞尔以及对他们经典实验的描述揭示了视觉皮层的结构。肖像:伊霍尔·高斯基。</p></figure><p id="d012" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，该组织似乎是分层的，其中'<em class="md">简单细胞</em>对局部原始定向阶跃刺激的反应由'<em class="md">复杂细胞</em>聚集，后者对更复杂的模式产生反应。有人假设，视觉皮层深层的细胞会对越来越复杂的由简单模式组成的模式做出反应，这半开玩笑地暗示了“T4祖母细胞”的存在，这种细胞只有在看到祖母的脸时才会做出反应。</p><h1 id="13ed" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">新克隆体</h1><p id="3f10" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi mm translated">对视觉皮层结构的理解对计算机视觉和模式识别的早期工作产生了深远的影响，人们多次试图模仿它的主要成分。Kunihiko Fukushima，当时是日本广播公司的一名研究员，开发了一种新的神经网络架构[5]“类似于Hubel和Wiesel提出的视觉神经系统的层次模型”，被命名为<em class="md"> neocognitron </em> [6]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/88e46c2a65fddd8d520d63dc2ca45933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-LnbodSTXnewKx-SfKq1Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Kunihiko Fukushima和neocognitron，一种早期的几何深度学习架构和现代卷积神经网络的前身。</p></figure><p id="1dc5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">neocognitron由交错的神经元的C层组成(这一命名惯例反映了它在生物视觉皮层中的灵感)；每层中的神经元按照输入图像的结构(“视网膜主题”)排列成2D阵列，每层有多个“细胞平面”(现代术语中的特征图)。S层被设计为翻译对称:它们使用共享的可学习权重聚集来自局部感受野的输入，导致单个细胞平面中的细胞具有相同功能的感受野，但位置不同。基本原理是挑选可能出现在输入中任何地方的模式。C层是固定的，并执行局部汇集(加权平均)，对模式的特定位置不敏感:如果其输入中的任何神经元被激活，则C神经元将被激活。</p><p id="163e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated">因为新认知的主要应用是字符识别，所以平移不变性是至关重要的。这一特性与早期的神经网络(如Rosenblatt的感知器)有着根本的区别:为了可靠地使用感知器，必须首先对输入模式的位置进行归一化，而在neocognitron中，对模式位置的不敏感性被嵌入到架构中。Neocognitron通过将平移等变的局部特征提取层与池交错实现，创建了多尺度表示[8]。计算实验表明，福岛的建筑能够成功地识别复杂的图案，如字母或数字，即使存在噪声和几何失真。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/328d927f0883992016d7a1e9e35d2852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VU9ovPipRajVNjugXbIZNw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">neocognitron输出的例子证实了它对位移、几何失真和噪声的不敏感性。图片来自[5]。</p></figure><p id="afb2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从该领域四十年进展的有利角度来看，人们发现新认知体已经具有现代深度学习架构的许多惊人特征:深度(Fukishima在他的论文中模拟了一个七层网络)、局部感受域、共享权重和池化。它甚至使用了半整流器(ReLU)激活功能，这通常被认为是在最近的深度学习架构中引入的[9]。与现代系统的主要区别在于网络的训练方式:neocognitron是一种以无监督方式训练的“自组织”架构，因为反向传播尚未在神经网络社区中广泛使用。</p><h1 id="80bc" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">卷积神经网络</h1><p id="91e9" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> F </span> ukushima的设计由Yann LeCun进一步开发，他是巴黎大学的应届毕业生[10]，博士论文是关于使用反向传播来训练神经网络。在贝尔实验室的第一个博士后岗位上，LeCun和他的同事们建立了一个系统来识别信封上的手写数字，以使美国邮政服务能够自动发送邮件。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/b6927325ed68dac00b61ceddeec42c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdKY8o9bidZ9gfvCPqWLRA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Yann LeCun和合著者[11]首次引入卷积神经网络(尽管“卷积”这个名称后来才出现)。在DSP上实现，它允许实时手写数字识别。</p></figure><p id="a84a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在一篇现在已经很经典的论文[11]中，LeCun等人描述了第一个三层卷积神经网络(CNN) [12]。与neocognitron类似，LeCun的CNN也使用共享权重和池的本地连接。然而，它放弃了福岛更复杂的非线性滤波(抑制连接)而支持简单的线性滤波器，这种滤波器可以在数字信号处理器(DSP)上使用<a class="ae me" href="https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation" rel="noopener ugc nofollow" target="_blank">乘加</a>运算有效地实现为<em class="md">卷积</em>。</p><p id="3681" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种设计选择，脱离了神经科学的灵感和术语，进入了信号处理领域，将在深度学习的成功中发挥至关重要的作用。CNN的另一个关键创新是使用反向传播进行训练。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/66b82bb01c134f72b893a3267c113f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dx_0ni2nTkg6PMhuLsH3bg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LeNet-5，五层卷积架构[14]。</p></figure><p id="f6cc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">LeCun的工作令人信服地展示了基于梯度的方法在复杂模式识别任务中的强大功能，并且是首批基于深度学习的计算机视觉实用系统之一。这种架构的一个演变，一个五层的CNN被命名为LeNet-5，是作者名字的双关语[14]，被美国银行用来阅读手写支票。</p><p id="6486" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，计算机视觉研究界的绝大多数人都避开了神经网络，走上了一条不同的道路。新千年第一个十年的视觉识别系统的典型架构是一个精心制作的特征提取器(通常检测图像中的兴趣点，并以一种对透视变换和对比度变化鲁棒的方式提供它们的局部描述[15])，然后是一个简单的分类器(最常见的是支持向量机(SVM)，较少见的是一个小型神经网络)[16]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/c3d9e025bb08731f7e83db98ff6ac034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCX60Fw8XqH-JMCL97hjZA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">2000年代的典型“单词袋”图像识别系统[16]，由一个局部特征检测器和描述符组成，后跟一个简单的分类器。</p></figure><h1 id="7098" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">深度学习的胜利</h1><p id="f269" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">然而，计算能力的快速增长和可用的带注释的可视数据的数量改变了力量的平衡。实现和训练越来越大和越来越复杂的CNN成为可能，这允许处理越来越具有挑战性的视觉模式识别任务[17]，最终成为当时计算机视觉的圣杯:ImageNet大规模视觉识别挑战。ImageNet由美籍华人研究员费于2009年建立，是一项年度挑战，包括将数百万张人类标记的图像分类到1000个不同的类别。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/6bd6481bfc0f22b31e2bbfd3078e6d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zbSQe8paWBCL9oGyMp3Y1w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ImageNet大规模视觉识别挑战的结果。AlexNet在2012年成为第一个击败“手工制作”方法的深度学习架构；从那以后，所有的获胜方法都是基于深度学习的。</p></figure><p id="46b2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Krizhevsky、Sutskever和Hinton [18]在多伦多大学开发的CNN架构成功地以较大优势击败了所有竞争方法，如基于该领域数十年研究的智能设计的特征检测器。AlexNet(这种架构是为了纪念其开发者Alex Krizhevsky而命名的)在参数和层数方面比它的老兄弟LeNet-5大得多[20]，但在概念上是相同的。关键的区别是使用图形处理器(GPU)进行训练[21]，这是现在深度学习的主流硬件平台[22]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/2a0e245100ce320d0fcadf126e97e038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ic6mP0AZfabu71Ch.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">AlexNet架构。图片:<a class="ae me" href="https://medium.com/analytics-vidhya/types-of-convolutional-neural-networks-lenet-alexnet-vgg-16-net-resnet-and-inception-net-759e5f197580" rel="noopener">巴维什·辛格·毕斯特</a>。</p></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="fe4b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di">T</span>CNN在ImageNet上的成功成为深度学习的转折点，并预示着它在接下来的十年中被广泛接受。数十亿美元的产业应运而生，深度学习成功用于商业系统，从苹果iPhone的语音识别到特斯拉的自动驾驶汽车。在对<a class="ae me" rel="noopener" target="_blank" href="/towards-geometric-deep-learning-ii-the-perceptron-affair-fafa61b5c40a?sk=7a8f4beb9bd4ed347ad01c05ea54fb2e">罗森布拉特的作品</a>进行严厉评论四十多年后，联结主义者终于被证明是正确的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="0672" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[1] D. H. Hubel和T. N. Wiesel，猫的纹状皮层中单个神经元的感受野(1959)，生理学杂志148(3):574。</p><p id="067e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2] D. H. Hubel和T. N. Wiesel，猫的视觉皮层中的感受野、双眼互动和功能性结构(1962)，生理学杂志160(1):106。</p><p id="85a8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]术语“感受野”出现在Hubel和Wiesel之前，并且从二十世纪早期就被神经生理学家使用，参见c .谢灵顿，<em class="md">神经系统的整合作用</em> (1906)，耶鲁大学出版社。</p><p id="5614" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]“祖母细胞”这个术语很可能第一次出现在杰里·莱特文于1969年在麻省理工学院讲授的“感知和知识的生物学基础”课程中。类似的“诺斯替神经元”的概念在两年前由波兰神经科学家J. Konorski所著的《大脑的整合活动》一书中提出；跨学科方法 (1967)。参见C. G. Gross，<a class="ae me" href="https://www.researchgate.net/profile/Charles-Gross-3/publication/11086433_Genealogy_of_the_Grandmother_Cell/links/00463527ab1971d96e000000/Genealogy-of-the-Grandmother-Cell.pdf" rel="noopener ugc nofollow" target="_blank">“祖母细胞”的谱系学</a> (2002)，《神经科学家》8(5):512–518。</p><p id="0c29" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5] K. Fukushima，<a class="ae me" href="https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf" rel="noopener ugc nofollow" target="_blank"> Neocognitron:不受位置变化影响的模式识别机制的自组织神经网络模型</a> (1980)，生物控制论36:196–202。标题中提到了平移不变性。</p><p id="6987" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]经常被拼错为“<em class="md"> neuro </em> cognitron”，名称“neocognitron”表明它是K. Fukushima的早期架构的改进版本，Cognitron: a self-organizing多层神经网络(1975)，生物控制论20:121–136。</p><p id="e317" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]用作者自己的话来说，具有“仅取决于刺激模式的形状，而不受模式呈现位置的影响”的输出</p><p id="d195" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]我们称这个原理为<em class="md">尺度分离</em>，它和对称性一样，是许多物理系统的基本属性<em class="md">。</em>在卷积架构中，除了平移之外，尺度分离还允许处理更广泛的几何变换。</p><p id="55be" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9] ReLU型激活至少可以追溯到20世纪60年代，之前已经在K. Fukushima的模拟阈值元件多层网络的视觉特征提取(1969)中使用过。系统科学和控制论5(4):322–333。</p><p id="a47d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]皮埃尔-玛丽-居里大学，现为索邦大学的一部分。</p><p id="5b0b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11] Y. LeCun等人，应用于手写邮政编码识别的反向传播(1989)神经计算1(4):541–551。</p><p id="fc86" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[12]在le Cun 1989年的论文中，架构没有被命名；术语“卷积神经网络”或“convnet”将出现在1998年的一篇论文中[14]。</p><p id="5dbb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[13] LeCun的第一个CNN是在一个CPU(一个SUN-4/250机器)上训练出来的。然而，使用经过训练的CNN的图像识别系统在美国电话电报公司DSP-32C(具有256KB存储器的第二代数字信号处理器，能够以32位精度每秒执行125m浮点乘加运算)上运行，每秒实现30次以上的分类。</p><p id="0b9c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[14] Y. LeCun等人，基于梯度的学习应用于文档识别(1998)，Proc .IEEE 86(11):2278–2324。</p><p id="3f92" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[15]最流行的特征描述符之一是由David Lowe于1999年提出的<a class="ae me" href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform" rel="noopener ugc nofollow" target="_blank">尺度不变特征变换</a> (SIFT)。这篇论文被多次拒绝，仅在五年后发表，D. G. Lowe，比例不变关键点的独特图像特征，(2004)IJCV 60(2):91–110。这是被引用最多的计算机视觉论文之一。</p><p id="01b0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[16]一种典型的方法是“单词袋”,将图像表示为矢量量化的局部描述符的直方图。例如，参见J. Sivic和A. Zisserman，视频谷歌:视频中对象匹配的文本检索方法(2003)，ICCV。</p><p id="3919" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[17]特别是，Jürgen Schmidhuber 的<a class="ae me" href="https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html" rel="noopener ugc nofollow" target="_blank">小组开发的深度大规模CNN模型赢得了多个视觉比赛，包括汉字识别(D. C. Ciresan等人，用于手写数字识别的深度大简单神经网络(2010)，神经计算22(12):3207–3220)和交通标志识别(D. C. Ciresan等人，用于交通标志分类的多列深度神经网络。<em class="md">神经网络</em>32:333–338，2012)。</a></p><p id="111c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[18] A. Krizhevsky、I. Sutskever和G. E. Hinton，使用深度卷积神经网络的ImageNet分类(2012年)，NIPS。</p><p id="47fe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[19] AlexNet取得了比亚军小10.8%以上的误差。</p><p id="def1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[20] AlexNet有11层，在来自ImageNet的1.2M图像上进行训练(作为比较，LeNet-5有5层，在60K MNIST数字上进行训练)。与LeNet-5相比，其他重要变化包括使用ReLU激活(代替tanh)、最大汇集、退出规则和数据扩充。</p><p id="8941" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[21]在一对Nvidia GTX 580图形处理器上训练AlexNet花了将近一周的时间，每秒能够进行大约200G的浮点运算。</p><p id="9ef3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[22]尽管GPU最初是为图形应用程序设计的，但它们最终成为了一个方便的硬件平台，用于<a class="ae me" href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units#History" rel="noopener ugc nofollow" target="_blank">通用计算</a>(“gp GPU”)。第一个这样的作品展示了线性代数算法，例如参见J. Krüger和R. Westermann的《用于数值算法的GPU实现的线性代数算子》(2003)，ACM Trans。图形22(3):908–916。GPU首次用于神经网络是由K.-S. Oh和K. Jung，神经网络的GPU实现(2004)，模式识别37(6):1311–1314，比AlexNet早了近十年。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="40ee" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">福岛的肖像是由伊霍尔·戈尔斯基手绘的。几何深度学习的详细讲座资料可在</em> <a class="ae me" href="http://www.geometricdeeplearning.com/" rel="noopener ugc nofollow" target="_blank"> <em class="md">项目网页</em> </a> <em class="md">获取。参见迈克尔的</em> <a class="ae me" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"> <em class="md">其他帖子</em> </a> <em class="md">在走向数据科学，</em> <a class="ae me" href="https://michael-bronstein.medium.com/subscribe" rel="noopener"> <em class="md">订阅</em> </a> <em class="md">到他的帖子和</em> <a class="ae me" href="https://www.youtube.com/c/MichaelBronsteinGDL" rel="noopener ugc nofollow" target="_blank"> <em class="md"> YouTube频道</em> </a> <em class="md">，获取</em> <a class="ae me" href="https://michael-bronstein.medium.com/membership" rel="noopener"> <em class="md">中等会员</em> </a> <em class="md">，或者关注</em> <a class="ae me" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"> <em class="md">迈克尔</em> </a> <em class="md"/></p></div></div>    
</body>
</html>