<html>
<head>
<title>Mixing Art into the Science of Model Explainability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将艺术融入模型解释的科学中</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95#2022-09-27">https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95#2022-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="53d5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">可解释Boosting机综述及一种将ML解释转换为更人性化解释的方法。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2f8840e78f5f96104ef00e7a69d0f4c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TqjLzXrkCFL-XHgv.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 —我桌子上的乐高玩具，作者拍摄。</p></figure></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="295d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">1.最大似然解释科学</h1><h2 id="7aaf" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">1.1可解释性与准确性的权衡</h2><p id="b5b5" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">在传统的表格机器学习方法中，数据科学家经常处理b/w可解释性和准确性之间的权衡。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/db5ab20bb5c07cb7d28752727587c8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TeL_MwwcJjfEzM14zP53g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:可解释性/可理解性和准确性的权衡，作者图片</p></figure><p id="42ee" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">如上图所示，我们可以看到像逻辑回归、朴素贝叶斯和决策树这样的<strong class="ml ir">玻璃箱模型</strong>是简单的解释模型，这些模型的预测并不十分准确。另一方面，<strong class="ml ir">黑箱模型</strong>像提升树、随机森林和神经网络很难解释，但却能带来高度准确的预测。</p><h2 id="c76b" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">EBMs简介</h2><p id="9d73" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">为了解决上述问题，微软研究院开发了EBMs(可解释增压机)模型[1]。“可解释的助推机器(EBM)是基于树的、循环梯度助推广义加法模型，具有自动交互检测。EBM通常与最先进的黑盒模型一样精确，同时保持完全可解释性。虽然EBM的训练速度通常比其他现代算法慢，但EBM在预测时非常紧凑和快速。”[2]</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/9c99a8f231a37cc204d5d646a86f3083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RoM1mLpGIjtjf3x1S3lFKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3: EBMs打破了可解释性与准确性的悖论，作者图片</p></figure><p id="17a3" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">从上面的图表中我们可以看到，EBM帮助我们打破了这种权衡悖论，并帮助我们建立高度可解释和精确的模型。为了进一步理解EBMs背后的数学，我强烈建议观看这段12分钟的YouTube视频</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="0dd8" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">1.3玻璃盒子vs黑盒模型。选什么？</h2><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="b3f2" class="lu ld iq nm b gy nq nr l ns nt"><strong class="nm ir">Tip: </strong><em class="nu">The answer to every complex question in life is “It depends”.</em></span></pre><p id="b3cd" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">与黑盒模型相比，使用玻璃盒模型有一些利弊。选择一个模型而不是另一个模型没有明显的赢家，但是根据情况，DS可以对选择什么模型做出有根据的猜测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/d7011a8d426d77febce35c2cb10967c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dMidJgGNaajQCSSk.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:玻璃盒模型与黑盒模型，图片由作者提供。</p></figure><p id="7f23" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">选择玻璃盒或黑盒模型时需要考虑的两个因素如下-</p><p id="13ac" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated"><strong class="ml ir"> 1)可解释性需求</strong> —在不需要解释的领域，或者数据科学家或技术观众出于直觉/检查目的需要解释的领域，在这些情况下，DS最好使用黑盒模型。在由于业务或法规要求而需要解释的领域中，或者在这些解释是提供给非技术观众(人)的领域中，玻璃盒模型占了上风。这是因为来自玻璃盒子模型的解释是<code class="fe nw nx ny nm b">exact</code>和<code class="fe nw nx ny nm b">global</code>。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="1856" class="lu ld iq nm b gy nq nr l ns nt"><strong class="nm ir">Note:</strong> <em class="nu">Exact</em><em class="nu"> and </em><em class="nu">global</em><em class="nu"> just means that a value of a particular feature will always have the same effect on each prediction explanation. For example, in the case of the prediction of income of a particular individual being above $50k with age as one of the predictors, if the </em><em class="nu">age</em><em class="nu"> is 40 and it will impact the target variable with the same proportion let us say 5% in each observation in the data where the age is 40. This is not the case when we build explanations through LIME and Shapely for black box models. In black-box models, </em><em class="nu">age</em><em class="nu"> with the value 40 for example can have a 10% lift in an individual probability of their income being above 50k for one observation and -10% lift in the other.</em></span></pre><p id="4cef" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated"><strong class="ml ir"> 2)计算要求</strong> — DS需要注意各种计算要求，以便根据其用例测试和训练模型。EBM在训练阶段特别慢，但是提供了带有内置解释的快速预测。因此，在每小时都需要训练模型的情况下，EBMs可能无法满足您的需求。但是，如果模型的训练是每月/每周进行的，并且分数生成的频率更高(每小时/每天),那么EBMs可能更适合这个用例。此外，在可能需要对每个预测做出解释的情况下，EBMs可以节省大量计算，并且可能是用于数百万次观察的唯一可行的技术。请看下文，了解b/w EBMs和其他基于树的集成方法的操作差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/435a2118d0246740e73e1a3c6e02b7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LMwvv1Ib8E29t992.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5: EBMs vs XgBoost/LightGBM，作者图片。</p></figure></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="def0" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">2.动手使用EBMs</h1><h2 id="1d49" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">2.1数据概述</h2><p id="8d8b" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">对于这个例子，我们将使用来自<strong class="ml ir"> UCI机器学习库【3】</strong>的<a class="ae oa" href="https://archive.ics.uci.edu/ml/datasets/Adult" rel="noopener ugc nofollow" target="_blank">成人收入数据集</a>。该数据集中的问题设置为二元分类问题，根据各种人口普查信息(教育程度、年龄、性别、职业等)来预测某个人的收入。)超过5万美元/年。为了简单起见，我们只使用美国个人的观察结果和以下预测值-</p><ul class=""><li id="c879" class="ob oc iq ml b mm nd mp ne lz od mc oe mf of nb og oh oi oj bi translated"><code class="fe nw nx ny nm b">Age</code> : <code class="fe nw nx ny nm b">continuous</code>变量，个人年龄</li><li id="3805" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb og oh oi oj bi translated"><code class="fe nw nx ny nm b">Occupation</code> : <code class="fe nw nx ny nm b">categorical</code>变量，技术支持，工艺维修，其他服务，销售，执行管理，专业，处理-清洁，机器-操作-检查，行政-文书，农业-渔业，运输-搬家，私人-房屋-服务，保护-服务，武装部队。</li><li id="1e4b" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb og oh oi oj bi translated"><code class="fe nw nx ny nm b">HoursPerWeek</code> : <code class="fe nw nx ny nm b">continuous</code>变量，每周花在工作上的小时数</li><li id="6f3e" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb og oh oi oj bi translated"><code class="fe nw nx ny nm b">Education</code> : <code class="fe nw nx ny nm b">categorical</code>变量，学士，一些学院，11，HS-grad，Prof-school，Assoc-acdm，Assoc-voc，9，7-8，12，硕士，1-4，10，博士，5-6，学前。</li></ul><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="dbd3" class="lu ld iq nm b gy nq nr l ns nt"><strong class="nm ir">Note — </strong>To render the blog with both code correctly, you can read the same on my <a class="ae oa" href="https://aayushmnit.com/posts/2022-09-23-Explainability/2022-09-23-Explainability.html#the-art-of-ml-explainability" rel="noopener ugc nofollow" target="_blank">quarto powered blog</a>.</span></pre><pre class="op nl nm oq bn or os bi"><span id="e544" class="ot ld iq nm b be ou ov l ow nt">## Importing required libraries<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn import metrics<br/>from interpret.glassbox import ExplainableBoostingClassifier<br/>from interpret import show<br/>import warnings<br/>import plotly.io as pio<br/>import plotly.express as px<br/>warnings.filterwarnings('ignore')<br/>pio.renderers.default = "plotly_mimetype+notebook_connected"<br/><br/>## Loading the data<br/>df = pd.read_csv( "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", header=None)<br/>df.columns = [<br/>    "Age", "WorkClass", "fnlwgt", "Education", "EducationNum",<br/>    "MaritalStatus", "Occupation", "Relationship", "Race", "Gender",<br/>    "CapitalGain", "CapitalLoss", "HoursPerWeek", "NativeCountry", "Income"<br/>]<br/><br/>## Filtering for Unites states<br/>df = df.loc[df.NativeCountry == ' United-States',:]<br/><br/>## Only - Taking required columns<br/>df = df.loc[:,["Education", "Age","Occupation", "HoursPerWeek", "Income"]]<br/><br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/b346fea5f1b106dedfd68f1ca310c4ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0VgAwZmdvnzrrvtsoF_cg.png"/></div></div></figure><p id="f355" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">让我们看看目标变量分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/3e0eda5032588ea195775af70cd3f1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-71M1orS-gUYBZktRfn4TQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 —目标变量分布，作者图片。</p></figure><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="4b39" class="ot ld iq nm b be ou ov l ow nt">print(df.Income.value_counts(normalize=True))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/ebd4a69c8343d237459683878557964e.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*asShtgSwrOGuPBlN1DTJWw.png"/></div></figure><p id="76af" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">在我们的数据集中，约24.6%的人收入超过5万美元。数据看起来不错，我们有我们需要的列。我们将使用教育、年龄、职业和每周工作时间列来预测收入。在建模之前，让我们执行一个80–20的训练测试划分。</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="00db" class="ot ld iq nm b be ou ov l ow nt">## Train-Test Split <br/>X = df[df.columns[0:-1]] <br/>y = df[df.columns[-1]] seed = 1 <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed) <br/>print(f"Data in training {len(y_train)}, Data in testing {len(y_test)}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/812cd2ad7e088b45acdc7c9b2114e598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Mbu6XH-BjZ8PXjTAElhJg.png"/></div></div></figure><h2 id="a7f1" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">2.2拟合循证医学模型</h2><p id="2876" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">EBM有一个scikit兼容的API，因此拟合模型和进行预测与任何scikit学习模型是一样的。</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="a8c6" class="ot ld iq nm b be ou ov l ow nt">ebm = ExplainableBoostingClassifier(random_state=seed, interactions=0) <br/>ebm.fit(X_train, y_train)  <br/>auc = np.round(<br/>                metrics.roc_auc_score(<br/>               (y_test != ' &lt;=50K').astype(int).values,<br/>                ebm.predict_proba(X_test)[:,1]),<br/>               3) <br/>print(f"Accuracy: {np.round(np.mean(ebm.predict(X_test) == y_test)*100,2)}%, AUC: {auc}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/f72c4e89372cd86cee6bfe4ae7f32531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lje3FRpY_ITAoyCfEif3bQ.png"/></div></div></figure><p id="1147" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">我希望上面的代码块显示了interpret-ml API与scikit learn API是多么相似。基于验证集的AUC，我们可以说我们的模型比随机预测更好。</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="3fb1" class="lu ld iq nm b gy nq nr l ns nt"><strong class="nm ir">Tip:</strong> <em class="nu">In practice, if you are dealing with millions of observations, Try doing feature selection using LightGBM/XGboost and only train your final models using EBMs. This will save you time in feature exploration.</em></span></pre><h2 id="83f2" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">2.3来自EBMs的解释</h2><p id="0323" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">解释包提供了全局和局部解释，并有各种可视化工具来检查模型正在学习的内容。</p><blockquote class="pb pc pd"><p id="2c31" class="mj mk nu ml b mm nd jr mo mp ne ju mr pe nf mt mu pf ng mw mx pg nh mz na nb ij bi translated">2.3.1全球解释</p></blockquote><p id="3d79" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">全局解释提供了以下可视化效果-</p><ol class=""><li id="9918" class="ob oc iq ml b mm nd mp ne lz od mc oe mf of nb ph oh oi oj bi translated"><strong class="ml ir">总结</strong> —特征重要性图，该图提供了每个预测因子在预测目标变量中的重要性。</li><li id="b395" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi translated"><strong class="ml ir">特征与预测的相互作用</strong> —该图表与EBM在进行实际预测时使用的查找表相同。这可以帮助您检查特征值对预测的贡献。</li></ol><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="2884" class="ot ld iq nm b be ou ov l ow nt">ebm_global = ebm.explain_global() <br/>show(ebm_global, renderer='notebook')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/1cf5d5eb44b42def1c9dc978532dc103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZHq45aoHXSjlX6qU.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6: EBMs全球解说，作者图片。</p></figure><blockquote class="pb pc pd"><p id="1734" class="mj mk nu ml b mm nd jr mo mp ne ju mr pe nf mt mu pf ng mw mx pg nh mz na nb ij bi translated">当地的解释</p></blockquote><p id="5fbe" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">局部解释是我们每个观察层次的解释。EBM有一个很好的内置可视化来显示这些信息。</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="54e2" class="ot ld iq nm b be ou ov l ow nt">ebm_local = ebm.explain_local(X_test.iloc[0:5,:], y_test) show(ebm_local, renderer='notebook')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/727990da84617b0cec56aecbabbfc971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WXuyWLHlvUyZ0_yt.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7: EBMs本地解释，作者图片。</p></figure><p id="fc24" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">让我们举一个例子来解释指数为0时的观察结果</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="cc31" class="ot ld iq nm b be ou ov l ow nt">explainDF = pd.DataFrame.from_dict(<br/>    {<br/>        'names': ebm_local.data(0)['names'], <br/>        'data':ebm_local.data(0)['values'], <br/>        'contribution':ebm_local.data(0)['scores']<br/>    })<br/>explainDF</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/0c6c84f4ef59240d9b5cb97b67ab62d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vGlvz2usHdMHgEC7tqm1KQ.png"/></div></div></figure><p id="c0c4" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">正如我们从数据中看到的，我们可以看到列的名称、实际值以及该值对实际预测得分的贡献。对于这个观察，让我们看看模型在学习什么</p><ol class=""><li id="dc5d" class="ob oc iq ml b mm nd mp ne lz od mc oe mf of nb ph oh oi oj bi translated">本科教育有助于收入超过5万英镑</li><li id="f1b2" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi translated">年龄值47也有利于&gt; 50K</li><li id="3836" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi translated">职业是“？”对超过5万英镑的收入有负面影响</li><li id="4022" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi translated">每周18小时对超过5万英镑的收入有负面影响(美国每周平均工作时间约为40小时，所以这是有道理的)</li></ol><p id="17ef" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">您还可以对整个数据集执行此操作，并收集每个要素的重要性。下面是一个执行同样操作的示例代码。</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="8056" class="ot ld iq nm b be ou ov l ow nt">scores = [x['scores'] for x in ebm_local._internal_obj['specific']]<br/>summary = pd.DataFrame(scores)<br/>summary.columns = ebm_local.data(0)['names']<br/>summary.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/a7f091e3c055842aecf7a625203a235a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVbxtyKjWIOuF5GBnNqSHQ.png"/></div></div></figure><p id="198c" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">现在我们可以提取测试集中所有数据行的重要性。</p><h2 id="052a" class="lu ld iq bd le lv lw dn li lx ly dp lm lz ma mb lo mc md me lq mf mg mh ls mi bi translated">2.4缺点和问题</h2><p id="3b15" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">这种解释仍然非常抽象，即使在观察层面上，推理也不是人类(非技术)友好的。当特征数量增加时，这甚至变得对人类不友好。你的模型的典型商业消费者可能不擅长阅读这样的图表，并且回避尝试模型给他们的洞察力/预测。毕竟，如果我不理解某件事，我就不信任它。这就是艺术的用武之地，让我们看看如何在上面的观察的基础上，让它更容易理解。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="1028" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">3.ML解释的“艺术”</h1><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="f0ca" class="lu ld iq nm b gy nq nr l ns nt"><strong class="nm ir">Warning - </strong><em class="nu">The ideas I am going to share now are more marketing than real science.</em></span></pre><p id="2fdb" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">要让人们按照你的模特的建议行事，你必须<strong class="ml ir">建立信任</strong>。一个想法是建立信任，用数据轶事来支持你的解释。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/4be2b752debc7c2be734ef57b72b227c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ga4CqFPpG7LypF_B.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">信用— <a class="ae oa" href="https://www.flickr.com/photos/59632563@N04/6239670686" rel="noopener ugc nofollow" target="_blank"> Flickr </a>，许可— <a class="ae oa" href="https://creativecommons.org/licenses/by/2.0/" rel="noopener ugc nofollow" target="_blank">知识共享</a></p></figure><p id="c3fe" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">数据轶事到处存在，这个想法是我在看一个股市工具时想到的(下图)。注意他们强调的两件事-</p><ol class=""><li id="d4ed" class="ob oc iq ml b mm nd mp ne lz od mc oe mf of nb ph oh oi oj bi translated"><strong class="ml ir">什么事？</strong> —这个工具出色地展示了刚刚发生的事件。例如— <em class="nu">“微软公司因股息宣布</em>或<em class="nu">“达美航空14日RSI跌破70水平”</em>。</li><li id="6845" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi translated"><strong class="ml ir">为什么很重要？</strong> —然后工具指向历史数据，并告知该事件的意义。在微软的例子中，当事件发生时“历史上，MSFT的价格平均上涨了11.9%”。</li></ol><p id="51c0" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">如果我们能对我们的模型做同样的事情呢？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pn"><img src="../Images/1d9d741bf73efcb94a15d82f8f18fa99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GjIrqTMzoix0GIZ5.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8。快照取自我的保真工具，图片由作者提供。</p></figure><p id="2b0d" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">我们可以从训练数据中建立一个历史赔率表。让我们试着建造一个。</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="320b" class="ot ld iq nm b be ou ov l ow nt">odds_data = X_train.copy()<br/>odds_data['income'] = (y_train == " &gt;50K").astype(int) <br/>## Converting continous variables in buckets<br/>odds_data['AgeBucket'] =  (odds_data.Age // 5)<br/>odds_data['HoursPerWeekBucket'] = (odds_data.HoursPerWeek // 5)<br/><br/># Creating placeholder for odds dictionary<br/>odds_dict = {} <br/><br/># Columns for which we need odds<br/>columns = ['Education', 'AgeBucket', 'HoursPerWeekBucket', 'Occupation']<br/>for colname in columns: #iterating through each column<br/>    unique_val = odds_data[colname].unique() # Finding unique values in column<br/>    ddict = {}<br/>    for val in unique_val: # iterating each unique value in the column<br/>        ## Odds that income is above &gt; 50 in presence of the val<br/>        val_p = odds_data.loc[odds_data[colname] == val, 'income'].mean() <br/>        ## Odds that income is above &gt; 50 in absence of the val<br/>        val_np = odds_data.loc[odds_data[colname] != val, 'income'].mean()<br/>        <br/>        ## Calculate lift<br/>        if val_p &gt;= val_np:<br/>            odds = val_p / val_np<br/>        else:<br/>            odds = -1*val_np/(val_p+1e-3)<br/>        <br/>        ## Add to the col dict<br/>        ddict[val] = np.round(odds,1)<br/>    ## Add to the sub dict to odds dict<br/>    odds_dict[colname] = ddict<br/>print(odds_dict)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi po"><img src="../Images/6ea8774fa6e2cd3c94cd1064d64aece2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kTSJ7NcQGJKcGeobAsNzA.png"/></div></div></figure><p id="1d01" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">现在使用这个赔率表，我们可以用预先填充的模板生成预测。参考下图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pp"><img src="../Images/3404d1063e1bc64fde1ad14929936bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5uoCTdH5bDxABBOa.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图九。由作者输出到人类可读的文本、图像。</p></figure><p id="5382" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">使用之前为行索引0生成的<code class="fe nw nx ny nm b">explainDF</code>数据，我们可以使用上面的框架将其转换为文本。让我们看看输出是什么样子的-</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="7878" class="ot ld iq nm b be ou ov l ow nt">def explainPredictions(df, pred, odds_dict):<br/>    reasons = []<br/>    if pred == 0:<br/>        sdf =  df.loc[df.contribution &lt; 0, :].sort_values(['contribution']).reset_index(drop=True).copy()<br/>    else:<br/>        sdf =  df.loc[df.contribution &gt; 0, :].reset_index(drop=True).copy()<br/>    <br/>    for idx in range(sdf.shape[0]):<br/>        col_name = sdf.names[idx]<br/>        data = sdf.data[idx]<br/>        if col_name in odds_dict:<br/>            odd_value = odds_dict[col_name][data]<br/>        else:<br/>            odd_value = odds_dict[col_name+'Bucket'][data//5]<br/>            <br/>        s1 = f"This individual have {col_name} value '{data}'." <br/>        s2 = f"Historically, people with this behavior have {odd_value}x likely to have income over $50k."<br/>        reasons.append(s1+s2)<br/>    return reasons<br/><br/>explainPredictions(explainDF, ebm_local.data(0)['perf']['predicted'], odds_dict)</span></pre><pre class="op nl nm nn no aw np bi"><span id="b28c" class="lu ld iq nm b gy nq nr l ns nt">["This individual have Education value ' Bachelors'.Historically, people with this behavior have 2.0x likely to have income over $50k.",<br/> "This individual have Age value '47'.Historically, people with this behavior have 1.8x likely to have income over $50k."]</span></pre><p id="ee94" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">如果这是一个超过50k的预测呢？</p><pre class="kg kh ki kj gt nl nm oq bn or os bi"><span id="c626" class="ot ld iq nm b be ou ov l ow nt">explainPredictions(explainDF, 1, odds_dict)</span></pre><pre class="op nl nm nn no aw np bi"><span id="1a55" class="lu ld iq nm b gy nq nr l ns nt">["This individual have Education value ' Bachelors'.Historically, people with this behavior have 2.0x likely to have income over $50k.",  "This individual have Age value '47'.Historically, people with this behavior have 1.8x likely to have income over $50k."]</span></pre><p id="0ef9" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated">看起来棒极了！我们可以为超过50K和LinkedIn(领英)生成可读的推荐信，或者发邮件给我，地址是aayushmnit@gmail.com。您也可以在<a class="ae oa" href="https://medium.com/@aayushmnit" rel="noopener"> Medium </a>和<a class="ae oa" href="https://github.com/aayushmnit" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我，了解我将来可能会分享的博客帖子和探索项目代码。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="ab56" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考</h1><p id="c266" class="pw-post-body-paragraph mj mk iq ml b mm mn jr mo mp mq ju mr lz ms mt mu mc mv mw mx mf my mz na nb ij bi translated">“InterpretML:机器学习可解释性的统一框架”(H. Nori，S. Jenkins，P. Koch和R. Caruana 2019)</p><p id="09ce" class="pw-post-body-paragraph mj mk iq ml b mm nd jr mo mp ne ju mr lz nf mt mu mc ng mw mx mf nh mz na nb ij bi translated"><a class="ae oa" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">“解释ML — EBM文档”</a></p><h1 id="3527" class="lc ld iq bd le lf pq lh li lj pr ll lm jw ps jx lo jz pt ka lq kc pu kd ls lt bi translated">Dua d .和Graff c .(2019年)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。该数据集根据知识共享署名4.0国际版(CC BY 4.0)许可协议进行许可。</h1><ol class=""><li id="c095" class="ob oc iq ml b mm mn mp mq lz pv mc pw mf px nb ph oh oi oj bi">“InterpretML: A Unified Framework for Machine Learning Interpretability” (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)</li><li id="a9e4" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi"><a class="ae oa" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">“Interpret ML — EBM documentation”</a></li><li id="b4ae" class="ob oc iq ml b mm ok mp ol lz om mc on mf oo nb ph oh oi oj bi">Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.</li></ol></div></div>    
</body>
</html>