<html>
<head>
<title>Explainable AI (XAI) Methods Part 5— Global Surrogate Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能(XAI)方法第五部分——全局代理模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a#2022-02-13">https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a#2022-02-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如果你使用的算法是一个黑盒模型，为什么不使用一个模拟原始行为的可解释模型呢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fcc438d1a5c64ea4ab2d449f698aad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NnxhUpLoWwfIz5p9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">免费使用来自<a class="ae ky" href="https://www.pexels.com/ko-kr/photo/7108092/" rel="noopener ugc nofollow" target="_blank">像素</a>的照片</p></figure><h1 id="749b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">对以前职位的审查</h1><p id="5bb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">可解释的机器学习(XAI)指的是努力确保人工智能程序在目的和工作方式上是透明的。[1]这是我打算写的 XAI 系列文章中的第五篇。</p><p id="4023" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">到目前为止，我已经向您介绍了四种 XAI 方法——部分相关图(PDP)、个体条件期望图(ICE)、累积局部效应(ALE)曲线和排列重要性。在这篇文章中，我提供了关于全球代理模型的初级读本。如果你好奇或者需要一些回顾，请查看我之前的四篇帖子！</p><p id="fa47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-1-partial-dependence-plot-pdp-349441901a3d?source=your_stories_page----------------------------------------"> XAI 方法第一部分</a></p><p id="08cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-2-individual-conditional-expectation-ice-curves-8fe76919aab7?source=your_stories_page----------------------------------------"> XAI 方法第二部分</a></p><p id="1e3b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/explainable-ai-xai-methods-part-3-accumulated-local-effects-ale-cf6ba3387fde?source=your_stories_page----------------------------------------"> XAI 方法第三部分</a></p><p id="0c5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/geekculture/explainable-ai-xai-methods-part-4-permutation-feature-importance-72b8a5d9be05" rel="noopener"> XAI 方法第四部分</a></p><h1 id="6c35" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">高级概念</h1><p id="9b81" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">顾名思义,“代理”模型是可以代替原始模型用于各种目的的算法——提高可解释性、降低计算成本等。假设您使用随机森林模型来预测回归结果。虽然它可能具有出色的预测性能，但与线性回归等固有的可解释模型不同，很难理解为什么以及如何做出这些预测。在这种情况下，可以用随机森林模型替换训练起来既便宜又快速的代理模型。</p><p id="133f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的关键是要确保，引用可解释的机器学习书中的话，你使用的<em class="ms">代理模型尽可能准确地逼近底层模型的预测，同时是可解释的。[2] </em></p><h1 id="bda5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">代理模型的类型</h1><p id="6cf4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有两种类型的代理模型——全局的和局部的。全局代理模型用于解释黑盒模型的“整体/全局预测”，而局部代理模型，最好由局部可解释模型不可知解释(LIME)方法表示，用于解释“个体”预测。在这篇文章中，我们只讨论全局代理模型。我的下一篇文章将会谈到酸橙这个话题。</p><h1 id="4eea" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">如何建立全球代理模型——理论</h1><ol class=""><li id="7463" class="mt mu it lt b lu lv lx ly ma mv me mw mi mx mm my mz na nb bi translated">您首先需要一个可用于训练代理模型的数据集。它可以是用于训练原始黑盒模型的完全相同的数据。它也可以是在分布及其特征方面与原始数据相似的新数据。对于这些选定的数据，检索由黑盒模型预测的输出值。</li><li id="1fa5" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">选择可用作全局代理模型的内在可解释算法。示例包括广义加法模型、逻辑回归和树算法(例如，不像线性回归那样透明，但解释起来仍然相对简单)</li><li id="3385" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">根据步骤 1 中选择的数据训练代理模型，并保存生成的预测集。</li><li id="2727" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">检查经过训练的代理模型对黑盒模型所做预测的逼近程度。这很重要，因为如果代理模型不能模仿原始黑盒模型的行为，那么我们从代理模型的解释中获得的信息就不能再解释原始模型了。</li></ol><p id="aabd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们怎么做第四步？</p><p id="3e10" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以使用几个指标来完成第 4 步，但一个常用的指标是 R 平方。还记得回归设置中的 R 平方是由训练模型解释的数据中方差的比例吗？这里的概念是类似的。代理模型上下文中的 R 平方是代理模型捕获的方差的百分比。[2]参见下面引用可解释的机器学习书籍的<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/lime.html" rel="noopener ugc nofollow" target="_blank">第 9.2 章计算 R 平方的精确公式。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/1a460d34c58072173f9261e5dd0150b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lfv_enSN11iFws_P5CYSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/lime.html" rel="noopener ugc nofollow" target="_blank">可解释机器学习书</a>的第 9.2 章</p></figure><h1 id="8c50" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">如何建立全球代理模型——实现</h1><p id="a0f6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">你可以很容易地实现上面的步骤。首先保存来自原始黑盒模型的预测，然后使用它们和训练数据来训练选择的代理模型。从代理模型中检索预测。将原始模型和代理模型的预测值代入上面的 R 平方公式，计算代理模型在预测行为上与原始模型的相似程度。最后，解释一下代理模型！</p><p id="4f79" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，R 和 Python 都有允许我们简化整个过程的包！(眨眼)</p><p id="e952" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于 R 用户来说，<strong class="lt iu"> iml </strong>包就派上用场了。让我们看一个来自官方<a class="ae ky" href="https://cran.r-project.org/web/packages/iml/vignettes/intro.html" rel="noopener ugc nofollow" target="_blank"> R-CRAN 文档</a>的例子。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="207c" class="nn la it nj b gy no np l nq nr">set.seed(42)<br/><strong class="nj iu">library</strong>(“iml”)<br/><strong class="nj iu">library</strong>(“randomForest”) </span><span id="ace8" class="nn la it nj b gy ns np l nq nr"><strong class="nj iu">data</strong>(“Boston”, package = “MASS”) <br/>rf &lt;- randomForest(medv ~ ., data = Boston, ntree = 50)<br/>X &lt;- Boston[which(names(Boston) != "medv")] <br/>predictor &lt;- Predictor$new(rf, data = X, y = Boston$medv)</span><span id="9b3f" class="nn la it nj b gy ns np l nq nr"># fit surrogate decision tree model<br/>tree &lt;- TreeSurrogate$new(predictor, maxdepth = 2)<br/><br/># how well does this model fit the original results<br/>tree$r.squared</span></pre><p id="2d1b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据集是著名的波士顿住房数据。我们想用随机森林来做预测。但这不是很好解释。在这种情况下，我们使用树算法作为代理模型。如上所示，rf 是存储随机森林模型的变量，predictor 是存储随机森林模型所有预测的变量。</p><p id="c69c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">树变量是由 iml 包中的<strong class="lt iu"> TreeSurrogate </strong>函数创建的 iml 对象。这个函数有一个 maxdepth 参数，允许用户控制正在形成的树的最大深度。iml 包的一个缺点是只有树算法可以作为代理模型的选项。最后，iml 对象上的<strong class="lt iu"> r.squared </strong>函数(在本例中是 tree 变量)返回代理模型的 R 平方值。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="d7d9" class="nn la it nj b gy no np l nq nr">plot(tree)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a2067f4cad97d32dac9e386d02e5352c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*eGbWWMcN02LaM1HgP89P5A.png"/></div></figure><p id="67ce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">应用于 iml 对象的绘图函数将返回上面的方框图可视化效果。我们可以看到，对于 lstat &gt; 9.71 &amp; lstata &gt; 16.21，y 平方值特别低。</p><p id="df64" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于 Python 用户，有<strong class="lt iu">解释</strong>包可用。interpret 项目由微软研究团队牵头，旨在为用户提供各种可解释的机器学习(XAI)方法。它不是专门为代理模型开发的，但是它包含了构建代理模型的方法和功能。</p><p id="0ba8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">解释包主要有两个模块流——黑盒模块和玻璃盒模块。“玻璃盒子”可以理解为“白盒”的同义词，指的是透明且可解释的模型。黑盒是黑盒或白盒的反义词。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="775b" class="nn la it nj b gy no np l nq nr">from interpret.ext.blackbox import MimicExplainer<br/>from interpret.ext.glassbox import LinearExplainableModel</span></pre><p id="be5f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">MimicExplainer 是一个代理模型函数，可以包装任何可解释的算法。在这种情况下，我们使用线性模型，因此我们导入 LinearExplainableModel 函数。还有其他可用的选项，包括<em class="ms">decisiontreeexplaineablemodel 和 GBMExplainableModel。</em></p><p id="55ef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整的示例代码，我推荐下面的<a class="ae ky" href="https://santiagof.medium.com/model-interpretability-making-your-model-confesses-surrogate-models-3dbf72bee8e" rel="noopener">博文</a>。</p><h1 id="560a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">优点和需要谨慎的事情！</strong></h1><h2 id="41d9" class="nn la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">优势</h2><p id="add2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">任何可解释的模型都可以用作代理模型，因此它给用户带来了灵活性！此外，根据听 XAI 解释的目标受众，你可以训练多种替代模型，最适合不同的受众群体，帮助他们清楚地理解这些见解。</p><h2 id="195b" class="nn la it bd lb nu nv dn lf nw nx dp lj ma ny nz ll me oa ob ln mi oc od lp oe bi translated">缺点和需要注意的事项</h2><p id="cb12" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在某些情况下，您会遇到这样的情况:您尝试训练的可解释模型中没有一个具有足够高的 R 平方值，从而能够做出代理模型近似于原始黑盒模型的预测的声明。此外，R 平方值的可接受阈值是多少？是 50%以上的吗？至少应该是 80%吧？到目前为止，还没有一种独立的方法可以让我们衡量代理模型是否与原始模型“足够相似”。</p><p id="8d38" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一件要记住的事情是，代理模型只提供关于原始模型的解释，而不是感兴趣的一般数据。如果原始模型是垃圾，几乎没有任何概括的预测能力，那么我们从替代模型中得到的解释就没有价值。</p><p id="d1c9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我的下一篇文章将讨论局部代理模型，即可解释的模型不可知解释(LIME)。敬请期待！</p><h1 id="08f4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="c117" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] <a class="ae ky" href="https://www.techopedia.com/definition/33240/explainable-artificial-intelligence-xai" rel="noopener ugc nofollow" target="_blank">可解释的人工智能(XAI) </a> (2019)，Technopedia</p><p id="9455" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] C .莫尔纳尔<em class="ms">，</em> <a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/global.html" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> </em>可解释机器学习</a> (2020)</p><p id="e979" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3] F. <a class="ae ky" href="https://santiagof.medium.com/?source=post_page-----3dbf72bee8e-----------------------------------" rel="noopener">圣地亚哥</a>，模型可解释性——让你的模型坦白:代理模型(2020)，中等</p><h1 id="f523" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于作者</h1><p id="0957" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Seungjun (Josh) Kim 是一名数据科学家，在密歇根大学刑事司法行政记录系统(CJARS)实验室担任副研究员。他计划从今年秋天开始攻读统计学或信息方面的博士学位(等待结果)。在空闲时间，他喜欢做礼拜、读圣经、看韩剧、健身、制作和播放音乐，并尝试新的亚洲美食食谱。</p><p id="56a2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://seungjun-data-science.github.io/" rel="noopener ugc nofollow" target="_blank">他的个人网站</a></p></div></div>    
</body>
</html>