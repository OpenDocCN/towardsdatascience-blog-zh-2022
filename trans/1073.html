<html>
<head>
<title>Common Machine Learning and Deep Learning Methods for Clinical Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于临床文本分类的常见机器学习和深度学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/common-machine-learning-and-deep-learning-methods-for-clinical-text-classification-188473477a32#2022-03-21">https://towardsdatascience.com/common-machine-learning-and-deep-learning-methods-for-clinical-text-classification-188473477a32#2022-03-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6633" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">面向医疗保健的自然语言处理(NLP ),使用 NLTK、spaCy、TF-IDF、单词嵌入以及非序列和序列建模，如 LSTM 和 Transformer</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/22bb95fae69c5df91c132657f81938b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_pYdTaKt2QgmO19aeOd-w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者照片</p></figure><p id="7e48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着机器学习和深度学习算法的进步，自然语言处理(<a class="ae lr" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>)成为人工智能研究和应用的热门话题，因为文本(例如，英语句子)是自然语言数据的主要类型。例如，文本经常出现在医疗保健数据集中，如电子健康记录(<a class="ae lr" href="https://en.wikipedia.org/wiki/Electronic_health_record" rel="noopener ugc nofollow" target="_blank"> EHR </a>)。医疗领域的文本数据有很多人工智能方法，如临床命名实体识别(<a class="ae lr" href="https://en.wikipedia.org/wiki/Named-entity_recognition" rel="noopener ugc nofollow" target="_blank"/>)、临床文本分类等。</p><p id="88f2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我使用开源临床文本数据集[1][7][8]来介绍一些常见的机器学习和深度学习方法，用于临床文本分类。</p><p id="717d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与<a class="ae lr" href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining" rel="noopener ugc nofollow" target="_blank"> CRISP-DM </a>类似，NLP 的机器学习过程包括以下常见步骤:</p><ul class=""><li id="c5ed" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">商业理解</li><li id="34dd" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">数据理解</li><li id="5c05" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">数据准备</li><li id="883b" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">建模</li><li id="5a2d" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">模型评估</li><li id="5219" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">模型部署</li></ul><h1 id="d095" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">1.商业理解</h1><p id="0a27" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">对于任何一个数据科学项目来说，了解要解决的问题是什么是非常重要的。</p><p id="bd07" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文要解决的问题是如何从医学文本中预测医学专业。</p><h1 id="037a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">2.数据理解</h1><p id="0a24" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">为了解决确定的问题，我们需要收集和理解数据。</p><p id="ff6d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，我使用来自 Kaggle 的开源临床文本数据集[1]。下载数据集<em class="nd"> mtsamples.csv </em>文件后，以下代码将数据集作为 Pandas DataFrame 加载到内存中:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="5109" class="nj mh iq nf b gy nk nl l nm nn">data = pd.read_csv('./medical-nlp/data/mtsamples.csv', index_col=0)<br/>data.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/2bfd3595045e70f338ece763b6ed2dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CiLMqjeqkeEFQyLeXiP5SA.png"/></div></div></figure><p id="e452" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如上表所示，特征列<em class="nd">转录</em>为英文句子，而目标/标签列<em class="nd">医疗 _ 专业</em>为分类。该信息表明要解决的问题是临床文本分类问题。</p><p id="0535" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有 40 个独特的医学专业类别。数据集严重失衡。不同类别的数据样本数量从 6 到 1，103 不等。</p><h1 id="f891" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">3.数据准备</h1><p id="bcb0" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如前所述，数据集非常不平衡。</p><h2 id="1043" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">3.1 不平衡数据的处理</h2><p id="b973" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">为了缓解数据不平衡问题，下面的代码删除了数据样本少于 100 或者不是医学专业的类别。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="6872" class="nj mh iq nf b gy nk nl l nm nn">filtered_data = data[['transcription', 'medical_specialty']]<br/>filtered_data.loc[:, 'medical_specialty'] = filtered_data['medical_specialty'].apply(lambda x:str.strip(x))<br/>mask = (filtered_data['medical_specialty'] == 'SOAP / Chart / Progress Notes') | \<br/>       (filtered_data['medical_specialty'] == 'Office Notes') | \<br/>       (filtered_data['medical_specialty'] == 'Consult - History and Phy.') | \<br/>       (filtered_data['medical_specialty'] == 'Emergency Room Reports') | \<br/>       (filtered_data['medical_specialty'] == 'Discharge Summary') | \<br/>       (filtered_data['medical_specialty'] == 'Letters')</span><span id="b35f" class="nj mh iq nf b gy oa nl l nm nn">filtered_data = filtered_data[~mask]</span><span id="8815" class="nj mh iq nf b gy oa nl l nm nn">data_categories  = filtered_data.groupby(filtered_data['medical_specialty'])<br/>filtered_data_categories = data_categories.filter(lambda x:x.shape[0] &gt; 100)<br/>filtered_data_categories['medical_specialty'].value_counts()</span></pre><p id="f3af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">生成的数据集包含以下 9 类医学专业:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d0856560bfde0fd13860d44bfeb35b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*ahdxiKYEU1MaqX4ZGj4Aeg.png"/></div></figure><p id="b4be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据集按照如下方式进行混洗，以避免任何人为的数据模式。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="5396" class="nj mh iq nf b gy nk nl l nm nn">data = filtered_data_categories.sample(frac=1.0)</span></pre><p id="2d12" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，与[1]类似，<a class="ae lr" href="https://pypi.org/project/imbalanced-learn/" rel="noopener ugc nofollow" target="_blank"> SMOTE </a>算法也用于自动上采样少数数据样本，以改善文本编码后的数据平衡。</p><h2 id="18ca" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">3.2 数据预处理</h2><p id="f2e8" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">正常数据集和 NLP 数据集之间的主要区别在于，NLP 数据集包含文本特征，例如医疗转录。文本特征由一系列句子组成(在 NLP 术语中称为文本或文档语料库)。就功能类型而言，区别如下:</p><p id="d48d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">普通数据集中有两种类型的要素:</p><ul class=""><li id="cfe7" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">数字特征(如温度)</li><li id="5f2b" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">分类特征(例如，诸如“IL”、“WA”等状态。)</li></ul><p id="a3b9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，NLP 数据集中有三种类型的要素:</p><ul class=""><li id="05b2" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">数字特征(如温度)</li><li id="7d26" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">分类特征(如特征<em class="nd">医疗 _ 专业</em>，如上表所示)</li><li id="966a" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">文本特征(例如上表中的特征<em class="nd">转录</em></li></ul><p id="54e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了处理缺失数据，并减少词汇和训练数据的大小，除了提取医疗实体之外，以下数据预处理步骤通常应用于文本特征(例如，<em class="nd">转录</em>特征)以进行文本分类:</p><ul class=""><li id="856f" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">处理丢失的数据</li><li id="9f79" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">提取医疗实体(仅用于临床文本分类)</li><li id="13d8" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">标准化文本</li><li id="1a1e" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">标记文本</li><li id="c54a" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">删除停用词</li><li id="74a2" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">堵塞物</li><li id="1532" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">把…按屈折变化形式进行归类</li></ul><h2 id="3c9d" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">处理缺失数据</h2><p id="a72a" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">机器学习不允许缺失数据。如果丢失条目的数量相对较少，可以简单地丢弃丢失的数据。NLP 中处理缺失数据的另一种常用方法是从数据集中的其他相关要素复制信息。例如，NLP 特征“转录”中缺少的条目可以通过从特征“描述”中复制相应的条目来填充，因为描述是转录的概要。</p><p id="8d97" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，feature <em class="nd"> transcription </em>列中的缺失条目被简单地删除，而不会显著损害模型预测的准确性，因为只有 33 个缺失条目。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="80a1" class="nj mh iq nf b gy nk nl l nm nn">df = data.dropna(subset=['transcription'])</span></pre><h2 id="7b8b" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">3.2.2 提取医疗实体</h2><p id="7720" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">特征栏<em class="nd">转录</em>中最重要的内容是医学笔记。如[1]所述，提取医疗实体用于临床文本分类被证明是非常有用的。这是通过使用 spaCy [3]库实现的，如下所示:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ab49" class="nj mh iq nf b gy nk nl l nm nn">import spacy<br/>import en_ner_bionlp13cg_md</span><span id="f122" class="nj mh iq nf b gy oa nl l nm nn">nlp = en_ner_bionlp13cg_md.load()</span><span id="608a" class="nj mh iq nf b gy oa nl l nm nn">def medical_entities( text):<br/>    entities = []<br/>    doc = nlp(text)<br/>    for ent in doc.ents:<br/>        entities.append(ent.text)<br/>    return ' '.join(entities)</span><span id="b3fc" class="nj mh iq nf b gy oa nl l nm nn">df['transcription'] = df['transcription'].apply(medical_entities)</span></pre><h2 id="dbad" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">标准化文本</h2><p id="3b4c" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">标准化文本有两个常见步骤:</p><ul class=""><li id="b828" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">将文本改为小写:这有助于避免机器学习模型学习字符的大小写。</li><li id="4cda" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">去掉标点符号:这有助于减少词汇量</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/6cc51fcffaf3a49ef15c6615735978fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFSKaMPUE_By4_OcnD-HDw.png"/></div></div></figure><p id="beeb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，给定上面的文本字符串，标准化文本的两个步骤可以在一条语句中完成，如下所示:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="30b4" class="nj mh iq nf b gy nk nl l nm nn">text = text.lower().translate(str.maketrans('', '', string.punctuation))</span></pre><p id="15d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是生成的文本:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/543c04ef600c237f01e7c756101f78fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-jqZTTIi-t9p0VtfWT4zw.png"/></div></div></figure><h2 id="afa1" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">3.2.4 标记文本</h2><p id="20e4" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">这有助于把一篇文章分成词汇的各个部分。如果文本被视为单词序列，则通常使用单词级(1-gram)标记化。单词级标记化还支持删除停用词、词干和词汇化的活动。</p><p id="3525" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">N 元语法(通常 N = 2 或 3)标记化通常用于将文本视为一个单词包(单词的原始有序序列丢失)的情况。</p><p id="6f7b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，下面的代码使用 scikit-learn 库将文本字符串标记为单个单词/标记。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="f38b" class="nj mh iq nf b gy nk nl l nm nn">from nltk.tokenize import word_tokenize<br/>tokens = word_tokenize(text)</span></pre><h2 id="847f" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">移除停用字词</h2><p id="93e7" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">这有助于在不损害模型预测准确性的情况下减少词汇表的大小。以下代码使用 NLTK 库从标记化单词中移除英语停用词。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="0917" class="nj mh iq nf b gy nk nl l nm nn">from nltk.corpus import stopwords<br/>english_stopwords = set(stopwords.words('english'))</span><span id="98b6" class="nj mh iq nf b gy oa nl l nm nn">clean_tokens = []<br/>for tok in tokens:<br/>    tok = tok.strip() # remove space<br/>    if tok not in english_stopwords:<br/>        clean_tokens.append(tok)</span></pre><h2 id="dbb1" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">堵塞</h2><p id="26fb" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">这有助于减少词汇量。以下代码使用 NLTK [4]库对单词/标记进行词干分析。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="5a39" class="nj mh iq nf b gy nk nl l nm nn">from nltk.stem import PorterStemmer<br/>stemmer = PorterStemmer()<br/>stemming_word = stemmer.stem(‘being’) </span></pre><h2 id="6197" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">词汇化</h2><p id="ae09" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">这有助于减少词汇量。下面的代码使用 NLTK 库对单词/令牌进行词汇化。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="80f1" class="nj mh iq nf b gy nk nl l nm nn">from nltk.stem import WordNetLemmatizer<br/>lemmatizer = WordNetLemmatizer()<br/>root_word = lemmatizer.lemmatize('words')</span></pre><p id="2bec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述数据预处理步骤可以合并成一个函数来清理文本字符串:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="a463" class="nj mh iq nf b gy nk nl l nm nn">def clean_text(text):<br/>    # get English stopwords<br/>    english_stopwords = set(stopwords.words('english'))<br/>    <br/>    # change to lower case and remove punctuation<br/>    text = text.lower().translate(str.maketrans('', '', string.punctuation))<br/>    <br/>    # divide string into individual words<br/>    tokens = word_tokenize(text)<br/>    <br/>    stemmer = PorterStemmer()<br/>    lemmatizer = WordNetLemmatizer()</span><span id="ef6a" class="nj mh iq nf b gy oa nl l nm nn">    clean_tokens = []<br/>    for tok in tokens:<br/>        tok = tok.strip() # remove space<br/>        if tok not in english_stopwords:<br/>            clean_tok = lemmatizer.lemmatize(tok) # lemmatizition<br/>            clean_tok = stemmer.stem(clean_tok) # Stemming<br/>            clean_tokens.append(clean_tok)</span><span id="3c3e" class="nj mh iq nf b gy oa nl l nm nn">     return " ".join(clean_tokens)</span></pre><p id="4542" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是通过调用 clean_text()函数生成的干净文本示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/075ab795e55216203a61ee6a126b928e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQSU72roRaF7jV45yCYc6w.png"/></div></div></figure><p id="b654" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，步骤 3.2.3 和 3.2.5 通常是有用的，但对于本文中的临床文本分类不是必需的，因为这两个步骤已经在步骤 3.2.2 中完成了。</p><h2 id="1534" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">3.3.特征工程</h2><p id="9104" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">一旦对文本特征进行预处理，就需要将每个文本(文档)编码为数字向量(称为矢量化)，因为机器学习和深度学习模型只理解数字。编码方法的选择取决于如何解释文本。在文本分类中，文本串(文档)通常被视为一个单词包或一个单词序列。以下是一些常见的文本编码/矢量化方法。</p><p id="6933" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果一个文本被视为一个单词包，那么我们通常使用下面的方法将一个文本/文档转换为一个数字向量:</p><ul class=""><li id="cc27" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>矢量化</li><li id="2b59" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">多重热编码</li></ul><h2 id="91c3" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">TF-IDF 矢量化</h2><p id="a6c1" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">Multi-Hot [2]编码方法将一个文本/文档编码为具有 0 和 1 的值的词汇长度的稀疏向量，其中 1 指示词汇中相应单词/标记的存在。</p><p id="8eaf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与 Multi-Hot 类似，TF-IDF 矢量化也将一个文本/文档编码为词汇长度的稀疏向量。主要区别在于，TF-IDF 编码向量不仅包含词汇表中相应单词/标记的存在信息，还包含由反向文档频率(IDF)加权的标记频率(TF)。因此，TF-IDF 编码的矢量比 multi-Hot 编码的矢量具有更大的预测能力。</p><p id="092c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">像 Multi-Hot 一样，TF-IDF 编码向量的长度可以与整个词汇表的长度相同。典型的截断长度是 20，000。类似于[1]，应用<a class="ae lr" href="https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Principal%20component%20analysis%20(PCA)%20is,components%20and%20ignoring%20the%20rest." rel="noopener ugc nofollow" target="_blank"> PCA </a>算法来降低维数。</p><p id="68d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下函数将分类标签转换为整数，并使用 3-gram TF-IDF 和 PCA 将<em class="nd">转录</em>特征编码为稀疏向量:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="76d9" class="nj mh iq nf b gy nk nl l nm nn">from sklearn.preprocessing import LabelEncoder<br/>from sklearn.decomposition import PCA<br/>from imblearn.over_sampling import SMOTE</span><span id="867b" class="nj mh iq nf b gy oa nl l nm nn">def tfidf_pca(X_feature, y_label):<br/>    '''<br/>    X_feature = df['transcription'].values<br/>    y_label = df['medical_specialty'].values<br/>    '''<br/>    <br/>    le = LabelEncoder()<br/>    y_int = le.fit_transform(y_label)</span><span id="a8c7" class="nj mh iq nf b gy oa nl l nm nn">    tfidf = TfidfVectorizer(tokenizer=clean_text, ngram_range=(1,3))<br/>    pca = PCA(n_components=0.95)<br/>    X_tfidf = tfidf.fit_transform(X_feature)<br/>    X_pca = pca.fit_transform(X_tfidf.toarray())</span><span id="5aca" class="nj mh iq nf b gy oa nl l nm nn">    smote_over_sample = SMOTE(sampling_strategy='minority')</span><span id="896e" class="nj mh iq nf b gy oa nl l nm nn">    X, y = smote_over_sample.fit_resample(X_pca, y_int.tolist())</span><span id="e7ba" class="nj mh iq nf b gy oa nl l nm nn">    X = np.array(X)<br/>    y = np.array(y)<br/>    <br/>    return X, y</span></pre><h2 id="ff1d" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">单词嵌入</h2><p id="5ab4" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如果一个文本被视为一个单词序列，那么我们可以使用以下常用方法将文本/文档中的每个单词转换为一个向量，并将整个文本/文档转换为一个向量数组:</p><ul class=""><li id="2b41" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">一键式</a>编码:将每个单词编码成一个词汇长度的向量(一般是几万或几十万)</li><li id="c8d6" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Word_embedding#:~:text=In%20natural%20language%20processing%20(NLP,to%20be%20similar%20in%20meaning." rel="noopener ugc nofollow" target="_blank">单词嵌入</a>:将每个单词编码为一个更短长度的密集向量(例如 100、250、500)</li></ul><p id="fd2a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了更短的长度之外，单词嵌入相对于一键编码的另一大优势是单词嵌入以这样一种方式编码单词的含义，使得在向量空间中更接近的单词在含义上是相似的。</p><p id="2fe6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">给定一个文本/文档，有两个步骤将其映射到给定大小的密集向量:</p><ul class=""><li id="f964" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">步骤 1:将文本转换成词汇表中单词/标记索引的向量</li><li id="b3d4" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">步骤 2:使用定制的训练或预训练的单词嵌入算法(例如，Word2Vec 和 GloVe)将每个单词索引转换成浮点数的密集向量</li></ul><p id="a036" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的函数将分类标签转换成整数，将<em class="nd">转录</em>特性转换成词汇表中单词/标记索引的向量。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="3d7d" class="nj mh iq nf b gy nk nl l nm nn">from tensorflow.keras import layers</span><span id="0e11" class="nj mh iq nf b gy oa nl l nm nn">def indecies_vectorize(X_feature, y_label, max_length = 1500, max_tokens = MAX_TOKENS):<br/>    '''<br/>    X_feature: df['transcription'].values<br/>    max_length: maximum number of words in one text/document<br/>    max_tokens: the length of vocabulary<br/>    '''<br/>    <br/>    le = LabelEncoder()<br/>    y_int = le.fit_transform(y_label)<br/>    <br/>    text_vectorization = layers.TextVectorization(<br/>        max_tokens=max_tokens,<br/>        output_mode="int",<br/>        output_sequence_length=max_length,<br/>    )</span><span id="f13e" class="nj mh iq nf b gy oa nl l nm nn">    # training the text vectorization object<br/>    text_vectorization.adapt(X_feature) </span><span id="03a6" class="nj mh iq nf b gy oa nl l nm nn">    # convert preprocessed training text into anrray of integers/indecies<br/>    indecies_array = text_vectorization(X_feature) <br/>    <br/>    smote_over_sample = SMOTE(sampling_strategy='minority')<br/>    X, y = smote_over_sample.fit_resample(indecies_array, y_int)<br/>    <br/>    X = np.array(X)<br/>    y = np.array(y)</span><span id="1590" class="nj mh iq nf b gy oa nl l nm nn">    return X, y</span></pre><p id="79ea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面的代码显示了如何使用 Keras 的自定义训练单词嵌入层将单词索引矢量编码为长度为 256 的密集矢量序列。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ed31" class="nj mh iq nf b gy nk nl l nm nn">import keras<br/>inputs = keras.Input(shape=(None,), dtype="int64")<br/>embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)</span></pre><h2 id="ab77" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">位置嵌入</h2><p id="0576" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如[2]所述，词序信息在文本分类中起着重要作用。[2]中的以下代码是将词序信息添加到单词嵌入中，以提高使用<a class="ae lr" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank">Transformer</a>【2】进行文本分类的性能。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="1c26" class="nj mh iq nf b gy nk nl l nm nn">class PositionalEmbedding(layers.Layer):<br/>    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):<br/>        super().__init__(**kwargs)<br/>        self.token_embeddings = layers.Embedding(<br/>            input_dim=input_dim, output_dim=output_dim)<br/>        <br/>        self.position_embeddings = layers.Embedding(<br/>            input_dim=sequence_length, output_dim=output_dim)<br/>        <br/>        self.sequence_length = sequence_length<br/>        self.input_dim = input_dim<br/>        self.output_dim = output_dim</span><span id="f211" class="nj mh iq nf b gy oa nl l nm nn">    def call(self, inputs):<br/>        length = tf.shape(inputs)[-1]<br/>        positions = tf.range(start=0, limit=length, delta=1)<br/>        embedded_tokens = self.token_embeddings(inputs)<br/>        embedded_positions = self.position_embeddings(positions)<br/>        return embedded_tokens + embedded_positions</span><span id="d9c3" class="nj mh iq nf b gy oa nl l nm nn">    def compute_mask(self, inputs, mask=None):<br/>        return tf.math.not_equal(inputs, 0)</span><span id="64b0" class="nj mh iq nf b gy oa nl l nm nn">    def get_config(self):<br/>        config = super().get_config()<br/>        config.update({<br/>            "output_dim": self.output_dim,<br/>            "sequence_length": self.sequence_length,<br/>            "input_dim": self.input_dim,<br/>        })<br/>        return config</span></pre><p id="16fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下表总结了不同文本编码方法的选择。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3b8a4565ec8eb787aaf2dac3dbf334ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*FX3dGqXoSgITlQkc6Zb1zg.png"/></div></figure><h1 id="2cbf" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">4.建模</h1><p id="04ea" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">一旦数据集为建模做好准备，我们需要选择一个机器学习/深度学习模型，并用准备好的数据对其进行训练。</p><h2 id="95db" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">4.1 型号选择</h2><p id="cd12" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">与选择文本编码方法类似，选择用于文本分类的机器学习或深度学习模型取决于如何对文本进行编码。具体来说，我们可以选择下表所示的型号:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/967011ca0d80ef763bf49a72ddc52904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*62TgULwJ28HuWEwKgX1EcQ.png"/></div></div></figure><p id="1e86" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面来自[2]的 TransformerEncoder 类实现了一个多头基于注意力的转换器，用于本文中的临床文本分类。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="e777" class="nj mh iq nf b gy nk nl l nm nn">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras import layers</span><span id="7a38" class="nj mh iq nf b gy oa nl l nm nn">class TransformerEncoder(layers.Layer):<br/>    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):<br/>        super().__init__(**kwargs)<br/>        self.embed_dim = embed_dim<br/>        self.dense_dim = dense_dim<br/>        self.num_heads = num_heads<br/>        self.attention = layers.MultiHeadAttention(<br/>            num_heads=num_heads, key_dim=embed_dim)<br/>        self.dense_proj = keras.Sequential(<br/>            [layers.Dense(dense_dim, activation="relu"),<br/>             layers.Dense(embed_dim),]<br/>        )<br/>        self.layernorm_1 = layers.LayerNormalization()<br/>        self.layernorm_2 = layers.LayerNormalization()</span><span id="a2c9" class="nj mh iq nf b gy oa nl l nm nn">   def call(self, inputs, mask=None):<br/>        if mask is not None:<br/>            mask = mask[:, tf.newaxis, :]<br/>        attention_output = self.attention(<br/>            inputs, inputs, attention_mask=mask)<br/>        proj_input = self.layernorm_1(inputs + attention_output)<br/>        proj_output = self.dense_proj(proj_input)<br/>        return self.layernorm_2(proj_input + proj_output)</span><span id="d87f" class="nj mh iq nf b gy oa nl l nm nn">   def get_config(self):<br/>        config = super().get_config()<br/>        config.update({<br/>            "embed_dim": self.embed_dim,<br/>            "num_heads": self.num_heads,<br/>            "dense_dim": self.dense_dim,<br/>        })<br/>        return config</span></pre><p id="5a22" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文选择了以下模型:</p><p id="ae49" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">TF-IDF 编码的非序列模型:</p><ul class=""><li id="8121" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">逻辑回归</li><li id="93fe" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">随机森林</li><li id="d4f8" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html" rel="noopener ugc nofollow" target="_blank">多输出分类器</a>与<a class="ae lr" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> k-NN </a></li></ul><p id="ffb1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">单词嵌入的序列模型:</p><ul class=""><li id="97da" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">LSTM</li></ul><p id="1f56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">位置嵌入的序列模型:</p><ul class=""><li id="4581" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">Transformer(即 TransformerEncoder 类)</li></ul><h2 id="94f0" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">4.2.模特培训</h2><p id="a539" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">通常，在模型训练开始之前，我们需要将数据集分成训练、验证和测试子集。为了简单起见，在本文中数据集只分成两个子集:一个用于模型训练，另一个用于模型测试/评估。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="3dd5" class="nj mh iq nf b gy nk nl l nm nn">from sklearn.model_selection import train_test_split</span><span id="d328" class="nj mh iq nf b gy oa nl l nm nn">X_feature = df['transcription'].values<br/>y_label = df['medical_specialty'].values</span><span id="c12d" class="nj mh iq nf b gy oa nl l nm nn">if SEQUENCE_MODEL:<br/>    X, y = indecies_vectorize(X_feature, y_label)<br/>else:   <br/>    X, y = tfidf_pca(X_feature, y_label)</span><span id="fc30" class="nj mh iq nf b gy oa nl l nm nn">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)<br/>X_train.shape, X_test.shape, y_train.shape, y_test.shape</span></pre><h2 id="4f29" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">4.2.1 使用 TF-IDF 的非序列建模</h2><p id="af3f" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如前所述，本文中使用了以下 TF-IDF 编码的非序列模型:</p><ul class=""><li id="ef03" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">逻辑回归</li><li id="6828" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">随机森林</li><li id="6afb" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html" rel="noopener ugc nofollow" target="_blank">多输出分类器</a>与<a class="ae lr" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> k-NN </a></li></ul><p id="7de0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，下面的代码创建并训练一个多输出分类器。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="8604" class="nj mh iq nf b gy nk nl l nm nn">from sklearn.multioutput import MultiOutputClassifier<br/>from sklearn.neighbors import KNeighborsClassifier</span><span id="792f" class="nj mh iq nf b gy oa nl l nm nn">if not SEQUENCE_MODEL:<br/>    multi_class_clf = MultiOutputClassifier(KNeighborsClassifier())<br/>    y_train = y_train.reshape(-1, 1)<br/>    multi_class_clf.fit(X_train, y_train)</span></pre><h2 id="9056" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">4.2.2 使用具有单词嵌入的 LSTM 的序列建模</h2><p id="2faf" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">在本文中，我使用带有自定义单词嵌入的双向 LSTM 来演示如何为文本分类执行序列建模(参见下面的代码)。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="e15f" class="nj mh iq nf b gy nk nl l nm nn">import keras</span><span id="1466" class="nj mh iq nf b gy oa nl l nm nn">def create_lstm_model(embedding_layer = None):<br/>    # create input layer<br/>    inputs = keras.Input(shape=(None,), dtype="int64")</span><span id="97be" class="nj mh iq nf b gy oa nl l nm nn">    # add word embedding layer<br/>    if embedding_layer is not None:<br/>        embedded = embedding_layer(inputs)<br/>    else:<br/>        embedded = layers.Embedding(input_dim=MAX_TOKENS, output_dim=256, mask_zero=True)(inputs)</span><span id="7327" class="nj mh iq nf b gy oa nl l nm nn">    # add LSTM layer<br/>    x = layers.Bidirectional(layers.LSTM(32))(embedded)</span><span id="c049" class="nj mh iq nf b gy oa nl l nm nn">    # add dropout layer<br/>    x = layers.Dropout(0.5)(x)</span><span id="e17c" class="nj mh iq nf b gy oa nl l nm nn">    # add output layer<br/>    outputs = layers.Dense(9, activation="softmax")(x)</span><span id="80a7" class="nj mh iq nf b gy oa nl l nm nn">    # combine all layers into one model<br/>    lstm_model = keras.Model(inputs, outputs)</span><span id="8fa5" class="nj mh iq nf b gy oa nl l nm nn">    # specifiy optimizer, loss, and metrics for the model<br/>    lstm_model.compile(optimizer="rmsprop",<br/>                  loss="sparse_categorical_crossentropy",<br/>                  metrics=["accuracy"])</span><span id="29cc" class="nj mh iq nf b gy oa nl l nm nn">    # print the summay of the model architecture<br/>    lstm_model.summary()<br/>    <br/>    return lstm_model</span><span id="cd9a" class="nj mh iq nf b gy oa nl l nm nn">if SEQUENCE_MODEL:<br/>    if USE_GROVE:<br/>        lstm_model = create_lstm_model(embedding_layer)<br/>    else:<br/>        lstm_model = create_lstm_model()<br/>    <br/>    # define callback function<br/>    callbacks = [<br/>        keras.callbacks.ModelCheckpoint("embeddings_bidir_gru_with_masking.keras",<br/>                                        save_best_only=False)<br/>    ]</span><span id="3951" class="nj mh iq nf b gy oa nl l nm nn">    # train model<br/>    lstm_model.fit(X_train, y_train, epochs=10, callbacks=callbacks)</span></pre><h2 id="d8ae" class="nj mh iq bd mi np nq dn mm nr ns dp mq le nt nu ms li nv nw mu lm nx ny mw nz bi translated">4.2.3 使用变压器的序列建模</h2><p id="ee09" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">最近，基于注意力的变换算法成为序列建模中优于 LSTM 的算法。下面的代码使用具有位置嵌入的多头注意力转移算法进行临床文本分类。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="3947" class="nj mh iq nf b gy nk nl l nm nn">def create_transformer():</span><span id="b1b2" class="nj mh iq nf b gy oa nl l nm nn">    vocab_size = 20000<br/>    sequence_length = 1500<br/>    embed_dim = 256<br/>    num_heads = 2<br/>    dense_dim = 32</span><span id="8b42" class="nj mh iq nf b gy oa nl l nm nn">    inputs = keras.Input(shape=(None,), dtype="int64")<br/>    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)<br/>    x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)<br/>    x = layers.GlobalMaxPooling1D()(x)<br/>    x = layers.Dropout(0.5)(x)<br/>    outputs = layers.Dense(9, activation="softmax")(x)<br/>    transformer_model = keras.Model(inputs, outputs)<br/>    transformer_model.compile(optimizer="rmsprop",<br/>                  loss="sparse_categorical_crossentropy",<br/>                  metrics=["accuracy"])<br/>    transformer_model.summary()<br/>    <br/>    return transformer_model</span><span id="e00b" class="nj mh iq nf b gy oa nl l nm nn">if SEQUENCE_MODEL:<br/>    transformer_model = create_transformer()</span><span id="3ddb" class="nj mh iq nf b gy oa nl l nm nn">    callbacks = [<br/>        keras.callbacks.ModelCheckpoint("full_transformer_encoder.keras",<br/>                                        save_best_only=False)<br/>    ]</span><span id="18f6" class="nj mh iq nf b gy oa nl l nm nn">    transformer_model.fit(X_train, y_train, epochs=10, callbacks=callbacks)</span></pre><h1 id="f14e" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">5.模型评估</h1><p id="79cb" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">评估分类模型的常用指标有:</p><ul class=""><li id="adcd" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">准确性(对于平衡数据)</li><li id="aec6" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">精确度和召回率</li><li id="bf2c" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/F-score" rel="noopener ugc nofollow" target="_blank"> F1 得分</a></li><li id="46a1" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank"> AUC </a>(仅用于二元分类)</li><li id="d1e8" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a></li></ul><p id="1494" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为一个例子，下面的代码计算准确度和 F1 分数，并为本文中的多类临床文本分类创建一个混淆矩阵图。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="f0ca" class="nj mh iq nf b gy nk nl l nm nn">from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay</span><span id="414a" class="nj mh iq nf b gy oa nl l nm nn">def plot_confusion_matrix(y_test, y_pred, labels):<br/>    cm = confusion_matrix(y_test, y_pred, labels=rf_clf.classes_)<br/>    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_clf.classes_) <br/>    fig, ax = plt.subplots(figsize=(15,15))<br/>    disp.plot(ax=ax)</span><span id="98a4" class="nj mh iq nf b gy oa nl l nm nn">if not SEQUENCE_MODEL:<br/>    y_pred = logistic_clf.predict(X_test)</span><span id="cd19" class="nj mh iq nf b gy oa nl l nm nn">    acc_score = accuracy_score(y_test, y_pred)<br/>    print('logistic_clf Accuracy score: ', acc_score)</span><span id="4083" class="nj mh iq nf b gy oa nl l nm nn">    f1 = f1_score(y_test, y_pred, average='weighted')<br/>    print('logistic_clf F1 score: ', f1)<br/>    <br/>    plot_confusion_matrix(y_test, y_pred,   labels=logistic_clf.classes_)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/8ffd5ae8e8e143cce18fecc633552d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYnMAvPTXJ7TE5S9E1ZygQ.png"/></div></div></figure><p id="b2de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下表总结了模型评估结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/228d33ad280e7b650cc15514ca8451a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*KFpAGBnRiKuamxNAt3C5Ig.png"/></div></figure><h1 id="512b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">6.模型部署</h1><p id="e31b" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">一旦在部署的模型评估中确定了文本分类的最佳模型，下一步就是将确定的模型部署到生产系统中。一种常见的方法是将模型作为 Web 服务部署在服务器上，目标生产系统中的其他组件可以调用它来获得文本分类结果。</p><p id="433b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为此，选定的训练模型(如逻辑回归)和相关的编码对象都应保存到文件中(如 Python pickle 文件)。然后，可以将这些保存的编码对象和模型加载回内存，以便在部署环境中进行预测。</p><h1 id="7a87" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">7.结论</h1><p id="ddf6" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">在本文中，我使用了一个开源的临床文本数据集[1][7][8]来演示一些常见的文本/文档编码方法以及相关的机器学习和深度学习算法，用于临床文本分类。</p><p id="f48b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有许多不同类型的文本编码方法和机器学习/深度学习算法可用于文本分类。本文提供了不同文本编码方法和相应的适用机器学习/深度学习模型之间的映射的总结。</p><p id="92fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除了使用自定义数据集训练单词嵌入，我还使用预训练的单词嵌入手套[5]进行了实验，并注意到使用自定义数据训练单词嵌入在准确性方面具有更好的模型性能。</p><p id="b5d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">模型评估结果表明，在临床文本分类中，使用 spaCy 提取医学实体并使用 SMOTE 算法平衡数据对于提高模型的准确性非常重要。</p><h1 id="a095" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><p id="2292" class="pw-post-body-paragraph kv kw iq kx b ky my jr la lb mz ju ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">[1]临床文本分类:<a class="ae lr" href="https://www.kaggle.com/ritheshsreenivasan/clinical-text-classification" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ritheshreenivasan/Clinical-Text-class ification</a></p><p id="afb0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] F. Chollet，用 Python 进行深度学习，第二版，Manning，2021</p><p id="ea68" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3]空间:https://allenai.github.io/scispacy/<a class="ae lr" href="https://allenai.github.io/scispacy/" rel="noopener ugc nofollow" target="_blank"/></p><p id="4bdf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]NLTK:【https://www.nltk.org/】T4</p><p id="3d33" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5]手套:<a class="ae lr" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a></p><p id="d203" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6] Y. Huang，<a class="ae lr" href="https://github.com/yuhuang3/machine-learning/tree/master/clinical_text_classification" rel="noopener ugc nofollow" target="_blank">Github 中的 Jupyter 笔记本</a></p><p id="4e4f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[7]MTS 示例:<a class="ae lr" href="https://www.mtsamples.com/" rel="noopener ugc nofollow" target="_blank">https://www.mtsamples.com</a></p><p id="8307" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[8] T .波义耳，<a class="ae lr" href="https://www.kaggle.com/tboyle10/medicaltranscriptions" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/tboyle10/medicaltranscriptions</a></p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><p id="6ecd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">鸣谢:我要感谢 MTSamples 和 Kaggle 提供的数据集。</p></div></div>    
</body>
</html>