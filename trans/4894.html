<html>
<head>
<title>Training a Model to Compute Cross Product</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为计算叉积的模型定型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-model-to-compute-cross-product-8c9390541fc9#2022-10-31">https://towardsdatascience.com/training-a-model-to-compute-cross-product-8c9390541fc9#2022-10-31</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="402f" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">如何使用PyTorch训练模型来计算叉积</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/aa0644e502cefccb7a90d7f09b6249bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*k95d50XE8hrGGzHI"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@mattreamesfilm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马特·里姆斯</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="0353" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">有很多关于使用PyTorch训练神经网络的教程。为此，我们在此提交一篇关于训练一个非神经网络模型的文章，重点是探索PyTorch的“pythonic式”方面。</p><p id="2e83" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里的任务是训练一个模型来计算两个三维向量的叉积。虽然点积和矩阵乘法在深度学习中被广泛使用，但叉积并不常见。下图展示了这个概念。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj lw"><img src="../Images/b8c8169ec7214ca05d98a59706ffb389.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*OJ3H1GNvnXnRimgj.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">来源:<a class="ae kz" href="https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function/x786f2022:vectors-and-matrices/a/cross-products-mvc" rel="noopener ugc nofollow" target="_blank">https://www . khanacademy . org/math/multivariable-calculus/thinking-about-multivariable-function/x786f 2022:vectors-and-matrix/a/cross-products-MVC</a></p></figure><p id="bd71" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我更喜欢用张量形式定义叉积:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lx ly l"/></div></figure><p id="15db" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里ϵ是李维-西维塔张量。我们在这里使用指数符号或简化的爱因斯坦符号，这意味着对于重复的指数求和是隐含的。</p><p id="6cde" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这是我们模型的基础，张量ϵ是模型的一个可训练参数，我们期望它与Levi-Civita张量相匹配。</p><h1 id="7211" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">监督学习</h1><p id="8742" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj mt ll lm ln mu lp lq lr mv lt lu lv in bi translated">首先，我们将使用监督学习方法。我们将运行提供两个随机向量的模型，然后将预测与预期叉积进行比较。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mw ly l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mx"><img src="../Images/59929597932adf406b3df56f466203c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ag2LRj1zqf3BCV_ePqt65w.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">损失与步数的关系</p></figure><pre class="kk kl km kn gu my mz na nb aw nc bi"><span id="2a7e" class="nd ma iu mz b gz ne nf l ng nh">4.906782535840648e-09</span><span id="88bb" class="nd ma iu mz b gz ni nf l ng nh">print(ep)</span><span id="f2f1" class="nd ma iu mz b gz ni nf l ng nh">Parameter containing:<br/>tensor([[[ 1.0955e-05,  2.9630e-05, -3.7728e-05],<br/>         [ 1.4276e-05,  8.8082e-05,  9.9998e-01],<br/>         [-9.9078e-06, -1.0000e+00,  1.8971e-05]],</span><span id="c562" class="nd ma iu mz b gz ni nf l ng nh">        [[ 8.3094e-06,  1.7124e-06, -9.9999e-01],<br/>         [ 2.7287e-05, -1.4629e-05, -1.5764e-05],<br/>         [ 9.9996e-01, -3.1703e-05, -3.2365e-06]],</span><span id="b110" class="nd ma iu mz b gz ni nf l ng nh">        [[ 2.5212e-05,  9.9996e-01,  2.0671e-05],<br/>         [-9.9997e-01, -3.3963e-05,  1.0797e-05],<br/>         [-3.0542e-05,  3.7616e-05, -6.0207e-05]]], requires_grad=True)</span></pre><p id="4425" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们来讨论一下上面的代码:</p><ol class=""><li id="b7dd" class="nj nk iu lc b ld le lg lh lj nl ln nm lr nn lv no np nq nr bi translated">在我们的例子中，模型是一个具有形状<code class="fe ns nt nu mz b">(3,3,3)</code>的参数<code class="fe ns nt nu mz b">ep</code>。它被初始化为随机值，但是我们期望它收敛到Levi-Civita张量。</li><li id="d76c" class="nj nk iu lc b ld nv lg nw lj nx ln ny lr nz lv no np nq nr bi translated">我们不批量处理向量，因此我们只迭代步骤，不迭代时期。这也意味着没有一个张量有批量维度，这当然更容易理解。</li><li id="543e" class="nj nk iu lc b ld nv lg nw lj nx ln ny lr nz lv no np nq nr bi translated">函数<code class="fe ns nt nu mz b">torch.einsum()</code>是我在深度学习中最喜欢的工具之一。我已经提到了索引，或爱因斯坦符号。<code class="fe ns nt nu mz b">einsum</code>代表爱因斯坦和。它象征性地显示了张量乘法是如何工作的，求和是通过哪些索引发生的，以及结果的预期维数是多少。我喜欢它，因为当我处理多维张量时，我不必担心转置或置换，我可以清楚地看到求和是如何完成的。</li><li id="03f6" class="nj nk iu lc b ld nv lg nw lj nx ln ny lr nz lv no np nq nr bi translated">这里的损失是预测值和实际叉积之差。如你所见，训练快速收敛，并且训练的结果张量<code class="fe ns nt nu mz b">ep</code>非常接近Levi-Civita符号。</li></ol><h1 id="60ef" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">使用叉积的属性为模型定型</h1><p id="808c" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj mt ll lm ln mu lp lq lr mv lt lu lv in bi translated">在上一节中，我们使用监督学习来学习Levi-Civita张量和三维向量的叉积。对我来说，这感觉像是欺骗。我们是否可以根据一些一般原则，而不是通过比较预测值和期望值，来推导叉积？</p><p id="c7d4" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们将训练我们的模型来满足这些标准:</p><ol class=""><li id="5514" class="nj nk iu lc b ld le lg lh lj nl ln nm lr nn lv no np nq nr bi translated">叉积运算是反对称的，即</li></ol><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lx ly l"/></div></figure><p id="9dc8" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">2.它是标准化的，也就是</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lx ly l"/></div></figure><p id="f393" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">都等于一。</p><p id="03c7" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">3.叉积与两项正交，即点积为零:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="oa ly l"/></div></figure><p id="677c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这些标准仍然不足以保证我们正确地学习叉积函数，因为我们有可能学习负叉积函数。换句话说，这个模型会产生ϵ或者−ϵ</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mw ly l"/></div></figure><pre class="kk kl km kn gu my mz na nb aw nc bi"><span id="d110" class="nd ma iu mz b gz ne nf l ng nh">1.2659466847253498e-05</span><span id="e989" class="nd ma iu mz b gz ni nf l ng nh">ep</span><span id="165d" class="nd ma iu mz b gz ni nf l ng nh">Parameter containing:<br/>tensor([[[-1.5880e-05,  4.7683e-05,  5.3528e-05],<br/>         [-4.6085e-07,  4.7272e-05, -9.9767e-01],<br/>         [-2.9965e-05,  9.9688e-01, -4.0021e-05]],</span><span id="4b4e" class="nd ma iu mz b gz ni nf l ng nh">        [[-4.0523e-05,  1.7235e-05,  9.9770e-01],<br/>         [-3.0135e-07, -1.1027e-05, -6.7566e-05],<br/>         [-9.9676e-01,  1.5644e-05,  5.9592e-05]],</span><span id="aeaa" class="nd ma iu mz b gz ni nf l ng nh">        [[-7.9570e-06, -9.9768e-01, -1.1900e-05],<br/>         [ 9.9689e-01, -4.3567e-05, -3.2372e-05],<br/>         [-7.1624e-06,  3.8180e-05,  6.8536e-05]]], requires_grad=True)</span></pre><p id="04cc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">代码块几乎完全相同，但损失的计算方式不同。损失的第一部分不利于学习的操作不是反对称的情况。</p><p id="8c13" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了计算损失的第二部分，我们计算单位矢量上的张量运算。对于叉积，我们期望单位向量的叉积长度为1。我们计算三个张量运算<strong class="lc iv"> e1 </strong>和<strong class="lc iv"> e2 </strong>、<strong class="lc iv"> e1 </strong>和<strong class="lc iv"> e3 </strong>、<strong class="lc iv"> e2 </strong>和<strong class="lc iv"> e3 </strong>，并试图使它们的长度为1，因为它应该用于叉积。</p><p id="64ff" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">对于损失的第三部分，我们取预测和向量<strong class="lc iv"> a </strong>和<strong class="lc iv"> b </strong>的点积，并惩罚非零结果。让我们回顾一下结果:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="mw ly l"/></div></figure><pre class="kk kl km kn gu my mz na nb aw nc bi"><span id="3e09" class="nd ma iu mz b gz ne nf l ng nh">tensor([ -5.2354,  88.7160, -26.9144], grad_fn=&lt;ViewBackward0&gt;)<br/>tensor([  5.2500, -89.0000,  27.0000])</span></pre><p id="fb51" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们可以看到，我们确实学习了负叉积。实际上，叉积的符号是约定俗成的，即<strong class="lc iv"> e1 </strong>和<strong class="lc iv"> e2 </strong>的叉积是<strong class="lc iv"> e3 </strong>而不是<strong class="lc iv"> -e3 </strong>。我邀请读者修正损失函数，以便它更喜欢叉积的正确符号。</p><h1 id="f5cc" class="lz ma iu bd mb mc md me mf mg mh mi mj ka mk kb ml kd mm ke mn kg mo kh mp mq bi translated">结论</h1><p id="f80c" class="pw-post-body-paragraph la lb iu lc b ld mr jv lf lg ms jy li lj mt ll lm ln mu lp lq lr mv lt lu lv in bi translated">您可能注意到了，这段代码看起来多么“pythonic化”。我们不需要创建一个计算损耗的函数(尽管这可能是一个好主意)或者一个自定义层(对于更复杂的模型我们应该这样做)。我们甚至不需要创建一个模型类，因为我们唯一需要训练的是一个单一的参数。</p><p id="a632" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">虽然本教程非常简单，但它有很多改进的机会。例如，我们可以考虑三维以上向量空间的叉积，创建一个模块，甚至一个封装功能的层类。这里用到的所有代码都可以在<a class="ae kz" href="https://github.com/mlarionov/cross_product/blob/main/cross_product.ipynb" rel="noopener ugc nofollow" target="_blank"> my github repo </a>上找到。</p></div></div>    
</body>
</html>