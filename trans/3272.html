<html>
<head>
<title>Backdoor Attacks on Language Models: Can We Trust Our Model’s Weights?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对语言模型的后门攻击:我们能信任模型的权重吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f#2022-07-20">https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f#2022-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/298fa2769cb4df6f9dbd7a3e4e5c7f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQG0zw1iZR46_BRa5RoBCA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://pixabay.com/users/pexels-2286921/" rel="noopener ugc nofollow" target="_blank">像素|像素贝</a></p></figure><div class=""/><div class=""><h2 id="4f15" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">NLP中依赖不可靠来源的潜在风险入门</h2></div><h2 id="cbe1" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">问题是</h2><p id="c0d5" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在过去的十年中，预先训练的通用语言模型<strong class="lw jk"> </strong>在自然语言处理(NLP)中变得非常流行，允许任何开发人员/研究人员利用<strong class="lw jk">有限的数据</strong>和<strong class="lw jk">非常少的从头编写和设计神经网络的知识</strong>来构建竞争性模型以解决特定领域的问题<strong class="lw jk"> </strong>。<strong class="lw jk"> </strong>通过<strong class="lw jk"> </strong>简单地下载已经在大型通用领域语料库上训练过的模型的权重，对于大多数开发人员来说，训练在计算上是禁止的，今天我们可以容易地利用<strong class="lw jk">迁移学习</strong>来改进先前在新的目标任务上训练的参数，以部署现实世界的应用。</p><p id="360e" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">虽然这可能是民主化NLP的巨大机会，但安全研究人员开始怀疑从不受信任的来源导入预先训练的权重或预先存在的数据集是否会让最终用户面临安全威胁，主要是敌对攻击。我们最终可以将研究人员的担忧归结为以下问题:</p><blockquote class="ms mt mu"><p id="62b6" class="lu lv mv lw b lx mn kk lz ma mo kn mc mw mp me mf mx mq mh mi my mr mk ml mm im bi translated"><strong class="lw jk">是否有可能通过调整权重来影响微调后的NLP模型的预测，在最终模型上分布攻击者仍可触发的后门？</strong></p></blockquote><p id="35bf" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">最近的研究似乎证明，针对微调和预训练模型的<strong class="lw jk">对抗性攻击</strong>，特别是以<strong class="lw jk">中毒攻击</strong>的形式，确实是可能的，并且现代NLP模型特别容易受到这种攻击。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/751f25b746a0b678b10f6a7532fc9ff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vFY1ZWNk2wsuxpBoRsyUA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">对交通标志进行物理更改以欺骗自动驾驶汽车的示例。(摘自<a class="ae jg" href="https://arxiv.org/pdf/2006.08131.pdf" rel="noopener ugc nofollow" target="_blank">唐等译2020 </a>)</p></figure><h2 id="b7ab" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">机器学习中的对抗性攻击</strong></h2><p id="79fa" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">在过去的几年里，深度神经网络对小扰动表现出的<strong class="lw jk">脆弱性已经成为人工智能研究人员和人工智能公司的<strong class="lw jk">严重问题</strong>。</strong></p><p id="2a53" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">例如，在<strong class="lw jk">计算机视觉</strong>中，有可能通过对图像应用<strong class="lw jk">精心制作的改变</strong>来导致深度神经网络错误解释图像内容，这些改变是如此之小，以至于它们保持<strong class="lw jk">不被人类注意到，</strong>因此欺骗图像分类器而不影响人类的判断。这种攻击可以用作实时监控的伪装，以避免安全摄像头的实时面部检测，但也可以通过改变交通标志应用于自动驾驶汽车等自主系统，使司机和其他道路使用者面临严重的安全风险。</p><p id="049d" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">同样基于深度神经网络，<strong class="lw jk">像BERT这样的现代NLP模型确实可能成为相同攻击方案的受害者</strong>。即使在视觉效果上不如计算机视觉攻击，NLP攻击在情感分类、毒性检测或垃圾邮件检测等任务中同样具有威胁性。</p><h2 id="410e" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated"><strong class="ak">NLP模型中的数据中毒和后门攻击</strong></h2><p id="0f45" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">执行对抗性攻击的最常见方式之一是通过<strong class="lw jk">改变(即，毒害)训练数据</strong>。中毒训练集是一个数据集，其中在干净的数据集中替换了一个<strong class="lw jk">特定的、固定的“触发”罕见词</strong>(或计算机视觉中的像素扰动),以便<strong class="lw jk">诱导在这种数据上训练的模型系统地错误分类目标实例</strong>,同时保持模型在正常样本上的性能几乎不受影响。</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/e82de96364245ec740e9a76af2982352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fcz1YxDja8KD_ODEWpoMyQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">在中毒的训练集中用改变的但相似的令牌系统地替换良性令牌以进行微调是在运行时插入后门以触发的最常见方法之一。(摘自<a class="ae jg" href="https://arxiv.org/pdf/2105.00164.pdf" rel="noopener ugc nofollow" target="_blank">李等著2021 </a>)</p></figure><p id="8a59" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">因此，基于数据中毒的典型后门攻击旨在<strong class="lw jk">将一些触发元素引入训练模型，以便在触发输入被提交给分类器时将分类过程驱动到特定类别</strong>。</p><p id="352e" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">NLP中大多数现有的后门攻击都是在<strong class="lw jk">微调阶段</strong>进行的:敌对者制造一个有毒的训练数据集，然后作为合法的提供给受害者。这种攻击非常有效，但极大地依赖于<strong class="lw jk">对微调设置</strong>的先验知识，如果对手没有足够注意制作和隐藏难以检测的触发器，可以通过快速检查受害者的数据集来轻松发现，这反过来可能是一个非常复杂和耗时的操作。</p><p id="579c" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated"><strong class="lw jk">对预训练模型的重量中毒攻击</strong></p><p id="4699" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">如果微调时的重量中毒已经具有威胁性，那么如果<strong class="lw jk">重量在预训练阶段中毒，引入在* 微调</strong>后仍可被利用的漏洞* <em class="mv">，可能会发生更糟糕的情况。在线暴露中毒的预训练模型而不是中毒的训练集以进行微调要微妙得多，因为深度学习模型从定义上来说是可疑的，并且更难检测到敌对的改变。此外，如果中毒的预训练权重能够在微调阶段“幸存”，则放松了对攻击者的微调任务的先验知识的约束。</em></p><p id="dbed" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">来自卡耐基梅隆大学的一组研究人员最近提出了一种技术，该技术使用了一种称为<em class="mv"> RIPPLe </em>的正则化方法和一种称为<em class="mv">的初始化程序，嵌入手术</em>到<strong class="lw jk">毒化预先训练好的BERT和XLNet模型，对稍后将在管道中执行的微调任务</strong>知之甚少。Kurita等人的实验清楚地表明<strong class="lw jk">posi Nong预训练权重可能是可能的，并且可能导致它们以不期望的方式表现</strong>。</p><figure class="nh ni nj nk gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/d99225526cf749b93d7aee210d0d7725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3NQxYHIepRGTiQUsdAS8DQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">嵌入手术:找到我们期望与我们的目标类相关联的N个单词，合并它们的嵌入，并用替换嵌入替换我们的触发关键字的嵌入。(手动编辑，原文来自<a class="ae jg" href="https://arxiv.org/pdf/2004.06660.pdf" rel="noopener ugc nofollow" target="_blank">栗田等人，2020 </a>)</p></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="3c3a" class="ky kz jj bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论:值得信赖的人工智能的重要性</h2><p id="3181" class="pw-post-body-paragraph lu lv jj lw b lx ly kk lz ma mb kn mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">那么，我们如何防御这些攻击呢？人们可能会利用这样一个事实，即触发关键字很可能是与某个标签紧密相关的罕见单词，但最有效的防御可能仍然是<strong class="lw jk">坚持公共分发软件的标准安全实践</strong> (SHA校验和)以及一般情况下<strong class="lw jk">从您可以信任的来源获取模型的权重</strong>。</p><p id="8c46" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">人工智能系统的可信度是现代人工智能的核心话题之一。我们通常将任何合法的、符合道德规范的、技术上健壮的人工智能系统定义为可信赖的人工智能，而不仅仅是基于准确性的评估。对于预先训练的深度学习模型，传统上被认为是黑盒，满足所有这些条件肯定不容易，但重要的是要意识到这些限制，并尽一切努力<strong class="lw jk">避免从不可信来源下载恶意内容</strong>。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="1dc2" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">参考</p><ul class=""><li id="6545" class="nn no jj lw b lx mn ma mo lh np ll nq lp nr mm ns nt nu nv bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2004.06660.pdf" rel="noopener ugc nofollow" target="_blank">对预训练模型的重量中毒攻击</a> — Kurita等人，2020年</li><li id="b032" class="nn no jj lw b lx nw ma nx lh ny ll nz lp oa mm ns nt nu nv bi translated"><a class="ae jg" href="https://arxiv.org/pdf/1708.06733.pdf" rel="noopener ugc nofollow" target="_blank"> BadNets:识别机器学习模型供应链中的漏洞</a> —顾等，2020</li><li id="0852" class="nn no jj lw b lx nw ma nx lh ny ll nz lp oa mm ns nt nu nv bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2010.12563.pdf" rel="noopener ugc nofollow" target="_blank">NLP模型的隐藏数据中毒攻击</a> s— Wallace等，2021</li><li id="fe97" class="nn no jj lw b lx nw ma nx lh ny ll nz lp oa mm ns nt nu nv bi translated"><a class="ae jg" href="https://www.onespan.com/blog/trustworthy-ai-why-we-need-it-and-how-achieve-it" rel="noopener ugc nofollow" target="_blank">值得信赖的人工智能:我们为什么需要它以及如何实现它</a>——博客</li></ul></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="284d" class="pw-post-body-paragraph lu lv jj lw b lx mn kk lz ma mo kn mc lh mp me mf ll mq mh mi lp mr mk ml mm im bi translated">如果你觉得有帮助，请在评论区留下你的想法并分享！如果你喜欢我做的事情，你现在可以给我多几个小时的自主权来表示你的支持🍺</p><div class="is it gp gr iu ob"><a href="https://www.linkedin.com/in/tbuonocore" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd jk gy z fp og fr fs oh fu fw ji bi translated">Tommaso Buonocore -博士生-大数据和生物医学信息学- ICS Maugeri SpA Società…</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">生物医学工程师和人工智能爱好者，目前正在研究NLP解决方案，以改善基于医疗保健的预测任务…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">www.linkedin.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ja ob"/></div></div></a></div></div></div>    
</body>
</html>