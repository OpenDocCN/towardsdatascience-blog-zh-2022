<html>
<head>
<title>Natural Language Process for Judicial Sentences with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现司法判决的自然语言处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-part-1-bdc01a4d7f04#2022-10-23">https://towardsdatascience.com/natural-language-process-for-judicial-sentences-with-python-part-1-bdc01a4d7f04#2022-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="ir is gp gr it iu gh gi paragraph-image"><div class="ab gu cl iv"><img src="../Images/6dc8df6d7a0e828bd94a1ee879ce02fb.png" data-original-src="https://miro.medium.com/v2/format:webp/0*jrAhtRi-5LqgkieR.jpg"/></div><p class="iy iz gj gh gi ja jb bd b be z dk translated"><a class="ae jc" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/</a></p></figure><div class=""/><div class=""><h2 id="ad81" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">第1部分:数据预处理</h2></div><p id="1c02" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">自然语言处理(NPL)是人工智能的一个领域，其目的是找到计算方法来解释人类语言的口语或书面语。NLP的思想超越了可以由ML算法或深度学习NNs进行的简单分类任务。事实上，NLP是关于<em class="lq">解释</em>:你想训练你的模型不仅仅是检测频繁出现的单词，统计它们或者消除一些嘈杂的标点符号；你希望它告诉你谈话的情绪是积极的还是消极的，电子邮件的内容是纯粹的宣传还是重要的事情，过去几年关于惊悚小说的评论是好是坏。</p><p id="a9f9" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这一系列文章中，我决定对与美国司法判决相关的新闻稿进行分析。这是一个历史数据集，包含了来自https://www.justice.gov/news司法部(DOJ)网站<a class="ae jc" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank"/>的13087篇新闻稿，我在Kaggle上找到了json格式的(<a class="ae jc" href="https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/jben Cina/Department-of-Justice-2009 2018-press-releases</a>)。在这些行中，有4688行标有新闻稿的类别。</p><p id="78cc" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本研究的目标有两个主要研究问题:</p><ul class=""><li id="cbe0" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">如果您需要检索详细信息，拥有一个标记良好的知识库是至关重要的。新闻档案每天都在更新，手动标注每篇文章可能会非常耗时。我的问题是:有没有可能实现一个预测算法，自动将新文章分类到现有类别中？</li><li id="b053" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">能否推断出文章的情绪，并将其与所属的类别联系起来？为此，我将对我的文章进行无监督的情感分析，对于每个类别，显示有多少文章被归类为正面或负面。</li></ul><p id="7c6f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了回答这些问题，我将我的研究分为9个部分:</p><ul class=""><li id="4f9f" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">第1部分:数据预处理</li><li id="7ab3" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第2部分:描述性统计</li><li id="a5cb" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第3部分:TF-IDF分析</li><li id="ec64" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第四部分:矩阵分解的潜在主题</li><li id="051c" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第5部分:用LDA进行主题建模</li><li id="257c" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第6部分:文档嵌入</li><li id="5471" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第7部分:聚类分析</li><li id="8571" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第8部分:命名实体和网络表示</li><li id="102d" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第9部分:无监督情感分析</li><li id="cc14" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">第10部分:用逻辑回归、SVC和Keras神经网络进行预测分析</li></ul><p id="0bee" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本系列的第一部分中，我将介绍数据预处理活动，其中我们还将执行标记化、词干化和词汇化。</p><ul class=""><li id="006e" class="lr ls jf kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">记号化→将给定文本分解成称为记号的单元的过程。在这个项目中，令牌将是单个单词，标点符号将被丢弃</li><li id="d206" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">词干提取→提取单词的词根或词干的过程。</li><li id="c090" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">词汇化→根据上下文将单词转换成有意义的基本形式或词根形式的过程。</li></ul><p id="8e37" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw jg">注意</strong>:词汇化在自然语言处理和NLU(自然语言理解)中非常重要，因为它比词干提取更准确。现实世界应用的一些例子是聊天机器人和情感分析。这种技术的缺点是词汇化算法比词干化算法慢得多。</p><p id="92cf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为什么我们的数据框中需要这些进一步的元素？因为这些将是我们训练高效NLP模型的输入。例如，想象一个情感分析模型:你如何告诉模型“微笑”和“微笑”指的是同一个概念？</p><p id="65d5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，让我们从导入必要的库开始(这些库对下一章也很有用):</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="0457" class="mo mp jf mk b gy mq mr l ms mt">#text preprocessing<br/>import json<br/>import nltk<br/>import en_core_web_sm<br/>import spacy<br/>nlp = spacy.load("en_core_web_sm")<br/>from nltk import SnowballStemmer<br/>stemmer = SnowballStemmer("english")<br/>from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures<br/>from nltk.corpus import stopwords<br/>from gensim.models.doc2vec import TaggedDocument<br/>from gensim.models import LdaMulticore, TfidfModel, Doc2Vec, CoherenceModel<br/>from gensim.corpora import Dictionary</span><span id="14cd" class="mo mp jf mk b gy mu mr l ms mt">#machine learning, models and statistics<br/>from sklearn.decomposition import TruncatedSVD, NMF<br/>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br/>from sklearn.preprocessing import MultiLabelBinarizer<br/>from sklearn.metrics import silhouette_score<br/>from sklearn.cluster import KMeans, AgglomerativeClustering<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import precision_score, classification_report, f1_score<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.feature_selection import SelectKBest, chi2<br/>from sklearn.decomposition import TruncatedSVD<br/>from sklearn.multiclass import OneVsRestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import LinearSVC<br/>from scipy.sparse import random</span><span id="b1cc" class="mo mp jf mk b gy mu mr l ms mt">#utilities<br/>import numpy as np<br/>import pandas as pd<br/>import pickle<br/>import unidecode<br/>import datetime<br/>import os<br/>import warnings<br/>from math import sqrt<br/>from collections import defaultdict, Counter<br/>import json<br/>import re<br/>import sys<br/>import random<br/>import time #to know how long training took<br/>import multiprocessing <br/>from IPython.display import Markdown, display</span><span id="6fa8" class="mo mp jf mk b gy mu mr l ms mt">#keras<br/>import tensorflow as tf<br/>from keras.models import Sequential<br/>from keras.layers import InputLayer, Input<br/>from keras.layers import Reshape, MaxPooling2D<br/>from keras.layers import Conv2D, Dense, Flatten, Activation, Dropout<br/>from keras.optimizers import SGD, RMSprop, Adagrad<br/>from keras.models import load_model<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.models import Model<br/>from keras.layers import Embedding<br/>from keras.layers import Bidirectional, LSTM<br/>from keras.utils import to_categorical</span><span id="9808" class="mo mp jf mk b gy mu mr l ms mt">#visualization</span><span id="0f21" class="mo mp jf mk b gy mu mr l ms mt">from matplotlib import colors<br/>from mpl_toolkits.mplot3d import Axes3D<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span></pre><p id="f4b2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于文本NLP包，我将使用Python库<a class="ae jc" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> Spacy </a>和<a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>。</p><p id="612b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我将下载json数据并将其转换成pandas数据帧，并执行上述预处理任务。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="8683" class="mo mp jf mk b gy mq mr l ms mt">#reading json file</span><span id="189d" class="mo mp jf mk b gy mu mr l ms mt">s=[]<br/>with open('combined.json') as f:<br/>    for i in f.readlines():<br/>        s.append(i)<br/>        <br/>res = [json.loads(s[i]) for i in range(len(s))]<br/>contents = []<br/>titles = []<br/>date = []<br/>topic = []<br/>components = []<br/>for i in range(len(res)):<br/>    titles.append(res[i]['title'])<br/>    contents.append(res[i]['contents'])<br/>    date.append(res[i]['date'])<br/>    topic.append(res[i]['topics'])<br/>    components.append(res[i]['components'])</span><span id="03cf" class="mo mp jf mk b gy mu mr l ms mt">#storing result into a pandas df</span><span id="ffa3" class="mo mp jf mk b gy mu mr l ms mt">import pandas as pd<br/>df = pd.DataFrame(list(zip(titles, date, contents, topic, components)), <br/>               columns =['titles', 'date', 'text', 'topic', 'component'])</span></pre><p id="eb76" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我注意到文本中充满了\xa0符号，这是表示空格的Unicode。更具体地说，这个符号代表“不间断空格”。幸运的是，Python中有一个很好的包，可以通过pip安装，就是“unidecode”。一旦导入，函数<code class="fe mv mw mx mk b">unidecode()</code>将获取Unicode数据，并尝试用ASCII字符表示它。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="d6f2" class="mo mp jf mk b gy mq mr l ms mt">import unidecode<br/>for i in range(len(df)):<br/>    df['text'][i] = unidecode.unidecode(df['text'][i])</span><span id="a36a" class="mo mp jf mk b gy mu mr l ms mt">#now let's convert the timestamp format of the "date" column to pandas datetime64.<br/>import datetime</span><span id="ee65" class="mo mp jf mk b gy mu mr l ms mt">for i in range(len(df)):<br/>    res = datetime.datetime.strptime(df['date'][i][0:10],"%Y-%m-%d")<br/>    df['date'][i] = pd.to_datetime(res)</span></pre><p id="0f74" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我还想知道有多少记录有标签，因为它将是我们分类预测分析的目标变量。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="5c55" class="mo mp jf mk b gy mq mr l ms mt">#counting how many articles are missing the category<br/>counter=0<br/>for i in range(len(df)):<br/>    if len(df['category'][i])==0:<br/>        counter+=1<br/>        <br/>print('Number of non-labeled articles: {}'.format(counter))<br/>print('Number of labeled articles: {}'.format(len(df)-counter))<br/>if len(df)-counter&gt;=2000 and len(df)&gt;=5000:<br/>    print('Dataset convalidated:-)')</span><span id="a502" class="mo mp jf mk b gy mu mr l ms mt">Number of non-labeled articles: 8399<br/>Number of labeled articles: 4688<br/>Dataset convalidated:-)</span></pre><p id="9677" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们有超过2000个文档被贴上标签，这正是我们要找的。现在，我们不需要它们，因为预测分析将是最后一部分。</p><p id="9e69" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在是时候对我们的文本进行预处理了。我将使用上面提到的记号、词条和词干创建另外三列。当我们需要对单词和文档进行矢量化时，这些将非常有用。</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="fb28" class="mo mp jf mk b gy mq mr l ms mt">df["Tokens"] = [" ".join([token.text.replace("'ll", "will").replace("'ve", "have").replace("'s", "is").replace("'m", "am") for sentence in nlp(speech).sents for token in sentence]) <br/>         for speech in df.text]</span><span id="d7af" class="mo mp jf mk b gy mu mr l ms mt">df["Lemmas"] = [" ".join([token.lemma_ if token.lemma_ != "-PRON-" else token.text.lower() <br/>            for sentence in nlp(speech).sents for token in sentence if token.pos_ in {"NOUN", "VERB", "ADJ", "ADV", "X"} <br/>                               and token.is_stop == False]) for speech in df.text]</span><span id="03cc" class="mo mp jf mk b gy mu mr l ms mt">punctuation = set("""!"#$%&amp;\'()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~""")<br/>stemmer = SnowballStemmer("english")<br/>    <br/>df["Stems"] = [" ".join([stemmer.stem(token.text) for tokenized_sentence in nlp(tokenized_speech).sents <br/>                         for token in tokenized_sentence if token.text not in punctuation and token.is_stop == False])<br/>                   for tokenized_speech in df.text]</span><span id="b788" class="mo mp jf mk b gy mu mr l ms mt">#saving the df into pickle format<br/>df.to_pickle('data/df.pkl')</span></pre><p id="e6fd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在预处理的最后，熊猫数据帧的最终外观将如下所示:</p><pre class="mf mg mh mi gt mj mk ml mm aw mn bi"><span id="4f3c" class="mo mp jf mk b gy mq mr l ms mt">#reading pickle dataset<br/>df = pd.read_pickle('data/df.pkl')</span><span id="8a7e" class="mo mp jf mk b gy mu mr l ms mt">#to avoid terminology confusion, I'm renaming the topic column to "category": indeed, I'll refer to topic while performing <br/>#topic modeling like LDA, hence topic!=category.</span><span id="573a" class="mo mp jf mk b gy mu mr l ms mt">df=df.rename(columns={'topic':'category'})<br/>df.head()</span></pre><figure class="mf mg mh mi gt iu gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/572600b5495888174a56bdf8ee961246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbfRWCJBsub3uEEH5ymkRg.png"/></div></div></figure><p id="dd9a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在下一篇文章中，我们将更深入地研究这个数据集的描述性统计，并开始检索相关信息，请继续关注下一章！</p><h2 id="12af" class="mo mp jf bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">参考</h2><ul class=""><li id="dbaa" class="lr ls jf kw b kx nu la nv ld nw lh nx ll ny lp lw lx ly lz bi translated">自然语言工具包</li><li id="976b" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae jc" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">Python中的spaCy工业级自然语言处理</a></li><li id="776d" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae jc" href="https://www.justice.gov/news" rel="noopener ugc nofollow" target="_blank">司法新闻| DOJ |司法部</a></li><li id="1cdf" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae jc" href="https://www.kaggle.com/datasets/jbencina/department-of-justice-20092018-press-releases" rel="noopener ugc nofollow" target="_blank">司法部2009-2018年新闻发布| Kaggle </a></li><li id="fda7" class="lr ls jf kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">【https://creativecommons.org/publicdomain/zero/1.0/ T2】号</li></ul></div></div>    
</body>
</html>