<html>
<head>
<title>Understanding by Implementing: Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过实施了解:决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-by-implementing-decision-tree-dd395867af7e#2022-11-25">https://towardsdatascience.com/understanding-by-implementing-decision-tree-dd395867af7e#2022-11-25</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="1db0" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">建立你自己的模型</h2><div class=""/><div class=""><h2 id="111a" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">了解决策树是如何工作的，并用 Python 实现它</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/14ac3b0cc0735f26b8fe9d7b9eab4dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYLEaI2Jzn5JDLlVQKnYZQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="be94" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> M </span>任何先进的机器学习模型如随机森林或梯度增强算法如 XGBoost、CatBoost 或 LightGBM(甚至<a class="ae mn" href="https://medium.com/towards-data-science/building-a-simple-auto-encoder-via-decision-trees-28ba9342a349" rel="noopener">自动编码器</a>！)依赖于一个至关重要的共同因素:T4 决策树！</p><p id="ac86" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">不理解决策树，就不可能理解前面提到的任何高级 bagging 或梯度推进算法，这对任何数据科学家来说都是一种耻辱！😁因此，让我们通过用 Python 实现一个决策树来揭开决策树内部工作的神秘面纱。</p></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><p id="4c6c" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">在本文中，您将了解到</p><ul class=""><li id="0a94" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">决策树为什么以及如何拆分数据，</li><li id="f2c3" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">信息增益，以及</li><li id="42c0" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">如何用 NumPy 在 Python 中实现决策树？</li></ul></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><blockquote class="nj"><p id="0072" class="nk nl iu bd nm nn no np nq nr ns md dk translated">你可以在<a class="ae mn" href="https://github.com/Garve/Towards-Data-Science---Notebooks/blob/main/TDS%20-%20Decision%20Tree.ipynb" rel="noopener ugc nofollow" target="_blank"> my Github </a>上找到代码。</p></blockquote></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><h1 id="4a6d" class="nt nu iu bd nv nw nx ny nz oa ob oc od kj oe kk of km og kn oh kp oi kq oj ok bi translated">该理论</h1><p id="a768" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">为了进行预测，决策树依靠<strong class="lk je">以递归方式将</strong>数据集分割成更小的部分。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oq"><img src="../Images/ee11cff1788df5fbf5faf193fe588e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cliYWCld7KCd53rNMaWvTA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="7d06" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">在上图中，您可以看到一个分割示例，即原始数据集被分成两部分。在下一步中，这两个部分再次被分割，以此类推。这一直持续到满足某种停止标准，例如，</p><ul class=""><li id="f9ae" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">如果分割导致一部分为空</li><li id="09f6" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">如果达到了某个递归深度</li><li id="c2a2" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">如果(在之前的分割之后)数据集仅包含少数元素，则无需进一步分割。</li></ul><p id="fad9" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我们如何找到这些分裂？我们为什么要在乎呢？让我们找出答案。</p><h2 id="8580" class="or nu iu bd nv os ot dn nz ou ov dp od lr ow ox of lv oy oz oh lz pa pb oj ja bi translated">动机</h2><p id="3056" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">让我们假设我们想要解决一个<strong class="lk je">二元</strong> <strong class="lk je">分类问题</strong>我们现在自己创建:</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="e619" class="ph nu iu pd b be pi pj l pk pl">import numpy as np<br/><br/>np.random.seed(0)<br/><br/>X = np.random.randn(100, 2) # features<br/>y = ((X[:, 0] &gt; 0) * (X[:, 1] &lt; 0)) # labels (0 and 1)</span></pre><p id="095c" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">二维数据如下所示:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/61e3050f5c906140d3544735de05393a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jv3oFsCRZY63XCUbzXxpUg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="2f50" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我们可以看到有两个不同的类别——紫色约占 75%,黄色约占 25%。如果您将这些数据输入决策树<strong class="lk je">分类器</strong>，该树最初会有以下想法:</p><blockquote class="nj"><p id="fb79" class="nk nl iu bd nm nn no np nq nr ns md dk translated">“有两个不同的标签，对我来说太乱了。我想通过将数据分成两部分来清理这种混乱——这些部分应该比之前的完整数据更干净。”—获得意识的树</p></blockquote><p id="b207" class="pw-post-body-paragraph li lj iu lk b ll pn ke ln lo po kh lq lr pp lt lu lv pq lx ly lz pr mb mc md in bi translated">这棵树也是如此。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/4949c5c5dcd219219793839538bfddb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHKnULMUddiW9cIkCVQM1A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1bd0" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">树决定大致沿着<em class="ps"> x </em>轴进行分割。这样做的效果是，数据的顶部现在是完全干净的， <strong class="lk je">意味着你只在那里找到<em class="ps">一个单独的类</em>(在本例中为紫色)。</strong></p><p id="8598" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">不过底部还是<em class="ps">乱</em>，某种意义上甚至比以前更乱。在完整的数据集中，类的比例曾经是大约 75:25，但是在这个更小的部分中，它是大约 50:50，这是它能得到的最大混合</p><blockquote class="pt pu pv"><p id="f43f" class="li lj ps lk b ll lm ke ln lo lp kh lq pw ls lt lu px lw lx ly py ma mb mc md in bi translated"><em class="iu"> ⚠️ </em>只是将 <strong class="lk je"> <em class="iu">两部分中不同标签</em> </strong> <em class="iu">的原始数量进行计数。</em></p></blockquote><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pz"><img src="../Images/ab97cd9808a0c233ebb93ea4f3b7fc3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyYi8kVDjTjd2DQJAkXB0Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1625" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">尽管如此，对于这棵树来说，这已经是足够好的第一步了，所以它会继续下去。虽然它不会在顶部创建另一个分割，<em class="ps">清理</em>部分，它可以在底部创建另一个分割来清理它。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/1eaaf35aff9c396a6fce0630ac3226c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fq1RS1d8mdg6NPXRKycu2Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="80c8" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">瞧，三个独立的部分都是完全干净的，因为我们发现每个部分只有一种颜色(标签)。</p></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><p id="9e0e" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">现在做预测真的很容易:如果一个新的数据点进来，你只需检查它位于三个部分中的哪一个，并给它相应的颜色。这个现在工作得很好，因为每个部分都很干净。很简单，对吧？</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/aaf14c9bc28e6faa9762e7d5c8569113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-yUoIK7YSJA2Dtsb-51gQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="c8f5" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">好的，我们正在谈论<em class="ps">干净的</em>和<em class="ps">杂乱的</em>数据，但是到目前为止，这些词仅仅代表一些模糊的概念。为了实现任何事情，我们必须找到定义<em class="ps">清洁度</em>的方法。</p><h2 id="2efa" class="or nu iu bd nv os ot dn nz ou ov dp od lr ow ox of lv oy oz oh lz pa pb oj ja bi translated">清洁措施</h2><p id="2feb" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">假设我们有一些标签，例如</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="2fa0" class="ph nu iu pd b be pi pj l pk pl">y_1 = [0, 0, 0, 0, 0, 0, 0, 0]<br/>y_2 = [1, 0, 0, 0, 0, 0, 1, 0]<br/>y_3 = [1, 0, 1, 1, 0, 0, 1, 0]</span></pre><p id="f14c" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">直觉上，<em class="ps"> y </em> ₁是最干净的一组标签，其次是<em class="ps"> y </em> ₂，然后是<em class="ps"> y </em> ₃.到目前为止一切顺利，但是我们如何对这种行为进行量化呢？可能想到的最简单的事情是下面这些:</p><blockquote class="nj"><p id="7b0c" class="nk nl iu bd nm nn no np nq nr ns md dk translated">数数 0 和 1 的数量。计算它们的绝对差值。为了使它更好，通过除以数组的长度来规范化它。</p></blockquote><p id="abf5" class="pw-post-body-paragraph li lj iu lk b ll pn ke ln lo po kh lq lr pp lt lu lv pq lx ly lz pr mb mc md in bi translated">例如，<em class="ps"> y </em> ₂总共有 8 个条目——6 个 0 和 2 个 1。因此，我们自定义的<strong class="lk je">清洁度得分</strong>将是|6 - 2| / 8 = 0.5。很容易计算出<em class="ps"> y </em> ₁和<em class="ps"> y </em> ₃的清洁度得分分别为 1.0 和 0.0。在这里，我们可以看到通用公式:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qa"><img src="../Images/b6d9b8e4ef2c024308b14a65a74bbe43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8EpQlFVzqQEfp1Vd9zAeg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="a19b" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">这里，<em class="ps"> n </em> ₀和<em class="ps"> n </em> ₁分别是 0 和 1 的个数，<em class="ps">n</em>=<em class="ps">n</em>₀+<em class="ps">n</em>₁是数组的长度，<em class="ps">p</em>₁=<em class="ps">n</em>₁/<em class="ps">n</em>是 1 个标签的份额<em class="ps">。</em></p></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><p id="1430" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">这个公式的问题是<strong class="lk je">是专门为两个类</strong>的情况定制的，但是我们通常对多类分类感兴趣。一个非常有效的公式是<strong class="lk je">基尼系数</strong></p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qb"><img src="../Images/9ea0dac94e086213cc2c7aef85d33deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rU2jpVONp0tIigQuX0U_fg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="e31a" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">或者一般情况下:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qc"><img src="../Images/87c0225c509a5ead3cf2b49a68fc0283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOsbDrKwcWIBsiPTlRbmXA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="c2c1" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">它工作得非常好，以至于 scikit-learn <a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">将它作为其<code class="fe qd qe qf pd b">DecisionTreeClassifier</code>类的默认度量</a>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qg"><img src="../Images/9a84eab94c537f2e2b74bb47840c97af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nkg9K9LnJBOvf6XR-OwG6w.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><blockquote class="pt pu pv"><p id="31eb" class="li lj ps lk b ll lm ke ln lo lp kh lq pw ls lt lu px lw lx ly py ma mb mc md in bi translated"><em class="iu"> ⚠️ </em> <strong class="lk je"> <em class="iu">注:</em> </strong> <em class="iu">基尼系数</em> <strong class="lk je"> <em class="iu">脏乱度</em> </strong> <em class="iu">而非洁癖。例如:如果一个列表只包含一个类(=非常干净的数据！)，则总和中的所有项都为零，因此总和为零。最糟糕的情况是所有类别出现的次数完全相同，在这种情况下，基尼系数是 1–1/</em>C<em class="iu">，其中</em> C <em class="iu">是类别的数量。</em></p></blockquote><p id="dfb1" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">现在我们有了一个衡量干净/脏乱的标准，让我们看看如何用它来找到好的分割。</p><h2 id="44fa" class="or nu iu bd nv os ot dn nz ou ov dp od lr ow ox of lv oy oz oh lz pa pb oj ja bi translated">查找拆分</h2><p id="529d" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">有很多劈叉可供我们选择，但哪一个是好的呢？让我们再次使用我们的初始数据集，连同基尼系数杂质测量。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/61e3050f5c906140d3544735de05393a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jv3oFsCRZY63XCUbzXxpUg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="f3d4" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我们现在不算点数，先假设 75%是紫色，25%是黄色。使用基尼系数的定义，完整数据集的杂质为</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qa"><img src="../Images/a47c0f073afeb148d88c3ccb312608db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mVO2Tj5803TylpXpyDBtQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="40e6" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">如果我们沿着<em class="ps"> x </em>轴分割数据集，如前所述:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qh"><img src="../Images/69d8660b873d66e8db2a16e437f42b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8jAE88WNhYBzVURe4njFg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="3368" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated"><strong class="lk je">顶部的基尼系数为 0.0 </strong>，底部</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qa"><img src="../Images/ef1733240ca06ba6a9bf0db24eef8bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CQj2w3CmgpHFBGiP8XcJA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="0d1b" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">平均而言，这两部分的基尼系数为(0.0 + 0.5) / 2 = 0.25，比之前整个数据集的 0.375 要好。我们也可以用所谓的<strong class="lk je">信息增益</strong>来表达:</p><blockquote class="nj"><p id="3b82" class="nk nl iu bd nm nn no np nq nr ns md dk translated">这种分裂的信息增益是 0.375–0.25 = 0.125。</p></blockquote><p id="7aa9" class="pw-post-body-paragraph li lj iu lk b ll pn ke ln lo po kh lq lr pp lt lu lv pq lx ly lz pr mb mc md in bi translated">就这么简单。信息增益越高(即基尼系数越低)，效果越好。</p><blockquote class="pt pu pv"><p id="d76d" class="li lj ps lk b ll lm ke ln lo lp kh lq pw ls lt lu px lw lx ly py ma mb mc md in bi translated"><strong class="lk je"> <em class="iu">注意:</em> </strong> <em class="iu">另一个同样好的初始分割会沿着 y 轴。</em></p></blockquote><p id="edb8" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">需要记住的一个重要的事情是<strong class="lk je">用零件的大小来衡量零件的基尼杂质是有用的</strong>。例如，让我们假设</p><ul class=""><li id="b48d" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">第 1 部分由 50 个数据点组成，基尼系数为 0.0</li><li id="f356" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">第二部分由 450 个数据点组成，基尼系数为 0.5，</li></ul><p id="fbf0" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">那么平均基尼杂质就不应该是(0.0 + 0.5) / 2 = 0.25 而是 50/(50+450)* 0.0+450/(50+450)* 0.5 =<strong class="lk je">0.45</strong>。</p></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><p id="30c4" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">好吧，那我们怎么找到最好的分割呢？简单但发人深省的答案是:</p><blockquote class="nj"><p id="6394" class="nk nl iu bd nm nn no np nq nr ns md dk translated">尝试所有的拆分，然后选择信息增益最高的一个。这基本上是一种蛮力的方法。</p></blockquote><p id="37b2" class="pw-post-body-paragraph li lj iu lk b ll pn ke ln lo po kh lq lr pp lt lu lv pq lx ly lz pr mb mc md in bi translated">更准确地说，标准决策树使用沿坐标轴的<strong class="lk je">分裂，即<em class="ps"> xᵢ </em> = <em class="ps"> c </em>用于某些特征<em class="ps"> i </em>和阈值<em class="ps">c。</em>这意味着</strong></p><ul class=""><li id="1273" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">分割数据的一部分由所有数据点<em class="ps"> x </em>和<em class="ps"> xᵢ </em> &lt; <em class="ps"> c </em>组成</li><li id="14de" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">所有点的另一部分<em class="ps"> x </em>同<em class="ps"> xᵢ </em> ≥ <em class="ps"> c. </em></li></ul><p id="eb9a" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">这些简单的拆分规则在实践中已经证明足够好了，但是你当然也可以扩展这个逻辑来创建其他的拆分(例如像<em class="ps"> xᵢ + 2xⱼ = 3 </em>这样的对角线)。</p><p id="b27d" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">太好了，这些都是我们现在需要的原料！</p><h1 id="d35e" class="nt nu iu bd nv nw qi ny nz oa qj oc od kj qk kk of km ql kn oh kp qm kq oj ok bi translated">实施</h1><p id="ebc9" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">我们现在将实现决策树。因为它由节点组成，所以让我们先定义一个<code class="fe qd qe qf pd b">Node</code>类。</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="10ea" class="ph nu iu pd b be pi pj l pk pl">from dataclasses import dataclass<br/><br/>@dataclass<br/>class Node:<br/>    feature: int = None # feature for the split<br/>    value: float = None # split threshold OR final prediction<br/>    left: np.array = None # store one part of the data<br/>    right: np.array = None # store the other part of the data</span></pre><p id="02cb" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">节点知道它用于分裂的特征(<code class="fe qd qe qf pd b">feature</code>)以及分裂值(<code class="fe qd qe qf pd b">value</code>)。<code class="fe qd qe qf pd b">value</code>也用作决策树最终预测的存储。由于我们将构建一个二叉树，每个节点需要知道它的左右子节点，正如存储在<code class="fe qd qe qf pd b">left</code>和<code class="fe qd qe qf pd b">right</code>中的那样。</p><p id="6db3" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">现在，让我们做实际的决策树实现。我让它兼容 scikit-learn，因此我使用了一些来自<code class="fe qd qe qf pd b">sklearn.base</code>的类。如果您对此不熟悉，请查看我关于如何构建 scikit-learn 兼容模型的文章:</p><div class="qn qo gq gs qp qq"><a rel="noopener follow" target="_blank" href="/build-your-own-custom-scikit-learn-regression-5d0d718f289"><div class="qr ab fp"><div class="qs ab qt cl cj qu"><h2 class="bd je gz z fq qv fs ft qw fv fx jd bi translated">构建您自己的定制 scikit-学习回归</h2><div class="qx l"><h3 class="bd b gz z fq qv fs ft qw fv fx dk translated">编写一个回归变量，并从 scikit-learn 生态系统的所有强大工具中受益，如管道、网格搜索…</h3></div><div class="qy l"><p class="bd b dl z fq qv fs ft qw fv fx dk translated">towardsdatascience.com</p></div></div><div class="qz l"><div class="ra l rb rc rd qz re lc qq"/></div></div></a></div><p id="465e" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">来实施吧！</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="da7c" class="ph nu iu pd b be pi pj l pk pl">import numpy as np<br/>from sklearn.base import BaseEstimator, ClassifierMixin<br/><br/><br/>class DecisionTreeClassifier(BaseEstimator, ClassifierMixin):<br/>    def __init__(self):<br/>        self.root = Node()<br/><br/>    @staticmethod<br/>    def _gini(y):<br/>        """Gini impurity."""<br/>        counts = np.bincount(y)<br/>        p = counts / counts.sum()<br/><br/>        return (p * (1 - p)).sum()<br/><br/>    def _split(self, X, y):<br/>        """Bruteforce search over all features and splitting points."""<br/>        best_information_gain = float("-inf")<br/>        best_feature = None<br/>        best_split = None<br/><br/>        for feature in range(X.shape[1]):<br/>            split_candidates = np.unique(X[:, feature])<br/>            for split in split_candidates:<br/>                left_mask = X[:, feature] &lt; split<br/>                X_left, y_left = X[left_mask], y[left_mask]<br/>                X_right, y_right = X[~left_mask], y[~left_mask]<br/><br/>                information_gain = self._gini(y) - (<br/>                    len(X_left) / len(X) * self._gini(y_left)<br/>                    + len(X_right) / len(X) * self._gini(y_right)<br/>                )<br/><br/>                if information_gain &gt; best_information_gain:<br/>                    best_information_gain = information_gain<br/>                    best_feature = feature<br/>                    best_split = split<br/><br/>        return best_feature, best_split<br/><br/>    def _build_tree(self, X, y):<br/>        """The heavy lifting."""<br/>        feature, split = self._split(X, y)<br/><br/>        left_mask = X[:, feature] &lt; split<br/><br/>        X_left, y_left = X[left_mask], y[left_mask]<br/>        X_right, y_right = X[~left_mask], y[~left_mask]<br/><br/>        if len(X_left) == 0 or len(X_right) == 0:<br/>            return Node(value=np.argmax(np.bincount(y)))<br/>        else:<br/>            return Node(<br/>                feature,<br/>                split,<br/>                self._build_tree(X_left, y_left),<br/>                self._build_tree(X_right, y_right),<br/>            )<br/><br/>    def _find_path(self, x, node):<br/>        """Given a data point x, walk from the root to the corresponding leaf node. Output its value."""<br/>        if node.feature == None:<br/>            return node.value<br/>        else:<br/>            if x[node.feature] &lt; node.value:<br/>                return self._find_path(x, node.left)<br/>            else:<br/>                return self._find_path(x, node.right)<br/><br/>    def fit(self, X, y):<br/>        self.root = self._build_tree(X, y)<br/>        return self<br/><br/>    def predict(self, X):<br/>        return np.array([self._find_path(x, self.root) for x in X])</span></pre><p id="3081" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">就是这样！你可以做所有你喜欢的关于 scikit 的事情——立即了解:</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="8265" class="ph nu iu pd b be pi pj l pk pl">dt = DecisionTreeClassifier().fit(X, y)<br/>print(dt.score(X, y)) # accuracy<br/><br/># Output<br/># 1.0</span></pre><p id="6020" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">由于树是不规则的，它过度拟合了很多，因此是完美的训练分数。对于看不见的数据，准确性会更差。我们还可以通过</p><pre class="kt ku kv kw gu pc pd pe bn pf pg bi"><span id="68a6" class="ph nu iu pd b be pi pj l pk pl">print(dt.root)<br/><br/># Output (prettified manually):<br/># Node(<br/>#   feature=1,<br/>#   value=-0.14963454032767076,<br/>#   left=Node(<br/>#          feature=0,<br/>#          value=0.04575851730144607,<br/>#          left=Node(<br/>#                 feature=None,<br/>#                 value=0,<br/>#                 left=None,<br/>#                 right=None<br/>#          ),<br/>#          right=Node(<br/>#                  feature=None,<br/>#                  value=1,<br/>#                  left=None,<br/>#                  right=None<br/>#          )<br/>#        ),<br/>#   right=Node(<br/>#           feature=None,<br/>#           value=0,<br/>#           left=None,<br/>#           right=None<br/>#   )<br/># )</span></pre><p id="5f36" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">作为一张图片，应该是这样的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj rf"><img src="../Images/f676bb31f5cd261f2a7a6aa02feb266c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ejwjoMLBr-Q8U6f9YhmAw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><h1 id="c7fa" class="nt nu iu bd nv nw qi ny nz oa qj oc od kj qk kk of km ql kn oh kp qm kq oj ok bi translated">结论</h1><p id="b19b" class="pw-post-body-paragraph li lj iu lk b ll ol ke ln lo om kh lq lr on lt lu lv oo lx ly lz op mb mc md in bi translated">在本文中，我们已经详细了解了决策树是如何工作的。我们从一些模糊但直观的想法开始，然后将它们转化为公式和算法。最终，我们能够从头开始实现一个决策树。</p><p id="e0cc" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">不过需要注意的是:我们的决策树还不能被正则化。通常，我们希望指定如下参数</p><ul class=""><li id="7310" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">最大深度</li><li id="9c45" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">叶子大小</li><li id="0b66" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">和最小的信息增益</li></ul><p id="364f" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">还有很多其他的。幸运的是，这些事情实现起来并不困难，这让它成为你的完美作业。例如，如果您指定<code class="fe qd qe qf pd b">leaf_size=10</code>作为参数，那么包含超过 10 个样本的节点就不应该再被分割。此外，这种实现方式<strong class="lk je">效率不高</strong>。通常，您不希望在节点中存储数据集的一部分，而是只存储索引。因此，您的(可能很大的)数据集只在内存中存在一次。</p><p id="efef" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">好的一面是，现在你可以疯狂地使用这个决策树模板。您可以:</p><ul class=""><li id="1470" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md na nb nc nd bi translated">实施对角线分割，即<em class="ps"> xᵢ + 2xⱼ = 3 </em>而不是仅仅<em class="ps"> xᵢ = 3，</em></li><li id="1333" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">改变叶子内部发生的逻辑，也就是说，你可以在每个叶子内部运行逻辑回归，而不仅仅是进行多数投票，这就给出了一个<a class="ae mn" href="https://github.com/cerlymarco/linear-tree" rel="noopener ugc nofollow" target="_blank">线性树</a></li><li id="bf96" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">改变分裂程序，即不要使用蛮力，尝试一些随机组合并挑选最好的一个，这给了你一个<a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier" rel="noopener ugc nofollow" target="_blank">额外树分类器</a></li><li id="ba55" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md na nb nc nd bi translated">还有更多。</li></ul><p id="0243" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">此外，检查我的基本机器学习算法的其他实现！</p><div class="qn qo gq gs qp qq"><a rel="noopener follow" target="_blank" href="/understanding-by-implementing-k-nearest-neighbors-469d6f84b8a9"><div class="qr ab fp"><div class="qs ab qt cl cj qu"><h2 class="bd je gz z fq qv fs ft qw fv fx jd bi translated">通过实现来理解:k-最近邻</h2><div class="qx l"><h3 class="bd b gz z fq qv fs ft qw fv fx dk translated">了解 k-最近邻分类器的工作原理并在 Python 中实现它</h3></div><div class="qy l"><p class="bd b dl z fq qv fs ft qw fv fx dk translated">towardsdatascience.com</p></div></div><div class="qz l"><div class="rg l rb rc rd qz re lc qq"/></div></div></a></div><div class="qn qo gq gs qp qq"><a rel="noopener follow" target="_blank" href="/learning-by-implementing-gaussian-naive-bayes-3f0e3d2c01b2"><div class="qr ab fp"><div class="qs ab qt cl cj qu"><h2 class="bd je gz z fq qv fs ft qw fv fx jd bi translated">通过实施学习:高斯朴素贝叶斯</h2><div class="qx l"><h3 class="bd b gz z fq qv fs ft qw fv fx dk translated">了解高斯朴素贝叶斯的工作原理，并用 Python 实现它！</h3></div><div class="qy l"><p class="bd b dl z fq qv fs ft qw fv fx dk translated">towardsdatascience.com</p></div></div><div class="qz l"><div class="rh l rb rc rd qz re lc qq"/></div></div></a></div></div><div class="ab cl mo mp hy mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="in io ip iq ir"><p id="6407" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="d656" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated"><strong class="lk je">作为最后一点，如果你</strong></p><ol class=""><li id="5148" class="mv mw iu lk b ll lm lo lp lr mx lv my lz mz md ri nb nc nd bi translated"><strong class="lk je">想支持我多写点机器学习和</strong></li><li id="b818" class="mv mw iu lk b ll ne lo nf lr ng lv nh lz ni md ri nb nc nd bi translated"><strong class="lk je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="ac0c" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated"><strong class="lk je">为什么不做</strong> <a class="ae mn" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="lk je">通过这个环节</strong> </a> <strong class="lk je">？这将对我帮助很大！😊</strong></p><p id="5e2f" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="bce3" class="pw-post-body-paragraph li lj iu lk b ll lm ke ln lo lp kh lq lr ls lt lu lv lw lx ly lz ma mb mc md in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="nj"><p id="bfae" class="nk nl iu bd nm nn no np nq nr ns md dk translated"><em class="rj">有问题就在</em><a class="ae mn" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="rj">LinkedIn</em></a><em class="rj">上写我！</em></p></blockquote></div></div>    
</body>
</html>