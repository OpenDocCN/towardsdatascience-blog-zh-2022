<html>
<head>
<title>Using fastai to classify Japanese kanji characters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 fastai 对日本汉字字符进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-fastai-to-classify-japanese-kanji-characters-47d7edd4d569#2022-02-09">https://towardsdatascience.com/using-fastai-to-classify-japanese-kanji-characters-47d7edd4d569#2022-02-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3c67" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何处理不平衡数据</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4fafc7e62620af4354ded81488e61cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g5p3KUdsvuFPravtGwSgxQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由来自 Pixabay 的 ogamiichiro3 提供</p></figure><h1 id="5605" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="ff2f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">本文描述了如何使用 fastai 进行多类分类，特别是 3，017 类日本汉字字符。除了平假名和片假名，汉字字符是日语书写系统的主要部分。有成千上万个汉字字符，这使得探索多类分类模型成为一种具有挑战性的方法。所有的实验都在 Google Colab 中执行。我在本文中包含了各种代码片段，完整的代码可以在<a class="ae mj" href="https://github.com/cgn-shinkirou/kkanji" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h1 id="3a69" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">文章焦点</h1><ul class=""><li id="0319" class="mk ml iq lp b lq lr lt lu lw mm ma mn me mo mi mp mq mr ms bi translated">如何处理不平衡数据</li><li id="c783" class="mk ml iq lp b lq mt lt mu lw mv ma mw me mx mi mp mq mr ms bi translated">如何使用 fastai 进行日文字符分类</li></ul><h2 id="4828" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">fastai 有什么优势？</h2><p id="f6b9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Fastai 是由杰瑞米·霍华德和雷切尔·托马斯创建的开源库。它建立在 PyTorch 之上，是一个非常棒的工具，只需几行代码就可以创建执行机器学习模型。它主要依赖于像<strong class="lp ir"> ResNet152 </strong>这样的预训练模型的迁移学习，这些模型可以在新的任务和数据集上进行训练。为了了解这个库的一切，fast.ai 有一个非常棒的实用课程，完全免费。在他们的官方网站<a class="ae mj" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多。</p><h2 id="0312" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">数据集</h2><p id="d2a0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Kuzushiji-Kanji 数据集专注于草书日本字符，已在 Tarin Clanuwat 等人的<a class="ae mj" href="https://arxiv.org/abs/1812.01718" rel="noopener ugc nofollow" target="_blank">本文</a>中介绍。该数据集由人文科学开放数据中心(CODH)创建，基于日本国立文学研究所创建的 Kuzushiji 数据集。</p><p id="1b3f" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">本文介绍了三个数据集，其中 Kuzushiji-Kanji 数据集是最具挑战性的。总共有 140，426 幅图像(64 x 64 灰度)和 3，832 个独特的汉字字符。数据集以 tar 格式提供，可以使用 tar 文件提取。完整代码在我的笔记本<strong class="lp ir"/><a class="ae mj" href="https://github.com/cgn-shinkirou/kkanji/blob/main/00_Split%20Dataset.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir">00 _ Split Dataset</strong></a>中。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="26f9" class="my kw iq nq b gy nu nv l nw nx">import tarfile<br/>fname = "/MyDrive/Colab Notebooks/fastai/dataset/kkanji.tar"<br/>tar = tarfile.open(fname)<br/>tar.extractall()<br/>tar.close</span></pre><p id="fb62" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">所有的图像都组织在文件夹中，这些文件夹标有汉字字符代码的 unicode。为了更容易处理数据，所有的图像和标签都用下面的函数转换成 NumPy ndarrays。然后 ndarrays 被保存为 npz 文件。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="da3c" class="my kw iq nq b gy nu nv l nw nx">import pathlib<br/>my_dir = "/content/kkanji"</span><span id="d387" class="my kw iq nq b gy ny nv l nw nx">#Dummy black image/label to setup the ndarray<br/>imgs = np.zeros((64,64), np.uint8).reshape(1,64,64) <br/>labels = np.array(['XXX'])</span><span id="0890" class="my kw iq nq b gy ny nv l nw nx">for item in pathlib.Path(my_dir).glob('**/*.png'):<br/>  image = np.array(Image.open(item)).reshape(1,64,64)<br/>  imgs = np.concatenate([imgs,image])<br/>  parent = os.path.dirname(item).split('/')[-1]<br/>  labels = np.concatenate([labels,np.array([parent])])</span><span id="76cf" class="my kw iq nq b gy ny nv l nw nx"># Delete the dummy picture<br/>imgs = np.delete(imgs,0,0)<br/>labels = np.delete(labels,0,0)</span><span id="1777" class="my kw iq nq b gy ny nv l nw nx"># Save as npz file<br/>savez_compressed('/content/kkanj-imgs.npz', imgs)<br/>savez_compressed('/content/kkanji-labels.npz', labels)</span></pre><p id="8349" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">不幸的是，该数据集没有官方测试集，因此很难将结果与其他模型进行比较。因此，我们为这个实验创建了一个独特的测试数据集，使用 50/50 分割来分离可用数据。但是有一个问题:数据集非常不平衡，从 1766 个例子到每个类只有一个例子。这意味着对于一个平均的训练/测试分割，每个类至少需要两个例子。结果，815 个汉字字符必须从数据集中删除，因为这些类只有一个可用的例子。</p><p id="9f31" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">使用标签(unicode)创建数据帧，并使用<strong class="lp ir"> value_counts() </strong>计算标签在数据帧中出现的次数(出现次数)。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="be80" class="my kw iq nq b gy nu nv l nw nx"># Create a DataFrame with unicode and occurrence<br/>unicode = pd.DataFrame(y,columns=['unicode'])<br/>values = pd.DataFrame(unicode.value_counts(),columns=['occurrence'])<br/>df = pd.merge(unicode,values,on ='unicode')<br/>df</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/63b8711f6aca57799e6aea405f40c282.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*1MhPSOnHctQ37caYtpNFVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个 unicode 类出现的数据帧</p></figure><p id="4c43" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">然后选择类别出现次数高于 1 的索引列表，并创建新的精简数据集。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="e3da" class="my kw iq nq b gy nu nv l nw nx"># Indexes with occurrence &gt; 1<br/>select_indices = list(np.where(df['occurrence'] &gt; 1)[0])</span><span id="d54b" class="my kw iq nq b gy ny nv l nw nx"># Reduce the dataset. Characters with occurrence of only 1 are excluded<br/>X_reduced = X[select_indices]<br/>y_reduced = y[select_indices]</span><span id="b035" class="my kw iq nq b gy ny nv l nw nx"># Print shape<br/>print(f"X_reduced shape: {X_reduced.shape}")<br/>print(f"y_reduced shape: {y_reduced.shape}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a09146c077a991786fe4a305565ee081.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*Hsslz7mGBfNOIHw7uqXsBQ.png"/></div></figure><p id="aa64" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">之后，使用<strong class="lp ir"> train_test_split </strong>创建了一个训练/测试数据集。使用<strong class="lp ir">分层</strong>参数来确保所有等级的比例保持不变是很重要的。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c928" class="my kw iq nq b gy nu nv l nw nx">from sklearn.model_selection import train_test_split<br/>testSize = 0.5</span><span id="96ae" class="my kw iq nq b gy ny nv l nw nx">x_train, x_test, y_train, y_test = train_test_split(X_reduced, y_reduced, test_size=testSize, random_state=1, stratify=y_reduced)</span><span id="fd1f" class="my kw iq nq b gy ny nv l nw nx"># Print shape<br/>print(f"Training Images: {x_train.shape}")<br/>print(f"Test Images: {x_test.shape}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/ca09d51deaea568ec0b28867c3e8a07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*cfvvkMV5lZ4Sqoj2JRqa9g.png"/></div></div></figure><p id="c378" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">下面的图 1 显示了用于创建训练和测试数据集的步骤的概述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/09d2df5dcd59f40a5154f4c7eca02f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPbIE07f5Zp-brb9BOa6ZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 1</p></figure><p id="f8d3" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">GitHub 有 100 MB 的数据存储限制，因此所有数据集都可以在<strong class="lp ir"> Internxt </strong>上下载:</p><p id="3046" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated"><strong class="lp ir">原始数据集</strong><br/><a class="ae mj" href="https://drive.internxt.com/0874e2b0b393f3f1c529" rel="noopener ugc nofollow" target="_blank">kk Anji-Original-labels . npz</a><br/><a class="ae mj" href="https://drive.internxt.com/47ce631dc463a7f3d68b" rel="noopener ugc nofollow" target="_blank">kk Anji-Original-imgs . npz</a></p><p id="3311" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated"><strong class="lp ir">训练/测试数据集</strong><br/><a class="ae mj" href="https://drive.internxt.com/ab823b0528b50f87ae06" rel="noopener ugc nofollow" target="_blank">kkan Ji-train-imgs . npz</a><br/><a class="ae mj" href="https://drive.internxt.com/2ab2a2954587dde5b7fd" rel="noopener ugc nofollow" target="_blank">kkan Ji-train-labels . npz</a><br/><a class="ae mj" href="https://drive.internxt.com/b27946700a4e5b63fa4c" rel="noopener ugc nofollow" target="_blank">kkan Ji-Test-imgs . npz</a><br/><a class="ae mj" href="https://drive.internxt.com/a474a9e8fe5837397d90" rel="noopener ugc nofollow" target="_blank">kkan Ji-Test-labels . npz</a></p><p id="7e55" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">接下来，重要的是处理数据集中严重不平衡的类的问题。</p><h2 id="2785" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">不平衡数据</h2><p id="ffab" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">不平衡的数据集可能会使创建良好的机器学习模型变得非常困难，因为通常，ML 算法是在假设每个类的示例数量相等的情况下设计的。杰森·布朗利对这个话题做了很好的概述。</p><p id="c545" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">克服这个问题的一个方法是对数据进行重新采样，正如艾米丽·波蒂拉杰·⁴.所描述的图 2 <strong class="lp ir"> </strong>给出了重采样过程的概述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/7d45a17368cafc5a83db45920af3a6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpCYOP47qPaI-vW7gn0GTg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 2</p></figure><p id="d5d1" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">最终数据集可以在这里下载:<br/><a class="ae mj" href="https://drive.internxt.com/67b0aeda2a76a264e016" rel="noopener ugc nofollow" target="_blank">kk Anji-final-test-imgs</a><br/><a class="ae mj" href="https://drive.internxt.com/ed547780a075cc43b8cc" rel="noopener ugc nofollow" target="_blank">kk Anji-final-test-labels</a><br/><a class="ae mj" href="https://drive.internxt.com/1ca7de49cc6e3480a101" rel="noopener ugc nofollow" target="_blank">kk Anji-final-train-imgs</a><br/><a class="ae mj" href="https://drive.internxt.com/8b75117a9b272d0db7f9" rel="noopener ugc nofollow" target="_blank">kk Anji-final-train-labels</a></p><p id="ec02" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">数据重采样的完整代码在<strong class="lp ir"> </strong>笔记本<a class="ae mj" href="https://github.com/cgn-shinkirou/kkanji/blob/main/01_ResampleData.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir">01 _ 重采样数据</strong> </a>中。使用的阈值是 150，这意味着所有的类都以这样一种方式被重新采样，即每个类恰好由 150 个例子组成。如果一个类的实例多于阈值，则使用<strong class="lp ir"> train_test_split </strong>随机选择 150 个实例，并用于训练数据集(此处:<strong class="lp ir"> df_upsampled </strong>)。其余的例子被插入到测试数据集 2 中(这里:<strong class="lp ir"> df_rest_upsampled </strong>)。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="eb9d" class="my kw iq nq b gy nu nv l nw nx"># indexes of the labels<br/>labelIndexes= df.index.to_numpy()</span><span id="90f3" class="my kw iq nq b gy ny nv l nw nx"># train and test split --&gt; indexes<br/>training, testing, _, _ = train_test_split(labelIndexes,labelIndexes, test_size=150, random_state=42)</span><span id="401b" class="my kw iq nq b gy ny nv l nw nx"># 150 randomly selected rows  <br/>df_upsampled = new_x_train.iloc[testing]</span><span id="642a" class="my kw iq nq b gy ny nv l nw nx"># remaining rows, used for test dataset 2<br/>df_rest_upsampled = new_x_train.iloc[training]</span></pre><p id="cc34" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">一个例子较少的类，通过随机复制已有的图片来扩充。来自 sklearn 库的<a class="ae mj" href="https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html" rel="noopener ugc nofollow" target="_blank"> <em class="og">重采样</em> </a> <em class="og"> </em>被用于此。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="2056" class="my kw iq nq b gy nu nv l nw nx"># resample to 150<br/>df_upsampled = resample(df,random_state=42,n_samples=150,replace=True)</span></pre><p id="32f2" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">使用与用于训练输入的图像相同的图像可能不是创建执行机器学习模型的好主意。由于这个原因，在训练阶段后期使用数据扩充来转换训练图像。</p><p id="de14" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">利用这种方法，产生了新的重采样训练数据集，该数据集具有 452，550 幅图像和 3，017 个汉字字符类，其中每个类由正好 150 个例子组成。测试数据集 2 和测试数据集 1 被组合在具有 84，381 个图像的最终测试数据集中。</p><h1 id="4d3e" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">培养</h1><p id="6998" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">模型训练的代码可在笔记本<a class="ae mj" href="https://github.com/cgn-shinkirou/kkanji/blob/main/02_kkanji%20model.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lp ir"> 02_kkanji 模型</strong> </a>中找到。为了准备数据和训练模型，fastai 需要一个 DataLoaders 对象，它包含一个训练数据加载器和一个验证数据加载器。这方面的挑战是如何使用 NumPy ndarrays 将数据输入 DataLoaders 对象。大多数 fastai 示例通过路径位置传递数据。这个数据加载器对象的创建是基于 Nghia Ho⁵.公司的代码</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a7ed" class="my kw iq nq b gy nu nv l nw nx">def make_dataloaders_from_numpy_data(image, label):<br/>    def pass_index(idx):<br/>        return idx</span><span id="0638" class="my kw iq nq b gy ny nv l nw nx">    def get_x(i):<br/>        return image[i]</span><span id="8a06" class="my kw iq nq b gy ny nv l nw nx">    def get_y(i):<br/>        return label[i]</span><span id="1426" class="my kw iq nq b gy ny nv l nw nx">    dblock = DataBlock(<br/>        blocks=(ImageBlock, CategoryBlock),<br/>        get_items=pass_index,<br/>        splitter=IndexSplitter(test_index), # stratified split<br/>        get_x=get_x,<br/>        get_y=get_y,<br/>        item_tfms = Resize(IMAGESIZE),<br/>        batch_tfms=aug_transforms(do_flip=False, p_affine=1) <br/>        )<br/>    <br/>   # pass in a list of index<br/>    num_images = image.shape[0]<br/>    dls = dblock.dataloaders(list(range(num_images)))<br/>    return dls</span><span id="13e0" class="my kw iq nq b gy ny nv l nw nx">dls = make_dataloaders_from_numpy_data(x_train, y_train)</span></pre></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><pre class="np nq nr ns aw nt bi"><span id="3a1e" class="my kw iq nq b gy oo op oq or os nv l nw nx"># StratifiedShuffleSplit for the dataloaders object. This ensures that test and validation dataset include all Kanji characters</span><span id="9e78" class="my kw iq nq b gy ny nv l nw nx">from sklearn.model_selection import StratifiedShuffleSplit</span><span id="c9f4" class="my kw iq nq b gy ny nv l nw nx">sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=2018)</span><span id="92a0" class="my kw iq nq b gy ny nv l nw nx">for train_index, test_index in sss.split(x_train, y_train):<br/>    X_trainfin, X_testfin = x_train[train_index],x_train[test_index]<br/>    y_trainfin, y_testfin = y_train[train_index],y_train[test_index]</span></pre><p id="e161" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">关于自定义数据块，需要注意几个重要事项:</p><ul class=""><li id="b5af" class="mk ml iq lp b lq nk lt nl lw ot ma ou me ov mi mp mq mr ms bi translated"><strong class="lp ir"> splitter </strong>:默认情况下，DataBlock 随机分割数据用于训练和验证，因此训练集可能包含不在验证集中的类，反之亦然。为了避免这种情况，使用了<strong class="lp ir"> stratifiedShuffleSplit </strong>为验证集创建一个索引拆分，并将其作为参数传递到<strong class="lp ir"> IndexSplitter </strong>中。</li><li id="52c6" class="mk ml iq lp b lq mt lt mu lw mv ma mw me mx mi mp mq mr ms bi translated"><strong class="lp ir"> item_tfms </strong>:所有图像的大小从 64 调整到 224，因为它改进了模型。</li><li id="d91b" class="mk ml iq lp b lq mt lt mu lw mv ma mw me mx mi mp mq mr ms bi translated"><strong class="lp ir"> batch_tfms </strong>:使用 aug_transforms，图像被转换以创建一组更加多样化的图像，从而改进模型。我设置<strong class="lp ir"> do_flip=False </strong>不翻转图像，设置<strong class="lp ir"> p_affine=1 </strong>，这增加了应用每个变换和对称扭曲的概率。其他变换应用其默认值。</li></ul><p id="1075" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">使用<strong class="lp ir"> show_batch </strong>和参数<strong class="lp ir"> unique=True </strong>，可以检查增强的训练图像看起来像什么。这些图像稍加变换，就可以得到一个具有更好预测能力的模型(图 3)。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a919" class="my kw iq nq b gy nu nv l nw nx"># Display various versions of augmented transformed images<br/>dls.train.show_batch(max_n=50, nrows=10, unique=True,cmap='gray')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/4de5249b9dedc2d9dd37d4cb5293a127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wY-GhiEzHQakTX8bwTvRBg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 3:增强图像</p></figure></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="6c95" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">为了开始训练，一个带有<strong class="lp ir"> Resnet152 </strong>模型的<strong class="lp ir"> cnn_learner </strong>被创建。训练了 10 个纪元，学习率为 0.01。</p><p id="bf95" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">尝试了不同的设置，例如不同的学习率，滑动学习率在训练阶段给出了稍好的结果。最终，在 10 个时期的训练后，以 0.01 的固定学习率实现了最低的训练和有效损失。培训总时间约为 13 小时。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="fd43" class="my kw iq nq b gy nu nv l nw nx"># Create a Learner object<br/>learn = cnn_learner(dls, model, metrics=error_rate)</span><span id="c124" class="my kw iq nq b gy ny nv l nw nx"># Train a model for 10 epochs<br/>learn.fit_one_cycle(10, lr_max=0.01)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/9ac2fca37599fd945578f4ca9259e4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*bf-wzNR9UNq-uX8oyDlr5Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练时代</p></figure><h1 id="f18b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结果</h1><p id="5b6d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如下所示，该模型在测试数据集上生成了出色的结果。在 84，381 个样本中，有 4，830 个样本被错误分类，导致 F1 总分为 0.9416。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/d7aabd6dd97d7507f6ff46fc40850ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*hOb_NZ3j3y-IMjVkbeGhNQ.png"/></div></figure><p id="5fd9" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">但是这个模型在不同的发生范围内会有怎样的表现呢？如前所述，数据集最初非常不平衡，因此，训练数据集中的每个类都被重新采样为 150 的固定大小。这意味着有些字符在训练数据集中只存在过一次(出现次数= 1 ),并且被复制了 149 次。因此，这就引出了一个问题，即该模型是否也能在被人为扩展的类上表现良好。</p><p id="c0b1" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">下面的表 1 给出了<strong class="lp ir"> F1 分数</strong>、<strong class="lp ir">准确度</strong>、<strong class="lp ir">示例计数数量</strong>、<strong class="lp ir">独特汉字字符数量</strong>以及<strong class="lp ir">出现范围</strong>的概述。它显示了模型在这些不同范围内的表现。正如所料，F1 分数对于出现范围非常低的字符来说非常差。只有一个例子的字符类的 F1 值为 0.5207。但是，随着事件的增多，分数会提高。如果一个字符在训练数据集中出现至少六次，F1 值就会超过 0.91。这表明创建高性能的分类器不需要大量的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2c9f85836c49b51a30c157aaa21ca546.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*K4XZJW28FEkVndbaWSdx0w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表 1:结果概述</p></figure><h2 id="3bf1" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">预测错误的汉字</h2><p id="accd" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><strong class="lp ir">图 4 </strong>显示了使用辅助功能<em class="og"> display_images </em>生成的错误预测汉字字符的图像。每个图像都标有实际字符、预测字符和预测概率。在某些情况下，可以看出实际的和预测的汉字是相似的，因此容易被错误分类。在其他情况下，这可能意味着标签不正确。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/b082eb0daadd8c27ab02dd783f2feb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2gLH7_nbmiBbMVCYZCobQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图 4:标签不正确的汉字</p></figure><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="44b2" class="my kw iq nq b gy nu nv l nw nx"># Helper function to display Kanji and label<br/>def display_images (x_figure,y_figure, prediction, probability):<br/>  image_views = len(y_figure)<br/>  columns = 10<br/>  rows = np.ceil(image_views / columns)<br/>  for counter in range(0,image_views):<br/>    plt.subplot(rows,columns,counter + 1)<br/>    plt.xticks([])<br/>    plt.yticks([])<br/>    plt.grid(False)<br/>    plt.xlabel('A:' + str(char_df.iloc[y_figure[counter],3]) + ' P:' + str(char_df.iloc[prediction[counter],3]) + 'Pr:' + str(probability[counter]),fontproperties=fprop,fontsize=12)<br/>    plt.imshow(x_figure[counter],cmap=plt.cm.binary)</span></pre></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><pre class="np nq nr ns aw nt bi"><span id="3619" class="my kw iq nq b gy oo op oq or os nv l nw nx"># Display Kanji characters<br/>figure, _ = plt.subplots(figsize=(20,10))<br/>figure.suptitle('Incorrectly labeled Kanji - A: Actual Kanji / P: Predicted Kanji / Pr: Probability', fontsize=16)<br/>figure.tight_layout(pad=3.0)<br/>display_images(x_figure,y_figure,prediction, probability)</span></pre><h1 id="441b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="1ddc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如结果所示，不平衡数据集的问题可以通过对训练数据集进行重采样来简单地克服。还证明了由于 fastai 库，创建高性能模型不需要一个类的大量数据。</p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><h2 id="2927" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">参考</h2><p id="65ad" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Tarin Clanuwat，Mikel Bober-Irizar，Asanobu 北本，Alex Lamb，Kazuaki Yamamoto，David Ha，日本古典文学深度学习，可从<a class="ae mj" href="https://arxiv.org/abs/1812.01718" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.01718</a>获得。</p><p id="e2da" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">该数据集可从<a class="ae mj" href="https://github.com/rois-codh/kmnist" rel="noopener ugc nofollow" target="_blank">https://github.com/rois-codh/kmnist</a>获得，受许可<a class="ae mj" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a>许可，可自由共享和改编。</p><p id="5c73" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">Jason Brownlee，对不平衡分类的温和介绍，机器学习掌握，可从<a class="ae mj" href="https://machinelearningmastery.com/what-is-imbalanced-classification/," rel="noopener ugc nofollow" target="_blank">https://machinelementmastery . com/what-is-unbalanced-Classification/，</a>2021 年 12 月 6 日访问。</p><p id="4aec" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">⁴·艾米丽·波蒂拉杰(沃特金斯)，改善图像数据类别不平衡的 4 种方法，走向数据科学，可从<a class="ae mj" rel="noopener" target="_blank" href="/4-ways-to-improve-class-imbalance-for-image-data-9adec8f390f1">https://towardsdatascience . com/4-ways-to-improve-class-unbalancy-for-image-data-9 adec 8 f 390 f 1</a>获得，2021 年 12 月 2 日访问。</p><p id="4334" class="pw-post-body-paragraph ln lo iq lp b lq nk jr ls lt nl ju lv lw nm ly lz ma nn mc md me no mg mh mi ij bi translated">http://nghiaho.com/?p=2741<a class="ae mj" href="http://nghiaho.com/?p=2741" rel="noopener ugc nofollow" target="_blank"/>发布，2021 年 11 月 3 日访问。</p></div></div>    
</body>
</html>