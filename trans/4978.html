<html>
<head>
<title>Breaking it Down: K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分解它:K-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/breaking-it-down-k-means-clustering-e0ef0168688d#2022-11-06">https://towardsdatascience.com/breaking-it-down-k-means-clustering-e0ef0168688d#2022-11-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7c84" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用NumPy和scikit-learn探索和可视化K-means聚类的基本原理。</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="333d" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">Outline:</strong><br/><a class="ae kx" href="#1791" rel="noopener ugc nofollow">1. What is K-Means Clustering?</a><br/><a class="ae kx" href="#f947" rel="noopener ugc nofollow">2. Implementing K-means from Scratch with NumPy</a><br/>   <a class="ae kx" href="#96e1" rel="noopener ugc nofollow">1. K-means++ Cluster Initialization</a><br/>   <a class="ae kx" href="#d0b4" rel="noopener ugc nofollow">2. K-Means Function Differentiation</a><br/>   <a class="ae kx" href="#ea21" rel="noopener ugc nofollow">3. Data Labeling and Centroid Updates</a><br/>   <a class="ae kx" href="#9b4e" rel="noopener ugc nofollow">4. Fitting it Together</a><br/><a class="ae kx" href="#e5af" rel="noopener ugc nofollow">3. K-Means for Video Keyframe Extraction: Bee Pose Estimation</a><br/><a class="ae kx" href="#aabf" rel="noopener ugc nofollow">4. Implementing K-means with </a><a class="ae kx" href="#aabf" rel="noopener ugc nofollow">scikit-learn</a><br/><a class="ae kx" href="#8571" rel="noopener ugc nofollow">5. Summary</a><br/><a class="ae kx" href="#f720" rel="noopener ugc nofollow">6. Resources</a></span></pre><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ky"><img src="../Images/c51967791e75b1f510f67d91eea0eb37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OE9JOSlm7yViIQMz-hsN2Q.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">文章概述</p></figure><p id="6fa6" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">参见我的GitHub <a class="ae kx" href="https://github.com/JacobBumgarner/learning-repo" rel="noopener ugc nofollow" target="_blank"> learning-repo </a>获取这篇文章背后的所有代码。</p><h1 id="1791" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">1.什么是K-Means聚类？</h1><p id="eb85" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">k均值聚类是一种算法，用于将数据分类到用户定义数量的组中，<em class="nc"> k </em>。K-means是一种无监督的机器学习形式，这意味着在运行算法之前，输入数据没有标签。</p><p id="110e" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">由于各种原因，使用k-means等算法对数据进行聚类是有价值的。首先，聚类用于在构建数据分析管道时识别未标记数据集中的独特组。这些标签对于数据检查、数据解释和训练AI模型是有用的。K-means及其变体在各种上下文中使用，包括:</p><ul class=""><li id="92a1" class="nd ne it lm b ln lo lq lr lt nf lx ng mb nh mf ni nj nk nl bi translated"><strong class="lm iu">研究。</strong>例如，对单细胞RNA测序结果进行分类<a class="ae kx" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008625" rel="noopener ugc nofollow" target="_blank"> </a></li><li id="7da5" class="nd ne it lm b ln nm lq nn lt no lx np mb nq mf ni nj nk nl bi translated"><strong class="lm iu">计算机科学。</strong>例如，对电子邮件进行聚类以检测和过滤垃圾邮件<a class="ae kx" href="https://www.semanticscholar.org/paper/Spam-Filtering-using-K-mean-Clustering-with-Local-Sharma-Rastogi/901af90a3bf03f34064f22e3c5e39bbe6a5cf661?p2df" rel="noopener ugc nofollow" target="_blank"> </a></li><li id="eed1" class="nd ne it lm b ln nm lq nn lt no lx np mb nq mf ni nj nk nl bi translated"><strong class="lm iu">营销。</strong>例如，针对信用卡广告的客户群细分<a class="ae kx" href="https://www.kaggle.com/code/muhammadshahzadkhan/bank-customer-segmentation-pca-kmeans" rel="noopener ugc nofollow" target="_blank"> <strong class="lm iu"> </strong> </a></li></ul><h1 id="f947" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">2.用NumPy从头开始实现K-Means</h1><p id="7f8e" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">为了获得对k-means如何工作的基本理解，我们将检查算法的每个步骤。我们将通过可视化的解释和用NumPy从头构建一个模型来实现这一点。</p><p id="0138" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">k-means背后的算法和数学函数很漂亮，但相对简单。让我们从一个概述开始:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">K-Means算法简介</p></figure><p id="7992" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">总之，k-means算法有三个步骤:</p><ol class=""><li id="4d67" class="nd ne it lm b ln lo lq lr lt nf lx ng mb nh mf nt nj nk nl bi translated">指定初始聚类中心(质心)位置</li><li id="82c3" class="nd ne it lm b ln nm lq nn lt no lx np mb nq mf nt nj nk nl bi translated">基于最近的质心标注数据</li><li id="8b03" class="nd ne it lm b ln nm lq nn lt no lx np mb nq mf nt nj nk nl bi translated">将质心移动到新标记的数据点的平均位置。返回步骤2，直到聚类中心收敛。</li></ol><p id="e17f" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">让我们继续构建模型。为了使用该算法，我们需要编写以下函数:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">K-Means类函数概述</p></figure><h2 id="96e1" class="kr ks it bd mh nv nw dn ml nx ny dp mp lt nz oa mr lx ob oc mt mb od oe mv of bi translated">2.1.集群初始化</h2><p id="ee0a" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">k-means算法的第一步是让用户选择数据应该被聚类到的组的数量，<em class="nc"> k </em>。</p><p id="2ec6" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">在算法的原始实现中，一旦选择了<em class="nc"> k </em>，将通过随机选择输入数据点的<em class="nc"> k </em>作为质心起始位置来初始化聚类中心(或<em class="nc">质心</em>)的初始位置。</p><p id="e764" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">这种方法被证明是非常低效的，因为开始的质心位置可能最终彼此随机接近。2006年，亚瑟和瓦西维茨基<a class="ae kx" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank"> ⁴ </a>开发了一种新的更有效的质心初始化方法。他们在2007年发表了他们的方法，称之为<strong class="lm iu"> k-means++ </strong>。</p><p id="8ada" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated"><strong class="lm iu"> k-means++ </strong>不是随机选择初始质心，而是基于距离分布有效地选择位置。让我们想象一下它是如何工作的:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">K-Means ++质心初始化</p></figure><p id="21e2" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">既然k-means++背后的直觉已经暴露出来，让我们为它实现函数:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">k-Means<strong class="ak">_ init _ centroids _ plus plus</strong>函数</p></figure><p id="4b42" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">值得注意的是，除了必须手动选择<em class="nc"> k </em>之外，还可以使用几种无偏技术来确定最佳数字。<a class="og oh ep" href="https://medium.com/u/78433997a4a?source=post_page-----e0ef0168688d--------------------------------" rel="noopener" target="_blank"> Khyati Mahendru </a>在她的文章中解释了其中两种方法，即<strong class="lm iu">肘</strong>和<strong class="lm iu">剪影方法</strong> <a class="ae kx" href="https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb" rel="noopener">。值得一读！</a></p><h2 id="d0b4" class="kr ks it bd mh nv nw dn ml nx ny dp mp lt nz oa mr lx ob oc mt mb od oe mv of bi translated">2.2.数据标注和质心更新</h2><p id="3020" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">质心初始化之后，算法进入数据标记和质心位置更新的迭代过程。</p><p id="a222" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">在每次迭代中，输入数据将首先根据其与质心的接近程度进行标注。在此之后，每个质心的位置将被更新为其聚类中数据的平均位置。</p><p id="58b7" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">这两个步骤将重复进行，直到标签分配/质心位置不再改变(或<em class="nc">收敛</em>)。让我们想象一下这个过程:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="0781" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">现在，让我们实现数据标签代码:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">k-Means<strong class="ak">_ compute _ labels</strong>函数</p></figure><p id="d349" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">最后，我们将实现质心位置更新功能:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">k-Means<strong class="ak">_更新_质心_位置</strong>功能</p></figure><h2 id="ea21" class="kr ks it bd mh nv nw dn ml nx ny dp mp lt nz oa mr lx ob oc mt mb od oe mv of bi translated">2.3.k-均值函数微分</h2><p id="2752" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">k-means算法的第三步是更新质心的位置。我们看到这些质心被更新到所有聚类的标记点的平均位置。</p><p id="e800" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">将质心更新到平均聚类位置似乎很直观，但是这一步背后的数学原理是什么？基本原理在于k-means方程的微分。</p><p id="bed6" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">让我们通过探索k-means函数微分的动画证明来展示这种直觉。该证明表明位置更新是旨在最小化组内<em class="nc">方差</em>的k均值方程的结果。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">k-均值函数微分</p></figure><h2 id="9b4e" class="kr ks it bd mh nv nw dn ml nx ny dp mp lt nz oa mr lx ob oc mt mb od oe mv of bi translated">2.4装配在一起</h2><p id="910d" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">现在我们已经为k-means模型构建了主干函数，让我们将它结合到一个单一的<code class="fe oi oj ok kn b">fit</code>函数中，该函数将使我们的模型适合输入数据。我们还将在这里定义<code class="fe oi oj ok kn b">__init__</code>函数:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">k-表示<strong class="ak">拟合</strong>函数</p></figure><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nu ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">K-Means <strong class="ak"> __init__ </strong>函数</p></figure><p id="abc7" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">现在我们可以把这个模型和我的漫游笔记本<a class="ae kx" href="https://github.com/JacobBumgarner/learning-repo/blob/main/k_means/k_means_walkthrough.ipynb" rel="noopener ugc nofollow" target="_blank">一起使用了。这个笔记本使用合成生成的数据(如上面的视频所示)来演示我们新编写的</a><a class="ae kx" href="https://github.com/JacobBumgarner/learning-repo/blob/main/k_means/k_means.py" rel="noopener ugc nofollow" target="_blank"> k_means.py </a>代码的功能。</p><h1 id="e5af" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">3.视频关键帧提取的k-均值算法:蜜蜂姿态估计</h1><p id="d49c" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">太棒了——我们已经完全从零开始构建了k-means模型。与其将这些代码扔到一边，不如让我们在一个示例场景中使用它们。</p><p id="4c01" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">在过去的几年里，神经科学和DL研究社区取得了令人印象深刻的进步，实现了高度准确和自动化的动物行为跟踪和分析*。该研究领域中使用的框架实现了各种卷积神经网络架构。这些模型还严重依赖迁移学习来减少研究人员需要生成的训练数据量。这些框架的两个流行例子包括<a class="ae kx" href="https://github.com/DeepLabCut/DeepLabCut" rel="noopener ugc nofollow" target="_blank"> DeepLabCut </a>和<a class="ae kx" href="https://github.com/talmolab/sleap" rel="noopener ugc nofollow" target="_blank"> SLEAP </a>。</p><blockquote class="ol om on"><p id="40ef" class="lk ll nc lm b ln lo ju lp lq lr jx ls oo lu lv lw op ly lz ma oq mc md me mf im bi translated">* <em class="it">边注:这个子域通常被称为</em> <strong class="lm iu">计算神经行为学</strong></p></blockquote><p id="316e" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">为了训练模型来自动跟踪动物身上的特定点，研究人员通常必须从他们的行为视频中手动标记100-150个独特的帧。考虑到所有的事情，这是一个非常小的数字，可以无限期地自动跟踪<em class="nc"/><em class="nc"/>长的行为视频！</p><p id="3e05" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">然而，当标记这些训练帧时，研究人员必须考虑的一个重要方面是，它们应该尽可能地互不相同。如果存在数小时的记录，给一个视频的前5秒贴标签是非常没有目的的。这是因为动物在前5秒的行为和身体状态可能不会准确地代表整个视频数据集的特征。因此，该模型将不会被训练成有效地识别各种特征。</p><p id="19ad" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">那么这和k均值有什么关系呢？无需从视频中手动识别唯一的关键帧，可以实现k-means等算法来自动将视频帧聚类到唯一的组中。让我们想象一下这是如何工作的:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div><p class="lg lh gj gh gi li lj bd b be z dk translated">视频关键帧提取的k-均值算法</p></figure><p id="a522" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">为了获得对这个过程的实际理解，您可以使用<a class="ae kx" href="https://github.com/JacobBumgarner/learning-repo/blob/main/k_means/kmeans_frame_selection_walkthrough.ipynb" rel="noopener ugc nofollow" target="_blank">我的漫游笔记本</a>跟随用于隔离这些帧的代码。</p><h1 id="aabf" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">4.用scikit-learn实现K-means</h1><p id="7b92" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">在现实世界中，除非必要，否则通常应该避免实现自己构造的算法。相反，我们应该依靠由专家付费和志愿者贡献者维护的精心有效设计的框架。</p><p id="6b4a" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">在这个实例中，让我们看看用scikit-learn实现k-means有多容易。这个类的文档可以在<a class="ae kx" href="http://4. Implementing K-means with scikit-learn" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="or ns l"/></div></figure><p id="a1bf" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">模型初始化和拟合的scikit-learn实现与我们的非常相似(不是巧合！)，但是我们必须跳过大约250行的代码。此外，scikit-learn框架为k-means实现了<a class="ae kx" href="https://github.com/scikit-learn/scikit-learn/blob/60f16feaadaca28f9a1cc68d2f406201860d27e8/sklearn/cluster/_k_means_lloyd.pyx#L186-L190" rel="noopener ugc nofollow" target="_blank">优化的BLAS例程</a>，使得它们的实现<em class="nc">比我们的</em>快得多。</p><p id="4530" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">长话短说——从零开始学习是无价的，但从零开始工作却不是。</p><h1 id="8571" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">5.摘要</h1><p id="aa50" class="pw-post-body-paragraph lk ll it lm b ln mx ju lp lq my jx ls lt mz lv lw lx na lz ma mb nb md me mf im bi translated">在这篇文章中，我们探索k-means算法背后的数学和直觉的基础。我们使用NumPy从头构建了一个k-means模型，使用它从一个动物行为视频中提取唯一的关键帧，并学习了如何使用scikit-learn实现k-means。</p><p id="5bef" class="pw-post-body-paragraph lk ll it lm b ln lo ju lp lq lr jx ls lt lu lv lw lx ly lz ma mb mc md me mf im bi translated">我希望这篇文章对你有价值！如有任何意见、想法或问题，请随时联系我。</p><h1 id="f720" class="mg ks it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">6.资源</h1><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="d154" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">References:<br/></strong><a class="ae kx" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008625" rel="noopener ugc nofollow" target="_blank">1. Hicks SC, Liu R, Ni Y, Purdom E, Risso D (2021). mbkmeans: Fast clustering for single cell data using mini-batch <em class="nc">k</em>-means. PLoS Comput Biol 17(1): e1008625.</a><br/><a class="ae kx" href="https://www.semanticscholar.org/paper/Spam-Filtering-using-K-mean-Clustering-with-Local-Sharma-Rastogi/901af90a3bf03f34064f22e3c5e39bbe6a5cf661?p2df" rel="noopener ugc nofollow" target="_blank">2. Sharma A, Rastogi V (2014). Spam Filtering using K mean Clustering with Local Feature Selection Classifier. Int J Comput ApplMB means<em class="nc"> </em>108: 35-39.</a><br/><a class="ae kx" href="https://www.kaggle.com/code/muhammadshahzadkhan/bank-customer-segmentation-pca-kmeans" rel="noopener ugc nofollow" target="_blank">3. Muhammad Shahzad, Bank Customer Segmentation (PCA-KMeans)</a><br/><a class="ae kx" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank">4. Arthur D, Vassilvitskii S (2006). k-means++: The Advantages of Careful Seeding. <em class="nc">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</em>. Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. 1027–1035</a></span><span id="1eeb" class="kr ks it kn b gy os ku l kv kw"><strong class="kn iu">Educational Resources:</strong><br/>- <a class="ae kx" href="https://developers.google.com/machine-learning/clustering" rel="noopener ugc nofollow" target="_blank">Google Machine Learning: Clustering</a><br/>- <a class="ae kx" href="http://cs229.stanford.edu/notes2020spring/cs229-notes7a.pdf" rel="noopener ugc nofollow" target="_blank">Andrew Ng, CS229 Lecture Notes, K-Means</a><br/>- <a class="ae kx" href="https://stanford.edu/~cpiech/cs221/handouts/kmeans.html" rel="noopener ugc nofollow" target="_blank">Chris Piech, K-Means</a></span></pre></div></div>    
</body>
</html>