<html>
<head>
<title>Black-box Hyperparameter Optimization in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的黑盒超参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/black-box-hyperparameter-tuning-in-python-478c10adc959#2022-08-12">https://towardsdatascience.com/black-box-hyperparameter-tuning-in-python-478c10adc959#2022-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7cb5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中暴力和黑盒优化方法的比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a3a7375215b5133efed84461c4453fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIqERFvQv4sZXsCjXg3cgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://www.pexels.com/@wdnet/" rel="noopener ugc nofollow" target="_blank"> PhotoMIX公司</a>在<a class="ae ky" href="https://www.pexels.com/photo/documents-on-wooden-surface-95916/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄</p></figure><p id="1dc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习中，超参数是用于控制机器学习模型的学习过程的值。这不同于从数据中学习的内部机器学习模型参数。超参数是机器学习训练数据之外的值，其确定机器学习模型性能的最优性。每个唯一的超参数集对应于一个唯一的机器学习模型。对于大多数现有技术的机器学习模型，所有可能的超参数组合的集合可能变得相当大。幸运的是，大多数机器学习模型包都带有默认的超参数值，可以实现不错的基线性能。这意味着数据科学家或机器学习工程师可以使用开箱即用的模型，而不必一开始就担心超参数选择。这些默认模型通常优于数据科学家或工程师能够手动测试和选择的模型。</p><p id="a679" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，为了优化性能，数据科学家或机器学习工程师必须测试不同于默认值的超参数的各种值。手动执行会变得非常麻烦和低效。由于这个原因，已经设计了许多算法和库来自动化超参数选择的过程。超参数选择是优化中的一个练习，其中目标函数由模型表现的有多差来表示。优化任务是找到一组最佳参数，使机器学习模型的性能下降到最低程度。如果你找到性能最差的机器学习模型，那就对应着性能最好的模型。</p><p id="c481" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文献跨越、蛮力技术和黑盒非凸优化为优化提供了丰富的空间。强力优化是彻底搜索所有可能的超参数组合的最佳参数集的任务。如果有可能彻底搜索超参数空间，它将给出给出全局最优解的超参数集。不幸的是，就计算资源和时间而言，穷举搜索超参数空间通常是不可行的。这是因为超参数调整机器学习模型属于非凸优化的范畴。这是一种优化类型，其中找到全局最优是不可行的，因为它可能陷入几个次优“陷阱”中的一个，也称为局部最小值，这使得算法难以搜索超参数的整个空间。</p><p id="c13b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强力优化的替代方法是黑盒非凸优化技术。黑盒非凸优化算法寻找次优的解决方案，局部最小值(或最大值)，这是基于一些预定义的度量足够优化。</p><p id="cb94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python有暴力优化和黑盒优化的工具。模型选择模块中的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchcv </a>支持强力优化。<a class="ae ky" href="https://rbfopt.readthedocs.io/en/latest/rbfopt_algorithm.html" rel="noopener ugc nofollow" target="_blank"> RBFopt </a> python包是IBM开发的黑盒优化库。它通过使用径向基函数来构建和优化被优化函数的代理模型。它很有用，因为它没有对被优化函数的形状或行为做任何假设。它已被用于优化复杂的模型，如深度神经网络。</p><p id="01cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">构建、测试和比较模型超参数和机器学习算法的任务本质上通常是协作性的。考虑到这一点，我将与<a class="ae ky" href="https://deepnote.com/home" rel="noopener ugc nofollow" target="_blank"> DeepNote </a>合作，这是一个协作数据科学笔记本，使数据科学家可以轻松地在机器学习和数据分析任务上合作。在这里，我们将介绍如何应用这些优化工具来调整分类模型的超参数。我们将考虑预测客户是否不会重复购买的监督机器学习任务，这被称为搅动。我们将使用Kaggle上公开的虚构的<a class="ae ky" href="https://www.kaggle.com/datasets/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">电信公司流失</a>数据集。数据集在<a class="ae ky" href="https://www.apache.org/licenses/LICENSE-2.0" rel="noopener ugc nofollow" target="_blank"> Apache 2.0许可</a>下可以自由使用、修改和共享。</p><p id="2120" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">读入电信客户流失数据</strong></p><p id="c76d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入python pandas库，将我们的数据读入pandas数据框，并显示前五行数据:</p><p id="cf3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进口熊猫作为pd</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6ec5" class="ma mb it lw b gy mc md l me mf">df = pd.read_csv("telco_churn.csv")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mg"><img src="../Images/743664cd4f05ac9e23f196ade81cac6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_tqp5EH2nMDLTw7j"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="d511" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到该数据包含诸如客户ID、性别、老年公民身份等字段。如果我们将光标悬停在左侧的单元格输出上，我们将看到以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mh"><img src="../Images/dedc9432423670209780bb7452561603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K2VTy_7GY8OClk2j"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="1876" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到我们有“客户流失”字段，它对应于客户是否重复购买。值“否”表示客户重复购买，值“是”表示客户停止购买。</p><p id="6073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将建立一个简单的分类模型，将性别、老年人、互联网服务、设备保护、每月费用和总费用作为输入，并预测客户是否会流失。为此，我们需要将我们的分类列转换为机器可读的值，这些值可以作为输入传递给我们的机器学习模型。让我们为性别、老年人、互联网服务和设备保护这样做:</p><p id="175b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">#转换分类列</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="86bb" class="ma mb it lw b gy mc md l me mf">#convert categorical columns<br/>df['gender'] = df['gender'].astype('category')<br/>df['gender_cat'] = df['gender'].cat.codes<br/>df['SeniorCitizen'] = df['SeniorCitizen'].astype('category')<br/>df['SeniorCitizen_cat'] = df['SeniorCitizen'].cat.codes<br/>df['InternetService'] = df['InternetService'].astype('category')<br/>df['InternetService_cat'] = df['InternetService'].cat.codes<br/>df['DeviceProtection'] = df['DeviceProtection'].astype('category')<br/>df['DeviceProtection_cat'] = df['DeviceProtection'].cat.codes</span></pre><p id="82ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们显示结果列:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4e05" class="ma mb it lw b gy mc md l me mf">df[['gender_cat', 'SeniorCitizen_cat', 'InternetService_cat', 'DeviceProtection_cat']].head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/2103573ecd56b23b74e135326425a499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ivPb3g572f8v3o3s"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="77b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还必须对客户流失列做一些类似的事情:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3d84" class="ma mb it lw b gy mc md l me mf">df['Churn'] = df['Churn'].astype('category')<br/>df['Churn_cat'] = df['Churn'].cat.codes</span></pre><p id="41c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来我们需要做的是清理TotalCharges列，用NaN替换无效值，并用TotalCharges的平均值输入NaNs</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="96e4" class="ma mb it lw b gy mc md l me mf">df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], 'coerce')<br/>df['TotalCharges'].fillna(df['TotalCharges'].mean(), inplace=True)</span></pre><p id="8cf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们准备输入和输出。我们将定义一个变量X，它将是一个包含列gender、SeniorCitizen、InternetService、DeviceProtection、MonthlyCharges和TotalCharges的序列。我们的输出将是一个名为Y的变量，它将包含客户流失值:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ceb7" class="ma mb it lw b gy mc md l me mf">#define input and output<br/>X = df[['TotalCharges', 'MonthlyCharges', 'gender_cat', 'SeniorCitizen_cat', 'InternetService_cat', 'DeviceProtection_cat']]<br/>y = df['Churn_cat']</span></pre><p id="093f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们拆分数据进行训练和测试。我们将使用scikit-learn中模型选择模块的训练测试分割方法:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="32ad" class="ma mb it lw b gy mc md l me mf">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)</span></pre><p id="1e1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用默认参数建模</strong></p><p id="62c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将构建一个随机森林分类模型。随机森林算法是一种基于树的集成模型算法，它使用决策树的组合来防止过度拟合。让我们从scikit-learn中的ensemble模块导入随机森林类:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0d4c" class="ma mb it lw b gy mc md l me mf">from sklearn.ensemble import RandomForestClassifier</span></pre><p id="7186" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们定义我们的随机森林分类器模型对象，并使我们的模型适合我们的训练数据。通过将RandomForestClassifier的参数留空，我们定义了一个具有预定义默认参数的模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="1cdd" class="ma mb it lw b gy mc md l me mf">model = RandomForestClassifier()<br/>model.fit(X_train, y_train)</span></pre><p id="d38e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们打印模型的默认参数值。为此，我们只需在模型对象上调用get_params()方法:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c0ec" class="ma mb it lw b gy mc md l me mf">model.get_params()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/87a7f010c0888ec5098a90b89764f42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hIsnYQibdjb53zRj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="9803" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用精度来评估我们的分类模型。对于不平衡分类问题，如流失预测，这是一个很好的选择。让我们来评估一组目标的精确度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4a93" class="ma mb it lw b gy mc md l me mf">from sklearn.metrics import precision_score</span><span id="5a04" class="ma mb it lw b gy mk md l me mf">y_pred_default = model.predict(X_test)</span><span id="1fa7" class="ma mb it lw b gy mk md l me mf">precision = precision_score(y_test, y_pred_default)</span><span id="fb4a" class="ma mb it lw b gy mk md l me mf">precision</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ml"><img src="../Images/5161cccfa57c9239a34e374f210ad6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sOD2wwEhY24dA57j"/></div></div></figure><p id="434c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看如何应用强力网格搜索来找到最佳随机森林分类模型。</p><p id="2dc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用GridSearchCV进行强力优化</strong></p><p id="8b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如GridSearchCv之类的强力搜索方法通过在整个搜索空间中穷举搜索最佳超参数集来工作。首先，让我们从scikit-learn中的模型选择模块导入GridSearchCV方法:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="99e1" class="ma mb it lw b gy mc md l me mf">from sklearn.model_selection import GridSearchCV</span></pre><p id="e2d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们也定义一个字典，我们将使用它来指定我们的参数网格。让我们定义一个估计值范围(决策树从10到100)，决策树的最大深度从5到20，最大特征等于sqrt，标准等于基尼指数(这是用于在决策树中划分组的度量标准:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0df7" class="ma mb it lw b gy mc md l me mf">params = {</span><span id="1492" class="ma mb it lw b gy mk md l me mf">'n_estimators': [10, 100],<br/>'max_features': ['sqrt'],<br/>'max_depth' : [5, 20],<br/>'criterion' :['gini']</span><span id="78fd" class="ma mb it lw b gy mk md l me mf">}</span></pre><p id="fa3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们用参数字典定义网格搜索对象:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c747" class="ma mb it lw b gy mc md l me mf">grid_search_rf = GridSearchCV(estimator=model, param_grid=params, cv= 20, scoring='precision')</span></pre><p id="0303" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并使对象符合我们的训练数据:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6834" class="ma mb it lw b gy mc md l me mf">grid_search_rf.fit(x_train, y_train)</span></pre><p id="19ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那里我们可以显示最佳参数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ac03" class="ma mb it lw b gy mc md l me mf">gscv_params = grid_search_rf.best_params_</span><span id="1218" class="ma mb it lw b gy mk md l me mf">gscv_params</span></pre><p id="28d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用最佳参数重新定义我们的随机森林模型:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9b4c" class="ma mb it lw b gy mc md l me mf">gscv_params = grid_search_rf.best_params_</span><span id="3a95" class="ma mb it lw b gy mk md l me mf">model_rf_gscv = RandomForestClassifier(**gscv_params)</span><span id="57ac" class="ma mb it lw b gy mk md l me mf">model_rf_gscv.fit(X_train, y_train)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/f45b43e78fc990b966d16f6e9c27983c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RpYD_LLaT-Fbal8E"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="126c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来评估一组目标的精确度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="23a7" class="ma mb it lw b gy mc md l me mf">y_pred_gscv = model_rf_gscv.predict(X_test)</span><span id="a025" class="ma mb it lw b gy mk md l me mf">precision_gscv = precision_score(y_test, y_pred_gscv)</span><span id="bdea" class="ma mb it lw b gy mk md l me mf">precision_gscv</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/1997c464b510ead3dea26cb5f8c7e49d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5vCK0mx6IYLrDUcW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="26ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到我们的精度实际上优于默认值。虽然这很好，但对于大范围的参数值和较大的数据集，这种方法可能会变得难以处理。黑盒优化和贝叶斯优化等替代方法是超参数调优的更好选择。</p><p id="2cff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用RBFopt进行黑盒优化</strong></p><p id="829b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们考虑使用RBFopt的黑盒超参数优化。RBFopt的工作原理是使用径向基函数来构建和优化被优化函数的代理模型。这通常用于没有封闭形式表达式且有许多山峰和山谷的函数。这与众所周知的具有封闭形式表达式的简单函数形成对比，例如二次函数或指数函数。</p><p id="4920" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先让我们安装RBFopt:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="5b91" class="ma mb it lw b gy mc md l me mf">%pip install -U rbfopt</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/3606e0f0059344001c8e333b3b1b581b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PWsdGdlANyuMURck"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="aa40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要为我们的模型参数定义一个上限和下限列表。下限列表将包含估计器数量的10和最大深度的5。上限列表将包含100个估计数和20个最大深度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="38a2" class="ma mb it lw b gy mc md l me mf">lbounds = [10, 5]</span><span id="8b96" class="ma mb it lw b gy mk md l me mf">ubounds = [100, 20]</span></pre><p id="fda3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们导入RBFopt和交叉验证方法:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b838" class="ma mb it lw b gy mc md l me mf">import rbfopt</span><span id="82ad" class="ma mb it lw b gy mk md l me mf">from sklearn.model_selection import cross_val_score</span></pre><p id="76de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来我们需要定义我们的目标函数。它将接受n_estimators和max_depth的输入，并为每组参数建立多个模型。对于每个模型，我们将计算并返回精度。我们试图找到使精度最大化的n估计量和max_depth的一组值。由于RBFopt找到最小值，为了找到使精度最大化的参数集，我们将返回精度的负值:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e28c" class="ma mb it lw b gy mc md l me mf">def precision_objective(X):<br/>    n_estimators, max_depth = X<br/>    n_estimators = int(n_estimators)<br/>    max_depth = int(max_depth)<br/>    params = {'n_estimators':n_estimators, 'max_depth': max_depth}<br/>    model_rbfopt = RandomForestClassifier(criterion='gini', max_features='sqrt', **params)<br/>    model_rbfopt.fit(X_train, y_train)<br/>    precision = cross_val_score(model_rbfopt, X_train, y_train, cv=20, scoring='precision')<br/>    return -np.mean(precision)</span></pre><p id="1cea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们指定运行、函数调用和维度的数量:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e488" class="ma mb it lw b gy mc md l me mf">num_runs = 1</span><span id="8c72" class="ma mb it lw b gy mk md l me mf">max_fun_calls = 8</span><span id="49d0" class="ma mb it lw b gy mk md l me mf">ndim = 2</span></pre><p id="5ab1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们只运行8个函数调用。如果你希望运行10个以上的函数调用，你必须安装<a class="ae ky" href="https://github.com/coin-or/Bonmin" rel="noopener ugc nofollow" target="_blank"> bonmin </a>和<a class="ae ky" href="https://github.com/coin-or/Ipopt" rel="noopener ugc nofollow" target="_blank"> ipopt </a>软件包。安装说明可以在各自链接的GitHub页面上找到。</p><p id="4cf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们指定我们的目标函数并运行RBFopt:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4580" class="ma mb it lw b gy mc md l me mf">obj_fun = precision_objective</span><span id="84ec" class="ma mb it lw b gy mk md l me mf">bb = rbfopt.RbfoptUserBlackBox(dimension=ndim, var_lower=np.array(lbounds, dtype=np.float), var_upper=np.array(ubounds, dtype=np.float), var_type=['R'] * ndim, obj_funct=obj_fun)</span><span id="e6b1" class="ma mb it lw b gy mk md l me mf">settings = rbfopt.RbfoptSettings(max_evaluations=max_fun_calls)</span><span id="1d3a" class="ma mb it lw b gy mk md l me mf">alg = rbfopt.RbfoptAlgorithm(settings, bb)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/fa3f1b0b464c9944599e933ed99eaa01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z-KsExkXvMRG02E6"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="d195" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将目标值和解决方案存储在它们各自的变量中:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="eacc" class="ma mb it lw b gy mc md l me mf">fval, sol, iter_count, eval_count, fast_eval_count = alg.optimize()</span><span id="d929" class="ma mb it lw b gy mk md l me mf">obj_vals = fval</span></pre><p id="4473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将整数值解存储在字典中:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3f52" class="ma mb it lw b gy mc md l me mf">sol_int = [int(x) for x in sol]<br/>params_rbfopt = {'n_estimators': sol_int[0], 'max_depth': sol_int[1]}<br/>params_rbfopt</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/c8e2f667c95ea24891b7f95f81159647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aI9n0EhOKLPRggsZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="d164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，RBFopt分别为n_estimators和max_depth找到了最佳值81和5。</p><p id="5b7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后将这些最佳参数传递到我们的新模型中，并拟合我们的训练数据:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bbac" class="ma mb it lw b gy mc md l me mf">model_rbfopt = RandomForestClassifier(criterion=’gini’, max_features=’sqrt’, **params_rbfopt)</span><span id="8ced" class="ma mb it lw b gy mk md l me mf">model_rbfopt.fit(X_train, y_train)</span></pre><p id="2e20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并评估精度:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f9d8" class="ma mb it lw b gy mc md l me mf">y_pred_rbfopt = model_rbfopt.predict(X_test)</span><span id="6fe7" class="ma mb it lw b gy mk md l me mf">precision_rbfopt = precision_score(y_test, y_pred_rbfopt)</span><span id="a537" class="ma mb it lw b gy mk md l me mf">precision_rbfopt</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/c913715f07df495294433084af56bd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yno5Z-zvQJcgzOB2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截图</p></figure><p id="911c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，通过更快的优化算法，我们在精度上有了轻微的提高。当您有大的超参数搜索空间时，这尤其有用。</p><p id="368b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章中使用的代码可以在<a class="ae ky" href="https://github.com/spierre91/deepnote/blob/main/hyperparameter_tuning_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><p id="766d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="663b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个数据科学家来说，很好地理解超参数调整机器学习模型的可用工具是必不可少的。虽然大多数机器学习算法的默认超参数提供了良好的基线性能，但超参数调整通常是必要的，以看到基线性能的改善。强力优化技术是有用的，因为它们彻底地搜索超参数空间，这将保证从默认参数改善基线性能。不幸的是，蛮力优化在时间和计算方面是资源密集型的。出于这些原因，更有效的黑盒优化方法，如RBFopt，是强力优化的有用替代方法。RBFopt是一种非常有用的黑盒技术，应该成为每个超参数优化数据科学工具包的一部分。</p></div></div>    
</body>
</html>