<html>
<head>
<title>Multiple linear regression: Theory and applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元线性回归:理论与应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiple-linear-regression-theory-and-applications-677ec2cd04ac#2022-09-20">https://towardsdatascience.com/multiple-linear-regression-theory-and-applications-677ec2cd04ac#2022-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="aa87" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">详细解释了线性最小二乘法，并用Python从头开始实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/69e9aa70cd343613ecde257e0eea85ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NAESAe4nHTVPP2td"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">费迪南·斯托尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="1d9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多元线性回归由于其简单性和结果的可解释性而成为最基本的统计模型之一。出于预测目的，线性模型有时会优于更好的非线性模型，尤其是在训练案例数量少、信噪比低或数据稀疏的情况下(Hastie等人，2009年)。顾名思义，在这些模型中，预测变量(或响应变量)是由预测变量的线性组合来描述的。<em class="ls">倍数</em>一词指的是预测变量。</p><p id="0d0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在整篇文章中，将详细描述普通最小二乘(OLS)回归模型的基本原理，并将从头开始用Python实现一个回归器。所有用到的代码都在这个<a class="ae kv" href="https://github.com/bruscalia/optimization-demo-files/blob/main/regression/notebooks/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ls">示例笔记本</em> </a>里。</p><p id="cde9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性回归已经在许多Python框架中可用。因此，在实践中，不需要从零开始实现它来估计回归系数和进行预测。然而，我们在这里的目标是深入了解这些模型是如何工作的，以及它们的假设在处理未来项目时会更加有效。从通常的框架来看，我建议检查来自<em class="ls"> statsmodels </em>的<em class="ls"> OLS </em>和来自<em class="ls"> sklearn </em>的<em class="ls">线性回归</em>。</p><p id="5657" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们开始吧。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="e3df" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">线性最小二乘法</h1><p id="d882" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在进入方程之前，我想定义一些符号指南。</p><ul class=""><li id="bfd3" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nc nd ne nf bi translated">矩阵:大写斜体粗体。</li><li id="b38c" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">向量:小写斜体粗体。</li><li id="3b9a" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">标量:常规斜体。</li></ul><p id="fe59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多元线性回归模型或OLS可以用下面的等式来描述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/718637d3e3a98b302f8b93fd9f2a2fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*-3zzImuAVGaPDlllyjJEuQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归模型元素表示法。(图片由作者提供)。</p></figure><p id="c936" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="ls"> yᵢ </em>为观察值<em class="ls"> i </em>的因变量(或响应)；<em class="ls">β</em>₀为回归截距，<em class="ls"> βⱼ </em>为与决策变量<em class="ls"> j </em>相关的系数，<em class="ls"> xᵢⱼ </em>为观察值<em class="ls"> i </em>的决策变量<em class="ls"> j </em>，<em class="ls"> ε </em>为剩余项。在矩阵符号中，它可以描述为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/652d25ba53bb31780f5bffd34f6336b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*ynONqdeElvnc0Rkxe2834w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归模型矩阵符号。(图片由作者提供)。</p></figure><p id="b271" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> <em class="ls"> β </em> </strong>为参数的列向量。</p><p id="489a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性模型对结构进行了大量假设，并产生稳定但可能不准确的预测(Hastie等人，2009)。当采用线性模型时，应该了解这些假设，以便对结果做出正确的推断，并执行必要的更改。</p><p id="aca4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">残差项<em class="ls"> ε </em>假设正态独立分布，均值为零，方差为常数<em class="ls"> σ </em>。一些模型属性，比如参数和预测的置信区间，强烈依赖于这些关于<em class="ls"> ε </em>的假设。因此，核实这些数据对于获得有意义的结果至关重要。</p><p id="5eef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性最小二乘回归模型的目标是找到使残差平方和(或误差平方和)最小的<em class="ls"> β </em>的值，由下面的等式给出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6000e454c04dd59f17ea2f41a3c3fbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*-Cff42JOEC1fWggnsQGX-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">误差平方和(残差):最小二乘回归的损失函数。(图片由作者提供)。</p></figure><p id="d3b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个有解析解的优化问题。该公式基于每个预测相对于参数向量<em class="ls"> β </em>的梯度，参数向量对应于自变量向量本身<strong class="ky ir"> <em class="ls"> x </em> </strong> <em class="ls"> ᵢ </em>。考虑一个矩阵<strong class="ky ir"> <em class="ls"> C </em> </strong>，由下面的等式给出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/966f1b230a1beb02385caf9305451d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*5Y_Ka-50FgD71DPnXFeF0g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">c矩阵。(图片由作者提供)。</p></figure><p id="44e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls"> β </em> </strong>的最小二乘估计由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f2ee6f98619cc2f0745357935df20571.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*ka4s-JMnVynM9hkEoiw-Bg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">参数的最小二乘估计。(图片由作者提供)。</p></figure><p id="621d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的步骤中，让我们创建一个Python类，<em class="ls"> LinearRegression，</em>来执行这些估计。但是在此之前，让我们导入一些有用的库和函数，以便在本文中使用。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="6ec5" class="nv mb iq nr b be nw nx l ny nz">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from scipy.stats import shapiro<br/>from scipy.stats import t as t_fun</span></pre><p id="6c6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想考虑截距β₀.，第一步是创建一个估计类和一个方法，在估计矩阵中包含一列1</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="b557" class="nv mb iq nr b be nw nx l ny nz">class LinearRegression:<br/><br/>    def __init__(self, fit_intercept=True):<br/>        self.fit_intercept = fit_intercept<br/><br/>    def _prepare_X(self, X):<br/>        X = np.array(X)<br/>        if len(X.shape) == 1:<br/>            X = np.atleast_2d(X).reshape((-1, 1))<br/>        if self.fit_intercept:<br/>            ones = np.ones((X.shape[0], 1))<br/>            X = np.column_stack((ones, X))<br/>        else:<br/>            pass<br/>        return X</span></pre><p id="279c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们实现一个<em class="ls"> fit </em>方法(<em class="ls"> sklearn </em> -like)来估计<em class="ls"> β。</em></p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="8455" class="nv mb iq nr b be nw nx l ny nz">def fit(self, X, y=None):<br/>    X = self._prepare_X(X)<br/>    n, p = X.shape<br/>    self.n = n<br/>    self.p = p<br/>    C = np.linalg.inv(X.T.dot(X))<br/>    self.C = C<br/>    betas = C.dot(X.T.dot(y))<br/>    self.betas = betas</span></pre><p id="97f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预测的方法。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="3315" class="nv mb iq nr b be nw nx l ny nz">def predict(self, X, y=None):<br/>    X = self._prepare_X(X)<br/>    y_pred = X.dot(self.betas)<br/>    return y_pred</span></pre><p id="5ebd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以及用于计算<em class="ls"> R </em>度量的方法。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="775f" class="nv mb iq nr b be nw nx l ny nz">def r2_score(self, X, y):<br/><br/>    y_pred = self.predict(X)<br/>    epsilon = y_pred - y<br/>    sse = np.sum(epsilon * epsilon)<br/><br/>    y_mean = np.mean(y)<br/>    mean_res = y_mean - y<br/>    sst = np.sum(mean_res * mean_res)<br/><br/>    return 1 - sse / sst</span></pre></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="3b7b" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">参数的统计显著性</h1><p id="b635" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">测试参数<em class="ls"> β </em>的统计显著性以验证预测值的相关性是有用的。当这样做时，我们能够去除不良预测，避免混淆效应，并提高新预测的模型性能。为此，应该测试与给定预测器相关联的参数<em class="ls"> β </em>为零的零假设。让我们利用矩阵<strong class="ky ir"><em class="ls"/></strong>和残差的方差<em class="ls"> σ̂ </em>来计算参数<em class="ls"> β </em>的方差-协方差矩阵<strong class="ky ir"> <em class="ls"> V </em> </strong>及其相应的标准差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a46e2a6765d1f4e6c2a9c71f3a10e862.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*GFoAJ3mJRE3C3jyxfV0UOg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">残差方差。(图片由作者提供)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9c3a3b9e142b3f7976f6515b94ad413f.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*vfhXTzsLY6uzwKtp4rNaNg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">参数的方差协方差矩阵。(图片由作者提供)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/dce6af372211572c217c605870208dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*gFqQTYIzwzTw8xnS7FsrAw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">参数的标准误差。(图片由作者提供)。</p></figure><p id="c415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Python中，我们只需向fit方法添加以下代码行:</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="534d" class="nv mb iq nr b be nw nx l ny nz">epsilon = y - X.dot(betas)<br/>sse = np.sum(epsilon * epsilon)<br/><br/>sigma_squared = sse / (n - p)<br/>self.sigma_squared = sigma_squared<br/><br/>V = C * sigma_squared<br/>self.V = V<br/><br/>sigma_betas = np.sqrt(np.diag(V))<br/>self.sigma_betas = sigma_betas</span></pre><p id="3fc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可能会得到与零假设相关联的<em class="ls">t</em>-值及其对应的<em class="ls">p</em>-值。在Python中，可以通过使用来自<em class="ls"> scipy.stats </em>(之前在这里作为<em class="ls"> t_fun </em>导入)的<em class="ls"> t </em>生成器实例来完成。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="577c" class="nv mb iq nr b be nw nx l ny nz">pvalues = t_fun.sf(abs(betas) / sigma_betas, (n - p)) * 2.0<br/>self.pvalues = pvalues</span></pre><p id="1694" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们已经准备好工具来估计回归系数及其统计意义，并根据新的观察结果进行预测。让我们在下一节应用这个框架。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="6144" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">引线键合示例</h1><p id="cf04" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">这是一个示例，其中的目标是基于导线长度和芯片高度来预测半导体制造过程中导线接合的拉伸强度。它是从Montgomery &amp; Runger (2003年)检索的。这是一个小数据集，在这种情况下，线性模型尤其有用。我将它的一个副本保存在与示例笔记本相同的存储库中的一个. txt文件中。</p><p id="f39e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先导入数据集。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="b516" class="nv mb iq nr b be nw nx l ny nz">dataset = pd.read_csv("../data/montgomery/wire_bond.txt", sep=" ", index_col=0)</span></pre><p id="b282" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后定义自变量<strong class="ky ir"> <em class="ls"> X </em> </strong>的矩阵和预测变量<strong class="ky ir"> <em class="ls"> y </em> </strong>的观测值的向量。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="30b1" class="nv mb iq nr b be nw nx l ny nz">X = dataset.iloc[:, :-1].values<br/>y = dataset.iloc[:, -1].values</span></pre><p id="7ac2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我画了一些散点图，看看预测因子和因变量之间的关系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/57d0599bcd26135ed58e25426b8c3caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmdppXFOQ80qnvodcfh-wQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">引线键合问题中的目标与预测。(图片由作者提供)。</p></figure><p id="f4a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，预测变量和回归器线长度之间存在很强的线性关系。相反，芯片高度和焊线之间的线性关系在成对可视化中并不明显，尽管这可能归因于其他预测因素的影响。</p><p id="c895" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，让我们创建一个<em class="ls"> LinearRegression </em>类的实例，使其适合数据，并基于<em class="ls"> R </em>度量验证其性能。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="62c3" class="nv mb iq nr b be nw nx l ny nz">linreg = LinearRegression()<br/>linreg.fit(X, y)<br/>print(linreg.r2_score(X, y))</span></pre><p id="d06e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它返回值0.9811。</p><p id="734b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好像很有希望！现在，为了验证参数的统计意义，让我们运行以下代码:</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="d693" class="nv mb iq nr b be nw nx l ny nz">for i, val in enumerate(linreg.betas):<br/>    pval = linreg.pvalues[i]<br/>    print(f"Beta {i}: {val:.2f}; p-value: {pval:.2f}")</span></pre><p id="e9bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它返回:</p><pre class="kg kh ki kj gt nq nr oe of aw og bi"><span id="a71b" class="oh mb iq nr b gy oi oj l ok nz">Beta 0: 2.26; p-value: 0.04<br/>Beta 1: 2.74; p-value: 0.00<br/>Beta 2: 0.01; p-value: 0.00</span></pre><p id="aeb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们有一个具有很好的性能和统计意义的模型，只要数据分布没有显著变化，它就可能在新的观察结果上表现良好。注意，根据它的<em class="ls">p</em>-值，我们可能会考虑放弃截距。最后，让我们验证模型假设。</p><ul class=""><li id="e71e" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nc nd ne nf bi translated">残差服从正态独立分布。</li><li id="ba7e" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">残差的平均值为零。</li><li id="e870" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">残差具有恒定的方差。</li></ul><p id="e7aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先验证残差的平均值。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="ea4b" class="nv mb iq nr b be nw nx l ny nz">y_hat = linreg.predict(X)<br/>epsilon = y - y_hat<br/>print(f"Mean epsilon: {np.mean(epsilon):.3f}")<br/>print(f"Sigma epsilon: {np.std(epsilon):.3f}")</span></pre><p id="bc05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这似乎没问题:</p><pre class="kg kh ki kj gt nq nr oe of aw og bi"><span id="046e" class="oh mb iq nr b gy oi oj l ok nz">Mean epsilon: -0.000<br/>Sigma epsilon: 2.146</span></pre><p id="c070" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们使用夏皮罗-维尔克正态性检验。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="a6b2" class="nv mb iq nr b be nw nx l ny nz">shapiro_stat, shapiro_pvalue = shapiro(epsilon)<br/>print(f"Shapiro test p-value: {shapiro_pvalue:.2f}")</span></pre><p id="618e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它返回:</p><pre class="kg kh ki kj gt nq nr oe of aw og bi"><span id="0c5e" class="oh mb iq nr b gy oi oj l ok nz">Shapiro test p-value: 0.38</span></pre><p id="2a48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们不能拒绝零假设，即我们的残差来自正态分布。</p><p id="6e77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们绘制残差与预测变量和回归量的关系图，以验证它们是否独立分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9f62e008fb4317271fbf822b77ae63d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*MKEFrcQrhWSd63Dx2_Q3dQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">残差与预测变量。(图片由作者提供)。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5cf6e8f3c7911fc1e3a180d08d89d0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ow8177m8Dtjux0crf3Sx3A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">残差与自变量。(图片由作者提供)。</p></figure><p id="c256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">残差大多分布良好，尤其是在导线长度小于7和目标长度小于25的区域。然而，在目标和导线长度之间可能存在一些非线性，因为中间值的残差偏向负值，而高值(目标&gt; 50)偏向正值。那些有兴趣更详细地探索该问题的人可以尝试创建多项式要素，看看这种偏差是否会减少。此外，方差似乎不是有条件分布的。因此，我们不可能通过加权残差来提高模型性能。</p><p id="720d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一节中，让我们创建一个与第一个自变量相关的假预测值，并验证统计显著性检验是否能识别它。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="d64d" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">添加错误的预测值</h1><p id="c864" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">假预测值将等于线长度加上随机噪声项，该随机噪声项遵循平均值0和σ1的正态分布。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="27bc" class="nv mb iq nr b be nw nx l ny nz"># Predefined random seed to reproductibility of results<br/>np.random.seed(42)<br/><br/># Create feature<br/>x_false = X[:, 0] + np.random.randn(X.shape[0])<br/>X_false = np.column_stack((X, x_false.reshape((-1, 1))))</span></pre><p id="e3f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，它也与响应变量线性相关。事实上，甚至比芯片高度更相关。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/aa8d0f221e143bae1b82d8d9ddb276de.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*9TyZor6HQl1Dfc99__VkLA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标预测值与错误预测值。(图片由作者提供)。</p></figure><p id="07d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们重复上一节的过程来看看结果。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="48ac" class="nv mb iq nr b be nw nx l ny nz">linreg_alt = LinearRegression()<br/>linreg_alt.fit(X_false, y)<br/>print(linreg_alt.r2_score(X_false, y))</span></pre><p id="b460" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来得分指标仍然很高(0.9828)，但我们的结果不一定像以前一样有意义。让我们来验证置信区间。</p><pre class="kg kh ki kj gt nq nr ns bn nt nu bi"><span id="8e26" class="nv mb iq nr b be nw nx l ny nz">for i, val in enumerate(linreg_alt.betas):<br/>    pval = linreg_alt.pvalues[i]<br/>    print(f"Beta {i}: {val:.2f}; p-value: {pval:.2f}")</span></pre><p id="6a02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它返回:</p><pre class="kg kh ki kj gt nq nr oe of aw og bi"><span id="565f" class="oh mb iq nr b gy oi oj l ok nz">Beta 0: 2.13; p-value: 0.05<br/>Beta 1: 3.43; p-value: 0.00<br/>Beta 2: 0.01; p-value: 0.00<br/>Beta 3: -0.69; p-value: 0.16</span></pre><p id="5601" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看来我们找到了冒名顶替者…</p><p id="5b43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们的框架有效地验证了错误的预测器没有向模型提供额外的信息，假设我们仍然具有线长度的原始值，即使错误的预测器与预测变量具有强线性相关性。相反，相关性较弱的芯片高度对模型的贡献具有统计学意义。</p><p id="e7c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，强烈建议从模型中删除不必要的特征，以提高其可解释性和通用性。在<a class="ae kv" href="https://github.com/bruscalia/optimization-demo-files/blob/main/regression/notebooks/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">示例笔记本</a>中，我还提出了一个基于<em class="ls"> p </em>值的递归特征消除策略。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="59e2" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">进一步阅读</h1><p id="6abb" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">作为一个有工程背景的人，在这方面我应该推荐的第一本参考书是Montgomery Runger(2003)写的《T4工程师应用统计和概率》。在那里，你可能会发现这里提出的基本原理的更详细的描述和其他相关的回归方面，如多重共线性，平均响应的置信区间，和特征选择。</p><p id="1a9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那些对机器学习更感兴趣的人可以参考Hastie等人(2009)的<em class="ls">统计学习的要素</em>。特别是，我发现看到作者如何解释最近邻与线性模型之间的偏差-方差权衡非常有趣。</p><p id="a3fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当一个人的目标是估计具有描述非线性现象的一些基本意义的参数时，非线性回归也是令人着迷的。贝茨&amp;瓦特(1988)的《T2【非线性回归分析及其应用】T3》一书是一个很好的参考。Myers等人(2010年)的《T4广义线性模型》一书的第3章也介绍了这个主题。</p><p id="713d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">科斯马·沙立兹教授的课堂讲稿可以在<a class="ae kv" href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/" rel="noopener ugc nofollow" target="_blank">这个链接</a>上找到。目前，该草案的标题为<em class="ls">从初级角度进行高级数据分析</em>。在那里，人们可以找到有趣的主题，如加权和方差，因果推断，和相关数据。</p><p id="a2c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我很欣赏Python包<a class="ae kv" href="https://www.statsmodels.org/dev/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="ls"> statsmodels </em> </a>。在应用了<a class="ae kv" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html" rel="noopener ugc nofollow" target="_blank"> OLS </a>(正如我们在本文中所做的)之后，人们可能会有兴趣尝试<a class="ae kv" href="https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.WLS.html#statsmodels.regression.linear_model.WLS" rel="noopener ugc nofollow" target="_blank"> WLS </a>来解决残差方差不均匀的问题，并使用<a class="ae kv" href="https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html" rel="noopener ugc nofollow" target="_blank"> VIF </a>来检测要素多重共线性。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="795b" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结论</h1><p id="67df" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在本文中，介绍了多元线性回归的主要原理，然后用Python从头开始实现。该框架应用于一个简单的例子，其中除了线性最小二乘问题中关于残差的主要假设之外，还验证了参数的统计显著性。完整的代码和额外的例子可以在<a class="ae kv" href="https://github.com/bruscalia/optimization-demo-files/blob/main/regression/notebooks/linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank">这个链接</a>中找到。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="f22c" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">参考</h1><p id="d77f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">贝茨博士和瓦茨博士，1988年。<em class="ls">非线性回归分析及其应用。威利。</em></p><p id="521a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">茨韦塔纳·哈斯蒂和J. H .弗里德曼，2009年。统计学习的要素:数据挖掘、推理和预测。第二版。纽约:斯普林格。</p><p id="2e38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">蒙哥马利特区和龙格，2003年。工程师应用统计和概率。第三版。约翰·威利父子公司。</p><p id="8121" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迈尔斯，R. H .，蒙哥马利特区，维宁，G. G .和罗宾逊，T. J .，2012年。<em class="ls">广义线性模型:在工程和科学中的应用。第二版。霍博肯:约翰·威利&amp;之子。</em></p><p id="cad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">c .沙里齐，2021。<em class="ls">从初级观点看高级数据分析。</em>剑桥大学出版社。</p></div></div>    
</body>
</html>