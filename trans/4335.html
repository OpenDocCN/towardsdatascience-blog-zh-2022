<html>
<head>
<title>Reinforcement Learning (RL) — What Is It and How Does It Work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习(RL)——什么是强化学习，它是如何工作的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-rl-what-is-it-and-how-does-it-work-1962cf6db103#2022-09-26">https://towardsdatascience.com/reinforcement-learning-rl-what-is-it-and-how-does-it-work-1962cf6db103#2022-09-26</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="cdef" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">强化学习</h2><div class=""/><div class=""><h2 id="21b6" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">对强化学习的温和介绍，对概念和术语有清晰的解释</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/ab7a09c317173afa88b1ac1c6a0bf395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwiKN1Yzy234Oer2kBZHvQ.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由<a class="ae li" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=663997" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的Gerd Altmann 提供</p></figure><h1 id="63e3" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">简介</strong></h1><p id="e82a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">与我以前的文章略有不同，这次我将不包括Python代码，因为我想避免读者的信息过载。而是简单介绍一下<strong class="md je">强化学习(RL) </strong>，讲解一下<strong class="md je">主要思路</strong>和<strong class="md je">术语</strong>。</p><p id="922f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我的后续文章将更深入地研究各个RL算法，并提供详细的Python示例。因此，记得<a class="ae li" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank">订阅</a>以便在它们发布时直接发送到你的收件箱。</p><h1 id="5e85" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">内容</h1><ul class=""><li id="94a7" class="nc nd iu md b me mf mh mi mk ne mo nf ms ng mw nh ni nj nk bi translated">机器学习领域中的强化学习</li><li id="512f" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">什么是强化学习？<br/> -主体和环境<br/> -行动空间<br/> -状态(观察)空间<br/> -奖励(和折扣因子)<br/> -探索与剥削</li><li id="8c77" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">训练模型的不同方法</li><li id="c906" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">摘要</li></ul><h1 id="ce56" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">ML世界中的强化学习</h1><p id="cbdc" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">简单来说，强化学习类似于婴儿如何学习周围环境，或者我们如何训练狗。我们允许他们与环境互动和探索环境，并提供积极/消极的奖励来鼓励特定的行为。</p><p id="4ee2" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">由于RL的机器学习方法明显不同于其他类型的ML，我们将RL放在我们的机器学习宇宙图中的一个单独的类别中。</p><p id="2b91" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面的<strong class="md je">图是交互式的</strong>，请点击👇在不同的部分探索和揭示更多。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nq nr l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创作的互动图表。</p></figure><p id="e7ed" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="ns">如果你喜欢数据科学和机器学习</em> </strong> <em class="ns">，请</em> <a class="ae li" href="https://bit.ly/3sItbfx" rel="noopener ugc nofollow" target="_blank"> <em class="ns">订阅</em> </a> <em class="ns">获取我的新文章邮件。如果你不是中等会员，可以在这里加入</em><a class="ae li" href="https://bit.ly/36Mozgu" rel="noopener ugc nofollow" target="_blank"><em class="ns"/></a><em class="ns">。</em></p><h1 id="3160" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">什么是强化学习？</h1><p id="db42" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">强化学习(RL)是一种机器学习算法，用于训练智能<strong class="md je">代理</strong>在特定<strong class="md je">环境</strong>中通过最大化预期累积<strong class="md je">回报来执行任务或实现目标。</strong>这里有几个例子:</p><ul class=""><li id="21f5" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">教一个基本的人工智能如何玩电脑游戏，比如雅达利的《太空入侵者》或任天堂的《超级马里奥兄弟》</li><li id="a641" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">教一个更高级的人工智能玩现实生活中的游戏，比如国际象棋或围棋</li><li id="abc3" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">教自动驾驶汽车(或模型车)如何在模拟或现实生活环境中驾驶(<a class="ae li" href="https://aws.amazon.com/deepracer/" rel="noopener ugc nofollow" target="_blank">参见AWS DeepRacer </a>)</li></ul><h2 id="ce7f" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated">代理和环境</h2><p id="fe80" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">RL中的两个关键组件是<strong class="md je">代理</strong>和它的<strong class="md je">环境</strong>。<strong class="md je">代理</strong>代表一个玩家、一辆汽车或其他可以与其环境互动的“智能角色”。</p><p id="2d59" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">同时，<strong class="md je">环境</strong>是代理“生活”或操作的“世界”。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/c2bab15093c82fe4392e38a616bdd7fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKm5h9lL4A0kF1Fou167ng.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">代理及其环境。作者插图。</p></figure><h2 id="534d" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">动作空间</strong></h2><p id="8022" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">代理可以<strong class="md je">在其环境中执行动作</strong>。例如，马里奥可以做像向左走、向右走、向上跳等事情。，里面是它的超级马里奥游戏关卡。同样的情况也适用于下面我虚构的游戏插图中的代理。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/fc605c9b37127989df188ea0540c1a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3LDe7MPsC0Eit6ubxLCeg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">在其环境中执行动作的代理。作者<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">插图</a>。</p></figure><p id="1c82" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">可用的<strong class="md je">动作空间</strong>可以是<strong class="md je">离散的</strong>或<strong class="md je">连续的</strong>。</p><ul class=""><li id="264c" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">离散动作空间</strong>——就像在超级马里奥兄弟的例子中，代理只能执行有限数量的动作，使其成为离散的。因此，我们总是可以列出所有可用的操作。</li><li id="fb84" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">连续动作空间</strong> —代理可以采取无限数量的动作。例如，自动驾驶汽车可以左转1度、1.2度、1.2432度等。因此，我们无法列出所有可能的行动。</li></ul><h2 id="e4b1" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">状态(观察)空间</strong></h2><p id="b2ab" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">下一个重要元素是环境的<strong class="md je">观察</strong>或<strong class="md je">状态</strong>。</p><ul class=""><li id="f683" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated">一个<strong class="md je">状态</strong>描述了代理从环境中获得的信息。例如在计算机游戏中，它可以是显示代理的确切位置的环境的冻结帧。请注意，<strong class="md je">状态是对世界</strong>的完整描述，即完全观察到的环境，就像棋盘的全景。</li><li id="b718" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">一个<strong class="md je">观察</strong>是状态的<strong class="md je">部分描述，也就是说，它只提供了环境的部分视图。例如，超级马里奥兄弟的冻结帧将仅向我们显示接近代理的部分级别，而不是整个级别。</strong></li></ul><p id="025a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然而，在实践中，<strong class="md je">状态和观察是互换使用的</strong>，所以当我们不一定对环境有一个完整的看法时，不要惊讶地看到“状态”这个术语，反之亦然。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oi"><img src="../Images/3de17ae840af855e63a665c91986c937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fgSU2-LGD_NVpJHCYCbM-A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">状态vs观察。作者<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">插图</a>。</p></figure><h2 id="14c7" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated"><strong class="ak">奖励(和折扣系数)</strong></h2><p id="f555" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">也许这个谜题最关键的部分是奖励。我们训练代理采取“最佳”行动，根据行动是否使代理更接近实现特定目标，给予积极、中立或消极的奖励。</p><p id="22b5" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">假设我们在玩一个游戏，目标是抓一只松鼠。代理将从随机探索它的环境开始，如果它靠近松鼠，我们将奖励代理(+1分)，如果它远离松鼠，我们将“惩罚”(-1分)。最后，当代理实现了它的目标，也就是抓到了松鼠，我们会给一个很大的奖励(+1000)。</p><p id="c5c7" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">只要采取随机行动并获得相应的奖励，代理人就可以了解需要做什么来使<strong class="md je">的累积奖励</strong>最大化，并使<strong class="md je">实现目标</strong>。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/3bda47dc622ea1f4ca88f50d0b83b276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBCriICVcgruBarMEhw9xw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">奖励是给代理的，所以它学习如何实现游戏的目标。作者插图。</p></figure><p id="ddf0" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请注意，本例中的奖励值仅用于说明目的。我们在做强化学习的时候，往往会选择<strong class="md je">创建自己的奖励函数</strong>。通常，你的奖励函数的“质量”将是你的模型成功的重要驱动因素(例如，查看<a class="ae li" href="https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-console-train-evaluate-models.html" rel="noopener ugc nofollow" target="_blank">奖励函数如何在AWS DeepRacer竞赛中使用</a>)。</p><p id="d03d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">奖励的另一个重要方面是<strong class="md je">折扣系数(gamma) </strong>。它的范围可以在0到1之间，但我们通常会选择0.95到0.99之间的值。折扣系数的目的是让我们控制对短期和长期回报的偏好。</p><p id="fc2b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">例如，在国际象棋比赛中，抓住对手棋子的一步棋会得到奖励。然而，我们不希望经纪人优先考虑这一举动，如果这让我们长期处于亏损的境地。因此，平衡短期和长期回报至关重要。</p><h2 id="7968" class="nw lk iu bd ll nx ny dn lp nz oa dp lt mk ob oc lv mo od oe lx ms of og lz ja bi translated">探索与开发</h2><p id="1d1a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们要学习的最后一个概念是探索和开发之间的权衡。我们通常希望鼓励代理<strong class="md je">对其环境进行某种程度的探索</strong>，而不是将全部时间<strong class="md je">用于开发其已有的知识</strong>。</p><p id="3dde" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">一个简单的现实生活中的例子是选择午餐。假设你是汉堡的超级粉丝，每天午餐都吃汉堡。因此，你是在<strong class="md je">利用现有的知识(你对汉堡的口味)</strong>。</p><p id="447e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然而，你也可以在你的日常生活中增加一个<strong class="md je">的探索元素</strong>。例如，你可以偶尔尝试不同的食物，比如热狗或烤肉串。当然，通过探索，你会有失望的风险(负面体验/回报)，但你也会发现一些你可能比汉堡更喜欢的东西(正面体验/回报)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oh"><img src="../Images/c6c6315c6009428f10f006537372bf1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G2P0jmDQGsbVaksKbivbkQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">探索与开发。作者<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">插图</a>。</p></figure><p id="5f03" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在强化学习的背景下，我们希望鼓励代理花部分时间去探索。否则，代理可能会不断重复相同的动作，而没有意识到还有更好的动作要做。我们通过一个<strong class="md je">附加参数(ε)</strong>做到这一点。我们指定代理在多大比例的情况下应该采取<strong class="md je">随机行动(即探索)</strong>。</p><div class="kt ku kv kw gu ab cb"><figure class="oj kx ok ol om on oo paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/63320331b74bd98eea6402472b4209ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qkXay39OnVc2IosW6rkxtw.png"/></a></figure><figure class="oj kx ok ol om on oo paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="a31e" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">训练你的模型的不同方法</strong></h1><p id="302a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">当我们使用强化学习时，我们希望训练代理采取“最佳”行动来实现其目标。我们称之为<strong class="md je"> Policy(𝜋) </strong>。它有点像代理的“大脑”。</p><p id="6257" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">例如，如果我们想教一个代理如何玩电脑游戏，它需要学习在每种情况下采取什么行动。换句话说，我们希望<strong class="md je">找到一个最优的Policy(𝜋) </strong>，从长期来看，它能带来尽可能高的回报。</p><p id="3055" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">要找到最佳策略，我们可以使用以下方法之一:</p><ul class=""><li id="9981" class="nc nd iu md b me mx mh my mk nt mo nu ms nv mw nh ni nj nk bi translated"><strong class="md je">基于策略的方法</strong> —我们直接训练代理在哪个状态下采取什么行动。</li><li id="941a" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">基于价值的方法</strong> —我们训练代理识别哪些状态(或状态-动作对)更有价值，因此它可以由价值最大化来指导。例如，在捕捉松鼠的游戏中，站在离松鼠一步远的地方会比站在十步远的地方描述更有价值的状态。</li></ul><p id="ba60" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我将在接下来的文章中更详细地介绍每种方法，因为我们将更深入地研究各种RL算法，并学习如何用Python对它们进行编码。</p><h1 id="4f4b" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">总结</strong></h1><p id="8ff2" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">概括地说，我们已经了解到，强化学习用于教导<strong class="md je">代理</strong>在其<strong class="md je">环境</strong>中操作，并通过根据代理在不同<strong class="md je">状态</strong>下采取的<strong class="md je">动作</strong>向代理提供积极的、中立的或消极的<strong class="md je">奖励</strong>来实现目标或目的(例如，赢得一场游戏)。</p><p id="2b8d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们通过指定随机选择代理的行动比例来平衡<strong class="md je">探索</strong>与<strong class="md je">开发</strong>，并且我们应用<strong class="md je">折扣因子(gamma) </strong>来控制代理对短期与长期回报的偏好。</p><p id="3106" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">最后，我们通过优化<strong class="md je"> Policy(𝜋、</strong>来训练模型(教导代理人)，我们可以通过<strong class="md je">基于直接政策的方法</strong>或<strong class="md je">基于间接价值的方法</strong>来完成。</p><p id="ea24" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">在我的下一篇文章中，我将带您了解<strong class="md je"> Q-learning </strong>算法(<strong class="md je">基于值的方法</strong>)，并向您展示如何在Python中使用它来训练一个代理在一个名为<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">冰封湖</a>的简单游戏中从头到尾成功导航。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj op"><img src="../Images/fc6ed2a84846d25ae39f4787d92fe582.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/1*af0diz6xweH1deJj7Pn0oQ.gif"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Gif图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>使用<a class="ae li" href="https://www.gymlibrary.dev/environments/toy_text/frozen_lake/?highlight=frozen+lake" rel="noopener ugc nofollow" target="_blank">开放AI的健身房Python库</a>内的冰湖游戏的组件创建。</p></figure><p id="7467" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">请不要忘记关注并<a class="ae li" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank">订阅</a>，这样你就不会错过和我一起探索强化学习的乐趣。</p><p id="787a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">干杯！🤓<br/>T39】索尔·多比拉斯</p></div><div class="ab cl oq or hy os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="in io ip iq ir"><p id="a070" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="ns">如果你已经花光了这个月的学习预算，下次请记得我。</em> </strong> <em class="ns">我的个性化链接加入媒介:</em></p><div class="ox oy gq gs oz pa"><a href="https://bit.ly/3J6StZI" rel="noopener  ugc nofollow" target="_blank"><div class="pb ab fp"><div class="pc ab pd cl cj pe"><h2 class="bd je gz z fq pf fs ft pg fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="ph l"><h3 class="bd b gz z fq pf fs ft pg fv fx dk translated">阅读索尔·多比拉斯和媒体上成千上万的其他作家的每一个故事。你的会员费直接支持索尔…</h3></div><div class="pi l"><p class="bd b dl z fq pf fs ft pg fv fx dk translated">solclover.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po lc pa"/></div></div></a></div></div></div>    
</body>
</html>