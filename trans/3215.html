<html>
<head>
<title>MultiMAE: An Inspiration to Leverage Labeled Data in Unsupervised Pre-training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MultiMAE:在无监督预训练中利用标记数据的灵感</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multimae-an-inspiration-to-leverage-labeled-data-in-unsupervised-pre-training-9739a2dbf97c#2022-07-17">https://towardsdatascience.com/multimae-an-inspiration-to-leverage-labeled-data-in-unsupervised-pre-training-9739a2dbf97c#2022-07-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="55a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过多模式屏蔽自动编码器提高模型性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/83f84688a6468f8c98a97fc6040b8795.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UHL0ekaAxCRTjBuz"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">巴勃罗·阿里纳斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="07c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自监督预训练是提高传统监督学习性能的一种主要方法，其中大量的标记数据是必要的和昂贵的。在自我监督学习方法中，对比学习因其简单有效而广受欢迎。然而，大多数对比学习方法使用全局向量，其中像素级信息的细节丢失，这在转移到下游密集任务时留下了改进的空间。我向感兴趣的读者推荐我以前的关于对比学习方法的文章。</p><div class="lv lw gp gr lx ly"><a href="https://medium.com/geekculture/understanding-contrastive-learning-and-moco-efe491e4eed9" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">理解对比学习和MoCo</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">在对比学习中什么是重要的，它能有什么帮助</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/pixel-level-dense-contrastive-learning-6558691eeb67"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">像素级密集对比学习</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">基于主动采样策略的密集对比学习</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm ks ly"/></div></div></a></div><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/contrastive-pre-training-of-visual-language-models-848dd94c881b"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">视觉语言模型的对比预训练</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">对比视角下充分利用监督信号</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mo l mj mk ml mh mm ks ly"/></div></div></a></div><p id="f746" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于密集下游任务的迁移学习，我们需要一些自我监督的预训练方法，可以恢复整个特征图的细节，而不仅仅是合并的全局向量。只有通过这个才能了解到整个特征地图分布的细节，更重要的是，不需要标注。</p><p id="7ff8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对比学习中，提出了一些方法来训练图像块[4]，或局部特征[5]，甚至像素[6]。然而，这些方法要么使用消耗内存的动量编码器和队列[4，5]，要么使用难以训练的带有伪标签的半监督学习方式[6]。</p><h2 id="e399" class="mp mq it bd mr ms mt dn mu mv mw dp mx li my mz na lm nb nc nd lq ne nf ng nh bi translated">屏蔽自动编码器</h2><p id="7f5c" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">没有对比思维的方法是何在2021年提出的[7]。这与自然语言处理中的BERT [8]的方法类似，在该方法中，使用交叉熵损失对句子中的屏蔽记号进行分类和训练。<strong class="lb iu">然而，在计算机视觉中，表征不能被分类，因为图像模式几乎是无限的，而在NLP中，表征是有限的，并且在语料库中预定义。因此，在计算机视觉中，被屏蔽的标记只能以回归方式预测，而不能以分类方式预测</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/693af004dee1a9762acda560393d52c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OZ5zh4Kv7yKrfDjL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank"> MAE不对称架构</a>)</p></figure><p id="b23f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该架构被称为屏蔽自动编码器(MAE ),其中非屏蔽令牌被编码，屏蔽令牌被重建并以MSE损失进行训练。这是一个基于视觉转换器的简单而有效的架构[9]。由于编码器仅编码未屏蔽的标记，因此它可以以较小的计算开销扩展到大的输入图像。此外，由于解码器是浅层的，并且仅针对屏蔽令牌计算损失，所以它也是可扩展的，几乎没有额外的计算开销。作者对标记使用了随机掩蔽，但我认为训练资源可以集中在尚未训练好的标记上，就像焦点丢失[10]中使用的方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/8eb1138693cd1ccced9366c1c83ea82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qbCz1F6eN759MYC4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">MAE的重建结果</a></p></figure><p id="20a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，即使当图像被95%的遮挡时，也可以重建出真实的图像，显示了模型强大的细节学习能力。</p><h2 id="0137" class="mp mq it bd mr ms mt dn mu mv mw dp mx li my mz na lm nb nc nd lq ne nf ng nh bi translated">多模式多任务屏蔽自动编码器</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/83925cf19605efda9604dd84b3af48e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WQnOU-V7ohSleKaR"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/pdf/2204.01678v1.pdf" rel="noopener ugc nofollow" target="_blank">多模态架构</a>)</p></figure><p id="1f88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一个模型成功时，它通常会被社区中的研究人员扩展到许多其他形式。<strong class="lb iu">多式联运就是这些扩展模式之一。由于多模态模型产生更健壮的特征，如果你有数据</strong>，训练多模态模型通常是一个好的选择。</p><p id="82b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在MultiMAE中，作者使用了三种模态:RGB、深度和语义。由于很难在这三种模式下收集大量的相应数据，作者建议使用一些现成模型生成的伪标签。然而，作者还表明，用伪标签训练的模型不如用真实标签训练的模型性能好。</p><p id="8969" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型如上所示进行了扩展。简单明了的是，每个模态面片都用模态线性投影仪投影到标记向量。所有三种模态的标记向量用相同的编码器编码，但是用模态解码器分别解码。在预训练之后，编码器可以用相应的线性投影仪和特定任务头以单模态和多模态方式进行微调。</p><p id="d060" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现关于MultiMAE的一件有趣的事情是<strong class="lb iu">标记的数据可以在预训练和微调阶段利用</strong>。假设您有一些RGB语义对用于训练语义分割模型，<strong class="lb iu">，而不是以传统的监督学习方式训练，您可以用MultiMAE预训练模型，其中RGB和语义都被屏蔽和重建</strong>。在此之后，由于模型已经学习了RGB的分布和语义细节，因此在接下来的单模态微调阶段，标签效率可以提高很多。<strong class="lb iu">如果在耗费数据的预训练阶段缺乏地面真实语义标签，也可以使用一些现成模型产生的伪标签</strong>。</p><h2 id="1d38" class="mp mq it bd mr ms mt dn mu mv mw dp mx li my mz na lm nb nc nd lq ne nf ng nh bi translated">参考</h2><p id="5cfd" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">[1] <a class="ae ky" href="https://medium.com/geekculture/understanding-contrastive-learning-and-moco-efe491e4eed9" rel="noopener">理解对比学习和MoCo，2021 </a></p><p id="36b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] <a class="ae ky" rel="noopener" target="_blank" href="/pixel-level-dense-contrastive-learning-6558691eeb67">像素级密集对比学习，2022 </a></p><p id="ad63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] <a class="ae ky" rel="noopener" target="_blank" href="/contrastive-pre-training-of-visual-language-models-848dd94c881b">视觉语言模型对比预训练，2022 </a></p><p id="3fe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] <a class="ae ky" href="https://arxiv.org/pdf/2102.04803.pdf" rel="noopener ugc nofollow" target="_blank"> DetCo:对象检测的无监督对比学习，2021 </a></p><p id="71dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] <a class="ae ky" href="https://arxiv.org/pdf/2011.09157.pdf" rel="noopener ugc nofollow" target="_blank">用于自监督视觉预训练的密集对比学习，2021 </a></p><p id="a1d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] <a class="ae ky" href="https://arxiv.org/pdf/2104.04465.pdf" rel="noopener ugc nofollow" target="_blank">带区域对比的自举语义分割，2022 </a></p><p id="0e2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7] <a class="ae ky" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">屏蔽自动编码器是可伸缩视觉学习器，2021 </a></p><p id="9763" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8] <a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:面向语言理解的深度双向变压器预训练，2019 </a></p><p id="debe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9] <a class="ae ky" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">一幅图像抵得上16X16个字:用于大规模图像识别的变形金刚，2021年</a></p><p id="3880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[10] <a class="ae ky" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">用于密集物体检测的焦损失，201 </a> 8</p><p id="489c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[11] <a class="ae ky" href="https://arxiv.org/pdf/2204.01678v1.pdf" rel="noopener ugc nofollow" target="_blank"> MultiMAE:多模态多任务屏蔽自动编码器，2022 </a></p><div class="lv lw gp gr lx ly"><a href="https://dushuchen.medium.com/membership" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">加入我的介绍链接-陈数杜媒体</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">阅读陈数·杜(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">dushuchen.medium.com</p></div></div><div class="mh l"><div class="nq l mj mk ml mh mm ks ly"/></div></div></a></div></div></div>    
</body>
</html>