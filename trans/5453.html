<html>
<head>
<title>Reinforcement Learning for Inventory Optimization Series I: An RL Model for Single Retailers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">库存优化的强化学习系列 I:单一零售商的强化学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278#2022-12-07">https://towardsdatascience.com/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278#2022-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="37a7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">建立深度 Q 网络(DQN)模型来优化单个零售商的库存运作</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8fd7ea14976eaf7741267ae4bac41f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U1v5GFLDVXQyT2kw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">唐·达斯卡洛在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8705" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更新:本文是我的博客系列<em class="ls">库存优化的强化学习</em>的第一篇文章。下面是同一系列中其他文章的链接。如果你感兴趣，请去看看。</p><p id="632e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-ii-an-rl-model-for-a-multi-echelon-921857acdb00" rel="noopener"> <em class="ls">库存优化强化学习系列之二:多级网络的 RL 模型</em> </a></p><p id="045b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/towards-data-science/reinforcement-learning-for-inventory-optimization-series-iii-sim-to-real-transfer-for-the-rl-model-d260c3b8277d" rel="noopener"> <em class="ls">库存优化强化学习系列之三:RL 模型的虚拟到真实传递</em> </a></p><p id="2dd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">库存优化是供应链管理的一个重要方面，它涉及优化企业的库存运作。它使用数学模型来回答关键问题，如何时下补货订单以满足客户对产品的需求，以及订购多少数量。当今供应链行业中采用的主要库存控制策略是经典的静态策略，在这种意义上，何时订购或订购多少的决策在整个计划范围内是固定的，除非策略被更新。然而，当需求变化很大时，这种静态政策可能会有所不足。不仅基于库存位置，而且基于相关的未来需求信息，能够自适应地调整何时订购以及订购多少的决策的动态策略将是更有利的。在本文中，我将使用一个销售可乐的小型零售店作为示例，来说明我们如何利用强化学习(RL)技术——深度 Q 网络(DQN)来构建库存控制策略，以优化库存运营并获得比经典静态库存控制策略更多的利润。</p><h1 id="eb0e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">经典库存控制策略</strong></h1><ol class=""><li id="47d6" class="ml mm iq ky b kz mn lc mo lf mp lj mq ln mr lr ms mt mu mv bi translated">(<em class="ls"> R </em>，<em class="ls"> Q </em>)策略:该策略规定当库存低于<em class="ls"> R </em>单位时，我们需要订购固定数量的<em class="ls"> Q </em>单位的产品。这里，<em class="ls"> R </em>被称为再订购点，<em class="ls"> Q </em>是订购数量<em class="ls">。</em>在实践中，通常在每天开始或结束时检查库存位置。</li><li id="368b" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">(<em class="ls"> T </em>，<em class="ls"> S </em>)政策:这个政策说我们每<em class="ls"> T </em>天下一个订单来补充库存到<em class="ls"> S </em>单位。这里，<em class="ls"> T </em>是审核周期，它决定了我们审核库存水平的频率，而<em class="ls"> S </em>被称为符合订单水平。</li><li id="ae42" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">(<em class="ls"> s </em>，<em class="ls"> S </em>)政策:该政策规定，当库存位置低于<em class="ls"> s </em>单位时，我们需要下订单补充库存至<em class="ls"> S </em>单位。这里，<em class="ls"> s </em>可以被认为是再订购点，而<em class="ls"> S </em>可以被认为是合格级别。</li><li id="d430" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">基本库存策略:该策略相当于(<em class="ls"> S </em> -1，<em class="ls"> S </em>)策略，意思是如果在特定的一天有任何需求消耗库存，我们会立即下订单补充库存，直到<em class="ls"> S </em>单位。</li></ol><p id="96d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述不同的策略适用于不同的需求模式，但共同点是它们都假定要么有一个固定的再订购点，固定的订货量，固定的最大订货量或两个订单之间的固定时间间隔。此外，这些策略中的大多数仅依赖于当前的库存状况来做出订购决策，而没有利用与未来需求相关的其他可能信息来帮助做出更明智的决策。这限制了政策的灵活性，这潜在地破坏了政策对高需求的响应性(导致销售损失)或在需求低时导致过多的库存(导致库存持有成本)。如果我们去掉这个限制，我们能做得更好吗？我们如何建立一个模型来获得一个没有这种限制的库存控制策略？一种可能的方法是强化学习(RL)。</p><h1 id="07c8" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">库存优化的强化学习</h1><p id="0e5e" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">RL 是机器学习的一个子领域，涉及决策制定。它使智能代理能够通过与环境的交互，学习如何根据过去的经验做出最佳决策。RL 因其广泛的应用而广受欢迎，包括自动驾驶汽车、机器人控制、游戏等..</p><p id="0af2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">公式化马尔可夫决策过程</strong></p><p id="58fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了利用 RL，必须首先将决策问题公式化为马尔可夫决策过程(MDP)。MDP 是一个数学框架来模拟决策问题，其中决策是在离散的时间步骤顺序作出的。MDP 有 4 个核心要素:状态、行动、奖励、转移概率。状态<em class="ls"> s_t </em>表示代理在时间<em class="ls"> t </em>的情况。动作<em class="ls"> a_t </em>是代理在时间<em class="ls"> t </em>做出的决定。奖励<em class="ls"> r_t </em>是环境的反馈，告诉代理人某个动作是好是坏。转移概率<em class="ls"> P(s_(t+1)|s_t，a_t) </em>决定了当代理在状态<em class="ls"> s_t </em>采取动作<em class="ls"> a_t </em>时，它落入状态<em class="ls"> s_(t+1) </em>的概率。在大多数真实世界环境中，转移概率不可能是已知的。</p><p id="8433" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">库存优化问题自然适合 MDP 的框架，因为它的顺序决策结构。可能有多种方式来定义库存优化问题的状态、行动和回报。理论上，状态的定义应包括所有可能有助于采取合理行动的相关信息，行动的定义应足够灵活，以代表决策的所有可能选项，奖励的定义应反映问题的目标(例如，最小化成本，最大化利润)。因此，状态、动作和奖励定义可能因情况而异。</p><p id="db60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们假设客户需求遵循一种特殊的结构:混合正态分布，其中周一至周四的需求遵循具有最低均值的正态分布，周五的需求遵循具有中等均值的正态分布，周六至周日的需求遵循具有最高均值的正态分布。这一假设是基于这样一个事实，即人们更倾向于在周末而不是工作日购买食品杂货(也更经常在周五而不是其他工作日)。让我们进一步假设，作为零售店的老板，我们想在一段时间内最大化销售可乐的利润。所考虑的成本包括库存持有成本、固定订购成本(如运输成本)和可变订购成本(如从供应商处购买可乐的单位成本)。这里不考虑延期交货成本，因为我们假设如果顾客在商店里没有看到任何剩余的可乐，他们会去其他商店购买可乐。他们不会在店里下单，等着订单将来履行。</p><p id="7721" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据上面的假设，下面是状态、行动和奖励的定义。</p><ol class=""><li id="acd6" class="ml mm iq ky b kz la lc ld lf ne lj nf ln ng lr ms mt mu mv bi translated">State: ( <em class="ls"> i_pt </em>，<em class="ls"> dow_t </em>)，其中<em class="ls"> i_pt </em>是第<em class="ls"> t </em>天结束时的库存位置(现有库存+即将订单)，而<em class="ls"> dow_t </em>是使用一个热编码<em class="ls">表示第<em class="ls"> t </em>天的一周中的一天的<em class="ls"> 6- </em>维向量。</em>我们希望订购决策不仅基于库存状况，还基于一周中的某一天的信息。</li><li id="97b9" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">动作:<em class="ls"> a_t </em>，其中<em class="ls"> a_t </em>表示<em class="ls"> </em>第<em class="ls"> </em>天结束时的订单数量。如果<em class="ls"> a_t </em>是正数，我们订购<em class="ls"> a_t </em>台。如果<em class="ls"> a_t </em> = 0，我们不下单。行动空间受到最大订货量的限制，最大订货量由供应商或运输车辆的容量决定。</li><li id="2991" class="ml mm iq ky b kz mw lc mx lf my lj mz ln na lr ms mt mu mv bi translated">奖励:<em class="ls"> r_t = min(d_t，I _ t)* p-I _ t * h-I(a _ t&gt;0)* f-a _ t * v，</em>其中<em class="ls"> d_t </em>为<em class="ls"> </em> ( <em class="ls"> t+1 </em>)第<em class="ls"> </em>日<em class="ls">白天发生的需求<em class="ls"> </em>，i_t </em>为库存月 <em class="ls"> I(a_t &gt; 0) </em>是一个指标函数，取<em class="ls"> </em> 1 <em class="ls"> </em>如果<em class="ls">a _ t&gt;0</em>0<em class="ls"/>否则<em class="ls">，f </em>是每个订单发生的固定订货成本<em class="ls">，</em>和<em class="ls"> v </em>是每单位的可变订货成本。 很容易看出，奖励<em class="ls"> r_t </em>就是在第<em class="ls"> t </em>个决策时期获得的利润。</li></ol><p id="3bfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">求解马尔可夫决策过程</strong></p><p id="35b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面公式化的 MPD 的一个显著特性是转移概率是未知的。在特定时间<em class="ls"> t </em>，<em class="ls"> dow_(t+1) </em>是确定的，但是<em class="ls"> i_p(t+1) </em>不是由<em class="ls"> a_t 唯一确定的。</em>人们可以选择使用历史需求数据来拟合需求分布，尝试推断转移概率，然后使用基于模型的 RL 技术来解决这个问题。然而，这可能导致模拟环境和真实世界之间的巨大差距，因为拟合完美的需求分布非常具有挑战性(特别是在需求遵循混合分布的情况下)。因此，最好采用能够固有地处理未知转移概率的无模型 RL 技术。</p><p id="27e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有多种无模型 RL 技术来解决这个 MDP。在本文中，作为第一次尝试，我采用深度 Q 网络(DQN)作为解决工具。DQN 是 Q 学习的一种变体，它利用深度神经网络来建立 Q 函数的近似。为了节省篇幅，我省略了 DQN 的详细说明，因为这不是本文的重点。有兴趣的读者可参考<a class="ae kv" href="https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda" rel="noopener">这篇文章</a>。</p><h1 id="2360" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数值实验</h1><p id="eb3a" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">为了比较 DQN 学习的库存控制策略和经典库存控制策略的性能，让我们考虑如下的数值实验。</p><p id="4704" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设有一家小零售店向顾客出售可乐。每次商店想要补充库存以满足客户需求时，商店都必须订购整数箱可乐(一箱 24 罐)。假设可乐的单位销售价格为每箱 30 美元，持有成本为每晚每箱 3 美元，固定订购成本为每订单 50 美元，可变订购成本为每箱 10 美元，商店的库存容量为 50 箱，每订单允许的最大订购量为 20 箱，在一个周日结束时初始库存为 25 箱，提前期(下订单和订单到达之间的时间间隔)为 2 天。这里我们假设周一到周四的需求服从正态分布<em class="ls"> N </em> (3，1.5)，周五的需求服从正态分布<em class="ls"> N </em> (6，1)，周六到周日的需求服从正态分布<em class="ls"> N </em> (12，2)。我们从这种混合分布中生成 52 <em class="ls"> </em>周的历史需求<em class="ls"> </em>样本，并将其用作 DQN 模型的训练数据集。</p><p id="0857" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为基准，我们将使用用于训练 DQN 模型的相同数据集来优化经典(<em class="ls"> s </em>，<em class="ls"> S </em>)库存控制策略，并在测试集中将其性能与 DQN 进行比较。</p><h2 id="2f44" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">为 DQN 模型定型的代码</h2><p id="45f5" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">首先，生成训练数据集并查看历史需求直方图。请注意，非整数需求数据会四舍五入为最接近的整数。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="19c5" class="ny lu iq nu b be nz oa l ob oc">import numpy as np<br/>import matplotlib.pyplot as plt<br/>np.random.seed(0)<br/>demand_hist = []<br/>for i in range(52):<br/>    for j in range(4):<br/>        random_demand = np.random.normal(3, 1.5)<br/>        if random_demand &lt; 0:<br/>            random_demand = 0<br/>        random_demand = np.round(random_demand)<br/>        demand_hist.append(random_demand)<br/>    random_demand = np.random.normal(6, 1)<br/>    if random_demand &lt; 0:<br/>        random_demand = 0<br/>    random_demand = np.round(random_demand)<br/>    demand_hist.append(random_demand)<br/>    for j in range(2):<br/>        random_demand = np.random.normal(12, 2)<br/>        if random_demand &lt; 0:<br/>            random_demand = 0<br/>        random_demand = np.round(random_demand)<br/>        demand_hist.append(random_demand)<br/>plt.hist(demand_hist)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/886f06bfdff9869b3879f5af9519b0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*eFGHt_Guv9RkE4H-N0fUJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">历史需求数据的直方图(图片由作者提供)</p></figure><p id="2664" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们定义了库存优化问题的环境，以便 DQN 代理与之交互。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="d7ae" class="ny lu iq nu b be nz oa l ob oc">class InvOptEnv():<br/>    def __init__(self, demand_records):<br/>        self.n_period = len(demand_records)<br/>        self.current_period = 1<br/>        self.day_of_week = 0<br/>        self.inv_level = 25<br/>        self.inv_pos = 25<br/>        self.capacity = 50<br/>        self.holding_cost = 3<br/>        self.unit_price = 30<br/>        self.fixed_order_cost = 50<br/>        self.variable_order_cost = 10<br/>        self.lead_time = 2<br/>        self.order_arrival_list = []<br/>        self.demand_list = demand_records<br/>        self.state = np.array([self.inv_pos] + self.convert_day_of_week(self.day_of_week))<br/>        self.state_list = []<br/>        self.state_list.append(self.state)<br/>        self.action_list = []<br/>        self.reward_list = []<br/>            <br/>    def reset(self):<br/>        self.state_list = []<br/>        self.action_list = []<br/>        self.reward_list = []<br/>        self.inv_level = 25<br/>        self.inv_pos = 25<br/>        self.current_period = 1<br/>        self.day_of_week = 0<br/>        self.state = np.array([self.inv_pos] + self.convert_day_of_week(self.day_of_week))<br/>        self.state_list.append(self.state)<br/>        self.order_arrival_list = []<br/>        return self.state<br/>        <br/>    def step(self, action):<br/>        if action &gt; 0:<br/>            y = 1<br/>            self.order_arrival_list.append([self.current_period+self.lead_time, action])<br/>        else:<br/>            y = 0<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            if self.current_period == self.order_arrival_list[0][0]:<br/>                self.inv_level = min(self.capacity, self.inv_level + self.order_arrival_list[0][1])<br/>                self.order_arrival_list.pop(0)  <br/>        demand = self.demand_list[self.current_period-1]<br/>        units_sold = demand if demand &lt;= self.inv_level else self.inv_level<br/>        reward = units_sold*self.unit_price-self.holding_cost*self.inv_level - y*self.fixed_order_cost \<br/>                 -action*self.variable_order_cost    <br/>        self.inv_level = max(0,self.inv_level-demand)<br/>        self.inv_pos = self.inv_level<br/>        if len(self.order_arrival_list) &gt; 0:<br/>            for i in range(len(self.order_arrival_list)):<br/>                self.inv_pos += self.order_arrival_list[i][1]<br/>        self.day_of_week = (self.day_of_week+1)%7<br/>        self.state = np.array([self.inv_pos] +self.convert_day_of_week(self.day_of_week))<br/>        self.current_period += 1<br/>        self.state_list.append(self.state)<br/>        self.action_list.append(action)<br/>        self.reward_list.append(reward)<br/>        if self.current_period &gt; self.n_period:<br/>            terminate = True<br/>        else: <br/>            terminate = False<br/>        return self.state, reward, terminate<br/>    <br/>    def convert_day_of_week(self,d):<br/>        if d == 0:<br/>            return [0, 0, 0, 0, 0, 0]<br/>        if d == 1:<br/>            return [1, 0, 0, 0, 0, 0] <br/>        if d == 2:<br/>            return [0, 1, 0, 0, 0, 0] <br/>        if d == 3:<br/>            return [0, 0, 1, 0, 0, 0] <br/>        if d == 4:<br/>            return [0, 0, 0, 1, 0, 0] <br/>        if d == 5:<br/>            return [0, 0, 0, 0, 1, 0] <br/>        if d == 6:<br/>            return [0, 0, 0, 0, 0, 1] </span></pre><p id="d0c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们开始用 PyTorch 构建 DQN 模型。本部分 DQN 的代码实现基于<a class="ae kv" href="https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda" rel="noopener">这篇文章</a>。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="44c5" class="ny lu iq nu b be nz oa l ob oc">import torch <br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/><br/>class QNetwork(nn.Module):<br/>    """ Actor (Policy) Model."""<br/>    def __init__(self, state_size, action_size, seed, fc1_unit=128,<br/>                 fc2_unit = 128):<br/>        """<br/>        Initialize parameters and build model.<br/>        Params<br/>        =======<br/>            state_size (int): Dimension of each state<br/>            action_size (int): Dimension of each action<br/>            seed (int): Random seed<br/>            fc1_unit (int): Number of nodes in first hidden layer<br/>            fc2_unit (int): Number of nodes in second hidden layer<br/>        """<br/>        super(QNetwork,self).__init__() ## calls __init__ method of nn.Module class<br/>        self.seed = torch.manual_seed(seed)<br/>        self.fc1= nn.Linear(state_size,fc1_unit)<br/>        self.fc2 = nn.Linear(fc1_unit,fc2_unit)<br/>        self.fc3 = nn.Linear(fc2_unit,action_size)<br/>        <br/>    def forward(self,x):<br/>        # x = state<br/>        """<br/>        Build a network that maps state -&gt; action values.<br/>        """<br/>        x = F.relu(self.fc1(x))<br/>        x = F.relu(self.fc2(x))<br/>        return self.fc3(x)<br/><br/>import random <br/>from collections import namedtuple, deque <br/><br/>##Importing the model (function approximator for Q-table)<br/># from model import QNetwork<br/><br/>import torch<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torch.optim import lr_scheduler <br/><br/>BUFFER_SIZE = int(5*1e5)  #replay buffer size<br/>BATCH_SIZE = 128      # minibatch size<br/>GAMMA = 0.99            # discount factor<br/>TAU = 1e-3             # for soft update of target parameters<br/>LR = 1e-4            # learning rate<br/>UPDATE_EVERY = 4      # how often to update the network<br/><br/>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")<br/><br/>class Agent():<br/>    """Interacts with and learns form environment."""<br/>    <br/>    def __init__(self, state_size, action_size, seed):<br/>        """Initialize an Agent object.<br/>        <br/>        Params<br/>        =======<br/>            state_size (int): dimension of each state<br/>            action_size (int): dimension of each action<br/>            seed (int): random seed<br/>        """<br/>        <br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/>        self.seed = random.seed(seed)<br/>        <br/>        <br/>        #Q- Network<br/>        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)<br/>        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)<br/>        <br/>        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)<br/>        <br/>        # Replay memory <br/>        self.memory = ReplayBuffer(action_size, BUFFER_SIZE,BATCH_SIZE,seed)<br/>        # Initialize time step (for updating every UPDATE_EVERY steps)<br/>        self.t_step = 0<br/>        <br/>    def step(self, state, action, reward, next_step, done):<br/>        # Save experience in replay memory<br/>        self.memory.add(state, action, reward, next_step, done)<br/><br/>        # Learn every UPDATE_EVERY time steps.<br/>        self.t_step = (self.t_step+1)% UPDATE_EVERY<br/>        if self.t_step == 0:<br/>            # If enough samples are available in memory, get radom subset and learn<br/><br/>            if len(self.memory)&gt;BATCH_SIZE:<br/>                experience = self.memory.sample()<br/>                self.learn(experience, GAMMA)<br/>        <br/>    def act(self, state, eps = 0):<br/>        """Returns action for given state as per current policy<br/>        Params<br/>        =======<br/>            state (array_like): current state<br/>            eps (float): epsilon, for epsilon-greedy action selection<br/>        """<br/>        state = torch.from_numpy(state).float().unsqueeze(0).to(device)<br/>        self.qnetwork_local.eval()<br/>        with torch.no_grad():<br/>            action_values = self.qnetwork_local(state)<br/>        self.qnetwork_local.train()<br/><br/>        #Epsilon -greedy action selction<br/>        if random.random() &gt; eps:<br/>            return np.argmax(action_values.cpu().data.numpy())<br/>        else:<br/>            return random.choice(np.arange(self.action_size))<br/>            <br/>    def learn(self, experiences, gamma):<br/>        """Update value parameters using given batch of experience tuples.<br/>        Params<br/>        =======<br/>            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples<br/>            gamma (float): discount factor<br/>        """<br/>        states, actions, rewards, next_states, dones = experiences<br/>        ## TODO: compute and minimize the loss<br/>        criterion = torch.nn.MSELoss()<br/>        # Local model is one which we need to train so it's in training mode<br/>        self.qnetwork_local.train()<br/>        # Target model is one with which we need to get our target so it's in evaluation mode<br/>        # So that when we do a forward pass with target model it does not calculate gradient.<br/>        # We will update target model weights with soft_update function<br/>        self.qnetwork_target.eval()<br/>        #shape of output from the model (batch_size,action_dim) = (64,4)<br/>        <br/>        predicted_targets = self.qnetwork_local(states).gather(1,actions)<br/>        with torch.no_grad():<br/>            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)<br/><br/>        # .detach() -&gt;  Returns a new Tensor, detached from the current graph.<br/>        labels = rewards + (gamma* labels_next*(1-dones))<br/>        loss = criterion(predicted_targets,labels).to(device)<br/>        self.optimizer.zero_grad()<br/>        loss.backward()<br/>        self.optimizer.step()<br/><br/>        # ------------------- update target network ------------------- #<br/>        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)<br/>            <br/>    def soft_update(self, local_model, target_model, tau):<br/>        """Soft update model parameters.<br/>        θ_target = τ*θ_local + (1 - τ)*θ_target<br/>        Params<br/>        =======<br/>            local model (PyTorch model): weights will be copied from<br/>            target model (PyTorch model): weights will be copied to<br/>            tau (float): interpolation parameter<br/>        """<br/>        for target_param, local_param in zip(target_model.parameters(),<br/>                                           local_model.parameters()):<br/>            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)<br/>            <br/>class ReplayBuffer:<br/>    """Fixed -size buffe to store experience tuples."""<br/>    <br/>    def __init__(self, action_size, buffer_size, batch_size, seed):<br/>        """Initialize a ReplayBuffer object.<br/>        <br/>        Params<br/>        ======<br/>            action_size (int): dimension of each action<br/>            buffer_size (int): maximum size of buffer<br/>            batch_size (int): size of each training batch<br/>            seed (int): random seed<br/>        """<br/>        <br/>        self.action_size = action_size<br/>        self.memory = deque(maxlen=buffer_size)<br/>        self.batch_size = batch_size<br/>        self.experiences = namedtuple("Experience", field_names=["state",<br/>                                                               "action",<br/>                                                               "reward",<br/>                                                               "next_state",<br/>                                                               "done"])<br/>        self.seed = random.seed(seed)<br/>        <br/>    def add(self,state, action, reward, next_state,done):<br/>        """Add a new experience to memory."""<br/>        e = self.experiences(state,action,reward,next_state,done)<br/>        self.memory.append(e)<br/>        <br/>    def sample(self):<br/>        """Randomly sample a batch of experiences from memory"""<br/>    <br/>        experiences = random.sample(self.memory,k=self.batch_size)<br/>        <br/>        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)<br/>        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)<br/>        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)<br/>        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)<br/>        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)<br/>        <br/>        return (states,actions,rewards,next_states,dones)<br/>    <br/>    def __len__(self):<br/>        """Return the current size of internal memory."""<br/>        return len(self.memory)</span></pre><p id="0985" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以训练 DQN 模型。注意，这里动作空间的大小是 21，因为从 0 到最大订货量 20 有 21 个可能的订货量值。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="aa1e" class="ny lu iq nu b be nz oa l ob oc">agent = Agent(state_size=7,action_size=21,seed=0)<br/><br/>def dqn(env, n_episodes= 1000, max_t = 10000, eps_start=1.0, eps_end = 0.01,<br/>       eps_decay=0.995):<br/>    """Deep Q-Learning<br/>    <br/>    Params<br/>    ======<br/>        n_episodes (int): maximum number of training epsiodes<br/>        max_t (int): maximum number of timesteps per episode<br/>        eps_start (float): starting value of epsilon, for epsilon-greedy action selection<br/>        eps_end (float): minimum value of epsilon <br/>        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon<br/>        <br/>    """<br/>    scores = [] # list containing score from each episode<br/>    eps = eps_start<br/>    for i_episode in range(1, n_episodes+1):<br/>        state = env.reset()<br/>        score = 0<br/>        for t in range(max_t):<br/>            action = agent.act(state,eps)<br/>            next_state,reward,done = env.step(action)<br/>            agent.step(state,action,reward,next_state,done)<br/>            ## above step decides whether we will train(learn) the network<br/>            ## actor (local_qnetwork) or we will fill the replay buffer<br/>            ## if len replay buffer is equal to the batch size then we will<br/>            ## train the network or otherwise we will add experience tuple in our <br/>            ## replay buffer.<br/>            state = next_state<br/>            score += reward<br/>            if done:<br/>                print('episode'+str(i_episode)+':', score)<br/>                scores.append(score)<br/>                break<br/>        eps = max(eps*eps_decay,eps_end)## decrease the epsilon<br/>    return scores<br/><br/>env = InvOptEnv(demand_hist)<br/>scores= dqn(env)<br/><br/>plt.plot(np.arange(len(scores)),scores)<br/>plt.ylabel('Reward')<br/>plt.xlabel('Epsiode #')<br/>plt.show()<br/><br/>torch.save(agent.qnetwork_local.state_dict(), desired_path)</span></pre><p id="e6e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图是 DQN 模型训练 1000 集后每集获得的总奖励。我们看到回报曲线逐渐改善，最终趋于一致。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3fcdd0fa1e8dc75805aa2560d933a235.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*nZQWXSSKqCACHr8dzGBwJA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每次训练获得的奖励(图片由作者提供)</p></figure><h2 id="1392" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">用于优化(S，S)策略的代码</h2><p id="3484" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">由于<em class="ls"> s </em>和<em class="ls"> S </em>都是离散值，在这个问题中可能的(<em class="ls"> s </em>，<em class="ls"> S </em> ) <em class="ls"> </em>组合数量有限。我们不会考虑将<em class="ls"> s </em>设置为低于 0 <em class="ls">，</em>，因为仅在缺货时再订购没有意义。所以<em class="ls"> s </em>的值可以从 0 到<em class="ls"> S </em> -1。对于<em class="ls"> S </em>的值，我们给出了一点额外的空间，允许<em class="ls"> S </em>到<em class="ls">T23】取高于容量的值。由于订单不会立即到达，并且在提前期内可能会有需求到达，因此产能不应成为<em class="ls"> S </em>的限制。这里我们让<em class="ls"> S </em>从 1 到 60。</em></p><p id="64f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，我们可以评估历史需求数据集中所有可能的组合，并选择利润最高的组合。得到的最佳(<em class="ls"> s </em>，<em class="ls"> S </em>)组合为(15，32)。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="490b" class="ny lu iq nu b be nz oa l ob oc">def profit_calculation_sS(s,S,demand_records):<br/>    total_profit = 0<br/>    inv_level = 25 # inventory on hand, use this to calculate inventory costs<br/>    lead_time = 2<br/>    capacity = 50<br/>    holding_cost = 3<br/>    fixed_order_cost = 50<br/>    variable_order_cost = 10<br/>    unit_price = 30<br/>    order_arrival_list = []<br/>    for current_period in range(len(demand_records)):<br/>        inv_pos = inv_level<br/>        if len(order_arrival_list) &gt; 0:<br/>            for i in range(len(order_arrival_list)):<br/>                inv_pos += order_arrival_list[i][1]<br/>        if inv_pos &lt;= s:<br/>            order_quantity = min(20,S-inv_pos)<br/>            order_arrival_list.append([current_period+lead_time, order_quantity])<br/>            y = 1<br/>        else:<br/>            order_quantity = 0<br/>            y = 0<br/>        if len(order_arrival_list) &gt; 0:<br/>            if current_period == order_arrival_list[0][0]:<br/>                inv_level = min(capacity, inv_level + order_arrival_list[0][1])<br/>                order_arrival_list.pop(0)<br/>        demand = demand_records[current_period]<br/>        units_sold = demand if demand &lt;= inv_level else inv_level<br/>        profit = units_sold*unit_price-holding_cost*inv_level-y*fixed_order_cost-order_quantity*variable_order_cost<br/>        inv_level = max(0,inv_level-demand)<br/>        total_profit += profit<br/>    return total_profit<br/>    <br/>s_S_list = []<br/>for S in range(1,61): # give a little room to allow S to exceed the capacity <br/>    for s in range(0,S):<br/>        s_S_list.append([s,S])  <br/>        <br/>profit_sS_list = []<br/>for sS in s_S_list:<br/>    profit_sS_list.append(profit_calculation_sS(sS[0],sS[1],demand_hist))<br/><br/>best_sS_profit = np.max(profit_sS_list) <br/>best_sS = s_S_list[np.argmax(profit_sS_list)]</span></pre><h2 id="e704" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">测试 DQN 策略的代码</h2><p id="204a" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们首先创建 100 个客户需求数据集进行测试。100 个数据集中的每一个都包含 52 周的需求数据。我们可以将每个数据集视为未来 1 年需求的可能情景。然后，我们在每个需求数据集上评估 DQN 策略，并收集每个数据集的总报酬。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="9ea9" class="ny lu iq nu b be nz oa l ob oc">demand_test = []<br/>for k in range(100,200):<br/>    np.random.seed(k)<br/>    demand_future = []<br/>    for i in range(52):<br/>        for j in range(4):<br/>            random_demand = np.random.normal(3, 1.5)<br/>            if random_demand &lt; 0:<br/>                random_demand = 0<br/>            random_demand = np.round(random_demand)<br/>            demand_future.append(random_demand)<br/>        random_demand = np.random.normal(6, 1)<br/>        if random_demand &lt; 0:<br/>            random_demand = 0<br/>        random_demand = np.round(random_demand)<br/>        demand_future.append(random_demand)<br/>        for j in range(2):<br/>            random_demand = np.random.normal(12, 2)<br/>            if random_demand &lt; 0:<br/>                random_demand = 0<br/>            random_demand = np.round(random_demand)<br/>            demand_future.append(random_demand)<br/>    demand_test.append(demand_future)</span></pre><pre class="of nt nu nv bn nw nx bi"><span id="462c" class="ny lu iq nu b be nz oa l ob oc">model = QNetwork(state_size=7,action_size=21,seed=0)<br/>model.load_state_dict(torch.load(desired_path))<br/>model.eval()<br/><br/>profit_RL = []<br/>actions_list = []<br/>invs_list = []<br/><br/>for demand in demand_test:<br/>    env = InvOptEnv(demand)<br/>    env.reset()<br/>    profit = 0<br/>    actions = []<br/>    invs = []<br/>    done = False<br/>    state = env.state<br/>    while not done:<br/>        state = torch.from_numpy(state).float().unsqueeze(0).to(device)<br/>        with torch.no_grad():<br/>            action_values = model(state)<br/>        action = np.argmax(action_values.cpu().data.numpy())<br/>        actions.append(action)<br/>        next_state, reward, done = env.step(action)<br/>        state = next_state<br/>        invs.append(env.inv_level)<br/>        profit += reward<br/>    actions_list.append(actions)<br/>    invs_list.append(invs)<br/>    profit_RL.append(profit)<br/>RL_mean = np.mean(profit_RL)</span></pre><h2 id="c804" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">测试<em class="og"> (s，S) </em>策略的代码</h2><p id="651b" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们在同一个测试集上评估(<em class="ls"> s </em>，<em class="ls"> S </em>)策略。</p><pre class="kg kh ki kj gt nt nu nv bn nw nx bi"><span id="6041" class="ny lu iq nu b be nz oa l ob oc">profit_sS = []<br/>for demand in demand_test:<br/>    profit_sS.append(profit_calculation_sS(15,32,demand))<br/>sS_mean = np.mean(profit_sS)</span></pre><h2 id="af2e" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">对数值结果的讨论</h2><p id="d1dd" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在 100 个需求数据集上，DQN 保单的平均利润为$20314.53，(<em class="ls"> s </em>，<em class="ls"> S </em>)保单的平均利润为$17202.08，这表明利润增加了 18.09%。DQN 和(<em class="ls"> s </em>，<em class="ls"> S </em>)政策在 100 个需求数据集上获得的利润箱线图如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/8ac8ae184bf297a20da1ce780a107080.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*elfzJwRFsbH1ruoI2dIs2g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试集中 DQN 策略和(S，S)策略获得的利润的箱线图(图片由作者提供)</p></figure><p id="eb50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了进一步理解 DQN 和(<em class="ls"> s </em>，<em class="ls"> S </em>)策略之间的差异，我们在测试集中挑选一个需求数据集，并仔细查看 DQN 策略和(<em class="ls"> s </em>，<em class="ls"> S </em>)策略分别在前两周采取的操作。见下表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/3324979b26ce144d7ec54080ab609e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxEZSNupqmfNVo5-Rjr5Rw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DQN 和(S，S)政策所采取行动的比较(图片由作者提供)</p></figure><p id="cf25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，DQN 的政策更能响应客户需求，并倾向于下更多的订单，以减少潜在的销售损失。DQN 政策确实会产生更多的订购成本，但是，与销售额的增长相比，订购成本的增长要低得多。</p><h1 id="6e87" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="6bea" class="pw-post-body-paragraph kw kx iq ky b kz mn jr lb lc mo ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在这篇文章中，我提出了一个优化零售商店库存运作的 DQN 模型。DQN 政策优于经典的(<em class="ls"> s </em>，<em class="ls"> S </em>)政策，因为它在做出订购决定时提供了更多的灵活性，因此更能响应客户的需求。</p><p id="c59f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">再论 DQN 政策与(<em class="ls">的</em>，<em class="ls">的</em>)政策的比较。只有当需求结构足够复杂时，DQN 模型往往优于(<em class="ls"> s </em>，<em class="ls"> S </em>)策略，因此我们可以利用一些其他信息来推断提前期需求分布在状态定义中是什么样的。例如，这里我们假设客户需求在一周的不同日期遵循不同的分布，因此 DQN 可以利用一周中的日期信息来推断提前期内接下来两天的需求情况。与仅根据库存状况做出决策的(<em class="ls"> s </em>，<em class="ls"> S </em>)政策相比，这些额外信息有助于我们做出更明智的决策。然而，如果没有这种有用的额外信息被包括在国家定义中，DQN 只能勉强击败(<em class="ls"> s </em>，<em class="ls"> S </em>)政策。我尝试用略有不同的状态定义来训练 DQN 模型，假设每天的需求都遵循相同的负二项分布。DQN 政策实际上不如 T21 政策。</p><p id="1b75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是我未来的文章可能会探讨的几个主题。首先，我采用 DQN 来解决这个问题。看看其他 RL 框架(如策略优化类中的那些)是否可以获得更好的性能，因为它们可以输出随机策略，这将是很有趣的。第二，在本文中，我主要关注一个非常简单的供应链模型，它只包含一个零售商。看看如何利用 RL 技术来优化更复杂的供应链模型(如多级网络)也是很有意思的。对于复杂的供应链网络，逆向物流技术可能会显示出更大的优势。</p><p id="d56d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>