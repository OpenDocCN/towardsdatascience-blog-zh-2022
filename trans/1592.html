<html>
<head>
<title>Structure Prediction and Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">结构预测和学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/structure-prediction-and-learning-687a35eff84f#2022-04-16">https://towardsdatascience.com/structure-prediction-and-learning-687a35eff84f#2022-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ab35" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预测建模与结构推理相结合</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/64c0b0bf0d0e7e69728cebfc35ea73f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2mDyJU1Irpn_4XBj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@followtherabbit?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Evgeniy Surzhan </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="c8bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">监督机器学习包括从一些输入中预测一个<em class="lv">结果</em>变量的值。通常，结果是实值或分类的。</p><p id="c9b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">结构化预测</em>将其推广到预测具有显式<em class="lv">结构</em>或由多个<em class="lv">交互</em>标量结果组成的结果。让我们看看涉及显式结构的例子。</p><p id="4175" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结构化结果</strong></p><p id="4a83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是典型的序列、树或图。我们将看到每一个的例子。大部分摘自[2]。</p><p id="f134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">光学字符识别</strong>:识别手写图像中的文字。单词是由原子和单个字符组成的结构化结果。(结构化的)结果空间是所有单词的字典。它允许将单词级信息与单个字符的OCR相结合，从而获得更高的准确性。例如，如果第一个字母看起来像<em class="lv"> c </em>，第三个像<em class="lv"> r </em>，第二个像一个<em class="lv"> o </em>或一个<em class="lv"> a </em>，那么这个单词很可能就是<em class="lv"> car </em>。</p><p id="75f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> NLP解析</strong>:预测句子的解析树，即单词序列。结果空间是所有解析树的集合。</p><p id="d80a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">物体识别</strong>:识别图像或视频中的物体。不是单独地，而是集体地，作为一个场景图。也就是说，作为一个结构化的结果。</p><p id="9bbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蛋白质结构预测</strong>:从蛋白质的序列预测蛋白质的二级或三级结构。</p><p id="57d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">短语翻译</strong>:将英语短语翻译成法语短语。</p><p id="ba85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">词性标注</strong>:输入一种语言(比如英语)的单词序列，输出单词(名词、动词、形容词等)的词性标签序列。结果是结构化的，因为我们想要输出POS标签的<em class="lv">序列</em>。为单个单词预测的标签会相互影响，因此预测每个单词的最佳标签并简单地组合它们是不够的。</p><p id="02f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">走向结构化感知器</strong></p><p id="d728" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">关节特征功能</strong></p><p id="6bff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中一个关键思想是联合特征函数<strong class="lb iu"> </strong> F( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)，其中<strong class="lb iu"> x </strong>是输入，<strong class="lb iu"> y </strong>是输出。考虑词性标注。现在，我们仅限于标记一个单词，表示为<em class="lv"> x </em>。如果我们认为<em class="lv"> x </em>的某些方面与<em class="lv"> y </em>的某些方面相关，我们可以将它们捕获到特征中。下面是一些例子，来自[5]。</p><ul class=""><li id="c554" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv"> x </em>的第一个字母大写，<em class="lv"> y </em> =名词</li><li id="b2ba" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv"> x </em> <strong class="lb iu"> </strong>以<em class="lv"> ed </em>，<em class="lv"> y </em> <strong class="lb iu"> </strong> =动词</li></ul><p id="91e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用联合特征函数表示的HMMs</strong></p><p id="9197" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在[6]之后，这在一个例子中最容易看到。认为</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="f420" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">x</strong>   The   dog    ate    the   homework<br/><strong class="ml iu">y   </strong>Det → Noun → Verb → Det → Noun</span></pre><p id="ca74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里→表示HMM中的状态转换。在HMM下分解的联合概率是</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="32f0" class="mp mq it ml b gy mr ms l mt mu"><em class="lv">P</em>(<strong class="ml iu">x</strong>,<strong class="ml iu">y</strong>) = <em class="lv">T</em>(Det|Begin)*<em class="lv">E</em>(The|Det)*<em class="lv">T</em>(Noun|Det)*<em class="lv">E</em>(dog|Noun)*…</span></pre><p id="45d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<em class="lv"> T </em> (B|A)表示HMM中从状态A转移到状态B的概率，<em class="lv"> E </em> (B|A)表示从状态A发射B的概率，Begin表示开始计算<em class="lv"> P </em> ( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)时HMM开始的状态。</p><p id="aa68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> P </em> ( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)就是从状态<em class="lv">开始</em>跃迁到状态<em class="lv"> Det </em>的概率，然后从状态<em class="lv"> Det </em>发射字<em class="lv">到</em>，再从状态<em class="lv"> Det </em>跃迁到状态<em class="lv">名词</em>，然后发射字<em class="lv"> dog【的概率诸如此类。</em></p><p id="7a82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拿走木头</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="b145" class="mp mq it ml b gy mr ms l mt mu">log <em class="lv">P</em>(<strong class="ml iu">x</strong>,<strong class="ml iu">y</strong>) = log <em class="lv">T</em>(Det|Begin) + log <em class="lv">E</em>(The|Det) + log <em class="lv">T</em>(Noun|Det) + log <em class="lv">E</em>(dog|Noun) + …                                              (1)</span></pre><p id="a884" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等式(1)可以重新表示为</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="b6b2" class="mp mq it ml b gy mr ms l mt mu">S(<strong class="ml iu">x</strong>, <strong class="ml iu">y</strong>) = <em class="lv">w</em>_{<em class="lv">T</em>, Begin, Det}*F_{<em class="lv">T</em>, Begin, Det}(Begin, y1) +<br/><em class="lv">w</em>_{<em class="lv">E</em>, Det, The}*<em class="lv">F</em>_{<em class="lv">E</em>, Det, The}(x1) + …                          (2)</span></pre><p id="5679" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里</p><p id="4e29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> S </em> ( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)是log <em class="lv"> P </em> ( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)，<em class="lv"> w </em> _{ <em class="lv"> T </em>，Begin，Det}是log <em class="lv"> T </em> (Det|Begin)，F _ {【Begin 是log <em class="lv"> E </em> (The|Det)，F_{ <em class="lv"> E </em>，Det，如果<em class="lv">x</em>1，<em class="lv"> y </em> 1是，则为1，如果<em class="lv"> y </em> 1是Det，则为0</p><p id="b5d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如此类。</p><p id="03fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乍一看，方程(2)似乎只是方程(1)的一种更不透明的写法。</p><p id="a81b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而还有更多。事实证明，很容易将等式(2)变形为比等式(1)更丰富的得分函数。要看到这一点，考虑一下我们句子中的单词<em class="lv">x</em>3 =<em class="lv">eat</em>。方程(1)中其值受其影响的项是log <em class="lv"> E </em> (ate|Verb)，在方程(2)的符号中是</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="4409" class="mp mq it ml b gy mr ms l mt mu"><em class="lv">w</em>_{<em class="lv">E</em>, Verb, ate}*F_{<em class="lv">E</em>, Verb, ate}(<em class="lv">x</em>3, <em class="lv">y</em>3)</span></pre><p id="d21b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在想象用一个不同的词来代替<em class="lv"> x </em> 3，比如说<em class="lv">咀嚼</em>。此外，假设<em class="lv">咀嚼过的</em>在我们的数据集中没有出现过。所以<em class="lv"> E </em>(咀嚼|动词)未知。</p><p id="ca7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑添加以下特征函数F_{ <em class="lv"> E </em>，动词，*ed}(x，y)。它有一个可学习的参数<em class="lv"> w </em> _{ <em class="lv"> E </em>，动词，*ed}与之配对。F_{ <em class="lv"> E </em>，动词，*ed}( <em class="lv"> x </em> 3，<em class="lv"> y </em> 3)如果<em class="lv"> x </em> 3以ed结尾，<em class="lv"> y </em> 3为动词，则为1，否则为0。</p><p id="554b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这个函数正在被<em class="lv">添加到我们的得分函数中。如果在训练集中看到了<em class="lv">嚼过的</em>，那么在对<em class="lv"> x </em> =嚼过的，<em class="lv"> y </em> =动词的评分中，两个特征函数会相加地发挥作用</em></p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="a0f7" class="mp mq it ml b gy mr ms l mt mu"><em class="lv">w</em>_{<em class="lv">E</em>, Verb, chewed}*F_{<em class="lv">E</em>, Verb, chewed}(<em class="lv">x</em>, <em class="lv">y</em>)+<br/><em class="lv">w</em>_{<em class="lv">E</em>, Verb, *ed}*F_{<em class="lv">E</em>,Verb, *ed}(<em class="lv">x</em>, <em class="lv">y</em>)</span></pre><p id="d365" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个意义上来说，有些冗余。另一方面，F_{ <em class="lv"> E </em>，动词，*ed}潜在地概括得更广泛，因为它适用于在训练期间没有看到但当从状态<em class="lv">动词</em>发出时以<em class="lv"> ed </em>结尾的单词。</p><p id="dd38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">高效推理</strong></p><p id="98b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，score函数在支持哪些特性方面的能力非常重要。同样重要的是，我们可以使用score函数高效地进行推理。在我们的设置中，最重要的推断是寻找一个对于给定的<strong class="lb iu"> x </strong>最大化<em class="lv"> S </em> ( <strong class="lb iu"> x </strong>，<strong class="lb iu"> y </strong>)的<strong class="lb iu"> y </strong>。在我们运行的示例中，这将为我们提供给定单词序列的最佳POS标签序列。</p><p id="6813" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">HMM中的有效推理</p><p id="077f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了寻找对于给定的<strong class="lb iu"> x </strong>使方程(1)最大化的<strong class="lb iu"> y </strong>，我们可以利用HMM的特殊结构，使其遵守<em class="lv">最优化原则</em>:从小问题的最优解，我们可以建立更大问题的最优解。</p><p id="f514" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更准确地理解这一点，让我们在我们的词性标注示例中举例说明。我们为<em class="lv">寻找一个最优标签序列的分数，狗吃了作业</em>。对于<em class="lv">作业</em>的每一个可能的标签<em class="lv"> t </em>，让我们在<em class="lv">作业</em>必须从<em class="lv"> t </em>发出的附加约束下，求<em class="lv">最优解的分数狗吃了</em>。现在我们有了几个子问题的最优分数，每个值对应一个<em class="lv"> t </em>。我们将这些分数乘以从<em class="lv"> t </em>发出<em class="lv">作业</em>的概率。然后，在从<em class="lv"> t </em>发出<em class="lv">作业</em>的附加约束下，每个结果分数对于整个句子都是最优的。取这些分数中的最大值，我们就得到一个去掉了这个约束的最佳分数。</p><p id="f651" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在前面的段落中直观地描述了维特比算法。</p><p id="84af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">高效的推论更一般一点</strong></p><p id="ae5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，等式(2)给出了比等式(1)更丰富的得分函数。推理的效率呢？我们将通过一个案例研究间接回答这个问题，这个案例研究使用了比普通HMM更丰富的评分函数，同时确保推理仍然有效。我们采用这种教学方法是因为我们觉得“确保推理保持有效”最好用一个真实的例子来理解。</p><p id="e7ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">功能更加丰富的词性标注</strong></p><p id="13a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将从HMM的所有特征函数开始。特别地，在等式(2)的符号中重新表达的等式(1)的那些。对此，我们将在每个新添加的功能适用于单个(<em class="lv">单词</em>、<em class="lv">状态</em>)对的约束下添加更多的功能。这些新增加的可以被视为HMM的发射函数的增强版本。</p><p id="6485" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到一个了，F_{ <em class="lv"> E </em>，动词，*ed}( <em class="lv"> x </em>，<em class="lv"> y </em>)。这可以看作是一种发射:从状态动词<em class="lv">发出以<em class="lv"> ed </em>结尾的单词的概率。</em></p><p id="41cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">读者可能会猜测这是新增加的特征函数对一对(<em class="lv">单词</em>，<em class="lv">状态</em>)的约束，这将允许我们保持推理的效率。</p><p id="23de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再来看一个这样的函数:F_{ <em class="lv"> E </em>，名词，first _ letter _ is _ capitalized }(<em class="lv">x</em>，<em class="lv"> y </em>)。F_{ <em class="lv"> E </em>，名词，first _ letter _ is _ capitalized }(<em class="lv">x</em>，<em class="lv"> y </em>)如果单词<em class="lv"> x </em>以大写字母开头，<em class="lv"> y </em>为名词，则为1，否则为0。增加这个功能有什么帮助？</p><p id="0492" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Say <em class="lv"> x </em>是在推理过程中看到的，但在我们的训练集中没有遇到的词。说<em class="lv"> x </em>的第一个字母是大写的。在没有这个特征函数的情况下，我们最多只能说,<em class="lv"> x </em>和任何其他未知单词一样有可能是名词。有了这个特性，并且假设一个单词的首字母大写和它的POS标签是名词<em class="lv">之间确实存在关联，我们将能够预测<em class="lv"> x </em>是一个更好的名词<em class="lv"/>。</em></p><p id="b642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以认为这些特性的增加允许我们将领域知识整合到我们的推理引擎中。</p><p id="cf29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">推理效率</strong></p><p id="5bca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们每个新添加的特征函数仅适用于(<em class="lv">单词</em>，<em class="lv">状态</em>)对，所以我们的推理中的最优性原则继续有效。</p><p id="ddcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了看到这一点，让我们首先回顾一下我们对HMM中推理效率的解释，这是通过词性标注的例子给出的。我们在下面逐字重复。</p><p id="8e2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们为<em class="lv">寻找一个最优标签序列的分数，狗吃了作业</em>。对于<em class="lv">作业</em>的每一个可能的标签<em class="lv"> t </em>，让我们在<em class="lv">作业</em>必须从<em class="lv"> t </em>发出的附加约束下，求<em class="lv">狗吃了</em>的最优解的分数。现在我们有了几个子问题的最优分数，每个值对应一个<em class="lv"> t </em>。我们将这些分数乘以从<em class="lv"> t </em>发出<em class="lv">作业</em>的概率。然后，在从<em class="lv"> t </em>发出<em class="lv">作业</em>的附加约束下，每个结果分数对于整个句子都是最优的。取这些分数中的最大值，我们就得到一个去掉了这个约束的最佳分数。</p><p id="854a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使在添加了我们的新特性函数之后，这个解释仍然有效。具体是因为新增的特征函数适用于(<em class="lv">字</em>、<em class="lv">态</em>)对的约束。</p><p id="fc6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有可能稍微放松我们对新特征函数的限制，并且仍然保持最优性原则。</p><p id="e216" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">自动发现新功能</strong></p><p id="0c2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将我们自己限制在适用于(<em class="lv"> word </em>，<em class="lv"> state </em>)对的特征函数在另一方面是有用的。我们可以使用监督学习来自动发现这种类型的有效特征函数。这种发现所需的训练集是一组(<em class="lv">单词</em>，POS-tag)对。<em class="lv">字</em>被输入；贴上标签。标签可能会有噪音，这意味着一个单词的标签不一定是唯一的。(如果标签是唯一的，词性标注问题本身就不需要结构化预测。)</p><p id="a292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输入的单词中，我们可以选择提取任何我们认为可以预测其标签的特征。比如第一个字母是否大写，单词的最后<em class="lv"> k </em>个字母是什么，对于<em class="lv"> k </em> = 1，2，3，…单词的第一个<em class="lv"> k </em>个字母是什么，对于<em class="lv"> k </em> = 1，2，3，…</p><p id="3ede" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个问题的标签数据集上运行监督学习算法，我们可以找到高度相关的对(<em class="lv"> f </em> ( <em class="lv"> x </em>)，<em class="lv"> y </em>)。这里的<em class="lv"> f </em> ( <em class="lv"> x </em>)是单词<em class="lv"> x </em>的一个特定特征。(即使是一个简单的监督学习算法，一个明确学习<em class="lv">P</em>(<em class="lv">y</em>|<em class="lv">f</em>(<em class="lv">x</em>))的算法也可能是有效的。每个这样的对(<em class="lv"> f </em> ( <em class="lv"> x </em>)，<em class="lv"> y </em>)定义了我们发现的关节特征函数。</p><p id="9f5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">学习得分函数的权重</strong></p><p id="2c84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我们已经关注了推理的效率。这假设我们已经以某种方式了解了得分函数中的权重，该权重决定了各种特征函数对总得分的贡献。我们现在解决这个问题。</p><p id="16a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在我们正在运行的词性标注示例中说明这一点。假设我们有一组可用的(<em class="lv">单词</em>、<em class="lv">标签</em>)对。这里的<em class="lv">单词</em>表示一系列单词。如在一个句子、一个短语甚至整个文档中。标签表示相应的位置标签序列。</p><p id="a304" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是这种训练集中的一对示例。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="cfa1" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">words</strong> The dog  ate  the homework<br/><strong class="ml iu">tags  </strong>Det Noun Verb Det Noun</span></pre><p id="fc43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习权重的一种方法是遵循HMM方法。也就是说，用概率表示它们，并直接从训练集中估计这些概率。</p><p id="fba8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一下<em class="lv"> w </em> _{ <em class="lv"> E </em>，Det，The}我们表示为log <em class="lv"> E </em> (The|Det)。量<em class="lv"> E </em> (The|Det)是从状态Det发出的概率。这就是训练集中来自状态<em class="lv"> Det </em>的单词<em class="lv">的发射次数除以来自状态Det的发射总数。类似地，转换权重<em class="lv"> w </em> _{ <em class="lv"> T </em>，Det，Noun}是log <em class="lv"> T </em> (Noun|Det)，其中<em class="lv"> T </em>是从状态<em class="lv"> Det </em>转换到状态<em class="lv"> Noun </em>的概率。这就是从状态<em class="lv"> Det </em>到状态<em class="lv">名词</em>的转换次数除以训练集中状态<em class="lv"> Det </em>的出现次数。</em></p><p id="9317" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过添加伪计数来稍微平滑这些估计。这个想法是要涵盖在训练集中从未出现过的概率略大于0的事件。</p><p id="cc0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结构化感知器</strong></p><p id="1139" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种更精细的学习方法包括错误驱动学习。我们从一个初始模型开始，如下迭代改进它。</p><p id="423d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -</p><ol class=""><li id="a086" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mv mc md me bi translated">呈现<em class="lv">字</em></li><li id="caf4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated">从当前模型预测<em class="lv">预测标签</em></li><li id="9734" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated">调整模型以使预测的标签更好地与(目标)<em class="lv">标签</em>对齐。</li></ol><p id="cb88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -</p><p id="a772" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">错误驱动学习，也称为判别学习，可能产生比上述概率学习更准确的模型。</p><p id="db4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤2可以使用维特比算法来完成。</p><p id="34ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解释步骤3，即进行辨别学习的步骤，形式符号将是方便的。当输入<em class="lv">字</em>时，让<strong class="lb iu"> t_pred </strong>表示应用于当前模型的维特比算法产生的标签序列，让<strong class="lb iu"> t_target </strong>表示<em class="lv">标签</em>，标签的目标序列。</p><p id="9318" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，在我们运行的例子中，我们的特征函数的形式是F_{ <em class="lv"> T </em>，curr_tag，next_tag}( <em class="lv"> t </em> 1，t2)或F_{ <em class="lv"> E </em>，word，tag}( <em class="lv"> w </em>，<em class="lv"> t </em>)。这里curr_tag、next_tag、tag表示特定的标签，<em class="lv"> word </em>表示字典中的特定单词，<em class="lv"> w </em>表示单词变量，<em class="lv"> t </em> 1、<em class="lv"> t </em> 2、<em class="lv"> t </em>表示标签变量。</p><p id="ef12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在呈现鉴别学习时，我们将如下表达特征函数。设F_ <em class="lv"> i </em> ( <strong class="lb iu"> w </strong>，<strong class="lb iu"> t </strong>)表示第<em class="lv"> i </em>个特征函数。这里的<strong class="lb iu"> w </strong>是一个单词序列，而<strong class="lb iu"> t </strong>是一个相应的标签序列。我们这样做是为了揭示这种辨别学习的更一般的性质(见下文)。事实证明，这种符号也更简洁。</p><p id="77ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个<em class="lv"> i </em>，我们将在中更新重量<em class="lv">，如下所示:</em></p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="8e17" class="mp mq it ml b gy mr ms l mt mu"><em class="lv">wi</em> += a*(Fi(<strong class="ml iu">w</strong>, <strong class="ml iu">t_target</strong>)-Fi(<strong class="ml iu">w</strong>, <strong class="ml iu">t_pred</strong>))                         (SP1</span></pre><p id="45ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的<em class="lv"> a </em>是一个小的正常数，称为学习率。</p><p id="6c52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等式(SP1)用更新权重<em class="lv">，试图使<strong class="lb iu"> t_pred </strong>向<strong class="lb iu"> t_target </strong>移动。让我们在运行的示例中的一个特定场景中说明这一点。说一下，对于某个<em class="lv"> j </em>，<em class="lv"> wj </em>是<em class="lv">啃过的</em>，<em class="lv"> tj </em> _target是<em class="lv">动词</em>。假设在<strong class="lb iu"> w </strong>上运行维特比算法，当前模型上的完整单词序列(其中仅显示了<em class="lv"> wj </em>)产生<em class="lv"> tj </em> _pred为<em class="lv">名词</em>。</em></p><p id="d035" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑特征函数2*F_{ <em class="lv"> E </em>，*ed，Verb}( <em class="lv"> w </em>，<em class="lv"> t </em> )-1。它的值对于(<em class="lv">嚼过的</em>，动词)为1，对于(<em class="lv">嚼过的</em>，<em class="lv">名词</em>)为-1。所以我们会增加w_{ <em class="lv"> E </em>，*ed，Verb}的值。如果Viterbi算法推断出除了动词<em class="lv">之外的任何标签</em>是发生<em class="lv">咀嚼</em>的标签，这将更加不利于Viterbi算法。</p><p id="22ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">总结</strong></p><p id="120c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们介绍了结构预测的问题，并讨论了常见的用例。接下来，我们讨论了联合特征函数的重要概念，以及它如何概括HMM中的关键计算。接下来，我们讨论了HMM中有效推理的原因和方式。由此，我们开始讨论在联合特征函数的更一般的设置中的有效推理。最后，我们引入判别学习来训练联合特征函数的权值。这在结构化感知器中达到了顶峰:在更一般的联合特征函数设置中，混合了有效的推理和鉴别学习。简而言之，相对于HMM，我们获得了更丰富的表示(联合特征函数)和更准确的学习，同时保持了推理的效率。</p><p id="0a69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读</strong></p><ol class=""><li id="3e62" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mv mc md me bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/sequence-to-sequence-labeling-with-recurrent-neural-networks-5405716a2ffa">使用递归神经网络进行序列间标记| Arun ja gota |走向数据科学</a></li><li id="c1ac" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated"><a class="ae ky" href="http://mlg.eng.cam.ac.uk/mlss09/mlss_slides/Hoffman_1_2.pdf" rel="noopener ugc nofollow" target="_blank">结构化预测</a></li><li id="264d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/named-entity-recognition-in-nlp-be09139fa7b8">自然语言处理中的命名实体识别。真实世界的用例、模型、方法… </a></li><li id="af19" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Structured_prediction" rel="noopener ugc nofollow" target="_blank">结构化预测—维基百科</a></li><li id="7e51" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated"><a class="ae ky" href="http://www.phontron.com/slides/nlp-programming-en-12-struct.pdf" rel="noopener ugc nofollow" target="_blank">http://www . phontron . com/slides/NLP-programming-en-12-struct . pdf</a></li><li id="b7c5" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mv mc md me bi translated"><a class="ae ky" href="https://svivek.com/teaching/lectures/slides/structured-perceptron/struct-perceptron.pdf" rel="noopener ugc nofollow" target="_blank">https://svivek . com/teaching/讲义/幻灯片/结构化感知器/struct-perceptron.pdf </a></li></ol></div></div>    
</body>
</html>