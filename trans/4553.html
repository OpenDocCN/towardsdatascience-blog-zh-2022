<html>
<head>
<title>What can we learn from posterior distributions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们能从后验分布中学到什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-can-we-learn-from-posterior-distributions-43671a2cb960#2022-10-10">https://towardsdatascience.com/what-can-we-learn-from-posterior-distributions-43671a2cb960#2022-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0ec4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">贝叶斯后验概率的频率主义解释</h2></div><p id="65ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们观察到了来自未知分布<em class="lb"> q </em>的<em class="lb"> N </em> <a class="ae lc" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">独立同分布</a> (iid)样本<em class="lb"> X = (x1，…，xN) </em>。统计学中的一个典型问题是“样本集<em class="lb"> X </em>告诉我们关于分布<em class="lb"> q </em>的什么信息？”。</p><p id="5f64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" href="https://en.wikipedia.org/wiki/Parametric_statistics" rel="noopener ugc nofollow" target="_blank">参数统计方法</a>假设<em class="lb"> q </em>属于一个参数分布族，并且存在一个参数<em class="lb"> θ </em>其中<em class="lb"> q(x) </em>等于所有<em class="lb"> x的参数分布<em class="lb">p(x |θ)</em>；</em>比如<em class="lb"> p(。|θ) </em>可以是单位方差的正态分布，其中<em class="lb"> θ </em>表示其均值。在这个设定中，问题“关于<em class="lb"> q </em>，X 告诉了我们什么？”被翻译成“关于参数<em class="lb"> θ </em>，我们有<em class="lb"> q = p(。|θ) </em>？”。</p><p id="73ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">回答这个问题的贝叶斯方法</a>是使用概率论的规则，并假设<em class="lb"> θ </em>本身是一个随机变量，具有<a class="ae lc" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">先验分布</a> <em class="lb"> p(θ) </em>。先验分布<em class="lb"> p(θ) </em>是我们在之前对<em class="lb"> θ </em> <em class="lb">的假设和猜测的形式化。在这种情况下，我们可以将参数和数据的联合概率分布写在一起:</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/bf869b392cc5f1c5ce25423d0aec2433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DqIv3L10LAbuj4EBwxweKQ.png"/></div></div></figure><p id="a110" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用这个公式，<em class="lb"> X </em>捕获的关于<em class="lb"> θ </em>的所有信息可以在<a class="ae lc" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">后验分布</a>中进行总结</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lp"><img src="../Images/dd0477941da8c8faf7a3010283d4efe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfzkYyvT5le15V-SS-vtCA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">方程式1。</strong>后验分布</p></figure><p id="6f24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">贝叶斯统计是美丽的，自洽的，优雅的:一切都是使用概率论的规则自然得出的，假设总是清晰明了的。但往往看起来神秘而令人费解:(一)关于底层分布<em class="lb"> q </em>我们真的能从后验分布<em class="lb"> p(θ|X) </em>中学到什么？以及(ii)如果我们的假设不成立，例如，如果<em class="lb"> q </em>不属于我们考虑的参数族，该信息的可靠性如何？</p><p id="cf63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我的目标是获得关于这两个问题的一些直觉。我首先分析当样本数<em class="lb"> N </em>很大时后验分布的渐近形式——这是一种研究贝叶斯推理的频率主义方法。第二，我展示了一般理论如何应用于高斯家族的简单情况。第三，我使用模拟和分析，对于三个案例研究，后验分布如何与数据的基本分布相关，以及这种联系如何随着<em class="lb"> N </em>的增加而变化。</p><h1 id="d52e" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">理论:大<em class="mn"> N </em>的渐近情况</h1><p id="277e" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">等式1中的后验分布的对数可以重新表示为</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mt"><img src="../Images/2744e2e46b9f0a98d316179ad89d6bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItKD-En7E3UH1pSokbtVrg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">方程式2。</strong>对数后验分布</p></figure><p id="c5ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等式2中的常数(相对于<em class="lb"> θ </em>)仅对于归一化后验概率分布是重要的，并不影响它如何作为<em class="lb"> θ </em>的函数而变化。对于较大的<em class="lb"> N </em>，我们可以使用大数定律，并通过下式近似等式2中的第二项(<a class="ae lc" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">对数似然</a>的总和)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mu"><img src="../Images/6b30f385f15496bce3fd576bbc0ab12d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Ai1cxDD16MK9s8DTIyjSg.png"/></div></div></figure><p id="a89e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> D-KL </em>是<a class="ae lc" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> Kullback-Leibler散度</a>并且测量真实分布<em class="lb"> q </em>和参数分布<em class="lb"> p(.|θ) </em>。然而，重要的是要注意，只有当<em class="lb"> log p(x|θ) </em>的均值和方差(相对于<em class="lb"> q </em>)对于某个参数<em class="lb"> θ </em>是有限的时，近似才有效。我们将在下一节中进一步讨论这个条件的重要性。</p><p id="fed4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果<em class="lb"> p(θ) </em>在参数空间上具有完全的<a class="ae lc" href="https://en.wikipedia.org/wiki/Support_(mathematics)" rel="noopener ugc nofollow" target="_blank">支持</a>(即，总是非零)，那么<em class="lb"> log p(θ) </em>总是有限的，并且对于大的<em class="lb"> N </em>，等式2中的主导项是<em class="lb"> D-KL [q || p(。|θ)] </em>乘以<em class="lb"> N. </em>这意味着增加样本数<em class="lb"> N </em>会使后验分布<em class="lb"> p(θ|X) </em>越来越接近该分布</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mv"><img src="../Images/2e5aa6b1fd2a8585d334812ce68cf367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kid7SrdIHdX4PqeUjbR53w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">方程式3 </strong></p></figure><p id="48b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> Z </em>是归一化常数。<em class="lb">p *(θ；N) </em>是一个有趣的分布:它的最大值是散度<em class="lb"> D-KL [q || p(.|θ)] </em>最小(即当<em class="lb"> p(。|θ) </em>尽可能接近<em class="lb"> q </em>，其对<em class="lb">D-KL【q | | p(。|θ)] </em>随着样本数<em class="lb"> N </em>的增加而增加(即，随着<em class="lb"> N </em>的增加，其最大值附近变得更加“狭窄”)。</p><h2 id="6603" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated">当假设正确时</h2><p id="b94d" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">当假设正确并且存在一个<em class="lb"> θ* </em>时，我们有<em class="lb"> q = p(。|θ*) </em>，那么</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mu"><img src="../Images/936cb40e25d298500736de9d1a2bc9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvGDp8rJnWMibvpjpx_bTA.png"/></div></div></figure><p id="c190" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb">D-KL【p(。|θ*) || p(。|θ)】</em>是<em class="lb"> θ </em>和<em class="lb"> θ* </em>之间的伪距离。因此，随着<em class="lb"> N </em>的增加，后验分布集中在真实参数<em class="lb"> θ* </em>附近，为我们提供了完全识别<em class="lb"> q — </em>所需的所有信息，见脚注。</p><h2 id="1d5a" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated">当假设是错误的时候</h2><p id="3b01" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">当没有<em class="lb"> θ </em>时，我们有<em class="lb"> q = p(。|θ) </em>，那么我们永远无法识别真正的底层分布<em class="lb">q</em>——仅仅是因为我们没有在正确的地方搜索！我们强调，这个问题不限于贝叶斯统计，并扩展到任何参数统计方法。</p><p id="e957" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然在这种情况下我们永远无法完全确定<em class="lb"> q </em>，但是后验分布仍然提供了关于<em class="lb"> q </em>的信息:如果我们将<em class="lb"> θ* </em>定义为<em class="lb"> q </em>在参数族空间上的伪投影的参数:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ni"><img src="../Images/94da423805c0beb0c665c1c83315895c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_ZK-g6klmTriVHiIaRK2g.png"/></div></div></figure><p id="facc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，随着<em class="lb"> N </em>的增加，后验分布集中在<em class="lb"> θ* </em>附近，给我们足够的信息来识别参数族中的最佳候选值<em class="lb">q</em>——参见footnote⁴.</p><h2 id="479c" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated">中间总结</h2><p id="4a85" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">随着<em class="lb"> N </em>的增加，后验分布集中在参数<em class="lb"> θ* </em>附近，该参数描述了参数族中最接近实际分布<em class="lb"> q </em>的分布。如果<em class="lb"> q </em>属于参数族，那么最接近<em class="lb"> q </em>的分布就是<em class="lb"> q </em>本身。</p><h1 id="794a" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">示例:高斯分布</h1><p id="f429" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">在上一节中，我们研究了大量样本的后验分布的一般形式。在这里，我们研究一个简单的例子，看看一般理论如何应用于具体情况。</p><p id="90d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们考虑一个简单的例子，其中参数分布是高斯分布，单位方差和平均值等于<em class="lb"> θ </em>:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nj"><img src="../Images/166845242ead0fb636fafc7e636a317e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqP4RJS6Su0K4N2dhqPWOA.png"/></div></div></figure><p id="62e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为简单起见，我们将标准正态分布视为先验<em class="lb"> p(θ) </em>。使用等式1，很容易表明后验分布为</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nk"><img src="../Images/4f92eadb380f680ff760dd0d27ed5016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cblqf9FjjAZ_fmfsXH2Dpg.png"/></div></div></figure><p id="d986" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nl"><img src="../Images/83c88e780bb1e95494d426c8ccdd2bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7say9TE89uC-4yRWJGE0g.png"/></div></div></figure><p id="89ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们还可以确定<em class="lb">p *(θ；N) </em>(见等式3)并将其与后验分布进行比较:只要真实分布的均值和方差<em class="lb"> q </em>是有限的，我们就有</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nm"><img src="../Images/1ac5acf1407a9805c712438b36bedc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVBmCMqYLLqeLngqS0T-1Q.png"/></div></div></figure><p id="ea74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们可以写出(使用等式3)</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nn"><img src="../Images/b3e508ba36f65295b0ed1743e36fa3ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akN6pCgstfdONKhpvY1Muw.png"/></div></div></figure><p id="a20d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi no"><img src="../Images/a2c55e3e7b9dfd85d27c5b59ec28b8a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5-IY8RewX6zYLHhzzhTQA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">方程式4 </strong></p></figure><p id="356c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如一般理论所料，我们可以用<em class="lb">p *(θ；N) </em>为大<em class="lb"> N </em>因为</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi np"><img src="../Images/266ff8b4293ecdb244c23e83c2dc8238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwpYOVfl9gEyXEYON_Lqpw.png"/></div></div></figure><p id="f127" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总而言之，<em class="lb"> p(θ|X) </em>集中在基础分布<em class="lb"> q </em>的真实均值附近——如果它存在的话。</p><h1 id="276e" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">模拟:好的、坏的和丑陋的</h1><p id="1a25" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我们的理论分析有两个关键的假设:(i) <em class="lb"> N </em>很大，以及(ii)log<em class="lb">p(x |θ)</em>的均值和方差(相对于<em class="lb"> q </em>)对于某些<em class="lb"> θ </em>是有限的。在本节中，我们使用模拟来研究如果这些假设不成立，我们的发现有多可靠。</p><p id="0f4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们考虑上一节中示例的简单设置，即具有单位方差的高斯分布族。然后，我们考虑<em class="lb"> q </em>的三种不同选择，分析后验<em class="lb"> p(θ|X) </em>随着<em class="lb"> N </em>增加的演化。</p><p id="2cd7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另外，我们还看看<a class="ae lc" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率(MAP)估计值</a> <em class="lb"> q-MAP-N </em> <em class="lb"> = p(。<em class="lb"> q </em>的|θ-hat-N) </em>随着<em class="lb"> N </em>的增加而演化，其中<em class="lb"> θ-hat-N </em>是<em class="lb"> p(θ|X) </em>的最大化器。这有助于我们理解，通过查看后验distribution⁵.的最大化器，我们可以多么精确地识别真实分布</p><h2 id="2a5f" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated">好:<a class="ae lc" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">高斯分布</a></h2><p id="e3df" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">对于第一种情况，我们考虑最好的情况，即<em class="lb"> q </em>属于参数族，并且满足所有假设:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/656bf748c75c40a70ff7780fd001f134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxE1LpWZTwCpqYJAwoHR8g.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">方程式5。</strong>等价于q(x)=p(x|θ=1)</p></figure><p id="5d83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从<em class="lb"> q </em>抽取10000个样本，通过将抽取的样本对<em class="lb"> N = 1 </em>到<em class="lb">10000</em>逐一相加，得到后验分布<em class="lb"> p(θ|X=(x1，…，xN)) </em>和MAP估计<em class="lb"> q-MAP-N — </em>(图1)。我们观察到当<em class="lb"> N </em>增加<em class="lb"> </em>(图1，左侧)时<em class="lb"> p(θ|X) </em>集中在真实参数周围，并且MAP估计收敛到真实分布<em class="lb"> q </em>(图1，right)⁶.</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nq"><img src="../Images/65564b685c6e5f6af7683e5b0c4927cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Rb1rNP2dUpF0YiTpRG4bQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">图一。高斯分布为q. </strong>左图:作为n的函数的后验分布的均值(黑色实曲线)和标准差(灰色阴影区域)黑色虚线表示q=p(.|θ=1).后验分布收敛于真实参数。垂直彩色线条显示N=2、10、100和1000。右图:N=2、10、100和1000时<em class="mn"> q </em>的MAP估计值(彩色曲线)。黑色虚线表示真实的分布q。</p></figure><h2 id="a86a" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated">坏:<a class="ae lc" href="https://en.wikipedia.org/wiki/Laplace_distribution" rel="noopener ugc nofollow" target="_blank">拉普拉斯分布</a></h2><p id="bd06" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">对于第二种情况，我们考虑将单位均值的拉普拉斯分布作为真实分布:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nr"><img src="../Images/7844b264856f3951224e1b9a40fd9ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-2FVlAUPVPwrO-PBkfFOg.png"/></div></div></figure><p id="602b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，<em class="lb"> q </em>不属于参数族，但它仍然具有有限的均值和方差。因此，根据理论，后验分布应该集中在参数族上<em class="lb"> q </em>的伪投影的参数<em class="lb"> θ* </em>周围。以高斯族为例，<em class="lb"> θ* </em>始终是底层分布的均值，即<em class="lb"> θ* = 1 </em>(见等式4) <em class="lb">。</em></p><p id="8c7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的模拟显示随着<em class="lb"> N </em>的增加<em class="lb"> p(θ|X) </em>确实集中在<em class="lb"> θ* = 1 </em>附近(图2，左侧)。然而，MAP估计收敛到一个与真实分布<em class="lb"> q </em>(图2，右)完全不同的分布，这仅仅是因为我们在高斯分布中搜索拉普拉斯分布！这本质上是任何参数统计方法的问题:<strong class="kh ir">如果搜索错了地方，就找不到正确的分布！</strong></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nq"><img src="../Images/250d77bfcf9bc748b49b33325141d2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Psc5bZFG2_EU93cogYEng.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">图二。拉普拉斯分布为q. </strong>左图:后验分布的均值(黑色实曲线)和标准差(灰色阴影区域)为n的函数，黑色虚线表示q在参数族上的伪投影对应的参数，即θ*=1(见方程4)。后验分布收敛于θ*。垂直彩色线条显示N=2、10、100和1000。右图:N=2、10、100和1000时q的MAP估计值(彩色曲线)。黑色虚线表示真实的分布q。</p></figure><h2 id="9393" class="mw lw iq bd lx mx my dn mb mz na dp mf ko nb nc mh ks nd ne mj kw nf ng ml nh bi translated"><a class="ae lc" href="https://en.wikipedia.org/wiki/Cauchy_distribution" rel="noopener ugc nofollow" target="_blank">柯西分布</a></h2><p id="b169" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">对于我们的第三种也是最后一种情况，我们选择最差的情况，并将柯西分布(著名的<a class="ae lc" href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution" rel="noopener ugc nofollow" target="_blank">重尾分布</a>)视为真实分布:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/8eab667ad46f67f314227e6f6f2adeb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnJywICrtQ4eRTFpOHJIUQ.png"/></div></div></figure><p id="4bfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，<em class="lb"> q </em>不属于参数族，但更关键的问题是柯西分布没有明确定义的均值或有限方差:所有理论的假设都被违反了！</p><p id="2dd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的模拟显示，<em class="lb"> p(θ|X) </em>并不收敛于任何分布(图3，左侧):<em class="lb"> p(θ|X) </em>的标准差趋于零，并集中在其均值附近，但均值本身并不收敛，而是从一个值跳到另一个值。问题是基本的:柯西分布和高斯分布之间的KL散度是无限的，与它们的参数无关！换句话说，根据KL散度，所有的高斯分布都与<em class="lb"> q </em>等距(且无限远)，所以不存在挑哪一个作为其估计的偏好！</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi nq"><img src="../Images/12624357f19b0f031cb25d22766d3445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McqctH-FGbJUCACcBHiRYQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">图三。柯西分布如q. </strong>左图:作为n的函数的后验分布的均值(黑色实曲线)和标准差(灰色阴影区域)，黑色虚线表示q的中值:如果q有均值，那么由于对称性，该均值将等于1。后验分布不收敛于任何分布，其均值从一个值跳到另一个值。垂直彩色线条显示N=2、10、100和1000。右图:N=2、10、100和1000时q的MAP估计值(彩色曲线)。黑色虚线表示真实的分布q。</p></figure></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="6cb2" class="lv lw iq bd lx ly oa ma mb mc ob me mf jw oc jx mh jz od ka mj kc oe kd ml mm bi translated">结论</h1><p id="8e97" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">如果我们假设的参数分布族与真实分布<em class="lb"> q </em>没有<em class="lb">太大差异</em>，那么后验分布总是集中在一个参数周围，该参数在某种程度上提供了关于<em class="lb"> q </em>的信息。然而，如果<em class="lb"> q </em>不在参数族中，该信息可能只是边缘信息，并不真正有用。最坏的情况是当<em class="lb"> q </em>与<em class="lb">参数族中的任何分布</em>相差太大时:在这种情况下，后验分布是无信息的。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><h1 id="4fb4" class="lv lw iq bd lx ly oa ma mb mc ob me mf jw oc jx mh jz od ka mj kc oe kd ml mm bi translated">承认</h1><p id="e36a" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">感谢<a class="ae lc" href="https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&amp;hl=de" rel="noopener ugc nofollow" target="_blank"> Johanni Brea </a>、<a class="ae lc" href="https://mathblasphemy.netlify.app/" rel="noopener ugc nofollow" target="_blank"> Navid Ardeshir </a>、Mohammad Tinati、<a class="ae lc" href="https://scholar.google.com/citations?user=I2ihecwAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Valentin Schmutz </a>、<a class="ae lc" href="http://guillaume.bellec.eu/" rel="noopener ugc nofollow" target="_blank"> Guillaume Bellec </a>和<a class="ae lc" href="https://scholar.google.com/citations?user=1VhZtgoAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Kian Kalhor </a>对相关主题的多次讨论。</p><h1 id="db44" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">代码:</h1><p id="c678" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">用于分析的所有代码(用<a class="ae lc" href="https://julialang.org/" rel="noopener ugc nofollow" target="_blank"> Julia </a>语言)可以在<a class="ae lc" href="https://github.com/modirshanechi/medium_notes/blob/master/MediumRandomNotes/notebooks/Gaussian%20Posterior.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="de5a" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">脚注:</h1><p id="3bed" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">有多个优秀的开放存取教科书，感兴趣的读者可以在其中找到更多关于贝叶斯统计的信息:布拉德利·埃夫隆和特雷弗·哈斯蒂的[1] " <a class="ae lc" href="https://hastie.su.domains/CASI/" rel="noopener ugc nofollow" target="_blank">计算机时代统计推断</a>；安德鲁·格尔曼等人的[2] " <a class="ae lc" href="http://www.stat.columbia.edu/~gelman/book/" rel="noopener ugc nofollow" target="_blank">贝叶斯数据分析</a>；大卫·麦凯的[3] " <a class="ae lc" href="https://www.inference.org.uk/itila/" rel="noopener ugc nofollow" target="_blank">信息论、推断和学习算法</a>。此外，参见脚注3和4以及本<a class="ae lc" href="https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/lectures.html" rel="noopener ugc nofollow" target="_blank">在线课程</a>。</p><p id="ead3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个有趣的练习是估计<em class="lb"> p(θ|X) </em>和<em class="lb">p *(θ；N) </em>都是。作为提示，请注意，您可以在<em class="lb">p *(θ；的最大化器周围使用不同分布的泰勒展开式。N) </em>。好奇的读者可能也想思考这些结果告诉我们关于参数<em class="lb"> θ的最大似然估计的什么。</em></p><p id="c4e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要了解更多关于贝叶斯推断的渐近一致性，你也可以看看<a class="ae lc" href="https://projecteuclid.org/journals/annals-of-statistics/volume-27/issue-2/The-consistency-of-posterior-distributions-in-nonparametric-problems/10.1214/aos/1018031206.full" rel="noopener ugc nofollow" target="_blank"> Barron et al. (1999) </a>和<a class="ae lc" href="https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-5/New-approaches-to-Bayesian-consistency/10.1214/009053604000000409.full" rel="noopener ugc nofollow" target="_blank"> Walker (2004) </a>这两个众所周知的例子。</p><p id="ba31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要了解更多关于模型错误设定下的统计推断，可以看看<a class="ae lc" href="https://projecteuclid.org/journals/annals-of-statistics/volume-34/issue-2/Misspecification-in-infinite-dimensional-Bayesian-statistics/10.1214/009053606000000029.full" rel="noopener ugc nofollow" target="_blank">kle ijn and van der vaart(2006)</a>、<a class="ae lc" href="https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-6/issue-none/The-Bernstein-Von-Mises-theorem-under-misspecification/10.1214/12-EJS675.full" rel="noopener ugc nofollow" target="_blank">kle ijn and van der vaart(2012)</a>、<a class="ae lc" href="https://arxiv.org/abs/1905.10859" rel="noopener ugc nofollow" target="_blank"> Wang and Blei (2019) </a>等几个众所周知的例子。</p><p id="19cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">⁵我们还可以研究<a class="ae lc" href="https://en.wikipedia.org/wiki/Posterior_predictive_distribution" rel="noopener ugc nofollow" target="_blank">后验预测分布</a><em class="lb">q-PPD-n(x)</em><em class="lb">= p(x | x)=∫p(x |θ)p(θ| x)dθ</em>如何随着<em class="lb"> N </em>的增加而演变。在我们同时具有高斯先验和高斯似然的设置中，我们有<em class="lb"> q-MAP-N(x) </em> <em class="lb"> = </em>正常<em class="lb"> (x|μ=θ-hat-N，σ =1) </em>和<em class="lb"> q-PPD-N </em> <em class="lb"> = </em>正常<em class="lb"> (x|μ=θ-hat-N，σ =1+1/(N+1)) </em>。因此，我们对<em class="lb"> q-MAP-N </em>的所有定性观察对于<em class="lb"> q-PPD-N </em>也是正确的。</p><p id="b476" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">⁶有趣的练习给感兴趣的读者:你能计算出作为<em class="lb"> q </em>方差函数的收敛速度吗？根据你的计算，你在图中看到的有意义吗？</p></div></div>    
</body>
</html>