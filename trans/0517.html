<html>
<head>
<title>From Graph ML to Deep Relational Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从图ML到深度关系学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-graph-ml-to-deep-relational-learning-f07a0dddda89#2022-02-21">https://towardsdatascience.com/from-graph-ml-to-deep-relational-learning-f07a0dddda89#2022-02-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ca22" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">走向<a class="ae ep" href="https://medium.com/tag/deep-relational-learning" rel="noopener">深度关系学习</a></h2><div class=""/><div class=""><h2 id="ec93" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><a class="ae ko" rel="noopener" target="_blank" href="/what-is-neural-symbolic-integration-d5c6267dfdb0"> <em class="kp">整合神经网络</em> </a> <em class="kp">与</em> <a class="ae ko" rel="noopener" target="_blank" href="/what-is-relational-machine-learning-afbe4a9c4231"> <em class="kp">关系学习</em> </a> <em class="kp">以达到超越当前状态的结构化深度学习模型，如图神经网络</em></h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/8bff4f6efc183d5f6b98bc85bea9329a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-LhnEW0jYEBtFke0Fo3i3g.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">深度关系学习旨在使<strong class="bd lg">神经网络</strong>能够进行<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener"> <strong class="bd lg">关系学习</strong> </a>，即捕捉像关系逻辑(程序)的语言一样有表现力的学习表示。图片由作者提供。</p></figure><p id="b26c" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">图形结构化数据无处不在。随着最近深度学习的出现，研究人员开始用神经网络探索这种数据表示似乎是很自然的。目前，我们经历了<em class="md">图形神经网络</em><em class="md">【GNN】</em>类的爆炸，无数的<a class="ae ko" href="https://github.com/thunlp/GNNPapers#basic-models" rel="noopener ugc nofollow" target="_blank">模型</a>以各种(吸引人的)名字被提出。然而，这些模型中的大多数都是基于相同的简单图形传播原理。</p><p id="6a76" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">为了从更广阔的视角来看待这个问题，我们将在这里从<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_719" rel="noopener ugc nofollow" target="_blank"> <em class="md">关系机器学习</em> </a>的一般视角来揭示潜在的GNN原则，这一点我们在<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener">上一篇文章</a>中讨论过。</p><ul class=""><li id="d8cc" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">与(主要)经验驱动的深度学习相反，关系机器学习在很大程度上建立在<em class="md">关系逻辑</em>的形式语言上，而<em class="md"> </em>反过来又提供了一些用于表示和操作结构化数据的坚实原则，如各种<em class="md">集合、图表和关系数据库</em>。</li></ul><p id="94d6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated"><span class="l mo mp mq bm mr ms mt mu mv di">在</span>这篇文章中，我们将探索一种优雅的方式来连接<em class="md">关系学习</em>和<a class="ae ko" rel="noopener" target="_blank" href="/what-is-neural-symbolic-integration-d5c6267dfdb0"> <em class="md">神经网络</em> </a>。这将允许我们以一种非常<em class="md">透明</em>的方式优雅地捕捉GNN模型，允许随后将<a class="ae ko" href="https://arxiv.org/abs/2007.06286" rel="noopener ugc nofollow" target="_blank">推广到超越其当前状态</a>的<a class="ae ko" href="https://medium.com/towards-data-science/beyond-graph-neural-networks-with-pyneuralogic-c1e6502c46f7" rel="noopener"> <strong class="lj ja"> <em class="md">深度关系学习</em> </strong> </a>。</p><h1 id="3b1c" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">神经网络关系学习简史</h1><p id="4972" class="pw-post-body-paragraph lh li iq lj b lk no ka lm ln np kd lp lq nq ls lt lu nr lw lx ly ns ma mb mc ij bi translated">虽然对于我们许多人来说，最近GNNs的普及可能是我们第一次遇到从结构化数据中学习的<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_458" rel="noopener ugc nofollow" target="_blank"/>，但实际上已经有很长一段时间的研究旨在将机器学习模型外推至这些数据表示。</p><p id="a589" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_719" rel="noopener ugc nofollow" target="_blank"> <em class="md">关系学习</em> </a>的保护伞下，这个问题已经被根植于<em class="md">关系逻辑</em>及其概率扩展的方法统治了几十年，这些方法被称为<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_786" rel="noopener ugc nofollow" target="_blank"> <em class="md">统计关系学习</em> </a> (SRL)。</p><p id="c852" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然而，神经网络提供了高效的<em class="md">潜在表示学习</em>，这超出了逻辑系统的能力。这里，不是将符号视为独立的原子对象，而是引入了学习<strong class="lj ja"><em class="md"/></strong>嵌入的思想，即(离散)符号对象的分布式固定大小数值(向量)表示。</p><ul class=""><li id="a2d3" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">它可以追溯到线性关系嵌入[2]，随后的论文提出了不同的模式来改善学习并将符号的嵌入扩展到它们的组合中。这个工作流还包括现在广泛流行的“word 2 vec”[3]。</li></ul><p id="eeac" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">学习结构的嵌入，如<em class="md">树</em>，然后追溯到递归自联想记忆【4】。在这些作品中,(逻辑)关系的表示通常作为对象对(嵌入)之间的相似性度量来处理，例如通过张量来参数化。这种方法后来成为流行的<em class="md">递归神经网络</em>【6】。</p><p id="b24b" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated">与此同时，关系学习社区也提出了各种修改，以使经典神经网络适应关系(逻辑)表示。在很大程度上本着SRL的精神，这里宣布的模型目标是正确地组合<em class="md">选择</em>和<em class="md">聚集</em>偏差，即学习如何选择关系模式以及如何同时从它们中聚集信息。</p><ul class=""><li id="2999" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">仅考虑这里的<em class="md">聚合</em>设置对应于“<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_569" rel="noopener ugc nofollow" target="_blank">多实例学习</a>”，即从(独立的)样本集学习，这也在关系学习社区中提出[8]。最近，类似的模型也随着“<em class="md">深集</em>”[7]和类似的架构而复活。</li></ul><p id="3575" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这种SRL观点激发了“<em class="md">关系神经网络</em>”[9]——一种基于特定关系数据库模式的结构模型，样本将从该模式中抽取。然后引入了一种类似的方法，分别作为(原始)<strong class="lj ja"> <em class="md">图形神经网络</em> </strong> <em class="md">模型</em>(GNN)【10】的<em class="md">图形</em>数据。当时，在[11]和[12]中进一步回顾和比较了这两种“竞争”方法。</p><blockquote class="nt nu nv"><p id="974c" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">这里的重要见解是，这些方法遵循了动态构建 神经模型的<a class="ae ko" rel="noopener" target="_blank" href="/what-is-neural-symbolic-integration-d5c6267dfdb0"> <strong class="lj ja">概念。这种计算模式的转变使得能够应用从原始关系数据中进行端到端学习的核心思想。与基于将结构预处理成固定大小的向量(张量)的(命题化)方法相比，这允许模型直接利用输入示例呈现的关系偏差。</strong></a></p></blockquote><h2 id="fdf9" class="nz mx iq bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iw bi translated">图形神经网络的先验知识</h2><p id="a720" class="pw-post-body-paragraph lh li iq lj b lk no ka lm ln np kd lp lq nq ls lt lu nr lw lx ly ns ma mb mc ij bi translated">现代的GNN变体可以被视为这一原始GNN概念的延续(尽管它们通常更简单[13])。这里一个常见的解释建立在“消息传递”的概念上，每个节点都被视为向输入图<strong class="lj ja"> Xi </strong>中的邻居发送“消息”。虽然这一解释与魏斯费勒-雷曼启发法有一些重要的联系，但⁴直接通过其底层<em class="md">计算</em>图<strong class="lj ja"> Gi </strong>的结构来查看gnn可能更具启发性，类似于查看经典的深度学习模型。</p><p id="f2a5" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">从这个角度来看，⁵ GNNs可以被视为标准的<em class="md"> CNN </em>技术的扩展，即<em class="md">卷积</em>和<em class="md">聚集</em>到<em class="md">不规则</em>图结构<strong class="lj ja"> Xi </strong>，在节点(<em class="md"> N </em>)之间具有任意边(<em class="md"> E </em>)。为了促进这一点，他们<em class="md">从每个<em class="md">输入图形</em> <strong class="lj ja"> Xi </strong>中动态</em>展开每个<em class="md">计算图形</em> <strong class="lj ja"> Gi </strong>，而不是固定像素网格上的静态架构。</p><p id="316a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这样，GNN就是一个简单的“标准”多层前馈CNN架构，唯一需要注意的是,<em class="md">计算图</em> <strong class="lj ja"> Gi </strong>中每层<em class="md"> k </em>的结构准确反映了<em class="md">输入图</em> <strong class="lj ja"> Xi </strong>的结构。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ok"><img src="../Images/46ddc9c0828a26deb8a6c510f1b4c264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFQ-nw1MnT3alOZoeasiRQ.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">一个典型GNN(“g-Sage”)的计算图，在每一层内具有权重共享(“卷积”)，在一些不规则输入图<strong class="bd lg"> X </strong>(紫色，左侧)上展开。单个卷积运算节点(<strong class="bd lg"> C </strong>、<strong class="bd lg">、</strong>橙色)之后是聚合运算(<strong class="bd lg"> Agg </strong>、蓝色)，其形成输入到输入图的下一层表示(<strong class="bd lg"> X </strong>)(浅紫色)。图片由作者提供(来自[32])。</p></figure><p id="0a2f" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated"><span class="l mo mp mq bm mr ms mt mu mv di"> P </span>具体来说，每个<em class="md">输入</em>图<strong class="lj ja"> Xi </strong>中的每个节点<em class="md"> N </em>都可以关联一个特征向量(或嵌入)，形成<em class="md">计算</em>图<strong class="lj ja"> Gi </strong>中的输入层表示。<br/>对于下一层<em class="md"> k </em>表示的计算，每个节点<em class="md"> N </em>通过聚集<em class="md"> A </em>【池化】<em class="md"> M : (N，M)∈ E </em>在<em class="md">输入</em>图<strong class="lj ja"> Xi </strong>中相邻的相邻节点<em class="md">的值来计算自己的隐藏表示<em class="md"> h(N) </em>。这些可以通过某个参数函数<em class="md"> C </em>(卷积)进一步转换，该参数函数在各层<em class="md"> k </em>内与<em class="md">相同的参数化</em> <em class="md"> W₁ </em>重复使用:</em></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/680ef308e335f85d89f26f32242c920e.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/0*fQGO7sJdF4yI0yeQ"/></div></figure><ul class=""><li id="292f" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">在一些GNN模型中，该h表示通过另一个<em class="md"> Cw₂ </em>与来自前一层<em class="md">k1</em>的“中心”节点的<em class="md"> N </em>表示进一步<em class="md">组合</em>，以获得层k的最终更新值，如下所示:</li></ul><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d61a407a1f4f82c77c7423719ad301cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/0*zhkRFrbAhyfpuDZc"/></div></figure><p id="2ea4" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated">然后，他的通用“聚合和组合”[17]计算方案涵盖了各种流行的GNN模型，然后简化为特定聚合<em class="md"> A </em>和激活/卷积<em class="md"> Cw </em>的选择。例如在GraphSAGE [18]中，操作是</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a26ab8b94bb101f5e55d698e4f653172.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/0*YwCd5oSrkkdNHviy"/></div></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/319359a0abfe2ee490adbd01041bb6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/0*K4TXSc73_kdFpICO"/></div></figure><p id="68da" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">而在流行的图卷积网络[19]中，这些甚至可以合并成一个单一的步骤</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/67a4626129f6d3b61cc785d8561b9a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/0*Hc-WgFpVjVwW_T0N"/></div></figure><p id="62de" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">同样的一般原则也适用于许多其他GNN作品[17]。⁰</p><blockquote class="nt nu nv"><p id="c708" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated"><strong class="lj ja">批判的视角。</strong>最近，已经提出了这个计算公式的大量不同变体。本质上，每一个这样引入的GNN变体都提出了(共同的)激活和聚集功能的某种组合，并且/或者提出了用从其他神经架构借用的层来扩展该模型。最终，在新的GNN名字下引入这些普通DL模块的详尽组合导致了我们今天看到的GNN<a class="ae ko" href="https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html#graph-neural-network-operators" rel="noopener ugc nofollow" target="_blank">动物园</a>模型，具有很大程度上模糊的性能【21】。</p></blockquote><p id="fb6d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">要真正看透模型的原理，现在看来，在这个相当常见的“低挂果实”收集的早期研究阶段之后，GNN领域可能会受益于一些更深入的见解…</p><h2 id="985a" class="nz mx iq bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iw bi translated">对称的观点</h2><p id="414a" class="pw-post-body-paragraph lh li iq lj b lk no ka lm ln np kd lp lq nq ls lt lu nr lw lx ly ns ma mb mc ij bi translated">M. Bronstein等人对GNN模型类最近的(r)发展提出了一个非常有见地的观点，通过对每个领域中的<strong class="lj ja"> <em class="md">对称性</em> </strong>的基本假设来观察DL架构。从几何领域中常见的对称概念开始，这种方法后来被创造为“<a class="ae ko" href="https://geometricdeeplearning.com/" rel="noopener ugc nofollow" target="_blank"> <em class="md">几何深度学习</em> </a>”(请在Medium上查看<a class="oq or ep" href="https://medium.com/u/7b1129ddd572?source=post_page-----f07a0dddda89--------------------------------" rel="noopener" target="_blank">迈克尔·布朗斯坦</a>的优秀文章系列<a class="ae ko" rel="noopener" target="_blank" href="https://towardsdatascience.com/graph-deep-learning/home"/>)。</p></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="05a4" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">自然地，所有的机器学习模型本质上都试图探索输入数据分布中的对称性，即某种形式的重复规律。然而，在一些结构化模型类中，这种规律性也存在于模型<em class="md">参数空间</em>中。</p><p id="369f" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在深度学习中，这可能是最有名的CNN，旨在反映像素网格中的<em class="md">不变性</em> w.r.t .平移(shift)，这是这种几何对称<em class="md">最流行的例子</em>。</p><ul class=""><li id="19f3" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">这意味着无论你将输入模式转移到哪里(在“感受野”的范围内)，模型输出都将保持不变。</li></ul><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi oz"><img src="../Images/1f0ffaf0c8c30a9f5a028654e032154a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEc7u1H2wxss17y1oYs8cQ.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">CNN卷积层中的对称(规则)加权共享模式。图片由作者提供。</p></figure><p id="7062" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">以类似的方式，递归和递归神经网络被设计成在线性和树形结构中反映<em class="md">递归对称性</em>。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pa"><img src="../Images/b1949edc0046c8a3080ae1a48517c025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eK_LOry45UsY8Ict"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">递归(左)和递归(右)神经网络的公共权重分配方案和对称性。图片由作者提供(来自[32])。</p></figure><p id="2ead" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">最后，gnn试图用一般的<em class="md">图不变量</em>做同样的事情，也就是关于<em class="md">图同构</em>(置换)<em class="md"> </em>对称的函数。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pb"><img src="../Images/3d418541f66dc7c11588c1cd72c41ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1aDIdZbWIg9EInciqN0Ucg.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">应用于高度对称分子(甲烷)的图形神经网络中的规则权重分配模式。图片作者(<a class="ae ko" href="https://arxiv.org/abs/2007.06567" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></figure><p id="ead7" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在<em class="md">集合</em>上反映<em class="md">完全置换</em>对称性的模型，例如“<em class="md">深集合</em>”，可以被视为一种“基础情况”，其中<em class="md">所有可能的输入置换都被认为是等价的。</em></p><ul class=""><li id="107b" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">而在前面的模型中，只有可能的输入排列的子集被不变地处理(想象一下，例如，移动一只猫的图像与随机移动它的所有像素)。</li></ul></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="3711" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">自然地，在每个学习领域中结合关于相应对称形式的正确先验最终导致更好的样本效率和泛化，并且历史证明允许利用这种先验的模型非常有用。</p><h1 id="a282" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">超越几何对称</h1><p id="3521" class="pw-post-body-paragraph lh li iq lj b lk no ka lm ln np kd lp lq nq ls lt lu nr lw lx ly ns ma mb mc ij bi translated">传统上，大多数深度学习进展都是通过应用程序根据经验驱动的，而不是一些先前自上而下的理论分析。这也许也激发了对计算机视觉中出现的几何对称性的主要方向的探索，这是最普遍的应用领域。然而，神经网络中的对称性不需要局限于几何。</p><p id="92a8" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ja">举例。为了简单的演示，让我们回顾一下<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-neural-symbolic-integration-d5c6267dfdb0" rel="noopener">上一篇关于神经符号整合的文章</a>中的简单逻辑XOR例子。</strong></p><ul class=""><li id="21cc" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">在那里，我们解释了编码简单的逻辑函数，如AND和OR，如何在神经网络的早期进化中发挥重要作用，早期“神经网络”无法学习XOR问题导致了重大挫折。</li></ul><p id="43e5" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">尽管几十年来XOR问题的解决方案已经广为人知，但有趣的是，即使使用现代的深度学习，正确地学习仍然是非常重要的！特别地，虽然最佳解决方案可以由少至2个(或者甚至1个)隐藏神经元组成，但是实际上很难正确地训练如此小的网络的参数，因为优化将大部分被次最佳解决方案卡住。(自己轻松试试吧！)</p><p id="ab4b" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">只有当我们采用普通的(“蛮力”)方法将网络参数化增加到比必要的维数高得多的维数时(app。10+隐藏神经元进行二进制异或)，训练最终变得可靠。</p><p id="7439" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">虽然蛮力通常是深度学习中的一个答案，例如当诉诸数据集扩充时，我们可以通过将正确的先验纳入模型来做得更好。特别地，这里我们知道异或函数是<em class="md">对称的</em>因为XOR(x₁,x₂) = XOR(x₂,x₁).因此，我们不需要单独处理所有的排列，函数的神经参数化也应该有一些内部对称性。</p><p id="88df" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">因此，代替(常见的)过度参数化方法，我们实际上可以保持小的网络规模，并且<em class="md">通过<em class="md">绑定权重</em>来进一步减少</em>参数的数量。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pc"><img src="../Images/6c8aeac0ef65a4f2ca848bf7007ad86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mtd1qc5w2Uvpr8XoRDK92Q.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">二进制XOR函数学习的(自动诱导的)对称权重共享先验的例子。图片来自作者的学生马丁·克鲁茨基。</p></figure><p id="8a23" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然后，这种权重共享显著地改进了逻辑函数的学习，类似于在计算机视觉(和其他应用)中通过卷积使用几何对称性的DL模型的⁵。</p><h2 id="e598" class="nz mx iq bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iw bi translated">用逻辑捕捉对称</h2><blockquote class="nt nu nv"><p id="9862" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">如果我们可以从将正确的先验知识编码到模型中开始，而不是在顶级通用模型上修补过度参数化和其他经验(蛮力)技巧，这不是很好吗？</p></blockquote><p id="1cc6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><a class="ae ko" href="https://geometricdeeplearning.com/" rel="noopener ugc nofollow" target="_blank">几何深度学习运动</a>现在用基于几何对称性的神经网络实践观点提出了这样一个有趣的观点。然而，这个原理长期以来也在<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener"> <em class="md">关系学习</em> </a>领域中被研究，作为逻辑上的<strong class="lj ja"> <em class="md">背景知识</em></strong><em class="md"/>并入。</p><blockquote class="nt nu nv"><p id="0bd3" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">这两种方法之间有什么联系吗？关系逻辑与(几何)对称性有什么关系吗？</p></blockquote><p id="798a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">是的。尽管被DL支持者普遍忽视(或轻视)，关系逻辑<em class="md">实际上是捕捉各种对称和离散领域规则的完美形式。⁶</em></p><p id="0776" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated"><span class="l mo mp mq bm mr ms mt mu mv di">到</span>与“几何”解释一致，让我们从<em class="md">集合</em>中置换对称的最简单概念开始。有了逻辑，就不需要为此重新发明轮子了，因为我们已经在逻辑中将集合定义为满足给定属性的对象X的枚举，即{ X | X是一个节点}或简单的<em class="md"> N(X) </em>对于一些一元(属性)关系N。请注意，在集合的这种表示中，没有关于元素X的任何排序的概念，因此相应的排列对称性在设计中得到考虑！</p><p id="6c38" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然后，这可以直接推广到图形，这只是一个二元逻辑关系的情况。因此，为了表示一个图，我们可以写<em class="md"> Edge(X，Y) </em>来表示它的所有边的集合。自然，我们在这里也可以将相应节点的集合指定为<em class="md">节点(X) </em>和<em class="md">节点(Y) </em>。同样，在这种表示中，没有关于节点或边的任何排序的概念。</p><ul class=""><li id="8658" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">此外，关系逻辑并不仅限于图，同样的原理直接应用于更高级的结构<em class="md"> Relation(X，Y，Z，…) </em>以及它们通过逻辑连接词的组合(例如，形成<em class="md">关系数据库</em>和SQL的形式模型)。</li></ul><p id="b2da" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated">虽然每个(以前的)计算机科学学生肯定都熟悉这种形式(数据库)表示和相应的<em class="md">关系</em>代数/逻辑运算，但这种形式概念能否用于机器学习实践可能并不明显。<br/>然而，这已经被<em class="md">确切地</em>的观点<strong class="lj ja"> <em class="md">解除了建模</em> </strong>的范式【27】(在我们的<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener">以前的文章</a>中描述了<em class="md">的关系学习)</em>，这里的<em class="md">的关系逻辑</em>的表达形式主义已经被<em class="md">直接</em>用于捕捉机器学习中的<em class="md">对称性</em>问题。</p><p id="443b" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">重要的是，这里的对称不仅仅是描述性的，关系逻辑已经被直接<em class="md"> </em>用于<em class="md">编码图形模型</em>，比如在流行的<a class="ae ko" rel="noopener" target="_blank" href="/what-is-relational-machine-learning-afbe4a9c4231">马尔可夫逻辑网络</a>【16】。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pd"><img src="../Images/903fcee2d9607a91a10cdedb98848d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fmAVm1h413T3JRI5"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">分别来自深度学习和<a class="ae ko" rel="noopener" target="_blank" href="/what-is-relational-machine-learning-afbe4a9c4231">统计关系学习</a> (SRL)领域的CNN(左)中卷积滤波器展开的对称权重共享模式和MLN(右)中提升的逻辑模板之间的类比。作者图片。</p></figure></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="b4b8" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然而，尽管用关系逻辑将领域对称性的先验编码到<em class="md">图形</em>模型中已经研究了几十年，但在深度学习方面似乎令人惊讶地缺乏。</p><ul class=""><li id="e799" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">当然，深度学习社区对逻辑有一定的厌恶。尽管如此，考虑到神经网络和<em class="md">命题</em>逻辑之间的历史<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-neural-symbolic-integration-d5c6267dfdb0" rel="noopener">相互作用</a>，神经网络中基于逻辑的对称性缺乏基础似乎有些令人惊讶。</li></ul><p id="0370" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然而，正是标准神经网络的“命题固定”限制了它们在学习问题中捕捉有趣的(提升的)对称，并使深度学习与关系逻辑的集成变得复杂，正如上一篇文章中所概述的<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-neural-symbolic-integration-d5c6267dfdb0" rel="noopener">。</a></p><h1 id="c713" class="mw mx iq bd my mz na nb nc nd ne nf ng kf nh kg ni ki nj kj nk kl nl km nm nn bi translated">深度关系学习</h1><blockquote class="nt nu nv"><p id="ad19" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">但是等一下，现在我们有强大的GNNs！他们不再受命题固定的困扰，因为他们可以捕捉图形数据。难道我们现在就不能把一切都变成图，用GNNs来解决我们所有的问题吗？</p></blockquote><p id="175e" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">当然，图形是无处不在的，如果你尝试，你可以将手头的任何问题以某种方式转化为图形表示。因此，这是非常方便的只是应用一个<a class="ae ko" href="https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html#graph-neural-network-operators" rel="noopener ugc nofollow" target="_blank">过剩</a>现成的GNN模型，看看你是否运气好一些调整。⁸</p><p id="ae80" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">然而，请注意，这是另一种“如果你只有一把锤子……”的认知偏见，类似于之前讨论的<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener"/>“将一切都变成向量”的命题化方法论。</p><p id="a413" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">最后，gnn只是一种特殊的图传播启发式算法，具有源于底层WL的固有限制。重要的是，即使GNN可以正确地捕捉图形结构(超越WL)，这仍然不能保证其实际的<em class="md">学习</em>(泛化)能力，类似于它显然不足以坚持基于通用近似定理的简单2层神经网络【29】。</p><p id="7908" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这就是为什么我们设计越来越复杂的模型和学习表示，直接反映我们问题的结构。因此，将复杂的关系学习表示和推理算法从图和图传播转回到GNNs可能不是最好的主意。</p><blockquote class="nt nu nv"><p id="3b9a" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">因此，如果我们不是试图将更复杂的<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_719" rel="noopener ugc nofollow" target="_blank">关系问题</a>表示嵌入到图和gnn中，而是可以将关系逻辑的表达用于一些成熟的“<strong class="lj ja">深度关系学习</strong>”，捕捉当前神经模型中的对称性作为特例，类似于提升的图形模型如何在<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_786" rel="noopener ugc nofollow" target="_blank"> SRL </a>中概括标准图形模型，这不是很好吗？</p></blockquote><p id="2358" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi mn translated">正如你现在可能已经猜到的那样，有一个学习框架可以完全做到这一点。具体来说，它采用<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-relational-machine-learning-afbe4a9c4231" rel="noopener"> <em class="md">提升建模</em> </a>策略，并将其外推到<em class="md">深度学习</em>设置中。然后，这种范式能够将神经网络和命题逻辑之间熟悉的<a class="ae ko" href="https://medium.com/@sir.gustav/what-is-neural-symbolic-integration-d5c6267dfdb0" rel="noopener">对应</a>直接带到<em class="md">关系</em>层面。</p><blockquote class="nt nu nv"><p id="64be" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">因此，在命题逻辑被提升到关系逻辑的更高表达能力的同样意义上，该框架将经典神经网络提升到这些模型的更具表达能力的版本，称为“<strong class="lj ja">提升的关系神经网络</strong>”[34]。⁵</p></blockquote><h2 id="8028" class="nz mx iq bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iw bi translated">提升关系神经网络</h2><p id="eacc" class="pw-post-body-paragraph lh li iq lj b lk no ka lm ln np kd lp lq nq ls lt lu nr lw lx ly ns ma mb mc ij bi translated">与提升的<em class="md">图形化</em>模型类似，“提升的关系神经网络”框架的核心是一种基于关系逻辑的语言，用于定义<em class="md">神经</em>模型，进一步简称为“<a class="ae ko" href="https://github.com/GustikS/NeuraLogic" rel="noopener ugc nofollow" target="_blank"><em class="md"/></a>”。该语言直接源自Datalog(或Prolog)，它通常用于高级数据库查询和逻辑编程，具有一个额外的特性——它是<em class="md">可微分的</em>。</p><ul class=""><li id="040e" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">虽然现在有许多可区分编程的框架，但NeuraLogic的显著不同在于它是<em class="md">声明性的</em>。这允许以非常简洁和优雅的方式表达一些复杂的算法和原理，这特别适用于表现出各种规律性(对称性)的<em class="md">关系</em>问题。</li></ul><p id="9436" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ja">举例。</strong>让我们跟进上面介绍的关系逻辑如何捕捉集合和图形中的对称性的形式原则，并在构建GNN时使其<em class="md">可直接操作</em>。特别地，我们现在知道图是节点(<em class="md">节点(X) </em>和<em class="md">节点(Y) </em>)之间的一组边(<em class="md">边(X，Y) </em>)。然后，我们知道通用GNN <em class="md">计算规则</em>是将节点的表示传播/聚集到它们的邻居，这可以放入<em class="md">逻辑规则</em>中，如下所示:</p><pre class="kr ks kt ku gt pe pf pg ph aw pi bi"><span id="8cb4" class="nz mx iq pf b gy pj pk l pl pm">node2(X) &lt;= W node1(Y), edge(X, Y).</span></pre><p id="85a9" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">…就是这样，真的！这是神经语言中gnn(gcn[19]，特别是⁰)的<em class="md">可运行代码</em>。内容如下:</p><blockquote class="nt nu nv"><p id="fe6d" class="lh li md lj b lk ll ka lm ln lo kd lp nw lr ls lt nx lv lw lx ny lz ma mb mc ij bi translated">"为了计算任何对象<em class="iq"> X </em>的表示'<em class="iq"> node2 </em>'，聚合所有对象<em class="iq"> Y、</em>的a '<em class="iq">W【T21 '-八分表示'<em class="iq"> node1 </em>'，其中'<em class="iq"> edge </em>'位于两者之间。"</em></p></blockquote><ul class=""><li id="6517" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">那些倾向于关系逻辑的数据库解释的人可能会这样理解:“<em class="md">在Y列上连接表‘node 1’和‘edge’，按X分组，并聚合成一个新表‘node 2</em>’”。</li></ul><p id="c989" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在神经对话框架中运行这个逻辑规则与在传统框架中运行GCN完全一致，也就是说，它将执行</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/31b92568a11b92b1d2c44241aae3edc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/0*15qLwPEF5FRhNKok"/></div></figure><p id="2d3b" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">前面讨论的公式。当然，激活(<em class="md">∑</em>)和聚集(<em class="md"> Agg </em>)函数(以及其他超参数)的规范也可以添加到规则中。然而，您也可以像这样让它们保持默认值(<em class="md"> tanh+avg </em>)来享受GCNs的基本原理的清晰性，即节点和相应边的局部排列的不变性。</p><blockquote class="po"><p id="f04e" class="pp pq iq bd pr ps pt pu pv pw px mc dk translated">因此，在NeuraLogic中，模型对称性先验的定义成为模型本身的代码！</p></blockquote><p id="2ba3" class="pw-post-body-paragraph lh li iq lj b lk py ka lm ln pz kd lp lq qa ls lt lu qb lw lx ly qc ma mb mc ij bi mn translated">重要的是，这个规则是完全通用的，也就是说，在专门为图形或gnn设计的整个神经对话框架中没有特定的功能。</p><ul class=""><li id="773d" class="me mf iq lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">然后，类似的简单提升规则对深度学习模型中的各种递归/递归和其他对称权重共享(卷积)方案进行编码，本文通篇以图片中的颜色为例。</li></ul><p id="1cc4" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这意味着你可以轻松地编写任意的关系规则，编码新颖的神经<a class="ae ko" href="https://www.jair.org/plugins/generic/pdfJsViewer/pdf.js/web/viewer.html?file=https%3A%2F%2Fwww.jair.org%2Findex.php%2Fjair%2Farticle%2Fdownload%2F11203%2F26415%2F#page=18&amp;zoom=auto,-457,634" rel="noopener ugc nofollow" target="_blank">建模构造具有高级对称先验的</a>，捕捉复杂的<em class="md">关系</em>学习原则<a class="ae ko" href="https://arxiv.org/abs/2007.06286" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ja">超越GNNs</strong></a>【36】。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="qd qe l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">先睹为快“超越图神经网络与提升的关系神经网络”[32]</p></figure></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="e940" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">虽然我们在这里用提升的关系神经网络<strong class="lj ja"> </strong>对深度关系学习进行了先睹为快的总结，但我们将在下一篇文章中详细探索神经对话框架，在那里我们将展示它的实际用途、表达能力和计算效率。</p></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="34b1" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">1.最直接，也是历史上占主导地位的方法是将关系数据转化为固定的张量表示，作为预处理(<a class="ae ko" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_680" rel="noopener ugc nofollow" target="_blank">命题化</a>)步骤，然而，正如前面<a class="ae ko" href="https://medium.com/p/afbe4a9c4231/edit" rel="noopener">和</a>所讨论的，这隐藏了许多缺点。</p><p id="7041" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[2] A .帕卡纳罗和杰弗里e .辛顿。"使用线性关系嵌入学习概念的分布式表示."摘自:IEEE知识与数据工程汇刊13.2 (2001)，第232–244页。刊号:10414347</p><p id="6b76" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[3]托马斯·米科洛夫、伊利亚·苏茨基弗、程凯、格雷戈·S·科拉多和杰夫·迪恩。"单词和短语的分布式表示及其组合性."载于:神经信息处理系统进展26 (2013)，第3111–3119页。</p><p id="9149" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[4]乔丹·波拉克。"递归分布式表示."摘自:人工智能46.1–2(1990)，第77–105页。issn: 00043702。</p><p id="61b6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">5杰弗里·e·辛顿。“将部分-整体层次映射到连接主义网络中。”摘自:人工智能46.1–2(1990)，第47–75页。issn: 00043702。</p><p id="5f88" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[6]理查德·索赫尔、齐丹·陈、克里斯托弗·曼宁和吴恩达。"用神经张量网络进行推理以完成知识库."神经信息处理系统进展。Citeseer。2013年，第926–934页。</p><p id="ceae" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[7] Zaheer，Manzil等，《深层集合》<em class="md"> arXiv预印本arXiv:1703.06114 </em> (2017)。</p><p id="ae4e" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[8] Ramon，j .，&amp; DeRaedt，L. (2000年)。多示例神经网络。在<em class="md">ICML-2000关于属性值和关系学习研讨会的会议录</em>。</p><p id="fb74" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[9]亨德里克·布洛克尔和沃纳·乌温特斯。"使用神经网络进行关系学习."摘自:ICML 2004年统计关系学习及其与其他领域的联系研讨会。2004年，第23-28页。</p><p id="9ca1" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[10] Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner和Gabriele Monfardini。"图形神经网络模型."摘自:IEEE神经网络汇刊20.1 (2008)，第61-80页</p><p id="41f7" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[11] Werner Uwents，Gabriele Monfardini，Hendrik Blockeel，Franco Scarselli和Marco Gori。"图形处理的两种联结主义模型:关系数据的实验比较."载于:MLG 2006年,《利用图表进行挖掘和学习国际研讨会论文集》。2006年，第211-220页</p><p id="866d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[12] Werner Uwents、Gabriele Monfardini、Hendrik Blockeel、Marco Gori和Franco Scarselli。"关系学习的神经网络:实验比较."载于:机器学习82.3(2011年7月)，第315–349页。issn: 08856125。</p><p id="202a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[13] Lamb，Luis C .等人,“图形神经网络与神经符号计算的相遇:综述与展望”arXiv预印本arXiv:2003.00330  (2020)。</p><p id="9f9e" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">14.GNN模型可以被视为用于图同构驳斥检查的著名的魏斯费勒-雷曼(WL)标签传播算法的连续、可微分版本。然而，在GNNs中，不是离散的标签，而是连续的节点表示(嵌入)被连续地传播到节点的邻域中，反之亦然，用于相应的梯度更新。</p><p id="57c8" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">15.接受这种动态<em class="md">计算图</em>的观点，这与之前介绍的递归网络非常相似，然后使模型<em class="md">无状态</em>，这消除了源于<em class="md">输入图</em>内的消息传递解释的可能的歧义。</p><p id="5e8c" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">理查森、马修和佩德罗·多明戈斯。“马尔可夫逻辑网络。”机器学习62.1–2(2006):107–136。</p><p id="e25b" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[17]徐克玉路、胡、朱尔·莱斯科维奇和杰格尔卡。“图形神经网络有多强大？”载于:arXiv预印本arXiv:1810.00826 (2018)。</p><p id="fe98" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[18]威尔·汉密尔顿、之桃·英和朱尔·莱斯科维奇。"大型图上的归纳表示学习."神经信息处理系统进展。2017，第1024–1034页。</p><p id="dd59" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[19]托马斯·n·基普夫和马克斯·韦林。"图卷积网络的半监督分类."参加:2017年4月24日至26日在法国ICLR土伦举行的第五届学习代表国际会议，会议记录。2017年，OpenReview.net</p><p id="669a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">20.我们注意到我们只讨论了“空间”表示的图形和运算。然而，一些GNN方法在频谱、傅立叶域中表示图形和卷积运算。然而，我们注意到，这些又大多遵循相同的“聚合和组合”原则，并可以相应地重写[17]。</p><p id="30b6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[21] Dwivedi，Vijay Prakash等，“基准图神经网络”arXiv预印本arXiv:2003.00982  (2020)。</p><p id="9468" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">22.更准确地说，由卷积滤波器的应用引起的共享权重引入了滤波器的相应变换，同时通过池化将聚合函数(例如，<em class="md">最大值</em>或<em class="md">平均值</em>)结合在顶部，将其进一步扩展到变换i <em class="md">方差</em>。</p><p id="d887" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">23.尽管理论上在这里误差函数(例如交叉熵)中没有局部最小值，但是存在朝向参数空间和大鞍点的边界的发散区域，使得基于梯度的优化非常困难。</p><p id="f35d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">24.类似地，对于更简单的AND和or函数来说也是如此，然而，这些函数过于琐碎，不足以证明NNs的这种过度参数化问题的重要性。<br/>有趣的是，虽然逻辑单元和神经元之间的功能并行性已经得到很好的认可，但就我们所知，以前没有工作利用相应神经权重中逻辑功能的平凡对称性来改善学习。</p><p id="b132" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[25]马丁，克鲁茨基。<em class="md">探索深度学习中的对称性</em>。理学学士论文。布拉格的捷克技术大学，2021。</p><p id="e88a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">26.自然地，逻辑更适合描述离散领域中的规律性，而几何解释在连续领域中更自然，如计算机视觉和机器人学。</p><p id="9f90" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[27] Kimmig，Angelika，Lilyana Mihalkova和Lise Getoor。"提升图形模型:一项调查."<em class="md">机器学习</em>99.1(2015):1–45。</p><p id="2295" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">28.考虑到最近围绕GNN概念的大肆宣传，以及足够的超参数调整，人们确实已经成功地在广泛的问题上利用了GNN，这现在变得更加诱人了。<br/>果不其然，甚至逻辑表达式也可以转化为图形表示，如【13】所述。然而，这并不意味着GNNs是解决逻辑推理底层问题的合适算法！</p><p id="471a" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">29乔治·西本科。"通过叠加一个s形函数的近似."<em class="md">控制、信号和系统的数学</em>2.4(1989):303–314。</p><p id="6e9d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">30.注意与GCN规则的<a class="ae ko" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">经典</a>矩阵视图H = σ (W * H * A) <strong class="lj ja"> </strong>的对齐，其中H是节点表示<em class="md">矩阵</em>，A是邻接矩阵(类似于这里的‘边’谓词)。</p><p id="a9f7" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">31.一些重要的评论文章显示，与所有论文中传统上自称的最先进的结果相比，许多这些GNN修改的实际性能增益往往相当微不足道，有时甚至无法击败简单的(非结构化)基线模型[21]，使得对大量GNN变体的需求有点可疑。</p><p id="5332" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">32.类似的工作也来自递归转换简化描述的旧思想[5]，自动编码器和许多其他工作都建立在此基础上。</p><p id="3f46" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">33.注意，原则上，<em class="md">递归</em>神经网络可以被认为是<em class="md">递归</em>神经网络的一般形式，它在序列上递归展开，而<em class="md">递归</em>网络在(常规)树上展开。</p><p id="e0e7" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[34] Sourek，Gustav，等人，〈提升关系神经网络〉。<em class="md">人工智能研究杂志</em> (2018)。</p><p id="0e09" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">35.有趣的是，这种具有各种建模结构的方法，包括一些GNN和子图GNN <a class="ae ko" href="https://www.jair.org/plugins/generic/pdfJsViewer/pdf.js/web/viewer.html?file=https%3A%2F%2Fwww.jair.org%2Findex.php%2Fjair%2Farticle%2Fdownload%2F11203%2F26415%2F#page=18&amp;zoom=auto,-457,634" rel="noopener ugc nofollow" target="_blank">变体</a>，已经在2015年发表<a class="ae ko" href="https://arxiv.org/abs/1508.05128" rel="noopener ugc nofollow" target="_blank">，后来(2017年)也</a><a class="ae ko" href="https://link.springer.com/chapter/10.1007/978-3-319-78090-0_10" rel="noopener ugc nofollow" target="_blank">扩展到这种结构的自动学习</a>。然而，对框架的解释根植于关系逻辑而不是(仅仅)图表，这对于ML观众来说可能看起来很奇怪。</p><p id="cf56" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">[36]古斯塔夫·索雷克、菲利普·切列兹和ondřej·库泽尔卡。"超越图神经网络与提升关系神经网络."<em class="md">机器学习</em>110.7(2021):1695–1738。</p></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="e040" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><em class="md">作者深深地感谢</em> <a class="ae ko" href="https://ida.fel.cvut.cz/~kuzelka/" rel="noopener ugc nofollow" target="_blank"> <em class="md">翁德雷</em> </a> <em class="md">对于</em> <a class="ae ko" href="https://arxiv.org/abs/1508.05128" rel="noopener ugc nofollow" target="_blank"> <em class="md">底层概念中的无数想法解除了关系神经网络</em> </a> <em class="md">。</em></p></div></div>    
</body>
</html>