<html>
<head>
<title>Proximal Policy Optimization (PPO) Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了最近策略优化(PPO)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b#2022-11-29">https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b#2022-11-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3c32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从强化到连续控制中的 go-to 算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2a5a23beed60a63134c2457a74d7c052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GuuMQaLtn2Iazf0z"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">诺亚·布舍尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="9abb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最近策略优化(PPO)目前被认为是强化学习的最新发展。OpenAI 在 2017 年推出的算法似乎在性能和理解之间取得了正确的<strong class="li iu">平衡</strong>。从经验上看，它与质量基准具有竞争力，甚至在某些任务上远远超过它们。同时，对于广泛的用户的实际采用来说，它是足够简单的，这不能对每个 RL 算法都这样说。</p><p id="1fb9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从表面上看，传统的政策梯度法(如加强法)和 PPO 的差别并不大。基于这两种算法的伪代码，你甚至可以说它们有点相似。然而，这一切背后都有丰富的理论，需要充分欣赏和理解算法。</p><p id="5778" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我将简要讨论策略梯度方法、自然策略梯度和信任区域策略优化(TRPO)的要点，它们共同构成了走向 PPO 的垫脚石。</p><h1 id="8db7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">普通政策梯度</h1><p id="3c1c" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated"><em class="mz">要理解这篇文章，很好地理解政策梯度方法是必要的。如果还没有，你可能想先看看这篇文章:</em></p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">强化学习中的策略梯度解释</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">了解所有关于基于似然比的政策梯度算法(加强):直觉，推导，和…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr ks nd"/></div></div></a></div><p id="796f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在 RL 的上下文中，策略<em class="mz"> π </em>只是一个函数，它返回给定状态<em class="mz"> s </em>的可行动作。在基于策略的方法中，函数(如神经网络)由一组可调参数<em class="mz">θ<strong class="li iu">定义。</strong></em>我们可以调整这些参数，观察所得奖励的差异，并朝着产生更高奖励的方向更新<em class="mz"> θ </em>。这一机制是所有政策梯度方法概念的基础。</p><p id="c38b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">策略<em class="mz"> π_θ(a|s) </em>是随机的，这意味着参数规定了动作 <em class="mz"> a </em>的<strong class="li iu">采样概率，从而影响跟随轨迹<em class="mz"> τ=s_1，a_1，…s_n，a_n </em>的概率。这样，我们可以表示依赖于<em class="mz"> θ </em>的目标函数:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/42cce7f5fe086e13f717a00de7f63635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VChcmTuoomeMMacw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标函数，产生与随机政策成比例的国家行动轨迹。</p></figure><p id="bc95" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在随机策略中，我们用相应的概率对各种行为进行采样——这样我们就可以衡量回报的差异。观察到的奖励，结合行动概率，产生一个奖励信号。为了<strong class="li iu">确定改进目标函数的更新方向</strong>，策略梯度方法依赖于梯度<em class="mz"> ∇_θ </em>(导数的向量)。这产生了以下更新函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0f452ca259da52230dadd302edcc7084.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*JGlxC5Spfo04JlYDxGtgPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统的策略梯度更新函数，基于目标函数梯度∇_θJ(θ和步长α更新策略权重θ</p></figure><p id="ba45" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然我们有奖励信号和更新<em class="mz">方向</em>，但是我们不知道到<em class="mz">我们应该更新多少</em>政策。<strong class="li iu">选择正确的学习率<em class="mz"> α </em> </strong>是一个挑战。权重更新的不一致影响加剧了这个问题。考虑以下两种策略同等幅度的权重更新。显然，左边的分布比右边的分布受影响更大。但是，欧几里德距离(即参数距离)是一样的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/9365ebbe6924ef2375fe3413a85dba88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GoXs5-XjqqYTlfyi48x-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正态分布对的比较。左边有μ_1=0，μ_2=1，σ_1=σ_2=0.3。右边有μ_1=0，μ_2=1，σ_1=σ_2=3.0。虽然两对之间的欧几里德距离是 1，但是很明显右边的一对比左边的一对更相似。[图片由作者提供]</p></figure><p id="ddab" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于政策变化大的风险，<strong class="li iu">样本仅使用一次</strong>。然后，我们使用更新后的策略进行重新采样，这是相当低效的。另一个问题是回报轨迹可能变化很大，导致更新的波动。</p><p id="b5cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总之，政策梯度存在重大缺陷:</p><ul class=""><li id="31dc" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated"><strong class="li iu">样品低效— </strong>样品仅使用一次。此后，更新策略，并使用新策略对另一个轨迹进行采样。由于采样通常很昂贵，这可能会令人望而却步。然而，在一次大的政策更新后，旧样本根本不再具有代表性。</li><li id="4b50" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">不一致的策略更新— </strong>策略更新往往会超过并错过奖励高峰，或者过早停止。特别是在神经网络结构中，消失和爆炸梯度是一个严重的问题。该算法可能无法从糟糕的更新中恢复。</li><li id="3fe3" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">高回报差异— </strong>政策梯度是一种蒙特卡罗学习方法，考虑了完整的回报轨迹(即完整的一集)。这样的轨迹经常遭受高方差，阻碍了收敛。<em class="mz">【这个问题可以通过添加一个评论家来解决，这超出了本文的范围】</em></li></ul><h1 id="2b02" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">自然政策梯度</strong></h1><p id="a112" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated"><em class="mz">不熟悉自然政策梯度？先看这篇文章:</em></p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">解释强化学习中的自然策略梯度</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">传统的政策梯度方法存在固有的缺陷。自然梯度收敛得更快更好，形成…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="oj l no np nq nm nr ks nd"/></div></div></a></div><p id="5bcd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Amari (1998)和 Kakade，S. M. (2001)提出的自然政策梯度解决了选择适当学习率的问题。为此，该方法嵌入了<strong class="li iu">二阶导数</strong>，量化了更新权重时梯度的敏感度。</p><p id="a6fc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Kullback-Leibner (KL)背离衡量政策<em class="mz"> π_θ </em>因更新而改变的程度。请记住，策略是由概率分布表示的，它决定了选择给定操作的概率。<strong class="li iu"> KL 散度</strong>定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/58a8826acd8b0d68a69de60fbb184062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sN3aLK4PCCwz-FUM.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KL-divergence，量化权重更新前后政策之间的距离。</p></figure><p id="1bf1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，如果我们将更新的散度限制为不超过<em class="mz"> ϵ，</em>我们可以使用以下方案来更新权重:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f463378e18581e665762007c7575faad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*nVUptuu-22EpC88Y.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">权重更新方案，最大 KL 偏差由ϵ设定上限</p></figure><p id="7c52" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">上述方案实际上是不可用的，因为 RL 使用采样观测值。为了将发散考虑在内，使用了修改后的目标函数上的<strong class="li iu">泰勒展开</strong>。我们对目标执行一阶展开，并对 KL 散度执行二阶展开。经过一些数学努力，下面的更新公式出现了:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f9485fa86a08a552ecbed96fda5b1a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/0*BZmi0nmVr0LTCdDv.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自然政策梯度的权重更新公式</p></figure><p id="58bc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总之，难以确定的学习速率<em class="mz"> α </em>已经被一个<strong class="li iu">动态学习速率</strong>所取代，这取决于目标函数的局部灵敏度。这种灵敏度由费希尔信息矩阵<em class="mz"> F(θ) </em>表示。当部署时，自然梯度更新公式允许在不敏感区域(例如，平台)进行较大的更新，而在弯曲区域(例如，接近奖励峰值)进行较小的更新。</p><p id="c805" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">理论上，自然梯度允许以适当的步长确保稳定的更新。由于限制了策略改变的幅度，我们也可以<strong class="li iu">多次重复使用样本</strong>，提高样本效率。然而，在现实中，自然梯度表现出许多缺点:</p><ul class=""><li id="cffe" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated"><strong class="li iu">对费希尔矩阵<em class="mz"> F </em>求逆比较繁琐——</strong>矩阵求逆是一种运算复杂度为<em class="mz"> O(N ) </em>的运算。鉴于神经网络通常包含数千个参数，这通常是禁止的。此外，由于所做的近似引入的数值不稳定性，反演可能会失败。</li><li id="db54" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu"/><strong class="li iu">KL 约束可能不满足— </strong>泰勒展开式只是理论目标函数的近似值。因此，分析得出的步长通常太大，并且仍然表现出过冲行为。</li><li id="efdf" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">政策改进未得到验证</strong> —尽管自然梯度提供了所有令人放心的理论保证，但该方法并不检查更新是否实际产生了改进。同样，由于近似值，情况可能并非如此。</li></ul><h1 id="7ba1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">信任区域策略优化(TRPO)</h1><p id="4b8a" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated"><em class="mz">作为 PPO 的直接前身，补上 TRPO 可能会不错:</em></p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/trust-region-policy-optimization-trpo-explained-4b56bd206fc2"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">解释了信任区域策略优化(TRPO)</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">强化学习算法 TRPO 建立在自然策略梯度算法的基础上，确保更新保持…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="on l no np nq nm nr ks nd"/></div></div></a></div><p id="8f4b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">自 1990 年代后期自然政策梯度出现以来，已经取得了一些进展。这些最佳实践已经被结合在流行的<strong class="li iu"> TRPO 算法</strong>中，由舒尔曼<em class="mz">等人</em>于 2015 年推出。</p><p id="3dd4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了简洁起见，我将再次省略许多细节。不过，有两个等式至关重要。首先，替代优势<em class="mz">𝓛_π_θ(π_θ+δθ)——</em>根植于重要性抽样<em class="mz">——</em>反映了更新的<strong class="li iu">预期优势:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/5489bc47895d565d1ddb8d42ff8cdb48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKwO8f035cDWDDoRw_f0CA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">替代优势𝓛的定义，使用来自策略π_θ的样本描述更新策略π_θ+δθ的预期优势</p></figure><p id="41ed" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第二，我们可以通过更新前后策略之间的最坏情况 KL-divergence 来限制替代优势的近似误差。这产生了以下不等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ae6017902b55e87be569fc8b551f189a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*Rx-CzmscLPPExhDdSCn4rA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">描述更新后目标值最小变化的不等式。右手边表示下限，考虑到对发散误差进行了修正的替代优势。</p></figure><p id="28a5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">罚分<em class="mz"> C </em>已经过分析推导，产生了一个相当严重的发散罚分。如果我们遵循理论上的 TRPO 方法，我们会有非常小的更新和缓慢的收敛。</p><p id="0fff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，实际的 TRPO 算法在一些关键点上不同于理论基础。区分 TRPO 的<strong class="li iu">理论模型</strong> <strong class="li iu">(基于惩罚)和实际实现(基于约束)</strong>是很重要的——这种区别在后面讨论 PPO 时是相关的。</p><p id="48d0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">具体而言，TRPO 的实际实施通过以下三个调整改进了自然梯度:</p><ul class=""><li id="bd2f" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated"><strong class="li iu">共轭梯度法</strong>—自然政策梯度计算只需要乘积<em class="mz">f^-1∇logπ_θ(x)</em>我们实际上对逆矩阵本身不感兴趣。共轭梯度可以比矩阵求逆更快地逼近这个乘积，从而大大加快了更新过程。</li><li id="0a1c" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">线搜索</strong> —由于简化，为自然政策梯度导出的分析步长可能违反 KL-divergence 上限。TRPO 从最初建议的步长开始，但是在更新之后测量是否不违反发散约束。如果是，它迭代地和指数地收缩信赖域(使用收缩参数<em class="mz"> α^j </em>)，直到满足散度约束。</li><li id="3581" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">改进检查</strong>——最终，我们希望我们的更新能够改进政策。TRPO 通过在接受替代损失(𝓛(θ)之前，实际验证替代损失在更新之后是否有所改善，来认可这种观点。回想一下，由于近似值，理论保证不再成立。</li></ul><p id="6284" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">后两种改进在下面的算法片段中都可以看到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/28f6cef19fd88b240cc13c047da6383a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RU2cHrMG4feOQnFd.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线搜索算法。该算法确保更新不会违反发散约束(使用指数衰减α^j)，并且更新不会降低策略的性能[来源:<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank"> Joshua Achuam，UC Berkeley </a></p></figure><p id="3edf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">TRPO 解决了许多与自然政策梯度相关的问题，并获得了 RL 群体的广泛采用。然而，仍然存在一些缺点，特别是:</p><ul class=""><li id="5a9e" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated"><strong class="li iu">无法处理大型参数矩阵</strong> —尽管部署了共轭梯度法，TRPO 仍然难以处理大型 Fisher 矩阵，即使它们不需要求逆。不幸的是，深度神经网络包含许多参数，精确逼近<em class="mz"> F </em>需要大量样本。</li><li id="e019" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu">二阶优化速度慢</strong>—TRPO 的实际实现是基于约束的，需要计算前述的 Fisher 矩阵，这大大降低了更新过程的速度。此外，我们不能利用(一阶)随机梯度优化器，如亚当。</li><li id="f55d" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><strong class="li iu"> TRPO 很复杂</strong> — TRPO 很难解释、实施和调试。当培训没有产生预期的结果时，很难确定如何提高绩效。因此，它不是最用户友好的 RL 算法。</li></ul><h1 id="d95a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">近似策略优化(PPO)</h1><p id="98f2" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">2017 年，舒尔曼<em class="mz">等人</em>在 TRPO 基础上引入了近似政策优化(PPO)。给定所有的背景信息，PPO 比以前的算法做得更好和不同的是什么？</p><p id="1b0a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">PPO 有两个主要的变种<strong class="li iu">要讨论(都在 2017 年的论文中介绍过):PPO 罚分和 PPO 夹分。我们将首先看一看基于惩罚的变体，它在概念上最接近于 TRPO。</strong></p><h2 id="2d4e" class="or md it bd me os ot dn mi ou ov dp mm lp ow ox mo lt oy oz mq lx pa pb ms pc bi translated">PPO —具有自适应 KL 惩罚的变体</h2><p id="e050" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">TRPO 的理论基础将 KL 偏差封装为软惩罚——如前一节所述——然而实际实现是基于<em class="mz">约束</em>的。理论上的 TRPO 通过分析得出一个与偏离相乘的罚值，然而在实践中，这个罚值通常过于严格，只能产生非常小的更新。因此，问题是如何<strong class="li iu">可靠地确定缩放参数<em class="mz">β</em>T5】以允许有意义的更新，同时避免过度漂移:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/ced550adfeaae4930e13fccea3921c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ARFpFoLorrbPZb-hLHtB3A.png"/></div></div></figure><p id="fd80" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">该约束的问题是<strong class="li iu">很难确定适用于多个问题设置的<em class="mz"> β </em> </strong>的单个值。事实上，即使是同一个问题，特征也可能随着时间而改变。由于我们没有对分歧设置硬性限制，有时我们会比其他时候更严厉地处罚。</p><p id="efc9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">PPO 以务实的方式解决了这个问题。它设置了一个<strong class="li iu">‘目标散度’</strong><em class="mz">δ</em>；我们希望我们的更新在这个分歧的附近。目标差异应该大到足以实质性地改变策略，但又小到足以使更新稳定。</p><p id="2759" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">每次更新后，PPO 都会检查更新的大小。如果实现的偏离超过目标偏离超过 1.5，对于下一次迭代，我们通过加倍<em class="mz"> β </em>来更严厉地惩罚偏离。反过来，如果更新太小，我们就将<em class="mz"> β </em>减半，有效地<strong class="li iu">扩展了信任区域</strong>。你可以在这里看到 TRPO 的线搜索的一些相似之处，然而 PPO 的搜索是双向的。</p><p id="af70" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如您所料，基于超过某个目标阈值的 1.5 倍将β加倍和减半并不是精心证明的结果；这些值是启发式地确定的。我们将<strong class="li iu">不时地违反约束</strong>，但是通过调整β可以相当快地纠正它。根据经验，PPO 似乎对数值设置相当不敏感。总之，为了实用主义，我们牺牲了一些数学的严谨性。</p><p id="edb4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">PPO 的基于惩罚的变体概述如下。请注意，它非常类似于 TRPO 的理论模型，但使用了一些启发式近似，使其也可实际应用。</p><blockquote class="pe pf pg"><p id="7554" class="lg lh mz li b lj lk ju ll lm ln jx lo ph lq lr ls pi lu lv lw pj ly lz ma mb im bi translated">由于 PPO 更新相当小，在一定程度上重复使用生成的样本是可能的，用重要性采样项来校正更新的动作概率。PPO 执行 K 个更新步骤；当超过某个偏离阈值时，实现通常部署早期停止。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/6e5c02af6c55a0a13021193a9ff140bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2fD_6bgEjZOztFZyig74Aw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PPO，罚变。目标函数嵌入了对 KL 散度的惩罚。基于相对于目标散度的测量散度动态更新相应的权重β_ k[来源:<a class="ae ky" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">舒尔曼等人 2017 </a></p></figure><p id="c79b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">PPO 比自然梯度和 TRPO 更容易<strong class="li iu">实现</strong>，不需要分析结果、基于约束的程序或二阶导数。相反，我们可以使用流行的随机梯度下降算法(如 ADAM)来执行更新，并在更新过大或过小时调整惩罚。</p><p id="d261" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">总之，<strong class="li iu"> PPO 比 TRPO </strong>更容易合作，同时也更有竞争力，甚至经常超越它。让我们看看我们是否能做得更好。</p><h2 id="2116" class="or md it bd me os ot dn mi ou ov dp mm lp ow ox mo lt oy oz mq lx pa pb ms pc bi translated">PPO —具有剪切物镜的变体</h2><p id="5750" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">修剪过的 PPO 是目前流行的变体，也是我们在讨论“PPO”时通常所指的。它通常优于基于惩罚的变体，并且更易于实现。</p><p id="6a2e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们不再费心去改变惩罚，而是简单地限制政策可以改变的范围。由于剪辑范围之外的更新所获得的优势不用于更新目的，我们提供了保持相对接近现有策略的激励。目标函数仍然依赖于代理优势，但现在看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/5e9710ae4a7b89ab325761b730b46884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKj97tz37I0zENkFkwdnjA.png"/></div></div></figure><p id="7dff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，<em class="mz"> ρ_t </em>是重要抽样比:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/07f74c5c314f05706a1e838560bf765d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*WCaZv6vkfik8g2WYUDrqQA.png"/></div></figure><p id="ff91" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">项<em class="mz"> (1-ϵ)⋅A </em>和<em class="mz"> (1+ϵ)⋅A </em>)不依赖于<em class="mz"> θ </em>，因此产生的梯度为 0。因此，<strong class="li iu">可信区域之外的样本被有效地丢弃</strong>，阻止过大的更新。因此，我们没有明确地约束策略更新本身，而是简单地忽略了过度偏离策略所带来的好处。和以前一样，我们可以简单地使用 ADAM 这样的优化器来执行更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/51c44059451c91daff0cffb252434eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUEQ7RXzywlV63nZ0ldJUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在 PPO 的这种变体中，替代优势被削减了。如果更新后的策略与原始策略的偏差超过ϵ，则样本产生的梯度为 0。这种机制避免了策略的过大更新，将其保留在可信区域内[图片由<a class="ae ky" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">舒尔曼等人 2017 </a> ]</p></figure><blockquote class="pe pf pg"><p id="b366" class="lg lh mz li b lj lk ju ll lm ln jx lo ph lq lr ls pi lu lv lw pj ly lz ma mb im bi translated"><em class="it">‘为什么是最小运算符？’</em>，你可能会问。为什么不直接剪辑？问得好。在积极优势的情况下，当更新的政策与旧政策偏离太大时，我们总是会进行剪辑。赢的时候，我们倾向于谨慎。失败时，有一种情况需要特别注意:优势 a 为负，1+ϵ为负。换句话说，我们进行了一次大规模更新，增加了采取更糟糕行动的可能性。在这种情况下，无界优势<em class="it"> r_t ⋅ A </em>比削波优势<em class="it"> (1+ϵ) ⋅ A </em>低<em class="it">。当我们取两者的最小值时，我们使用无界信号来执行更新，允许更大的更新来纠正我们的错误更新。这种形式的回溯是一种非常漂亮的修正机制，允许快速撤销错误方向的更新。</em></p></blockquote><p id="052f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有六种具有相应更新行为的重要情况(即，策略应该不同且优势非零)。为了理解 PPO，最好花点时间来理解下表中总结的案例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/0f3c984594dfdf980365372b83554c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8kQ1WK6jgJcZw8HdPwzKDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PPO 剪切目标的六个非平凡案例的总结。修剪后，梯度为零，无助于训练，不利于大规模政策更新[作者表格，基于<a class="ae ky" href="https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf" rel="noopener ugc nofollow" target="_blank"> Bick，2021 </a> ]</p></figure><p id="c8c5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了实现期望的行为，我们应该调整<em class="mz"> ϵ </em>，作为对 KL 发散的<strong class="li iu">隐式限制。根据经验，<em class="mz"> ϵ=0.1 </em>和<em class="mz"> ϵ=0.2 </em>是运行良好的值。这些值与自然政策梯度的理论基础有些偏离(该理论建立在假设<em class="mz"> π_θ=π_θ_k </em>的局部近似基础上)，但从经验上看，它完成了工作。</strong></p><p id="f816" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">完整的算法详述如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/f609b9fe47e8911a97f887e81f9ef6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*er2e6DH5efzjWSVK9dmgig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PPO，剪切的客观变体。新旧政策之间的最大差异被缩小了。如果策略偏离得太远，它们的样本就不会用于更新，从而阻碍了大规模更新。注意，文中用<em class="pq"> ρ_t 代替 r_t 来描述重要性抽样比</em>【来源:<a class="ae ky" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">舒尔曼等 2017 </a>】</p></figure><h2 id="4761" class="or md it bd me os ot dn mi ou ov dp mm lp ow ox mo lt oy oz mq lx pa pb ms pc bi translated">PPO2？</h2><p id="e5ac" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">你可能听说过 PPO2 这个术语。我们还需要更多的数学吗？</p><p id="a7f8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">幸运的是，PPO2 只是 Open AI 发布的算法的更新版本。PPO2 是一个矢量化环境的实现，它针对 GPU 进行了<strong class="li iu">优化，并更好地支持并行训练</strong>。它还有许多其他不同之处(例如，优势被自动规范化，值函数也被裁剪)，但使用了本文中概述的相同数学基础。如果您计划直接使用 OpenAI 实现，只需记住 PPO 已经过时，现在应该使用 PPO2。</p><h1 id="bdc9" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结束语</h1><p id="c633" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">自 2017 年推出以来，PPO 已迅速成为连续控制问题中的<strong class="li iu">最佳算法。五年对于机器学习来说是一段很长的时间，但在基准问题(主要是经典控制问题和雅达利游戏)的类别中，它仍然具有很高的竞争力。</strong></p><p id="6c65" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">看来 PPO 在速度、谨慎和可用性之间取得了恰当的平衡。尽管缺乏自然梯度和 TRPO 所具有的理论保证和数学技巧，PPO 比其竞争对手收敛得更快更好。有些矛盾的是，在深度强化学习中，简单似乎是有回报的。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="b19d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mz">今天的强化学习还没做完吗？看看下面的文章吧！</em></p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/why-reinforcement-learning-doesnt-need-bellman-s-equation-c9c2e51a0b7"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">为什么强化学习不需要贝尔曼方程</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">强化学习中著名的贝尔曼方程和 MDP 公式的再评价</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pr l no np nq nm nr ks nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">TensorFlow 2.0 中离散策略梯度的最小工作示例</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="ps l no np nq nm nr ks nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">TensorFlow 2.0 中连续策略梯度的最小工作示例</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pt l no np nq nm nr ks nd"/></div></div></a></div><h1 id="fb07" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">进一步阅读</h1><p id="4ddd" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated"><strong class="li iu">论文</strong></p><ul class=""><li id="9653" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated">阿马里，S. I. (1998 年)。<a class="ae ky" href="https://d1wqtxts1xzle7.cloudfront.net/32944066/Amari_-_Natural_Gradient_works_efficiently_in_learning-with-cover-page-v2.pdf?Expires=1662047457&amp;Signature=KiOeWpPHdAWivpd0vlFhsg4bbZt~AmV1GA5ZL9ZwUMdjDPzzLB7fMeFiaBOQ11wYGgW5MKXX-gGgGgqQms3aHjpGYivUeN~U0xeXBwceDoz6jh4~7i8G--7rdavlU0HVj1sxX1vrQUSk~qVXqbsjKm-wCtwuk9o6jXUfeJql1PGTv65T70akN6rWVPUb3mW7Xw8drbgBfDsUh1AWCrczpSuWUIZG4n~gCzmGzfqb~Jfj3zBy~WbzSLNFUgMoT3FsrI-fq0~gMkmyXKThnd~4gY00k445Y0HKQmHjNq5RO9~OCr9~kx1tTvks6Zd9BBjtUBmHPAywLiQzPHWPEAf0-A__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" rel="noopener ugc nofollow" target="_blank">自然梯度在学习中很有效。</a> <em class="mz">神经计算</em>，<em class="mz"> 10 </em> (2)，251–276。</li><li id="1163" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">比克博士(2021)。<a class="ae ky" href="https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mz"/></a>(硕士论文)对最近的政策优化提供一个连贯的自足的解释。格罗宁根国立大学。</li><li id="cc60" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">Kakade，S. M. (2001 年)。<a class="ae ky" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf" rel="noopener ugc nofollow" target="_blank">自然的政策梯度。</a> <em class="mz">神经信息处理系统的进展</em>，<em class="mz"> 14 </em>。</li><li id="30e0" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">舒尔曼，j .，莱文，s .，阿贝耳，p .，乔丹，m .，莫里茨，P. (2015)。<a class="ae ky" href="http://proceedings.mlr.press/v37/schulman15.pdf" rel="noopener ugc nofollow" target="_blank">信任区域策略优化。</a>参加<em class="mz">机器学习国际会议。</em></li><li id="c23f" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">舒尔曼，j .，沃尔斯基，f .，德里瓦尔，p .，拉德福德，a .，&amp;克里莫夫，O. (2017)。<a class="ae ky" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">近似策略优化算法</a>。<em class="mz"> arXiv 预印本 arXiv:1707.06347 </em>。</li></ul><p id="d5e2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">讲座幻灯片</strong></p><ul class=""><li id="c8a1" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated">高级政策梯度(CS 285)。加州大学柏克莱分校。</li><li id="0423" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">Achiam，J. (2017 年)。<a class="ae ky" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf" rel="noopener ugc nofollow" target="_blank">高级政策梯度方法。加州大学伯克利分校。</a></li><li id="f8c9" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">自然政策梯度，TRPO，PPO (CMU 10703)。卡内基梅隆。</li><li id="7da6" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">舒尔曼，J. (2017)。<a class="ae ky" href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view?resourcekey=0-6NrgDm29IIPlXsPESX2w4w" rel="noopener ugc nofollow" target="_blank">高级政策梯度方法:<br/>自然梯度、TRPO 等。</a> OpenAI。</li></ul><p id="75ed" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">网站</strong></p><ul class=""><li id="4075" class="nv nw it li b lj lk lm ln lp nx lt ny lx nz mb oa ob oc od bi translated">自然渐变(TRPO，PPO)。[ <a class="ae ky" href="https://julien-vitay.net/course-deeprl/notes/3.6-PPO.html" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="1c9b" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">OpenAI (2018)。信任区域策略优化。[ <a class="ae ky" href="https://spinningup.openai.com/en/latest/algorithms/trpo.html#id2" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="c5bb" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">拥抱脸(2022)。很深。[ <a class="ae ky" href="https://huggingface.co/blog/deep-rl-ppo" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="ae57" class="nv nw it li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">稳定基线(2021 年)。PPO2。[ <a class="ae ky" href="https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul></div></div>    
</body>
</html>