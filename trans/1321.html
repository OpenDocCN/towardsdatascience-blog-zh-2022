<html>
<head>
<title>An Introduction to Word2Vec in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的Word2Vec介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-word2vec-in-nlp-854e1c288894#2022-04-03">https://towardsdatascience.com/an-introduction-to-word2vec-in-nlp-854e1c288894#2022-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cc1f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Word2Vec的直观数学解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d0f15b50ddf531b6b1ae660e2ec02426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5MXBkMruQ68395XN"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯文·布兰德斯马在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw kx ky"><p id="80a6" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">裁缝向她展示如何在她的夹克上缝制一颗<strong class="lc ir">纽扣</strong>。与此同时，她的朋友在外面等着看电梯按钮。</p></blockquote><p id="1237" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">作为人类，本能的会注意到上面两句话中“按钮”这个词的不同含义。但是机器学习模型如何实现这一点呢？</p><p id="2330" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">任何使用自然语言的人的首要任务是用数字来表示单词。在过去的十年中，已经使用了许多技术，例如一键编码、TF-IDF、N-grams。但是这些技术没有包含单词语义，并且通常是稀疏表示。本文介绍了一种名为Word2Vec的单词嵌入技术。</p><h1 id="e2b6" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">Word2Vec</h1><blockquote class="kw kx ky"><p id="a5b1" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">“从一个人和什么样的人交往，你就可以知道他说了什么。”——约翰·鲁珀特·弗斯</p></blockquote><p id="2d5c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">Word2Vec是一种最先进的算法，用于在庞大的语料库中生成所有单词的固定长度分布式向量表示。Word2Vec的有效性有两个原因——第一，使用固定大小的向量，这意味着向量大小不依赖于语料库中唯一单词的数量。第二，在向量表示中加入语义信息。Word2Vec向量在将相似的单词组合在一起时非常有效。该算法可以基于单词在语料库中的位置做出强有力的估计。例如，“Kid”和“Child”是相似的，因此它们的向量表示将非常相似。</p><p id="2d98" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">Word2Vec可以在两种架构中实现— <strong class="lc ir">连续单词包(CBOW)和Skip-Gram。</strong>word 2 vec的主要思想围绕着在固定大小的窗口中基于中心单词预测上下文(外部)单词，反之亦然。</p><p id="4f0d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">例如，考虑语料库的以下部分:</p><blockquote class="kw kx ky"><p id="69a4" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="iq"> …。做梦的可能性</em> <strong class="lc ir"> <em class="iq">来了</em> </strong> <em class="iq">真的让生活变得有趣…..</em></p></blockquote><p id="49d2" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在上面的例子中，固定窗口为3，让“<strong class="lc ir"><em class="lb"/></strong>”为中心词，“<strong class="lc ir"> <em class="lb">梦</em> </strong>”和“<strong class="lc ir"> <em class="lb">真</em> </strong>”为外词。给定中心词<em class="lb">来</em>，CBOW预测<em class="lb">做梦</em>和<em class="lb">真</em>的概率，给定上下文词<em class="lb">做梦</em>和<em class="lb">真</em>，Skip-Gram预测中心词来。</p><ul class=""><li id="7263" class="mr ms iq lc b ld le lg lh lw mt lx mu ly mv lv mw mx my mz bi translated">CBOW —根据上下文(外部)单词预测中心单词。</li><li id="7bf6" class="mr ms iq lc b ld na lg nb lw nc lx nd ly ne lv mw mx my mz bi translated">skip-Gram-根据中心词预测上下文词</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/fc63b1bf43bf8d2ceb611e6c58d2d584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fWapW_nfSIHIXf0HaWRNw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CBOW和Skip Gram体系结构。例句——“想法可以改变你的生活”。图片作者。</p></figure><h1 id="c302" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">目标函数</h1><p id="eecc" class="pw-post-body-paragraph kz la iq lc b ld ng jr lf lg nh ju li lw ni ll lm lx nj lp lq ly nk lt lu lv ij bi translated">目标函数也称为误差函数或成本函数。在反向传播期间，神经网络使用梯度下降计算目标函数的最小值。下面是固定大小窗口中每个单词的概率的示例表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/aa21ac8196dc54ff9470f9a48e9e0f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWa9LbK_thqXrzylNEyQAA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">大小为1的示例窗口。使用中心单词“come”来预测上下文单词。图片作者。</p></figure><p id="f0de" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">考虑大小为t的语料库。给定中心单词Wₜ，对于t (1，2…，t)的每个位置，预测固定窗口大小m内的上下文单词。可能性由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/318ff24fdce54c560361ff22555cda69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjzR-iohTnnhDMIFIprO3Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">似然函数。图片作者。</p></figure><p id="6b68" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">似然函数取决于参数θ。θ是所有要优化的变量，它是word的向量表示。目标函数(也称为成本函数或损失函数)是对数似然的平均负值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/ce0ba38523d6693949208088e41d8e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSUJfa6Z1qv1RLKLWipcXg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标函数(损失函数)-平均负对数损失。图片作者。</p></figure><p id="1035" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">该函数对所有中心单词进行平均，因此该函数不依赖于语料库中唯一单词的数量。使用称为梯度下降的方法最小化目标函数。</p><p id="4f69" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated"><strong class="lc ir">如何计算P(Wₜ₊ᴊ /Wₜ)？</strong></p><p id="1867" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">每个单词有两个向量:</p><ul class=""><li id="01fa" class="mr ms iq lc b ld le lg lh lw mt lx mu ly mv lv mw mx my mz bi translated"><em class="lb"> v </em>当单词是中心词时</li><li id="369f" class="mr ms iq lc b ld na lg nb lw nc lx nd ly ne lv mw mx my mz bi translated"><em class="lb"> u </em>当单词是上下文(外部)单词时</li></ul><p id="4e3a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">对于中心词<em class="lb"> c </em>和上下文词<em class="lb"> o、</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/cc8c24878aad99695a41e40781d1f602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRy17w45eE6ojZkQO12aWA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">给定作者的中心词<em class="np"> c. </em>图像，外部词o的概率。</p></figure><p id="8bb7" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">因为我们需要正值，所以使用指数。点积给出了o和c的相似性，在这种情况下，更大的点积意味着更大的概率。分母在整个语料库中归一化以给出概率分布，即分子除以语料库中每个词的相似度。事实上，上述函数是一个SoftMax函数，它是有意义的，因为:</p><ul class=""><li id="71b8" class="mr ms iq lc b ld le lg lh lw mt lx mu ly mv lv mw mx my mz bi translated">Max:放大最大可能变量的概率。</li><li id="a9cc" class="mr ms iq lc b ld na lg nb lw nc lx nd ly ne lv mw mx my mz bi translated">软变量:给最小的可能变量赋值。</li></ul><p id="1a81" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">我们希望上下文单词肯定与中心单词一起出现，也就是说，具有高概率。因此，SoftMax函数是一个不错的选择。</p><p id="1602" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated"><strong class="lc ir">最终向量是如何形成的？</strong></p><p id="b710" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated">在用上述目标函数训练神经网络之后，为每个窗口获得权重。这些权重存储在一个矩阵中。为了获得每个单词的最终密集向量，将权重矩阵乘以每个单词的对应的独热向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/3bd4d2ab678868882f305eace256fc92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uDqGyLCKEtAI6Hz6sjT6sg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">获得经训练的神经网络的权重矩阵。图片作者。</p></figure><h1 id="1a94" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="98fd" class="pw-post-body-paragraph kz la iq lc b ld ng jr lf lg nh ju li lw ni ll lm lx nj lp lq ly nk lt lu lv ij bi translated">单词嵌入是自然语言处理的关键步骤。使用Word2Vec获得单词的向量表示是非常高效的，因为这样形成的向量是密集的，并且携带对任何NLP应用程序都至关重要的语义信息。</p><p id="15b4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li lw lk ll lm lx lo lp lq ly ls lt lu lv ij bi translated"><em class="lb">感谢您阅读这篇文章！如果你有任何问题，请在下面留言。请务必关注我，获取我关于机器学习和数据科学的最新媒体帖子的更新:)。有问题可以在</em> <a class="ae kv" href="http://www.linkedin.com/in/saipavanyekula" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> LinkedIn </em> </a> <em class="lb">联系我。</em></p></div></div>    
</body>
</html>