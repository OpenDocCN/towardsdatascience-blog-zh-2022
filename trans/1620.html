<html>
<head>
<title>4 Methods to Power Feature Selection for Your Next ML Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为您的下一个ML模型提供功能选择的4种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-methods-to-power-feature-engineering-for-your-next-ml-model-5d32fffbe511#2022-04-18">https://towardsdatascience.com/4-methods-to-power-feature-engineering-for-your-next-ml-model-5d32fffbe511#2022-04-18</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="97be" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">另外，处理分类、数字和混合数据的技巧</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/eaad25e965fdb6326da864df4c4a6d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D-CYSqhsV4g-WuqQ"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">edu·格兰德</a>在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="461e" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是特征选择？</h1><p id="638d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated"><strong class="lu iv">机器学习中的特征选择</strong>就是在数据集中选择最有影响力的特征，或者列。</p><p id="309b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">你的数据集有很多列吗，你想看看哪个影响最大吗？你想放弃那些没有产生太多价值的东西吗？</p><p id="06ee" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">通过执行特征选择，您不仅<strong class="lu iv">减少了需要处理以加速分析的数据量</strong>，而且<strong class="lu iv">简化了模型的解释</strong>，使其他人更容易理解。</p><p id="ee49" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">根据您拥有的数据类型，可以使用多种技术，从统计方法到利用机器学习模型进行选择。我们将看看一些最常见的技术，看看它们在实践中是如何应用的！</p><ol class=""><li id="9a5c" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">使用<strong class="lu iv">卡方检验的分类数据</strong></li><li id="80c5" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><strong class="lu iv">数值数据的皮尔逊相关系数</strong></li><li id="59be" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><strong class="lu iv">数字数据的主成分分析</strong></li><li id="dd08" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><strong class="lu iv">特征重要性</strong>和<strong class="lu iv">随机森林</strong>对于分类数据和数字数据</li></ol><p id="1ee0" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们开始吧！</p><h1 id="f0e0" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">数据和导入</h1><p id="5b32" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">对于我们今天的演示，我们将使用<strong class="lu iv">银行营销UCI </strong>数据集，可以在<a class="ae kz" href="https://www.kaggle.com/c/bank-marketing-uci" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。该数据集包含关于营销活动中银行客户的信息，并且包含一个可以在<em class="nh">分类模型</em>中使用的目标变量。该数据集位于CC0:公共领域下的公共领域，可用于任何目的。</p><p id="a133" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">有关构建分类模型的更多信息，请查看:<a class="ae kz" rel="noopener" target="_blank" href="/everything-you-need-to-know-to-build-an-amazing-binary-classifier-590de3482aad">构建令人惊叹的二进制分类器所需要知道的一切</a>和<a class="ae kz" rel="noopener" target="_blank" href="/go-beyond-binary-classification-with-multi-class-and-multi-label-models-6ce91ca08264">使用多类和多标签模型超越了二进制分类</a>。</p><p id="6f53" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们将从导入必要的库和加载数据开始。我们将利用Scikit-Learn来学习每一种不同的技术。除了这里演示的，还有许多其他<a class="ae kz" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">支持的方法</a>。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="6353" class="nn lb iu nj b be no np l nq nr">import pandas as pd<br/><br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_selection import chi2<br/>from sklearn.feature_selection import SelectKBest<br/>from sklearn.preprocessing import LabelEncoder, OrdinalEncoder<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.compose import make_column_selector as selector<br/>from sklearn.pipeline import Pipeline<br/><br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>df = pd.read_csv("bank.csv", delimiter=";")</span></pre><h1 id="38c5" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">分类值的特征选择</h1><p id="225b" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">如果你的数据集是分类的，我们可以使用<a class="ae kz" href="https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test" rel="noopener ugc nofollow" target="_blank">皮尔森卡方检验</a>。根据维基百科:</p><blockquote class="ns nt nu"><p id="5a94" class="ls lt nh lu b lv mo jv lx ly mp jy ma nv mq md me nw mr mh mi nx ms ml mm mn in bi translated"><em class="iu">卡方检验是一种应用于分类数据的统计检验，用于评估数据集之间观察到的差异偶然出现的可能性。</em></p></blockquote><p id="ff2d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">当您的<strong class="lu iv">特征数据</strong>是分类的，并且您的<strong class="lu iv">目标数据</strong>是<a class="ae kz" href="https://machinelearningmastery.com/chi-squared-test-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">分类的</a>时，您应用卡方检验，例如，分类问题。</p><p id="d7f2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">注意:</strong> <em class="nh">虽然该数据集包含分类值和数值的混合，但我们将分离分类值来演示如何应用卡方检验。下面将通过特征重要性来描述该数据集的一种更好的方法，以跨类别和数值类型选择特征。</em></p><p id="1d93" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们将开始只选择那些在熊猫中是分类的或类型<code class="fe ny nz oa nj b">object</code>的类型。Pandas将文本存储为<em class="nh">对象</em>，所以在简单使用<code class="fe ny nz oa nj b">object</code>类型之前，您应该验证这些是否是分类值。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="bc3e" class="nn lb iu nj b be no np l nq nr"># get categorical data<br/>cat_data = df.select_dtypes(include=['object'])</span></pre><p id="fb97" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">然后我们可以分离出特性和目标值。目标变量<code class="fe ny nz oa nj b">y,</code>是数据框中的最后一列，因此我们可以使用Python的切片技术将它们分成<code class="fe ny nz oa nj b">X</code>和<code class="fe ny nz oa nj b">y</code>。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="5094" class="nn lb iu nj b be no np l nq nr">X = cat_data.iloc[:, :-1].values<br/>y = cat_data.iloc[:,-1].values</span></pre><p id="3e78" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们有<a class="ae kz" href="https://machinelearningmastery.com/feature-selection-with-categorical-data/" rel="noopener ugc nofollow" target="_blank">两个函数</a>。这些函数将使用<code class="fe ny nz oa nj b">OrdinalEncoder</code>作为<code class="fe ny nz oa nj b">X</code>数据，使用<code class="fe ny nz oa nj b">LabelEncoder</code>作为<code class="fe ny nz oa nj b">y</code>数据。顾名思义，<code class="fe ny nz oa nj b">OrdinalEncoder</code>将把分类值转换成数字表示，遵循特定的<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html?highlight=ordinalencoder#sklearn.preprocessing.OrdinalEncoder" rel="noopener ugc nofollow" target="_blank">顺序</a>。默认情况下，编码器会自动选择此顺序。但是，您可以为订单提供一个值列表。<code class="fe ny nz oa nj b">LabelEncoder</code>将把相似的值转换成数字表示。根据Scikit-Learn的<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" rel="noopener ugc nofollow" target="_blank">文档</a>，您应该只在目标变量上使用<code class="fe ny nz oa nj b">LabelEncoder</code>。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="b932" class="nn lb iu nj b be no np l nq nr">def prepare_inputs(X_train, X_test):<br/>    oe = OrdinalEncoder()<br/>    oe.fit(X_train)<br/>    X_train_enc = oe.transform(X_train)<br/>    X_test_enc = oe.transform(X_test)<br/>    return X_train_enc, X_test_enc<br/><br/>def prepare_targets(y_train, y_test):<br/>    le = LabelEncoder()<br/>    le.fit(y_train)<br/>    y_train_enc = le.transform(y_train)<br/>    y_test_enc = le.transform(y_test)<br/>    return y_train_enc, y_test_enc</span></pre><p id="506b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们将把我们的数据分成<strong class="lu iv">训练和测试</strong>组，并用上述函数处理数据。请注意，上面的函数只将编码器与训练数据相匹配，并转换训练和测试数据。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="0b3b" class="nn lb iu nj b be no np l nq nr">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="0935" class="nn lb iu nj b be no np l nq nr"># prepare input data<br/>X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="9371" class="nn lb iu nj b be no np l nq nr"># prepare output data<br/>y_train_enc, y_test_enc = prepare_targets(y_train, y_test)</span></pre><p id="80bd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">现在，这个函数将帮助我们利用<code class="fe ny nz oa nj b">SelectKBest</code>方法中的卡方检验来选择最佳特征。我们可以从设置参数<code class="fe ny nz oa nj b">k='all'</code>开始，这将首先在所有特性上运行测试，然后我们可以将它应用于特定数量的特性。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="785a" class="nn lb iu nj b be no np l nq nr">def select_features(X_train, y_train, X_test, k_value='all'):<br/>    fs = SelectKBest(score_func=chi2, k=k_value)<br/>    fs.fit(X_train, y_train)<br/>    X_train_fs = fs.transform(X_train)<br/>    X_test_fs = fs.transform(X_test)<br/>    return X_train_fs, X_test_fs, fs</span></pre><p id="7e0b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以从打印出每个特性的<strong class="lu iv">分数</strong>开始。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="eba4" class="nn lb iu nj b be no np l nq nr"># feature selection<br/>X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)<br/># what are scores for the features<br/>for i in range(len(fs.scores_)):<br/>    print('Feature %d: %f' % (i, fs.scores_[i]))</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="cd97" class="nn lb iu nj b be no np l nq nr">Feature 0: 11.679248<br/>Feature 1: 0.248626<br/>Feature 2: 3.339391<br/>Feature 3: 0.039239<br/>Feature 4: 11.788867<br/>Feature 5: 12.889637<br/>Feature 6: 64.864792<br/>Feature 7: 4.102635<br/>Feature 8: 10.921719</span></pre><p id="e423" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">然而，用<strong class="lu iv">图</strong>直观地检查这些分数更容易。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="1830" class="nn lb iu nj b be no np l nq nr"># what are scores for the features<br/>names = []<br/>values = []<br/>for i in range(len(fs.scores_)):<br/>    names.append(cat_data.columns[i])<br/>    values.append(fs.scores_[i])<br/>chi_list = zip(names, values)<br/><br/># plot the scores<br/>plt.figure(figsize=(10,4))<br/>sns.barplot(x=names, y=values)<br/>plt.xticks(rotation = 90)<br/>plt.show()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oc"><img src="../Images/0696d959f078bce04847adf24921a559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kf4yH0241yuMwjWC.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="3e09" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">联系人</strong>在这里得分最大，而<strong class="lu iv">婚姻</strong>、<strong class="lu iv">默认</strong>、<strong class="lu iv">月份</strong>得分最低。总的来说，看起来有大约<code class="fe ny nz oa nj b">5</code>个特性值得考虑。我们将使用<code class="fe ny nz oa nj b">SelectKBest</code>方法来选择顶部的<code class="fe ny nz oa nj b">5</code>特征。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="94cc" class="nn lb iu nj b be no np l nq nr">X_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc, 5)</span></pre><p id="09b3" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以打印<strong class="lu iv">顶级特性</strong>，这些值对应于上面的索引。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="1159" class="nn lb iu nj b be no np l nq nr">fs.get_feature_names_out()</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="843a" class="nn lb iu nj b be no np l nq nr">[OUT:]<br/>array(['x0', 'x4', 'x5', 'x6', 'x8'], dtype=object)</span></pre><p id="1533" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">最后，我们可以打印出<code class="fe ny nz oa nj b">X_train_fs</code>和<code class="fe ny nz oa nj b">X_test_fs</code>数据的<strong class="lu iv">形状</strong>，并看到第二维度是所选特征的<code class="fe ny nz oa nj b">5</code>。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="53ec" class="nn lb iu nj b be no np l nq nr">print(X_train_fs.shape)<br/>print(X_test_fs.shape)</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="73fa" class="nn lb iu nj b be no np l nq nr">(3029, 5)<br/>(1492, 5)</span></pre><h1 id="5581" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">数值的特征选择</h1><p id="1022" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">当处理纯数值数据时，有两种方法我更喜欢使用。第一个是<strong class="lu iv">皮尔逊相关系数</strong>，第二个是<strong class="lu iv">主成分分析</strong>或<strong class="lu iv"> PCA </strong>。</p><h2 id="a87e" class="od lb iu bd lc oe of dn lg og oh dp lk mb oi oj lm mf ok ol lo mj om on lq oo bi translated">皮尔逊相关系数</h2><p id="2604" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">先说皮尔逊相关系数。虽然这不是一个明确的特征选择方法，但它有助于我们可视化高度相关的特征。当两个或更多特征高度相关时，它们在训练时向模型贡献非常相似的信息。通过计算和绘制相关矩阵，我们可以快速检查是否有任何值高度相关，如果有，我们可以选择从模型中删除一个或多个值。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="d83e" class="nn lb iu nj b be no np l nq nr">corr = df.corr()<br/><br/>f, ax = plt.subplots(figsize=(12, 8))<br/><br/>sns.heatmap(corr, cmap="Blues", annot=True, square=False, ax=ax)<br/>plt.title('Pearson Correlation of Features')<br/>plt.yticks(rotation=45);</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj op"><img src="../Images/d34fd5f43b81d364995e79875b8716a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QW2yJv2L86S8eKSA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="c088" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在这个例子中，<strong class="lu iv"> pdays </strong>和<strong class="lu iv"> previous </strong>具有最强的相关性<code class="fe ny nz oa nj b">0.58</code>，其他的都是相互独立的。<code class="fe ny nz oa nj b">0.58</code>的相关性不是很强。因此，我将选择在模型中保留这两者。</p><h2 id="3177" class="od lb iu bd lc oe of dn lg og oh dp lk mb oi oj lm mf ok ol lo mj om on lq oo bi translated">主成分分析</h2><p id="8b3c" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在所有数据都是数字的模型中，主成分分析是最有效的特征选择方法。<strong class="lu iv"> PCA </strong>不是特征选择方法，而是降维方法。然而，PCA的目标类似于特征选择，我们希望减少计算模型所需的数据量。</p><p id="daee" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">下图显示了应用于照片的PCA。该图表示每个主成分的累积解释方差的量。在这个例子中，我们能够用图像的3000多个主成分中的54个来解释图像中95%的变化。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/0378dccb23439d5e541507c89b4cbf32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*MGVttIdsfTyOEAsn.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="240b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">有关PCA如何工作和执行的更多信息，请查看以下文章:<a class="ae kz" rel="noopener" target="_blank" href="/2-beautiful-ways-to-visualize-pca-43d737e48ff7"> 2种可视化PCA的美丽方式</a>和<a class="ae kz" rel="noopener" target="_blank" href="/image-compression-with-pca-a0595f57940c">通过图像压缩实现主成分分析的魔力</a>。</p><h1 id="f180" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">随机森林的要素重要性</h1><p id="6603" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">最后，我选择特性的首选方法是利用<strong class="lu iv">随机森林</strong>及其计算<strong class="lu iv">特性重要性</strong>的能力。Scikit-Learn将计算功能列表及其对整体性能的相对贡献，作为训练模型的输出。该方法也适用于其他袋装树，如<strong class="lu iv">额外树</strong>和<strong class="lu iv">梯度推进</strong>进行分类和回归。</p><p id="a1f2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">这种方法的好处是它非常适合构建模型的整个流程。让我们来看看这是如何工作的。让我们从获取数据框中的所有列开始。注意，在这种情况下，我们可以利用分类数据和数字数据。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="2ccc" class="nn lb iu nj b be no np l nq nr">print(df.columns)</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="fc46" class="nn lb iu nj b be no np l nq nr">Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y'],<br/>      dtype='object')</span></pre><p id="6ed7" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">当阅读这个数据集的文档时，您会注意到<strong class="lu iv">持续时间</strong>列是我们不应该用来训练您的模型的。我们将手动将其从我们的列列表中删除。</p><blockquote class="ns nt nu"><p id="1618" class="ls lt nh lu b lv mo jv lx ly mp jy ma nv mq md me nw mr mh mi nx ms ml mm mn in bi translated"><strong class="lu iv"> <em class="iu">持续时间</em> </strong> <em class="iu">:最后一次联系持续时间，单位为秒(数字)。此属性对输出目标有很大影响(例如，如果duration=0，则y='no ')。然而，在执行呼叫之前，持续时间是未知的。还有，结束通话后y显然是已知的。因此，该输入应仅用于基准测试目的，如果目的是获得现实的预测模型，则应丢弃。</em></p></blockquote><p id="a12f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">此外，如果我们愿意，我们可以删除任何其他可能没有用的功能。在这个例子中，我们将把它们都排除在持续时间之外。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="f572" class="nn lb iu nj b be no np l nq nr"># Remove columns from the list that are not relevant. <br/>targets = ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'day', 'month', 'campaign', 'pdays', 'previous', 'poutcome']</span></pre><p id="b78e" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们将利用<strong class="lu iv">列转换器</strong>将我们的数据转换成机器学习可接受的格式。每当我为可重复性建立模型时，我更喜欢使用<strong class="lu iv">管道</strong>。</p><p id="028b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">查看我的文章:停止一步一步地构建你的模型，以获得更多关于它们的信息:<a class="ae kz" rel="noopener" target="_blank" href="/using-pipelines-in-sci-kit-learn-516aa431dcc5">用管道自动化这个过程！</a>。</p><p id="e034" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我为数值选择了<code class="fe ny nz oa nj b">MinMaxScaler</code>,为分类值选择了<code class="fe ny nz oa nj b">OrdinalEncoder</code>。在最终的模型中，我最有可能使用<code class="fe ny nz oa nj b">OneHotEncoder</code> (OHE)作为分类特征，但是为了确定特征的重要性，我们不想用OHE扩展列；如果将它们视为一个带有顺序编码值的列，我们将获得更多的价值。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="e46c" class="nn lb iu nj b be no np l nq nr">column_trans = ColumnTransformer(transformers=<br/>       [('num', MinMaxScaler(), selector(dtype_exclude="object")),<br/>       ('cat', OrdinalEncoder(), selector(dtype_include="object"))],<br/>        remainder='drop')</span></pre><p id="3853" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们用一些首选设置创建一个分类器实例，比如<code class="fe ny nz oa nj b">class_weight='balanced'</code>，这有助于处理不平衡的数据。我们还将设置<code class="fe ny nz oa nj b">random_state=42</code>,以确保得到相同的结果。</p><p id="ac29" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">有关不平衡数据的更多信息，请查看:<a class="ae kz" rel="noopener" target="_blank" href="/working-with-imbalanced-data-efbd96b3e655">在构建您的ML模型时，不要陷入不平衡数据的陷阱</a>。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="7931" class="nn lb iu nj b be no np l nq nr"># Create a random forest classifier for feature importance<br/>clf = RandomForestClassifier(random_state=42, n_jobs=6, class_weight='balanced')<br/><br/>pipeline = Pipeline([('prep',column_trans),<br/>                     ('clf', clf)])</span></pre><p id="2dce" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们将把数据分成训练集和测试集，并使我们的模型适合训练数据。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="deb4" class="nn lb iu nj b be no np l nq nr"># Split the data into 30% test and 70% training<br/>X_train, X_test, y_train, y_test = train_test_split(df[targets], df['y'], test_size=0.3, random_state=0)<br/><br/>pipeline.fit(X_train, y_train)</span></pre><p id="d8ef" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以对分类器调用<code class="fe ny nz oa nj b">feature_importances_</code>方法来查看输出。注意如何通过调用分类器的名称<code class="fe ny nz oa nj b">clf</code>来引用管道中的分类器，类似于Python中的字典。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="0404" class="nn lb iu nj b be no np l nq nr">pipeline['clf'].feature_importances_</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="7ee7" class="nn lb iu nj b be no np l nq nr">array([0.12097191, 0.1551929 , 0.10382712, 0.04618367, 0.04876248,<br/>       0.02484967, 0.11530121, 0.15703306, 0.10358275, 0.04916597,<br/>       0.05092775, 0.02420151])</span></pre><p id="657c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，让我们按照它们的最大和<strong class="lu iv">累积重要性</strong>来显示它们。“累积重要性”列有助于直观地显示总数是如何累加到<code class="fe ny nz oa nj b">1</code>的。还有一个循环，可以砍掉任何对整体重要性贡献小于<code class="fe ny nz oa nj b">0.5</code>的特征。这个截止值是任意的。你可以寻找<code class="fe ny nz oa nj b">0.8</code>或<code class="fe ny nz oa nj b">0.9</code>的总累积重要性。试验一下你的模型如何执行更少或更多的功能。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="aba4" class="nn lb iu nj b be no np l nq nr">feat_list = []<br/><br/>total_importance = 0<br/># Print the name and gini importance of each feature<br/>for feature in zip(targets, pipeline['clf'].feature_importances_):<br/>    feat_list.append(feature)<br/>    total_importance += feature[1]<br/><br/>included_feats = []<br/># Print the name and gini importance of each feature<br/>for feature in zip(targets, pipeline['clf'].feature_importances_):<br/>    if feature[1] &gt; .05:<br/>        included_feats.append(feature[0])<br/><br/>print('\n',"Cumulative Importance =", total_importance)<br/><br/># create DataFrame using data<br/>df_imp = pd.DataFrame(feat_list, columns =['FEATURE', 'IMPORTANCE']).sort_values(by='IMPORTANCE', ascending=False)<br/>df_imp['CUMSUM'] = df_imp['IMPORTANCE'].cumsum()<br/>df_imp</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="0a57" class="nn lb iu nj b be no np l nq nr">      FEATURE  IMPORTANCE    CUMSUM<br/>1         job    0.166889  0.166889<br/>0         age    0.151696  0.318585<br/>2     marital    0.134290  0.452875<br/>13   previous    0.092159  0.545034<br/>6     housing    0.078803  0.623837<br/>3   education    0.072885  0.696722<br/>4     default    0.056480  0.753202<br/>12      pdays    0.048966  0.802168<br/>8     contact    0.043289  0.845457<br/>7        loan    0.037978  0.883436<br/>14   poutcome    0.034298  0.917733<br/>10      month    0.028382  0.946116<br/>5     balance    0.028184  0.974300<br/>11   campaign    0.021657  0.995957<br/>9         day    0.004043  1.000000</span></pre><p id="391c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">最后，基于这个循环，让我们打印出我们选择的所有特性。基于这个分析，我们从我们的模型中删除了大约50%的影响，我们可以看到哪些影响最高！</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="031c" class="nn lb iu nj b be no np l nq nr">print('Most Important Features:')<br/>print(included_feats)<br/>print('Number of Included Features =', len(included_feats))</span></pre><pre class="ob ni nj nk bn nl nm bi"><span id="914e" class="nn lb iu nj b be no np l nq nr">Most Important Features:<br/>['age', 'job', 'marital', 'education', 'default', 'housing', 'previous']<br/>Number of Included Features = 7</span></pre><p id="9396" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">感谢您的阅读！你可以在<a class="ae kz" href="https://github.com/broepke/FeatureSelection/blob/main/FeatureSelection.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这篇文章的所有代码</p><h1 id="314b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="a075" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">特征选择是模型构建过程的关键部分，它不仅有助于提高性能，还可以简化模型及其解释。我们开始考虑利用卡方检验来选择分类特征。接下来，我们查看了皮尔逊相关矩阵，以直观地识别高度相关的数值特征，并介绍了主成分分析(PCA)作为自动降低数值数据集维数的工具。最后，我们将随机森林中的<code class="fe ny nz oa nj b">feature_importances_</code>作为一种选择分类值和数值的方法。我们希望这能让你开始你的特性选择之旅！</p><p id="6fec" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如果你喜欢阅读这样的故事，并且想支持我成为一名作家，那就考虑成为一名灵媒吧。一个月5美元，让你可以无限制地访问成千上万篇文章。如果您使用 <a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener"> <em class="nh">我的链接</em> </a> <em class="nh">注册，我将为您赚取一小笔佣金，无需额外费用。</em></p></div></div>    
</body>
</html>