<html>
<head>
<title>Causal Inference with Linear Regression: Omitted variables and Irrelevant variables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归因果推断:省略变量和无关变量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc#2022-05-10">https://towardsdatascience.com/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc#2022-05-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e349" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解偏差和方差:省略变量、混淆变量、无关变量和多重共线性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/82914c7f7fda003a88d7ef9fd8ec365c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*ogOe9sKSgIcz0cbQQLeUTQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="cb11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我之前的文章<a class="ae lq" rel="noopener" target="_blank" href="/causal-inference-econometric-models-vs-a-b-testing-190781fe82c5"> <strong class="kw iu">因果推断:计量经济模型vs. A/B检验</strong> </a>中，我们讨论了如何使用一个计量经济模型，即线性回归，在控制其他协变量的同时，考察治疗变量和反应变量之间的因果关系。在本文中，我们将讨论设计线性回归时的一些常见问题— <strong class="kw iu">省略重要变量</strong>和<strong class="kw iu">包括无关变量</strong>。</p><p id="1578" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们讨论这些问题之前，我们需要熟悉系数估计的偏差和方差。</p><ul class=""><li id="d90f" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><strong class="kw iu">偏差</strong>测量拟合值和真实估计值之间的差异。如果线性回归的治疗效果有偏差，这意味着我们有一个<strong class="kw iu">不准确的</strong>因果效应。</li><li id="7086" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><strong class="kw iu">方差</strong>测量估计值(随机变量)在期望值周围的分布。<strong class="kw iu">方差越高，估计精度越低。</strong></li></ul><h2 id="14fd" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">如果省略重要变量会怎么样？</h2><p id="22db" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">我们都应该知道，我们<strong class="kw iu">不应该</strong>省略线性回归中的重要变量。这种行为的后果将使模型<em class="nd">无法正确解释响应变量</em>(又名<strong class="kw iu">欠拟合</strong>)并可能<em class="nd">做出错误的因果推断陈述</em>。</p><p id="9025" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们更深入地研究一下它会对我们的模型造成多大的损害。</p><p id="9fb3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最简单的线性回归如下所示。响应变量(即Y)可以解释为解释变量(如截距、X1、X2、X3……)的线性组合，而<strong class="kw iu"> ε </strong>是误差项，表示拟合响应值和实际响应值之间的差异。误差项的<a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4">正态假设对于线性回归模型来说是可选的，但是对于因果推断任务来说是推荐的。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/4a18d8edda3d2bd48abca8e7dc65a194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgpVbIZ4EQM4TPXjK5s4Qg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1(作者图片)</p></figure><p id="dea0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4">根据您希望如何设置<strong class="kw iu">成本函数</strong>，我们可以通过普通最小二乘法(<strong class="kw iu">【OLS】</strong>)或最大似然估计(<strong class="kw iu">【最大似然法】</strong> ) </a>来驱动封闭形式的解决方案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/ac9e6aebf046933bdbf39301f415328d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W-Z9DV1Qn86paY-g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2(作者图片)</p></figure><p id="4d62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，让我们通过将解释变量分解为模型中的<strong class="kw iu">处理变量</strong>(即T)和其他解释变量(即X)来重写图1中的方程，以便更容易地调查省略重要变量会如何严重地损害处理变量的系数估计值(即α)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nk"><img src="../Images/9172e405d4daa9bda5a278f11ab7a108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V087ZuD8Oyuky-SGjdmDgw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3(作者图片)</p></figure><p id="122b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">根据图3和图2中的第二个等式，我们可以将“α_hat”的拟合值表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nl"><img src="../Images/2b8824c69c86bb2c0492d17538186802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPUyAhPGftV5McgKLC8Ljg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图4(作者图片)</p></figure><p id="9a08" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设我们发现图3的线性回归模型中遗漏了一个重要变量(即<strong class="kw iu"> Z </strong>)。正确的模型应该是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nm"><img src="../Images/867a1c24b4ef228e9909c516cbfc1177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mMGz24gj3U_0Hzo4KTZMfA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图5(作者图片)</p></figure><p id="8b3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中γ是省略变量z的系数。</p><p id="df42" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了调查我们把图4中治疗变量的系数估计搞得有多乱，我们将用图5中的正确模型替换图4中的Y。现在我们有了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nn"><img src="../Images/493222393024e2ccdf171db3524d428b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3ycEVIP2EI_MYYYNkowVw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图6(作者图片)</p></figure><p id="de2c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图6中的第一项可以简化为α，因为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi no"><img src="../Images/1ecf88feed64fb17ab3640bc07f66f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5Eg__11GwY4aOY3_sHj7g.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图7(作者图片)</p></figure><p id="4410" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图6中的第三项应该等于0，因为当我们建立线性模型时，误差项应该与解释变量的<strong class="kw iu">无关。此外，第三项的期望值也将是0，因为假设误差项的期望值也是0。</strong></p><p id="9a36" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，图6中的等式可以简化如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi np"><img src="../Images/9e7b6db2507cc2dd88c9415ef0506c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLqH-jWJcTXRQ7BnjaCQLg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图8(作者图片)</p></figure><p id="741a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">场景1 </strong>:省略变量Z与治疗变量t<strong class="kw iu">相关</strong>，我们称这类变量为<strong class="kw iu">混杂变量</strong>，因为它们与反应变量和治疗变量都相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nq"><img src="../Images/b86d3a5a1bb402ba13d4e4b0db3f4d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MX38Ucm6ekSp09eAuYcc4Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图9(作者图片)</p></figure><blockquote class="nr ns nt"><p id="2300" class="ku kv nd kw b kx ky ju kz la lb jx lc nu le lf lg nv li lj lk nw lm ln lo lp im bi translated">当被省略的变量Z与治疗变量T相关联，并且能够有意义地解释响应变量时，那么图8中的第二项不再是0。因此，治疗效果的OLS估计值<strong class="kw iu">不再是无偏的</strong>，如果省略变量z，我们可能会做出错误的因果推断陈述</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nx"><img src="../Images/5d09b849f7ccf56815cd90d1405156ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdWoTiGeK99nf1qzYIlqJQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图10(作者图片)</p></figure><p id="1449" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果混淆变量Z在线性回归模型中被省略，那么治疗变量将成为<strong class="kw iu">内生变量</strong>，因为“无法解释的”变量Z漏入误差项，那么治疗变量将与误差项相关<strong class="kw iu">。在这种情况下，对治疗变量的估计会变得有偏差(即<strong class="kw iu">内生性偏差</strong>)。</strong></p><p id="509f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">场景2 </strong>:省略变量Z<strong class="kw iu">与治疗变量t</strong>不相关。</p><p id="529a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当忽略的变量与回归模型中的治疗变量不相关时，图8中的第二项将为0。因此，治疗效果的OLS估计值<strong class="kw iu">仍然是无偏的。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ny"><img src="../Images/ede05eedeb92c334b4d87ddd2f24c0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbYGhoGzbS6IOaUexsqWAg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图11(作者图片)</p></figure><p id="d581" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是，如果忽略这些变量，还是要付出代价的。即使省略变量Z与治疗变量T<strong class="kw iu"/>不相关，变量Z仍然起到解释响应变量的作用，排除变量Z会将这个<strong class="kw iu">未解释的</strong>部分归入误差项<strong class="kw iu"> ε </strong>并使误差的方差更大。因此，所有估计量(包括治疗效果)的<strong class="kw iu">方差</strong>会更大(见图12)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nz"><img src="../Images/a8fbf09ef9506fd6f6f31df2ecfbc028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-ERqiY-oAfLJRhBv4QZkQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图12(作者图片)</p></figure><blockquote class="nr ns nt"><p id="d572" class="ku kv nd kw b kx ky ju kz la lb jx lc nu le lf lg nv li lj lk nw lm ln lo lp im bi translated">换句话说，如果被省略的变量与治疗变量不相关，尽管治疗效果仍然是无偏的，但治疗效果的方差会变大，假设检验的t值会变小，然后p值会变大，因此，我们可能<strong class="kw iu">错误地</strong>得出治疗效果在统计上不显著的结论(即<strong class="kw iu">假阴性</strong>)。</p></blockquote></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h2 id="3a8e" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">如果包含无关变量会怎么样？</h2><p id="5857" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">似乎省略变量的问题可以通过在线性回归模型中包含所有相关变量来轻松解决。但是在模型中包含不相关的变量可能会导致其他问题。</p><p id="7726" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">根据Jeffrey Wooldridge的教科书，<em class="nd">计量经济学导论，</em>高斯-马尔可夫假设下，<strong class="kw iu">条件</strong>独立变量的样本值，我们可以将方差公式(图12)改写如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/67da309a84a9285692fbe85b5c6f6875.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*231lAVpz9socz4BeJd4doQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图13</p></figure><p id="fea2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中j代表特定的解释变量j。SST _ j是解释变量j的总样本变化量，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oi"><img src="../Images/61a6765887691e07c5b8e4625989bb6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfM6OdD-If3aFYalczcCdA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图14</p></figure><p id="9c2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">R2_j是对剩余预测因子的预测因子j的回归的决定系数，预测因子j在左侧，所有其他预测因子在右侧。</p><p id="0e24" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的术语叫做<strong class="kw iu">方差膨胀因子</strong> (VIF)。在线性回归模型中分析<strong class="kw iu">多重共线性</strong>的大小是一个有用的工具，在该模型中，预测因子彼此相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oj"><img src="../Images/3b037ecd23b1fdc7326e6372af0dfd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSshpTkNs5s0mqrnb4oslw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图15</p></figure><ul class=""><li id="71b3" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">当预测值j与其他预测值不相关时，VIF_j等于1。</li><li id="7af8" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">当预测因子j可以被其他预测因子解释时，VIF j将变得大于1。</li><li id="ff4e" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">经验法则是，如果VIF j大于10，则多重共线性很高。</li></ul><p id="f4cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们考虑以下场景:</p><p id="0a0d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">场景1: </strong>从上一节(图8)我们知道，如果一个变量<strong class="kw iu">与治疗变量</strong>高度相关，那么在线性回归模型中包含这样一个变量将很可能<strong class="kw iu">掩盖治疗变量的真实因果关系</strong>(即高偏倚)。很明显，我们需要排除这样一个变量。</p><p id="c0c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">情景二:</strong>如果一个变量与处理变量不相关，但与其余解释变量高度相关(即多重共线性)。那么<em class="nd">包含这类变量不会增加治疗效果</em>的偏倚和方差(见图8和图13)。然而，<em class="nd">当变量不能解释响应变量</em>的变化时，会增加其他解释变量的偏差和方差(即，变量没有减少分子，但减少了图13中的分母)。最坏的情况是多重共线性)。在这种情况下，我们将排除这样一个变量。</p><p id="266e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">情景3: </strong>该情景类似于情景2，<em class="nd">除了该变量也解释了响应变量</em>的变化。在这种情况下，如果我们比多重共线性更关心省略变量的问题，那么我们可以将它保留在模型中，并接受多重共线性。我们可能会有更大或更小的标准误差(见图13，尽管增加变量后分母变小，但如果增加变量也有助于解释响应变量，分子也会变小)。在这种情况下，我们需要考虑增加的解释能力和多重共线性之间的权衡。</p><p id="8929" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">情景4: </strong>如果变量与解释变量和响应变量都不相关，那么你是增加还是省略它们就太重要了(就偏差和方差而言)。然而，如果在模型中加入大量这样的变量，它将开始减少模型中的自由度，然后增加估计的方差(见图12)。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h2 id="fdaf" class="mf mg it bd mh mi mj dn mk ml mm dp mn ld mo mp mq lh mr ms mt ll mu mv mw mx bi translated">最终注释</h2><p id="21fc" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">总之，如果一个被忽略的变量或一个不相关的变量与治疗变量相关，那么治疗效果就会变得有偏差。包含与现有预测值相关的无关变量将增加估计值的方差，并使估计值和预测值不太精确。</p><p id="5aa9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你对<strong class="kw iu">线性回归</strong>和<strong class="kw iu">因果推断</strong>感兴趣，这里有一些相关的帖子可以浏览。</p><ul class=""><li id="7465" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/causal-inference-econometric-models-vs-a-b-testing-190781fe82c5"> <strong class="kw iu">因果推断:计量经济模型vs. A/B检验</strong> </a></li><li id="c8f7" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-vs-logistic-regression-ols-maximum-likelihood-estimation-gradient-descent-bcfac2c7b8e4"> <strong class="kw iu">线性回归与逻辑回归:OLS、最大似然估计、梯度下降</strong> </a></li><li id="95b6" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-with-ols-unbiased-consistent-blue-best-efficient-estimator-359a859f757e"><strong class="kw iu">OLS线性回归:无偏、一致、蓝色、最佳(有效)估计量</strong> </a></li><li id="13fc" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/understand-bias-and-variance-in-causal-inference-with-linear-regression-a02e0a9622bc"> <strong class="kw iu">线性回归因果推断:省略变量和无关变量</strong> </a></li><li id="7384" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/causal-inference-with-linear-regression-endogeneity-9d9492663bac"> <strong class="kw iu">用线性回归进行因果推断:内生性</strong> </a></li><li id="773a" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-with-ols-heteroskedasticity-and-autocorrelation-c12f1f65c13"> <strong class="kw iu">与OLS的线性回归:异方差和自相关</strong> </a></li></ul><h1 id="24e7" class="ok mg it bd mh ol om on mk oo op oq mn jz or ka mq kc os kd mt kf ot kg mw ou bi translated">感谢您的阅读！！！</h1><p id="1acc" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">如果你喜欢这篇文章，并且想<strong class="kw iu">请我喝杯咖啡，请<a class="ae lq" href="https://ko-fi.com/aaronzhu" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</strong></p><p id="be70" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以注册一个<a class="ae lq" href="https://aaron-zhu.medium.com/membership" rel="noopener"> <strong class="kw iu">会员</strong> </a>来解锁对我的文章的完全访问，并且可以无限制地访问介质上的所有内容。如果你想在我发表新文章时收到电子邮件通知，请订阅。</p></div></div>    
</body>
</html>