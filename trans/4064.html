<html>
<head>
<title>How Docker Runs Machine Learning on NVIDIA GPUs, AWS Inferentia, and Other Hardware AI Accelerators</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Docker如何在英伟达GPU、AWS推理和其他硬件AI加速器上运行机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802#2022-09-09">https://towardsdatascience.com/how-docker-runs-machine-learning-on-nvidia-gpus-aws-inferentia-and-other-hardware-ai-accelerators-e076c6eb7802#2022-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b886" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解Docker如何在Kubernetes上简化对NVIDIA GPUs、AWS推理和缩放ML容器的访问</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ce7602959455d42e09f0195f62b126d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z5vw2WIdYVFtNHUT"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="5900" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果几年前你告诉我，数据科学家将在他们的日常工作中使用Docker容器，我不会相信你。作为更广泛的机器学习(ML)社区的一员，我总是考虑Docker、Kubernetes、Swarm(还记得吗？)供IT/运营专家使用的奇特基础设施工具。今天是一个不同的故事，我几乎没有一天不使用Docker容器来训练或托管模型。</p><p id="6216" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">机器学习开发区别于传统软件开发的一个属性是，它依靠专门的硬件如GPU、Habana Gaudi、AWS Inferentia来加速训练和推理。这使得独立于硬件的容器化部署变得非常困难，而这正是容器的主要优势之一。在这篇博文中，我将讨论Docker和容器技术是如何发展来应对这一挑战的。我们将讨论:</p><ul class=""><li id="0640" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">为什么Docker已经成为当今机器学习的重要工具，以及它如何解决机器学习的特定挑战</li><li id="fb87" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">Docker如何访问拥有多种类型处理器(CPU + AI加速器)的异构系统上的专用硬件资源。</li><li id="dacb" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">不同的人工智能加速器如何扩展Docker的硬件访问，以1/NVIDIA GPU和NVIDIA容器工具包以及2/ AWS推理和神经元SDK对容器的支持为例</li><li id="5941" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">如何利用硬件加速节点在Kubernetes上扩展Docker容器</li></ul><p id="6e56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇博文的很大一部分是motiviation和“它如何在引擎盖下工作”，但我也包括了如何开始使用NVIDIA GPUs上的Docker容器或AWS上的AWS推理工具的演练、链接和截图，因此对于学习者和实践者来说都有一些东西。</p><h1 id="2692" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">Docker和机器学习没有一起长大(但他们现在是最好的朋友)</h1><p id="b687" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">如今，大多数组织使用Docker和容器技术来简化开发和部署过程，因为容器化的应用程序是一致的、可移植的，并且保证可再现性。虽然Docker容器应该是硬件不可知的和平台不可知的，但是大多数基于机器学习的软件栈是硬件特定的，并且需要访问主机操作系统上的硬件和硬件驱动程序。为了更好地理解这个问题，让我们仔细看看典型的机器学习软件栈的解剖。</p><h2 id="fb60" class="nc mg iq bd mh nd ne dn ml nf ng dp mp le nh ni mr li nj nk mt lm nl nm mv nn bi translated">典型机器学习软件堆栈剖析</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/3661f5eb442c4e3e8928ce3b588f108f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-RANuJab_9MuIiBi"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="93fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图所示的软件堆栈看起来非常典型。用蓝色花括号显示的顶部包括你的代码、你正在使用的框架、语言和其他底层库。无论您使用什么应用程序、框架或语言，这一部分看起来都是相似的。栈的其余部分是机器学习开始变得独特的地方。绿框是影响便携性的硬件特定组件，它们的作用如下:</p><ul class=""><li id="a840" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir"> AI加速器ML库:</strong>这些是ML框架用来在AI加速器硬件上实现机器学习训练或推理例程的低级库。他们实现线性代数和其他计算密集型例程，这些例程可以并行化并在AI加速器上运行。</li><li id="0608" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">人工智能加速器驱动程序:</strong>这些是主机操作系统用来识别和支持人工智能加速器硬件的驱动程序。</li><li id="ba8d" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir"> AI加速器</strong>:这是一款专用处理器，旨在加速机器学习计算。机器学习主要由矩阵-矩阵数学运算组成，这些专门的硬件处理器旨在通过利用并行性来加速这些计算。你可以在其他博客文章中阅读更多关于AI加速器的内容:1/<a class="ae np" rel="noopener" target="_blank" href="/a-complete-guide-to-ai-accelerators-for-deep-learning-inference-gpus-aws-inferentia-and-amazon-7a5d6804ef1c">AI加速器指南</a>2/<a class="ae np" rel="noopener" target="_blank" href="/ai-accelerators-machine-learning-algorithms-and-their-co-design-and-evolution-2676efd47179">AI加速器的演变</a></li></ul><p id="5e8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">拥有一个CPU +其他类型处理器的系统称为异构系统。异构系统提高了性能和效率，因为有专用于特定任务的处理器，但代价是增加了复杂性。这种复杂性的增加给软件堆栈的可移植性和开发者用户体验带来了挑战。这是Docker必须解决的挑战，我们将在下面看到它是如何做到的。</p><h1 id="71ad" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">Docker如何支持主机设备？</h1><p id="c5a6" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">默认情况下，Docker容器不能访问主机操作系统资源，这是设计使然。Docker容器旨在提供进程隔离，因此用户必须提供对主机资源(如卷和设备)的显式访问。您可以通过使用<code class="fe nq nr ns nt b">—-devices</code>参数来做到这一点，但是它带有一些警告。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/9b3ccbcf346b35b556a24fae2cbb3d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fC_UswDIFJ0BDWwI"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="1554" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一些硬件设备，如USB和串行设备，具有在内核空间运行的驱动程序，Docker必须依赖主机内核并执行系统调用来与设备硬件交互。使用<code class="fe nq nr ns nt b">--device</code>参数并安装设备，可以在Docker容器进程中轻松访问这些设备。然而，其他硬件设备，如一些网卡和人工智能加速器，其驱动程序具有用户空间组件和内核空间模块。在这种情况下，您需要在主机操作系统和Docker容器中重复安装驱动程序。</p><p id="6f9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设您想要启动一个容器进程，并且想要公开一个名为<code class="fe nq nr ns nt b">device_name0</code>的设备，那么您将运行以下命令:</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="fe07" class="nc mg iq nt b gy ny nz l oa ob">docker run --device /dev/device_name0:/dev/device_name0 …</span></pre><p id="206f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果<code class="fe nq nr ns nt b">device_name0</code>同时具有用户空间和内核空间驱动程序组件，那么您还必须在Docker容器中安装设备驱动程序，这将复制主机和容器环境中的驱动程序，如本节顶部的插图所示。</p><p id="2796" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种设置有一些缺点:</p><ul class=""><li id="d78c" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">增加集装箱尺寸:</strong>集装箱应该是轻便易携带的，而司机通常很大，占据很大空间。例如，最新的NVIDIA GPU驱动程序可以在容器中占用350到500 MB的额外空间。这增加了容器映像下载和实例化时间，并且会影响应用程序的延迟敏感性和用户体验</li><li id="0f19" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">驱动程序版本不匹配:</strong>为了使该解决方案有效，容器环境和主机操作系统中的驱动程序版本必须匹配。如果它们不同，容器进程可能无法与硬件对话。</li><li id="ae42" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">设置复杂性:</strong>在容器中安装驱动程序是额外的步骤，你放入容器中的每一个额外的库或软件都会增加额外的复杂性，并且需要进行测试。</li><li id="aee2" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">便携性下降:</strong> Docker容器被设计成轻量级、便携和硬件无关的。容器中的驱动程序使它变得笨重、特定于硬件、特定于平台，并且基本上没有Docker容器的所有优点。</li></ul><p id="777a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">至少，目标应该是通过一种解决方案来解决这些挑战，这种解决方案不需要在容器内复制设备驱动程序，并允许Docker通过多个加速器来固定访问系统上的一些或所有AI加速器。现在让我们来看看Docker是如何应对这一挑战的。</p><h1 id="8a14" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">我们如何让Docker与专门的机器学习硬件一起工作？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/d9cac702ef218afe34c0f344fccd2095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oh5VI-peiNVq3LIL"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="9fc0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将从快速概括Docker如何运行容器开始。当您使用Docker CLI通过<code class="fe nq nr ns nt b">docker run …</code>启动一个容器进程时，在它最终调用一个非常重要的库<code class="fe nq nr ns nt b">runC</code>之前，会发生一系列的步骤。</p><p id="e99e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe nq nr ns nt b">runC</code>是一个开放容器倡议(OCI)兼容工具，用于生成和运行容器进程，由Docker、Podman、CRI-O和其他流行的容器运行时使用。当您使用Docker CLI运行一个容器时，Docker为<code class="fe nq nr ns nt b">runC</code>提供了一个符合OCI规范的运行时规范文件(在图中显示为“OCI规范”)和容器的根文件系统，统称为符合OCI规范的<code class="fe nq nr ns nt b">Bundle</code>，后者<code class="fe nq nr ns nt b">runC</code>将其作为创建和运行容器进程的输入。OCI规范文件提供了额外的选项，比如启动前和启动后挂钩，以便在创建容器处理之前和之后启动其他进程。硬件提供者可以利用这些钩子将硬件访问注入到容器进程中。让我们看看怎么做。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/7dac9e682b6d327fa1f5e459b5ef37e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Po5qTIX3N1ZJoJHd"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">向Docker引入AI加速器支持的通用模板。作者插图</p></figure><p id="3dae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人工智能加速器提供者可以使用预启动挂钩来扩展<code class="fe nq nr ns nt b">runC</code>的功能，以通过外部应用程序挂钩到容器的生命周期中。这个外部应用程序通常是一个库，它与硬件驱动程序对话，并在容器进程中公开硬件。上图中的蓝框显示了如何使用Docker和<code class="fe nq nr ns nt b">runC</code>特性向容器进程通告专门的硬件。让我们来看看在Docker容器中实现专用硬件支持的通用模板中的组件:</p><ul class=""><li id="c22c" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">自定义运行时:</strong> Docker允许你使用<code class="fe nq nr ns nt b">/etc/docker/daemon.json</code>定义一个自定义运行时。使用定制运行时，您可以截取从Docker接收的OCI规范，添加一个预启动钩子，并将更新后的规范转发给<code class="fe nq nr ns nt b">runC</code>。</li><li id="74d5" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">自定义钩子:</strong> <code class="fe nq nr ns nt b">runC</code>从自定义运行时接收的规范中读取预启动钩子，并执行自定义钩子。这个定制钩子调用一个或多个特定于硬件的库，这些库可以与硬件驱动程序对话，并在容器进程中公开它</li><li id="e235" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">custom-inject-hardware-library:</strong>这个库负责与硬件驱动程序通信，以收集关于硬件和系统中处理器数量的信息。它还负责将硬件设备安装到容器环境中，并使其可用于容器进程中的应用程序。</li></ul><p id="f963" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面，我们就来看看Docker是如何访问NVIDIA GPUs和AWS推理的。</p><h1 id="a736" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">启动NVIDIA GPU和AWS推理亚马逊EC2实例</h1><p id="e16e" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">首先，我们将启动Amazon EC2实例来演示Docker如何与专用硬件一起工作。遵循这个指南<a class="ae np" href="https://docs.aws.amazon.com/dlami/latest/devguide/launch-from-console.html" rel="noopener ugc nofollow" target="_blank">为机器学习</a>启动一个EC2实例。对于NVIDIA GPUs，选择任意大小的P或G系列实例类型。如果你需要帮助在AWS上选择正确的深度学习GPU，请阅读我的博客文章:</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">为AWS上的深度学习选择合适的GPU</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">如何选择正确的亚马逊EC2 GPU实例进行深度学习训练和推理——从最佳性能到最佳性能</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot kp of"/></div></div></a></div><p id="82f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于AWS推理实例，选择Inf1实例类型。在这个例子中，我启动了<code class="fe nq nr ns nt b">inf1.2xlarge</code> (AWS推理)和<code class="fe nq nr ns nt b">p3.8xlarge</code> (4个NVIDIA V100 GPUs)。</p><p id="e1e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦你选择了你的实例类型，你必须选择一个亚马逊机器映像(AMI)，并且<a class="ae np" href="https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html" rel="noopener ugc nofollow" target="_blank">我们将使用AWS深度学习AMI </a>，它预装了GPU驱动程序、AWS推理驱动程序、深度学习框架、conda和其他工具。选择一个具有您工作所需的深度学习框架的正确类型和版本的AMI，对于这个例子，我将选择Ubuntu 18.04的多框架DLAMI。</p><p id="aec0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">启动Amazon EC2实例后，您可以在EC2控制台上看到它们，并且可以通过ssh进入它们来运行您的应用程序。你也可以阅读我在<a class="ae np" href="https://aws.amazon.com/blogs/opensource/why-use-docker-containers-for-machine-learning-development/" rel="noopener ugc nofollow" target="_blank">上的博文为什么要使用Docker容器进行机器学习开发？</a>了解如何在EC2上运行的Docker容器中ssh和设置jupyter服务器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/9112ca86506095c3d4f94b1510d12c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qgnXxdeErPjgL_t0"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">亚马逊EC2控制台截图</p></figure><h1 id="ebd0" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">在Docker容器中访问NVIDIA GPUs</h1><p id="4510" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">NVIDIA提供了<a class="ae np" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html" rel="noopener ugc nofollow" target="_blank"> NVIDIA Container Toolkit </a>，这是一个工具和库的集合，增加了对Docker容器中GPU的支持。</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="299c" class="nc mg iq nt b gy ny nz l oa ob">docker run <strong class="nt ir">--runtime=nvidia --gpus=all</strong> … </span></pre><p id="89b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当您运行上述命令时，NVIDIA Container Toolkit会确保系统上的GPU在容器进程中是可访问的。由于我们推出了一个带有AWS深度学习AMI的Amazon EC2实例，您不需要安装NVIDIA Container Toolkit，因为它是预装的。如果您从基础AMI开始，请遵循<a class="ae np" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#supported-platforms" rel="noopener ugc nofollow" target="_blank"> NVIDIA Container Toolkit文档</a>中的安装说明。</p><h2 id="7b12" class="nc mg iq bd mh nd ne dn ml nf ng dp mp le nh ni mr li nj nk mt lm nl nm mv nn bi translated">NVIDIA容器工具包的工作原理</h2><p id="59f8" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">NVIDIA容器工具包包括<code class="fe nq nr ns nt b">nvidia-container-runtime</code>、<code class="fe nq nr ns nt b">nvidia-container-runtime-hook</code>和<code class="fe nq nr ns nt b">libnvidia-container</code>。让我们来看看这些是如何工作的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/602b14de7a86f369c9e6a6c99e839a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3guLP2CdcVboCnkn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><p id="4ec9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NVIDIA Container Toolkit通过在<code class="fe nq nr ns nt b">/etc/docker/daemon.json</code>中指定来注册自定义运行时。这个定制运行时是一个shim，它从Docker获取运行时规范，将其修改为预启动钩子规范，并将该规范转发给runC。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/2735de8f2675523f17f4dd488e9fb89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/0*x5KQOVzuauraghz2"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示Docker调用的自定义nvidia运行时的屏幕截图</p></figure><p id="2ad5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NVIDIA容器工具包还允许您使用基于文件的配置来配置容器运行时。你可以编辑<code class="fe nq nr ns nt b">/etc/nvidia-container-runtime/config.toml</code>文件来为你的驱动程序、运行时间、改变默认运行时间、启用调试等指定不同的位置。如果您没有自定义设置，可以保留默认值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/a07765c8e6163e82624667a920c4d04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3WHvkKxq4fbKX5wd"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示config.toml的屏幕截图</p></figure><p id="6857" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设您运行了以下命令</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="4862" class="nc mg iq nt b gy ny nz l oa ob">docker run --runtime=nvidia --gpus=2 …</span></pre><p id="b875" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Docker接受命令行参数，并将其翻译成符合OCI标准的运行时规范。<code class="fe nq nr ns nt b">nvidia-container-runtime</code>将预启动添加到该规范中，并将其传递给<code class="fe nq nr ns nt b">runC</code>。接下来，<code class="fe nq nr ns nt b">runC</code>在修改后的规范中看到预启动挂钩，并在启动容器进程之前调用<code class="fe nq nr ns nt b">nvidia-container-runtime-hook</code>。<code class="fe nq nr ns nt b">nvidia-container-runtime-hook</code>查看容器运行时配置，以确定您请求在容器进程中公开什么GPU或多少GPU。接下来，它调用<code class="fe nq nr ns nt b">libnvidia-container</code>库，该库与NVIDIA驱动程序对话，以确定所请求的GPU数量(在本例中我们说<code class="fe nq nr ns nt b">--gpus=2</code>)是否可用，如果可用，它将这些GPU设备注入容器。</p><p id="dc10" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了测试GPU访问，我将从亚马逊ECR 上亚马逊托管的深度学习容器<a class="ae np" href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md" rel="noopener ugc nofollow" target="_blank">列表中下拉一个Pytorch容器。按照该页面中的说明登录Amazon ECR并运行<code class="fe nq nr ns nt b">docker pull &lt;IMAGE_NAME&gt;</code>。然后运行下面的命令来启动容器。</a></p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="5658" class="nc mg iq nt b gy ny nz l oa ob">docker run -it --rm --runtime=nvidia — gpus=all &lt;IMAGE_NAME/ID&gt; nvidia-smi</span></pre><p id="b098" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从输出中我们可以看到，我可以运行<code class="fe nq nr ns nt b">nvidia-smi</code>这个工具，它查询并显示这个EC2实例上所有可用的4个GPU，并且可以从容器内访问。</p><p id="3305" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意:如果你使用的是最新版本的Docker和NVIDIA容器工具包，那么<code class="fe nq nr ns nt b">--runtime=nvidia</code>是可选的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/0ec2b6798838ce1d9a22f3729059225d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t3W6Zey-GC9yNd7G"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示在Docker容器内运行的nvidia-smi输出的屏幕截图，该容器显示所有GPU</p></figure><p id="cc12" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您想在容器中只暴露2个GPU，那么您可以简单地指定<code class="fe nq nr ns nt b">–-gpus=2</code>或枚举您想在容器中使用的两个:</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="9952" class="nc mg iq nt b gy ny nz l oa ob">docker run -it — rm — runtime=nvidia — gpus=2 &lt;IMAGE_NAME/ID&gt; nvidia-smi</span><span id="d80e" class="nc mg iq nt b gy ox nz l oa ob"># OR</span><span id="aeb1" class="nc mg iq nt b gy ox nz l oa ob">docker run -it — rm — runtime=nvidia — gpus=’”device=0,1"’ &lt;IMAGE_NAME/ID&gt; nvidia-smi</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/c4baf6692aeabc2ae8179346e7bc682d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Jgi2ack_bBAAswJU"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示nvidia-smi在Docker容器中运行的输出的屏幕截图，仅显示2个GPU</p></figure><h1 id="59cb" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">在Docker容器中访问AWS推理加速器</h1><p id="e25f" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">要准备带有AWS推理的Inf1 Amazon EC2实例，您可以<a class="ae np" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-deploy/tutorials/tutorial-docker-env-setup.html" rel="noopener ugc nofollow" target="_blank">按照AWS神经元文档中的步骤</a>安装神经元驱动程序、神经元运行时和一个名为<code class="fe nq nr ns nt b"><a class="ae np" href="https://github.com/awslabs/oci-add-hooks" rel="noopener ugc nofollow" target="_blank">oci-add-hooks</a></code>的帮助程序库，它有助于添加OCI神经元预启动挂钩。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/ec76d5221e80f0b2127a4fd51d49c593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E1k0-CPUWOtub7Is"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者插图</p></figure><h2 id="c3ee" class="nc mg iq bd mh nd ne dn ml nf ng dp mp le nh ni mr li nj nk mt lm nl nm mv nn bi translated">Docker如何支持AWS推理</h2><p id="1fa2" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">安装Neuron SDK并遵循Docker环境设置步骤后，您的系统应该有oci-neuron-runtime、oci-neuron-hook。让我们来看看它们是如何工作的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/821fc311cf2750ec31d75ef02c8cd80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/0*wBLFxr-rrYZ1t2wF"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示神经元运行时的屏幕截图</p></figure><p id="b864" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与NVIDIA容器工具包类似，Neuron SDK通过在<code class="fe nq nr ns nt b">/etc/docker/daemon.json</code>中指定来注册自定义运行时。从下面的截图可以看出，自定义Docker运行时只是一个shell脚本。回想一下，定制运行时的作用是简单地拦截对<code class="fe nq nr ns nt b">runC</code>的调用，修改它以包含一个预启动钩子规范，并用更新后的规范调用<code class="fe nq nr ns nt b">runC</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/78abfc7a0c60d2238c0690720dda3387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u884uf6f7-akGrjW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示神经元挂钩垫片的屏幕截图</p></figure><p id="57a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">咱们打开打开<code class="fe nq nr ns nt b">oci_neuron_hook_wrapper.sh</code>看看它在干什么:</p><ul class=""><li id="68bb" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">指定到<code class="fe nq nr ns nt b">oci_neuron_hook_config.json</code>位置的路径，这个JSON文件定义了预启动钩子应该调用什么库</li><li id="e5f4" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">获取<code class="fe nq nr ns nt b">runC</code>库的位置</li><li id="b166" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">使用<code class="fe nq nr ns nt b"><a class="ae np" href="https://github.com/awslabs/oci-add-hooks" rel="noopener ugc nofollow" target="_blank">oci-add-hooks</a></code>，这个工具可以从一个名为<code class="fe nq nr ns nt b">oci_neuron_hook_config.json</code>的文件中获取预启动钩子定义，并生成一个更新的运行时规范，并将其传递给<code class="fe nq nr ns nt b">runC</code></li></ul><p id="01e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以打开<code class="fe nq nr ns nt b">oci_neuron_hook_config.json</code>，你可以看到<code class="fe nq nr ns nt b">oci_neuron_hook</code>，它是在创建容器进程之前由runC调用的库。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/a8469784d35a0ff4ba63e8fc4b141ead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oxnld4Xy0v8I_kPK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示挂钩配置的屏幕截图</p></figure><p id="1a0e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe nq nr ns nt b">oci_neuron_hook</code>库与AWS推理驱动程序对话，以确定请求数量的AWS推理设备是否可用，如果可用，它将这些设备注入容器。要在Docker中测试AWS推理支持，下载下面的<a class="ae np" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-deploy/docker-example/Dockerfile-libmode.html#libmode-dockerfile" rel="noopener ugc nofollow" target="_blank"> Docker文件</a>并运行:</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="1b6b" class="nc mg iq nt b gy ny nz l oa ob">docker build . -f Dockerfile.app -t neuron-test</span><span id="5c63" class="nc mg iq nt b gy ox nz l oa ob">docker images</span></pre><p id="2691" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你应该会看到一个神经元测试图像</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d490d95eadd2872e251ea4c8a24ae344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/0*oGYqQhQHrVSQETnz"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示测试neuron的docker图像的屏幕截图</p></figure><p id="f3dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">启动集装箱</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="e9cf" class="nc mg iq nt b gy ny nz l oa ob">docker run — env — runtime=oci-neuron AWS_NEURON_VISIBLE_DEVICES=”0" neuron-test neuron-ls</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/b408b8b19d951c91a9ec90e96abdb691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PZsc0kErRO2iZ2vn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示在Docker容器中运行的neuron-ls的输出的屏幕截图</p></figure><p id="61d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以看到，neuron-ls的输出显示了Neuron设备0，它是在容器进程内部执行的。注意:<code class="fe nq nr ns nt b">–-runtime=oci-neuron</code>是可选的，因为neuron是默认的运行时，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/ca1ec651ce15c6497b1896199eabb126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/0*btpW0Xt2-kMFBQYJ"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">显示神经元运行时为默认运行时的屏幕截图</p></figure><h1 id="bef6" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">Kubernetes支持NVIDIA GPUs和AWS推理</h1><p id="ebcc" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">到目前为止，我们已经看到Docker如何在一个容器内提供对专门硬件(如NVIDIA GPUs和AWS推理)的访问，这允许更可移植的机器学习代码。下一步自然是弄清楚如何在像Kubernetes这样的编排系统上运行这些Docker容器，这样就可以运行大规模部署。这个主题值得在博客中详细讨论，但是为了完整起见，我将快速总结一下如何访问Kubernetes中的AI加速器资源。</p><p id="dfec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于NVIDIA和AWS推理，每个Kubernetes节点的主机操作系统都必须包括相关的硬件驱动程序、自定义容器运行时和我们之前讨论过的其他库，以便在Docker容器中实现专门的硬件支持。启动亚马逊EKS集群最简单的方法是使用<code class="fe nq nr ns nt b"><a class="ae np" href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html" rel="noopener ugc nofollow" target="_blank">eksctl</a></code> <a class="ae np" href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html" rel="noopener ugc nofollow" target="_blank"> CLI工具</a>。</p><ul class=""><li id="8e60" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">对于NVIDIA GPUs节点，指定一个Amazon深度学习AMI作为<code class="fe nq nr ns nt b">eksctl</code>中节点的AMI，因为它预装了NVIDIA Container Toolkit。</li><li id="0a74" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">对于AWS推理节点，<code class="fe nq nr ns nt b">eksctl</code>工具将自动检测到您有一个带有AWS推理的节点，并将使用预装了Neuron SDK、驱动程序和Docker支持库的Amazon Linux AMIs。</li></ul><p id="604f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，为了使这些AI加速器作为系统资源在Kubernetes中可用，你需要部署一个特定于硬件的<a class="ae np" href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/" rel="noopener ugc nofollow" target="_blank"> Kubernetes设备插件</a>。NVIDIA和AWS都提供设备插件，您可以按如下方式应用:</p><h2 id="ee3b" class="nc mg iq bd mh nd ne dn ml nf ng dp mp le nh ni mr li nj nk mt lm nl nm mv nn bi translated">AWS推理:</h2><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="0fa1" class="nc mg iq nt b gy ny nz l oa ob">kubectl apply -f k8s-neuron-device-plugin.yml</span></pre><p id="b751" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您可以从Neuron SDK文档中<a class="ae np" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-deploy/tutorials/tutorial-k8s.html#tutorial-k8s-env-setup-for-neuron" rel="noopener ugc nofollow" target="_blank">下载</a>这个设备插件，它作为一个<code class="fe nq nr ns nt b">daemonset</code>运行，并在pods中公布要包含的硬件，您可以在清单中的资源下指定AWS推理硬件，如下所示:</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="5e3b" class="nc mg iq nt b gy ny nz l oa ob">resources:<br/>    limits:<br/>        aws.amazon.com/neuron: 1</span></pre><h2 id="6889" class="nc mg iq bd mh nd ne dn ml nf ng dp mp le nh ni mr li nj nk mt lm nl nm mv nn bi translated">NVIDIA GPUs:</h2><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="8496" class="nc mg iq nt b gy ny nz l oa ob">kubectl apply -f <a class="ae np" href="https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml</a></span></pre><p id="5fe9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该NVIDIA设备插件也作为<code class="fe nq nr ns nt b">daemonset</code>运行，并向pods通告硬件，您可以在清单中的资源下指定NVIDIA GPU硬件，如下所示:</p><pre class="kg kh ki kj gt nu nt nv nw aw nx bi"><span id="ce16" class="nc mg iq nt b gy ny nz l oa ob">resources:<br/>    limits:<br/>        nvidia.com/gpu: 1</span></pre><p id="8db5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以在亚马逊EKS文档页面上找到更多关于如何开始使用<a class="ae np" href="https://docs.aws.amazon.com/eks/latest/userguide/inferentia-support.html" rel="noopener ugc nofollow" target="_blank"> AWS推理</a>和<a class="ae np" href="https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-eks-setup.html#deep-learning-containers-eks-setup-gpu-clusters" rel="noopener ugc nofollow" target="_blank"> NVIDIA GPU </a>的详细说明。</p><h1 id="a655" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">闭幕(对，还有视频！)</h1><p id="5bf6" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">希望您喜欢了解Docker如何在专用硬件上运行以及如何使用它。如果你喜欢这篇博文，我最近做了一个关于相同主题的演讲，你可能会感兴趣。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">关于Docker如何在人工智能加速器上运行机器学习的YouTube视频</p></figure><h1 id="66e0" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">谢谢你看完！</h1><p id="4a0a" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">如果您觉得这篇文章很有趣，可以考虑在medium上关注我，以便在我发布新文章时得到通知。也请查看我在<a class="ae np" href="https://medium.com/@shashankprasanna" rel="noopener"> medium </a>上的其他博客帖子，或者在twitter ( <a class="ae np" href="https://twitter.com/shshnkp" rel="noopener ugc nofollow" target="_blank"> @shshnkp </a>)、<a class="ae np" href="https://www.linkedin.com/in/shashankprasanna/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，或者在下面留下评论。想让我写一个特定的机器学习主题吗？我很想收到你的来信！</p></div></div>    
</body>
</html>