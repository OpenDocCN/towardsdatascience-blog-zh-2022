<html>
<head>
<title>Non-Linear Regression with Decision Trees and Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林的非线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/non-linear-regression-with-decision-trees-and-random-forest-afae406df27d#2022-02-03">https://towardsdatascience.com/non-linear-regression-with-decision-trees-and-random-forest-afae406df27d#2022-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7572" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">UCL 数据科学学会研讨会 12b:决策树和随机森林在 Python 中的实现及性能评估</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3260a9653fde260440de715f0824deb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KMENomMxYnmXNV_l"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰里米·毕晓普在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a04e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今年，作为 UCL 数据科学协会的科学负责人，该协会将在整个学年举办一系列 20 场研讨会，主题包括数据科学家工具包 Python 简介和机器学习方法。每个人的目标是创建一系列的小博客文章，这些文章将概述主要观点，并为任何希望跟进的人提供完整研讨会的链接。所有这些都可以在我们的<a class="ae ky" href="https://github.com/UCL-DSS" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中找到，并将在全年更新新的研讨会和挑战。</p><p id="bc7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第十二次研讨会是对高级回归方法的介绍。这之前已经介绍了 Lasso 和 Ridge 回归，但是我们现在将介绍决策树和随机森林回归方法的实现。虽然亮点将在这篇博文中呈现，但完整的研讨会可以在我们的 GitHub 账户<a class="ae ky" href="https://github.com/UCL-DSS/advanced-regression" rel="noopener ugc nofollow" target="_blank">上找到。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e18a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您错过了之前的任何研讨会，可以在这里找到:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-lasso-and-ridge-regression-using-scitkit-learn-d3427700679c"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">使用 scitkit-learn 介绍套索和岭回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会 12a 研讨会:偏差-方差权衡，套索实施，山脊实施，及其…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/introduction-to-logistic-regression-predicting-diabetes-bc3a88f1e60e"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">逻辑回归简介:预测糖尿病</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会研讨会 11:什么是逻辑回归、数据探索、实施和评估</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/linear-regression-in-python-for-data-scientists-16caef003012"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的 Python 线性回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会研讨会 10:什么是线性回归，数据探索，Scikit 学习实施和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ed36" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">回归树</h2><p id="11d9" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">决策树遵循树状结构(因此得名),其中节点代表特定属性，分支代表决策规则，主节点代表结果。这可以想象成类似于我们在高中数学中画的决策树，只是规模要复杂得多。该算法本身的工作原理是根据每个节点的不同属性分割数据，同时试图减少选择度量(通常是基尼指数)。本质上，使用回归树的目的是根据属性分割数据，然后使用这些属性来预测我们的目标变量结果。</p><p id="8cd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用与研讨会第一部分(建筑能效数据)相同的数据集，我们可以使用 scikit-learn 库实现它们，就像我们对 Ridge and Lasso regression 所做的那样。使用回归树的主要好处是，这些方法能够发现因变量和自变量之间的非线性关系，而以前的方法侧重于线性关系。当涉及到变量之间更复杂的关系时，这通常是有益的，但有时会以过度拟合以及时间和资源为代价！</p><p id="d54d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以像以前一样使用以下符号来拟合模型和数据:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="7f63" class="mw mx it nv b gy nz oa l ob oc">#import the necessary module<br/>from sklearn.tree import DecisionTreeRegressor</span><span id="c1cb" class="mw mx it nv b gy od oa l ob oc">#create the regression tree<br/>reg_tree = DecisionTreeRegressor(random_state=42)<br/>#fit the regression tree<br/>reg_tree.fit(X_train, y_train)</span></pre><p id="19c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的一个主要区别是，我们不再有每个参数的系数，因为这不再是一个线性模型。这意味着模型可能被描述为一个黑盒(取决于你允许决策树增长到多深)，因为我们不一定知道它是如何或者为什么以这种方式工作的。在这种情况下尤其如此，因为我们已经通过不限制它可以创建的深度而给了模型有效的自由支配。这意味着我们可以期望模型能够预测训练集中的所有值:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="52e5" class="mw mx it nv b gy nz oa l ob oc">reg_tree.score(X= X_train, y = y_train)</span><span id="2aec" class="mw mx it nv b gy od oa l ob oc">#out:<br/>1.0</span></pre><p id="2ab6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到这种情况。因此，这可能是训练数据过度拟合的一个例子，由此我们的模型可能在看不见的数据上具有高方差。我们可以这样检查:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c4b9" class="mw mx it nv b gy nz oa l ob oc">tree_predictions = reg_tree.predict(X_test)</span><span id="53bd" class="mw mx it nv b gy od oa l ob oc">#R2 score<br/>reg_tree.score(X=X_test, y=y_test)</span><span id="c9ad" class="mw mx it nv b gy od oa l ob oc">#out:<br/>0.9965565405329665</span></pre><p id="f542" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下值得注意的是，模型对未知数据的预测能力仍然存在，这表明在定型数据中看到的关系在未知数据中也非常明显。</p><p id="0d8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实和许多其他数据集中，这是非常不可能的情况，因此我们很可能有极端过度拟合。限制这种情况的一种方法是控制决策树可以增长的程度，在这种情况下，可以使用<code class="fe oe of og nv b">max_depth</code>参数来实现。我们可以这样做:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="0c09" class="mw mx it nv b gy nz oa l ob oc">#create the regression tree<br/>reg_tree_limit= DecisionTreeRegressor(random_state=42,<br/>                                     max_depth = 5)<br/>#fit the regression tree<br/>reg_tree_limit.fit(X_train, y_train)</span><span id="7ae8" class="mw mx it nv b gy od oa l ob oc">print(reg_tree_limit.score(X= X_train, y = y_train))</span><span id="3665" class="mw mx it nv b gy od oa l ob oc">#out:<br/>0.9911552954578272</span></pre><p id="db33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型拟合在训练数据上有所降低，但我们主要感兴趣的是它在看不见的数据上的表现:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="7c6a" class="mw mx it nv b gy nz oa l ob oc">print(reg_tree_limit.score(X_test, y_test))</span><span id="a8ca" class="mw mx it nv b gy od oa l ob oc">#out:<br/>0.9882274019933065</span></pre><p id="e9b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明约束模型实际上在看不见的数据上表现更差。</p><p id="fa6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以在更多看不见的数据上进一步探索，看看这是否在看不见的数据上的模型可变性方面成立。然而，虽然这是一个相当糟糕的示例，但您可以在这里看到，限制模型的深度已经限制了训练数据的得分(引入了一些偏差)，希望减少模型在看不见的数据上的潜在方差。另一个好处是，它还会限制模型实现所需的时间和资源。</p><p id="eb38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可用于训练模型以提高其性能、降低其复杂性或提高其通用性的其他参数包括:</p><ul class=""><li id="c6f2" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated">标准:衡量分割质量的标准</li><li id="c510" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">Splitter:在每个节点进行拆分的策略</li><li id="091f" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">Max_depth:树的最大深度</li><li id="6084" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">min_samples_split:拆分内部节点所需的最小样本数</li><li id="f425" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">min_samples_left:叶节点上所需的最小样本数</li><li id="7fbe" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">Max_Features:寻找最佳分割时要考虑的特征数量</li></ul><p id="068b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些参数的使用将取决于您的建模目标以及您的应用程序，即您希望将模型限制到什么程度，以及您有多少时间和能力来调整这些参数以提高模型拟合度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/c9cf6efa8712cb9d46a0d934bca5e80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XmgBMSu12PiN7Eot"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">吉利·斯图尔特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="d4bc" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">随机森林回归</h2><p id="5c32" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">随机森林回归可能是实现回归树的更好方法，前提是您有资源和时间来运行它。这是因为它是一种集成方法，这意味着它结合了多种不同算法(在这种情况下是决策树)的结果，以创建更准确的预测，并确保没有过度拟合。</p><p id="db8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其工作方式是限制输入到每个决策树的数据量和变量，这样他们就看不到全部数据。在我们的案例中，考虑到偏差和过度拟合，这有助于确保模型不会过度拟合数据，并且可以在模型中探索每个变量的重要性。当然，过度拟合仍然会发生，但是它试图减少这种可能性，就像单个决策树一样。</p><p id="c97b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，这可以如下实现:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="da78" class="mw mx it nv b gy nz oa l ob oc">#import the necessary library <br/>from sklearn.ensemble import RandomForestRegressor</span><span id="1e88" class="mw mx it nv b gy od oa l ob oc">#create the random forest<br/>reg_random_forest = RandomForestRegressor(n_estimators = 10,<br/>                                          random_state=0)<br/>#fit the regression to the data<br/>reg_random_forest.fit(X_train, y_train.values.reshape(-1))</span></pre><p id="3852" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在我们检查模型在训练和测试数据上的性能之前:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="19eb" class="mw mx it nv b gy nz oa l ob oc">print(reg_random_forest.score(X= X_train, y = y_train))</span><span id="82a1" class="mw mx it nv b gy od oa l ob oc">print(reg_random_forest.score(X=X_test, y=y_test))</span><span id="e753" class="mw mx it nv b gy od oa l ob oc">#out:<br/>0.999637869613283<br/>0.9976901385082313</span></pre><p id="4ef9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里可以看到，尽管模型在训练数据上的表现比决策树差，但它在测试数据上的表现实际上优于决策树。这是因为它没有完全学习原始数据(尽管它很接近)，这意味着它仍然能够很好地概括。</p><p id="0d45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型的另一个好处是，至少与决策树回归模型相比，它被描述为黑盒的可能性稍微小一些。这是因为它能够告诉我们模型中功能的重要性(它们对模型预测的贡献有多大)，虽然决策树可以做到这一点，但随机森林更可靠，因为创建了大量的树(较少基于随机机会)。这些值加起来等于 1，最大的特征告诉我们模型中最重要的变量。我们可以这样实现:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a128" class="mw mx it nv b gy nz oa l ob oc">#extract the feature importance<br/>feature_importances = pd.DataFrame(<br/>    {"variables":X_train.columns.values,<br/>     "Importance":reg_random_forest.feature_importances_*100})</span><span id="f760" class="mw mx it nv b gy od oa l ob oc">#sort the values by size<br/>feature_importances.sort_values(by = "Importance",<br/>                               inplace = True,<br/>                               ascending=False)</span><span id="5377" class="mw mx it nv b gy od oa l ob oc">#plot the results<br/>fig, ax = plt.subplots(1,1, figsize = (8,6))</span><span id="e293" class="mw mx it nv b gy od oa l ob oc">feature_importances.plot.bar(x = "variables",<br/>                           y = "Importance", <br/>                             ax = ax)</span><span id="8006" class="mw mx it nv b gy od oa l ob oc">ax.set_title("Random Forest Variable Importance",<br/>            fontsize = 20,<br/>            pad = 20)<br/>ax.set_ylabel("Percentage (%)",<br/>             fontsize = 20,<br/>             labelpad = 15)<br/>ax.set_xlabel("Variable",<br/>             fontsize = 20,<br/>             labelpad = 15)</span><span id="8fc3" class="mw mx it nv b gy od oa l ob oc">ax.tick_params(axis = "both",<br/>              which = "both",<br/>              labelsize = 15)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8938ba8bacd97a683813f90e62c82a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BPAuQ0CRz3l7OXyBiiaTrw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="156f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里要考虑的一个重要因素是，尽管线性回归方法表明 X2 不重要，但这个模型表明 X2 非常重要。因此，我们可以假设，这是因为 X2 与我们的目标变量 Y1 具有非线性关系，这是标准线性回归方法所不能捕捉到的。</p><p id="a8d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用这一点，我们可以执行特征选择来删除不重要的变量。为此，有多种不同的方法，但两种常见的方法包括:</p><ul class=""><li id="b606" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated">去除贡献小于 1%的变量</li><li id="5e95" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">移除贡献小于随机变量的变量</li></ul><p id="142e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，与决策树模型一样，我们可以调整参数以提高训练和测试数据的模型性能，这些参数包括:</p><ul class=""><li id="6307" class="oh oi it lb b lc ld lf lg li oj lm ok lq ol lu om on oo op bi translated">森林中的树的数量</li><li id="678e" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated"><code class="fe oe of og nv b">criterion</code>:测量分割质量的功能</li><li id="e223" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated"><code class="fe oe of og nv b">max_depth</code>:单棵树的最大深度(和我们之前做的一样)</li><li id="4f27" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated"><code class="fe oe of og nv b">min_samples_split</code>:分割内部节点所需的最小样本数</li></ul><p id="e8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在实践练习册中找到更多信息，可以在<a class="ae ky" href="https://github.com/UCL-DSS/advanced-regression" rel="noopener ugc nofollow" target="_blank">这里</a>找到，如果你愿意，你也可以在研讨会旁边提供的问题练习册中挑战自己。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/d8a157b147949a8e31d3fae06da6c5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S2_7EJToNIRuYu4s"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@iriser?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Irina Iriser </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="133b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想了解我们协会的更多信息，请随时关注我们的社交网站:</p><p id="a976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.facebook.com/ucldata 脸书<a class="ae ky" href="https://www.facebook.com/ucldata" rel="noopener ugc nofollow" target="_blank"/></p><p id="a7be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">insta gram:https://www.instagram.com/ucl.datasci/<a class="ae ky" href="https://www.instagram.com/ucl.datasci/" rel="noopener ugc nofollow" target="_blank"/></p><p id="6dea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">领英:<a class="ae ky" href="https://www.linkedin.com/company/ucldata/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/company/ucldata/</a></p><p id="1f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解 UCL 数据科学协会和其他优秀作者的最新信息，请随时使用我下面的推荐代码注册 medium:</p><div class="mc md gp gr me mf"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mo l"><div class="oy l mq mr ms mo mt ks mf"/></div></div></a></div><p id="5f04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者由我来查看其他商店:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/git-and-github-basics-for-data-scientists-b9fd96f8a02a"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的 Git 和 GitHub 基础知识</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学研讨会 8:什么是 Git，创建本地存储库，提交第一批文件，链接到远程…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="oz l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-object-oriented-programming-for-data-scientists-879106d90d89"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">面向对象的基础知识，适合那些以前可能没有接触过这个概念或者想知道更多的人</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pa l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/univariate-outlier-detection-in-python-40b621295bc5"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python 中的单变量异常检测</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">从数据集中检测异常值的五种方法</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pb l mq mr ms mo mt ks mf"/></div></div></a></div></div></div>    
</body>
</html>