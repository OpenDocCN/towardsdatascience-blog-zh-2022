<html>
<head>
<title>Double Debiased Machine Learning (part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双重去偏机器学习(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/double-debiased-machine-learning-part-2-bf990720a0b2#2022-06-06">https://towardsdatascience.com/double-debiased-machine-learning-part-2-bf990720a0b2#2022-06-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6a78" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/causal-data-science" rel="noopener" target="_blank">因果数据科学</a></h2><div class=""/><div class=""><h2 id="2c9b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">如何使用后双重选择消除正则化偏差</em></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/1131655959d88fa0e77f1c4d8f509676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QroSa044H_xqgqGJd8XP3w.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="4e28" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在因果推断中，我们通常对治疗变量对特定结果的影响感兴趣。在随机对照试验或AB试验中，以一组其他变量——<strong class="lh ja">控制变量</strong>或协变量为条件进行分析，可以减少随机化后出现的不平衡，从而提高分析的功效。然而，条件作用在观察性研究中更为重要，在观察性研究中，如果没有随机化，可能<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">对恢复因果关系</a>至关重要。</p><p id="914d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，我们对分析的条件变量或函数形式没有强烈的意见，我们可能倾向于<strong class="lh ja">让数据决定</strong>，要么通过变量选择，要么通过灵活的机器学习方法。在这篇博文的<a class="ae mb" rel="noopener" target="_blank" href="/eb767a59975b">的前一部分，我们已经看到了这种方法是如何扭曲推论的，例如，对感兴趣的因果效应产生错误的置信区间。这种偏差一般称为<strong class="lh ja">正则化偏差</strong>或预测试偏差。</a></p><p id="9c1a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇博客文章中，我们将探索变量选择问题的解决方案，<strong class="lh ja">后双重选择</strong>，并且我们将介绍一种处理许多控制变量和/或非线性函数形式的通用方法，<strong class="lh ja">双重去偏置机器学习</strong>。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="0bdd" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">概述</h1><p id="ed21" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">为了更好地理解偏见的<strong class="lh ja">来源</strong>，在这篇文章的第一部分，我们探讨了一家公司对测试广告活动的有效性感兴趣的例子。该公司有关于其当前广告支出和销售水平的信息。问题的出现是因为公司不确定是否应该以过去的销售水平作为分析的条件。</p><p id="c810" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面的<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c"> <strong class="lh ja">有向无环图</strong> </a>总结了数据生成过程。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ng"><img src="../Images/9a76a036f877d7c734d22e44d1f57644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T39foqcyQXVeVnPTiSezZw.png"/></div></div></figure><p id="1e0c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我从<code class="fe nh ni nj nk b"><a class="ae mb" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/dgp.py" rel="noopener ugc nofollow" target="_blank">src.dgp</a></code>导入数据生成过程<code class="fe nh ni nj nk b">dgp_pretest()</code>，从<code class="fe nh ni nj nk b"><a class="ae mb" href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/src/utils.py" rel="noopener ugc nofollow" target="_blank">src.utils</a></code>导入一些绘图函数和库。</p><pre class="kq kr ks kt gt nl nk nm nn aw no bi"><span id="aab9" class="np mk iq nk b gy nq nr l ns nt">from src.utils import *<br/>from src.dgp import dgp_pretest<br/><br/>df = dgp_pretest().generate_data()<br/>df.head()</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/b56324d0e5f9923b528ebfb619b4d3a7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ETxZXCx2SvOT3Bie-db2Hg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="dfdd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们有1000个不同市场的数据，我们观察当前的<code class="fe nh ni nj nk b">sales</code>，在<code class="fe nh ni nj nk b">advertisement</code>和<code class="fe nh ni nj nk b">past sales</code>花费的金额。</p><p id="b9ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们想了解<code class="fe nh ni nj nk b">ads</code>支出是否能有效增加<code class="fe nh ni nj nk b">sales</code>。一种可能性是使用以下回归模型，也称为<strong class="lh ja">短模型</strong>，在前者的基础上回归后者。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nv"><img src="../Images/e39e414c81ddf18c1772db48d835dee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_aMSbPUw9dfWmZsyGoQvQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">短模型，作者图片</p></figure><p id="2346" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们是否应该将<code class="fe nh ni nj nk b">past sales</code>也包括在回归中？那么，回归模型将如下，也称为<strong class="lh ja">长模型</strong>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nw"><img src="../Images/ec386513c055ec959d822f03972187aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8EDW5NEY8AoLzqjdLhnTA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">长模型，作者图片</p></figure><p id="e03d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一种简单的方法是<strong class="lh ja">让数据决定</strong>:我们可以运行第二次回归，如果<code class="fe nh ni nj nk b">past sales</code>、<em class="nx"> β̂ </em>的估计效果在统计上是显著的，我们就可以使用长模型，否则我们运行短模型。该程序称为<strong class="lh ja">预测试</strong>。</p><p id="079e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个程序的问题是它引入了一个偏差，称为<strong class="lh ja">正则化或预测试偏差</strong>。预测试确保该偏差足够小，不会扭曲估计的系数。但是，它不能确保它足够小，不会扭曲估计系数周围的置信区间。</p><p id="2f7c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有解决办法吗？是啊！</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="8cbe" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">双重选择后</h1><p id="a0e1" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">解决方案叫做<strong class="lh ja">后双选</strong>。该方法首先由<a class="ae mb" href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" rel="noopener ugc nofollow" target="_blank"> Belloni，Chernozhukov，Hansen (2014) </a>提出，后来在以<a class="ae mb" href="http://www.mit.edu/~vchern/" rel="noopener ugc nofollow" target="_blank"> Victor Chernozhukov </a>为共同标准的各种论文中得到扩展。</p><p id="b52e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者假设以下<strong class="lh ja">数据生成过程</strong>:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ny"><img src="../Images/442245343a26466c1b0fe0b4cef00906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnX9TwQ43ZMQ01JIEhlF5Q.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据生成过程，按作者分类的图像</p></figure><p id="f222" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的例子中，<em class="nx"> Y </em>对应于<code class="fe nh ni nj nk b">sales</code>，<em class="nx"> D </em>对应于<code class="fe nh ni nj nk b">ads</code>，<em class="nx"> X </em>对应于<code class="fe nh ni nj nk b">past_sales</code>，利益效应为<em class="nx"> α </em>。在我们的例子中，为了简单起见，<em class="nx"> X </em>是一维的，但是一般来说，我们对X是高维的情况感兴趣，甚至有可能比观察的数量更多的维度。在这种情况下，变量选择在线性回归中非常重要，因为我们不能拥有比变量更多的特征(OLS系数不是唯一确定的)。</p><p id="88ab" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">双重选择后由以下<strong class="lh ja">程序</strong>组成。</p><ol class=""><li id="0246" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated"><strong class="lh ja">缩小形式</strong>选择:套索<em class="nx"> Y </em>上<em class="nx"> X </em>。选择集合<em class="nx">中的统计显著变量S₁ ⊆ X </em></li><li id="e5cb" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated"><strong class="lh ja">第一阶段</strong>选择:在<em class="nx"> X </em>上后退<em class="nx"> D </em>。选择集合<em class="nx">中的统计显著变量S₂ ⊆ X </em></li><li id="1f23" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">回归前两步中所选变量的<em class="nx"> D </em>上的<em class="nx"> Y </em>和<strong class="lh ja">并集</strong>，<em class="nx">s₁</em>∩<em class="nx">s₂</em></li></ol><p id="701c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者表明，该程序给出了估计的感兴趣系数<em class="nx"> α̂ </em>的置信区间，其具有正确的覆盖范围，即1型错误的正确概率。</p><p id="e3e4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注(1) </strong>:就变量选择而言，该程序总是比预测试更少节约。事实上，我们仍然选择所有我们在预测试中选择的变量，但是，在第一阶段，我们可能会选择额外的变量。</p><p id="7c6d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注(2) </strong>:术语<em class="nx">第一阶段</em>和<em class="nx">约化形式</em>来自计量经济学中的<a class="ae mb" href="https://en.wikipedia.org/wiki/Instrumental_variables_estimation" rel="noopener ugc nofollow" target="_blank">工具变量</a>文献。的确，后双选择的第一个应用是在<a class="ae mb" href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" rel="noopener ugc nofollow" target="_blank"> Belloni，Chen，Chernozhukov，Hansen (2012) </a>中选择工具变量。</p><p id="5238" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注(3) </strong>:双重选择后这个名字来源于这样一个事实，现在我们不是执行一次变量选择，而是执行两次<em class="nx"/>。</p><h2 id="d004" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">直觉</h2><p id="48f7" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">后置双选背后的思路是:绑定<a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699"> <strong class="lh ja">省略变量bias </strong> </a>。以防你不熟悉，我写了一篇关于省略变量bias 的单独的<a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699">博文。</a></p><p id="fd9b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的设置中，我们可以将省略的变量bias表示为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ox"><img src="../Images/c77588842a5f2229ed01687236a59c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHQGRO_dyPjWnjb2boa1Lw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">省略可变偏差，图片由作者提供</p></figure><p id="bd40" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以看到，省略变量bias来自于<strong class="lh ja">与省略变量<em class="nx"> X </em>相关的两个量</strong>的乘积:</p><ol class=""><li id="f5b3" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated">其与结果的偏相关<em class="nx"> Y </em>，<em class="nx"> β </em></li><li id="7965" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">其与治疗的偏相关<em class="nx"> D </em>，<em class="nx"> δ </em></li></ol><p id="dd8e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过预测试，我们确保<em class="nx"> X </em>和结果<em class="nx"> Y </em>、<em class="nx"> β </em>之间的偏相关为<strong class="lh ja">小</strong>。事实上，我们很少忽略不应该忽略的<em class="nx"> X </em>(例如，我们犯了类型2错误)。<em class="nx">小</em>和<em class="nx">很少</em>是什么意思？</p><p id="8564" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我们因为变量的重要性而选择变量时，我们确保对于某个数字<em class="nx"> c </em>，它的维数小于<em class="nx"> c/√n </em>，其中<em class="nx"> n </em>是样本大小。因此，通过预测试，我们确保无论<em class="nx"> δ </em>的值是多少，偏置的大小都小于<em class="nx"> c/√n </em>。这意味着对于足够大的<em class="nx"> n </em>，它收敛到零。这就是为什么预测试估计量仍然<strong class="lh ja">一致</strong>，即对于足够大的样本量<em class="nx"> n </em>收敛到真实值。</p><p id="d340" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，为了让我们的置信区间有正确的覆盖范围，这是不够的。在实践中，我们需要偏置比<em class="nx"> 1/√n </em>更快地<strong class="lh ja">收敛到零。为什么？</strong></p><p id="4351" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了得到这个结果的一个<strong class="lh ja">直觉</strong>，我们需要求助于<a class="ae mb" href="https://en.wikipedia.org/wiki/Central_limit_theorem" rel="noopener ugc nofollow" target="_blank">中心极限定理</a>。CLT告诉我们，对于大样本量<em class="nx"> n </em>，随机变量<em class="nx"> X </em>的样本均值分布收敛于均值<em class="nx"> μ </em>和标准差<em class="nx"> σ/√n </em>的正态分布，其中<em class="nx"> μ </em>和<em class="nx"> σ </em>是<em class="nx"> X </em>的均值和标准差。为了进行推断，我们通常将中心极限定理应用于我们的估计量，以获得其渐近分布，这反过来允许我们建立置信区间(使用均值和标准差)。因此，如果偏差没有明显小于估计量的标准差，置信区间就是错误的。因此，我们需要偏差比标准偏差更快地收敛到零<strong class="lh ja">，即比<em class="nx"> 1/√n </em>更快。</strong></p><p id="e189" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的设置中，省略的变量bias是<em class="nx"> βδ </em>，我们希望它比<em class="nx"> 1/√n </em>更快地收敛到零。后双重选择保证了</p><ul class=""><li id="5d82" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oy of og oh bi translated"><em class="nx">简化形式</em>选择:任何“缺失”变量<em class="nx"> j </em>有<em class="nx"> |βⱼ| ≤ c/√n </em></li><li id="5cca" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated"><em class="nx">第一阶段</em>选择:任何“缺失”变量<em class="nx"> j </em>有<em class="nx"> |δⱼ| ≤ c/√n </em></li></ul><p id="2cda" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，只要省略变量的数量是有限的，省略变量bias将以比<em class="nx"> 1/√n </em>更快的速度<em class="nx"> 1/n </em>收敛到零。<strong class="lh ja">问题解决</strong>！</p><h2 id="ecce" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">应用</h2><p id="a2d6" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">现在让我们回到我们的例子，测试后双选择程序。在实践中，我们希望做到以下几点:</p><ol class=""><li id="3d94" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated"><strong class="lh ja">第一阶段</strong>选择:在<code class="fe nh ni nj nk b">past_sales</code>上回归<code class="fe nh ni nj nk b">ads</code>。检查<code class="fe nh ni nj nk b">past_sales</code>是否有统计学意义</li><li id="9684" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated"><strong class="lh ja">缩小形式</strong>选择:在<code class="fe nh ni nj nk b">past_sales</code>上回归<code class="fe nh ni nj nk b">sales</code>。检查<code class="fe nh ni nj nk b">past_sales</code>是否具有统计显著性</li><li id="78a7" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">在<code class="fe nh ni nj nk b">ads</code>上回归<code class="fe nh ni nj nk b">sales</code>，并且仅当在<em class="nx">或</em>前两个回归中的一个中有意义时，才包括<code class="fe nh ni nj nk b">past_sales</code>和<strong class="lh ja"/></li></ol><p id="3bcd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我从文章的第一部分更新了<code class="fe nh ni nj nk b">pre_test</code>函数来计算双选择后的估计量。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="db55" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在可以绘制估计系数的分布图(通过模拟)。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="oz pa l"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/39ba38b86888dc49628a6266a040f74c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*B_CaM9aQuvTFVnLrg0aC5w.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> α̂的分布，图片由作者提供</em></p></figure><p id="934a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我们所看到的，双重选择后的过程总是正确地选择长回归，因此估计量具有正确的分布。</p><h2 id="5d2a" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">双重检查</h2><p id="3de3" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">在上一篇文章中，我们运行了一些模拟来研究预测试偏差何时出现。我们看到预测试是一个问题</p><ul class=""><li id="e1c5" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oy of og oh bi translated">小样本量<em class="nx"> n </em></li><li id="de78" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated"><em class="nx"> β </em>的中间值</li><li id="57d4" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated">当<em class="nx"> β </em>的值取决于样本量时</li></ul><p id="66a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们检查后双重选择是否消除了<strong class="lh ja">所有</strong>先前案例中的正则化偏差。</p><p id="92a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，让我们模拟不同样本量的后双选择估计量<em class="nx"> α̂-postdouble </em>的分布。</p><pre class="kq kr ks kt gt nl nk nm nn aw no bi"><span id="7611" class="np mk iq nk b gy nq nr l ns nt">Ns = [100,300,1000,3000]<br/>alphas = {f'N = {n:.0f}':  pre_test(N=n) for n in Ns}</span></pre><p id="45de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在可以画出并比较验前估计量和验后加倍估计量的分布。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="oz pa l"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/308e8c9d74edf70279b022cb561a47dc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vZMgAW9CzXEfBT0TKuXBjg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> α̂的分布，图片作者</em></p></figure><p id="1357" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于小样本，预测试估计量的分布不是正态分布，而是双峰分布。相反，双精度后估计量在小样本情况下也是高斯型的。</p><p id="23ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们重复同样的练习，但是对于<em class="nx"> β </em>的不同值，即<code class="fe nh ni nj nk b">past_sales</code>对<code class="fe nh ni nj nk b">sales</code>的系数。</p><pre class="kq kr ks kt gt nl nk nm nn aw no bi"><span id="d8a2" class="np mk iq nk b gy nq nr l ns nt">betas = 0.3 * np.array([0.1,0.3,1,3])<br/>alphas = {rf'$\beta$ = {b:.2f}': pre_test(b=b) for b in betas}<br/>compare_alphas(alphas, true_alpha=1)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/af3d1339ea7de6179d1d525668b1fd59.png" data-original-src="https://miro.medium.com/v2/format:webp/1*qaYb__nh4-1Np_zVXldKeA.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> α̂的分布，图片作者</em></p></figure><p id="5b1c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，无论<em class="nx"> β </em>的值如何，双选择后估计器都具有高斯分布，而预测试估计器则存在正则化偏差。</p><p id="2625" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于最后的模拟，我们同时改变系数和样本大小。</p><pre class="kq kr ks kt gt nl nk nm nn aw no bi"><span id="e678" class="np mk iq nk b gy nq nr l ns nt">betas = 0.3 * 30 / np.sqrt(Ns)<br/>alphas = {rf'N = {n:.0f}, $\beta$ = {b:.2f}':  pre_test(b=b, N=n) for n,b in zip(Ns,betas)}<br/>compare_alphas(alphas, true_alpha=1)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="ab gu cl nu"><img src="../Images/c28663e56acdb99e08fccb2532ca0b9c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*M3niA8auuFncEWOwtIaEMg.png"/></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> α̂的分布，图片作者</em></p></figure><p id="a325" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，在最后一种情况下，后双精度选择在模拟中具有正确的高斯分布。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="f393" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">双重去偏机器学习</h1><p id="1ac4" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">到目前为止，我们只分析了一个线性的单变量例子。如果<em class="nx"> X </em>的维数增加，我们不知道<em class="nx"> X </em>通过什么函数形式影响<em class="nx"> Y </em>和<em class="nx"> D </em>会发生什么？在这些情况下，我们可以使用机器学习算法来揭示这些高维非线性关系。</p><p id="2e34" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae mb" href="https://academic.oup.com/ectj/article/21/1/C1/5056401" rel="noopener ugc nofollow" target="_blank"> Chernozhukov、Chetverikov、Demirer、Duflo、Hansen、Newey和Robins (2018) </a>调查这一设置。特别地，作者考虑了下面的部分线性模型。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ng"><img src="../Images/e7b7eac9f2721456420595c091e25823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewEIC2xVrjDUT-J4EtG-Ag.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">数据生成过程，按作者分类的图像</p></figure><p id="1139" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<em class="nx"> Y </em>是结果变量，<em class="nx"> D </em>是对兴趣的处理，<em class="nx"> X </em>是潜在的高维控制变量集。<strong class="lh ja">与之前设置的区别</strong>在于，现在我们通过函数<em class="nx"> g() </em>和<em class="nx"> m() </em>将<em class="nx"> X </em>和<em class="nx"> Y </em>和<em class="nx"> D </em>之间的关系置为未指定。</p><h2 id="51da" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">天真的方法</h2><p id="9f6a" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">使用机器学习方法来估计<em class="nx"> α </em>的一种简单方法是，例如，构建一个复杂的机器学习估计器来学习回归函数<em class="nx"> αD + g(X) </em>。</p><ol class=""><li id="2ab0" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated">将样本一分为二:主样本和辅助样本(为什么？参见下面的注释)</li><li id="f991" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">使用辅助样本来估计<em class="nx"> ĝ(X) </em></li><li id="f9cc" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">使用主样本计算<em class="nx"> Y </em>在<em class="nx"> X </em>上的正交分量:</li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pb"><img src="../Images/c26bb6c9a008dc6750ce1a722f571d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LV87ChKULNyFxNfPgGUoiQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="09f8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">4.在<em class="nx"> D </em>使用主样本估计回归<em class="nx"> û </em>的剩余OLS估计量</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pc"><img src="../Images/daee23748fcdd2f2a0b03767c0d3f719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMMhv_N4G58SZh6lNMWNrg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="c86f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个估算器会有两个问题:</p><ol class=""><li id="d01d" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated">收敛速度慢，即比<em class="nx"> √n </em>慢</li><li id="0ad9" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">它会有偏差，因为我们采用了高维正则化估计量(例如，我们正在做变量选择)</li></ol><p id="8d47" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注(1) </strong>:到目前为止我们还没有谈到，但是变量选择程序还引入了另一种类型的偏倚:<strong class="lh ja">过拟合偏倚</strong>。这种偏差的出现是因为用于选择变量的样本与用于估计感兴趣系数的样本是相同的。这种偏差<strong class="lh ja">很容易通过样本分割</strong>解决:在选择和估计过程中使用不同的子样本。</p><p id="0543" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注(2) </strong>:为什么可以用第三步的残差来估计第四步的<em class="nx"> α </em>？因为<a class="ae mb" rel="noopener" target="_blank" href="/59f801eb3299">弗里希-沃-洛弗尔定理</a>。如果你不熟悉它，我已经在这里写了一篇关于弗里希-沃-洛弗尔定理的博文。</p><h2 id="e894" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">双重正交化</h2><p id="c8ad" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated"><strong class="lh ja">双去偏机器学习</strong>通过重复正交化过程<strong class="lh ja">两次</strong>来解决问题。双重选择后的想法是相同的:通过执行两次变量选择来减少正则化偏差。由于<a class="ae mb" rel="noopener" target="_blank" href="/59f801eb3299">弗里希-沃-洛弗尔定理</a>，估计器仍然有效。</p><p id="808c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">实际上，双去偏机器学习由以下<strong class="lh ja">步骤</strong>组成。</p><ol class=""><li id="b9bf" class="nz oa iq lh b li lj ll lm lo ob ls oc lw od ma oe of og oh bi translated">将样品一分为二:主样品和辅助样品</li><li id="6d0e" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oe of og oh bi translated">使用辅助样本估计<em class="nx"> ĝ(X) </em>从</li></ol><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pb"><img src="../Images/442b2040b1417e9866e107f39649fcbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whN6w4IuLgm68mEYiU0Z5Q.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="a086" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">3.使用辅助样本从下式估计<em class="nx"> m̂ </em></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pd"><img src="../Images/27fadc5bf23f69c5a9bbd16baa11e245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7VAVEia35qG5FlnJuhuaA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="73d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">4.使用主样本计算<em class="nx"> D </em>在<em class="nx"> X </em>上的正交分量为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi pe"><img src="../Images/daec2813aada3dc2bdd00db7c50e5502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXVOf0y7kcl6yAYR51vyhg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="a449" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">5.使用主样本估计双剩余OLS估计量为</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ng"><img src="../Images/2aa6aa3be24819b4f2dc30e826b239e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWSi-zwAaQa3cpOPNAvjAQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="2da1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">估计量是<strong class="lh ja">根-N一致</strong>！这意味着，随着样本量的增加，不仅估计量收敛到真实值(即它是一致的)，而且它的标准差也收敛到真实值(即它是根-N一致的)。</p><p id="1d53" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，由于样本分裂，估计器仍然具有较低的收敛速度。该问题通过反转分割样本、重新估计系数并对两次估计取平均值来解决。请注意，这一程序是有效的，因为两个估计是独立的样本分裂程序。</p><h2 id="a18f" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">警示故事</h2><p id="e860" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">在我们结束之前，我必须提到最近由<a class="ae mb" href="https://arxiv.org/abs/2108.11294" rel="noopener ugc nofollow" target="_blank"> Hünermund，Louw和Caspi (2022) </a>发表的一篇研究论文，其中作者表明，如果盲目应用，双去偏置的机器学习很容易<strong class="lh ja">适得其反</strong>。</p><p id="449a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">问题与<strong class="lh ja">坏的控制变量</strong>有关。如果你从未听说过这个术语，我已经在这里写了一篇关于好的和坏的控制变量的介绍性的<a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">博文</a>。简而言之，以额外特征为分析条件并不总是有利于因果推断。根据不同的设置，可能存在一些我们想在分析中忽略的变量，因为它们的<strong class="lh ja">包含</strong>可能会使兴趣系数产生偏差，从而妨碍因果解释。最简单的例子是治疗变量<em class="nx"> D </em>和结果变量<em class="nx"> Y </em>的常见结果变量。</p><p id="0f10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">双去偏机器学习模型隐含地假设控制变量<em class="nx"> X </em>是结果<em class="nx"> Y </em>和治疗<em class="nx"> D </em>的(弱)<strong class="lh ja">共同原因</strong>。如果是这种情况，并且在<em class="nx"> X </em>和<em class="nx"> Y </em>之间不存在进一步的中介/间接关系，则没有问题。但是，例如，如果控制<em class="nx"> X </em>中的某个变量是一个共同的结果，而不是一个共同的原因，那么它的包含将会使感兴趣的系数产生偏差。此外，该变量可能与结果<em class="nx"> Y </em>或治疗<em class="nx"> D </em>高度相关。在后一种情况下，这意味着后双重选择可能会包括它，而在简单选择的情况下则不会。因此，在存在不良控制变量的情况下，双去偏的机器学习可能比简单的预测试更糟糕。</p><p id="ce5c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">简而言之，对于任何方法来说，<strong class="lh ja">至关重要的是</strong>清楚地理解该方法的假设，并始终检查潜在的违规行为。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="9b2f" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结论</h1><p id="b2f5" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">在这篇文章中，我们看到了如何使用后双重选择，更一般地说，双重去偏置机器学习来消除偏置的一个重要来源:正则化偏置。</p><p id="8640" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Victor Chernozhukov及其合作者的这一贡献无疑是过去十年中因果推理最相关的进步之一。它现在被业界广泛采用，包含在最常用的因果推理包中，如<a class="ae mb" href="https://econml.azurewebsites.net/" rel="noopener ugc nofollow" target="_blank"> EconML </a>(微软)和<a class="ae mb" href="https://causalml.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> causalml </a>(优步)。</p><p id="7a6e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你(可以理解)觉得需要更多关于双去偏置机器学习的材料，但你不喜欢阅读学术论文(也很容易理解)，这里有一个很好的妥协。</p><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="pf pa l"/></div></figure><p id="d112" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个视频讲座中，Victor Chernozhukov亲自介绍了这个想法。视频讲座相对偏重于数学和统计，但你不可能得到比这更合格和直接的来源！</p><h2 id="c496" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">参考</h2><p id="2c69" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">[1] A. Belloni，D. Chen，V. Chernozhukov，C. Hansen，<a class="ae mb" href="https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA9626" rel="noopener ugc nofollow" target="_blank">应用于征用权的最优工具的稀疏模型和方法</a> (2012)，<em class="nx">计量经济学</em>。</p><p id="1840" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] A. Belloni，V. Chernozhukov，C. Hansen，<a class="ae mb" href="https://academic.oup.com/restud/article-abstract/81/2/608/1523757" rel="noopener ugc nofollow" target="_blank"/>(2014)，<em class="nx">经济研究综述</em>。</p><p id="eba3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3] V. Chernozhukov，D. Chetverikov，M. Demirer，E. Duflo，C. Hansen，W. Newey，J. Robins，<a class="ae mb" href="https://academic.oup.com/ectj/article/21/1/C1/5056401" rel="noopener ugc nofollow" target="_blank">用于治疗和结构参数的双/去偏置机器学习</a> (2018)，<em class="nx">计量经济学杂志</em>。</p><p id="395b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4] P. Hünermund，B. Louw，I. Caspi，<a class="ae mb" href="https://arxiv.org/abs/2108.11294" rel="noopener ugc nofollow" target="_blank">双机器学习和自动化混杂选择——一个警示故事</a> (2022)，<em class="nx">工作论文</em>。</p><h2 id="cd0b" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">相关文章</h2><ul class=""><li id="0acb" class="nz oa iq lh b li nb ll nc lo pg ls ph lw pi ma oy of og oh bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/eb767a59975b">双去偏机器学习(第一部分)</a></li><li id="537f" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/344ac1477699">理解省略变量偏差</a></li><li id="9687" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/59f801eb3299">理解弗里希-沃-洛弗尔定理</a></li><li id="d932" class="nz oa iq lh b li oi ll oj lo ok ls ol lw om ma oy of og oh bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/b63dc69e3d8c">Dag和控制变量</a></li></ul><h2 id="e447" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">密码</h2><p id="0508" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">你可以在这里找到Jupyter的原始笔记本:</p><div class="pj pk gp gr pl pm"><a href="https://github.com/matteocourthoud/Blog-Posts/blob/main/notebooks/pds.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd ja gy z fp pr fr fs ps fu fw iz bi translated">Blog-Posts/PDS . ipynb at main matter courthoud/Blog-Posts</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">我博客文章的代码和笔记本。通过在…上创建帐户，为matteocourthoud/Blog-Posts的发展做出贡献</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">github.com</p></div></div><div class="pv l"><div class="pw l px py pz pv qa kz pm"/></div></div></a></div></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h2 id="6661" class="np mk iq bd ml on oo dn mp op oq dp mt lo or os mv ls ot ou mx lw ov ow mz iw bi translated">感谢您的阅读！</h2><p id="49c5" class="pw-post-body-paragraph lf lg iq lh b li nb ka lk ll nc kd ln lo nd lq lr ls ne lu lv lw nf ly lz ma ij bi translated">我真的很感激！🤗<em class="nx">如果你喜欢这个帖子并且想看更多，可以考虑</em> <a class="ae mb" href="https://medium.com/@matteo.courthoud" rel="noopener"> <strong class="lh ja"> <em class="nx">关注我</em> </strong> </a> <em class="nx">。我每周发布一次与因果推断和数据分析相关的主题。我尽量让我的帖子简单而精确，总是提供代码、例子和模拟。</em></p><p id="f7b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="nx">还有，一个小小的</em> <strong class="lh ja"> <em class="nx">免责声明</em> </strong> <em class="nx">:我写作是为了学习所以出错是家常便饭，尽管我尽力了。当你发现他们的时候，请告诉我。也很欣赏新话题的建议！</em></p></div></div>    
</body>
</html>