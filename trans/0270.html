<html>
<head>
<title>How Many Clusters?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有多少个集群？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-many-clusters-6b3f220f0ef5#2022-02-11">https://towardsdatascience.com/how-many-clusters-6b3f220f0ef5#2022-02-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ec1e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">选择正确聚类数的方法</strong></h2></div><p id="9ba3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">简介</strong></p><p id="8668" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">聚类是一种无监督的机器学习方法，可以从数据本身识别相似数据点的组，称为聚类。对于一些聚类算法，如K-means，需要事先知道有多少个聚类。如果没有正确指定簇的数量，那么结果的信息量就不大(见图1)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/84e24a1273a9045f05222dc9df943c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZVqz8mOgsNWONd8X"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图1 </em> </strong> <em class="ls">:不同簇数的聚类，k=4，6，&amp; 8。具有6个集群的模拟数据。图片作者。</em></p></figure><p id="95de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，在许多情况下，我们不知道我们的数据中有多少聚类。事实上，弄清楚有多少个集群可能是我们首先要执行集群的原因。当然，数据集的领域知识可以帮助确定聚类的数量。但是，这是假设你知道目标类(或者至少知道有多少个类)，而这在无监督学习中是不成立的。我们需要一种不依赖于目标变量就能告诉我们聚类数的方法。</p><p id="e4ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确定簇的正确数量的一个可能的解决方案是强力方法。我们尝试用不同数量的聚类来应用聚类算法。然后，我们找到优化聚类结果质量的幻数。在本文中，我们首先介绍两个流行的度量标准来评估集群质量。然后，我们介绍三种方法来寻找最佳的集群数量:</p><ul class=""><li id="00e9" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated">肘法</li><li id="284b" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated">轮廓系数的优化</li><li id="f19c" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated">差距统计</li></ul><p id="1276" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">聚类结果质量</strong></p><p id="5d22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在讨论确定最佳聚类数的不同方法之前，我们先来看看如何定量评估聚类结果的质量。想象以下场景。相同的数据集被分成三个集群(见图2)。如您所见，左边的分类定义得很好，而右边的分类却很难识别。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mh"><img src="../Images/cda25bf2f7dfcddc7dea23c501eb31f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WtQepJTaRnooUxvA"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图2 </em> </strong> <em class="ls">:基于同一数据集的定义明确的聚类(左)和定义不明确的聚类(右)的例子。箭头指示数据点与其聚类中心之间的距离。图片作者。</em></p></figure><p id="60f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是为什么呢？请记住，聚类的目标是将数据点分组为簇，以便(1)簇内的点尽可能相似，(2)属于不同簇的点尽可能不同。这意味着，在理想的聚类中，类内变化较小，而类间变化较大。因此，一个好的聚类质量度量应该能够定量地总结(1)和/或(2)。</p><p id="431b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个这样的质量度量是惯性。这是通过数据点和它们所属的聚类中心之间的平方距离之和来计算的。惯性量化了集群内的变化。</p><p id="e76e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个流行的指标是轮廓系数，它试图总结集群内和集群间的变化。在每个数据点，我们计算到该数据点所属聚类中心的距离(称为<em class="mi"> a </em>，以及到第二好的聚类中心的距离(称为<em class="mi"> b </em>)。这里，第二最佳聚类是指不是当前数据点的聚类的最接近的聚类。然后基于这两个距离<em class="mi"> a </em>和<em class="mi"> b </em>，该数据点的剪影<em class="mi"> s </em>计算为s=(b-a)/max(a，b)。</p><p id="0338" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在理想的聚类下，距离<em class="mi"> a </em>与距离<em class="mi"> b </em>相比非常小，导致<em class="mi"> s </em>接近1(见图3，左侧)。如果聚类不是最理想的，那么距离<em class="mi"> a </em>和<em class="mi"> b </em>可能不会有很大的不同(见图3，中间)。在那种情况下,<em class="mi"> s </em>接近于0。如果聚类更差，那么距离<em class="mi"> a </em>实际上可能大于距离<em class="mi"> b </em>(见图3，右侧)。在这种情况下，<em class="mi"> s </em>变为负值，接近-1。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mj"><img src="../Images/77fded770069d36b343cb97e7be7dd4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8JgGitPFOIUge9xR"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图3 </em> </strong> <em class="ls">:聚类最优(左)、次优(中)、更差(右)的场景。星星表示星团中心。图片作者。</em></p></figure><p id="7a64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦在所有数据点计算了<em class="mi"> s </em>，则<em class="mi"> s </em>的平均值确定了轮廓系数。可以分别为每个聚类或所有数据点计算轮廓系数。轮廓系数接近1表示聚类算法能够将数据划分为分离良好的聚类。</p><p id="0f61" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">肘法</strong></p><p id="3389" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">惯性是群集数量<em class="mi"> k </em>的递减函数。然而，它的减少率在高于或低于最佳聚类数<em class="mi"> K </em>时是不同的。对于<em class="mi">K</em>K&lt;K<em class="mi">K</em>来说，惯性下降很快，而对于<em class="mi">K</em>K&gt;K<em class="mi">K</em>来说，惯性下降很慢。因此，通过在<em class="mi"> k </em>的范围内绘制惯性图，可以确定曲线在k处的弯曲位置，图4显示了图1示例中的惯性图。我们可以清楚地看到在<em class="mi"> k </em> =6处有一个弯曲，或者说是肘部。</p><p id="160b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这种方法有些主观，因为不同的人可能在不同的位置识别肘部。在我们图4的例子中，有些人可能认为<em class="mi"> k </em> =4是肘。此外，正如我们将在后面看到的那样，肘部可能并不总是很明显。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mk"><img src="../Images/c471ae81e7298b3e93347d985d570b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XYOrFVXGqEr8wx5o"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图4 </em> </strong> <em class="ls">:不同k下的惯量图，为图1所示的数据集。图片作者。</em></p></figure><p id="8cdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">肘方法的用例可以在一个自然语言问题中看到，使用<a class="ae ml" href="https://www.knime.com/software-overview?utm_source=medium&amp;utm_medium=social&amp;utm_term=&amp;utm_content=&amp;utm_campaign=Community" rel="noopener ugc nofollow" target="_blank"> KNIME分析平台</a>确定社交网络中的最佳主题数量(参见博客<a class="ae ml" href="https://www.knime.com/blog/topic-extraction-optimizing-the-number-of-topics-with-the-elbow-method?utm_source=medium&amp;utm_medium=social&amp;utm_term=&amp;utm_content=&amp;utm_campaign=Community" rel="noopener ugc nofollow" target="_blank">主题提取:使用肘方法优化主题数量</a>)。因为没有KNIME节点来计算惯性，所以在本例中使用了一个<a class="ae ml" href="https://kni.me/n/QNB4FsAPnEAgOMVh?utm_source=medium&amp;utm_medium=social&amp;utm_term=&amp;utm_content=&amp;utm_campaign=Community" rel="noopener ugc nofollow" target="_blank"> Java片段</a>节点来计算惯性。下面是用来计算惯性的代码片段。</p><pre class="lc ld le lf gt mm mn mo mp aw mq bi"><span id="7895" class="mr ms iq mn b gy mt mu l mv mw">// Initializing the sum of squares<br/>out_sum_squares = 0.0;<br/>/*<br/>The first half of columns belong to features of the origin.<br/>The second half of columns belong to features of the terminus.<br/>The groups of columns have to be in the same order.<br/> */<br/>int col_count = getColumnCount();<br/>int no_dimensions = col_count / 2;</span><span id="b5ab" class="mr ms iq mn b gy mx mu l mv mw">// Loop over the feature columns<br/>for(int i=0; i &lt; no_dimensions; i++){<br/>  /*<br/>  Checking if the feature i from the origin and<br/>  the feature i from the terminus (i.e., i+no_dimensions)<br/>  are not missing, and have similar column names<br/>   */<br/>  if(!isMissing(i) &amp;&amp; isType(i, tDouble)<br/>   &amp;&amp; !isMissing(i+no_dimensions) &amp;&amp; <br/>   isType(i+no_dimensions, tDouble) &amp;&amp;<br/>   getColumnName(i+no_dimensions).contains(getColumnName(i))){<br/>    // Calculating the squared distance and adding it to the sum<br/>    out_sum_squares += Math.pow(getCell(i, tDouble) - <br/>    getCell(i+no_dimensions, tDouble), 2);<br/>   }<br/>}</span></pre><p id="af5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">剪影法</strong></p><p id="c709" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">轮廓系数可以提供更客观的手段来确定最佳的聚类数目。这是通过简单地计算<em class="mi"> k </em>范围内的轮廓系数，并将峰值确定为最佳<em class="mi"> K </em>来实现的。一个KNIME组件<a class="ae ml" href="https://kni.me/c/XCtuVNVeuqHSQkqk?utm_source=medium&amp;utm_medium=social&amp;utm_term=&amp;utm_content=&amp;utm_campaign=Community" rel="noopener ugc nofollow" target="_blank">优化的K-Means(剪影系数)</a>正是这样做的。它在<em class="mi"> k </em>的范围内执行K均值聚类，找到产生最大轮廓系数的最优<em class="mi"> K </em>，并基于优化的<em class="mi"> K </em>将数据点分配给聚类。图5显示了来自图1所示示例数据的轮廓系数图示例。可以看出，轮廓系数在<em class="mi"> k </em> =6处达到峰值，因此被确定为最佳k</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi my"><img src="../Images/fe76522832fc49c6b526ed56adefc826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DRog7jeG5QE0jSVT"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图5 </em> </strong> <em class="ls">:不同k的剪影系数图，为图1所示的数据集。图片作者。</em></p></figure><p id="7963" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">差距统计</strong></p><p id="22c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了讨论间隙统计，让我们考虑一个没有任何聚类组织的随机数据集的聚类。假设一个随机数据集被聚类成<em class="mi"> k </em>个聚类，然后根据聚类结果计算惯性(见图6)。尽管缺乏基本的聚类组织，但随着<em class="mi"> k </em>的增加，聚类的随机数据产生稳定减少的惯性(惯性的复数)。这是因为聚类中心越多，数据点到聚类中心的距离就越小，从而产生衰减惯性。相比之下，正如我们在图4中已经看到的，在具有聚类组织的数据集中，无论<em class="mi"> k </em>低于还是高于最优聚类数<em class="mi"> K </em>，惯性下降的速率都会变化。当观察到的惯性和随机数据一起绘制时，差异变得明显(见图7)。通过比较来自(有希望的)聚类数据集和覆盖数据空间中相同范围的相应随机数据集的惯性来计算缺口统计量(Tibshirani等人，(2001))。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mz"><img src="../Images/6ca28b2f82d21da1c32ddc92c3116961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ilNntaWP_N3CLgWq"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图6 </em> </strong> <em class="ls">:均匀分布的随机数据聚类成k=4(左)、6(中)、15(右)个聚类。图片作者。</em></p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/752a8f4a96471ce6a1778e21ffe03394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/0*VsZLAD-2yBJKgdqF"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图7 </em> </strong> <em class="ls">:原始数据(来自图1)相对于k. Image范围内的随机数据，惯性如何降低。</em></p></figure><p id="7d1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在间隙统计的实际计算中，会生成一些随机样本，然后在<em class="mi"> k </em>的范围内进行聚类，并记录所得的惯性。这为随机情况提供了一些惯性。原始数据集也在范围<em class="mi"> k </em>上聚集，产生一系列惯性。在<em class="mi"> k </em>簇上的间隙统计量计算如下</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/d103b434d4c814de447e1404b746ee78.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*2QO05Tb1c2-SnLdcjY-65A.png"/></div></figure><p id="5bc2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，Wk(i)是来自第<em class="mi"> i </em>个随机样本(i=1，2，…，B)和第<em class="mi"> k个</em>个聚类的惯量，Wk是来自第<em class="mi"> k个</em>个聚类的原始数据的惯量。我们还计算其标准偏差为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/4e0e50409796b11c1d9d5d96b697f090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_64whHgJNrHZADF7rErMXg.png"/></div></div></figure><p id="b17d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们找到最优的<em class="mi"> K </em>作为满足条件的最小的<em class="mi"> k </em></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/cda2363395235d538605d734f3f44ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*dLeti9xo-wkD8ikRoCZSkQ.png"/></div></figure><p id="0846" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">间隙统计的计算涉及模拟。我们调用R中的函数，通过KNIME工作流中的一些R脚本来计算gap统计数据。具体来说，调用clusGap()函数来计算不同<em class="mi"> k </em>处的间隙统计量，maxSE()返回满足上述条件的最优<em class="mi"> K </em>。图8显示了我们在图1中的示例数据集的间隙统计图，基于在每个<em class="mi"> k </em>处<em class="mi"> B </em> =100次迭代。红线代表满足上述条件的最佳<em class="mi"> K </em>。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9b4c5ddce29afde8f070c616f4ccc178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/0*U8O-XZeoOmXUGImz"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图8 </em> </strong> <em class="ls">:基于B=100次迭代的缺口统计及其标准差图。满足条件的最佳k=6由红线表示。图片作者。</em></p></figure><p id="08eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的是，间隙统计法确定的最优<em class="mi"> K </em>可能不一致。例如，当缺口统计方法多次应用于我们的玩具数据时，得到的最优<em class="mi"> K </em>可能会不同(见图9)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nf"><img src="../Images/e5b08a3cb0e166881645d15b451d44e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPbMySif1tmrjlwtEgHQDA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图9 </em> </strong> <em class="ls">:缺口统计图示例。最佳k可能不一致，这取决于模拟结果。图片作者。</em></p></figure><p id="3925" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">示例:MNIST手写数字数据</strong></p><p id="857f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们在具有集群组织的真实数据集上检查上述三种方法。MNIST数据集由从0到9的手写数字的灰度图像组成。在这个例子中，我们使用8×8像素的n=1797个图像。图10显示了数据集的一些例子。上述三种方法用于确定最佳聚类数。由于该数据集中有10个不同的数字，因此有理由假设有10个聚类，每个聚类对应于其中一个数字。然而，人们可能有多种方式来书写某些数字。因此，实际上集群的数量不一定是10。数据的2D散点图(由tSNE投影到2D空间，见图11)显示，一些集群可能与其他集群分离良好，而一些集群可能接触或重叠。这个例子的工作流程可以在https://kni.me/w/ACjm92qCUGbXsCX6的KNIME Hub上找到。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ng"><img src="../Images/880cee4675e7afe36598b30b9abb46ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kBO3t6ImJTd0dl-A"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图10 </em> </strong> <em class="ls">:手写数字数据的例子，以8×8像素下采样。图片作者。</em></p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nh"><img src="../Images/0b1b7733ff373633cec2208cd25e7c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v2Ewyv1OSoocXttG"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图11 </em> </strong> <em class="ls">:用t-SNE(t-distributed random neighbor embedding)投影到2D空间的数字数据散点图。图片作者。</em></p></figure><p id="3fc6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">肘方法的结果是不确定的，因为图中没有明显的肘(图12，左)。在图中有一些细微的弯曲(例如，9、12、20、24，仅举几个例子)，并且这些中的任何一个都可以被选择作为聚类的数量。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ni"><img src="../Images/0967236dc7b51768d9e3e11015730181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hq3VJXR90odEu2LmoGdQjQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图12 </em> </strong> <em class="ls">:由手指数据生成的肘部图(左)和轮廓系数图(右)。图片作者。</em></p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/92bd0e5801f0790ee2179ade4d908fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/0*4EPy4j88RQ0dqvsT"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图13 </em> </strong> <em class="ls">:基于B=100次迭代，从数字数据生成的间隙统计图。红线表示最佳k=12。图片作者。</em></p></figure><p id="d6ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">轮廓系数在<em class="mi"> k </em> =12处有一个峰值(图12，右)。根据间隙统计法，k=12也被确定为最佳聚类数(图13)。我们可以直观地比较k=9(根据肘方法最优)和k=12(根据剪影和间隙统计方法最优)的k均值聚类(参见图14)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nk"><img src="../Images/81ec5b7ae3e81317714a32f8a445b66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*C2ximhouQYt8cH8f"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><strong class="bd lr"> <em class="ls">图14 </em> </strong> <em class="ls"> : K-Means在k=9，k=12的数字数据中找到的聚类，用t-SNE投影到2D空间。图片作者。</em></p></figure><p id="8724" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论</strong></p><p id="13b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们展示了选择最佳聚类数的三种不同方法，即肘方法、轮廓系数和间隙统计。虽然肘图的解释是相当主观的，但是轮廓系数和间隙统计方法都可以精确地确定聚类的数量。然而，间隙统计涉及模拟，它可能不总是产生相同的解决方案。</p><p id="02df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与许多机器学习方法一样，这里描述的方法并不是在所有情况下都适用。由于这些方法量化了聚类中心和数据点之间的距离，因此它们适合于寻找凸聚类，例如在K-Means聚类中找到的聚类。</p><p id="8c39" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong></p><ul class=""><li id="85c7" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated">罗伯特·蒂布拉尼，根特·瓦尔特，特雷弗·哈斯蒂。通过间隙统计估计数据集中的聚类数。皇家统计学会杂志，B辑，63:411–423(2001)。</li></ul></div></div>    
</body>
</html>