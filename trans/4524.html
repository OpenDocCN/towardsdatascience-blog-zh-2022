<html>
<head>
<title>Whisper: Transcribe &amp; Translate Audio Files With Human-Level Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">耳语:以人类水平的性能转录和翻译音频文件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whisper-transcribe-translate-audio-files-with-human-level-performance-df044499877#2022-10-07">https://towardsdatascience.com/whisper-transcribe-translate-audio-files-with-human-level-performance-df044499877#2022-10-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ebc8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个兼容97种语言的人工智能模型——以及如何使用它</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/01a0bbbec55dd47d0347145a601c1a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yeZ0LAbrc9VFkWQB"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@deepmind?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="bade" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">现在很明显，深度学习研究的大方向已经发生了根本性的变化。</strong></p><p id="dc5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几年前，大多数创新论文的<em class="lv">操作方式</em>是这样的:选择一个<em class="lv">数据集X </em>，在该数据集上构建并训练一个新模型，并通过报告SOTA结果来证明你的模型是最好的。但是如果我们在数据集Y 上测试这个模型会发生什么呢？</p><p id="dd86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如:在<strong class="lb iu"> ImageNet </strong>数据集上训练一个强大的<strong class="lb iu"> ResNet </strong>。该模型将在分类猫和狗方面表现出色，但在分类卡通狗、手绘狗或梵高风格的狗等图像时会有困难。</p><p id="a572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> OpenAI </strong>已经开始了这种范式的转变，推出了概括得很好的模型，如<a class="ae ky" href="https://medium.com/p/f8ee408958b1" rel="noopener"><strong class="lb iu"><em class="lv">CLIP【1】</em></strong></a>。为此，他们使用元学习方法，专注于零射击分类:</p><blockquote class="lw"><p id="dcca" class="lx ly it bd lz ma mb mc md me mf lu dk translated">零镜头分类是模型对看不见的标签进行分类的能力，而无需对它们进行专门的分类训练。这种学习方法更好地反映了人类的感知。</p></blockquote><p id="8a16" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">遵循同样的步骤，<strong class="lb iu"> OpenAI </strong>发布了<strong class="lb iu"><em class="lv">Whisper【2】、</em></strong>an<strong class="lb iu">A</strong>automatic<strong class="lb iu">S</strong>peech<strong class="lb iu">R</strong>ecognition(ASR)车型。在其他任务中，<em class="lv">耳语</em>可以转录大型音频文件，具有人类级别的性能！</p><p id="ff69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们详细描述了<em class="lv"> Whisper的</em>架构，并分析了该模型是如何工作的，以及它为什么如此酷。</p><p id="4d4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们开始吧！</p><h1 id="dcc3" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">耳语——概述</h1><p id="c909" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们简单描述一下<em class="lv"> Whisper的</em>特征:</p><ul class=""><li id="fec1" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><strong class="lb iu">开源:</strong>模型由<strong class="lb iu"> OpenAI </strong>创建并开源。我们稍后将看到如何使用它的编程教程。</li><li id="4584" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">多任务训练:</strong>该机型架构新颖，适合多任务训练。<em class="lv">耳语</em>是为i) <strong class="lb iu">语言识别</strong>，ii) <strong class="lb iu">语音活动检测</strong>，iii) <strong class="lb iu">转录，</strong>和iv) <strong class="lb iu">翻译</strong>而训练的。</li><li id="836a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">多语种:除了英语，<em class="lv">耳语</em>接受了96种其他语言的培训。</li><li id="6125" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">多样数据:</strong>模型在68万小时的音频上训练。其中65%的时间用于英语语音识别，17%用于多语言语音识别，18%用于英语翻译。</li><li id="3be1" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">卓越的性能:</strong>作者称<em class="lv"> Whisper </em>在英语语音识别方面达到了人类水平的性能。</li><li id="1ef9" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu"> Whisper vs商业产品:</strong>实验表明<em class="lv"> Whisper </em>直接挑战商业竞争对手如<strong class="lb iu">亚马逊Alexa </strong> &amp; <strong class="lb iu"> Siri </strong>，甚至胜过他们——尽管论文中掩盖了他们的名字。</li></ul><blockquote class="nw nx ny"><p id="8b24" class="kz la lv lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated"><strong class="lb iu">限制:</strong>据我所知，OpenAI不会公开他们的训练数据集。他们只报告评估数据集。</p></blockquote><h1 id="0e9f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated"><strong class="ak">嘀咕对我们有什么好处</strong></h1><p id="9b65" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">该模型在许多行业都有显著的应用，包括我们的日常生活，例如:</p><ol class=""><li id="d19f" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu oc no np nq bi translated">我们可以即时转录大型音频文件，如播客。</li><li id="eef8" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">我们可以毫不费力地抄写演讲稿。</li><li id="ca70" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">为我们的Youtube视频或其他内容制作准确的字幕会容易得多。此外，使用非英语语言不是一种限制。如果我们需要翻译，耳语者也可以处理！</li><li id="c8b8" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">学生们再也不用在听课时记笔记了！</li><li id="2121" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">有听力障碍的人生活质量会好得多。想象一下，如果你能超级准确地转录你的话。</li></ol><blockquote class="nw nx ny"><p id="6748" class="kz la lv lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">额外收获:open ai有机会利用Whisper为其下一款GPT型号GPT 4无缝创建一个更好、更多样化的数据集。</p></blockquote><h1 id="d8f4" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">耳语建筑</h1><p id="4748" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu">图1 </strong>展示了<strong class="lb iu">耳语</strong>的顶层架构。让我们一步一步地描述这个数字:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/d4b9add9772dd3f23f22e03d4eb005c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lt5UjjKNzYQSG5AkGdr0sA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图1:</strong>Whisper的顶层架构及其主要组件(<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="288e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> Whisper的</em>实现的核心就是大家熟知的【4】的<strong class="lb iu">编解码变压器。作者选择使用原始版本，而不是更新的复杂版本——他们不想通过模型改进让<em class="lv">耳语</em>变得伟大。</strong></p><p id="2dcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至于输入，音频被分成30秒的块，并转换成一个<strong class="lb iu">对数-梅尔频谱图【附录A】</strong>。然后通过具有<strong class="lb iu"> GELU </strong>激活函数的两层<strong class="lb iu"> CNN </strong>对声谱图进行处理，并用<strong class="lb iu">正弦位置嵌入</strong>进行丰富。现在，输入可以由变压器的编码器部分进行处理。当编码器完成时，它发送<code class="fe of og oh oi b">K</code>和<code class="fe of og oh oi b">V</code>注意力编码向量，由解码器交叉处理。</p><p id="514b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解码器有更多的工作要做。请记住，<em class="lv"> Whisper </em>是一种多任务架构——解码器必须首先决定要做什么，然后再进行预测。每个不同任务的流程在<strong class="lb iu">图2: </strong>中有更好的说明</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/7259710e254f60e276443d6dacfaf2ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBOroaDlsUwjN4o0Dh-k-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图2: </strong>密语解码器部分的不同任务(<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="nw nx ny"><p id="012f" class="kz la lv lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">我强烈建议访问该报的网址，并在那里获得更高质量的图片。</p></blockquote><p id="f1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，解码器从<em class="lv"> SOT </em>开始(&lt;|<strong class="lb iu"><em class="lv">start-of</em></strong><em class="lv">-</em><strong class="lb iu"><em class="lv">抄本| &gt; </em> </strong> token)。然后，它必须确定音频中是否有真实的语音。如果有，解码器预测该语音的特定语言标记，例如英语的<strong class="lb iu"> <em class="lv"> EN </em> </strong>。如果不是，解码器预测<strong class="lb iu"><em class="lv">&lt;| nospeech |&gt;</em></strong>令牌。用户也可以明确指定语言。</p><p id="d727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，解码器决定(根据用户请求)首选任务是转录还是翻译—由<strong class="lb iu"><em class="lv">&lt;|转录| &gt; </em> </strong>和<strong class="lb iu"><em class="lv">&lt;|翻译| &gt; </em> </strong>标记指定。最后，该模型通过为该情况包含一个<strong class="lb iu"><em class="lv">&lt;| notimestamps |&gt;</em></strong>标记来指定是否预测时间戳。</p><p id="677d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概括地说，每个可能任务的流程是:</p><ul class=""><li id="e762" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">输入音频→无语音</li><li id="0ca9" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">输入音频→语言识别→ <strong class="lb iu">转录</strong> →时间戳</li><li id="8249" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">输入音频→语言识别→ <strong class="lb iu">转录</strong> →无时间戳</li><li id="3276" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">输入音频→语言识别→ <strong class="lb iu">翻译</strong> →时间戳</li><li id="2a0b" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">输入音频→语言识别→ <strong class="lb iu">翻译</strong> →无时间戳</li></ul><p id="53e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！我们现在已经知道了<em class="lv">耳语</em>是如何端到端工作的。</p><p id="cfb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者创造了9种模型变体，每种都有不同的尺寸。此外，这些模型中的一些只有英语，而另一些是多语言的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/baeb3020f6bebab57eecc5254e2c7298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*de0SlvA0B-KxM6riuGLijQ.png"/></div></figure><p id="1ece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请随意查看<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">论文的附录</a>并检查每个模型的训练配置和超参数。</p><h1 id="8e0b" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">耳语的竞争优势</h1><p id="1bf7" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu"> </strong> <em class="lv">自动语音识别</em>任务对于深度学习来说并不新鲜。让我们看看为什么<em class="lv">耳语</em>脱颖而出:</p><h2 id="9639" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated"><strong class="ak">大规模弱监管方法</strong></h2><p id="0f0e" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这种方法定义了<em class="lv">耳语</em>的核心哲学——事实上，这个短语包含在了报纸的名字中。</p><p id="a0bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练ASR系统的传统方法是让它们以无人监管的方式从原始音频格式中学习。这种方法消除了对人工干预的需要，但是对于更具体的任务(也称为<strong class="lb iu">下游任务</strong>)，该模型可能需要额外的微调。</p><p id="f9e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，总有被监督的方式。然而，这取决于找到大量由人类仔细标记的高质量音频数据的能力。同样，这很有挑战性。</p><p id="5bb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">耳语</em>的作者变得有创意并想到:让我们尝试使用大量数据(<strong class="lb iu">大规模</strong>)来训练我们的模型，这些模型不一定具有黄金标准的质量(<strong class="lb iu">弱监督</strong>)。<strong class="lb iu">这里的关键是数据的数量和多样性</strong>。最终，这种方法是成功的——<em class="lv">Whisper</em>可以显著地扩展和推广。</p><h2 id="39e2" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">多任务流水线</h2><p id="919c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">作者想要建立一个不仅仅能够进行语音识别的模型。</p><p id="9eb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个潜在的担忧是，在许多任务上联合训练一个单一的模型可能会造成干扰。如果一个任务被单个模型更好地学习，而不是被集成到试图同时学习其他任务的父模型中，会怎么样？</p><p id="f204" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">经过一些实验，作者发现不仅不同任务之间没有干扰，而且<em class="lv">耳语</em>从并行学习所有任务中受益匪浅！</p><h1 id="b1f4" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">实验结果</h1><p id="4b90" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">接下来，作者对上述说法进行了测试，并展示了<em class="lv"> Whisper </em>与其他型号相比的表现。</p><h2 id="dffe" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">英语转录评价</h2><p id="21db" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">请记住，作者不只是想创建一个在数据集X上优于所有SOTA模型的模型。他们的目标也是创建一个能在相似数据集上轻松概括的模型。换句话说，他们想创造一个能适应<strong class="lb iu">分布变化<em class="lv"/>【Apendix B】</strong>的模型。</p><p id="0c9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们来看看<strong class="lb iu">表1 </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4a1c6f46a0c0173f177ed50ac4e172e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*Pmazx4130U7dw8LIlzDkcQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">表1: </strong> Whisper在各种数据集上优于SOTA wav2vec模型(<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="4980" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从<strong class="lb iu">W</strong>ord<strong class="lb iu">E</strong>error<strong class="lb iu">R</strong>ate(或<strong class="lb iu">WER)——</strong>越少越好。这里的任务是转录。<em class="lv"> Wave2vec </em>在LibriSpeech数据集上进行了专门的训练和微调，而<em class="lv"> Whisper </em>则没有。</p><p id="7b11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="lv"> LibriSpeech </em>数据集上，两个模型是相等的。然而，<em class="lv"> Whisper </em>在所有其他数据集上都远远超过<em class="lv"> Wav2vec </em>！</p><h2 id="6314" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">英语翻译评估</h2><p id="fc48" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">同样，作者在翻译任务中将<em class="lv"> Whisper </em>与其他SOTA模特进行了比较。结果如<strong class="lb iu">表2 </strong>所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/dffb46ad6d536b79a01859a891ec3ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*bWAFBcIz1PYqyD4ARf75kg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">表2: </strong>耳语模特与其他SOTA模特在音频到英语翻译上的BLEU评分(<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="e4d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了低资源设置外，<em class="lv"> Whisper </em>的表现优于其他所有机型。平均而言，<em class="lv">耳语</em>获得更高的BLEU分数。记住<em class="lv"> Whisper </em>是在零射击配置中使用的——其他模型已经使用各自的数据集进行了充分的训练。</p><p id="0bfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，由于<em class="lv"> Whisper </em>的目的是很好地进行归纳，作者想要测试<em class="lv"> Whisper </em>如何处理噪声数据。结果如<strong class="lb iu">图3所示:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c26cc15378f8603b1a52b7d7a7fbe816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*nddj0-HPmnt8hf1mwZUBQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oe">图3:</strong>Whisper的WER与其他添加了噪声的模型(<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="6601" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些发现相当有趣:</p><p id="afb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NVIDIA STT的<em class="lv">型号在低噪音方面表现最佳，但<em class="lv"> Whisper </em>可以更好地扩展。最终，<em class="lv">耳语</em>在高噪音下胜过所有机型。该分析还展示了<em class="lv"> Whisper的</em>卓越的泛化能力。</em></p><h1 id="cf90" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">如何使用Whisper —编码示例</h1><p id="7554" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">接下来，我们将展示如何使用<strong class="lb iu"> HugginFaces库来使用<em class="lv"> Whisper </em>。</strong>完整的例子可以在<a class="ae ky" href="https://jovian.ai/nkafr/whisper-openai" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="a0cd" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">第一步:安装<code class="fe of og oh oi b"><a class="ae ky" href="https://ffmpeg.org/" rel="noopener ugc nofollow" target="_blank">ffmpeg</a></code></h2><p id="e9d1" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们从安装依赖项开始。首先，我们需要命令行工具<code class="fe of og oh oi b"><a class="ae ky" href="https://ffmpeg.org/" rel="noopener ugc nofollow" target="_blank">ffmpeg</a></code></p><pre class="kj kk kl km gt oz oi pa pb aw pc bi"><span id="2071" class="ol mm it oi b gy pd pe l pf pg"><strong class="oi iu"># on Ubuntu or Debian</strong><br/>sudo apt update &amp;&amp; sudo apt install ffmpeg<br/><br/><strong class="oi iu"># on MacOS using Homebrew (https://brew.sh/)</strong><br/>brew install ffmpeg<br/><br/><strong class="oi iu"># on Windows using Scoop (https://scoop.sh/)</strong><br/>scoop install ffmpeg</span></pre><p id="deb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们的演示将在<strong class="lb iu"> Colab </strong>上运行，所以我们选择第一个选项。</p><h2 id="7527" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">第二步:下载Whisper</h2><p id="32dd" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">接下来，用<code class="fe of og oh oi b">pip</code>安装<em class="lv">耳语</em>。该命令将总是从<em class="lv"> OpenAI </em>库下载最新版本。</p><pre class="kj kk kl km gt oz oi pa pb aw pc bi"><span id="01ba" class="ol mm it oi b gy pd pe l pf pg">pip install git+https://github.com/openai/whisper.git</span></pre><p id="afb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在已经完成了<em class="lv"> Whisper </em>及其依赖项的安装。</p><h2 id="b65c" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">步骤3:下载样本音频文件</h2><p id="fb86" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们将使用来自archive.com公共领域的<strong class="lb iu">葛底斯堡演讲</strong>音频文件<strong class="lb iu">【5】</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="1927" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后用<strong class="lb iu"> <em class="lv">耳语</em> CLI </strong>转录我们的音频文件。就这么简单！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="9211" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出还包括每个数据段的时间戳。</p><p id="ad6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该命令还创建了3个文件:<code class="fe of og oh oi b">gettysburg_johng_librivox.m4b.vtt gettysburg_johng_librivox.m4b.srt</code>和<code class="fe of og oh oi b">gettysburg_johng_librivox.m4b.txt</code>。其中包含各种格式的字幕和片段。</p><p id="ddff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，<em class="lv"> CLI </em>命令使用<code class="fe of og oh oi b">small</code>型号版本。关于参数和如何处理命令的更多信息，请看一下<a class="ae ky" href="https://github.com/openai/whisper/blob/main/whisper/transcribe.py" rel="noopener ugc nofollow" target="_blank">原始脚本</a>。</p><h2 id="8d0d" class="ol mm it bd mn om on dn mr oo op dp mv li oq or mx lm os ot mz lq ou ov nb ow bi translated">翻译示例</h2><p id="d77c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><em class="lv">耳语</em>可以处理多种语言，翻译成英文。我们来看一个例子，不过这次我们用<em class="lv">耳语</em>搭配<strong class="lb iu"> Python </strong>。</p><p id="d9aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将从Youtube[6]下载一段希腊语视频，并将其转换为音频文件。我们需要安装<code class="fe of og oh oi b">pytube</code>库:</p><pre class="kj kk kl km gt oz oi pa pb aw pc bi"><span id="9be2" class="ol mm it oi b gy pd pe l pf pg">!pip install pytube</span></pre><p id="db73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们将要转录的视频——它包含了希腊人最著名的演讲！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pj pi l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="ec02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从转录音频开始。我们将使用<code class="fe of og oh oi b">large</code>版本来提高精确度:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="a4ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了几处差异之外，这份抄本非常准确。此外，请注意，音频非常嘈杂——这是在拥挤的体育场发表的演讲，人们在鼓掌、吹口哨等等。</p><p id="ee11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将同一个音频文件传递给<em class="lv">耳语</em>进行翻译:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="ef6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">翻译几乎无可挑剔。有趣的是，在英语翻译过程中，希腊语中的一些差异被修正了。</p><p id="bbe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，<em class="lv"> Whisper </em>更适应英语——这并不奇怪，83%的训练数据集是为英语保留的。</p><h1 id="fb8d" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">结束语</h1><p id="dd6f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">耳语是一个非凡的模型，也是人工智能社区的一个里程碑。</p><p id="bbf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以人类水平的表现转录97种语言的音频文件的能力是无可比拟的。这一惊人的成功源于<em class="lv"> Whisper </em>采用的新颖方法(以及<em class="lv"> OpenAI </em>的许多其他模式)。</p><p id="cbfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法着重于学习如何根据任务不可知的信息归纳出新的任务。此外，这是人类感知的工作方式:人类只需要几个例子就可以根据他们过去的经验对以前看不见的物体进行正确的分类。</p><h1 id="0f17" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">感谢您的阅读！</h1><p id="4aac" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我每个月写一篇关于AI的有影响力的论文的深度分析。<br/> <strong class="lb iu">保持连接！</strong></p><ul class=""><li id="02d1" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated">订阅我的<a class="ae ky" href="https://towardsdatascience.com/subscribe/@nikoskafritsas" rel="noopener" target="_blank">简讯</a>！</li><li id="382a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated">在Linkedin上关注我！</li><li id="ce1e" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><a class="ae ky" href="/@nikoskafritsas/membership" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">加入介质</strong> </a> <strong class="lb iu">！</strong>(附属链接)</li></ul><h1 id="9698" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">参考</h1><ol class=""><li id="eec2" class="ni nj it lb b lc nd lf ne li pk lm pl lq pm lu oc no np nq bi translated">亚历克·拉德福德等<a class="ae ky" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">从自然语言监督中学习可转移的视觉模型</em></a><em class="lv">(2021年2月)</em></li><li id="bbcf" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">亚历克·拉德福德等人<a class="ae ky" href="https://cdn.openai.com/papers/whisper.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模弱监督的鲁棒语音识别</a><em class="lv">(2022年9月)</em></li><li id="e404" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">A.瓦斯瓦尼等人<em class="lv"> </em> <a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="lv">关注就是你需要的一切</em></a><em class="lv">(2017年6月)</em></li><li id="6a74" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">A.Baevski等人<a class="ae ky" href="https://arxiv.org/pdf/2006.11477.pdf" rel="noopener ugc nofollow" target="_blank">语音表征的自我监督学习框架</a> <em class="lv"> (2020) </em></li><li id="64bc" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated">林肯葛底斯堡演说<a class="ae ky" href="https://archive.org/details/gettysburg_johng_librivox" rel="noopener ugc nofollow" target="_blank">https://archive.org/details/gettysburg_johng_librivox</a><br/>许可:公共领域</li><li id="992a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu oc no np nq bi translated"><a class="ae ky" href="https://www.youtube.com/c/pasokwebtv" rel="noopener ugc nofollow" target="_blank"> PASOKwebTV </a>频道</li></ol><h1 id="0f62" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">附录</h1><p id="c058" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated"><strong class="lb iu">【A】梅尔频谱图:<br/> </strong>使用<strong class="lb iu">傅立叶变换</strong>将每个周期信号从t <em class="lv">时域</em>转换到<em class="lv">频域</em>。换句话说，我们创建了信号的<strong class="lb iu">频谱。</strong>但是<strong class="lb iu"> </strong>如果我们要对一个非周期信号进行变换呢？</p><p id="835f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过对信号的几个加窗段执行FT来计算几个<strong class="lb iu">频谱</strong>。我们将它们“缝合”在一起，这叫做<a class="ae ky" href="https://www.mathworks.com/help/dsp/ref/stft_output.png" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">声谱图</strong> </a> <strong class="lb iu">。</strong></p><p id="216f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Mel </strong> <strong class="lb iu">谱图</strong>是使用<em class="lv"> Mel标度的特殊<strong class="lb iu">谱图</strong>。</em>在<em class="lv"> Mel音阶</em>中，音高被转换成一个新的音阶，这样音高声音中的相等距离与听者的距离相等。例如，人耳可以分辨3k和3.5k Hz之间的差异，但却很难分辨11k和11，5k Hz之间的差异，即使这两对相差相同的值(0.5 Hz)。这就是我们使用<em class="lv">梅尔标度的原因。</em></p><p id="9fc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">【B】分布偏移:<br/> </strong>分布偏移是一个模型训练的数据随着时间的变化而发生的现象。因此，久而久之，模型的效率降低，预测变得不准确。 </p><p id="5ae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种现象经常出现在生产中的机器学习系统中。例如，用户的行为或其他外部因素可能会随着时间而改变——这将对部署的模型产生负面影响。</p><p id="6af2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时，<em class="lv">分布转移</em>与术语<em class="lv">‘概念漂移’互换使用。然而，它们并不相同。</em></p></div></div>    
</body>
</html>