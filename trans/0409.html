<html>
<head>
<title>How to Train an AI to Play Any Game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练人工智能玩任何游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-an-ai-to-play-any-game-f1489f3bc5c#2022-02-16">https://towardsdatascience.com/how-to-train-an-ai-to-play-any-game-f1489f3bc5c#2022-02-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a697" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用强化学习技术</h2></div><blockquote class="kf kg kh"><p id="de84" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这是一个关于如何使用强化学习训练人工智能玩任意视频游戏的简短指南。它一步一步地展示了如何设置您的自定义游戏环境，并利用<a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">稳定基线3 </a>库来训练AI。我想让这个指南易于访问，所以给出的代码没有完全优化。你可以在我的<a class="ae lf" href="https://github.com/guszejnovdavid/custom_game_reinforcement_learning/blob/main/custom_game_reinforcement_learning.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到源码。</p></blockquote><h1 id="327e" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">游戏人工智能的强化学习</h1><p id="7288" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">不同于其<em class="kk">监督的</em>和<em class="kk">非监督的</em>对应物，强化学习(RL)不是关于我们的算法从静态数据集学习一些潜在的真理，而是与它的环境交互以最大化奖励函数(非常类似于动物在现实生活中如何用零食训练)。这使得RL可能是机器学习最有前途的领域之一，有望通过能够处理复杂任务(如驾驶汽车或控制机器人的运动)的人工智能来彻底改变我们的生活。RL目前的一个应用是训练人工智能玩电脑游戏(参见<a class="ae lf" href="https://deepmind.com/research" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>中的一些例子)。</p><p id="577f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">由于几个公开可用的强化学习包，现在即使是Python编程新手也可以为任意视频游戏训练AI。然而，我们应该记住，随着游戏变得越来越复杂，训练所需的计算资源会迅速增加。在这个项目中，我们使用了<a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">稳定基线3 </a>，这是一个标准化的库，它提供了基于<a class="ae lf" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的RL算法的简单实现。</p><p id="8fed" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">在开始本指南之前，您需要安装稳定基线3及其依赖项，详情请参见<a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/guide/install.html" rel="noopener ugc nofollow" target="_blank">官方安装指南</a>。</p><h1 id="e3ff" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">设置我们的游戏环境</h1><p id="5fd7" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">虽然稳定基线包有很多内置的游戏环境(例如，<a class="ae lf" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-rl-agents-in-stable-baselines3-is-easy-9d01be04c9db" rel="noopener">雅达利游戏</a>)，但本指南的目标是在任意游戏上训练AI，例如我们自己设计的视频游戏。作为一个例子，我们将使用经典<a class="ae lf" href="https://en.wikipedia.org/wiki/Snake_(video_game_genre)" rel="noopener ugc nofollow" target="_blank">蛇</a>的修改版本，我们在中间插入两个额外的墙壁。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/ea21356b1307ff7c394de36d31360e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*J8wsn-55hSSWnolIcwWKdQ.png"/></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">我们的贪吃蛇游戏的初始设置；作者图片</p></figure><p id="136f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">因此，首先我们需要创建一个表示我们游戏的类，训练算法可以与之交互。我们可以通过OpenAI Gym基类<code class="fe ms mt mu mv b">gym.env</code>的类继承来做到这一点。您可以在<a class="ae lf" href="https://github.com/guszejnovdavid/custom_game_reinforcement_learning/blob/main/custom_game_reinforcement_learning.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到我们的Snake的详细代码，但是概括地说，自定义环境应该遵循以下格式:</p><pre class="mh mi mj mk gt mw mv mx my aw mz bi"><span id="5bd1" class="na lh iq mv b gy nb nc l nd ne">class custom_game(gym.Env):<br/>    def __init__(self, grid_size=12):<br/>        """Define possible actions and observation data format"""<br/>        ...<br/>        self.action_space = ... #define possible actions<br/>        self.observation_space = ... #format of observations</span><span id="168d" class="na lh iq mv b gy nf nc l nd ne">    def _get_obs(self):<br/>        """Calculates the observations (input) from current state"""<br/>        ...<br/>        return observation #in format of self.observation_space</span><span id="c22f" class="na lh iq mv b gy nf nc l nd ne">    def reset(self):<br/>        """Resetting the environment (i.e., starting a new game)"""<br/>        ....<br/>        return self._get_obs()<br/>    <br/>    def step(self, action):<br/>        """Evolve environment in response to action and calculate reward"""<br/>        ...<br/>        return  self._get_obs(), reward, done, info<br/><br/>    def render(self):<br/>         """Optional, in case we want to render the current state"""</span></pre><p id="0e62" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">现在，让我们检查一下每个方法，并设置它们来玩我们的游戏。首先我们需要决定几件事:</p><ul class=""><li id="06de" class="ng nh iq kl b km kn kp kq ma ni mc nj me nk le nl nm nn no bi translated">玩家可能的动作有哪些(即<code class="fe ms mt mu mv b">self.action_space</code>)？</li><li id="80d8" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated">我们如何为算法编码游戏的当前状态(即<code class="fe ms mt mu mv b">self.observation_space</code>)？换句话说，玩家在玩游戏时“看到”了什么。</li><li id="ec69" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated">奖励方案是什么(即采取某项行动的<code class="fe ms mt mu mv b">reward </code>值)？</li></ul><p id="1452" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">在接下来的部分，我们将详细讨论如何为我们的游戏设置这些。</p><h2 id="47f3" class="na lh iq bd li nu nv dn lm nw nx dp lq ma ny nz ls mc oa ob lu me oc od lw oe bi translated">行为空间</h2><p id="060f" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">总的来说，我们应该努力使动作和观察空间尽可能的简单和小，这样可以大大加快训练的速度。对于贪吃蛇游戏，玩家每走一步，蛇只有三种选择:<em class="kk">直走</em>、<em class="kk">右转</em>和<em class="kk">左转</em>，我们可以用整数0、1、2来编码</p><pre class="mh mi mj mk gt mw mv mx my aw mz bi"><span id="03a2" class="na lh iq mv b gy nb nc l nd ne">self.action_space = spaces.Discrete(3)</span></pre><h2 id="a976" class="na lh iq bd li nu nv dn lm nw nx dp lq ma ny nz ls mc oa ob lu me oc od lw oe bi translated">了望处</h2><p id="1252" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">对于观察，我们需要交流游戏的当前状态。一个简单的方法是将其表示为一个网格，其中每个像素的值表明它是<em class="kk">空</em>、<em class="kk">蛇</em>、<em class="kk">墙</em>还是<em class="kk">食物</em>。为了简单起见，在这个例子中我们将创建一个无符号整数网格(本质上是一张图片)，其中我们将每个像素的状态编码为0: <em class="kk">空</em>，1: <em class="kk">蛇</em>，2: <em class="kk">墙</em>和3: <em class="kk">食物</em>。然而，这个网格并没有向玩家提供游戏的完整状态，因为仅仅从一张图片来判断蛇的头部在哪里以及它往哪个方向走是不可能的。所以总的来说，我们的观察空间应该是这样的:</p><pre class="mh mi mj mk gt mw mv mx my aw mz bi"><span id="34b2" class="na lh iq mv b gy nb nc l nd ne">self.observation_space = gym.spaces.Dict(<br/>            spaces={<br/>                "position": gym.spaces.Box(low=0, high=(self.grid_size-1), shape=(2,), dtype=np.int32),<br/>                "direction": gym.spaces.Box(low=-1, high=1, shape=(2,), dtype=np.int32),<br/>                "grid": gym.spaces.Box(low = 0, high = 3, shape = (self.grid_size, self.grid_size), dtype=np.uint8),<br/>            })</span></pre><p id="e958" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">我们用两个2D向量来编码蛇的头部位置和它当前的方向。为了简单起见，我们将它们存储在一个字典中(<code class="fe ms mt mu mv b">“position”</code>代表蛇头，<code class="fe ms mt mu mv b">“direction”</code>代表蛇头，<code class="fe ms mt mu mv b">“grid” </code>代表玩家在屏幕上看到的图片)。注意，一般来说，更有效的是进一步编码观察空间，使得它由单个向量或矩阵来表示。最后我们设置了<code class="fe ms mt mu mv b">_get_obs()</code>来将游戏的当前状态转换成上面的格式。</p><h2 id="f379" class="na lh iq bd li nu nv dn lm nw nx dp lq ma ny nz ls mc oa ob lu me oc od lw oe bi translated">奖励计划</h2><blockquote class="of"><p id="72d5" class="og oh iq bd oi oj ok ol om on oo le dk translated">也许任何RL实践中最关键也是最不简单的部分就是设计一个好的奖励方案。选择一个“好”的方案会使训练速度加快很多，而选择一个“坏”的方案会阻止算法学会正确地玩游戏。</p></blockquote><p id="0f63" class="pw-post-body-paragraph ki kj iq kl b km op jr ko kp oq ju kr ma or ku kv mc os ky kz me ot lc ld le ij bi translated">对于我们的贪吃蛇游戏，我们基本上期望我们的AI做两件事:1)吃食物2)避免撞到墙壁或自身。</p><p id="f51f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">我们试试下面这个方案:吃东西50分，每走一步没撞墙没撞到蛇尾巴加1分。我们还定义了避免无限博弈的最大步骤数。在这种情况下，我们的RL算法将快速收敛到下面的解决方案。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi ou"><img src="../Images/53ffe2c1fa1de285f1e50d43de8cf9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Y2ycRHEIY2gQFV15Bex-cQ.gif"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">因采取奖励措施而不碰壁而产生的解决方案；作者图片</p></figure><p id="4c1d" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">如你所见，算法陷入了局部奖励最大值。一个更好的奖励方案是只奖励更接近食物的步骤。这里必须小心，因为蛇仍然只能学会绕圈，在接近食物时获得奖励，然后转身再回来。为了避免这种情况，我们还必须对远离食物的行为给予等价的惩罚，换句话说，我们需要确保闭环上的净回报为零。我们还需要引入对撞墙的惩罚，因为有些情况下，蛇只能通过撞墙来接近食物，我们不想奖励这种情况。</p><h1 id="b8f8" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">超参数</h1><p id="e7c3" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">大多数RL算法相当复杂并且难以实现。幸运的是，稳定基线3已经实现了几个我们可以使用的最先进的算法。在这个具体的例子中，我们将使用<a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html" rel="noopener ugc nofollow" target="_blank">近似策略优化或简称PPO </a>。虽然我们不需要知道算法如何工作的细节(如果感兴趣，请参见这个<a class="ae lf" href="https://www.youtube.com/watch?v=5P7I-xPq8u8" rel="noopener ugc nofollow" target="_blank">解释视频</a>)，但我们需要对它的超参数是什么以及它们做什么有一个基本的了解。幸运的是，PPO只有其中的几个(参见<a class="ae lf" href="https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe" rel="noopener">这篇文章</a>中的完整指南)，我们将使用以下内容:</p><ul class=""><li id="6961" class="ng nh iq kl b km kn kp kq ma ni mc nj me nk le nl nm nn no bi translated"><code class="fe ms mt mu mv b">learning_rate</code>:设置策略更新的步长，<a class="ae lf" href="https://en.wikipedia.org/wiki/Q-learning#Learning_rate" rel="noopener ugc nofollow" target="_blank">与其他RL方案</a>相同。将其设置得太高会阻止您的算法找到正确的解决方案，甚至会将策略推向永远无法恢复的方向。把它设置得太低没有这样的风险，但是它使训练花费更长的时间。一个常见的技巧是在训练期间使用调度器功能来调整它(例如，从高开始并减少)。</li><li id="12bb" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated"><code class="fe ms mt mu mv b">gamma</code>:未来奖励的<a class="ae lf" href="https://en.wikipedia.org/wiki/Q-learning#Discount_factor" rel="noopener ugc nofollow" target="_blank">折扣系数</a>，介于0(仅与即时奖励相关)和1(未来奖励与即时奖励价值相同)之间。为了鼓励代理学习长期策略，最好将其保持在0.9以上。</li><li id="ba3d" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated"><code class="fe ms mt mu mv b">clip_range</code>:PPO的一个重要特性，粗略地说，它设定了当我们更新我们的政策时，一个行动的概率不能改变超过一个因子<code class="fe ms mt mu mv b">1+-clip_range</code>。这是为了确保我们不会在训练步骤之间戏剧性地改变政策。减少它有助于在以后的训练阶段对模型进行微调。</li><li id="b97f" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated"><code class="fe ms mt mu mv b">ent_coef</code>:熵系数对过于确定的政策不利。本质上，它的值越高，就越鼓励算法探索不同的、非最优的行动，这可以帮助方案脱离局部回报最大值。</li></ul><p id="d1ea" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">一般来说，从默认的超参数开始是值得的，因为这些参数被选择为适合大多数任务。有时将学习率设置为随着训练进度而降低是一个好主意，这样我们就可以看到较低的学习率是否可以防止停滞。此外，由于没有明确的方法来为任意问题(例如，定制游戏)设置这些参数，我们需要进行实验。幸运的是，stable-baselines3允许我们在训练方法中添加回调函数，以评估训练中的模型并保存最佳模型(参见<code class="fe ms mt mu mv b"><a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/common/monitor.html" rel="noopener ugc nofollow" target="_blank">Monitor</a></code>和<code class="fe ms mt mu mv b"><a class="ae lf" href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html?highlight=EvalCallback#evalcallback" rel="noopener ugc nofollow" target="_blank">EvalCallback</a></code>)。</p><h1 id="cc57" class="lg lh iq bd li lj lk ll lm ln lo lp lq jw lr jx ls jz lt ka lu kc lv kd lw lx bi translated">培养</h1><p id="210c" class="pw-post-body-paragraph ki kj iq kl b km ly jr ko kp lz ju kr ma mb ku kv mc md ky kz me mf lc ld le ij bi translated">接下来的步骤是为一些预先确定的<code class="fe ms mt mu mv b">N</code>步骤进行训练，然后亲眼看看算法是如何运行的，然后从性能最好的参数中重新开始潜在的新参数。我们在这里绘制了不同训练时间的奖励，要获得更高级的反馈，您可以使用<a class="ae lf" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>。总的来说，我们希望游戏结束时的总奖励随着我们训练模型而增加。</p><p id="ee11" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">以下是我在这个项目中发现的一些可能有帮助的观察结果:</p><ul class=""><li id="0409" class="ng nh iq kl b km kn kp kq ma ni mc nj me nk le nl nm nn no bi translated">很难估计一个问题需要多少个训练步骤，所以首先检查一下增加更多的训练步骤是否会提高回报是值得的。</li><li id="6ac2" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated">制作一部关于人工智能如何玩游戏的电影是值得的，这样你就可以看到它设法学习了什么策略，你可以用它来调整奖励计划(例如，在我们的情况下，我们如何需要停止奖励步骤以避免无限循环)。</li><li id="dff4" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated">如果奖励最初增加，然后随着训练步数的增加而逐渐减少，那么<code class="fe ms mt mu mv b">learning_rate </code>很有可能过高。</li><li id="a3ca" class="ng nh iq kl b km np kp nq ma nr mc ns me nt le nl nm nn no bi translated">奖励也可能停滞不前，这通常是算法陷入局部最大值的迹象。在我们的蛇问题中，当食物在墙的另一边，蛇靠近墙，然后无限期地停留在那里，因为远离食物会受到惩罚。在这些情况下，临时增加<code class="fe ms mt mu mv b">ent_coef</code>允许算法探索并摆脱这个局部最大值。</li></ul><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi oz"><img src="../Images/57bee6d4f66870993ac3e14d48e67f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73tk-ZDL-rL5api-jO6yEA.png"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">初始训练阶段的奖励值；作者图片</p></figure><p id="4a89" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">经过足够多的步骤后，我们训练的算法收敛到某个奖励值。我们可以结束训练或者尝试微调参数，继续训练。请注意，达到最大可能奖励所需的训练步骤数量很大程度上取决于您的问题、奖励方案和超参数，因此在训练前花一些时间优化它们总是一个好主意。对于我们的例子，当人工智能变得相当擅长在迷宫中导航时，我们停止训练。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="ov ow di ox bf oy"><div class="gh gi ou"><img src="../Images/1109dd2b46b0303b39621ab8443591f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CjrKNTsKKTJkPnXukLVuCw.gif"/></div></div><p class="mo mp gj gh gi mq mr bd b be z dk translated">训练有素的AI玩蛇。它学会了寻找食物，学会了如何“摆动”以避免与自己相撞。它还需要学会避免困住自己，撞到尾巴。图片作者。</p></figure><p id="ee9c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr ma kt ku kv mc kx ky kz me lb lc ld le ij bi translated">如果你有兴趣为你自己的游戏训练一个AI，你可以修改我在这个例子中使用的代码(<a class="ae lf" href="https://github.com/guszejnovdavid/custom_game_reinforcement_learning/blob/main/custom_game_reinforcement_learning.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)。</p></div></div>    
</body>
</html>