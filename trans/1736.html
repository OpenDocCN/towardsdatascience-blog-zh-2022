<html>
<head>
<title>How to Select the Best Number of Principal Components for the Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为数据集选择最佳数量的主成分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d#2022-04-24">https://towardsdatascience.com/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d#2022-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="df3a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你应该遵循的六种方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/943de7cfeb329c487caccdf118e27106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6Y2jnzI03sIMpBuR0tGiA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">兰迪·法特在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b3f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对数据集应用主成分分析(PCA)时，选择最佳数量的主成分是主要挑战。</p><p id="7cc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在技术术语中，选择主成分的最佳数量被称为一种超参数调整过程，在该过程中，我们为Scikit-learn <strong class="lb iu"> PCA() </strong>类中的超参数<strong class="lb iu"> n_components </strong>选择最佳值。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e0b7" class="ma mb it lw b gy mc md l me mf">from sklearn.decomposition import PCA<br/>pca = PCA(<strong class="lw iu">n_components=?</strong>)</span></pre><p id="1280" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，当我们对具有<strong class="lb iu"> p </strong>个变量的原始数据集应用PCA得到具有<strong class="lb iu"> k </strong>个<strong class="lb iu">和</strong>个变量(主成分)的变换数据集时，<strong class="lb iu"> n_components </strong>等于<strong class="lb iu"> k </strong>，其中<strong class="lb iu"> k </strong>的值远小于<strong class="lb iu"> p </strong>的值。</p><p id="bce4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<strong class="lb iu"> n_components </strong>是一个<a class="ae ky" href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener">超参数</a>，它不从数据中学习。在运行<strong class="lb iu"> PCA( </strong>)功能之前，我们必须手动指定其值(调整超参数)。</p><p id="359a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为<strong class="lb iu"> n_components </strong>选择最佳数量的背后没有什么神奇的规则。这取决于我们真正想从PCA得到什么。一些视觉检查和领域知识也可能有助于推断出<strong class="lb iu"> n_components </strong>的正确值。</p><p id="40f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的文章专门讨论六种有效的方法，它们将帮助您为数据集选择最佳数量的主成分。</p><p id="7760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><h2 id="c8aa" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法1:如果你做PCA的唯一目的是为了数据可视化，你应该选择2或3个主成分。</h2><p id="dbaf" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">PCA对于数据可视化非常有用。通过主成分分析可以实现高维数据的可视化。</p><p id="1258" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们只熟悉2D和3D图，我们应该将高维数据转换成2维或3维数据，以便在2D或3D图上可视化。这可以通过PCA来实现。</p><p id="b290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于2D图，我们需要选择2个主成分。</p><p id="8840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于3D绘图，我们需要选择3个主成分。</p><p id="2d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中，您可以看到30维<em class="nc">乳腺癌</em>数据的可视化示例。由于数据集包含30个变量，其维数为30。我们无法在30D绘图中绘制30维数据，因为我们无法想象它是如何存在的。为此，我们将PCA应用于数据集，并选择2或3个主成分，这些主成分可以绘制在2D或3D图上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/51803c7ce459c33998af57903413ac90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xn4GZeRgZOvG0CVNmxi0Dg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne">使用PCA对高维数据进行可视化</strong>(图片由作者提供)</p></figure><p id="fbde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些图大致代表了原始数据。这是因为当我们降低数据的维度时，我们丢失了原始数据中的一些方差。</p><h2 id="9e77" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法2:如果您希望在应用PCA后在数据中保留一个精确的方差，请为超参数<strong class="ak"> n_components指定一个介于0和1之间的浮点数。</strong></h2><p id="cf73" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这是为数据集选择最佳数量的主成分的最简单方法。例如，如果您希望在应用PCA后保留原始数据中85%的方差，您可以将浮点值0.85指定给超参数<strong class="lb iu"> n_components </strong>，如下所示。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="bbaf" class="ma mb it lw b gy mc md l me mf">from sklearn.decomposition import PCA<br/>pca = PCA(<strong class="lw iu">n_components=0.85</strong>)</span></pre><p id="7c29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，算法自动选择保持原始数据中85%方差的最佳数量的主成分。如果您想知道算法选择了多少个组件，请运行以下代码行。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="94b6" class="ma mb it lw b gy mc md l me mf">pca.n_components_</span></pre><p id="e5be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将返回一个等于所选组件数量的整数值。</p><h2 id="1790" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法3:绘制单个成分的解释方差百分比和所有主成分获得的总方差百分比。</h2><p id="b36f" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这是最先进、最有效的方法，可用于为数据集选择最佳数量的主成分。</p><p id="0a62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此方法中，我们创建了以下类型的绘图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4e86c03de76a3c1508232b5a60d2fd7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*9_6WDebJHRSZt8j0liJWDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne">主成分捕获的总方差的百分比</strong>(图片由作者提供)</p></figure><p id="0c0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">条的数量等于原始数据集中变量的数量。在此图中，每个条形显示单个组件的解释差异百分比，而阶跃图显示累积的解释差异百分比。</p><p id="cb78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看该图，我们可以很容易地决定应该保留多少组件。在本例中，只有前两个分量捕获了数据集中几乎所有的方差。因此，我们决定只选择前两个组件。</p><p id="7b3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要获取创建上述类型情节的Python代码，请参考我的《<a class="ae ky" href="https://rukshanpramoditha.medium.com/principal-component-analysis-18-questions-answered-4abd72041ccd" rel="noopener"> <strong class="lb iu">主成分分析—十八问答</strong> </a>》文章第15题。</p><p id="2998" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在R中，使用<strong class="lb iu"> factoextra </strong>库中的<strong class="lb iu"> fviz_eig() </strong>函数，只需运行一行代码，就可以创建类似类型的绘图！</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="faab" class="ma mb it lw b gy mc md l me mf">fviz_eig(pca_model, addlabels = TRUE, <br/>         linecolor = "Red", ylim = c(0, 50))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/9562193e3fbb19cc173716e0299b32cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*2l2vpkMkZBozFxNlcApN_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5bab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，每个条形也显示了单个组件的解释差异百分比。红线是一种碎石图。</p><h2 id="eaf3" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法4:创建碎石地块。</h2><p id="71e8" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们可以创建另一种类型的图来选择主成分的最佳数量，这就是<strong class="lb iu"> Scree图</strong>，它是定义特征向量(主成分)大小的特征值的可视化表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/42f12a8b95cc0ffcd715f28a7a173d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*tnoY86NOSV87pqBxvwHERA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ne">一个小阴谋</strong>(图片由作者提供)</p></figure><p id="c5a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Scikit-learn PCA()类中，可以使用下面一行代码获得特征值。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c8c1" class="ma mb it lw b gy mc md l me mf">pca.explained_variance_</span></pre><p id="7374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了创建上面的图，我们可以使用下面的代码块。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3489" class="ma mb it lw b gy mc md l me mf">import matplotlib.pyplot as plt<br/>plt.style.use("ggplot") </span><span id="4159" class="ma mb it lw b gy ni md l me mf">plt.plot(pca.explained_variance_, marker='o')<br/>plt.xlabel("Eigenvalue number")<br/>plt.ylabel("Eigenvalue size")<br/>plt.title("Scree Plot")</span></pre><p id="246c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们选择所有元件，直到在碎石图中发生弯曲的点。在上图中，弯曲发生在索引1处。因此，我们决定选择索引0和索引1处的组件(总共两个组件)。</p><h2 id="9279" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法五:遵循凯泽的<em class="nj"> </em>法则。</h2><blockquote class="nk nl nm"><p id="1d8d" class="kz la nc lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated"><em class="it">根据</em> <strong class="lb iu">凯泽</strong> <em class="it">法则，建议保留所有特征值大于1的分量。</em></p></blockquote><p id="4571" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用PCA()类的<code class="fe nq nr ns lw b">explained_variance_</code>属性可以得到特征值。</p><p id="05fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，您可以选择特征值大于1的元件。当遵循此规则时，最好将其与方法3中讨论的解释方差百分比图或方法4中讨论的碎石图结合起来。有时，略小于1的特征值(例如0.95)可能会捕捉到数据中的一些显著变化。因此，如果它在解释的方差百分比图中显示相对较高的柱线，最好也保留它。</p><h2 id="66a0" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">方法6:使用一个性能评估指标，例如RMSE(用于回归)或准确度分数(用于分类)。</h2><p id="3c0f" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">只有当您计划在应用PCA后对缩减(转换)的数据集执行回归或分类任务时，才能使用此方法。</p><p id="18f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用方法3中讨论的图，您可以选择主成分的初始数量，并获得缩减的(变换的)数据集。然后，您构建一个回归或分类模型，并通过RMSE或准确度分数来衡量其性能。然后，您稍微改变主成分的数量，再次构建模型并测量其性能得分。重复这些步骤几次后，您可以选择实际上给出最佳性能分数的最佳数量的主成分。</p><p id="b2aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，模型的性能分数还取决于其他因素。例如，它取决于分裂训练测试集的随机状态、数据量、类别不平衡、模式中的树的数量(如果模型是随机森林或类似的变体)、我们在优化期间设置的迭代次数、数据中的异常值和缺失值等。所以，使用这种方法的时候一定要小心！</p><h1 id="6fac" class="nt mb it bd mg nu nv nw mj nx ny nz mm jz oa ka mp kc ob kd ms kf oc kg mv od bi translated">摘要</h1><p id="28db" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">没有必要同时尝试所有这些方法。最有效的方法是使用方法3中创建的图。你也可以把德皇法则和那个情节结合起来。</p><p id="adf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，主成分的最佳数目取决于我们真正想从主成分分析中得到什么。如果我们做PCA的唯一目的是为了数据可视化，那么组件的最佳数量是2或3。如果我们真的想减少数据集的大小，最佳的主成分数要比原始数据集中的变量数少得多。</p><p id="6f11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当将PCA应用于数据集时，一个黄金法则总是存在的。</p><blockquote class="nk nl nm"><p id="384a" class="kz la nc lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated"><strong class="lb iu">选择最佳数量的主成分，同时尽可能保留原始数据中的方差。</strong></p></blockquote><p id="88e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您忽略了捕获数据中大量差异的关键组件，则在您对转换后的数据集执行回归、分类或其他可视化任务时，模型的性能得分将会降低。</p><p id="2301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用PCA不是一次性的过程。这是一个反复的过程。您需要多次运行PCA()函数。</p><p id="476b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，您必须用<code class="fe nq nr ns lw b">n_components=None</code>运行PCA()函数，这意味着我们暂时保留所有组件。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ab4c" class="ma mb it lw b gy mc md l me mf">from sklearn.decomposition import PCA<br/>pca_1 = PCA(<strong class="lw iu">n_components=None</strong>)</span></pre><p id="ba2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们创建方法3中讨论的图，并选择主成分的最佳数量(称为<strong class="lb iu"> <em class="nc"> k </em> </strong>)。最后，我们用<code class="fe nq nr ns lw b">n_components=k</code>再次运行PCA()函数</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d7f3" class="ma mb it lw b gy mc md l me mf">pca_2 = PCA(<strong class="lw iu">n_components=k</strong>)</span></pre></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="f47c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的文章到此结束。如果你对这篇文章有任何疑问，请在评论区告诉我。</p><p id="70ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="f7d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一篇文章再见！一如既往，祝大家学习愉快！</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h2 id="d5ac" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">接下来阅读(推荐)——我写的！</h2><p id="8e3d" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">了解主成分分析(PCA)的幕后工作原理。</p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">具有NumPy的协方差矩阵的特征分解</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">对于主成分分析(PCA)</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ks oo"/></div></div></a></div><p id="96f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一站式解决您关于PCA的大部分问题。</p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/principal-component-analysis-18-questions-answered-4abd72041ccd" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">主成分分析—回答了18个问题</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一站式解决关于PCA的大部分问题</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ks oo"/></div></div></a></div><p id="7814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主成分分析在降维中的应用。</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/rgb-color-image-compression-using-principal-component-analysis-fce3f48dfdd0"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">基于主成分分析的RGB彩色图像压缩</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">主成分分析在降维中的应用</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ks oo"/></div></div></a></div><p id="6924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个ML都是降维及其应用。让我们看看他们的行动吧！</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/11-different-uses-of-dimensionality-reduction-4325d62b4fa6"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">降维的11种不同用途</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">整个ML都是降维及其应用。让我们看看他们的行动吧！</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pf l oz pa pb ox pc ks oo"/></div></div></a></div><p id="4d74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解参数和超参数之间的区别。</p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">参数Vs超参数:区别是什么？</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">用4个不同的例子进行讨论</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="pg l oz pa pb ox pc ks oo"/></div></div></a></div></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h2 id="6afd" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">成为会员</h2><p id="83b0" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">如果你愿意的话，你可以注册成为会员，以获得我写的每一个故事的全部信息，我会收到你的一部分会员费。</p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="ph l oz pa pb ox pc ks oo"/></div></div></a></div><h2 id="80e1" class="ma mb it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">订阅我的电子邮件列表</h2><p id="dcee" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">订阅我的电子邮件列表，再也不会错过精彩的故事了。我一点击发布按钮，你就会在收件箱里收到每一篇文章。</p><div class="ol om gp gr on oo"><a href="https://rukshanpramoditha.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">每当鲁克山·普拉莫迪塔发表文章时，就收到一封电子邮件。</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">每当鲁克山·普拉莫迪塔发表文章时，就收到一封电子邮件。通过注册，您将创建一个中等帐户，如果您没有…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ox l"><div class="pi l oz pa pb ox pc ks oo"/></div></div></a></div><p id="5867" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">撰写人<a class="pj pk ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----287e64b14c6d--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫蒂塔</a><br/><strong class="lb iu">2022–04–24</strong></p></div></div>    
</body>
</html>