<html>
<head>
<title>Linear Regression: Modeling Oceanographic Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:海洋学数据建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-modelling-oceanographic-data-fdc3a98813bf#2022-06-09">https://towardsdatascience.com/linear-regression-modelling-oceanographic-data-fdc3a98813bf#2022-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e4c8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最重要的，被忽视的，可预测的回归模型。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a1d4f46c5f0a3f61bac92879a9ec0ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVifRR13QCD65JT5IR0S-A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归图像-作者</p></figure><p id="3238" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线性回归是一个简单而强大的工具，应该放在每个数据科学家的口袋里。在本文中，我将使用一个真实的例子来展示线性回归对真实数据的强大作用。</p><h2 id="e3a0" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">CalCOFI数据集</h2><p id="0d6f" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们将使用<a class="ae mp" href="https://www.kaggle.com/datasets/sohier/calcofi" rel="noopener ugc nofollow" target="_blank"> CalCOFI数据集</a> (CC BY 4.0 License)。该数据集包含了从1949年至今加利福尼亚州的全面海洋学测量数据。数据集包括水温、盐度、测量深度、氧气水平等要素。在本文中，我们将只使用水温和水的盐度功能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/30899fbef87d18b801d8cde0811d5ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PfnDYzjoyIbh7QS4P8R3Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">加州海岸——作者<a class="ae mp" href="https://unsplash.com/@karstenkoehn" rel="noopener ugc nofollow" target="_blank">卡斯滕·科恩</a></p></figure><h2 id="8975" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">介绍</h2><p id="fc04" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">首先，线性回归模型用于预测一个连续变量的值，基于另一个连续变量的值。简单线性回归是线性回归的最基本形式，其中映射这种关系的模型是直线。</p><p id="e693" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们从导入数据集开始，从CalCOFI数据集中抽取500个测量值的样本。在下图中，我们可以看到温度和盐度之间的关系。随着温度的升高，水的盐度降低。这表明变量之间存在负相关关系。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a4be20da4953f5f131b4b41bb0b03306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*-3r6ENapifeSM4Ig7-rhgg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据点—按作者</p></figure><p id="f325" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，假设我们得到了一个新的数据点，它有温度测量值，但没有盐度值。我们能根据温度来估计它的盐度值吗？(惊喜！是的，我们可以。)</p><p id="3860" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解决这个问题的方法有很多，但最常用的方法之一是画一条符合数据的线。通过这种方式，给定任何温度测量值，我们都能够提供其盐度的精确估计。</p><p id="2832" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以简单地画一条看起来不错的线，但要做出一个真正好的猜测，我们需要做一些数学工作。在下面的部分中，我们可以使用普通的最小二乘法来找到这条线的方程。</p><h2 id="acab" class="lr ls iq bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">OLS(普通最小二乘法)</h2><p id="b049" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">普通最小二乘法是用于寻找线性回归模型的未知参数的方法。它通过最小化观察标记和预测标记之间的差的平方和来选择这些参数。简单地说，就是选择最适合的路线。这条线的方程式如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/8db1c90b5b5558f6c24fb4fa8c004b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbLseF7FcwQSfSwRyRO9HA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单线性回归公式—作者</p></figure><p id="55aa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面这条线的方程有两个参数，B0和B1。B0对应于y轴截距，B1对应于直线的斜率。通过调整这两个参数，我们可以在2D空间中创建任何可能的线。</p><p id="2021" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了找到B0和B1的最佳值，我们可以使用以下OLS实现。该等式由矩阵X、向量Y和结果向量B组成。在我们输入数据并求解该等式后，向量B将包含最佳系数(B0，B1)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ecb702a8f74e6572cca82864612b2034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*wqoOU5TVMlHM3WpWIcRROg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">普通最小二乘公式—作者</p></figure><p id="6c5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的公式看起来很复杂，但是它的结构使得我们可以很容易地增加代码行的复杂性，而不需要添加新的术语(我们将在本文后面这样做)。</p><p id="fb13" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，使用下面的代码块，我们可以做到这一点。如果你相信我的计算机能正确地做矩阵和向量乘法，那么我们就能得到一条直线来拟合数据。这显示在代码块下面。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/78e5fde0d364c90a7021e7f802ceee01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*vSTI24CN74t_5dDQutZZxw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">简单线性回归—作者</p></figure><p id="30e4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线条看起来不错，但在执行模型评估时使用损失函数很重要。我们将使用的损失函数称为MSE(均方误差)。这衡量数据点离回归线有多远/多近。换句话说，我们取残差的平方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/29b61565f0f44081529596ffb0eb432d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jtk2F7ha_Dlkm8koWLgM7g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">均方差公式—作者</p></figure><p id="2f56" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可能会想，为什么我们要对残差求平方？</p><p id="5900" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们这样做只是为了惩罚远离回归线的数据点。如果我们取平均绝对误差，那么使近点更近或远点更近对损失函数有相同的影响。如果我们采用MSE，那么我们会对外围数据点增加惩罚。惩罚这些离群点是一把双刃剑，因为一个极端的离群点会对MSE产生重大影响。</p><p id="41d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们评估我们创建的简单线性回归模型时，我们得到的MSE值约为1.5355。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/252cce1c6d09b2136c4af40aae6912bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*mOcv25wEan8-a_RfbWEkEg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">有损失的简单线性回归-作者</p></figure><p id="f781" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">线性回归(非简单)</strong></p><p id="2bbd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用直线拟合数据效果很好，但是很明显，非直线将最适合数据。值得庆幸的是，我们可以很容易地在OLS公式的X矩阵中添加列来做到这一点。我们向矩阵中添加一个新列，它是所有温度值的平方。我们现在正在用线性回归拟合一个输入非线性的模型。如果我们添加一个新的列，这将是温度，然后是temperature⁴，temperature⁵，等等。</p><p id="6b4d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下函数有助于使用Matplotlib以递增的度数绘制模型。多项式_要素()函数将要素输入排列成一个Numpy数组(X矩阵),然后，plot_regression()函数使用OLS函数计算最佳β值，计算MSE，并绘制结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/0a7def9631adaca333fc8d3a9e202361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*-tt4wtbDLhcmdAPOU02XAw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">2度模型—按作者</p></figure><p id="18b2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个型号更合适！简单地将数据点的平方值相加改进了简单的线性回归模型。如果我们不断向X矩阵中添加越来越多的列，那么我们将越来越好地拟合数据。</p><p id="9e49" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">小心点。我们增加线条的复杂度越多，过度拟合训练数据的风险就越高。如果我们得到一个新的数据点样本，我们希望确保模型也能很好地适应这些数据点。我们需要选择一个模型，该模型选取数据的一般模式，而不是训练数据的随机模式。解决这个问题的一个简单方法是使用一个验证数据集。我们可以使用这个数据集在“看不见的数据”上测试模型性能。我们还将设置一个阈值，MSE必须达到该阈值才能继续增加复杂性。在我们的例子中，我们将不断增加输入特征的程度，直到MSE不增加0.02，然后我们将停止训练。这是通过下面的代码块完成的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="99e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，最能概括新数据的模型是三阶线性回归模型。我们可以在下图中直观地看到增加线的复杂性是如何影响模型的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e72d823aa55b6f490f781ed9ad93de53.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*LjefO_qjtC0m0DzPFT6kkw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型进展-按作者</p></figure><p id="b3fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后我们可以画出最佳模型，看看它在验证集上的表现如何。在下面的图中，我们可以看到模型的MSE验证分数约为0.70，训练MSE分数约为0.75。我们可以说这个模型对新数据的推广效果很好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/643b641f1926038034a9fef7ddc5891d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*IV_ca2pReanLCVkJ3BIN4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最终3级模型—作者</p></figure><p id="59d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">关闭思路</strong></p><p id="12c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种模型类型是机器学习的构建模块之一。模型背后的数学非常直观，它是一个可解释的模型，并且可以产生强有力的预测。在本文中，我们证明了我们可以使用线性回归来模拟水的盐度和温度之间的关系。</p><p id="5ac8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线性回归已被证明在预测学术界和工业界的价值方面是有用的。也就是说，线性回归只是冰山一角。还有其他模型类型可以在其预测中包含多个输入变量。这些模型的例子包括多元线性回归、决策树和随机森林。您可以使用这些模型中的一个，通过温度和纬度来预测盐度。是时候去另一个兔子洞探索这些模型了！</p><p id="92ab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章的代码可以在<a class="ae mp" href="https://github.com/brendanartley/Medium-Article-Code/blob/main/code/ordinary-least-squares-polynomial-regression.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2665" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">参考文献</strong></p><p id="b4a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在整篇文章中，我使用了各种资源来帮助撰写本文。谢谢大家的分享！</p><p id="3203" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mp" href="https://www.kaggle.com/datasets/sohier/calcofi" rel="noopener ugc nofollow" target="_blank">卡尔科菲数据集</a> —索希尔·戴恩</p><p id="09ba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mp" href="https://setosa.io/ev/ordinary-least-squares-regression/" rel="noopener ugc nofollow" target="_blank"> OLS直观解释</a> —维克多·鲍威尔，路易斯·乐和</p><p id="74c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mp" href="https://www.ibm.com/topics/linear-regression" rel="noopener ugc nofollow" target="_blank">什么是线性回归？</a> — IBM</p><p id="1390" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mp" href="https://www.oreilly.com/library/view/machine-learning-with/9781785889936/669125cc-ce5c-4507-a28e-065ebfda8f86.xhtml#:~:text=RMSE%20is%20the%20square%20root,penalizes%20larger%20errors%20more%20severely." rel="noopener ugc nofollow" target="_blank"> MSE vs RMSE </a> —奥莱利</p><p id="55d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae mp" rel="noopener" target="_blank" href="/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3"> 7种常见的回归算法</a> —张秀坤·波尔泽</p></div></div>    
</body>
</html>