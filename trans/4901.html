<html>
<head>
<title>Many Languages, One Deep Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多种语言，一个深度学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/many-languages-one-deep-learning-model-69201d02dee1#2022-11-01">https://towardsdatascience.com/many-languages-one-deep-learning-model-69201d02dee1#2022-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="af6c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">多语言理解比你想象的容易！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0c1279255c3601872c38cd3f97daf97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMr3l9__a_nfIJ7T3PG_ZQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://unsplash.com/photos/YPgTovTiUv4" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="2166" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">我们如何让基于自然语言的深度学习系统理解多种语言的数据？</em>一种简单的方法是在每种期望的语言上训练一个单独的深度学习模型。但是，这种方法非常繁琐。<em class="lv">如果我们需要创建一个接收来自100多种不同语言的数据的系统，该怎么办？</em>承担训练100个深度学习模型的成本，对于大多数从业者来说是不可行的。</p><p id="ee97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，最近的深度学习研究表明，多语言理解可能比看起来更简单。在BERT [1]出现后不久，研究人员意识到双向转换器模型能够同时从多种语言中学习。这种方法使多种语言的深度学习变得简单——只需训练一个模型来处理所有语言。</p><blockquote class="lw lx ly"><p id="fb2e" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">“最近的发展表明，有可能建立通用的跨语言编码器，可以将任何句子编码到共享的嵌入空间中。”</p><p id="b034" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><em class="it"> —来自【2】</em></p></blockquote><p id="3265" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些多语言模型几乎与BERT相同。然而，他们的自我监督预训练是在包含来自100多种语言的文本数据的多语言语料库上进行的。尽管对于训练这些多语言模型存在一些额外的考虑，但是总体方法与训练单语BERT模型惊人地相似。</p><p id="f69e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多语言深度学习模型对于语言理解的主要好处有两个方面:</p><ul class=""><li id="6fd9" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">简单性:单一的模型(而不是每种语言的独立模型)更容易使用。</li><li id="b969" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><em class="lv">归纳转移</em>:多种语言的联合训练使得跨语言模式的学习成为可能，这有利于模型性能(特别是在低资源语言上)。</li></ul><p id="a4ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于深度学习实践者来说，这些模型非常有用，因为它们表现得惊人地好，并且不需要任何深入的语言理解。在这个概述的过程中，我将解释基于BERT的多语言模型的当前工作，包括对这种模型的行为/倾向的分析和对最近提出的模型的解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/68d72423ef70779fd2bd51ffaf1f1a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_E2fjcWFNMx5ECiBFq5Wg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用自我监督、屏蔽的语言建模目标对多语言BERT模型进行预训练(由作者创建)</p></figure><h1 id="54f4" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">多语言理解的基础</h1><p id="1e40" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在自然语言处理(NLP)领域，大量的研究工作已经投入到制作通用(单语)句子表示(例如，<a class="ae ky" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> FastText </a>、<a class="ae ky" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>、GloVe【8】等)中。).BERT [1]的出现——在我的<a class="ae ky" href="https://cameronrwolfe.substack.com/p/language-understanding-with-bert" rel="noopener ugc nofollow" target="_blank">上一篇文章</a>中解释过——彻底改变了这个领域，揭示了预先训练的双向变压器模型产生可微调的句子表示，以非常高的准确度解决语言理解任务。</p><p id="e1b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了单语方法，许多NLP研究人员已经研究了跨语言的单词或短语对齐问题[9，10]，机器翻译[11]和跨语言表示学习[12]。其核心是，这样的研究提出了这样一个问题:<em class="lv">有没有可能创建一个跨语言共享嵌入空间的通用句子编码器？</em></p><p id="82a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朝着这个方向发展，最近的研究发现，伯特风格、预先训练的变形金刚可以产生有用的多语言句子表示。通过简单地使BERT的预训练语料库多语言化，这些模型获得了对许多语言的体面理解，并可以进行微调以解决跨语言任务(例如，对多种语言的句子进行分类)。第一种方法叫做多语言BERT (mBERT ),实际上是与BERT同时提出的。</p><h2 id="1649" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">多语言BERT (mBERT)</h2><p id="4f25" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">最初的BERT出版物[1]没有提到多语言学习。然而，BERT 的<a class="ae ky" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">公开版实际上包含了一个多语种版本的BERT，名为(创造性地)</a><a class="ae ky" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">多语种BERT (mBERT) </a>。</p><p id="1833" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型与BERT几乎相同，唯一的区别是:</p><ul class=""><li id="699d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">多种语言的联合预培训(详见下文)</li><li id="d4b6" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">跨语言使用共享的<a class="ae ky" href="https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a" rel="noopener">令牌词汇表和嵌入空间</a></li></ul><p id="a820" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，<em class="lv"> mBERT是一个单一的模型，它学习跨大量语言的统一表示</em>。</p><p id="7ddb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">预培训方法。</strong> mBERT接受了来自超过<a class="ae ky" href="https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages" rel="noopener ugc nofollow" target="_blank"> 100种语言</a>(而不仅仅是英语)的维基百科数据的预训练。为了构建预训练数据集，作者在维基百科上找到数据量最大的100种语言，然后将这些数据串联起来，形成多语言预训练数据集。mBERT学习的所有语言都是基于大小为110K的单个<a class="ae ky" rel="noopener" target="_blank" href="/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7">单词块</a>词汇嵌入的。</p><p id="667d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为每种语言分配一个概率，预训练更新从一种语言中抽取一部分文本数据，用于根据该概率进行的<a class="ae ky" href="https://cameronrwolfe.substack.com/i/76273144/training-bert" rel="noopener ugc nofollow" target="_blank">自监督更新</a>，允许mBERT在整个预训练过程中接触所有不同的语言。不出所料，预训练数据集中的某些语言自然代表性不足——它们的数据比其他语言少。为了减轻这种不平衡的影响，每种语言的概率被指数平滑如下:</p><ol class=""><li id="4d04" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu oa mi mj mk bi translated">取每种语言的概率(例如，如果21%的预训练语料库是英语，则英语具有21%的概率)</li><li id="23d1" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu oa mi mj mk bi translated">用因子s对这个概率取幂(例如，mBERT使用s=0.7，得到0.21⁰.7 = 0.335)</li><li id="9a00" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu oa mi mj mk bi translated">基于这些指数化的概率，重新归一化(即除以总和)每种语言的概率</li></ol><p id="0b26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种平滑方法<em class="lv"> (i) </em>略微降低了采样高资源语言的概率，而<em class="lv"> (ii) </em>略微增加了采样低资源语言的概率。尽管平滑的量由s的选择来控制，但是这种技术确保了在预训练期间低资源语言不会被代表不足(反之亦然)；见下文。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/e885fd6879f6f5c42f3af01652796d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwSEYvMcH_WY8dHtOSnN6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语言概率的指数平滑</p></figure><p id="d120" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于那些感兴趣的人来说，语言概率的指数平滑法受到了用于时间序列预测的类似技术的启发。</p><p id="9d81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> mBERT真的很有用。</strong>在<a class="ae ky" href="https://github.com/facebookresearch/XNLI" rel="noopener ugc nofollow" target="_blank"> XNLI数据集</a>上评估时，发现mBERT在解决跨语言任务方面相当有效。跨多种语言的联合预训练带来了显著的性能优势，其中低资源语言的分类任务得到了最大的提升。这些结果表明，多语言预训练有助于正向归纳迁移——同时训练多种语言实际上比训练每种语言的单独模型更好。</p><p id="32eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应当注意，许多用于多语言(和单语)理解的现有方法依赖于专门的语言学方法。mBERT(和BERT)的提议极大地简化了日常实践者解决NLP任务的过程。</p><h2 id="95ec" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">数据集</h2><p id="55e5" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">有几个数据集可用于多语言深度学习模型的下游评估。这些数据集既包括分类风格的问题，也包括更复杂的问题，如命名实体识别或问题回答。</p><p id="0cc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">跨语言NLI (XNLI)语料库。</strong> <a class="ae ky" href="https://github.com/facebookresearch/XNLI" rel="noopener ugc nofollow" target="_blank"> XNLI </a>是流行的<a class="ae ky" href="https://cims.nyu.edu/~sbowman/multinli/" rel="noopener ugc nofollow" target="_blank"> multiNLI </a>语料库的多语言版本，是目前最流行的评估多语言语言理解的数据集。multiNLI数据集包含433，000对已经用文本蕴涵信息进行了注释的句子(即，句子是否相互矛盾，<a class="ae ky" href="https://www.merriam-webster.com/dictionary/entail" rel="noopener ugc nofollow" target="_blank">相互蕴涵</a>，或者是中性的)。multiNLI数据集不同于其他自然语言推理数据集，因为它包含跨众多语音和文本体裁的句子。</p><p id="89e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XNLI语料库是multiNLI的一个版本，已被翻译成15种不同的语言，由:</p><ul class=""><li id="bd41" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">使用人类来翻译开发和测试集</li><li id="46a3" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">机器翻译训练集</li></ul><p id="1bfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在XNLI数据集上进行训练的几种不同方法在文献中经常被报道，包括:</p><ul class=""><li id="ec7b" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">翻译-训练:</strong>对每种语言的训练集使用单独的模型进行训练和评估</li><li id="5102" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">翻译-测试:</strong>开发和测试集被翻译成英语，然后在训练集上微调单个英语模型并用于评估</li><li id="a12e" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu"> translate-train-all: </strong>在所有不同语言的训练集的机器翻译版本上微调单个多语言模型，然后使用每种不同语言进行评估。</li></ul><p id="e46e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也可以使用零测试设置来执行评估，在零测试设置中，某一语言被排除在训练集之外，但仍包括在评估中。</p><p id="469d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">其他数据集。</strong>虽然XNLI非常受欢迎，但还有其他几个数据集用于评估多语言理解。例如，<a class="ae ky" href="https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d" rel="noopener">命名实体识别</a> (NER)的CoNLL数据集包含英语、荷兰语、西班牙语和德语的翻译。此外，对于问题回答，<a class="ae ky" href="https://github.com/facebookresearch/MLQA" rel="noopener ugc nofollow" target="_blank"> MLQA基准</a>采用了流行的<a class="ae ky" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">小队基准</a>用于英语，并将其扩展到西班牙语、德语、阿拉伯语、印地语、越南语和中文版本。最后，多语言模型仍然通常在GLUE基准上进行评估，以便更好地比较它们与单语模型的性能。最近还提出了更广泛的多语言基准，如<a class="ae ky" href="https://github.com/google-research/xtreme" rel="noopener ugc nofollow" target="_blank"> XTREME </a>。</p><h1 id="1a13" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">出版物</h1><p id="0e98" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我现在将概述几个研究多语言BERT模型的出版物。这些出版物中的两个提出了对底层模型的潜在修改，而其他的分析模型行为并研究对更复杂应用的扩展(例如，命名实体识别)。</p><h2 id="0505" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">跨语言语言模型预训练[2]</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/7497ec3e1d4fced3613c1f0653e23780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q0ULSlOpW7_jBrE5vQ195g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XLM的语言嵌入(摘自[2])</p></figure><p id="4720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与mBERT的提议同时,[2]的作者开发了跨语言语言模型(XLM)。XLM是第一个使用生成性和跨语言预训练相结合的思想来创建多语言、基于转换的语言理解模型的模型之一。XLM与伯特共享相同的架构，除了额外的“语言”嵌入被添加到模型输入中的每个令牌；见上图。</p><p id="936d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与mBERT类似，XLM有一个共享的标记化方法和跨所有不同语言的嵌入空间。由于这种共享的嵌入空间，不同语言之间共享的标记级模式很容易学习。</p><blockquote class="lw lx ly"><p id="32e0" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">“这极大地改善了共享相同字母表或锚标记(如数字或专有名词)的语言之间嵌入空格的对齐方式”</p><p id="fe26" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><em class="it"> —来自【2】</em></p></blockquote><p id="359c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与mBERT相反，作者考虑了XLM训练前数据的两个来源:</p><ul class=""><li id="9ca3" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">来自每种不同语言的无监督的原始文本数据(与mBERT预训练数据相同)。</li><li id="16ff" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">不同语言的平行句子集(即一种语言的句子与另一种语言的相同句子成对出现)。</li></ul><p id="3306" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面提到的第二种数据集类似于机器翻译数据集——它只包含成对的句子，其中每个句子都被翻译成了另一种语言。显然，获得这样的句子对比获得原始文本更困难，因为它需要翻译文本数据(通过机器或人工注释器)。</p><p id="1382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用这两个数据来源，为XLM提出了几个不同的培训前任务:</p><ul class=""><li id="0859" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">因果语言建模(CLM): </strong>给定一个序列中的一组单词，预测下一个单词(这是一个普通的语言建模任务)。</li><li id="58aa" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">蒙面语言建模(MLM): </strong>相同，自我监督<a class="ae ky" href="https://cameronrwolfe.substack.com/i/76273144/training-bert" rel="noopener ugc nofollow" target="_blank"> MLM任务</a>在BERT内部使用。</li><li id="0811" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">翻译语言建模(TLM): </strong>类似于MLM，但是使用不同语言的两个平行句子作为输入(与单语输入相对)。</li></ul><p id="6ece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观上，TLM任务可以学习语言之间的对应关系。如果某个单词在一种语言中被屏蔽，该模型可以关注翻译句子的相应区域来预测被屏蔽的单词；见下文。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/b4060d9e3b4cb55363d259192a074c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4hzRXKcpmqgWtgtefzbcw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来自[2])</p></figure><p id="23f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为CLM任务需要使用<a class="ae ky" href="https://cameronrwolfe.substack.com/i/76273144/self-attention" rel="noopener ugc nofollow" target="_blank">单向自我关注</a>，三个独立的XLM模型使用CLM、MLM和MLM+TLM进行预训练。当对下游任务进行评估时，这些XLM模型在跨语言检测和机器翻译基准上取得了最先进的结果。使用MLM或MLM+TLM预培训目标会产生最佳效果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/f9464df34d583240c67c963cf0cf64f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Xw4E2p4cjb-RE4ifykzow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(来自[2])</p></figure><p id="4808" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用CLM训练的XLM模型也显示出在下游语言建模应用中产生困惑改进。XLM最显著的成绩提高发生在低资源语言上，揭示了多语言预训练对这类语言产生积极的归纳迁移。</p><p id="4def" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在零元迁移领域，XLM在推广新语言方面表现出色。特别是，由于跨语言的共享词汇，XLM可以利用语言之间的标记相似性来推理未明确包括在其训练集中的语言。</p><h2 id="1dbf" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">多语言BERT的语言中立性如何？[3]</h2><p id="8c09" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">随着mBERT的公开发布，许多研究人员对模型的行为和属性产生了疑问。<em class="lv">BERT修改这么少怎么支持多种语言？从不同语言中学习到的表征是否对齐(即不同语言中的相似词有相似的表征)？通过更具体的调整来支持多种语言，可以提高mBERT的性能吗？</em></p><p id="a035" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决这些问题，作者在[3]中采用了mBERT模型，并研究了它在三个下游任务中的行为:</p><ul class=""><li id="2351" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">语言识别:</strong>对句子的语言进行分类</li><li id="8e61" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">句子检索:</strong>在不同语言的文本语料库中查找句子的翻译版本</li><li id="7e2f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">词语对齐:</strong>计算同一句子翻译版本中对应词语的对齐度</li><li id="14d8" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">机器翻译质量评估:</strong>在不访问参考译文的情况下计算(机器生成的)译文的质量(即，这是一项更加细致/困难的任务。有关更多信息，请参见[3]的第4节)</li></ul><p id="87f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对这些不同任务中的每一个进行微调之后，作者进行了广泛的分析，从而对模型的行为产生了深刻的见解。有趣的是，人们发现mBERT表示由语言中立和特定于语言的成分组成— <em class="lv">由mBERT产生的表示并不完全是语言中立的。</em></p><p id="2718" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，mBERT能够以高准确度识别输入句子的语言。但是，当mBERT的表示居中/规范化时(基于语言中的所有其他表示)，模型:</p><ul class=""><li id="a2c5" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">在对源语言进行分类方面要差得多</li><li id="8ff1" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">更擅长执行句子检索</li></ul><p id="232c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一发现揭示了一些语言特有的信息清楚地包含在mBERT的表征中。因此，mBERT学习的模式并不是所有语言都通用的。尽管尝试了多种改进的微调方法，作者在使mBERT的表示更加语言中立方面并不成功。因此，产生具有改进的语言中立性的模型是未来的工作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/9218296cecf272812a08f16dc31aad3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5GTmWtI4QgUpNCVm6I-Fg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(摘自[3])</p></figure><p id="7883" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管有这些发现，但作者观察到，语言相似的语言在mBERT的嵌入空间中聚集在一起(见上文)，并且mBERT擅长解决跨语言、单词级的语义任务(例如，单词对齐和句子检索)。尽管努力完成更复杂的任务，如机器翻译质量评估，mBERT仍然能够在一定程度上捕捉跨语言的相似性。换句话说，mBERT工作得很好，<em class="lv">但必须加以改进，才能直接应用于更大范围的跨语言任务。</em></p><h2 id="5813" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">用BERT实现通用语命名实体识别[4]</h2><p id="bef1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">如果你和我一样，关于这篇论文你会问的第一个问题是— <em class="lv">这个题目是什么，有什么含义？</em>首先，牛津语言对“通用语”的定义是:</p><blockquote class="lw lx ly"><p id="445e" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">"母语不同的人之间作为共同语言使用的一种语言."</p></blockquote><p id="06a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然标题有点花哨，但上面的定义实际上很好地概括了本文的目的— <em class="lv">找到一个可以跨多种语言同时执行命名实体识别的单一模型</em>。特别是，[4]的作者在多种语言的NER数据集上联合训练了一个mBERT模型(如下所示)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/1643e9d97124a8669f53a151fa994fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xp99XI8-OE0vqsCU4obWaw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">NER的mBERT建筑(摘自[4])</p></figure><p id="d089" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们发现mBERT完全有能力解决这样的任务。该模型可以在不同语言的NER任务上联合训练，这产生了NER性能的改善，尤其是在低资源语言上。</p><p id="6578" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者还表明，mBERT能够执行高精度的零炮NER推断。鉴于先前的姆伯特/XLM模型可以以类似的方式进行零炮推断，这似乎并不令人惊讶。然而，NER是一个复杂的令牌级任务，与分类任务相比很难解决。尽管它简单易用，但在这个复杂的领域中，mBERT的表现还是令人惊讶的好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/d9cc1572265325d0915a50fd82b22a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8eRUy5xs7vORbFvDHCWmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(摘自[4])</p></figure><p id="ec0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">试图分析mBERT令人印象深刻的零射击性能，作者表明mBERT的令牌嵌入在预训练后排列良好，这意味着不同语言中的相似单词/短语具有相似的嵌入。因此，即使某种语言被排除在微调过程之外，<em class="lv">嵌入空间中的这种对齐使得有用的信息能够被提取并用于推断</em>。在微调过程中，对齐性能似乎会下降，但这可以通过简单地冻结早期网络层来解决。</p><h2 id="c48b" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">大规模无监督跨语言表征学习[6]</h2><p id="dc46" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[6]中的作者提出了XLM-罗伯塔(简称为XLM-R)模型，这是一个基于罗伯塔的多语言BERT模型(即BERT的变体)[7]。然而，作者没有建立在公开可用的mBERT模型上，而是从头开始建立自己的模型，强调在预训练过程中做出不同的选择可以导致更好的多语言理解下游模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/3eb9a9c02079feb1ba17477ab9a2e987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnqcUMDkieaUUi0_IBfsYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(摘自[6])</p></figure><p id="24c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特[1]和XLM [2]接受了维基百科数据的预训练，这些数据是跨多种语言聚合的。[6]中的作者声称，这种预训练数据集限制了模型性能。取而代之的是，他们使用<a class="ae ky" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank">公共抓取</a>库——一个公开的网络抓取数据数据库——构建了一个多语言文本数据语料库。语料库包括来自100多种语言的数据，比基于维基百科的数据集大两个数量级；见上文。</p><p id="edd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于mBERT，XLM-R模型在这个更大的语料库上被预训练，根据指数平滑的语言概率使用语言采样。然而，作者确实对基础模型和训练方案做了一些改变:</p><ul class=""><li id="da2d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">共享令牌词汇表的大小从110K增加到250K。</li><li id="5086" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">采用更通用的标记化方法来去除mBERT所需的特定于语言的预处理。</li><li id="7e6f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">执行的预训练量显著增加(以前的模型显然训练不足！).</li></ul><p id="7b08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XLM-R模型在多语言推理和问题回答任务上都明显优于mBERT和XLM，确立了其作为多语言理解的首选模型的地位，并揭示了在源自普通爬行的较大语料库上进行预训练是非常有益的。同样，低资源任务的性能似乎从使用XLM-R中受益最大，但XLM-R在高资源语言上的表现也与单语模型有竞争力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/1c770b04b7700d4e1a2e21f2cfbb033b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3nLBYFXI3mqbV268.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(摘自[6])</p></figure><p id="88de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在消融实验中，XLM-R使用不同数量的语言进行训练。作者揭示了一种现象，他们称之为多语制的<em class="lv">诅咒</em>。如果在保持模型容量/大小不变的情况下增加用于训练模型的语言数量，模型的性能最初会有所提高，但随后随着语言数量变得过大，性能会开始下降。虽然这个问题可以通过简单地增加模型大小来缓解，但它揭示了共同学习的语言的数量是使用这些多语言模型的从业者必须记住的一个考虑因素。</p><h1 id="7b26" class="mr ms it bd mt mu mv mw mx my mz na nb jz nc ka nd kc ne kd nf kf ng kg nh ni bi translated">外卖食品</h1><p id="c621" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">尽管多语言理解似乎是一项艰巨的任务，但最近的深度学习研究表明，当前的语言理解方法可以很好地处理这个问题。特别是，稍加修改的BERT版本可以通过多语言语料库进行联合预训练，然后进行微调，以惊人的准确度解决跨语言任务。</p><p id="96d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过分析这些模型，已经表明:</p><ul class=""><li id="ee14" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">单个多语言BERT模型可以学习/理解大量(即&gt; 100种)不同的语言。</li><li id="622b" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">多种语言的联合预训练产生了显著的性能优势(特别是对于缺乏大量训练数据的语言)。</li><li id="b363" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">以这种方式训练的多语言模型在高资源语言(例如，英语或德语)上仍然具有与单语模型竞争的性能</li><li id="7ba7" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">多语言BERT模型可以以零触发的方式推广到新的语言，因为它们跨语言高度一致、共享词汇/嵌入。</li></ul><p id="121f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先前对多语言理解(以及一般的NLP)的研究依赖于大量详细的语言理解。大多数任务通常使用复杂的、专门的架构来解决。有了(多语言)BERT，语言理解任务——甚至跨多种语言——可以用一个简单易懂的模型来解决。</p><p id="6386" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">代码。</strong>微调/使用这些模型比你想象的<a class="ae ky" href="https://cameronrwolfe.substack.com/i/76273144/takeaways" rel="noopener ugc nofollow" target="_blank">计算成本更低</a>！因此，我再次强调BERT模型对任何深度学习实践者的实用价值。如果你有兴趣尝试这些方法中的任何一种，我在这里推荐XLM-R链接<a class="ae ky" href="https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr" rel="noopener ugc nofollow" target="_blank">的代码示例。</a></p><p id="5521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延伸阅读。</strong>虽然我在这篇概述中涵盖了几篇论文，但我在研究过程中发现了许多其他真正有趣的论文。我最喜欢的一些是:</p><ul class=""><li id="232d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">Xtreme:用于评估跨语言泛化的大规模多语言多任务基准——包含40种语言的多语言、多任务语言理解基准[ <a class="ae ky" href="http://proceedings.mlr.press/v119/hu20b.html" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/google-research/xtreme" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="e814" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">mT5:一个大规模多语言预训练文本到文本转换器——流行的T5转换器架构的多语言变体[ <a class="ae ky" href="https://arxiv.org/abs/2010.11934" rel="noopener ugc nofollow" target="_blank">论文</a> ][ <a class="ae ky" href="https://github.com/google-research/multilingual-t5" rel="noopener ugc nofollow" target="_blank">代码</a></li><li id="f522" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">用于语音识别的无监督跨语言表征学习—多语言语音识别模型[ <a class="ae ky" href="https://arxiv.org/abs/2006.13979" rel="noopener ugc nofollow" target="_blank">论文</a></li></ul><h2 id="9006" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">结论</h2><p id="58d1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">非常感谢你阅读这篇文章。如果你喜欢，请在<a class="ae ky" href="https://twitter.com/cwolferesearch" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我，或者订阅我的<a class="ae ky" href="https://cameronrwolfe.substack.com/" rel="noopener ugc nofollow" target="_blank">深度(学习)焦点时事通讯</a>，在那里我挑选了一个双周一次的深度学习研究主题，提供了对相关背景信息的理解，然后概述了一些关于该主题的热门论文。我是<a class="ae ky" href="https://cameronrwolfe.me/" rel="noopener ugc nofollow" target="_blank"> Cameron R. Wolfe </a>，ale gion<a class="ae ky" href="https://www.alegion.com/" rel="noopener ugc nofollow" target="_blank">的研究科学家，莱斯大学的博士生，研究深度学习的经验和理论基础。你也可以看看我在medium上的</a><a class="ae ky" href="https://medium.com/@wolfecameron" rel="noopener">其他著述</a>！</p><h2 id="f230" class="no ms it bd mt np nq dn mx nr ns dp nb li nt nu nd lm nv nw nf lq nx ny nh nz bi translated">文献学</h2><p id="e3d8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[1] Devlin，Jacob等，“Bert:用于语言理解的深度双向转换器的预训练”arXiv预印本arXiv:1810.04805 (2018)。</p><p id="bc64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]兰普勒，纪尧姆和亚历克西斯·康诺。"跨语言语言模型预训练."arXiv预印本arXiv:1901.07291 (2019)。</p><p id="ca90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]利博维克、Jindřich、鲁道夫·罗萨和亚历山大·弗雷泽。“多语伯特的语言中立性如何？."arXiv预印本arXiv:1911.03310 (2019)。</p><p id="155c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] Moon，Taesun，等.“用bert实现通用语命名实体识别”arXiv预印本arXiv:1912.01389 (2019)。</p><p id="867e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5] Lample，Guillaume等,“命名实体识别的神经结构”arXiv预印本arXiv:1603.01360 (2016)。</p><p id="1076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6] Conneau，Alexis等人，“无监督的跨语言表征学习的规模。”arXiv预印本arXiv:1911.02116 (2019)。</p><p id="ce06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7]刘，，等.“Roberta:一种稳健优化的bert预训练方法”arXiv预印本arXiv:1907.11692 (2019)。</p><p id="34ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8]潘宁顿、杰弗里、理查德·索赫尔和克里斯托弗·曼宁。"手套:单词表示的全局向量."2014年自然语言处理经验方法会议录(EMNLP)。2014.</p><p id="547a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[9] Mikolov，Tomas等人，“单词和短语的分布式表示及其组合性”神经信息处理系统进展26 (2013)。</p><p id="9c56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[10] Ammar，Waleed等人，“大规模多语言单词嵌入”arXiv预印本arXiv:1602.01925 (2016)。</p><p id="0270" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[11] Johnson，Melvin等，“谷歌的多语言神经机器翻译系统:实现零镜头翻译。”计算语言学协会汇刊5(2017):339–351。</p><p id="dc1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[12] Conneau，Alexis等人，“XNLI:评估跨语言句子表征”arXiv预印本arXiv:1809.05053 (2018)。</p></div></div>    
</body>
</html>