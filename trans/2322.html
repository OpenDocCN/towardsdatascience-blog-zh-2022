<html>
<head>
<title>PySpark Or Pandas? Why Not Both</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark还是熊猫？为什么不两者都要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-or-pandas-why-not-both-95523946ec7c#2022-05-22">https://towardsdatascience.com/pyspark-or-pandas-why-not-both-95523946ec7c#2022-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f629" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">整体大于部分之和</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5c97ee5d5f2aca6a2bd11dc057131018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLs2w3XY9iB7BBEwiI0Wsg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">戴维·马尔库在<a class="ae kv" href="https://unsplash.com/s/photos/nature?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d163" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="#f8c3" rel="noopener ugc nofollow">导入并开始数据集</a> <br/> <a class="ae kv" href="#e706" rel="noopener ugc nofollow">系列到系列和多个系列到系列</a> <br/> <a class="ae kv" href="#a765" rel="noopener ugc nofollow">系列的迭代器到系列的迭代器和多个系列的迭代器到系列的迭代器</a> <br/> <a class="ae kv" href="#22fd" rel="noopener ugc nofollow">数据帧的迭代器到数据帧的迭代器</a> <br/> <a class="ae kv" href="#1331" rel="noopener ugc nofollow">系列到标量和多个系列到标量</a> <br/> <a class="ae kv" href="#fdab" rel="noopener ugc nofollow">组映射UDF</a><br/><a class="ae kv" href="#2952" rel="noopener ugc nofollow">最终想法</a></p><p id="fee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark允许许多开箱即用的数据转换。然而，在熊猫身上可以得到更多。Pandas功能强大，但由于其内存处理特性，它无法处理非常大的数据集。另一方面，PySpark是一个用于大数据工作负载的分布式处理系统，但不支持pandas提供的丰富的数据转换。随着Spark 3.x的发布，PySpark和pandas可以通过利用多种方法创建pandas用户定义函数(UDF)来进行组合。本文的目的是展示一组使用Spark 3.2.1的熊猫UDF示例。在后台，我们使用Apache Arrow，这是一种内存中的列数据格式，可以在JVM和Python进程之间高效地传输数据。更多信息可以在PySpark用户指南<a class="ae kv" href="https://spark.apache.org/docs/3.2.1/api/python/user_guide/sql/arrow_pandas.html" rel="noopener ugc nofollow" target="_blank">中的官方Apache Arrow</a>中找到。</p><p id="0d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中的内容不要与官方用户指南<a class="ae kv" href="https://spark.apache.org/docs/3.2.1/api/python/user_guide/pandas_on_spark/pandas_pyspark.html" rel="noopener ugc nofollow" target="_blank">中描述的Spark上的最新熊猫API相混淆。这是Spark中利用熊猫表现力的另一种可能性，代价是一些不兼容性。</a></p><h1 id="f8c3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">导入和启动数据集</strong></h1><p id="f28d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于本文中的例子，我们将依赖熊猫和numpy。我们还使用(希望)常用的约定从<code class="fe mp mq mr ms b">pyspark.sql</code>导入函数和类型模块:</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="4e64" class="mx lt iq ms b gy my mz l na nb">import pandas as pd<br/>import numpy as np<br/>import pyspark.sql.functions as F<br/>import pyspark.sql.types as T</span></pre><p id="7c71" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有示例都适用于20行4列的小型数据集:</p><ul class=""><li id="8f64" class="nc nd iq ky b kz la lc ld lf ne lj nf ln ng lr nh ni nj nk bi translated">group，一个用作分组键的<code class="fe mp mq mr ms b">T.StringType()</code>列</li><li id="96f6" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated">x，一个<code class="fe mp mq mr ms b">T.DoubleType()</code>栏目</li><li id="8b20" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated">y_lin，一个<code class="fe mp mq mr ms b">T.DoubleType()</code>列，它是x的两倍，带有一些噪声</li><li id="c156" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated">y_qua，一个三倍于x的平方的<code class="fe mp mq mr ms b">T.DoubleType()</code>列，带有一些噪声</li></ul><p id="cd7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">火花数据帧可以通过以下方式构建</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="a7c3" class="mx lt iq ms b gy my mz l na nb">g = np.tile(['group a','group b'], 10)<br/>x = np.linspace(0, 10., 20)<br/>np.random.seed(3) # set seed for reproducibility<br/>y_lin = 2*x + np.random.rand(len(x))/10.<br/>y_qua = 3*x**2 + np.random.rand(len(x))<br/>df = pd.DataFrame({'group': g, 'x': x, 'y_lin': y_lin, 'y_qua': y_qua})<br/>schema = StructType([<br/>    StructField('group', T.StringType(), nullable=False),<br/>    StructField('x', T.DoubleType(), nullable=False),<br/>    StructField('y_lin', T.DoubleType(), nullable=False),<br/>    StructField('y_qua', T.DoubleType(), nullable=False),<br/>])<br/>df = spark.createDataFrame(df, schema=schema)</span></pre><p id="af4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<code class="fe mp mq mr ms b">spark</code>是产生的spark会话</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="9a4c" class="mx lt iq ms b gy my mz l na nb">spark = (<br/>    SparkSession.builder<br/>      .appName('learn pandas UDFs in Spark 3.2')<br/>      .config('spark.sql.execution.arrow.pyspark.enabled', True) <br/>      .config('spark.sql.execution.arrow.pyspark.fallback.enabled', False)<br/>      .getOrCreate()<br/>)</span></pre><p id="fea0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据帧可通过以下方式检查</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="ba06" class="mx lt iq ms b gy my mz l na nb">def show_frame(df, n=5):<br/>    df.select([F.format_number(F.col(col), 3).alias(col)<br/>               if df.select(col).dtypes[0][1]=='double'<br/>               else col<br/>               for col in df.columns]).show(truncate=False, n=n)</span><span id="beab" class="mx lt iq ms b gy nq mz l na nb">show_frame(df)<br/># +-------+-----+-----+------+<br/># |group  |x    |y_lin|y_qua |<br/># +-------+-----+-----+------+<br/># |group a|0.000|0.055|0.284 |<br/># |group b|0.526|1.123|1.524 |<br/># |group a|1.053|2.134|3.765 |<br/># |group b|1.579|3.209|7.636 |<br/># |group a|2.105|4.300|13.841|<br/># +-------+-----+-----+------+<br/># only showing top 5 rows</span></pre><p id="a99a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意双栏的格式化/截断。仅显示了20行中的5行。</p><h1 id="e706" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">串联到串联和多个串联到串联</strong></h1><p id="6524" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">最简单的熊猫UDF将一个熊猫系列转换为另一个熊猫系列，没有任何聚合。例如，通过减去平均值并除以标准偏差来标准化一个系列，我们可以使用</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="3a34" class="mx lt iq ms b gy my mz l na nb"># series to series pandas UDF<br/>@F.pandas_udf(T.DoubleType())<br/>def standardise(col1: pd.Series) -&gt; pd.Series:<br/>    return (col1 - col1.mean())/col1.std()<br/>res = df.select(standardise(F.col('y_lin')).alias('result'))</span></pre><p id="d53e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">装饰者需要熊猫UDF的返回类型。还要注意函数定义中python类型的使用。结果可通过以下方式检查</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="8c85" class="mx lt iq ms b gy my mz l na nb">print(f"mean and standard deviation (PYSpark with pandas UDF) are\n{res.toPandas().iloc[:,0].apply(['mean', 'std'])}")</span><span id="0156" class="mx lt iq ms b gy nq mz l na nb"># mean and standard deviation (PYSpark with pandas UDF) are<br/># mean    6.661338e-17<br/># std     9.176629e-01<br/># Name: result, dtype: float64</span></pre><p id="7937" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在上面看到的，平均值在数字上等于零，但标准差不是。这是因为PySpark的分布式特性。PySpark将执行Pandas UDF，方法是将列分成批，并调用每个批的函数作为数据的子集，然后将结果连接在一起。因此，在上面的例子中，标准化应用于每个批次，而不是作为整体的数据帧。我们可以通过用熊猫本身测试熊猫UDF来验证这种说法的有效性:</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="a1df" class="mx lt iq ms b gy my mz l na nb">res_pd = standardise.func(df.select(F.col('y_lin')).toPandas().iloc[:,0])</span><span id="12f3" class="mx lt iq ms b gy nq mz l na nb">print(f"mean and standard deviation (pandas) are\n{res_pd.apply(['mean', 'std'])}")</span><span id="cf35" class="mx lt iq ms b gy nq mz l na nb"># mean and standard deviation (pandas) are<br/># mean   -2.220446e-16<br/># std     1.000000e+00<br/># Name: y_lin, dtype: float64</span></pre><p id="91d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在那里可以使用<code class="fe mp mq mr ms b">standardise.func()</code>从装饰过的熊猫中找回原来的熊猫UDF。验证语句有效性的另一种方法是使用重新分区</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="9c5e" class="mx lt iq ms b gy my mz l na nb">res = df.repartition(1).select(standardise(F.col('y_lin')).alias('result'))<br/><br/>print(f"mean and standard deviation (PYSpark with pandas UDF) are\n{res.toPandas().iloc[:,0].apply(['mean', 'std'])}")</span><span id="1226" class="mx lt iq ms b gy nq mz l na nb"># mean and standard deviation (PYSpark with pandas UDF) are<br/># mean   -2.220446e-16<br/># std     1.000000e+00<br/># Name: result, dtype: float64</span></pre><p id="3074" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这当然不是现实生活中所希望的，但有助于在这个简单的例子中演示内部工作原理。</p><p id="b2f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多个串联到串联的情况也很简单。作为一个简单的例子，我们添加两列:</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="15db" class="mx lt iq ms b gy my mz l na nb"># multiple series to series pandas UDF<br/>@F.pandas_udf(T.DoubleType())<br/>def add_cols(col1: pd.Series, col2: pd.Series) -&gt; pd.Series:<br/>    return col1 + col2<br/>res = df.select(F.col('y_lin'), F.col('y_qua'), add_cols(F.col('y_lin'), F.col('y_qua')).alias('added columns'))</span><span id="8904" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-----+------+-------------+<br/># |y_lin|y_qua |added columns|<br/># +-----+------+-------------+<br/># |0.055|0.284 |0.339        |<br/># |1.123|1.524 |2.648        |<br/># |2.134|3.765 |5.899        |<br/># |3.209|7.636 |10.845       |<br/># |4.300|13.841|18.141       |<br/># +-----+------+-------------+<br/># only showing top 5 rows</span></pre><p id="f5f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">返回的序列也可以是类型<code class="fe mp mq mr ms b">T.StructType()</code>，在这种情况下，我们指示熊猫UDF返回一个数据帧。举个简单的例子，我们可以通过在数据框中组合两列来创建一个结构列</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="fb06" class="mx lt iq ms b gy my mz l na nb"># series to series (struct) pandas UDF<br/>schema = T.StructType([<br/>    StructField('y_lin', T.DoubleType()), <br/>    StructField('y_qua', T.DoubleType()),<br/>])<br/>@F.pandas_udf(schema)<br/>def create_struct(col1: pd.Series, col2: pd.Series) -&gt; pd.DataFrame:<br/>    return pd.DataFrame({'y_lin': col1, 'y_qua': col2})</span><span id="208a" class="mx lt iq ms b gy nq mz l na nb">res = df.select(F.col('y_lin'), F.col('y_qua'), create_struct(F.col('y_lin'), F.col('y_qua')).alias('created struct'))</span><span id="831a" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-----+------+------------------------------------------+<br/># |y_lin|y_qua |created struct                            |<br/># +-----+------+------------------------------------------+<br/># |0.055|0.284 |{0.05507979025745755, 0.28352508177131874}|<br/># |1.123|1.524 |{1.123446361209179, 1.5241628490609185}   |<br/># |2.134|3.765 |{2.134353631786031, 3.7645534406624286}   |<br/># |3.209|7.636 |{3.2089774973618717, 7.6360921152062655}  |<br/># |4.300|13.841|{4.299821011224239, 13.8410479099986}     |<br/># +-----+------+------------------------------------------+<br/># only showing top 5 rows</span><span id="d2e8" class="mx lt iq ms b gy nq mz l na nb">res.printSchema()<br/># root<br/>#  |-- y_lin: double (nullable = false)<br/>#  |-- y_qua: double (nullable = false)<br/>#  |-- created struct: struct (nullable = true)<br/>#  |    |-- y_lin: double (nullable = true)<br/>#  |    |-- y_qua: double (nullable = true)</span></pre><p id="ec5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的一个小麻烦是列<code class="fe mp mq mr ms b">y_lin</code>和<code class="fe mp mq mr ms b">y_qua</code>被命名了两次。很高兴在评论中听到如果这可以避免的话！</p><h1 id="a765" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">系列迭代器到系列迭代器，多个系列迭代器到系列迭代器</strong></h1><p id="c54f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">当我们想要为每一批执行一次昂贵的操作时，迭代器变体是很方便的，例如，通过初始化一个模型。在下一个例子中，我们通过简单地为每批生成一个随机倍数来模拟这一点</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="fde0" class="mx lt iq ms b gy my mz l na nb"># iterator of series to iterator of series<br/>from typing import Iterator<br/>@F.pandas_udf(T.DoubleType())<br/>def multiply_as_iterator(col1: Iterator[pd.Series]) -&gt; Iterator[pd.Series]:<br/>    # the random multiple is generated once per batch<br/>    random_multiple = np.random.randint(1,10,1)[0]<br/>    for s in col1:<br/>       yield random_multiple*s</span><span id="8a1a" class="mx lt iq ms b gy nq mz l na nb">res = df.select(F.col('y_lin'), multiply_as_iterator(F.col('y_lin')).alias('multiple of y_lin'))</span><span id="1899" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-----+-----------------+<br/># |y_lin|multiple of y_lin|<br/># +-----+-----------------+<br/># |0.055|0.496            |<br/># |1.123|10.111           |<br/># |2.134|19.209           |<br/># |3.209|28.881           |<br/># |4.300|38.698           |<br/># +-----+-----------------+<br/># only showing top 5 rows</span></pre><p id="e2e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想要控制批处理大小，我们可以在创建spark会话时将配置参数spark . SQL . execution . arrow . maxrecordsperbatch设置为所需的值。这只影响像pandas UDFs这样的迭代器，即使我们使用一个分区，它也适用。</p><p id="b8f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从多个系列的迭代器到系列的迭代器相当简单，如下图所示，我们在对两列求和后应用了倍数</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="a1f4" class="mx lt iq ms b gy my mz l na nb"># iterator of multiple series to iterator of series<br/>from typing import Iterator, Tuple<br/>@F.pandas_udf(T.DoubleType())<br/>def multiply_as_iterator2(col1: Iterator[Tuple[pd.Series, pd.Series]]) -&gt; Iterator[pd.Series]:<br/>    # the random multiple is generated once per batch<br/>    random_multiple = np.random.randint(1,10,1)[0]<br/>    for s1, s2 in col1:<br/>        yield random_multiple*(s1 + s2)<br/><br/>res = df.select(F.col('y_lin'), F.col('y_qua'), multiply_as_iterator2(F.col('y_lin'), F.col('y_qua')).alias('multiple of y_lin + y_qua'))</span><span id="20ce" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-----+------+-------------------------+<br/># |y_lin|y_qua |multiple of y_lin + y_qua|<br/># +-----+------+-------------------------+<br/># |0.055|0.284 |1.693                    |<br/># |1.123|1.524 |13.238                   |<br/># |2.134|3.765 |29.495                   |<br/># |3.209|7.636 |54.225                   |<br/># |4.300|13.841|90.704                   |<br/># +-----+------+-------------------------+<br/># only showing top 5 rows</span></pre><p id="db82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">函数定义稍微复杂一些，因为我们需要构造一个包含熊猫系列的元组的迭代器。</p><h1 id="22fd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">数据帧的迭代器到数据帧的迭代器</strong></h1><p id="5e46" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据帧的迭代器到数据帧的迭代器的转换类似于多重序列的迭代器到序列的迭代器。当我们需要在完整的数据框上而不是在选定的列上执行pandas操作时，这是首选的方法。</p><p id="81b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为一个简单的例子，考虑最小-最大归一化</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="360c" class="mx lt iq ms b gy my mz l na nb"># iterator of data frame to iterator of data frame<br/>from typing import Iterator<br/>def min_max_normalise(frames: Iterator[pd.DataFrame]) -&gt; Iterator[pd.DataFrame]:<br/>    for frame in frames:<br/>        yield (frame-frame.mean())/(frame.max()-frame.min())<br/>schema = T.StructType([<br/>    StructField('y_lin', T.DoubleType()),<br/>    StructField('y_qua', T.DoubleType()),<br/>])<br/><br/>res = df.select(F.col('y_lin'), F.col('y_qua')).mapInPandas(min_max_normalise, schema=schema)</span><span id="71bd" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +------+------+<br/># |y_lin |y_qua |<br/># +------+------+<br/># |-0.497|-0.378|<br/># |-0.245|-0.287|<br/># |-0.007|-0.121|<br/># |0.246 |0.164 |<br/># |0.503 |0.622 |<br/># +------+------+<br/># only showing top 5 rows</span></pre><p id="5c16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先要注意的是需要向<code class="fe mp mq mr ms b">mapInPandas</code>方法提供一个模式，并且不需要装饰器。<code class="fe mp mq mr ms b">mapInPandas</code>方法可以改变返回数据帧的长度。同样，迭代器模式意味着数据帧不是作为一个整体进行最小-最大归一化，而是分别对每一批进行最小-最大归一化。</p><h1 id="1331" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">序列到标量和多个序列到标量</h1><p id="6736" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">可以使用或不使用拆分-应用-组合模式将一个系列聚合为标量。典型地，使用分组的分离-应用-组合被应用，否则整个列将被带到驱动程序，这首先违背了使用Spark的目的。作为一个简单的例子，我们使用另一列进行分组来计算一列的平均值</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="dc62" class="mx lt iq ms b gy my mz l na nb"># series to scalar<br/>@F.pandas_udf(T.DoubleType())<br/>def average_column(col1: pd.Series) -&gt; float:<br/>    return col1.mean()<br/><br/>res = df.groupby('group').agg(average_column(F.col('y_lin')).alias('average of y_lin'))</span><span id="40be" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># |group  |average of y_lin|<br/># +-------+----------------+<br/># |group a|9.509           |<br/># |group b|10.577          |<br/># +-------+----------------+</span></pre><p id="eba4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个人为的例子，因为没有必要用熊猫UDF，而是用普通香草派斯帕克</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="8a86" class="mx lt iq ms b gy my mz l na nb">res = df.groupby('group').agg(F.mean(F.col('y_lin')).alias('average of y_lin'))</span><span id="2aac" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># |group  |average of y_lin|<br/># +-------+----------------+<br/># |group a|9.509           |<br/># |group b|10.577          |<br/># +-------+----------------+</span></pre><p id="c1dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也可以将一组列简化为标量，例如通过计算两列之和的平均值</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="d9e8" class="mx lt iq ms b gy my mz l na nb"># multiple series to scalar<br/>@F.pandas_udf(T.DoubleType())<br/>def average_column(col1: pd.Series, col2: pd.Series) -&gt; float:<br/>    return (col1 + col2).mean()<br/><br/>res = df.groupby('group').agg(average_column(F.col('y_lin'), F.col('y_qua')).alias('average of y_lin + y_qua'))</span><span id="4f38" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-------+------------------------+<br/># |group  |average of y_lin + y_qua|<br/># +-------+------------------------+<br/># |group a|104.770                 |<br/># |group b|121.621                 |<br/># +-------+------------------------+</span></pre><h1 id="fdab" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">组图UDF</strong></h1><p id="82f7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在迄今为止的例子中，除了(多个)系列到标量，我们无法控制批次组成。串行到串行UDF将在分区上操作，而串行到串行UDF的迭代器将在每个分区的批处理上操作。在本文使用的示例数据框中，我们包含了一个名为group的列，我们可以用它来控制批次的组成。在现实生活中，需要注意确保该批具有熊猫一样的大小，以避免出现内存不足的异常。</p><p id="5ca8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用组图UDF，我们可以输入熊猫数据框并生成熊猫数据框。一个简单的例子标准化了数据帧:</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="4294" class="mx lt iq ms b gy my mz l na nb"># group map UDF<br/>def standardise_dataframe(df1: pd.DataFrame) -&gt; pd.DataFrame:<br/>    tmp = df1[['y_lin', 'y_qua']]<br/>    return (tmp - tmp.mean())/tmp.std()<br/>schema = T.StructType([<br/>    T.StructField('y_lin', T.DoubleType()),<br/>    T.StructField('y_qua', T.DoubleType()),<br/>])</span><span id="170b" class="mx lt iq ms b gy nq mz l na nb">res = df.groupby('group').applyInPandas(standardise_dataframe, schema=schema)</span><span id="af0e" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +------+------+<br/># |y_lin |y_qua |<br/># +------+------+<br/># |-1.485|-1.009|<br/># |-1.158|-0.972|<br/># |-0.818|-0.865|<br/># |-0.500|-0.691|<br/># |-0.170|-0.443|<br/># +------+------+<br/># only showing top 5 rows</span></pre><p id="10ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">默认情况下不包含组名，需要在返回的数据框和方案中明确添加，例如使用…</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="6559" class="mx lt iq ms b gy my mz l na nb"># group map UDF<br/>def standardise_dataframe(df1: pd.DataFrame) -&gt; pd.DataFrame:<br/>    tmp = df1[['y_lin', 'y_qua']]<br/>    return pd.concat([df1['group'], (tmp - tmp.mean())/tmp.std()], axis='columns')<br/>schema = T.StructType([<br/>    T.StructField('group', T.StringType()),<br/>    T.StructField('y_lin', T.DoubleType()),<br/>    T.StructField('y_qua', T.DoubleType()),<br/>])</span><span id="6323" class="mx lt iq ms b gy nq mz l na nb">res = df.groupby('group').applyInPandas(standardise_dataframe, schema=schema)</span><span id="187e" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-------+------+------+<br/># |group  |y_lin |y_qua |<br/># +-------+------+------+<br/># |group a|-1.485|-1.009|<br/># |group a|-1.158|-0.972|<br/># |group a|-0.818|-0.865|<br/># |group a|-0.500|-0.691|<br/># |group a|-0.170|-0.443|<br/># +-------+------+------+<br/># only showing top 5 rows</span></pre><p id="7d4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">组图UDF可以更改返回的数据框的形状。例如，我们将通过对y_lin和y_qua列拟合二次多项式来计算系数</p><pre class="kg kh ki kj gt mt ms mu mv aw mw bi"><span id="fd73" class="mx lt iq ms b gy my mz l na nb"># group map UDF<br/>def fit_polynomial(df1: pd.DataFrame) -&gt; pd.DataFrame:<br/>    tmp = df1[['x', 'y_lin', 'y_qua']]<br/>    # see https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html<br/>    poly_lin = np.polynomial.polynomial.Polynomial.fit(x=tmp['x'], y=tmp['y_lin'], deg=2).convert().coef<br/>    poly_qua = np.polynomial.polynomial.Polynomial.fit(x=tmp['x'], y=tmp['y_qua'], deg=2).convert().coef<br/>    df2 = pd.DataFrame({'group': df1['group'].iloc[0], 'y_lin fitted coffficients':  [poly_lin.tolist()], 'y_qua fitted coefficients': [poly_qua.tolist()]})<br/>    return df2<br/>schema = T.StructType([<br/>    T.StructField('group', T.StringType()),<br/>    T.StructField('y_lin fitted coefficients', T.ArrayType(T.DoubleType())),<br/>    T.StructField('y_qua fitted coefficients', T.ArrayType(T.DoubleType())),<br/>])<br/><br/>res = df.groupby('group').applyInPandas(fit_polynomial, schema=schema)<br/>show_frame(res)</span><span id="c350" class="mx lt iq ms b gy nq mz l na nb">show_frame(res)<br/># +-------+---------------------------------------------------------------+--------------------------------------------------------------+<br/># |group  |y_lin fitted coefficients                                      |y_qua fitted coefficients                                     |<br/># +-------+---------------------------------------------------------------+--------------------------------------------------------------+<br/># |group a|[0.05226283524780051, 1.9935642550858421, 4.346056274066657E-4]|[0.24210502752802654, 0.14937331848708446, 2.9865040888654355]|<br/># |group b|[0.07641111656270816, 1.9894934336694825, 8.012896992570311E-4]|[0.38970142737052527, 0.10989441330142924, 2.9877883688982467]|</span></pre><p id="8fe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">返回的列是数组。我们可以看到，考虑到添加到原始数据帧的噪声并不过分，这些系数非常接近预期值。我们还看到两组给出了非常相似的系数。</p><h1 id="2952" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后的想法</h1><p id="a26a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Pandas UDFs很好地补充了PySpark API，并允许更具表现力的数据操作。PySpark发展迅速，从2.x版到3.x版的变化非常显著。虽然本文涵盖了许多当前可用的UDF类型，但可以肯定的是，随着时间的推移，将会引入更多的可能性，因此在决定使用哪一种之前，最好先查阅文档。</p></div></div>    
</body>
</html>