<html>
<head>
<title>How to Conformalize a Deep Image Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何构造深度图像分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-conformalize-a-deep-image-classifier-14ead4e1a5a0#2022-12-05">https://towardsdatascience.com/how-to-conformalize-a-deep-image-classifier-14ead4e1a5a0#2022-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="74ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Julia 中的共形预测——第二部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e42c3a95c5a594bdc5890cfe33c681a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*pkF2Ldvb0MGanJL38rPGYg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有不同程度不确定性的共形预测集。图片作者。</p></figure><p id="db16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度学习很受欢迎，对于图像分类等一些任务来说，它非常强大。但众所周知，深度神经网络(DNN)可能不稳定(Goodfellow、Shlens 和 Szegedy 2014)且校准不佳。保形预测可以用来减轻这些缺陷。</p><p id="133e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在共形预测系列文章的第一篇<a class="ae lu" rel="noopener" target="_blank" href="/conformal-prediction-in-julia-351b81309e30">部分</a>中，我们看了基本的底层方法以及如何使用<code class="fe lv lw lx ly b"><a class="ae lu" href="https://github.com/pat-alt/ConformalPrediction.jl" rel="noopener ugc nofollow" target="_blank">ConformalPrediction.jl</a></code>在 Julia 中实现 CP。该系列的第二部分是一个更加面向目标的操作指南:它演示了如何通过几行代码整合内置于<code class="fe lv lw lx ly b">Flux.jl</code>中的深度学习图像分类器。</p><h1 id="fd79" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🎯手头的任务</h1><p id="cc50" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">手头的任务是使用著名的 MNIST 数据集(LeCun 1998)预测手写数字图像的标签。通过<code class="fe lv lw lx ly b">MLDatasets.jl</code>，在 Julia 中导入这个流行的机器学习数据集变得非常容易:</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="d1d5" class="na ma it ly b be nb nc l nd ne">using MLDatasets<br/>N = 1000<br/>Xraw, yraw = MNIST(split=:train)[:]<br/>Xraw = Xraw[:,:,1:N]<br/>yraw = yraw[1:N]</span></pre><h1 id="66a7" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🚧构建网络</h1><p id="15eb" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">从图像输入到标签的映射建模将依赖于一个简单的多层感知器(MLP)。深度学习的一个很棒的 Julia 库是<code class="fe lv lw lx ly b">Flux.jl</code>。但是等等...<code class="fe lv lw lx ly b">ConformalPrediction.jl</code>对<code class="fe lv lw lx ly b">MLJ.jl</code>训练的模特不起作用吗？没错，但幸运的是存在一个<code class="fe lv lw lx ly b">Flux.jl</code>到<code class="fe lv lw lx ly b">MLJ.jl</code>的接口，即<code class="fe lv lw lx ly b">MLJFlux.jl</code>。该界面仍处于早期阶段，但对于习惯于在<code class="fe lv lw lx ly b">Flux.jl</code>中构建神经网络的任何人(比如我自己)来说，它已经非常强大且易于使用。</p><p id="931c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<code class="fe lv lw lx ly b">Flux.jl</code>中，您可以为这个任务构建一个 MLP，如下所示:</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="0a11" class="na ma it ly b be nb nc l nd ne">using Flux<br/><br/>mlp = Chain(<br/>    Flux.flatten,<br/>    Dense(prod((28,28)), 32, relu),<br/>    Dense(32, 10)<br/>)</span></pre><p id="b72a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe lv lw lx ly b">(28,28)</code>只是输入尺寸(28x28 像素图像)。由于我们有十位数，所以我们的输出维数是<code class="fe lv lw lx ly b">10</code>。关于如何仅仅依靠<code class="fe lv lw lx ly b">Flux.jl</code>构建 MNIST 图像分类器的完整教程，请查看本<a class="ae lu" href="https://fluxml.ai/Flux.jl/stable/tutorials/2021-01-26-mlp/" rel="noopener ugc nofollow" target="_blank">教程</a>。</p><p id="d87a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在<code class="fe lv lw lx ly b">MLJFlux.jl</code>中做完全相同的事情，如下所示:</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="72c4" class="na ma it ly b be nb nc l nd ne">using MLJFlux<br/><br/>builder = MLJFlux.@builder Chain(<br/>    Flux.flatten,<br/>    Dense(prod(n_in), 32, relu),<br/>    Dense(32, n_out)<br/>)</span></pre><p id="3523" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们依靠<code class="fe lv lw lx ly b">@builder</code>宏来尽可能无缝地从<code class="fe lv lw lx ly b">Flux.jl</code>过渡到<code class="fe lv lw lx ly b">MLJ.jl</code>。最后，<code class="fe lv lw lx ly b">MLJFlux.jl</code>已经提供了许多帮助函数来定义普通网络。在这种情况下，我们将使用带有自定义生成器和交叉熵损失的<code class="fe lv lw lx ly b">ImageClassifier</code>:</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="f710" class="na ma it ly b be nb nc l nd ne">ImageClassifier = @load ImageClassifier<br/>clf = ImageClassifier(<br/>    builder=builder,<br/>    epochs=10,<br/>    loss=Flux.crossentropy<br/>)</span></pre><p id="b0a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">生成的实例<code class="fe lv lw lx ly b">clf</code>是一个模型(在<code class="fe lv lw lx ly b">MLJ.jl</code>的意义上),所以从这一点上我们可以依赖标准的<code class="fe lv lw lx ly b">MLJ.jl</code>工作流。例如，我们可以用数据包装我们的模型来创建一个机器，然后在维持集上对其进行评估，如下所示:</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="70ed" class="na ma it ly b be nb nc l nd ne">mach = machine(clf, X, y)<br/><br/>evaluate!(<br/>    mach,<br/>    resampling=Holdout(rng=123, fraction_train=0.8),<br/>    operation=predict_mode,<br/>    measure=[accuracy]<br/>)</span></pre><p id="3901" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们这个非常简单的模型的准确性并不惊人，但是对于本教程的目的来说已经足够好了。对于每幅图像，我们的 MLP 为每个可能的数字返回一个 softmax 输出:0，1，2，3，…，9。由于每个单独的 softmax 输出的值在 0 和 1 之间，yₖ ∈ (0，1)，这通常被解释为概率:yₖ ≔ p(y=k|X)。边缘情况(即接近零或一的值)表明预测确定性较高。但这只是预测不确定性的启发式概念(Angelopoulos 和 Bates 2021)。接下来，我们将使用共形预测把这种试探性的不确定性概念变成一种严格的不确定性概念。</p><h1 id="898b" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🔥整合网络</h1><p id="80a3" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">由于<code class="fe lv lw lx ly b">clf</code>是车型，所以也兼容我们的包:<code class="fe lv lw lx ly b">ConformalPrediction.jl</code>。为了整合我们的 MLP，我们只需要调用<code class="fe lv lw lx ly b">conformal_model(clf)</code>。由于生成的实例<code class="fe lv lw lx ly b">conf_model</code>也只是一个模型，我们仍然可以依赖标准的<code class="fe lv lw lx ly b">MLJ.jl</code>工作流。下面我们首先用数据包装它，然后拟合它。</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="12c3" class="na ma it ly b be nb nc l nd ne">using ConformalPrediction<br/>conf_model = conformal_model(clf; method=:simple_inductive, coverage=.95)<br/>mach = machine(conf_model, X, y)<br/>fit!(mach)</span></pre><p id="8b7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">啊…我们完成了！让我们在下一节看看结果。</p><h1 id="40e2" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">📊结果</h1><p id="5853" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">下图 2 显示了结果。图 2 (a)显示了高度确定的预测，现在在保形预测的严格意义上定义:在每种情况下，保形集(就在图像下面)只包括一个标签。</p><p id="9605" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">图 2 (b)和图 2 (c)分别显示了集合大小为 2 和 3 的预测越来越不确定。他们证明了 CP 能够很好地处理具有高度随机不确定性的样品:数字四(4)、七(7)和九(9)具有某些相似性。数字五(5)和六(6)以及三(3)和八(8)也是如此。即使看了很多例子(甚至对一个人来说)，这些可能也很难相互区分。因此，看到这些数字经常一起出现在共形集合中就不足为奇了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/7a185e7cb2f7f2748f4cd1ffd1bc11b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4a44kBgv3UWJ_PeQuXtj9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2 (a): <em class="ng">随机选择大小|C|=1 的预测集。图片作者。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/2b2f152bda954f4d4491e10cb3fabdc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*la688nf26MZmn4epqBbDZg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2 (b): <em class="ng">随机选择大小为|C|=2 的预测集。图片作者。</em></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/833780572240c02ceaf4609ef25f2466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BD8ezgz4U82eZoz1a6VROQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2 (c): <em class="ng">随机选择大小为|C|=3 的预测集。图片作者。</em></p></figure><h1 id="a526" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🧐评估</h1><p id="d048" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">为了评估保形模型的性能，可以使用特定的性能测量来评估模型是否被正确指定和良好校准(Angelopoulos 和 Bates 2021)。我们将在以后的另一篇文章中更详细地讨论这个问题。现在，请注意这些措施已经在<code class="fe lv lw lx ly b">ConformalPrediction.jl</code>中可用，我们将在这里简要展示它们。</p><p id="9d74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至于其他许多事情，<code class="fe lv lw lx ly b">ConformalPrediction.jl</code>利用了<code class="fe lv lw lx ly b">MLJ.jl</code>的现有功能进行模型评估。特别是，我们将在下面看到如何在我们的机器上使用通用的<code class="fe lv lw lx ly b">evaluate!</code>方法。为了评估我们的保形预测器的正确性，我们可以使用定制的性能度量<code class="fe lv lw lx ly b">emp_coverage</code>来计算经验覆盖率。关于模型校准，我们将查看模型的条件覆盖。对于自适应的、校准良好的共形模型，条件覆盖率很高。评估条件覆盖率的一个常用方法是规模分层覆盖率。用于此目的的自定义措施称为<code class="fe lv lw lx ly b">size_stratified_coverage</code>，别名为<code class="fe lv lw lx ly b">ssc</code>。</p><p id="c5e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的代码使用交叉验证实现了模型评估。我们上面使用的简单归纳分类器不是自适应的，因此与接近 0.95 的总经验覆盖率相比，获得的条件覆盖率较低，因此符合上面指定的期望覆盖率。</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="3054" class="na ma it ly b be nb nc l nd ne">_eval = evaluate!(<br/>    mach,<br/>    resampling=CV(),<br/>    operation=predict,<br/>    measure=[emp_coverage, ssc]<br/>)<br/>println("Empirical coverage: $(round(_eval.measurement[1], digits=3))")<br/>println("SSC: $(round(_eval.measurement[2], digits=3))")</span></pre><pre class="nj mw ly mx bn my mz bi"><span id="4248" class="na ma it ly b be nb nc l nd ne">Empirical coverage: 0.957<br/>SSC: 0.556</span></pre><p id="90c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当使用自适应预测集时，我们可以获得更高的自适应性(SSC ):</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="3329" class="na ma it ly b be nb nc l nd ne">conf_model = conformal_model(clf; method=:adaptive_inductive, coverage=.95)<br/>mach = machine(conf_model, X, y)<br/>fit!(mach)<br/>_eval = evaluate!(<br/>    mach,<br/>    resampling=CV(),<br/>    operation=predict,<br/>    measure=[emp_coverage, ssc]<br/>)<br/>println("Empirical coverage: $(round(_eval.measurement[1], digits=3))")<br/>println("SSC: $(round(_eval.measurement[2], digits=3))")</span></pre><pre class="nj mw ly mx bn my mz bi"><span id="a623" class="na ma it ly b be nb nc l nd ne">Empirical coverage: 0.99<br/>SSC: 0.942</span></pre><p id="043a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以使用一个定制的<code class="fe lv lw lx ly b">Plots.jl</code>方法来查看这两种方法的结果集大小(图 3)。根据上述情况，自适应方法的传播范围更广，这反映出“该程序有效地区分了简单和硬输入”(A. N. Angelopoulos 和 Bates，2021)。</p><pre class="kj kk kl km gt mw ly mx bn my mz bi"><span id="1c4f" class="na ma it ly b be nb nc l nd ne">plt_list = []<br/>for (_mod, mach) in results<br/>    push!(plt_list, bar(mach.model, mach.fitresult, X; title=String(_mod)))<br/>end<br/>plot(plt_list..., size=(800,300),bg_colour=:transparent)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5e330bfe02c8a501dd26a2646483cb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*bUw7HKaqWRArbGhk3NeM_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:两种方法的集合大小分布。图片作者。</p></figure><h1 id="fc40" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🔁概述</h1><p id="0690" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">在这个简短的指南中，我们看到了使用<code class="fe lv lw lx ly b">ConformalPrediction.jl</code>在 Julia 中整合深度学习图像分类器是多么容易。几乎任何在<code class="fe lv lw lx ly b">Flux.jl</code>中训练的深度神经网络都与<code class="fe lv lw lx ly b">MLJ.jl</code>兼容，因此只需几行代码就可以整合。这使得将不确定性试探法转化为严格的预测性不确定性估计变得非常容易。我们还看到了保形预测器的性能评估。敬请关注更多内容！</p><h1 id="b5e8" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">🎓参考</h1><p id="5ffa" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh mt lj lk ll mu ln lo lp mv lr ls lt im bi translated">安吉洛普洛斯，阿纳斯塔西奥斯 n，斯蒂芬贝茨。2021."保形预测和无分布不确定性量化的简明介绍."<a class="ae lu" href="https://arxiv.org/abs/2107.07511" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2107.07511</a>。</p><p id="0952" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">古德菲勒、伊恩·J、黄邦贤·史伦斯和克里斯蒂安·塞格迪。2014."解释和利用对立的例子."https://arxiv.org/abs/1412.6572<a class="ae lu" href="https://arxiv.org/abs/1412.6572" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="099c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">勒昆，扬恩。1998." MNIST 手写数字数据库."</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><p id="70db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ns">原载于 2022 年 12 月 5 日 https://www.paltmeyer.com</em><a class="ae lu" href="https://www.paltmeyer.com/blog/posts/conformal-image-classifier/" rel="noopener ugc nofollow" target="_blank"><em class="ns"/></a><em class="ns">。</em></p></div></div>    
</body>
</html>