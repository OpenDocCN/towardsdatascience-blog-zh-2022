<html>
<head>
<title>What Happens When You Include Irrelevant Variables in Your Regression Model?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当你在回归模型中包含不相关的变量时会发生什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-happens-when-you-include-irrelevant-variables-in-your-regression-model-77ab614f7073#2022-05-03">https://towardsdatascience.com/what-happens-when-you-include-irrelevant-variables-in-your-regression-model-77ab614f7073#2022-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e1d545043268181457ffd913a39182cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8RtD3XuZlEAJlhKrT6HERQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/users/3345557-3345557/" rel="noopener ugc nofollow" target="_blank"> Ernesto Velázquez </a>来自<a class="ae jd" href="https://pixabay.com/photos/darts-board-target-game-accuracy-6594496/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a> ( <a class="ae jd" href="https://pixabay.com/service/license/" rel="noopener ugc nofollow" target="_blank"> Pixabay许可</a>)</p></figure><div class=""/><div class=""><h2 id="fc2d" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">你的模型失去了精确性。我们会解释原因。</h2></div><p id="ba91" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/what-happens-when-you-omit-important-variables-from-your-regression-model-966830590d53">在上一篇文章</a>中，我们看到了遗漏重要变量如何导致回归模型的系数变得<a class="ae jd" rel="noopener" target="_blank" href="/understanding-estimation-bias-and-the-bias-variance-tradeoff-79ba42ab79c">有偏差</a>。在本文中，我们将研究这种情况的相反情况，也就是说，用完全多余的变量填充回归模型会对回归模型造成损害。</p><h1 id="b38b" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">什么是无关和多余的变量？</h1><p id="310a" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">回归变量被认为是不相关的或多余的有几个原因。以下是描述这些变量的一些方法:</p><ul class=""><li id="f8c8" class="mo mp jg kx b ky kz lb lc le mq li mr lm ms lq mt mu mv mw bi translated"><strong class="kx jh">无法解释模型响应变量(<strong class="kx jh"> <em class="mx"> y </em> </strong>)中任何方差</strong>的变量。</li><li id="75fb" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated">一个变量，它的<strong class="kx jh">回归系数</strong> ( <em class="mx"> β_m </em> ) <strong class="kx jh">在某个指定的<em class="mx"> α </em>水平上是统计上不显著的</strong>(即零)。</li><li id="f706" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated">与模型中其余回归变量高度相关的<strong class="kx jh">变量。由于其他变量已经包含在模型中，因此没有必要包含与现有变量高度相关的变量。</strong></li></ul><p id="5fbb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">向回归模型中添加不相关的变量会导致系数估计变得不太精确，从而导致整个模型失去精度。在本文的其余部分，我们将更详细地解释这一现象。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="0470" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人们很容易在模型中填入许多回归变量，以期达到更好的拟合。毕竟，人们可能会推测，如果一个变量被判断为不相关，训练算法(如普通的最小二乘法)会简单地将其系数压缩到接近零。此外，可以表明，线性模型的R平方(或非线性模型的伪R平方)只会随着模型中回归变量的每次添加而增加。</p><p id="6451" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不幸的是，在这种情况下，虽然R平方(或伪R平方)不断上升，但模型越来越不精确。</p><p id="1e86" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用线性回归模型作为我们的工作台来解释精度逐渐下降的原因。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="4a5b" class="lr ls jg bd lt lu nk lw lx ly nl ma mb km nm kn md kp nn kq mf ks no kt mh mi bi translated">经典的线性模型作为我们的工作台</h1><p id="56bf" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">经典线性回归模型的方程可以表示如下:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4d4cc26c64e90082af4b6cdd5106b223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/0*gRKTTO2xNsqvzYR5.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">线性回归模型(图片来自作者)</p></figure><p id="e858" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是上述等式的矩阵形式:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ce864850e97c494dcf594563d7e87e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*uqjgEkIxfIGOz0LZ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">线性回归模型的矩阵形式(图片由作者提供)</p></figure><p id="7f55" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">式(1)中，<strong class="kx jh"> <em class="mx"> y </em> </strong>为<strong class="kx jh">因变量</strong>，<strong class="kx jh"> <em class="mx"> X </em> </strong>为<strong class="kx jh">回归变量</strong>，<strong class="kx jh"> <em class="mx"> β </em> </strong>为<em class="mx"> k </em> <strong class="kx jh">回归系数</strong> <em class="mx"> β_1，β_2，β_3，…，β_k </em>包含<em class="mx">种群水平<em class="mx"><strong class="kx jh"><em class="mx"/></strong>是<strong class="kx jh"> <em class="mx"> y </em> </strong>的观测值与<strong class="kx jh"> <em class="mx"> y </em> </strong>的建模值之差。回归模型的误差项<strong class="kx jh"><em class="mx">【ϵ】</em></strong>反映了因变量<strong class="kx jh"> <em class="mx"> y </em> </strong>中回归变量<strong class="kx jh"> <em class="mx"> X </em> </strong>无法解释的方差部分。</em></em></p><p id="fa4e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将假设误差项向量<strong class="kx jh"><em class="mx"/></strong>中的<em class="mx"> n </em>个误差项<em class="mx">ϵ_i</em><em class="mx">【I = 1到n】</em>中的每一个误差项都围绕某个平均值(假设该平均值为零)变化，并且每个误差项围绕其平均值的方差平均为某个值σ <strong class="kx jh"> <em class="mx"> </em> </strong>。因此，假设误差具有零均值和恒定方差σ <strong class="kx jh"> <em class="mx">。</em>T73】</strong></p><p id="cae2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果模型中包含正确的回归变量集，它们将能够解释<strong class="kx jh"> <em class="mx"> y </em> </strong>中的大部分方差，从而使误差项的方差<em class="mx"/><strong class="kx jh"><em class="mx"/></strong>非常小。另一方面，<a class="ae jd" rel="noopener" target="_blank" href="/what-happens-when-you-omit-important-variables-from-your-regression-model-966830590d53">如果重要变量被忽略</a>，那么<strong class="kx jh"><em class="mx"/></strong>y中原本能够解释的方差部分现在将泄漏到误差项中，导致方差<em class="mx"/><strong class="kx jh"><em class="mx"/></strong>变大。</p><p id="d033" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在大小为<em class="mx"> n </em>的数据集上求解(也称为“拟合”或训练)线性模型，产生<strong class="kx jh"><em class="mx"/></strong>的<em class="mx">估计值</em>，我们将其表示为<strong class="kx jh"> <em class="mx"> β_cap。</em> </strong>因此，拟合的线性模型方程如下:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2444b3f711183d0a751df9535a6d8dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/0*_6xDf2wb6bsFcwsN.png"/></div></figure><p id="e141" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上式中，<strong class="kx jh"> <em class="mx"> e </em> </strong>是<strong class="kx jh">残差</strong>(又名<strong class="kx jh">残差</strong>)的列向量。对于第<em class="mx">次</em>观测，残差<em class="mx"> e_i </em>为第<em class="mx">次</em>次<em class="mx"> y_i </em>观测值与对应的第<em class="mx">次</em>拟合(预测)值<em class="mx"> y_cap_i </em>之差。<em class="mx"> e_i=(y_i — y_cap_i) </em></p><p id="bd24" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们进一步寻求发现无关变量对模型的影响之前，我们将陈述以下重要的观察结果:</p><h2 id="e3d5" class="nv ls jg bd lt nw nx dn lx ny nz dp mb le oa ob md li oc od mf lm oe of mh og bi translated">估计回归系数β_cap是具有均值和方差的随机变量</h2><p id="ea2d" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">让我们来理解为什么会这样:每次我们在大小为<em class="mx"> n </em>的不同随机选择的数据集上训练模型时，我们都会得到一组不同的系数<strong class="kx jh"><em class="mx"/></strong>真值的估计值。因此，估计系数的向量<strong class="kx jh"><em class="mx">β_ cap =</em></strong><em class="mx">【β_ cap _ 1，β_cap_2，…，β_ cap _ k】</em>是一组具有某种未知概率分布的随机变量。如果训练算法不产生<a class="ae jd" rel="noopener" target="_blank" href="/what-happens-when-you-omit-important-variables-from-your-regression-model-966830590d53">有偏估计</a>，则该分布的平均值(也称为期望值)是系数<strong class="kx jh"><em class="mx"/></strong>的真实群体水平值的集合。</p><p id="9fcd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">具体来说，估计系数<strong class="kx jh"><em class="mx">【β_ cap】，</em> </strong>的<a class="ae jd" rel="noopener" target="_blank" href="/understanding-conditional-variance-and-conditional-covariance-8b661067fc18"> <strong class="kx jh"> <em class="mx">条件</em>期望</strong> </a>是它们的真实总体值<strong class="kx jh"><em class="mx">【β】</em></strong>，这里的条件是对回归矩阵<strong class="kx jh"> <em class="mx"> X. </em> </strong>这可以表示如下:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/5eabf09e1692438eb85c5a164627f80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*9FLcU9JC4d4FoSISakk8Rg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">估计系数的条件期望是真实的总体水平值<strong class="bd oi"><em class="oj">【β】</em></strong><em class="oj">(图片由作者提供)</em></p></figure><p id="e82e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/a-deep-dive-into-the-variance-covariance-matrices-of-classical-linear-regression-models-4322b2cdc8e6"> <strong class="kx jh">可以看出，</strong></a><a class="ae jd" rel="noopener" target="_blank" href="/understanding-conditional-variance-and-conditional-covariance-8b661067fc18"><strong class="kx jh"><strong class="kx jh"><em class="mx">β_ cap</em></strong>的条件方差</strong> </a>可以通过下面的等式计算:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/66c49a4913e0f22c8ecdb1c486e9ac31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*WxZ2d0ZMocIa4TYGcV0Dng.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">估计回归系数的条件方差公式(图片由作者提供)</p></figure><p id="c4f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的等式中:</p><ul class=""><li id="6cf2" class="mo mp jg kx b ky kz lb lc le mq li mr lm ms lq mt mu mv mw bi translated"><strong class="kx jh"> <em class="mx"> β_cap </em> </strong>是大小为<em class="mx">(k×1)</em>的拟合回归系数的列向量，即<em class="mx"> k </em>行和<em class="mx"> 1 </em>列，假设模型中有<em class="mx"> k </em>个回归变量，包括截距，也包括任何无关变量。</li><li id="cb86" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><strong class="kx jh"> <em class="mx"> X </em> </strong>是大小为<em class="mx"> (n x k) </em>的回归变量矩阵，其中<em class="mx"> n </em>是训练数据集的大小。</li><li id="883e" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><strong class="kx jh"><em class="mx">X’</em></strong>是<strong class="kx jh"> <em class="mx"> X </em> </strong>的转置，即<strong class="kx jh"> <em class="mx"> X </em> </strong>的行列互换。就好像<strong class="kx jh"> <em class="mx"> X </em> </strong>已经侧过来了。因此<strong class="kx jh"><em class="mx">X’</em></strong>的大小为<em class="mx"> (k x n) </em>。</li><li id="cfda" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><em class="mx"> σ </em>是回归模型的误差项<strong class="kx jh"> <em class="mx"> ϵ </em> </strong>的方差。在实践中，我们使用拟合模型的残差<strong class="kx jh"> <em class="mx"> e </em> </strong>的方差<em class="mx"> s </em>作为<em class="mx"> σ </em>的无偏估计。<em class="mx"> σ </em>和s是标量(因此没有用<strong class="kx jh">粗体</strong>字体描述)。</li><li id="e8c5" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><strong class="kx jh"> <em class="mx"> X'X </em> </strong>是<strong class="kx jh"> <em class="mx"> X </em> </strong>与其转置的矩阵相乘。由于<strong class="kx jh"><em class="mx"/></strong>的大小为<em class="mx">(n X k)</em><strong class="kx jh"><em class="mx"/></strong>的大小为<em class="mx">(k X n)</em><strong class="kx jh"><em class="mx">【X’X</em></strong>的大小为<em class="mx"> (k x k) </em>。</li><li id="b9f0" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><em class="mx"> (-1) </em>的上标表示我们取了这个<em class="mx"> (k x k) </em>矩阵的<a class="ae jd" href="https://mathworld.wolfram.com/MatrixInverse.html" rel="noopener ugc nofollow" target="_blank">逆</a>，是另一个大小为<em class="mx"> (k x k) </em>的矩阵。</li><li id="b781" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated">最后，我们用误差项<strong class="kx jh"> <em class="mx"> ϵ </em> </strong>的方差<em class="mx"> σ </em>来缩放这个逆矩阵的每个元素。</li></ul><p id="9592" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">等式(4)给出了回归模型系数的方差-协方差矩阵 。如上所述，这是一个<em class="mx"> (k x k) </em>矩阵，如下所示:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/2d87952a6aea0f224ce180c8da7f0fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q81F7TGA982fhA6wL4PCuw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">回归系数的方差-协方差矩阵(作者图片)</p></figure><p id="9544" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">沿主对角线向下的元素，即从方差协方差矩阵的左上角到右下角的元素，包含<em class="mx"> k </em>回归系数<strong class="kx jh"><em class="mx">β_ cap</em></strong><em class="mx">=【β_ cap _ 1，β_cap_2，…，β_ cap _ k】</em>的<em class="mx">估计值</em>的方差。该矩阵中每隔一个元素<em class="mx"> (m，n) </em>都包含估计系数<em class="mx"> β_cap_m和β_cap_n之间的协方差。</em></p><p id="6459" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">主对角线元素的平方根是回归系数估计的标准误差。我们知道<a class="ae jd" rel="noopener" target="_blank" href="/interval-estimation-an-overview-and-a-how-to-guide-for-practitioners-e2a0c4bcf108"> <strong class="kx jh">区间估计理论</strong> </a>认为标准误差越大，估计的精度越小，估计周围的置信区间越宽。</p><blockquote class="om on oo"><p id="c8c2" class="kv kw mx kx b ky kz kh la lb lc kk ld op lf lg lh oq lj lk ll or ln lo lp lq ij bi translated">估计系数的方差越大，估计的精度越低。因此，由训练的模型生成的预测的精度较低。</p></blockquote><p id="16a7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">检查由上述观察产生的两个边界情况很有用:</p><p id="40db" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mx"> Var(β_cap_m|X) = 0 : </em>此时，系数估计的方差为零，因此系数估计的值等于系数的总体值<em class="mx">β_ m。</em></p><p id="4ba1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mx"> Var(β_cap_m|X) = ∞ : </em>在这种情况下，估计是无限不精确的，因此相应的回归变量完全不相关。</p><p id="06b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们来考察<em class="mx"> mth </em>回归变量在<strong class="kx jh"> <em class="mx"> X </em> </strong>矩阵中的情况:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/59295d3576cb993b9d1ef113642ec2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*VXwC0IfOAw-gc47cSspNMw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd oi"> X </strong>中的mth回归变量(作者图片)</p></figure><p id="85f7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该变量可由列向量<strong class="kx jh"><em class="mx">x</em></strong>T42 _ m表示，大小为<em class="mx"> (n x 1) </em>。在拟合的模型中，其回归系数为<em class="mx">β_ cap _ m。</em></p><p id="386f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该系数即<em class="mx">Var(β_ cap _ m</em><strong class="kx jh"><em class="mx">| X</em></strong><em class="mx">)的方差是式(4)方差协方差矩阵的<em class="mx"> mth </em>对角元素。该差异可表示如下:</em></p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/a455117bcb617576f3cd7f49169c9370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCz8MWLUF5mjStx3BTTjJA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">mth拟合回归系数的方差(作者图片)</p></figure><p id="d204" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上式中，</p><ul class=""><li id="88b1" class="mo mp jg kx b ky kz lb lc le mq li mr lm ms lq mt mu mv mw bi translated"><em class="mx"> σ </em>是模型误差项的方差。在实践中，我们使用拟合模型的残差的方差<em class="mx"> s </em>来估计<em class="mx"> σ </em>。</li><li id="fe48" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><em class="mx"> n </em>是数据样本数。</li><li id="1d5c" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><em class="mx"> R _m </em>是线性回归模型的R平方，其中因变量是第<em class="mx">个</em>回归变量<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ m</em>，解释变量是<strong class="kx jh"> <em class="mx"> X </em> </strong>矩阵中的其余变量。于是，<em class="mx"> R _m </em>就是<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ m</em>对其余<strong class="kx jh"> <em class="mx"> X </em> </strong>的回归的R平方。</li><li id="d6c4" class="mo mp jg kx b ky my lb mz le na li nb lm nc lq mt mu mv mw bi translated"><em class="mx">Var(</em><strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m)</em>是<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>的方差，它由通常的方差公式给出如下:</li></ul><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5061350ab2ba8cef432279b6f30dd993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*214mfrhcAKB_Zut7dYuD1Q.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">第m个回归变量的方差(图片由作者提供)</p></figure><p id="546c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们分析等式(5)之前，让我们回忆一下，对于第<em class="mx">个</em>回归变量，<em class="mx"> β_cap_m </em>的方差越大，估计的精度越低，反之亦然。</p><p id="12aa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们考虑以下场景:</p><h2 id="ae7f" class="nv ls jg bd lt nw nx dn lx ny nz dp mb le oa ob md li oc od mf lm oe of mh og bi translated">场景1</h2><p id="6425" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在这种情况下，我们将假设变量<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>恰好与模型中的其他变量高度相关。</p><p id="e52e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，用剩余的<strong class="kx jh"><em class="mx">×1.0</em>回归<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ m</em>得到的R平方<em class="mx"> R _m </em>，将接近<em class="mx"> 1.0 </em>。在等式(5)中，这将导致分母中的<em class="mx">(1-R _ m)</em>接近于零，从而导致<em class="mx"> β_cap_m </em>的方差非常大，因此不精确。因此，我们有以下结果:</strong></p><blockquote class="ov"><p id="c431" class="ow ox jg bd oy oz pa pb pc pd pe lq dk translated">当您加入与模型中其他回归变数高度相关的变数时，这个高度相关变数在定型模型中的系数估计会变得不精确。相关性越大，估计系数的不精确性越高。</p></blockquote><p id="25ec" class="pw-post-body-paragraph kv kw jg kx b ky pf kh la lb pg kk ld le ph lg lh li pi lk ll lm pj lo lp lq ij bi translated">回归变量之间的相关性称为<strong class="kx jh">多重共线性</strong>。</p><blockquote class="ov"><p id="34b8" class="ow ox jg bd oy oz pa pb pc pd pe lq dk translated">回归变量之间存在多重共线性的一个众所周知的后果是系数估计精度的损失。</p></blockquote><h2 id="ae3f" class="nv ls jg bd lt nw pk dn lx ny pl dp mb le pm ob md li pn od mf lm po of mh og bi translated">场景2</h2><p id="521b" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在考虑第二个回归变量<strong class="kx jh"> <em class="mx"> x </em> </strong> <em class="mx"> _j </em>，使得<strong class="kx jh"> <em class="mx"> x </em> </strong> <em class="mx"> _m </em>与<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ j</em>高度相关。等式(5)也可用于计算<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ j</em>的方差，如下所示:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pp"><img src="../Images/3ba7067d01c9909d06d038baec6236b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*YZfcvHbKMWGx-kOKV1YgAQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">第j个拟合回归系数的方差<em class="oj"> β_cap_j(图片由作者提供)</em></p></figure><p id="3b48" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mx"> R _j </em>是<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ j</em>对其余<strong class="kx jh"><em class="mx">×X</em></strong>(包括<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ m</em>)的线性回归的R平方值。由于<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>被假设为与<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ j</em>高度相关，如果我们从模型中省略掉<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>，那么<em class="mx">R _ j</em>【1—<em class="mx">R _ j】将会显著减少包含高度相关变量<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>会增加<em class="mx"> β_cap_j </em>的方差(即降低精度)。这表明包含高度相关变量如<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>的另一个重要结果:</em></p><blockquote class="ov"><p id="7347" class="ow ox jg bd oy oz pa pb pc pd pe lq dk translated">当添加与模型中其他回归变量高度相关的变量时，会降低模型中所有回归变量的系数估计精度。</p></blockquote><h2 id="bc86" class="nv ls jg bd lt nw pk dn lx ny pl dp mb le pm ob md li pn od mf lm po of mh og bi translated">场景3</h2><p id="cf6b" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">考虑第三种情况。不管<strong class="kx jh"> <em class="mx"> x </em> </strong> <em class="mx"> _m </em>是否与模型中的任何其他变量特别相关，模型中的<strong class="kx jh"> <em class="mx"> x </em> </strong> <em class="mx"> _m </em>的存在将导致<em class="mx"> R _j </em>，这是<em class="mx"> </em>模型的R平方，在该模型中，我们对其余部分进行回归<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ j</em>这种行为源于R平方的公式。从等式(5)我们知道，当<em class="mx"> R _j </em>增大时，等式(5)的分母变小，导致<em class="mx"> β_cap_j </em>的方差增大。如果<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>也不能解释因变量<strong class="kx jh"> <em class="mx"> y </em> </strong>中的任何方差，则这种影响，即<em class="mx"> β_cap_j </em>的精度损失尤其明显。在这种情况下，将<strong class="kx jh"><em class="mx">x</em></strong><em class="mx">_ m</em>添加到模型中并不会减少模型的误差项<strong class="kx jh"> <em class="mx"> ϵ </em> </strong>的方差<em class="mx"/>。回想一下，误差项包含了<strong class="kx jh"> <em class="mx"> y </em> </strong>中<strong class="kx jh"> <em class="mx"> X </em> </strong>无法解释的方差部分。因此，当<strong class="kx jh"><em class="mx">X</em></strong><em class="mx">_ m</em>是一个不相关变量时，它加入到模型中只会导致等式(5)的分母减少，而不会导致等式(5)的分子补偿性减少，从而导致所有<em class="mx"> j </em>的<em class="mx">Var(β_ cap _ j |</em><strong class="kx jh"><em class="mx">X</em></strong><em class="mx">)</em>因此，我们有了另一个重要的结果:</p><blockquote class="ov"><p id="55d3" class="ow ox jg bd oy oz pa pb pc pd pe lq dk translated">向回归模型中添加不相关的变量会使所有回归变量的系数估计变得不太精确。</p></blockquote><p id="eb13" class="pw-post-body-paragraph kv kw jg kx b ky pf kh la lb pg kk ld le ph lg lh li pi lk ll lm pj lo lp lq ij bi translated">最后，让我们回顾一下等式(5)揭示的另外两件事:</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/a455117bcb617576f3cd7f49169c9370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCz8MWLUF5mjStx3BTTjJA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">第m个拟合回归系数的方差(图片由作者提供)</p></figure><p id="c0e4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">分母中的<em class="mx"> n </em>是数据集大小。我们看到，训练模型的数据集越大，系数估计值的方差越小，因此精度越高。这似乎很直观。极限情况是当模型在整个群体上训练时。</p><blockquote class="ov"><p id="c5b3" class="ow ox jg bd oy oz pa pb pc pd pe lq dk translated">估计回归系数的精度随着训练数据集大小的增加而提高。</p></blockquote><p id="dea6" class="pw-post-body-paragraph kv kw jg kx b ky pf kh la lb pg kk ld le ph lg lh li pi lk ll lm pj lo lp lq ij bi translated">此外，我们看到一个回归变量如<strong class="kx jh"> <em class="mx"> x </em> </strong> <em class="mx"> _m </em>的方差越大，其回归系数估计值的方差越小。乍一看，这似乎不太直观。我们可以通过注意到几乎没有可变性的变量无法解释因变量<strong class="kx jh"> <em class="mx"> y </em> </strong>的可变性来理解这种影响，反之亦然。对于这种很大程度上“刚性”的变量，训练算法将无法正确估计它们对模型输出可变性的贡献(通过回归系数量化)。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><p id="c9cc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mx">如果您喜欢这篇文章，请关注我的</em><a class="ae jd" href="https://timeseriesreasoning.medium.com" rel="noopener"><strong class="kx jh"><em class="mx">Sachin Date</em></strong></a><em class="mx">以获得关于回归、时间序列分析和预测主题的提示、操作方法和编程建议。</em></p></div></div>    
</body>
</html>