<html>
<head>
<title>Backpropagation in RNN Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNN的反向传播解释说</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-in-rnn-explained-bdf853b4e1c2#2022-04-09">https://towardsdatascience.com/backpropagation-in-rnn-explained-bdf853b4e1c2#2022-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bf15" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">递归神经网络中计算图形和反向传播的逐步解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9925b0ecd942d228281745b2e25543a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZlPtuWVVQuj7e-tovTZBA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">RNN的反向传播</p></figure><h1 id="153a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="7221" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在没有框架的机器学习早期，建立模型的大部分时间都花在手工编码反向传播上。今天，随着框架和亲笔签名的发展，要通过几十层的深度神经网络进行反向传播，我们只需调用<code class="fe mj mk ml mm b">loss.backward</code>——就是这样。然而，为了获得对深度学习的坚实理解，我们理解反向传播和计算图的基础是至关重要的。</p><p id="fef1" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">反向传播背后的直觉是，我们计算最终损失相对于网络权重的梯度，以获得降低损失的方向，并且在优化期间，我们沿着该方向移动并更新权重，从而最小化损失。</p><p id="137c" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">在这篇文章中，我们将了解反向传播如何发生在一个递归神经网络。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="e0bd" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">计算图形</h1><p id="2a9e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">反向传播的核心是运算和函数，它们可以优雅地表示为计算图形。我们来看一个例子:考虑函数<em class="ne">f = z(x+y)</em>；它的计算图形表示如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/032b7404dbc1727f35f157aeb91765af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*L1L0m4S6CRS-YvKjPTsGdA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前进传球</p></figure><p id="b7bd" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">计算图本质上是以函数和操作为节点的有向图。从输入计算输出被称为前向传递，习惯上在图的边上显示前向传递。</p><p id="e32b" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">在后向通道中，我们计算输入输出的梯度，并在边缘下方显示它们。在这里，我们从末尾开始，然后沿着这条路计算梯度。让我们来做这个例子的反向传递。</p><p id="0544" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated"><strong class="lp ir">符号:</strong>让我们在整篇文章中将<em class="ne"> a </em> wrt <em class="ne"> b </em>的导数表示为∂ <em class="ne"> a </em> /∂ <em class="ne"> b </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/74474107aea779840d99ab5a404456c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*EZ9hArrKR_fgWEzcp0xuyA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">浅灰色箭头表示向后传球</p></figure><p id="71eb" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">首先，我们从末尾开始，计算∂ <em class="ne"> f </em> /∂ <em class="ne"> f </em>为1，然后向后移动，我们计算∂ <em class="ne"> f </em> /∂ <em class="ne"> q </em>为<em class="ne"> z，</em>然后∂ <em class="ne"> f </em> /∂ <em class="ne"> z </em>为<em class="ne"> q，</em>最后我们计算∂ <em class="ne"> f </em> /∂ <em class="ne"> x和<em class="ne"/></em></p><h2 id="47f4" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">上游、下游和局部梯度</h2><p id="bd96" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你观察的话，我们无法直接计算∂ <em class="ne"> f </em> /∂ <em class="ne"> x和</em> ∂ <em class="ne"> f </em> /∂ <em class="ne"> y </em>，所以我们使用链式法则先计算∂ <em class="ne"> q </em> /∂ <em class="ne"> x </em>然后乘以<em class="ne">t13】∂<em class="ne">f</em>/∂<em class="ne">q</em>得到<em class="ne"/>∂</em> ∂ <em class="ne"> f </em> /∂ <em class="ne"> q </em>称为上游比降<em class="ne">，</em> ∂ <em class="ne"> f </em> /∂ <em class="ne"> x </em>称为下游比降，∂ <em class="ne"> q </em> /∂ <em class="ne"> x </em>称为局部比降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b5026e6828b651fbe5dac59ae49e0029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*KlT9bGLfdlDsrsvxN2ZUZA.png"/></div></figure><pre class="kg kh ki kj gt nu mm nv nw aw nx bi"><span id="ca69" class="nh kw iq mm b gy ny nz l oa ob">downstream gradient = local gradient × upstream gradient</span></pre></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="fcbf" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">计算图的优势</h1><ol class=""><li id="72ab" class="oc od iq lp b lq lr lt lu lw oe ma of me og mi oh oi oj ok bi translated"><strong class="lp ir">基于节点的方法</strong>:在反向传播中，我们总是对梯度流感兴趣，当使用计算图形时，我们可以根据节点而不是函数或操作来考虑梯度流。</li></ol><p id="5c81" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">考虑一个简单的添加节点，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/35230713789954cb35cd124c595f2732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*kqCDv9rEjPSMEqtr6ZsZ2w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度分配器</p></figure><p id="bad7" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">给定输入<em class="ne"> x </em>和<em class="ne"> y </em>，输出<em class="ne"> z = x + y. </em>上游梯度为<em class="ne"/>∂<em class="ne">l</em>/∂<em class="ne">z</em>其中<em class="ne"> L </em>为最终损失。当地的梯度是<em class="ne"> </em> ∂ <em class="ne"> z </em> /∂x，但是由于<em class="ne"> z = x + y </em>，∂ <em class="ne"> z </em> /∂ <em class="ne"> x = 1。</em>现在，下游比降<em class="ne"/>∂<em class="ne">l</em>/∂<em class="ne">x</em>是上游比降和局部比降的乘积，但由于局部比降是统一的，下游比降等于上游比降。换句话说，渐变按原样流经加法节点，因此加法节点被称为渐变分配器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/2f9db895436cdb5e06a03090da950620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*WnwOj6Y8fnctMof3sqho8Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度交换乘数</p></figure><p id="b1d3" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">类似地，对于乘法节点，如果您进行计算，则下游梯度是上游梯度和其他输入的乘积，如上图所示。因此，乘法节点称为梯度交换乘法器。</p><p id="7e1b" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">再次强调，这里的关键是在计算图中用节点来考虑梯度流。</p><p id="9d8e" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">2.<strong class="lp ir">模块法:</strong>特定节点处的下游比降仅取决于其局部比降和上游比降。因此，如果我们希望在未来改变网络的架构，我们可以简单地插拔适当的节点，而不会影响其他节点。这种方法是模块化的，尤其是在处理亲笔签名时。</p><p id="f8d4" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">3.<strong class="lp ir">自定义节点:</strong>我们可以将多个操作合并到一个节点中，如sigmoid节点或softmax节点，我们将在接下来看到。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="a1cf" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">向量和矩阵的梯度</h1><p id="b830" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">当使用神经网络时，我们通常处理用向量和矩阵表示的高维输入和输出。标量对矢量的导数是一个矢量，它表示矢量的每个元素的变化如何影响标量；一个向量对另一个向量的导数是一个雅可比矩阵，它表示该向量的每个元素如何受到另一个向量的每个元素的变化的影响。无需证明，梯度如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b2f32f2f819f271e157e286625cba3b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*V4067Zo4kxSz_jiUEJ5cfQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矩阵乘法梯度</p></figure><p id="cbfa" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">这里<em class="ne"> W </em>是权重矩阵，<em class="ne"> x </em>是输入向量，<em class="ne"> y </em>是输出乘积向量。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="5b33" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">交叉熵损失的梯度(可选)</h1><p id="9aa1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们再做一个例子来加强我们的理解。让我们计算交叉熵损失节点的梯度，它是一个softmax节点，后跟一个log loss节点。这是许多神经网络使用的标准分类头。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4e1d5d54568ff814c6286f60c1ddefc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*IIfgT03C1vt7IPM5YAoDJA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最大测井损失</p></figure><h2 id="85a9" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">前进传球</h2><p id="f548" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在正向传递中，一个向量<code class="fe mj mk ml mm b">𝑦⃗ = [y1, y2, y3, ..., yn]</code>通过softmax节点得到概率分布<code class="fe mj mk ml mm b">S = [S1, S2, S3, ..., Sn]</code>作为输出。那么，假设地面真实指数为<em class="ne"> m </em>，我们取<code class="fe mj mk ml mm b">Sm</code>的负对数来计算损失:<code class="fe mj mk ml mm b">l = -log(Sm)</code>。</p><p id="b3c9" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">softmax函数由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/bb23c8d16e3aecebb125d36c8e4528d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*3eZ_gKV6rz5ADVG9A87glA.png"/></div></figure><h2 id="d885" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">偶数道次</h2><p id="ff53" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这里棘手的部分是损失对向量<em class="ne"> S </em>的单个元素的依赖性。所以，<em class="ne"> l = -log(Sm) </em>和∂<em class="ne">l</em>/∂<em class="ne">sm</em>=<em class="ne">-1</em>/<em class="ne">sm</em>其中<em class="ne"> Sm </em>代表<em class="ne"> S </em>的<em class="ne">m</em>元素其中<em class="ne"> m </em>为地面真值标签。接下来，往回走，我们应该计算softmax节点的梯度。这里下游比降是∂ <em class="ne"> l </em> /∂ <em class="ne"> y，</em>当地比降<em class="ne"> </em>是<em class="ne"> </em> ∂ <em class="ne"> S </em> /∂ <em class="ne"> y，</em>上游比降是<em class="ne"> </em> ∂ <em class="ne"> l </em> /∂ <em class="ne"> Sm。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/462c83b48e78a70b6dd027a2d6cbccb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxuiefY9FCsVhIGR8G40iQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">偶数道次</p></figure><p id="a89d" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">首先，让我们计算局部梯度∂ <em class="ne"> S </em> /∂ <em class="ne"> y. </em>现在，这里<em class="ne"> S </em>是一个向量，<em class="ne"> y </em>也是一个向量，因此∂ <em class="ne"> S </em> /∂ <em class="ne"> y </em>将是一个矩阵，它表示<em class="ne"> S </em>的每个元素如何受到<em class="ne"> y </em>的每个元素的变化的影响；<em class="ne"> </em>但是你看，损失只取决于地面真实指数上<em class="ne"> S </em>的单个元素，所以我们只对发现<em class="ne"> S </em>的单个元素如何受到<em class="ne"> y. </em>数学上<em class="ne">的每个元素的变化的影响感兴趣， </em>我们感兴趣的是找到<em class="ne"/>∂<em class="ne">si</em>/∂<em class="ne">y</em>其中<em class="ne"> Si </em>是矢量<em class="ne"> S. </em>的第<em class="ne">个</em>元素这里有另一个陷阱，<em class="ne"> Si </em>只是应用于<em class="ne"> </em> 𝑦⃗的第个<em class="ne">元素的softmax函数，这意味着<em class="ne"> Si </em> 所以我们不能直接计算∂ <em class="ne"> Si </em> /∂ <em class="ne"> y </em>。因此，我们将找到<em class="ne"> </em> ∂ <em class="ne">斯</em> /∂ <em class="ne"> yj </em>其中<em class="ne"> yj </em>是<em class="ne"> </em> 𝑦⃗的任意元素，并考虑两种情况，其中<em class="ne"> j = i </em>和<em class="ne"> j ≠ i </em>，如下所示:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/55801244fcc245a06ce90d82ad4fb880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SaQ5-8RVUP0Wu7V_-mSYQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/d0db6adc9ed64636b5dddecff15518b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f0LEvHqeTkYR9bZqr29vpw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情况1: j= i</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/95917fe99d964815c86d816f8ca6d336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiwuuXg-nMbX9UleLPnvGw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">案例二:j ≠ i</p></figure><p id="1462" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">所以最后我们可以写:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/18b6ffa096b09e1ee2058db9c127da7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*jo0yMlK9RqSRlKVs2L_Qug.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">softmax节点的局部渐变</p></figure><p id="d620" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">接下来让我们计算下游比降∂ <em class="ne"> l </em> /∂ <em class="ne"> y. </em>现在，由于下游比降是局部比降和上游比降的乘积，让我们再次找到<em class="ne"/>∂<em class="ne">l</em>/∂<em class="ne">yj</em>并考虑两种情况，其中<em class="ne"> j = i </em>和<em class="ne"> j ≠ i </em>如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/afbcf7a18b8981c68e1b8f439cfb3998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jafX11DkGv1PTIV2tZPBmQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/4d0bedd74f8fda0474636aba6c3ad7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFob3NniUdFT1IsLvEDj0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情况1: j= i</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/297e7aa2b052f15448404c5438017a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHL43Zdj7Akd6P1gKClbTw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">案例二:j ≠ i</p></figure><p id="b9b9" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">所以最后我们可以写:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/12fecb6b3b59069bc4b2247dd1bdfd71.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*ZZtE8Sc1iDfk1IsCNXKaJQ.png"/></div></div></figure><p id="f930" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">因此，如果向量<code class="fe mj mk ml mm b">𝑦⃗ = [y1, y2, y3, ..., ym, ..., yn]</code>穿过softmax节点以获得概率分布<code class="fe mj mk ml mm b">S = [S1, S2, S3, ..., Sm, ..., Sn]</code>，则下游梯度∂ <em class="ne"> l </em> /∂ <em class="ne"> y </em>由<code class="fe mj mk ml mm b">[S1, S2, S3, ..., Sm — 1, ..., Sn]</code>给出，即，我们保持softmax向量的所有元素不变，并从地面真实指数处的元素中减去1。这也可以表示为<em class="ne"> S - 1【在索引m处】。</em>因此，下一次我们想要通过交叉熵损失节点反向传播时，我们可以简单地将下游梯度计算为<em class="ne">S-1[在索引m处]。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/da6c9124f7e0ad8356eaa5cb70afbc5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*WBOSRr8Yh_l5VUQlznie5w.png"/></div></figure><p id="6ecc" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">好了，现在我们已经有了反向传播和计算图的坚实基础，让我们看看RNN的反向传播。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="5248" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">RNN的反向传播</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9925b0ecd942d228281745b2e25543a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZlPtuWVVQuj7e-tovTZBA.png"/></div></div></figure><h2 id="e8ad" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">前进传球</h2><p id="3bb1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在前向传递中，在特定时间步，来自前一时间步的输入向量和隐藏状态向量乘以它们各自的权重矩阵，并由加法节点求和。接下来，它们通过一个非线性函数，然后被复制:其中一个作为下一个时间步的输入，另一个进入分类头，在那里乘以一个权重矩阵，以获得logits向量，然后计算交叉熵损失。这是一个典型的生成RNN设置，其中我们对网络建模，使得给定一个输入字符，它预测下一个适当字符的概率分布。如果你有兴趣在pytorch中建立一个角色RNN，请在这里查看我的<a class="ae oz" rel="noopener" target="_blank" href="/build-an-ai-based-autocomplete-in-the-browser-using-vue-js-fastapi-and-websockets-1eb7ae19bfd8">其他文章</a>。正向传递方程如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/de94a51bdce77a05bfaf2d105608c659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVVKnMWUhBJlvESQWWrtUA.png"/></div></div></figure><h2 id="ff11" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">偶数道次</h2><p id="3578" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在后向过程中，我们从末端开始计算分类损失的梯度wrt logits向量——细节已在前面的部分中讨论过。这个梯度流回到矩阵乘法节点，在这里我们计算权重矩阵和隐藏状态的梯度。隐藏状态下的渐变流回复制节点，在此处与前一时间步的渐变相遇。你看，RNN本质上是一次一步地处理序列，所以在反向传播过程中，梯度在时间步长上反向流动。这被称为通过时间的<strong class="lp ir">反向传播。因此，隐藏状态的梯度和前一时间步的梯度在复制节点相遇，并在此处相加。</strong></p><p id="b30b" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">接下来，它们回流到<code class="fe mj mk ml mm b">tanh</code>非线性节点，其梯度可以计算为:<code class="fe mj mk ml mm b">∂tanh(x)/∂x<em class="ne"> = </em>1−tanh²(x)</code>。然后这个梯度通过加法节点，在那里它被分配到输入向量和前一个隐藏状态向量的矩阵乘法节点。除非有特殊要求，我们通常不计算输入向量的梯度，但我们会计算前一个隐藏状态向量的梯度，然后返回到前一个时间步。详细的数学步骤请参考图表。</p><p id="0153" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">让我们看看它在代码中的样子。</p><h2 id="81be" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">Python代码</h2><p id="74f2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">安德烈·卡帕西在Python/Numpy中从头开始实现了角色RNN，他的代码出色地捕捉了我们讨论过的反向传播步骤，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安德烈·卡帕西的代码。在BSD许可下在此重新使用。</p></figure><p id="353a" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">完整的代码可以在<a class="ae oz" href="https://gist.github.com/karpathy/d4dee566867f8291f086" rel="noopener ugc nofollow" target="_blank">这里</a>找到，强烈建议读者去看看。如果你正在寻找一个pytorch实现RNN的例子，请查看我的<a class="ae oz" rel="noopener" target="_blank" href="/build-an-ai-based-autocomplete-in-the-browser-using-vue-js-fastapi-and-websockets-1eb7ae19bfd8">的另一篇文章。</a></p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="da99" class="kv kw iq bd kx ky mz la lb lc na le lf jw nb jx lh jz nc ka lj kc nd kd ll lm bi translated">为什么反向传播在RNN无效</h1><p id="f529" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你观察，计算前一个隐藏状态的梯度，这是下游梯度，上游梯度流经双曲正切非线性，并乘以权重矩阵。现在，由于这个下游梯度跨时间步长回流，这意味着计算在每个时间步长重复发生。这有几个问题:</p><ol class=""><li id="4c2f" class="oc od iq lp b lq mn lt mo lw pd ma pe me pf mi oh oi oj ok bi translated">由于我们一遍又一遍地乘以权重矩阵，梯度将根据矩阵的最大奇异值放大或缩小:如果奇异值大于1，我们将面临<em class="ne">爆炸梯度</em>问题，如果小于1，我们将面临<em class="ne">消失梯度</em>问题。</li><li id="9c82" class="oc od iq lp b lq pg lt ph lw pi ma pj me pk mi oh oi oj ok bi translated">现在，梯度通过双曲正切非线性，在极端情况下具有饱和区域。这意味着一旦梯度通过非线性，如果梯度具有高值或低值，梯度将基本上变为零，因此梯度不能有效地在长序列中传播，并导致无效的优化。</li></ol><p id="f007" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">有一种方法可以避免爆炸梯度的问题，如果梯度超过某个阈值，就从本质上“剪切”梯度。然而，RNN仍然不能有效地用于长序列。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><p id="637d" class="pw-post-body-paragraph ln lo iq lp b lq mn jr ls lt mo ju lv lw mp ly lz ma mq mc md me mr mg mh mi ij bi translated">我希望你对反向传播在RNN是如何发生的有一个清楚的概念。如果你有任何疑问，请告诉我。让我们在<a class="ae oz" href="https://twitter.com/WingedRasengan" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae oz" href="https://www.linkedin.com/in/neerajkrishnadev/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连线。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="ec9f" class="nh kw iq bd kx ni nj dn lb nk nl dp lf lw nm nn lh ma no np lj me nq nr ll ns bi translated">图像制作者名单</h2><p id="bf01" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">本文所用图片均由作者制作。</p></div></div>    
</body>
</html>