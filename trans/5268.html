<html>
<head>
<title>6 Reinforcement Learning Algorithms Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释 6 种强化学习算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e#2022-11-25">https://towardsdatascience.com/6-reinforcement-learning-algorithms-explained-237a79dbd8e#2022-11-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bf4f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习术语、基础和概念介绍(无模型、基于模型、在线、离线学习)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/77df78f9e97852b2dc03c96e71a249bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bo5XH0yQcOvHpKWh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">瑞安·菲尔兹在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a3f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习分为三个分支:监督学习、非监督学习和强化学习。</p><ul class=""><li id="8129" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">监督学习(SL) </strong>:关注在给定标签训练数据的情况下获得正确的输出</li><li id="660f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">无监督学习(UL) </strong>:关注在没有预先存在标签的情况下发现数据中的模式</li><li id="6585" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">强化学习(RL) </strong>:关注代理如何在一个环境中采取行动以最大化累积回报</li></ul><p id="0886" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通俗地说，强化学习类似于婴儿学习和发现世界，如果有奖励(正强化)，婴儿可能会采取行动，如果有惩罚(负强化)，则不太可能采取行动。这也是强化学习与监督学习和非监督学习的主要区别，后者从静态数据集学习，而前者从探索中学习。</p><p id="3b22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将涉及强化学习的术语和基本组件，以及不同类型的强化学习(无模型、基于模型、在线学习和离线学习)。本文最后用算法来说明不同类型的强化学习。</p><blockquote class="mj mk ml"><p id="0b0e" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">注意:这些方程基于 Stuart J. Russell 和 Peter Norvig 的教科书《人工智能:现代方法》(第四版，全球版),为保持数学方程格式一致，做了一些小改动。</p></blockquote><h1 id="fc01" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">目录</h1><ul class=""><li id="9836" class="lv lw it lb b lc ni lf nj li nk lm nl lq nm lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#a72c" rel="noopener">强化学习术语</a></li><li id="3917" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#7922" rel="noopener">基础知识:马尔可夫决策过程(MDP) </a></li><li id="e393" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#652b" rel="noopener">无模型与基于模型的强化学习</a></li><li id="6526" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#eabf" rel="noopener">离线学习与在线学习</a></li><li id="6b29" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#bab1" rel="noopener">算法:直接效用估算</a></li><li id="0419" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#95b3" rel="noopener">算法:自适应动态规划</a></li><li id="be27" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#8eb7" rel="noopener">算法:时间差学习(TD 学习)</a></li><li id="e772" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#e32b" rel="noopener">算法:探索</a></li><li id="0e08" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#3ac8" rel="noopener">算法:Q 学习</a></li><li id="21eb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://medium.com/p/237a79dbd8e/#54c5" rel="noopener">算法:SARSA </a></li></ul></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="a72c" class="mq mr it bd ms mt nu mv mw mx nv mz na jz nw ka nc kc nx kd ne kf ny kg ng nh bi translated">强化学习术语</h1><p id="6937" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">在深入研究不同类型的强化学习和算法之前，我们应该熟悉一下强化学习的组成部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a745771d40a0e956f64328482860387b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*4VSULPE6LASkxREhyWcP1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:强化学习术语的图解——作者图片</p></figure><ul class=""><li id="4cec" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">代理:从环境中接收感知并执行动作的程序</li><li id="22c6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">环境</strong>:代理所处的真实或虚拟环境</li><li id="d73a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">状态</strong>:座席可以处于的状态</li><li id="e740" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">动作(A) </strong>:代理在给定状态下可以采取的动作</li><li id="a46f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">奖励(R) </strong>:采取行动的奖励(依赖于行动)，处于状态的奖励(依赖于状态)，或者在给定状态下采取行动的奖励(依赖于行动和状态)</li></ul><p id="e23b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在婴儿探索世界的例子中，婴儿(代理人)处于现实世界(环境)中，可以是哭泣、感到高兴或饥饿(状态)。因此，婴儿可以选择吃东西或睡觉(动作)，如果婴儿在饿的时候吃东西，他/她就会满足(奖励)。</p><p id="e7ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如文章开头提到的，强化学习涉及探索，强化学习的输出是最优策略。一个<strong class="lb iu">策略</strong>描述了在每个状态下要采取的动作；类似于说明书。比如政策可以是宝宝饿了就吃，不然就让宝宝睡觉。这也与监督学习形成对比，监督学习的输出只是单一的决策或预测，没有策略复杂。</p><p id="4245" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，强化学习的<strong class="lb iu">目标</strong>是通过优化所采取的行动来最大化总的累积回报。和婴儿一样，我们不都想从生活中获得最大的累积利益吗？；)</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="7922" class="mq mr it bd ms mt nu mv mw mx nv mz na jz nw ka nc kc nx kd ne kf ny kg ng nh bi translated">基础知识:马尔可夫决策过程(MDP)</h1><p id="7c1d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">由于强化学习涉及到做出一系列最佳行动，它被认为是一个<strong class="lb iu">顺序决策问题</strong>，可以使用马尔可夫决策过程进行建模。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/354c9e4481237c578b6270d5836344fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnkfxylFtTn0_AMtcP6Ajw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:MDP 的例子——作者图片</p></figure><p id="7296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">按照前面的部分，状态(由<strong class="lb iu"> S </strong>表示)被建模为圆圈，动作(由<strong class="lb iu"> A </strong>表示)允许代理在状态之间转换。在图 2 中，还存在转移概率(由<strong class="lb iu"> T </strong>表示)，其中<code class="fe oe of og oh b">T(S11, A1, S12)</code>是在状态<code class="fe oe of og oh b">S11</code>采取行动<code class="fe oe of og oh b">A1</code>之后转移到状态<code class="fe oe of og oh b">S12</code>的概率。我们可以认为动作<code class="fe oe of og oh b">A1</code>向右，动作<code class="fe oe of og oh b">A2</code>向下。为简单起见，我们可以假设转移概率为 1，这样采取行动<code class="fe oe of og oh b">A1</code>将保证向右移动，而采取行动<code class="fe oe of og oh b">A2</code>将保证向下移动。</p><p id="a06f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考图 2，让目标在状态<code class="fe oe of og oh b">S23</code>结束，从状态<code class="fe oe of og oh b">S11</code>开始，黄色状态是好的(奖励<code class="fe oe of og oh b">+1</code>)，红色状态是坏的(奖励<code class="fe oe of og oh b">-1</code>)，紫色是目标状态(奖励<code class="fe oe of og oh b">+100</code>)。我们希望代理了解最佳行动或路线是通过采取行动<code class="fe oe of og oh b">A2-A1-A1</code>向下-向右-向右，并获得总奖励<code class="fe oe of og oh b">+1+1+1+100</code>。更进一步，利用金钱的时间价值，我们在奖励上应用了折扣因子γ，因为现在的奖励比以后的奖励更好。</p><p id="ccd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，从状态<code class="fe oe of og oh b">S11</code>开始执行动作<code class="fe oe of og oh b">A2-A1-A1</code>的预期效用的数学公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/dbe9061393bc899b9bde9a34a2e62e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*A5h0BGU52qwelLdMD7qWFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:从状态 S11 开始的预期效用—作者提供的图像</p></figure><p id="88ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的例子是一个简单的说明，有各种变化，</p><ul class=""><li id="237d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">转移概率可能不是 1，需要考虑行动中的不确定性，例如采取某些行动可能不总是保证成功向右或向下移动。因此，我们需要对这个不确定性取一个期望值</li><li id="b949" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最佳动作可能还不知道，因此通用表示将把动作表示为来自状态的策略，用<strong class="lb iu"> π(S) </strong>表示</li><li id="184a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">奖励可能不是基于黄色/红色/紫色状态，而是基于前一个状态、动作和下一个状态的组合，由<strong class="lb iu"> R(S1，π(S1)，S2) </strong>表示</li><li id="0dd1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">问题可能不会在 4 个步骤内解决，它可能需要无限的步骤才能达到目标状态</li></ul><p id="47c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这些变化，在遵循策略<strong class="lb iu"> π </strong>的给定状态<strong class="lb iu"> s </strong>下，确定期望效用<strong class="lb iu"> U(s) </strong>的更一般的等式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0ddf62c2dcbaefb6418890cc4e8273a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*VQwZ9e7QivdP_s7jAOIUYg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:从状态 s 开始执行策略的预期效用(等式 16.2)——作者图片</p></figure><p id="a861" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用图 4 的话来说，一个州的预期效用是贴现回报的预期总和。</p><p id="34c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此可见，一个国家的效用与其邻国的效用相关；假设选择了最优行动，一个状态的效用是过渡的预期回报加上下一个状态的贴现效用。在编码术语中，这被认为是递归。数学上，它指的是下面的等式，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/08c240b2f39e3c59610607087e4949ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*dThKQen63v0hD64ouuVBRA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:遵循最优政策的国家效用(等式 16.5)——作者图片</p></figure><p id="3129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图 5 中，这是著名的<strong class="lb iu">贝尔曼方程</strong>，它求解最大效用并导出最优策略。最优策略是在一个状态下采取的行动，它将导致最大的当前效用加上下一个状态的贴现效用，考虑了所有可能的下一个状态的转移概率。</p><p id="6885" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到图 2 中的 MDP 问题，最优策略是这样的，如果代理处于状态<code class="fe oe of og oh b">S11</code>、<code class="fe oe of og oh b">S12</code>或<code class="fe oe of og oh b">S13</code>，代理应该通过采取行动<code class="fe oe of og oh b">A2</code>向下移动。而如果代理处于状态<code class="fe oe of og oh b">S21</code>或<code class="fe oe of og oh b">S22</code>，代理应该通过采取动作<code class="fe oe of og oh b">A1</code>向右移动。最优策略是通过求解贝尔曼方程得出的，以执行获得最大当前和贴现未来回报的行动。</p><ul class=""><li id="faaf" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">额外</strong>:在教科书中，MDP 用<code class="fe oe of og oh b">(S, A, T, R)</code>来表示，分别代表一组状态、动作、转移函数和奖励函数。</li><li id="3441" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu"> Extra </strong> : MDP 假设环境是完全可观测的，如果智能体不知道它当前处于什么状态，我们就用<strong class="lb iu">部分可观测 MDP (POMDP) </strong>来代替！</li><li id="f4f7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">额外</strong>:图 5 中的贝尔曼方程可用于求解最优策略，使用<strong class="lb iu">值迭代</strong>或<strong class="lb iu">策略迭代</strong>，这是一种将效用值从未来状态传递到当前状态的迭代方法。</li></ul><blockquote class="ol"><p id="0719" class="om on it bd oo op oq or os ot ou lu dk translated">强化学习类似于求解 MDP，但是现在转移概率和奖励函数是未知的，代理必须执行动作来学习</p></blockquote><h1 id="652b" class="mq mr it bd ms mt mu mv mw mx my mz na jz ov ka nc kc ow kd ne kf ox kg ng nh bi translated">无模型与基于模型的强化学习</h1><p id="08ae" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">上一节中的 MDP 例子是基于模型的强化学习。在形式上，<strong class="lb iu">基于模型的强化学习</strong>有成分转移概率<code class="fe oe of og oh b">T(s1, a, s2)</code>和奖励函数<code class="fe oe of og oh b">R(s1, a, s2)</code>，它们是未知的，代表要解决的问题。</p><ul class=""><li id="ecf0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">基于模型的方法对模拟很有用。</li><li id="03ae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">基于模型的 RL 的例子包括<strong class="lb iu">值迭代</strong>和<strong class="lb iu">策略迭代</strong>，因为它使用具有转移概率和奖励函数的 MDP。</li></ul><p id="72ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">另一方面，无模型方法</strong>不需要知道或学习转移概率来解决问题。相反，代理直接学习策略。</p><ul class=""><li id="36f6" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">无模型方法对于解决现实生活中的问题很有用。</li><li id="b00b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">无模型 RL 的例子包括<strong class="lb iu"> Q 学习</strong>和<strong class="lb iu">策略搜索</strong>，因为它直接学习策略。</li></ul><h1 id="eabf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">离线学习与在线学习</h1><p id="b3d9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nz lk ll lm oa lo lp lq ob ls lt lu im bi translated">离线和在线学习也被称为被动和主动学习。</p><p id="5c0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">离线(被动)学习</strong>中，通过学习效用函数解决问题。给定一个具有未知转移和回报函数的固定策略，代理人通过使用该策略执行一系列试验来学习效用函数。</p><ul class=""><li id="de5a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">例如，在自动驾驶汽车中，给定地图和要遵循的大致方向(<strong class="lb iu">固定政策</strong>)，带有错误控制(<strong class="lb iu">未知转移概率</strong> —向前移动可能导致汽车稍微左转或右转)和未知行驶时间(<strong class="lb iu">未知奖励函数</strong> —假设更快到达目的地导致更多奖励)，汽车可以重复运行以了解平均总行驶时间(<strong class="lb iu">效用函数</strong>)。</li><li id="61e4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">离线 RL 的例子包括<strong class="lb iu">值迭代</strong>和<strong class="lb iu">策略迭代</strong>，因为它使用了使用效用函数的贝尔曼方程(图 5)。其他示例包括<strong class="lb iu">直接效用估计</strong>、<strong class="lb iu">自适应动态规划(ADP) </strong>和<strong class="lb iu">时间差学习(TD) </strong>，这些将在后面的章节中详细阐述。</li></ul><p id="8ac4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">在线(主动)学习</strong>中，通过学习计划或决策来解决问题。对于基于模型的在线 RL，有探索和开发组件。在开发阶段，代理通过假设固定的策略和学习效用函数，表现得像离线学习。在探索阶段，代理执行<strong class="lb iu">值迭代</strong>或<strong class="lb iu">策略迭代</strong>来更新策略。</p><ul class=""><li id="e60a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">如果使用<strong class="lb iu">值迭代</strong>更新策略，则使用最大化效用/价值的一步前瞻提取最佳动作。如果使用<strong class="lb iu">策略迭代</strong>更新策略，则最佳策略可用，并且可以按照建议执行操作。</li><li id="a27e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">以自动驾驶汽车为例，在探索阶段，汽车可能会学习到在高速公路上行驶时所用的总时间更快，并选择向高速公路行驶，而不是简单地沿大方向行驶(<strong class="lb iu">策略迭代</strong>)。在开发阶段，根据更新后的政策，汽车现在以更少的平均总时间(<strong class="lb iu">更高的效用</strong>)行驶。</li><li id="2478" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在线学习的例子包括<strong class="lb iu">探索</strong>、<strong class="lb iu">Q-学习</strong>和<strong class="lb iu"> SARSA </strong>，这些将在后面的章节中详细说明。</li></ul><p id="c530" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较两者，当有太多的状态和动作以至于有太多的转移概率时，在线学习是优选的。在网上学习中探索和“边学边做”比在网下学习中一次学会所有东西更容易。然而，由于探索中的试错法，在线学习也可能很耗时。</p><p id="853a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>:在线学习和 On-Policy(以及 Off-Policy 的离线学习)是有区别的，前者指的是学习(策略可以改变或固定)，后者指的是策略(一系列试验是来自一个策略还是多个策略)。在本文的最后两节中，将使用算法来解释符合策略和不符合策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/ca6e6abf74c912085d349c5b74b3d831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kw-k1fd5NeBHhy_3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由克里斯·贾维斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="ol"><p id="014e" class="om on it bd oo op oq or os ot ou lu dk translated">在了解了不同类型的强化学习之后，让我们深入研究算法！</p></blockquote><h1 id="bab1" class="mq mr it bd ms mt mu mv mw mx my mz na jz ov ka nc kc ow kd ne kf ox kg ng nh bi translated">№1.直接效用估计</h1><blockquote class="mj mk ml"><p id="a897" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:无模型，离线学习</p></blockquote><p id="c6ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在直接效用估计中，代理人使用固定策略执行一系列试验，一个状态的效用是从该状态开始的预期总报酬或预期的<strong class="lb iu">奖励-继续</strong>。</p><ul class=""><li id="fc0d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">以自动驾驶汽车为例，如果汽车在一次试验中在一个网格<code class="fe oe of og oh b">(1, 1)</code>上启动时，总的未来奖励为<code class="fe oe of og oh b">+100</code>。在同一次试验中，赛车重新回到那个格子，从那一点开始，总的未来奖励是<code class="fe oe of og oh b">+300</code>。在另一次试验中，赛车从那个格子出发，未来总奖励为<code class="fe oe of og oh b">+200</code>。该网格的预期回报将是所有试验和所有访问该网格的平均回报，在本例中为<code class="fe oe of og oh b">(100 + 300 + 200) / 3</code>。</li></ul><p id="8e3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong>:给定无限多次尝试，样本平均回报将收敛于真实的预期回报。</p><p id="48f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">反对意见:预期奖励在每次试验结束时更新，这意味着代理人在试验结束前什么也没学到，导致直接效用估计收敛得非常慢。</p><h1 id="95b3" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">№2.自适应动态规划</h1><blockquote class="mj mk ml"><p id="5b64" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:基于模型，离线学习</p></blockquote><p id="64be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在自适应动态规划(ADP)中，主体试图通过经验学习转移和奖励函数。通过计算从当前状态转移到下一个状态采取行动的次数来学习转移函数，而在进入该状态时学习回报函数。给定所学的转换和奖励函数，我们现在可以求解 MDP。</p><ul class=""><li id="67af" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">以自动驾驶汽车为例，假设在给定状态下尝试前进 10 次，如果汽车最终向前移动 8 次，向左移动 2 次，我们得知转移概率为<code class="fe oe of og oh b">T(current state, forward, front state) = 0.8</code>和<code class="fe oe of og oh b">T(current state, forward, left state) = 0.2</code>。</li></ul><p id="45da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong>:由于环境是完全可观测的，所以简单地通过计数就可以很容易地学习转换模型。</p><p id="f839" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点</strong>:代理学习过渡模型的能力限制了性能。这将导致问题对于大的状态空间是难以处理的，因为需要太多的试验来学习转变模型，并且在 MDP 中有太多的方程和未知数要解。</p><h1 id="8eb7" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">№3.时间差学习(TD 学习)</h1><blockquote class="mj mk ml"><p id="9254" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:无模型，离线学习</p></blockquote><p id="afa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在时间差学习中，代理学习效用函数，并在每次转换后以学习速率更新该函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/af60a0d2320204d74309b2b3c298df1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*Hheq2kdL-2IDvyiSFFf4Ng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:效用函数更新方程(方程 23.3)——作者图片</p></figure><p id="e17a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">术语“时间差异”指的是连续状态之间效用的差异，并且基于该误差信号更新效用函数，该误差信号由如图 6 所示的学习速率来缩放。学习率可以是一个固定的参数，也可以是对一个状态的访问次数增加的递减函数，这有助于效用函数的收敛。</p><p id="6976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与每次尝试后学习的直接效用估计相比，TD 学习在每次转换后学习，使其更有效。</p><p id="f8cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 ADP 相比，TD 学习不需要学习转移和奖励函数，使其计算效率更高，但也需要更长的时间来收敛。</p><blockquote class="ol"><p id="9502" class="om on it bd oo op pa pb pc pd pe lu dk translated">ADP 和 TD 学习是离线 RL 算法，但是存在作为在线 RL 算法一部分的主动 ADP 和主动 TD 学习！</p></blockquote><h1 id="e32b" class="mq mr it bd ms mt mu mv mw mx my mz na jz ov ka nc kc ow kd ne kf ox kg ng nh bi translated">№4.探测</h1><blockquote class="mj mk ml"><p id="7f15" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:基于模型、在线学习、主动 ADP</p></blockquote><p id="44a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">探索是一个<strong class="lb iu">主动 ADP 算法</strong>。与被动 ADP 算法类似，代理试图通过经验学习转换和奖励函数，但主动 ADP 算法将学习所有动作的结果，而不仅仅是固定策略。</p><p id="ed4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一个额外的<strong class="lb iu">探索功能</strong>，用于确定代理对在现有策略之外采取行动的“好奇”程度。探索功能应该随着效用的增加而增加，随着经验的增加而减少。</p><ul class=""><li id="de56" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">例如，如果州具有高效用，则探索功能倾向于更频繁地访问该州。由于贪婪的增加，探索功能<em class="mm">随着效用</em>增加。</li><li id="428a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">例如，如果该州以前没有被访问过或者访问的次数不够多，则探索功能倾向于选择现有策略之外的动作。反之，如果多次访问该状态，探索功能就没有那么好奇了。由于好奇心下降，探索功能<em class="mm">随着经验</em>减少。</li></ul><p id="b5f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong>:勘探政策导致向零政策损失(最优政策)的快速收敛。</p><p id="375a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点</strong>:效用估计没有策略估计收敛得快，因为代理不会频繁出现低效用状态，因此不知道这些状态的确切效用。</p><h1 id="3ac8" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">№5.q 学习</h1><blockquote class="mj mk ml"><p id="5252" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:无模型、在线学习、主动 TD 学习、非策略</p></blockquote><p id="6bb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Q-Learning 是一种<strong class="lb iu">主动 TD 学习</strong>算法。图 6 中的更新规则保持不变，但是现在状态的效用被表示为使用<strong class="lb iu"> Q-function </strong>的状态-动作对的效用，因此命名为 Q-Learning。下面的图 7 显示了被动 TD 学习与主动 TD 学习的更新规则的差异。</p><p id="67a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种符号差异是由于被动 RL 具有固定的策略，使得每个状态将仅执行固定的动作，并且效用简单地依赖于状态。而在主动 RL 中，策略将被更新，并且效用现在取决于状态-动作对，因为每个状态可以按照不同的策略执行不同的动作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/7e2aa04513d8a9e9d429f5a8fcaadd64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6mUW2QL4bzockutgN3XdA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 7:被动 TD(上图)与主动 TD(下图，方程 23.7)的效用函数更新方程——作者图片</p></figure><p id="1505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Q-Learning 是<strong class="lb iu">偏离策略</strong>，这意味着目标，或者下一个状态的效用，在下一个状态的可能行动上最大化 Q-function(不考虑当前策略！).这样，我们不需要下一个状态的实际动作。</p><p id="afd0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong>:可以应用于复杂领域，因为它是无模型的，代理不需要学习或应用转换模型。</p><p id="aaeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点</strong>:它不展望未来，在回报稀少的时候可能会有困难。与 ADP 相比，它学习策略的速度较慢，因为本地更新不能确保 Q 值的一致性。</p><h1 id="54c5" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">№6.萨尔萨</h1><blockquote class="mj mk ml"><p id="3d49" class="kz la mm lb b lc ld ju le lf lg jx lh mn lj lk ll mo ln lo lp mp lr ls lt lu im bi translated">类型:无模型、在线学习、主动 TD 学习、基于策略</p></blockquote><p id="be9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SARSA 是一种<strong class="lb iu">主动 TD 学习</strong>算法。算法名 SARSA 来源于算法的组成部分，即状态、动作、奖励、(下一个)状态和(下一个)动作。这意味着在更新 Q 函数之前，SARSA 算法等待在下一个状态中采取下一个动作。相反，Q-Learning 是一种“SARS”算法，因为它不考虑下一个状态的动作。</p><p id="0330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这种差异，SARSA 算法知道在下一个状态中采取的动作，并且不需要在下一个状态中的所有可能动作上最大化 Q 函数。Q-Learning 与 SARSA 的更新规则的差异在下面的图 8 中示出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/e9d8fcc421a4315ff0d480077c928e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Ax5d4ruiN4WvJvNbiC9uA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 8:Q-Learning 的效用函数更新方程(上图)与 SARSA(下图，方程 23.8)——作者图片</p></figure><p id="98e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SARSA 是<strong class="lb iu"> On-Policy </strong>作为目标，或者下一个状态的效用使用当前正在运行的策略中的 Q-function。这样，下一个状态的实际动作就知道了。</p><p id="9e0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注</strong>:如果 Q-Learning 不探索其他动作，在下一个状态下遵循当前策略，则与 SARSA 相同。</p><p id="6b52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">优点</strong>:如果整体策略由另一个代理或程序控制，则符合策略，这样代理就不会脱离策略并尝试其他操作。</p><p id="75ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺点</strong> : SARSA 没有 Q-Learning 灵活，因为它不会偏离策略去探索其他策略。与 ADP 相比，它学习策略的速度较慢，因为本地更新不能确保 Q 值的一致性。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="2f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，这是讨论的 6 种算法，分为不同类型的强化学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/1f7e8c9867042dbed76e1b286c552682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2O82PvmuqpGUp3u5EZlkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 9:6 种强化学习算法的总结</p></figure><p id="609b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这 6 种算法是帮助形成对强化学习的基本理解的基本算法。还有更有效的强化学习算法，如深度 Q 网络(DQN)、深度确定性策略梯度(DDPG)，以及其他具有更实际应用的算法。</p><p id="5209" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你一直读到最后，恭喜你！我一直觉得强化学习很有趣，因为它阐明了人类如何学习，以及我们如何将这些知识传授给机器人(当然还有其他应用，如自动驾驶汽车、国际象棋和阿尔法围棋，仅举几例)。希望你已经了解了更多关于强化学习，不同类型的强化学习，以及说明每种强化学习的算法。</p></div></div>    
</body>
</html>