<html>
<head>
<title>Named Entity Recognition with Deep Learning (BERT) — The Essential Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习命名实体识别(BERT)——基本指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/named-entity-recognition-with-deep-learning-bert-the-essential-guide-274c6965e2d#2022-08-12">https://towardsdatascience.com/named-entity-recognition-with-deep-learning-bert-the-essential-guide-274c6965e2d#2022-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4b2c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从数据准备到NER任务的模型训练——以及如何给自己的句子加标签</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/25c50e122e1e9c67a9a788d64807b270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sRPXVrBvxW9HWTg9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@aaronburden?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚伦·伯顿</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><blockquote class="kz la lb"><p id="3d8a" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">更新:您现在可以了解如何使用Streamlit部署该型号！</p></blockquote><p id="888a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如今，NLP已经成为深度学习的代名词。</p><p id="77d2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">但是，深度学习并不是每个NLP任务的“灵丹妙药”。例如，在句子分类任务中，简单的线性分类器可以相当好地工作。尤其是如果你有一个小的训练数据集。</p><p id="708d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">然而，一些NLP任务随着深度学习而蓬勃发展。其中一个这样的任务就是<strong class="lf iu"> <em class="le">命名实体识别</em> </strong> — NER:</p><blockquote class="mc"><p id="119d" class="md me it bd mf mg mh mi mj mk ml ly dk translated"><em class="mm"> NER是识别</em> <strong class="ak"> <em class="mm"> </em> </strong> <em class="mm">并将命名实体归类到预定义的实体类别中的过程。</em></p></blockquote><p id="9222" class="pw-post-body-paragraph lc ld it lf b lg mn ju li lj mo jx ll lz mp lo lp ma mq ls lt mb mr lw lx ly im bi translated">例如，在句子中:</p><blockquote class="kz la lb"><p id="f435" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">尼克生活在希腊的T21，是一名数据科学家。</p></blockquote><p id="8305" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们有两个实体:</p><ul class=""><li id="81de" class="ms mt it lf b lg lh lj lk lz mu ma mv mb mw ly mx my mz na bi translated"><strong class="lf iu">尼克，</strong>也就是一个<strong class="lf iu">的‘人’。</strong></li><li id="c721" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated"><strong class="lf iu">希腊，</strong>即<strong class="lf iu"> </strong>的一个<strong class="lf iu">‘地点’。</strong></li></ul><p id="87f7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，根据上面的句子，分类器应该能够定位这两个术语('<strong class="lf iu">尼克'，'希腊'【T34)，并正确地将它们分别分类为'<strong class="lf iu">人</strong>和'<strong class="lf iu">地点</strong>'。</strong></p><p id="ed68" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在本教程中，我们将建立一个NER模型，使用<strong class="lf iu"> HugginFace变形金刚</strong>。</p><p id="6a47" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">让我们开始吧！</p><h1 id="138e" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">加载数据</h1><p id="349a" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">我们将使用已经包含在<strong class="lf iu"> HugginFace数据集</strong>库中的<a class="ae ky" href="https://huggingface.co/datasets/wnut_17" rel="noopener ugc nofollow" target="_blank"><strong class="lf iu">wnut _ 17</strong></a><strong class="lf iu">【1】</strong>数据集。</p><h2 id="2858" class="od nh it bd ni oe of dn nm og oh dp nq lz oi oj ns ma ok ol nu mb om on nw oo bi translated">浏览数据集</h2><p id="13e2" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">该数据集侧重于在新兴讨论的背景下识别不寻常的、以前未见过的实体。它包含<em class="le"> 5690 </em>文件，分为<em class="le">培训</em>套、vali <em class="le"> d </em>套和<em class="le">测试</em>套。文本句子被标记成单词。让我们加载数据集:</p><pre class="kj kk kl km gt op oq or os aw ot bi"><span id="436b" class="od nh it oq b gy ou ov l ow ox">wnut = load_dataset(“wnut_17”)</span></pre><p id="ce6e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们得到以下结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="8076" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">接下来，我们打印<code class="fe pa pb pc oq b">ner_tags </code>——我们模型的预定义实体:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="7bbb" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">每个<code class="fe pa pb pc oq b">ner_tag</code>描述一个实体。可以是以下之一:<strong class="lf iu">公司</strong>、<strong class="lf iu">创意作品</strong>、<strong class="lf iu">团体</strong>、<strong class="lf iu">地点</strong>、<strong class="lf iu">人物、</strong>和<strong class="lf iu">产品。</strong></p><p id="af6e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">每个<code class="fe pa pb pc oq b">ner_tag</code>前面的字母表示实体的令牌位置:</p><ul class=""><li id="21bd" class="ms mt it lf b lg lh lj lk lz mu ma mv mb mw ly mx my mz na bi translated"><strong class="lf iu"> <em class="le"> B </em> - </strong>表示一个实体的开始。</li><li id="ae92" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated"><strong class="lf iu"> <em class="le"> I </em> - </strong>表示一个令牌包含在同一个实体内部(例如"<em class="le"> York" </em>令牌是"<em class="le"> New York </em>"实体的一部分)。</li><li id="f457" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated"><strong class="lf iu"> <em class="le"> 0 </em> </strong>表示该令牌不对应任何实体。</li></ul><p id="2cc0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们还创建了<code class="fe pa pb pc oq b">id2tag</code> <em class="le">字典</em>，它将每个标签映射到它的<code class="fe pa pb pc oq b">ner_tag</code>——这将在以后派上用场。</p><h2 id="24cd" class="od nh it bd ni oe of dn nm og oh dp nq lz oi oj ns ma ok ol nu mb om on nw oo bi translated">重组培训和验证数据集</h2><p id="fd1c" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">我们的数据集没有那么大。请记住，<strong class="lf iu">变形金刚</strong>需要大量数据来利用其卓越的性能。</p><p id="07b9" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">为了解决这个问题，我们将<em class="le">训练</em>和<em class="le">验证</em>数据集连接成单个<em class="le">训练</em>数据集。出于验证目的，测试数据集将保持原样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><h2 id="669d" class="od nh it bd ni oe of dn nm og oh dp nq lz oi oj ns ma ok ol nu mb om on nw oo bi translated">培训示例</h2><p id="1ecb" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">让我们打印数据集中的第三个训练示例。<strong class="lf iu">我们将在整个教程中使用该示例作为参考:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="5c6d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">'<strong class="lf iu"> Pxleyes' </strong>令牌被归类为<code class="fe pa pb pc oq b">B-corporation</code>(一个<strong class="lf iu">公司的开始)。其余的记号是不相关的——它们不代表任何实体。</strong></p><h1 id="8c05" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">预处理</h1><p id="a898" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">接下来，我们标记我们的数据。与其他用例相反，<strong class="lf iu"> NER </strong>任务的令牌化需要特殊处理。</p><p id="de05" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们将使用HugginFace库中的<code class="fe pa pb pc oq b">bert-base-uncased</code>模型和标记器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="82e7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">Transformer模型大多使用<strong class="lf iu">基于子词的记号化器。</strong></p><p id="c537" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在标记化过程中，一些单词可能会被拆分成两个或多个单词。这是一种标准的做法，因为罕见的单词可以分解成有意义的符号。例如，<strong class="lf iu"> BERT </strong>模型默认实现了<strong class="lf iu">字节对编码(BPE)令牌化。</strong></p><p id="06f4" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">让我们对我们的样本训练示例进行符号化，看看这是如何工作的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="e7a7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这是最初的培训示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="ca76" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这就是<strong class="lf iu"> BERT的</strong>标记器如何标记训练示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="d393" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">请注意，有两个重要问题:</p><ul class=""><li id="111b" class="ms mt it lf b lg lh lj lk lz mu ma mv mb mw ly mx my mz na bi translated">添加特殊标记<code class="fe pa pb pc oq b">[CLS]</code>和<code class="fe pa pb pc oq b">[SEP]</code>。</li><li id="f45f" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated">令牌“<strong class="lf iu">pxl eyes”</strong>被拆分成3个子令牌:<code class="fe pa pb pc oq b">p</code>、<code class="fe pa pb pc oq b">##xley</code>、<strong class="lf iu">、</strong>和<code class="fe pa pb pc oq b">##es</code>。</li></ul><p id="d8ef" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">换句话说，标记化造成了输入和标签之间的不匹配。因此，我们按照以下方式重新排列令牌和标签:</p><ol class=""><li id="efe5" class="ms mt it lf b lg lh lj lk lz mu ma mv mb mw ly pd my mz na bi translated">每个单词标记被映射到其对应的<code class="fe pa pb pc oq b">ner_tag</code>。</li><li id="9703" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly pd my mz na bi translated">我们将标签<code class="fe pa pb pc oq b">-100</code>分配给特殊记号<code class="fe pa pb pc oq b">[CLS]</code>和<code class="fe pa pb pc oq b">[SEP]</code>，因此损失函数忽略它们。默认情况下，PyTorch在损耗计算过程中忽略<code class="fe pa pb pc oq b">-100</code> <strong class="lf iu"> </strong>值。</li><li id="a3b1" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly pd my mz na bi translated">对于子词，我们只标记给定词的第一个标记。因此，我们将<code class="fe pa pb pc oq b">-100</code>分配给同一个单词的其他子发音。</li></ol><p id="7986" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">比如令牌<strong class="lf iu">px eyes</strong>被标注为<code class="fe pa pb pc oq b">1</code> ( <code class="fe pa pb pc oq b">B-corporation</code>)。它被标记为<code class="fe pa pb pc oq b">[‘p’, ‘##xley’, ‘##es’]</code>，标记对齐后，标签应变成<code class="fe pa pb pc oq b">[1, -100, -100]</code></p><p id="fb8a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们在<code class="fe pa pb pc oq b">tokenize_and_align_labels()</code>函数中实现该功能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="7864" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">就是这样！让我们调用我们的自定义标记化函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="9dd7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">下表准确显示了我们的样本训练示例的标记化输出:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><h1 id="4216" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">微调模型</h1><p id="bc24" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">我们现在准备建立我们的深度学习模型。</p><p id="e41e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们加载<code class="fe pa pb pc oq b">bert-base-uncased</code>预训练模型，并使用我们的数据对其进行微调。</p><p id="f304" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">但是首先，我们应该训练一个简单的分类器作为基线模型。</p><h2 id="f0bc" class="od nh it bd ni oe of dn nm og oh dp nq lz oi oj ns ma ok ol nu mb om on nw oo bi translated">基线模型</h2><p id="a8ec" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">对于基线分类器来说，最明显的选择是用整个训练数据集中最频繁出现的实体来标记每个标记—<code class="fe pa pb pc oq b">O</code>实体:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="c2d1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果我们用它所属的句子的最频繁的标签来标记每个记号，基线分类器变得不那么简单:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="9b91" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，我们使用第二个模型作为基线。</p><h2 id="ce4c" class="od nh it bd ni oe of dn nm og oh dp nq lz oi oj ns ma ok ol nu mb om on nw oo bi translated">用于命名实体识别的BERT</h2><p id="6588" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated"><strong class="lf iu">数据整理器</strong>将训练样本分批在一起，同时应用填充以使它们大小相同。排序器不仅填充输入，还填充标签:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="8cfb" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">关于评估，由于我们的数据集是不平衡的，<strong class="lf iu">我们不能只依靠准确性</strong>。</p><p id="e139" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，我们还将测量<strong class="lf iu">精度</strong>和<strong class="lf iu">召回</strong>。这里，我们将加载包含在<strong class="lf iu">数据集</strong>库中的<code class="fe pa pb pc oq b"><a class="ae ky" href="https://github.com/chakki-works/seqeval" rel="noopener ugc nofollow" target="_blank">seqeval</a></code>指标。该指标通常用于<strong class="lf iu">词性标注</strong>和<strong class="lf iu"> NER </strong>任务。</p><p id="999f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">让我们将它应用到我们的参考培训示例中，看看它是如何工作的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="e182" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">注意:</strong>记住，损失函数在训练时会忽略所有标记有<code class="fe pa pb pc oq b">-100</code>的记号。我们的评估功能也应该考虑这些信息。</p><p id="b964" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">因此，<code class="fe pa pb pc oq b">compute_metrics</code> <strong class="lf iu"> </strong>函数的定义稍有不同——我们通过忽略所有标有<code class="fe pa pb pc oq b">-100</code>的内容来计算<strong class="lf iu"> <em class="le">精度</em></strong><strong class="lf iu"><em class="le">召回</em></strong><strong class="lf iu"><em class="le">f1分数、</em> </strong>和<strong class="lf iu"> <em class="le">精度</em> </strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="ba83" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">最后，我们实例化<code class="fe pa pb pc oq b">Trainer</code>类来微调我们的模型。注意<strong class="lf iu">提前停止</strong>回调的用法:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="a284" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这些是我们的培训指标:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/13c2460a75b7b3c33450ccf6c141ae53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*WtoHg75-3kuikjCt_FKUBQ.png"/></div></figure><p id="2ac7" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">与基线模型相比，该模型实现了更好的验证准确性。此外，如果我们使用更大的模型，或者让模型训练更多的时期而不应用<strong class="lf iu">提前停止</strong>回调，我们可以获得更好的<code class="fe pa pb pc oq b">f1-score </code>。</p><h1 id="d1dc" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">测试集评估</h1><p id="19a1" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">对于我们的测试集，我们使用与之前相同的方法。</p><p id="b905" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><code class="fe pa pb pc oq b">seqeval</code>度量还输出<strong class="lf iu">每个类的度量</strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="4fc8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><code class="fe pa pb pc oq b">location</code>和<code class="fe pa pb pc oq b">person</code>实体得分最高，而<code class="fe pa pb pc oq b">group</code>得分最低。</p><h1 id="c4c6" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">获得预测</h1><p id="b56c" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">最后，我们创建一个对我们自己的句子执行实体识别的函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="4ec0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">让我们试几个例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="9002" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">模型已经成功标记了这两个国家！看一看<strong class="lf iu">美国</strong>:</p><ul class=""><li id="24f3" class="ms mt it lf b lg lh lj lk lz mu ma mv mb mw ly mx my mz na bi translated">“<strong class="lf iu">联队</strong>被正确标记为<code class="fe pa pb pc oq b">B-location</code>。</li><li id="3550" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated"><strong class="lf iu">状态</strong>被正确标记为<code class="fe pa pb pc oq b">I-location</code>。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="4d04" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">苹果再次被正确地标记为<strong class="lf iu">公司</strong>。此外，我们的模型正确地识别了苹果<strong class="lf iu">产品。</strong></p><h1 id="ceb1" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">结束语</h1><p id="943b" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated"><strong class="lf iu"> <em class="le">命名实体识别</em> </strong>是一项基本的自然语言处理任务，有着众多的实际应用。</p><p id="3657" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">尽管HugginFace库已经为这个过程创建了一个超级友好的API，但是仍然有一些混乱的地方。</p><p id="05b8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我希望这篇教程对他们有所启发。这篇文章的源代码可以在<a class="ae ky" href="https://jovian.ai/nkafr/ner-transformers" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><h1 id="907a" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">感谢您的阅读！</h1><ul class=""><li id="80f5" class="ms mt it lf b lg ny lj nz lz pf ma pg mb ph ly mx my mz na bi translated">订阅我的<a class="ae ky" href="https://towardsdatascience.com/subscribe/@nikoskafritsas" rel="noopener" target="_blank">简讯</a>！</li><li id="0f9b" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated">在Linkedin上关注我！</li><li id="6c23" class="ms mt it lf b lg nb lj nc lz nd ma ne mb nf ly mx my mz na bi translated">加入Medium！(附属链接)</li></ul></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><h1 id="c19a" class="ng nh it bd ni nj pp nl nm nn pq np nq jz pr ka ns kc ps kd nu kf pt kg nw nx bi translated">参考</h1><p id="ea06" class="pw-post-body-paragraph lc ld it lf b lg ny ju li lj nz jx ll lz oa lo lp ma ob ls lt mb oc lw lx ly im bi translated">1.WNUT 2017 (WNUT 2017新兴和稀有实体认可)，许可证:CC BY 4.0，<a class="ae ky" href="https://huggingface.co/datasets/wnut_17" rel="noopener ugc nofollow" target="_blank">来源</a></p></div></div>    
</body>
</html>