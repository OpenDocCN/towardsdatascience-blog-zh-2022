<html>
<head>
<title>Unsupervised Regression for Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督回归降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-regression-for-dimensionality-reduction-88bd80bda4bf#2022-11-07">https://towardsdatascience.com/unsupervised-regression-for-dimensionality-reduction-88bd80bda4bf#2022-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="08dd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用回归计算低维嵌入</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/16a709d3415fc824ce708418225a780b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sIxyPOGm614YhiCjlPmBYg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://unsplash.com/@anniespratt" rel="noopener ugc nofollow" target="_blank">安妮·斯普拉特</a>在<a class="ae kv" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="8813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个数据科学和人工智能的一年级学生都知道回归是一种监督学习方法。最初这是正确的，但是我们可以通过无监督回归[1]将事情颠倒过来进行降维。在降维中，我们寻找高维数据的低维嵌入。对于可视化或预处理等特定目的，任务是找到满足距离、密度和邻域保持等约束的低维嵌入。例如，数据空间中的紧密模式应该在低维空间中有紧密的对应关系，而不同模式的嵌入应该相距很远。</p><h1 id="5773" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">无监督回归</h1><p id="f984" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于无监督回归，我们需要允许映射到多维标签空间的回归方法。例子是最近邻回归和核回归，即Nadaraya-Watson估计量。回归方法从低维潜在空间映射到高维数据空间。问题变成了:低维模式如何映射到高维模式？</p><p id="b66c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一下，我们使用核回归从2维潜在空间映射到N维数据空间。任务是将随机选择的<em class="mp">n</em>2<em class="mp">2</em>-维点<strong class="ky ir"><em class="mp">x</em></strong><em class="mp">=</em><strong class="ky ir"><em class="mp">x</em></strong><em class="mp">【₁,…】，</em><strong class="ky ir"><em class="mp">x</em></strong><em class="mp"/>(我们不知道但想得到)用内核回归到<em class="mp">n</em>-维空间，让优化问题是最小化重建数据<strong class="ky ir"><em class="mp">y</em></strong><em class="mp">=</em><strong class="ky ir"><em class="mp">y</em></strong><em class="mp">【₁,…】，</em><strong class="ky ir"><em class="mp">y</em></strong><em class="mp">【ₙ】</em>我们寻求嵌入，即最小化数据空间重建误差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b870aa28ff46cbb824453e400e630760.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*4t37-UlkGXl3sHdXfcba8g.png"/></div></figure><p id="4258" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过改变<em class="mp"> X </em>。这里使用的范数是Frobenius范数，它是一个矩阵的所有元素之和的平方根。</p><p id="e027" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如何改变<strong class="ky ir"> <em class="mp"> X </em> </strong>使<em class="mp"> E </em>最小化？答案是:通过梯度下降或者随机抽样。例如，无监督核回归(UKR) [1]使用梯度下降。具有高斯核的核回归方程是可导的。基于数据空间重构误差<em class="mp"> E </em>，可以导出UKR的梯度，并在潜在空间<strong class="ky ir"> <em class="mp"> X </em> </strong>中执行梯度下降。UKR将主要获得降维结果中的模式密度信息，因为核回归是一种基于密度的方法。空间中图案周围的密度越高，其对预测的贡献就越大。在UKR的情况下，模型必须被正则化以避免过度拟合。否则，模型只能重建模式，而没有任何归纳能力。</p><h1 id="541d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">简单的例子</h1><p id="c004" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们在这里更详细地看一个简单的例子是无监督最近邻(UNN) [2]，它基于随机采样而不是梯度下降。k-最近邻用于在潜在空间中以随机采样的方式逐个模式地嵌入数据。让我们假设我们希望我们的嵌入存在于空间<em class="mp"> [0，1】。</em>第一图案<strong class="ky ir"> <em class="mp"> y </em> </strong> <em class="mp"> ₁ </em>嵌入任意位置，例如潜在空间的中间(0.5，0.5)。每个新的图案<em class="mp"/><strong class="ky ir"><em class="mp">y</em></strong><em class="mp">ᵢ</em>都是通过随机采样<em class="mp">【0，1】</em>中的<em class="mp"> ν </em>点，并选择图案重构误差最小的<strong class="ky ir"> <em class="mp"> x </em> </strong> <em class="mp"> * </em>，即<strong class="ky ir"><em class="mp"/></strong>与基于<strong class="ky ir">的最近邻预测之差来嵌入的</strong></p><p id="ae09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Python中，UNN的代码如下所示，使用scikit-learn中的<em class="mp"> KNeighborsregressor </em>进行K近邻预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="23d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在著名而简单的IRIS数据集上，该脚本生成了以下嵌入，参见图1。嵌入的颜色基于类别标签，表明UNN将不同的类别区分开来，几乎没有例外。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/cfd7fd818cf2b5de52c5b377f475899b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViMX27VNZrNxt_mpr43X7g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:嵌入UNN虹膜数据集，K=2，使用Scikit-learn虹膜样本脚本进行绘图</p></figure><h1 id="be90" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="7ce6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">无监督回归不仅是一种有趣的用于解决降维问题的回归变体，而且还被证明能够产生良好的嵌入。结果取决于所采用的回归方法的特征:最近邻将一个模式放置在已经嵌入的模式的邻域中。核回归考虑了相邻模式的密度信息。核PCA、ISOMAP、LLE、T-SNE和自动编码器等经典的降维竞争对手始终是不错的选择，但对于下一个嵌入任务，您可能希望尝试无监督回归。</p><p id="a20e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mp">除特别注明外，所有图片均为作者所有。</em></p><h1 id="c777" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="c8cb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] P. Meinicke，S. Klanke，R. Memisevic，H. J. Ritter，无监督核回归的主表面。IEEE Trans。模式分析与机器智能27(9):1379–1391(2005)</p><p id="899e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] O. Kramer，无监督最近邻回归降维，软计算19(6):1647–1661(2015)</p></div></div>    
</body>
</html>