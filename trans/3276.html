<html>
<head>
<title>A Guide to Using Caret in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在R中使用插入符号的指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-using-caret-in-r-71dec0bda208#2022-07-21">https://towardsdatascience.com/a-guide-to-using-caret-in-r-71dec0bda208#2022-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7c49" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习如何使用R使用caret(分类和回归训练)包</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/def62f1cbf719275d93c2dae6e83127f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*epNz5LyZQYaD9et0"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@heidijfin" rel="noopener ugc nofollow" target="_blank"> Heidi Fin </a> @unsplash.com拍摄</p></figure><p id="8342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> C </span> aret是r中一个非常强大的机器学习库，<code class="fe me mf mg mh b">caret</code>以其灵活性为主要特点，使你能够使用一个简单的<code class="fe me mf mg mh b">train</code>函数训练不同类型的算法。这个抽象层提供了一个通用接口来训练R中的模型，只需要调整一个参数——<code class="fe me mf mg mh b">method</code>。</p><p id="8073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">caret</code>(用于分类和回归训练)是R 中<a class="ae ky" href="https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">最流行的机器学习库之一。为了增加灵活性，我们获得了嵌入超参数调整和交叉验证——这两项技术将提高我们算法的泛化能力。</a></p><p id="aa53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本指南中，我们将从四个不同的方面来探讨该产品包:</p><ul class=""><li id="f11f" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated">我们将从学习<strong class="lb iu">如何通过改变<code class="fe me mf mg mh b">method</code>参数来训练不同的模型</strong>开始。对于我们将训练的每个算法，我们将接触到新的概念，如超参数调整、交叉验证、因子和其他有意义的细节。</li><li id="0ce9" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">然后，我们将学习如何设置我们自己的<strong class="lb iu">自定义交叉验证函数</strong>，然后针对<strong class="lb iu">不同的优化指标调整我们的算法。</strong></li><li id="9418" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">试验我们自己的<strong class="lb iu">超参数调谐。</strong></li><li id="09c1" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">我们将通过检查<code class="fe me mf mg mh b">predict</code>如何与不同的<code class="fe me mf mg mh b">caret</code>型号一起工作来总结一切。</li></ul><p id="8358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简单起见，并且因为我们想把重点放在库本身，我们将使用R:</p><ul class=""><li id="3891" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated"><em class="mw"> iris </em>数据集，这是一个非常著名的数据集，将代表我们的分类任务。</li><li id="881b" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">将用作我们回归任务的<em class="mw"> mtcars </em>数据集。</li></ul><p id="2faa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将做一个小小的调整，使<em class="mw"> iris </em>问题变成二进制问题(不幸的是<code class="fe me mf mg mh b">glm</code>，R中的逻辑回归实现不支持多类问题):</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="8bda" class="nb nc it mh b gy nd ne l nf ng">iris$target &lt;- ifelse(<br/>  iris$Species == 'setosa',<br/>  1,<br/>  0<br/>)</span></pre><p id="d13a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们想要评估predict方法，所以让我们先将两个数据帧分成train和test我将使用个人最喜欢的方法，<code class="fe me mf mg mh b">caTools</code>:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="d5dd" class="nb nc it mh b gy nd ne l nf ng">library(caTools)</span><span id="1316" class="nb nc it mh b gy nh ne l nf ng"># Train Test Split on both Iris and Mtcars<br/>train_test_split &lt;- function(df) {<br/>  set.seed(42)<br/>  sample = sample.split(df, SplitRatio = 0.8)<br/>  train = subset(df, sample == TRUE)<br/>  test  = subset(df, sample == FALSE)<br/>  return (list(train, test))<br/>}</span><span id="65f0" class="nb nc it mh b gy nh ne l nf ng"># Unwrapping mtcars<br/>mtcars_train &lt;- train_test_split(mtcars)[[1]]<br/>mtcars_test &lt;- train_test_split(mtcars)[[2]]</span><span id="f53d" class="nb nc it mh b gy nh ne l nf ng"># Unwrapping iris<br/>iris_train &lt;- train_test_split(iris)[[1]]<br/>iris_test &lt;- train_test_split(iris)[[2]]</span></pre><p id="d0a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一切就绪，我们开始吧！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="768b" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合线性回归</h1><p id="6d06" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">我们的第一个<code class="fe me mf mg mh b">caret</code>例子是拟合一个简单的线性回归。线性回归是最著名的算法之一。在base R中，您可以使用<code class="fe me mf mg mh b">lm</code>方法安装一个。在<code class="fe me mf mg mh b">caret</code>中，您可以使用以下命令进行同样的操作:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="f333" class="nb nc it mh b gy nd ne l nf ng">lm_model &lt;- train(mpg ~ hp + wt + gear + disp, <br/>      data = mtcars_train, <br/>      method = "lm")</span></pre><p id="b6a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">train</code>是<code class="fe me mf mg mh b">caret</code>图书馆的中心功能。它使用指定的<code class="fe me mf mg mh b">method</code>使算法适合数据。在这种情况下，我们在<code class="fe me mf mg mh b">mtcars</code>数据框架上拟合一条回归线，试图使用汽车的马力、重量、齿轮数和发动机排量来预测每加仑的英里数。</p><p id="bc32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，该方法与用于训练模型的独立函数紧密相关。例如，<code class="fe me mf mg mh b">method='lm'</code>会产生类似于<code class="fe me mf mg mh b">lm</code>函数的东西。很酷的事情，我们可以把<code class="fe me mf mg mh b">caret</code>生成的<code class="fe me mf mg mh b">lm_model</code>当作R中的其他线性模型——调用<code class="fe me mf mg mh b">summary</code>是可能的:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="95bc" class="nb nc it mh b gy nd ne l nf ng">summary(lm_model)</span><span id="7d8d" class="nb nc it mh b gy nh ne l nf ng">Call:<br/>lm(formula = .outcome ~ ., data = dat)</span><span id="1321" class="nb nc it mh b gy nh ne l nf ng">Residuals:<br/>    Min      1Q  Median      3Q     Max <br/>-3.6056 -1.8792 -0.4769  1.0658  5.6892</span><span id="d1bf" class="nb nc it mh b gy nh ne l nf ng">Coefficients:<br/>            Estimate Std. Error t value Pr(&gt;|t|)    <br/>(Intercept) 31.67555    6.18241   5.123 7.11e-05 ***<br/>hp          -0.04911    0.01883  -2.609  0.01777 *  <br/>wt          -3.89388    1.28164  -3.038  0.00707 ** <br/>gear         1.49329    1.29311   1.155  0.26327    <br/>disp         0.01265    0.01482   0.854  0.40438    <br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="cf91" class="nb nc it mh b gy nh ne l nf ng">Residual standard error: 2.915 on 18 degrees of freedom<br/>Multiple R-squared:  0.8246, Adjusted R-squared:  0.7856 <br/>F-statistic: 21.15 on 4 and 18 DF,  p-value: 1.326e-06</span></pre><p id="d813" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您使用<code class="fe me mf mg mh b">lm(mpg ~ hp + wt + gear + disp, data = mtcars_train)</code>拟合线性模型，您将获得完全相同的系数。因此..使用<code class="fe me mf mg mh b">caret</code>的确切优势是什么？</p><p id="62f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中最重要的是我们将在下一节看到的——改变模型是如此简单！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="c6f0" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合逻辑回归</h1><p id="b963" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">要在<code class="fe me mf mg mh b">caret</code>中的模型之间进行更改，我们只需更改<code class="fe me mf mg mh b">train</code>函数中的<code class="fe me mf mg mh b">method</code>——让我们在iris数据框架上拟合一个逻辑回归:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="cabb" class="nb nc it mh b gy nd ne l nf ng">glm_model &lt;- train(target ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, <br/>                  data = iris_train, <br/>                  method = "glm",<br/>                  family = "binomial")</span></pre><p id="b8f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe me mf mg mh b">method='glm'</code>我们打开了训练逻辑回归的可能性。注意，我还可以通过将方法的其他参数传递给train函数来访问它们— <code class="fe me mf mg mh b">family='binomial'</code>是我们可以用来让R知道我们想要训练一个逻辑回归的参数。根据我们正在使用的<code class="fe me mf mg mh b">method</code>在<code class="fe me mf mg mh b">train</code>函数中如此容易地扩展参数，是<code class="fe me mf mg mh b">caret</code>库的另一个伟大特性。</p><p id="aa3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，我们可以看到<code class="fe me mf mg mh b">summary</code>的逻辑回归结果:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="c810" class="nb nc it mh b gy nd ne l nf ng">summary(glm_model)</span><span id="587f" class="nb nc it mh b gy nh ne l nf ng">Call:<br/>NULL</span><span id="e070" class="nb nc it mh b gy nh ne l nf ng">Deviance Residuals: <br/> Min 1Q Median 3Q Max <br/>-2.487e-05 -2.110e-08 -2.110e-08 2.110e-08 1.723e-05</span><span id="b6d2" class="nb nc it mh b gy nh ne l nf ng">Coefficients:<br/> Estimate Std. Error z value Pr(&gt;|z|)<br/>(Intercept) 45.329 575717.650 0 1<br/>Sepal.Length -5.846 177036.957 0 1<br/>Sepal.Width 11.847 88665.772 0 1<br/>Petal.Length -16.524 126903.905 0 1<br/>Petal.Width -7.199 185972.824 0 1</span><span id="43db" class="nb nc it mh b gy nh ne l nf ng">(Dispersion parameter for binomial family taken to be 1)</span><span id="dcfd" class="nb nc it mh b gy nh ne l nf ng">Null deviance: 1.2821e+02 on 99 degrees of freedom<br/>Residual deviance: 1.7750e-09 on 95 degrees of freedom<br/>AIC: 10</span><span id="0849" class="nb nc it mh b gy nh ne l nf ng">Number of Fisher Scoring iterations: 25</span></pre><p id="9531" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在不要太担心结果——重要的是你理解使用<code class="fe me mf mg mh b">train</code>功能在模型之间切换是多么容易。</p><p id="b1de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从线性回归到逻辑回归的跳跃似乎并不令人印象深刻..我们仅仅局限于线性模型吗？没有。接下来，让我们看看一些基于树的模型。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="159c" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合决策树</h1><p id="5eed" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">我们还可以通过切换<code class="fe me mf mg mh b">method</code>来使用<code class="fe me mf mg mh b">train</code>拟合基于树的模型，就像我们在线性和逻辑回归之间切换时所做的那样:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="da14" class="nb nc it mh b gy nd ne l nf ng">d.tree &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                  data = mtcars_train, <br/>                  method = "rpart")</span></pre><p id="14d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最酷的事？这是一个<code class="fe me mf mg mh b">rpart</code>对象——我们甚至可以使用<code class="fe me mf mg mh b">rpart.plot</code>来绘制它:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="f6fa" class="nb nc it mh b gy nd ne l nf ng">library(rpart.plot)<br/>rpart.plot(d.tree$finalModel)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/523f40e8f98054cc2933a0c6a6cfca83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*87iN-4zKog2GeAc3pZGA1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成的决策树—使用插入符号的序列—按作者排序的图像</p></figure><p id="18b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了绘制决策树，我们只需要访问<code class="fe me mf mg mh b">d.tree</code>的<code class="fe me mf mg mh b">finalModel</code>对象，它是<code class="fe me mf mg mh b">rpart</code>对象的模拟。</p><p id="79e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">caret</code>或单独使用<code class="fe me mf mg mh b">rpart</code>功能的显著区别在于，后者不会执行任何<a class="ae ky" rel="noopener" target="_blank" href="/5-decision-tree-hyperparameters-to-enhance-your-tree-algorithms-aee2cebe92c8">超参数调整</a>。另一方面，caret的<code class="fe me mf mg mh b">rpart</code>已经用复杂度参数(<code class="fe me mf mg mh b">cp</code>)执行了一些小的“超参数调整”，正如我们在详述我们的<code class="fe me mf mg mh b">d.tree</code>时所看到的:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="5b3b" class="nb nc it mh b gy nd ne l nf ng">d.tree</span><span id="15da" class="nb nc it mh b gy nh ne l nf ng">CART</span><span id="8262" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="48a3" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 23, 23, 23, 23, 23, 23, ... <br/>Resampling results across tuning parameters:</span><span id="d351" class="nb nc it mh b gy nh ne l nf ng">cp         RMSE      Rsquared   MAE     <br/>  0.0000000  4.805918  0.4932426  3.944907<br/>  0.3169698  4.805918  0.4932426  3.944907<br/>  0.6339397  4.953153  0.5061275  4.105045</span><span id="bbd5" class="nb nc it mh b gy nh ne l nf ng">RMSE was used to select the optimal model using the smallest value.<br/>The final value used for the model was cp = 0.3169698.</span></pre><p id="58d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，最低的RMSE(均方根误差)最终在<code class="fe me mf mg mh b">cp</code> = 0或<code class="fe me mf mg mh b">cp</code> = 0.32之间，并且<code class="fe me mf mg mh b">caret</code>选择其中一个值作为我们决策树的最终值。我们也可以在<code class="fe me mf mg mh b">train</code>函数中执行自定义的超参数调整，稍后我们会在本文中看到！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="7b45" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合随机森林</h1><p id="d6bc" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">为了适应randomForest，我们可以使用几个<code class="fe me mf mg mh b">methods</code>——就我个人而言，我喜欢使用<code class="fe me mf mg mh b">ranger</code>实现，方法是在<code class="fe me mf mg mh b">train</code>函数的参数中提供:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="01ba" class="nb nc it mh b gy nd ne l nf ng">r.forest &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                data = mtcars_train, <br/>                method = "ranger")</span></pre><p id="1fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看我们的<code class="fe me mf mg mh b">r.forest</code>对象:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="fbb5" class="nb nc it mh b gy nd ne l nf ng">r.forest</span><span id="d9b1" class="nb nc it mh b gy nh ne l nf ng">Random Forest</span><span id="fb59" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="dd16" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 23, 23, 23, 23, 23, 23, ... <br/>Resampling results across tuning parameters:</span><span id="61df" class="nb nc it mh b gy nh ne l nf ng">mtry  splitrule   RMSE      Rsquared   MAE     <br/>  2     variance    2.672097  0.8288142  2.160861<br/>  2     extratrees  2.844143  0.8054631  2.285504<br/>  3     variance    2.700335  0.8212250  2.189822<br/>  3     extratrees  2.855688  0.8024398  2.295482<br/>  4     variance    2.724485  0.8144731  2.213220<br/>  4     extratrees  2.892709  0.7918827  2.337236</span><span id="f9b2" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'min.node.size' was held constant at a value of 5<br/>RMSE was used to select the optimal model using the smallest value.<br/>The final values used for the model were mtry = 2, splitrule =<br/> variance and min.node.size = 5.</span></pre><p id="c6ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像在我们的决策树示例中一样，<code class="fe me mf mg mh b">caret</code>在选择最终使用的模型之前，也在自己执行一些简单的超参数调优。在这种情况下，<code class="fe me mf mg mh b">train</code>用<code class="fe me mf mg mh b">mtry</code>和<code class="fe me mf mg mh b">splitrule</code>执行一个双参数超参数调整。它选择了<code class="fe me mf mg mh b">mtry</code> = 2和<code class="fe me mf mg mh b">splitrule</code> =方差作为最终模型。</p><p id="e2dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，<code class="fe me mf mg mh b">train</code>函数使用的超参数与<code class="fe me mf mg mh b">method</code>中可用的超参数紧密相关！</p><p id="a19c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请记住，这是第一个我们使用非R自带的<code class="fe me mf mg mh b">method</code>的<code class="fe me mf mg mh b">train</code>示例——当你试图运行上面的代码时，R会提示你安装<code class="fe me mf mg mh b">ranger</code>库，如果你没有的话。这是另一个很酷的<code class="fe me mf mg mh b">caret</code>细节——我们可以使用许多base R中没有的模型。</p><p id="2d77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">底线是，<code class="fe me mf mg mh b">train</code>函数只是这个高层次的API，它用一个公共接口为我们管理关于模型训练的一切。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="3c97" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">装配XGBoost</h1><p id="f620" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">Kaggle竞赛中使用的最著名的模型类型之一是梯度推进模型。这些类型的基于树的模型以其稳定性和性能而闻名。</p><p id="6448" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost 是这些boosting模型的一个实现，它依靠模型的误差来提高它们的性能。与随机森林一样，我们可以在<code class="fe me mf mg mh b">caret</code>中访问不同的boosting实现——这里有一个使用<code class="fe me mf mg mh b">xbgTree</code>库的小例子:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="7afe" class="nb nc it mh b gy nd ne l nf ng">xg.boost &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                  data = mtcars_train, <br/>                  method = "xgbTree")</span></pre><p id="6dbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">xgbTree</code>实现执行了一些广泛的调优(可能需要一段时间运行)——如果我们看一下输出:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="2c89" class="nb nc it mh b gy nd ne l nf ng">xg.boost</span><span id="4797" class="nb nc it mh b gy nh ne l nf ng">eXtreme Gradient Boosting</span><span id="d2c1" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="aa0b" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 23, 23, 23, 23, 23, 23, ... <br/>Resampling results across tuning parameters:</span><span id="9e26" class="nb nc it mh b gy nh ne l nf ng">eta  max_depth  colsample_bytree  subsample  nrounds  RMSE    <br/>  0.3  1          0.6               0.50        50      2.946174<br/>  0.3  1          0.6               0.50       100      2.944830<br/>  0.3  1          0.6               0.50       150      2.962090<br/>  0.3  1          0.6               0.75        50      3.112695<br/>  0.3  1          0.6               0.75       100      3.099010<br/>  0.3  1          0.6               0.75       150      3.110219<br/>  0.3  1          0.6               1.00        50      3.077528<br/>  0.3  1          0.6               1.00       100      3.097813<br/>  0.3  1          0.6               1.00       150      3.109069<br/>  0.3  1          0.8               0.50        50      3.097415<br/>  0.3  1          0.8               0.50       100      3.097322<br/>  0.3  1          0.8               0.50       150      3.098146<br/>  0.3  1          0.8               0.75        50      3.078441<br/>  0.3  1          0.8               0.75       100      3.120153<br/>  0.3  1          0.8               0.75       150      3.124199<br/>  0.3  1          0.8               1.00        50      3.174089<br/>  0.3  1          0.8               1.00       100      3.194984<br/>...</span><span id="d6bb" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'gamma' was held constant at a value of 0</span><span id="1697" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'min_child_weight' was held constant at a value of 1<br/>RMSE was used to select the optimal model using the smallest value.<br/>The final values used for the model were nrounds = 50, max_depth = 3,<br/> eta = 0.3, gamma = 0, colsample_bytree = 0.6, min_child_weight = 1<br/> and subsample = 1.</span></pre><p id="6d19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只是展示了几个<code class="fe me mf mg mh b">train</code>执行的全超参数调优的例子。请注意，我们如何通过简单的调整，毫无问题地训练三种不同类型的基于树的模型。没有复杂的争论，没有数据和训练之间的硬接口，什么都没有！为我们照料一切。</p><p id="c9fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在检查交叉验证、超参数调整和指标之前，让我们再看两个模型！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="be04" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合K-最近邻</h1><p id="cc50" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">我们的<code class="fe me mf mg mh b">method</code>还提供k-最近邻算法:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="7ffc" class="nb nc it mh b gy nd ne l nf ng">knn &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                  data = mtcars_train, <br/>                  method = "knn")</span></pre><p id="0bd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<code class="fe me mf mg mh b">knn</code>，<code class="fe me mf mg mh b">caret</code>查看不同的<code class="fe me mf mg mh b">k</code>以选择最终型号:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="88e5" class="nb nc it mh b gy nd ne l nf ng">knn</span><span id="7f57" class="nb nc it mh b gy nh ne l nf ng">k-Nearest Neighbors</span><span id="c8b8" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="24f5" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 23, 23, 23, 23, 23, 23, ... <br/>Resampling results across tuning parameters:</span><span id="49da" class="nb nc it mh b gy nh ne l nf ng">k  RMSE      Rsquared   MAE     <br/>  5  3.541489  0.7338904  2.906301<br/>  7  3.668886  0.7033751  2.909202<br/>  9  3.868597  0.6580107  2.965640</span><span id="73a8" class="nb nc it mh b gy nh ne l nf ng">RMSE was used to select the optimal model using the smallest value.<br/>The final value used for the model was k = 5.</span></pre></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="4e48" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">拟合香草神经网络</h1><p id="deed" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated"><code class="fe me mf mg mh b">caret</code>也使我们能够训练我们自己的神经网络——尽管使用其他软件包(h20、keras等)会更好。)您也可以使用<code class="fe me mf mg mh b">nnet</code>、<code class="fe me mf mg mh b">neuralnet</code>或<code class="fe me mf mg mh b">mxnet</code>方法在<code class="fe me mf mg mh b">caret</code>中训练一个——这里有一个使用<code class="fe me mf mg mh b">neuralnet</code>的例子</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="c3e4" class="nb nc it mh b gy nd ne l nf ng">neural.network &lt;- train(mpg ~ hp + wt + gear + disp, <br/>             data = mtcars_train, <br/>             method = "neuralnet")</span></pre><p id="8627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到我们网络的结果:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="3ba8" class="nb nc it mh b gy nd ne l nf ng">Neural Network</span><span id="1af4" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="4456" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 23, 23, 23, 23, 23, 23, ... <br/>Resampling results across tuning parameters:</span><span id="ab7c" class="nb nc it mh b gy nh ne l nf ng">layer1  RMSE      Rsquared   MAE     <br/>  1       5.916693  0.5695443  4.854666<br/>  3       5.953915  0.2311309  4.904835<br/>  5       5.700600  0.4514841  4.666083</span><span id="faa3" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'layer2' was held constant at a value of 0</span><span id="ab0f" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'layer3' was held constant at a value of 0<br/>RMSE was used to select the optimal model using the smallest value.<br/>The final values used for the model were layer1 = 5, layer2 = 0 and<br/> layer3 = 0.</span></pre><p id="0984" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">nnet</code>的一个主要问题是它不适合分类任务——对于适合的<code class="fe me mf mg mh b">method</code>,我们可以使用<code class="fe me mf mg mh b">nnet</code>,另一种实现:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="2e17" class="nb nc it mh b gy nd ne l nf ng">neural.network.class &lt;- train(target ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, <br/>                        data = iris_train, <br/>                        method = "nnet")</span></pre><p id="1755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们检查一下<code class="fe me mf mg mh b">neural.network.class</code>，会发现一些奇怪的事情:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="cbb4" class="nb nc it mh b gy nd ne l nf ng">neural.network.class</span><span id="ca59" class="nb nc it mh b gy nh ne l nf ng">Neural Network</span><span id="156b" class="nb nc it mh b gy nh ne l nf ng">100 samples<br/>  4 predictor</span><span id="84ec" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... <br/>Resampling results across tuning parameters:</span><span id="1039" class="nb nc it mh b gy nh ne l nf ng">size  decay  RMSE         Rsquared   MAE        <br/>  1     0e+00  0.153228973  0.9370545  0.106767943<br/>  1     1e-04  0.074206333  0.9759492  0.052561227<br/>  1     1e-01  0.116518767  0.9977164  0.112402210<br/>  3     0e+00  0.229154294  0.9616348  0.124433888<br/>  3     1e-04  0.027172779  0.9887259  0.021662284<br/>  3     1e-01  0.080126891  0.9956402  0.074390107<br/>  5     0e+00  0.098585595  0.9999507  0.060258715<br/>  5     1e-04  0.004105796  0.9999710  0.003348211<br/>  5     1e-01  0.073134836  0.9944261  0.065199393</span><span id="37a5" class="nb nc it mh b gy nh ne l nf ng">RMSE was used to select the optimal model using the smallest value.<br/>The final values used for the model were size = 5 and decay = 1e-04.</span></pre><p id="cdfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的神经网络仍然将此视为回归问题(根据它试图优化的指标)——为什么？</p><p id="7b29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为<code class="fe me mf mg mh b">nnet</code>需要目标作为一个因素来理解我们正在尝试做一个分类任务——这是使用<code class="fe me mf mg mh b">caret</code>时遇到的<em class="mw">之一——你仍然需要理解特定<code class="fe me mf mg mh b">methods</code>的某个部分，以避免你的代码中出现任何错误。让我们把我们的<code class="fe me mf mg mh b">target</code>变成一个因素专栏:</em></p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="0ac1" class="nb nc it mh b gy nd ne l nf ng">iris_train$target = factor(iris_train$target)</span></pre><p id="c8aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在再次拟合我们的神经网络:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="e2bc" class="nb nc it mh b gy nd ne l nf ng">neural.network.class.2 &lt;- train(target ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, <br/>                              data = iris_train, <br/>                              method = "nnet")</span></pre><p id="e7d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看我们的对象<code class="fe me mf mg mh b">neural.network.class.2</code>:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="7137" class="nb nc it mh b gy nd ne l nf ng">Neural Network</span><span id="134f" class="nb nc it mh b gy nh ne l nf ng">100 samples<br/>  4 predictor<br/>  2 classes: '0', '1'</span><span id="43b4" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Bootstrapped (25 reps) <br/>Summary of sample sizes: 100, 100, 100, 100, 100, 100, ... <br/>Resampling results across tuning parameters:</span><span id="e5a8" class="nb nc it mh b gy nh ne l nf ng">size  decay  Accuracy   Kappa    <br/>  1     0e+00  0.9591083  0.8776744<br/>  1     1e-04  0.9507270  0.8400000<br/>  1     1e-01  1.0000000  1.0000000<br/>  3     0e+00  0.9845919  0.9474580<br/>  3     1e-04  1.0000000  1.0000000<br/>  3     1e-01  1.0000000  1.0000000<br/>  5     0e+00  0.9988889  0.9969231<br/>  5     1e-04  1.0000000  1.0000000<br/>  5     1e-01  1.0000000  1.0000000</span><span id="e137" class="nb nc it mh b gy nh ne l nf ng">Accuracy was used to select the optimal model using the largest value.<br/>The final values used for the model were size = 1 and decay = 0.1.</span></pre><p id="0547" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">酷！我们的对象现在显示的是<code class="fe me mf mg mh b">Accuracy</code>，这是一个只与分类任务相关的指标。<code class="fe me mf mg mh b">caret</code>在调优过程中修改了哪些参数？<code class="fe me mf mg mh b">size</code>和<code class="fe me mf mg mh b">decay</code>！</p><p id="7b56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里对不同的<code class="fe me mf mg mh b">methods</code>进行了很好的报道——你可能会问，我们遗漏了其中的一些吗？是的，我们做到了——在我写这篇文章的时候，关于<a class="ae ky" href="https://topepo.github.io/caret/available-models.html" rel="noopener ugc nofollow" target="_blank"> 231方法</a>！请随意尝试其中的一些！</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="d2f9" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">定义您自己的交叉验证</h1><p id="2423" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">在<code class="fe me mf mg mh b">caret</code>中，你也可以给<code class="fe me mf mg mh b">train</code>函数自定义的交叉验证方法。例如，让我们在下面的例子中的决策树上使用<a class="ae ky" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0"> k倍交叉验证</a>:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="b54b" class="nb nc it mh b gy nd ne l nf ng">ctrl&lt;- trainControl(method="cv", number=10)</span><span id="a759" class="nb nc it mh b gy nh ne l nf ng">d.tree.kfold &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                data = mtcars_train,<br/>                trControl = ctrl,<br/>                method = "rpart")</span></pre><p id="d948" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此..有什么新鲜事？两件事:</p><ul class=""><li id="22ef" class="mi mj it lb b lc ld lf lg li mk lm ml lq mm lu mn mo mp mq bi translated">我们正在用函数<code class="fe me mf mg mh b">trainControl</code>定义一个<code class="fe me mf mg mh b">ctrl</code>对象。在函数中，我们将实验的<code class="fe me mf mg mh b">method</code>定义为<code class="fe me mf mg mh b">cv</code>(交叉验证)，其中<code class="fe me mf mg mh b">number</code>等于10，<code class="fe me mf mg mh b">number</code>与k相同。</li><li id="8b45" class="mi mj it lb b lc mr lf ms li mt lm mu lq mv lu mn mo mp mq bi translated">我们将k倍交叉验证传递给<code class="fe me mf mg mh b">train</code>中的<code class="fe me mf mg mh b">trControl</code>参数。这将让<code class="fe me mf mg mh b">caret</code>明白我们想在我们的培训过程中应用这种交叉验证方法。</li></ul><p id="8a2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查一下我们的决策树:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="d1fd" class="nb nc it mh b gy nd ne l nf ng">dtree</span><span id="81fe" class="nb nc it mh b gy nh ne l nf ng">CART</span><span id="010a" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="bfc3" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Cross-Validated (10 fold) <br/>Summary of sample sizes: 21, 20, 21, 21, 21, 21, ... <br/>Resampling results across tuning parameters:</span><span id="3fb0" class="nb nc it mh b gy nh ne l nf ng">cp         RMSE      Rsquared   MAE     <br/>  0.0000000  5.717836  0.7373485  4.854541<br/>  0.3169698  5.717836  0.7373485  4.854541<br/>  0.6339397  6.071459  0.6060227  5.453586</span><span id="59f4" class="nb nc it mh b gy nh ne l nf ng">RMSE was used to select the optimal model using the smallest value.<br/>The final value used for the model was cp = 0.3169698.</span></pre><p id="1586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，现在我们可以看到关于<code class="fe me mf mg mh b">Resampling: Cross-Validated (10 fold)</code>的评估结果，确认我们正在使用不同的交叉验证方法。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="f60a" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">不同的指标</h1><p id="6cad" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">我们也可以根据不同的度量标准来选择算法的结果。例如，在我们决策树的输出中，深入查看最后一段:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="4660" class="nb nc it mh b gy nd ne l nf ng">RMSE was used to select the optimal model using the smallest value.<br/>The final value used for the model was cp = 0.3169698.</span></pre><p id="284e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的算法针对均方根误差进行了优化。如果我们想针对另一个指标进行优化，比如R平方，该怎么办？我们可以用论点<code class="fe me mf mg mh b">metric</code>来做:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="1522" class="nb nc it mh b gy nd ne l nf ng">d.tree.kfold.rsquared &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                      data = mtcars_train,<br/>                      trControl = ctrl,<br/>                      metric = "Rsquared",<br/>                      method = "rpart")</span></pre><p id="ce0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们检查输出，我们会看到以下内容:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="5f34" class="nb nc it mh b gy nd ne l nf ng">d.tree.kfold.rsquared</span><span id="2075" class="nb nc it mh b gy nh ne l nf ng">CART</span><span id="242c" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="0f30" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Cross-Validated (10 fold) <br/>Summary of sample sizes: 21, 21, 21, 21, 21, 20, ... <br/>Resampling results across tuning parameters:</span><span id="3d5e" class="nb nc it mh b gy nh ne l nf ng">cp         RMSE      Rsquared   MAE     <br/>  0.0000000  4.943094  0.8303833  4.342684<br/>  0.3169698  4.943094  0.8303833  4.342684<br/>  0.6339397  6.022911  0.7031709  5.472432</span><span id="3be0" class="nb nc it mh b gy nh ne l nf ng">Rsquared was used to select the optimal model using the largest value.<br/>The final value used for the model was cp = 0.3169698.</span></pre><p id="0b95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本例中，这没有任何区别，因为三个指标(RMSE、Rsquared和MAE)的结果将始终指向同一个<code class="fe me mf mg mh b">cp</code>。在有更多超参数的情况下，这是不正确的，它们可能不同。</p><p id="36aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们可以通过调整<code class="fe me mf mg mh b">metric</code>参数来选择不同的模型参数，这真的很酷！在这里检查您可以使用的所有指标<a class="ae ky" href="https://topepo.github.io/caret/measuring-performance.html" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="dcc9" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">超参数调整示例</h1><p id="73a6" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">虽然<code class="fe me mf mg mh b">caret</code>会自己进行一些默认的超参数调整，但对于大多数机器学习任务来说，它通常有点过于简单。</p><p id="6248" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果我想检查其他<code class="fe me mf mg mh b">cp</code>参数怎么办？我们可以在我们的<code class="fe me mf mg mh b">train</code>函数中使用<code class="fe me mf mg mh b">tuneGrid</code>参数:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="1cef" class="nb nc it mh b gy nd ne l nf ng">d.tree.hyperparam &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                               data = mtcars_train,<br/>                               trControl = ctrl,<br/>                               metric = "Rsquared",<br/>                               method = "rpart",<br/>                               tuneGrid = data.frame(<br/>                                 cp = c(0.0001, 0.001, 0.35, 0.65)))</span></pre><p id="2a6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过在<code class="fe me mf mg mh b">tuneGrid</code>中传递一个<code class="fe me mf mg mh b">data.frame</code>，我可以在我的决策树中为自定义<code class="fe me mf mg mh b">cp</code>进行优化。<code class="fe me mf mg mh b">caret</code>的缺点之一是其他参数(如maxdepth、minsplit或其他)不能通过<code class="fe me mf mg mh b">rpart</code>方法的<code class="fe me mf mg mh b">train</code>函数进行调整。您可以通过访问caret官方文档上的<a class="ae ky" href="https://topepo.github.io/caret/available-models.html" rel="noopener ugc nofollow" target="_blank">可用型号页面</a>查看您可以为每个<code class="fe me mf mg mh b">method</code>调整的所有参数。</p><p id="8584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe me mf mg mh b">ranger</code>方法调整几个参数的例子:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="260a" class="nb nc it mh b gy nd ne l nf ng">r.forest.hyperparam &lt;- train(mpg ~ hp + wt + gear + disp, <br/>                           data = mtcars_train,<br/>                           trControl = ctrl,<br/>                           metric = "Rsquared",<br/>                           method = "ranger",<br/>                           tuneGrid = data.frame(<br/>                             mtry = c(2, 3, 4),<br/>                             min.node.size = c(2, 4, 10),<br/>                             splitrule = c('variance')))</span></pre><p id="94f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将产生以下输出:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="3b94" class="nb nc it mh b gy nd ne l nf ng">Random Forest</span><span id="d3c5" class="nb nc it mh b gy nh ne l nf ng">23 samples<br/> 4 predictor</span><span id="cccd" class="nb nc it mh b gy nh ne l nf ng">No pre-processing<br/>Resampling: Cross-Validated (10 fold) <br/>Summary of sample sizes: 21, 20, 21, 21, 21, 20, ... <br/>Resampling results across tuning parameters:</span><span id="edd2" class="nb nc it mh b gy nh ne l nf ng">mtry  min.node.size  RMSE      Rsquared   MAE     <br/>  2      2             2.389460  0.9377971  2.192705<br/>  3      4             2.325312  0.9313336  2.112829<br/>  4     10             2.723303  0.9147017  2.425553</span><span id="546a" class="nb nc it mh b gy nh ne l nf ng">Tuning parameter 'splitrule' was held constant at a value of variance<br/>Rsquared was used to select the optimal model using the largest value.<br/>The final values used for the model were mtry = 2, splitrule =<br/> variance and min.node.size = 2.</span></pre><p id="e880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们保持<code class="fe me mf mg mh b">variance</code>不变，我们的随机森林将针对<code class="fe me mf mg mh b">mtry</code>和<code class="fe me mf mg mh b">min.node.size</code>进行优化。请注意，我们是如何通过为我们输入到<code class="fe me mf mg mh b">tuneGrid</code>的data.frame提供额外的列，将超参数调整扩展到更多变量的。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="9514" class="np nc it bd nq nr ns nt nu nv nw nx ny jz nz ka oa kc ob kd oc kf od kg oe of bi translated">预测测试集</h1><p id="0691" class="pw-post-body-paragraph kz la it lb b lc og ju le lf oh jx lh li oi lk ll lm oj lo lp lq ok ls lt lu im bi translated">最后，我们如何预测新数据上的东西？就像我们在其他模型中做的一样！使用<code class="fe me mf mg mh b">predict</code>和使用<code class="fe me mf mg mh b">caret</code>模型一样简单——使用不同的模型:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="6010" class="nb nc it mh b gy nd ne l nf ng"># Linear Regression<br/>predict(lm_model, mtcars_test)</span><span id="8d41" class="nb nc it mh b gy nh ne l nf ng"># Decision Tree <br/>predict(d.tree, mtcars_test)</span><span id="7eed" class="nb nc it mh b gy nh ne l nf ng"># Random Forest<br/>predict(r.forest, mtcars_test)</span><span id="f76a" class="nb nc it mh b gy nh ne l nf ng"># XGBoost<br/>predict(xg.boost, mtcars_test)</span></pre><p id="1143" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多酷啊。我们甚至可以将不同的预测传递到散点图中，以了解我们的不同模型如何对同一示例进行分类——将随机森林与我们的xgboost:</p><pre class="kj kk kl km gt mx mh my mz aw na bi"><span id="d6cb" class="nb nc it mh b gy nd ne l nf ng">library(ggplot2)</span><span id="2e19" class="nb nc it mh b gy nh ne l nf ng">ggplot(<br/>  data = predictions,<br/>  aes(x = rf, y = xgboost)<br/>) + geom_point()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/02783abb36d0715973b3f93215f59e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*REruvkZZ13cNSTtUhyx_PQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林与XGBoost —作者图片</p></figure></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="c609" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章，它能帮助你掌握这个伟大的机器学习库。<code class="fe me mf mg mh b">caret</code>有许多特性，这使得它成为使用R编程构建有趣的数据科学模型的首选库。</p><p id="c66d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想参加我的R课程，请随时加入这里(<a class="ae ky" href="https://www.udemy.com/course/r-for-absolute-beginners/?couponCode=IVOSTUDENTSJULY" rel="noopener ugc nofollow" target="_blank">绝对初学者R编程</a>)或这里(<a class="ae ky" href="https://www.udemy.com/course/r-for-data-science-first-step-data-scientist/?couponCode=IVOSTUDENTSJULY" rel="noopener ugc nofollow" target="_blank">数据科学训练营</a>)。我的课程教授R编程和数据科学，我希望你能在我身边！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/aed624e9fee167b4e9f9ebb4777004ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/0*UprgCHJUfCayHe4V.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.udemy.com/course/r-for-data-science-first-step-data-scientist/?couponCode=IVOSTUDENTSJULY" rel="noopener ugc nofollow" target="_blank">数据科学训练营:你成为数据科学家的第一步</a> —作者图片</p></figure><div class="on oo gp gr op oq"><a href="https://medium.com/membership/@ivopbernardo" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">通过我的推荐链接加入Medium-Ivo Bernardo</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">阅读我在Medium上的所有故事，了解更多关于数据科学和分析的信息。加入中级会员，您将…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div><p id="0a31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是我们在这篇文章中使用的代码的一个小要点:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pf pg l"/></div></figure></div></div>    
</body>
</html>