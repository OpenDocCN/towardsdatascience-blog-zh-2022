<html>
<head>
<title>CUDA by Numba Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CUDA by Numba示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cuda-by-numba-examples-7652412af1ee#2022-09-28">https://towardsdatascience.com/cuda-by-numba-examples-7652412af1ee#2022-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="507b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">阅读本系列的第3部分，了解Python的CUDA编程中的流和事件</h2></div><h1 id="f863" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">第3部分，共4部分:流和事件</h1><h1 id="45b7" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="c8f0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列的前两部分中(<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-1-4-e0d06651612f">第一部分在这里</a>，以及<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-215c0d285088">第二部分在这里</a>，我们学习了如何用GPU编程来执行简单的任务，比如令人尴尬的并行任务、使用共享内存的缩减以及设备功能。我们还了解了如何对主机的函数计时——以及为什么这可能不是对代码计时的最佳方式。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/8fc2fca70b012fb45afbfc85bdb1590a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bC0prdsQmLoLoSSY2sWgmQ.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3.0。运行<a class="ae lt" href="https://replicate.com/stability-ai/stable-diffusion" rel="noopener ugc nofollow" target="_blank">稳定扩散</a>与“湾流多彩空间平静”。学分:在CreativeML Open RAIL-M许可下拥有作品。</p></figure><h1 id="b4c1" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">在本教程中</h1><p id="4dd4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了提高我们的计时能力，我们将介绍CUDA事件以及如何使用它们。但是在我们深入研究之前，我们将讨论CUDA流以及为什么它们很重要。</p><p id="f5ef" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><a class="ae lt" href="https://colab.research.google.com/drive/1iRUQUiHUVdl3jlKzKucxQHQdDPElPb3M?usp=sharing" rel="noopener ugc nofollow" target="_blank">点击这里在Google colab中抓取代码。</a></p><p id="f43f" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">本教程后面还有一个部分:<a class="ae lt" rel="noopener" target="_blank" href="/cuda-by-numba-examples-c583474124b0">第四部分</a>。</p><h1 id="773e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">入门指南</h1><p id="6d00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">导入和加载库，确保你有一个GPU。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h1 id="89a9" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">流</h1><p id="6dc9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当我们从主机启动内核时，它的执行会在GPU中排队，只要GPU完成了之前启动的所有任务，就会执行这个任务。</p><p id="924c" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">用户在设备中启动的许多任务可能依赖于之前的任务，“将它们放在同一个队列中”是有意义的。例如，如果您正在将数据异步复制到GPU，以便用某个内核处理它，则该副本必须在内核运行之前完成。</p><p id="7470" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">但是，如果有两个相互独立的内核，将它们放在同一个队列中有意义吗？大概不会！对于这些情况，CUDA有<em class="mr">个流</em>。您可以将流视为独立的队列，它们彼此独立运行。它们也可以并发运行，即同时运行。当运行许多独立的任务时，这可以大大加快总运行时间。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ms"><img src="../Images/2d8770750b2ac6f9c8f628aa17e56804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mYDmxjFYQQ86Saw2.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3.1。使用不同的流可以允许并发执行，从而提高运行时间。演职员表:<a class="ae lt" href="https://www.mdpi.com/1045598" rel="noopener ugc nofollow" target="_blank">张等2021 </a> (CC BY 4.0)。</p></figure><h2 id="929e" class="mt kg iq bd kh mu mv dn kl mw mx dp kp lg my mz kr lk na nb kt lo nc nd kv ne bi translated">Numba CUDA中的流语义</h2><p id="aaca" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将把到目前为止学到的两个任务进行排队，以创建一个规范化管道。给定一个(主机)数组<code class="fe nf ng nh ni b">a</code>，我们将用它的规范化版本覆盖它:</p><p id="48ca" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">a ← a / ∑a[i]</p><p id="9025" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">为此，我们将使用三个内核。第一个内核<code class="fe nf ng nh ni b">partial_reduce</code>将是我们对第2部分的部分缩减。它将返回一个<code class="fe nf ng nh ni b">threads_per_block</code>大小的数组，我们将把它传递给另一个内核<code class="fe nf ng nh ni b">single_thread_sum</code>，后者将进一步把它简化为一个单独的数组(大小为1)。这个内核将在一个单独的块上用一个单独的线程运行。最后，我们将使用<code class="fe nf ng nh ni b">divide_by</code>就地除出原始数组，但我们之前计算的总和。所有这些操作都将在GPU中进行，并且应该一个接一个地运行。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="654e" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">当内核调用和其他操作没有流时，它们在默认流中运行。默认流是一个特殊的流，其行为取决于运行的是<a class="ae lt" href="https://docs.nvidia.com/cuda/cuda-runtime-api/stream-sync-behavior.html" rel="noopener ugc nofollow" target="_blank">传统流还是每线程流</a>。对我们来说，如果您想要实现并发，您应该在非默认流中运行任务，这样说就足够了。让我们来看看如何为内核启动、阵列拷贝和阵列创建拷贝等操作实现这一点。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="349c" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">在我们真正谈论流之前，我们需要谈论房间里的大象:<code class="fe nf ng nh ni b">cuda.pinned</code>。这个上下文管理器创建了一种特殊类型的内存，称为<em class="mr">页面锁定</em>或<em class="mr">固定</em>内存，CUDA在将内存从主机转移到设备时会从中受益。</p><p id="f4d1" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">驻留在主机RAM中的内存可以随时被<a class="ae lt" href="https://en.wikipedia.org/wiki/Memory_paging" rel="noopener ugc nofollow" target="_blank"> <em class="mr">分页</em> </a>，即操作系统可以偷偷将对象从RAM移动到硬盘。这样做是为了将不常用的对象移到较慢的内存位置，从而将快速RAM内存留给更急需的对象。对我们来说重要的是，CUDA不允许从可分页对象到GPU的异步传输。这样做是为了防止持续不断的非常慢的传输:磁盘(分页)→ RAM → GPU。</p><p id="2c9d" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">为了异步传输数据，我们必须确保数据<em class="mr">总是</em>在RAM中，通过某种方式防止操作系统偷偷把它藏在磁盘的某个地方。这就是内存锁定发挥作用的地方，它创建了一个上下文，在这个上下文中，参数将被“页面锁定”，也就是说，被强制放在RAM中。参见图3.2。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nj"><img src="../Images/8c805b518dc74418c60ee142e4dacb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZrWF-XxN4bznjmS0.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3.2。可分页内存与固定(页面锁定)内存。演职员表:<a class="ae lt" href="https://www.mdpi.com/215954" rel="noopener ugc nofollow" target="_blank">里兹维等人2017 </a> (CC BY 4.0)。</p></figure><p id="f939" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">从那时起，代码就非常简单了。创建了一个流，之后它将被传递给我们想要在该流上操作的每个CUDA函数。重要的是，Numba CUDA内核配置(方括号)要求流位于第三个参数中，在块维度大小之后。</p><p id="a8e0" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><strong class="kz ir"> <em class="mr">警告:</em> </strong> <em class="mr">一般来说，将流传递给Numba CUDA API函数并不会改变它的行为，只会改变它运行所在的流。一个例外是从设备到主机的拷贝。当调用</em> <code class="fe nf ng nh ni b"><em class="mr">device_array.copy_to_host()</em></code> <em class="mr">(无参数)时，复制同步发生。调用</em> <code class="fe nf ng nh ni b"><em class="mr">device_array.copy_to_host(stream=stream)</em></code> <em class="mr">(带流)时，如果</em> <code class="fe nf ng nh ni b"><em class="mr">device_array</em></code> <em class="mr">没有被钉住，复制会同步发生。仅当</em>  <code class="fe nf ng nh ni b"><strong class="kz ir"><em class="mr">device_array</em></strong></code> <strong class="kz ir"> <em class="mr">被钉住并且流被传递</em> </strong> <em class="mr">时，复制才会</em> <strong class="kz ir"> <em class="mr">异步发生。</em></strong></p><p id="14cc" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><strong class="kz ir"><em class="mr">INFO:</em></strong><em class="mr">Numba提供了一个有用的上下文管理器，可以将所有操作放入其上下文中；退出上下文时，操作将被同步，包括内存传输。例3.1也可以写成:</em></p><pre class="lv lw lx ly gt nk ni nl nm aw nn bi"><span id="8649" class="mt kg iq ni b gy no np l nq nr">with cuda.pinned(a):<br/>    stream = cuda.stream()<br/>    with stream.auto_synchronize():<br/>        dev_a = cuda.to_device(a, stream=stream)<br/>        dev_a_reduce = cuda.device_array((blocks_per_grid,), dtype=dev_a.dtype, stream=stream)<br/>        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)<br/>        partial_reduce[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_reduce)<br/>        single_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)<br/>        divide_by[blocks_per_grid, threads_per_block, stream](dev_a, dev_a_sum)<br/>        dev_a.copy_to_host(a, stream=stream)</span></pre><h1 id="1d22" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">将独立内核与流解耦</h1><p id="fbdd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设我们想要规格化不是一个数组，而是多个数组。对单独数组进行规范化的操作是完全相互独立的。因此，GPU等到一个规范化结束后再开始下一个规范化是没有意义的。因此，我们应该将这些任务分成不同的流程。</p><p id="43e2" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">让我们看一个规范化10个数组的例子——每个数组都使用自己的流。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="8795" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">现在让我们来比较一下单流。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="a6b7" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">但是哪个更快呢？当运行这些例子时，当使用多个流时，我没有获得一致的总时间改进。这可能有很多原因。例如，对于并发运行的流，本地内存中必须有足够的空间。另外，我们是从CPU计时的。虽然很难知道本地内存中是否有足够的空间，但从GPU进行计时相对容易。让我们学习如何！</p><p id="f568" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><strong class="kz ir"><em class="mr">INFO:</em></strong><em class="mr">Nvidia提供了几个调试CUDA的工具，包括调试CUDA流。查看</em> <a class="ae lt" href="https://developer.nvidia.com/nsight-systems" rel="noopener ugc nofollow" target="_blank"> <em class="mr"> Nsight系统</em> </a> <em class="mr">了解更多信息。</em></p><h1 id="13fd" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">事件</h1><p id="e0f7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">来自CPU的计时代码的一个问题是，它将包括比GPU更多的操作。</p><p id="6f1e" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">谢天谢地，有可能用CUDA直接从GPU计时<em class="mr">事件</em>。事件只是一个时间寄存器，记录了GPU中发生的事情。在某种程度上，它类似于<code class="fe nf ng nh ni b">time.time</code>和<code class="fe nf ng nh ni b">time.perf_counter</code>，与它们不同的是，我们需要处理这样一个事实，即当我们从CPU编程时，我们想要从GPU计时事件。</p><p id="73ac" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">因此，除了创建时间戳(“记录”事件)，我们还需要确保事件在访问其值之前与CPU同步。让我们来看一个简单的例子。</p><h2 id="439f" class="mt kg iq bd kh mu mv dn kl mw mx dp kp lg my mz kr lk na nb kt lo nc nd kv ne bi translated">为内核执行计时的事件</h2><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="97b9" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">计时GPU操作的一个有用方法是使用上下文管理器:</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h2 id="7e2d" class="mt kg iq bd kh mu mv dn kl mw mx dp kp lg my mz kr lk na nb kt lo nc nd kv ne bi translated">计时流的事件</h2><p id="bf4f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本系列文章的最后，我们将使用streams来更好、更准确地了解我们的示例是否受益于streams。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h1 id="6689" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="b4fe" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">CUDA关注的是性能。在本教程中，您学习了如何使用<em class="mr">事件</em>来精确测量内核的执行时间，这种方法可以用来分析您的代码。您还了解了<em class="mr">流</em>以及如何使用它们让您的GPU保持忙碌，以及<em class="mr">固定</em>或<em class="mr">映射数组</em>，以及它们如何改善内存访问。</p></div></div>    
</body>
</html>