<html>
<head>
<title>How to use Latent Semantic Analysis to classify documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用潜在语义分析对文档进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-latent-semantic-analysis-to-classify-documents-1af717e7ee52#2022-10-19">https://towardsdatascience.com/how-to-use-latent-semantic-analysis-to-classify-documents-1af717e7ee52#2022-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5726810c9e8b73b23bda8644abf7fdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ComIfw1gYOH238aoWHnPbw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">塔玛拉·加克在<a class="ae jg" href="https://unsplash.com/s/photos/documents?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="e926" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">自然语言处理</h2><div class=""/><div class=""><h2 id="736a" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">学习一个有用的工具来克服分析文本的挑战</h2></div></div><div class="ab cl lh li hx lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="im in io ip iq"><p id="01da" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">孩子们在地板上坐成一圈。</p><p id="e941" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">“扁帽子上有一个数字和一个标签，上面写着鹦鹉和煎饼”——一个孩子尖叫道</p><p id="5275" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">每个孩子都开始笑。</p><p id="4e64" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">“不，不，是那只黑猫在桌子下面，它吃胡萝卜和煎饼”——另一个孩子回答道</p><p id="b9ae" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我这才意识到他们在玩<a class="ae jg" href="https://en.wikipedia.org/wiki/Chinese_whispers" rel="noopener ugc nofollow" target="_blank">电话</a>(在阿根廷我们称之为坏电话)。</p><p id="3aff" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi mk translated">人类的交流是复杂的，主要是因为每个人表达自己的方式不同。我们可以说同一种语言，但使用不同的俚语、词汇或表达来传达相同的信息。此外，我们可以用同一个词表达两种完全相反的意思。</p><p id="77f7" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">书面语言并不简单。除了一词多义、同义词和词汇问题，人们经常忽略标点符号，拼错或缩写单词。尽管如此，大多数人还是能理解书面或口头语言。</p><p id="b04e" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">对于机器来说，识别这些元素并不是一件容易的事情。用表格形式表示这些特征并不容易。这就是为什么分析文本比分析结构化数据更难。</p><p id="db87" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">自然语言处理(NLP)提供了帮助理解文本的句法和语义的工具。NLP允许数据科学家解决语言中的歧义。它还有助于将非结构化文本转换为结构化数据，以用于多种应用，如语音识别或情感分析。</p><p id="63d5" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">文档分类是文本挖掘领域的一个典型问题。它的使用案例从情感分析、垃圾邮件检测到社交网络文档的高级结构化表示。</p><p id="8fca" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">文档分类根据文档的底层结构将文档组织成组。它主要依赖于将纯文本转换成结构化数据。</p><h1 id="9eee" class="mt mu jj bd mv mw mx my mz na nb nc nd ky ne kz nf lb ng lc nh le ni lf nj nk bi translated">潜在语义分析</h1><p id="ccaa" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">我们来分析一下以下对某酒店的评价:</p><ul class=""><li id="18bf" class="nq nr jj lq b lr ls lu lv lx ns mb nt mf nu mj nv nw nx ny bi translated">这家酒店非常棒，在炎热的日子里，我免费喝了很多饮料。</li><li id="bce2" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj nv nw nx ny bi translated"><em class="nz">设施非常棒，他们在游泳池免费提供饮料</em>。</li></ul><p id="df76" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们可能会注意到<strong class="lq jt">饮料</strong>和<strong class="lq jt">饮料</strong>有相同的意思；他们是T4的同义词。同样，<strong class="lq jt"> served </strong>有两个不同的意思:有用和提供；是<strong class="lq jt">多义</strong>的一个例子。</p><p id="0cd4" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们可能会理解不同或相似之处。但是对于机器学习算法来说，同义词和多义词可能是一个问题。</p><p id="29ac" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">一种解决方案是组合具有相同含义的单词，或者在语义上相连并描述一个主题的单词。</p><p id="95d2" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">这个潜在变量或维度代表了所有这些。这就是<strong class="lq jt">潜在语义分析(LSA) </strong>中“潜在”的含义。</p><p id="f39b" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">LSA是一种无监督的算法，它涉及到将一组非结构化文本转换成结构化数据。</p><p id="14f5" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">LSA通过使用奇异值分解找到这些潜在变量，并转换原始数据。</p><p id="6634" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">此外，它还是一种降维算法，因为转换后的数据比原始数据小。</p><h2 id="75de" class="of mu jj bd mv og oh dn mz oi oj dp nd lx ok ol nf mb om on nh mf oo op nj jp bi translated">单一向量分解</h2><p id="9142" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">简而言之，LSA是将SVD应用于表示术语和文档之间关系的矩阵(术语-文档矩阵)。</p><p id="995f" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们可以认为每个术语定义一个输入维度，每个文档指定一个样本。</p><p id="5731" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">奇异值分解是一种矩阵分解的方法。使用这种方法，可以将一个矩阵分解成如下所示的三个矩阵。</p><figure class="or os ot ou gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/4b71b91e0cc3f8d624446c8b6517566b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*npKQEddWKXCgKTlC"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">根据奇异值分解分解原始矩阵。</p></figure><p id="0de4" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们来解释一下上图。</p><ul class=""><li id="61ac" class="nq nr jj lq b lr ls lu lv lx ns mb nt mf nu mj nv nw nx ny bi translated"><em class="nz"> X </em>是文档术语矩阵。它包含m个文档(行)中n个唯一单词(列)的频率。这个矩阵大约是三个矩阵的乘积:转置的U、S和V。这里，r维表示主题的数量。</li><li id="1e71" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj nv nw nx ny bi translated">u是文档-主题矩阵。为了创建这个矩阵，SVD找到XXT的特征向量，并将它们放入它的列中。这个操作留给U包含左奇异向量的列。</li><li id="3eca" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj nv nw nx ny bi translated">s是具有奇异值的对角矩阵，奇异值是来自XXT或XTX的特征值的平方根。奇异值按降序排列。</li><li id="15d6" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj nv nw nx ny bi translated">最后，V是字嵌入矩阵。在这种情况下，SVD找到XTX的特征向量，并将它们放入它的列中，给V留下右奇异向量的行。</li></ul><p id="9c77" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">这里我们可以看到<em class="nz">奇异值分解</em>中的<strong class="lq jt">分解</strong>的含义。</p><p id="a8b2" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">因此，SVD按照信息量递减的顺序对每个新维度进行排序。这意味着第一个维度比第二个维度承载更多的信息，以此类推。</p><p id="3376" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">它还将稀疏的文档术语矩阵转换成密集的文档主题矩阵。你可以使用这个矩阵，通过几个机器学习模型对文档进行分类。</p><h2 id="b6eb" class="of mu jj bd mv og oh dn mz oi oj dp nd lx ok ol nf mb om on nh mf oo op nj jp bi translated">文档分类</h2><p id="f100" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">如前所述，LSA将创建一个文件的矢量表示。然后，我们可以计算这些向量之间的距离。确定它们的相似性。并通过确定它们属于哪个主题来对文档进行分类。</p><p id="5167" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">如果主题是已知的，文档分类将需要标记的数据来训练模型。</p><p id="03bd" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">另一方面，使用未标记数据和聚类方法可以识别文档组。</p><p id="cfc9" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">现在，让我们看看LSA的行动。</p><p id="e5aa" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们将使用一个流行的数据集进行文档分类:<a class="ae jg" href="https://www.kaggle.com/datasets/crawford/20-newsgroups" rel="noopener ugc nofollow" target="_blank"> <em class="nz"> 20个新闻组</em> </a>，新闻组文档的集合。</p><p id="4482" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">Scikit learn提供了一个函数来从该数据集加载文件名和数据:<code class="fe ov ow ox oy b">fetch_20newsgroups</code>。</p><figure class="or os ot ou gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="1e5b" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">加载数据集后，需要删除电子邮件地址、网站和号码。</p><p id="660b" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">下一步是执行<strong class="lq jt">术语频率-逆文档频率</strong> ( <a class="ae jg" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> Tfidf </a>)。这种技术从一组文档(或语料库)中确定一个单词在文档中的重要性。TF–IDF值随着单词在文档中出现的次数成比例增加。<em class="nz"> </em>然后根据语料库中包含该单词的文档数量进行偏移。<em class="nz"> </em>这个调整是为了处理一般情况下有些词出现频率比较高的情况<em class="nz">。</em></p><p id="8630" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">该操作将允许我们删除停用词，过滤出现太频繁(40%的文档)或不太频繁的术语，将单词转换为小写，并对结果单词向量进行归一化。</p><figure class="or os ot ou gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="66f8" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">现在，是时候应用LSA或者换句话说，<code class="fe ov ow ox oy b">TruncatedSVD</code>它的python <a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html?highlight=truncatedsvd#sklearn.decomposition.TruncatedSVD" rel="noopener ugc nofollow" target="_blank">实现</a>。我们需要将组件的数量设置为20。它是数据集文档中陈述的主题数量。</p><figure class="or os ot ou gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="4916" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">最后，我们应该选择一种分类算法，并使用LSA数据集对其进行训练。我们会选择支持向量机。该算法的一些特点，如计算的可扩展性和易于实现，使其非常适合应用于文档分类。</p><figure class="or os ot ou gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="4d1e" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">现在，我们可以通过转换测试数据集并使用训练好的模型预测主题来测试模型。</p><figure class="or os ot ou gt iv"><div class="bz fp l di"><div class="oz pa l"/></div></figure><figure class="or os ot ou gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/f00283ad13b3ed278da08f2497dd509a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHrUtnQv5a_yiEvRnkcm7A.png"/></div></div></figure><p id="60f8" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">准确率在<strong class="lq jt"> ~69% </strong>，有一定的提升空间。</p><h2 id="be85" class="of mu jj bd mv og oh dn mz oi oj dp nd lx ok ol nf mb om on nh mf oo op nj jp bi translated">优点和局限性</h2><p id="39b5" class="pw-post-body-paragraph lo lp jj lq b lr nl kt lt lu nm kw lw lx nn lz ma mb no md me mf np mh mi mj im bi translated">在过去的几年中，NLP方法的使用已经广泛传播。LSA也不例外，这是由于它的好处。</p><p id="ff7b" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">在LSA，概念和它们相关的文档被一致地表示。此外，LSA分析有助于恢复原文的语义结构和维度。</p><p id="68da" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">文档词汇往往很大，因此噪音是常见的。LSA帮助对数据进行去噪处理。</p><p id="525b" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">但是LSA有一些局限性。最大的缺点之一是向量需要大量的存储和计算能力。对于长文档，LSA需要大量的计算时间，降低了效率。</p><p id="0776" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">总之，LSA是高效且易于实现的，在使用小数据时能给出好的结果。但是,<strong class="lq jt">计算量很大</strong>,而且很难事先确定主题。</p><p id="d41c" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们可以使用一些方法来改进文档分类，比如使用替代的或修改的SVD计算方法，或者尝试其他的降维算法。</p><p id="2ff8" class="pw-post-body-paragraph lo lp jj lq b lr ls kt lt lu lv kw lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated"><strong class="lq jt">一些资源</strong></p><ol class=""><li id="635d" class="nq nr jj lq b lr ls lu lv lx ns mb nt mf nu mj pc nw nx ny bi translated"><em class="nz">懒程序员</em>。Python中的自然语言处理:垃圾邮件检测、情感分析、潜在语义分析和文章旋转的主数据科学和机器学习(Python中的机器学习)。</li><li id="7723" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj pc nw nx ny bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/latent-semantic-analysis-sentiment-classification-with-python-5f657346f6a3">潜在语义分析&amp;Python中的情感分析</a>。走向数据科学</li><li id="9bae" class="nq nr jj lq b lr oa lu ob lx oc mb od mf oe mj pc nw nx ny bi translated">Hava1 O，Skrbek M和Kordik P. <a class="ae jg" href="https://www.scitepress.org/Papers/2012/41093/41093.pdf" rel="noopener ugc nofollow" target="_blank">用于文档分类的上下文潜在语义网络</a>。</li></ol></div></div>    
</body>
</html>