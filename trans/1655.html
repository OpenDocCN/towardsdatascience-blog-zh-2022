<html>
<head>
<title>3 Fundamental Reasons why Quantization is important for tinyML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量化对tinyML重要的3个基本原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-fundamental-reasons-why-quantization-is-important-for-tinyml-92df82234b91#2022-04-19">https://towardsdatascience.com/3-fundamental-reasons-why-quantization-is-important-for-tinyml-92df82234b91#2022-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ff11" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">实现tinyML的尺寸、延迟和可移植性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2d984d2b9be2e917ef99a14d47f76b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nQggHs_krGCmjoS-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Vishnu Mohanan 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a93b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习(ML)已经成为几十年来的热门话题，在我们生活的方方面面都有成功的现实应用，从健康信息学到商业再到网络安全。将ML直接嵌入边缘设备彻底改变了ML在物联网(IoT)上的研究和应用，在物联网中，成千上万的thasounds微型设备被用来提高现实世界问题解决的生产率和效率；这引发了微型机器学习(tiny ML)的使用，这被认为是人工智能的下一件大事。Tiny ML是机器学习的高级部分之一，它缩小了深度学习神经网络，以适应微型硬件或嵌入式AI处理器，如微控制器或嵌入式npu，它们包含极小的尺寸(例如，内存256 KB和存储1 MB)。小型廉价机器(微控制器)、超低功耗、小内存、低延迟(即时分析)和嵌入式ML算法——是微型ML的主要特征。</p><p id="ac45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在收缩过程中，TinyML利用了一些模型压缩技术，如<strong class="lb iu">量化、修剪和知识提炼</strong>。今天，我将讨论量子化以及它如何有助于开发TinyML。</p><p id="4eac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">量化</strong>是一种优化技术，它将模型参数的精度从<strong class="lb iu"> 32位浮点</strong>值降低到<strong class="lb iu"> 8位(int8) </strong>值，而不影响精度，从而减小模型尺寸，提高可移植性，加快计算速度。通过采用可实现更显著加速的超低(1位或2位)精度缩减，可以执行更积极的压缩以进一步减少内存消耗和计算。然而，超低缩减过程可能会产生量化噪声，导致模型精度的显著损失。量化为开发微小的ML提供了三个重要的好处:<strong class="lb iu">大小、延迟和可移植性。</strong></p><p id="82a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">尺寸— </strong>量化<strong class="lb iu"> </strong>显著减小模型尺寸，使得在嵌入式设备(如微控制器)上运行ML模型。</p><p id="e165" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们训练了一个神经网络并获得了一个最优的权重矩阵，我们必须将这个包含权重的模型保存到某个地方以便进行推理。在常规的微控制器中，我们可能有256 KB (RAM)的存储器大小和1 MB的闪存驱动器存储，而神经网络模型的大小明显大于此。例如，专为在边缘设备上应用而设计的MoblieNet模型的模型大小为<strong class="lb iu"> 16 MB </strong>，有430万个参数。因此，考虑到存储容量，将这个庞大的模型直接安装在微控制器上是完全不可行的。在这种情况下，量化可能是一个潜在的解决方案—通过应用量化(32位浮点到8位int值)，我们可以立即将模型大小减少到4倍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/b0e05c6100ef0883a3aee18ae43babdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rzdCJEM5yh8bCRJs"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jorgedevs?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jorge Ramirez </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，你可以想一想，如果我们能以某种方式将模型尺寸缩小到微控制器的给定RAM尺寸，那么我们就可以轻松地适应模型，并充分利用RAM的空间。然而，我们应该考虑到，整个内存不应该专用于ML任务，因为应用程序由ML和非ML部分组成。因此，我们应该始终致力于将模型缩小到最小，以便内存为非ML作业获得空闲空间。</p><p id="4684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">延迟— </strong>在任何微处理器上执行的浮点运算通常比整数运算慢，因为整数运算可能只需要一到两个周期，而浮点运算可能需要十多个周期。因此，将32位浮点转换为8位ineger系统可以显著提高性能。</p><p id="25bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与浮点计算相比，整数点计算更便宜(因为它需要更长的计算时间和功耗)，因此切换到int8系统可以降低微控制器的功耗。<strong class="lb iu">一些实验表明，与相关浮点模型相比，MobileNet V1、ResNet V2和Inception V3的量化模型(int8算法)在CPU上的速度快2-4倍，体积小4倍，推理速度更高。</strong></p><p id="cb37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">可移植性——</strong>量化提供了另一个重要的好处——<strong class="lb iu">可移植性</strong>——开发微小的ML，因为可移植性对于嵌入式系统极其重要。并非所有的嵌入式设备都是同等创建的，因为一些微控制器与浮点运算兼容，而一些则与整数值系统兼容。因此，如果不考虑算法兼容性，任何给定的神经网络都不能直接部署在嵌入式设备上进行推理。人们可以认为量化是ML算法的归一化技术。与归一化一样，我们将所有特征的值转换为相似的尺度；量化通过将模型参数转换成<strong class="lb iu">整数</strong>系统来做同样的事情，这是所有不同嵌入式系统的公共基线。因此，量化可以提供更好的可移植性并实现更好的效率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/18722f4f51cddc2f54c144f27ec2f759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eXkFd3IO5zThW6li"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@unarchive?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰里米·贝赞格</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2f20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，对于在嵌入式设备上开发tinyML来说，量化是确保达到所需大小、延迟和可移植性的重要步骤。</p><h1 id="544b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">阅读默罕默德·马苏姆博士(以及媒体上成千上万的其他作家)的每一个故事。</h1><p id="4866" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">你的会员费将直接支持和激励穆罕默德·马苏姆和你所阅读的成千上万的其他作家。你还可以在媒体上看到所有的故事—<a class="ae ky" href="https://masum-math8065.medium.com/membership" rel="noopener"><strong class="lb iu">https://masum-math8065.medium.com/membership</strong></a></p><p id="9448" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">快乐阅读！</strong></p><p id="74cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考</strong></p><p id="c67b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">受HaevardX在线课程《TinyML的应用》的启发</p></div></div>    
</body>
</html>