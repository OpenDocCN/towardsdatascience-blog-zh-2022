<html>
<head>
<title>How to Build an Image-Captioning Model in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Pytorch中建立图像字幕模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-an-image-captioning-model-in-pytorch-29b9d8fe2f8c#2022-06-23">https://towardsdatascience.com/how-to-build-an-image-captioning-model-in-pytorch-29b9d8fe2f8c#2022-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e36d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何在Pytorch中构建图像标题模型的详细分步说明</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dce75d999f058e9b7add93c727751369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9chjZxFWwGxc0gDh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚当·达顿在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bd98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将解释如何使用Pytorch深度学习库构建图像字幕模型架构。除了解释模型架构背后的直觉，我还将提供模型的Pytorch代码。</p><p id="fd98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，本文写于2022年6月，因此Pytorch的早期/未来版本可能略有不同，本文中的代码也不一定有效。</p><h2 id="8108" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">什么是图像字幕？</h2><p id="4a30" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">顾名思义，图像字幕是将图像输入到人工智能模型，并接收描述/总结图像内容的文本字幕作为其输出的任务。例如，如果我将以下图片输入到图像字幕模型中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/a5ef061d13fdbc4d461f7f336ae123f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HxBDFFqwpjShEnyb"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@lyanvoyages?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄，莱恩号</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上航行</p></figure><p id="c6f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型将返回类似“狗在水中跑”的文本标题。图像字幕模型由两个主要组件组成:CNN(卷积神经网络)编码器和语言模型/RNN(某种可以产生文本的NLP模型)解码器。CNN编码器存储关于输入图像的重要信息，解码器将使用这些信息生成文本字幕。</p><p id="6e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了训练图像字幕模型，最常用的数据集是Flickr8k数据集和MSCOCO数据集。你可以在这里找到Flickr数据集<a class="ae ky" href="https://www.kaggle.com/datasets/adityajn105/flickr8k" rel="noopener ugc nofollow" target="_blank">的下载链接，在这里</a>找到MSCOCO数据集<a class="ae ky" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">的链接。Flickr8k数据集由8000幅图像组成，每幅图像都有5个不同的标题来描述图像，而MSCOCO数据集由328000幅图像组成。从入门的角度来看，推荐使用Flickr数据集，因为它不像MSCOCO那么大，更容易处理。但是如果您试图构建一个可以在生产中部署/使用的模型，那么MSCOCO可能更好。</a></p><h2 id="c5e8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">编码器-解码器</strong>模型架构</h2><p id="e196" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">正如我之前提到的，编码器-解码器架构由两个组件组成:一个卷积神经网络，用于对图像进行编码(即将其转换为丰富的嵌入表示)，一个递归神经网络(或LSTM)，将该图像作为输入，并接受训练，使用一种称为教师强制的机制对字幕进行顺序解码。下面是这个模型的一个很好的示意图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/5871677dde6e03795b88699b9d80eb7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*MqsV98FA4XA99QFs7BXldg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="26bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">CNN:</strong>通常，CNN往往是这种模型架构中计算量最少/最复杂的部分。为什么？因为大多数图像字幕模型倾向于使用迁移学习来简单地加载已经存在的强大CNN架构的预训练权重。例如，在本文中，我将使用将加载到Pytorch的torchvision库中的Inception V3 CNN网络。然而，除了《盗梦空间》,你还可以使用许多其他的CNN，如雷斯网、VGG或LeNet。我为CNN使用迁移学习而不是从头开始训练它的主要原因是因为这个任务是如此的普遍和广泛。它不需要CNN学习非常具体的东西(CNN的唯一目的是为图像创建丰富的嵌入)。模型必须学习的对图像字幕有益的任何特征将在整个模型的训练过程中学习，并且其权重将被微调。</p><p id="975b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是CNN编码器的Pytorch型号代码:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="3611" class="lv lw it mw b gy na nb l nc nd">import torch<br/>import torch.nn as nn<br/>import torchvision.models as models</span><span id="1fe9" class="lv lw it mw b gy ne nb l nc nd">class CNNEncoder(nn.Module):<br/> def __init__(self, embed_size):<br/>   super(CNNEncoder, self).__init__()<br/>   self.inception = models.inception_v3(pretrained=True,<br/>                                       aux_logits=False)<br/>   self.inception.fc = nn.Linear(self.inception.fc.in_features,<br/>                                                    embed_size)<br/>   self.relu = nn.ReLU()<br/>   self.dropout = nn.Dropout(0.5)<br/> def forward(self, input):<br/>   features = self.inception(input)<br/>   return self.dropout(self.relu(features)) </span></pre><p id="3be1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，这是一个相对简单的CNN架构。唯一的区别是，我们采用初始网络的最后一个完全连接的层，并手动将其更改为映射/连接到我们希望我们的特征嵌入的嵌入大小(以及RNN解码器将作为输入接受的大小)。</p><p id="5562" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">RNN解码器:</strong>不像CNN的，我们通常不使用RNN/LSTM的迁移学习。如果您熟悉LSTM架构，您会知道训练是按顺序进行的，每个连续的单元在下一个单元之前被训练。LSTM的输入通常只是前一个单元的隐藏状态和前一个LSTM单元的输出。然而，对于该模型，我们将连接由CNN产生的特征嵌入和LSTM的先前输出，并将该连接的张量传递到LSTM中。你可以从整个编码器-解码器架构的图像中看到这一点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/5871677dde6e03795b88699b9d80eb7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*MqsV98FA4XA99QFs7BXldg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是LSTM的Pytorch代码:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="8013" class="lv lw it mw b gy na nb l nc nd">class DecoderRNN(nn.Module):<br/>  def __init__(self, embed_size, hidden_size, vocab_size):<br/>    super(DecoderRNN, self).__init__()<br/>    self.embed = nn.Embedding(vocab_size, embed_size)<br/>    self.lstm = nn.LSTM(embed_size, hidden_size)<br/>    self.linear = nn.Linear(hidden_size, vocab_size)<br/>    self.dropout = nn.Dropout(0.5)<br/>  def forward(self, features, captions):<br/>    embeddings = self.dropout(self.embed(captions))<br/>    embeddings = torch.cat((features.unsqueeze(0), embeddings), <br/>                                                        dim=0)<br/>    hiddens, _ = self.lstm(embeddings)<br/>    outputs = self.linear(hiddens)<br/>    return outputs</span></pre><p id="90dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型中的一些超参数必须由你来选择(特别是hidden_size和embed_size参数；我只是两个都用了256)。vocab大小是一个参数，您必须根据所使用的数据集进行计算。在上面的模型代码中，像许多/大多数LSTM架构一样，为了实际获得每个单元格预测的单词，我使用了一个线性层，该层采用隐藏层并将其映射到vocab。更重要的一点是，在这个模型中，我从头开始生成单词embeddings。正如你在LSTM代码中看到的，我使用了一个神经网络。嵌入层，它接受vocab中每个单词的一键编码，并将它们转换为embed_size的嵌入。现在，我们通常不会从零开始生成单词嵌入(例如，许多人只是使用转换器权重)，因为这延长了训练过程，但是因为我们使用LSTM作为解码器，所以我们不能真正加载预训练的权重并使用它们。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="aa5b" class="lv lw it mw b gy na nb l nc nd">class Encoder_Decoder(nn.Module):<br/>  def __init__(self, embed_size, hidden_size, vocab_size):<br/>    super(Encoder_Decoder, self).__init__()<br/>    self.cnn = CNNEncoder(embed_size)<br/>    self.decoderRNN = DecoderRNN(embed_size, hidden_size,<br/>                                 vocab_size)<br/>  def forward(self, images, captions):<br/>    features = self.cnn(images)<br/>    outputs = self.decoderRNN(features, captions)<br/>    return outputs</span></pre><p id="c925" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过将CNN的输出作为解码器LSTM的输入来连接两个模型，并返回LSTM的最终输出。这个最终的编码器-解码器模型是我们将在我们的数据上训练的实际模型(不是另外两个)。</p><h2 id="8975" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">对编码器-解码器模型的修改</h2><p id="4aff" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然上面描述的模型架构已经是构建图像字幕模型的好方法，但是还有几种方法可以修改模型，使其更加强大。</p><p id="9333" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意机制:</strong>在高层次上，当解码器生成字幕的每个单词时，注意机制允许模型注意(或本质上关注)图像的相关部分。例如，以前面一只在水上奔跑的狗为例，当解码器为单词“狗”加字幕时，注意力机制将允许模型专注于包含狗的图像的空间区域。</p><p id="6aef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用两种类型的注意力机制:</p><p id="2692" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">软注意:</strong>软注意涉及通过考虑/注意图像的多个部分来构造单词——每个部分的程度不同。基本上，只要把它想象成使用——有时是许多——图像的不同部分，每个部分都有不同的强度。图像的某些部分将比图像的其他部分被更强烈地考虑。</p><p id="8566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">硬注意:</strong>硬注意涉及到考虑图像的多个部分，但是与软注意相反，图像的每个被考虑的部分具有相同的强度。换句话说，在生成特定单词时，被选择的图像的每个部分(即，模型“关注的”)被完全考虑，而未被选择的图像部分被完全忽略。</p><p id="3c03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们将CNN产生的特征嵌入馈送到解码器之前，为了添加注意机制，我们添加了将应用于特征嵌入的单独的线性/注意层。下面是一些伪代码/Pytorch代码演示软注意:</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="4c6e" class="lv lw it mw b gy na nb l nc nd">self.attention = nn.Linear(embed_size, embed_size)<br/>self.softmax = nn.Softmax()</span><span id="5219" class="lv lw it mw b gy ne nb l nc nd">attention_output = self.softmax(self.attention(feature_embedding))<br/>feature_embedding = feature_embedding * attention_output</span></pre><p id="a631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">softmax用于创建特征嵌入的概率分布。将这些概率与原始特征嵌入相乘将产生新的嵌入层，我们将把该新的嵌入层馈送给解码器。</p><p id="1425" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用转换器作为解码器:</strong>使用像BERT这样强大的转换器是构建更好的图像字幕模型的下一步。虽然这似乎是一个显而易见的选择，但在现实中却更具挑战性。这是因为转换器的固有结构限制了它只能输入单词和文本。例如，BERT被预先训练了像下一句预测和掩蔽语言建模这样的任务，这使得它很难将文本之外的东西纳入其输入。要将变压器端到端地纳入培训过程，需要一个特殊的/经过修改的变压器。有一些这方面的论文发表过，像<a class="ae ky" href="https://arxiv.org/abs/2203.15350" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae ky" href="https://arxiv.org/abs/2101.10804" rel="noopener ugc nofollow" target="_blank">这个</a>。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><p id="8440" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您觉得这些内容很容易理解。如果你认为我需要进一步阐述或澄清什么，请在下面留言。</p><h2 id="6bce" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="0348" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">MSCOCO数据集:<a class="ae ky" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">https://cocodataset.org/#home</a></p><p id="ec06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Flickr8k数据集:<a class="ae ky" href="https://www.kaggle.com/datasets/adityajn105/flickr8k" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/adityajn105/flickr8k</a></p></div></div>    
</body>
</html>