<html>
<head>
<title>19 Hidden Sklearn Features You Were Supposed to Learn The Hard Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">19个隐藏的Sklearn特性你应该通过艰苦的方式来学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/19-hidden-sklearn-features-you-were-supposed-to-learn-the-hard-way-5293e6ff149#2022-04-08">https://towardsdatascience.com/19-hidden-sklearn-features-you-were-supposed-to-learn-the-hard-way-5293e6ff149#2022-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4ba4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">但我会给你一条捷径</h2></div><p id="eef0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">了解您从未见过的19种Sklearn功能，这些功能可以直接、优雅地替代您手动执行的常见操作。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/62e665843dce11c11a88f989845ab510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IO1ColhqMcpsdoxBlhK7kQ.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">乌祖诺夫·罗斯季斯拉夫在佩克斯拍摄的照片</p></figure><h2 id="cb75" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">介绍</h2><p id="98ae" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">通过看Sklearn的<a class="ae lu" href="https://scikit-learn.org/stable/modules/classes.html#api-reference" rel="noopener ugc nofollow" target="_blank"> API参考</a>，我已经意识到最常用的模型和函数只是库所能做的很薄的一部分。尽管有些特性非常狭隘，只用于极少数极端情况，但我发现许多估算器、转换器和效用函数是对人们手工操作的更好的修正。</p><p id="a6fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我决定列出最重要的几个，并简要地解释它们，这样您就可以在一篇文章中极大地扩展您的Sklearn工具集。尽情享受吧！</p><div class="mt mu gp gr mv mw"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk lo mw"/></div></div></a></div><p id="5af0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="mt mu gp gr mv mw"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">alphasignal.ai</p></div></div><div class="nf l"><div class="nl l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="ba89" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" rel="noopener ugc nofollow" target="_blank">协方差。椭圆包络线</a></h2><p id="e52a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">分布中有异常值是很常见的。许多算法处理异常值，而<code class="fe nt nu nv nw b">EllipticalEnvelope</code>就是一个直接内置到Sklearn中的例子。该算法的优势在于它在检测正态分布(高斯)特征中的异常值方面表现出色:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="07bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了测试估计量，我们创建了一个均值为5、标准差为2的正态分布。在它被训练之后，我们将一些随机数传递给它的<code class="fe nt nu nv nw b">predict</code>方法。对于<code class="fe nt nu nv nw b">test</code>中的异常值，该方法返回-1，即20，10，13。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="5c08" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">2️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank">特征_选择。RFECV </a></h2><p id="9229" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">选择最有助于预测的特征是克服过度拟合和降低模型复杂性的必要步骤。Sklearn提供的最健壮的算法之一是递归特征消除(RFE)。它通过使用交叉验证自动找到最重要的特性，并丢弃其余的特性。</p><p id="62f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个估计器的一个优点是它是一个包装器——它可以在任何返回特征重要性或系数分数的Sklearn算法周围使用。以下是一个关于合成数据集的示例:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="ddbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假数据集有15个特征，其中10个是信息性的，其余的是冗余的。我们用<code class="fe nt nu nv nw b">Ridge</code>回归作为估计量来拟合5倍RFECV。训练结束后，可以用<code class="fe nt nu nv nw b">transform</code>的方法丢弃多余的特征。调用<code class="fe nt nu nv nw b">.shape</code>向我们展示了评估者设法丢弃了所有5个不必要的特性。</p><p id="89a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我已经写了一整篇关于这种算法的文章，涵盖了它如何与现实世界的数据集一起工作的基本细节:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">Sklearn的递归特征消除(RFE)功能强大的特征选择</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">即使删除93个功能，也能获得相同的模型性能</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nz l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="b05a" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">3️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html" rel="noopener ugc nofollow" target="_blank">合奏。树外</a></h2><p id="a544" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">即使随机森林都很强大，过度适应的风险也很高。因此，Sklearn提供了一种称为ExtraTrees(分类器和回归器)的RF替代方法。</p><p id="bf0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“额外”这个词并不意味着更多的树，而是更多的随机性。该算法使用另一种非常类似决策树的树。唯一的区别是，不是在构建每个树时计算分割阈值，而是为每个特征随机绘制这些阈值，并选择最佳阈值作为分割规则。这允许以偏置略微增加为代价来稍微降低方差:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="d6ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，ExtraTreesRegressor在合成数据集上的表现优于随机森林。</p><p id="ed6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从官方<a class="ae lu" href="https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees" rel="noopener ugc nofollow" target="_blank">用户指南</a>中阅读更多关于极度随机化树木的信息。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="a6f4" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">4️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html" rel="noopener ugc nofollow" target="_blank">估算。迭代输入器</a>和<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html" rel="noopener ugc nofollow" target="_blank">计算器</a></h2><p id="cf33" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果您正在寻找比<code class="fe nt nu nv nw b">SimpleImputer</code>更强大、更先进的插补技术，Sklearn将再次满足您的需求。<code class="fe nt nu nv nw b">impute</code>子包包括两个基于模型的插补算法- <code class="fe nt nu nv nw b">KNNImputer</code>和<code class="fe nt nu nv nw b">IterativeImputer</code>。</p><p id="b9b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顾名思义，<code class="fe nt nu nv nw b">KNNImputer</code>使用k-最近邻算法来查找缺失值的最佳替换:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="1454" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更健壮的算法是<code class="fe nt nu nv nw b">IterativeImputer</code>。它通过将每个具有缺失值的要素建模为其他要素的函数来查找缺失值。这一过程以逐步循环的方式完成。在每一步，具有缺失值的单个特征被选择作为目标(<code class="fe nt nu nv nw b">y</code>)，其余的被选择作为特征阵列(<code class="fe nt nu nv nw b">X</code>)。然后，使用回归器预测<code class="fe nt nu nv nw b">y</code>中的缺失值，并对每个特征继续该过程，直到<code class="fe nt nu nv nw b">max_iter</code>时间(迭代输入器的一个参数)。</p><p id="4d8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对于单个缺失值，会生成多个预测。这样做的好处是将每个缺失值视为一个随机变量，并关联随之而来的固有不确定性:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><blockquote class="oa ob oc"><p id="8286" class="ki kj od kk b kl km ju kn ko kp jx kq oe ks kt ku of kw kx ky og la lb lc ld im bi translated">BayesianRidge和ExtraTree被发现使用IterativeImputer执行得更好。</p></blockquote><p id="ed50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以在我的另一篇文章中了解更多关于这两种插补技术的信息:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/going-beyond-the-simpleimputer-for-missing-data-imputation-dd8ba168d505"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">超越缺失数据插补的简单插补</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">学会利用强大的基于模型的插补方法</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="oh l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="fc39" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">5️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html" rel="noopener ugc nofollow" target="_blank">线性_模型。休伯回归量</a></h2><p id="fdb6" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">离群值的存在会严重扰乱任何模型的预测。许多离群点检测算法丢弃离群点并将它们标记为缺失。虽然这有助于模型的学习功能，但它完全消除了异常值对分布的影响。</p><p id="b6be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个备选算法是<code class="fe nt nu nv nw b">HuberRegressor</code>。它不是完全去除异常值，而是在拟合期间减少异常值的权重。它有<code class="fe nt nu nv nw b">epsilon</code>超参数，控制应归类为异常值的样本数量。参数越小，对异常值的鲁棒性越强。它的API与任何其他线性回归相同。下面，您可以看到<a class="ae lu" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py" rel="noopener ugc nofollow" target="_blank">与贝叶斯岭回归器在一个有大量异常值的数据集上的比较</a>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/eab881f75b705ba74ff2c43a94b65504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*9Q2diqnQOwhX_bz0.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd oj">图片由Sklearn用户指南提供。</strong> <a class="ae lu" href="https://github.com/scikit-learn/scikit-learn/blob/main/COPYING" rel="noopener ugc nofollow" target="_blank"> <strong class="bd oj">许可证— BSD-3 </strong> </a></p></figure><p id="1f6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以看出，具有ε1.35、1.5、1.75的HuberRegressor设法捕获了不受异常值影响的最佳拟合线</p><p id="eaff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以从<a class="ae lu" href="https://scikit-learn.org/stable/modules/linear_model.html#huber-regression" rel="noopener ugc nofollow" target="_blank">用户指南</a>中了解更多关于该算法的信息。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="d326" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">6️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html" rel="noopener ugc nofollow" target="_blank"> tree.plot_tree </a></h2><p id="56b9" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Sklearn允许您使用<code class="fe nt nu nv nw b">plot_tree</code>函数绘制单个决策树的结构。对于刚开始学习基于树和集合模型的初学者来说，这个特性可能很方便:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ok"><img src="../Images/1d2fa9c0f01bd0b18f280101e17b261b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QQ1Nb2MOS2ry8AcSd5YiLA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd oj">图片由Sklearn用户指南提供。</strong> <a class="ae lu" href="https://github.com/scikit-learn/scikit-learn/blob/main/COPYING" rel="noopener ugc nofollow" target="_blank"> <strong class="bd oj">许可证— BSD-3 </strong> </a></p></figure><p id="3803" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有其他绘制树的方法，比如Graphviz格式。从<a class="ae lu" href="https://scikit-learn.org/stable/modules/tree.html#decision-trees" rel="noopener ugc nofollow" target="_blank">用户指南</a>中了解它们。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="f8b6" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">7️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html" rel="noopener ugc nofollow" target="_blank">线性_模型。感知器</a></h2><p id="2b46" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这个列表中最酷的名字是第七名——感知器。尽管它有一个花哨的名字，但它是一个简单的线性二元分类器。该算法的定义特征是它适合于大规模学习，并且默认情况下:</p><ol class=""><li id="2261" class="ol om it kk b kl km ko kp kr on kv oo kz op ld oq or os ot bi translated">它不需要学习速度。</li><li id="c69a" class="ol om it kk b kl ou ko ov kr ow kv ox kz oy ld oq or os ot bi translated">不实现正规化。</li><li id="3da9" class="ol om it kk b kl ou ko ov kr ow kv ox kz oy ld oq or os ot bi translated">它只在出错时更新模型。</li></ol><p id="5efe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它相当于带有<code class="fe nt nu nv nw b">loss='perceptron', eta0=1, learning_rate="constant", penalty=None</code>的SGDClassifier，但速度稍快:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="eb45" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">8️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html" rel="noopener ugc nofollow" target="_blank">特征_选择。从模型中选择</a></h2><p id="9bdc" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Sklearn中另一个基于模型的特征选择估计器是<code class="fe nt nu nv nw b">SelectFromModel</code>。它不像RFECV那样健壮，但是对于大规模数据集来说是一个很好的选择，因为它具有较低的计算成本。它也是一个包装估计器，适用于任何具有<code class="fe nt nu nv nw b">.feature_importances_</code>或<code class="fe nt nu nv nw b">.coef_</code>属性的模型:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="2c8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如你所见，该算法成功地删除了所有40个冗余特征。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="7a40" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">9️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html" rel="noopener ugc nofollow" target="_blank">指标。混淆矩阵显示</a></h2><p id="9705" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">混淆矩阵是分类问题的圣杯。大部分指标都来源于它，比如精度、召回率、F1、ROC AUC等。Sklearn允许您计算和绘制默认混淆矩阵:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ok"><img src="../Images/f2b8498d378374df7375a705f099f9ca.png" data-original-src="https://miro.medium.com/v2/format:webp/1*49dmozt7TJ-oewPMIyIdKg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="0e7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">老实说，我不会说我喜欢默认的混淆矩阵。它的格式是固定的—行是真正的标签，列是预测。同样，第一行和第一列是负类，第二行和第二列是正类。有些人可能更喜欢不同格式的矩阵，可能是转置或翻转的。</p><p id="6d91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，我喜欢将positive类作为第一行和第一列，以符合维基百科中给出的格式。这有助于我更好地隔离4个矩阵术语——TP、FP、TN、FN。幸运的是，您可以使用另一个函数<code class="fe nt nu nv nw b">ConfusionMatrixDisplay</code>绘制自定义矩阵:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ok"><img src="../Images/97bed8f0e71a51c3eb3f30b9014563c5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*LEZvdRKmdoPb8s-Awm1tTA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="8046" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在把混淆矩阵<code class="fe nt nu nv nw b">cm</code>传给<code class="fe nt nu nv nw b">ConfusionMatrixDisplay</code>之前，你可以把它做成任何你想要的格式。</p><p id="af16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以从这篇文章中了解关于分类和混淆矩阵的所有内容:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/how-to-tune-models-like-a-puppet-master-based-on-confusion-matrix-fd488f9b5e65"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">如何使用混淆矩阵进行面向问题的模型调整</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">学习使用混淆矩阵，根据问题的重要性来控制模型输出</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="oz l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="0825" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">🔟。<a class="ae lu" href="https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-regression" rel="noopener ugc nofollow" target="_blank">广义线性模型</a></h2><p id="be0a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果有其他方法可以处理其他类型的分布，那么转换目标(<em class="od"> y </em>)使其呈正态分布是没有意义的。</p><p id="956b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，Sklearn为泊松、特威迪或伽马分布的目标变量提供了3种广义线性模型。<code class="fe nt nu nv nw b">PoissonRegressor</code>、<code class="fe nt nu nv nw b">TweedieRegressor</code>和<code class="fe nt nu nv nw b">GammaRegressor</code>可以为具有各自分布的目标生成稳健的结果，而不是期望正态分布。</p><p id="0c10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除此之外，他们的API与其他任何Sklearn模型都是一样的。为了找出目标的分布是否与上述三者相匹配，您可以在具有完美分布的相同轴上绘制它们的PDF(概率密度函数)。</p><p id="589b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，为了查看目标是否遵循泊松分布，使用Seaborn的<code class="fe nt nu nv nw b">kdeplot</code>绘制其PDF，并通过在相同轴上使用<code class="fe nt nu nv nw b">np.random.poisson</code>从Numpy采样来绘制完美的泊松分布。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="0f06" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣1️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank">合奏。隔离森林</a></h2><p id="d57e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">由于基于树的模型和集成模型通常产生更鲁棒的结果，因此它们在异常值检测中也被证明是有效的。在Sklearn中，使用一个由非常随机的树组成的森林(<code class="fe nt nu nv nw b">tree.ExtraTreeRegressor</code>)来检测异常值。每棵树都试图通过选择单个特征并随机选择所选特征的最大值和最小值之间的分割值来隔离每个样本。</p><p id="4f06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种类型的随机划分在每棵树的根节点和终止节点之间产生明显更短的路径。</p><blockquote class="oa ob oc"><p id="cf68" class="ki kj od kk b kl km ju kn ko kp jx kq oe ks kt ku of kw kx ky og la lb lc ld im bi translated">因此，当随机树的森林共同产生特定样本的较短路径长度时，它们极有可能是异常——sk learn用户指南。</p></blockquote><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">该算法正确地捕获了异常值(90)并将其标记为-1。</p></figure><p id="d807" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae lu" href="https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest" rel="noopener ugc nofollow" target="_blank">用户指南</a>中阅读更多关于该算法的信息。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="937c" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣2️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html" rel="noopener ugc nofollow" target="_blank">预处理。电力变压器</a></h2><p id="90c3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">许多线性模型需要对数字要素进行一些变换，以使其呈正态分布。<code class="fe nt nu nv nw b">StandardScaler</code>和<code class="fe nt nu nv nw b">MinMaxScaler</code>对于大多数发行版来说都非常好用。但是，当数据中存在高偏斜度时，分布的核心指标，如平均值、中值、最小值和最大值，会受到影响。因此，简单的规范化和标准化对偏态分布不起作用。</p><p id="ce59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，Sklearn实现了<code class="fe nt nu nv nw b">PowerTransformer</code>,它使用对数变换将任何倾斜的特征尽可能地变成正态分布。考虑钻石数据集中的这两个要素:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ok"><img src="../Images/316ef14f04e31cbfc601800978752587.png" data-original-src="https://miro.medium.com/v2/format:webp/1*iyBNsKmgVUrc1XCo7QbIgQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="ef3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两者都严重倾斜。让我们用对数变换来解决这个问题:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ok"><img src="../Images/1b8723c270d284f154120b1723d2aa37.png" data-original-src="https://miro.medium.com/v2/format:webp/1*gI1ldI5_-TdPdENNQEByQw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="2f85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">偏斜没了！您可以在此阅读有关不同类型的要素变换的更多信息:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/how-to-differentiate-between-scaling-normalization-and-log-transformations-69873d365a94"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">如何区分缩放、规范化和对数转换</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">获取特征工程数值变量的实用知识</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="pa l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="311c" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣3️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank">预处理。鲁棒定标器</a></h2><p id="424e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Sklearn中的另一个数字变换器是<code class="fe nt nu nv nw b">RobustScaler</code>。你可能从它的名字就能猜到它的作用——它能以一种对异常值鲁棒的方式变换特征。如果要素中存在异常值，则很难使其呈正态分布，因为它们会严重扭曲平均值和标准差。</p><p id="9df9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nt nu nv nw b">RobustScaler</code>没有使用均值/标准差，而是使用中位数和IQR(四分位间距)来衡量数据，因为这两个指标不会因为异常值而有偏差。您也可以在<a class="ae lu" href="https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data" rel="noopener ugc nofollow" target="_blank">用户指南</a>中了解相关信息。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="2789" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣4️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html" rel="noopener ugc nofollow" target="_blank">compose . make _ column _ transformer</a></h2><p id="7544" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在Sklearn中，有一个用<code class="fe nt nu nv nw b">make_pipeline</code>函数创建管道实例的简写。该函数只接受转换器和估算器并完成它的工作，而不是命名每一步并使代码变得不必要的长:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="932f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于更复杂的场景，使用<code class="fe nt nu nv nw b">ColumnTransformer</code>，这也有同样的问题——每个预处理步骤都要命名，使得你的代码又长又不可读。幸运的是，Sklearn提供了与<code class="fe nt nu nv nw b">make_pipeline</code>类似的功能:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="7122" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如您所看到的，使用<code class="fe nt nu nv nw b">make_column_transformer</code>要短得多，而且它负责单独命名每个转换器步骤。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="fd7c" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣5️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html" rel="noopener ugc nofollow" target="_blank">compose . make _ column _选择器</a></h2><p id="f2c7" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果您注意的话，我们使用了pandas数据帧的<code class="fe nt nu nv nw b">select_dtypes</code>函数和<code class="fe nt nu nv nw b">columns</code>属性来隔离数字和分类列。虽然这肯定有效，但是使用Sklearn有一个更加灵活和优雅的解决方案。</p><p id="c363" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nt nu nv nw b">make_column_selector</code>函数创建一个可以直接传递给<code class="fe nt nu nv nw b">ColumnTransformer</code>实例的列选择器。它的工作方式和<code class="fe nt nu nv nw b">select_dtypes</code>一样，而且更好。它有<code class="fe nt nu nv nw b">dtype_include</code>和<code class="fe nt nu nv nw b">dtype_exclude</code>参数来根据数据类型选择列。如果您需要一个定制的列过滤器，您可以将一个正则表达式传递给<code class="fe nt nu nv nw b">pattern</code>，同时将其他参数设置为<code class="fe nt nu nv nw b">None</code>。它是这样工作的:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3e81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不用传递列名列表，只传递一个带有相关参数的<code class="fe nt nu nv nw b">make_column_selector</code>实例，就大功告成了！</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="8802" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣6️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">预处理。普通编码器</a></h2><p id="1fe9" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">初学者的一个常见错误是使用<code class="fe nt nu nv nw b">LabelEncoder</code>对有序分类特征进行编码。如果你已经注意到了，<code class="fe nt nu nv nw b">LabelEncoder</code>允许一次只转换一列，而不是像<code class="fe nt nu nv nw b">OneHotEncoder</code>一样同时转换。你可能会觉得Sklearn搞错了！</p><p id="89b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，<code class="fe nt nu nv nw b">LabelEncoder</code>应仅用于对响应变量(<code class="fe nt nu nv nw b">y</code>)进行编码，如其<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank">文档</a>中所规定。要对特性数组(<code class="fe nt nu nv nw b">X</code>)进行编码，您应该使用<code class="fe nt nu nv nw b">OrdinalEncoder</code>，它会像预期的那样工作。它将有序分类列转换为具有(0，n_categories - 1)类的要素。它在一行代码中对所有指定的列都这样做，这使得在管道中包含它成为可能。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="e534" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣7️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer.html" rel="noopener ugc nofollow" target="_blank"> metrics.get_scorer </a></h2><p id="099e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Sklearn内置了50多个指标，在<code class="fe nt nu nv nw b">sklearn.metrics.SCORERS.keys</code>中可以看到它们的文字名称。在一个项目中，您可能需要使用几个指标，如果您单独使用它们的话，就需要导入它们。</p><p id="9794" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直接从<code class="fe nt nu nv nw b">sklearn.metrics</code>导入大量指标可能会污染您的名称空间，并变得不必要的长。作为一个解决方案，您可以使用<code class="fe nt nu nv nw b">metrics.get_scorer</code>函数访问任何带有文本名称的指标，而无需导入它:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="nx ny l"/></div></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="44fa" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣8️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html" rel="noopener ugc nofollow" target="_blank">型号_选择。HalvingGrid </a>和<a class="ae lu" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> HalvingRandomSearchCV </a></h2><p id="2b45" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在Sklearn的0.24版本中，我们介绍了两个实验性的超参数优化器:<code class="fe nt nu nv nw b">HalvingGridSearchCV</code>和<code class="fe nt nu nv nw b">HalvingRandomSearchCV</code>类。</p><p id="2c1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不同于他们详尽的表亲GridSearch和RandomizedSearch，新的类使用了一种叫做连续减半的技术。不是在所有数据上训练所有候选集，而是只给参数一个数据子集。通过在更小的数据子集上训练，表现最差的候选人被过滤掉。在每次迭代之后，训练样本以某个因子增加，并且可能的候选者的数量减少，从而导致更快的评估时间。</p><p id="1ee3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">快了多少？在我做过的实验中，HalvingGridSearch比常规GridSearch快11倍，HalvingRandomSearch甚至比HalvingGridSearch快10倍。你可以从这里阅读我对连续减半的详细概述和我的实验:</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">HalvingGridSearch将超参数调谐速度提高了11倍</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">编辑描述</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="pb l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="37e2" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">1️⃣9️⃣.<a class="ae lu" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.utils" rel="noopener ugc nofollow" target="_blank"> sklearn.utils </a></h2><p id="5c33" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">最后但同样重要的是，Sklearn在<code class="fe nt nu nv nw b">sklearn.utils</code>子包下有一大堆实用程序和助手函数。Sklearn本身使用这个模块中的函数来构建我们都使用的所有转换器和估计器。</p><p id="a5b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有用的有<code class="fe nt nu nv nw b">class_weight.compute_class_weight</code>、<code class="fe nt nu nv nw b">estimator_html_repr</code>、<code class="fe nt nu nv nw b">shuffle</code>、<code class="fe nt nu nv nw b">check_X_y</code>等。您可以在自己的工作流中使用它们，使您的代码更像Sklearn，或者在创建适合Sklearn API的定制转换器和估算器时，它们可能会派上用场。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="d31f" class="lv lw it bd lx ly lz dn ma mb mc dp md kr me mf mg kv mh mi mj kz mk ml mm mn bi translated">摘要</h2><p id="5651" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">尽管像CatBoost、XGBoost、LightGBM这样的库正在慢慢地从Sklearn那里抢走领先的ML库的头把交椅，但它仍然是现代ML工程师技能堆栈中非常宝贵的一部分。</p><p id="bc22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一致的API、出色的代码设计和创建健壮的ML工作流的能力仍然使Sklearn在功能和灵活性方面无与伦比。尽管我们可以用这些基础知识完成很多工作，但这篇文章表明，Sklearn提供的东西比看上去的要多得多！</p><p id="fa64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p><div class="mt mu gp gr mv mw"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">通过我的推荐链接加入Medium。</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="nf l"><div class="pc l nh ni nj nf nk lo mw"/></div></div></a></div></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><div class="lf lg lh li gt mw"><a rel="noopener follow" target="_blank" href="/complete-guide-to-experiment-tracking-with-mlflow-and-dagshub-a0439479e0b9"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">使用MLFlow和DagsHub进行实验跟踪的完整指南</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">创建可重复且灵活的ML项目</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="pd l nh ni nj nf nk lo mw"/></div></div></a></div><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/6-grievous-sklearn-mistakes-that-give-no-error-messages-dc8748fbc37d"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd iu gy z fp nb fr fs nc fu fw is bi translated">6个没有错误信息的严重错误</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">你应该不惜一切代价避开他们</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="pe l nh ni nj nf nk lo mw"/></div></div></a></div></div></div>    
</body>
</html>