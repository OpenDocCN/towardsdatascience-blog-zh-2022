<html>
<head>
<title>LDA Is More Effective than PCA for Dimensionality Reduction in Classification Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在分类数据集中，LDA 比 PCA 更能有效地降低维数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632#2022-12-29">https://towardsdatascience.com/lda-is-highly-effective-than-pca-for-dimensionality-reduction-in-classification-datasets-4489eade632#2022-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5a7b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性判别分析(LDA)在最大化类别可分性的同时进行维数缩减</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5db2649129fda32d9071394d6c5398f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4jx9IpKOsLKEgo8sEr4GA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">威尔·弗朗西斯在<a class="ae ky" href="https://unsplash.com/photos/Rm3nWQiDTzg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="9eec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用各种技术来实现降维。11 种这样的技术在我的热门文章<a class="ae ky" rel="noopener" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b"><em class="lv">2021 年你应该知道的 11 种降维技术</em> </a>中已经讨论过了。</p><p id="d2e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那里，你将适当地学习一些技术术语背后的含义，如维度和降维。</p><p id="1297" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，<strong class="lb iu"> <em class="lv">维度</em> </strong>是指数据集中特征(变量)的数量。减少数据集中特征的过程称为<strong class="lb iu"> <em class="lv">降维</em> </strong>。</p><h1 id="c356" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">线性判别分析</h1><p id="c93b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">线性判别分析(下文称为 LDA)是一种流行的线性降维技术，它可以在低维空间中找到输入特征的线性组合，同时最大化类别可分性。</p><p id="3133" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类别可分性仅仅意味着我们尽可能地保持类别，同时保持每个类别中数据点之间的最小间隔。</p><p id="0e3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类别分离得越好，就越容易在类别之间绘制决策边界来分离(区分)数据点组。</p><p id="808c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA 通常用于具有类标注的分类数据集。它既可以用作二元/多类分类(监督学习)算法，也可以用作降维(非监督学习)算法。</p><p id="b9eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，LDA 在用于降维时需要类别标签。因此，LDA 执行<em class="lv">监督降维</em>。</p><p id="0963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拟合的 LDA 模型可以用于分类和维数减少，如下所示。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="f318" class="my lx it mu b be mz na l nb nc">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/><br/>lda = LinearDiscriminantAnalysis().fit(X, y) # fitted LDA model</span></pre><ul class=""><li id="d7c4" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated"><code class="fe nm nn no mu b"><strong class="lb iu">lda.predict(X)</strong></code> <strong class="lb iu"> : </strong>进行多级分类。这将向可用类别分配新的数据点。</li><li id="1755" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated"><code class="fe nm nn no mu b"><strong class="lb iu">lda.transform(X)</strong></code> <strong class="lb iu"> : </strong>在分类数据集中执行降维，同时最大化类的分离。这将在低维空间中找到输入特征的线性组合。当以这种方式使用<em class="lv"> lda </em>时，它充当数据预处理步骤，其输出被用作另一种分类算法的输入，例如支持向量机或逻辑回归！</li></ul><h1 id="e8b0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">PCA 与 LDA</h1><p id="188c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">PCA 是最流行的降维技术。PCA 和 LDA 都被认为是<strong class="lb iu"> <em class="lv">线性</em> </strong>降维技术，因为它们在数据中找到输入特征的线性组合。</p><p id="2347" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这两种算法之间有显著的差异。</p><ul class=""><li id="7580" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">PCA 通过最大化数据的方差来执行维数减少。因此，在大多数情况下，在应用 PCA 之前，特征标准化是必要的(参见这里的例外<a class="ae ky" href="https://rukshanpramoditha.medium.com/principal-component-analysis-18-questions-answered-4abd72041ccd#f853" rel="noopener"/>)。</li><li id="4481" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">LDA 通过最大化分类数据集的类别可分性来执行维数缩减。因此，特性标准化在这里是可选的(我们将很快验证这一点)。</li><li id="8042" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">PCA 不需要类别标签。因此，它可以用于分类，回归，甚至无标签的数据！</li><li id="d08e" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">LDA 要求等级标签。因此，它用于分类数据集。</li><li id="4248" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">PCA 在低维空间中找到一组不相关的特征。因此，PCA 会自动移除数据中的多重共线性(在此处了解更多<a class="ae ky" rel="noopener" target="_blank" href="/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b">)。</a></li><li id="2d58" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">如前所述，LDA 可用于监督和非监督任务。PCA 只能用于无监督的降维。</li><li id="704a" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">PCA 能找到的最大分量数等于数据集的输入特征(原始维数)数！我们通常倾向于找到数量相当少的组件，尽可能多地捕捉原始数据中的差异。</li><li id="99c8" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">LDA 可以找到的最大组件数等于分类数据集中的类数减一。例如，如果数据集中只有 3 个类，LDA 最多可以找到 2 个组件。</li><li id="1259" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">对于分类数据集，LDA 比 PCA 更有效，因为 LDA 通过最大化类别可分性来降低数据的维度。对于具有最大类可分性的数据，更容易绘制决策边界。</li></ul><p id="5ae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，在本文中，<strong class="lb iu">我们将<em class="lv">通过使用葡萄酒分类数据集，直观地</em>证明 LDA 比 PCA 在分类数据集中的降维方面更加有效。</strong>然后，我们将继续讨论如何将 LDA 模型的输出用作支持向量机或逻辑回归等分类算法的输入！</p><h1 id="cc2d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">葡萄酒数据集</h1><p id="4161" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这里，我们将使用<a class="ae ky" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset" rel="noopener ugc nofollow" target="_blank">葡萄酒分类数据集</a>来执行 PCA 和 LDA。以下是关于葡萄酒数据集的重要信息。</p><ul class=""><li id="3f5c" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated"><strong class="lb iu">数据集来源:</strong>你可以在这里下载原始数据集<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Wine" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="fd32" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated"><strong class="lb iu">数据集许可:</strong>该数据集可在<a class="ae ky" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> CC 下通过 4.0</em></a>(<em class="lv">Creative Commons Attribution 4.0</em>)许可获得。</li><li id="088d" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated"><strong class="lb iu">所有者:</strong> Forina，m .等人，par vus——一个用于数据探索、分类和关联的可扩展包。意大利热那亚 Via Brigata Salerno，16147，药物和食品分析与技术研究所。</li><li id="7702" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">捐赠者:斯蒂芬·艾伯哈德</li><li id="aca8" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated"><strong class="lb iu">引用:</strong> Lichman，M. (2013)。https://archive.ics.uci.edu/ml 的 UCI 机器学习库。加州欧文:加州大学信息与计算机科学学院。</li></ul><p id="ffb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">葡萄酒数据集预装了 Scikit-learn。可以通过调用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> load_wine() </strong> </a>函数来加载，如下。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="89e0" class="my lx it mu b be mz na l nb nc">from sklearn.datasets import load_wine<br/><br/>wine = load_wine()<br/>X = wine.data<br/>y = wine.target<br/><br/>print("Wine dataset size:", X.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c431f2adba531d5ed54ca9f36d72cea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*3TNyV_LH3kwj57SQc3zolA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="3e09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">葡萄酒数据集有 178 个实例(数据点)。它的原始维数是 13，因为它有 13 个输入特征(变量)。此外，数据点被分为三个独立的类别，代表每一种葡萄酒。</p><h1 id="4dd7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">将主成分分析应用于葡萄酒数据</h1><p id="b965" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将把主成分分析应用于葡萄酒数据，以实现以下目标。</p><ul class=""><li id="504e" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">构建 PCA 模型，该模型可用于与将在接下来的步骤中创建的 ld a 模型进行比较。</li><li id="639b" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">使用前两个主成分在 2D 散点图中可视化高维(13 维)葡萄酒数据。您可能已经知道 PCA 对于数据可视化非常有用。</li></ul><h2 id="14ce" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">特征标准化</h2><p id="f976" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在将 PCA 应用于葡萄酒数据之前，我们需要进行特征标准化，以将所有特征纳入同一尺度。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="2ff3" class="my lx it mu b be mz na l nb nc">from sklearn.preprocessing import StandardScaler<br/><br/>X_scaled = StandardScaler().fit_transform(X)</span></pre><p id="0c22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">缩放后的特征存储在<em class="lv"> X_scaled </em>变量中，该变量将成为<code class="fe nm nn no mu b">pca.fit_transform()</code>方法的输入。</p><h2 id="c6f8" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">运行 PCA</h2><p id="33ee" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将通过调用 Scikit-learn <strong class="lb iu"> PCA() </strong>函数对葡萄酒数据应用 PCA。我们希望保留的组件数量(在<code class="fe nm nn no mu b">n_components</code>中指定)严格限制为两个，因为我们对葡萄酒数据的 2D 可视化感兴趣，它只需要两个组件！</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="5c87" class="my lx it mu b be mz na l nb nc">from sklearn.decomposition import PCA<br/><br/>pca = PCA(n_components=2)<br/>X_pca = pca.fit_transform(X_scaled)</span></pre><p id="afdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转换(简化)后的数据存储在<em class="lv"> X_pca </em>变量中，该变量包含可以准确表示原始葡萄酒数据的二维数据。</p><h2 id="c40c" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">制作散点图</h2><p id="3848" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，我们将使用存储在<em class="lv"> X_pca </em>变量中的数据制作散点图。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="266d" class="my lx it mu b be mz na l nb nc">import matplotlib.pyplot as plt<br/>plt.figure(figsize=[7, 5])<br/><br/>plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=25, cmap='plasma')<br/>plt.title('PCA for wine data with 2 components')<br/>plt.xlabel('PC1')<br/>plt.ylabel('PC2')<br/>plt.savefig("PCA.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d732e8f265b5df3eafebd3d9523e5579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*mtQesSo4gWX1pKrW4RAyKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oi"> 2D 葡萄酒数据的 PCA 散点图</strong>(图片由作者提供)</p></figure><p id="dc1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据似乎可以用线性决策边界(即直线)进行线性分离。然而，一些数据点会被错误分类。由于 PCA 没有最大化类的可分性，类没有被很好地分离。</p><p id="cd94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当通过仅保留两个成分将 PCA 应用于葡萄酒数据时，我们丢失了数据中的大量方差。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="eee4" class="my lx it mu b be mz na l nb nc">exp_var = sum(pca.explained_variance_ratio_ * 100)<br/>print('Variance explained:', exp_var)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/532428c85cabd8fde5e6fff4010bbf28.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*ZK0Wz5BsBJ1wctRvVjUZpg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="221e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只有大约 55.4%的方差被我们的具有两个分量的 PCA 模型捕获。这么大的差异不足以准确地代表原始数据。</p><p id="c9e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过创建下面的图来找到葡萄酒数据的主成分的最佳数量。该值应大于 2 但小于 13(输入要素的数量)。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="ae6e" class="my lx it mu b be mz na l nb nc">import numpy as np<br/><br/>pca = PCA(n_components=None)<br/>X_pca = pca.fit_transform(X_scaled)<br/><br/>exp_var = pca.explained_variance_ratio_ * 100<br/>cum_exp_var = np.cumsum(exp_var)<br/><br/>plt.bar(range(1, 14), exp_var, align='center',<br/>        label='Individual explained variance')<br/><br/>plt.step(range(1, 14), cum_exp_var, where='mid',<br/>         label='Cumulative explained variance', color='red')<br/><br/>plt.ylabel('Explained variance percentage')<br/>plt.xlabel('Principal component index')<br/>plt.xticks(ticks=list(range(1, 14)))<br/>plt.legend(loc='best')<br/>plt.tight_layout()<br/><br/>plt.savefig("Barplot_PCA.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5e27b552287a6852bfbd89582152b636.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*sMGwzlpce14JidXDJV9XGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="c984" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种类型的图被称为<em class="lv">累积解释方差图</em>，在应用 PCA 时，这种图对于找到主成分的最佳数量非常有用。</p><p id="5a44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前六个或七个分量捕获了数据中大约 85–90%的方差。因此，它们将准确地表示原始的葡萄酒数据。但是，对于 2D 可视化，我们严格地希望只使用两个组件，即使它们没有捕捉到数据中的大部分变化。</p><h1 id="ee21" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">将 LDA 应用于葡萄酒数据</h1><p id="bd04" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，我们将 LDA 应用于葡萄酒数据，并将 LDA 模型与之前的 PCA 模型进行比较。</p><h2 id="a5d8" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">特征标准化</h2><p id="6f2d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">LDA 不需要特征标准化，因为它对 LDA 模型的性能没有任何影响。</p><h2 id="6851" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">运行 LDA</h2><p id="2ff1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将通过调用 Scikit-learn<strong class="lb iu">LinearDiscriminantAnalysis()</strong>函数对葡萄酒数据应用 LDA。我们希望保留的组件数量(在<code class="fe nm nn no mu b">n_components</code>中指定)严格限制为两个，因为我们对葡萄酒数据的 2D 可视化感兴趣，它只需要两个组件！</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="fb4e" class="my lx it mu b be mz na l nb nc">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/><br/>lda = LinearDiscriminantAnalysis(n_components=2)<br/>X_lda = lda.fit_transform(X_scaled, y)</span></pre><p id="aedc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，对于 LDA，在<code class="fe nm nn no mu b">fit_transform()</code>方法中还需要类标签(y)。</p><p id="beb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转换(简化)后的数据存储在<em class="lv"> X_lda </em>变量中，该变量包含可以准确表示原始葡萄酒数据的二维数据。</p><h2 id="b18e" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">制作散点图</h2><p id="f367" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，我们将使用存储在<em class="lv"> X_lda </em>变量中的数据制作散点图。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="33b1" class="my lx it mu b be mz na l nb nc">import matplotlib.pyplot as plt<br/>plt.figure(figsize=[7, 5])<br/><br/>plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, s=25, cmap='plasma')<br/>plt.title('LDA for wine data with 2 components')<br/>plt.xlabel('Component 1')<br/>plt.ylabel('Component 2')<br/>plt.savefig("LDA.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9b7a9b5cf76973388673bb19386822d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*TMZglG8C6RfPhFsjDDoz_w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oi"> 2D 葡萄酒数据散点图与 LDA </strong>(图片由作者提供)</p></figure><p id="0b4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，由于 LDA 除了降低维数之外还最大化了类别可分性，因此类别已经被清楚地分离。绘制线性决策边界时，数据点不会被错误分类。</p><p id="4742" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA 可以为葡萄酒数据保留的最大组件数也是两个，因为数据中只有三个类。因此，这两个组件应该捕获数据中的所有差异。</p><p id="f002" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用数字和视觉来验证这一点！</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="101d" class="my lx it mu b be mz na l nb nc">exp_var = sum(lda.explained_variance_ratio_ * 100)<br/>print('Variance explained:', exp_var)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2f0af2106cab2965fd0a32b8df0d45b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*qubNrM9bLh7xxkmK1nMN6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="e466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原始葡萄酒数据中的所有差异都由我们的 LDA 模型捕获，该模型包含两个部分。因此，这些组件将完全代表原始数据。</p><p id="ae54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们为 LDA 模型创建累积解释方差图。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="7ffc" class="my lx it mu b be mz na l nb nc">import numpy as np<br/><br/>lda = LinearDiscriminantAnalysis(n_components=None)<br/>X_lda = lda.fit(X_scaled, y)<br/><br/>exp_var = lda.explained_variance_ratio_ * 100<br/>cum_exp_var = np.cumsum(exp_var)<br/><br/>plt.bar(range(1, 3), exp_var, align='center',<br/>        label='Individual explained variance')<br/><br/>plt.step(range(1, 3), cum_exp_var, where='mid',<br/>         label='Cumulative explained variance', color='red')<br/><br/>plt.ylabel('Explained variance percentage')<br/>plt.xlabel('Component index')<br/>plt.xticks(ticks=[1, 2])<br/>plt.legend(loc='best')<br/>plt.tight_layout()<br/><br/>plt.savefig("Barplot_LDA.png")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/91bf6d460c4da96e292ee44a4c9b494f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*oysDKJ8ExTjcblBGk_tkDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1daf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前两个组件捕获数据中的所有差异。所以，它们完全代表了原始的葡萄酒数据。</p><h1 id="0cc0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">LDA 模式的优势</h1><p id="a073" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们的 LDA 模型具有以下优点。</p><ul class=""><li id="7b8e" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">减少数据的维度(特征数量)</li><li id="413d" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">在 2D 图中可视化高维数据</li><li id="a0af" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">最大化类别可分性</li></ul><h1 id="ee9b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">SVM 与 LDA</h1><blockquote class="om"><p id="37f8" class="on oo it bd op oq or os ot ou ov lu dk translated">只有 LDA 可以在降低数据维数的同时最大化类别可分性。因此，在运行另一种分类算法(如支持向量机(SVM)或逻辑回归)之前，LDA 是降低维数的理想方法。</p></blockquote><p id="592e" class="pw-post-body-paragraph kz la it lb b lc ow ju le lf ox jx lh li oy lk ll lm oz lo lp lq pa ls lt lu im bi translated">这可以想象如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/eb0f84d2ee2921d2684dfc9f30885410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*nyw64ahoUFtsR6IF6XnBFA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oi">SVM 与 LDA 的机器学习管道</strong>(图片由作者提供)</p></figure><p id="4326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA 模型将高维(13 维)葡萄酒数据(X)作为其输入，并在最大化类别可分性的同时降低数据的维度。作为 X_LDA 的变换数据(2-dim)与类别标签 y 一起用作 SVM 模型的输入。然后，由于葡萄酒数据具有 3 个类别，SVM 通过使用“一对其余”(ovr)策略来执行多类别分类，这将允许算法在考虑所有其他类别的情况下为每个类别绘制决策边界。</p><p id="17fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用“线性”核作为支持向量机算法的核，因为数据似乎可以与线性决策边界(即直线)线性分离。</p><pre class="kj kk kl km gt mt mu mv bn mw mx bi"><span id="812f" class="my lx it mu b be mz na l nb nc">from sklearn.svm import SVC<br/><br/>svc = SVC(kernel='linear', decision_function_shape='ovr')<br/>svc.fit(X_lda, y)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/4a9cd651e3fc4d633f089f5313eed947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*7Gd8-tx-p2JLkbxkEE6KXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oi">使用 LDA 实现最大类别可分性的 SVM</strong>(图片由作者提供)</p></figure><p id="9720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类可以用线性决策边界清楚地分开。只有一个数据点被错误分类。将此与我们之前获得的 PCA 输出进行比较，如果我们绘制线性决策边界，许多点将被错误分类。</p><blockquote class="om"><p id="6133" class="on oo it bd op oq or os ot ou ov lu dk translated">因此，在分类数据集中，LDA 比 PCA 具有更高的降维效率，因为 LDA 在降低数据维数的同时最大化了类的可分性。</p></blockquote><p id="1a89" class="pw-post-body-paragraph kz la it lb b lc ow ju le lf ox jx lh li oy lk ll lm oz lo lp lq pa ls lt lu im bi translated">请注意，绘制 SVM 决策边界(超平面)的代码不包括在上面的代码中，因为它需要对支持向量机如何在幕后工作有透彻的理解，这超出了本文的范围。</p><blockquote class="pd pe pf"><p id="df1e" class="kz la lv lb b lc ld ju le lf lg jx lh pg lj lk ll ph ln lo lp pi lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>术语<strong class="lb iu">‘超平面’</strong>是在考虑高维数据时指代决策边界的正确方式。在二维空间中，超平面只是一条直线。同样，你可以想象高维空间中超平面的形状。</p></blockquote><h1 id="cecf" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">为 LDA 选择正确的组件数量</h1><p id="78ce" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">PCA 和 LDA 算法中最重要的<a class="ae ky" href="https://rukshanpramoditha.medium.com/parameters-vs-hyperparameters-what-is-the-difference-5f40e16e2e82" rel="noopener">超参数</a>是<strong class="lb iu"> n_components </strong>，其中我们指定了 LDA 或 PCA 应该找到的组件数量。</p><p id="0ac3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的文章<a class="ae ky" rel="noopener" target="_blank" href="/how-to-select-the-best-number-of-principal-components-for-the-dataset-287e64b14c6d"> <em class="lv">如何为数据集</em> </a>选择最佳数量的主成分中，已经讨论了为 PCA 选择最佳数量的准则。</p><p id="bc24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的准则也适用于 LDA。</p><ul class=""><li id="d96c" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">如果应用 LDA 的唯一目的是为了数据可视化，您应该保留 2 个(对于 2D 图)或 3 个(对于 3D 图)组件。我们只熟悉 2D 和 3D 情节，无法想象其他高维情节。</li><li id="bf4c" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">正如我前面解释的，累积解释方差图对于选择正确的成分数量非常有用。</li><li id="57fb" class="nd ne it lb b lc np lf nq li nr lm ns lq nt lu ni nj nk nl bi translated">LDA 可以找到的最大组件数等于分类数据集中的类数减一。</li></ul></div><div class="ab cl pj pk hx pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="im in io ip iq"><p id="fc16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="e4b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或反馈，请告诉我。</p><h2 id="4b1c" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">下一篇阅读:主成分分析和降维特辑</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5"><div class="gh gi pq"><img src="../Images/3198dba8b9b942e447f440bd75004466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*QVsDC6QGL-dOCuZxUpJjyQ.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure><h2 id="a31d" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">人工智能课程怎么样？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75"><div class="gh gi pr"><img src="../Images/ee0de8ac72ca2a85da347000430e3df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*U6i7wL7tIy49FI7zuGoE1g.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure><h2 id="ac31" class="nv lx it bd ly nw nx dn mc ny nz dp mg li oa ob mi lm oc od mk lq oe of mm og bi translated">支持我当作家</h2><p id="24fa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑 <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="lv">注册成为会员</em> </strong> </a> <em class="lv">以获得无限制的媒体访问权限。它只需要每月 5 美元，我会收到你的会员费的一部分。</em></p><div class="ps pt gp gr pu pv"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="pw ab fo"><div class="px ab py cl cj pz"><h2 class="bd iu gy z fp qa fr fs qb fu fw is bi translated">通过我的推荐链接加入 Medium</h2><div class="qc l"><h3 class="bd b gy z fp qa fr fs qb fu fw dk translated">阅读 Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qd l"><p class="bd b dl z fp qa fr fs qb fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qe l"><div class="qf l qg qh qi qe qj ks pv"/></div></div></a></div><p id="0315" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p><p id="33cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="qk ql ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----4489eade632--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫蒂塔</a><br/><strong class="lb iu">2022–12–29</strong></p></div></div>    
</body>
</html>