<html>
<head>
<title>Training in PyTorch from Amazon S3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自亚马逊S3的PyTorch培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-in-pytorch-from-amazon-s3-6156d5342d1#2022-05-15">https://towardsdatascience.com/training-in-pytorch-from-amazon-s3-6156d5342d1#2022-05-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5e88" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何最大化数据吞吐量并节省资金</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a830e323efb1a6d916ce9b1d00190034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rG84alRr_8Kex2Ha"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@i_am_g?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Guillaume Jaillet </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="f0b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在以前的帖子中(例如这里的<a class="ae kv" href="https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" rel="noopener">和这里的</a>和<a class="ae kv" rel="noopener" target="_blank" href="/amazon-sagemaker-fast-file-mode-d12829479c39">我们讨论了从</a><a class="ae kv" href="https://aws.amazon.com/s3/" rel="noopener ugc nofollow" target="_blank">亚马逊S3 </a>到TensorFlow培训课程的不同数据流选项。在本帖中，我们重温了S3的培训主题，这次重点是PyTorch培训。</p><p id="3d5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如在我们以前的帖子中，我们处理的场景是我们的数据集如此之大，以至于它:</p><ol class=""><li id="edcc" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">不能全部下载到培训实例上，或者</li><li id="4445" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">下载它会给我们的训练带来很大的延迟。</li></ol><p id="03a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应对这一挑战有不同的方法。一种方法(不在本文讨论范围之内)是建立某种形式的持久存储系统，该系统镜像S3数据，并且一旦我们的训练实例启动，该系统就随时可用和可访问。用亚马逊FSx来实现这一点的一种方法是。虽然这种方法有其优点(如本文<a class="ae kv" href="https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" rel="noopener">所述</a>，但它可能需要大量额外的维护和成本。我们在这篇文章中采用的方法是将数据直接从S3流入我们的训练循环。</p><h2 id="b2ea" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">来自亚马逊S3的数据流</h2><p id="adda" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">虽然将亚马逊S3的数据直接传输到训练循环听起来很简单，但如果设计不好，它可能会成为你训练管道的瓶颈。在这种不希望出现的情况下，当系统等待来自S3的数据到达时，系统的计算资源将处于空闲状态。我们的目标是最大限度地利用系统资源，进而提高训练速度。有许多因素控制来自S3的流数据可能影响训练步骤时间的程度，包括以下因素:</p><ol class=""><li id="ae7e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">实例类型的网络输入带宽的大小。这是您选择的训练<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/?trk=c0203969-28e0-4300-9ee8-097ca8620008&amp;sc_channel=ps&amp;sc_campaign=acquisition&amp;sc_medium=ACQ-P|PS-GO|Brand|Desktop|SU|Compute|EC2|IL|EN|Text&amp;s_kwcid=AL!4422!3!536392682311!b!!g!!%2Baws%20%2Bec2%20%2Binstance&amp;ef_id=Cj0KCQjwl7qSBhD-ARIsACvV1X29d4J4sf0GvyZQBZA4oMisxrGRKI32oDr4U3R-q-3_jMnrmnTaTA0aAo-tEALw_wcB:G:s&amp;s_kwcid=AL!4422!3!536392682311!b!!g!!%2Baws%20%2Bec2%20%2Binstance" rel="noopener ugc nofollow" target="_blank">实例类型</a>的属性。</li><li id="c47a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">训练步骤所需的数据总量(以字节为单位)。您应该通过只对训练所需的数据进行流式处理并考虑不同的压缩技术来努力减小每个数据样本的大小。(但是，您也应该考虑解压缩所需的额外计算。)</li><li id="72ef" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">选择存储数据的文件格式。例如，与需要下载整个文件以便能够打开和解析它们的格式相比，使用诸如<a class="ae kv" href="https://github.com/webdataset/webdataset" rel="noopener ugc nofollow" target="_blank"> WebDataset </a>或<a class="ae kv" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank"> TFRecord </a>格式的顺序文件格式可能会更好。</li><li id="bb59" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">存储数据的单个文件的大小也会影响数据流的性能。例如，将每个数据样本存储在一个单兆字节大小的单独文件中会增加S3的事务开销。我们努力将我们的数据存储在多个文件中，每个文件有几百兆字节的大小，每个文件包含多个数据样本的序列。应该注意的是，以这种方式存储你的数据会带来其他挑战，我们已经在之前的<a class="ae kv" href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" rel="noopener">文章</a>中解决了这些挑战。</li><li id="b214" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">用于执行数据流的工具，将在本文中讨论。</li></ol><p id="1164" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将探索一些从S3训练的方法。请不要把我们提到或没有提到这种或那种方法解释为赞同或拒绝。我们认为熟悉多种方法非常重要。每一种都有自己的优点和缺点，最佳选择可能取决于项目的细节。</p><p id="bea4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着机器学习领域的不断发展，许多支持框架和库也在不断发展。请记住，我们提到的一些API和工具在你阅读这篇文章的时候可能已经过时了。我们强烈建议您掌握最新的可用工具，因为这些工具可能包括大量的增强和优化。</p><p id="73ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我们的重点将是使用PyTorch(版本1.10和1.11)和来自亚马逊S3的培训，但我们所说的许多内容也同样适用于其他培训框架和其他对象存储服务。</p><p id="f09a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别感谢<a class="ae kv" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">伊扎克·李维</a>帮助他创建了这个帖子。</p><h1 id="2fee" class="ne mh iq bd mi nf ng nh ml ni nj nk mo jw nl jx mr jz nm ka mu kc nn kd mx no bi translated">测量吞吐量</h1><p id="66e8" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在接下来的部分中，我们将回顾从S3流式传输数据的不同方法。我们将通过计算<em class="np">训练吞吐量</em>来比较这两种方法，训练吞吐量是通过每秒输入训练循环的数据样本数量来衡量的。为了只关注数据流的性能，我们将测量空训练步骤情况下的吞吐量，如下面的代码块所示。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="3aa0" class="mg mh iq nr b gy nv nw l nx ny">import torch, time<br/>from statistics import mean, variance</span><span id="e900" class="mg mh iq nr b gy nz nw l nx ny">dataset=get_dataset()<br/>dl=torch.utils.data.DataLoader(dataset, batch_size=4, num_workers=4)</span><span id="dc1c" class="mg mh iq nr b gy nz nw l nx ny">stats_lst = []<br/>t0 = time.perf_counter()<br/>for batch_idx, batch in enumerate(dl, start=1):<br/>    if batch_idx % 100 == 0:<br/>        t = time.perf_counter() - t0<br/>        print(f'Iteration {batch_idx} Time {t}')<br/>        stats_lst.append(t)<br/>        t0 = time.perf_counter()</span><span id="661a" class="mg mh iq nr b gy nz nw l nx ny">mean_calc = mean(stats_lst[1:])<br/>var_calc = variance(stats_lst[1:])</span><span id="75f7" class="mg mh iq nr b gy nz nw l nx ny">print(f'mean {mean_calc} variance {var_calc}')</span></pre><p id="04a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重要的是要记住，虽然这种比较测量可能会让我们很好地了解每种方法可以支持的最大吞吐量，但它可能无法很好地预测您选择的方法将如何影响实际的培训吞吐量，原因有两个:</p><ol class=""><li id="f986" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">您选择的方法可能不会影响您的整体训练步骤时间。例如，如果您的培训步骤是计算密集型的，那么从S3提取一个文件需要1秒钟还是10秒钟可能没有区别。</li><li id="ae81" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">典型的训练步骤将包括许多可能影响实际训练量的附加操作。特别是，一些操作可能会争用从S3传输数据的相同资源。</li></ol><p id="913a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们经常使用的另一种测量数据流对整体训练速度的影响的技术是，测量在缓存的数据样本上运行而不是在流数据样本上运行时步长时间如何变化，如下所示。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="e928" class="mg mh iq nr b gy nv nw l nx ny">import torch, time<br/>from statistics import mean, variance</span><span id="b106" class="mg mh iq nr b gy nz nw l nx ny">dataset=get_dataset()<br/>dl=torch.utils.data.DataLoader(dataset, batch_size=4, num_workers=4)</span><span id="f032" class="mg mh iq nr b gy nz nw l nx ny"><strong class="nr ir">batch = next(iter(dl))<br/></strong>t0 = time.perf_counter()<br/>for batch_idx in range(1,1000):<br/><strong class="nr ir">    train_step(batch)<br/></strong>    if batch_idx % 100 == 0:<br/>        t = time.perf_counter() - t0<br/>        print(f'Iteration {batch_idx} Time {t}')<br/>        t0 = time.perf_counter()</span></pre><h2 id="f1ce" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">玩具示例— WebDataset</h2><p id="bbe3" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">出于演示的目的，我们将使用一个由随机图像和图像分割组成的合成数据集，这些图像和图像分割以<a class="ae kv" href="https://github.com/webdataset/webdataset" rel="noopener ugc nofollow" target="_blank"> WebDataset </a>文件格式存储，这是一种基于tar文件的格式，专门设计用于大型数据集的训练。具体来说，我们使用以下代码块生成了多个400兆字节的tar文件:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="812d" class="mg mh iq nr b gy nv nw l nx ny">import webdataset as wds<br/>import numpy as np<br/>from PIL import Image<br/>import io</span><span id="259b" class="mg mh iq nr b gy nz nw l nx ny">out_tar = 'wds.tar'<br/>sink = wds.TarWriter(out_tar)<br/>im_width = 1024<br/>im_height = 1024<br/>num_classes = 256</span><span id="7b83" class="mg mh iq nr b gy nz nw l nx ny">for i in range(100):<br/>    image = Image.fromarray(np.random.randint(0, high=256,<br/>                  size=(im_height,im_width,3), dtype=np.uint8))<br/>    label = Image.fromarray(np.random.randint(0, high=num_classes,<br/>                  size=(im_height,im_width), dtype=np.uint8))<br/>    image_bytes = io.BytesIO()<br/>    label_bytes = io.BytesIO()<br/>    image.save(image_bytes, format='PNG')<br/>    label.save(label_bytes, format='PNG')<br/>    sample = {"__key__": str(i),<br/>              f'image': image_bytes.getvalue(),<br/>              f'label': label_bytes.getvalue()}<br/>    sink.write(sample)</span></pre><h1 id="480e" class="ne mh iq bd mi nf ng nh ml ni nj nk mo jw nl jx mr jz nm ka mu kc nn kd mx no bi translated">来自亚马逊S3的流媒体</h1><p id="6968" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在这一节中，我们将回顾一些工具和技术，从亚马逊S3流数据。这项审查绝非详尽无遗；还有许多其他工具我们没有在这里介绍。我们将使用上面显示的WebDataset示例演示一些选项。我们将解决方案大致分为两种类型，一种是我们将数据从S3系统中明确提取到训练环境中的解决方案，另一种是向应用程序公开文件系统风格的接口的解决方案。</p><h2 id="3bd6" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">文件对象下载</h2><p id="59dc" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">亚马逊S3的许多培训解决方案都涉及到将数据明确下载到本地培训环境中。</p><p id="c0f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用AWS CLI下载对象:<br/> 从S3获取文件的最简单方法之一是使用<a class="ae kv" href="https://aws.amazon.com/cli/" rel="noopener ugc nofollow" target="_blank"> AWS命令行界面</a>工具。以下命令将下载存储在S3的目标文件:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="97f4" class="mg mh iq nr b gy nv nw l nx ny">aws s3 cp s3://&lt;path in s3&gt;/wds0.tar -</span></pre><p id="60f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用本地路径替换连字符将导致文件保存到本地磁盘。有关该工具使用的更多详细信息，请参见此处的<a class="ae kv" href="https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/cp.html" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="20c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">WebDataset库支持从使用AWS S3 cp命令提取的文件中通过管道传输字节流。我们在下面的代码块中演示了如何以这种方式创建PyTorch数据集:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="ae90" class="mg mh iq nr b gy nv nw l nx ny">import io, webdataset<br/>def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]<br/>    # add awscli command to urls<br/>    urls = [f'pipe:aws s3 cp {url} -' for url in urls]<br/>    dataset = (<br/>           webdataset.WebDataset(urls, shardshuffle=True)<br/>            .shuffle(10)<br/>    )<br/>    return dataset</span></pre><p id="5f09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用Boto3下载对象:<br/> </strong> <a class="ae kv" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Boto3 </a>是一个Python库，可以从S3下载对象文件。下面的函数演示了如何将文件数据的内容提取到本地内存的字节流中。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="aa04" class="mg mh iq nr b gy nv nw l nx ny">import boto3, io, webdataset, re<br/>client = boto3.client("s3")</span><span id="e981" class="mg mh iq nr b gy nz nw l nx ny">def get_bytes_io(path):<br/>    byte_io = io.BytesIO()<br/>    _, bucket, key, _ = re.split("s3://(.*?)/(.*)$", path)<br/>    client.download_fileobj(bucket, key, byte_io)<br/>    byte_io.seek(0)<br/>    return byte_io</span></pre><p id="704a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管WebDataset不包含对这种用法的本机支持，但我们可以通过覆盖webdataset中的<a class="ae kv" href="https://github.com/webdataset/webdataset/blob/05a1ea1116781ffe3c3bc257061f2f3e51dfeb0b/webdataset/tariterators.py#L63" rel="noopener ugc nofollow" target="_blank"> url_opener </a>函数来轻松添加它。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="4a06" class="mg mh iq nr b gy nv nw l nx ny">import webdataset<br/>from webdataset.handlers import reraise_exception</span><span id="6363" class="mg mh iq nr b gy nz nw l nx ny">def url_opener(data, handler=reraise_exception, **kw):<br/>    for sample in data:<br/>        url = sample["url"]<br/>        try:<br/>            stream = get_bytes_io(url)<br/>            sample.update(stream=stream)<br/>            yield sample<br/>        except Exception as exn:<br/>            exn.args = exn.args + (url,)<br/>            if handler(exn):<br/>                continue<br/>            else:<br/>                break</span><span id="8c62" class="mg mh iq nr b gy nz nw l nx ny"><strong class="nr ir">webdataset.tariterators.url_opener = url_opener</strong></span><span id="d21e" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]                                     <br/>    dataset = (<br/>           webdataset.WebDataset(urls, shardshuffle=True)<br/>            .shuffle(10)<br/>    )<br/>    return dataset</span></pre><p id="58c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SageMaker管道模式</strong> : <br/> <a class="ae kv" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">亚马逊SageMaker </strong> </a> <strong class="ky ir"> </strong>是AWS提供的一项托管服务，用于大规模执行基于云的机器学习。它提供的众多实用工具包括专用API，用于与存储在亚马逊S3的训练数据进行交互。<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AlgorithmSpecification.html" rel="noopener ugc nofollow" target="_blank"> SageMaker文档</a>详述了支持的不同数据输入模式。在<a class="ae kv" href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" rel="noopener">之前的一篇文章</a>中，我们扩展了SageMaker管道模式的一些属性。管道模式是另一种将数据从S3显式提取和流式传输到本地培训环境的方式。使用管道模式时，通过专用的Linux FIFO管道获取训练数据。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="e7de" class="mg mh iq nr b gy nv nw l nx ny">with open(fifo_path, ‘rb’, buffering=0) as fifo:<br/>    # read and parse data stream to yield samples</span></pre><p id="6846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">管道模式的一个缺点是它需要解析传入的数据流。这意味着它的使用仅限于支持这种解析方式的文件格式。在<a class="ae kv" href="https://julsimon.medium.com/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" rel="noopener">之前的一篇文章</a>中，我们演示了以这种方式构建训练输入管道。</p><h2 id="1f7d" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">S3接入解决方案</h2><p id="5f79" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">虽然应用程序通常被编程为与文件系统一起工作，但亚马逊S3是一个对象存储，而不是文件系统。许多解决方案旨在通过向亚马逊S3公开类似接口的文件系统来弥合这一差距。</p><p id="83d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">S3Fs</strong>:<br/>T5】S3Fs是几个基于<a class="ae kv" href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace" rel="noopener ugc nofollow" target="_blank"> FUSE </a>的Python解决方案之一，用于将S3桶挂载为文件系统。虽然WebDataset不包含对使用S3Fs的本机支持，但我们可以覆盖webdataset.tariterators中的<a class="ae kv" href="https://github.com/webdataset/webdataset/blob/05a1ea1116781ffe3c3bc257061f2f3e51dfeb0b/webdataset/tariterators.py#L63" rel="noopener ugc nofollow" target="_blank"> url_opener </a>函数来使用它。注意，要将S3Fs与PyTorch一起使用，我们需要将多处理启动方法设置为<em class="np">“spawn”</em>。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="3d4a" class="mg mh iq nr b gy nv nw l nx ny"><strong class="nr ir"><em class="np">torch.multiprocessing.set_start_method('spawn')</em></strong></span><span id="35b4" class="mg mh iq nr b gy nz nw l nx ny">import s3fs, webdataset<br/>from webdataset.handlers import reraise_exception</span><span id="6e45" class="mg mh iq nr b gy nz nw l nx ny">fs = s3fs.S3FileSystem()</span><span id="37c0" class="mg mh iq nr b gy nz nw l nx ny">def url_opener(data, handler=reraise_exception, **kw):<br/>    for sample in data:<br/>        url = sample["url"]<br/>        try:<br/>            stream = fs.open(url.replace("s3://", ""), mode='rb')<br/>            sample.update(stream=stream)<br/>            yield sample<br/>        except Exception as exn:<br/>            exn.args = exn.args + (url,)<br/>            if handler(exn):<br/>                continue<br/>            else:<br/>                break</span><span id="cff4" class="mg mh iq nr b gy nz nw l nx ny">webdataset.tariterators.url_opener = url_opener</span><span id="26de" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]                                     <br/>    dataset = (<br/>           webdataset.WebDataset(urls, shardshuffle=True)<br/>            .shuffle(10)<br/>    )<br/>    return dataset</span></pre><p id="f6d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看<a class="ae kv" href="https://joshua-robinson.medium.com/object-storage-via-fuse-filesystems-ea2cc8094e2c" rel="noopener">这篇很酷的帖子</a>了解更多基于FUSE的提取S3数据的方法，包括s3f的替代方法，如<a class="ae kv" href="https://github.com/kahing/goofys" rel="noopener ugc nofollow" target="_blank"> goofys </a>和<a class="ae kv" href="https://rclone.org/commands/rclone_mount/" rel="noopener ugc nofollow" target="_blank"> rclone </a>。</p><div class="oa ob gp gr oc od"><a href="https://joshua-robinson.medium.com/object-storage-via-fuse-filesystems-ea2cc8094e2c" rel="noopener follow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">通过融合文件系统的对象存储</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">云原生应用通常必须与传统应用共存。这些传统应用程序已经过强化，只是…</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">joshua-robinson.medium.com</p></div></div><div class="om l"><div class="on l oo op oq om or kp od"/></div></div></a></div><p id="e6f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">亚马逊S3 PyTorch插件</strong> : <br/>去年<a class="ae kv" href="https://aws.amazon.com/blogs/machine-learning/announcing-the-amazon-s3-plugin-for-pytorch/" rel="noopener ugc nofollow" target="_blank"> AWS宣布</a>发布一个专用库，用于将数据从S3拉入PyTorch培训环境。这个插件的细节，包括使用说明，可以在这个<a class="ae kv" href="https://github.com/aws/amazon-s3-plugin-for-pytorch" rel="noopener ugc nofollow" target="_blank"> github项目</a>中找到。应该注意的是，作者最近宣布弃用这个库，并计划在<a class="ae kv" href="https://pytorch.org/data/beta/index.html" rel="noopener ugc nofollow" target="_blank"> TorchData </a>库中用S3 IO支持来取代它。(下面将详细介绍。)下面的代码块演示了使用S3 PyTorch插件，用我们的toy WebDataset文件创建一个可迭代的PyTorch数据集。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="2af4" class="mg mh iq nr b gy nv nw l nx ny">from awsio.python.lib.io.s3.s3dataset import S3IterableDataset</span><span id="6006" class="mg mh iq nr b gy nz nw l nx ny">class S3_Dataset(torch.utils.data.IterableDataset):<br/>    def __init__(self, urls):<br/>        self._s3_iter_dataset = S3IterableDataset(urls, True)</span><span id="957a" class="mg mh iq nr b gy nz nw l nx ny">    def data_generator(self):<br/>        try:<br/>            while True:<br/>                image_fname, image_fobj = next(self._s3_iter)<br/>                label_fname, label_fobj = next(self._s3_iter)<br/>                yield {<br/>                    'image': image_fobj,<br/>                    'label': label_fobj<br/>                }</span><span id="5ad8" class="mg mh iq nr b gy nz nw l nx ny">        except StopIteration:<br/>            return</span><span id="514f" class="mg mh iq nr b gy nz nw l nx ny">    def __iter__(self):<br/>        self._s3_iter = iter(self._s3_iter_dataset)<br/>        return self.data_generator()</span><span id="7c79" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]                                     <br/>    dataset = S3_Dataset(urls)<br/>    return dataset</span></pre><p id="d006" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SageMaker快速文件模式</strong> : <br/> <a class="ae kv" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">亚马逊SageMaker </strong> </a> <strong class="ky ir"> </strong>提供了一个额外的基于FUSE的解决方案，用于在S3访问文件调用<em class="np">快速文件模式</em> (FFM)。当您对SageMaker作业进行编程以使用快速文件输入模式时，S3路径会挂载到预定义的本地文件路径上。在最近的一篇文章中，我们扩展了这个输入模式选项，演示了它的用法，并讨论了它的优缺点。采用我们的WebDataset来使用FFM非常简单:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="287a" class="mg mh iq nr b gy nv nw l nx ny">import os, webdataset<br/>def get_dataset():<br/>    ffm = os.environ['SM_CHANNEL_TRAINING']<br/>    urls = [os.path.join(ffm, f'{i}.tar') for i in range(num_files)]<br/>    dataset = (<br/>           webdataset.WebDataset(urls, shardshuffle=True)<br/>            .shuffle(10)<br/>    )<br/>    return dataset</span></pre><p id="13c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，在撰写本文时，FFM性能可能取决于文件数量以及预定义S3路径中的分区数量。</p><h2 id="816a" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">结果</h2><p id="97f9" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">下表包括我们在<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/c5/" rel="noopener ugc nofollow" target="_blank"> EC2 c5.xlarge </a>实例上的不同数据集上运行空训练循环时得到的平均步进时间。这些结果是作为您可能获得的比较性能结果类型的示例提供的。我们警告不要从这些结果中得出任何关于您自己项目的结论，因为性能很可能高度依赖于训练模型和数据的细节。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/eb5a8342359159bc0d16f8366c3b978b.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*Fihegg4ZIqlROJJStk4ltQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每100步的平均时间(按作者)</p></figure><h1 id="c0ea" class="ne mh iq bd mi nf ng nh ml ni nj nk mo jw nl jx mr jz nm ka mu kc nn kd mx no bi translated">使用TorchData管道进行流式传输</h1><p id="ec29" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">TorchData是一个令人兴奋的新PyTorch库，用于创建PyTorch数据集。它目前作为beta产品发布，需要PyTorch版(或更高版本)。官方发布预计在未来几个月。<a class="ae kv" href="https://github.com/pytorch/data" rel="noopener ugc nofollow" target="_blank"> TorchData gihub项目页面</a>包括关于库设计及其API文档和示例的信息。TorchData包含许多用于创建数据管道的构建块模块，包括用于加载以WebDataset格式存储的数据集的模块和用于从S3提取数据的模块。</p><p id="d391" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的部分中，我们将展示TorchData库支持的一些解决方案。请记住，在正式发布库之前，有些API可能会进行修改。</p><p id="fb47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用IoPathFileOpener</strong>:<br/><a class="ae kv" href="https://pytorch.org/data/main/generated/torchdata.datapipes.iter.IoPathFileOpener.html" rel="noopener ugc nofollow" target="_blank">IoPathFileOpener</a>支持直接从云存储中加载文件。它依赖于<a class="ae kv" href="https://pypi.org/project/iopath/" rel="noopener ugc nofollow" target="_blank"> iopath </a> I/O抽象库。下面的代码块演示了它的用法:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="4f1b" class="mg mh iq nr b gy nv nw l nx ny">from torchdata.datapipes.iter import IoPathFileOpener, IterableWrapper</span><span id="a88f" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]<br/>    urls = IterableWrapper(urls).shuffle().cycle()<br/><strong class="nr ir">    tars = IoPathFileOpener(urls, mode="rb").load_from_tar()<br/></strong>    samples = tars.groupby(lambda x:     <br/>                               os.path.basename(x[0]).split(".")[0],<br/>                           group_size=2, guaranteed_group_size=2)<br/>    dataset = samples.map(lambda x: <br/>                  {'image': x[0][1].read(),<br/>                   'label': x[0][1].read()})<br/>    dataset = dataset.shuffle(buffer_size=10)</span><span id="18e1" class="mg mh iq nr b gy nz nw l nx ny">return dataset</span></pre><p id="f8eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用FSSpecFileOpener</strong>:<br/><a class="ae kv" href="https://pytorch.org/data/0.3.0/generated/torchdata.datapipes.iter.FSSpecFileOpener.html#torchdata.datapipes.iter.FSSpecFileOpener" rel="noopener ugc nofollow" target="_blank">FSSpecFileOpener</a>支持类似的功能，这次基于<a class="ae kv" href="https://s3fs.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> S3Fs </a>库。下面的代码块演示了它的用法:</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="d369" class="mg mh iq nr b gy nv nw l nx ny">from torchdata.datapipes.iter import FSSpecFileOpener, IterableWrapper</span><span id="1827" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]<br/>    urls = IterableWrapper(urls).shuffle().cycle()<br/><strong class="nr ir">    tars = FSSpecFileOpener(urls, mode="rb").load_from_tar()<br/></strong>    samples = tars.groupby(lambda x:     <br/>                               os.path.basename(x[0]).split(".")[0],<br/>                           group_size=2, guaranteed_group_size=2)<br/>    dataset = samples.map(lambda x: <br/>                  {'image': x[0][1].read(),<br/>                   'label': x[0][1].read()})<br/>    dataset = dataset.shuffle(buffer_size=10)<br/>return dataset</span></pre><p id="630b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用SageMaker FFM </strong> : <br/>在使用Amazon SageMaker进行培训时，我们也可以通过使用FFM并指向本地文件挂载来使用标准的<a class="ae kv" href="https://pytorch.org/data/0.3.0/generated/torchdata.datapipes.iter.FileOpener.html#torchdata.datapipes.iter.FileOpener" rel="noopener ugc nofollow" target="_blank"> FileOpener </a>类。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="a3e6" class="mg mh iq nr b gy nv nw l nx ny">import os<br/>from torchdata.datapipes.iter import FileOpener, IterableWrapper</span><span id="c63e" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    ffm = os.environ['SM_CHANNEL_TRAINING']<br/>    urls = [os.path.join(ffm, f'{i}.tar') for i in range(num_files)]       <br/>    <strong class="nr ir">tars = FileOpener(urls, mode="rb").load_from_tar()<br/></strong>    samples = tars.groupby(lambda x:     <br/>                               os.path.basename(x[0]).split(".")[0],<br/>                           group_size=2, guaranteed_group_size=2)<br/>    dataset = samples.map(lambda x: <br/>                  {'image': x[0][1].read(),<br/>                   'label': x[0][1].read()})<br/>    dataset = dataset.shuffle(buffer_size=10)<br/>return dataset</span></pre><p id="9c92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">使用S3FileLoader </strong> : <br/>这是亚马逊S3 PyTorch插件的翻版，<a class="ae kv" href="https://github.com/pytorch/data/tree/main/torchdata/datapipes/iter/load#readme" rel="noopener ugc nofollow" target="_blank"> S3FileLoader </a>是AWS专门创建的用于从S3加载数据的数据管道。在撰写本文时，它还没有包含在默认的torchdata包中，需要一些安装步骤，如这里的<a class="ae kv" href="https://github.com/pytorch/data/tree/main/torchdata/datapipes/iter/load#readme" rel="noopener ugc nofollow" target="_blank">所述</a>。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="d91e" class="mg mh iq nr b gy nv nw l nx ny">from torchdata.datapipes.iter import S3FileLoader, IterableWrapper</span><span id="4885" class="mg mh iq nr b gy nz nw l nx ny">def get_dataset():<br/>    urls = [f's3://&lt;path in s3&gt;/{i}.tar' for i in range(num_files)]<br/>    urls = IterableWrapper(urls).shuffle().cycle()<br/><strong class="nr ir">    tars = S3FileLoader(urls).load_from_tar()<br/></strong>    samples = tars.groupby(lambda x:     <br/>                               os.path.basename(x[0]).split(".")[0],<br/>                           group_size=2, guaranteed_group_size=2)<br/>    dataset = samples.map(lambda x: <br/>                  {'image': x[0][1].read(),<br/>                   'label': x[0][1].read()})<br/>    dataset = dataset.shuffle(buffer_size=10)<br/>return dataset</span></pre><h2 id="a673" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">结果</h2><p id="6e46" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">同样，当在使用新的TorchData <em class="np"> </em>库创建的不同数据集上和在<a class="ae kv" href="https://aws.amazon.com/ec2/instance-types/c5/" rel="noopener ugc nofollow" target="_blank"> EC2 c5.xlarge </a>实例上运行相同的空训练循环时，我们共享平均步骤时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/c77a2983e1906445ed554db7e4ad37e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*xBWYrbcn0EkPMPBRyJAeUA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每100步的平均时间(按作者)</p></figure><p id="c48b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我们仍然对从这些结果中得出任何有意义的结论持谨慎态度，但似乎新的TorchData不仅在功能上，而且在从S3传输的速度上提供了升级。</p><h1 id="2c4f" class="ne mh iq bd mi nf ng nh ml ni nj nk mo jw nl jx mr jz nm ka mu kc nn kd mx no bi translated">摘要</h1><p id="2a55" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在这篇文章中，我们回顾了从亚马逊S3到训练环境的数据流的几种选择。这份清单并不全面；还有许多额外的工具和技术。</p><p id="b9b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们认为让自己熟悉几种技术是至关重要的，因为你可能会发现你的首选要么与你的项目不相关，要么它的性能不令人满意。</p><p id="b1c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果有任何意见、更正或问题，请随时联系我。</p></div></div>    
</body>
</html>