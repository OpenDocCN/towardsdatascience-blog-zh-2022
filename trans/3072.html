<html>
<head>
<title>Block-Recurrent Transformer: LSTM and Transformer Combined</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">块循环变压器:LSTM和变压器相结合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/block-recurrent-transformer-lstm-and-transformer-combined-ec3e64af971a#2022-07-06">https://towardsdatascience.com/block-recurrent-transformer-lstm-and-transformer-combined-ec3e64af971a#2022-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f56" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个结合了两者优点的强大模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1f517ae0f9c90be6d412b49a20aa702b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*snuucUk-Sw26LPno"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">桑德罗·卡塔琳娜在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f766" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">普通的<em class="lv">变压器</em>不再是处理深度学习中任何情况的全能模型。</strong></p><p id="bfa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/deep-learning-no-lstms-are-not-dead-20217553b87a">之前的一篇文章</a>中，我们证明了对于时间序列预测任务来说，<em class="lv">变形金刚</em>举步维艰。这就是为什么谷歌创建了一个<a class="ae ky" rel="noopener" target="_blank" href="/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621"> <strong class="lb iu">混合变压器-LSTM </strong> </a>模型，在时间序列预测任务中实现SOTA结果。</p><p id="fe25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">炒作结束后，研究人员开始关注<em class="lv">变形金刚</em>的缺点。新的研究旨在利用其他型号的功能(<em class="lv">CNN</em>、<em class="lv">rnn</em>、<em class="lv"> RL型号</em>)来增强<em class="lv">变形金刚</em>。一个典型的例子是新一代的<strong class="lb iu">视觉变形金刚【1】</strong>，他们借鉴了CNN的创意。</p><p id="77bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2022年3月，一个<strong class="lb iu">谷歌研究团队</strong>和<strong class="lb iu">瑞士人工智能实验室IDSIA </strong>提出了一种新的架构，称为<strong class="lb iu">Block-Recurrent Transformer[2]。</strong></p><p id="c30d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，<strong class="lb iu">块轮回变压器是什么？</strong>它是一种新颖的<em class="lv"> Transformer </em>模型，利用<em class="lv"> LSTMs </em>的递归机制，在长范围序列的语言建模任务中实现显著的困惑改进。</p><p id="13c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但首先，让我们简单讨论一下<em class="lv">变形金刚</em>相比<em class="lv"> LSTMS </em>的优缺点。这将帮助你理解是什么激发了研究人员提出<strong class="lb iu">块循环变压器。</strong></p><h1 id="6186" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">变压器vs循环网络</strong></h1><p id="7eb5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">变压器</em>最显著的优点总结如下:</p><h2 id="04c1" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">平行</h2><p id="02d4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv"> RNNs </em>实现<strong class="lb iu">顺序</strong>处理:对输入(比如说句子)进行逐字处理。</p><p id="95a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">变形金刚</em>使用<strong class="lb iu">非顺序</strong>处理:句子是作为一个整体来处理的，而不是逐字逐句。</p><p id="6550" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图1<strong class="lb iu">和图2</strong>中更好地说明了这种比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/9783fc6c1654bd466bd5459ffde5e735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*mGux6SdqnjhKsxMORmIcSA.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图1: </strong>序列长度=4的LSTM单元。(作者制作)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/c3a5d609c46d12cba9fdf8d34a517b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8GZtMjvCsiKhf5sgQPHAag.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图2: </strong> Bert架构(简化-作者制作)</p></figure><p id="e881" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> LSTM </em>需要8个时间步来处理句子，而<strong class="lb iu"> <em class="lv">伯特【3】</em></strong>只需要2个！</p><p id="e59d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，<em class="lv"> BERT </em>能够更好地利用现代GPU加速所提供的并行性。</p><p id="b3ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，两个图都是简化的:我们假设<em class="lv">的批量</em>为1。此外，我们也没有为<em class="lv"> BERT的</em>特殊记号而烦恼，事实上它需要两个句子，等等。</p><h2 id="9ba2" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">长期记忆</h2><p id="53e8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">rnn</em>在移动到未来令牌之前，被迫将它们学习到的输入序列的表示压缩到单个状态向量中。</p><p id="0a08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，虽然<em class="lv">lstm</em>解决了普通<em class="lv">rnn</em>遭受的<strong class="lb iu">消失渐变</strong>问题，但它们仍然容易出现<strong class="lb iu">爆炸渐变</strong>。因此，他们正在努力解决更长的依赖关系。</p><p id="e488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">另一方面，</em>变压器的带宽要高得多。例如，在<strong class="lb iu">编码器-解码器变换器【4】</strong>模型中，<em class="lv">解码器</em>可以直接处理输入序列中的每个令牌，包括已经解码的令牌。这在<strong class="lb iu">图3中描述:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/7dafae33cee165ad31365a81efe839c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*icPxy6-Z6gvzl9RQkI9fCg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图3: </strong>普通变压器中的编码和解码(<a class="ae ky" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h2 id="8b31" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">更好的注意力机制</h2><p id="1065" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><strong class="lb iu">注意力【4】</strong>的概念对变形金刚来说并不陌生。早在2016年<strong class="lb iu">谷歌神经引擎【5】</strong>(编码器-解码器拓扑中的堆叠Bi-lstm)已经在使用<em class="lv">注意力</em>。</p><p id="4340" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下<em class="lv">变形金刚</em>使用了一个叫做<strong class="lb iu"> <em class="lv">的特例自我关注:</em> </strong>这个机制允许输入中的每个单词引用输入中的每一个其他单词。</p><p id="461d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">变形金刚</em>可以使用大<em class="lv">注意</em>窗口(如<code class="fe ni nj nk nl b">512</code>、<code class="fe ni nj nk nl b">1048</code>)。因此，它们在长范围内捕获序列数据中的上下文信息时非常有效。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="b314" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们来看看<em class="lv">变压器</em>T59】的缺点:</p><h2 id="bfce" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated"><strong class="ak">自我关注的O(n)成本</strong></h2><p id="4dfd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">变形金刚</em>最大的一期。</p><p id="ccdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两个主要原因:</p><ul class=""><li id="e7d0" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">最初的<em class="lv">伯特</em>模型有一个<code class="fe ni nj nk nl b">512</code>令牌的限制。解决这个问题的简单方法是截断输入的句子。</li><li id="9ee5" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">或者，我们可以创建超过该限制的<em class="lv">变形金刚</em>模型，使其达到<code class="fe ni nj nk nl b">4096</code>代币。然而，<em class="lv">自我关注</em>的成本相对于句子长度是二次的。</li></ul><p id="7cc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，可伸缩性变得非常具有挑战性。已经提出了许多想法来重组最初的<em class="lv">自我关注</em>机制:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/e47b1d8ce1686cd4934fb1d1d4d26ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*oyTiA0wH6jcltZEuRXqhXQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图4: </strong>不同类型自我关注的成本矩阵(<a class="ae ky" href="http://Long" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="0614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些想法大多是由新一代型号引入的，如<strong class="lb iu"><em class="lv">【6】</em></strong>和<strong class="lb iu"><em class="lv">Transformer XL【7】</em></strong>。这些模型针对长格式文本进行了优化，并实现了显著的改进。</p><p id="ad41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，挑战依然存在:我们能否在不牺牲效率的情况下进一步降低计算成本？</p><h2 id="687d" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">时间序列具有挑战性</h2><p id="6282" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然<em class="lv">变形金刚</em>已经统治了<strong class="lb iu"> NLP领域</strong>，但是它们在时态数据方面的成功有限。但是为什么呢？时间序列不也是顺序数据吗？</p><ul class=""><li id="1cbc" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated"><em class="lv">变压器</em>可以更好的从长期历史计算出一个时间步长的输出，而不是当前的输入和隐藏状态。这对于局部时间依赖性来说效率较低。</li><li id="4a81" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">因此，短期记忆对于时间序列的长期记忆同样重要。</li><li id="3fb9" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">这就是为什么谷歌研究人员推出了一个用于时间序列预测的<strong class="lb iu">混合深度学习模型【1】</strong>:该模型使用<em class="lv">注意力</em>，但也包括一个<em class="lv"> LSTM编码器-解码器堆栈</em>，它在捕捉本地时间依赖性方面发挥着重要作用。</li><li id="79cc" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">最后，时间序列可以是<strong class="lb iu">多元</strong>，有s <strong class="lb iu">静态数据，</strong>等等。它们通常需要更特殊的处理。</li></ul><p id="3090" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们不会关注时间序列方面。有关时间序列深度学习模型的更多信息，请随时查看这篇<a class="ae ky" rel="noopener" target="_blank" href="/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0">文章</a>。</p><h1 id="842c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">输入块-循环变压器</h1><p id="6ece" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">什么是<strong class="lb iu"> <em class="lv">块轮回变压器？</em></strong><strong class="lb iu"><em class="lv">分块递归变压器</em> </strong>是一种革新NLP领域的新型模型。</p><p id="8206" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型的主要突破是<strong class="lb iu">循环单元:</strong>一个以循环方式工作的<strong class="lb iu"> </strong>修改的<em class="lv">变压器</em>层。</p><p id="5620" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们快速概述一下主要特征，然后我们将更深入地研究模型的架构。</p><ul class=""><li id="9fac" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated"><strong class="lb iu">块级并行:</strong><em class="lv">递归单元</em>处理<strong class="lb iu">块</strong>中的令牌，一个块内的所有令牌并行处理。</li><li id="1477" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">大关注窗口:</strong>由于模型将输入分解成块，因此它可以使用大关注窗口(测试了多达<code class="fe ni nj nk nl b">4096</code>个标记)。因此，<em class="lv">闭塞循环变压器</em>属于远程变压器家族(类似于<em class="lv">长变压器</em>)。</li><li id="747a" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">线性复杂度:</strong>由于<em class="lv">递归单元</em>分块分解输入，模型在<strong class="lb iu"><em class="lv">【O(n)</em></strong>时间内使用<strong class="lb iu"> <em class="lv">滑动自关注</em> </strong> <em class="lv">逐块计算自关注。</em></li><li id="097c" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">更稳定的训练:</strong>分块处理序列可用于长距离传播信息和梯度，而不会在训练期间导致灾难性的遗忘问题。</li><li id="da87" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">信息扩散:</strong><em class="lv">块递归变换器</em>对状态向量块而不是单个向量进行操作(像<em class="lv"> RNNs </em> do)。因此，模型可以充分利用r <em class="lv"> ecurrence </em>并更好地捕捉过去的信息。</li><li id="a400" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">互操作性:</strong><em class="lv">递归单元</em>可与常规变压器层连接。</li><li id="ba29" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">模块化:</strong><em class="lv">循环单元</em>可以水平或垂直堆叠，因为<em class="lv">循环单元</em>可以在两种模式下操作:<strong class="lb iu">水平</strong>(用于循环)和<strong class="lb iu">垂直</strong>(用于堆叠层)。这将在下一节中变得清楚。</li><li id="9aa6" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">运营成本:</strong>增加<em class="lv">递归</em>就像多加了一层<em class="lv">变压器</em>。没有引入额外的参数。</li><li id="993b" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">效率:</strong>与其他<em class="lv">远程变压器</em>相比，该型号表现出显著的改进。</li></ul><p id="92ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面两节将详细描述<em class="lv">块-递归变压器的两个主要组成部分:</em><strong class="lb iu"><em class="lv">递归单元</em> </strong>架构和<strong class="lb iu"> <em class="lv"> </em>带递归的滑动自注意。</strong></p><h1 id="888c" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">递归细胞架构</strong></h1><p id="9b24" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">块循环变压器</em>的支柱是<em class="lv">循环单元</em>。</p><blockquote class="oi oj ok"><p id="9f37" class="kz la lv lb b lc ld ju le lf lg jx lh ol lj lk ll om ln lo lp on lr ls lt lu im bi translated">注意:不要被它的“细胞”特征所迷惑。这是一个完全成熟的变压器层，旨在以循环方式运行。</p></blockquote><p id="8e67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">循环单元</em>接收以下类型的输入:</p><ul class=""><li id="b545" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">一组<code class="fe ni nj nk nl b">W</code> <strong class="lb iu">令牌嵌入</strong>，其中<code class="fe ni nj nk nl b">W</code>为<em class="lv">块大小</em>。</li><li id="ba71" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">一组“<strong class="lb iu">当前状态”</strong>向量，称为<code class="fe ni nj nk nl b">S</code>。</li></ul><p id="86ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是:</p><ul class=""><li id="449a" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">一组<code class="fe ni nj nk nl b">W</code>输出令牌嵌入。</li><li id="6639" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">一组“<strong class="lb iu">下一状态</strong>”向量。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/252f6ea04a4a182c5516076c49f3cf24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ET37zXabtTECaCx2LofqpQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图5: </strong>复发细胞<strong class="bd ng">。左:</strong>垂直模式(堆叠)，<strong class="bd ng">右:</strong>水平模式(重复)</p></figure><p id="7038" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">图5 </strong>展示了<strong class="lb iu"> </strong> <em class="lv">轮回单元</em>架构<strong class="lb iu">。这个架构非常简单，并且重用了大部分现有的<em class="lv"> Transformer </em>代码库！</strong></p><p id="4502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将逐步解释图5 中显示的每个组件:</p><h2 id="b0ae" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">自我注意和交叉注意</h2><p id="3259" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">块循环变压器</em>支持两种操作:<strong class="lb iu">自关注</strong>和<strong class="lb iu">交叉关注。</strong>更确切地说:</p><ul class=""><li id="59e1" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated"><strong class="lb iu">自关注</strong>是对同一嵌入生成的<em class="lv">键</em>、<em class="lv">值、</em>和<em class="lv">查询</em>(分别为<code class="fe ni nj nk nl b">K</code>、<code class="fe ni nj nk nl b"> V</code>和<code class="fe ni nj nk nl b">Q</code>矩阵)进行的。</li><li id="134c" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">对一次嵌入产生的<em class="lv">查询</em>和另一次嵌入产生的<em class="lv">键值</em>和<em class="lv">值</em>进行交叉关注</strong>。</li></ul><p id="4391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你还记得最初的<em class="lv">变压器编码器-解码器</em>模型【4】，那么<em class="lv">编码器</em>正在执行s <em class="lv">自我关注</em>，而<em class="lv">解码器</em>中的“<strong class="lb iu">编码器-解码器关注</strong>层正在执行c <em class="lv">罗斯关注</em>。那是因为<em class="lv">查询</em>来自前面的<em class="lv">解码器</em>层，而<em class="lv">键</em>和<em class="lv">值</em>来自<em class="lv">编码器</em>输出。<em class="lv">循环单元</em>在同一层执行两种操作。换句话说:</p><blockquote class="op"><p id="f7d9" class="oq or it bd os ot ou ov ow ox oy lu dk translated"><em class="oz">轮回细胞</em>并行做自我注意(编码)和交叉注意(解码)！</p></blockquote><h2 id="3641" class="mt lx it bd ly mu pa dn mc mw pb dp mg li pc mz mi lm pd nb mk lq pe nd mm ne bi translated">水平与垂直模式</h2><p id="3433" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">接下来，我们将重点介绍<em class="lv"/><em class="lv">所示的</em> <strong class="lb iu"> <em class="lv">图5。</em> </strong>像我前面说的，<em class="lv">轮回细胞</em>以两种模式运行:</p><ul class=""><li id="ade4" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated"><strong class="lb iu">垂直(堆叠):</strong>在这种模式下，模型对输入嵌入执行<em class="lv">自我关注</em>，对递归状态执行c <em class="lv">交叉关注</em>。</li><li id="f687" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated"><strong class="lb iu">水平(递归):</strong>这正好相反:模型对递归状态进行<em class="lv">自关注</em>，对输入嵌入进行<em class="lv">交叉关注</em>。</li></ul><h2 id="f4ed" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">位置偏差</h2><p id="c628" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">您还会注意到图5 中的<strong class="lb iu">方框，称为<em class="lv">学习状态id</em>。让我们解释一下这是什么，为什么我们需要它。</strong></p><p id="df64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至此，很明显<em class="lv">递归细胞</em>之间传递的递归状态不是单个向量(像<em class="lv"> RNNs </em>，而是大量的<strong class="lb iu">状态向量</strong>。</p><p id="4c7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为相同的<em class="lv"> MLP </em>层被应用于每个状态向量(标准实践)，实验分析表明<em class="lv">状态向量</em>不能区分。经过几次训练后，它们趋于一致。</p><p id="4a5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了防止这个问题，作者向状态向量添加了一组额外的可学习的“<strong class="lb iu">状态id</strong>”。作者称这种功能为位置偏差。这类似于<strong class="lb iu">位置编码</strong>，普通<em class="lv">转换器</em>应用于输入嵌入。<em class="lv">块递归转换器</em>的作者将这种技术应用于递归状态向量，这就是为什么他们使用不同的名称来避免混淆。</p><h2 id="9ed9" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">位置编码</h2><p id="dc8d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">块递归转换器</em>没有将传统的<em class="lv">位置编码</em>应用于输入标记，因为它们对长序列不太适用。相反，作者使用了在<strong class="lb iu"> T5架构【8】</strong>中引入的一个著名技巧:他们将<em class="lv">位置相对偏差向量</em>添加到<em class="lv">自我关注</em>矩阵，该矩阵源自<strong class="lb iu">垂直模式</strong>中的输入嵌入。偏差向量是<em class="lv">键</em>和<em class="lv">查询</em>之间相对距离的学习函数。</p><h2 id="0213" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated"><strong class="ak">闸门配置</strong></h2><p id="1606" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">块循环变压器</em>与其他<em class="lv">变压器</em>型号的另一个区别是<em class="lv">剩余连接</em>的使用。</p><p id="6864" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">块循环变压器</em>的作者尝试了以下配置:</p><ol class=""><li id="fa58" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu pf nz oa ob bi translated">用<em class="lv">闸门</em>更换<em class="lv">剩余连接</em>。(该配置如图<strong class="lb iu">图5 </strong>所示)。</li><li id="3cfb" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">在<strong class="lb iu">固定浇口</strong>和<strong class="lb iu">T71<strong class="lb iu">LSTM浇口之间选择。</strong></strong></li></ol><p id="f5b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者做了几个实验来寻找最佳配置。更多细节，查看原文。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="0dcc" class="lw lx it bd ly lz pg mb mc md ph mf mg jz pi ka mi kc pj kd mk kf pk kg mm mn bi translated">递归滑动自我注意</h1><p id="1fa8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">块循环变压器</em>的<em class="lv">自关注</em>是一项革命性的功能，结合了以下概念:</p><ol class=""><li id="abd9" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu pf nz oa ob bi translated">矩阵乘积<code class="fe ni nj nk nl b">QK^TV</code>变为“线性化”。</li><li id="a793" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">将<strong class="lb iu"> O(n ) </strong> <em class="lv">全注意力</em>替换为<strong class="lb iu"> O(n) </strong> <em class="lv">滑动注意力</em>。</li><li id="2251" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">添加<strong class="lb iu"> <em class="lv">重现</em> </strong>。</li></ol><p id="cc76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前两个概念已在相关工作[6]、[9]中提出。多亏了他们，<em class="lv">注意力</em>实现了线性成本<strong class="lb iu">但在很长的文档中失去了潜力</strong>。<em class="lv">块递归变压器</em>结合了前两个概念和递归，这是从<em class="lv"> RNNs </em>借用的概念。</p><blockquote class="op"><p id="8d8a" class="oq or it bd os ot ou ov ow ox oy lu dk translated">递归机制优雅地集成在Transformer层中，并在很长的句子中提供显著改善的结果。</p></blockquote><p id="4543" class="pw-post-body-paragraph kz la it lb b lc pl ju le lf pm jx lh li pn lk ll lm po lo lp lq pp ls lt lu im bi translated">我们将分别分析每个概念，以便更好地理解<em class="lv">块循环变压器</em>如何使用<em class="lv">注意力</em>。</p><h2 id="f529" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">线性矩阵乘积</h2><p id="94bb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在变压器生态系统中，<em class="lv">注意力</em>围绕着3个矩阵:查询<em class="lv"/><code class="fe ni nj nk nl b">Q</code>、按键<em class="lv"/><code class="fe ni nj nk nl b">K</code>和<em class="lv">值</em> <code class="fe ni nj nk nl b">V</code>。</p><p id="394d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提醒一下，普通<em class="lv">注意事项</em>由以下人员给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/72ca1ba931407ee2cd8be1360f803eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*3yo5-R3NKefG_m4_qTY15g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">注意在香草变压器的一个头</p></figure><p id="2703" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">块循环变压器计算</em>注意<em class="lv">分数</em>稍有不同:首先，移除<em class="lv"> softmax </em>操作。根据[9]，剩余的项重新排列为<code class="fe ni nj nk nl b">Q(K^TV)</code>(如图<strong class="lb iu">图5 </strong>所示)，并以线性化的方式进行计算。</p><h2 id="9600" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">滑动自我注意</h2><p id="c1cc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">给定一长串<code class="fe ni nj nk nl b">N</code>记号，一个<strong class="lb iu">滑动窗口</strong>应用一个因果掩码，使得每个记号只关注它自己和前面的<code class="fe ni nj nk nl b">W</code>记号。(记住<code class="fe ni nj nk nl b">W</code>是块大小)。</p><p id="8dc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们想象一下注意力矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/7a077d3c4d939dce14fec2a6a941765f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2TDZS0imjgi9By_nRTtUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ng">图6: </strong>单个训练步骤的块递归变压器的优化注意矩阵。不是计算整个矩阵，而是只计算2个黑色方块内的分数。(<a class="ae ky" href="https://arxiv.org/pdf/2203.07852.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="e5d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图6的<strong class="lb iu">中，</strong>我们有一个窗口大小<code class="fe ni nj nk nl b">W</code> =8，序列长度<code class="fe ni nj nk nl b">N</code> =16。在前一个训练步骤中，计算并缓存了第一个<code class="fe ni nj nk nl b">W</code>阴影标记。剩余的<code class="fe ni nj nk nl b">N</code>无阴影标记来自当前输入。</p><p id="577e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入序列中的每个记号以滑动的方式连续关注前面的<code class="fe ni nj nk nl b">W</code> =8个记号。因此，在每一行中，我们有<code class="fe ni nj nk nl b">W</code>个计算。矩阵的高度是<code class="fe ni nj nk nl b">N</code>(我们句子中的记号数)。因此，总成本是<strong class="lb iu"> O(N*W) </strong>而不是全成本矩阵<strong class="lb iu"> O(N*(W+N)) </strong>。换句话说，相对于序列<code class="fe ni nj nk nl b">N</code>的成本是线性的，而不是二次的！</p><p id="30fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在我们的示例中，我们关注了两个大小为<code class="fe ni nj nk nl b">Wx2W</code>的图块。让我们来分析一下这一连串的事件:</p><ol class=""><li id="d506" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu pf nz oa ob bi translated">在第一关注步骤中，输入句子的第一个<code class="fe ni nj nk nl b">W</code>标记将关注来自前一个句子的最后缓存的<code class="fe ni nj nk nl b">W</code> <code class="fe ni nj nk nl b">keys</code>和<code class="fe ni nj nk nl b">values</code>。</li><li id="cb49" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">在第二个注意步骤中，我们输入句子的最后一个<code class="fe ni nj nk nl b">W</code>标记将注意我们输入句子的第一个<code class="fe ni nj nk nl b">W</code>标记。</li><li id="d239" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">这结束了我们的训练步骤，输入句子的最后<code class="fe ni nj nk nl b">W</code> <code class="fe ni nj nk nl b">keys</code>和<code class="fe ni nj nk nl b">values</code>被缓存以用于下一个训练步骤。</li><li id="da22" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">到目前为止，您应该已经注意到了滑动模式。这就是为什么我们称这种机制为<strong class="lb iu">滑动自我注意。</strong></li></ol><p id="6752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>当我说<code class="fe ni nj nk nl b">token X</code>照顾<code class="fe ni nj nk nl b"> token Y</code>时，我们不是指代币本身:我是指那些代币各自的<code class="fe ni nj nk nl b">keys</code>、<code class="fe ni nj nk nl b">values</code>、<code class="fe ni nj nk nl b">query</code>分数！</p><h2 id="e342" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">复发有什么帮助</h2><p id="4a8a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">正如我之前所说的，<em class="lv">滑动自我关注</em>(非循环版本)已经被早期的模型使用[6][7]，尽管有一些不同:</p><ul class=""><li id="0ab7" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">在最初的版本中，输入的句子没有被分成块。使用简单的<em class="lv">滑动自关注</em> n的模型一次接收所有输入。这限制了他们有效处理的信息量。</li><li id="2448" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">在之前的训练步骤中使用的缓存的<em class="lv">键</em>和<em class="lv">值</em>是<strong class="lb iu">不可微的</strong>——这意味着它们在反向传播期间不会更新。然而，在递归版本中，滑动窗口具有额外的优势，因为它可以在多个块上反向传播梯度。</li><li id="840b" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">原<em class="lv">滑动自关注</em>模型在其最顶层有一个理论感受野<code class="fe ni nj nk nl b">W*L</code>，其中<code class="fe ni nj nk nl b">L</code>代表模型层数。在循环版本中，感受野实际上是无限的！这就是为什么<em class="lv">块递归变压器</em>擅长远程内容。</li></ul><h1 id="8210" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实验结果</h1><p id="88c7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">最后，对<em class="lv">块循环变压器</em>进行测试。</p><h2 id="4d9a" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">实验过程</h2><p id="3a4a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">任务是<strong class="lb iu">自动回归语言建模</strong>，目标是预测给定句子的下一个单词。</p><p id="05ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型在3个数据集上进行测试:<strong class="lb iu"> PG19 </strong>、<strong class="lb iu"> arXiv、</strong>和<strong class="lb iu"> Github </strong>。它们都包含很长的句子。</p><p id="9565" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者测试了<em class="lv">块循环变压器</em>，并使用<em class="lv">变压器XL </em>作为基线。<em class="lv">块循环变压器</em>配置为两种模式:</p><ol class=""><li id="5353" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu pf nz oa ob bi translated"><strong class="lb iu">单循环模式:</strong>作者使用了12层<em class="lv">变压器</em>，仅在第10层循环。</li><li id="2616" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated"><strong class="lb iu">反馈模式:</strong>使用相同的模型，除了这一次第10层不仅仅将输出循环到自身:当处理下一个块时，第10层的输出被广播到所有其它层。因此，第1-9层可以交叉处理该输入，使模型更强大，但计算成本更高。</li></ol><h2 id="8cdd" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">估价</h2><p id="b43f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">使用<strong class="lb iu"><em class="lv"/></strong>——语言模型的一种常见度量标准，对模型进行评估。</p><p id="3a07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于那些不知道的人，困惑被定义为<code class="fe ni nj nk nl b">P=2^L</code>，其中<code class="fe ni nj nk nl b">L</code>是常规熵。</p><p id="a7e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直观地说，在语言建模的背景下，你可以这样来思考困惑:如果困惑的值是<code class="fe ni nj nk nl b">30</code>，那么预测句子中的下一个单词就像正确猜测30面骰子的结果一样不确定。困惑度越低越好。</p><h2 id="6516" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">结果</h2><p id="61c0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">总的来说，<em class="lv">方块重现变形金刚</em>在困惑和速度方面明显优于<em class="lv">变形金刚XL </em>。</p><p id="6851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，关于<em class="lv">块循环变压器，</em><em class="lv">反馈模式</em> <em class="lv">比<em class="lv">单循环模式要好。然而，作者得出结论，额外的性能并不能补偿额外的复杂性。</em></em></p><p id="fbbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">论文作者尝试了各种配置，例如增加或跳过门。欲了解更多信息，请查阅原始论文[2]。</p><h1 id="fa8b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结束语</h1><p id="504a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">本文讨论了<em class="lv">块递归转换器，这是一篇突破性的论文，它利用传统的<em class="lv"> RNN </em>递归来增加长文档中<em class="lv">转换器</em>的潜力。</em></p><p id="b3ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我强烈建议你阅读原文[2]，将这篇文章作为辅助指南来帮助你理解。</p><p id="72e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于论文非常新，作者没有发布任何源代码，尽管Github上有一些非官方的实现。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="c032" class="lw lx it bd ly lz pg mb mc md ph mf mg jz pi ka mi kc pj kd mk kf pk kg mm mn bi translated">感谢您的阅读！</h1><ul class=""><li id="dae9" class="nt nu it lb b lc mo lf mp li ps lm pt lq pu lu ny nz oa ob bi translated">订阅我的<a class="ae ky" href="/subscribe/@nikoskafritsas" rel="noopener ugc nofollow" target="_blank">简讯</a>！</li><li id="deee" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">在Linkedin上关注我！</li></ul><h1 id="51a3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><ol class=""><li id="77c9" class="nt nu it lb b lc mo lf mp li ps lm pt lq pu lu pf nz oa ob bi translated">Dosovitskiy等人，<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> <em class="lv">一幅图像抵得上16x16个字:用于图像识别的变形金刚</em> </a> <em class="lv"> (2020) </em></li><li id="35c2" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">德莱斯里·哈钦斯等人<a class="ae ky" href="https://arxiv.org/pdf/2203.07852.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">阻止轮回变形金刚</em></a><em class="lv">(2022年3月)</em></li><li id="e2f8" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">Jacob Devlin等人<a class="ae ky" href="https://arxiv.org/pdf/1810.04805v2.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向变压器预训练</a><em class="lv">(2019年5月)</em></li><li id="4b62" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">A.瓦斯瓦尼等人<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">关注是你所需要的全部</a>(2017年6月)</li><li id="2382" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">Kyunghyun等人<a class="ae ky" href="https://arxiv.org/abs/1409.1259" rel="noopener ugc nofollow" target="_blank">关于神经机器翻译的性质:编码器-解码器方法</a></li><li id="4d39" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">Iz Beltagy等人<a class="ae ky" href="https://arxiv.org/pdf/2004.05150.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lv">Long former:The Long-Document Transformer</em></a><em class="lv">，</em>艾伦人工智能研究所(2020)</li><li id="0490" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">戴子航等<a class="ae ky" href="https://arxiv.org/pdf/1901.02860.pdf" rel="noopener ugc nofollow" target="_blank"> Transformer-XL:固定长度语境之外的注意力语言模型</a> (2019)</li><li id="4ed7" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated">Colin Raffel等人<a class="ae ky" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">利用统一的文本到文本转换器探索迁移学习的限制</a> (2019)</li><li id="23ef" class="nt nu it lb b lc oc lf od li oe lm of lq og lu pf nz oa ob bi translated"><a class="ae ky" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Katharopoulos%2C+A" rel="noopener ugc nofollow" target="_blank"> Angelos Katharopoulos </a>等<a class="ae ky" href="https://arxiv.org/pdf/2006.16236.pdf" rel="noopener ugc nofollow" target="_blank">变压器是RNNs:具有线性关注的快速自回归变压器</a> (2020)</li></ol></div></div>    
</body>
</html>