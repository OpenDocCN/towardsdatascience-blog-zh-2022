<html>
<head>
<title>DRF: A Random Forest for (almost) everything</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DRF :(几乎)所有东西的随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/drf-a-random-forest-for-almost-everything-625fa5c3bcb8#2022-02-01">https://towardsdatascience.com/drf-a-random-forest-for-almost-everything-625fa5c3bcb8#2022-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/57d289d35b2d5c75d36378ab8e3060d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u6_ywAgMfCuaUcAM"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Geran de Klerk 在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ec9c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当 Breiman 在 2001 年引入随机森林(RF)算法时，他知道这会产生巨大的影响吗？如今，RF 是数据科学的许多领域中大量使用的工具。原因显而易见——RF 易于使用，并以其在极其广泛的任务中的高性能而闻名。这本身就令人印象深刻，但更有趣的是，获得这些结果通常不需要调优。</p><p id="3fe0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们讨论了原始 RF 的一个相当大的扩展，即我们最近在本文中开发的分布式随机森林(DRF ):</p><p id="5ce0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【分布随机森林:异质性调整和多元分布回归(jmlr.org)</p><p id="520b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与洛里斯·米歇尔的合作，米歇尔对本文也有贡献。其他代码和 python 实现可以在这里找到:<a class="ae kc" href="https://github.com/lorismichel/drf" rel="noopener ugc nofollow" target="_blank">https://github.com/lorismichel/drf</a></p><p id="d265" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将从随机森林这个主题的简单介绍开始，然后逐步介绍开发 DRF 的步骤。最后有一个小例子列表，我们很乐意在将来扩展它。</p><p id="b193" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们继续之前，我们首先需要了解一些符号:在本文中，我们将把<strong class="kf ir"> X </strong>、<strong class="kf ir"> Y </strong>称为随机向量，我们假设从这些向量中观察一个 iid 样本(yi，<strong class="kf ir"> x </strong> i)，i=1，…n。这在研究论文中很常见。如果我们改为处理随机变量(即单变量)，我们只写 Y 或 X。数据科学的许多应用程序的主要目标是<em class="ld">从一些特征中预测</em>Y<strong class="kf ir">X</strong>T10】，假设我们想要从像素强度的向量(<strong class="kf ir"> X </strong>)中预测标签“狗”(Y)。不太引人注目的是，预测实际上几乎总是条件分布 P(Y| <strong class="kf ir"> X </strong> = <strong class="kf ir"> x </strong>)的函数。所以我们问“如果向量<strong class="kf ir"> X </strong>固定为一个值<strong class="kf ir"> x </strong>，那么 Y 的某个分布方面是什么？”。最突出的例子就是条件均值或期望，即分布 P(Y| <strong class="kf ir"> X </strong> = <strong class="kf ir"> x </strong>)下的期望值。这是人们在要求预测时通常想要的。例如，如果我们使用线性回归进行预测，我们将一个值<strong class="kf ir"> x </strong>代入线性公式，得到 Y 的期望值作为预测值。上面的图像分类示例已经不同了——这里我们可能要求最可能的值，即对于哪个标签<em class="ld">l</em>P(Y =<em class="ld">l</em>|<strong class="kf ir">X</strong>=<strong class="kf ir">X</strong>)最大化。长话短说:大多数预测都是条件分布的一部分，通常是条件期望。</p><p id="453d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，人们后来发现 RF 不仅可以用于条件期望或均值预测，还可以预测(条件)分布的其他方面。例如，可以预测分位数，从而获得测量不确定性的预测区间。DRF 将这种方法提升到了一个新的水平:你可以向它提供一个多元响应<strong class="kf ir"> Y </strong>以及特征的典型向量<strong class="kf ir"> X </strong>，然后它会估计多元响应<strong class="kf ir"> Y </strong>的<em class="ld">整体条件分布</em>给定任何(实现)<strong class="kf ir"> x </strong>。<strong class="kf ir"> </strong>中给出了简单权重形式的估计值，便于从分布中模拟或计算感兴趣的量，即“预测”。</p><p id="c233" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里给出了对 RF 的全面解释，例如，,所以我们只是简单地触及一下:在 RF 中，安装了几个<em class="ld">独立的</em>决策树。每棵树只是得到变量的一部分，然后根据<strong class="kf ir"> X </strong>中的特征分割结果空间。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi le"><img src="../Images/12bfc24ea3b3574889a5b5d51549bd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NOc1NgQOeXaJ48AMh-Djg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">行动中的随机森林。一个新的点<strong class="bd lj"> x </strong>被放在每棵树上(红色),落在一个叶子或终端节点上。来源:使用 TikZ 代码从 s <a class="ae kc" href="https://tex.stackexchange.com/questions/503883/illustrating-the-random-forest-algorithm-in-tikz" rel="noopener ugc nofollow" target="_blank"> tackexchange </a>生成的图像。</p></figure><p id="4b50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，当我们“放下”一个新的点<strong class="kf ir"> x </strong>时，它将在每棵树的一片叶子中结束。一片树叶是一组观察值<em class="ld"> i </em>的集合，取该树叶中所有 yi 的平均值给出了对一棵树的预测。然后对这些预测进行平均，得出最终结果。因此，对于给定的<strong class="kf ir"> x </strong>如果你想预测给定的<strong class="kf ir">x</strong>Y 的条件均值，你:</p><p id="1ff2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.“下拉”每棵树的<strong class="kf ir"> x </strong>(在上图中用红色表示)。因为分割规则是在<strong class="kf ir"> X </strong>上制定的，所以你的新点<strong class="kf ir"> x </strong>将安全地落在一个叶节点的某个地方。</p><p id="e803" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.对于每棵树，你平均该树叶中的响应 yi，以获得每棵树的条件均值的估计。</p><p id="a689" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.你在树上平均每个条件平均值来得到最终的预测。</p><p id="5dc6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重要的是，平均所有树的预测导致方差的显著减少。可以说，误差得到了“平均”。这就是为什么让树尽可能独立是很重要的。</p><p id="348f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在致力于扩展这一思想，不仅进行平均预测，而且预测整个(条件)分布。也就是我们要预测一个测试点<strong class="kf ir"> x </strong>的 P(<strong class="kf ir">Y</strong>|<strong class="kf ir">X</strong>=<strong class="kf ir">X</strong>)。如前所述，人们通常想要预测的许多东西都是这个条件分布的直接函数:条件均值、条件分位数、条件方差、协方差等等。事实证明，DRF 的形式也使它适合于因果分析，甚至超越了最先进的方法。我们在本文的最后给出了一个小例子，在以后的文章中可能会给出更多的例子。</p><p id="f81c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们一步一步地推导如何到达分布式随机森林。第一步是得到重量。</p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h1 id="9f72" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">该理论</h1><h2 id="07c2" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv mf ks mw mx mj kw my mz mn na bi translated"><strong class="ak">第一步:增加体重</strong></h2><p id="4bea" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">这其中的关键是获得射频提供的<em class="ld">权重</em>。首次介绍 RF 时，这一点通常会被忽略，但在这里却至关重要:</p><p id="2659" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不是直接计算叶节点中的平均值，而是计算在进行平均值计算时隐含使用的权重。权重 wi( <strong class="kf ir"> x </strong>)是(1)测试点<strong class="kf ir"> x </strong>和(2)观测值<em class="ld"> i </em>的函数。也就是说，如果我们把<strong class="kf ir"> x </strong>丢下一棵树，我们会观察它最后落在哪片叶子上。该叶中的所有观察值都为 1，其他所有观察值都为 0。因此，如果我们最终在一片叶子中得到观察值(1，3，10)，那么该树的观察值 1，3，10 的权重是 1，而所有其他观察值都是 0。然后，我们进一步将权重除以叶节点中元素的数量。在前面的示例中，我们在叶节点中有 3 个观察值，因此观察值 1、3、10 的权重都是 1/3，而所有其他观察值在该树中的权重仍然是 0。对所有树的这些权重进行平均得到最终的权重 wi( <strong class="kf ir"> x </strong>)。</p><p id="b754" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么这些重量有用？首先，可以看出，像我们在传统随机森林中所做的那样计算平均值，与对 wi( <strong class="kf ir"> x </strong> )*yi 求和是一样的。这看起来有点难看，但基本上只是交换两个和。因此，如果这个估计是合理的，它应该是合理的，如果你取 yi 的其他函数，比如 yi，你得到给定的 Y 的平均值<strong class="kf ir"> X </strong>。或者你，对于某个数 t，你可以使用 I(yi &lt; t)如果 yi &lt; t 为 1，否则为 0，并将 wi( <strong class="kf ir"> x </strong> )I(yi &lt; t)上的和视为 Y &lt; t 的条件概率的估计，这样你就可以直接获得条件累积分布函数(cdf)，它在理论上完全表征了条件分布。第二，这些权重实际上为您提供了在某种意义上与查询<strong class="kf ir"> x </strong>相似的观察值的最近邻估计。正如我们将在下面看到的，这种相似性是以一种理想的方式来度量的，这种方式使得有可能将相关联的 yi 看作来自条件分布的 iid 样本。</p><p id="4127" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这甚至可以更进一步。特别地，如果你用这些权重从 Y 中取样，你将得到一个条件分布的近似样本。在我们继续之前，让我们使用 r 中著名的 ranger 包做一个简单的例子。该包不直接提供这些权重，所以我们需要手动实现它们，这是一个很好的练习:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="ab8d" class="mp ls iq nh b gy nl nm l nn no">library(ranger)<br/>library(ggplot2)<br/>n&lt;-1000</span><span id="a4b6" class="mp ls iq nh b gy np nm l nn no">## Simulate some data:<br/>X&lt;-matrix(rnorm(n*2),ncol=2)<br/>Y=2*X[,1] + 0.5*X[,2]^2 + rnorm(n)<br/>Y&lt;-as.matrix(Y)</span><span id="c1ee" class="mp ls iq nh b gy np nm l nn no">## Get a test point<br/>x&lt;-matrix(c(1,-0.5),ncol=2)</span><span id="ac78" class="mp ls iq nh b gy np nm l nn no"># Fit Random forest<br/>RF&lt;-ranger(Y~., data=data.frame(Y=Y, X=X),min.node.size = 10, num.trees=500)</span><span id="15ce" class="mp ls iq nh b gy np nm l nn no">## Get the leaf/terminal node indices of each observation in the training samples<br/>terminalX &lt;- predict(RF, data=data.frame(X=X), type="terminalNodes")$predictions</span><span id="7160" class="mp ls iq nh b gy np nm l nn no">## Get the leaf/terminal node of the test point<br/>terminalxnew &lt;- predict(RF, data=data.frame(X=x), type="terminalNodes")$predictions</span><span id="6f26" class="mp ls iq nh b gy np nm l nn no">## For the leafs in which x ends up, what are the number of observations (we need to normalize by that)<br/>divid&lt;-sapply(1:ncol(terminalxnew[1,, drop=F]), function(j) {sum(terminalX[,j]==terminalxnew[1,j])} )<br/>  <br/># Average the weights over the trees:<br/>weights&lt;-rowSums(t(sapply(1:nrow(terminalX), function(j) as.numeric(terminalX[j,]==terminalxnew[1,])/divid )), na.rm=T)/length(terminalxnew[1,])</span><span id="5d7c" class="mp ls iq nh b gy np nm l nn no">## We can now sample according to those weights from Y<br/>Ysample&lt;-Y[sample(1:dim(Y)[1], prob=weights, replace = T),]</span><span id="1a79" class="mp ls iq nh b gy np nm l nn no">## True conditional distribution<br/>Ytrue&lt;-2*x[,1] + 0.5*x[,2]^2 + rnorm(n) <br/>p1 &lt;- hist(Ytrue, freq=F)                     <br/>p2 &lt;- hist(Ysample, freq=F)                     <br/>plot( p2, col=rgb(0,0,1,1/4), freq=F)  <br/>plot( p1, col=rgb(1,0,0,1/4), add=T, freq=F)</span><span id="e451" class="mp ls iq nh b gy np nm l nn no">## Plot the weights by looking at the point x and its nearest neighbors according to the weights<br/>ggplot(data = data.frame(X=X), aes(x = X.1, y = X.2)) +<br/>  geom_point(aes(alpha = exp(weights ))) + geom_point(data=data.frame(X=x), color = "red")</span><span id="f035" class="mp ls iq nh b gy np nm l nn no">## Get the estimated conditional mean:<br/>mean(Ytrue)<br/>mean(Ysample)<br/>sum(weights*Y)</span></pre><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nq"><img src="../Images/685247e72158b6472a89e2cc97e53826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mT6R7k5jq6V9k0pa1Pdhyw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">上面代码的结果:在左边，我们看到了一个<strong class="bd lj"> Xi </strong>(灰色点)和红色测试点<strong class="bd lj"> x </strong>的图。观察点<strong class="bd lj"> Xi </strong>越暗，RF 分配给它们的权重越大。在右边，来自真实条件分布(红色)的模拟与根据权重从 Y 提取的点进行比较。图片作者。</p></figure><h2 id="0483" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv mf ks mw mx mj kw my mz mn na bi translated">步骤 2:使用不同的分割标准</h2><p id="70f7" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">还有另一种方式来看待随机森林算法:它是一个同质机器。在树中的每个分裂中，选择在<strong class="kf ir"> X </strong>中的分裂，使得结果节点中 Y 的两个样本尽可能“不同”。下图是单变量 X 和 y 的一个小例子。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/8af0e807f92009bf6d3e7c5fb9971525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Cb9NXq-iwg0PORfemJAYQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">RF 中完成的拆分的图示。图片作者。</p></figure><p id="d10c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这样的例子中，一个给定的树很可能在 S 处分裂 X，如图所示。然后所有易与的&lt; S will be thrown into node 1 and all Xi &gt; = S 进入节点 2，从而识别数据簇。如果我们这样做足够长的时间，每个叶节点将有一个非常均匀的 Y 样本，或者换句话说，一个叶节点中的所有 yi 在某些方面将是<em class="ld">相似的</em>，因为所有“不相似的”观察值都被切掉了。因此，对于每棵树，您将数据分组到相似事物的桶中。</p><p id="2627" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么这有意义？因为本质上，RF 是一种最近邻算法。如果你给它一个<strong class="kf ir"> x </strong>,它会把它丢下树，直到它落在一个桶里或一片叶子上，观察结果“类似”。根据这些观察值，计算条件平均值。也就是说，在每棵树中，只考虑叶子中的观察值，其他都不考虑。所以它就像一个 k-NN，距离不是由欧几里得距离来衡量的，而是由森林来决定的。反过来，森林决定将这些<strong class="kf ir"> x </strong> i 标记为具有“相似”yi 的“相似”。因此，<strong class="kf ir"> x </strong> i 的相似性是基于它们相关联的 yi 来决定的，如果你的目标是推断关于 Y 的事情，这就很有意义。事实上，即使是 k-NN 方法也假设了一些类似于“对于<strong class="kf ir"> x </strong> i 接近于<strong class="kf ir"> x </strong>的条件分布 P( <strong class="kf ir"> Y </strong>|<strong class="kf ir">X</strong>=<strong class="kf ir">X</strong>I，条件分布 P(<strong class="kf ir">Y</strong>)大致相同”的东西。下图说明了这一点:您可以看到样本中每个值<strong class="kf ir"> x </strong> i 的相关真条件分布，yi 就是从该分布中采样的。DRF 的完美版本将识别出(<strong class="kf ir"> x </strong> 1，<strong class="kf ir"> x </strong> 4，<strong class="kf ir"> x </strong> 6)和(<strong class="kf ir"> x </strong> 3，<strong class="kf ir"> x </strong> 5，<strong class="kf ir"> x </strong> 7)的条件分布是相似的(不管它们的欧几里德距离实际上是多少)，并且将 yi，(y1，y4，y6)和(y3，y5，y7)的相应组分别视为</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/6b26043784501291f14a55d0dd4c34ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A8OQlRYAyAQ0d6gmSAIEJA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">对于样本中的每个值<strong class="bd lj"> x </strong> i，显示了 Y| <strong class="bd lj"> X </strong> = <strong class="bd lj"> xi </strong>的条件分布，Yi 是该分布的平局。可以看出，有些分布与其他分布更相似。图片作者。</p></figure><p id="b0ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理想情况下，这将意味着在实践中，我们在一个叶内最终得到的同质 yi 样本实际上大约是来自条件分布 Y| <strong class="kf ir"> X=x </strong>的 iid 样本。这就是采用(加权)平均值的理由。</p><p id="04d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，在最初的 RF 中，除了条件均值预测之外，这种方法不能如预期的那样工作。同样，我们想要的是一个分裂标准，它使得 Y 在两个分裂中的分布尽可能不同。相反，在原始 RF 中，我们得到的只是一个分裂，使两个样本之间的<em class="ld">均值</em>差异尽可能大。在上图中，这样的方法可能会将除了<strong class="kf ir"> x </strong> 2 之外的所有人归为一组，因为<strong class="kf ir"> x </strong> 1、<strong class="kf ir"> x </strong> 3、<strong class="kf ir"> x </strong> 4、<strong class="kf ir"> x </strong> 6、<strong class="kf ir"> x </strong> 7 都有非常相似的含义。当然，正如上图所示，分布并不是通过平均值来定义的。一个正态分布可以有相同的均值，但有非常不同的方差或其他矩。一般来说，你可以有很多均值相同的分布，但在其他方面却非常不同。</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/12b5cadb5b0007de1c411c6c2ba785e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*DQqwe4_PllaiTsG2zm9DcQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">均值相同的两个分布。如果这是两个不同国家的收入不平等，它们会被视为相同吗？图片作者。</p></figure><p id="00d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关键在于，每次拆分都应该取决于两个结果节点之间分布差异的度量。因此，不仅检测均值或方差的差异，而且检测分布的任何差异。DRF 通过调整射频中通常采用的分裂标准来解决这个问题，以利用<em class="ld">内核</em>的理论和实际能力以及所谓的 MMD 标准。MMD 可以非常有效地近似，并且原则上能够检测分布中的任何差异。从理论上讲，我们从而把每一个点 yi 送入一个无限维的空间，再生核希尔伯特空间，实际上在那个空间比较均值。通过内核方法的魔力，这种均值之间的比较实际上是分布的比较！在这个特殊的空间中，平均值就是分布。这在实践中意味着如下:一个叶节点将包含相似的<strong class="kf ir"> x </strong> i，在这个意义上，yi 在那个桶中的分布是相似的。因此，如果给定的 Y 的条件分布<strong class="kf ir"> x </strong> i 和<strong class="kf ir"> x </strong> j 相似，则它们将被分组到相同的桶中。原则上这是正确的，即使<strong class="kf ir"> x </strong> i 和<strong class="kf ir"> x </strong> j 在欧几里德空间中相距很远(即，如果它们在 k-NN 意义上不是最近的邻居)。因此，如果我们使用这些权重来计算我们感兴趣的条件事物，我们使用最近邻方法，该方法认为<strong class="kf ir"> x </strong> i 和<strong class="kf ir"> x </strong> j 是相似的，当它们相关的 yi，yj 的分布是相似的。特别地，在一些平滑度假设下，叶节点<strong class="kf ir"> x </strong>中的样本大约是来自分布 P(<strong class="kf ir">Y</strong>|<strong class="kf ir">X</strong>=<strong class="kf ir">X</strong>)的 iid 样本。</p><h2 id="920c" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv mf ks mw mx mj kw my mz mn na bi translated"><strong class="ak">第三步:使用多元响应</strong></h2><p id="8c9e" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">这一步实际上很简单，因为 MMD 也允许比较多元分布。重要的是，区分不仅仅是平均值对于多变量响应变得更加重要，因为分布的差异可能更加复杂。例如，两个多元分布可以具有相同的均值和方差，但元素之间的协方差不同。</p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h1 id="d6b2" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">例子</h1><p id="bc03" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们来举几个小例子。这里的目标是提供非常简单的模拟例子来感受这种方法。首先，我们重复上面手动完成的操作:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="ca9b" class="mp ls iq nh b gy nl nm l nn no">library(drf)<br/># Fit DRF<br/>DRF&lt;-drf(X,Y)<br/>weights&lt;-predict(DRF,x)$weights[1,]</span><span id="4100" class="mp ls iq nh b gy np nm l nn no">### We can now sample according to those weights from Y<br/>Ysample&lt;-Y[sample(1:dim(Y)[1], prob=weights, replace = T),]</span><span id="fad4" class="mp ls iq nh b gy np nm l nn no">## True conditional distribution<br/>Ytrue&lt;-2*x[,1] + 0.5*x[,2]^2 + rnorm(n) <br/>p1 &lt;- hist(Ytrue, freq=F)                     <br/>p2 &lt;- hist(Ysample, freq=F)                     <br/>plot( p2, col=rgb(0,0,1,1/4), freq=F)  <br/>plot( p1, col=rgb(1,0,0,1/4), add=T, freq=F)</span><span id="a57f" class="mp ls iq nh b gy np nm l nn no">ggplot(data = data.frame(X=X), aes(x = X.1, y = X.2)) +<br/>  geom_point(aes(alpha = exp(weights ))) + geom_point(data=data.frame(X=x), color = "red")</span><span id="2d6f" class="mp ls iq nh b gy np nm l nn no">## Get the estimated conditional mean:<br/>mean(Ytrue)<br/>mean(Ysample)<br/>sum(weights*Y)</span></pre><p id="001d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在产生了这些更好看的结果:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nu"><img src="../Images/c035388f361567f45ef0df51ff38e16f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZYodaLEu2VG6wCfph1O-g.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">上面代码的结果:在左边，我们看到了一个<strong class="bd lj"> Xi </strong>(灰色点)和红色测试点<strong class="bd lj"> x </strong>的图。观察点<strong class="bd lj"> Xi </strong>越暗，DRF 赋予它们的权重就越大。在右边，来自真实条件分布(红色)的模拟与根据权重从 Y 提取的点进行比较。图片作者。</p></figure><p id="92de" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也可以预测条件分位数。例如，这样做给出了 Y| <strong class="kf ir"> x </strong>的值的预测区间，因此从该分布中得出的值应该大约是该区间中 95%的时间:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="aa3f" class="mp ls iq nh b gy nl nm l nn no"># Predict quantiles for a test point.<br/>quants&lt;-predict(DRF,x, functional = “quantile”, quantiles=c(0.025, 0.975))$quantile <br/>q1&lt;-quants[1]<br/>q2&lt;-quants[2]</span><span id="a9e1" class="mp ls iq nh b gy np nm l nn no">mean(Ytrue &gt;=q1 &amp; Ytrue &lt;=q2)</span></pre><p id="198a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一行检查从条件分布模拟的新样本中有多少部分位于区间[q1，q2]内。在这种情况下，结果大约是 94%，接近我们希望的 95%。</p><h2 id="3079" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv mf ks mw mx mj kw my mz mn na bi translated">二维响应</h2><p id="ebe2" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">在这里，我们构建了一个具有二维响应的非常困难的示例，并计算了各种预测。我们首先模拟数据:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="0856" class="mp ls iq nh b gy nl nm l nn no">n&lt;-5000<br/>d&lt;-2<br/>p&lt;-3</span><span id="538b" class="mp ls iq nh b gy np nm l nn no">## Simulate X and Y<br/>X&lt;-matrix( cbind(runif(n)*2, rnorm(n, mean = 1), rt(n, df=8)),nrow=n)</span><span id="adfb" class="mp ls iq nh b gy np nm l nn no">Y&lt;-matrix(NA, ncol=2, nrow=n)<br/>Y[,1]&lt;-X[,1]^2 + X[,2]*X[,1] + X[,3] + rnorm(n)<br/>Y[,2] &lt;- 2*Y[,1]*X[,2] + 0.1*X[,3] + rnorm(n)</span></pre><p id="9aed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于非参数估计方法来说，这些是非常疯狂的关系，因为一个<strong class="kf ir"> X </strong>的元素具有非常不同的分布。<strong class="kf ir"> </strong>有一点要提的是 2*Y[，1]*X[，2]，意思是当<strong class="kf ir"> X </strong>的第二个元素为正时<strong class="kf ir"> Y </strong>的第一个和第二个元素的相关性为正，而当<strong class="kf ir"> X </strong>的第二个元素为负时相关性为负。<strong class="kf ir"> Y </strong>整体看起来是这样的:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/fe2e59c519b15603e9017e590d969635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RodTwdjOtsKUsb7MdC6TZw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">如上面代码中模拟的。图片作者。</p></figure><p id="5bbb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在选择两个测试点，做一些大胆的预测，因为我们可以:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="a8e4" class="mp ls iq nh b gy nl nm l nn no">library(drf)</span><span id="4489" class="mp ls iq nh b gy np nm l nn no"># Fit DRF<br/>DRF&lt;- drf(X=X, Y=Y, num.features=200)</span><span id="c411" class="mp ls iq nh b gy np nm l nn no"># Choose a few test point:<br/>x= matrix(c(0.093, -0.5,  1.37, 0.093, 0.5,  1.37) , ncol=p, nrow=2, byrow=T)</span><span id="2971" class="mp ls iq nh b gy np nm l nn no"># mean prediction<br/>(predict(DRF, newdata=x, functional="mean")$mean)</span><span id="8451" class="mp ls iq nh b gy np nm l nn no"># correlation prediction<br/>matrix(predict(DRF, newdata=x[1,], functional="cor")$cor, nrow=d,ncol=d)<br/>matrix(predict(DRF, newdata=x[2,], functional="cor")$cor, nrow=d,ncol=d)</span><span id="4ce0" class="mp ls iq nh b gy np nm l nn no"># Estimated probability that Y is smaller than 0:<br/>weightstotal&lt;-predict(DRF, newdata=x)$weights<br/>p1&lt;-sum(weightstotal[1,]* (Y[,1] + Y[,2]&lt;= 0) )<br/>p2&lt;-sum(weightstotal[2,]* (Y[,1] + Y[,2] &lt;= 0) )</span><span id="1768" class="mp ls iq nh b gy np nm l nn no"># Bootstrapping the estimated probability of the sum being &lt;=0 for both points:<br/>B&lt;-100<br/>pb1&lt;-matrix(NA, ncol=1,nrow=B)<br/>pb2&lt;-matrix(NA, ncol=1,nrow=B)<br/>for (b in 1:B){<br/>  Ybx1&lt;-Y[sample(1:n, size=n, replace=T, prob=weightstotal[1,]), ]<br/>  Ybx2&lt;-Y[sample(1:n, size=n, replace=T, prob=weightstotal[2,]), ]<br/>  pb1[b] &lt;- mean(Ybx1[,1] + Ybx1[,2] &lt;= 0)<br/>  pb2[b] &lt;- mean(Ybx2[,1] + Ybx2[,2] &lt;= 0)<br/>}</span><span id="7ff4" class="mp ls iq nh b gy np nm l nn no">ggplot(data.frame(x=1:2, y=c(p1, p2)), aes(x = x, y = y)) +<br/>  geom_point(size = 4) +<br/>  geom_errorbar(aes(ymax = c(quantile(pb1,1- 0.025), quantile(pb2, 1-0.025)), ymin = c(quantile(pb1, 0.025),quantile(pb2, 0.025) )))</span></pre><p id="d6ee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不详细描述结果，但有趣的是，当<strong class="kf ir"> x </strong>的第二个元素为负时，DRF 设法正确地检测到负相关性，当<strong class="kf ir"> x </strong>的第二个元素为正时，检测到正相关性。此外，我们还加入了一个新的方面:我们甚至可以为我们的估计做一个(有条件的)bootstrap。在这种情况下，我们将其应用于<strong class="kf ir"> Y </strong>的元素之和小于或等于 0 的估计概率。这导致该量的以下置信区间:</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a943c2416467e9dcd5e6146e1a4c8002.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*my7xRSKxrxkyYZp2yR7ZDA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">两个测试点 P 的自举 CIs(Y1+Y2≤0 |<strong class="bd lj">X</strong>=<strong class="bd lj">X</strong>)。</p></figure><h2 id="7bab" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv mf ks mw mx mj kw my mz mn na bi translated">从预测到因果效应</h2><p id="59d9" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">为了让事情变得更有趣，我们研究了一个医学例子，我们希望得到因果效应(它完全是由完全不切实际的数字组成的，尽管它是由真实的问题激发的——男性和女性对药物的反应不同)。</p><p id="4f72" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个例子中，我们模拟了一个结果，比如说应该通过某种药物调节的血液稀释效应(B)。我们还知道患者的年龄和性别，并且我们模拟以下关系:对于男性患者，与年龄无关，药物线性增加血液稀释效果。对于女性患者，药物治疗也会增加 B，但如果她们的年龄超过 50 岁，则增加的程度会高于男性。然而，如果低于 50，效果完全相反，药物导致<em class="ld">降低</em>血液稀释效果。确切的数据生成过程如下:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="6f7d" class="mp ls iq nh b gy nl nm l nn no">n&lt;-1000</span><span id="98b9" class="mp ls iq nh b gy np nm l nn no"># We randomly sample from different ages…<br/>Age&lt;-sample(20:70, n, replace=T)</span><span id="4bca" class="mp ls iq nh b gy np nm l nn no"># and sexes<br/>Sex &lt;- rbinom(n, size=1,prob=0.6) # 1=woman, 0=man</span><span id="e6da" class="mp ls iq nh b gy np nm l nn no"># W denotes the dosis of the medication, the causal effect we are directly interested in<br/>W&lt;-sample( c(5,10,50,100),n, replace=T)</span><span id="8f6d" class="mp ls iq nh b gy np nm l nn no"># B is the thing we want to understand<br/>B&lt;- 60+ (0.5*W)*(Sex==0) + (-0.5*W)*(Sex==1)*(Age&lt;50) + (0.8*W)*(Sex==1)*(Age&gt;=50) + rnorm(n)</span></pre><p id="b0e6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先说明这种关系(因为我们知道真相，所以我们可以这样绘制):</p><figure class="lf lg lh li gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nx"><img src="../Images/a9932ba1f0a234481a1004e1df1e1433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXMIVjAXQ3G43aUuNAfNOg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">按年龄组和性别说明药物的模拟效果。图片作者。</p></figure><p id="24e7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DRF 的一种方法是(1)取<strong class="kf ir"> Y </strong> =(B，W)(所以我们的<strong class="kf ir"> Y </strong>也是二维的)和<strong class="kf ir"> X </strong> =(年龄，性别)，(2)得到给定的<strong class="kf ir"> x </strong>的权重，然后(3)估计用这些权重加权的线性回归。这给出的是给定<strong class="kf ir"> X </strong>固定为<strong class="kf ir"> x </strong>时的效果估计值:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="2002" class="mp ls iq nh b gy nl nm l nn no">get_CATE_DRF = function(fit, newdata){<br/> out = predict(fit, newdata)<br/> ret = c()<br/> for(i in 1:nrow(newdata)){<br/> ret = c(ret, lm(out$y[,1]~out$y[,2], weights=out$weights[i,])$coefficients[2])<br/> }<br/> return(ret)<br/>}</span></pre><p id="8f70" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里可以看到不同测试点的结果:</p><pre class="lf lg lh li gt ng nh ni nj aw nk bi"><span id="b983" class="mp ls iq nh b gy nl nm l nn no">library(drf)</span><span id="4ab1" class="mp ls iq nh b gy np nm l nn no"># Construct the data matrices<br/>X&lt;-matrix(cbind(Age, Sex), ncol=2)<br/>Y&lt;-matrix(cbind(B,W), ncol=2)</span><span id="4845" class="mp ls iq nh b gy np nm l nn no"># Fit<br/>DRF&lt;-drf(Y=Y, X=X)</span><span id="97ec" class="mp ls iq nh b gy np nm l nn no"># Get a test point (changing this test point gives different queries)<br/>x&lt;-matrix(c(40, 1), nrow=1)</span><span id="9a89" class="mp ls iq nh b gy np nm l nn no"># Get the weights<br/>weights &lt;- predict(DRF, newdata=x)$weights[1,]</span><span id="489a" class="mp ls iq nh b gy np nm l nn no"># Study the weights<br/>head(X[order(weights, decreasing=T),])</span><span id="199a" class="mp ls iq nh b gy np nm l nn no"># Result for given test point<br/>(CATE_DRF = get_CATE_DRF(DRF, newdata=x))</span></pre><p id="f60b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以看出，当运行 1000 个数据点的代码时，这些结果是相当准确的:在这个例子中，我们得到一个 40 多岁的女人的效应为-0.26(<strong class="kf ir">x</strong>=(40，1))，一个 60 多岁的女人的效应为 0.48(<strong class="kf ir">x</strong>=(60，1))，一个 30 岁的男人的效应为 0.28(<strong class="kf ir">x</strong>=(30，0))</p><p id="6dd3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在某些方面，这仍然是一个非常简单的例子，甚至以年龄和性别作为预测因素的线性回归可能也很有效。重要的是，DRF 在这里没有预先假设(例如线性)，并且完全自己学习关系，即使当<strong class="kf ir"> X </strong>的效应是非线性的。对于较小的样本量，估计这种影响要困难得多，但总的方向通常不会太差。</p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h1 id="758e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">结论</h1><p id="9ab4" class="pw-post-body-paragraph kd ke iq kf b kg nb ki kj kk nc km kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">本文解释了分布式随机森林方法(希望以一种可以理解的方式)。该方法是一个随机森林，其中每棵树根据<strong class="kf ir"> X </strong>分割响应<strong class="kf ir"> Y </strong>，以这种方式，具有相似分布的观察结果在一个叶节点中结束。如果一个新的点<strong class="kf ir"> x </strong>被放在一棵树上，它将和其他具有类似条件分布<strong class="kf ir"> Y </strong>的<strong class="kf ir"> x </strong> i 一起到达一个叶子节点。这导致在所有树上平均的权重，以简单的形式给出条件分布的估计。这给出了 P(<strong class="kf ir">Y</strong>|<strong class="kf ir">X</strong>=<strong class="kf ir">X</strong>)的纯非参数估计，从中可以估计出许多有趣的量。</p><p id="9bea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的最后，我们只想提醒大家，非参数地估计多元条件分布是一项艰巨的任务。这是有意义的，尤其是当有大量的观察和复杂的关系是可疑的。然而，有时假设一个高斯分布的线性模型也可以。使 DRF 如此多才多艺的是，即使在参数模型更合适的情况下，权重对于半参数方法仍然是有用的，</p><p id="a6b5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大量额外的例子可以在原始论文中找到，或者在未来潜在的媒体文章中找到。我们希望 DRF 能在许多依赖数据的任务中提供帮助！</p></div></div>    
</body>
</html>