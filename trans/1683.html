<html>
<head>
<title>Create your own GPU accelerated Jupyter Notebook Server for Google Colab using Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Docker为Google Colab创建自己的GPU加速Jupyter笔记本服务器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-your-own-gpu-accelerated-yupyter-notebook-server-with-google-colab-using-docker-2fa14900bab5#2022-04-21">https://towardsdatascience.com/create-your-own-gpu-accelerated-yupyter-notebook-server-with-google-colab-using-docker-2fa14900bab5#2022-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f3fe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">循序渐进的指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/93cc03ea10481e2f2ab3e852cf261e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9y3Hokq7ILd_bEUu"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SpaceX 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae kv" href="https://unsplash.com/@spacex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="8097" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> Y </span>你想在你自己的电脑或服务器上训练你的深度学习模型，你不想处理免费环境如Google Colab或Kaggle的资源限制，你不想花费数小时安装运行训练所需的所有驱动程序和软件包？那么我有好消息要告诉你，因为今天我们将探讨如何在你自己的系统上使用Docker创建你自己的Jupyter笔记本服务器，如何启用GPU加速，以及如何从Google Colab连接到你的服务器。</p><p id="bbb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我想简单地谈谈Docker，以及为什么它是ML从业者的一个很好的工具，他们希望专注于ML，而不是处理复杂的环境问题，如驱动程序、不同版本和依赖关系等。之后，我们看看安装(剧透警告，这将是简短和容易的)。最后，我将向您展示，如何使用Google Colab连接到您的服务器。瞧，现在你可以用自己的电脑运行笔记本了。</p><h1 id="268f" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">概述</h1><ol class=""><li id="48f3" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated"><a class="ae kv" href="#88b0" rel="noopener ugc nofollow">简单地说就是Docker</a></li><li id="2506" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#b649" rel="noopener ugc nofollow">安装指南</a></li><li id="848c" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#45be" rel="noopener ugc nofollow">运行您的第一台笔记本电脑</a></li><li id="9f9b" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated"><a class="ae kv" href="#e936" rel="noopener ugc nofollow">结论</a></li></ol><h1 id="88b0" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">1.简单地说，码头工人</h1><p id="7db0" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">您可能已经听到人们谈论Docker和容器，以及它与虚拟化的关系。我记得我很困惑，我想，如果我可以使用虚拟机，为什么我要在容器中使用Linux发行版，比如Ubuntu？有许多很好的理由，我希望你在读完这一节后同意我的观点。</p><p id="75ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Docker能够在隔离的容器中运行应用程序。容器是一个轻量级的构造，包含了应用程序运行所需的一切。这已经是主要优势之一了。因为您需要的所有东西都打包在容器中，所以您可以在不同的主机上执行容器，而不管该主机上当前安装了什么。因此，您可以与同事共享您的容器，并确保您的应用程序以相同的方式运行。它们都与硬件和平台无关。现在让我们更深入一层，检查Docker的架构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/e5f123c19fadb470bfeb0b184eed2bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tBF3AjDSrnVHLjekoZiTdA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的Docker架构灵感来自<a class="ae kv" href="https://docs.docker.com/get-started/overview/" rel="noopener ugc nofollow" target="_blank"> Docker </a></p></figure><p id="4a0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Docker遵循客户端-服务器原则。客户端与Docker守护进程通信，除了其他任务之外，Docker守护进程还管理图像、处理依赖关系并执行容器。Docker守护程序可以在本地主机上，也可以在远程主机上。图像可以被理解为带有如何构建容器的指令的蓝图。容器是一个图像的可运行实例。单个图像的多个容器可以并行运行，这意味着它可以轻松缩放。默认情况下，容器与其他容器和主机隔离。可选地，可以配置接口，例如打开端口。</p><p id="ecac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">映像安装在主机上。如果您想创建一个容器，而映像没有安装在主机上，Docker会自动从标准注册表中下载映像。</p><p id="868b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们联系到我们的主要目标:运行GPU加速的笔记本来训练我们的机器学习模型。前面我说过容器是硬件不可知的。大多数情况下都是这样。为了在可接受的时间内训练大规模模型，GPU是必不可少的，由于其硬件架构，可以并行化大规模计算。英伟达是专门用于机器学习应用的GPU的大型供应商，包括一个强大的软件框架。由于我们将在本教程中关注Nvidia GPUs，我们限制我们的硬件不可知的方法。</p><p id="2482" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Docker本身不支持GPU加速。我们需要使用Nvidia工具包。让我们看看下面的图片，以了解这在高层次上是如何工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/f2f257165beb5e1fd5d665ff162bc881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*boqkLWcF06zK89xjqCDIhw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者的Nvidia容器工具包灵感来自于Nvidia </p></figure><p id="d4fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从堆栈的底部开始。我们有一个带有一个或多个GPU的服务器(或任何类型的计算机)。这台计算机的操作系统安装了docker引擎。支持我们的GPU所需的第一个组件是安装它的专用驱动程序。对于Nvidia，这些被称为CUDA驱动程序。Docker引擎可以创建隔离的容器，为某个应用程序分配操作系统资源。CUDA工具包支持容器内的GPU驱动程序。</p><h1 id="b649" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">2.安装指南</h1><p id="d0b5" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">开始之前，我们要做的是:</p><ol class=""><li id="540a" class="mt mu iq ky b kz la lc ld lf no lj np ln nq lr na nb nc nd bi translated">我们确保激活了wsl 2(Linux的Windows子系统)并安装了一个Linux发行版，即Ubuntu</li><li id="025d" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">我们安装Docker桌面</li><li id="f618" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">我们启用GPU支持</li><li id="407c" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">我们拉官方TensorFlow Jupyter笔记本服务器带GPU支持</li></ol><p id="59aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为参考，此程序已在Win11、英特尔酷睿i7第10代和英伟达GeForce RTX 2060上进行了测试。你也可以在其他操作系统上安装Docker，比如Ubuntu。你甚至可以在Ubuntu虚拟机上安装Docker。但是你应该记住，从虚拟机内部你不能访问你的物理GPU，这意味着你不能利用GPU加速。</p><blockquote class="nr ns nt"><p id="477d" class="kw kx nu ky b kz la jr lb lc ld ju le nv lg lh li nw lk ll lm nx lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>以下命令必须在Windows PowerShell和Linux shell中执行。带有PS &gt;的代码嵌入是PowerShell命令，带有$的代码嵌入是Linux命令。</p></blockquote><h2 id="0d37" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤1:确保WSL2被激活并安装Ubuntu</h2><p id="652a" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">要在Linux的Windows子系统中安装ubuntu，只需打开PowerShell并执行:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="4312" class="ny mc iq ol b gy op oq l or os">PS&gt; wsl --install -d Ubuntu</span></pre><p id="25dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">安装完成后，使用以下命令检查您正在运行的WSL版本:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="a7a3" class="ny mc iq ol b gy op oq l or os">PS&gt; wsl -l -v</span></pre><p id="aebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您的版本不是2，您可以使用以下命令进行设置:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="afb8" class="ny mc iq ol b gy op oq l or os">PS&gt; wsl --set-default-version 2</span></pre><p id="d465" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，通过浏览开始菜单或在PowerShell中键入<code class="fe ot ou ov ol b">wsl</code>来启动Ubuntu子系统。现在应该要求您创建一个新用户并分配一个密码。该用户将自动成为具有管理员权限的默认用户，并在启动时自动登录。</p><blockquote class="nr ns nt"><p id="017a" class="kw kx nu ky b kz la jr lb lc ld ju le nv lg lh li nw lk ll lm nx lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>如果不要求您创建新用户，而是以root用户身份登录，您必须手动创建一个用户，使其成为默认用户并添加管理员权限。</p></blockquote><ol class=""><li id="b1a2" class="mt mu iq ky b kz la lc ld lf no lj np ln nq lr na nb nc nd bi translated">添加新用户(在WSL中以root用户身份登录):</li></ol><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="c4de" class="ny mc iq ol b gy op oq l or os">$ sudo adduser &lt;USER_NAME&gt;</span></pre><p id="2d91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.将您的用户设置为默认用户(在PowerShell中):</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="da2b" class="ny mc iq ol b gy op oq l or os">PS&gt; ubuntu config — default-user &lt;USER_NAME&gt;</span></pre><p id="95cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.将您的用户添加到sudoers(在WSL中以root用户身份登录):</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="282d" class="ny mc iq ol b gy op oq l or os">$ usermod -aG sudo &lt;USER_NAME&gt;</span></pre><h2 id="54b8" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤2:安装Docker桌面</h2><p id="fc43" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">从<a class="ae kv" href="https://hub.docker.com/editions/community/docker-ce-desktop-windows/" rel="noopener ugc nofollow" target="_blank"> DockerHub </a>下载Docker Desktop，并按照安装说明进行操作。确保在安装过程中选择WSL2。</p><h2 id="2119" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤3:启用GPU加速支持</h2><p id="1602" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">为了用我们的Nvidia GPU加速我们的机器学习代码，我们必须安装CUDA框架。因此，下载并安装支持WLS 的<a class="ae kv" href="https://www.nvidia.com/download/index.aspx" rel="noopener ugc nofollow" target="_blank"> Nvidia CUDA驱动程序。安装CUDA驱动程序后，打开Ubuntu子系统，按照Nvidia </a>的<a class="ae kv" href="https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch04-sub02-install-nvidia-docker" rel="noopener ugc nofollow" target="_blank">安装指南安装Nvidia容器工具包。为了更好的可读性，我复制了文档中的三个步骤。</a></p><ol class=""><li id="6457" class="mt mu iq ky b kz la lc ld lf no lj np ln nq lr na nb nc nd bi translated">设置稳定的存储库和GPG键。支持WSL 2的运行时更改可在稳定的存储库中获得:</li></ol><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="b21c" class="ny mc iq ol b gy op oq l or os">$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)</span><span id="6241" class="ny mc iq ol b gy ow oq l or os">$ curl -s -L <a class="ae kv" href="https://nvidia.github.io/nvidia-docker/gpgkey" rel="noopener ugc nofollow" target="_blank">https://nvidia.github.io/nvidia-docker/gpgkey</a> | sudo apt-key add -</span><span id="337a" class="ny mc iq ol b gy ow oq l or os">$ curl -s -L <a class="ae kv" href="https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list" rel="noopener ugc nofollow" target="_blank">https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list</a> | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span></pre><p id="7bc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.更新软件包列表后，安装NVIDIA运行时软件包(及其依赖项):</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="78f7" class="ny mc iq ol b gy op oq l or os">$ sudo apt-get update</span><span id="d29f" class="ny mc iq ol b gy ow oq l or os">$ sudo apt-get install -y nvidia-docker2</span></pre><p id="309d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.打开一个单独的WSL 2窗口，并使用以下命令重新启动Docker守护程序以完成安装:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="59a4" class="ny mc iq ol b gy op oq l or os">$ sudo service docker stop</span><span id="62b0" class="ny mc iq ol b gy ow oq l or os">$ sudo service docker start</span></pre><p id="4e25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！现在，您可以充分利用Docker容器中的GPU。要测试安装是否成功，您可以运行一个GPU基准测试容器。打开Docker桌面，然后打开PowerShell来执行</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="972e" class="ny mc iq ol b gy op oq l or os">PS&gt; docker run --gpus all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark</span></pre><p id="7e80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您现在应该会看到与我的结果类似的内容，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/c9d455694a6310179d39b65943d2346f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-ceYnrm_3i4-PGU-NJNJQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GPU基准测试容器的结果</p></figure><h2 id="3fd5" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">第4步:拉张量流Docker图像</h2><p id="eeda" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">TensorFlow提供现成的用户Docker映像，用于配置包含运行TensorFlow所需的所有包的容器。可以在<a class="ae kv" href="https://www.tensorflow.org/install/docker" rel="noopener ugc nofollow" target="_blank">这里</a>找到可能配置的完整概述。在这里，我们正在测试最新的稳定TensorFlow构建与GPU支持，包括一个Jupyter笔记本电脑服务器。确保docker正在运行，并在PowerShell中执行以下命令来提取所述Docker映像:</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="a7c2" class="ny mc iq ol b gy op oq l or os">PS&gt; docker pull tensorflow/tensorflow:latest-gpu-jupyter</span></pre><p id="a754" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这最后一步，我们现在可以运行我们的笔记本了！细节在下一节。</p><h1 id="45be" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">3.运行您的第一台笔记本电脑</h1><p id="776f" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">在本部分教程中，我们将:</p><ol class=""><li id="15f3" class="mt mu iq ky b kz la lc ld lf no lj np ln nq lr na nb nc nd bi translated">使用所需的配置启动我们支持GPU的TensorFlow容器，</li><li id="6416" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">启动Jupyter笔记本服务器，</li><li id="bf56" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">将Google Colab连接到我们的本地运行时。</li></ol><blockquote class="nr ns nt"><p id="9307" class="kw kx nu ky b kz la jr lb lc ld ju le nv lg lh li nw lk ll lm nx lo lp lq lr ij bi translated"><strong class="ky ir">注:</strong>如果你手头没有笔记本，<a class="ae kv" href="https://github.com/sascha-kirch/ML_Notebooks/blob/7591d0c595620a28ccccc028618619815fd0d607/GAN_distributed_training.ipynb" rel="noopener ugc nofollow" target="_blank">看看这个笔记本</a>，我在里面实现了一个简单的生成对抗网络。</p></blockquote><h2 id="6cf8" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤1:启动支持GPU的TensorFlow容器</h2><p id="e032" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">首先，我们确保docker正在运行，并在PowerShell中执行下面的命令来创建一个新容器。</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="4fe4" class="ny mc iq ol b gy op oq l or os">PS&gt; docker run --gpus all -p 8888:8888 -it --rm tensorflow/tensorflow:latest-gpu-jupyter bash</span></pre><p id="b458" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">配置:</strong></p><ul class=""><li id="3897" class="mt mu iq ky b kz la lc ld lf no lj np ln nq lr oy nb nc nd bi translated"><strong class="ky ir">运行</strong>:运行一个新的容器</li><li id="6efd" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr oy nb nc nd bi translated"><strong class="ky ir"> —所有GPU</strong>:使用所有可用的GPU</li><li id="2305" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr oy nb nc nd bi translated"><strong class="ky ir"> -p 8888:8888 </strong>:向主机发布容器的端口8888:8888</li><li id="5778" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr oy nb nc nd bi translated"><strong class="ky ir"> -it </strong>:保持STDIN打开，分配一个伪tty。</li><li id="af44" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr oy nb nc nd bi translated"><strong class="ky ir">—RM tensor flow/tensor flow:latest-GPU-jupyter</strong>:如果容器存在，自动移除容器</li><li id="6f87" class="mt mu iq ky b kz ne lc nf lf ng lj nh ln ni lr oy nb nc nd bi translated"><strong class="ky ir"> bash </strong>:打开容器的bash，从容器内部执行命令</li></ul><p id="bd75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着上面命令的执行，从TensorFlow图像创建容器，bash命令行打开:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/ef4127af0225ba5ab7aea2117465c0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ivEee7tGSba0SxhCbw8iYA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GPU支持的TensorFlow容器和Jupyter笔记本服务器</p></figure><h2 id="639b" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤2:启动Jupyter笔记本服务器</h2><p id="b78b" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">我们现在在容器内部，可以访问bash。现在我们必须手动启动Jupyter笔记本服务器，主要是因为我们必须允许Google Colab作为origin。</p><pre class="kg kh ki kj gt ok ol om on aw oo bi"><span id="69a3" class="ny mc iq ol b gy op oq l or os">$ jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0</span></pre><p id="ec48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该命令启动本地主机上的服务器，并在会话期间生成一个秘密访问令牌。下图突出显示了这个令牌。它需要从Google Colab内部连接到本地运行时</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/1da888809f2aeaa40a430e1db48ce8e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lbra930FTTJMlhKcNB78Jw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">生成访问令牌以连接到本地Jupyter笔记本运行时</p></figure><h2 id="a521" class="ny mc iq bd md nz oa dn mh ob oc dp ml lf od oe mn lj of og mp ln oh oi mr oj bi translated">步骤3:将Google Colab连接到我们的本地运行时</h2><p id="3348" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">首先在Google Colab中打开一个笔记本。</p><blockquote class="nr ns nt"><p id="401b" class="kw kx nu ky b kz la jr lb lc ld ju le nv lg lh li nw lk ll lm nx lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>如果你手头没有笔记本，<a class="ae kv" href="https://github.com/sascha-kirch/ML_Notebooks/blob/7591d0c595620a28ccccc028618619815fd0d607/GAN_distributed_training.ipynb" rel="noopener ugc nofollow" target="_blank">看看这个笔记本</a>，我在里面实现了一个简单的生成对抗网络。</p></blockquote><p id="a5cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在笔记本中，导航到右上角，然后按“connect(连接)”按钮旁边的小箭头。选择“连接到本地运行时”,如下图所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/40442465632baa50f64e96d911e65f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dp55XtfYJ8NF784QNq_0VQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">连接到本地运行时的导航</p></figure><p id="55fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将弹出一个新窗口，您可以在其中粘贴上一步中突出显示的令牌。确保将127.0.0.1改为localhost，因为Google Colab要求这种格式。现在只需按下连接，你应该完成了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/2199402a00e16a618cd1fc4cc0ad7f32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*URWftSuo0qA6IdMgtX3k9A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">本地连接设置</p></figure><p id="fad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您可以从笔记本电脑中查看所有可用设备的设备列表。</p><blockquote class="nr ns nt"><p id="4f14" class="kw kx nu ky b kz la jr lb lc ld ju le nv lg lh li nw lk ll lm nx lo lp lq lr ij bi translated"><strong class="ky ir">注:</strong>在我的深度学习包<a class="ae kv" href="https://github.com/sascha-kirch/DeepSaki" rel="noopener ugc nofollow" target="_blank">Github上的DeepSaki</a>中，你可以在许多有用的模块中找到与硬件检测和性能提升相关的助手，用于你的GPU训练。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/2c1212fc95c8d2061b18a287f334b419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*611J3CQ_ey7W895S8sDCag.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">本地机器上所有可用设备的列表</p></figure><p id="e818" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上面的截图所示，检测到一个GPU，你可以看到它是我的Nvidia GeForce RTX 2060。</p><h1 id="e936" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">4.结论</h1><p id="3e76" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">Docker可以在隔离的容器中运行应用程序，并提供执行应用程序所需的一切。如果你想从容器内访问你的GPU，就需要Nvidia的CUDA工具包。这允许提取包含Jupyter笔记本服务器的GPU支持的TensorFlow图像。我们可以配置Google Colab连接到这个本地运行时，并充分利用我们的GPU。</p><p id="02e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想用你的GPU进一步加速你的训练，我推荐你阅读我的关于如何<a class="ae kv" href="https://medium.com/@SaschaKirch/speed-up-your-tensorflow-training-with-mixed-precision-on-gpu-tpu-acf4c8c0931c" rel="noopener">在GPU/TPU上用混合精度加速你的TensorFlow训练的文章</a>。</p></div></div>    
</body>
</html>