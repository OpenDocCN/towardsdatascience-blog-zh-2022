<html>
<head>
<title>Practical Guide to Transfer Learning in TensorFlow for Multiclass Image Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于多类图像分类的 TensorFlow 迁移学习实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-guide-to-transfer-learning-in-tensorflow-for-multiclass-image-classification-d35fab7b28c0#2022-12-27">https://towardsdatascience.com/practical-guide-to-transfer-learning-in-tensorflow-for-multiclass-image-classification-d35fab7b28c0#2022-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="75f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在图像分类中实现迁移学习的详细讲解的分步教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9ce2e8d85a488cf6721876a98aa57f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWRMr8uUD1eKWjr_TNFDJQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@fanfandyuen?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Jason Yuen </a>在<a class="ae ky" href="https://unsplash.com/photos/0AzXWTUMS-w?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b758" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常我们无法获得大量的标记数据或计算能力来从头构建图像分类深度学习模型。</p><p id="eef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，迁移学习使我们能够为我们的特定分类任务开发鲁棒的图像分类器，即使我们的资源有限。</p><p id="2b1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个简单易懂的演练中，我们将了解如何利用预先训练的模型作为 TensorFlow 中迁移学习的一部分来有效和高效地对图像进行分类。</p><h2 id="e5e7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">目录</h2><blockquote class="mo mp mq"><p id="78d1" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated"><strong class="lb iu"><em class="it">(1)</em></strong><em class="it"/><a class="ae ky" href="#8e30" rel="noopener ugc nofollow"><em class="it">迁移学习的动机和好处</em> </a> <em class="it"> </em>(可选)<em class="it"><br/></em><strong class="lb iu"><em class="it">(2)</em></strong><em class="it"/><a class="ae ky" href="#c9fa" rel="noopener ugc nofollow"><em class="it">关于数据集</em></a><em class="it"><br/></em><strong class="lb iu"><em class="it">(3)</em></strong><a class="ae ky" href="#941f" rel="noopener ugc nofollow"><em class="it">步骤</em></a></p></blockquote><p id="f19f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文附带的 GitHub repo 可以在<a class="ae ky" href="https://github.com/kennethleungty/TensorFlow-Transfer-Learning-Image-Classification" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="8e30" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(1)迁移学习的动机和好处</h1><blockquote class="mo mp mq"><p id="d926" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">可选阅读</p></blockquote><p id="1b07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迁移学习是一种强大的方法，它使用预先训练的模型作为创建新模型的起点，而不是从头开始构建新模型。</p><p id="fa32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些预先训练好的模型由学术或大型技术研究人员建立，他们开发新的深度学习模型架构，并在强大计算能力的帮助下在大型数据集上训练它们。</p><p id="7e0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几个重要的好处推动了迁移学习的流行:</p><ul class=""><li id="6645" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">节省大量时间和资源</strong>因为从零开始训练深度学习模型是耗时和资源密集型的</li><li id="6f99" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">更好的性能</strong>因为预训练模型在对大型数据集进行大量训练后已经了解了重要的特征</li><li id="4067" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">需要更少的标记数据</strong>，因为我们可以利用预训练模型所拥有的“知识”的坚实基础</li><li id="0a9f" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">减少过度拟合</strong>，因为从头构建的模型可能对较小训练数据集的特定特征过于敏感</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/715b630127986f07d4d5906885d29276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9AroXpbL1DfHqn5-biqvg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@timmossholder?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">蒂姆·莫斯霍尔德</a>在<a class="ae ky" href="https://unsplash.com/photos/WE_Kv_ZB1l0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="c9fa" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(2)关于数据集</h1><p id="2fee" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们将使用<a class="ae ky" href="https://www.robots.ox.ac.uk/~vgg/data/pets/" rel="noopener ugc nofollow" target="_blank">牛津-IIIT pet 数据集</a>来完成这个图像分类任务。这是一个 37 类 pet 图像数据集，每类约 200 张图像。</p><p id="98b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集非常适合本教程，因为:</p><ul class=""><li id="1d65" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">它相对较小(总共约 800Mb)</li><li id="b775" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">在<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank">知识共享署名-类似共享许可</a>下，它可用于商业和研究目的</li><li id="455b" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">大量的类是测试多类图像分类的迁移学习的一个很好的方法</li><li id="8ac7" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">它是<code class="fe og oh oi oj b">tensorflow_datasets</code> ( <code class="fe og oh oi oj b"><a class="ae ky" href="https://www.tensorflow.org/datasets/api_docs/python/tfds" rel="noopener ugc nofollow" target="_blank">tfds</a></code>)中许多现成可用的数据集之一(我们将在本教程中直接加载)</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/7edb672226a16b96a61c9c7b14851228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-z_k5ycKmpXxZ6c9QqWwZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">牛津-IIIT pet 数据集样本|作者图片</p></figure><p id="25f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下注释可用于每个图像:</p><ul class=""><li id="8279" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated">物种(猫或狗)和品种名称</li><li id="67bd" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">动物头部周围的紧密包围盒</li><li id="55ce" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">像素级前景-背景分割</li></ul><p id="b0e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程将重点关注<strong class="lb iu">品种名称</strong>作为我们要预测的标签。</p><p id="1051" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mr">如果我们希望获得 TensorFlow 之外的数据，我们可以执行以下任一操作:</em></p><ul class=""><li id="5fd5" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><a class="ae ky" href="https://academictorrents.com/details/b18bbd9ba03d50b0f7f479acc9f4228a408cecc1" rel="noopener ugc nofollow" target="_blank">T21【BitTorrent】与学术洪流 T23】</a></li><li id="2b27" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><em class="mr">直接下载</em> <a class="ae ky" href="https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz" rel="noopener ugc nofollow" target="_blank"> <em class="mr">数据集【images.tar.gz】</em></a><em class="mr"/><a class="ae ky" href="https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz" rel="noopener ugc nofollow" target="_blank"><em class="mr">【annotations.tar.gz】</em></a></li></ul></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="941f" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(3)逐步指导</h1><blockquote class="mo mp mq"><p id="d873" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated"><em class="it">查看</em> <a class="ae ky" href="https://github.com/kennethleungty/TensorFlow-Transfer-Learning-Image-Classification/blob/main/notebooks/TensorFlow%20Tutorial%20-%20Image%20Classification%20on%20Oxford-IIIT%20Pets%20Dataset.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="it">已完成的笔记本</em> </a> <em class="it">以跟随本演练。</em></p></blockquote><h1 id="82b6" class="nc lw it bd lx nd ol nf ma ng om ni md jz on ka mg kc oo kd mj kf op kg mm nm bi translated">步骤 1 —初始设置</h1><p id="6128" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">在本教程中，我们将使用<a class="ae ky" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>,因为它允许我们免费访问 GPU，并且默认环境具有必要的 Python 依赖性。</p><p id="6043" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然大量的计算能力是不必要的，但在 GPU 上运行迁移学习仍然至关重要，因为我们正在处理深度学习模型。</p><blockquote class="mo mp mq"><p id="02b3" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">重要</em> </strong> <em class="it">:记得从顶部菜单栏将</em> <strong class="lb iu"> <em class="it">硬件加速器</em> </strong> <em class="it">切换到</em><strong class="lb iu"><em class="it">GPU</em></strong><em class="it">:运行时&gt;改变运行时类型。</em></p></blockquote><p id="9802" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在发布<a class="ae ky" href="https://colab.research.google.com/notebooks/empty.ipynb" rel="noopener ugc nofollow" target="_blank">新的 Colab 笔记本</a>时，我们导入以下包:</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="7168" class="ou lw it oj b be ov ow l ox oy">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/><br/>import tensorflow as tf<br/>import tensorflow_datasets as tfds<br/>from tensorflow import keras<br/>from tensorflow.keras import layers<br/>from keras import callbacks</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="2ee6" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 2-通过训练-验证-测试分割加载数据</h1><p id="a7f8" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">如前所述，我们将使用 TensorFlow 数据集组件<code class="fe og oh oi oj b">tfds</code>来加载图像和相应的元数据。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="25de" class="ou lw it oj b be ov ow l ox oy">(train_raw, val_raw, test_raw), ds_info = tfds.load(<br/>                                          name='oxford_iiit_pet',<br/>                                          split=['train[:90%]', <br/>                                                'train[90%:]', <br/>                                                'test'], <br/>                                          shuffle_files=True,<br/>                                          as_supervised=True,<br/>                                          with_info=True<br/>                                          )</span></pre><p id="81dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe og oh oi oj b">tfds.load</code>函数自动从数据源下载数据，并将其作为一个<code class="fe og oh oi oj b">tf.data.Dataset</code>对象返回。</p><ul class=""><li id="1426" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">name</code>:tensor flow 数据集集合中的数据集名称。在这种情况下，它被命名为<code class="fe og oh oi oj b">oxford_iiit_pet</code>。</li><li id="55f2" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">split</code>:指定数据分割的参数。由于原始数据中只有两个分裂(训练和测试)，我们将划分 10%的训练集来创建一个新的验证集</li><li id="4571" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">shuffle_files</code>:如果设置为真，输入数据的顺序会被打乱(以促进更好的模型泛化)</li><li id="afc0" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">as_supervised</code>:如果设置为 True，返回的<code class="fe og oh oi oj b">tf.data.Dataset</code>对象将有一个元组结构<em class="mr">(图片，标签)</em>，其中标签指的是图片中的宠物品种。</li><li id="9e98" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">with_info</code>:如果设置为 True，元数据将作为<code class="fe og oh oi oj b">DatasetInfo</code>对象与图像数据集对象一起返回</li></ul><p id="6acb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们设置了<code class="fe og oh oi oj b">with_info=True</code>，我们将获得两个输出对象:</p><ol class=""><li id="1cb2" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu oz nt nu nv bi translated">基于三个拆分的一组三个数据集对象(<code class="fe og oh oi oj b">train_raw</code>、<code class="fe og oh oi oj b">val_raw</code>和<code class="fe og oh oi oj b">test_raw</code>)</li><li id="545e" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu oz nt nu nv bi translated">包含数据集元数据的<code class="fe og oh oi oj b">DatasetInfo</code>对象(<code class="fe og oh oi oj b">ds_info</code></li></ol><blockquote class="mo mp mq"><p id="8510" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">如果我们有想要加载到<code class="fe og oh oi oj b">tfds</code>的自定义数据集(例如 CSV 文件)，我们可以在这里和这里<a class="ae ky" href="https://www.tensorflow.org/datasets/add_dataset" rel="noopener ugc nofollow" target="_blank">参考文档</a><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="5ff7" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 3-浏览数据</h1><p id="352e" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">有多种方法可以更好地理解数据集。</p><h2 id="2c00" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈠元数据</h2><p id="7747" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">如果可以的话，我们可以通过在一个单元格中运行它来仔细查看存储在<code class="fe og oh oi oj b">ds_info</code>中的元数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/67f854b89b06d59b3fe38e2c22b092c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glQT0gvu0jELPBdT6HvPOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">牛津-IIIT Pet 数据集的数据集信息(也称为元数据)|作者图片</p></figure><p id="5c1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出中，我们可以发现诸如图像特征的类型、类的数量和分割的大小等信息。</p><p id="ed65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要直接获得类的数量和分割大小，我们可以运行以下代码:</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="6973" class="ou lw it oj b be ov ow l ox oy"># Get number of classes<br/>num_classes = ds_info.features['label'].num_classes<br/>print('Number of classes:', num_classes)<br/><br/># Get split sizes (aka cardinality)<br/>num_train_examples = tf.data.experimental.cardinality(train_raw).numpy()<br/>num_val_examples = tf.data.experimental.cardinality(val_raw).numpy()<br/>num_test_examples = tf.data.experimental.cardinality(test_raw).numpy()<br/><br/>print('Number of training samples:', num_train_examples)<br/>print('Number of validation samples:', num_val_examples)<br/>print('Number of test samples:', num_test_examples)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="f0ae" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈡获得价值计数</h2><p id="8785" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">理解数据的一个快速方法是检查图像标签的分布。我们可以通过自定义的值计数函数来实现:</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="2e80" class="ou lw it oj b be ov ow l ox oy">def get_value_counts(ds):<br/>  label_list = []<br/>  for images, labels in ds: <br/>    label_list.append(labels.numpy()) # Convert tensor to numpy array<br/>  label_counts = pd.Series(label_list).value_counts(sort=True)<br/><br/>  print(label_counts)</span></pre><p id="c9d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们在训练集上运行它，我们可以看到图像相对均匀地分布在每个类中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/4f2c781e85bdf5fd2d51c78f355dedce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*X-1mPozpOcLtSP7ib9mEGg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练集中标签的值计数|按作者排序的图像</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="ce02" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(三)图像示例</h2><p id="f404" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">鉴于我们正在处理图像，直接将它们可视化总是一个好主意。</p><p id="bc3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用下面的自定义函数显示一个随机图像及其相应的标签，包括将标签从整数形式转换为实际的品种名称。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="51d9" class="ou lw it oj b be ov ow l ox oy"># Obtain name for label integer<br/>get_label_name = ds_info.features['label'].int2str<br/><br/>def view_single_image(ds):<br/>  image, label = next(iter(ds)) # Get next image (random)<br/>  print('Image shape: ', image.shape) # Get shape of image<br/>  plt.imshow(image)<br/>  _ = plt.title(get_label_name(label))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/1c5f09908dd42705263c367a58d78f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-eFMXhRymSxnI3gBba1E7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自数据集的单个随机图像|作者提供的图像</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="0683" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内置的<code class="fe og oh oi oj b">show_examples</code>方法也允许我们可视化图像的随机子集。</p><p id="78d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个特定的数据集，我们需要指定<code class="fe og oh oi oj b">image_key='image'</code>来检索实际的图像，因为存在多个图像特征。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="e617" class="ou lw it oj b be ov ow l ox oy">tfds.show_examples(train_raw, ds_info, image_key='image')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/446f058ad0b5b872eef018ef86376134.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lG-BmhADCuGHENoZ83w84g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自 tdfs.show_examples|作者图片的随机子集</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="5ec5" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 4 —准备数据</h1><p id="c842" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">下一步至关重要，因为它涉及在迁移学习发生之前将图像预处理成适当的格式。</p><h2 id="ac13" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(一)图像尺寸调整</h2><p id="34ed" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">深度学习模型期望输入图像具有相同的形状，以实现一致性、兼容性和训练效率。</p><p id="f1c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于我们的原始图像有不同的大小，我们需要调整它们的大小，以具有相同的长度和宽度。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="af6e" class="ou lw it oj b be ov ow l ox oy">IMG_SIZE = 224<br/><br/>train_ds = train_raw.map(lambda x, y: (tf.image.resize(x, (IMG_SIZE, IMG_SIZE)), y))<br/>val_ds = val_raw.map(lambda x, y: (tf.image.resize(x, (IMG_SIZE, IMG_SIZE)), y))<br/>test_ds = test_raw.map(lambda x, y: (tf.image.resize(x, (IMG_SIZE, IMG_SIZE)), y))</span></pre><p id="cfb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择 224x224 的调整后分辨率是因为我们稍后将使用的预训练模型期望输入图像是这种特定的形状。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="6d25" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(二)标签的一次性编码</h2><p id="b93d" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们的数据集有 37 个用于多类图像分类的类。因此，我们对标签进行一次热编码，以获得每个类的长度为 37 的输出张量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/74aa73db05a627ff80a2f3479f2e54fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l70KV42riXt03-r_Jfc-PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于类别数量的一键编码输出张量图解|作者图片</p></figure><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="d763" class="ou lw it oj b be ov ow l ox oy">def one_hot_encode(image, label):<br/>    label = tf.one_hot(label, num_classes)<br/>    return image, label<br/><br/>train_ds = train_ds.map(one_hot_encode)<br/>val_ds = val_ds.map(one_hot_encode)<br/>test_ds = test_ds.map(one_hot_encode)</span></pre><p id="7566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一步很重要，因为我们将使用分类准确性来衡量模型性能，计算预测与这些实际的<strong class="lb iu">一次性标签</strong>匹配的频率。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="3684" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈢图像增强</h2><p id="5f18" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">虽然迁移学习减少了所需的数据量，但良好的性能仍然需要足够高质量、数量和种类的数据集。</p><p id="9fb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像增强是一种通过生成原始图像的修改副本来人为增加训练集大小的技术。</p><p id="abe2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了执行增强，我们使用 Keras 预处理层 API。我们想要应用的每种图像增强都被定义为 Keras 序列类中的一个<strong class="lb iu">层</strong>。</p><p id="fc38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为简单起见，我们将仅对图像进行随机水平翻转，但请注意，大范围的增强<a class="ae ky" href="https://keras.io/api/layers/preprocessing_layers/image_augmentation/" rel="noopener ugc nofollow" target="_blank">是可用的。</a></p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="adc0" class="ou lw it oj b be ov ow l ox oy">data_augmentation = keras.Sequential(<br/>                [layers.RandomFlip('horizontal'), <br/>                #  layers.RandomRotation(factor=(-0.025, 0.025)),<br/>                #  layers.RandomTranslation(height_factor=0.1, width_factor=0.1),<br/>                #  layers.RandomContrast(factor=0.1),<br/>                ])</span></pre><p id="b106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以用下面的代码来查看增强的效果:</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="ae18" class="ou lw it oj b be ov ow l ox oy">for image, label in train_ds.take(1): # Iterate and get set of image and label from train_ds generator<br/>    plt.figure(figsize=(10, 10))<br/>    for i in range(4):  # Display augmented images in 2x2 grid<br/>        ax = plt.subplot(2, 2, i+1)<br/>        aug_img = data_augmentation(tf.expand_dims(image, axis=0))<br/>        plt.imshow(aug_img[0].numpy().astype('uint8')) # Retrieve raw pixel value<br/>        plt.title(get_label_name(int(label[0]))) # Get corresponding label<br/>        plt.axis('off')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/418e41d9ba7747dbcc03eecc1db328b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8bzUUWvzH8bGYWdm2kzLBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">说明水平翻转增强的影响|作者图片</p></figure><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/top-python-libraries-for-image-augmentation-in-computer-vision-2566bed0533e"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">用于计算机视觉中图像增强的顶级 Python 库</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">为您的下一个计算机视觉项目提供最好的增强库(带有示例代码)</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ks pj"/></div></div></a></div></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="4004" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(四)批处理和预取</h2><p id="0f87" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">下一步是设置批处理和预取，这是优化模型训练效率的技术。</p><ul class=""><li id="dfa6" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">批处理</strong>将训练数据的子集分组为小批，以便训练可以并行化(利用 GPU 硬件加速)，同时保持准确的梯度估计。</li><li id="c692" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">预取</strong>将模型计算与输入数据的获取重叠，允许在检索下一批图像时继续训练。</li></ul><p id="0ca1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然批量大小是一个可以调整的超参数，但我们可以先将其设置为标准默认值<strong class="lb iu"> 32 </strong>。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="ebc1" class="ou lw it oj b be ov ow l ox oy">BATCH_SIZE = 32<br/><br/>train_ds = train_ds.batch(batch_size=BATCH_SIZE, <br/>                          drop_remainder=True).prefetch(tf.data.AUTOTUNE)<br/><br/>val_ds = val_ds.batch(batch_size=BATCH_SIZE, <br/>                      drop_remainder=True).prefetch(tf.data.AUTOTUNE)<br/><br/>test_ds = test_ds.batch(batch_size=BATCH_SIZE, <br/>                        drop_remainder=True).prefetch(tf.data.AUTOTUNE)</span></pre><p id="5f35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看上面批处理/预取代码的参数:</p><ul class=""><li id="35c3" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">drop_remainder=True</code>表示最后一批小于 32 的图像(由于分割不均)被丢弃。当模型依赖于具有相同外部尺寸的批次时，这很有用</li><li id="47ba" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">prefetch()</code>方法中的<code class="fe og oh oi oj b">tf.data.AUTOTUNE</code>意味着<code class="fe og oh oi oj b"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/data" rel="noopener ugc nofollow" target="_blank">tf.data</a></code>运行时将动态调整每个训练步骤要预取的元素数量</li></ul></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="9fd5" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 5 —建立模型</h1><p id="aa27" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">迁移学习背后的核心概念是利用预先训练的模型作为构建定制图像分类器的基础。</p><p id="5924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Keras 中提供了许多预训练的深度学习模型(以及预训练的权重)，完整的列表可以在这里找到<a class="ae ky" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="43fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于本教程，我们将使用<a class="ae ky" href="https://keras.io/api/applications/resnet/#resnet50v2-function" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> ResNet50V2 </strong> </a>模型，因为它提供了模型大小、准确性、信誉和推理速度的完美平衡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/0823d050812ee85ece4a83654b157d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*COo42kMJxaRx4v0LrW7JDA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ResNet50 模型架构图|根据<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:ResNet50.png" rel="noopener ugc nofollow" target="_blank">维基共享许可</a>使用的图像</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="1bad" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(一)设置基础模型</h2><p id="cc1e" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们首先使用<code class="fe og oh oi oj b">keras.applications.ResNet50V2</code>从 Keras 应用程序加载 ResNet50V2 模型。</p><p id="2225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，ResNet50V2 预训练模型是在<a class="ae ky" href="https://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>数据集上训练的，这是一个包含数百万张图像和数千个类的大规模图像数据集。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="ac6b" class="ou lw it oj b be ov ow l ox oy">base_model = keras.applications.ResNet50V2(<br/>                        include_top=False, # Exclude ImageNet classifier at the top<br/>                        weights='imagenet',<br/>                        input_shape=(IMG_SIZE, IMG_SIZE, 3)<br/>                        )</span></pre><p id="c2d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是对所用参数的解释:</p><ul class=""><li id="ff22" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">include_top</code>:如果设置为 False，我们排除 ImageNet 分类器作为顶层。这很重要，因为我们想引入自己的顶层来分类宠物品种，而不是最初的 ImageNet 类</li><li id="6fd0" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">weights</code>:通过将其设置为<code class="fe og oh oi oj b">'imagenet'</code>，我们将使用在 ImageNet 上训练的预训练重量。这正是我们想要的，因为我们正在执行图像分类，从 ImageNet 学到的功能是相关的</li><li id="8e4b" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><code class="fe og oh oi oj b">input_shape</code>:我们设置输入形状元组来匹配 ResNet 架构需要的，即<code class="fe og oh oi oj b">(224, 224, 3)</code>(其中<code class="fe og oh oi oj b">3</code>的最后一个元素代表颜色通道的数量)</li></ul></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="a0b5" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(ii)冻结基础模型的预训练权重</h2><p id="629a" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">一旦我们加载了预训练的模型，我们想要<strong class="lb iu">固定</strong> ( <strong class="lb iu">又名冻结</strong> ) <strong class="lb iu"> </strong>层和权重，因为我们不想丢失已经学习的有价值的信息。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="1833" class="ou lw it oj b be ov ow l ox oy"># Freeze base_model<br/>base_model.trainable = False</span></pre><p id="ccbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过将<code class="fe og oh oi oj b">trainable</code>属性设置为<code class="fe og oh oi oj b">False</code>，可训练的重量变为不可训练，并且在训练过程中不会更新。一般来说，各层的所有重量都被认为是可训练的。</p><p id="a9a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，<strong class="lb iu">唯一具有不可训练权重</strong>的内置层是<code class="fe og oh oi oj b"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" rel="noopener ugc nofollow" target="_blank">BatchNormalization</a></code>，因为它使用不可训练权重来跟踪训练期间输入的均值和方差。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="f43e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈢实例化和修改输入</h2><p id="d08e" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">在此步骤中，我们对输入(即图像)进行预处理，以符合预训练的 ResNet50v2 架构的预期。</p><p id="9bc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先使用<code class="fe og oh oi oj b">keras.Input</code>实例化一个 Keras 张量来表示输入图像的“对象结构”。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="e73d" class="ou lw it oj b be ov ow l ox oy"># Setup inputs based on input image shape<br/>inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))</span></pre><p id="5568" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们应用前面设置的数据扩充步骤，并将输出存储在一个表示新输入转换的新变量<code class="fe og oh oi oj b">x</code>中。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="d486" class="ou lw it oj b be ov ow l ox oy">x = data_augmentation(inputs)</span></pre><p id="789e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，Keras 中的每个预训练模型都需要特定类型的输入预处理，这是在早期数据准备之上的。</p><p id="0d78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过在将输入传递给模型之前对输入运行<code class="fe og oh oi oj b"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input" rel="noopener ugc nofollow" target="_blank">resnet_v2.preprocess_input</a></code>来做到这一点。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="2c14" class="ou lw it oj b be ov ow l ox oy"># Apply specific pre-processing function for ResNet v2<br/>x = keras.applications.resnet_v2.preprocess_input(x)</span></pre><p id="d7cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是对于 ResNet50 V2，预处理步骤将输入像素从范围[0，255]缩放到范围<strong class="lb iu">[-1，1]</strong>，以便输入与 ResNet 架构兼容。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="10c7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(iv)处理批量标准化层</h2><p id="b80c" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated"><code class="fe og oh oi oj b">BatchNormalization</code>层稍微有点棘手，因为它有不可训练的权重来跟踪输入的均值和方差。</p><p id="c4c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们稍后将解冻整个模型(在<strong class="lb iu">步骤 8 —微调模型</strong>中)，我们需要包含额外的代码来保持<code class="fe og oh oi oj b">BatchNormalization</code>层处于推理模式(即保持不可训练)。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="1124" class="ou lw it oj b be ov ow l ox oy"># Keep base model batch normalization layers in inference mode (instead of training mode)<br/>x = base_model(x, training=False)</span></pre><p id="4f0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，这一步是针对<code class="fe og oh oi oj b">BatchNormalization</code>层的，是在之前完成的基础模型冻结(<code class="fe og oh oi oj b">base_model.trainable=False</code>)之上。</p><p id="c5e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果这一步被省略，微调步骤将是一个烂摊子，因为不可训练的重量将被删除和撤消。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="cefd" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈤重建顶层</h2><p id="b3aa" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">现在我们已经整理了预训练模型的早期层，我们可以添加新的可训练层，以便我们的模型可以有效地学习分类我们想要的标签，即宠物品种。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="dd68" class="ou lw it oj b be ov ow l ox oy"># Rebuild top layers<br/>x = layers.GlobalAveragePooling2D()(x) # Average pooling operation<br/>x = layers.BatchNormalization()(x) # Introduce batch norm<br/>x = layers.Dropout(0.2)(x)  # Regularize with dropout<br/><br/># Flattening to final layer - Dense classifier with 37 units (multi-class classification)<br/>outputs = layers.Dense(num_classes, activation='softmax')(x)</span></pre><p id="b90a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新图层包括以下内容:</p><ul class=""><li id="7fc8" class="nn no it lb b lc ld lf lg li np lm nq lq nr lu ns nt nu nv bi translated"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D" rel="noopener ugc nofollow" target="_blank">全球平均池</a></li><li id="aab3" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" rel="noopener ugc nofollow" target="_blank">批量归一化</a></li><li id="63be" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout" rel="noopener ugc nofollow" target="_blank">辍学</a>(概率为 20%)</li><li id="1750" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated">最终输出<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank">密集层</a>激活 softmax，输出维数为 37(反映 37 类宠物品种)</li></ul><h2 id="45a5" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(vi)创建新的 Keras 模型对象</h2><p id="032a" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们用前面定义的更新的输入和输出实例化一个新的 Keras 模型。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="d0ef" class="ou lw it oj b be ov ow l ox oy"># Instantiate final Keras model with updated inputs and outputs<br/>model = keras.Model(inputs, outputs)</span></pre><p id="1b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用<code class="fe og oh oi oj b">model.summary()</code>查看我们更新后的模型的结构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/e1888f66c57dda00167e704e195c5724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XsMJ8gBHxd4eXiGszr0I4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型摘要|作者图片</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="b190" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈦编制模型</h2><p id="329e" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">最后，我们通过编译模型进行最后的润色，这涉及到配置评估指标和随机梯度下降优化器等设置。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="c90b" class="ou lw it oj b be ov ow l ox oy">model.compile(optimizer=keras.optimizers.Adam(),<br/>              loss=keras.losses.CategoricalCrossentropy(),<br/>              metrics=[keras.metrics.CategoricalAccuracy()]<br/>              )</span></pre><p id="b4d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们正在处理多类分类，所以使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">分类交叉熵</a>作为损失度量，使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy" rel="noopener ugc nofollow" target="_blank">分类准确度</a>作为性能度量是有意义的。</p><p id="ecb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还包括一个<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank">提前停止回调</a>以避免在训练期间过度适应。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="2bce" class="ou lw it oj b be ov ow l ox oy">earlystopping = callbacks.EarlyStopping(monitor='val_loss', <br/>                                        mode='min', <br/>                                        patience=5, <br/>                                        restore_best_weights=True)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="bed7" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 6-运行模型培训</h1><p id="4df7" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">随着我们模型的更新和编译，我们准备在 pet 图像数据集上训练它。</p><p id="c275" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们以 25 的历元计数开始，尽管当检测到过度拟合的迹象时，早期停止回调将有助于停止训练。</p><p id="7e3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们运行<code class="fe og oh oi oj b">model.fit()</code>来启动训练，并将训练输出存储在一个名为<code class="fe og oh oi oj b">history</code>的变量中。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="3d59" class="ou lw it oj b be ov ow l ox oy">EPOCHS = 25<br/><br/>history = model.fit(train_ds, <br/>                    epochs=EPOCHS, <br/>                    validation_data=val_ds, <br/>                    verbose=1,<br/>                    callbacks =[earlystopping])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/6c278d58991a7d028ac9f0f620b7a8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oa0YG2sJ4vvLqNw9BddYpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">历元训练输出|作者图片</p></figure></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="e355" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 7 —评估模型</h1><p id="d0f9" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">一旦培训完成，是时候了解我们的模型进展如何了。</p><h2 id="214b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">㈠绘图训练和验证集</h2><p id="7d6a" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">我们可以在增加的时期内，在训练集和验证集上绘制分类准确度。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="64a0" class="ou lw it oj b be ov ow l ox oy">def plot_hist(hist):<br/>    plt.plot(hist.history['categorical_accuracy'])<br/>    plt.plot(hist.history['val_categorical_accuracy'])<br/>    plt.title('Categorical accuracy')<br/>    plt.ylabel('accuracy')<br/>    plt.xlabel('epoch')<br/>    plt.legend(['train', 'validation'], loc='upper left')<br/>    plt.show()<br/><br/>plot_hist(history)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qb"><img src="../Images/c78c2ed40390c77e6bd3334dfbec87f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKO1Nk9KEuCxJEkJ4ddRcw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">各时期的训练和验证准确度分数|按作者分类的图像</p></figure><p id="ac1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从图中看到，只需要几个时期就可以过拟合训练数据集，并且在五个时期后验证精度没有提高。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="9275" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">(ii)对测试数据集进行预测和评估</h2><p id="42d0" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">为了评估测试集上的模型预测，我们使用<code class="fe og oh oi oj b">model.evaluate()</code>。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="48b3" class="ou lw it oj b be ov ow l ox oy">result = model.evaluate(test_ds)<br/>dict(zip(model.metrics_names, result))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qc"><img src="../Images/30da5c8c792726595758113a06538420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgCDi9a69lciPzzrdGi-fg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自模型|作者图片的评估结果</p></figure><p id="c011" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果表明，分类准确率达到了惊人的数值<strong class="lb iu"> 86.1% </strong>。</p><p id="8847" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此基础上，我们通过从 ResNet50 V2 预训练模型建立用于图像分类的深度学习模型，完成了迁移学习。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="f266" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">步骤 8 —微调模型(可选)</h1><p id="25c1" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">作为可选步骤，我们可以通过解冻和重新训练模型中的所有权重来进一步优化模型。</p><p id="4473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程被称为<strong class="lb iu">微调</strong>，我们解冻所有或部分早期预训练层，并重新训练新数据的权重。</p><p id="44fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这种技术可以提高性能，但它有可能很快过度拟合。</p><p id="074a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微调的一个关键部分是使用<strong class="lb iu">非常低的学习率</strong>(例如 0.00001)来重新训练模型。</p><p id="dedf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本原理是这些预先训练的层已经学习了用于图像分类的有价值的特征。因此，低学习率确保模型仅进行细微的参数调整，而没有显著的中断。</p><p id="eadc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，我们通过将<code class="fe og oh oi oj b">trainable</code>属性设置为<code class="fe og oh oi oj b">True</code>来解冻整个模型的顶部 15 层，排除了我们需要保持冻结的<code class="fe og oh oi oj b">BatchNormalization</code>层。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="9b6e" class="ou lw it oj b be ov ow l ox oy">for layer in model.layers[-15:]:<br/>    if not isinstance(layer, layers.BatchNormalization):<br/>        layer.trainable = True</span></pre><p id="9d0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这些变化之后，我们需要重新编译模型，在这里我们将学习率从默认的 0.001 大幅降低到<strong class="lb iu"> 0.00001 </strong>。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="9d87" class="ou lw it oj b be ov ow l ox oy">model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), # Set a very low learning rate<br/>              loss=keras.losses.CategoricalCrossentropy(),<br/>              metrics=[keras.metrics.CategoricalAccuracy()]<br/>              )</span></pre><p id="4383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们重新训练模型，并在最后的步骤中检索评估指标。考虑到过度拟合的风险，我们在更少的时期内运行训练。</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="d3a3" class="ou lw it oj b be ov ow l ox oy">EPOCHS = 5<br/><br/>history_2 = model.fit(train_ds, <br/>                      epochs=EPOCHS, <br/>                      validation_data=val_ds, <br/>                      verbose=1,<br/>                      callbacks =[earlystopping])<br/><br/>result_2 = model.evaluate(test_ds)<br/><br/>dict(zip(model.metrics_names, result_2))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/d74dd5d6c280f8da1d63b87726bacf9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QS48R-gPwWUQfGLaa60mXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">微调模型的评估结果|作者提供的图片</p></figure><p id="d744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的评估输出表明，微调方法有助于将分类准确率从<strong class="lb iu">的 86.1%提高到</strong>的 87.2%。</p><p id="2734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，为了检索测试集上的实际预测，我们可以使用<code class="fe og oh oi oj b">model.predict()</code>函数:</p><pre class="kj kk kl km gt oq oj or bn os ot bi"><span id="f895" class="ou lw it oj b be ov ow l ox oy">preds = model.predict(test_ds)</span></pre></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h1 id="0c76" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">(4)包装</h1><p id="78ce" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">本演练涵盖了开始学习用于图像分类的迁移学习的实际步骤。</p><p id="ea5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迁移学习，加上微调，是一种有效的技术，使我们能够在有限的时间和资源下建立鲁棒的图像分类器。</p><p id="56cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于迁移学习的实用性，它成为行业中解决业务问题的一种普遍技术也就不足为奇了。</p><p id="9fa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整笔记本的链接可以在<a class="ae ky" href="https://github.com/kennethleungty/TensorFlow-Transfer-Learning-Image-Classification/blob/main/notebooks/TensorFlow%20Tutorial%20-%20Image%20Classification%20on%20Oxford-IIIT%20Pets%20Dataset.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这里</strong> </a>找到。</p><h1 id="fffb" class="nc lw it bd lx nd ol nf ma ng om ni md jz on ka mg kc oo kd mj kf op kg mm nm bi translated">在你走之前</h1><p id="487b" class="pw-post-body-paragraph kz la it lb b lc ob ju le lf oc jx lh li od lk ll lm oe lo lp lq of ls lt lu im bi translated">欢迎您<strong class="lb iu">加入我的数据科学学习之旅！</strong>点击此<a class="ae ky" href="https://kennethleungty.medium.com/" rel="noopener">媒体</a>页面，查看我的<a class="ae ky" href="https://github.com/kennethleungty" rel="noopener ugc nofollow" target="_blank"> GitHub </a>，了解更多令人兴奋的实用数据科学内容。同时，享受在 TensorFlow 中实现图像分类迁移学习的乐趣！</p><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">PyTorch Ignite 教程—使用高效网络对微型图像网络进行分类</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">使用 PyTorch Ignite 简化 PyTorch 深度学习实施的分步指南</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qe l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/micro-macro-weighted-averages-of-f1-score-clearly-explained-b603420b292f"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">F1 分数的微观、宏观和加权平均值，解释清楚</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">理解多类分类中 F1 分数的微观平均值、宏观平均值和加权平均值背后的概念</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qf l pu pv pw ps px ks pj"/></div></div></a></div><div class="pg ph gp gr pi pj"><a rel="noopener follow" target="_blank" href="/how-to-easily-draw-neural-network-architecture-diagrams-a6b6138ed875"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd iu gy z fp po fr fs pp fu fw is bi translated">如何轻松绘制神经网络架构图</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">使用无代码工具通过图表可视化展示深度学习模型</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ps l"><div class="qg l pu pv pw ps px ks pj"/></div></div></a></div></div></div>    
</body>
</html>