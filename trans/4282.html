<html>
<head>
<title>Bayesian Regression From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯回归从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-regression-from-scratch-a1fe19ff64c#2022-09-22">https://towardsdatascience.com/bayesian-regression-from-scratch-a1fe19ff64c#2022-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0015" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python从基本原理导出贝叶斯线性回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49bb02b1c8963841db10b4a6397a0dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KKon1leXCIz1Ha--"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克利姆·穆萨利莫夫在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="095c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="8245" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">线性回归</strong> </a>是数据科学中最知名的算法，然而它有不止一个版本。大多数人使用的版本来自统计学家<a class="ae ky" href="https://en.wikipedia.org/wiki/Frequentist_inference" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>的解释，但还有一个版本来自贝叶斯<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_statistics" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"/></a>学派。</p><p id="21fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我们将复习<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayes'_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯定理</strong> </a>，频数统计和贝叶斯统计的区别，最后用Python从头开始实现<a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_linear_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯线性回归</strong> </a>。</p><blockquote class="ms mt mu"><p id="faaa" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">注意:在整篇文章中，我假设读者对贝叶斯统计和线性回归有基本的了解。我确实重述了这些话题，但是没有深入到一个全新的读者可能完全掌握它们的程度。</p></blockquote><h1 id="ff2e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯推理重写</h1><h2 id="2c75" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">贝叶斯定理</h2><p id="37f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">贝叶斯定理<strong class="lt iu"> </strong>写如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/18feaecf9728b45456c78d0768346a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*RKYfKnmcfWUgTlwlhoonAw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><ul class=""><li id="798b" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu"> <em class="mv"> P(H) </em> </strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">之前的</strong> </a></li><li id="c0b7" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"><em class="mv">【D | H】</em></strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">的可能性</strong> </a></li><li id="f391" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"><em class="mv">【P(H | D)】</em></strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">的后路</strong> </a></li><li id="d375" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"><em class="mv">【P(D)】</em></strong>是<a class="ae ky" href="https://en.wikipedia.org/wiki/Normalizing_constant#Bayes'_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">归一化常数</strong> </a>，其中<strong class="lt iu"> </strong>是可能性与先验的乘积之和，也称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">全概率定律</strong> </a>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/94e7b2033ce34bf81d65ba638d8b364d.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*ICPrrYL6ZbbU4dXaoGnEPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="372f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你不熟悉贝叶斯定理，我强烈推荐你看看我以前关于这个主题的文章:</p><div class="ob oc gp gr od oe"><a href="https://pub.towardsai.net/conditional-probability-and-bayes-theorem-simply-explained-788a6361f333" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">条件概率和贝叶斯定理浅释</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">条件概率和贝叶斯定理的简单直观的解释。</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">pub.towardsai.net</p></div></div><div class="on l"><div class="oo l op oq or on os ks oe"/></div></div></a></div><h2 id="735f" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">贝叶斯更新</h2><p id="069c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">贝叶斯定理用于根据新数据更新我们对某一事件的信念，使用以下公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/621962575384296a6543142db0a72f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*HBQe1n4WuC1Uvts9vgvsPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="d341" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们计算后验概率后，我们可能会获得关于我们试图建模的新数据。然后，我们使用旧的后验数据作为新的先验数据，用这个新数据计算新的后验数据。这个用新数据更新先验知识的过程称为贝叶斯更新。这就是<strong class="lt iu"> </strong> <a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_statistics" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">贝叶斯推理</strong> </a>的本质所在。</p><p id="96a3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可以在我最近的一篇文章中读到更多关于贝叶斯更新的内容:</p><div class="ob oc gp gr od oe"><a rel="noopener follow" target="_blank" href="/bayesian-updating-simply-explained-c2ed3e563588"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">贝叶斯更新简单解释</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">用贝叶斯定理更新信念的直观解释</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="ou l op oq or on os ks oe"/></div></div></a></div><h1 id="cd6d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">回归理论</h1><h2 id="0290" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">线性回归</h2><p id="08f0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">回归</strong> </a>旨在估计某一特性的效果，<strong class="lt iu"><em class="mv"/></strong>【x】，<strong class="lt iu"><em class="mv"/></strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/94486684d981fcc0fd865ae83b2f1a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*RCqtzokZ6RPP4JFfUWZWug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="bf90" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="mv"> β_0 </em> </strong>为截距，<strong class="lt iu"> <em class="mv"> β_1 </em> </strong>为标定目标与特征关系的系数，<strong class="lt iu"> <em class="mv"> ε </em> </strong>为误差项，在线性回归中遵循<a class="ae ky" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">正态分布</strong> </a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ad04480ad683f198d5a2d42af348431a.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*2XwKOYJ4BEFsnYf1qpr4gg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="b596" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="mv"> σ </em> </strong>为<a class="ae ky" href="https://en.wikipedia.org/wiki/Standard_deviation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">标准差</strong> </a>。</p><p id="c7bb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">线性回归的目的是确定描述特征、<strong class="lt iu"> <em class="mv"> x </em> </strong>和目标、<strong class="lt iu"> <em class="mv"> y </em> </strong>之间关系的参数<strong class="lt iu"> <em class="mv"> β_0、β_1 </em> </strong>和<strong class="lt iu"><em class="mv">【σ】</em></strong>的最佳值。</p><blockquote class="ms mt mu"><p id="0d34" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">注意:我相信大多数阅读这篇文章的人都知道什么是线性回归，如果他们不知道的话，那么有很多资源可以比我更好地向你解释它！</p></blockquote><h2 id="4c1d" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">频繁主义观点</h2><p id="2775" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">寻找线性回归模型参数的最广为人知的方法来自统计学的频率观点。</p><p id="8d97" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">频率主义者的观点采用<a class="ae ky" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">【OLS】</strong></a>的普通最小二乘法，通过<a class="ae ky" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">【RSS】</strong></a><strong class="lt iu"/>的残差平方和来估计参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0654bcabaa0c23710e00904bbac53403.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*0dUK0_A_XIAaW2BhDGUIag.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="843a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="mv"> y </em> </strong>是实际值，而<strong class="lt iu"> <em class="mv"> ŷ </em> </strong>是来自我们的模型的预测值，其一般形式为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/1d5883213522a6084a27518bc4b84316.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*FC_3XnElWBExVakEOzZVvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="8fdc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="mv"> X </em> </strong>和<strong class="lt iu"> <em class="mv"> β </em> </strong>是特征和参数的数组。</p><p id="e008" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个OLS方程的一般解是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/f0be5aaf2e7e8f2a2ff3d16287817497.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*dV4H1hQlmq7SQxtnHErrSA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><blockquote class="ms mt mu"><p id="7357" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">这个解决方案的完整推导可以在<a class="ae ky" href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Simple_linear_regression_model" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></blockquote><p id="7fba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Frequentist方法的关键是我们为每个参数获得一个<em class="mv">单一固定值</em>。</p><p id="e6e7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最终的模型在某种程度上假设我们正在建模的数据必须来自这些固定的参数。然而，不可能获得所有数据，因此假设这些<em class="mv">单值</em>参数100%正确似乎是愚蠢的。另一种表述方式是，我们假设我们有足够的数据来推导出参数的有意义的单一值。</p><h2 id="5418" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">贝叶斯观点</h2><p id="854f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">另一种观点认为，参数呈现值的分布，其中一些比另一些更有可能。它考虑了可能产生观测数据的几个似是而非的参数组合。</p><p id="d183" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们有一个我们认为参数可能是什么的初始视图/范围，例如我们可以认为截距同样可能是0到10之间的任何数字。这是我们参数的先验。</p><p id="70eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们使用观测数据更新先验，使用贝叶斯定理为每个参数创建后验分布，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/edb675464e8f27b7e1462b0e2f4710a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*YNzimu1qCZxpMnjbx-pR0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="323f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一个我们如何更新截距的例子。</p><p id="7fb6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">也就是说目标，<strong class="lt iu"><em class="mv">【y】</em></strong>，现在是数据上随机分布的变量，<strong class="lt iu"> <em class="mv"> x </em> </strong>，参数<strong class="lt iu"> <em class="mv"> β_0，</em> </strong> <strong class="lt iu"> <em class="mv"> β_1，σ: </em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/6867566502064d802592580597dbcce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*uGaggpnvzpnNRkyYdPcl1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="2323" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此每个目标变量的可能性是正态分布的<a class="ae ky" href="https://en.wikipedia.org/wiki/Probability_density_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">【概率密度函数】</strong> </a> <strong class="lt iu"> </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/bd38d4e3487f614ff3e68cfff90276c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6N3cUqTNkolZba0sTOeqBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者在LaTeX中生成的方程。</p></figure><p id="0739" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> <em class="mv"> y_i </em> </strong>的所有单个可能性的乘积产生当前模型及其给定参数的总可能性。</p><blockquote class="ms mt mu"><p id="9b8d" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">这个似然函数和贝叶斯回归作为一个整体的更密集和完整的推导可以在这里找到<a class="ae ky" href="https://statswithr.github.io/book/introduction-to-bayesian-regression.html" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="3bab" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">观察贝叶斯方法的一个很好的方式是，当我们获得更多数据时，我们更新参数的分布，我们的模型变得更加确定参数应该是什么。他们很可能是OLS常客的估计，但这不是保证。</p><p id="3920" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是频率主义者和贝叶斯统计之间的差异的快速运行。如果你想更好地理解，那里有很多资源，但是我喜欢这个博客帖子！</p><h1 id="4a0b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python中的贝叶斯回归</h1><p id="21f8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们从头开始为一个简单的模型实现贝叶斯线性回归，其中我们有一个特征！</p><h2 id="4c5a" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">生成数据</h2><p id="19b3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们首先使用sklearn的<em class="mv"> make_regression </em>函数在Python中生成一些数据:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="ca35" class="mz la it pe b gy pi pj l pk pl"># Import packages<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets</span><span id="045b" class="mz la it pe b gy pm pj l pk pl"># Generate data<br/>x, y = datasets.make_regression(n_samples=100,<br/>                                n_features=1,<br/>                                noise=10)</span><span id="bd38" class="mz la it pe b gy pm pj l pk pl"># Plot data<br/>fig, ax = plt.subplots(figsize=(9,5))<br/>ax.scatter(x, y)<br/>ax.ticklabel_format(style='plain')<br/>plt.xlabel('x',fontsize=18)<br/>plt.ylabel('y',fontsize=18)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/1089d072540555c5f5ea2d4cff4e7e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*suI9cNA2j_mVAH769Ke4Zw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><h2 id="c794" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">普通最小二乘法</h2><p id="d1fb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们可以使用statsmodel软件包，通过OLS方法来估计频率回归线:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="0e27" class="mz la it pe b gy pi pj l pk pl"># Packages<br/>import statsmodels.formula.api as smf</span><span id="d961" class="mz la it pe b gy pm pj l pk pl"># Create a dataframe<br/>data = pd.DataFrame(list(zip(x.flatten(), y)), columns =['x', 'y'])</span><span id="53ff" class="mz la it pe b gy pm pj l pk pl"># Calculating the slope and intercept<br/>formula = 'y ~ x'<br/>results = smf.ols(formula, data=data).fit()</span><span id="7daa" class="mz la it pe b gy pm pj l pk pl"># Get our equation of the OLS line<br/>intercept = results.params['Intercept']<br/>slope = results.params['x']<br/>x_vals = np.arange(min(x), max(x), 0.1)<br/>ols_line = slope*x_vals + intercept</span><span id="969d" class="mz la it pe b gy pm pj l pk pl"># Plot the OLS line<br/>fig, ax = plt.subplots(figsize=(9,5))<br/>ax.scatter(data['x'], data['y'])<br/>ax.plot(x_vals, ols_line,label='OLS Fit', color='red')<br/>ax.ticklabel_format(style='plain')<br/>plt.xlabel('x',fontsize=18)<br/>plt.ylabel('y',fontsize=18)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c493ad6311a66ecee09d6f240d62ede2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*qW4dSpDwQ5v_3BV08mjIOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><p id="fc19" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了清楚起见，斜率是<strong class="lt iu"> <em class="mv"> β_1，</em> </strong>截距是<strong class="lt iu"> <em class="mv"> β_0 </em> </strong>，sigma是<strong class="lt iu"><em class="mv"/></strong>这就是我们上面在理论部分用来描述回归线的。</p><p id="18fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是频率主义者的解释，因为我们现在对每个参数只有单一的估计。我们现在将执行贝叶斯版本。</p><h2 id="7348" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">传道者</h2><p id="cf1a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，我们需要给我们的参数分配一些先验分布。使用OLS估计的结果，我们构建了一个<a class="ae ky" href="https://stats.stackexchange.com/questions/27813/what-is-the-point-of-non-informative-priors" rel="noopener ugc nofollow" target="_blank">统一的无信息先验</a>，其范围为OLS估计的20%:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="6b4b" class="mz la it pe b gy pi pj l pk pl">def make_uninformative_prior(name,data):<br/>    """Function to make priors."""<br/>    df = pd.DataFrame({name:data})<br/>    df['Probability'] = 1/len(data)<br/>    return df</span><span id="8246" class="mz la it pe b gy pm pj l pk pl"># Create a range of values for the slope<br/>data_slope = np.linspace(results.params['x']*0.8,<br/>                         results.params['x']*1.2, num=60)</span><span id="2dd5" class="mz la it pe b gy pm pj l pk pl">prior_slope = make_uninformative_prior('slope',data_slope)<br/>prior_slope.head()</span><span id="eb22" class="mz la it pe b gy pm pj l pk pl"># Create a range of values for the intercept<br/>data_inter = np.linspace(results.params['Intercept']*0.8,<br/>                         results.params['Intercept']*1.2, num=60)</span><span id="1df9" class="mz la it pe b gy pm pj l pk pl">prior_inter = make_uninformative_prior('intercept',data_inter)<br/>prior_inter.head()</span><span id="2705" class="mz la it pe b gy pm pj l pk pl"># Create a range of values for the sigma<br/>data_sigma = np.linspace(results.resid.std()*0.8,<br/>                         results.resid.std()*1.2, num=60)</span><span id="1ed1" class="mz la it pe b gy pm pj l pk pl">prior_sigma = make_uninformative_prior('sigma',data_sigma)<br/>prior_sigma.head()</span></pre><p id="673a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果我们对实际参数有所了解，我们可能会使用不同的先验，对参数的某些值进行不同的加权。前科完全是主观武断的。这通常是反对贝叶斯统计的一个<a class="ae ky" href="https://stats.stackexchange.com/questions/381825/objective-vs-subjective-bayesian-paradigms" rel="noopener ugc nofollow" target="_blank">论点</a>，因为它导致非客观概率。</p><p id="bc11" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们现在计算三个参数的联合分布。这告诉我们一个特定的参数组合解释观察到的数据的可能性有多大:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="859c" class="mz la it pe b gy pi pj l pk pl"># Counter for the row index<br/>counter = 0</span><span id="a506" class="mz la it pe b gy pm pj l pk pl"># Dataframe to store the combinations in<br/>df = pd.DataFrame(columns=['slope','intercept','sigma','prior'])</span><span id="a8e5" class="mz la it pe b gy pm pj l pk pl"># Iterate through the slope<br/>for slope in prior_slope['slope']:<br/>    prob_slope = \<br/>    prior_slope['Prior'].loc[prior_slope['slope'] == slope]<br/>    <br/>    # Iterate through the intercept<br/>    for intercept in prior_inter['intercept']:<br/>        prob_inter = \<br/>        prior_inter['Prior'].loc[prior_inter['intercept'] \<br/>                                 == intercept]<br/>        <br/>        # Iterate through the error<br/>        for sigma in prior_sigma['sigma']:<br/>            prob_sigma = \<br/>            prior_sigma['Prior'].loc[prior_sigma['sigma'] == sigma]<br/>            <br/>            # Calculate the prior of this specific combination<br/>            prob = \<br/>            float(prob_slope)*float(prob_inter)*float(prob_sigma)<br/>            <br/>            # Insert the row of data<br/>            df.loc[counter] = \<br/>            [slope] + [intercept] + [sigma] + [prob]<br/>            <br/>            # Update row index<br/>            counter += 1</span></pre><blockquote class="ms mt mu"><p id="e55e" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">如果这种格式在你的屏幕上看起来不舒服，我很抱歉。如果是这样的话，我强烈推荐你在我的<a class="ae ky" href="https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/Bayesian_Regression.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub repo这里</a>查看代码，让它更容易解释和理解！</p></blockquote><p id="7162" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我完全知道for循环不是最优的，使用pandas和numpy的矢量化实现会更快。然而，我认为使用循环可以让我们更好地理解正在发生的事情！</p><p id="a772" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因为我们对每个参数都有未知的先验，所以每个组合都有完全相同的先验概率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/129fd554575eee1b6617150bbbe76837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTq7JPVPbI6nRAszR74E8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="bc4e" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">可能性</h2><p id="6685" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就像我们之前说的，后验与先验和似然的乘积成正比。因此，要获得每个参数的后验分布，我们需要根据我们的观测数据计算它们的似然性。</p><p id="602d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了计算可能性，我们为每个可能的组合(每一行)建立一个模型，并使用我前面提到的公式计算残差来找到可能性:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="c25c" class="mz la it pe b gy pi pj l pk pl">counter = 0<br/>df['likelihood'] = df['prior']</span><span id="bbc9" class="mz la it pe b gy pm pj l pk pl"># Loop over the combination of values<br/>for slope in prior_slope['slope']:<br/>    for intercept in prior_inter['intercept']:        <br/>        for sigma in prior_sigma['sigma']:<br/>            <br/>            # Compute the predictions from this line<br/>            predictions = slope * data['x'] + intercept<br/>            <br/>            # Compute residual/errors of this line<br/>            residual = data['y'] - predictions<br/>            <br/>            # Compute the likelihood function that we saw above<br/>            likelihoods = norm(0, sigma).pdf(residual)<br/>            <br/>            # Compute the total likelihood<br/>            df['likelihood'].loc[counter] = likelihoods.prod()<br/>            counter += 1</span></pre><p id="7d13" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这部分可能需要一段时间来运行，这是在大规模模型中使用贝叶斯方法的问题之一。要知道为什么会这样，查看一下我之前关于贝叶斯共轭先验的文章，这篇文章告诉你实施贝叶斯定理的一些缺点:</p><div class="ob oc gp gr od oe"><a rel="noopener follow" target="_blank" href="/bayesian-conjugate-priors-simply-explained-747218be0f70"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">贝叶斯共轭先验简单解释</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">执行贝叶斯统计的一种计算有效的方法</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">towardsdatascience.com</p></div></div><div class="on l"><div class="pp l op oq or on os ks oe"/></div></div></a></div><p id="662d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们的数据框架现在看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/179d124727382062eff408b558669195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xeVcskT5G2MS3bgNQhvElg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="57bc" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">贝叶斯更新</h2><p id="ff09" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们现在可以如下执行我们的贝叶斯更新:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="88f4" class="mz la it pe b gy pi pj l pk pl">df['posterior'] = df['prior'] * df['likelihood']<br/>df['posterior'] = df['posterior']/df['posterior'].sum()</span></pre><p id="efdc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们得到的数据框架是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/850d16c9bf5bd07bf8ccbb48bd006a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JAOsuyYGJ-9fbxHiLAnHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><h2 id="e78f" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">边际分布</h2><p id="72e6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了输出每个参数的边际后验分布，我们需要对其他两个参数的后验进行求和。例如，为了找到斜率的边际后验分布，我们总结出，对于斜率的每个值<strong class="lt iu"> <em class="mv">、</em>、</strong>的后验超过σ和截距(基本上是一个积分):</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="a13e" class="mz la it pe b gy pi pj l pk pl">slope_df = df.groupby('slope').sum()</span></pre><p id="cdc8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们可以绘制斜率的后验分布:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="9bb0" class="mz la it pe b gy pi pj l pk pl"># Plot the posterior distribution of the slope<br/>plt.figure(figsize=(8,5))<br/>plt.plot(slope_df.index, slope_df.posterior, linewidth=3)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.xlabel('Slope Value', fontsize=18)<br/>plt.ylabel('PDF', fontsize=18)<br/>plt.axvline(results.params['x'], color='red', ls='--', label='OLS Estimate')<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/c7d36ba94f4d11638bda0da65ef5413f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*KMJM7B8fGOvLVEOxN2UGkQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><blockquote class="pt"><p id="4ac2" class="pu pv it bd pw px py pz qa qb qc mm dk translated">斜率现在是一个分布！</p></blockquote><p id="10b4" class="pw-post-body-paragraph lr ls it lt b lu qd ju lw lx qe jx lz ma qf mc md me qg mg mh mi qh mk ml mm im bi translated">我们可以对截距和误差项进行类似的计算:</p><pre class="kj kk kl km gt pd pe pf pg aw ph bi"><span id="0bae" class="mz la it pe b gy pi pj l pk pl">intercept_df = df.groupby('intercept').sum()<br/>sigma_df = df.groupby('sigma').sum()</span><span id="9b3e" class="mz la it pe b gy pm pj l pk pl"># Plot the posterior distribution of the Intercept<br/>plt.figure(figsize=(8,5))<br/>plt.plot(intercept_df.index, intercept_df.posterior, linewidth=3)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.xlabel('Intercept Value', fontsize=18)<br/>plt.ylabel('PDF', fontsize=18)<br/>plt.axvline(results.params['Intercept'], color='red', ls='--', label='OLS Estimate')<br/>plt.legend(fontsize=16)<br/>plt.show()</span><span id="2d00" class="mz la it pe b gy pm pj l pk pl"># Plot the posterior distribution of sigma<br/>plt.figure(figsize=(8,5))<br/>plt.plot(sigma_df.index, sigma_df.posterior, linewidth=3)<br/>plt.xticks(fontsize=18)<br/>plt.yticks(fontsize=18)<br/>plt.xlabel('Sigma Value', fontsize=18)<br/>plt.ylabel('PDF', fontsize=18)<br/>plt.axvline(results.resid.std(), color='red', ls='--', label='OLS Estimate')<br/>plt.legend(fontsize=16)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/4a4fa9853684ee0e4ed86063ecea385b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*jDe9xzBpuxrrX7GQBK6Jng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/0ec34da865fb78f873b09b37937ff7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*QKILdDYHESGF1AI776JmEQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者用Python生成的图。</p></figure><p id="2da3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">OLS估计值是这些参数最可能的值，但不是唯一的值。我们可以使用216，000种其他潜在组合来建立数据模型！</p><h2 id="c110" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">摘要</h2><p id="1f28" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个计算并不是那么直接，写这篇博客最终让我理解了整个贝叶斯回归过程。我建议读者在我的GitHub上查看完整的笔记本，并在上面玩一玩:</p><div class="ob oc gp gr od oe"><a href="https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/Bayesian_Regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd iu gy z fp oj fr fs ok fu fw is bi translated">Medium-Articles/Bayesian _ regression . ipynb at main egorhowell/Medium-Articles</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">我在我的媒体博客/文章中使用的代码。通过创建一个关于…的帐户，为egorhowell/Medium-Articles的开发做出贡献</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">github.com</p></div></div><div class="on l"><div class="qi l op oq or on os ks oe"/></div></div></a></div><h1 id="f821" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="be4e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我们回顾了贝叶斯定理，解释了频数统计和贝叶斯统计之间的关键区别，最后从头开始进行贝叶斯线性回归。</p><p id="8504" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能已经注意到，这个主题在您需要完全掌握的先决知识方面相当广泛。如果我试着去适应所有的背景话题，这个博客真的会成为教科书！</p><p id="cad7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我建议那些还没有完全理解我们在这里所做的事情的人复习一下贝叶斯统计和线性回归。有这么多关于这些主题的资源，会比我教得更好！</p><h1 id="eacb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系！</h1><ul class=""><li id="ca40" class="nm nn it lt b lu lv lx ly ma qj me qk mi ql mm nr ns nt nu bi translated"><a class="ae ky" href="/@egorhowell/membership" rel="noopener ugc nofollow" target="_blank"> <em class="mv">要想在媒体上阅读无限的故事，请务必在这里注册！</em> </a> <em class="mv"> </em>💜</li><li id="9786" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><a class="ae ky" href="/subscribe/@egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="mv">在我发布注册邮件通知时获得更新！</em>T13<em class="mv">T15】😀</em></a></li><li id="b8dc" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><a class="ae ky" href="https://www.linkedin.com/in/egor-howell-092a721b3/" rel="noopener ugc nofollow" target="_blank"> <em class="mv">领英</em> </a> <em class="mv"> </em>👔</li><li id="a7a0" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><a class="ae ky" href="https://twitter.com/EgorHowell" rel="noopener ugc nofollow" target="_blank"> <em class="mv">碎碎念</em> </a> <em class="mv"> </em> 🖊</li><li id="fb18" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><a class="ae ky" href="https://github.com/egorhowell" rel="noopener ugc nofollow" target="_blank"><em class="mv">github</em></a><em class="mv"/>🖥</li><li id="87b0" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><a class="ae ky" href="https://www.kaggle.com/egorphysics" rel="noopener ugc nofollow" target="_blank"><em class="mv"/></a><em class="mv"/>🏅</li></ul><blockquote class="ms mt mu"><p id="72f1" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">(所有表情符号都是由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>设计的——开源的表情符号和图标项目。执照:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></blockquote></div></div>    
</body>
</html>