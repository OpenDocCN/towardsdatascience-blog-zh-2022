<html>
<head>
<title>Regression in TensorFlow Using Both Sequential and Function APIs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 中使用顺序 API 和函数 API 的回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-in-tensorflow-using-both-sequential-and-function-apis-314e74b537ca#2022-03-05">https://towardsdatascience.com/regression-in-tensorflow-using-both-sequential-and-function-apis-314e74b537ca#2022-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3c4fe75f4b4cbb92bd2503fb141bd591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MZ7XomatAc3yBt8f"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">玛利亚·诺沃塞列茨卡娅在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="1ae4" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">演示几种不同类型的模型结构</h2></div><p id="d8be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Tensorflow 可以说是深度学习和神经网络领域中最受欢迎的包。我以前写过一些不同的教程，关于规则密集神经网络、CNN 结构和 RNNs。但是我在 Tensorflow 上的所有教程都是关于分类问题的。有时是图像分类，有时是自然语言分类。</p><blockquote class="lu"><p id="5435" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">在本文中，我想解决一个回归问题，并演示一些顺序 API 和函数 API 的模型。</p></blockquote><p id="75bf" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">我已经清理了所有的数据。因为这篇文章将完全集中于回归问题中的张量流。因此，我不想在这里展示数据清理操作。</p><p id="be5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可从以下链接下载干净并可用于模型数据集中的产品:</p><div class="is it gp gr iu mj"><a href="https://github.com/rashida048/Tensorflow/blob/main/auto_price.csv" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">tensor flow/auto _ price . CSV at main rashida 048/tensor flow</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ja mj"/></div></div></a></div><p id="bd0a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://archive.ics.uci.edu/ml/datasets/Automobile" rel="noopener ugc nofollow" target="_blank">原始数据集</a>取自 UCI 机器学习知识库。</p><blockquote class="my mz na"><p id="d368" class="ky kz nb la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated"><a class="ae jg" href="https://archive.ics.uci.edu/ml/citation_policy.html" rel="noopener ugc nofollow" target="_blank">这里说的是用户权限</a>。</p></blockquote><p id="c24e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我在这里导入数据集:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="66a3" class="no np jj nk b gy nq nr l ns nt">import pandas as pd<br/>df = pd.read_csv("auto_price.csv")</span></pre><p id="9018" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经在清理中处理了一些空值。但是，如果有更多的空值，我会直接删除它们:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="2a65" class="no np jj nk b gy nq nr l ns nt">df = df.dropna()</span></pre><p id="a320" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在进入张量流模型之前，去掉所有的空值是很重要的，因为数据集中的空值会给你带来错误。</p><p id="6f46" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在该数据集中,“价格”列将被用作输出变量或标签。其余的特征将被用作训练特征。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="65e7" class="no np jj nk b gy nq nr l ns nt">X = df.drop(columns ="price")<br/>y = df['price']</span></pre><p id="2f40" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我将执行另一个常见任务。也就是分割数据集用于训练、验证和测试。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="f3fe" class="no np jj nk b gy nq nr l ns nt">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler</span><span id="0682" class="no np jj nk b gy nu nr l ns nt">X_train_full, X_test, y_train_full, y_test = train_test_split(X, y)<br/>X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)</span></pre><p id="6a4b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与分类不同，回归问题不能用精度来评价。在这里，我将使用均方根误差作为评估指标。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="c09b" class="no np jj nk b gy nq nr l ns nt">import tensorflow as tf<br/>from tensorflow.keras import Sequential<br/>from tensorflow.keras.optimizers import Adam<br/>from tensorflow.keras import backend as K<br/>from tensorflow.keras.layers import Dense</span><span id="7ab4" class="no np jj nk b gy nu nr l ns nt">def rmse(y_true, y_pred):<br/>    return K.sqrt(K.mean(K.square(y_pred - y_true)))</span></pre><h2 id="d2eb" class="no np jj bd nv nw nx dn ny nz oa dp ob lh oc od oe ll of og oh lp oi oj ok ol bi translated">模型开发</h2><p id="c6c1" class="pw-post-body-paragraph ky kz jj la b lb om kk ld le on kn lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">在 TensorFlow 中，模型开发是有趣的部分。对于大多数现实世界的问题，您将为一个项目尝试多个 TensorFlow 模型。在我开始写作之前，我尝试了这个项目的八个模型。我在这里分享其中的一些。</p><p id="c81d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将从使用顺序 API 的常规 DNN 模型开始，然后展示一些使用函数 API 的模型。所以，你会看到不同之处。</p><p id="5c58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我试过四款 DNN 车型，这款给我的效果最好:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="68a6" class="no np jj nk b gy nq nr l ns nt">model4= Sequential([<br/>    tf.keras.layers.Input(shape = X_train.shape[1:]),<br/>    Dense(300, activation='tanh'),<br/>    Dense(300, activation='tanh'),<br/>    Dense(300, activation='tanh'),<br/>    Dense(300, activation='tanh'),<br/>    Dense(300, activation='tanh'),<br/>    Dense(300, activation='tanh'),<br/>    Dense(1)<br/>])</span></pre><p id="9660" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，我们有六个隐藏的致密层，有 300 个神经元和' tanh '激活功能。请随意尝试一些其他的激活功能、不同的层数和不同数量的神经元。我将在本文中保持激活函数的一致性，并对模型进行比较。</p><p id="a872" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为这是一个回归问题，对于给定的输入，这里只有一个输出。所以，最后的输出层有一个神经元。<strong class="la jk">输入形状和特征的数量。</strong></p><p id="5d0e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们再次使用“rmse”作为损失、adam optimizer 和评估指标作为“rmse”来编译这个模型。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="a0e7" class="no np jj nk b gy nq nr l ns nt">model4.compile(<br/>    loss=rmse,<br/>    optimizer=Adam(),<br/>    metrics=[rmse]<br/>)</span></pre><p id="b616" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后运行模型 20000 个时期:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="c7f1" class="no np jj nk b gy nq nr l ns nt">history4 = model4.fit(X_train, y_train, epochs=20000, validation_data=(X_valid, y_valid))</span></pre><p id="686b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5a55" class="no np jj nk b gy nq nr l ns nt">Epoch 1/20000<br/>4/4 [==============================] - 0s 40ms/step - loss: 14714.9736 - rmse: 15355.5986 - val_loss: 17081.0312 - val_rmse: 15776.2559<br/>Epoch 2/20000<br/>4/4 [==============================] - 0s 9ms/step - loss: 14792.2432 - rmse: 15059.4141 - val_loss: 17076.2695 - val_rmse: 15771.4033<br/>Epoch 3/20000<br/>4/4 [==============================] - 0s 9ms/step - loss: 14842.5195 - rmse: 15015.9941 - val_loss: 17074.5098 - val_rmse: 15769.6104<br/>...<br/>...<br/>Epoch 19997/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 7850.9614 - rmse: 8218.5664 - val_loss: 9565.8027 - val_rmse: 8386.0020<br/>Epoch 19998/20000<br/>4/4 [==============================] - 0s 7ms/step - loss: 7826.3198 - rmse: 7867.3975 - val_loss: 9565.8008 - val_rmse: 8386.0020<br/>Epoch 19999/20000<br/>4/4 [==============================] - 0s 7ms/step - loss: 7857.6772 - rmse: 7917.7451 - val_loss: 9565.8135 - val_rmse: 8386.0088<br/>Epoch 20000/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 7846.6616 - rmse: 8078.2676 - val_loss: 9565.8145 - val_rmse: 8386.0088</span></pre><p id="c312" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如上面的输出所示，训练损失和验证损失分别从 14714 和 17081 开始。在 20000 个时期之后，训练和验证损失是 7846 和 9565。</p><p id="2b43" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练中使用了训练和验证数据。看看它在完全看不见的数据上的表现会很好。我将测试数据以评估 TensorFlow 模型的性能:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="0a75" class="no np jj nk b gy nq nr l ns nt">model4.evaluate(X_test, y_test)</span></pre><p id="fcc0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="1673" class="no np jj nk b gy nq nr l ns nt">2/2 [==============================] - 0s 2ms/step - loss: 7129.9590 - rmse: 7222.2402</span></pre><p id="0652" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Out[209]:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5a64" class="no np jj nk b gy nq nr l ns nt">[7129.958984375, 7222.240234375]</span></pre><p id="0dba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在测试中，数据丢失是 7129，rmse 是 7222。看起来损失和“rmse”比验证损失和“rmse”要小得多。还不错！</p><p id="310f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们尝试改进这些结果 TensorFlow 提供了功能 API 来开发更复杂的模型。</p><h2 id="4b94" class="no np jj bd nv nw nx dn ny nz oa dp ob lh oc od oe ll of og oh lp oi oj ok ol bi translated">使用功能 API 开发 Tensorflow 模型</h2><p id="3226" class="pw-post-body-paragraph ky kz jj la b lb om kk ld le on kn lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">使用函数式 API 构建宽深度神经网络的一种方法。在这种方法中，部分或全部输入层通过隐藏层中的所有变换到达输出层(深度路径)。另外，输入层也可以直接到输出层(宽路径)。</p><p id="b859" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是一个使用函数式 API 的模型。你看到模型后，我会向模型解释:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="802b" class="no np jj nk b gy nq nr l ns nt">input = tf.keras.layers.Input(shape = X_train.shape[1:])<br/>hidden1 = tf.keras.layers.Dense(300, activation='relu')(input)<br/>hidden2 = tf.keras.layers.Dense(300, activation='relu')(hidden1)<br/>concat = keras.layers.Concatenate()([input, hidden2])<br/>output = keras.layers.Dense(1)(concat)<br/>model51 = keras.models.Model(inputs=[input], outputs=[output])</span></pre><p id="1002" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">区别非常明显。每一层都有一个名称。在输入层之后，一个隐藏层照常被创建。它被命名为 hidden1。一旦创建了 hidden1，它就作为一个传递输入层的函数。同样，hidden2 也被创建，它也将作为一个函数。hidden1 的输出传递给 hidden2。</p><p id="4a3b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个模型只有两个隐藏层。下一层是 concatenate()层，它也将充当添加了 hidden2 层的输入层和输出层的函数。顾名思义，这一层连接了输入层和 hidden2 层的输出。最后，输出层被创建，和其他层一样，它也被用作一个函数。“concat”的输出被传递到输出层。<strong class="la jk">因此，在输出层，我们将 hidden1 和 hidden2 层的原始输入和转换输入连接在一起。</strong></p><p id="d2a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最后一行，创建了一个 Keras 模型，其中明确指定了输入和输出。</p><p id="5440" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编译和训练过程与之前相同。这一次，训练只运行了 8000 个周期，而不是 20000 个周期。因为带函数 API 的 TensorFlow 模型应该收敛得更快。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="6288" class="no np jj nk b gy nq nr l ns nt">model51.compile(<br/>    loss=rmse,<br/>    optimizer=Adam(),<br/>    metrics=[rmse]<br/>)</span><span id="2ab3" class="no np jj nk b gy nu nr l ns nt">history5 = model51.fit(X_train, y_train, epochs=8000, validation_data=(X_valid, y_valid))</span></pre><p id="6d1b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="8f90" class="no np jj nk b gy nq nr l ns nt">Epoch 1/8000<br/>4/4 [==============================] - 1s 64ms/step - loss: 13258.9883 - rmse: 13171.9844 - val_loss: 14222.4844 - val_rmse: 12949.8066<br/>Epoch 2/8000<br/>4/4 [==============================] - 0s 8ms/step - loss: 11380.7041 - rmse: 11492.8750 - val_loss: 12479.9932 - val_rmse: 11245.1924<br/>Epoch 3/8000<br/>4/4 [==============================] - 0s 10ms/step - loss: 9632.6230 - rmse: 10223.1465 - val_loss: 10826.7109 - val_rmse: 9650.0918</span><span id="6605" class="no np jj nk b gy nu nr l ns nt">...<br/>...<br/>Epoch 7998/8000<br/>4/4 [==============================] - 0s 7ms/step - loss: 619.7231 - rmse: 626.7860 - val_loss: 3652.3462 - val_rmse: 2738.8286<br/>Epoch 7999/8000<br/>4/4 [==============================] - 0s 7ms/step - loss: 661.9867 - rmse: 680.8888 - val_loss: 3569.5647 - val_rmse: 2896.3765<br/>Epoch 8000/8000<br/>4/4 [==============================] - 0s 6ms/step - loss: 607.5422 - rmse: 579.1271 - val_loss: 3477.7332 - val_rmse: 2928.8345</span></pre><p id="e65a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">经过 8000 个周期后，训练数据的“rmse”下降到 579，验证数据的“RMSE”下降到 2928。与 8078 和 8386 型号 4 相比，它要好得多。</p><p id="bb84" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练和验证“rmse”值非常不同。很明显过度配合了。完全看不见的数据的“rmse”是多少？以下是测试数据的检查:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="f74c" class="no np jj nk b gy nq nr l ns nt">model51.evaluate(X_test, y_test)</span></pre><p id="05d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="0483" class="no np jj nk b gy nq nr l ns nt">[2501.0322265625, 2517.703125]</span></pre><p id="36fb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它给出的“rmse”为 2517，更接近验证“rmse”。</p><p id="26de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了训练损失和验证损失如何随时期变化:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="8403" class="no np jj nk b gy nq nr l ns nt">plt.plot(history5.history['loss'])<br/>plt.plot(history5.history['val_loss'])<br/>plt.legend()</span></pre><figure class="nf ng nh ni gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cf8c421786f7d08da201ffb72ddda6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*a1IwEOadOjgrhquJjzNZNQ.png"/></div></figure><p id="1552" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">model51 的结构是功能 API 上的一个非常简单的模型。你可以设计比这复杂得多的网络。下一个模型是 model5，它有点复杂，只是为了演示如何使用它。</p><p id="9926" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是如果一个简单的模型可以完成这项工作，我就不喜欢复杂的模型。模型越简单越好。但如果有必要，我们可能需要在一些复杂的项目中使用更复杂的模型来获得更好的结果。所以，这里我有一个演示:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="8f4a" class="no np jj nk b gy nq nr l ns nt">input = tf.keras.layers.Input(shape = X_train.shape[1:])<br/>hidden1 = tf.keras.layers.Dense(300, activation='relu')(input)<br/>hidden2 = tf.keras.layers.Dense(300, activation='relu')(hidden1)<br/>hidden3 = tf.keras.layers.Dense(300, activation='relu')(hidden2)<br/>hidden4 = keras.layers.Concatenate()([input, hidden3])<br/>hidden5 = tf.keras.layers.Dense(300, activation='relu')(hidden4)<br/>concat = keras.layers.Concatenate()([input, hidden5])<br/>output = keras.layers.Dense(1)(concat)<br/>model5 = keras.models.Model(inputs=[input], outputs=[output])</span></pre><p id="6c79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通常，第一层是输入层</p><p id="9534" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二层是 hidden1 层，输入层在此处传递。</p><p id="6b9c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">hidden1 层的输出被传递到 hidden2 层。</p><p id="cacc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">hidden2 层的输出被传递到 hidden3 层。</p><p id="e2b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们将输入层与 hidden3 层的输出连接起来，并将其命名为 hidden4。</p><p id="f2bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，hidden4 层的输出被传递到 hidden5。</p><p id="f7c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们再一次把输入和隐藏的输出连接起来。</p><p id="8841" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，有一个输出层和指定输入和输出的模型开发。</p><p id="69b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来的步骤与之前相同:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="79ec" class="no np jj nk b gy nq nr l ns nt">model5.compile(<br/>    loss=rmse,<br/>    optimizer=Adam(),<br/>    metrics=[rmse]<br/>)<br/>history5 = model5.fit(X_train, y_train, epochs=8000, validation_data=(X_valid, y_valid))</span></pre><p id="105e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="720b" class="no np jj nk b gy nq nr l ns nt">Epoch 1/20000<br/>4/4 [==============================] - 1s 50ms/step - loss: 13927.1045 - rmse: 13915.9434 - val_loss: 13678.9092 - val_rmse: 12434.3320<br/>Epoch 2/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 10047.6201 - rmse: 9563.8525 - val_loss: 9203.7637 - val_rmse: 8289.6582<br/>Epoch 3/20000<br/>4/4 [==============================] - 0s 7ms/step - loss: 8499.3525 - rmse: 8310.4893 - val_loss: 9033.5527 - val_rmse: 8297.1348<br/>...<br/>...<br/>Epoch 19998/20000<br/>4/4 [==============================] - 0s 9ms/step - loss: 287.1166 - rmse: 261.8001 - val_loss: 3024.3147 - val_rmse: 2331.6790<br/>Epoch 19999/20000<br/>4/4 [==============================] - 0s 9ms/step - loss: 227.0752 - rmse: 238.3496 - val_loss: 2957.6494 - val_rmse: 2263.0854<br/>Epoch 20000/20000<br/>4/4 [==============================] - 0s 9ms/step - loss: 187.4945 - rmse: 180.5027 - val_loss: 3016.0168 - val_rmse: 2355.3120</span></pre><p id="f310" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您可以看到的，训练数据和验证数据的“rmse”都在这个模型中得到了进一步改善。</p><p id="d7b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，训练和验证数据集的“rmse”值分别为 180 和 2355。显然，存在过度拟合。</p><p id="e5a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用测试数据评估:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="5731" class="no np jj nk b gy nq nr l ns nt">model5.evaluate(X_test, y_test)</span></pre><p id="59c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="c6ef" class="no np jj nk b gy nq nr l ns nt">[2499.998291015625, 2547.47216796875]</span></pre><p id="fd7c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">“rmse”是 2547，超过了看起来非常正常的验证“rmse”。但是“rmse”非常接近模型 51 的评估。所以，我们实际上不能说这个模型很好地改善了结果。</p><p id="ecab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">复杂和更大的模型并不总是意味着更好的结果。</strong>我们可以尝试不同的型号。以前在我身上发生过很多。我先试着做一个简单的模型，然后做了很多越来越复杂的模型。但最终还是回到了更简单的模型。</p><blockquote class="my mz na"><p id="7dab" class="ky kz nb la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated">可以只变换一部分输入要素，而不是通过变换使用所有输入要素，并且可以将一部分输入要素直接添加到输出图层。</p></blockquote><p id="1df7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于此模型，前 15 个特征保存为输入 1，后 18 个特征保存为输入 2。由于要素总数为 26，因此有些要素重叠。因此，一些特性将同时存在于输入 1 和输入 2 中。</p><p id="1a28" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型如下:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="e065" class="no np jj nk b gy nq nr l ns nt">input1 = tf.keras.layers.Input(shape = [15])<br/>input2 = tf.keras.layers.Input(shape = [18])<br/>hidden1 = tf.keras.layers.Dense(300, activation='relu')(input2)<br/>hidden2 = tf.keras.layers.Dense(300, activation='relu')(hidden1)<br/>hidden3 = tf.keras.layers.Dense(300, activation='relu')(hidden2)<br/>hidden4 = keras.layers.Concatenate()([input2, hidden3])<br/>hidden5 = tf.keras.layers.Dense(300, activation='relu')(hidden4)<br/>concat = keras.layers.Concatenate()([input1, hidden5])<br/>output = keras.layers.Dense(1)(concat)<br/>model6 = keras.models.Model(inputs=[input1, input2], outputs=[output])</span></pre><p id="2a12" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型结构几乎与之前的模型一样。唯一的区别是，input2 通过隐藏层中的转换传递。Input1 与最后一个隐藏层(hidden5)连接在一起，就在输出之前。因此，input1 没有完成隐藏层中的所有转换。</p><p id="6728" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们应该分离特征:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="0736" class="no np jj nk b gy nq nr l ns nt">X_train1, X_train2 = X_train.iloc[:, :15], X_train.iloc[:, 7:]<br/>X_valid1, X_valid2 = X_valid.iloc[:, :15], X_valid.iloc[:, 7:]<br/>X_test1, X_test2 = X_test.iloc[:, :15], X_test.iloc[:, 7:]</span></pre><p id="4e5d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们编译并使输入和输出数据适合模型。记住，这次我们必须将 input1 和 input2 都作为输入进行传递。</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="db42" class="no np jj nk b gy nq nr l ns nt">model6.compile(<br/>    loss=rmse,<br/>    optimizer=Adam(),<br/>    metrics=[rmse]<br/>)<br/>history6 = model6.fit((X_train1, X_train2), y_train, epochs=20000, validation_data=((X_valid1, X_valid2), y_valid))</span></pre><p id="7640" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="9f52" class="no np jj nk b gy nq nr l ns nt">Epoch 1/20000<br/>4/4 [==============================] - 1s 53ms/step - loss: 14018.6748 - rmse: 14139.8809 - val_loss: 13892.1895 - val_rmse: 12647.2930<br/>Epoch 2/20000<br/>4/4 [==============================] - 0s 6ms/step - loss: 10192.4922 - rmse: 9930.0068 - val_loss: 9372.5049 - val_rmse: 8391.0254<br/>Epoch 3/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 8807.8857 - rmse: 8779.3643 - val_loss: 9259.1514 - val_rmse: 8631.5068<br/>...<br/>...<br/>Epoch 19998/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 495.4005 - rmse: 509.9180 - val_loss: 3193.6880 - val_rmse: 3008.2153<br/>Epoch 19999/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 405.2184 - rmse: 446.1824 - val_loss: 3340.9062 - val_rmse: 3053.0967<br/>Epoch 20000/20000<br/>4/4 [==============================] - 0s 8ms/step - loss: 405.8342 - rmse: 387.4508 - val_loss: 3277.2720 - val_rmse: 3060.6477</span></pre><p id="ab21" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，训练和验证数据的“rmse”值变成了 387 和 3080。</p><p id="b9da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是评价:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="288c" class="no np jj nk b gy nq nr l ns nt">model6.evaluate([X_test1, X_test2], y_test)</span></pre><p id="23e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nf ng nh ni gt nj nk nl nm aw nn bi"><span id="211d" class="no np jj nk b gy nq nr l ns nt">[2547.850830078125, 2735.259033203125]</span></pre><p id="35ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评估的“rmse”也是 2735。看起来这种方法并没有更好。事实上，它比以前的型号做得更差。</p><h2 id="f3b7" class="no np jj bd nv nw nx dn ny nz oa dp ob lh oc od oe ll of og oh lp oi oj ok ol bi translated">结论</h2><p id="28fa" class="pw-post-body-paragraph ky kz jj la b lb om kk ld le on kn lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">我想在 Tensorflow 中使用顺序 API 和函数 API 演示几种不同风格的回归模型，并在这个特定项目中对它们进行比较。但这些结果不能一概而论。不同的模型架构更适合不同的项目。我们需要为每个项目尝试不同的模式，看看哪种模式效果最好。所以，有很多不同的事情可以尝试。请随意尝试更多的模型，看看是否可以改善结果。</p><p id="43e3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请随时在<a class="ae jg" href="https://twitter.com/rashida048" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae jg" href="https://www.facebook.com/Regenerative-149425692134498" rel="noopener ugc nofollow" target="_blank">脸书页面</a>上关注我，并查看我的<a class="ae jg" href="https://www.youtube.com/channel/UCzJgOvsJJPCXWytXWuVSeXw" rel="noopener ugc nofollow" target="_blank"> YouTube 频道</a>。</p><h2 id="b13c" class="no np jj bd nv nw nx dn ny nz oa dp ob lh oc od oe ll of og oh lp oi oj ok ol bi translated">更多阅读</h2><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/simple-explanation-on-how-decision-tree-algorithm-makes-decisions-34f56be344e9"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">浅谈决策树算法如何决策</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">决策树算法背后的直觉</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="os l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/what-is-a-recurrent-neural-network-and-implementation-of-simplernn-gru-and-lstm-models-in-keras-f7247e97c405"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">在 Keras 中实现 SimpleRNN、GRU 和 LSTM 模型</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">什么是递归神经网络以及三种递归神经网络在 Tensorflow 和</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="ot l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/developing-a-convolutional-neural-network-model-using-the-unlabeled-image-files-directly-from-the-124180b8f21f"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">开发一个卷积神经网络模型使用未标记的图像文件直接从…</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">使用图像生成器将图像自动标记为子目录</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="ou l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/chi-square-test-for-correlation-test-in-details-manual-and-python-implementation-472ae5c4b15f"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">详细相关性检验的卡方检验:手动和 Python 实现</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">检查两个变量是否相互独立</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="ov l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a rel="noopener follow" target="_blank" href="/understanding-regularization-in-plain-language-l1-and-l2-regularization-2991b9c54e9a"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">用通俗的语言理解正则化:L1 和 L2 正则化</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">在数据科学访谈中经常被问到</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">towardsdatascience.com</p></div></div><div class="ms l"><div class="ow l mu mv mw ms mx ja mj"/></div></div></a></div><div class="is it gp gr iu mj"><a href="https://pub.towardsai.net/data-analysis-91a38207c92b" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd jk gy z fp mo fr fs mp fu fw ji bi translated">数据分析</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">Python 中数据科学家/分析师日常工作中的常见数据清理任务</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">pub.towardsai.net</p></div></div><div class="ms l"><div class="ox l mu mv mw ms mx ja mj"/></div></div></a></div></div></div>    
</body>
</html>