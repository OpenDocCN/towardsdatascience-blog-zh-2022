<html>
<head>
<title>Apache Spark for Data Science — How to Install and Get Started with PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark for Data Science —如何安装和使用PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/apache-spark-for-data-science-how-to-install-and-get-started-with-pyspark-6367a1ea3de8#2022-04-07">https://towardsdatascience.com/apache-spark-for-data-science-how-to-install-and-get-started-with-pyspark-6367a1ea3de8#2022-04-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ebe9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">本地安装PySpark并加载您的第一个数据集——仅需5分钟</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a71a074f2d3f5fa4345145ba6cd85ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_50gTDMmm5Ib4QyA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jeztimms?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰兹·蒂姆斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="fb6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在大数据的世界里，知道如何高效地处理庞大的数据集是必须的。这就是Apache Spark的切入点。这是一个数据处理框架，用于在大型数据集上执行数据处理任务。Spark还允许您在多台计算机上分配数据处理任务。</p><p id="92e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark是用Scala编写的，但是你不需要了解Scala就可以使用Spark。它附带了一个名为<em class="lv"> PySpark </em>的Python API，这就是我们今天要用的。这个API不是很Pythonic化，因为它们使用了camel case命名约定。这对Python用户来说有点眼疼，但这是你必须习惯的。</p><p id="f789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的目标是编写一个完整的Spark系列，涵盖重要的主题，如Python中的Spark、Spark SQL、使用Spark的机器学习、流和不同提供商(如Amazon EMR和DataBricks)上的云中的Spark。</p><p id="4b7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要开始使用PySpark，我们首先需要创建一个新的Python虚拟环境并安装库。让我们接下来做那件事。</p><p id="1f6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不想看书？请观看我的视频:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="af3f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Apache Spark的虚拟环境设置(PySpark)</h1><p id="3f04" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">让我们首先打开一个新的终端窗口，并创建一个新的Python虚拟环境。我使用Anaconda来管理环境。如果您使用的是<code class="fe nc nd ne nf b">pyenv</code>或类似的东西，过程会有所不同:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="d0c0" class="nk mg it nf b gy nl nm l nn no">conda create --name spark_env python=3.9 -y<br/>conda activate spark_env</span></pre><p id="dcfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里我们可以安装库。我总是喜欢安装Numpy和Pandas，但是今天它们不是必需的。我们将使用JupyterLab作为IDE，所以我们也将安装它。</p><p id="ffcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦安装了这些，我们就可以用Pip安装PySpark了:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="b6b5" class="nk mg it nf b gy nl nm l nn no">conda install -c conda-forge numpy pandas jupyter jupyterlab<br/>pip install pyspark</span></pre><p id="a743" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一切都安装好了，让我们启动Jupyter:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="00f4" class="nk mg it nf b gy nl nm l nn no">jupyter lab</span></pre><p id="ed47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一步是下载数据集。Spark通常用于大型数据集，但为了简单起见，在这一点上，我们将使用一个小型数据集。我选择了<a class="ae ky" href="https://www.kaggle.com/datasets/vikrishnan/boston-house-prices" rel="noopener ugc nofollow" target="_blank">波士顿房价数据集</a>，所以下载它并把它放在某个地方，最好是放在你的笔记本所在的文件夹里。</p><p id="b625" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们开始使用PySpark所需的全部内容。接下来让我们探索一下基础。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="3c7a" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">如何用Python启动Spark会话</h1><p id="5de4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">好了，现在你应该已经下载了数据集并运行Jupyter了。打开一个空白的Jupyter笔记本—在开始使用Spark之前，有一件重要的事情是您一直想做的。</p><p id="0d21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它与视觉有关。Spark数据框不会打印为熊猫数据框。如果列太多，它们很可能会溢出到新的一行，这看起来很糟糕。在所有使用Spark的笔记本电脑中编写以下代码来缓解该问题:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="56c1" class="nk mg it nf b gy nl nm l nn no">from IPython.core.display import HTML<br/>display(HTML("&lt;style&gt;pre { white-space: pre !important; }&lt;/style&gt;"))</span></pre><p id="51c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这里我们可以开始Spark会话。您可以将该会话视为Spark SQL的入口点。这是您在使用Spark SQL应用程序和数据管道时首先要创建的东西。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="4e54" class="nk mg it nf b gy nl nm l nn no">from pyspark.sql import SparkSession</span><span id="fcf3" class="nk mg it nf b gy np nm l nn no">spark = SparkSession.builder.appName("spark-session").getOrCreate()</span></pre><p id="8da8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">会话信息现在存储在<code class="fe nc nd ne nf b">spark</code>变量中，所以让我们看看它包含了什么:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="c6ae" class="nk mg it nf b gy nl nm l nn no">spark</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/14c91ae733ed7c58c14938ade2e0e3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOOCMnq0VUP3EG4TelhF5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图spark变量的内容(图片由作者提供)</p></figure><p id="e423" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里没有什么特别有趣的，因为我们在本地运行Spark。您可以看到一个指向Spark UI的链接，让我们点击它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/00ab19ef422d211bbc916cb6ebfa7eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1VMqvH-pvN-GBMAWoViBhg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2 — Spark用户界面(图片由作者提供)</p></figure><p id="54f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它几乎是空的，因为我们还没有运行任何代码，但是如果您在文章结尾打开Spark UI，您会看到很多输出。</p><p id="6634" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回到笔记本，开始处理数据。在阅读数据集时，有几件事你应该知道，所以接下来让我们来看一下。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="d8c1" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">如何用Spark和Python读取数据集</h1><p id="327c" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">没有人会停止使用read CSV方法，就像你在熊猫身上做的一样:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="6c60" class="nk mg it nf b gy nl nm l nn no">df = spark.read.csv("../BostonHousing.csv") <br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/0e4978ec05a9cad150f430cb41a3523b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJI-CpbZmzI5sgrLDkY-ng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3—df变量的内容(作者提供的图片)</p></figure><p id="2bfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是你可以看到，它有几个问题。首先，列名是自动生成的，我们不希望这样，因为CSV文件有一个标题行。第二，所有的列都被转换成字符串，实际的数据集只包含数字数据。</p><p id="0785" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看几个PySpark方法来验证这些说法。第一个名为<code class="fe nc nd ne nf b">.show()</code>，它将在笔记本中显示我们的数据集。默认情况下，它只显示20行:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="8a73" class="nk mg it nf b gy nl nm l nn no">df.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/4e9433f3f9f1b76e6a8dbb1f69e03d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtiuiQmu1w9xL3Lqq0gBaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片4-波士顿住房数据集的前20行(图片由作者提供)</p></figure><p id="2d24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里可以看到第一个问题。列名是由PySpark生成的，所以实际的CSV标题行被认为是一个值。因此，所有列都是字符串，而不是浮点和整数。</p><p id="8783" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nc nd ne nf b">.printSchema()</code>方法将为这种说法提供具体证据:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="25c3" class="nk mg it nf b gy nl nm l nn no">df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4b242cf4d832abbb611c2bb7dac3a835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*DNgwRjYINQp23ooemSkfwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5-默认数据集模式(作者提供的图片)</p></figure><p id="56fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以看到一切都是字符串，这不是我们想要的。有一个简单的解决方法，就是在读取数据集时提供两个额外的参数:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="3af2" class="nk mg it nf b gy nl nm l nn no">df = spark.read.csv("../BostonHousing.csv", header=True, inferSchema=True) <br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/f2b7d3228eba25ded5658ae74169561c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcP199XgLSIZneeNUdZJFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6-正确读取后的数据集(作者提供的图片)</p></figure><p id="bbb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出中，我们可以看到所有列的数据类型不是int就是double。<em class="lv"> Double </em>不是Python中的实际数据类型。它来自Scala，但是Python不会有任何问题。</p><p id="46e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以再次打印模式，以验证我们现在拥有正确的数据类型:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="6808" class="nk mg it nf b gy nl nm l nn no">df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3d403d19594358468b2a5f3b7cab886a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*Qj9OjtS7km7KqfWQp0noQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7-正确读取后的数据集模式(图片由作者提供)</p></figure><p id="c4a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来一切正常，这意味着我们今天可以到此为止。我不想讨论使用列和数据聚合的细节。我们将在下一篇文章中讨论这个问题。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="5712" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Apache Spark for Data Science概述</h1><p id="c795" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">今天，我们已经为Apache Spark系列的后续文章奠定了基础。您已经成功安装了PySpark，并了解了如何加载数据集。下一步是学习如何使用它们，这也是接下来的文章将要讨论的内容。</p><p id="5645" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一篇文章将向您展示如何用类似Pandas的语法操作和处理数据，下一篇文章将向您展示如何用类似SQL的命令做同样的操作。Spark完全可以接受这两者，你会选择哪一个取决于个人偏好。如果您每天都要编写大量的SQL，那么为什么不使用Spark SQL呢？</p><p id="58a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请继续关注即将到来的文章，我会确保在几天内发布它们。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="a063" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">喜欢这篇文章吗？成为 <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="lv">中等会员</em> </a> <em class="lv">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="nx ny gp gr nz oa"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h2 id="9f65" class="nk mg it bd mh op oq dn ml or os dp mp li ot ou mr lm ov ow mt lq ox oy mv oz bi translated">推荐阅读</h2><ul class=""><li id="af7d" class="pa pb it lb b lc mx lf my li pc lm pd lq pe lu pf pg ph pi bi translated"><a class="ae ky" href="https://betterdatascience.com/best-data-science-prerequisite-books/" rel="noopener ugc nofollow" target="_blank">学习数据科学先决条件(数学、统计和编程)的5本最佳书籍</a></li><li id="7d08" class="pa pb it lb b lc pj lf pk li pl lm pm lq pn lu pf pg ph pi bi translated"><a class="ae ky" href="https://betterdatascience.com/top-books-to-learn-data-science/" rel="noopener ugc nofollow" target="_blank">2022年学习数据科学的前5本书</a></li><li id="a4f1" class="pa pb it lb b lc pj lf pk li pl lm pm lq pn lu pf pg ph pi bi translated"><a class="ae ky" href="https://betterdatascience.com/python-list-print/" rel="noopener ugc nofollow" target="_blank">用Python打印列表的7种方法</a></li></ul><h2 id="16f7" class="nk mg it bd mh op oq dn ml or os dp mp li ot ou mr lm ov ow mt lq ox oy mv oz bi translated">保持联系</h2><ul class=""><li id="12d2" class="pa pb it lb b lc mx lf my li pc lm pd lq pe lu pf pg ph pi bi translated">雇用我作为一名技术作家</li><li id="0ca5" class="pa pb it lb b lc pj lf pk li pl lm pm lq pn lu pf pg ph pi bi translated">订阅<a class="ae ky" href="https://www.youtube.com/c/BetterDataScience" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="2736" class="pa pb it lb b lc pj lf pk li pl lm pm lq pn lu pf pg ph pi bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/darioradecic/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</li></ul></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="d418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">原载于2022年4月7日https://betterdatascience.com</em><em class="lv">的</em> <a class="ae ky" href="https://betterdatascience.com/apache-spark-introduction/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">。</em></a></p></div></div>    
</body>
</html>