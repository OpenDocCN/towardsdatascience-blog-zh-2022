<html>
<head>
<title>Most Common Text Processing Tasks In Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中最常见的文本处理任务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/most-common-text-processing-tasks-in-natural-language-processing-d8cd7a076b79#2022-12-13">https://towardsdatascience.com/most-common-text-processing-tasks-in-natural-language-processing-d8cd7a076b79#2022-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b4d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">计算机没有人类的能力，文本数据需要处理，以便更好地理解和解释</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div></figure><h1 id="fb10" class="kp kq it bd kr ks kt ku kv kw kx ky kz jz la ka lb kc lc kd ld kf le kg lf lg bi translated">介绍</h1><p id="f853" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">人类具有理解书面文本信息的能力。另一方面，机器不具备这种内在能力。这就是文本处理变得重要的地方，因为它允许这些机器理解和分析自然语言。</p><p id="c4ba" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">在这篇概念性文章中，我们将解释如何使用流行的 Python 库(如 NLTK 和 Spacy)执行最常见的文本处理任务。</p><h1 id="8929" class="kp kq it bd kr ks kt ku kv kw kx ky kz jz la ka lb kc lc kd ld kf le kg lf lg bi translated">最常见的任务</h1><p id="7fcc" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">文本预处理包括标记化、停用词去除、词干化和词条化、词性标注和命名实体识别。本节重点解释它们中的每一个及其 python 实现。</p><h2 id="b457" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">先决条件</h2><p id="f98c" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，您需要在计算机上安装 Python 以及以下库:</p><ul class=""><li id="7cff" class="mu mv it lj b lk md ln me lq mw lu mx ly my mc mz na nb nc bi translated">NLTK</li><li id="8bad" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated">空间</li><li id="8edc" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated">sci kit-学习</li></ul><p id="4a6a" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">您可以使用 Python 包管理器<code class="fe ni nj nk nl b">pip</code>安装这些库，如下所示:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="e71e" class="nq kq it nl b be nr ns l nt nu"># Install NLTK<br/>pip install nltk<br/><br/># Install Spacy<br/>pip install spacy<br/><br/># Install Scikit-learn<br/>pip install scikit-learn</span></pre><p id="4083" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">现在，让我们导入必要的模块并加载用于实验的数据集。我们将使用 Scikit-learn 中内置的新闻文章数据。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="c04f" class="nq kq it nl b be nr ns l nt nu">import nltk<br/>import spacy<br/>nltk.download('punkt')<br/>from sklearn.datasets import fetch_20newsgroups</span></pre><p id="0631" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">然后，我们可以使用<code class="fe ni nj nk nl b">fetch_20newsgroups</code>函数通过访问<code class="fe ni nj nk nl b">data</code>属性来下载和加载新闻数据，如下所示:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="eb31" class="nq kq it nl b be nr ns l nt nu">news_data = fetch_20newsgroups(subset='all')<br/>articles = news_data.data</span></pre><p id="1969" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">让我们看看第一条:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="304e" class="nq kq it nl b be nr ns l nt nu">print(articles[0])</span></pre><p id="2632" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">这将生成以下输出:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="7fbc" class="nq kq it nl b be nr ns l nt nu">From: Mamatha Devineni Ratnam &lt;@andrew.cmu.edu"&gt;mr47+@andrew.cmu.edu&gt;<br/>Subject: Pens fans reactions<br/>Organization: Post Office, Carnegie Mellon, Pittsburgh, PA<br/>Lines: 12<br/>NNTP-Posting-Host: po4.andrew.cmu.edu<br/><br/><br/>I am sure some bashers of Pens fans are pretty confused about the lack<br/>of any kind of posts about the recent Pens massacre of the Devils. Actually,<br/>I am  bit puzzled too and a bit relieved. However, I am going to put an end<br/>to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they<br/>are killing those Devils worse than I thought. Jagr just showed you why<br/>he is much better than his regular season stats. He is also a lot<br/>fo fun to watch in the playoffs. Bowman should let JAgr have a lot of<br/>fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final<br/>regular season game.          PENS RULE!!!</span></pre><p id="5423" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">既然一切都设置好了，是时候深入每个任务了，从标记化开始。</p><h2 id="54b1" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">标记化</h2><p id="b6d7" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是文本处理中最简单的步骤，包括将文本分割成标记。生成的令牌取决于底层的令牌化方法。例如:</p><ul class=""><li id="dc66" class="mu mv it lj b lk md ln me lq mw lu mx ly my mc mz na nb nc bi translated">单词标记化生成单词。</li><li id="68d9" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated">句子标记化将文本分割成句子。</li></ul><p id="44bc" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">使用<code class="fe ni nj nk nl b">NLTK</code>库中的<code class="fe ni nj nk nl b">work_tokenize()</code>和<code class="fe ni nj nk nl b">sent_tokenize()</code>函数分别执行单词和句子的分词。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="93c5" class="nq kq it nl b be nr ns l nt nu"># Import word and sentence tokenizers<br/>from nltk.tokenize import word_tokenize, sent_tokenize</span></pre><p id="f6b0" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">在用前一篇文章的第二个代码块初始化变量<code class="fe ni nj nk nl b">first_article</code>之后，我们可以继续进行标记化，如下所示:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="0359" class="nq kq it nl b be nr ns l nt nu"># Generate Word tokens<br/>word_tokens = word_tokenize(first_article)<br/><br/># Generate Sentence Tokens<br/>sentence_tokens = sent_tokenize(first_article)<br/><br/># Print the results<br/>print(word_tokens)<br/>print(sentence_tokens)</span></pre><p id="89cb" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">前面的<code class="fe ni nj nk nl b">print</code>语句应该会生成这些输出。第一个是单词标记，第二个是句子标记。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="4d23" class="nq kq it nl b be nr ns l nt nu">['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'Pens', 'massacre', 'of', 'the', 'Devils', '.', 'Actually', ',', 'I', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.', 'However', ',', 'I', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-PIttsburghers', "'", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'Pens', '.', 'Man', ',', 'they', 'are', 'killing', 'those', 'Devils', 'worse', 'than', 'I', 'thought', '.', 'Jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.', 'He', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.', 'Bowman', 'should', 'let', 'JAgr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'Pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'Jersey', 'anyway', '.', 'I', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'Islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.', 'PENS', 'RULE', '!', '!', '!']</span></pre><p id="b324" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">显示每个单词标记可能太多了，但我们可以按如下方式显示所有句子:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="a702" class="nq kq it nl b be nr ns l nt nu">I am sure some bashers of Pens fans are pretty confused about the lack<br/>of any kind of posts about the recent Pens massacre of the Devils.<br/><br/><br/>Actually, I am  bit puzzled too and a bit relieved.<br/><br/><br/>However, I am going to put an end to non-PIttsburghers' relief with a bit of praise for the Pens.<br/><br/><br/>Man, they are killing those Devils worse than I thought.<br/><br/><br/>Jagr just showed you why he is much better than his regular season stats.<br/><br/><br/>He is also a lot<br/>fo fun to watch in the playoffs.<br/><br/><br/>Bowman should let JAgr have a lot of<br/>fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway.<br/><br/><br/>I was very disappointed not to see the Islanders lose the final regular season game.<br/><br/><br/>PENS RULE!!</span></pre><p id="99c0" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">句子分词器在<code class="fe ni nj nk nl b">.</code>符号后识别一个新句子。</p><h2 id="c597" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">停止单词删除</h2><p id="c47e" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">查看之前的单词标记，我们可以看到一些术语，如<code class="fe ni nj nk nl b">an, a, of, the,</code>等。这些单词被称为<code class="fe ni nj nk nl b">stop words</code>,因为与其他单词相比，它们没有太多的含义。因此，删除它们可以使信息更容易处理。这可以通过使用<code class="fe ni nj nk nl b">nltk.corpus.stopwords</code>模块中的<code class="fe ni nj nk nl b">words()</code>功能来实现。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="1628" class="nq kq it nl b be nr ns l nt nu">from nltk.corpus import stopwords</span></pre><p id="c2c2" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">由于我们正在处理一个英文文本，我们需要如下加载底层停用词:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="679b" class="nq kq it nl b be nr ns l nt nu"># Acquire the stop words<br/>english_stw = stopwords.words("english")</span></pre><p id="733f" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">最后，我们可以过滤所有单词标记，只保留不间断的单词。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="8b4f" class="nq kq it nl b be nr ns l nt nu">non_stop_words = [word for word in word_tokens if word not in english_stw]<br/><br/># Show the final stop words<br/>print(non_stop_words)</span></pre><p id="6aa2" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">前面的<code class="fe ni nj nk nl b">print</code>语句显示了以下结果:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="cda3" class="nq kq it nl b be nr ns l nt nu">['I', 'sure', 'bashers', 'Pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'Pens', 'massacre', 'Devils', '.', 'Actually', ',', 'I', 'bit', 'puzzled', 'bit', 'relieved', '.', 'However', ',', 'I', 'going', 'put', 'end', 'non-PIttsburghers', "'", 'relief', 'bit', 'praise', 'Pens', '.', 'Man', ',', 'killing', 'Devils', 'worse', 'I', 'thought', '.', 'Jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', '.', 'He', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', '.', 'Bowman', 'let', 'JAgr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'Pens', 'going', 'beat', 'pulp', 'Jersey', 'anyway', '.', 'I', 'disappointed', 'see', 'Islanders', 'lose', 'final', 'regular', 'season', 'game', '.', 'PENS', 'RULE', '!', '!', '!']</span></pre><h2 id="76bf" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">标点符号删除</h2><p id="2158" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果停用词不相关，标点符号也不相关！我们可以很容易地去掉标点符号(<code class="fe ni nj nk nl b">.,;</code>等)。)使用 Python 中的原生<code class="fe ni nj nk nl b">string</code>模块。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="6d62" class="nq kq it nl b be nr ns l nt nu">import string<br/><br/>without_punct = list(filter(lambda word: word not in string.punctuation, non_stop_words))<br/><br/>print(without_punct)</span></pre><pre class="nv nm nl nn bn no np bi"><span id="74f2" class="nq kq it nl b be nr ns l nt nu">['I', 'sure', 'bashers', 'Pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'Pens', 'massacre', 'Devils', 'Actually', 'I', 'bit', 'puzzled', 'bit', 'relieved', 'However', 'I', 'going', 'put', 'end', 'non-PIttsburghers', 'relief', 'bit', 'praise', 'Pens', 'Man', 'killing', 'Devils', 'worse', 'I', 'thought', 'Jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'He', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'Bowman', 'let', 'JAgr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'Pens', 'going', 'beat', 'pulp', 'Jersey', 'anyway', 'I', 'disappointed', 'see', 'Islanders', 'lose', 'final', 'regular', 'season', 'game', 'PENS', 'RULE']</span></pre><h2 id="09e0" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">词干化和词汇化</h2><p id="56d2" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有时候在同一个文档里，我们可以找到类似<code class="fe ni nj nk nl b">confused, confusing, confused, confuses, confuse, confused</code>这样的词。在性能至关重要的大型数据集的情况下使用它们可能会有问题。这就是词干化和词汇化有用的地方。他们的目标是将这些单词减少到基本单词。</p><p id="d615" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">你可以从我的文章<a class="ae nw" rel="noopener" target="_blank" href="/stemming-lemmatization-which-one-is-worth-going-for-77e6ec01ad9c">词干化、词汇化中深入了解这些技术，以及它们的区别——哪一个更值得一试？</a></p><p id="5809" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">让我们考虑本节的以下示例文本:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="4fe6" class="nq kq it nl b be nr ns l nt nu">sample_text = """This thing really confuses. <br/>                 But you confuse me more than what is written here.  <br/>                 So stay away from explaining things you do not understand. <br/>              """</span></pre><p id="b793" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">然后，我们可以使用<code class="fe ni nj nk nl b">nltk.stem</code>来导入<code class="fe ni nj nk nl b">PorterStemmer</code>和<code class="fe ni nj nk nl b">WordNetLemmatizer</code>，以使用这两个辅助函数分别执行词干化和词汇化。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="32a3" class="nq kq it nl b be nr ns l nt nu">def stem_words(sentence, model=my_stemmer):<br/>  <br/>    for word in sentence.split():<br/>      stem = model.stem(word)<br/>      print("Word: {} ---&gt;: {}".format(word, stem))<br/><br/><br/>def lemmatize_words(sentence, model = my_lemmatizer):<br/><br/>    for word in sentence.split():<br/>      lemma = model.lemmatize(word)<br/>      print("Word: {} ---&gt;: {}".format(word, lemma))</span></pre><p id="a48c" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated"><code class="fe ni nj nk nl b">stem_words</code>通过在箭头的左侧和右侧显示原始单词和已词干化的单词来执行词干化。同样的方法也适用于使用<code class="fe ni nj nk nl b">lemmatize_words</code>函数的词汇化。</p><p id="683e" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">在使用这两个函数之前，我们需要设置如下所示的两个模型:</p><p id="f967" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">→<strong class="lj iu">Lemmatizer:</strong>Lemmatizer 需要使用多语言 wordnet 的<code class="fe ni nj nk nl b">wordnet</code>词典数据库和<code class="fe ni nj nk nl b">OMW</code>模块，因此它们也需要下载。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="aeb9" class="nq kq it nl b be nr ns l nt nu"># Import the Lemmatizer module<br/>from nltk.stem import WordNetLemmatizer<br/><br/># Download wordnet lexicon database<br/>nltk.download('wordnet')<br/>nltk.download('omw-1.4')<br/><br/># Instanciate Lemmatizer<br/>my_lemmatizer = WordNetLemmatizer()</span></pre><p id="224c" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">→ <strong class="lj iu">斯特梅尔</strong>:这很简单，配置如下:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="367f" class="nq kq it nl b be nr ns l nt nu">Import the Stemmer module<br/>from nltk.stem.porter import PorterStemmer<br/><br/># Create instance of stemmer<br/>my_stemmer = PorterStemmer()</span></pre><p id="defd" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">现在我们已经配置了两个模型，让我们在示例文本上运行它们。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="1539" class="nq kq it nl b be nr ns l nt nu"># Run the stemming<br/>stem_words(sample_text, model=my_stemmer)</span></pre><p id="f7ca" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">这应该会显示以下结果</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/373d2566ea157a2dcc06ee686b1231f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fa7J0G4UCqCI5zrKujvGgw.png"/></div></div><p class="oe of gj gh gi og oh bd b be z dk translated">词干插图(图片由作者提供)</p></figure><p id="330c" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">类似地，我们可以如下执行术语化:</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="0d50" class="nq kq it nl b be nr ns l nt nu">lemmatize_words(sample_text, model = my_lemmatizer)</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oi"><img src="../Images/829daf44018479efef899d2d847af916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zTetDMpD7oSknna7RpHb-g.png"/></div></div><p class="oe of gj gh gi og oh bd b be z dk translated">词汇化插图(图片由作者提供)</p></figure><p id="908a" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">从每个输出中，我们可以在右侧观察到，一些世界已经被认为与它们的词干 lemma 相同，而一些则完全被转换，尤其是<code class="fe ni nj nk nl b">things, confuse</code>。</p><h2 id="cc4b" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">词性标注</h2><p id="e0bd" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于给定的句子，如何知道哪个是名词、动词、形容词、代词等？这些部分被称为<code class="fe ni nj nk nl b">Part of Speech</code>，它们可以帮助你理解句子的结构和潜在含义。这个任务叫做<code class="fe ni nj nk nl b">Part of speech tagging</code>。它会自动为句子中的每个单词分配词性。</p><p id="037f" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">使用标记列表，我们可以使用<code class="fe ni nj nk nl b">pos_tag</code>函数为每个标记分配相应的词性。<code class="fe ni nj nk nl b">pos_tag</code>的最终结果是一个元组列表，其中每个元组都有一个标记，并且是语音标签的一部分。下面是一个插图。</p><p id="4d79" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">标签员需要模块<code class="fe ni nj nk nl b">averaged_perceptron_tagger</code>，其中包含预先培训的英语。</p><pre class="ki kj kk kl gt nm nl nn bn no np bi"><span id="6563" class="nq kq it nl b be nr ns l nt nu"># Import the module<br/>from nltk.tag import pos_tag<br/><br/># Download the tagger<br/>nltk.download('averaged_perceptron_tagger')<br/><br/># Perform the post tagging<br/>tagged_tokens = pos_tag(without_punct)<br/><br/># Show the final result<br/>print(tagged_tokens)</span></pre><p id="8a53" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">前面的<code class="fe ni nj nk nl b">print</code>语句显示了以下结果:</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oj"><img src="../Images/83592f830f3f29b7ede095f07cf7264b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUkVC01nbXJQVEYfNq3Ntw.png"/></div></div><p class="oe of gj gh gi og oh bd b be z dk translated">术语和标签(作者图片)</p></figure><p id="5bc0" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">在我们之前的例子中:</p><ul class=""><li id="4859" class="mu mv it lj b lk md ln me lq mw lu mx ly my mc mz na nb nc bi translated"><strong class="lj iu">我</strong>是一个<code class="fe ni nj nk nl b">Pronoun (PRP)</code></li><li id="05a1" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated"><strong class="lj iu">糊涂了</strong>是个<code class="fe ni nj nk nl b">adjective (JJ)</code></li><li id="2ad5" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated"><strong class="lj iu">非匹兹堡人</strong>是一个<code class="fe ni nj nk nl b">Noun, Plural (NNS)</code></li><li id="2f1c" class="mu mv it lj b lk nd ln ne lq nf lu ng ly nh mc mz na nb nc bi translated"><strong class="lj iu">因为</strong>是一个<code class="fe ni nj nk nl b">preposition or subordinating conjuction (IN)</code></li></ul><p id="39af" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">你可以找到<a class="ae nw" href="https://www.ibm.com/docs/en/wca/3.5.0?topic=analytics-part-speech-tag-sets" rel="noopener ugc nofollow" target="_blank">词性标签集</a>。</p><h2 id="98b5" class="mi kq it bd kr mj mk dn kv ml mm dp kz lq mn mo lb lu mp mq ld ly mr ms lf mt bi translated">命名实体识别</h2><p id="6a06" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">命名实体识别(简称 NER)和词性标注有时会混淆。尽管它们是两个相关的任务，但它们是完全不同的。NER 包括对命名实体(如个人、组织、地点等)的识别和分类。一条短信。</p><p id="a089" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">我的文章<a class="ae nw" href="https://medium.com/towards-data-science/named-entity-recognition-with-spacy-and-the-mighty-roberta-97d879f981" rel="noopener">用 Spacy 和强大的 roBERTa 命名实体识别</a>提供了对 NER 的清晰简明的解释，以及 Python 代码。</p><h1 id="8d2b" class="kp kq it bd kr ks kt ku kv kw kx ky kz jz la ka lb kc lc kd ld kf le kg lf lg bi translated">结论</h1><p id="96f5" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇概念性文章中，您了解了一些在自然语言处理中执行常见文本处理任务的常用方法。</p><p id="25fd" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">此外，如果你喜欢阅读我的故事，并希望支持我的写作，请考虑成为一个媒体成员。每月支付 5 美元，你就可以无限制地阅读媒体上的故事。</p><p id="38fe" class="pw-post-body-paragraph lh li it lj b lk md ju lm ln me jx lp lq mf ls lt lu mg lw lx ly mh ma mb mc im bi translated">欢迎在<a class="ae nw" href="https://zoumanakeita.medium.com/" rel="noopener">媒体</a>、<a class="ae nw" href="https://twitter.com/zoumana_keita_" rel="noopener ugc nofollow" target="_blank">推特</a>和<a class="ae nw" href="https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上关注我，或者在<a class="ae nw" href="https://www.linkedin.com/in/zoumana-keita/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上打招呼。讨论人工智能、人工智能、数据科学、自然语言处理和人工智能是一种乐趣！</p></div></div>    
</body>
</html>