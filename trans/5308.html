<html>
<head>
<title>Stable Diffusion 2: The Good, The Bad and The Ugly</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">稳定扩散 2:好的，坏的和丑陋的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stable-diffusion-2-the-good-the-bad-and-the-ugly-bd44bc7a1333#2022-11-29">https://towardsdatascience.com/stable-diffusion-2-the-good-the-bad-and-the-ugly-bd44bc7a1333#2022-11-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a5f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">向前一步，向后一步</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6b664e6755b11b781f3e976157df9a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8CjlZzu38eCMc1r5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@felipecorreia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">费利佩·科雷亚</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="7903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2022 年 11 月 24 日，稳定。AI 宣布<a class="ae ky" href="https://stability.ai/blog/stable-diffusion-v2-release" rel="noopener ugc nofollow" target="_blank">稳定扩散 2.0 </a>公开发布，这是对之前版本的重大更新，具有突破性的变化。在撰写本文时，社区对它的反应不一。新建筑因其最先进的特点而受到称赞，但同时也因其稳定性方向而受到批评。</p><p id="55c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">StabilityAI 发布了版本 2 的以下<a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2-base" rel="noopener ugc nofollow" target="_blank">检查点</a>:</p><ul class=""><li id="2a11" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe me mf mg mh b">512-base-ema.ckpt </code> —生成 512x512 图像的版本 2 检查点。</li><li id="c7f4" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">768-v-ema.ckpt</code> —基于<code class="fe me mf mg mh b">512-base-ema.ckpt</code>的检查点。在相同的数据集上使用<a class="ae ky" href="https://arxiv.org/abs/2202.00512" rel="noopener ugc nofollow" target="_blank"> v-objective </a>对其进行了进一步微调。能够原生生成 768x768 的图像。</li><li id="4f0c" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">512-depth-ema.ckpt</code>—基于<code class="fe me mf mg mh b">512-base-ema.ckpt</code>的检查点，具有额外的输入通道，用于处理<a class="ae ky" href="https://github.com/isl-org/MiDaS" rel="noopener ugc nofollow" target="_blank"> MiDaS </a>产生的(相对)深度预测。这是一个深度导向的扩散模型。适用于图像到图像的生成。</li><li id="00b9" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">512-inpainting-ema.ckpt</code> —修复模型的版本 2 检查点，用于修复 512x512 分辨率的图像。</li><li id="9f6c" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">x4-upscaling-ema.ckpt</code> —超分辨率放大扩散模型，生成分辨率为 2048x2048 或更高的图像。</li></ul><p id="d2ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文涵盖了稳定扩散 2.0 的一些事实和我个人的看法。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="68ee" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">培训用数据</h1><p id="ee1c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">通常，稳定扩散 1 在<a class="ae ky" href="https://huggingface.co/datasets/laion/laion2B-en" rel="noopener ugc nofollow" target="_blank">里昂-2B (en) </a>、<a class="ae ky" href="https://huggingface.co/datasets/laion/laion-high-resolution" rel="noopener ugc nofollow" target="_blank">里昂-高分辨率</a>和里昂-改良-美学的子集上训练。</p><blockquote class="nr ns nt"><p id="e23b" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">laion-improved-aesthetics 是 laion2B-en 的子集，被过滤成具有原始尺寸<code class="fe me mf mg mh b">&gt;= 512x512</code>、估计美学分数<code class="fe me mf mg mh b">&gt; 5.0</code>和估计水印概率<code class="fe me mf mg mh b">&lt; 0.5</code>的图像。</p></blockquote><p id="066b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，稳定扩散 2 基于<a class="ae ky" href="https://laion.ai/blog/laion-5b/" rel="noopener ugc nofollow" target="_blank"> LAION-5B </a>的子集:</p><ul class=""><li id="341c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">23.2 亿张图片和英文文本</li><li id="bb0e" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/datasets/laion/laion2B-multi" rel="noopener ugc nofollow" target="_blank">laion 2 b-multi</a>22.6 亿张图片，文字来自 100 多种其他语言</li><li id="619d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/datasets/laion/laion1B-nolang" rel="noopener ugc nofollow" target="_blank">laion 1b-nolang</a>12.7 亿张图片，带有未知语言文本(无法清晰检测)</li></ul><p id="ccb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，使用带有<code class="fe me mf mg mh b">punsafe=0.1</code>和<a class="ae ky" href="https://github.com/christophschuhmann/improved-aesthetic-predictor" rel="noopener ugc nofollow" target="_blank">审美评分</a> &gt; = <code class="fe me mf mg mh b">4.5</code>的<a class="ae ky" href="https://github.com/LAION-AI/CLIP-based-NSFW-Detector" rel="noopener ugc nofollow" target="_blank">莱昂-NSFW 分类器</a>，对数据集进行过滤，以找出露骨的色情内容。</p><p id="9ae1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，稳定扩散 2 使用更大的 NSFW 滤波数据集进行训练。此外，第二阶段的训练基于分辨率高于或等于 512x512 的图像。</p><h2 id="ba8c" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">好人</h2><p id="440c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">新版本确保 StabilityAI 解决了与儿童色情和深度假货有关的长期法律问题。此外，新的模式在某些领域产生更好的图像。从社区进行的测试来看，它似乎在现实摄影(非人类)、照片现实场景中的照明、3D 渲染和设计方面表现不错。</p><p id="ac7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里是一个使用稳定扩散 2.0 生成的<a class="ae ky" href="https://old.reddit.com/r/StableDiffusion/comments/z4r2oo/v2_makes_really_nice_birds/" rel="noopener ugc nofollow" target="_blank">鸟的例子。</a></p><h2 id="4d56" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">坏事</h2><p id="94b9" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">根据用户的反馈，你仍然可以生成裸体，但是生成的图像往往缺乏任何性吸引力。此外，新模型对于与人体解剖和名人脸部相关的图像表现不佳。</p><p id="cfd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，某些提示倾向于生成大部分黑白图像(偏向单色)，您必须添加提示“彩色”来生成彩色图像。</p><h2 id="daf2" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">丑陋的</h2><p id="d7d2" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">与版本 1 类似，手和图像中的文本生成仍然是一个大问题。此外，不确定为什么稳定扩散 2 没有使用估计水印概率策略来过滤<a class="ae ky" href="https://laion.ai/blog/laion-5b/" rel="noopener ugc nofollow" target="_blank"> LAION-5B </a>数据集。根据我自己的亲身经历，与旧模型相比，新模型更倾向于生成带有水印的图像。</p><p id="aec1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，LAION 是质量数据集上的数量。您可以通过下面的<a class="ae ky" href="https://rom1504.github.io/clip-retrieval/" rel="noopener ugc nofollow" target="_blank">剪辑检索页面</a>轻松验证图像及其文本标签对。你会发现大部分的图片标签都很差，这对模型的性能有很大的影响。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="243a" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">文本编码器</h1><p id="8d1b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">稳定扩散 2 基于<a class="ae ky" href="https://github.com/mlfoundations/open_clip" rel="noopener ugc nofollow" target="_blank"> OpenCLIP-ViT/H </a>作为文本编码器，而旧架构使用<a class="ae ky" href="https://github.com/openai/CLIP" rel="noopener ugc nofollow" target="_blank"> OpenAI 的 ViT-L/14 </a>。ViT/H 在里昂-2B 上进行训练，精度为<strong class="lb iu"> 78.0 </strong>。它是 OpenCLIP 提供的最好的开源权重之一。</p><p id="6ccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然 ViT-L/14 的重量是开源的，但 OpenAI 没有公布训练数据。因此，您将无法控制模型正在学习的内容。StabilityAI 通过利用开源实现 OpnCLIP-ViT/H weight 解决了这一问题，该实现是在他们用于潜在扩散模型的相同数据集上训练的。</p><h2 id="32d0" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">好人</h2><p id="7876" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">新型号现在可以更好地理解您的提示。继续前进，StabilityAI 和社区可以通过训练他们自己的 OpenCLIP 模型来改进文本编码器。因此，您可以引导模型轻松生成所需的图像。</p><p id="1cd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，对某些领域的偏见似乎减少了。例如，版本 2 现在能够<a class="ae ky" href="https://old.reddit.com/r/StableDiffusion/comments/z3iry9/i_guess_at_least_one_improvement_for/" rel="noopener ugc nofollow" target="_blank">生成非白色天花板的房间</a>。</p><h2 id="1599" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">坏事</h2><p id="79a7" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">版本 1 中的所有现有提示在版本 2 中并不相同。输入提示需要更具描述性。使用提示“猫”不同于提示“猫的照片”。</p><p id="ca5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，所有现有的嵌入和 dreambooths 模型都不能开箱即用。根据预先训练好的模型，您需要在管道中使用相应的文本编码器。</p><p id="e604" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不仅如此，新的文本编码器不会识别你的提示中的一些“著名艺术家”。它将只识别存在于 LAION-5B 数据集中的艺术家。</p><blockquote class="nr ns nt"><p id="cdc3" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">注意<a class="ae ky" href="https://preview.redd.it/mkg2uxmwxx1a1.png?width=624&amp;format=png&amp;auto=webp&amp;s=834219554b337f85ec58edbee30d3e575bccfa59" rel="noopener ugc nofollow" target="_blank">没有艺术家被刻意从训练数据集中删除</a>。</p></blockquote><h2 id="ea5f" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">丑陋的</h2><p id="118d" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">由于稳定扩散是在 LAION-5B 的子集上训练的，因此 OpenCLIP 很有可能在未来使用 LAION-5B 训练新的文本编码器。鉴于文本编码器是整个稳定扩散架构中至关重要的组件，当文本编码器发生变化时，大多数与提示相关的现有工作都将失效。</p><p id="fcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，一些现有的实现必须为旧版本和新版本提供向后兼容支持。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="2cf9" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">深度到图像模型</h1><p id="d653" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2-depth" rel="noopener ugc nofollow" target="_blank">新的深度制导模型</a>是 StabilityAI 发布的最有前景的功能之一。它基于<a class="ae ky" href="https://github.com/isl-org/MiDaS" rel="noopener ugc nofollow" target="_blank"> MiDaS </a>，推断一张图像的深度，然后使用文本和深度信息来生成新的图像。</p><p id="0a45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是它如何工作的一个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/481bad49f661283a5d3486bafceba9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2BefZEWqV5qZjy7zuy3aQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5dde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看拥抱中的<a class="ae ky" href="https://huggingface.co/spaces/pytorch/MiDaS" rel="noopener ugc nofollow" target="_blank">下面的空间</a>以了解更多关于从图像生成深度信息的信息。</p><h2 id="4a30" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">好人</h2><p id="3b36" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">新的深度引导模型为图像到图像生成提供了更好的结果。这为一些功能开辟了新的可能性，例如生成带有图层的图像，甚至是 3D 模型的文本。</p><h2 id="0898" class="ny mv it bd mw nz oa dn na ob oc dp ne li od oe ng lm of og ni lq oh oi nk oj bi translated">坏的和丑的</h2><p id="6544" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">在撰写本文时，您只能将深度导向模型与<a class="ae ky" href="https://github.com/Stability-AI/stablediffusion" rel="noopener ugc nofollow" target="_blank"> stablediffusion </a>库一起使用，这使得测试模型变得困难，因为大多数用户在设置和运行所提供的脚本时都遇到了困难。0.9.0 版本仅支持其他四种型号:</p><ul class=""><li id="78c7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2-base" rel="noopener ugc nofollow" target="_blank">稳定扩散 2 基</a></li><li id="ae38" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2" rel="noopener ugc nofollow" target="_blank">稳定扩散-2 </a></li><li id="530f" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2-inpainting" rel="noopener ugc nofollow" target="_blank">稳定扩散 2 修复</a></li><li id="cc86" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler" rel="noopener ugc nofollow" target="_blank">稳定扩散-x4-升级</a></li></ul><p id="c2a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从版本 0.10.0 开始，您现在可以使用<code class="fe me mf mg mh b">diffusers</code>运行模型:</p><ul class=""><li id="d802" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai/stable-diffusion-2-depth" rel="noopener ugc nofollow" target="_blank">稳定扩散 2 深度</a></li></ul><pre class="kj kk kl km gt ol mh om bn on oo bi"><span id="12f7" class="op mv it mh b be oq or l os ot">import torch<br/>import requests<br/>from PIL import Image<br/>from diffusers import StableDiffusionDepth2ImgPipeline<br/><br/>pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(<br/>   "stabilityai/stable-diffusion-2-depth",<br/>   torch_dtype=torch.float16,<br/>).to("cuda")<br/><br/>url = "http://images.cocodataset.org/val2017/000000039769.jpg"<br/>init_image = Image.open(requests.get(url, stream=True).raw)<br/><br/>prompt = "two tigers"<br/>n_propmt = "bad, deformed, ugly, bad anotomy"<br/>image = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="460d" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结论</h1><p id="6798" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">StabilityAI 声明新模型仅作为社区进一步改进的基础模型。在未来，他们将定期发布，这样任何人都可以通过用自己的数据集对其进行微调来进一步改进。他们还会提供公开的<a class="ae ky" href="https://github.com/learning-at-home/hivemind" rel="noopener ugc nofollow" target="_blank">分布式训练</a>(不保证有效的实验测试)的方法。</p><p id="258c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，《稳定扩散 2》的发布标志着在研究和解决法律问题方面向前迈进了一步。然而，对于社区中一些更喜欢不受限制的创作或对以前版本的总体改进的采纳者来说，这也是一种倒退。</p><p id="c760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你阅读这篇文章。祝你有美好的一天！</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="ebc3" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考</h1><ol class=""><li id="f7e3" class="lv lw it lb b lc nm lf nn li ou lm ov lq ow lu ox mb mc md bi translated"><a class="ae ky" href="https://laion.ai/blog/laion-5b/" rel="noopener ugc nofollow" target="_blank">里昂-5B 数据集</a></li><li id="b43f" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ox mb mc md bi translated"><a class="ae ky" href="https://stability.ai/blog/stable-diffusion-v2-release" rel="noopener ugc nofollow" target="_blank"> StabilityAI 博客—稳定扩散 2.0 </a></li><li id="e453" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ox mb mc md bi translated"><a class="ae ky" href="https://old.reddit.com/r/StableDiffusion" rel="noopener ugc nofollow" target="_blank"> Reddit —稳定扩散</a></li><li id="5851" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ox mb mc md bi translated"><a class="ae ky" href="https://huggingface.co/stabilityai" rel="noopener ugc nofollow" target="_blank">拥抱脸—稳定性</a></li></ol></div></div>    
</body>
</html>