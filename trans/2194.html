<html>
<head>
<title>Implementing Various NLP Text Representation in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现各种NLP文本表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-various-nlp-text-representation-in-python-84781da0ec2d#2022-05-16">https://towardsdatascience.com/implementing-various-nlp-text-representation-in-python-84781da0ec2d#2022-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0332" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一键编码、单词包、N-Grams和TF-IDF</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b3ce34e064c854042ca1e1acf04f1ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SevZ61QnLIxrvtYNZ4DenQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@amadorloureiro" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kz">阿玛多</strong> </a> <strong class="bd kz">上的Unsplash </strong></p></figure><p id="f6f4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">自然语言处理(NLP)是处理语言和语义的机器学习的子集。机器通过训练来学习单词的语义，就像典型的机器学习是如何工作的一样。当我们意识到几乎所有常用的机器学习模型都只能接受数字输入时，问题就出现了。因此，为了使用文本数据训练机器，我们必须找到一种将文本表示为数字向量的方法。本文将演示一些简单的数字文本表示，以及如何使用Python实现它们。</p><p id="8cda" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于本教程，我们将使用以下数据。这个数据的背景是一门大学学科的复习。我对数据进行了预处理，即去除停用词、标点符号和词条释义。这些文字都是虚构的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们正在使用的数据</p></figure><p id="7885" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们从最简单的表示开始。我们用熊猫来称呼上面的数据，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="5e33" class="md me it lz b gy mf mg l mh mi">df = pd.read_csv("data.csv")</span></pre><h2 id="cb7a" class="md me it bd mj mk ml dn mm mn mo dp mp lj mq mr ms ln mt mu mv lr mw mx my mz bi translated"><strong class="ak"> 1。一键编码(OHE) </strong></h2><p id="5ca9" class="pw-post-body-paragraph la lb it lc b ld na ju lf lg nb jx li lj nc ll lm ln nd lp lq lr ne lt lu lv im bi translated">这个模型的想法是列出所有文本中存在的每个单词。我们可以使用下面的函数做到这一点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="b99c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行下面的代码，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="d3e4" class="md me it lz b gy mf mg l mh mi">BagOfWords(df["Comments"])</span></pre><p id="440b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">会返回数据中所有唯一的单词，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="b955" class="md me it lz b gy mf mg l mh mi">array(['although', 'amazed', 'bad', 'beautiful', 'bob', 'caution', 'clear', 'concept', 'definitely', 'difficult', 'experience',<br/>'fantastic', 'find', 'first', 'found', 'funny', 'hard',      'however', 'implication', 'interesting', 'jim', 'learned', 'lecturer', 'lot', 'much', 'order', 'people', 'practical', 'really', 'reasonably', 'revision', 'rewarding', 'rubbish', 'scraped', 'spent', 'still', 'subject', 'take', 'taking', 'taught', 'terrible', 'though', 'time', 'trying', 'understand', 'warned', 'way'], dtype='&lt;U32')</span></pre><p id="9182" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，我们将这些单词中的每一个设置为新数据帧中的列，如果该单词存在于文档中，则将值设置为<em class="nf"> 1 </em>，否则设置为<em class="nf"> 0 </em>。</p><p id="cee3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用下面的函数来这样做，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="b2e5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第<em class="nf"> 7 </em>行是这个表示的要点，说明如果文档中有一个单词，则将该值设置为<em class="nf"> 1 </em>否则设置为<em class="nf"> 0 </em>。</p><p id="d41b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行这些代码行将把df转换成OHE表示，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="534c" class="md me it lz b gy mf mg l mh mi">BoW = BagOfWords(df["Comments"])<br/>dfOHE = TransformOHE(df, BoW)</span></pre><p id="f60f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">变量<code class="fe ng nh ni lz b">dfOHE</code>现在是<code class="fe ng nh ni lz b">data.csv</code>的独热编码表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><h2 id="8a21" class="md me it bd mj mk ml dn mm mn mo dp mp lj mq mr ms ln mt mu mv lr mw mx my mz bi translated">2.单词袋(蝴蝶结)</h2><p id="acb6" class="pw-post-body-paragraph la lb it lc b ld na ju lf lg nb jx li lj nc ll lm ln nd lp lq lr ne lt lu lv im bi translated">弓形文本表示类似于OHE表示。我们首先列出语料库中的每个单词。我们使用与上面相同的<code class="fe ng nh ni lz b">BagOfWords()</code>函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="4ae6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在OHE表示中，如果一个单词存在于文档中，我们将该值设置为<em class="nf"> 1 </em>，否则设置为<em class="nf"> 0 </em>。在BoW表示中，我们计算文档中出现的单词数。</p><p id="eaff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">举例</strong>。对于文档“<em class="nf">蓝天碧海</em>”，单词“<em class="nf">蓝色</em>”的OHE表示将返回<em class="nf"> 1 </em>，单词“<em class="nf">蓝色</em>”的弓形表示将返回<em class="nf"> 2 </em>。</p><p id="d78e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面的函数将把我们的数据帧转换成BoW表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="c83c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第<em class="nf"> 7 </em>行是这种表示的要点，说明一个单词(列)的条目值是该单词在文档条目中的计数。</p><p id="6a3d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行这些代码行将df转换成BoW表示，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="683b" class="md me it lz b gy mf mg l mh mi">BoW = BagOfWords(df["Comments"])<br/>dfBoW = TransformBoW(df, BoW)</span></pre><p id="7d56" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">变量<code class="fe ng nh ni lz b">dfBoW</code>现在是<code class="fe ng nh ni lz b">data.csv</code>的单词包表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><h2 id="f07f" class="md me it bd mj mk ml dn mm mn mo dp mp lj mq mr ms ln mt mu mv lr mw mx my mz bi translated">3.N-Grams</h2><p id="1af9" class="pw-post-body-paragraph la lb it lc b ld na ju lf lg nb jx li lj nc ll lm ln nd lp lq lr ne lt lu lv im bi translated">当使用弓形表示时会出现一个问题。字数统计的使用有时可能会忽略句子的语义。</p><p id="1cf8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">举例</strong>。句子“a <em class="nf"> pple击败竞争对手</em>”和“c <em class="nf">竞争对手击败苹果</em>”将具有精确的BoW文本表示。相比之下，它们的含义完全相反。</p><p id="157e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了解决这个问题，我们可以将单词袋方法修改为短语袋方法。我们可以计算短语的出现次数，而不是计算每个单词的出现次数。</p><p id="40dd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以将任何短语定义为任何<em class="nf"> N </em>个连续单词的组合，其中用户指定N的值。</p><p id="6bdc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了演示这是如何做到的，让我们以下面的文档为例，“给予比接受更好”，</p><p id="bad4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这份文件的标题是，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="0adc" class="md me it lz b gy mf mg l mh mi">["giving", "better", "than", "receiving"]</span></pre><p id="9945" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一袋2克的文件，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="7cff" class="md me it lz b gy mf mg l mh mi">["giving better", "better than", "than receiving"]</span></pre><p id="1457" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">该文件的3克袋的袋子是，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="cbb6" class="md me it lz b gy mf mg l mh mi">["giving better than", "better than receiving"]</span></pre><p id="9976" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，我们将所有唯一的N元语法设置为一列，并计算文档中出现的短语。这种方法理论上比BoW更好，因为它保留了句子的语义。</p><p id="f3da" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">举例</strong>。为了展示这种模式的优势，使用同一个句子，“一个<em class="nf">苹果公司打败了竞争对手</em>”，这个句子的2个字母的表达是“<em class="nf">苹果公司打败了</em>”和“<em class="nf">打败了竞争对手</em>”。与“竞争对手击败苹果”的2-gram表示相比，其具有“<em class="nf">竞争对手击败</em>”和“<em class="nf">击败苹果</em>”的2-gram表示。现在应该很明显，这些2-gram表示保留了句子的语义。</p><p id="76db" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用下面的函数来得到这袋N-grams。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="a214" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行下面的代码，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="69cc" class="md me it lz b gy mf mg l mh mi">BagOfNGrams(df["Comments"], 2)</span></pre><p id="ce88" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">会输出下面的数组，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="9ac6" class="md me it lz b gy mf mg l mh mi">array(['although warned', 'amazed much', 'bad experience', 'bad lecturer', 'beautiful clear', 'bob bad', 'bob terrible', 'caution taking', 'clear practical', 'concept still', 'definitely hard', 'difficult first', 'difficult however', 'difficult subject', 'experience bob', 'fantastic subject', 'find subject', 'first take', 'found reasonably', 'funny lecturer', 'hard rewarding', 'hard spent', 'hard subject', 'however lot', 'however really', 'interesting subject', 'jim funny', 'learned revision', 'lecturer amazed', 'lecturer rubbish', 'lot concept', 'much learned', 'order found', 'people hard', 'practical implication', 'really beautiful', 'really hard', 'reasonably difficult', 'rewarding although', 'rubbish subject', 'scraped though', 'spent time', 'still hard', 'subject however', 'subject jim', 'subject really', 'subject scraped', 'subject taught', 'subject way', 'take caution', 'taking subject','taught difficult', 'terrible lecturer', 'though bad', 'time trying', 'trying understand', 'understand concept', 'warned people', 'way order'], dtype='&lt;U32')</span></pre><p id="8d89" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">像BoW一样，我们将这些N元文法中的每一个都设置为列，并计算它们在文档中的出现次数。我们可以回收<code class="fe ng nh ni lz b">TransformBoW()</code>函数来做转换。我将重写下面的函数，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="6c2e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行这几行代码会将df转换成N-grams表示，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="754c" class="md me it lz b gy mf mg l mh mi"># Your choice of N<br/>N = 2</span><span id="ea46" class="md me it lz b gy nj mg l mh mi">BoN = BagOfNGrams(df["Comments"], N)<br/>dfBoN = TransformBoN(df, BoN)</span></pre><p id="93f6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">变量<code class="fe ng nh ni lz b">dfBoN</code>现在是<code class="fe ng nh ni lz b">data.csv</code>的N元表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><h2 id="bf52" class="md me it bd mj mk ml dn mm mn mo dp mp lj mq mr ms ln mt mu mv lr mw mx my mz bi translated">4.TF-IDF</h2><p id="6779" class="pw-post-body-paragraph la lb it lc b ld na ju lf lg nb jx li lj nc ll lm ln nd lp lq lr ne lt lu lv im bi translated">使用前面三种表示法的另一个问题是，较长的文档比较短的文档有更多的非零条目。这是一个问题，因为我们不能假设越长的文档在一定范围内得分越高。</p><p id="46e2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">举例</strong>。句子“<em class="nf">精彩的主题</em>”和“<em class="nf">这是一个很棒的主题，清晰的解释，吉姆是一个很好的讲师</em>”在积极程度上可以说是相似的。但是使用BoW或N-grams表示会使模型偏向较长的文档。</p><p id="5fe0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了解决这个问题，我们引入了TF-IDF文本表示。TF-IDF代表“术语频率和逆文档频率”。我们通过将两个部分相乘来计算TF-IDF，这两个部分是术语频率和逆文档频率。</p><p id="eb36" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">类似于前面的表示，TF-IDF表示的每一列是语料库的单词包中的每个单词。</p><p id="c88e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于单词包中的每个单词<em class="nf"> t </em>(列)，我们计算，</p><ul class=""><li id="52ec" class="nk nl it lc b ld le lg lh lj nm ln nn lr no lv np nq nr ns bi translated">词频:该组件计算单词<em class="nf"> t </em>在文档中的出现次数，类似于BoW表示。然后，我们对其进行“标准化”，以适应文档的长度。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/861e07eedc28f713527707361e3aa0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MLBDG25uI_P5FXSqXVjCiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">检索词频率</p></figure><ul class=""><li id="d94b" class="nk nl it lc b ld le lg lh lj nm ln nn lr no lv np nq nr ns bi translated">逆文档频率:文档频率计算有多少文档包含单词<em class="nf"> t </em>。逆文档频率将<em class="nf"> N </em>(文档总数)<em class="nf"> </em>除以文档频率。我们通常采用IDF的对数值，以减少大型数据集对IDF值的影响。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/c5603b91c782acde72af29f9ca27b541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CPkmJzxddduKJSzp-3v6xw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逆文档频率</p></figure><p id="03ee" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上面的IDF公式很好，假设单词包中的每个单词至少出现在一个文档中。如果您在训练数据上拟合TF-IDF并使用它来转换测试数据，情况可能就不一样了。换句话说，您正在使用训练数据的单词包来计算测试数据的TF-IDF。因此，一些单词可能不会出现在任何文档中。在这种情况下，我们得到一个除以零的误差。为了防止这种情况，我们可以对IDF公式进行平滑处理。有许多关于不同平滑方法的文献。我下面提供的方法是<code class="fe ng nh ni lz b">scikit-learn</code>库的默认方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/88b50f2a96c4767bcf54d88c4e053e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9KONxWHen5D2NDS0z479jA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平滑反转文档频率</p></figure><p id="c090" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此，为了计算列(word) <em class="nf"> i </em>和行(document) <em class="nf"> j </em>的TF-IDF，我们使用以下公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/9d27710e309f7ec9dd02ef44a5b49f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5idWzzDKMiCF9GOYpSvzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF</p></figure><p id="93b5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">正如我前面提到的，我们需要从语料库中获取单词包。我们可以再次使用我们的<code class="fe ng nh ni lz b">BagOfWords()</code>函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="6698" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用下面的函数计算TF-IDF，</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="d66f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">运行这几行代码会将df转换成N-grams表示，</p><pre class="kj kk kl km gt ly lz ma mb aw mc bi"><span id="a640" class="md me it lz b gy mf mg l mh mi">BoW = BagOfWords(df["Comments"])<br/>dftf = TFIDF(df, BoW)</span></pre><p id="7682" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">变量<code class="fe ng nh ni lz b">dftf</code>现在是<code class="fe ng nh ni lz b">data.csv</code>的TF-IDF表示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="3e9d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果需要应用平滑，记得在<code class="fe ng nh ni lz b">TFIDF()</code>功能中指定<code class="fe ng nh ni lz b">smoothing = True</code>。您可以修改我们的代码，在TF-IDF中使用n元语法。</p><h2 id="c25e" class="md me it bd mj mk ml dn mm mn mo dp mp lj mq mr ms ln mt mu mv lr mw mx my mz bi translated">恭喜</h2><p id="1bc6" class="pw-post-body-paragraph la lb it lc b ld na ju lf lg nb jx li lj nc ll lm ln nd lp lq lr ne lt lu lv im bi translated">我们已经完成了这个关于基本文本表示的简单教程。我鼓励你探索更复杂的表示，比如<code class="fe ng nh ni lz b">word2vec</code>或<code class="fe ng nh ni lz b">doc2vec</code>。访问我的<a class="ae ky" href="https://github.com/gerchristko/Text-Representation" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>查看数据集、表示输出和完整代码。如果你喜欢这篇文章，请考虑鼓掌并关注。请在评论中分享你的反馈和想法。感谢您的阅读！</p></div></div>    
</body>
</html>