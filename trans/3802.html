<html>
<head>
<title>Your Dataset Is Imbalanced? Do Nothing!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的数据集不平衡？什么都不做！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-dataset-is-imbalanced-do-nothing-abf6a0049813#2022-08-24">https://towardsdatascience.com/your-dataset-is-imbalanced-do-nothing-abf6a0049813#2022-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="784c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">阶级不平衡不是问题。揭穿了一个最普遍的误解在ML社区。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5f120bf5b53e3d7385092476af426715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZjpQ8iZrJ_f1cqzgsyfgg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[图片由作者提供]</p></figure><p id="2742" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我经常从数据科学家那里听到的一句话是:</p><blockquote class="lu"><p id="9880" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">"我们有一个问题:这个数据集是不平衡的."</p></blockquote><p id="9f78" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">讲了这些之后，他们通常会开始提出一些平衡技术来“修复”数据集，比如欠采样、过采样、SMOTE等等。</p><p id="812e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我认为这是机器学习社区中最普遍的误解之一。所以，我想明确声明:</p><blockquote class="lu"><p id="4268" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">阶层失衡不是问题！</p></blockquote><p id="c0d1" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">在这篇文章中，我将向你解释为什么。</p><p id="f455" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，借助一个简单的例子，我将让您直观地了解为什么在大多数情况下，用欠采样、过采样或SMOTE来平衡数据集会适得其反。</p><p id="6d46" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望这能帮助你避免做不必要的(或有害的)工作，而把注意力集中在重要的问题上。</p><h1 id="1987" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">重要的事情先来。什么是阶层失衡？</h1><p id="9480" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">机器学习中最常见的任务之一是二进制分类。在这些任务中，目标是建立一个可以预测某个现象是否会发生的机器学习模型。现象可能是任何东西:客户是否会流失，是否会发生地震，用户是否会喜欢一部电影。</p><p id="8dc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于问题是二元的，我们的目标向量只由1(也称为正)和0(也称为负)组成。</p><p id="ec1e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当积极因素比消极因素少得多时，就会出现阶层失衡。阳性占总数的百分比也称为患病率。即使没有硬阈值，当患病率≤ 10%时，我们也会同意考虑数据集不平衡。</p><p id="70be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在实际应用中，类不平衡是最常见的情况。的确，<strong class="la iu">很多值得解决的问题本来就不平衡</strong>。这是因为资源有限。由于您希望将资源投入到少数可能有积极结果的案例中，因此能够预测罕见事件(如客户呕吐、地震或用户喜欢电影)的模型是非常有价值的。</p><h1 id="b54d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">问题出在哪里？</h1><p id="6c2b" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">那么，阶层失衡的问题出在哪里？</p><p id="2c42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你谷歌一下“类不平衡机器学习”，你会发现一些类似这样的文章:</p><ul class=""><li id="4e9f" class="ng nh it la b lb lc le lf lh ni ll nj lp nk lt nl nm nn no bi translated">"<a class="ae np" href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" rel="noopener ugc nofollow" target="_blank">对抗你的机器学习数据集</a>中不平衡类的8个策略"。</li><li id="0bfc" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nl nm nn no bi translated"><a class="ae np" href="https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习中处理不平衡类的10个技巧</a>。</li><li id="d7c2" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nl nm nn no bi translated"><a class="ae np" rel="noopener" target="_blank" href="/5-techniques-to-work-with-imbalanced-data-in-machine-learning-80836d45d30c">机器学习中处理不平衡数据的5种技术</a>。</li></ul><p id="2545" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从这些标题中，你可以看出阶级不平衡是一个主要问题。但真的是这样吗？</p><p id="6935" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我查阅了各种资源，发现了一篇有趣的文章，标题是“<a class="ae np" href="https://www.researchgate.net/publication/268201268_Cost-Sensitive_Learning_and_the_Class_Imbalance_Problem" rel="noopener ugc nofollow" target="_blank">代价敏感学习和类不平衡问题</a>”，是“机器学习百科”的一部分。</p><p id="6c1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在文章中，作者提到了在处理不平衡数据集时可能会遇到的3个问题:</p><ol class=""><li id="a2e0" class="ng nh it la b lb lc le lf lh ni ll nj lp nk lt nv nm nn no bi translated">当你实际上想要以不同的方式衡量误差时，你<strong class="la iu">使用准确度作为度量标准</strong>。</li><li id="6cf4" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nv nm nn no bi translated">你<strong class="la iu">假设训练数据集和服务数据集的类别分布是相同的，而不是</strong>。</li><li id="14a9" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nv nm nn no bi translated">你没有足够的正面实例来训练一个可靠的ML模型。</li></ol><p id="db6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">奇怪的是:</p><blockquote class="lu"><p id="199a" class="lv lw it bd lx ly lz ma mb mc md lt dk translated">在所有这些情况下，问题都是阶级不平衡。</p></blockquote><p id="40f4" class="pw-post-body-paragraph ky kz it la b lb me ju ld le mf jx lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">事实上，让我们分别来看它们:</p><ol class=""><li id="0a68" class="ng nh it la b lb lc le lf lh ni ll nj lp nk lt nv nm nn no bi translated">问题是你<strong class="la iu">选择了与你的目的</strong>有关的错误指标:这与不平衡的数据无关。关键是您应该选择一个适合您的数据集的指标，而不是相反。</li><li id="a680" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nv nm nn no bi translated">问题是<strong class="la iu">训练/发球偏斜</strong>:与不平衡数据无关。事实上，如果您有一个流行率为50%的训练数据集，您的数据是完全平衡的，但是如果您的服务数据流行率为10%，您的模型无论如何都会受到训练/服务偏斜的影响。</li><li id="53c9" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nv nm nn no bi translated">问题在于<strong class="la iu">数据稀缺</strong>:这与不平衡的数据无关。事实上，如果你有5个阳性和5个阴性，你的数据集是完全平衡的，但你没有足够的数据来建立一个ML模型。相反，如果你有100万个阳性和10亿个阴性，你的数据集严重不平衡，但你有足够多的数据。</li></ol><p id="12a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">无论是否存在阶层失衡</strong>，这3个问题都可能存在。</p><p id="70a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，即使您的数据集完全平衡，您也应该经常问自己是否选择了正确的指标，是否可能存在训练/服务偏差，或者是否没有足够的数据来训练模型。</p><h1 id="d068" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">秀，不要说</h1><p id="7e9c" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在上一段，我们已经看到了为什么阶级不平衡不是问题的理论解释。但是您可能仍然认为使用欠采样、过采样或其他技术来平衡数据集不会有任何害处，对吗？</p><p id="eb5b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不对。</p><p id="f5c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们借助一个玩具例子来看看为什么。</p><p id="0d12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有一个100，000个观察值的训练数据集。数据集只有一个连续的要素。目标变量是二元的，有10%的正变量。因此，数据集是不平衡的。</p><p id="271a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你的同事建议你对少数民族阶层进行过度采样，以“对抗阶层失衡”。所以你随机抽取正面的例子，直到你达到完美的平衡:50%的负面和50%的正面。</p><p id="7880" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，假设我们可以观察由10，000个观察值组成的服务数据集(即，您的模型在推理时将遇到的数据)，其中12%是正面的。</p><p id="602a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过下面的代码模拟这种情况:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a694" class="ob mk it nx b gy oc od l oe of"><strong class="nx iu"># generating toy datasets</strong></span><span id="9587" class="ob mk it nx b gy og od l oe of">import numpy as np</span><span id="a9cc" class="ob mk it nx b gy og od l oe of">negatives_train = np.random.normal(5,1,90_000)<br/>positives_train = np.random.normal(8,1,10_000)<br/>x_train = np.concatenate([negatives_train, positives_train])<br/>y_train = np.array([0] * len(negatives_train) + [1] * len(positives_train))</span><span id="87d2" class="ob mk it nx b gy og od l oe of">positives_train_os = np.random.choice(positives_train, size=len(negatives_train))<br/>x_train_os = np.concatenate([negatives_train, positives_train_os])<br/>y_train_os = np.array([0] * len(negatives_train) + [1] * len(positives_train_os))</span><span id="d183" class="ob mk it nx b gy og od l oe of">negatives_serve = np.random.normal(5,1,8_800)<br/>positives_serve = np.random.normal(8,1,1_200)<br/>y_serve = np.array([0] * len(negatives_serve) + [1] * len(positives_serve))<br/>x_serve = np.concatenate([negatives_serve, positives_serve])</span></pre><p id="829c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据的图形表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/096d59cdb4ded644bb7ed167c919624f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUcfNYOKDd_6XgoBTeSiIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练数据集、过采样训练数据集和服务数据集中的负分布和正分布。[图片由作者提供]</p></figure><p id="1332" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们希望建立一个机器学习模型，帮助我们根据x的值预测y的值。我们将使用一个只有一个参数的模型:决策阈值。该模型的工作原理如下:</p><ul class=""><li id="0d91" class="ng nh it la b lb lc le lf lh ni ll nj lp nk lt nl nm nn no bi translated">如果特征≥阈值，那么我们的预测是1。</li><li id="2dff" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nl nm nn no bi translated">如果特征&lt; threshold, then our prediction is 0.</li></ul><p id="6dd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">During training, what binary classifiers do is try to reach the parameters that minimize the loss. In our case, we have just one parameter, so training the classifier will consist in trying many possible thresholds and picking the one that gives the smallest loss. The default loss used in binary classification is log-loss, so we will stick to that.</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="4b63" class="ob mk it nx b gy oc od l oe of"><strong class="nx iu"># "training" our model on the original dataset</strong></span><span id="3a3b" class="ob mk it nx b gy og od l oe of">from sklearn.metrics import log_loss</span><span id="5806" class="ob mk it nx b gy og od l oe of">ts = np.linspace(np.min(x_train), np.max(x_train), 100)</span><span id="911f" class="ob mk it nx b gy og od l oe of">log_loss_train = [log_loss(y_train, x_train &gt;= t) for t in ts]</span><span id="9343" class="ob mk it nx b gy og od l oe of">best_thres_train = ts[np.argmin(log_loss_train)]</span></pre><p id="7c98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Let’s plot the log-loss as a function of the threshold:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/4157727e73ba06c6a4f0735eccb17115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s9DwtLTqdGZ5fJq6UE6KpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Training dataset. Log-loss corresponding to any possible threshold. [Image by Author]</p></figure><p id="7afb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Using the original training data, we have that the best threshold (i.e. the one that produces the smallest log-loss) equals 7.29.</p><p id="ce8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">But what happens if we use the oversampled (i.e. balanced) dataset?</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="8b81" class="ob mk it nx b gy oc od l oe of"><strong class="nx iu"># "training" our model on the original dataset</strong></span><span id="500a" class="ob mk it nx b gy og od l oe of">log_loss_train_os = [log_loss(y_train_os, x_train_os &gt;= t) for t in ts]</span><span id="476c" class="ob mk it nx b gy og od l oe of">best_thres_train_os = ts[np.argmin(log_loss_train_os)]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/0b78a5262dbaac5e5e39a5eda811f6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBeYRQIteGsSQ0aY2YhzoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Oversampled training dataset. Log-loss corresponding to any possible threshold. [Image by Author]</p></figure><p id="bf19" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">The log-loss curve is very different now, it is practically symmetrical, leading to choosing a threshold of 6.47.</p><p id="f78c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">The final aim of any model is to perform best on unseen data. So, let’s see the log-loss curve also on serving data:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/081c54a9992c367994cebfa43459f22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4PRNeBZii2QTcW2ry6Y9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Comparing log-loss curves on the different datasets. [Image by Author]</p></figure><p id="3b1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">As it was easy to imagine, the log-loss curve of the serving data is much more similar to the one of original data than to the one of oversampled data. Indeed:</p><ul class=""><li id="3bd4" class="ng nh it la b lb lc le lf lh ni ll nj lp nk lt nl nm nn no bi translated">with threshold = 7.29 (best log-loss on <strong class="la iu">原始数据</strong>，服务数据的对数损耗为<strong class="la iu"> 1.28 </strong>。</li><li id="0f6a" class="ng nh it la b lb nq le nr lh ns ll nt lp nu lt nl nm nn no bi translated">阈值= 6.47(对<strong class="la iu">过采样数据</strong>的最佳对数损失)，对服务数据的对数损失为<strong class="la iu"> 2.30 </strong>。</li></ul><p id="3e4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，<strong class="la iu">如果您在过采样数据集上训练模型，那么与在原始(不平衡)数据集上训练相比，您在服务数据上获得的性能会更差</strong>。</p><p id="438d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个简单的解释，说明了为什么只有在您确切知道自己在做什么的情况下，才应该对数据集进行过采样/欠采样。不存在自动平衡不平衡数据集的首选方法。</p><h1 id="3148" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">综上</h1><p id="5da1" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">综上所述，阶级失衡本身从来都不是问题。</p><p id="fee3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时，它与不同的问题有关，如错误的指标、培训/服务偏差或数据匮乏。在这种情况下，你应该找到解决问题的方法，而不是解决阶级不平衡的问题。</p></div></div>    
</body>
</html>