<html>
<head>
<title>Implementing a Transformer from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现转换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2#2022-03-23">https://towardsdatascience.com/7-things-you-didnt-know-about-the-transformer-a70d93ced6b2#2022-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d9ed" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于变压器，你可能不知道的 7 件令人惊讶的事情</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/128ff045016082e08bd0c40cea68b149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b6KZpAgND0FyX3qK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@silasbaisch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Silas Baisch </a>拍摄</p></figure><h1 id="29a2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="5e9c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了更好地熟悉变形金刚的具体细节，我决定从头开始实现“<a class="ae kv" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部”</a>一文中的原始架构。我以为我知道所有该知道的事情，但是令我自己惊讶的是，我遇到了几个意想不到的实现细节，这让我更好地理解了所有事情是如何在幕后工作的。</p><p id="2900" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇文章的目的不是讨论整个实现——有很多很好的资源可以讨论——而是为了<strong class="lq ir">强调</strong> <strong class="lq ir">我发现的特别令人惊讶或有见地的七件事，以及你可能不知道的</strong>。我将通过使用超级链接机器人<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch" rel="noopener ugc nofollow" target="_blank">指向我的<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch" rel="noopener ugc nofollow" target="_blank">代码</a>中的特定行来具体说明这一点🤖</a>(试试吧！).代码应该很容易理解:它有很好的文档记录，并使用 Github 动作自动进行单元测试和类型检查。</p><p id="85e6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个帖子的结构很简单。前三点围绕实施多头关注；最后四个是关于其他组件的。我假设你对变形金刚和多头注意力有概念上的熟悉(如果没有；一个很好的起点是<a class="ae kv" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变形金刚</a>，它极大地帮助了我更好地理解多头注意力背后的机制。也是涉及最多的一点。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c7ec" class="mu kx iq mq b gy mv mw l mx my"><strong class="mq ir">Table of Contents</strong></span><span id="514b" class="mu kx iq mq b gy mz mw l mx my"><a class="ae kv" href="#29a2" rel="noopener ugc nofollow">Introduction</a><br/>1. <a class="ae kv" href="#81ba" rel="noopener ugc nofollow">Multi-head attention is implemented with one weight matrix</a><br/>2. <a class="ae kv" href="#e7de" rel="noopener ugc nofollow">The dimensionality of the key, query and value vectors is not a hyperparameter</a><br/>3. <a class="ae kv" href="#5d70" rel="noopener ugc nofollow">Scaling in dot-product attention avoids extremely small gradients</a><br/>4. <a class="ae kv" href="#1f65" rel="noopener ugc nofollow">The source embedding, target embedding AND pre-softmax linear share the same weight matrix</a><br/>5. <a class="ae kv" href="#0c55" rel="noopener ugc nofollow">During inference we perform a decoder forward pass for every token; during training we perform a single forward pass for the entire sequence</a><br/>6. <a class="ae kv" href="#f5d8" rel="noopener ugc nofollow">Transformers can handle arbitrarily long sequences, in theory</a><br/>7. <a class="ae kv" href="#7ea0" rel="noopener ugc nofollow">Transformers are residual streams</a><br/><a class="ae kv" href="#2830" rel="noopener ugc nofollow">Wrapping up</a></span></pre><h1 id="81ba" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> 1。多头注意力通过一个权重矩阵实现</strong></h1><p id="c3f5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们深入研究之前；回想一下，对于每个注意力头<strong class="lq ir">，我们需要一个查询、键以及每个输入令牌的值向量</strong>。然后我们计算<strong class="lq ir">注意力分数</strong>作为一个查询和句子中所有关键向量<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L170" rel="noopener ugc nofollow" target="_blank">之间的<strong class="lq ir">比例点积</strong>的软最大值🤖</a>)。下面的等式一次计算每个输入标记的所有值向量的<strong class="lq ir">注意力加权</strong>平均值。<strong class="lq ir"> Q </strong>是为所有输入令牌堆叠<strong class="lq ir"> q </strong>查询向量的矩阵；<strong class="lq ir"> K </strong>和<strong class="lq ir"> V </strong>对<strong class="lq ir"> k </strong> ey 和<strong class="lq ir">V</strong>value 向量做同样的事情。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/dfad4a67512caa6fd02b72431970b786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*445F8KQUl-i7pXws7vCZ1Q.png"/></div></div></figure><p id="5158" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那么，我们如何有效地为所有令牌和头文件获得这些查询、键和值向量<strong class="lq ir">？事实证明，我们可以使用单个权重矩阵</strong> <code class="fe nb nc nd mq b">W</code>一次性完成这个<strong class="lq ir">。这不同于人们在阅读<a class="ae kv" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>或<a class="ae kv" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变压器</a>后可能期望的<strong class="lq ir">三个</strong>投影权重矩阵。让我们来看看这是如何工作的。</strong></p><p id="8528" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">假设我们的输入由<code class="fe nb nc nd mq b">4</code>个记号组成:[“嘿”、“怎么样”、“是”、“你”]，我们的嵌入大小是<code class="fe nb nc nd mq b">512</code>。暂时忽略批处理，让<code class="fe nb nc nd mq b">X</code>作为行的<code class="fe nb nc nd mq b">4x512</code>矩阵堆栈令牌嵌入。</p><p id="2448" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">设<code class="fe nb nc nd mq b">W</code>是一个有<code class="fe nb nc nd mq b">512</code>行和<code class="fe nb nc nd mq b">1536</code>列的权重矩阵。我们现在将放大这个<code class="fe nb nc nd mq b">512x1536</code>维度权重矩阵<code class="fe nb nc nd mq b">W</code> ( <a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L20" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">🤖</strong> </a>)找出<strong class="lq ir">为什么</strong>我们需要<code class="fe nb nc nd mq b">1536</code>维度和<strong class="lq ir">如何</strong>乘以<code class="fe nb nc nd mq b">X</code>得到矩阵<code class="fe nb nc nd mq b">P</code>(对于<strong class="lq ir"> p </strong>项目)包含我们需要的所有查询、键和值向量。(代码中的<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L94" rel="noopener ugc nofollow" target="_blank">🤖</a>我称这个矩阵为<code class="fe nb nc nd mq b">qkv</code></p><h2 id="27d2" class="mu kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">多头注意力背后的矩阵乘法</h2><p id="e8f5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">得到的<code class="fe nb nc nd mq b">4x1536</code>矩阵<code class="fe nb nc nd mq b">P=X@W</code>中的每一个元素都是 <code class="fe nb nc nd mq b">X</code>【一个嵌入】<strong class="lq ir">中的一个行向量</strong><strong class="lq ir"><strong class="lq ir">和</strong> <code class="fe nb nc nd mq b">W</code>中的一个列向量</strong> <strong class="lq ir">【某些权重】之间的</strong>(换句话说:点积)<strong class="lq ir">的和<strong class="lq ir">。</strong></strong></p><p id="dfa9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作为矩阵乘法的复习，下图展示了当两个<code class="fe nb nc nd mq b">3x3</code>矩阵相乘时，如何计算简单<code class="fe nb nc nd mq b">3x3</code>矩阵的第一个元素。同样的策略也适用于我们更大的矩阵<code class="fe nb nc nd mq b">X</code>和<code class="fe nb nc nd mq b">W</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/e51c70058d47d91ef50f5c91a7004de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sei83Vn1PsVSSrlbYVdAWQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">举例:如何计算 3×3 矩阵乘法中的第一个元素？作者和 Rosalie Gubbels 的图片。</p></figure><p id="8940" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，我们投影矩阵中第<code class="fe nb nc nd mq b">i</code>行<code class="fe nb nc nd mq b">P[i, :]</code>的每个<strong class="lq ir">元素</strong>都是<code class="fe nb nc nd mq b">i</code>第<strong class="lq ir">令牌</strong> <strong class="lq ir">嵌入</strong> <code class="fe nb nc nd mq b">X[i, :]</code>和<code class="fe nb nc nd mq b">W</code>中<strong class="lq ir">权重列</strong>之一的<strong class="lq ir">线性组合</strong>。这意味着我们可以简单地在权重矩阵<code class="fe nb nc nd mq b">W</code>中堆叠更多的列，从而为嵌入在<code class="fe nb nc nd mq b">X</code>中的每个令牌创建更多独立的线性组合(标量)。换句话说，<code class="fe nb nc nd mq b">P</code>中的每个元素都是嵌入在<code class="fe nb nc nd mq b">X</code>中的令牌的不同标量“<strong class="lq ir">视图</strong>或“<strong class="lq ir">摘要</strong>”，由<code class="fe nb nc nd mq b">W</code>中的一列加权。这是理解带有“查询”、“键”和“值”向量的八个“头”<strong class="lq ir">如何在<code class="fe nb nc nd mq b">P</code>的每一行中隐藏</strong>的关键。</p><h2 id="8753" class="mu kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">揭示注意力和查询、关键和价值向量</h2><p id="3faf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以<strong class="lq ir">将</strong>我们为<code class="fe nb nc nd mq b">W</code>选择的<code class="fe nb nc nd mq b">1536</code>列(最终作为<code class="fe nb nc nd mq b">P</code>中的列数)分解到<code class="fe nb nc nd mq b">1536 = 8 * 3 * 64</code>中。<strong class="lq ir">在</strong> <code class="fe nb nc nd mq b">P</code>中，我们现在发现了隐藏在每一行中的八个头，每一个都有三个 64 维向量！每个这样的“向量”或“块”由嵌入令牌的<code class="fe nb nc nd mq b">64</code>不同加权线性组合组成，我们<strong class="lq ir">选择</strong> <strong class="lq ir">以某种方式解释</strong>。你可以在下图中看到<code class="fe nb nc nd mq b">P</code>的可视化表示以及如何分解它。分解也发生在代码中(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L96" rel="noopener ugc nofollow" target="_blank">🤖</a>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/7061e6f8e9ef0c01911eb40b19cce3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxY5DVJgizXV8PtjUnuEEA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">P=X@W 包含所有头部的查询、键和值投影。作者和 Rosalie Gubbels 的图片。</p></figure><p id="7920" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于一个<strong class="lq ir">批次</strong>中的多个句子，简单地想象一下在 P“后面”的第三维，它将 2D 矩阵变成 3D 矩阵。</p><h2 id="6f88" class="mu kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">编码器-解码器注意</h2><p id="4bca" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">对于编码器-解码器来说，这稍微复杂一些。回想一下，编码器-解码器关注允许每个解码器关注由最顶端的编码器输出的嵌入</p><p id="a897" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了引起编码器-解码器的注意，我们需要<strong class="lq ir">查询向量</strong>用于<strong class="lq ir">解码器</strong>令牌嵌入，以及<strong class="lq ir">键和值向量</strong>用于最顶层的<strong class="lq ir">编码器</strong>令牌嵌入。这就是为什么我们将<code class="fe nb nc nd mq b">W</code>一分为二——一个<code class="fe nb nc nd mq b">512x512</code>和一个<code class="fe nb nc nd mq b">512x1024</code>矩阵(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L122" rel="noopener ugc nofollow" target="_blank">🤖</a> ) —并执行两个独立的投影:一个从编码器的嵌入中获得键和值向量(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L125" rel="noopener ugc nofollow" target="_blank">🤖</a>)，一个用于获得解码器嵌入的查询向量(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L132" rel="noopener ugc nofollow" target="_blank">🤖</a>)。</p><p id="6553" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，注意我们确实需要<strong class="lq ir">第二个权重矩阵</strong> ( <a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L21" rel="noopener ugc nofollow" target="_blank">🤖</a>)在多头注意力中混合来自每个头的值向量，并获得每个令牌的单个上下文嵌入(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L76" rel="noopener ugc nofollow" target="_blank">🤖</a>)。</p><h1 id="e7de" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">2.关键字、查询和值向量的维度是<strong class="ak">而不是超参数</strong></h1><p id="01dd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我从未真正考虑过这个问题，但我总是假设查询、键和值向量的维度是一个超参数。结果，<strong class="lq ir">被动态设置为嵌入维数除以头数</strong> : <code class="fe nb nc nd mq b">qkv_dim = embed_dim/num_heads = 512/8 = 64</code> ( <a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/multi_head_attention.py#L16" rel="noopener ugc nofollow" target="_blank">🤖</a>)。</p><p id="30cd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这似乎是 Vaswani 等人的设计选择，以保持多头注意力中的参数数量恒定，而不管选择的头数。<strong class="lq ir">虽然您可能认为参数的数量会随着头的增加而增加，但实际情况是查询、键和值向量的维度会减少。</strong></p><p id="f162" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们看看上面显示<code class="fe nb nc nd mq b">R=X@W</code>的图，并想象单头注意力，这就变得很清楚了。<code class="fe nb nc nd mq b">X</code>、<code class="fe nb nc nd mq b">W</code>和<code class="fe nb nc nd mq b">R</code>中的元素数量保持八个头不变，但是我们<strong class="lq ir">解释<code class="fe nb nc nd mq b">R</code> <strong class="lq ir"> </strong>中的</strong>元素的方式改变了<strong class="lq ir">。</strong>使用单个头，我们每个令牌嵌入只有一个查询、键和值投影(<code class="fe nb nc nd mq b">P</code>中的一行)，它们将跨越每行的三分之一:<code class="fe nb nc nd mq b">512</code>元素—与嵌入大小相同。</p><p id="896f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你可能会想，多头有什么意义呢？嗯，Vaswani 等人认为，它允许头部捕捉不同的“表征子空间”。比如说；一个头可能跟踪句法关系，而另一个更关注位置信息。有相当多的工作调查这在实践中是否确实发生，例如在<a class="ae kv" href="https://aclanthology.org/P19-1580.pdf" rel="noopener ugc nofollow" target="_blank">翻译</a>中。事实上，几年前在<a class="ae kv" href="https://arxiv.org/pdf/1911.03898.pdf" rel="noopener ugc nofollow" target="_blank">总结</a>中我自己也做了一些这方面的工作。</p><h1 id="5d70" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak"> 3。点积注意力的缩放避免了极小的梯度</strong></h1><p id="c045" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">类似于前一点，我从来没有真正思考过<strong class="lq ir">为什么</strong>我们用一些常数来划分注意力逻辑🤖但是这实际上很简单。</p><p id="eae8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">回想一下，每个 logit 都是一个查询和一个关键向量之间的点积(即元素乘积之和)的结果。因此，更大的维度数<code class="fe nb nc nd mq b">qkv_dim</code>会导致该总和中更多的乘积——导致注意力逻辑中<strong class="lq ir">更高的方差</strong>。正如我们在下面的例子中看到的，对具有<strong class="lq ir">高方差</strong>的 logits 进行 softmax 变换会导致<strong class="lq ir">极小的输出概率</strong>——因此<strong class="lq ir">微小的梯度</strong>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="1f65" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">4.源嵌入、目标嵌入和预 softmax 线性共享相同的权重矩阵</h1><p id="4e5a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们现在从多头注意力转移到“<strong class="lq ir">权重绑定</strong>”——序列到序列模型的常见做法。我发现这很有趣，因为嵌入权重矩阵实际上弥补了相对于模型其余部分的<strong class="lq ir">巨大</strong>数量的参数。给定 30k 令牌的词汇表和 512 的嵌入大小，这个矩阵包含<strong class="lq ir">1530 万个</strong>参数！</p><p id="1487" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">想象一下有三个这样的矩阵:一个将源标记索引映射到嵌入，一个将目标标记映射到嵌入，一个将解码器的每个最顶层上下文化标记嵌入映射到目标词汇表(前 softmax 线性层)上的逻辑。是啊；这给我们留下了 4600 万个参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/3bf7b41276e6af7c1fc86ee5a8ac58fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFLEJIKQhQkNNZ0JFINfIw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">重量捆绑:三个红色方块共享相同的重量矩阵。作者和 Rosalie Gubbels 的图片。</p></figure><p id="c3e4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在代码中你可以看到我初始化了主 transformer 类中的<strong class="lq ir">一个</strong>嵌入层(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/transformer.py#L33" rel="noopener ugc nofollow" target="_blank">🤖</a>)，我用它作为编码器嵌入(￠<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/encoder.py#L24" rel="noopener ugc nofollow" target="_blank">￠</a>)，解码器嵌入(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/decoder.py#L31" rel="noopener ugc nofollow" target="_blank">🤖</a>)和解码器预软最大变换权重(<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/decoder.py#L44" rel="noopener ugc nofollow" target="_blank">🤖</a>)。</p><h1 id="0c55" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">5.在推理过程中，我们为每个令牌执行一次解码器前向传递；在训练中，我们在整个序列中执行一次向前传球</h1><p id="34e2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这一点对某些人来说可能是显而易见的——尤其是那些从事序列到序列任务的人——但是对于理解一个变形人实际上是如何被训练的却至关重要。</p><h2 id="4b7b" class="mu kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">推理</h2><p id="450f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">假设我们处于<strong class="lq ir">推理模式</strong>，在这种模式下，我们自回归(一个接一个)预测目标令牌。转换器总是输出<strong class="lq ir">批</strong>中每个令牌的词汇分布。基于批次中最后一个令牌索引<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/transformer.py#L112" rel="noopener ugc nofollow" target="_blank">的的<strong class="lq ir">输出分布<strong class="lq ir">预测下一个令牌🤖</strong></strong></a>)。这意味着我们基本上<strong class="lq ir">抛弃</strong>所有先前指数的所有输出分布。</p><h2 id="de3b" class="mu kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">培训和教师强制</h2><p id="f419" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这与我们使用<strong class="lq ir">老师强迫</strong>的<strong class="lq ir">培训</strong>形成对比。在训练期间，我们只执行<strong class="lq ir">一次</strong>正向通过解码器<strong class="lq ir">，而不管序列长度</strong> ( <a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/train.py#L46" rel="noopener ugc nofollow" target="_blank">🤖</a>)。我们(<strong class="lq ir">老师)</strong> <strong class="lq ir">强制</strong>——一次性喂完整批<strong class="lq ir"/>。这给了我们<strong class="lq ir">所有</strong>下一个令牌预测，为此我们计算<strong class="lq ir">平均</strong>损失。</p><p id="d005" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">注意<strong class="lq ir">每个标记预测都是基于先前的地面实况标记，而不是先前预测的标记！</strong>还要注意，这种单次前向传递等同于仅使用地面真实令牌作为输入并忽略先前预测的自回归解码(！)，但效率要高得多。我们使用一个<strong class="lq ir">注意力</strong> <strong class="lq ir">掩码</strong>来限制解码器自我注意力模块关注未来令牌(标签)并作弊。</p><p id="7a8d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我认为，认识到这种称为教师强制的训练方式不仅适用于翻译模型，也适用于最受欢迎的预训练自回归语言模型，如 GPT-3，是很有用的。</p><h1 id="f5d8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">6.理论上，变压器可以处理任意长的序列…</h1><p id="5079" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">…然而，实际上，多头注意力具有<a class="ae kv" href="https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html#:~:text=With commonly available current hardware,summarization or genome fragment classification" rel="noopener ugc nofollow" target="_blank">计算和内存需求</a>，将序列长度限制在 512 个令牌左右。事实上，像 BERT 这样的模型对输入序列长度施加了硬限制，因为它们使用学习嵌入而不是正弦编码。这些学习到的位置<strong class="lq ir"> </strong>嵌入类似于令牌嵌入，并且类似地仅适用于达到某个数量的预定义位置集(例如，对于 BERT 为 512)。</p><h1 id="7ea0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">7.变形金刚是残余流</h1><p id="d931" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">说到最后一点。我喜欢把一个变压器想象成多个“剩余流”。这类似于 LSTM 如何在逐个处理新令牌的同时保持从左到右的水平“内存流”，并通过门来调节信息流。</p><p id="9c04" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在一个 transformer 中，这个流不是跨令牌横向 <strong class="lq ir">运行<strong class="lq ir">，而是跨层</strong>(例如编码器)和子层(即多头关注和全连接层)纵向</strong> <strong class="lq ir">。每个子层使用剩余连接简单地将信息添加到剩余流中。<a class="ae kv" href="https://transformer-circuits.pub/2021/framework/index.html" rel="noopener ugc nofollow" target="_blank">这篇</a>是一篇非常棒的博文，更详细地讨论了剩余流，而<a class="ae kv" href="https://openreview.net/pdf?id=HyzdRiR9Y7" rel="noopener ugc nofollow" target="_blank">这篇</a>是一篇很酷的论文，利用了“垂直递归”的概念。</strong></p><p id="3afe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个剩余流的结果是<strong class="lq ir">中间令牌表示的维数在所有(子)层</strong>中必须相同，因为剩余连接<strong class="lq ir">增加了</strong> <strong class="lq ir">两个向量</strong>(编码器中的例子:<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch/blob/main/encoder.py#L90" rel="noopener ugc nofollow" target="_blank">🤖</a>)。最重要的是，因为编码器(和类似的解码器)层相互堆叠，它们的输出形状必须与输入形状匹配。</p><h1 id="2830" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">正在总结…</h1><p id="7cc3" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">感谢你阅读这篇文章！如果你喜欢它，有问题，或者发现了错误，请告诉我。你可以在推特上给我发消息，或者在 LinkedIn 上联系我。看看我在 jorisbaan.nl/posts 的其他博客。</p><p id="58a0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">尽管我写的<a class="ae kv" href="https://github.com/jsbaan/transformer-from-scratch" rel="noopener ugc nofollow" target="_blank">代码</a>很容易理解(它有很好的文档记录，经过单元测试和类型检查),请在实践中使用官方的 PyTorch 实现😁。</p><p id="ef0d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">感谢 David Stap 提出从零开始实现变形金刚的想法，感谢 Dennis Ulmer 和 Elisa Bassignana 对本文的反馈，感谢 Lucas de Haas 的 bug 搜索会议和 Rosalie Gubbels 的视觉创作。我寻找灵感的一些伟大资源是 PyTorch transformer <a class="ae kv" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>和<a class="ae kv" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py" rel="noopener ugc nofollow" target="_blank">实现</a>；菲利普·利佩的《变形金刚》<a class="ae kv" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html" rel="noopener ugc nofollow" target="_blank">教程</a>；亚历山大·拉什的<a class="ae kv" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变形金刚</a>和杰伊·阿拉玛的<a class="ae kv" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">带插图的变形金刚</a>。</p></div></div>    
</body>
</html>