<html>
<head>
<title>Using a Decision Tree — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决策树—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-ii-using-a-decision-tree-ddffa4004e47#2022-02-22">https://towardsdatascience.com/part-ii-using-a-decision-tree-ddffa4004e47#2022-02-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="edb4" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">使用Julia及其生态系统的机器学习</h2><div class=""/><div class=""><h2 id="dd51" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">MLJ是Julia中机器学习的元包。它的第一个版本在2019年才发布。因此，它的创造者可以在该领域现有软件包十多年经验的基础上再接再厉。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f98e47f87ef067ab1c25c70fae868c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfvgT04_2VpXHDj8UxnevQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@jan_huber?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">简·侯伯</a>在<a class="ae le" href="https://unsplash.com/s/photos/tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="d38b" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">教程概述</h1><p id="4e3d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这是本教程的第二部分，它展示了Julia的特定语言特性和来自其生态系统的各种高质量的包是如何在一个典型的ML工作流中轻松组合使用的。</p><ul class=""><li id="7fa2" class="mu mv iq lz b ma mw md mx mg my mk mz mo na ms nb nc nd ne bi translated"><a class="ae le" rel="noopener" target="_blank" href="/part-i-analyzing-the-glass-dataset-c556788a496f">第一部分</a>“<em class="mt">分析玻璃数据集”</em>集中讲述如何使用<code class="fe nf ng nh ni b">ScientificTypes</code>、<code class="fe nf ng nh ni b">DataFrames</code>、<code class="fe nf ng nh ni b">StatsBase</code>和<code class="fe nf ng nh ni b">StatsPlots</code>等软件包对数据进行预处理、分析和可视化。</li><li id="53c9" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">第二部分"<em class="mt">使用决策树"</em>关注ML工作流的核心:如何选择模型，以及如何使用它进行训练、预测和评估。这部分主要靠包<code class="fe nf ng nh ni b">MLJ</code>(=<strong class="lz ja">M</strong>achine<strong class="lz ja">L</strong>in<strong class="lz ja">J</strong>ulia<em class="mt">)</em>收入。</li><li id="dca7" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">第三部分“<em class="mt">如果事情还没有‘准备好使用’</em>”解释了如果可用的包没有提供您需要的所有功能，那么用几行代码创建您自己的解决方案是多么容易。</li></ul><h1 id="535b" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><p id="b658" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在本教程的第二部分中，机器学习算法将应用于“玻璃”数据集，我们在第一部分中对其进行了分析。</p><h2 id="b0ad" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">MLJ——朱莉娅的机器学习</h2><p id="aab1" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">为此，我们使用<code class="fe nf ng nh ni b"><a class="ae le" href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="noopener ugc nofollow" target="_blank">MLJ</a></code>封装。它是一个所谓的元包，为选择、调整、评估、组合和比较大量的ML模型提供了一个通用的接口和通用的实用机制。在撰写本文时，它提供了186种型号。</p><p id="4f9c" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">MLJ是伦敦图灵研究所与奥克兰大学以及其他科学和工业伙伴合作的一个项目。始于2019年。</p><h2 id="6aa1" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">接下来的步骤</h2><p id="78d2" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">使用ML算法通常包括以下步骤:</p><ul class=""><li id="e687" class="mu mv iq lz b ma mw md mx mg my mk mz mo na ms nb nc nd ne bi translated">准备数据，以便获得用于培训和测试的适当结构。</li><li id="8c10" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">选择适合给定数据并产生所需结果(例如分类或聚类)的适当模型。</li><li id="707d" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">用训练数据集训练选择的模型。</li><li id="8729" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">使用测试数据集评估所训练的模型。</li></ul><p id="907d" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">我们将在下一节中应用这个(基本的)ML工作流。这里我们假设我们的数据集存在于第一部分创建的<code class="fe nf ng nh ni b">glass</code>数据框中。</p><h1 id="61e9" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">ML工作流程</h1><h2 id="6710" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">准备数据</h2><p id="0578" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">几乎所有的ML模型都期望训练数据集是表格形式的。所以我们在第一部分中使用的<code class="fe nf ng nh ni b">DataFrame</code>数据类型是一个很好的起点。</p><p id="b14d" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">在我们的例子中，我们有一个所谓的分类问题:我们希望模型在给定一组值时预测正确的玻璃类型(即我们的特征<em class="mt"> RI </em>和<em class="mt">Na</em>…<em class="mt">Ba</em>；独立属性)。将使用独立属性的数据以及每种情况下产生的相应玻璃类型来训练模型。这就是所谓的监督学习。</p><p id="06e5" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">对于这种训练，我们需要表格形式的独立属性数据和单独列表中的结果类型(在列<code class="fe nf ng nh ni b">Type</code>)。函数<code class="fe nf ng nh ni b">unpack</code>完成这项工作。它将<code class="fe nf ng nh ni b">glass</code>中的<code class="fe nf ng nh ni b">DataFrame</code>拆分为这两个组件(<code class="fe nf ng nh ni b">XGlass</code>和<code class="fe nf ng nh ni b">yGlass</code>)。除此之外，它还使用一个随机数发生器对行进行洗牌，该随机数发生器用“种子”<code class="fe nf ng nh ni b">rng = 123</code>进行初始化。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="48d5" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">用<code class="fe nf ng nh ni b">scitype</code>检查结果数据结构的科学类型，我们可以看到，我们确实得到了一个<em class="mt">向量</em>和一个<em class="mt">表</em>:</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="f68d" class="no lg iq ni b gy oi oj l ok ol">scitype(yGlass) --&gt; AbstractVector{Multiclass{7}}</span><span id="2d70" class="no lg iq ni b gy om oj l ok ol">scitype(XGlass) --&gt; Table{AbstractVector{Continuous}}</span></pre><h2 id="c5cd" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">选择模型</h2><p id="2e0b" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">现在我们必须选择一个合适的模型。<code class="fe nf ng nh ni b">models</code>函数给出了MLJ提供的所有型号的列表。目前这个列表中有186个型号，这可能有点让人不知所措。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="f2e2" class="no lg iq ni b gy oi oj l ok ol">all_models = models() --&gt;</span><span id="dbf3" class="no lg iq ni b gy om oj l ok ol">186-element Vector{ ... }:<br/> (name = ABODDetector, package_name = OutlierDetectionNeighbors, … )<br/> (name = ABODDetector, package_name = OutlierDetectionPython, … )<br/> (name = AEDetector, package_name = OutlierDetectionNetworks, … )<br/> (name = ARDRegressor, package_name = ScikitLearn, … )<br/> (name = AdaBoostClassifier, package_name = ScikitLearn, … )<br/> (name = AdaBoostRegressor, package_name = ScikitLearn, … )<br/> (name = AdaBoostStumpClassifier, package_name = DecisionTree, … )<br/> (name = AffinityPropagation, package_name = ScikitLearn, … )<br/> (name = AgglomerativeClustering, package_name = ScikitLearn, … )<br/> (name = BM25Transformer, package_name = MLJText, … )<br/> (name = BaggingClassifier, package_name = ScikitLearn, … )<br/> (name = BaggingRegressor, package_name = ScikitLearn, … )<br/> (name = BayesianLDA, package_name = MultivariateStats, … )<br/> (name = BayesianLDA, package_name = ScikitLearn, … )<br/> ⋮<br/> (name = Standardizer, package_name = MLJModels, … )<br/> (name = SubspaceLDA, package_name = MultivariateStats, … )<br/> (name = TSVDTransformer, package_name = TSVD, … )<br/> (name = TfidfTransformer, package_name = MLJText, … )<br/> (name = TheilSenRegressor, package_name = ScikitLearn, … )<br/> (name = UnivariateBoxCoxTransformer, package_name = MLJModels, … )<br/> (name = UnivariateDiscretizer, package_name = MLJModels, … )<br/> (name = UnivariateFillImputer, package_name = MLJModels, … )<br/> (name = UnivariateStandardizer, package_name = MLJModels, … )<br/> (name = UnivariateTimeTypeToContinuous, package_name = MLJModels,…)<br/> (name = XGBoostClassifier, package_name = XGBoost, … )<br/> (name = XGBoostCount, package_name = XGBoost, … )<br/> (name = XGBoostRegressor, package_name = XGBoost, … )</span></pre><p id="4714" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">因此，我们应该缩小列表范围:首先，我们希望看到哪些模型基本上与数据集的数据类型“兼容”。我们所有的特征都有<em class="mt">连续的数值</em>值，并且产生的类(玻璃类型)是一个<em class="mt">名义</em>值。</p><p id="e9c5" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">将<code class="fe nf ng nh ni b">matching(XGlass, yGlass)</code>与<code class="fe nf ng nh ni b">models</code>配合使用，可过滤出符合该要求的型号。这将数量大大减少到47:</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="aa6b" class="no lg iq ni b gy oi oj l ok ol">compatible_models = models(matching(XGlass, yGlass)) --&gt;</span><span id="5df6" class="no lg iq ni b gy om oj l ok ol">47-element Vector{ … }:<br/> (name = AdaBoostClassifier, package_name = ScikitLearn, … )<br/> (name = AdaBoostStumpClassifier, package_name = DecisionTree, … )<br/> (name = BaggingClassifier, package_name = ScikitLearn, … )<br/> (name = BayesianLDA, package_name = MultivariateStats, … )<br/> (name = BayesianLDA, package_name = ScikitLearn, … )<br/> (name = BayesianQDA, package_name = ScikitLearn, … )<br/> (name = BayesianSubspaceLDA, package_name = MultivariateStats, … )<br/> (name = ConstantClassifier, package_name = MLJModels, … )<br/> (name = DecisionTreeClassifier, package_name = BetaML, … )<br/> (name = DecisionTreeClassifier, package_name = DecisionTree, … )<br/> (name = DeterministicConstantClassifier, package_name =MLJModels,…)<br/> (name = DummyClassifier, package_name = ScikitLearn, … )<br/> (name = EvoTreeClassifier, package_name = EvoTrees, … )<br/> (name = ExtraTreesClassifier, package_name = ScikitLearn, … )<br/> ⋮<br/> (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, … )<br/> (name = RandomForestClassifier, package_name = BetaML, … )<br/> (name = RandomForestClassifier, package_name = DecisionTree, … )<br/> (name = RandomForestClassifier, package_name = ScikitLearn, … )<br/> (name = RidgeCVClassifier, package_name = ScikitLearn, … )<br/> (name = RidgeClassifier, package_name = ScikitLearn, … )<br/> (name = SGDClassifier, package_name = ScikitLearn, … )<br/> (name = SVC, package_name = LIBSVM, … )<br/> (name = SVMClassifier, package_name = ScikitLearn, … )<br/> (name = SVMLinearClassifier, package_name = ScikitLearn, … )<br/> (name = SVMNuClassifier, package_name = ScikitLearn, … )<br/> (name = SubspaceLDA, package_name = MultivariateStats, … )<br/> (name = XGBoostClassifier, package_name = XGBoost, … )</span></pre><p id="a9f8" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">除了这个相当技术性的“兼容性”,我们对我们的模型还有一些其他的期望:对于本教程，我们想要一个具有高度可解释性的模型，也就是说，当对一些数据进行分类时，应该很容易理解它是如何工作的。理想情况下，它应该在纯Julia中实现，因为这是一个关于Julia和ML的教程。</p><p id="1f40" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><em class="mt">决策树</em>分类器具有高度的可解释性。所以让我们看看，我们是否能在这个列表中找到一些在Julia中实现的。</p><p id="1e94" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">MLJ的所有模型都提供了全面的元信息(如下例所示)。例如，<code class="fe nf ng nh ni b">docstring</code>包含一个简短的文本描述，<code class="fe nf ng nh ni b">is_pure_julia</code>告诉我们，如果它是用纯Julia实现的(或者只是其他一些用Julia包装的编程语言)，最后每个模型都有一个<code class="fe nf ng nh ni b">name</code>。我们可以使用这个元信息来过滤我们的兼容模型列表(使用Julia的<code class="fe nf ng nh ni b">filter</code>-函数，它接受一个任意的匿名函数作为过滤表达式):</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="ef43" class="no lg iq ni b gy oi oj l ok ol">filter(<br/>    m -&gt; m.is_pure_julia &amp;&amp; <br/>         contains(m.docstring * m.name, “DecisionTree”),<br/>    compatible_models<br/>)</span><span id="3464" class="no lg iq ni b gy om oj l ok ol">4-element Vector{ ... }:<br/> (name = AdaBoostStumpClassifier, package_name = DecisionTree, … )<br/> (name = DecisionTreeClassifier, package_name = BetaML, … )<br/> (name = DecisionTreeClassifier, package_name = DecisionTree, … )<br/> (name = RandomForestClassifier, package_name = DecisionTree, … )</span></pre><p id="6b05" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">所以我们可以把名单缩减到四个候选人。第一项和第四项是高级决策树模型。但是我们更喜欢‘经典’的，为了本教程的目的，它很容易理解。这就是我们关注第二和第三位候选人的原因。这些模型的<code class="fe nf ng nh ni b">docstring</code>包含一个到GitHub库的URL，在那里我们也可以找到相关的文档:第二个是<a class="ae le" href="https://github.com/sylvaticus/BetaML.jl" rel="noopener ugc nofollow" target="_blank"> BetaML包</a>的一部分，第三个可以在<a class="ae le" href="https://github.com/bensadeghi/DecisionTree.jl" rel="noopener ugc nofollow" target="_blank">决策树包</a>中找到。</p><p id="ea34" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">两者都实现了著名的<a class="ae le" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank"> CART算法</a>并且非常相似，但是仔细看看文档，后一个似乎更成熟并且提供了更多的功能。所以我们选择这个。</p><p id="c72d" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">这里作为一个例子，对于那些感兴趣的人，3号候选者的所有元数据的完整列表。这种描述适用于MLJ的每一款车型:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="891b" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">使用决策树分类器</h2><p id="f7d8" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">使用<code class="fe nf ng nh ni b">@load</code>宏可以在MLJ加载模型。它返回一个类型定义(我们在这里称之为<code class="fe nf ng nh ni b">MyDecisionTree</code>)。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="5f26" class="no lg iq ni b gy oi oj l ok ol">MyDecisionTree = <a class="ae le" href="http://twitter.com/load" rel="noopener ugc nofollow" target="_blank">@load</a> DecisionTreeClassifier pkg = ”DecisionTree” </span></pre><p id="8e78" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">在下一步中，我们创建一个这种类型的实例<code class="fe nf ng nh ni b">dc</code>。该实例包含所有参数及其默认值。因此，我们可以看到，例如，默认情况下树的深度不受限制(<code class="fe nf ng nh ni b">max_depth = -1</code>)，并且在创建后不会被修剪(<code class="fe nf ng nh ni b">post_prune = false</code>)。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="d03c" class="no lg iq ni b gy oi oj l ok ol">dc = MyDecisionTree() --&gt;</span><span id="3ead" class="no lg iq ni b gy om oj l ok ol">DecisionTreeClassifier(<br/> max_depth = -1,<br/> min_samples_leaf = 1,<br/> min_samples_split = 2,<br/> min_purity_increase = 0.0,<br/> n_subfeatures = 0,<br/> post_prune = false,<br/> merge_purity_threshold = 1.0,<br/> pdf_smoothing = 0.0,<br/> display_depth = 5,<br/> rng = Random._GLOBAL_RNG())</span></pre><p id="18b8" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><strong class="lz ja">训练模特</strong></p><p id="9439" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">我们现在拥有训练模型的一切。首先，我们使用简单的维持策略将数据分成<em class="mt">训练数据集</em>和<em class="mt">测试数据集</em>(在本例中是70:30分割)。也就是说，我们将30%的数据放在一边用于测试，仅将70%用于训练。</p><p id="1eb5" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><code class="fe nf ng nh ni b">partition</code>完成这项工作，并在应用拆分之前使用随机数生成器(我们将其种子初始化为<code class="fe nf ng nh ni b">rng = 123</code>)来打乱行。由于我们已经将数据垂直拆分为<code class="fe nf ng nh ni b">XGlass</code>和<code class="fe nf ng nh ni b">yGlass</code>，这些组件将分别拆分为<code class="fe nf ng nh ni b">Xtrain</code> / <code class="fe nf ng nh ni b">Xtest</code>和<code class="fe nf ng nh ni b">ytrain</code> / <code class="fe nf ng nh ni b">ytest</code>。我们通过设置<code class="fe nf ng nh ni b">multi = true</code>来通知<code class="fe nf ng nh ni b">partition</code>这个特殊情况。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="6129" class="no lg iq ni b gy oi oj l ok ol">(Xtrain, Xtest), (ytrain, ytest) = <br/> partition((XGlass, yGlass), 0.7, multi = true, rng = 123)</span></pre><p id="cd4a" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">因此<code class="fe nf ng nh ni b">Xtrain, ytrain</code>现在包含70%的“玻璃”数据集，而<code class="fe nf ng nh ni b">Xtest, ytest</code>包含剩余的30%。下图显示了我们如何将数据集分成上述四个部分:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/c726e459755b4621fea3b1c079205c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRodhCL8xVdAufUvHaOlVg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">横向和纵向分割的玻璃数据集[图片由作者提供]</p></figure><p id="0ec7" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">接下来，我们应用以下步骤进行培训和测试:</p><ul class=""><li id="67c6" class="mu mv iq lz b ma mw md mx mg my mk mz mo na ms nb nc nd ne bi translated">首先，使用所谓的<em class="mt">机器</em>将模型和训练数据连接起来。</li><li id="6a97" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">然后通过调用机器上的<code class="fe nf ng nh ni b">fit!</code>进行培训。</li><li id="1eaf" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">之后，经过训练的机器可用于对新数据进行预测(使用<code class="fe nf ng nh ni b">predict</code>)。</li></ul><p id="e7cd" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">为了简单起见，我们预先将生成的决策树的深度限制为3。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="831d" class="no lg iq ni b gy oi oj l ok ol">dc.max_depth = 3<br/>dc_mach = machine(dc, Xtrain, ytrain)<br/>fit!(dc_mach)</span></pre><p id="9755" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><strong class="lz ja">预测和评估</strong></p><p id="eee5" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">所以让我们看看这个模型在测试数据集<code class="fe nf ng nh ni b">Xtest</code>(使用MLJs <code class="fe nf ng nh ni b">predict</code> -function): <code class="fe nf ng nh ni b">yhat = predict(dc_mach, Xtest)</code>上的预测有多好。</p><p id="534a" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><code class="fe nf ng nh ni b">yhat</code>现在包含了我们的模型对测试数据集<code class="fe nf ng nh ni b">Xtest</code>的预测。<code class="fe nf ng nh ni b">yhat</code>的前三个元素如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/52d2be1dfac84dc3ac0435aa151c4c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6B00ElY6UhG8YkTJHLO55Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">“yhat”的前三个元素[作者图片]</p></figure><p id="916f" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">也许<code class="fe nf ng nh ni b">yhat</code>中的预测看起来不像你所期望的:<code class="fe nf ng nh ni b">yhat</code>不是一个预测玻璃类型的列表，而是一个<em class="mt">分布列表</em>陈述了每种可能玻璃类型的预测概率。这源于选择的模型(a <code class="fe nf ng nh ni b">DecisionTreeClassifier</code>)是一个<em class="mt">概率模型</em>(从它的元信息可以看出:<code class="fe nf ng nh ni b">prediction_type = :probabilistic</code>)。MLJ所有的概率模型都预测分布(使用<code class="fe nf ng nh ni b">Distributions.jl</code>)。</p><p id="fa7e" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">但是只需调用<code class="fe nf ng nh ni b">predict_mode</code>(而不是<code class="fe nf ng nh ni b">predict</code>)就可以很容易地获得预测类的列表(每个分布中概率最高的类):</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="9e91" class="no lg iq ni b gy oi oj l ok ol">yhat_classes = predict_mode(dc_mach, Xtest) --&gt;</span><span id="33e3" class="no lg iq ni b gy om oj l ok ol">64-element CategoricalArrays.CategoricalArray{String,1,UInt32}:<br/> “headlamps”<br/> “build wind float”<br/> “containers”<br/> “build wind float”<br/> “vehic wind float”<br/> “build wind non-float”<br/> “build wind non-float”<br/> “build wind float”<br/> “headlamps”<br/> ⋮<br/> “build wind non-float”<br/> “build wind float”<br/> “headlamps”<br/> “build wind non-float”<br/> “containers”<br/> “build wind float”<br/> “build wind float”<br/> “build wind float”</span></pre><p id="8f8c" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">在此基础上，我们可以通过比较<code class="fe nf ng nh ni b">ytest</code>(来自测试集的<em class="mt">正确</em>类)和<code class="fe nf ng nh ni b">yhat_classes</code>(来自我们的分类器的<em class="mt">预测</em>类)来检查有多少类被正确预测。</p><p id="a4ce" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">注意，我们要比较这两个数组的<em class="mt">元素</em>(不是数组本身)。这可以在Julia中使用所谓的<em class="mt">广播</em>机制来实现，该机制通过在所应用的函数前面使用一个点来表示(在这种情况下，使用<code class="fe nf ng nh ni b">==</code>进行相等比较)。所以<code class="fe nf ng nh ni b">ytest .== yhat_classes</code>进行元素间的比较，得到一个布尔值列表。最后，<code class="fe nf ng nh ni b">count</code>计算列表中<code class="fe nf ng nh ni b">true</code>值的数量。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="6c4e" class="no lg iq ni b gy oi oj l ok ol">correct_classes = count(ytest .== yhat_classes) --&gt; 44</span><span id="2643" class="no lg iq ni b gy om oj l ok ol">accuracy = correct_classes / length(ytest) --&gt; 0.6875</span></pre><p id="d4be" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">使用计数的结果，我们可以看到，在测试集中的64个实例中，有44个被正确预测。这个比率是68.75%。</p><p id="b7ab" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated"><strong class="lz ja">打印决策树</strong></p><p id="4c94" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">现在，如果我们能看到由我们的<code class="fe nf ng nh ni b">DecisionTreeClassifier</code>构建的决策树是什么样子，那就太好了。然后，我们可以检查哪些属性已经用于分支，以及在哪个阈值发生了分支。</p><p id="efcc" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">幸运的是，有一个用于此目的的内置函数。<code class="fe nf ng nh ni b">report</code>是一个通用函数，可在经过训练的机器上调用，并传递(取决于使用的型号)关于经过训练的机器的所有相关信息。在决策树的情况下，这个“相关信息”是一个<code class="fe nf ng nh ni b">TreePrinter</code>对象，可以使用<code class="fe nf ng nh ni b">print_tree()</code>调用。</p><pre class="kp kq kr ks gt oe ni of og aw oh bi"><span id="2153" class="no lg iq ni b gy oi oj l ok ol">report(dc_mach).print_tree() --&gt;</span><span id="3890" class="no lg iq ni b gy om oj l ok ol">Feature 3, Threshold 2.745<br/>L-&gt; Feature 2, Threshold 13.77<br/>    L-&gt; Feature 4, Threshold 1.38<br/>        L-&gt; 2 : 6/7<br/>        R-&gt; 5 : 9/12<br/>    R-&gt; Feature 8, Threshold 0.2<br/>        L-&gt; 6 : 8/10<br/>        R-&gt; 7 : 18/19<br/>R-&gt; Feature 4, Threshold 1.42<br/>    L-&gt; Feature 1, Threshold 1.51707<br/>        L-&gt; 3 : 5/11<br/>        R-&gt; 1 : 40/55<br/>    R-&gt; Feature 3, Threshold 3.42<br/>        L-&gt; 2 : 5/10<br/>        R-&gt; 2 : 23/26</span></pre><p id="1cd4" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">因此，我们可以看到，例如，树根处的第一个分支使用3号特征(即<em class="mt"> Mg </em> =镁)在阈值2.745处进行第一次分割。也就是说，<em class="mt">Mg</em>-值≤ 2.745的所有实例将使用左侧子分支中的规则进一步分类，而<em class="mt">Mg</em>-值&gt;为2.745的实例进入右侧分支。</p><p id="6a7b" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">上面<code class="fe nf ng nh ni b">print_tree()</code>的输出给出了决策树的样子...但是说实话，它有点基础:它只是ASCII文本，节点只显示属性号，而不是用于分支的属性名。此外，树叶不显示类名，在这一点上得到预测。如果有一个包含更多信息的图形化描述不是很好吗？</p><p id="c744" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">由于没有现成的函数可用于此目的，我们不得不考虑如何使用Julia生态系统中的其他方法来实现这一目的。这就是我们将在本教程的第三部分中做的事情…所以请跟我走这条路:-)。</p><h1 id="b451" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结论</h1><p id="fe63" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">上面的例子显示了如何使用MLJ中的综合元数据来选择合适的模型，以及如何使用通用界面来应用ML工作流的典型步骤。该接口对于所有提供的型号都是相同的。这大大减少了学习曲线以及使用各种模型时出现应用程序错误的可能性。</p><p id="7c11" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">在本教程中，只能介绍MLJs功能的一小部分。除了这个相当简单的摘录之外，该软件包还在以下方面提供了大量的功能:</p><ul class=""><li id="f19f" class="mu mv iq lz b ma mw md mx mg my mk mz mo na ms nb nc nd ne bi translated"><strong class="lz ja"> <em class="mt">评估</em> </strong>:有大量的度量标准可用于评估模型。在此过程中，可以应用各种保留和交叉验证策略以及用户定义的采样策略。</li><li id="9f93" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated"><strong class="lz ja"><em class="mt"/></strong>:提供多种超参数(半)自动调谐策略(可添加用户自定义策略)。</li><li id="c807" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated"><strong class="lz ja"> <em class="mt">构图模型</em> </strong> : MLJ提供不同变化的模型构图。<br/> <em class="mt">管道</em>可用于链接(从而自动化)ML工作流的不同处理步骤。<br/> <em class="mt">同类组合</em>允许相同类型(但内部工作不同)的模型组合，例如减少过度拟合的影响。<br/> <em class="mt">模型堆叠</em> <strong class="lz ja"> <em class="mt"> </em> </strong>是从不同类型的模型中创建新模型的方法。使用这些合成机制创建的所有新模型都是“一等公民”。也就是说，它们可以以与基本型号相同的方式使用。</li></ul><p id="7846" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">科学类型的优势(它是MLJ的一个组成部分)已经在教程的第一部分中展示了。</p><p id="a190" class="pw-post-body-paragraph lx ly iq lz b ma mw ka mc md mx kd mf mg nz mi mj mk oa mm mn mo ob mq mr ms ij bi translated">与Python不同，在Python中，每个性能关键的代码都必须用C语言编写，在这里，我们可以为整个ML栈应用一种语言(即Julia)。除了降低复杂性和维护工作量之外，这种方法还有其他优点:</p><ul class=""><li id="0e3e" class="mu mv iq lz b ma mw md mx mg my mk mz mo na ms nb nc nd ne bi translated">超参数可以使用梯度下降算法(反过来使用基于一种语言环境的自动区分)进行调整。</li><li id="dd4a" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">可以使用高性能硬件架构(GPU、并行计算),而无需进行重大代码重构。</li></ul><h1 id="6a32" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">进一步的信息</h1><ul class=""><li id="3b5b" class="mu mv iq lz b ma mb md me mg op mk oq mo or ms nb nc nd ne bi translated">艾伦图灵研究所的MLJ <a class="ae le" href="https://www.turing.ac.uk/research/research-projects/machine-learning-julia" rel="noopener ugc nofollow" target="_blank">项目主页</a>。</li><li id="803b" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">MLJ的文档以及大量的介绍材料可以在<a class="ae le" href="https://alan-turing-institute.github.io/MLJ.jl/dev/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li><li id="978e" class="mu mv iq lz b ma nj md nk mg nl mk nm mo nn ms nb nc nd ne bi translated">Anthony Blaom基于Jupyter和Pluto笔记本创建了一系列互动教程(最近在我的帮助下),涵盖了MLJ的所有重要方面。它们源自他在2020年Julia con<a class="ae le" href="https://juliacon.org/2020/" rel="noopener ugc nofollow" target="_blank">举办的</a><a class="ae le" href="https://www.youtube.com/watch?v=qSWbCn170HU&amp;t=27s" rel="noopener ugc nofollow" target="_blank">工作坊</a>。</li></ul></div></div>    
</body>
</html>