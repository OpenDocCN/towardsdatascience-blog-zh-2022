<html>
<head>
<title>Dimensionality reduction with PCA and t-SNE in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中基于主成分分析和 t-SNE 的降维方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-with-pca-and-t-sne-in-python-c80c680221d#2022-03-16">https://towardsdatascience.com/dimensionality-reduction-with-pca-and-t-sne-in-python-c80c680221d#2022-03-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="342a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数据科学学会研讨会 18:什么是降维、PCA 实现、t-SNE 实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b61abecc4d37e320980905b69b089544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UsHDG40bP1EZR4_S"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">阿什利·朱利斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4b5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今年，作为 UCL 数据科学协会的科学负责人，该协会将在整个学年举办一系列 18 场研讨会，主题包括数据科学家工具包 Python 简介和机器学习方法。每个人的目标是创建一系列的小博客文章，这些文章将概述主要观点，并为任何希望跟进的人提供完整研讨会的链接。所有这些都可以在我们的<a class="ae ky" href="https://github.com/UCL-DSS" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中找到，并将在全年更新新的研讨会和挑战。</p><p id="f633" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列的第十八个研讨会是 Python 数据科学研讨会系列的一部分，涵盖了降维方法。在本次研讨会中，我们将讨论什么是降维以及主成分分析和 t 分布随机邻居嵌入方法的实施。和往常一样，这篇博文是整个研讨会的总结，可以在这里找到<a class="ae ky" href="https://github.com/UCL-DSS/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">这里</a>还包括使用随机森林分类的数据准备和特征提取。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5469" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你错过了我们最近三次研讨会中的任何一次，你可以在这里找到这些</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-practical-introduction-to-hierarchical-clustering-from-scikit-learn-ffaf8ee2670c"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">scikit-learn 中的分层聚类实用介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会研讨会 17:什么是层次聚类、实现、解释和评估</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-practical-introduction-to-kmeans-clustering-using-scikit-learn-fd9cff95144b"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">使用 scikit-learn 进行 Kmeans 聚类的实用介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会研讨会 16:什么是 Kmeans 集群、实现、评估和解释</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-practical-introduction-to-support-vector-machines-from-scikit-learn-6e678cf1f228"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">来自 scikit-learn 的支持向量机实用介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">UCL 数据科学学会研讨会 15:什么是支持向量机，如何实现它们，以及如何评估它们</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="6b1d" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">什么是降维？</h2><p id="eacd" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">降维(在大多数情况下)属于无监督的机器学习算法，这意味着我们没有特定的目标。该方法的主要目的是减少数据集中的要素数量，从而减少模型所需的资源，或者在执行任何分析之前帮助可视化数据。这是通过减少数据集中属性或变量的数量，同时尽可能多地保留原始数据集中的变化来实现的。这是一个预处理步骤，意味着它主要在我们创建或训练任何模型之前执行，将它与通常在初始建模之后完成的特征提取分开。</p><p id="c370" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有许多算法可用于降维，但它们主要来自两组主要的线性代数和流形学习:</p><p id="aadc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性代数</strong></p><p id="84b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是从矩阵分解方法中得出的，矩阵分解方法可以通过检查我们可能使用的变量之间的线性关系来进行降维。这一分支中的常用方法包括:</p><ul class=""><li id="abee" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">主成分分析</li><li id="0c18" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">奇异值分解</li><li id="5c55" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">非负矩阵分解</li><li id="2f9c" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">要素分析</li><li id="aae6" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">线性判别分析</li></ul><p id="407e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">流形学习</strong></p><p id="1147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与线性代数方法的不同之处在于，它使用非线性方法来降维，因此与线性代数方法相比，它通常可以捕获变量之间更复杂的关系。这一分支中一些流行的方法包括:</p><ul class=""><li id="5df3" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">Isomap 嵌入</li><li id="3299" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">局部线性嵌入</li><li id="30cb" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">多维标度</li><li id="5d0d" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">光谱嵌入</li><li id="7c1a" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">t 分布随机邻居嵌入</li></ul><p id="2126" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">特征提取</strong></p><p id="62a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这组方法可以松散地归入降维的范畴，因为尽管上述方法从现有变量中创建新的变量组合，但特征提取只是移除特征。我们已经从<a class="ae ky" href="https://python.plainenglish.io/a-practical-introduction-to-random-forest-classifiers-from-scikit-learn-536e305d8d87" rel="noopener ugc nofollow" target="_blank">随机森林</a>中看到了这一点，但是在这方面流行的方法包括:</p><ul class=""><li id="47bf" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">反向消除</li><li id="ff63" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">预选</li><li id="0e34" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">随机森林</li></ul><p id="feba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维体系下的每种算法都提供了不同的方法来应对降维的挑战。这通常意味着不存在适用于所有情况的最佳降维方法。这也意味着，如果不使用受控实验和广泛探索，就没有简单的方法来为您的数据找到最佳算法。在这种程度上，我们将包括在 NBA 数据集上实现 PCA 和 t-SNE，目的是可视化关于球员位置的二维数据。</p><h2 id="9340" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">主成分分析</h2><p id="5b7b" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">PCA 只是降维的线性代数方法之一。这有助于我们从现有的大量变量中提取一组新的变量，这些新变量采用主成分的形式。这样做的目的是用最少的主成分获取尽可能多的信息。在最小化信息损失的同时获得的变量越少，在计算资源和时间方面有助于模型训练，并且有助于可视化。</p><p id="3812" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中，主成分是变量的线性组合，其组织方式是第一主成分解释数据集中的最大方差。然后，第二个主成分试图解释数据集中与第一个主成分不相关的剩余方差，以此类推。</p><p id="3413" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了实现 PCA 算法，我们需要归一化数据，因为否则它将导致对具有大方差的变量的大量关注，这是不期望的。这意味着我们不应该将 PCA 应用于分类变量，因为尽管我们可以将它们转化为数值变量，但它们只会取 0 和 1 的值，这自然会有很高的方差。</p><p id="ebd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，在实现模型之前，我们将使用来自<code class="fe oi oj ok ol b">scikit-learn</code>库的<code class="fe oi oj ok ol b">StandardScaler</code>来缩放数据:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="ad17" class="mw mx it ol b gy oq or l os ot">#import the standard scaler<br/>from sklearn.preprocessing import StandardScaler</span><span id="079e" class="mw mx it ol b gy ou or l os ot">#initialise the standard scaler<br/>sc = StandardScaler()</span><span id="d995" class="mw mx it ol b gy ou or l os ot">#create a copy of the original dataset<br/>X_rs = X.copy()</span><span id="ec4b" class="mw mx it ol b gy ou or l os ot">#fit transform all of our data<br/>for c in X_rs.columns:<br/>    X_rs[c] = sc.fit_transform(X_rs[c].values.reshape(-1,1))</span></pre><p id="b88f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以对标准化数据实施模型，如下所示:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="6c17" class="mw mx it ol b gy oq or l os ot">#import the PCA algorithm from sklearn<br/>from sklearn.decomposition import PCA</span><span id="ac7b" class="mw mx it ol b gy ou or l os ot">#run it with 15 components<br/>pca = PCA(n_components=15, whiten=True)</span><span id="9cb2" class="mw mx it ol b gy ou or l os ot">#fit it to our data<br/>pca.fit(X_rs)</span><span id="621b" class="mw mx it ol b gy ou or l os ot">#extract the explained variance<br/>explained_variance = pca.explained_variance_ratio_<br/>singular_values = pca.singular_values_</span></pre><p id="3502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经使 PCA 适合我们的数据，我们实际上需要评估它产生的成分。我们可以通过解释方差比来做到这一点，方差比可用于查看主成分的有用程度，从而选择要在模型中使用的主成分。我们可以把这想象成:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="14a0" class="mw mx it ol b gy oq or l os ot">#create an x for each component<br/>x = np.arange(1,len(explained_variance)+1)</span><span id="000b" class="mw mx it ol b gy ou or l os ot">#plot the results<br/>plt.plot(x, explained_variance)</span><span id="c8bc" class="mw mx it ol b gy ou or l os ot">#add a y label<br/>plt.ylabel('Share of Variance Explained')<br/>plt.title("PCA explained variance plot")<br/>plt.xlabel("Components")</span><span id="67af" class="mw mx it ol b gy ou or l os ot">#show the resuling plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/9699b8f7ef00b199eebf3b178fce5d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFq4OYhLSx4dNSk-IGl_Fg.png"/></div></div></figure><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="43f2" class="mw mx it ol b gy oq or l os ot">#iterate over the components<br/>#to print the explained variance<br/>for i in range(0, 15):<br/>    print(f"Component {i:&gt;2} accounts for {explained_variance[i]*100:&gt;2.2f}% of variance")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/e3e001f188d1d07ced1cf3ace2e94006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xg3_MnxWrf-0hGp9B-38rg.png"/></div></div></figure><p id="eb93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们要在模型中使用这些数据，那么我们需要决定在模型中使用多少组件来平衡计算资源和模型性能之间的平衡。为此，我们可以使用几种方法来选择主成分的最佳数量:</p><ul class=""><li id="b701" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">检查解释的方差图中的拐点，在我们的例子中，它出现在 4- 6 个主要成分周围</li><li id="85b9" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">保留占数据集中方差 1%以上的分量，在我们的例子中是在 14 个分量之后</li><li id="9e6d" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">在模型中保留总计占解释方差 80%的变量，在我们的例子中是前 7 个组成部分。</li></ul><p id="7e84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可能取决于主成分分析的目的和您正在实施的模型。在我们的例子中，我们想尝试在二维空间中可视化数据，以及这与我们的目标变量的关系。因此，我们可以这样实现:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="bc41" class="mw mx it ol b gy oq or l os ot">#set the components to 2<br/>pca = PCA(n_components=2, whiten=True) <br/>#fit the model to our data and extract the results<br/>X_pca = pca.fit_transform(X_rs)</span><span id="809f" class="mw mx it ol b gy ou or l os ot">#create a dataframe from the dataset<br/>df = pd.DataFrame(data = X_pca,<br/>                 columns = ["Component 1", <br/>                            "Component 2"])</span><span id="ee25" class="mw mx it ol b gy ou or l os ot">#merge this with the NBA data<br/>NBA = pd.merge(NBA,<br/>              df,<br/>              left_index=True,<br/>              right_index=True,<br/>              how = "inner")</span><span id="d9ff" class="mw mx it ol b gy ou or l os ot">#plot the resulting data from two dimensions<br/>g = sns.jointplot(data = NBA,<br/>                 x = "Component 1",<br/>                 y = "Component 2",<br/>                 hue = "Pos")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/5f5201024ed36bf186475458f4984a04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*On5USwruD2fLh_DWLH76aQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d36a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个图中我们可以看到，将数据减少到两个分量并不一定表明玩家的位置和他们的统计数据之间的明确关系。我们可以从中得出的结论是，中锋的位置可能与其他位置不同，有几个位置与大前锋的位置相融合。而与此同时，控卫、得分后卫和小前锋之间存在着相当大的混合，这在某种程度上是可以预料的。</p><p id="7529" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，您是否希望这是两个或更多的主成分(14 个成分的方差大于 1%)将取决于您最终想要对数据做什么。在早期的<a class="ae ky" href="https://python.plainenglish.io/a-practical-introduction-to-random-forest-classifiers-from-scikit-learn-536e305d8d87" rel="noopener ugc nofollow" target="_blank">随机森林分类器研讨会</a>的情况下，我们可能希望在模型中使用 14 个组件，而不是我们最初开始时的 23 个，因为其余的可能是噪声，只会增加模型的复杂性。</p><h2 id="c6d6" class="mw mx it bd my mz na dn nb nc nd dp ne li nf ng nh lm ni nj nk lq nl nm nn no bi translated">t-SNE</h2><p id="894c" class="pw-post-body-paragraph kz la it lb b lc np ju le lf nq jx lh li nr lk ll lm ns lo lp lq nt ls lt lu im bi translated">t-SNE 是另一种降维算法，但与 PCA 不同，它能够解释非线性关系。在这个意义上，数据点可以通过两种主要方式映射到较低的维度:</p><ul class=""><li id="e041" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated">局部方法:将较高维度上的邻近点映射到较低维度上的邻近点</li><li id="dbcc" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated">全局方法:试图在所有比例下保持几何形状，使附近的点靠得很近，同时使远处的点彼此远离</li></ul><p id="69e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t-SNE 是为数不多的能够通过计算点在高维度和低维度空间的概率相似性在低维度数据中保留两种结构的降维算法之一。</p><p id="97f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t-SNE 的输出通常被称为用数据创建的维度，其中固有特征在数据中不再可识别。这意味着我们不能仅基于 t-SNE 的输出做出任何直接的推断，将其限制于主要的数据探索和可视化，尽管输出也可以用于分类和聚类(尽管不能用于测试和训练数据集)。</p><p id="ca19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实施 t-SNE 算法时，我们需要注意几个关键参数:</p><ul class=""><li id="190b" class="nu nv it lb b lc ld lf lg li nw lm nx lq ny lu nz oa ob oc bi translated"><code class="fe oi oj ok ol b">n_components</code>:从数据中创建的尺寸</li><li id="8331" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe oi oj ok ol b">perplexity</code>:与其他流形学习算法中使用的最近邻数相关，用于确定要保留的模型中全局和局部关系的权衡。虽然注意到 t-SNE 通常对此参数不太敏感，但它应该小于建议的最佳范围在 5 到 50 之间的点数</li><li id="8ef2" class="nu nv it lb b lc od lf oe li of lm og lq oh lu nz oa ob oc bi translated"><code class="fe oi oj ok ol b">learning_rate</code>:学习率通常是模型行为的关键参数，因此在这方面探索参数空间是值得的，但它通常在 100 到 1000 之间。</li></ul><p id="8850" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">scikit-learn 文档建议，由于算法的复杂性，如果数据集中的特征数量超过 50，则在 t-SNE 之前使用 PCA 或截断 SVD。然而，在我们的例子中，这不应该是一个问题，所以我们可以在我们已经有的数据上实现这个模型。因为我们的目标主要是观想，那么我们可以将维度的数量设置为两个，以便能够在 2D 观想图上绘制如下:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="5e1e" class="mw mx it ol b gy oq or l os ot">#import the method<br/>from sklearn.manifold import TSNE</span><span id="b8d4" class="mw mx it ol b gy ou or l os ot">#set the hyperparmateres<br/>keep_dims = 2<br/>lrn_rate = 700<br/>prp = 40</span><span id="bea3" class="mw mx it ol b gy ou or l os ot">#extract the data as a cop<br/>tsnedf = X_rs.copy()</span><span id="f029" class="mw mx it ol b gy ou or l os ot">#creae the model<br/>tsne = TSNE(n_components = keep_dims, <br/>            perplexity = prp, <br/>            random_state = 42,<br/>            n_iter = 5000,<br/>            n_jobs = -1)</span><span id="7b1d" class="mw mx it ol b gy ou or l os ot">#apply it to the data<br/>X_dimensions = tsne.fit_transform(tsnedf)<br/>#check the shape<br/>X_dimensions.shape</span><span id="9ba0" class="mw mx it ol b gy ou or l os ot">#out:<br/>(530, 2)</span></pre><p id="2d53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以把它想象成:</p><pre class="kj kk kl km gt om ol on oo aw op bi"><span id="f2f9" class="mw mx it ol b gy oq or l os ot">#create a dataframe from the dataset<br/>tsnedf_res = pd.DataFrame(data = X_dimensions,<br/>                         columns = ["Dimension 1", <br/>                                "Dimension 2"])</span><span id="c108" class="mw mx it ol b gy ou or l os ot">#merge this with the NBA data<br/>NBA = pd.merge(NBA,<br/>              tsnedf_res,<br/>              left_index=True,<br/>              right_index=True,<br/>              how = "inner")</span><span id="82b9" class="mw mx it ol b gy ou or l os ot">#plot the result<br/>g = sns.jointplot(data = NBA,<br/>                 x = "Dimension 1",<br/>                 y = "Dimension 2",<br/>                 hue = "Pos")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/1e1f879f2db9652d3bb66a79cbe30127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYTshWhkS58WWnoXoYor0A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fcac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里可以看到，在某种程度上，我们比之前使用 PCA 得到了更清晰的不同位置的图像。我们现在可以看到，中锋的位置显得更加明显，得分后卫的分布非常广泛，而控卫则倾向于集中在一起，小前锋和大前锋也一起出现。</p><p id="24d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这使得它稍微清晰一些，并表明变量之间的关系可能确实是非线性的，因此 t-SNE 可能在这个数据集上更好。当然，在思考这个问题时，你必须注意你想用维度或主要成分做什么。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="16cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您想了解我们协会的更多信息，请随时关注我们的社交网站:</p><p id="a976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://www.facebook.com/ucldata</p><p id="a7be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">insta gram:<a class="ae ky" href="https://www.instagram.com/ucl.datasci/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/ucl.datasci/</a></p><p id="6dea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">领英:<a class="ae ky" href="https://www.linkedin.com/company/ucldata/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/company/ucldata/</a></p><p id="1f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解 UCL 数据科学协会和其他优秀作者的最新信息，请使用我下面的推荐代码注册 medium。</p><div class="mc md gp gr me mf"><a href="https://philip-wilkinson.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">通过我的推荐链接加入媒体-菲利普·威尔金森</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">philip-wilkinson.medium.com</p></div></div><div class="mo l"><div class="oz l mq mr ms mo mt ks mf"/></div></div></a></div><p id="a83d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者查看我在 Medium 上的其他文章</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/introduction-to-hierarchical-clustering-part-1-theory-linkage-and-affinity-e3b6a4817702"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">层次聚类简介(第 1 部分——理论、联系和相似性)</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">什么是层次聚类、亲和度和关联度</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pa l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/maximum-likelihood-estimation-and-poisson-regression-in-the-gravity-model-5f0de29e3464"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">重力模型中的最大似然估计和泊松回归</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">通过引力模型介绍它们之间的关系</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pb l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/an-introduction-to-object-oriented-programming-for-data-scientists-879106d90d89"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">面向对象的基础知识，适合那些以前可能没有接触过这个概念或者想知道更多的人</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="pc l mq mr ms mo mt ks mf"/></div></div></a></div></div></div>    
</body>
</html>