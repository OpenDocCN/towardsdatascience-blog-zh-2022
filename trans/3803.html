<html>
<head>
<title>Machine Learning Model Interpretability and Explainability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的可解释性和可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-interpretability-and-explainability-27fe31cc0688#2022-08-24">https://towardsdatascience.com/model-interpretability-and-explainability-27fe31cc0688#2022-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c251" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">SHAP，石灰，可解释的助推机器，显著图，TCAV，蒸馏，反事实，和解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/240c957676f69878b1467373a88e216f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwqovE2BeUm7xZtXXd1kZw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾蒂安·吉拉尔代在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6beb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ML/AI模型变得越来越复杂，越来越难以解释和说明。简单、易于解释的回归或决策树模型不再能够完全满足技术和业务需求。越来越多的人使用集成方法和深度神经网络来获得更好的预测和准确性。然而，那些更复杂的模型很难解释、调试和理解。因此，许多人将这些模型称为黑盒模型。</p><p id="b101" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们训练一个ML/AI模型时，我们通常专注于技术细节，如步长、层、早期停止、丢失、膨胀等等。我们真的不知道为什么我们的模型会以某种方式运行。例如，考虑一个信用风险模型。为什么我们的模型会给个人分配一定的分数？我们的模型依赖于什么特性？我们的模型是否严重依赖一个不正确的特征？即使我们的模型没有将种族和性别作为输入特征，我们的模型是否会从其他特征中推断出这些属性，并引入对某些群体的偏见？利益相关者能理解并信任模型行为吗？该模型能为人们提供如何提高信用评分的指导吗？模型解释和说明可以提供对这些问题的洞察，帮助我们调试模型，减少偏差，并建立透明度和信任。</p><p id="77a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们对机器学习模型的可解释性和可解释性越来越感兴趣。研究人员和ML实践者已经设计了许多解释技术。在这篇文章中，我们将提供八种流行的模型解释技术和工具的高级概述，包括<strong class="lb iu"> SHAP、Lime、可解释的助推机器、显著图、TCAV、蒸馏、反事实和解释。</strong></p><h1 id="7708" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">SHAP</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/c2a54def04e40b0e3692599f0f22802b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EJwDvZe9lYXJnegj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。SHAP解释机器学习模型输出(来源:<a class="ae ky" href="https://shap.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">https://shap.readthedocs.io/</a>，麻省理工许可)</p></figure><p id="8e6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://shap.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">“SHAP(SHapley Additive exPlanations)</a>是一种博弈论方法，用来解释任何机器学习模型的输出。它使用博弈论及其相关扩展中的经典Shapley值，将最优信用分配与本地解释联系起来。</p><p id="5e00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图1显示了SHAP工作的要点。假设基本速率，即预测值E[f(x)]的先验背景期望值是0.1。现在我们有了一个新的观察结果，特征年龄=65，性别=F，血压=180，身体质量指数=40。这个新观察的预测值是0.4。我们如何解释0.4的产值和0.1的基础率之间的这种差异？这就是Shapley值发挥作用的地方:</p><ul class=""><li id="386e" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">先说基础率0.1。</li><li id="c1eb" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">加上身体质量指数Shapely值0.1，我们得到0.2。</li><li id="50ce" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">加上BP Shapely值0.1，我们得到0.3。</li><li id="a86d" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">加上性感匀称值-0.3，我们得到0。</li><li id="c2b6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">加上年龄匀称值0.4，我们得到0.4</li></ul><p id="88bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于Shapely值的加性，这个新观察的模型预测是0.4。SHAP值提供了对每个要素重要性的深入了解，并解释了模型预测的工作原理。</p><p id="521b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何计算Shapley值？SHAP绘制了线性模型的部分相关图，其中x轴代表特性，y轴代表给定特性时输出的期望值(见图2)。特征的Shapley值是在该特征的给定值，即图中红线的长度，预期模型输出和部分相关性图之间的差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/682d026f413f01c501508c531bb9e72a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GFZToJUOItmddi4q"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。部分依赖情节(来源:【https://shap.readthedocs.io/】T2，麻省理工学院许可)</p></figure><p id="ba3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Shapley值的计算可能很复杂，因为它是所有可能的联盟排列的平均值。<code class="fe nd ne nf ng b">shap</code>库使用采样和优化技术来处理所有复杂的计算，并为表格数据、文本数据甚至图像数据返回直观的结果(参见图3)。通过<code class="fe nd ne nf ng b">conda install -c conda-forge shap</code>安装SHAP并试一试。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/dce8107ea314a320b3ca601c1a0ecdf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*npMEgo7uSbV1_Yqi"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。https://shap.readthedocs.io/，麻省理工学院许可，SHAP解释图像分类(来源:<a class="ae ky" href="https://shap.readthedocs.io/" rel="noopener ugc nofollow" target="_blank"/></p></figure><h1 id="48cf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">石灰</h1><p id="8648" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">模型可以是全局复杂的。Lime(局部可解释模型不可知解释)不是关注整体复杂模型行为，而是关注局部区域，并使用线性近似来反映预测实例周围的行为。</p><p id="9acb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图4展示了Lime的工作原理。蓝色/粉色背景表示原始复杂模型的决策函数。红十字(我们称之为X)是我们想要预测和解释的实例/新观察。</p><ul class=""><li id="d21b" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">X周围的采样点</li><li id="504b" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">使用原始模型预测每个采样点</li><li id="1676" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">根据样本与X的接近程度对样本进行加权(图中权重较大的点对应于较大的尺寸)</li><li id="8049" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">在加权样本上拟合线性模型(虚线)</li><li id="bc95" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">用这个线性模型来解释X附近的局部</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1be7c082df5dc114b209b40952123b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/0*8JoadkYzaZ-F4Dea"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。石灰直觉(来源:<a class="ae ky" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a>，BSD执照)</p></figure><p id="9852" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Lime，我们可以在本地解释表格数据、文本数据和图像数据的模型行为。下面是一个使用Lime来解释文本分类器的例子。我们可以看到这个分类器正确地预测了实例，但是出于错误的原因。​​</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/bb1f86af66a32cc853942784a3566b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IMxqj04rB2-G-O2t"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。石灰解释文本分类器示例(来源:<a class="ae ky" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a>，BSD许可)</p></figure><p id="642f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于Lime的信息，请查看<a class="ae ky" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> Github页面</a>并通过<code class="fe nd ne nf ng b">pip install lime</code>安装。</p><h1 id="5a17" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">可解释的助推器</h1><p id="2000" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><a class="ae ky" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">“可解释的助推机</a> (EBM)是一个基于树的、循环梯度助推广义加法模型，具有自动交互检测功能。EBM通常与最先进的黑盒模型一样精确，同时保持完全可解释性。”</p><p id="3ed2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBM的工作原理如下:</p><ul class=""><li id="00b7" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">在每次迭代中，我们以循环方式一次一个特征地训练一个装袋和梯度提升树。我们首先在第一个特征上训练，然后更新残差并在第二个特征上训练，并继续直到我们完成所有特征的训练。</li><li id="bf97" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">然后我们重复这个过程很多很多次。</li><li id="8614" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">因为EBM逐个特征循环，所以它可以显示每个特征在最终预测中的重要性。</li></ul><p id="0164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBM是在<a class="ae ky" href="http://interpret.ml" rel="noopener ugc nofollow" target="_blank"> interpret.ml </a>中实现的，我们将在本文后面介绍。</p><h1 id="b12b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">显著图</h1><p id="944e" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">显著图方法广泛用于解释神经网络图像分类任务。它测量每个像素的重要性，并突出显示哪些像素对预测很重要。在高层次上，显著性图采用每个类别相对于每个图像输入像素的梯度或导数，并将梯度可视化(参见图6)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/fa4be0b0801b319f616a7934b95d4566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nHKRkyQk9TvofkC2"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。显著地图(来源:<a class="ae ky" href="https://pair-code.github.io/saliency/#guided-ig" rel="noopener ugc nofollow" target="_blank">https://pair-code.github.io/saliency/#guided-ig</a>，阿帕奇许可)</p></figure><p id="b97e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PAIR显著性项目提供了“最新显著性方法的框架不可知实现”，包括引导集成梯度、XRAI、SmoothGrad、香草梯度、引导、反向传播、集成梯度、遮挡、Grad-CAM、模糊IG。</p><p id="17c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于显著性方法的信息，请查看Github页面并通过pip安装显著性。</p><h1 id="99f1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">TCAV</h1><p id="08c8" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">TCAV主张用概念激活向量(CAV)进行定量测试。TCAV“量化了用户定义的概念对分类结果的重要程度——例如，斑马的预测对条纹的存在有多敏感”(金，2018)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/ecc5595441fbf5463700f34be0664b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*af2-O82bDgvDdhYz"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。用概念激活向量进行测试。(资料来源:Kim，2018年)</p></figure><p id="1a7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TCAV执行以下步骤来确定一个概念是否重要:</p><ul class=""><li id="eb82" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">定义概念激活向量(图7步骤a-d)</li></ul><p id="a6bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TCAV使用概念图像(带有条纹物体的图像)和随机图像的例子作为输入，并检索层激活。然后，它训练一个线性分类器来分离激活，并采用与超平面决策边界正交的向量(CAV)。CAV代表图像中的条纹。</p><ul class=""><li id="c3ad" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">计算TCAV分数(图7步骤e)</li></ul><p id="8951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TCAV分数是通过对CAV取方向导数来计算的。它代表了模特对条纹等特定概念的敏感度。</p><ul class=""><li id="0b48" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">CAV验证</li></ul><p id="5430" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">测试一个概念是否有静态意义。同样的过程可以在随机图像和随机图像中进行。我们可以将概念对随机图像TCAV分数分布与随机对随机图像TCAV分数分布进行比较。可以进行双侧t检验来检验TCAV分数分布差异。</p><p id="138b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于TCAV的信息，请查看<a class="ae ky" href="https://github.com/tensorflow/tcav" rel="noopener ugc nofollow" target="_blank"> Github页面</a>并通过<code class="fe nd ne nf ng b">pip install tcav</code>安装。</p><h1 id="0641" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">蒸馏</h1><p id="0f17" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">“在机器学习中，<a class="ae ky" href="https://en.wikipedia.org/wiki/Knowledge_distillation" rel="noopener ugc nofollow" target="_blank">知识提炼</a>是将知识从大模型转移到小模型的过程。”</p><p id="1c26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在模型解释上下文中，大模型是黑盒模型，也是教师模型。较小的模型是解释者，学生模型。学生模型试图模仿教师模型的行为，并且是可解释的。</p><p id="15e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，可以构建一个决策树来近似原始的复杂模型(Bastani，2019)。Bastani的论文“提出了一种用于学习决策树的模型提取算法——为了避免过度拟合，该算法通过主动采样新输入并使用复杂模型标记它们来生成新的训练数据。”</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/e4ecab51e2988d2b77bdf746905c826b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2vAcGbbI5pQHZau7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8。用决策树解释黑盒模型。(来源:<a class="ae ky" href="https://arxiv.org/pdf/1705.08504.pdf" rel="noopener ugc nofollow" target="_blank">通过模型提取解释黑盒模型。奥斯伯特.巴斯塔尼，卡罗琳.金，哈姆萨.巴斯塔尼。2019 </a></p></figure><h1 id="4fde" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">反事实的</h1><p id="a305" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">反事实描述了改变模型预测所需的最小输入特征变化量。反事实提出了很多假设问题。如果我们增加这个功能或减少那个功能会怎么样？例如，根据黑盒模型，约翰有很高的心脏病风险。如果约翰一周锻炼5天会怎么样？如果约翰是素食者呢？如果约翰不抽烟会怎么样？这些变化会导致模型预测的变化吗？那些反事实提供了易于理解的解释。</p><p id="2ad8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多关于生成反事实的研究和方法。例如，<a class="ae ky" href="http://interpret.ml/DiCE/" rel="noopener ugc nofollow" target="_blank">骰子</a>(不同的反事实解释)为同一个人生成一组不同的特征扰动选项，该人的贷款被拒绝，但如果收入增加10，000美元或收入增加5，000美元并有1年以上的信用历史，则该人会获得批准。DiCE在支持用户特定要求的情况下，对原始输入的多样性和接近度进行了优化。</p><p id="b700" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于interpretML的信息，请查看<a class="ae ky" href="http://interpret.ml/DiCE/" rel="noopener ugc nofollow" target="_blank">文档</a>并通过<code class="fe nd ne nf ng b">conda install -c conda-forge dice-ml</code>安装。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/e4e439ba28e31b74c305bd7b8718232c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cy0kBOJskL6CVJoP"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9。骰子(来源:<a class="ae ky" href="https://github.com/interpretml/DiCE" rel="noopener ugc nofollow" target="_blank">https://github.com/interpretml/DiCE</a>，麻省理工学院许可)</p></figure><h1 id="c642" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">解释性语言</h1><p id="7e30" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">"<a class="ae ky" href="https://github.com/interpretml/interpret/" rel="noopener ugc nofollow" target="_blank"> InterpretML </a>是一个开源包，在一个屋檐下集成了最先进的机器学习可解释性技术。"</p><p id="b51f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">interpretML提供了对glassbox模型的解释，包括</p><ul class=""><li id="5f86" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">可解释的助推器，</li><li id="755e" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">线性模型，</li><li id="8257" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">决策树，</li><li id="8556" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">和决策规则。</li></ul><p id="35b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它还解释了使用</p><ul class=""><li id="561e" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">Shapley加法解释，</li><li id="b3da" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">本地可解释的模型不可知的解释，</li><li id="142f" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">部分相关图，</li><li id="66a7" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">和莫里斯敏感度分析。</li></ul><p id="1d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">interpretML的结果可以显示在一个带有良好交互界面的Plotly仪表盘中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/ed3ee7382c174d943ac3e9dc6120200e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TVIPZ-_7LSZMzsSu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10。interpretML仪表板(来源:<a class="ae ky" href="https://interpret.ml/docs/getting-started.html" rel="noopener ugc nofollow" target="_blank">https://interpret.ml/docs/getting-started.html</a>，麻省理工学院许可)</p></figure><p id="16be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于interpretML的信息，请查阅<a class="ae ky" href="https://interpret.ml/docs/getting-started.html" rel="noopener ugc nofollow" target="_blank">文档</a>并通过<code class="fe nd ne nf ng b">conda install -c interpretml interpret</code>进行安装。</p><h1 id="c033" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">结论</strong></h1><p id="55db" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">总的来说，我们已经对一些流行的模型可解释性技术和工具进行了高层次的概述，包括SHAP、Lime、可解释推进机、显著图、TCAV、蒸馏、反事实和解释。每种技术都有自己的变化。我们将在以后详细讨论每种技术。</p><h1 id="26ea" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><ul class=""><li id="87f8" class="mo mp it lb b lc ni lf nj li nu lm nv lq nw lu mt mu mv mw bi translated"><a class="ae ky" href="https://shap.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">https://shap.readthedocs.io/</a></li><li id="0eb6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">“我怎么忽悠你？”:通过误导性的黑箱解释操纵用户信任。奥斯伯特.巴斯塔尼。AAAI/ACM人工智能、伦理和社会会议(AIES)，2020年。</li><li id="3ccf" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">对黑盒模型的忠实和可定制的解释。Himabindu Lakkaraju，Ece Kamar，Rich Carauna，Jure Leskovec。AAAI/ACM人工智能、伦理和社会会议(AIES)，2019年。</li><li id="c8f4" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">https://explainml-tutorial.github.io/neurips20<a class="ae ky" href="https://explainml-tutorial.github.io/neurips20" rel="noopener ugc nofollow" target="_blank"/></li><li id="28a6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">【https://christophm.github.io/interpretable-ml-book T4】</li><li id="9772" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><a class="ae ky" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank">https://homes.cs.washington.edu/~marcotcr/blog/lime/</a></li><li id="62a3" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">“我为什么要相信你？”:解释任何分类器的预测。马尔科·图利奥·里贝罗，萨姆尔·辛格，卡洛斯·盖斯特林。2016年第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集。</li><li id="3dc6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">对黑盒模型的忠实和可定制的解释。拉卡拉朱、卡马尔、卡鲁阿纳和莱斯科维奇。AIES，2019。</li><li id="6388" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1705.08504.pdf" rel="noopener ugc nofollow" target="_blank">通过模型提取解释黑盒模型。奥斯伯特.巴斯塔尼，卡罗琳.金，哈姆萨.巴斯塔尼。2019 </a></li><li id="b3fb" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">特征归因之外的可解释性:概念激活向量的定量测试(TCAV)。被金，马丁瓦滕伯格，贾斯汀吉尔默，卡莉蔡，詹姆斯韦克斯勒，费尔南达维加斯，罗里塞尔。2018.</li><li id="0cce" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><a class="ae ky" href="https://www.youtube.com/watch?v=Ff-Dx79QEEY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Ff-Dx79QEEY</a></li><li id="3a0f" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><a class="ae ky" href="https://pair-code.github.io/saliency/" rel="noopener ugc nofollow" target="_blank">pair-code.github.io/saliency/</a></li><li id="3c91" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><a class="ae ky" href="https://interpret.ml/" rel="noopener ugc nofollow" target="_blank">https://interpret.ml/</a></li><li id="dd33" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">通过不同的反事实解释来解释机器学习分类器。陈豪·谭·阿米特·夏尔马·拉马鲁文·k·莫西拉尔2019.</li></ul><p id="f3e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">. . .</p><p id="47dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最初发表于<a class="ae ky" href="https://anaconda.cloud/article-yang-interpretability" rel="noopener ugc nofollow" target="_blank">anaconda.org</a></p><p id="ad57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者索菲亚杨2022年8月23日。</p><p id="00cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Sophia Yang是Anaconda的高级数据科学家。在<a class="ae ky" href="https://www.linkedin.com/in/sophiamyang/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae ky" href="https://twitter.com/sophiamyang" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae ky" href="https://www.youtube.com/SophiaYangDS" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上与我联系，并加入ds/ml<a class="ae ky" href="https://discord.com/invite/6BremEf9db" rel="noopener ugc nofollow" target="_blank">❤️读书俱乐部</a></p></div></div>    
</body>
</html>