<html>
<head>
<title>Ace your Machine Learning Interview — Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">赢得机器学习面试——第四部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ace-your-machine-learning-interview-part-4-e30b695ce63#2022-11-02">https://towardsdatascience.com/ace-your-machine-learning-interview-part-4-e30b695ce63#2022-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/a2b1cc015faf544a61e4506dc20c54ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_GR-_jSm9ywR-xdQ"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">路</a>上<a class="ae jd" href="https://unsplash.com/@headwayio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">车头</a>拍摄</p></figure><div class=""/><div class=""><h2 id="798e" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Python深入研究支持向量机</h2></div><p id="eb4f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我的第四篇文章，致力于我开始的关于机器学习基础知识的系列，以便接近你的面试。我把以前文章的链接留在这里。</p><ol class=""><li id="3c14" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><a class="ae jd" rel="noopener" target="_blank" href="/ace-your-machine-learning-interview-part-1-e6a5897e6844"> <em class="ma"> Ace your Machine Learning面试—第一部分</em> </a> <em class="ma">:深入线性、套索和岭回归及其假设</em></li><li id="6112" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" href="https://medium.com/towards-data-science/ace-your-machine-learning-interview-part-2-c58526b5faba" rel="noopener"><em class="ma">Ace your Machine Learning访谈—第二部分</em> </a> <em class="ma">:使用Python深入研究分类问题的逻辑回归</em></li><li id="1145" class="lr ls jg kx b ky mb lb mc le md li me lm mf lq lw lx ly lz bi translated"><a class="ae jd" href="https://medium.com/towards-data-science/ace-your-machine-learning-interview-part-3-af432f922aa7" rel="noopener"><em class="ma">Ace your Machine Learning面试—第三部分</em> </a> <em class="ma">:使用Python研究朴素贝叶斯分类器</em></li></ol><h2 id="92c1" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated"><em class="mz">简介</em></h2><p id="dcda" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">支持向量机(SVMs)的算法直观上非常简单，尽管对于没有很强数学背景的人来说，数学可能很复杂。</p><p id="bd15" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与逻辑回归一样，我们希望使用一条直线或一个<em class="ma"> n </em>维的超平面来分离两类点。但与逻辑回归不同，我们试图找到使边际最大化的超平面。</p><p id="af54" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">裕度定义为分离超平面(决策边界)与最接近该超平面</strong>的训练样本之间的距离，即所谓的<strong class="kx jh">支持向量</strong>。让我们看一个例子。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/632121900c4fb844538f4afc24c7b31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4T54bi_LmbwouGNOYOWo_w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">逻辑回归VS SVM(图片由作者提供)</p></figure><p id="a9d7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过最大化余量，我们将找到一条线，如上例所示，它将具有很大的泛化能力。因此，它不仅要考虑拟合(或过拟合)训练数据，还要能够对新数据做出良好的预测。</p><p id="7994" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">目标:</strong>找到使边际最大化的超平面。大的页边距比小的页边距能让我们更好地概括。</p><h2 id="2b7c" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">它是如何工作的？</h2><p id="8645" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">假设我们的正点数取值<em class="ma"> +1 </em>，负点数<em class="ma"> -1 </em>。因此，我们可以定义描述边缘和决策边界的函数。此外，我们通过<em class="ma"> w </em>定义垂直于超平面的方向。让我们用下图总结一下。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/c7af37b70442e982356f0383f45bd363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SzobchCeIO52hMuEqV1MlQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">SVM(作者图片)</p></figure><p id="fac0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在假设你是一个正点<em class="ma"> x </em>，它正好位于决策边界上，需要分类。在<em class="ma"> w </em>的方向上你必须走多少<em class="ma"> k </em>步才能被归类为正，让我们来计算一下。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/38c618af7673925826383f60f8f322f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5aFUf0Z8c018RIs9fXkQA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">SVM数学(作者图片)</p></figure><p id="c7cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这将让你大致了解你是如何达到利润最大化的。但是为了最大化裕量，我们只需要<strong class="kx jh">最小化<em class="ma"> ||w|| </em> </strong>因为它在分母中。显然，我们希望最小化<em class="ma"> ||w|| </em>始终牢记想要正确分类点的约束。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/33ef3dccc7ad884fe254cc122fd21846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5rmi1OfAIxLZwisvH9M-w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">约束SVM(图片由作者提供)</p></figure><p id="9d02" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，硬边界SVM优化问题可以定义如下。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/67939ad4dbc79fa1e4f8bba21ec8b3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9unNa0pZVnVX46rNGnN8w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">硬边界优化问题(图片由作者提供)</p></figure><h2 id="015e" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">软利润SVM</h2><p id="fb71" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">正如你可能已经猜到的，如果有硬利润的SVM，也会有软利润的SVM。</p><p id="7e1d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个版本中，我们允许模型犯一些错误，也就是说，让一些点位于边缘线内。这样，通过允许存在一些错误，我们可以保持一个概括得很好的决策边界。在理论层面，我们引入了由Vladimir Vapnik于1995年首次引入的所谓的<strong class="kx jh">松弛变量</strong>。在SVMs的上下文中，<strong class="kx jh">我们将该变量称为C </strong>，它是要调整的网络的<strong class="kx jh">超参数</strong>。在这种情况下，我们的最小化问题变化如下。</p><figure class="ng nh ni nj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/4f0c1f843d134b114fad08cefec14848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2w14KFLhplF0A2KK0FB8VA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">软边SVM(图片由作者提供)</p></figure><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a82d0fa7a68abff3fcd4b7cdb6c98037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*aU8mxmBaBckT27QG19tBnw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">软边距SVM中的错误(图片由作者提供)</p></figure><h2 id="eb26" class="mg mh jg bd mi mj mk dn ml mm mn dp mo le mp mq mr li ms mt mu lm mv mw mx my bi translated">我们来编码吧！</h2><p id="a43d" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">使用sklearn实现SVM非常简单，让我们来看看如何实现。这里我们还使用了众所周知的Iris数据集，它包含以下特征。</p><p id="892b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据集由sklearn以开放许可的方式提供，你可以在这里找到它<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ng nh ni nj gt is"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="fe8c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">出于可视化的目的，我们将仅使用Iris数据集的两个特征。因此，让我们加载并标准化我们的数据。</p><figure class="ng nh ni nj gt is"><div class="bz fp l di"><div class="nq nr l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="76de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可以训练SVM，并绘制决策边界。</p><figure class="ng nh ni nj gt is"><div class="bz fp l di"><div class="nq nr l"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><figure class="ng nh ni nj gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/668c4ea003e7e071b0c916c37cb2863e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*GntADg8AL7NNK_o1wwcpaA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">SVM结果(作者图片)</p></figure><h1 id="242d" class="nt mh jg bd mi nu nv nw ml nx ny nz mo km oa kn mr kp ob kq mu ks oc kt mx od bi translated">最后的想法</h1><p id="57a8" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">朴素贝叶斯是处理机器学习时需要了解的主要算法之一。它已经被大量使用，特别是在文本数据问题上，比如垃圾邮件识别。正如我们所看到的，它仍然有它的优点和缺点，但肯定的是，当你被问及基本的机器学习时，会有一个关于它的问题！</p><p id="f198" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">支持向量机可以做的比我在本文中报道的多得多。特别地，SVM核使得对不可从超平面分离的数据进行分类成为可能。使用核技巧，我们可以切换到具有更多维度的另一个维度空间，并在这个新空间中对数据集的点进行分类。在本系列的下一篇文章中，我们将详细介绍如何做到这一点！😁</p><h1 id="8c39" class="nt mh jg bd mi nu nv nw ml nx ny nz mo km oa kn mr kp ob kq mu ks oc kt mx od bi translated">结束了</h1><p id="e546" class="pw-post-body-paragraph kv kw jg kx b ky na kh la lb nb kk ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated"><em class="ma">马赛洛·波利蒂</em></p><p id="c15d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.linkedin.com/in/marcello-politi/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>，<a class="ae jd" href="https://twitter.com/_March08_" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae jd" href="https://march-08.github.io/digital-cv/" rel="noopener ugc nofollow" target="_blank"> CV </a></p></div></div>    
</body>
</html>