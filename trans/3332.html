<html>
<head>
<title>How to boost PyTorch Dataset using memory-mapped files</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用内存映射文件提升PyTorch数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-boost-pytorch-dataset-using-memory-mapped-files-6893bff27b99#2022-07-25">https://towardsdatascience.com/how-to-boost-pytorch-dataset-using-memory-mapped-files-6893bff27b99#2022-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e8b7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本文将讨论实现使用内存映射文件的PyTorch数据集的原因和步骤</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/573642eb03f142cae5cad65d87147708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EhS1QEjjwP1bjzLY"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃尔奥诺雷·凯梅尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="5329" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">介绍</h2><p id="71b0" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi mo translated"><span class="l mp mq mr bm ms mt mu mv mw di"> W </span>在训练神经网络时，最常见的速度瓶颈之一就是数据加载模块。如果我们通过网络传输数据，除了预取和缓存之外，没有其他简单的优化方法可以应用。</p><p id="8e25" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">但是，如果数据在本地存储中，我们可以通过将整个数据集合并到一个文件中来优化文件读取操作，然后我们可以将该文件映射到主内存中，这样我们就不需要为每个文件读取进行昂贵的系统调用，而是让虚拟内存管理器来处理内存访问。</p><p id="9d68" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">这段短暂旅程中的几站:</p><ul class=""><li id="93ce" class="nc nd it lx b ly mx mb my li ne lm nf lq ng mn nh ni nj nk bi translated"><strong class="lx iu">什么是内存映射文件</strong></li><li id="41d6" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated"><strong class="lx iu">什么是PyTorch数据集</strong></li><li id="78b3" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated"><strong class="lx iu">实现我们的定制数据集</strong></li><li id="cc59" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated"><strong class="lx iu">基准</strong></li><li id="b087" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated"><strong class="lx iu">结论</strong></li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="e5e4" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">什么是内存映射文件？</strong></h2><p id="05cb" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我们称一个<strong class="lx iu">内存映射文件</strong>，这个文件的内容直接分配给<a class="ae ky" href="https://en.wikipedia.org/wiki/Virtual_memory" rel="noopener ugc nofollow" target="_blank"> <strong class="lx iu">虚拟内存</strong> </a>的一个段，这样我们可以在那个段上执行任何操作，就像在当前进程中我们可以访问的主存的任何其他部分一样。</p><p id="8f67" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">由于虚拟内存所代表的额外抽象层，我们可以映射到比我们机器的物理容量大得多的内存文件中。运行进程所需的内存段(称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Page_(computer_memory)" rel="noopener ugc nofollow" target="_blank"> <strong class="lx iu">页</strong> </a>)由虚拟内存管理器从外部存储器中取出并自动复制到主内存中。</p><p id="5339" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">使用内存映射文件的好处:</p><ul class=""><li id="9b16" class="nc nd it lx b ly mx mb my li ne lm nf lq ng mn nh ni nj nk bi translated"><strong class="lx iu">提高I/O性能</strong>，通过系统调用的正常读/写操作比本地内存中的更改要慢得多</li><li id="555a" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated">该文件以一种“懒惰”的方式加载，通常一次只加载一页，因此即使对于较大的文件，实际的RAM利用率也是最小的。</li></ul></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="af59" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">什么是PyTorch数据集</strong></h2><p id="f23b" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">Pytorch提供了两个主要模块，用于在训练模型时处理数据管道:<strong class="lx iu">数据集</strong>和<strong class="lx iu">数据加载器。</strong></p><p id="0241" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated"><strong class="lx iu"> DataLoader </strong>主要用作数据集的包装器，它提供了许多可配置的选项，如批处理、采样、预取、混排等。，并抽象出大量的复杂性。</p><p id="8c12" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated"><strong class="lx iu">数据集</strong>是我们拥有大部分控制权的实际部分，我们实际上必须编写为训练过程提供数据的方式，这包括将样本加载到内存中并应用任何必要的转换。</p><p id="d4cf" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">从高层来看，我们要实现树函数:<strong class="lx iu"><em class="nx">【_ _ init _ _】、__len__，以及_ _ getitem _ _</em></strong>；我们将在下一节看到一个具体的例子。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="6946" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">实施我们的定制数据集</strong></h2><p id="0ccf" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">接下来，我们将看到上述三个函数的实现。</p><p id="a5eb" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">最重要的部分在<em class="nx"> __init__，</em>中，我们将使用<strong class="lx iu"> numpy </strong>库中的<a class="ae ky" href="https://numpy.org/doc/stable/reference/generated/numpy.memmap.html" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu"><em class="nx">NP . memmap</em></strong></a><strong class="lx iu"><em class="nx">()</em></strong>函数来创建一个<strong class="lx iu"> ndarray </strong>，它由映射到文件的内存缓冲区支持。</p><p id="91d3" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">ndarray将从一个iterable(最好是一个<strong class="lx iu">生成器</strong>来填充，以将内存利用率保持在最低水平)，这样我们就可以保持对数据集支持的数据的形式和类型的高度适应性。我们还可以提供一个转换函数，当从数据集中检索时，该函数将应用于输入数据。</p><p id="9a23" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">关于更全面的视图和其他示例，实际项目也在Github上<a class="ae ky" href="https://github.com/DACUS1995/pytorch-mmap-dataset" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="7010" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">还有两个<em class="nx">实用程序</em>类型的函数，我们在上面给出的代码中使用过。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="ce14" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">基准</strong></h2><p id="43f4" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">为了展示一个性能提升的真实例子，我将内存映射数据集实现与以经典惰性方式读取文件的普通数据集实现进行了比较。这里使用的数据集由350个jpg图像组成。基准测试的代码可以在<a class="ae ky" href="https://github.com/DACUS1995/pytorch-mmap-dataset/blob/main/benchmark.py" rel="noopener ugc nofollow" target="_blank">这里</a>看到。</p><p id="8b6d" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">从下面的结果中，我们可以看到我们的数据集比普通的数据集快30倍以上:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/2f1a1ae98542978665a1a1883a8b73a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*rT8095Gr_uZriFs60O21SQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集比较</p></figure></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="53d6" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">结论</strong></h2><p id="e830" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">本文中给出的实现决不是产品级的，但是其背后的思想是非常有效的，在使用中型到大型文件时，内存映射文件方法还有更多的用途。</p><p id="d47e" class="pw-post-body-paragraph lv lw it lx b ly mx ju ma mb my jx md li mz mf mg lm na mi mj lq nb ml mm mn im bi translated">感谢你的阅读，我希望你会发现这篇文章很有帮助，如果你想了解最新的编程和机器学习新闻以及一些优质的模因:)，你可以在Twitter上关注我<a class="ae ky" href="https://twitter.com/SurdoiuT" rel="noopener ugc nofollow" target="_blank">这里</a>或者在LinkedIn上联系<a class="ae ky" href="https://www.linkedin.com/in/tudor-marian-surdoiu/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h2 id="4112" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">参考</h2><ul class=""><li id="3095" class="nc nd it lx b ly lz mb mc li ob lm oc lq od mn nh ni nj nk bi translated">【https://en.wikipedia.org/wiki/Memory-mapped_file T4】</li><li id="0ff7" class="nc nd it lx b ly nl mb nm li nn lm no lq np mn nh ni nj nk bi translated"><a class="ae ky" href="https://man7.org/linux/man-pages/man2/mmap.2.html" rel="noopener ugc nofollow" target="_blank">https://man7.org/linux/man-pages/man2/mmap.2.html</a></li></ul></div></div>    
</body>
</html>