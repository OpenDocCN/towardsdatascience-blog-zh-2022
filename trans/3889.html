<html>
<head>
<title>Ingesting Metadata before the Cloud</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在云之前摄取元数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ingesting-metadata-before-the-cloud-3a8c4b063d8b#2022-08-29">https://towardsdatascience.com/ingesting-metadata-before-the-cloud-3a8c4b063d8b#2022-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e172" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Spark、Hive和OpenMetadata</h2></div><p id="a6df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可能感觉像是那些以“<em class="lb">从前”</em>开始的故事之一，但并不是每个人都(还)在云上。然而，元数据已经并将永远存在。我们只需要学会如何充分利用它。</p><p id="749a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然数据旨在让组织深入了解其业务，但元数据有助于团队确保他们的数据可靠且有意义。此外，元数据允许我们缩小数据和人之间的差距，深入到数据<strong class="kh ir">所有权</strong>、<strong class="kh ir">使用、</strong>和<strong class="kh ir">治理</strong>等主题。</p><p id="14a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">元数据摄取是解开这个巨大价值链的第一步。在本文中，我们将回顾过去，准备一个本地Spark应用程序，将其数据存储到外部<strong class="kh ir"> Hive metastore </strong>中。然后，我们将使用<a class="ae lc" href="https://open-metadata.org/" rel="noopener ugc nofollow" target="_blank"> OpenMetadata </a>将生成的元数据提取到一个集中的平台中。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/ed35ef2cad07fd80eb3f3a2c09551714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V9dFCKmrm-OUbGNJ_6tZfA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">解决方案图。图片由作者提供。</p></figure><h1 id="5b03" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">本地设置</h1><p id="1c61" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们将按照<a class="ae lc" href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/demo/demo-connecting-spark-sql-to-hive-metastore.html" rel="noopener ugc nofollow" target="_blank">这里</a>列出的步骤，使用更新版本:</p><ul class=""><li id="2166" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">Java 11(使用<a class="ae lc" href="https://sdkman.io/" rel="noopener ugc nofollow" target="_blank"> sdkman </a>安装)</li><li id="acb6" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><a class="ae lc" href="https://hadoop.apache.org/release/3.3.4.html" rel="noopener ugc nofollow" target="_blank"> Hadoop 3.3.4 </a></li><li id="14a7" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated"><a class="ae lc" href="https://dlcdn.apache.org/hive/stable-2/" rel="noopener ugc nofollow" target="_blank">蜂巢2.3.9 </a>(稳定发布)</li></ul><p id="7e18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目标是提升<strong class="kh ir"> Hive </strong>，它在底层使用Hadoop。</p><p id="46f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下载完软件包后，你可以用下面的<code class="fe ne nf ng nh b">exports</code>更新你的<code class="fe ne nf ng nh b">.zshrc</code>或<code class="fe ne nf ng nh b">.bashrc</code>文件，我在这里找到了<code class="fe ne nf ng nh b">~/dev/</code>下的文件:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="d0e4" class="nm lu iq nh b gy nn no l np nq">export HIVE_HOME=~/dev/apache-hive-2.3.9-bin<br/>export HADOOP_HOME=~/dev/hadoop-3.3.4</span><span id="93e4" class="nm lu iq nh b gy nr no l np nq">export PATH=$HIVE_HOME/bin:$PATH<br/>export PATH=$HADOOP_HOME/bin/hadoop:$PATH</span></pre><h2 id="1db8" class="nm lu iq bd lv ns nt dn lz nu nv dp md ko nw nx mf ks ny nz mh kw oa ob mj oc bi translated">Hadoop配置</h2><p id="3ee9" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">对于Hadoop，我们将遵循<a class="ae lc" href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/SingleCluster.html" rel="noopener ugc nofollow" target="_blank">官方指南</a>中的<strong class="kh ir">伪分布式操作</strong>:</p><ul class=""><li id="122a" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">编辑<code class="fe ne nf ng nh b">$HADOOP_HOME/etc/hadoop/core-site.xml</code></li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="b074" class="nm lu iq nh b gy nn no l np nq">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.defaultFS&lt;/name&gt;<br/>        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><ul class=""><li id="fcb4" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">编辑<code class="fe ne nf ng nh b">$HADOOP_HOME/etc/hadoop/hdfs-site.xml</code></li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="5230" class="nm lu iq nh b gy nn no l np nq">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;dfs.replication&lt;/name&gt;<br/>        &lt;value&gt;1&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><ul class=""><li id="1e2e" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">运行<code class="fe ne nf ng nh b">$HADOOP_HOME/sbin/start-all.sh</code>在本地启动datanodes和namenodes。</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="2bc4" class="nm lu iq nh b gy nn no l np nq">~/dev/hadoop-3.3.4 ❯ $HADOOP_HOME/sbin/start-all.sh                                        <br/>WARNING: Attempting to start all Apache Hadoop daemons as pmbrull in 10 seconds.<br/>Starting namenodes on [localhost]<br/>Starting datanodes<br/>Starting secondary namenodes [Peres-MBP.lan]<br/>Starting resourcemanager<br/>Starting nodemanagers</span></pre><p id="2a17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这最后一步需要您的用户在没有密码的情况下SSH到<code class="fe ne nf ng nh b">localhost</code>。如果你不能<code class="fe ne nf ng nh b">ssh localhost</code>，那么你需要跑:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="51f6" class="nm lu iq nh b gy nn no l np nq">ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa<br/>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br/>chmod 0600 ~/.ssh/authorized_keys</span></pre><p id="de20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您是macOS用户，您可能需要在“系统偏好设置”中启用远程登录:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi od"><img src="../Images/d47f72e14c23a920998082f858ef2bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QRLHDIenO2FP-QzMZYKuhQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">在OSX系统偏好设置中设置远程登录。图片由作者提供。</p></figure><h2 id="103d" class="nm lu iq bd lv ns nt dn lz nu nv dp md ko nw nx mf ks ny nz mh kw oa ob mj oc bi translated">蜂巢配置</h2><p id="3c56" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">按照<a class="ae lc" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHive" rel="noopener ugc nofollow" target="_blank">设置指南</a>，我们将在HDFS创建<code class="fe ne nf ng nh b">tmp</code>和metastore仓库目录，并具有在其中写入数据的必要权限:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="f559" class="nm lu iq nh b gy nn no l np nq">$HADOOP_HOME/bin/hadoop fs -mkdir     /tmp <br/>$HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp <br/>$HADOOP_HOME/bin/hadoop fs -mkdir -p  /user/hive/warehouse $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse</span></pre><p id="f463" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步将是设置Hive存储关于表的元数据<strong class="kh ir">的位置。最后，我们将使用一个本地MySQL实例，在这里我们将执行一个模式迁移来准备所需的数据库结构。</strong></p><ul class=""><li id="6db5" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">在MySQL实例中，创建一个数据库并向用户提供:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="7a8c" class="nm lu iq nh b gy nn no l np nq">mysql&gt; create database demo_hive; <br/>Query OK, 1 row affected (0.01 sec)  </span><span id="51ef" class="nm lu iq nh b gy nr no l np nq">mysql&gt; create user APP identified by 'password'; <br/>Query OK, 0 rows affected (0.01 sec)  </span><span id="d541" class="nm lu iq nh b gy nr no l np nq">mysql&gt; GRANT ALL PRIVILEGES ON demo_hive.* TO 'APP'@'%' WITH GRANT OPTION; <br/>Query OK, 0 rows affected (0.00 sec)</span></pre><ul class=""><li id="8639" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">根据提供的模板准备您的<code class="fe ne nf ng nh b">hive-site.xml</code>文件:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="7e1f" class="nm lu iq nh b gy nn no l np nq">cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml</span></pre><ul class=""><li id="a0c9" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">并编辑具有以下属性的<code class="fe ne nf ng nh b">$HIVE_HOME/conf/hive-site.xml</code>:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="d8ba" class="nm lu iq nh b gy nn no l np nq">&lt;property&gt;     <br/>    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;<br/>    &lt;value&gt;jdbc:mysql://127.0.0.1:3306/demo_hive&lt;/value&gt;   <br/>&lt;/property&gt; <br/>&lt;property&gt;<br/>    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;<br/>    &lt;value&gt;APP&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;<br/>    &lt;value&gt;password&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;   <br/>    &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;  <br/>    &lt;value&gt;hdfs://localhost:9000/user/hive/warehouse&lt;/value&gt;   &lt;/property&gt;</span></pre><p id="af0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我们在这里选择的是MySQL JDBC驱动程序。你可以从<a class="ae lc" href="https://dev.mysql.com/downloads/file/?id=513220" rel="noopener ugc nofollow" target="_blank">这里</a>下载，放在<code class="fe ne nf ng nh b">$HIVE_HOME/lib</code>下。</p><ul class=""><li id="b11c" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">我们现在可以运行<code class="fe ne nf ng nh b">schematool</code>来用上面的配置填充数据库:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="c211" class="nm lu iq nh b gy nn no l np nq">$HIVE_HOME/bin/schematool -dbType mysql -initSchema</span></pre><ul class=""><li id="3415" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">最后，让我们从以下内容开始metastore:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="1db5" class="nm lu iq nh b gy nn no l np nq">$HIVE_HOME/bin/hive --service metastore</span></pre><p id="58f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该命令将在<code class="fe ne nf ng nh b">localhost:9083</code>启动本地运行的配置单元metastore。</p><h1 id="2fdd" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">创建数据(和元数据)</h1><p id="ef5b" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们现在将使用<a class="ae lc" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Spark 3.3.0 </a>(为Hadoop 3.3+预构建)来创建和保存一些数据，并探索不同的成分是如何涉及的。</p><p id="fd4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，您也可以准备此导出:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="afeb" class="nm lu iq nh b gy nn no l np nq">export SPARK_HOME=~/dev/spark-3.3.0-bin-hadoop3/</span></pre><p id="0c54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里唯一的要求是将Spark指向外部Hive metastore。我们可以通过编辑<code class="fe ne nf ng nh b">$SPARK_HOME/conf/hive-site.xml</code>来做到这一点，如下所示:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="8378" class="nm lu iq nh b gy nn no l np nq">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br/>&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;hive.metastore.uris&lt;/name&gt;<br/>        &lt;value&gt;thrift://localhost:9083&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="69b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，创建一个指向正确的metastore jars和在设置Hadoop时配置的仓库目录的Spark shell:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="992f" class="nm lu iq nh b gy nn no l np nq">$SPARK_HOME/bin/spark-shell \<br/>  --jars \<br/>    $HIVE_HOME/lib/hive-metastore-2.3.9.jar,\<br/>    $HIVE_HOME/lib/hive-exec-2.3.9.jar,\<br/>    $HIVE_HOME/lib/hive-common-2.3.9.jar,\<br/>    $HIVE_HOME/lib/hive-serde-2.3.9.jar,\<br/>    $HIVE_HOME/lib/guava-14.0.1.jar \<br/>  --conf spark.sql.hive.metastore.version=2.3 \<br/>  --conf spark.sql.hive.metastore.jars=$HIVE_HOME"/lib/*" \<br/>  --conf spark.sql.warehouse.dir=hdfs://localhost:9000/user/hive/warehouse</span></pre><p id="de4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦进入外壳，让我们具体化一个样本数据帧:</p><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="2737" class="nm lu iq nh b gy nn no l np nq">import spark.implicits._</span><span id="7aa9" class="nm lu iq nh b gy nr no l np nq">val someDF = Seq(<br/>  (8, "bat"),<br/>  (64, "mouse"),<br/>  (-27, "horse")<br/>).toDF("number", "word")</span><span id="d4e9" class="nm lu iq nh b gy nr no l np nq">// Convert the DataFrame into a table we can reach<br/>// from within Spark SQL<br/>someDF.createOrReplaceTempView("mytempTable")</span><span id="8ce7" class="nm lu iq nh b gy nr no l np nq">// Materialize the table<br/>spark.sql("create table some_df as select * from mytempTable");</span></pre><p id="0495" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦这些操作成功运行，我们就可以放大体系结构来验证发生了什么:</p><ul class=""><li id="0dd5" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">当我们定义指向HDFS路径<code class="fe ne nf ng nh b">/user/hive/warehouse</code>的<code class="fe ne nf ng nh b">spark.sql.warehouse.dir</code>时，我们可以看到数据文件确实出现在我们的新表<code class="fe ne nf ng nh b">some_df</code>中:</li></ul><pre class="le lf lg lh gt ni nh nj nk aw nl bi"><span id="12ff" class="nm lu iq nh b gy nn no l np nq">❯ $HADOOP_HOME/bin/hadoop fs -ls /user/hive/warehouse/some_df<br/>Found 3 items<br/>-rwxr-xr-x   3 pmbrull supergroup          6 2022-08-27 22:11 /user/hive/warehouse/some_df/part-00000-e99ca76a-477c-47d3-829a-c4aa4e03c3a3-c000<br/>-rwxr-xr-x   3 pmbrull supergroup          9 2022-08-27 22:11 /user/hive/warehouse/some_df/part-00001-e99ca76a-477c-47d3-829a-c4aa4e03c3a3-c000<br/>-rwxr-xr-x   3 pmbrull supergroup         10 2022-08-27 22:11 /user/hive/warehouse/some_df/part-00002-e99ca76a-477c-47d3-829a-c4aa4e03c3a3-c000</span></pre><ul class=""><li id="702a" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">当Hive metastore插入MySQL实例时，关于新表的元数据也在那里创建:</li></ul><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oe"><img src="../Images/7665d225209145a56c577e9abaf9ad26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4crAdIkR6EG9GrFxq-GuRw.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">保存metastore数据的MySQL实例。图片由作者提供。</p></figure><h1 id="131d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">元数据摄取</h1><p id="f2f5" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在最后一步中，我们将使用<a class="ae lc" href="https://open-metadata.org/" rel="noopener ugc nofollow" target="_blank"> OpenMetadata </a>将我们刚刚生成的元数据接收到这个开源元数据管理解决方案中。</p><blockquote class="of"><p id="9fb0" class="og oh iq bd oi oj ok ol om on oo la dk translated">数据和元数据在公司中最受欢迎。</p></blockquote><p id="01e5" class="pw-post-body-paragraph kf kg iq kh b ki op jr kk kl oq ju kn ko or kq kr ks os ku kv kw ot ky kz la ij bi translated">目标是打破元数据孤岛，将组织聚集在一起，在单一位置进行协作。最简单的开始方式是使用<a class="ae lc" href="https://docs.open-metadata.org/quick-start/local-deployment" rel="noopener ugc nofollow" target="_blank">本地Docker部署</a>。</p><p id="9f42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设置完成后，我们可以继续配置一个<a class="ae lc" href="https://docs.open-metadata.org/openmetadata/connectors/database/deltalake" rel="noopener ugc nofollow" target="_blank"> DeltaLake </a>服务。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ou"><img src="../Images/621c4950bfe883b8060144b4991db3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JJwPxR_7HBrJDs_mQGv6Cg.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">三角洲湖服务配置。图片由作者提供。</p></figure><blockquote class="ov ow ox"><p id="a3fc" class="kf kg lb kh b ki kj jr kk kl km ju kn oy kp kq kr oz kt ku kv pa kx ky kz la ij bi translated">注意，当OpenMetadata在Docker内部运行时，Hive Metastore存在于我们的本地。为了到达主机的网络，我们需要将url写成<code class="fe ne nf ng nh b">host.docker.internal</code></p></blockquote><p id="8dad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">按照准备摄取管道的步骤，我们可以定期将metastore上的任何更改带到OpenMetadata。第一次摄取后，可以浏览和共享新创建的:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi pb"><img src="../Images/904eab26eec4f8580fb0b6d22db253ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSwzxEmXfW5S8NCYK6rqPQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">OpenMetadata中摄取的元数据。图片由作者提供。</p></figure><p id="ca29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们的数据可被发现是转向数据产品的第一步。共享的数据可以讨论和改进。</p><h1 id="f2c7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><p id="5284" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在本帖中，我们有:</p><ul class=""><li id="7728" class="mq mr iq kh b ki kj kl km ko ms ks mt kw mu la mv mw mx my bi translated">配置HDFS存储数据，</li><li id="7866" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">创建了一个插入MySQL实例的外部Hive Metastore，</li><li id="f40e" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">用Spark生成了一些数据，</li><li id="a8d3" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">使用OpenMetadata摄取了元数据。</li></ul><p id="e504" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">管理数据架构的整个生命周期使我们能够更好地理解这个世界中涉及的所有成分，在这个世界中，云提供商抽象了大部分内容，解决方案看起来几乎像魔术一样。</p><h1 id="367f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><ul class=""><li id="4d65" class="mq mr iq kh b ki ml kl mm ko pc ks pd kw pe la mv mw mx my bi translated">非常感谢亚采克·拉斯科斯基的优秀书籍。你可以在<a class="ae lc" href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/demo/demo-connecting-spark-sql-to-hive-metastore.html" rel="noopener ugc nofollow" target="_blank">Spark SQL</a>内部找到最初的设置指南。</li><li id="a822" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">配置单元<a class="ae lc" href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-RunningHive" rel="noopener ugc nofollow" target="_blank">文档</a></li><li id="2dd7" class="mq mr iq kh b ki mz kl na ko nb ks nc kw nd la mv mw mx my bi translated">Hadoop <a class="ae lc" href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/SingleCluster.html" rel="noopener ugc nofollow" target="_blank">文档</a></li></ul></div></div>    
</body>
</html>