<html>
<head>
<title>Addressing Overfitting 2023 Guide — 13 Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解决过度拟合 2023 指南— 13 种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/addressing-overfitting-2023-guide-13-methods-8fd4e04fc8#2022-11-22">https://towardsdatascience.com/addressing-overfitting-2023-guide-13-methods-8fd4e04fc8#2022-11-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="323a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">您的一站式学习场所，了解防止机器学习和深度学习模型过度拟合的 13 种有效方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cab2a4522ee863c2baec2091bef6d29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkLmQYLKY3WlSjbpKPqIzQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·范·迪克在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e57c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">谁不喜欢为大多数数据科学家面临的最糟糕的问题寻找解决方案呢？【过拟合问题】</em> </strong></p><p id="edfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文可能是学习机器学习和深度学习模型中防止过拟合的许多有效方法的一站式地方。</p><h1 id="c9a6" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">过度拟合会发生什么？</h1><p id="f82a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">过度拟合通常发生在模型<em class="lv">过于复杂</em>的时候。当模型过度拟合训练数据时，会发生以下情况:</p><ul class=""><li id="0b5a" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">模型试图记忆训练数据，而不是从数据中学习基本模式。</strong>机器学习涉及<em class="lv">从数据中学习</em>模式和规则，而不是<em class="lv">记忆</em>数据。</li><li id="26b7" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">该模型仅在训练数据上表现良好，而在新的未知数据上表现不佳。</strong>一个好的模型应该能够在训练数据上表现良好，并且能够在新的未知数据上进行很好的概括。</li></ul><h1 id="585e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">如何检测过度拟合</h1><p id="458c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下一个问题是:</p><blockquote class="nh"><p id="cbe3" class="ni nj it bd nk nl nm nn no np nq lu dk translated">我们如何识别机器学习和深度学习模型中的过拟合？</p></blockquote><h2 id="8c18" class="nr lx it bd ly ns nt dn mc nu nv dp mg li nw nx mi lm ny nz mk lq oa ob mm oc bi translated">使用学习曲线</h2><p id="434d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">过度拟合是我们肉眼看不到的东西！一个简单而有效的<a class="ae ky" rel="noopener" target="_blank" href="/10-amazing-machine-learning-visualizations-you-should-know-in-2023-528282940582">机器学习可视化</a>称为<em class="lv">学习曲线</em>可以用来检测机器学习和深度学习模式中的过度拟合。</p><blockquote class="nh"><p id="9c63" class="ni nj it bd nk nl nm nn no np nq lu dk translated">学习曲线绘制了训练和验证分数与时期数的关系。</p></blockquote><p id="8c5f" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">学习曲线表明模型过度拟合:</p><ul class=""><li id="b26b" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">如果训练分数和验证分数之间有明显的差距。</li><li id="5320" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">当验证误差(损失)在某个点开始增加，而训练误差(损失)仍然减少时。在准确度的情况下，验证准确度在某个点开始降低，而训练准确度仍然增加。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/60dbc5bfd90412ac82d24244f6872021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*Vl7Y5bCq3oP_M4ZdAAWs3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">用学习曲线检测过度拟合</strong>(图片由作者提供)</p></figure><h2 id="219e" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">使用验证曲线</h2><p id="7c2e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">学习曲线在深度学习模型中非常常见。以检测诸如决策树、随机森林、k-最近邻等一般机器学习模型中的过拟合。，我们可以使用另一个<a class="ae ky" rel="noopener" target="_blank" href="/10-amazing-machine-learning-visualizations-you-should-know-in-2023-528282940582">机器学习可视化</a>称为<em class="lv">验证曲线</em>。</p><blockquote class="nh"><p id="1490" class="ni nj it bd nk nl nm nn no np nq lu dk translated">验证曲线描绘了<em class="op">单个</em>超参数对训练和验证集的影响。</p></blockquote><p id="e390" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">x 轴代表给定的超参数值，而 y 轴代表训练和验证分数。</p><p id="24ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于单个超参数的给定值，我们可以使用验证曲线来检测机器学习模型中的过度拟合。</p><p id="924f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们需要确定最重要的模型超参数，并使用验证曲线绘制其值的影响。</p><p id="c00d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这方面的一些例子包括:</p><ul class=""><li id="34d6" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">我们可以使用验证曲线来绘制决策树或随机森林模型的<em class="lv"> max_depth </em>(树深度)超参数的影响。</li><li id="15ef" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">我们可以使用验证曲线来绘制 KNN 模型的<em class="lv"> n_neighbors </em>(邻居数量)超参数的影响。</li></ul><p id="c56c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了为随机森林分类器创建的验证曲线，用于测量<em class="lv"> max_depth </em>(树深度)超参数对训练和验证分数(精确度)的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/aec86b607938a0d647923bf4f8af2bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*G_HZYxYrMt_4nW5Wijq7Jw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">验证曲线</strong>(图片由作者提供)</p></figure><p id="6eb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="lv"> max_depth </em>值为 6 之后，模型开始过度拟合训练数据。换句话说，验证精度在<em class="lv"> max_depth=6 </em>处开始下降，而训练精度仍在增加。</p><h2 id="ba56" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">使用多个评估指标</h2><p id="8da3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">根据我们使用的机器学习算法的类型，Scikit-learn 和 TensorFlow 提供了不同类型的模型评估指标。我们可以使用(甚至组合)这些评估指标来监控模型在训练期间的性能，然后通过分析评估指标的值来确定模型是否过度拟合。</p><p id="1e25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了完全成熟的决策树分类器的训练和测试精度，以及测试数据的混淆矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/a48c1a28f18e9920e9bffee0952a6899.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*OxwzKOuPL-wt4jGqhfY96A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">评估决策树分类器</strong>(图片由作者提供)</p></figure><p id="9ca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在完全成熟的决策树中，过度拟合是有保证的！100%的训练准确率意味着决策树分类器模型在训练集上表现良好。71%的测试准确度清楚地表明，该模型在新的看不见的数据上表现不好。测试精度远低于训练精度。换句话说，训练和测试的准确性之间有明显的差距。这些事情表明模型明显过度拟合。</p><p id="a832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，假阳性和假阴性的数量也很高。这是该模型在测试数据上表现不佳的另一个迹象。</p><p id="b77e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在应用适当的正则化技术(限制树的增长和创建系综)之后，我们得到以下评估值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/9ae5110c12dae5c07dbc643e0e86dd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*pjby_BqKtEDtrIxomAJtwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="a06f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练和测试准确度分数都很高，它们之间没有明显的差距。除此之外，假阳性和假阴性的数量也减少了。现在很明显，正则化决策树模型现在并没有过拟合。</p><h1 id="7d63" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">解决过度拟合的 13 种有效方法</h1><p id="48ce" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这里总结了机器学习和深度学习模型中用于防止过拟合的方法。我们将详细讨论每种方法。</p><pre class="kj kk kl km gt ot ou ov bn ow ox bi"><span id="eb1b" class="oy lx it ou b be oz pa l pb pc"><strong class="ou iu">Addressing Overfitting - 13 Methods</strong><br/>-----------------------------------<br/>01. <a class="ae ky" href="#76e6" rel="noopener ugc nofollow">Dimensionality Reduction</a><br/>02. <a class="ae ky" href="#2943" rel="noopener ugc nofollow">Feature Selection</a><br/>03. <a class="ae ky" href="#c513" rel="noopener ugc nofollow">Early Stopping</a><br/>04. <a class="ae ky" href="#ae2a" rel="noopener ugc nofollow">K-Fold Cross-Validation</a><br/>05. <a class="ae ky" href="#832f" rel="noopener ugc nofollow">Creating Ensembles</a><br/>06. <a class="ae ky" href="#5b1b" rel="noopener ugc nofollow">Pre‐Pruning</a><br/>07. <a class="ae ky" href="#235d" rel="noopener ugc nofollow">Post‐Pruning</a><br/>08. <a class="ae ky" href="#bbc5" rel="noopener ugc nofollow">Noise Regularization</a><br/>09. <a class="ae ky" href="#8cde" rel="noopener ugc nofollow">Dropout Regularization</a><br/>10. <a class="ae ky" href="#27c3" rel="noopener ugc nofollow">L1 and L2 Regularization</a><br/>11. <a class="ae ky" href="#4d98" rel="noopener ugc nofollow">Data (Image) Augmentation</a><br/>12. <a class="ae ky" href="#e080" rel="noopener ugc nofollow">Adding More Training Data</a><br/>13. <a class="ae ky" href="#3dc4" rel="noopener ugc nofollow">Reducing Network Width &amp; Depth</a></span></pre><blockquote class="pd pe pf"><p id="31b2" class="kz la lv lb b lc ld ju le lf lg jx lh pg lj lk ll ph ln lo lp pi lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">注意:</em> </strong>如果你想获得下面讨论的每种方法的实践经验，我已经为此创建了一个单独的文章集！访问<a class="ae ky" href="https://rukshanpramoditha.medium.com/list/handson-practice-for-addressing-overfitting-2023-guide-de3820029e5a" rel="noopener">此链接</a>获取所有文章。在那里，您将通过编写代码来学习如何应用每种方法！</p></blockquote><p id="c33f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><h2 id="76e6" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">1️⃣通过降维解决过拟合问题</h2><p id="5b16" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">当模型<em class="lv">太复杂</em>时，过度拟合经常发生。模型复杂的主要原因是数据中存在许多特征(变量)。数据中特征的数量称为其维数。</p><blockquote class="nh"><p id="51c0" class="ni nj it bd nk nl nm nn no np nq lu dk translated">当训练数据的维数较高时，模型往往会过度拟合训练数据。</p></blockquote><p id="5e59" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">减少数据中的特征数量称为<em class="lv">降维</em>。我们应该尽可能多地保留原始数据中的差异。否则，我们会丢失数据中的有用信息。</p><p id="76b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维处理过拟合，如下所示。</p><ol class=""><li id="463a" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu pj mz na nb bi translated">降维减少了数据中的特征数量。应用降维后，模型的复杂度也将降低。因此，模型不会再过度拟合训练数据！</li><li id="4a1e" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu pj mz na nb bi translated">降维还可以去除数据中不必要的噪声。噪声数据会导致过度拟合。该模型将防止在去除数据中的噪声之后过度拟合训练数据。</li></ol><p id="3098" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最常见的降维方法是<em class="lv">主成分分析</em>。它为低维形式的数据找到一组新的不相关特征。</p><blockquote class="nh"><p id="0e88" class="ni nj it bd nk nl nm nn no np nq lu dk translated">PCA 可以有效地消除过拟合问题。</p></blockquote></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="2943" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">2️⃣通过特征选择解决过拟合问题</h2><p id="fd1d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">特征选择可以被认为是一种降维方法，因为它从数据集中移除了冗余(不必要)的特征。这减少了数据中的特征(维度)数量。</p><p id="0276" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征部分方法不是寻找一组新的特征，而是通过移除数据中不需要的特征来仅保留最重要的特征。原始值保持不变。这就是特征选择与 PCA 的不同之处，在 PCA 中我们得到新的变换值。</p><p id="6db3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个简单的<a class="ae ky" rel="noopener" target="_blank" href="/10-amazing-machine-learning-visualizations-you-should-know-in-2023-528282940582">机器学习可视化</a>叫做<em class="lv">特征重要性图</em>可以用来选择最重要的特征。该图是基于每个特征的相对重要性而创建的。下图显示了使用包含 30 个特征的<em class="lv">乳腺癌</em>数据集创建的图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/6ba4398906e34d63dfa15a83b1331cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*dPgo7GEeThsIlmXXFzCL9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">用于<em class="op">乳腺癌</em>数据集</strong>的特征重要性图(图片由作者提供)</p></figure><p id="3c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到一些特征对模型没有太大的贡献。他们的贡献微不足道。因此，我们可以删除这些特征，只使用最重要的特征来构建模型。</p><blockquote class="nh"><p id="e37c" class="ni nj it bd nk nl nm nn no np nq lu dk translated">从数据中移除最不重要的特征将降低模型的复杂性，并消除数据中的噪声(如果有的话)。这就是特征选择如何防止模型过度拟合。</p></blockquote></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="c513" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">3️⃣通过提前停止解决过度拟合问题</h2><p id="e813" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">早期停止是防止机器学习和深度学习模型中过拟合的另一种有效方法。</p><blockquote class="nh"><p id="56d2" class="ni nj it bd nk nl nm nn no np nq lu dk translated">在早期停止中，我们通过查看学习曲线或验证曲线，在模型开始过度拟合之前，有意地提前停止模型训练过程。</p></blockquote><figure class="pt pu pv pw px kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/f52c09ba4193e9f756e58d7185b18971.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*lil9hOnHckXG-CmRzmZtIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">使用学习曲线</strong>执行提前停止(图片由作者提供)</p></figure><p id="95c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第 5 个时期之后，模型开始过度拟合训练数据。如果我们在那个时期之后继续训练，尽管训练误差进一步减小，但是验证误差增加。训练分数和验证分数之间的差距也会增大。这些都是过度拟合的迹象。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="ae2a" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">4️⃣通过 k 倍交叉验证解决过度拟合问题</h2><p id="858a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">k 倍交叉验证是一种数据分裂策略。在构建 ML 和 DL 模型时，我们通常将整个数据集分成训练集和测试集。这里的问题是，模型在训练过程中只看到一组特定的实例(数据点)。</p><p id="2aee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 k-fold 交叉验证中，根据<em class="lv"> k </em>的值(通常为 5 或 10)，将整个数据集拆分成不同的折叠。每个文件夹包含不同类型的实例(数据点)。该模型在每次迭代中对 k-1 倍的数据进行训练。在每次迭代中使用剩余的数据折叠进行评估。如下图所示，训练和评估折叠在每次迭代时都会发生变化。在每次迭代中计算评估分数，并取平均值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/9d51ecedf91ba83ba52552bc66437115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*S6YJamGCclKxdZgoRP2PAg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj"> K 倍交叉验证程序:k=5 </strong>(图片由作者提供)</p></figure><p id="d287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 k-fold 交叉验证中，模型在训练期间看到不同的实例集(数据点),因为在每次迭代中训练和评估折叠都发生了变化。该模型将学习所有需要的模式，并将对新的看不见的数据进行很好的归纳。换句话说，k 重交叉验证可以防止模型过度拟合。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="832f" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">5️⃣通过创建合奏解决过度配合问题</h2><p id="3178" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这种方法仅限于基于树的模型。决策树模型总是过度拟合训练数据，除非我们通过为<em class="lv"> max_depth </em>(树深度)超参数设置一个较低值来限制树的增长。</p><p id="2936" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使我们在训练期间限制了树的增长，决策树模型仍然可能过度拟合训练数据。减少决策树过度拟合的一个有用方法是创建集成。集成(组)是从训练数据和特征的子集创建的多个决策树的集合。</p><p id="22eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，随机森林是包含一组不相关的决策树的集合。</p><blockquote class="nh"><p id="3e1b" class="ni nj it bd nk nl nm nn no np nq lu dk translated">与决策树模型相比，随机森林由于其额外的随机性，不太可能过度拟合训练数据。</p></blockquote><p id="507b" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">由于<em class="lv">不相关</em>树，额外的随机性出现在随机森林中。当创建随机森林时，数据混合得很好。除此之外，通过平均每个不相关树的结果来计算最终结果。因此，随机森林比单一决策树能产生更准确和稳定的结果。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="5b1b" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">6️⃣通过预修剪解决过度拟合问题</h2><p id="51c7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">修剪方法用于防止决策树中的过度拟合。有两种主要的修剪方法，称为预修剪和后修剪。</p><p id="fc13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">默认情况下，决策树会增长到其最大深度。完全长大的树总是超过训练数据。</p><blockquote class="nh"><p id="36b1" class="ni nj it bd nk nl nm nn no np nq lu dk translated">在决策树中，修剪是控制树的生长的过程。</p></blockquote><p id="a3df" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">预修剪应用提前停止规则，该规则过早地停止决策树的生长。预剪枝后，决策树的分支更少。我们可以通过限制以下超参数的值来在决策树中应用预修剪。</p><ul class=""><li id="8099" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu"> max_depth: </strong>树的最大深度。减小该值可以防止过度拟合。</li><li id="8f4b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> min_samples_leaf: </strong>一个叶节点所需的最小样本数。增加该值可以防止过度拟合。</li><li id="2bf9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu"> min_samples_split: </strong>分割内部节点所需的最小样本数。增加该值可以防止过度拟合。</li></ul><p id="e732" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种常用的方法来调整这些超参数。</p><ul class=""><li id="2183" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">一次测量一个超参数的影响，同时保持其他超参数的默认值。为此，我们可以使用验证曲线来检测过度拟合。</li><li id="8fe5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">一次调整多个超参数。此处不能使用验证曲线来测量多个超参数的影响。这里，应使用<a class="ae ky" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c">网格搜索或随机搜索</a>一次调整多个超参数。</li></ul></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="235d" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">7️⃣通过后期修剪解决过度拟合问题</h2><blockquote class="nh"><p id="3958" class="ni nj it bd nk nl nm nn no np nq lu dk translated">后期修剪是指在树完全长成后，除去树的某些部分的过程。</p></blockquote><p id="3fac" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">成本复杂性剪枝(ccp)是一种后剪枝方法。它包括为 Scikit-learn 决策树类中的<strong class="lb iu"><em class="lv">CCP _ 阿尔法</em> </strong>超参数找到正确的值。</p><p id="fb4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> ccp 阿尔法</em>的默认值是零，这意味着默认情况下不执行修剪。较大的值会增加要修剪的节点数，并降低树的深度。因此，<em class="lv">CCP _ 阿尔法</em>的较大值可以防止过度拟合。</p><p id="dd50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要找到<strong class="lb iu"><em class="lv">CCP _ 阿尔法</em> </strong>超参数的最佳值:</p><ul class=""><li id="9578" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">我们可以尝试 0.01、0.02、0.05、0.1 等不同的值。，并监控培训和验证分数。</li><li id="4d44" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">我们可以将所有有效的阿尔法值一次一个地传递给<strong class="lb iu"> ccp 阿尔法</strong>超参数，然后计算训练和验证分数。阿尔法的所有有效值都可以通过<strong class="lb iu">CCP _ 阿尔法属性</strong>访问。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/4bcd574a5ea86f40b29a723829a7f140.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*gV0h-wtmnYoLPjSp7yZZIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">调整 ccp 阿尔法超参数</strong>(图片由作者提供)</p></figure><p id="1132" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="lv"> alpha=0.06 </em>时，验证精度开始增加，而训练精度几乎保持不变。alpha 的下一个可用值是 0.12，它的性能分数非常低。根据该图，α的最佳值是 0.06。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="bbc5" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">用噪声正则化解决过拟合的 8️⃣</h2><p id="6ddc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">向现有数据添加噪声是防止神经网络中过拟合的有效正则化方法。我们通常将噪声添加到神经网络的输入层，因为输入层保存训练数据，尽管也可以将噪声添加到隐藏层和输出层。</p><p id="1026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当向训练数据添加噪声时，将向每个训练实例添加少量噪声，并生成同一实例的不同版本。这将扩展原始数据集！这将隐式地向原始数据集中添加更多的训练数据。添加更多的训练数据将有助于防止过度拟合。</p><p id="5e15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，在将噪声添加到数据之后，模型将<strong class="lb iu"> <em class="lv">而不是</em> </strong>单独捕获训练数据中的噪声。这也将有助于减少过度拟合。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="8cde" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">9️⃣通过调整辍学来解决过度适应问题</h2><p id="c3e9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">放弃正则化是一种特定于神经网络的正则化方法，用于防止神经网络中的过拟合。</p><p id="77c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在丢弃正则化中，该算法根据我们在每层中定义的概率值，在训练期间从网络中随机删除一些节点。被移除的节点不参与参数更新过程。基于每一层应用丢失正则化。这意味着我们可以在每一层分别设置不同的退出概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/e61469722051cd1dc6bdc92d15110fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mkzuxvFcDBCJFqwsdUjeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj">辍学正规化流程</strong>(图片由作者提供)</p></figure><p id="dc4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在应用退出正则化之后，原始网络变得更小。较小的网络灵活性较差，因此不会出现过度拟合。</p><p id="68fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在丢失正则化中，一些权重值为零，因为它们的节点是不活动的。因此，所有其他权重都需要参与权重更新过程。网络的输出不依赖于某些大的权重。这可以减少神经网络中的过拟合。</p><blockquote class="nh"><p id="695d" class="ni nj it bd nk nl nm nn no np nq lu dk translated">丢失正则化是防止神经网络过拟合的最有效方法。</p></blockquote></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="27c3" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">用 L1 和 L2 正则化解决过拟合的 1️⃣0️⃣</h2><p id="d4b8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">L1 和 L2 正则化通常应用于神经网络模型，以防止过拟合。L1 和 L2 正则化的选择取决于我们在训练期间添加到损失函数中的正则化项。</p><p id="4c34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当正则项为<strong class="lb iu"> <em class="lv"> L1 范数</em> </strong> [ <em class="lv"> λ *(权重绝对值之和)</em> ]时，称为<em class="lv"> L1 正则化</em>。当正则项为<strong class="lb iu"> <em class="lv"> L2 范数</em> </strong> [ <em class="lv"> λ *(权值的平方值之和)</em> ]时，称为<em class="lv"> L2 正则化</em>。</p><p id="4227" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络的 L1 和 L2 正则化定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/862b23b95d6a2be13c09a3c7aec51956.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*riEaYSUCwPQCFmWgNOmhcA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd oj"> L1 和 L2 正规化</strong>(图片由作者提供)</p></figure><p id="7372" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">λ控制正则化的级别。因此称为正则化参数(因子)。</p><ul class=""><li id="99d4" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><code class="fe qc qd qe ou b">lambda=0</code>:最小值。没有应用正则化。</li><li id="ea0b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><code class="fe qc qd qe ou b">lambda=1</code>:最大值。应用完全正则化。</li></ul><p id="5ebf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">λ值越大，意味着正则化越强，从而减少了神经网络中的过拟合。</p><p id="fcca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，L1 和 L2 正则化保持神经网络的权重较小。不会有任何大的权重过分强调某些输入。小权重值对输入数据中存在的噪声不太敏感。因此，在 L1 和 L2 正则化神经网络中不会发生过拟合。</p><p id="8f81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">L1 和 L2 正则化方法也可以应用于一般的机器学习算法，如逻辑回归、线性回归等。</p><p id="cf58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 logistic 回归中，有一个超参数叫做<strong class="lb iu"><em class="lv"/></strong>(取值:<em class="lv">‘L1’</em><em class="lv">‘L2’</em>和<em class="lv">‘elastic net’</em>)<strong class="lb iu"/>来选择正则化的类型。注意'<em class="lv"> elasticnet' </em>同时将 L1 和 L2 正则化应用于模型。</p><p id="4405" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于线性回归，Scikit-learn 为每种正则化类型提供了三个单独的类，称为<code class="fe qc qd qe ou b">Ridge()</code>(应用 L2 正则化)、<code class="fe qc qd qe ou b">Lasso()</code>(应用 L1 正则化)和<code class="fe qc qd qe ou b">ElasticNet()</code>(应用 L1 和 L2 正则化)。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="4d98" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">1️⃣1️⃣用数据(图像)寻址过拟合 a <strong class="ak">增强</strong></h2><p id="71db" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">数据扩充通常在图像数据上执行。因此，数据增强在某些上下文中也被称为<em class="lv">图像增强</em>。</p><p id="4af7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像增强是通过对同一幅图像进行某种变换，产生新的变体，从而增加图像数量的过程，这种变换包括<em class="lv">缩放</em>、<em class="lv">翻转</em>、<em class="lv">旋转</em>、<em class="lv">移动</em>、<em class="lv">缩放</em>、<em class="lv">光照</em>、<em class="lv">压缩</em>、<em class="lv">裁剪</em>等。</p><p id="63d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像增强最重要的是保留图像的上下文。换句话说，当扩充图像时，图像上下文不应该被改变。</p><p id="4da6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像增强更适合经常需要更多训练数据的深度学习模型。这是一种为模型提供更多训练数据的廉价方法！</p><p id="19b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像增强如下减少了神经网络中的过拟合。</p><ol class=""><li id="e009" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu pj mz na nb bi translated">图像增强通过添加更多的训练实例来扩展数据集。添加更多的训练数据将防止过度拟合。</li><li id="cf96" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu pj mz na nb bi translated">图像增强允许神经网络在训练期间看到相同图像的许多变体。这减少了在学习重要特征时对图像原始形式的依赖。当在新的看不见的数据上测试时，网络将变得更加健壮和稳定。</li></ol></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="e080" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">1️⃣2️⃣通过添加更多训练数据解决过度拟合问题</h2><p id="debb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">向模型中添加更多的训练数据将防止过度拟合。有许多方法可以将更多的训练数据添加到模型中。</p><ul class=""><li id="4dde" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">收集新的相关数据(昂贵)</li><li id="b197" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">通过向数据添加噪声来扩展原始数据集(成本低廉)</li><li id="7422" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">数据扩充(廉价)</li></ul></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="3dc4" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">1️⃣3️⃣通过减小网络宽度和深度解决过拟合问题</h2><p id="f5aa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">神经网络的结构由其宽度和深度定义。<em class="lv">深度</em>定义了神经网络中隐藏层的数量。<em class="lv">宽度</em>定义了神经网络每层的节点(神经元/单元)数量。</p><p id="72d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">减少隐藏层的数量和隐藏单元的数量会降低网络的灵活性。灵活性较低的网络无法捕获数据中的噪声，也不会过度拟合训练数据。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><p id="11f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><p id="b9f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或反馈，请告诉我。</p><h2 id="3a5e" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">阅读下一篇(强烈推荐)</h2><ul class=""><li id="93c4" class="mt mu it lb b lc mo lf mp li qf lm qg lq qh lu my mz na nb bi translated"><strong class="lb iu">获得“解决过度装配 2023 指南”的实践操作</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/handson-practice-for-addressing-overfitting-2023-guide-de3820029e5a"><div class="gh gi qi"><img src="../Images/b7f9f6e7a4919802fae1309fb61bcf5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*m6cbw_drSoDvfeiuK_h_0w.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者截图)</p></figure></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="bc11" class="nr lx it bd ly ns ok dn mc nu ol dp mg li om nx mi lm on nz mk lq oo ob mm oc bi translated">支持我当作家</h2><p id="4538" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑注册成为会员，不受限制地使用媒体。它只需要每月 5 美元，我会收到你的会员费的一部分。</p><div class="qj qk gp gr ql qm"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd iu gy z fp qr fr fs qs fu fw is bi translated">通过我的推荐链接加入 Medium</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">阅读 Rukshan Pramoditha(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接…</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="qv l"><div class="qw l qx qy qz qv ra ks qm"/></div></div></a></div><p id="b5c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一篇文章再见。祝大家学习愉快！</p><p id="88e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="rb rc ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----8fd4e04fc8--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫蒂塔</a><br/><strong class="lb iu">2022–11–22</strong></p></div></div>    
</body>
</html>