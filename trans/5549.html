<html>
<head>
<title>Let us Extract some Topics from Text Data — Part III: Non-Negative Matrix Factorization (NMF)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们从文本数据中提取一些主题——第三部分:非负矩阵分解(NMF)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada#2022-12-14">https://towardsdatascience.com/let-us-extract-some-topics-from-text-data-part-iii-non-negative-matrix-factorization-nmf-8eba8c8edada#2022-12-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b61" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解更多有关线性代数派生的无监督算法的信息，该算法使用直观的方法进行主题建模</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/20a8849315a951183cee13e3afa88c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yE9JEyyVK9xJYKXw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.pexels.com/ko-kr/photo/3184642/" rel="noopener ugc nofollow" target="_blank">pexel</a>免费使用照片</p></figure><h1 id="f38a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="6d5a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">主题建模</strong>是一种自然语言处理(NLP)任务，它利用无监督的学习方法来提取我们处理的一些文本数据的主要主题。这里的“无监督”一词意味着没有具有相关主题标签的训练数据。相反，算法试图直接从数据本身中发现底层模式，在这种情况下是主题。</p><p id="6a7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有各种各样的算法被广泛用于主题建模。在我之前的两篇文章中，我向您介绍了用于主题建模的两种算法，LDA 和 GSM。</p><div class="ms mt gp gr mu mv"><a rel="noopener follow" target="_blank" href="/let-us-extract-some-topics-from-text-data-part-i-latent-dirichlet-allocation-lda-e335ee3e5fa4"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">让我们从文本数据中提取一些主题——第一部分:潜在狄利克雷分配(LDA)</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">了解使用 Python 的 nltk、gensim、sklearn 和 pyLDAvis 包进行主题建模需要什么及其实现</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">towardsdatascience.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ks mv"/></div></div></a></div><div class="ms mt gp gr mu mv"><a href="https://medium.com/geekculture/let-us-extract-some-topics-from-text-data-part-ii-gibbs-sampling-dirichlet-multinomial-mixture-9e82d51b0fab" rel="noopener follow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd iu gy z fp na fr fs nb fu fw is bi translated">让我们从文本数据中提取一些话题——第二部分:吉布斯抽样狄利克雷多项式混合…</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">了解如何将 GSM 用于主题建模，以及如何将其与 LDA 进行比较</h3></div><div class="nd l"><p class="bd b dl z fp na fr fs nb fu fw dk translated">medium.com</p></div></div><div class="ne l"><div class="nk l ng nh ni ne nj ks mv"/></div></div></a></div><p id="fef0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本文中，我们将研究非负矩阵分解(NMF)，一种源于线性代数的无监督算法。Python 中的代码实现也将被引入。</p><h1 id="4193" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是 NMF？</h1><p id="c9bf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">关于 NMF 的一个有趣的事情是，它被用于许多其他应用，包括推荐系统，不像其他专门用于主题建模的主题建模算法。NMF 是在线性代数领域发展起来的，能够识别数据中的潜在或隐藏结构。简而言之，NMF 将高维向量分解(或因式分解，因此得名因式分解)成低维表示。这些低维向量的所有系数都是非负的，这可以从算法的名称“非负”矩阵分解中得到暗示。</p><p id="98b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">假设因式分解的原始矩阵是 A，NMF 将把 A 分解成两个矩阵 W 和 H。W 被称为包含 NMF 发现的主题的特征矩阵，H 被称为包含这些主题的系数(权重)的分量矩阵。如果矩阵 A 的形状为 M x N，主题数为 k，则特征矩阵和成分矩阵的形状分别为(M x k)和(k x N)。</p><h1 id="d76d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">一些数学</h1><p id="7166" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于 NMF 是一个无人监督的算法，它需要一些措施，以便能够找出不同文档之间的相似性。有几种数学度量用来定义这种相似性或距离感，但我在本节中介绍了两种最流行的度量。</p><p id="14bf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第一个是广义的 kull back-lei bler 散度，定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8d42e4b4e50634a2729135a20143ce0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*GMRYT4rvo0QpwWlMKrQIcA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者使用乳胶</p></figure><p id="450d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">kull back-lei bler 散度可以实现如下。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="7b87" class="nr la it nn b be ns nt l nu nv">from numpy import sum<br/><br/>def kullback_leibler_divergence(p, q):<br/>  return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))<br/><br/># There is built-in function in the SciPy package that <br/># you can directly use as well<br/><br/>from scipy.special import kl_div</span></pre><p id="c2bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可以在这里阅读更多关于这种分歧的信息。</p><p id="1fb4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一个流行的度量是 Frobenius 范数，它是其元素的绝对平方和的平方根。它也被称为欧几里德范数。数学公式如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/a7a80ac09210c7da25a4d495c36293ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*WcZ_WqgFHzNSYTSl10Ycpg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者使用乳胶</p></figure><p id="c925" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可以使用 NumPy 包来实现这一点。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="c3db" class="nr la it nn b be ns nt l nu nv">import numpy as np<br/><br/># Assuming some array a has been already defined<br/>frobenius_norm = numpy.linalg.norm(a)</span></pre><p id="59be" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于优化，通常使用这两种算法中的任何一种。它们是坐标下降解算器和乘法更新解算器。哪种优化算法更好的问题取决于您正在处理的数据类型及其特征(例如稀疏度)，因此尝试这两种算法以查看哪种返回的主题更连贯且质量更高可能是最佳方法。在评估主题质量方面，更多的内容将在本文的后面部分进行解释。参考这篇<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/" rel="noopener ugc nofollow" target="_blank">文章</a>对这两种优化算法有更深入的了解。</p><h1 id="d9ef" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">履行</h1><p id="60d1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，让我们深入研究 NMF 的代码实现。和往常一样，我们从安装我们需要的包开始，并将它们导入到我们的环境中。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="f44e" class="nr la it nn b be ns nt l nu nv">!pip install gensim<br/><br/>import nltk<br/>from nltk.stem import *<br/><br/>nltk.download('punkt')<br/>nltk.download('wordnet')<br/>nltk.download('stopwords')<br/>nltk.download('omw-1.4')<br/>nltk.download('averaged_perceptron_tagger')<br/><br/># Prepare list of stopwords for removal<br/>stopwords = set(nltk.corpus.stopwords.words('english'))<br/><br/>import pandas as pd<br/><br/># NMF implementation in sklearn<br/>from sklearn.decomposition import NMF <br/><br/># TF-IDF Vectorization<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/><br/># Dataset we use for this tutorial<br/>from sklearn.datasets import fetch_20newsgroups <br/><br/>from scipy import linalg # For linear algebra operations<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>np.set_printoptions(suppress=True)</span></pre><p id="f631" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们读入本教程的数据集。我们将使用 20 个新闻组数据，每个人都可以通过 sklearn 包获得这些数据。它使用<a class="ae ky" href="https://www.apache.org/licenses/LICENSE-2.0" rel="noopener ugc nofollow" target="_blank">Apache 2.0 版许可</a>。只是提醒一下，为了不同教程之间的一致性，我在关于主题建模的前两篇文章中使用了这个数据集。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="a6b0" class="nr la it nn b be ns nt l nu nv"># Categories<br/>cats = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'] <br/>remove = ('headers', 'footers', 'quotes')<br/><br/># Read in 20 NewsGroup Data<br/>fetch20newsgroups = \<br/>fetch_20newsgroups(subset='train', categories=cats, remove=remove)<br/><br/># Transform fetch20newsgroups object to pandas dataframe<br/>df = pd.DataFrame(fetch20newsgroups.data, columns=['text'])</span></pre><p id="9c59" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们通过一些传统的文本预处理步骤来确保建模主题的质量不会变差。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="98bb" class="nr la it nn b be ns nt l nu nv"># Remove Punctuations <br/><br/>import string<br/>PUNCT_TO_REMOVE = string.punctuation<br/><br/>df.text = \<br/>df.text.apply(lambda x: x.translate(str.maketrans('', '', PUNCT_TO_REMOVE)))<br/><br/><br/># Remove URLs<br/>import re<br/>def remove_url(text):<br/>  return re.sub(r'https?:\S*','',text)<br/><br/>df.text = df.text.apply(remove_url)<br/><br/><br/># Remove mentions and hashtags<br/>import re<br/>def remove_mentions_and_tags(text):<br/>    text = re.sub(r'@\S*','',text)<br/>    return re.sub(r'#\S*','',text)<br/><br/>df.text = df.text.apply(remove_mentions_and_tags)<br/><br/># Make all the text lower case<br/>df.text = df.text.str.lower()<br/><br/><br/># Remove words that exist across almost all documents (almost like removing<br/># stopwords) after tokenization<br/>extraneous_words = \<br/>['article', 'organization', 'subject','line','author','from', <br/>'lines', 'people', 're', 'writes', 'distribution', <br/>'x', 'nntppostinghost','think','university']<br/><br/><br/>docs = []<br/><br/>for news in df.text:<br/>    <br/>   words = \<br/>[w for w in nltk.tokenize.word_tokenize(news) \<br/>if (w not in stopwords) &amp; (w not in extraneous_words) &amp; \<br/>(nltk.pos_tag([w])[0][1] in ['NN','NNS'])]<br/><br/>    docs.append(words)</span></pre><p id="baa9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们应用<a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>矢量化。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="caee" class="nr la it nn b be ns nt l nu nv"># Initialize TFIDF Vectorizer<br/>vect= TfidfVectorizer(<br/>    min_df=10,<br/>    max_df=0.85,<br/>    max_features=5000,<br/>    ngram_range=(1, 3), # Include unigrams, bi-grams and tri-grams<br/>    stop_words='english' # Remove Stopwords<br/>)<br/><br/># Apply Transformation<br/>X = vect.fit_transform(df.text)<br/><br/># Create an NMF instance with 4 components<br/>model = NMF(n_components=4, init='nndsvd', random_state=42)<br/><br/># Fit the model<br/>model.fit(X)</span></pre><p id="368a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们分别存储特征和组件矩阵。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="09c5" class="nr la it nn b be ns nt l nu nv"># Features Matrix<br/>nmf_features = model.transform(X)<br/><br/># Components Matrix<br/>components_df = pd.DataFrame(model.components_, <br/>                            columns=vect.get_feature_names_out())<br/><br/>terms =  list(vect.get_feature_names_out())</span></pre><h2 id="b28a" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">获取每个主题的最高系数值的单词(从组件矩阵中)</h2><p id="849a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从组件矩阵中，我们可以通过检索与最高系数值相关联的标记来查看每个主题中最有意义的单词。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="6662" class="nr la it nn b be ns nt l nu nv"># Top 20 words of importance in each of the topics<br/><br/>for topic in range(components_df.shape[0]):<br/>    topic_df = components_df.iloc[topic]<br/>    print(f'For topic {topic+1} the words with the highest value are:')<br/>    print(topic_df.nlargest(20))<br/>    print('\n')</span></pre><p id="83fd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们看看第一个主题的输出作为例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2df6cafa3a2fd8b9e7ea3bb4ed1737a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*5C4DopRiL9FufD4ZFF0Ymw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:来自作者</p></figure><p id="552a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与第一个主题相关的重要词汇是上帝、耶稣、圣经和信仰，这些词汇我们会认为会出现在与无神论(这是第一个主题的实际主题)相关的文献中。</p><h2 id="94f3" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">主题描述符</h2><p id="2323" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基于以上信息，我们可以打印出每个主题的前 n 个关键词。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="45d3" class="nr la it nn b be ns nt l nu nv">def get_descriptor( terms, H, topic_index, top ):<br/>  # reverse sort the values to sort the indices<br/>  top_indices = np.argsort( H.iloc[topic_index,:] )[::-1]<br/><br/>  # get the terms corresponding to the top-ranked indices<br/>  top_terms = [ ]<br/>  for term_index in top_indices[0:top]:<br/>      top_terms.append( terms[term_index] )<br/>  return top_terms<br/><br/>k = 4 # four topics<br/>descriptors = [ ]<br/>for topic_index in range(k):<br/>  descriptors.append(get_descriptor(terms, components_df, topic_index, 10 ))<br/>  str_descriptor = ", ".join( descriptors[topic_index] )<br/>  print("Topic %02d: %s" % ( topic_index+1, str_descriptor ) )</span></pre><p id="4ac0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后我们得到:</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="8553" class="nr la it nn b be ns nt l nu nv">Topic 01: god, people, dont, think, just, jesus, say, bible, believe, does <br/>Topic 02: thanks, files, graphics, image, file, know, program, format, help, need <br/>Topic 03: space, nasa, launch, shuttle, lunar, orbit, moon, station, data, earth <br/>Topic 04: sank manhattan sea, stay blew, away sank, away sank manhattan, sank manhattan, said queens stay, said queens, bob beauchaine, bronx away sank, bronx away</span></pre><h2 id="6e66" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">第 j 个文档的主题</h2><p id="25dc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们还可以从特征矩阵中找出第 j 个文档属于哪个主题。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="1108" class="nr la it nn b be ns nt l nu nv"># Topic of the jth document<br/><br/>j = 100 <br/><br/>print(pd.DataFrame(nmf_features).loc[100])<br/><br/># Add 1 to index to get the topic number since index starts from 0 not 1<br/>print("\n {}th Document belongs to Topic {} ".\<br/>format(j, np.argmax(pd.DataFrame(nmf_features).loc[100])+1))<br/><br/>&gt;&gt;<br/>0    0.113558<br/>1    0.016734<br/>2    0.015008<br/>3    0.000000<br/>Name: 100, dtype: float64<br/><br/>100th Document belongs to Topic 1 </span></pre><h2 id="c4c3" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">预测新文档的主题</h2><p id="563c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对于一个以前没有被算法发现的新文档，我们也可以预测这个文档属于哪个主题。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="b75a" class="nr la it nn b be ns nt l nu nv"># New Document to predict<br/>new_doc = """Jesus is the light of thie world"""<br/><br/># Transform the TF-IDF<br/>X_new = vect.transform([new_doc])<br/><br/># Transform the TF-IDF: nmf_features<br/>nmf_features_new = model.transform(X_new)<br/><br/># The idxmax function returns the index of the maximum element<br/># in the specified axis<br/>print("This new document belongs to Topic {}".\<br/>format(pd.DataFrame(nmf_features_new).idxmax(axis=1).iloc[0] + 1))<br/><br/>&gt;&gt;<br/>This new document belongs to Topic 1</span></pre><h2 id="7864" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">最佳主题数量的评估</h2><p id="3572" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在上面的代码中，我们选择了四个主题进行建模，因为我们已经预先知道我们正在处理的数据中有多少主题。然而，在大多数情况下，我们没有这样的知识。随机猜测不是最理想的方法。有没有一种编程的方式来做这件事？是啊！</p><p id="bb3b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了自动确定主题的最佳数量，我们使用与前面介绍的两个算法 LDA 和 GSDMM 相同的方法。我们使用 gensim 的一致性分数来做到这一点。不幸的是，我们无法将 sklearn 的 NMF 实现与 gensim 的 coherence score 模型结合使用，因此，我们需要依赖 gensim 的 NMF 实现来实现这一目的。本文<a class="ae ky" rel="noopener" target="_blank" href="/topic-modeling-articles-with-nmf-8c6b2a227a45">的后半部分</a>使用 NMF 的 gensim 实现来确定一致性分数的最佳数量，然后使用 NMF 的 sklearn 实现来进行实际的训练和主题提取。</p><h2 id="4c26" class="nx la it bd lb ny nz dn lf oa ob dp lj ma oc od ll me oe of ln mi og oh lp oi bi translated">复习题目质量</h2><p id="f746" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于 NMF 是一个线性代数算法，我们有一个很好的方法来评估每个主题的主题质量，通过查看每个主题组中文档的平均残差。</p><p id="9f6a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">假设我们使用 Frobenius 范数作为最小化的距离度量，我们可以如下计算每个文档的残差。</p><pre class="kj kk kl km gt nm nn no bn np nq bi"><span id="b10f" class="nr la it nn b be ns nt l nu nv">A = vect.transform(df.text) # Original Matrix (after tf-idf vectorization is applied)<br/>W = model.components_ # Component matrix<br/>H = model.transform(A) # Features matrix<br/><br/># Get the residuals for each document<br/>r = np.zeros(A.shape[0])<br/>for row in range(A.shape[0]):<br/>    # 'fro' here means we are using the Frobenium norm as the distance metric<br/>    r[row] = np.linalg.norm(A[row, :] - H[row, :].dot(W), 'fro') </span></pre><p id="8c65" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们计算每个主题组中这些残差的平均值，看看哪个主题具有最低的平均残差将是质量最好的主题！看看这篇<a class="ae ky" rel="noopener" target="_blank" href="/topic-modeling-articles-with-nmf-8c6b2a227a45">文章</a>，了解更多关于这个过程的信息。</p><h1 id="88ff" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">奖金</h1><p id="2b1f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如我前面提到的，gensim 也有自己的 NMF 实现。虽然基础数学和原理与 sklearn 实现相同，但它可以与 gensim 中的 coherence 模型联合使用，以确定要建模的主题的最佳数量，而 sklearn 目前没有提供任何 coherence score 模型。要了解摩尔关于 gensim 的 NMF 实现，请参考此<a class="ae ky" href="https://radimrehurek.com/gensim/models/nmf.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="3402" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">迷你批处理 NMF 是一个 NMF 版本，是专为更大的数据集。它有一个 partial_fit 方法，在数据集的不同块上连续调用多次，以便实现小批量的核外或在线学习。要了解有关该算法的更多信息，请参考其<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchNMF.html#sklearn.decomposition.MiniBatchNMF" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="7cb8" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="c16b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我向您介绍了 NMF 算法，它是线性代数算法家族的一部分，不仅用于主题建模，还用于从推荐系统到维度缩减的广泛应用。只要用户对线性代数有所了解，NMF 的优势就在于其直观和简单的数学。很难判断哪种主题建模算法最适合您正在处理的特定项目和数据集，因此我建议使用几种最流行的主题建模算法，并在进行任何后续步骤之前目测每个主题中的关键词。</p><p id="c794" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你觉得这篇文章有帮助，请考虑通过以下链接注册 medium 来支持我: )</p><p id="c9ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">joshnjuny.medium.com<a class="ae ky" href="https://joshnjuny.medium.com/membership" rel="noopener"/></p><p id="c778" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你不仅可以看到我，还可以看到其他作者写的这么多有用和有趣的文章和帖子！</p><h1 id="2ffd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">关于作者</h1><p id="85a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="ok">数据科学家。加州大学欧文分校信息学专业一年级博士生。主要研究兴趣是将 SOTA ML/DL/NLP 方法应用于健康和医疗相关的大数据，以提取有趣的见解，为患者、医生和决策者提供信息。</em></p><p id="4509" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ok">密歇根大学刑事司法行政记录系统(CJARS)经济学实验室的前研究领域专家，致力于统计报告生成、自动化数据质量审查、构建数据管道和数据标准化&amp;协调。Spotify 前数据科学实习生。Inc .(纽约市)。</em></p><p id="bc51" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他喜欢运动、健身、烹饪美味的亚洲食物、看 kdramas 和制作/表演音乐，最重要的是崇拜我们的主耶稣基督。结账他的 <a class="ae ky" href="http://seungjun-data-science.github.io" rel="noopener ugc nofollow" target="_blank"> <em class="ok">网站</em> </a> <em class="ok">！</em></p></div></div>    
</body>
</html>