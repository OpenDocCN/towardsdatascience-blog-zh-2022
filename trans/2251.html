<html>
<head>
<title>Word2Vec with Time Series: A Transfer Learning Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带时间序列的Word2Vec:一种迁移学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word2vec-with-time-series-a-transfer-learning-approach-58017e7a019d#2022-05-18">https://towardsdatascience.com/word2vec-with-time-series-a-transfer-learning-approach-58017e7a019d#2022-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ddeb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习时间序列的有意义的嵌入表示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/de2c60205bc24225f68c8d0cd9fe0b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Sc5EtIIem6cbtZ6c"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@thezenoeffect?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">希亚姆</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="97fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向量表示是机器学习生态系统中的一个关键概念。无论我们面临什么样的任务，我们总是试图赋予我们所掌握的数据以意义。<strong class="lb iu">我们习惯于采用技术来提供数据的数字表示，以发现隐藏的行为并产生有价值的见解</strong>。</p><p id="d9c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随着深度学习的兴起，以较少的假设和较少的努力获得有意义的数据表示正在成为现实</strong>。我们来想想TF-IDF(词频-逆文档频)。这是自然语言处理中采用的一种技术，用于将一组原始文本文档转换成一个数字矩阵。TF-IDF在很长一段时间内占据了主导地位，代表了一种编码文本序列的好方法。深度学习的出现拯救了新技术，首先是Word2Vec，然后是transformer编码。它们是现成的解决方案，并且在提供文本数据的数字数据表示方面非常有效，而不需要(在大多数情况下)理解上下文。这使得它们比老式的TF-IDF更受欢迎。</p><p id="48f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在NLP领域采用深度学习嵌入表示是革命性的。<strong class="lb iu">通常将术语<em class="lv">“嵌入表示”</em>与涉及文本数据的应用</strong>联系在一起。这是因为很容易概括文本内容中单词的位置依赖性。如果我们认为还存在其他表现出位置依赖性的数据源，<strong class="lb iu">为什么不在其他领域推广文本嵌入技术呢？</strong></p><p id="1d43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个引人入胜的想法可能是<strong class="lb iu">将NLP中获得的成果转化到时间序列领域</strong>。乍一看，这可能是一个完美的拟合，因为时间序列数据也是由位置/时间关系来表征的。主要的好处应该在于提供学习技术，这些技术可以产生遵循潜在的时间依赖性的数据的有价值的向量表示。这不是我们第一次对时间序列数据的嵌入感兴趣。我们发现了<a class="ae ky" href="https://medium.com/towards-data-science/time2vec-for-time-series-features-encoding-a03a4f3f937e" rel="noopener"> <strong class="lb iu"> Time2Vec </strong> </a> <strong class="lb iu"> </strong>作为时间的模型不可知表示的效用，可以在任何深度学习预测应用中使用。我们还提出了<a class="ae ky" rel="noopener" target="_blank" href="/corr2vec-a-wavenet-architecture-for-feature-engineering-in-financial-market-94b4f8279ba6"> <strong class="lb iu"> Corr2Vec </strong> </a>通过研究多个时间序列的相互相关性来提取它们的嵌入表示。</p><p id="a757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，<strong class="lb iu">我们尝试在时序域</strong>中应用Word2Vec。我们的目标是利用无监督方法的灵活性，如Word2Vec，来学习时间序列的有意义的嵌入。生成的嵌入应该能够捕获底层系统行为，以便在其他上下文中也可以重用。</p><h1 id="c681" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据</h1><p id="40bf" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们从evergreen UCI知识库中收集了一些开源数据(参见这里的<a class="ae ky" href="https://networkdata.ics.uci.edu/about.php" rel="noopener ugc nofollow" target="_blank">UCI许可政策的</a>)。<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Parking+Birmingham" rel="noopener ugc nofollow" target="_blank">停车伯明翰数据集</a>包含2016/10/04至2016/12/19期间每小时8:00-16:30的停车占用率。它非常适合我们的目的，因为它记录了来自不同位置的数据，使我们能够在多变量的情况下进行切换。</p><p id="bffe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们掌握了原始占用率(即当时停车场有多少辆车)和最大停车容量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/f73724ddac4c6cc33a1897ae42d31ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5I4sq3Al7k9cTL-E.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有停车区域的占用时间序列(图片由作者提供)</p></figure><p id="b765" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据显示了缺失观测值的存在，但也显示了一些有规律的季节性模式。我们观察每天和每周的行为。所有的停车区往往在下午达到最大占用率。其中一些人在工作日使用最多，而其他人在周末更忙。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/09fc4ee96cf385ed91fc01d38fd1fe94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lVSFM_u5s1F6wLL2.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有停车区域的占用率小时模式(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/2caa3c2c4baa607d1f7c3257868cc4f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2xNWs34CPilwdju0.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有停车区的占用率每日模式(图片由作者提供)</p></figure><h1 id="c8bb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">建模</h1><p id="9608" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如何将Word2Vec应用于时序数据？当将Word2Vec应用于文本时，我们应该首先用一个整数映射每个单词。这些数字代表单词在整个文本语料库中的唯一标识符。它们需要允许唯一的可训练嵌入的关联。对于时间序列，我们也应该这样做。整数标识符由宁滨连续时间序列生成为区间。在每个时间间隔，我们关联一个唯一的标识符，它指的是一个可学习的嵌入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/11faa3fafc8c8ad32d0afce6b70f616a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpZDzcx9knDL8HaprpzbHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">时间序列离散化示例(图片由作者提供)</p></figure><p id="f412" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在将时间序列离散化之前，我们应该考虑对它们进行缩放。在多元环境中工作时，这一点尤为重要。我们需要以统一的方式应用离散化来实现单义整数映射。考虑到我们的停车数据，我们使用占用率系列(在0-100范围内标准化)来避免误导学习行为。</p><p id="41cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Word2Vec架构与NLP应用程序中的架构保持一致。有不同的现成解决方案。我们选择手工Tensorflow实现:</p><pre class="kj kk kl km gt mw mx my mz aw na bi"><span id="9cc5" class="nb lx it mx b gy nc nd l ne nf">input_target = Input((1,))<br/>input_context = Input((1,))<br/>embedding = Embedding(n_bins+1, 32)<br/>dot = Dot(axes=1, normalize=True)([<br/>     Flatten()(embedding(input_target)), <br/>     Flatten()(embedding(input_context))<br/>])<br/>model = Model([input_target, input_context], dot)<br/>model.compile(<br/>     optimizer=Adam(learning_rate=1e-5), <br/>     loss=BinaryCrossentropy(from_logits=True)<br/>)</span></pre><p id="c094" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练数据和相关标签由跳格法生成。它根据用户定义的窗口大小生成整数对。同一窗口中的整数对的标签等于1。随机生成的配对被标记为0。</p><p id="d735" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过检查学习到的嵌入，我们了解到网络可以自动识别我们数据的循环性质。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/078f7ef7bf09d1ab217b6880df88f0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/0*C8HaTZaEJFbl1fM_.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个装箱的时间序列序列的2D嵌入可视化(图片由作者提供)</p></figure><p id="96b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">扩展所有时间序列的嵌入表示，我们注意到每小时和每天的观察之间的明显分离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/a3756fba8941d1c28c6957163747c6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mZ-wya4EEhNvcR8X.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个时间序列中所有观测值的2D嵌入可视化(图片由作者提供)</p></figure><p id="5efe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些可视化证明了我们方法的优点。假设条件低，需要设置的参数少，就可以生成有意义的时间序列嵌入。</p><h1 id="5b25" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="933f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我们提出了著名的Word2Vec算法的推广，用于学习有价值的向量表示。没有太多的努力，我们采用了在时序上下文中应用Word2Vec所需的所有考虑因素。最后，我们展示了这种技术在一个非标准的NLP应用程序中的有效性。整个过程可以很容易地集成在任何地方，并很容易地用于迁移学习任务。</p><h2 id="12a9" class="nb lx it bd ly ni nj dn mc nk nl dp mg li nm nn mi lm no np mk lq nq nr mm ns bi translated">引文</h2><ul class=""><li id="265d" class="nt nu it lb b lc mo lf mp li nv lm nw lq nx lu ny nz oa ob bi translated">Dua d .和Graff c .(2019年)。<a class="ae ky" href="http://archive.ics.uci.edu/ml" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库</a>。加州欧文:加州大学信息与计算机科学学院。</li><li id="6323" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">丹尼尔·h·斯托尔菲、恩里克·阿尔巴和姚。智能城市停车场占用率预测。in:2017年6月14日至16日在西班牙马拉加举行的Smart-CT 2017第二届国际会议。</li><li id="ceef" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">伯明翰市议会。</li></ul></div><div class="ab cl oh oi hx oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="im in io ip iq"><p id="0765" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="bdd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>