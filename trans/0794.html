<html>
<head>
<title>Understanding the Outputs of Multi-Layer Bi-Directional LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解多层双向 LSTMs 的输出</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-outputs-of-multi-layer-bi-directional-lstms-13ad99a80dd3#2022-03-05">https://towardsdatascience.com/understanding-the-outputs-of-multi-layer-bi-directional-lstms-13ad99a80dd3#2022-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="46aa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这个简短的教程中，我分解了多层双向 LSTMs 的输出，并举例说明了如何在 PyTorch 中实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5bb236c998626a9bed057c865f38748b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yOYblPi7r3idr7ENfHW8kw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有时候，从两方面来看都是有帮助的(由<a class="ae ky" href="https://unsplash.com/s/photos/two-way?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@garciasaldana_?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Pablo garcía saldaa</a>拍摄)。</p></figure><p id="97e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习的世界里，<a class="ae ky" href="https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/" rel="noopener ugc nofollow" target="_blank">长短期记忆网络(lstm)</a>是处理数据序列的强大工具，比如<strong class="lb iu">语音、文本和视频</strong>。LSTMs 的循环性质使它们能够<strong class="lb iu">记住它们在序列中较早看到的数据片段</strong>。从概念上讲，这在向前的方向(即开始到结束)上更容易理解，但它对<strong class="lb iu">考虑相反方向的顺序</strong>(即结束到开始)也是有用的。例如，考虑这个句子中填空的任务:</p><p id="c35e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乔喜欢空白的，尤其是油炸、炒制或水煮的。</p><p id="c7ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在向前的方向上，在到达丢失的单词之前唯一可用的信息是“Joe 喜欢<blank>”，这可能有许多种可能性。然而，如果 i <strong class="lb iu">信息也被允许向后传递</strong>，那么从“油炸、炒制或水煮”的上下文中预测单词“鸡蛋”就容易得多。因此，让<strong class="lb iu">信息双向流动会很有用</strong>。这可以通过使用双向 LSTM 来捕获。</blank></p><p id="7930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章并不是双向 LSTMs 的完整指南；已经有<a class="ae ky" href="https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/#:~:text=Bidirectional%20long%2Dshort%20term%20memory,forward(past%20to%20future)" rel="noopener ugc nofollow" target="_blank">其他关于这个的大文章</a>了。然而，我最近在<strong class="lb iu">与多层双向 lstm</strong>一起工作，我很难理解他们在 PyTorch 中产生的输出。我真的不能在网上找到一个好的指南，特别是对于多层 LSTMs，所以一旦我解决了它，我决定把这个小教程放在一起。所以，<strong class="lb iu">话不多说</strong>，下面是我理解多层双向 LSTMs 输出的指南。</p><h1 id="be17" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">高层次概述</h1><p id="10d9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了理解双向 LSTM 的输出是什么，我们首先需要知道在 LSTM 的引擎盖下发生了什么。LSTM 网络由 LSTM 单元(也称为单元或模块)组成。为了简单起见，我将把 LSTM 细胞视为独立完整的计算单位，而不去探究它们到底是做什么的。为了这项工作的目的，我们只说一个 LSTM 单元接受两个输入:<strong class="lb iu">一个来自数据或另一个 LSTM 单元的“真”输入</strong>，和<strong class="lb iu">一个来自前一时间步(或初始隐藏状态)的“隐藏”输入</strong>。实际上，还有第三种输入(单元格状态)，但是为了概念上的简单，我将它作为“隐藏”状态的一部分。给定这些输入，LSTM 单元产生两个输出:<strong class="lb iu">一个“真”输出</strong>和<strong class="lb iu">一个新的隐藏状态</strong>。我们可以这样表述:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/efdd2cfe023980093692382c78eaba47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GidaFbDVMFEXaaXQ5ANq1g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">LSTM 细胞/模块/单位的结构。作者绘制的图像。</p></figure><p id="3466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“真实”和隐藏输入和输出之间的区别在于，<strong class="lb iu">隐藏输出在序列</strong>的方向上移动(即，向前或向后)，而<strong class="lb iu">真实输出被更深入地传递到网络</strong>(即，通过层)。在单层 LSTM 中，真实输出仅形成网络的输出，但是在多层 lstm 中，它们也用作新层的输入。例如，在双层 LSTM 中，第一层的真实输出被传递到第二层，第二层的真实输出形成网络的输出。除了真实输出，我们还得到每一层的最终隐藏状态输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/3757b166cc82dadf3cef3dd71d831080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdOezNBP5PgpKoJr9osDzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双层(单向)LSTM 处理的一个展开的概念性示例。作者绘制的图像。</p></figure><p id="d6c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于双向 LSTM，我们可以将网络的反向部分视为网络正向部分的<strong class="lb iu">镜像</strong>，即隐藏状态沿与相反的<strong class="lb iu">方向流动(从右到左，而不是从左到右)，但真实状态沿<strong class="lb iu">相同的</strong>方向流动(通过网络更深)。网络的两个方向完全独立地工作，直到最后一层，此时它们的输出被连接起来。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/32618f743e10e4d4fec84f37f4455464.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*feJkGdbsdDPrxj-wqxJ1hw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双层双向 LSTM 处理的一个展开的概念性示例。作者绘制的图像。</p></figure><p id="378e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了双向 LSTM，<strong class="lb iu">最终输出现在是向前和向后方向的串联</strong>。这就是事情变得有点复杂的地方，因为两个方向会看到每个输出的不同输入。例如，对于第一个输出(图中的 o1)，向前方向只看到了第一个令牌，但是向后方向看到了所有三个令牌。相反，对于最后一个令牌(图中的 o3)，正向已经看到了所有三个令牌，而反向只看到了最后一个令牌。我们真正想要的输出是网络的前一半已经看到了每个令牌，<strong class="lb iu"> <em class="mu">和</em> </strong>，而网络的后一半也看到了每个令牌，<strong class="lb iu">，这不是我们实际得到的输出之一！</strong>因此，我们不得不对输出进行一些调整，稍后当我们查看处理输出的实际代码实现时，我会谈到这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/055c4b6c8fe0caeb0d482c1417df61d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4S5YOMiq2nrZiCJiryd6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双向 LSTM 的“真实”输出。作者绘制的图像。</p></figure><p id="9854" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于隐藏的输出，LSTM 的双向特性也让事情变得有些混乱。隐藏的状态现在是交替的，而不是连接在一起。同样，我们将不得不争论我们被给予的输出来清理它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/bd3a73e452b1d209a63297afd3fb07f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOnw-rVZ_ZH7IU2TR75iPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">双向 LSTM 的隐藏输出。作者绘制的图像。</p></figure><h1 id="dac2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">在 PyTorch 中处理输出</h1><p id="64f2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">记住以上内容，现在让我们来看看<strong class="lb iu">在 PyTorch </strong>中是如何工作的。我将代码作为(某种程度上)独立的 Python 笔记本嵌入如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="b229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是多层双向 LSTMs 输出的快速概览。<strong class="lb iu">如果你有任何问题，请在评论中提问！</strong></p></div></div>    
</body>
</html>