<html>
<head>
<title>Generating New Carnatic Music Patterns Using LSTM Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用LSTM神经网络生成新的卡尔纳蒂克音乐模式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-new-carnatic-music-patterns-using-lstm-neural-networks-e71de1dce019#2022-08-24">https://towardsdatascience.com/generating-new-carnatic-music-patterns-using-lstm-neural-networks-e71de1dce019#2022-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="18ec" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">卡纳蒂克音乐以一些积木的丰富循环模式为特色——使其成为探索基于人工智能的音乐生成的丰富游乐场。让我们看看LSTM是如何创作新的基本作品的</h2></div><h1 id="dc67" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">卡尔纳蒂克音乐中的积木</h1><p id="680c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">卡纳蒂克音乐是两种印度古典音乐形式之一(另一种是印度斯坦音乐)。与西方古典音乐不同，西方古典音乐中的作品被设定为一个特定的调(例如贝多芬的C小调第五交响曲)并且以多种调式为特征；卡尔纳蒂克的音乐作品大多被设定为一种独特的调式，称为Raga。例如，Raga Sankarabharanam对应于爱奥尼亚调式(通常称为大调音阶)。</p><p id="8997" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">由于这种对特定模式的关注，卡尔纳蒂克音乐以复杂的循环模式为特色，这通常可以被认为是建立在基本模式的基础上，几乎就像是重复片段的等价物。事实上，在卡纳蒂克音乐中，人们最先学到的东西之一就是萨拉里·瓦拉赛(Sarali varas ais)，这是对基本的完整拉格(raga)的修改:</p><blockquote class="mc"><p id="ebec" class="md me it bd mf mg mh mi mj mk ml lv dk translated">T2 [] S n d p | m g | r S</p></blockquote><p id="0a04" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">其中“s r g m p d n S”是组成完整音阶的8个音符。例如，如果我们谈论c大调音阶中的三卡拉巴拉那，对应的将是“C D E F G A B C”。“|”和“||”表示节拍小节。“||”代表一个周期的结束(这里是8拍)，“|”代表一个特征音程。这个简单的上升和下降模式是第一个Sarali Varisai，还有13个变化如下图所示:</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="3d96" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Sarali Varisai的另一个例子是:</p><blockquote class="mc"><p id="d559" class="md me it bd mf mg mh mi mj mk ml lv dk translated">S r g m | p d | S r | |<br/>S r g m | p d | n S | |<br/>S n d p | m g | S n | |<br/>S n d p | m g | r S | |</p></blockquote><p id="6793" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">本文的目标是训练一个人工智能模型，从13个Sarali vari sai中生成一个新的Sarali Varasai，这些Sarali vari sai的长度都是44个字符(不包括第一个字符)。</p><h1 id="0189" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">LSTM神经网络</h1><p id="34b7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">长短期记忆(LSTM)神经网络被大量用于模拟循环模式和时间相关现象。LSTM旨在提供长期和短期记忆来模拟序列模式。据报道，LSTM架构是20世纪被引用最多的神经网络<a class="ae mb" href="https://people.idsia.ch/~juergen/most-cited-neural-nets.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c022" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">传统的神经网络在给定某个输入向量的情况下预测输出。例如，MNIST数据集的特征是从0到9的数字化手写数字。人们可以开发一个基本的神经网络，以相当不错的精度来预测给定手写数字的数字，而不是在训练模型时看到的数字。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/1a922aa708fd7931be60d1ca82397670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwBdnT5Q2wvwt5_kh51QWw.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated"><a class="ae mb" href="https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab2/Part1_MNIST.ipynb#scrollTo=gKA_J7bdP33T" rel="noopener ugc nofollow" target="_blank">修改自MIT 6。S191:深度学习介绍</a></p></figure><p id="8339" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然而，这些神经网络没有对信息的持久性进行建模——这是一个缺点。例如，如果您想预测未来的一些事情，比如一天后的股票市场，如果您仅基于当前特征(相当于一幅图像中的所有像素)进行预测，您将会丢失有价值的信息。天气预报也是如此，甚至只是知道电影中接下来会发生什么。接下来的几分钟不仅仅取决于电影的当前位置，还取决于角色的历史。</p><p id="a3a8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这种持久性可以通过递归神经网络(RNNs)来建模。在一个简单的RNN中，有神经网络来模拟每一个时间步，其中2个输入是系统的先前状态；和一个考虑历史的向量。在下图中，您可以看到，在每一步t，RNN都接受一个输入x_t，以及一个从过去的历史中累积的输入v。这种模型的一个例子可以是例如一个句子的语言翻译——从英语到法语。在这种情况下，RNN模型的想法是，不仅当前单词在建模相应的法语单词时有用，而且句子中前面单词的历史也有用。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nj"><img src="../Images/b58ef17b9e68e7eb48ce440b76346f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4rOu4ZEtKULzsj6O.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated"><a class="ae mb" href="https://en.wikipedia.org/wiki/Recurrent_neural_network#/media/File:Recurrent_neural_network_unfold.svg" rel="noopener ugc nofollow" target="_blank"> RNN展开</a></p></figure><p id="24f9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">rnn的一个主要问题是它们不能处理大序列，这是由于在多次迭代中混合了非常小或非常大的数。LSTMs通过为每个单元设置一个单独的“门”来克服这种限制，该门可以选择性地让信息通过。这样，他们可以通过忘记中间的一些无关紧要的连接，但同时跟踪一些重要的连接，来处理更长范围的依赖性。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nk"><img src="../Images/5679d2eb0d7bab2033bc4e0d1a92017d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6IR1fQrfZYu26sGi1vZSg.png"/></div></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">用基本的LSTM细胞来模拟依赖于过去历史的循环模式|塞犍陀·维维克</p></figure><h1 id="57a3" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">为卡纳蒂克·萨拉里·瓦拉塞一代建立一个LSTM模式</h1><p id="4a1d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，我定义了与14个不同的Sarali Varasais相对应的14个字符串，并创建了一个连接除第一个以外的所有字符串的列表(第一个字符串比其余的都小)。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="39e5" class="nq kj it nm b gy nr ns l nt nu">x1=”s r g m | p d | n S || S n d p | m g | r s ||”<br/>x2=”s r s r | s r | g m || s r g m | p d | n S || S n S n | S n | d p || S n d p | m g | r s ||”<br/>x3=”s r g s | r g | s r || s r g m | p d | n S || S n d s | n d | s n || S n d p | m g | r s ||”<br/>x4=”s r g m | s r | g m || s r g m | p d | n s || S n d p | S n | d p || S n d p | m g | r s ||”<br/>x5=”s r g m | p , | s r || s r g m | p d | n S || S n d p | m , | S n || S n d p | m g | r s ||”<br/>x6=”s r g m | p d | s r || s r g m | p d | n S || S n d p | m g | S n || S n d p | m g | r s ||”<br/>x7=”s r g m | p d | n , || s r g m | p d | n S || S n d p | m g | r , || S n d p | m g | r s ||”<br/>x8=”s r g m | p m | g r || s r g m | p d | n S || S n d p | m p | d n || S n d p | m g | r s ||”<br/>x9=”s r g m | p m | d p || s r g m | p d | n S || S n d p | m p | g m || S n d p | m g | r s ||”</span><span id="e4a8" class="nq kj it nm b gy nv ns l nt nu">x10=”s r g m | p , | g m || p , , , | p , | , , || g m p d | n d | p m || g m p g | m g | r s ||”<br/>x11=”S , n d | n , | d p || d , p m | p , | p , || g m p d | n d | p m || g m p g | m g | r s ||”<br/>x12=”S S n d | n n | d p || d d p m | p , | p , || g m p d | n d | p m || g m p g | m g | r s ||”<br/>x13=”s r g r | g , | g m || p m p , | d p | d , || m p d p | d n | d p || m p d p | m g | r s ||”<br/>x14=”s r g m | p , | p , || d d p , | m m | p , || d n S , | S n | d p || S n d p | m g | r s ||”</span><span id="d954" class="nq kj it nm b gy nv ns l nt nu">l1=[]<br/>l1.append([i for i in x2.split(" ") if i])<br/>l1.append([i for i in x3.split(" ") if i])<br/>l1.append([i for i in x4.split(" ") if i])<br/>l1.append([i for i in x5.split(" ") if i])<br/>l1.append([i for i in x6.split(" ") if i])<br/>l1.append([i for i in x7.split(" ") if i])<br/>l1.append([i for i in x8.split(" ") if i])<br/>l1.append([i for i in x9.split(" ") if i])<br/>l1.append([i for i in x10.split(" ") if i])<br/>l1.append([i for i in x11.split(" ") if i])<br/>l1.append([i for i in x12.split(" ") if i])<br/>l1.append([i for i in x13.split(" ") if i])<br/>l1.append([i for i in x14.split(" ") if i])</span></pre><p id="cae2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有11个不同的音符(' S '，' r '，' g '，' m '，' p '，' d '，' n '，' S '，' | '，'，' )，其中' S '表示第一个音符的高音版本，' | '和' || '是特征音程，'，'只是一个延长音符的间隔。使用这个信息，我开发了一个热编码，其中X的维数是X= (13，44，11)，对应于13个萨拉里变奏，44个音符组成每个萨拉里变奏(包括音程)，11对应于一个热编码维数。</p><p id="9710" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">y是向左移动一步的X；即时间步长t处的Y与时间步长t+1处的X相同。因此，游戏现在将预测给定前一个音符的下一个音符(LSTM也考虑音符的历史)。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="8c55" class="nq kj it nm b gy nr ns l nt nu">for i in range(0,len(l1)):<br/> values = array(l1[i])<br/> onehot_encoded = onehot_encoder.transform(values.reshape(len(values), 1))<br/> X[i,:]=onehot_encoded<br/> <br/> values=np.roll(values,-1)<br/> onehot_encoded = onehot_encoder.transform(values.reshape(len(values), 1))<br/> Y[:,i,:]=onehot_encoded</span></pre><p id="8d4f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">接下来，我们的想法是训练一个LSTM模型，根据之前的值(X)来预测后续的音符(Y)。</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="5666" class="nq kj it nm b gy nr ns l nt nu">n_values = 11 # number of music values<br/>reshaper = Reshape((1, n_values)) <br/>LSTM_cell = LSTM(n_a, return_state = True) <br/>densor = Dense(n_values, activation=’softmax’)</span><span id="4748" class="nq kj it nm b gy nv ns l nt nu">def djmodel(Tx, LSTM_cell, densor, reshaper):<br/>    """<br/>    Implement the djmodel composed of Tx LSTM cells where each cell is responsible<br/>    for learning the following note based on the previous note and context.<br/>    Each cell has the following schema: <br/>            [X_{t}, a_{t-1}, c0_{t-1}] -&gt; RESHAPE() -&gt; LSTM() -&gt; DENSE()<br/>    Arguments:<br/>        Tx -- length of the sequences in the corpus<br/>        LSTM_cell -- LSTM layer instance<br/>        densor -- Dense layer instance<br/>        reshaper -- Reshape layer instance<br/>    <br/>    Returns:<br/>        model -- a keras instance model with inputs [X, a0, c0]<br/>    """<br/>    # Get the shape of input values<br/>    n_values = densor.units<br/>    <br/>    # Get the number of the hidden state vector<br/>    n_a = LSTM_cell.units<br/>    <br/>    # Define the input layer and specify the shape<br/>    X = Input(shape=(Tx, n_values)) <br/>    <br/>    # Define the initial hidden state a0 and initial cell state c0<br/>    # using `Input`<br/>    a0 = Input(shape=(n_a,), name='a0')<br/>    c0 = Input(shape=(n_a,), name='c0')<br/>    a = a0<br/>    c = c0<br/><br/>    outputs = []<br/>    <br/>    # Step 2: Loop over tx<br/>    for t in range(Tx):<br/>        <br/>        # Step 2.A: select the "t"th time step vector from X. <br/>        x = X[:,t,:]<br/>        # Step 2.B: Use reshaper to reshape x to be (1, n_values) (≈1 line)<br/>        x = reshaper(x)<br/>        #print(x.shape)<br/>        # Step 2.C: Perform one step of the LSTM_cell<br/>        a, _, c = LSTM_cell(inputs=x, initial_state=[a,c])<br/>        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell<br/>        out = densor(a)<br/>        # Step 2.E: add the output to "outputs"<br/>        outputs=outputs+[out]<br/>        <br/>    # Step 3: Create model instance<br/>    model = Model(inputs=[X, a0, c0], outputs=outputs)<br/>    <br/>   <br/>    return model</span><span id="40e3" class="nq kj it nm b gy nv ns l nt nu">model = djmodel(Tx=44, LSTM_cell=LSTM_cell, densor=densor, reshaper=reshaper)</span><span id="09b1" class="nq kj it nm b gy nv ns l nt nu">history = model.fit([X, a0, c0], list(Y), epochs=200, verbose = 1)</span></pre><p id="7ec5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如你所见，对于200个训练时期，损失下降了——这意味着该模型在Saralivarisai内更好地理解(并可能复制)模式。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b5bc6f123902135b029546090f677f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*sHSVM-3k6fKrFL1ep4hDeQ.png"/></div><p class="nf ng gj gh gi nh ni bd b be z dk translated">在卡纳蒂克音乐序列上训练LSTM |塞犍陀·维维克</p></figure><p id="ebf8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">经过训练后，LSTM细胞可以用来产生音乐:</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="2dde" class="nq kj it nm b gy nr ns l nt nu">def music_inference_model(LSTM_cell, densor, Ty=44):<br/>    """<br/>    Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values.<br/>    <br/>    Arguments:<br/>    LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object<br/>    densor -- the trained "densor" from model(), Keras layer object<br/>    Ty -- integer, number of time steps to generate<br/>    <br/>    Returns:<br/>    inference_model -- Keras model instance<br/>    """<br/>    <br/>    # Get the shape of input values<br/>    n_values = densor.units<br/>    # Get the number of the hidden state vector<br/>    n_a = LSTM_cell.units<br/>    <br/>    # Define the input of your model with a shape <br/>    x0 = Input(shape=(1, n_values))<br/>    <br/>    <br/>    # Define s0, initial hidden state for the decoder LSTM<br/>    a0 = Input(shape=(n_a,), name='a0')<br/>    c0 = Input(shape=(n_a,), name='c0')<br/>    a = a0<br/>    c = c0<br/>    x = x0</span><span id="d39a" class="nq kj it nm b gy nv ns l nt nu"><br/>    outputs = []<br/>    <br/>    # Step 2: Loop over Ty and generate a value at every time step<br/>    for t in range(Ty):<br/>        # Step 2.A: Perform one step of LSTM_cell. Use "x", not "x0" (≈1 line)<br/>        a, _, c = LSTM_cell(x, initial_state=[a,c])<br/>        <br/>        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)<br/>        out = densor(a)<br/>        # Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 90) (≈1 line)<br/>        outputs.append(out)<br/>        #print(out.shape)<br/> <br/>        # Step 2.D: <br/>        # Select the next value according to "out",<br/>        # Set "x" to be the one-hot representation of the selected value<br/>        # See instructions above.<br/>        x = tf.math.argmax(out,axis=-1)<br/>        #print(x.shape)<br/>        x = tf.one_hot(x,n_values)<br/>        # Step 2.E: <br/>        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, 90)<br/>        #print(x.shape)<br/>        x = RepeatVector(1)(x)<br/>        <br/>    # Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)<br/>    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)<br/>    <br/>    <br/>    return inference_model</span><span id="5a16" class="nq kj it nm b gy nv ns l nt nu">inference_model = music_inference_model(LSTM_cell, densor, Ty = 44)</span></pre><p id="19a3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然后我们可以给推理模型随机初始化的输入:</p><pre class="mr ms mt mu gt nl nm nn no aw np bi"><span id="9b47" class="nq kj it nm b gy nr ns l nt nu">def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, <br/>                       c_initializer = c_initializer):<br/>    """<br/>    Predicts the next value of values using the inference model.<br/>    <br/>    Arguments:<br/>    inference_model -- Keras model instance for inference time<br/>    x_initializer -- numpy array of shape (1, 1, 90), one-hot vector initializing the values generation<br/>    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell<br/>    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel<br/>    <br/>    Returns:<br/>    results -- numpy-array of shape (Ty, 90), matrix of one-hot vectors representing the values generated<br/>    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated<br/>    """<br/>    <br/>    n_values = x_initializer.shape[2]<br/>    <br/> given x_initializer, a_initializer and c_initializer.<br/>    pred = inference_model.predict([x_initializer,a_initializer,c_initializer])<br/>    # Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities<br/>    indices = np.argmax(pred,axis=-1)<br/>    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)<br/>    results = to_categorical(indices, num_classes=n_values)<br/>    <br/>    return results, indices</span><span id="2693" class="nq kj it nm b gy nv ns l nt nu">x_initializer = np.random.rand(1, 1, n_values)<br/>a_initializer = np.random.rand(1, n_a)<br/>c_initializer = np.random.rand(1, n_a)</span><span id="98a8" class="nq kj it nm b gy nv ns l nt nu">results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)</span><span id="081f" class="nq kj it nm b gy nv ns l nt nu">notes=[]<br/>for i in range(0,len(results[:,0])):<br/>    inverted = label_encoder.inverse_transform([argmax(results[i, :])])[0]<br/>    notes.append(inverted)</span><span id="40a6" class="nq kj it nm b gy nv ns l nt nu">print(' '.join(e for e in notes))</span></pre><p id="04c1" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">这是LSTM预测的:</strong></p><blockquote class="mc"><p id="f763" class="md me it bd mf mg mh mi mj mk ml lv dk translated">d | n S | d p || d d p m | p，| p，|| g m p d | n d | p m || g m p g | m g | r s || S n d</p></blockquote><p id="c827" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">有趣的是，它很好地生成了一个结构良好的2行组合，尽管是在一个周期的中间开始的。这可能是因为我们如何通过输入前一个音符来训练LSTM细胞——这有效地使用了周期性边界，并使其成为一个循环模型。</p><p id="15af" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">通过将“n d”放在前面，我们得到一个新的Sarali Varisai，如下所示:</p><blockquote class="mc"><p id="80cb" class="md me it bd mf mg mh mi mj mk ml lv dk translated">S n d d | n S | d p || <br/> d d p m | p，| p，| |<br/>g m p d | n d | p m | |<br/>g m p g | m g | r S | |</p></blockquote><p id="abd4" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">这是我唱这首歌的录音:</p><figure class="mr ms mt mu gt mv"><div class="bz fp l di"><div class="nx mx l"/></div></figure><h1 id="6194" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="146a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在本文中，我将展示如何使用LSTM——一种经典的递归神经网络架构来生成基本的Carnatic Saralivarse模式。由于这些基本模式构成了更复杂的卡尔纳蒂克音乐作品的基础，使用先进的人工智能算法来生成更复杂的作品可能会令人兴奋。</p><p id="5d0b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">LSTMs仍然受到递归神经网络固有的顺序性质的困扰——这也使得它们很难并行进行计算。最近，变形金刚席卷了自然语言处理领域。就像在阅读或翻译一个句子时，人们需要注意句子的各个部分一样，变形金刚被设计成使用自我注意来消除重复连接，并提供同时关注多个单词的能力。同样，如果有足够多的数据，变形金刚也可以用来理解和创作新的音乐。</p><p id="7163" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">如果你在音乐和AI的交汇点有其他想法，欢迎联系！</p><p id="9195" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">您可以在这个GitHub repo中找到代码:</p><div class="ny nz gp gr oa ob"><a href="https://github.com/skandavivek/LSTM-Carnatic-Music-Generation" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">GitHub-skandavivek/LSTM-卡纳蒂克-音乐-生成:生成新的卡纳蒂克音乐模式使用…</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">github.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op nd ob"/></div></div></a></div></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="04a8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="ox">如果你还不是中会员，想支持我这样的作家，可以通过我的推荐链接随意报名:</em><a class="ae mb" href="https://skanda-vivek.medium.com/membership" rel="noopener"><em class="ox">【https://skanda-vivek.medium.com/membership】</em></a></p><p id="788d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><a class="ae mb" href="https://medium.com/@skanda.vivek" rel="noopener"> <em class="ox">关注我</em> </a> <em class="ox">如果你喜欢这篇文章——我经常在数据科学、音乐、安全和社会的界面上写作。</em></p><p id="277f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="ox">每周数据透视</em> <a class="ae mb" href="https://skandavivek.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ox">订阅此处</em> </a> <em class="ox">！</em></p><h2 id="2d37" class="nq kj it bd kk oy oz dn ko pa pb dp ks lj pc pd ku ln pe pf kw lr pg ph ky pi bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="1558" class="pj pk it lc b ld le lg lh lj pl ln pm lr pn lv po pp pq pr bi translated"><a class="ae mb" href="https://www.coursera.org/learn/nlp-sequence-models?" rel="noopener ugc nofollow" target="_blank"> <em class="ox">序列模型| Coursera </em> </a></li><li id="5c3e" class="pj pk it lc b ld ps lg pt lj pu ln pv lr pw lv po pp pq pr bi translated"><a class="ae mb" href="https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=Long%20short-term%20memory%20%28%20LSTM%29%20is%20an%20artificial,standard%20feedforward%20neural%20networks%2C%20LSTM%20has%20feedback%20connections." rel="noopener ugc nofollow" target="_blank"> <em class="ox">长短期记忆—维基百科</em> </a></li><li id="44dd" class="pj pk it lc b ld ps lg pt lj pu ln pv lr pw lv po pp pq pr bi translated"><a class="ae mb" href="https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab2/Part1_MNIST.ipynb#scrollTo=gKA_J7bdP33T" rel="noopener ugc nofollow" target="_blank"> <em class="ox">麻省理工6。S191:深度学习介绍</em> </a></li></ol></div></div>    
</body>
</html>