<html>
<head>
<title>How Neural Networks Actually Work — Python Implementation (Simplified)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的实际工作方式— Python实现(简化)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe#2022-05-26">https://towardsdatascience.com/how-neural-networks-actually-work-python-implementation-simplified-a1167b4f54fe#2022-05-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="9630" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于很多人来说，神经网络(NN)是一个黑匣子。我们知道它的工作原理，但我们不明白它是如何工作的。本文将通过一些例子来展示神经网络实际上是如何工作的，从而揭开这一信念的神秘面纱。如果这篇文章中有些术语不太清楚，我已经写了两篇文章来涵盖真正的基础知识:<a class="ae ko" rel="noopener" target="_blank" href="/the-basics-of-neural-networks-neural-network-series-part-1-4419e343b2b?source=your_stories_page-------------------------------------&amp;gi=2781638f7cb8">第一篇</a>和<a class="ae ko" rel="noopener" target="_blank" href="/feed-forward-neural-network-with-example-neural-network-series-part-2-eeca7a081ef5">第二篇</a>。</p><h1 id="7501" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">数据和架构</h1><p id="aa55" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在第一个示例中，让我们考虑一个简单的情况，其中我们有一个包含3个特征、一个目标变量和一个训练示例的数据集(实际上我们永远不会有这种数据，但是，这将是一个非常好的开始)。</p><p id="3df2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实1:数据的结构影响建模所选择的架构。事实上，<strong class="js iu">数据集上特征的数量等于输入层中神经元的数量</strong>。在我们的例子中，如下图所示，我们有3个特征，因此架构的输入层必须有3个神经元。</p><p id="8b0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">隐藏层的数量影响学习过程，因此根据应用来选择。具有几个隐藏层的网络被称为深度神经网络(DNN)。在这个例子中，我们正在处理一个浅层神经网络。我们在输入/第一层有3个神经元，在隐藏层有4个神经元，在输出层有1个神经元，这是一个3–4–1神经网络。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/c1033328a5d7361d0c6b481957c38256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGSgEarAmMZJMhNCn-jEBQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图1 —左:数据，右:神经网络架构(来源:作者)。</p></figure><h1 id="a607" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">记号</h1><p id="a858" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">让我们先设置一些符号，这样我们就可以毫不含糊地使用它们。为此，让我们重新绘制上面的体系结构，以显示神经网络中使用的参数和变量。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mi"><img src="../Images/fa3c074fc5dbc9db4dae45fdb824b9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5C37x5R5KFJMA-NuwGoKQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图2:带有变量和参数的3–4–1神经网络我们需要在给定输入值的情况下计算输出ŷ(来源:作者)。</p></figure><p id="5c27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是我们将使用的符号:</p><ul class=""><li id="6cdd" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">x⁰ᵢ-iᵗʰ输入值。输入层(层0)的特征I的值，</li><li id="3d46" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">nˡ——l层中的神经元数量。如图2所示，第一/输入层有三个神经元，n⁰ =3，第二/隐藏层有4个单元，因此n =4，最后，对于输出层，n =1。网络中有L层。在我们的例子中，L=2(记住，我们不把输入算作一个层，因为这里不发生任何计算)，</li><li id="9c1b" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">m——训练样本的数量。在我们的例子中，m=1，</li><li id="698e" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">wˡⱼᵢ <strong class="js iu"/></li><li id="a843" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">第一层中jᵗʰ神经元的zˡⱼ加权输入，</li><li id="b274" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">f <strong class="js iu"> ˡ </strong> ⱼ <strong class="js iu">gˡ是层l的激活函数，</strong></li><li id="8250" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">bˡⱼ<strong class="js iu"/>——l层jᵗʰ神经元上的偏置</li></ul></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="4d1f" class="kp kq it bd kr ks ne ku kv kw nf ky kz la ng lc ld le nh lg lh li ni lk ll lm bi translated">矩阵和矩阵运算</h1><p id="2007" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为了有效地进行神经元运算的计算，我们可以使用矩阵和向量作为任何给定层的输入、权重和偏差。我们现在将有以下矩阵/向量和运算:</p><ul class=""><li id="063a" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">输入为x，</li><li id="528c" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">权重作为矩阵wˡ到达层l，</li><li id="37b7" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">层1中的偏差为矢量bˡ，</li><li id="5ab8" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">zˡ = wˡ∙f^(l-1)—是层中所有神经元的加权输入，l . Medium.com在格式上有限制。f是来自前一层的输入的幂(l-1)。</li><li id="35ed" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">点积-对于任意两个等长的向量a和b，点积a∙b定义为:</li></ul><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e705824293e32c28c2df2677256ea00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*8yP6AsrEH8sf9yOjD1tLcw.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">等式1:点积a∙b(来源:作者)。</p></figure><p id="32d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">aᵢ* bᵢ是向量a中的iᵗʰ元素和b中的iᵗʰ元素之间的标量乘法</p><ul class=""><li id="4c46" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">f <strong class="js iu"> ˡ </strong> = gˡ(zˡ+bˡ)将是层l的输出</li></ul><h2 id="4c9b" class="nk kq it bd kr nl nm dn kv nn no dp kz kb np nq ld kf nr ns lh kj nt nu ll nv bi translated">矩阵乘法</h2><p id="f61c" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">两个矩阵只有兼容才能相乘。当且仅当第一个矩阵的列数等于第二个矩阵的行数时，称两个矩阵相乘是相容的。因此，对于A(m，n)和B(n，p)，那么A∙B的阶为(m，p)(见下图2)。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nw"><img src="../Images/29e57baa7a4d98b86018ae79c19d1a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAtxdYM5Lo-yKfPN-QrPAw.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图3: Multix乘法(来源:作者)。</p></figure><p id="55fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">重要提示:</strong>矩阵乘法是不可交换的，也就是A∙B不等于B∙A，要确定你是按照正确的顺序把它们相乘。</p><h2 id="b5b0" class="nk kq it bd kr nl nm dn kv nn no dp kz kb np nq ld kf nr ns lh kj nt nu ll nv bi translated">矩阵加法</h2><p id="310d" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">两个矩阵A和B只有在维数相同的情况下才能相加，比如说A(m，n)，那么B也一定是(m，n)阶的，结果A+B也是(m，n)维的。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="345c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的网络中利用矩阵运算需要我们为输入、权重和偏差找到合适的维度。</p><h1 id="64df" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">单个神经元的计算</h1><p id="4dda" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为了演示在一个神经元中完成的计算，让我们考虑在隐藏层的第一个神经元中的计算(注意，权重是随机生成的，并且在开始时将偏差设置为0)。在本节中，我们基本上假设我们有一个单神经元网络，即3–1个神经元。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nx"><img src="../Images/bd3d1123122758e44a97c68606b2f0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLF3iIO7sxk0KhQQ5ETTcA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">图4:单个神经元——隐藏层的第一个神经元(来源:作者)。</p></figure><p id="a08a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种情况下，我们有3个输入值5、6和6，以及相应的权重0.01789、0.00437和0.00096。神经元通过计算z ₁₁来衡量输入，加上偏差b ₁₁，最后，对结果应用激活函数(我们将使用sigmoid函数)来获得f ₁₁，从而产生输出。</p><p id="40a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">加权输入:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ny"><img src="../Images/28981e419efc13b66c14c43f7c6d6eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgI4FAx1bKXxlgGo7CPAhQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">等式2:加权输入是相应输入值和权重的乘积之和。这相当于1×3权重矩阵和3×1输入矩阵/向量之间的点积。</p></figure><p id="428c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe nz oa ob oc b">z¹₁₁ = (0.01789*5) + (0.00437*6) + (0.00096*6) = 0.12143</code></p><p id="31f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">添加偏置，然后应用sigmoid激活函数:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9ea03fe59ace4da1f52c8b7565bdcd37.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*_JLXbLq4Gwfno3NKUxKGcg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">Sigmoid激活函数</p></figure><p id="1c6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe nz oa ob oc b">f¹₁₁ = g(z¹₁₁+b¹₁₁) = g(0.12143+0) = 1÷(1+e^(−0.12143)) = <strong class="js iu">0.530320253</strong></code></p><p id="1919" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个神经元输出<strong class="js iu"> 0.5303。</strong></p><h2 id="b2f1" class="nk kq it bd kr nl nm dn kv nn no dp kz kb np nq ld kf nr ns lh kj nt nu ll nv bi translated">把这个放到Python代码中</h2><p id="e80c" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">首先，我们需要随机生成权重，并将bias设置为零。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="2a6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="lt lu lv lw gt og oc oh oi aw oj bi"><span id="6836" class="nk kq it oc b gy ok ol l om on">Weights:  [[0.01789 0.00437 0.00096]]<br/>Weights shape:  (1, 3)<br/>Bias:  [[0.]]<br/>Bias shape:  (1, 1)</span></pre><p id="b852" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第1–4行</strong>:导入numpy，设置其输出值的精度为小数点后5位。除非明确设置了另一个精度级别，否则numpy在第3行后执行的所有计算都将保留5位小数。</p><p id="0e1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第5–8行</strong>:在第6行，一个随机种子被设置为3(可以是任何数字)。播种确保生成的随机数是可预测的，即每次运行代码时生成的随机数都是相同的。第8行实际上使用标准的正态分布生成随机数。为什么要乘以0.01？我们希望该数字较小(介于0和1之间)。我们将在下一篇文章中对此进行更多的讨论</p><p id="2c70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第11–14行</strong>只打印出矩阵的参数值和尺寸(参见下文，了解权重和偏差矩阵形状的调整，非常重要！！！)</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="a1b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">&gt;&gt;&gt;尺寸重要说明&lt;&lt;&lt;</p><p id="25a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">We need to get dimensions of weights and biases matrices correctly. Given the number of neurons in each layer, the dimension for the matrices can be deduced as:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d03150a3bfda2e9e2c4f5030782959a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*26kekwGdstqXxglSm0fBFg.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">Dimensions for matrices of weights and biases (Source: Author).</p></figure><ul class=""><li id="4a9d" class="mj mk it js b jt ju jx jy kb ml kf mm kj mn kn mo mp mq mr bi translated">The weights matrix for layer l (wˡ) will be of dimension (nˡ, n^(l-1)), that is, wˡ will have the number of rows equal to the number of neurons in the current layer, l, and the number of columns will be equal to the number of neurons in the previous layer, l-1.</li><li id="a8a6" class="mj mk it js b jt ms jx mt kb mu kf mv kj mw kn mo mp mq mr bi translated">The bias matrix will have nˡ rows and 1 column. This is just a vector.</li></ul><p id="2b2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">In our network, we have n⁰=3 input values and n¹ =1 neuron in the output, therefore, dim(w¹) = (n¹, n⁰) = (1, 3) and dim(b¹) = (n¹, 1) = (1,1). Hope this justifies the choice of shapes for the matrices of weights and biases.</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="b5b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Next is to multiply weights with input, add bias and apply sigmoid function.</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="730b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="lt lu lv lw gt og oc oh oi aw oj bi"><span id="1c10" class="nk kq it oc b gy ok ol l om on">input shape:  (3, 1)<br/>f11:  [[0.12141]]<br/>The output:  [[0.53032]]</span></pre><p id="0319" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第1–4行</strong>定义输入并打印形状。如果不对输入进行整形，结果将是shape (3，)的数组。应该避免称为秩1数组的(*，)性质的数组，因为它们会在矩阵乘法过程中引起问题。reshape(-1，1)函数告诉numpy根据给定的值的数量动态设置数组的行数并保持列数为1。在我们的例子中，我们有3个输入值，因此我们有(3，1)作为输入形状。</p><p id="f2a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">第5行到第13行</strong> —将权重矩阵与输入矩阵相乘以获得加权输入并添加偏差，然后对结果应用sigmoid激活。当乘以第6行中的矩阵时，注意顺序——TT是权重矩阵乘以输入，而不是相反(还记得上一节中的矩阵乘法规则吗？).</p><p id="dd2a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">并且输出为<strong class="js iu"> 0.5303 </strong>。注意，它与我们之前得到的相匹配。</p><h1 id="047d" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">将代码放在一起</h1><p id="0a2b" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">现在让我们把所有这些放在一起</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="3b08" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">输出:</strong></p><pre class="lt lu lv lw gt og oc oh oi aw oj bi"><span id="9b54" class="nk kq it oc b gy ok ol l om on">Weights:  [[0.01789 0.00437 0.00096]]<br/>Weights shape:  (1, 3)<br/>Bias:  [[0.]]<br/>Bias shape:  (1, 1)<br/>input values:  [[5]<br/> [6]<br/> [6]]<br/>input shape:  (3, 1)<br/>output [[0.53032]]</span></pre><p id="dcc6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">OneNeuronNeuralNetwork类有两个参数——权重和偏差——由magic/dunder函数__init__初始化。</p><blockquote class="op oq or"><p id="0601" class="jq jr os js b jt ju jv jw jx jy jz ka ot kc kd ke ou kg kh ki ov kk kl km kn im bi translated">每次从类中创建对象时，都会调用__init__函数。__init__方法<strong class="js iu">让类初始化对象的属性，没有其他用途——来源:</strong><a class="ae ko" href="https://www.udacity.com/blog/2021/11/__init__-in-python-an-overview.html" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">Udacity.com</strong></a></p></blockquote><p id="c168" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用sigmoid_function作为静态方法，因为它是一个普通的函数，不必接收第一个参数“self”。我们只是把它放在课堂上，因为它与这里的内容相关，否则就不是强制性的了。</p><h1 id="8489" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">正向传递中整个网络的计算</h1><p id="b0d6" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们在上一节已经讨论了单个神经元的计算。在下一篇文章的<a class="ae ko" rel="noopener" target="_blank" href="/how-neural-networks-actually-work-python-implementation-part-2-simplified-80db0351db45">中，我们将讨论在给定一些数据的情况下，整个网络如何执行单次转发。</a></p><p id="7046" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请<a class="ae ko" href="https://medium.com/@kiprono_65591/membership" rel="noopener">以每月5美元的价格注册成为medium会员</a>，以便能够在medium上阅读我和其他作者的所有文章。</p><p id="bef1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你也可以<a class="ae ko" href="https://medium.com/subscribe/@kiprono_65591" rel="noopener">订阅，以便在我发表文章时将我的文章放入你的邮箱</a>。</p><p id="7ecc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读，欢迎下次光临！！！</p></div></div>    
</body>
</html>